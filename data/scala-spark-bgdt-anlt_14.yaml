- en: Time to Put Some Order - Cluster Your Data with Spark MLlib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 是时候整理一下了-用Spark MLlib对数据进行聚类
- en: '*"If you take a galaxy and try to make it bigger, it becomes a cluster of galaxies,
    not a galaxy. If you try to make it smaller than that, it seems to blow itself
    apart"*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*"如果你拿一个星系并试图让它变得更大，它就会成为一群星系，而不是一个星系。如果你试图让它变得比那小，它似乎会自己爆炸"*'
- en: '- Jeremiah P. Ostriker'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- Jeremiah P. Ostriker'
- en: 'In this chapter, we will delve deeper into machine learning and find out how
    we can take advantage of it to cluster records belonging to a certain group or
    class for a dataset of unsupervised observations. In a nutshell, the following
    topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入研究机器学习，并找出如何利用它来对无监督观测数据集中属于某一组或类的记录进行聚类。简而言之，本章将涵盖以下主题：
- en: Unsupervised learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Clustering techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类技术
- en: Hierarchical clustering (HC)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类（HC）
- en: Centroid-based clustering (CC)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于质心的聚类（CC）
- en: Distribution-based clustering (DC)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于分布的聚类（DC）
- en: Determining number of clusters
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定聚类数量
- en: A comparative analysis between clustering algorithms
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类算法之间的比较分析
- en: Submitting jobs on computing clusters
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算集群上提交作业
- en: Unsupervised learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In this section, we will provide a brief introduction to unsupervised machine
    learning technique with appropriate examples. Let's start the discussion with
    a practical example. Suppose you have a large collection of not-pirated-totally-legal
    mp3s in a crowded and massive folder on your hard drive. Now, what if you can
    build a predictive model that helps automatically group together similar songs
    and organize them into your favorite categories such as country, rap, rock, and
    so on. This act of assigning an item to a group such that a mp3 to is added to
    the respective playlist in an unsupervised way. In the previous chapters, we assumed
    you're given a training dataset of correctly labeled data. Unfortunately, we don't
    always have that extravagance when we collect data in the real-world. For example,
    suppose we would like to divide up a large amount of music into interesting playlists.
    How could we possibly group together songs if we don't have direct access to their
    metadata? One possible approach could be a mixture of various machine learning
    techniques, but clustering is often at the heart of the solution.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将用适当的示例简要介绍无监督机器学习技术。让我们从一个实际例子开始讨论。假设你在硬盘上有一个拥挤而庞大的文件夹里有大量非盗版-完全合法的mp3。现在，如果你可以建立一个预测模型，帮助自动将相似的歌曲分组并组织到你喜欢的类别中，比如乡村音乐、说唱、摇滚等。这种将项目分配到一个组中的行为，例如将mp3添加到相应的播放列表，是一种无监督的方式。在之前的章节中，我们假设你有一个正确标记数据的训练数据集。不幸的是，在现实世界中收集数据时，我们并不总是有这种奢侈。例如，假设我们想将大量音乐分成有趣的播放列表。如果我们没有直接访问它们的元数据，我们如何可能将歌曲分组在一起呢？一种可能的方法可能是混合各种机器学习技术，但聚类通常是解决方案的核心。
- en: In short, iIn unsupervised machine learning problem, correct classes of the
    training dataset are not available or unknown. Thus, classes have to be deduced
    from the structured or unstructured datasets as shown in *Figure 1*. This essentially
    implies that the goal of this type of algorithm is to preprocess the data in some
    structured ways. In other words, the main objective of the unsupervised learning
    algorithms is to explore the unknown/hidden patterns in the input data that are
    unlabeled*.* Unsupervised learning, however, also comprehends other techniques
    to explain the key features of the data in an exploratory way toward finding the
    hidden patterns. To overcome this challenge, clustering techniques are used widely
    to group unlabeled data points based on certain similarity measures in an unsupervised
    way.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在无监督机器学习问题中，训练数据集的正确类别不可用或未知。因此，类别必须从结构化或非结构化数据集中推导出来，如*图1*所示。这基本上意味着这种算法的目标是以某种结构化的方式预处理数据。换句话说，无监督学习算法的主要目标是探索未标记的输入数据中的未知/隐藏模式。然而，无监督学习也包括其他技术，以探索性的方式解释数据的关键特征，以找到隐藏的模式。为了克服这一挑战，聚类技术被广泛使用，以无监督的方式基于某些相似性度量对未标记的数据点进行分组。
- en: 'For an in-depth theoretical knowledge of how unsupervised algorithms work,
    please refer to the following three books: *Bousquet*, *O.; von Luxburg*, *U.;
    Raetsch*, *G., eds* (2004). *Advanced Lectures on Machine Learning*. *Springer-Verlag*.
    ISBN 978-3540231226\. Or *Duda*, *Richard O.*; *Hart*, *Peter E.*; *Stork*, *David
    G*. (2001). *Unsupervised Learning and Clustering*. *Pattern classification* (2nd
    Ed.). *Wiley*. ISBN 0-471-05669-3 and *Jordan*, *Michael I.*; *Bishop*, *Christopher
    M*. (2004) *Neural Networks*. In *Allen B. Tucker* *Computer Science Handbook,
    Second Edition* (Section VII: Intelligent Systems). *Boca Raton*, FL: Chapman
    and Hall/CRC Press LLC. ISBN 1-58488-360-X.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有关无监督算法工作原理的深入理论知识，请参考以下三本书：*Bousquet*，*O.；von Luxburg*，*U.；Raetsch*，*G.，编辑（2004）。*机器学习的高级讲座*。*Springer-Verlag*。ISBN
    978-3540231226。或者*Duda*，*Richard O.*；*Hart*，*Peter E.*；*Stork*，*David G*。（2001）。*无监督学习和聚类*。*模式分类*（第2版）。*Wiley*。ISBN
    0-471-05669-3和*Jordan*，*Michael I.*；*Bishop*，*Christopher M*。（2004）*神经网络*。在*Allen
    B. Tucker* *计算机科学手册，第二版*（第VII部分：智能系统）。*博卡拉顿*，FL：查普曼和霍尔/ CRC出版社。ISBN 1-58488-360-X。
- en: '![](img/00263.jpeg)**Figure 1:** Unsupervised learning with Spark'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00263.jpeg)**图1：**使用Spark进行无监督学习'
- en: Unsupervised learning example
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习示例
- en: 'In clustering tasks, an algorithm groups related features into categories by
    analyzing similarities between input examples where similar features are clustered
    and marked using circles around. Clustering uses include but are not limited to
    the following: search result grouping such as grouping customers, anomaly detection
    for suspicious pattern finding, text categorization for finding useful pattern
    in tests, social network analysis for finding coherent groups, data center computing
    clusters for finding a way to put related computers together, astronomic data
    analysis for galaxy formation, and real estate data analysis to identify neighborhoods
    based on similar features. We will show a Spark MLlib-based solution for the last
    use cases.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类任务中，算法通过分析输入示例之间的相似性将相关特征分组到类别中，其中相似的特征被聚类并用圆圈标记。聚类的用途包括但不限于以下内容：搜索结果分组，如客户分组，用于发现可疑模式的异常检测，用于在文本中找到有用模式的文本分类，用于找到连贯群体的社交网络分析，用于将相关计算机放在一起的数据中心计算集群，用于基于相似特征识别社区的房地产数据分析。我们将展示一个基于Spark
    MLlib的解决方案，用于最后一种用例。
- en: Clustering techniques
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类技术
- en: In this section, we will discuss clustering techniques along with challenges
    and suitable examples. A brief overview of hierarchical clustering, centroid-based
    clustering, and distribution-based clustering will be provided too.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论聚类技术以及相关挑战和适当的示例。还将提供对层次聚类、基于质心的聚类和基于分布的聚类的简要概述。
- en: Unsupervised learning and the clustering
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习和聚类
- en: Clustering analysis is about dividing data samples or data points and putting
    them into corresponding homogeneous classes or clusters. Thus a trivial definition
    of clustering can be thought as the process of organizing objects into groups
    whose members are similar in some way.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析是关于将数据样本或数据点分成相应的同类或簇的过程。因此，聚类的一个简单定义可以被认为是将对象组织成成员在某种方式上相似的组。
- en: A *cluster* is, therefore, a collection of objects that are *similar* between
    them and are *dissimilar* to the objects belonging to other clusters. As shown
    in *Figure 2*, if a collection of objects is given, clustering algorithms put
    those objects into a group based on similarity. A clustering algorithm such as
    K-means has then located the centroid of the group of data points. However, to
    make the clustering accurate and effective, the algorithm evaluates the distance
    between each point from the centroid of the cluster. Eventually, the goal of clustering
    is to determine the intrinsic grouping in a set of unlabeled data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*簇*是一组对象，它们在彼此之间是*相似*的，并且与属于其他簇的对象是*不相似*的。如*图2*所示，如果给定一组对象，聚类算法会根据相似性将这些对象放入一组中。例如，K均值这样的聚类算法已经找到了数据点组的质心。然而，为了使聚类准确和有效，算法评估了每个点与簇的质心之间的距离。最终，聚类的目标是确定一组未标记数据中的内在分组。
- en: '![](img/00059.jpeg)**Figure 2:** Clustering raw data'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00059.jpeg)**图2：** 聚类原始数据'
- en: Spark supports many clustering algorithms such as **K-means**, **Gaussian mixture**,
    **power iteration clustering** (**PIC**), l**atent dirichlet allocation** (**LDA**),
    **bisecting K-means**, and **Streaming K-means**. LDA is used for document classification
    and clustering commonly used in text mining. PIC is used for clustering vertices
    of a graph consisting of pairwise similarities as edge properties. However, to
    keep the objective of this chapter clearer and focused, we will confine our discussion
    to the K-means, bisecting K-means, and Gaussian mixture algorithms.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持许多聚类算法，如**K均值**，**高斯混合**，**幂迭代聚类**（**PIC**），**潜在狄利克雷分配**（**LDA**），**二分K均值**和**流式K均值**。LDA用于文档分类和文本挖掘中常用的聚类。PIC用于将具有成对相似性的图的顶点聚类为边属性。然而，为了使本章的目标更清晰和集中，我们将限制我们的讨论在K均值，二分K均值和高斯混合算法上。
- en: Hierarchical clustering
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: The hierarchical clustering technique is based on the fundamental idea of objects
    or features that are more related to those nearby than others far away. Bisecting
    K-means is an example of such hierarchical clustering algorithm that connects
    data objects to form clusters based on their corresponding distance.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类技术基于一个基本思想，即对象或特征与附近的对象比与远处的对象更相关。二分K均值就是这样一种层次聚类算法的例子，它根据它们的相应距离连接数据对象以形成簇。
- en: In the hierarchical clustering technique, a cluster can be described trivially
    by the maximum distance needed to connect parts of the cluster. As a result, different
    clusters will be formed at different distances. Graphically, these clusters can
    be represented using a dendrogram. Interestingly, the common name hierarchical
    clustering evolves from the concept of the dendrogram.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在层次聚类技术中，一个簇可以通过连接簇的部分所需的最大距离来简单描述。因此，不同的簇将在不同的距离下形成。从图形上看，这些簇可以使用树状图来表示。有趣的是，常见的名字层次聚类来源于树状图的概念。
- en: Centroid-based clustering
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于质心的聚类
- en: In centroid-based clustering technique, clusters are represented by a central
    vector. However, the vector itself may not necessarily be a member of the data
    points. In this type of learning, a number of the probable clusters must be provided
    prior to training the model. K-means is a very famous example of this learning
    type, where, if you set the number of clusters to a fixed integer to say K, the
    K-means algorithm provides a formal definition as an optimization problem, which
    is a separate problem to be resolved to find the K cluster centers and assign
    the data objects the nearest cluster center. In short, this is an optimization
    problem where the objective is to minimize the squared distances from the clusters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于质心的聚类技术中，聚类由一个中心向量表示。然而，这个向量本身不一定是数据点的成员。在这种类型的学习中，必须在训练模型之前提供一些可能的聚类。K均值是这种学习类型的一个非常著名的例子，如果将聚类的数量设置为一个固定的整数K，K均值算法提供了一个正式的定义作为一个优化问题，这是一个单独的问题，需要解决以找到K个聚类中心并将数据对象分配给最近的聚类中心。简而言之，这是一个优化问题，其目标是最小化聚类的平方距离。
- en: Distribution-based clustestering
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于分布的聚类
- en: Distribution-based clustering algorithms are based on statistical distribution
    models that provide more convenient ways to cluster related data objects to the
    same distribution. Although the theoretical foundations of these algorithms are
    very robust, they mostly suffer from overfitting. However, this limitation can
    be overcome by putting constraints on the model complexity.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基于分布的聚类算法基于提供更方便的方式将相关数据对象聚类到相同分布的统计分布模型。尽管这些算法的理论基础非常健全，但它们大多数时候会受到过拟合的影响。然而，这种限制可以通过对模型复杂性加以约束来克服。
- en: Centroid-based clustering (CC)
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于质心的聚类（CC）
- en: In this section, we discuss the centroid-based clustering technique and its
    computational challenges. An example of using K-means with Spark MLlib will be
    shown for a better understanding of the centroid-based clustering.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论基于质心的聚类技术及其计算挑战。我们将展示使用Spark MLlib的K均值的示例，以更好地理解基于质心的聚类。
- en: Challenges in CC algorithm
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CC算法中的挑战
- en: As discussed previously, in a centroid-based clustering algorithm like K-means,
    setting the optimal value of the number of clusters K is an optimization problem.
    This problem can be described as NP-hard (that is non-deterministic polynomial-time
    hard) featuring high algorithmic complexities, and thus the common approach is
    trying to achieve only an approximate solution. Consequently, solving these optimization
    problems imposes an extra burden and consequently nontrivial drawbacks. Furthermore,
    the K-means algorithm expects that each cluster has approximately similar size.
    In other words, data points in each cluster have to be uniform to get better clustering
    performance.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在像K均值这样的基于质心的聚类算法中，设置聚类数量K的最佳值是一个优化问题。这个问题可以被描述为NP-hard（即非确定性多项式时间难题），具有高算法复杂性，因此常见的方法是尝试只获得一个近似解。因此，解决这些优化问题会带来额外的负担，因此也会带来非平凡的缺点。此外，K均值算法期望每个聚类的大小大致相似。换句话说，每个聚类中的数据点必须是均匀的，以获得更好的聚类性能。
- en: Another major drawback of this algorithm is that this algorithm tries to optimize
    the cluster centers but not cluster borders, and this often tends to inappropriately
    cut the borders in between the clusters. However, sometimes, we can have the advantage
    of visual inspection, which is often not available for data on hyperplanes or
    multidimensional data. Nonetheless, a complete section on how to find the optimal
    value of K will be discussed later in this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的另一个主要缺点是，这个算法试图优化聚类中心，而不是聚类边界，这经常会导致不恰当地切割聚类之间的边界。然而，有时我们可以利用视觉检查的优势，这在超平面或多维数据上通常是不可用的。尽管如此，如何找到K的最佳值的完整部分将在本章后面讨论。
- en: How does K-means algorithm work?
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K均值算法是如何工作的？
- en: 'Suppose we have *n* data points *x[i]*, *i=1...n* that need to be partitioned
    into *k* clusters. Now that the target here is to assign a cluster to each data
    point. K-means then aims to find the positions *μ[i],i=1...k* of the clusters
    that minimize the distance from the data points to the cluster. Mathematically,
    the K-means algorithm tries to achieve the goal by solving the following equation,
    that is, an optimization problem:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有*n*个数据点*x[i]*，*i=1...n*，需要被分成*k*个聚类。现在目标是为每个数据点分配一个聚类。K均值的目标是找到最小化数据点到聚类的距离的聚类位置*μ[i],i=1...k*。从数学上讲，K均值算法试图通过解决以下方程来实现目标，即一个优化问题：
- en: '![](img/00320.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00320.jpeg)'
- en: In the preceding equation, *c[i]* is the set of data points assigned to cluster
    *i*, and *d(x,μ[i]) =||x−μ[i]||²[2]* is the Euclidean distance to be calculated
    (we will explain why we should use this distance measurement shortly). Therefore,
    we can understand that the overall clustering operation using K-means is not a
    trivial one but an NP-hard optimization problem. This also means that the K-means
    algorithm not only tries to find the global minima but also often gets stuck in
    different solutions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，*c[i]*是分配给聚类*i*的数据点集合，*d(x,μ[i]) =||x−μ[i]||²[2]*是要计算的欧几里德距离（我们将很快解释为什么我们应该使用这个距离测量）。因此，我们可以理解，使用K均值进行整体聚类操作不是一个平凡的问题，而是一个NP-hard的优化问题。这也意味着K均值算法不仅试图找到全局最小值，而且经常陷入不同的解决方案。
- en: 'Now, let''s see how we could formulate the algorithm before we can feed the
    data to the K-means model. At first, we need to decide the number of tentative
    clusters, *k* priory. Then, typically, you need to follow these steps:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在将数据提供给K均值模型之前，我们如何制定算法。首先，我们需要事先决定试探性聚类的数量*k*。然后，通常情况下，您需要按照以下步骤进行：
- en: '![](img/00367.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00367.jpeg)'
- en: Here *|c|* is the number of elements in *c*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*|c|*是*c*中的元素数量。
- en: Clustering using the K-means algorithm begins by initializing all the coordinates
    to centroids. With every pass of the algorithm, each point is assigned to its
    nearest centroid based on some distance metric, usually *Euclidean distance*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用K-means算法进行聚类的过程是通过将所有坐标初始化为质心开始的。在算法的每一次迭代中，根据某种距离度量，通常是*欧氏距离*，将每个点分配给其最近的质心。
- en: '**Distance calculation:** Note that there are other ways to calculate the distance
    too, for example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**距离计算：**请注意，还有其他计算距离的方法，例如：'
- en: '*Chebyshev distance* can be used to measure the distance by considering only
    the most notable dimensions.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*切比雪夫距离*可用于仅考虑最显著维度的距离测量。'
- en: The *Hamming distance* algorithm can identify the difference between two strings.
    On the other hand, to make the distance metric scale-undeviating, *Mahalanobis
    distance* can be used to normalize the covariance matrix. The *Manhattan distance*
    is used to measure the distance by considering only axis-aligned directions. The
    *Minkowski distance* algorithm is used to make the Euclidean distance, Manhattan
    distance, and Chebyshev distance. The *Haversine distance* is used to measure
    the great-circle distances between two points on a sphere from the location, that
    is, longitudes and latitudes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*Hamming距离*算法可以识别两个字符串之间的差异。另一方面，为了使距离度量尺度无关，可以使用*马哈拉诺比斯距离*来规范化协方差矩阵。*曼哈顿距离*用于仅考虑轴对齐方向的距离。*闵可夫斯基距离*算法用于计算欧氏距离、曼哈顿距离和切比雪夫距离。*Haversine距离*用于测量球面上两点之间的大圆距离，即经度和纬度。'
- en: 'Considering these distance-measuring algorithms, it is clear that the Euclidean
    distance algorithm would be the most appropriate to solve our purpose of distance
    calculation in the K-means algorithm. The centroids are then updated to be the
    centers of all the points assigned to it in that iteration. This repeats until
    there is a minimal change in the centers. In short, the K-means algorithm is an
    iterative algorithm and works in two steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些距离测量算法，很明显，欧氏距离算法将是解决K-means算法中距离计算目的最合适的方法。然后，质心被更新为该迭代中分配给它的所有点的中心。这一过程重复，直到中心发生最小变化。简而言之，K-means算法是一个迭代算法，分为两个步骤：
- en: '**Cluster assignment step**: K-means goes through each of the m data points
    in the dataset which is assigned to a cluster that is represented by the closest
    of the k centroids. For each point, the distances to each centroid is then calculated
    and simply pick the least distant one.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类分配步骤**：K-means遍历数据集中的每个m个数据点，将其分配给由k个质心中最接近的质心表示的聚类。对于每个点，然后计算到每个质心的距离，简单地选择最近的一个。'
- en: '**Update step**: For each cluster, a new centroid is calculated as the mean
    of all points in the cluster. From the previous step, we have a set of points
    which are assigned to a cluster. Now, for each such set, we calculate a mean that
    we declare a new centroid of the cluster.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新步骤**：对于每个聚类，计算所有点的平均值作为新的聚类中心。从上一步，我们有一组分配给一个聚类的点。现在，对于每个这样的集合，我们计算一个平均值，宣布它为聚类的新中心。'
- en: An example of clustering using K-means of Spark MLlib
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark MLlib的K-means聚类的示例
- en: 'To further demonstrate the clustering example, we will use the *Saratoga NY
    Homes* dataset downloaded from [http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html](http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html)
    as an unsupervised learning technique using Spark MLlib. The dataset contains
    several features of houses located in the suburb of the New York City. For example,
    price, lot size, waterfront, age, land value, new construct, central air, fuel
    type, heat type, sewer type, living area, pct.college, bedrooms, fireplaces, bathrooms,
    and the number of rooms. However, only a few features have been shown in the following
    table:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步演示聚类示例，我们将使用从[http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html](http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html)下载的*Saratoga
    NY Homes*数据集作为使用Spark MLlib的无监督学习技术。该数据集包含纽约市郊区房屋的几个特征。例如，价格、地块大小、水景、年龄、土地价值、新建、中央空调、燃料类型、供暖类型、下水道类型、居住面积、大学百分比、卧室、壁炉、浴室和房间数。然而，以下表格中只显示了一些特征：
- en: '| **Price** | **Lot Size** | **Water Front** | **Age** | **Land Value** | **Rooms**
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **价格** | **地块大小** | **水景** | **年龄** | **土地价值** | **房间数** |'
- en: '| 132,500 | 0.09 | 0 | 42 | 5,000 | 5 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 132,500 | 0.09 | 0 | 42 | 5,000 | 5 |'
- en: '| 181,115 | 0.92 | 0 | 0 | 22,300 | 6 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 181,115 | 0.92 | 0 | 0 | 22,300 | 6 |'
- en: '| 109,000 | 0.19 | 0 | 133 | 7,300 | 8 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 109,000 | 0.19 | 0 | 133 | 7,300 | 8 |'
- en: '| 155,000 | 0.41 | 0 | 13 | 18,700 | 5 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 155,000 | 0.41 | 0 | 13 | 18,700 | 5 |'
- en: '| 86,060 | 0.11 | 0 | 0 | 15,000 | 3 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 86,060 | 0.11 | 0 | 0 | 15,000 | 3 |'
- en: '| 120,000 | 0.68 | 0 | 31 | 14,000 | 8 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 120,000 | 0.68 | 0 | 31 | 14,000 | 8 |'
- en: '| 153,000 | 0.4 | 0 | 33 | 23,300 | 8 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 153,000 | 0.4 | 0 | 33 | 23,300 | 8 |'
- en: '| 170,000 | 1.21 | 0 | 23 | 146,000 | 9 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 170,000 | 1.21 | 0 | 23 | 146,000 | 9 |'
- en: '| 90,000 | 0.83 | 0 | 36 | 222,000 | 8 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 90,000 | 0.83 | 0 | 36 | 222,000 | 8 |'
- en: '| 122,900 | 1.94 | 0 | 4 | 212,000 | 6 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 122,900 | 1.94 | 0 | 4 | 212,000 | 6 |'
- en: '| 325,000 | 2.29 | 0 | 123 | 126,000 | 12 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 325,000 | 2.29 | 0 | 123 | 126,000 | 12 |'
- en: '**Table 1:** Sample data from the Saratoga NY Homes dataset'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**表1：**Saratoga NY Homes数据集的样本数据'
- en: The target of this clustering technique here is to show an exploratory analysis
    based on the features of each house in the city for finding possible neighborhoods
    for the house located in the same area. Before performing feature extraction,
    we need to load and parse the Saratoga NY Homes dataset. This step also includes
    loading packages and related dependencies, reading the dataset as RDD, model training,
    prediction, collecting the local parsed data, and clustering comparing.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的聚类技术的目标是基于城市中每栋房屋的特征进行探索性分析，以找到可能的邻近区域。在执行特征提取之前，我们需要加载和解析Saratoga NY Homes数据集。这一步还包括加载包和相关依赖项，将数据集读取为RDD，模型训练，预测，收集本地解析数据以及聚类比较。
- en: '**Step 1**. Import-related packages:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1**。导入相关的包：'
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2\. Create a Spark session - the entry point** - Here we at first set
    the Spark configuration by setting the application name and master URL. For simplicity,
    it''s standalone with all the cores on your machine:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2.创建一个Spark会话 - 入口点** - 在这里，我们首先通过设置应用程序名称和主URL来设置Spark配置。为了简单起见，它是独立的，使用您机器上的所有核心：'
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step 3\. Load and parse the dataset** - Read, parse, and create RDDs from
    the dataset as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3.加载和解析数据集** - 从数据集中读取、解析和创建RDD如下：'
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that, to make the preceding code work, you should import the following
    package:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了使前面的代码起作用，您应该导入以下包：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will get the following output:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下输出：
- en: '![](img/00339.jpeg)**Figure 3:** A snapshot of the Saratoga NY Homes dataset'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00339.jpeg)**图3：** Saratoga NY Homes数据集的快照'
- en: 'The following is the `parseLand` method that is used to create a `Land` class
    from an array of `Double` as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`parseLand`方法，用于从`Double`数组创建`Land`类如下：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And the `Land` class that reads all the features as a double is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 读取所有特征作为双精度的`Land`类如下：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you already know, to train the K-means model, we need to ensure all the
    data points and features to be numeric. Therefore, we further need to convert
    all the data points to double as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您已经知道的，为了训练K均值模型，我们需要确保所有数据点和特征都是数字。因此，我们进一步需要将所有数据点转换为双精度，如下所示：
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Step 4\. Preparing the training set** - At first, we need to convert the
    data frame (that is, `landDF`) to an RDD of doubles and cache the data to create
    a new data frame to link the cluster numbers as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4.准备训练集** - 首先，我们需要将数据框（即`landDF`）转换为双精度的RDD，并缓存数据以创建一个新的数据框来链接集群编号如下：'
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now that we need to convert the preceding RDD of doubles into an RDD of dense
    vectors as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要将前面的双精度RDD转换为密集向量的RDD如下：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Step 5\. Train the K-means model** - Train the model by specifying 10 clusters,
    20 iterations, and 10 runs as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5.训练K均值模型** - 通过指定10个集群、20次迭代和10次运行来训练模型如下：'
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The Spark-based implementation of K-means starts working by initializing a set
    of cluster centers using the K-means algorithm by *Bahmani et al.*, *Scalable
    K-Means++*, VLDB 2012\. This is a variant of K-means++ that tries to find dissimilar
    cluster centers by starting with a random center and then doing passes where more
    centers are chosen with a probability proportional to their squared distance to
    the current cluster set. It results in a provable approximation to an optimal
    clustering. The original paper can be found at [http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Spark的K均值实现通过使用*K-means++*算法初始化一组集群中心开始工作，这是由*Bahmani等人*在2012年的*可扩展K-Means++*中提出的一种K均值++的变体。这是一种试图通过从随机中心开始，然后进行更多中心的选择的传递，选择概率与它们到当前集群集的平方距离成比例的方法来找到不同的集群中心。它导致对最佳聚类的可证近似。原始论文可以在[http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf)找到。
- en: '**Step 6\. Evaluate the model error rate** - The standard K-means algorithm
    aims at minimizing the sum of squares of the distance between the points of each
    set, that is, the squared Euclidean distance, which is the WSSSE''s objective.
    The K-means algorithm aims at minimizing the sum of squares of the distance between
    the points of each set (that is, the cluster center). However, if you really wanted
    to minimize the sum of squares of the distance between the points of each set,
    you would end up with a model where each cluster is its own cluster center; in
    that case, that measure would be 0.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6.评估模型的错误率** - 标准的K均值算法旨在最小化每个集合点之间的距离的平方和，即平方欧几里得距离，这是WSSSE的目标。K均值算法旨在最小化每个集合点（即集群中心）之间的距离的平方和。然而，如果您真的想要最小化每个集合点之间的距离的平方和，您最终会得到一个模型，其中每个集群都是其自己的集群中心；在这种情况下，该度量将为0。'
- en: 'Therefore, once you have trained your model by specifying the parameters, you
    can evaluate the result by using **Within Set Sum of Squared Errors** (**WSSE**).
    Technically, it is something like the sum of the distances of each observation
    in each K cluster that can be computed as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一旦您通过指定参数训练了模型，就可以使用**平方误差和**（**WSSE**）来评估结果。从技术上讲，它类似于可以计算如下的每个K簇中每个观察的距离之和：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding model training set produces the value of WCSSS:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的模型训练集产生了WCSSS的值：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Step 7\. Compute and print the cluster centers** - At first, we get the prediction
    from the model with the ID so that we can link them back to other information
    related to each house. Note that we will use an RDD of rows that we prepared in
    step 4*:*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7.计算并打印集群中心** - 首先，我们从模型中获取带有ID的预测，以便我们可以将它们与与每栋房子相关的其他信息联系起来。请注意，我们将使用在步骤4中准备的行的RDD*:* '
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'However, it should be provided when a prediction is requested about the price.
    This should be done as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当需要关于价格的预测时，应该提供如下：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For better visibility and an exploratory analysis, convert the RDD to a DataFrame
    as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地可见性和探索性分析，将RDD转换为数据框如下：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This should produce the output shown in the following figure:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该产生以下图中显示的输出：
- en: '![](img/00044.gif)**Figure 4:** A snapshot of the clusters predicted'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00044.gif)**图4：** 预测集群的快照'
- en: 'Since there''s no distinguishable ID in the dataset, we represented the `Price`
    field to make the linking. From the preceding figure, you can understand where
    does a house having a certain price falls, that is, in which cluster. Now for
    better visibility, let''s join the prediction DataFrame with the original DataFrame
    to know the individual cluster number for each house:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集中没有可区分的ID，我们表示`Price`字段以进行链接。从前面的图中，您可以了解到具有特定价格的房屋属于哪个集群。现在为了更好地可见性，让我们将预测数据框与原始数据框连接起来，以了解每栋房子的个体集群编号：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should observe the output in the following figure:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该观察以下图中的输出：
- en: '![](img/00138.jpeg)**Figure 5:** A snapshot of the clusters predicted across
    each house'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00138.jpeg)**图5：** 预测每个房屋所在集群的快照'
- en: To make the analysis, we dumped the output in RStudio and generated the clusters
    shown in *Figure 6*. The R script can be found on my GitHub repositories at [https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics](https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics).
    Alternatively, you can write your own script and do the visualization accordingly.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行分析，我们将输出转储到RStudio中，并生成*图6*中显示的集群。R脚本可以在我的GitHub存储库[https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics](https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics)上找到。或者，您可以编写自己的脚本，并相应地进行可视化。
- en: '![](img/00142.jpeg)**Figure 6:** Clusters of the neighborhoods'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00142.jpeg)**图6：** 社区的集群'
- en: 'Now, for more extensive analysis and visibility, we can observe related statistics
    for each cluster. For example, below I printed thestatistics related to cluster
    3 and 4 in *Figure 8* and *Figure 9*, respectively:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了进行更广泛的分析和可见性，我们可以观察每个集群的相关统计数据。例如，下面我分别打印了*图8*和*图9*中与集群3和4相关的统计数据：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now get the descriptive statistics for each cluster as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在获取每个集群的描述性统计数据如下：
- en: '[PRE17]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'At first, let''s observe the related statistics of cluster 3 in the following
    figure:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们观察以下图中集群3的相关统计数据：
- en: '![](img/00353.jpeg)**Figure 7:** Statistics on cluster 3'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00353.jpeg)**图7：** 集群3的统计数据'
- en: 'Now let''s observe the related statistics of cluster 4 in the following figure:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们观察以下图中集群4的相关统计数据：
- en: '![](img/00377.jpeg)**Figure 8:** Statistics on cluster 4'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00377.jpeg)**图8：** 集群4的统计数据'
- en: Note that, since the original screenshot was too large to fit in this page,
    the original images were modified and the column containing other variables of
    the houses were removed.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于原始截图太大，无法放入此页面，因此对原始图像进行了修改，并删除了包含房屋其他变量的列。
- en: 'Due to the random nature of this algorithm, you might receive different results
    for each successful iteration. However, you can lock the random nature of this
    algorithm by setting the seed as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该算法的随机性质，每次成功迭代可能会得到不同的结果。但是，您可以通过以下方式设置种子来锁定该算法的随机性质：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Step 8\. Stop the Spark session** - Finally, stop the Spark session using
    the stop method as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8. 停止Spark会话** - 最后，使用stop方法停止Spark会话：'
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the preceding example, we dealt with a very small set of features; common-sense
    and visual inspection would also lead us to the same conclusions. From the above
    example using the K-means algorithm, we can understand that there are some limitations
    for this algorithm. For example, it's really difficult to predict the K-value,
    and with a global cluster it does not work well. Moreover, different initial partitions
    can result in different final clusters, and, finally, it does not work well with
    clusters of different sizes and densities.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们处理了一组非常小的特征；常识和视觉检查也会导致相同的结论。从上面使用K均值算法的例子中，我们可以理解这个算法的一些局限性。例如，很难预测K值，并且全局集群效果不佳。此外，不同的初始分区可能导致不同的最终集群，最后，它在不同大小和密度的集群中效果不佳。
- en: 'To overcome these limitations, we have some more robust algorithms in this
    book like MCMC (Markov Chain Monte Carlo; see also at [https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo))
    presented in the book: *Tribble*, *Seth D.*, **Markov chain Monte Carlo** algorithms
    using completely uniformly distributed driving sequences, Diss. Stanford University,
    2007.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些局限性，本书中还有一些更健壮的算法，如MCMC（马尔可夫链蒙特卡洛；也可参见[https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)）在书中介绍：*Tribble*,
    *Seth D.*, **Markov chain Monte Carlo** algorithms using completely uniformly
    distributed driving sequences, Diss. Stanford University, 2007.
- en: Hierarchical clustering (HC)
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类（HC）
- en: In this section, we discuss the hierarchical clustering technique and its computational
    challenges. An example of using the bisecting K-means algorithm of hierarchical
    clustering with Spark MLlib will be shown too for a better understanding of hierarchical
    clustering.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论层次聚类技术及其计算挑战。还将展示使用Spark MLlib的层次聚类的双分K均值算法的示例，以更好地理解层次聚类。
- en: An overview of HC algorithm and challenges
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HC算法概述和挑战
- en: 'A hierarchical clustering technique is computationally different from the centroid-based
    clustering in the way the distances are computed. This is one of the most popular
    and widely used clustering analysis technique that looks to build a hierarchy
    of clusters. Since a cluster usually consists of multiple objects, there will
    be other candidates to compute the distance too. Therefore, with the exception
    of the usual choice of distance functions, you also need to decide on the linkage
    criterion to be used. In short, there are two types of strategies in hierarchical
    clustering:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类技术在计算上与基于质心的聚类有所不同，距离的计算方式也不同。这是最受欢迎和广泛使用的聚类分析技术之一，旨在构建一个集群的层次结构。由于一个集群通常包含多个对象，也会有其他候选对象来计算距离。因此，除了通常选择的距离函数之外，您还需要决定要使用的链接标准。简而言之，层次聚类有两种策略：
- en: '**Bottom-up approach**: In this approach, each observation starts within its
    own cluster. After that, the pairs of clusters are merged together and one moves
    up the hierarchy.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自底向上方法**：在这种方法中，每个观察开始在自己的集群中。之后，将集群对合并在一起，然后向上移动层次结构。'
- en: '**Top-down approach**: In this approach, all observations start in one cluster,
    splits are performed recursively, and one moves down the hierarchy.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自顶向下方法**：在这种方法中，所有观察开始在一个集群中，递归地进行分裂，然后向下移动层次结构。'
- en: These bottom-up or top-down approaches are based on the s**ingle-linkage clustering**
    (**SLINK**) technique, which considers the minimum object distances, the **complete
    linkage clustering** (**CLINK**), which considers the maximum of object distances,
    and the u**nweighted pair group method with arithmetic mean** (**UPGMA**). The
    latter is also known as **average-linkage clustering**. Technically, these methods
    will not produce unique partitions out of the dataset (that is, different clusters).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些自底向上或自顶向下的方法基于**单链接聚类**（**SLINK**）技术，考虑最小对象距离，**完全链接聚类**（**CLINK**），考虑对象距离的最大值，以及**无权重对组平均法**（**UPGMA**）。后者也被称为**平均链接聚类**。从技术上讲，这些方法不会从数据集中产生唯一的分区（即不同的簇）。
- en: A comparative analysis on these three approaches can be found at [https://nlp.stanford.edu/IR-book/completelink.html.](https://nlp.stanford.edu/IR-book/completelink.html)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这三种方法的比较分析可以在[https://nlp.stanford.edu/IR-book/completelink.html](https://nlp.stanford.edu/IR-book/completelink.html)找到。
- en: 'However, the user still needs to choose appropriate clusters from the hierarchy
    for better cluster prediction and assignment. Although algorithms of this class
    like bisecting K-means are computationally faster than the K-means algorithm,
    there are three disadvantages to this type of algorithm:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，用户仍然需要从层次结构中选择适当的簇以获得更好的簇预测和分配。尽管像二分K-means这类的算法在计算上比K-means算法更快，但这种类型的算法有三个缺点：
- en: First, these methods are not very robust toward outliers or datasets containing
    noise or missing values. This disadvantage either imposes additional clusters
    or even causes other clusters to merge. This problem is commonly referred to as
    the chaining phenomenon, especially for single-linkage clustering.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，这些方法对异常值或包含噪声或缺失值的数据集不够稳健。这个缺点要么会导致额外的簇，要么甚至会导致其他簇合并。这个问题通常被称为链接现象，特别是对于单链接聚类。
- en: Second, from the algorithmic analysis, the complexity is for agglomerative clustering
    and for divisive clustering, which makes them too slow for large data sets.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，从算法分析来看，凝聚式聚类和分裂式聚类的复杂性分别为O(n^3)和O(n^2)，这使它们在处理大型数据集时过于缓慢。
- en: Third, SLINK and CLINK were previously used widely in data mining tasks as theoretical
    foundations of cluster analysis, but nowadays they are considered obsolete.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，SLINK和CLINK以前被广泛用于数据挖掘任务，作为聚类分析的理论基础，但现在被认为是过时的。
- en: Bisecting K-means with Spark MLlib
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark MLlib的二分K-means
- en: Bisecting K-means can often be much faster than regular K-means, but it will
    generally produce a different clustering. A bisecting K-means algorithm is based
    on the paper, *A comparison of document clustering* techniques by *Steinbach*,
    *Karypis*, and *Kumar*, with modification to fit with Spark MLlib.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 二分K-means通常比常规K-means快得多，但通常会产生不同的聚类。二分K-means算法基于*Steinbach*、*Karypis*和*Kumar*的论文《文档聚类技术的比较》，并进行了修改以适应Spark
    MLlib。
- en: Bisecting K-means is a kind of divisive algorithm that starts from a single
    cluster that contains all the data points. Iteratively, it then finds all the
    divisible clusters on the bottom level and bisects each of them using K-means
    until there are K leaf clusters in total or no leaf clusters divisible. After
    that, clusters on the same level are grouped together to increase the parallelism.
    In other words, bisecting K-means is computationally faster than the regular K-means
    algorithm. Note that if bisecting all the divisible clusters on the bottom level
    results in more than K leaf clusters, larger clusters will always get higher priority.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 二分K-means是一种分裂算法，从包含所有数据点的单个簇开始。然后，它迭代地在底层找到所有可分的簇，并使用K-means将它们中的每一个二分，直到总共有K个叶簇或没有可分的叶簇。之后，将同一级别的簇分组在一起以增加并行性。换句话说，二分K-means在计算上比常规K-means算法更快。请注意，如果在底层二分所有可分的簇导致超过K个叶簇，较大的簇将始终优先考虑。
- en: 'Note that if the bisecting of all the divisible clusters on the bottom level
    results in more than K leaf clusters, larger clusters will always get higher priority.
    The following parameters are used in the Spark MLlib implementation:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果在底层二分所有可分的簇导致超过K个叶簇，较大的簇将始终优先考虑。Spark MLlib实现中使用以下参数：
- en: '**K**: This is the desired number of leaf clusters. However, the actual number
    could be smaller if there are no divisible leaf clusters left during the computation.
    The default value is 4.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K**：这是期望的叶簇的数量。然而，如果在计算过程中没有可分的叶簇，则实际数量可能会更小。默认值为4。'
- en: '**MaxIterations**: This is the max number of K-means iterations to split the
    clusters. The default value is 20.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MaxIterations**：这是将簇分裂的K-means迭代的最大次数。默认值为20。'
- en: '**MinDivisibleClusterSize**: This is the minimum number of points. The default
    value is set as 1.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MinDivisibleClusterSize**：这是点的最小数量。默认值设置为1。'
- en: '**Seed**: This is a random seed that disallows random clustering and tries
    to provide almost similar result in each iteration. However, it is recommended
    to use a long seed value like 12345 and so on.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Seed**：这是一个随机种子，禁止随机聚类，并尝试在每次迭代中提供几乎相似的结果。但建议使用长种子值，如12345等。'
- en: Bisecting K-means clustering of the neighborhood using Spark MLlib
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark MLlib对邻域进行二分K-means聚类
- en: 'In the previous section, we saw how to cluster similar houses together to determine
    the neighborhood. The bisecting K-means is also similar to regular K-means except
    that the model training that takes different training parameters as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看到如何将相似的房屋聚类在一起以确定邻域。二分K-means与常规K-means类似，只是模型训练采用不同的训练参数，如下所示：
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should refer to the previous example and just reuse the previous steps
    to get the trained data. Now let''s evaluate clustering by computing WSSSE as
    follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该参考前面的示例，只需重复前面的步骤以获得训练数据。现在让我们通过计算WSSSE来评估聚类，如下所示：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You should observe the following output: `Within-Cluster Sum of Squares = 2.096980212594632E11`.
    Now for more analysis, please refer to step 5 in the previous section.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该观察到以下输出：`Within-Cluster Sum of Squares = 2.096980212594632E11`。现在，要进行更多分析，请参考上一节的第5步。
- en: Distribution-based clustering (DC)
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于分布的聚类（DC）
- en: In this section, we will discuss the distribution-based clustering technique
    and its computational challenges. An example of using **Gaussian mixture models**
    (**GMMs**) with Spark MLlib will be shown for a better understanding of distribution-based
    clustering.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论基于分布的聚类技术及其计算挑战。将展示使用Spark MLlib进行**高斯混合模型**（**GMMs**）的示例，以更好地理解基于分布的聚类。
- en: Challenges in DC algorithm
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DC算法中的挑战
- en: A distribution-based clustering algorithm like GMM is an expectation-maximization
    algorithm. To avoid the overfitting problem, GMM usually models the dataset with
    a fixed number of Gaussian distributions. The distributions are initialized randomly,
    and the related parameters are iteratively optimized too to fit the model better
    to the training dataset. This is the most robust feature of GMM and helps the
    model to be converged toward the local optimum. However, multiple runs of this
    algorithm may produce different results.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 像GMM这样的基于分布的聚类算法是一种期望最大化算法。为了避免过拟合问题，GMM通常使用固定数量的高斯分布对数据集进行建模。分布是随机初始化的，并且相关参数也经过迭代优化，以更好地适应训练数据集。这是GMM最健壮的特性，并有助于模型收敛到局部最优解。然而，多次运行该算法可能会产生不同的结果。
- en: In other words, unlike the bisecting K-means algorithm and soft clustering,
    GMM is optimized for hard clustering, and in order to obtain of that type, objects
    are often assigned to the Gaussian distribution. Another advantageous feature
    of GMM is that it produces complex models of clusters by capturing all the required
    correlations and dependence between data points and attributes.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，与二分K均值算法和软聚类不同，GMM针对硬聚类进行了优化，为了获得这种类型，对象通常被分配到高斯分布中。GMM的另一个有利特性是，它通过捕获数据点和属性之间的所有必要相关性和依赖关系来产生复杂的聚类模型。
- en: 'On the down-side, GMM has some assumptions about the format and shape of the
    data, and this puts an extra burden on us (that is, users). More specifically,
    if the following two criteria do not meet, performance decreases drastically:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 不利的一面是，GMM对数据的格式和形状有一些假设，这给我们（即用户）增加了额外的负担。更具体地说，如果以下两个标准不满足，性能会急剧下降：
- en: 'Non-Gaussian dataset: The GMM algorithm assumes that the dataset has an underlying
    Gaussian, which is generative distribution. However, many practical datasets do
    not satisfy this assumption that is subject to provide low clustering performance.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非高斯数据集：GMM算法假设数据集具有潜在的高斯生成分布。然而，许多实际数据集不满足这一假设，这会导致低聚类性能。
- en: If the clusters do not have even sizes, there is a high chance that small clusters
    will be dominated by larger ones.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果簇的大小不均匀，小簇被大簇主导的可能性很高。
- en: How does a Gaussian mixture model work?
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合模型是如何工作的？
- en: 'Using GMM is a popular technique of soft clustering. GMM tries to model all
    the data points as a finite mixture of Gaussian distributions; the probability
    that each point belongs to each cluster is computed along with the cluster related
    statistics and represents an amalgamate distribution: where all the points are
    derived from one of *K* Gaussian subdistributions having own probability. In short,
    the functionality of GMM can be described in a three-steps pseudocode:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GMM是一种流行的软聚类技术。GMM试图将所有数据点建模为有限数量的高斯分布的混合物；计算每个点属于每个簇的概率以及与簇相关的统计数据，并表示一个混合分布：其中所有点都来自于*K*个高斯子分布之一，具有自己的概率。简而言之，GMM的功能可以用三步伪代码来描述：
- en: '**Objective function:** Compute and maximize the log-likelihood using expectation–maximization
    (EM) as a framework'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**目标函数：**使用期望-最大化（EM）作为框架计算和最大化对数似然'
- en: '**EM algorithm:**'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**EM算法：**'
- en: '**E step:** Compute the posterior probability of membership -i.e. nearer data
    points'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**E步：**计算成员的后验概率-即更接近的数据点'
- en: '**M step:** Optimize the parameters.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M步：优化参数。
- en: '**Assignment:** Perform soft assignment during step E.'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分配：在步骤E期间执行软分配。
- en: Technically, when a statistical model is given, parameters of that model (that
    is, when applied to a data set) are estimated using the **maximum-likelihood estimation**
    (**MLE**). On the other hand, **EM** algorithm is an iterative process of finding
    maximum likelihood.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，当给定一个统计模型时，该模型的参数（即应用于数据集时）是使用**最大似然估计**（**MLE**）来估计的。另一方面，**EM**算法是一种找到最大似然的迭代过程。
- en: Since the GMM is an unsupervised algorithm, GMM model depends on the inferred
    variables. Then EM iteration rotates toward performing the expectation (E) and
    maximization (M) step.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GMM是一种无监督算法，GMM模型取决于推断变量。然后EM迭代旋转以执行期望（E）和最大化（M）步骤。
- en: 'The Spark MLlib implementation uses the expectation-maximization algorithm
    to induce the maximum-likelihood model from a given a set of data points. The
    current implementation uses the following parameters:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib实现使用期望最大化算法从给定的一组数据点中诱导最大似然模型。当前的实现使用以下参数：
- en: '**K** is the number of desired clusters to cluster your data points'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K**是要对数据点进行聚类的期望簇的数量'
- en: '**ConvergenceTol** is the maximum change in log-likelihood at which we consider
    convergence achieved.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ConvergenceTol**是我们认为达到收敛的对数似然的最大变化。'
- en: '**MaxIterations** is the maximum number of iterations to perform without reaching
    the convergence point.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MaxIterations**是在达到收敛点之前执行的最大迭代次数。'
- en: '**InitialModel** is an optional starting point from which to start the EM algorithm.
    If this parameter is omitted, a random starting point will be constructed from
    the data.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**InitialModel**是一个可选的起始点，从中开始EM算法。如果省略此参数，将从数据构造一个随机起始点。'
- en: An example of clustering using GMM with Spark MLlib
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark MLlib进行GMM聚类的示例
- en: 'In the previous sections, we saw how to cluster the similar houses together
    to determine the neighborhood. Using GMM, it is also possible to cluster the houses
    toward finding the neighborhood except the model training that takes different
    training parameters as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们看到了如何将相似的房屋聚集在一起以确定邻域。使用GMM，也可以将房屋聚类以找到邻域，除了模型训练，还需要不同的训练参数，如下所示：
- en: '[PRE22]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You should refer to the previous example and just reuse the previous steps
    of getting the trained data. Now to evaluate the model''s performance, GMM does
    not provide any performance metrics like WCSS as a cost function. However, GMM
    provides some performance metrics like mu, sigma, and weight. These parameters
    signify the maximum likelihood among different clusters (five clusters in our
    case). This can be demonstrated as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该参考前面的示例，只需重复获取训练数据的先前步骤。现在，为了评估模型的性能，GMM不提供任何性能指标，如WCSS作为成本函数。然而，GMM提供一些性能指标，如mu、sigma和weight。这些参数表示了不同簇（在我们的情况下为五个簇）之间的最大似然。这可以如下所示：
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You should observe the following output:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该观察以下输出：
- en: '![](img/00154.jpeg)**Figure 9:** Cluster 1![](img/00017.jpeg)**Figure 10:**
    Cluster 2![](img/00240.jpeg)**Figure 11:** Cluster 3![](img/00139.jpeg)**Figure
    12:** Cluster 4![](img/00002.jpeg)**Figure 13:** Cluster 5'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：聚类1 图10：聚类2 图11：聚类3 图12：聚类4 图13：聚类5
- en: The weight of clusters 1 to 4 signifies that these clusters are homogeneous
    and significantly different compared with cluster 5.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 簇1到4的权重表明，这些簇是同质的，并且与簇5相比显著不同。
- en: Determining number of clusters
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定聚类的数量
- en: The beauty of clustering algorithms like K-means algorithm is that it does the
    clustering on the data with an unlimited number of features. It is a great tool
    to use when you have a raw data and would like to know the patterns in that data.
    However, deciding the number of clusters prior to doing the experiment might not
    be successful but may sometimes lead to an overfitting or underfitting problem.
    On the other hand, one common thing to all three algorithms (that is, K-means,
    bisecting K-means, and Gaussian mixture) is that the number of clusters must be
    determined in advance and supplied to the algorithm as a parameter. Hence, informally,
    determining the number of clusters is a separate optimization problem to be solved.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 像K均值算法这样的聚类算法的美妙之处在于它可以对具有无限特征的数据进行聚类。当您有原始数据并想了解数据中的模式时，它是一个很好的工具。然而，在进行实验之前决定聚类的数量可能不成功，有时可能会导致过度拟合或拟合不足的问题。另一方面，所有三种算法（即K均值、二分K均值和高斯混合）的一个共同点是，聚类的数量必须事先确定并作为参数提供给算法。因此，非正式地说，确定聚类的数量是一个要解决的单独优化问题。
- en: In this section, we will use a heuristic approach based on the Elbow method.
    We start from K = 2 clusters, and then we ran the K-means algorithm for the same
    data set by increasing K and observing the value of cost function **Within-Cluster
    Sum of Squares** (**WCSS**). At some point, a big drop in cost function can be
    observed, but then the improvement became marginal with the increasing value of
    K. As suggested in cluster analysis literature, we can pick the K after the last
    big drop of WCSS as an optimal one.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用基于Elbow方法的一种启发式方法。我们从K = 2个簇开始，然后通过增加K并观察成本函数**簇内平方和**（**WCSS**）的值来运行相同数据集的K均值算法。在某个时候，可以观察到成本函数的大幅下降，但随着K值的增加，改进变得微不足道。正如聚类分析文献中建议的那样，我们可以选择WCSS的最后一个大幅下降后的K作为最佳值。
- en: 'By analysing below parameters, you can find out the performance of K-means:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析以下参数，您可以找出K均值的性能：
- en: '**Betweenness:** This is the between sum of squares also called as *intracluster
    similarity.*'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内部性：**这是平方和之间也称为*簇内相似度*。'
- en: '**Withiness:** This is the within sum of square also called *intercluster similarity.*'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内部性：**这是平方和之间也称为*簇内相似度*。'
- en: '**Totwithinss:** This is the sum of all the withiness of all the clusters also
    called *total intracluster similarity.*'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Totwithinss：**这是所有簇的内部性之和，也称为*总簇内相似度*。'
- en: It is to be noted that a robust and accurate clustering model will have a lower
    value of withiness and a higher value of betweenness. However, these values depend
    on the number of clusters, that is, K that is chosen before building the model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，一个健壮而准确的聚类模型将具有较低的内部性值和较高的内部性值。然而，这些值取决于聚类的数量，即在构建模型之前选择的K。
- en: Now let us discuss how to take advantage of the Elbow method to determine the
    number of clusters. As shown in the following, we calculated the cost function
    WCSS as a function of a number of clusters for the K-means algorithm applied to
    home data based on all the features. It can be observed that a big drop occurs
    when K = 5\. Therefore, we chose the number of clusters as 5, as shown in *Figure
    10*. Basically, this is the one after the last big drop.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论如何利用Elbow方法来确定聚类的数量。如下所示，我们计算了作为K均值算法应用于所有特征的家庭数据的聚类数量的成本函数WCSS。可以观察到当K
    = 5时有一个大幅下降。因此，我们选择了5个簇的数量，如*图10*所示。基本上，这是最后一个大幅下降后的一个。
- en: '![](img/00303.jpeg)**Figure 14:** Number of clusters as a function of WCSS'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：作为WCSS函数的聚类数量
- en: A comparative analysis between clustering algorithms
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法之间的比较分析
- en: 'Gaussian mixture is used mainly for expectation minimization, which is an example
    of an optimization algorithm. Bisecting K-means, which is faster than regular
    K-means, also produces slightly different clustering results. Below we try to
    compare these three algorithms. We will show a performance comparison in terms
    of model building time and the computional cost for each algorithm. As shown in
    the following code, we can compute the cost in terms of WCSS. The following lines
    of code can be used to compute the WCSS for the K-means and **b**isecting algorithms:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合主要用于期望最小化，这是优化算法的一个例子。比普通K-means更快的bisecting K-means也会产生略有不同的聚类结果。下面我们尝试比较这三种算法。我们将展示模型构建时间和每种算法的计算成本的性能比较。如下所示，我们可以计算WCSS的成本。以下代码行可用于计算K-means和**b**isecting算法的WCSS：
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'For the dataset we used throughout this chapter, we got the following values
    of WCSS:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中我们使用的数据集，我们得到了以下WCSS的值：
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This means that K-means shows slightly better performance in terms of the compute
    cost. Unfortunately, we don''t have any metrics like WCSS for the GMM algorithm.
    Now let''s observe the model building time for these three algorithms. We can
    start the system clock before starting model training and stop it immediately
    after the training has been finished as follows (for K-means):'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着K-means在计算成本方面表现略好一些。不幸的是，我们没有GMM算法的WCSS等指标。现在让我们观察这三种算法的模型构建时间。我们可以在开始模型训练之前启动系统时钟，并在训练结束后立即停止，如下所示（对于K-means）：
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For the training set we used throughout this chapter, we got the following
    values of model building time:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中我们使用的训练集，我们得到了以下模型构建时间的值：
- en: '[PRE27]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In different research articles, it has been found that the bisecting K-means
    algorithm has been shown to result in better cluster assignment for data points.
    Moreover, compared to K-means, bisecting K-means, alos converges well towards
    global minima. K-means on the other hand, gets stuck in local minima. In other
    words, using bisecting K-means algorithm, we can avoid the local minima that K-means
    can suffer from.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的研究文章中，已经发现，bisecting K-means算法已经被证明可以更好地为数据点分配聚类。此外，与K-means相比，bisecting
    K-means也更快地收敛到全局最小值。另一方面，K-means会陷入局部最小值。换句话说，使用bisecting K-means算法，我们可以避免K-means可能遭受的局部最小值。
- en: Note that you might observe different values of the preceding parameters depending
    upon your machine's hardware configuration and the random nature of the dataset.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，根据您的机器硬件配置和数据集的随机性质，您可能会观察到前述参数的不同值。
- en: More details analysis is up to the readers from the theoretical views. Interested
    readers should also refer to Spark MLlib-based clustering techniques at [https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html)
    to get more insights.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细的分析留给读者从理论角度来看。有兴趣的读者还应参考[https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html)中基于Spark
    MLlib的聚类技术，以获得更多见解。
- en: Submitting Spark job for cluster analysis
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提交Spark集群分析作业
- en: 'The examples shown in this chapter can be made scalable for the even larger
    dataset to serve different purposes. You can package all three clustering algorithms
    with all the required dependencies and submit them as a Spark job in the cluster.
    Now use the following lines of code to submit your Spark job of K-means clustering,
    for example (use similar syntax for other classes), for the Saratoga NY Homes
    dataset:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中展示的示例可以扩展到更大的数据集，以满足不同的目的。您可以将所有三种聚类算法与所有必需的依赖项打包，并将它们作为Spark作业提交到集群。现在使用以下代码行提交您的K-means聚类的Spark作业，例如（对其他类使用类似的语法），用于Saratoga
    NY Homes数据集：
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we delved even deeper into machine learning and found out how
    we can take advantage of machine learning to cluster records belonging to a dataset
    of unsupervised observations. Consequently, you learnt the practical know-how
    needed to quickly and powerfully apply supervised and unsupervised techniques
    on available data to new problems through some widely used examples based on the
    understandings from the previous chapters. The examples we are talking about will
    be demonstrated from the Spark perspective. For any of the K-means, bisecting
    K-means, and Gaussian mixture algorithms, it is not guaranteed that the algorithm
    will produce the same clusters if run multiple times. For example, we observed
    that running the K-means algorithm multiple times with the same parameters generated
    slightly different results at each run.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们更深入地研究了机器学习，并发现了如何利用机器学习来对无监督观测数据集中的记录进行聚类。因此，您学会了在可用数据上快速而有效地应用监督和无监督技术，以解决新问题的实际知识，这些知识是基于前几章的理解的一些广泛使用的示例。我们所说的示例将从Spark的角度进行演示。对于K-means、bisecting
    K-means和高斯混合算法中的任何一个，不能保证如果多次运行算法将产生相同的聚类。例如，我们观察到使用相同参数多次运行K-means算法会在每次运行时生成略有不同的结果。
- en: 'For a performance comparison between K-means and Gaussian mixture, see *Jung.
    et. al and cluster analysis* lecture notes. In addition to K-means, bisecting
    K-means, and Gaussian mixture, MLlib provides implementations of three other clustering
    algorithms, namely, PIC, LDA, and streaming K-means. One thing is also worth mentioning
    is that to fine tune clustering analysis, often we need to remove unwanted data
    objects called outlier or anomaly. But using distance based clustering it''s really
    difficult to identify such data pints. Therefore, other distance metrics other
    than Euclidean can be used. Nevertheless, these links would be a good resource
    to start with:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 有关K-means和高斯混合的性能比较，请参阅*Jung. et. al和聚类分析*讲义。除了K-means、bisecting K-means和高斯混合外，MLlib还提供了另外三种聚类算法的实现，即PIC、LDA和流式K-means。还值得一提的是，为了对聚类分析进行微调，通常需要删除不需要的数据对象，称为离群值或异常值。但是使用基于距离的聚类很难识别这样的数据点。因此，除了欧几里得距离之外，还可以使用其他距离度量。然而，这些链接将是开始的好资源：
- en: '[https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html](https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html)'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html](https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html)'
- en: '[https://github.com/keiraqz/anomaly-detection](https://github.com/keiraqz/anomaly-detection)'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://github.com/keiraqz/anomaly-detection](https://github.com/keiraqz/anomaly-detection)'
- en: '[http://www.dcc.fc.up.pt/~ltorgo/Papers/ODCM.pdf](https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html)'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://www.dcc.fc.up.pt/~ltorgo/Papers/ODCM.pdf](https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html)'
- en: In the next chapter, we will dig even deeper into tuning Spark applications
    for better performance. We will see some best practice to optimize the performance
    of Spark applications.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地挖掘调优Spark应用程序以获得更好性能的方法。我们将看到一些优化Spark应用程序性能的最佳实践。
