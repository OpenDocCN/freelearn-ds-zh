- en: Chapter 10. Advance Concepts in Storm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。风暴中的高级概念
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Building a Trident topology
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建Trident拓扑
- en: Understanding the Trident API
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Trident API
- en: Examples and illustrations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例和插图
- en: In this chapter, we will learn about transactional topologies and the Trident
    API. We will also explore the aspects of micro-batching and its implementation
    in Storm topology.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习事务性拓扑和Trident API。我们还将探讨微批处理的方面以及它在Storm拓扑中的实现。
- en: Building a Trident topology
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建Trident拓扑
- en: Trident gives a batching edge to the Storm computation. It lets developers use
    the abstracted layer for computations over the Storm framework, giving the advantage
    of stateful processing with high throughput for distributed queries.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Trident为Storm计算提供了批处理边缘。它允许开发人员在Storm框架上使用抽象层进行计算，从而在分布式查询中获得有状态处理和高吞吐量的优势。
- en: Well the architecture of Trident is the same as Storm; it's built on top of
    Storm to abstract a layer that adds the functionality of micro-batching and execution
    of SQL-like functions on top of Storm.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，Trident的架构与Storm相同；它是建立在Storm之上的，以在Storm之上添加微批处理功能和执行类似SQL的函数的抽象层。
- en: For the sake of analogy, one can say that Trident is a lot like Pig for batch
    processing in terms of concept. It has support for joins, aggregates, grouping,
    filters, functions, and so on.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了类比，可以说Trident在概念上很像Pig用于批处理。它支持连接、聚合、分组、过滤、函数等。
- en: Trident has basic batch processing features such as consistent processing and
    execution of process logic over the tuples exactly once.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Trident具有基本的批处理功能，例如一致处理和对元组的执行逻辑进行一次性处理。
- en: Now to understand Trident and its working; let's look at a simple example.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在要理解Trident及其工作原理；让我们看一个简单的例子。
- en: 'The example we have picked up would achieve the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择的例子将实现以下功能：
- en: Word count over the stream of sentences (a standard Storm word count kind of
    topology)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对句子流进行单词计数（标准的Storm单词计数拓扑）
- en: A query implementation to get the sum of counts for a set of listed words
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于获取一组列出的单词计数总和的查询实现
- en: 'Here is the code for dissection:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是解剖的代码：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now we have made sure about continuous input stream let''s look at the following
    snippet:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确保了连续的输入流，让我们看下面的片段：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This metadata information is stored for small batches wherein the batch size
    is a variant based on the speed of incoming tuples; it could be few hundred to
    millions of tuples based on the event **transactions per second** (**tps**).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些元数据信息存储在小批处理中，其中批处理大小是根据传入元组的速度变化的变量；它可以是几百到数百万个元组，具体取决于每秒的事件**事务数**（**tps**）。
- en: Now my spout reads and emits the stream into the field labeled as `sentence`.
    In the next line, we will split the sentence into words; that's the very same
    functionality that we deployed in our earlier reference to the `wordCount` topology.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我的喷口读取并将流发射到标记为`sentence`的字段中。在下一行，我们将句子分割成单词；这正是我们在前面提到的`wordCount`拓扑中部署的相同功能。
- en: 'The following is the code context capturing the working of the `split` functionality:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是捕捉`split`功能工作的代码上下文：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code snippet, we created a DRPC stream using `myTridentTopology`
    and over and above it, we have a function named `word`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用`myTridentTopology`创建了一个DRPC流，此外，我们还有一个名为`word`的函数。
- en: 'We split the argument stream into its constituent words; for example, my argument,
    `storm trident topology`, is split into individual words such as `storm`, `trident`,
    and `topology`*   Then the incoming stream is grouped by `word`*   Next, the state-query-operator
    is used to query the Trident-state-object that was generated by the first part
    of the topology:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将参数流分割成其组成的单词；例如，我的参数`storm trident topology`被分割成诸如`storm`、`trident`和`topology`等单词*
    然后，传入的流被按`word`分组* 接下来，状态查询操作符用于查询由拓扑的第一部分生成的Trident状态对象：
- en: State query takes in the word counts computed by an earlier section of the topology.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态查询接收拓扑先前部分计算的单词计数。
- en: It then executes the function as specified as part of the DRPC request to query
    the data.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后它执行作为DRPC请求的一部分指定的函数来查询数据。
- en: In this case, my topology is executing the `MapGet` function on the query to
    get the count of each word; the DRPC stream, in our case, is grouped in exactly
    the same manner as the `TridentState` in the preceding section of the topology.
    This arrangement guarantees that all my word count queries for each word are directed
    to the same Trident state partition of the `TridentState` object that would manage
    the updates for the word.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种情况下，我的拓扑正在执行查询的`MapGet`函数，以获取每个单词的计数；在我们的情况下，DRPC流以与拓扑前一部分中的`TridentState`完全相同的方式分组。这种安排确保了每个单词的所有计数查询都被定向到`TridentState`对象的相同Trident状态分区，该对象将管理单词的更新。
- en: '`FilterNull` ensures that the words that don''t have a count are filtered out*   The
    sum aggregator then sums all the counts to get the results, which are automatically
    returned back to the awaiting client'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FilterNull`确保没有计数的单词被过滤掉* 然后求和聚合器对所有计数求和以获得结果，结果会自动返回给等待的客户端'
- en: Having understood the execution as per the developer-written code, let's take
    a look at what's boilerplate to Trident and what happens automatically behind
    the scenes when this framework executes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解开发人员编写的代码执行之后，让我们看看Trident的样板文件以及当这个框架执行时自动发生的事情。
- en: We have two operations in our Trident word count topology that read from or
    write to state—`persistentAggregate` and `stateQuery`. Trident employs the capability
    to batch these operations automatically to that state. So for instance, the current
    processing requires 10 reads and writes to the database; Trident would automatically
    batch them together as one read and one write. This gets you performance and ease
    of computation where the optimization is handled by the framework.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的Trident单词计数拓扑中有两个操作，它们从状态中读取或写入——`persistentAggregate`和`stateQuery`。Trident具有自动批处理这些操作的能力，以便将它们批处理到状态。例如，当前处理需要对数据库进行10次读取和写入；Trident会自动将它们一起批处理为一次读取和一次写入。这为您提供了性能和计算的便利，优化由框架处理。
- en: Trident aggregators are other highly efficient and optimized components of the
    framework. They don't work by the rule to transfer all the tuples to one machine
    and then aggregate, instead they optimize the computation by executing partial
    aggregations wherever possible and then transfer the results over the network,
    thus saving on network latency. The approach employed here is similar to combiners
    of the MapReduce world.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trident聚合器是框架的其他高效和优化组件。它们不遵循将所有元组传输到一台机器然后进行聚合的规则，而是通过在可能的地方执行部分聚合，然后将结果传输到网络来优化计算，从而节省网络延迟。这里采用的方法类似于MapReduce世界中的组合器。
- en: Understanding the Trident API
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Trident API
- en: 'Trident API supports five broad categories of operations:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Trident API支持五大类操作：
- en: Operations for manipulations of partitioning local data without network transfer
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于操作本地数据分区的操作，无需网络传输
- en: Operations related to the repartitioning of the stream (involves the transfer
    of stream data over the network)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与流重新分区相关的操作（涉及通过网络传输流数据）
- en: Data aggregation over the stream (this operation do the network transfer as
    a part of operation)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流上的数据聚合（此操作作为操作的一部分进行网络传输）
- en: Grouping over a field in the stream
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流中字段的分组
- en: Merge and join
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并和连接
- en: Local partition manipulation operation
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地分区操作
- en: As the name suggests, these operations are locally operative over the batch
    on each node and no network traffic is involved for it. The following functions
    fall under this category.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，这些操作在每个节点上对批处理进行本地操作，不涉及网络流量。以下功能属于此类别。
- en: Functions
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 函数
- en: This operation takes single input value and emits zero or more tuples as the
    output
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此操作接受单个输入值，并将零个或多个元组作为输出发射
- en: The output of these function operations is appended to the end of the original
    tuple and emitted to the stream
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些函数操作的输出附加到原始元组的末尾，并发射到流中
- en: In cases where the function is such that no output tuple is emitted, the framework
    filters the input tuple too, while in other cases the input tuple is duplicated
    for each of the output tuples
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在函数不发射输出元组的情况下，框架也会过滤输入元组，而在其他情况下，输入元组会被复制为每个输出元组
- en: 'Let''s illustrate how this works with an example:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例来说明这是如何工作的：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now the next assumption, the input stream in the variable called `myTridentStream`
    has the following fields `["a", "b", "c" ]` and the tuples on the stream are depicted
    as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设，变量`myTridentStream`中的输入流具有以下字段`["a"，"b"，"c"]`，流中的元组如下所示：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output expected here is as per the function it should return `["a", "b",
    "c", "d"]`, so for the preceding tuples in the stream I would get the following
    output:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里期望的输出是根据函数应该返回`["a"，"b"，"c"，"d"]`，所以对于流中的前面的元组，我将得到以下输出：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Filters
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过滤器
- en: 'Filters are no misnomers; their execution is exactly the same as their name
    suggests: they help us decide whether or not we have to keep a tuple or not—they
    do exactly what filters do, that is, remove what is not required as per a given
    criteria.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器并非名不副实；它们的执行与其名称所示完全相同：它们帮助我们决定是否保留元组，它们确切地做到了过滤器的作用，即根据给定的条件删除不需要的内容。
- en: 'Let''s have a look at the following snippet to see a working illustration of
    filter functions:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看下面的片段，以查看过滤函数的工作示例：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s look at the sample tuples on the input stream with the fields as `[
    "a" , "b" , "c"]`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看输入流上的示例元组，字段为`["a"，"b"，"c"]`：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We execute or call the function as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行或调用函数如下：
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output would be as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: partitionAggregate
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: partitionAggregate
- en: The `partitionAggregate` function on each of the partitions over a set of tuples
    clubbed together as a batch. There is a behavioral difference between this function;
    compared to local functions that we have executed so far, this one emits a single
    output tuple for the stream on input tuples.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`partitionAggregate`函数对一批元组的每个分区进行操作。与迄今为止执行的本地函数相比，此函数之间存在行为差异，它对输入元组发射单个输出元组。'
- en: The following are other functions that can be used for various aggregates that
    can be executed over this framework.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可以用于在此框架上执行各种聚合的其他函数。
- en: Sum aggregate
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Sum聚合
- en: 'Here is how the call is made to the sum aggregator function:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对sum聚合器函数的调用方式：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s assume the input stream has the `["a", "b"]` fields, and the following
    are the tuples:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 假设输入流具有`["a"，"b"]`字段，并且以下是元组：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output will be as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: CombinerAggregator
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CombinerAggregator
- en: The implementation of this interface provided by the Trident API returns a single
    tuple with a single field as an output; internally, it executes an init function
    on each input tuple and then after that it combines the values until only one
    value is left, which is returned as an output. If the combiner functions encounter
    a partition that doesn't have any value, "0" is emitted.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Trident API提供的此接口的实现返回一个带有单个字段的单个元组作为输出；在内部，它对每个输入元组执行init函数，然后将值组合，直到只剩下一个值，然后将其作为输出返回。如果组合器函数遇到没有任何值的分区，则发射"0"。
- en: 'Here is the interface definition and its contracts:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是接口定义及其合同：
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following is the implementation for the count functionality:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是计数功能的实现：
- en: '[PRE18]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The biggest advantage these `CombinerAggregators` functions have over the `partitionAggregate`
    function is that it's a more efficient and optimized approach as it proceeds by
    performing partial aggregations before the transfer of results over the network.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些`CombinerAggregators`函数相对于`partitionAggregate`函数的最大优势在于，它是一种更高效和优化的方法，因为它在通过网络传输结果之前执行部分聚合。
- en: ReducerAggregator
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ReducerAggregator
- en: As the name suggests, this function produces an `init` value and then iterates
    over every tuple in the input stream to produce an output comprising of a single
    field and a single tuple.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，此函数生成一个`init`值，然后迭代处理输入流中的每个元组，以生成包含单个字段和单个元组的输出。
- en: 'The following is the interface contract for the `ReducerAggregate` interface:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`ReducerAggregate`接口的接口契约：
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is the implementation of this interface for count functionality:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是计数功能的接口实现：
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Aggregator
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Aggregator
- en: 'An `Aggregator` function is the most commonly used and versatile aggregator
    function. It has the ability to emit one or more tuples, and each can have any
    number of fields. They have the following interface signature:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`Aggregator`函数是最常用和多功能的聚合器函数。它有能力发出一个或多个元组，每个元组可以有任意数量的字段。它们具有以下接口签名：'
- en: '[PRE21]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The execution pattern is as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 执行模式如下：
- en: The `init` method is a predecessor to processing of every batch. It's called
    before the processing of each batch. On completion, it returns an object holding
    the state representation of the batch, and this is passed on to the subsequent
    aggregate and complete methods.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init`方法是每个批次处理之前的前导。它在处理每个批次之前被调用。完成后，它返回一个持有批次状态表示的对象，并将其传递给后续的聚合和完成方法。'
- en: Unlike the `init` method, the `aggregate` method is called once for every tuple
    in the batch partition. This method can store the state, and can emit the results
    depending upon functionality requirements.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与`init`方法不同，`aggregate`方法对批次分区中的每个元组调用一次。该方法可以存储状态，并根据功能要求发出结果。
- en: The complete method is like a postprocessor; it's executed at the end, when
    the batch partition has been completely processed by the aggregate.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: complete方法类似于后处理器；当批次分区被聚合完全处理时执行。
- en: 'The following is the implementation of the count as an aggregator function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是计数作为聚合器函数的实现：
- en: '[PRE22]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Numerous times we run into implementations requiring multiple aggregators to
    be executing simultaneously. In such cases, the concept of chaining comes in handy.
    Thanks to this functionality in the Trident API, we can build an execution chain
    of aggregators to be executed over batches of incoming stream tuples. Here is
    an example of these kinds of chains:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 许多时候，我们遇到需要同时执行多个聚合器的实现。在这种情况下，链接的概念就派上了用场。由于Trident API中的这个功能，我们可以构建一个聚合器的执行链，以便在传入流元组的批次上执行。以下是这种链的一个例子：
- en: '[PRE23]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The execution of this chain would run the specified `sum` and `count` aggregator
    functions on each partition. The output would be a single tuple, with two fields
    holding the values of `sum` and `count`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此链的执行将在每个分区上运行指定的`sum`和`count`聚合器函数。输出将是一个单个元组，其中包含`sum`和`count`的值。
- en: Operations related to stream repartitioning
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与流重新分区相关的操作
- en: As the name suggests, these stream repartitioning operations are related to
    the execution of functions to change the tuple partitions across the tasks. These
    operations involve network traffic and the results redistribute the stream, and
    can result in changes to an overall partitioning strategy thus impacting a number
    of partitions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，这些流重新分区操作与执行函数来改变任务之间的元组分区有关。这些操作涉及网络流量，结果重新分发流，并可能导致整体分区策略的变化，从而影响多个分区。
- en: 'Here are the repartitioning functions provided by the Trident API:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Trident API提供的重新分区函数：
- en: '`Shuffle`: This executes a rebalance kind of functionality and it employs a
    random round robin algorithm for an even redistribution of tuples across the partitions.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Shuffle`: 这执行一种重新平衡的功能，并采用随机轮询算法，以实现元组在分区之间的均匀重新分配。'
- en: '`Broadcast`: This does what the name suggests; it broadcasts and transmits
    each tuple to every target partition.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Broadcast`: 这就像其名称所示的那样；它将每个元组广播和传输到每个目标分区。'
- en: '`partitionBy`: This function works on hashing and mod on a set of specified
    fields so that the same fields are always moved to the same partitions. As an
    analogy, one can assume that the functioning of this is similar to the fields
    grouping that we learned about initially in Storm groupings.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`partitionBy`: 这个函数基于一组指定字段的哈希和模运算工作，以便相同的字段总是移动到相同的分区。类比地，可以假设这个功能的运行方式类似于最初在Storm分组中学到的字段分组。'
- en: '`global`: This is identical to the global grouping of streams in a Storm, and
    in this case, the same partition is chosen for all the batches.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global`: 这与Storm中流的全局分组相同，在这种情况下，所有批次都选择相同的分区。'
- en: '`batchGlobal`: All tuples in a batch are sent to the same partition (so they
    kind of stick together), but different batches can be delivered to different partitions.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batchGlobal`: 一个批次中的所有元组都被发送到同一个分区（所以它们在某种程度上是粘在一起的），但不同的批次可以被发送到不同的分区。'
- en: Data aggregations over the streams
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流上的数据聚合
- en: 'Storm''s Trident framework provides two kinds of operations for performing
    aggregations:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Storm的Trident框架提供了两种执行聚合的操作：
- en: '`aggregate`: We have covered this in an earlier section, and it works in isolated
    partitions without involving network traffic'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregate`: 我们在之前的部分中已经涵盖了这个，它在隔离的分区中工作，而不涉及网络流量'
- en: '`persistentAggregate`: This performs aggregate across partitions, but the difference
    is that it stores the results in a source of state'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`persistentAggregate`: 这在分区间执行聚合，但不同之处在于它将结果存储在状态源中'
- en: Grouping over a field in a stream
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流中字段的分组
- en: Grouping operations work in analogy to group by the operations in a relational
    model with the only differential being that the ones in the Storm framework execute
    over a stream of tuples from the input source.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 分组操作的工作方式类似于关系模型中的分组操作，唯一的区别在于Storm框架中的分组操作是在输入源的元组流上执行的。
- en: 'Let''s understand this more closely with the help of the following figure:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下图更仔细地了解这一点：
- en: '![Grouping over a field in a stream](img/00073.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![在流中对字段进行分组](img/00073.jpeg)'
- en: These operations in the Storm Trident run over a stream of tuples of several
    different partitions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Storm Trident中的这些操作在几个不同分区的元组流上运行。
- en: Merge and join
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并和连接
- en: 'The merges and joins APIs provide interfaces for merging and joining various
    streams together. This is possible using a variety of ways provided as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 合并和连接API提供了合并和连接各种流的接口。可以使用以下多种方式来实现这一点：
- en: '`Merge`: As the name suggests, `merge` merges two or more streams together
    and emits the merged stream as the output field of the first stream:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`合并`: 正如其名称所示，`merge`将两个或多个流合并在一起，并将合并后的流作为第一个流的输出字段发出：'
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Join`: This operation works as the traditional SQL `join` function, but with
    the difference that it applies to small batches instead of entire infinite streams
    coming out of the spout'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`连接`: 此操作与传统的SQL `join`函数相同，但不同之处在于它适用于小批量而不是从喷口输出的整个无限流'
- en: 'For example, consider a join function where Stream 1 has fields such as `["key",
    "val1", "val2"]` and Stream 2 has `["x", "val1"]`, and from these functions we
    execute the following code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个连接函数，其中Stream 1具有诸如`["key", "val1", "val2"]`的字段，Stream 2具有`["x", "val1"]`，并且从这些函数中我们执行以下代码：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As a result, Stream 1 and Stream 2 would be joined using `key` and `x`, wherein
    `key` would join the field for Stream 1 and `x` would join the field for Stream
    2.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，Stream 1和Stream 2将使用`key`和`x`进行连接，其中`key`将连接Stream 1的字段，`x`将连接Stream 2的字段。
- en: 'The output tuples emitted from the join would have the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从连接中发出的输出元组将如下所示：
- en: The list of all the join fields; in our case, it would be `key` from Stream
    1 and `x` from Stream 2.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有连接字段的列表；在我们的情况下，它将是Stream 1的`key`和Stream 2的`x`。
- en: A list of all the fields that are not join fields from all the streams involved
    in the join operation in the same order as they are passed to the `join` operation.
    In our case, it's `a` and `b` respectively for `val1` and `val2` of Stream 1,
    and `c` for `val1` from Stream 2 (note that this step also removes the ambiguity
    of field names if any ambiguity is present within the stream, in our case `val1`
    field was ambiguous between both the streams).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有参与连接操作的流中不是连接字段的字段列表，顺序与它们传递给`join`操作的顺序相同。在我们的情况下，对于Stream 1的`val1`和`val2`，分别是`a`和`b`，对于Stream
    2的`val1`是`c`（请注意，此步骤还会消除流中存在的任何字段名称的歧义，我们的情况下，`val1`字段在两个流之间是模棱两可的）。
- en: When operations like join happen on streams that are being fed in the topology
    from different spouts, the framework ensures that the spouts are synchronized
    with respect to batch emission, so that every join computation can include tuples
    from a batch of each spout.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当在拓扑中从不同的喷口中提供的流上发生像连接这样的操作时，框架确保喷口在批量发射方面是同步的，以便每个连接计算可以包括来自每个喷口的批量元组。
- en: Examples and illustrations
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例和插图
- en: One of the other out-of-the-box and popular implementations of Trident is reach
    topology, which is a pure DRPC topology that finds the reach of a URL on demand.
    Let's first understand some of the jargon before we delve deeper.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Trident的另一个开箱即用且流行的实现是reach拓扑，它是一个纯DRPC拓扑，可以根据需要找到URL的可达性。在我们深入研究之前，让我们先了解一些行话。
- en: Reach is basically a sum total of the count of Twitter users exposed to a URL.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Reach基本上是暴露给URL的Twitter用户数量的总和。
- en: 'Reach computation is a multistep process that can be attained by the following
    examples:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Reach计算是一个多步骤的过程，可以通过以下示例实现：
- en: Get all the users who have ever tweeted a URL
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取曾经发推特的URL的所有用户
- en: Fetch the follower tree of each of these users
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取每个用户的追随者树
- en: Assemble the huge follower sets fetched previously
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组装之前获取的大量追随者集
- en: Count the set
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算集合
- en: Well, looking at the skeletal algorithm entailed previously, you can make out
    that it is beyond the capability of a single machine and we'd need a distributed
    compute engine to achieve it. It's an ideal candidate of the Storm Trident framework,
    as you have the capability to execute highly parallel computations at each step
    across the cluster.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，看看之前的骨架算法，你会发现它超出了单台机器的能力，我们需要一个分布式计算引擎来实现它。这是Storm Trident框架的理想候选，因为您可以在整个集群中的每个步骤上执行高度并行的计算。
- en: Our Trident reach topology would be sucking data from two large data banks
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的Trident reach拓扑将从两个大型数据银行中吸取数据
- en: Bank A is the URL to the originator bank, wherein all the URLs would be stored
    along with the name of the user who had tweeted them
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 银行A是URL到发起者银行，其中将存储所有URL以及曾经发推特的用户的名称。
- en: Bank B is the user follower bank; this data bank will have a user to follow
    the mapping for all Twitter users
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 银行B是用户追随者银行；这个数据银行将为所有Twitter用户提供用户追随映射
- en: 'The topology would be defined as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑将定义如下：
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the preceding topology, we perform the following steps:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述拓扑中，我们执行以下步骤：
- en: Create a `TridentState` object for both data banks (URL to the originator Bank
    A and users to follow Bank B).
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为两个数据银行（URL到发起者银行A和用户到追随银行B）创建一个`TridentState`对象。
- en: The `newStaticState` method is used for the instantiation of state objects for
    data banks; we have the capability to run the DRPC queries over the source states
    created earlier.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`newStaticState`方法用于实例化数据银行的状态对象；我们有能力在之前创建的源状态上运行DRPC查询。'
- en: In execution, when the reach of a URL is to be computed, we perform a query
    using the Trident state for data bank A to fetch the list of all the users who
    have ever tweeted with this URL.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行中，当要计算URL的可达性时，我们使用数据银行A的Trident状态执行查询，以获取曾经发推特的所有用户的列表。
- en: The `ExpandList` function creates and emits one tuple for each of the tweeters
    of the URL in query.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ExpandList`函数为查询URL的每个推特者创建并发出一个元组。'
- en: Next, we fetch the follower of each tweeter fetched previously. This step needs
    the highest degree of parallelism, thus we use shuffle grouping here for even
    load distribution across all instances of the bolt. In our reach topology, this
    is the most intense compute step.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们获取先前获取的每个推特者的追随者。这一步需要最高程度的并行性，因此我们在这里使用洗牌分组，以便在所有螺栓实例之间均匀分配负载。在我们的reach拓扑中，这是最密集的计算步骤。
- en: Once we have the list of followers of the tweeter of the URL, we execute an
    operation analog to filter unique followers only.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了URL推特者的追随者列表，我们执行类似于筛选唯一追随者的操作。
- en: We arrive at unique followers by grouping them together and then using the `one`
    aggregator. The latter simply emits `1` for each group and in the next step all
    these are counted together to arrive at the reach.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过将追随者分组在一起，然后使用`one`聚合器来得到唯一的追随者。后者简单地为每个组发出`1`，然后在下一步将所有这些计数在一起以得出影响力。
- en: Then we count the followers (unique) thus arriving at the reach of the URL.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们计算追随者（唯一），从而得出URL的影响力。
- en: Quiz time
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测验时间
- en: 'Q.1\. State whether the following statements are true or false:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 状态是否以下陈述是真是假：
- en: DRPC is a stateless, Storm processing mechanism.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DRPC是一个无状态的，Storm处理机制。
- en: If a tuple fails to execute in a Trident topology, the entire batch is replayed.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果Trident拓扑中的元组执行失败，整个批次将被重放。
- en: Trident lets the user implement windowing functions over streaming data.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Trident允许用户在流数据上实现窗口函数。
- en: Aggregators are more efficient then partitioned Aggregators.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚合器比分区聚合器更有效。
- en: 'Q.2\. Fill in the blanks:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 填空：
- en: _______________ is the distributed version of RPC.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: _______________是RPC的分布式版本。
- en: _______________ is the basic micro-batching framework over Storm.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: _______________是Storm的基本微批处理框架。
- en: The ___________________functions are used to remove tuples based on certain
    criteria or conditions from the stream batches.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ___________________函数用于根据特定标准或条件从流批次中删除元组。
- en: Q.3\. Create a Trident topology to find the tweeters who have the maximum number
    of tweets in the last 5 minutes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 创建一个Trident拓扑，以查找在过去5分钟内发表最多推文的推特者。
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have pretty much covered everything about Storm and its
    advanced concepts with giving you the change to get hands-on with the Trident
    and DRPC topologies. You learned about Trident and its need and application, the
    DRPC topologies, and the various functions available in the Trident API.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们几乎涵盖了关于Storm及其高级概念的一切，并让您有机会亲自体验Trident和DRPC拓扑。您了解了Trident及其需求和应用，DRPC拓扑以及Trident
    API中提供的各种功能。
- en: In the next chapter, we will explore other technology components that go hand
    in hand with Storm and are necessary for building end-to-end solutions with Storm.
    We will touch upon areas of distributed caches and **Complex Event Processing**
    (**CEP**) with memcache and Esper in conjunction with Storm.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索与Storm紧密配合并且对于使用Storm构建端到端解决方案必不可少的其他技术组件。我们将涉及分布式缓存和与Storm一起使用memcache和Esper进行**复杂事件处理**（CEP）的领域。
