- en: Chapter 6. Kafka Integrations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。Kafka集成
- en: Consider a use case for a website where continuous security events, such as
    user authentication and authorization to access secure resources, need to be tracked,
    and decisions need to be taken in real time for any security breach. Using any
    typical batch-oriented data processing systems, such as Hadoop, where all the
    data needs to be collected first and then processed to reveal patterns, will make
    it too late to decide whether there is any security threat to the web application
    or not. Hence, this is the classical use case for real-time data processing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个网站的用例，其中需要跟踪连续的安全事件，例如用户身份验证和授权以访问安全资源，并且需要实时做出决策以应对任何安全漏洞。使用任何典型的面向批处理的数据处理系统，例如Hadoop，需要首先收集所有数据，然后进行处理以揭示模式，这将使得判断是否存在对Web应用程序的安全威胁变得太晚。因此，这是实时数据处理的经典用例。
- en: Let's consider another use case, where raw clickstreams generated by customers
    through website usage are captured and preprocessed. Processing these clickstreams
    provides valuable insight into customer preferences and these insights can be
    coupled later with marketing campaigns and recommendation engines to offer an
    analysis of consumers. Hence, we can simply say that this large amount of clickstream
    data stored on Hadoop will get processed by Hadoop MapReduce jobs in batch mode,
    not in real time.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑另一个用例，即通过网站使用生成的原始点击流被捕获和预处理。处理这些点击流可以为客户偏好提供有价值的见解，这些见解可以稍后与营销活动和推荐引擎相结合，以提供对消费者的分析。因此，我们可以简单地说，存储在Hadoop上的大量点击流数据将通过Hadoop
    MapReduce作业以批处理模式而不是实时模式进行处理。
- en: 'In this chapter, we shall be exploring how Kafka can be integrated with the
    following technologies to address different use cases, such as real-time processing
    using Storm, as Spark Streaming, and batch processing using Hadoop:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何将Kafka与以下技术集成，以解决不同的用例，例如使用Storm进行实时处理，使用Spark Streaming进行批处理：
- en: Kafka integration with Storm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka与Storm的集成
- en: Kafka integration with Hadoop
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka与Hadoop的集成
- en: So let's start.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Kafka integration with Storm
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka与Storm的集成
- en: Processing small amounts of data in real-time was never a challenge using technologies
    such as **Java Messaging Service** (**JMS**); however, these processing systems
    show performance limitations when dealing with large volumes of streaming data.
    Also, these systems are not good horizontally scalable solutions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用诸如**Java消息服务**（**JMS**）之类的技术实时处理少量数据从未是一个挑战；然而，当处理大量流数据时，这些处理系统显示出性能限制。此外，这些系统不是良好的横向可扩展的解决方案。
- en: Introducing Storm
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Storm
- en: '**Storm** is an open source, distributed, reliable, and fault-tolerant system
    for processing streams of large volumes of data in real-time. It supports many
    use cases, such as real-time analytics, online machine learning, continuous computation,
    and the **Extract Transformation Load** (**ETL**) paradigm.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**Storm**是一个用于实时处理大量数据流的开源、分布式、可靠和容错系统。它支持许多用例，如实时分析、在线机器学习、连续计算和**ETL**（**Extract
    Transformation Load**）范式。'
- en: 'There are various components that work together for streaming data processing,
    as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种组件一起工作进行流数据处理，如下所示：
- en: '**Spout**: This is a continuous stream of log data.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spout**：这是连续的日志数据流。'
- en: '**Bolt**: The spout passes the data to a component called **bolt**. A bolt
    consumes any number of input streams, does some processing, and possibly emits
    new streams. For example, emitting a stream of trend analysis by processing a
    stream of tweets.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bolt**：spout将数据传递给一个名为**bolt**的组件。Bolt可以消耗任意数量的输入流，进行一些处理，并可能发出新的流。例如，通过处理一系列推文来发出趋势分析流。'
- en: 'The following diagram shows spout and bolt in the Storm architecture:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Storm架构中的spout和bolt：
- en: '![Introducing Storm](img/3090OS_06_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![介绍Storm](img/3090OS_06_01.jpg)'
- en: 'We can assume a Storm cluster to be a chain of bolt components, where each
    bolt performs some kind of transformation on the data streamed by the spout. Other
    than spout and bolts, a few other components are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以假设Storm集群是一系列螺栓组件的链，其中每个螺栓对由spout流的数据进行某种转换。除了spout和bolts之外，还有一些其他组件，如下所示：
- en: 'Tuple: This is the native data structure (name list values of any data type)
    used by Storm.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元组：这是Storm使用的本机数据结构（任何数据类型的名称列表值）。
- en: 'Stream: This represents a sequence of tuples.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流：这代表一系列元组。
- en: 'Workers: These represent the Storm process.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Workers：这些代表Storm进程。
- en: 'Executors: A Storm thread launched by a Storm worker. Here, workers may run
    one or more executors and executors may run one or more Storm job(s) from a spout
    or bolt.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行器：由Storm工作启动的Storm线程。在这里，工作可能运行一个或多个执行器，执行器可能运行一个或多个来自spout或bolt的Storm作业。
- en: Next in the Storm cluster, jobs are typically referred to as **topologies**;
    the only difference is that these topologies run forever. For real-time computation
    on Storm, topologies that are nothing but graphs of computation are created. Typically,
    topologies define how data will flow from spouts through bolts. These topologies
    can be transactional or non-transactional in nature.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来在Storm集群中，作业通常被称为**拓扑**；唯一的区别是这些拓扑永远运行。对于Storm上的实时计算，通常会创建计算图形的拓扑。通常，拓扑定义了数据如何从spouts流经bolts。这些拓扑可以是事务性的或非事务性的。
- en: Note
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Complete information about Storm can be found at [http://storm-project.net/](http://storm-project.net/).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Storm的完整信息可以在[http://storm-project.net/](http://storm-project.net/)找到。
- en: The following section is useful if you have worked with Storm or have working
    knowledge of Storm.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经使用过Storm或对Storm有工作知识，下面的部分将会很有用。
- en: Integrating Storm
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成Storm
- en: We have already learned in the previous chapters that Kafka is a high-performance
    publisher-subscriber-based messaging system with highly scalable properties. Kafka
    spout is available for integrating Storm with Kafka clusters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在之前的章节中学习到，Kafka是一个具有高性能的基于发布-订阅的消息系统，具有高度可扩展的特性。Kafka spout可用于将Storm与Kafka集群集成。
- en: The Kafka spout is a regular spout implementation that reads the data from a
    Kafka cluster. This Kafka spout, which was available earlier at [https://github.com/wurstmeister/storm-kafka-0.8-plus](https://github.com/wurstmeister/storm-kafka-0.8-plus),
    is now merged into the core Storm project version 0.9.2-incubating and can be
    found at [https://github.com/apache/storm/tree/master/external/storm-kafka](https://github.com/apache/storm/tree/master/external/storm-kafka).
    This storm-kafka spout provides the key features such as support for dynamic discovery
    of Kafka brokers and "exactly once" tuple processing. Apart from the regular Storm
    spout for Kafka, it also provides the Trident spout implementation for Kafka.
    In this section, our focus will remain on the regular storm-kafka spout.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka spout是一个常规的spout实现，用于从Kafka集群中读取数据。这个Kafka spout之前可以在[https://github.com/wurstmeister/storm-kafka-0.8-plus](https://github.com/wurstmeister/storm-kafka-0.8-plus)找到，现在已经合并到核心Storm项目版本0.9.2-incubating中，并且可以在[https://github.com/apache/storm/tree/master/external/storm-kafka](https://github.com/apache/storm/tree/master/external/storm-kafka)找到。这个storm-kafka
    spout提供了关键功能，比如支持动态发现Kafka经纪人和“仅一次”元组处理。除了常规的Kafka Storm spout，它还提供了Kafka的Trident
    spout实现。在本节中，我们将重点放在常规的storm-kafka spout上。
- en: Note
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Trident is a high-level abstraction for doing real-time computing on top of
    Storm. It allows us to seamlessly intermix high throughput (millions of messages
    per second), stateful stream processing with low-latency distributed querying.
    For more information [https://storm.apache.org/documentation/Trident-tutorial.html](https://storm.apache.org/documentation/Trident-tutorial.html).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Trident是一个高级抽象，用于在Storm之上进行实时计算。它允许我们无缝地混合高吞吐量（每秒数百万条消息）、有状态的流处理和低延迟的分布式查询。更多信息请参见[https://storm.apache.org/documentation/Trident-tutorial.html](https://storm.apache.org/documentation/Trident-tutorial.html)。
- en: Both spout implementations use the `BrokerHost` interface that tracks Kafka
    broker host-to-partition mapping and `KafkaConfig` parameters. Two implementations,
    `ZkHosts` and `StaticHosts`, are provided for the `BrokerHost` interface.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 两个spout实现都使用`BrokerHost`接口来跟踪Kafka经纪人主机到分区的映射和`KafkaConfig`参数。`ZkHosts`和`StaticHosts`提供了`BrokerHost`接口的两个实现。
- en: 'The `ZkHosts` implementation is used for dynamically tracking Kafka broker-to-partition
    mapping with the help of Kafka''s zookeeper''s entries:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`ZkHosts`实现用于动态跟踪Kafka经纪人到分区的映射，借助Kafka的zookeeper条目：'
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding constructors are used to create the instance of `ZkHosts`. Here,
    `brokerZkStr` can be `localhost:9092` and `brokerZkPath` is the root directory
    under which all the topic and partition information is stored. The default value
    of `brokerZkPath` is `/brokers`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的构造函数用于创建`ZkHosts`的实例。在这里，`brokerZkStr`可以是`localhost:9092`，`brokerZkPath`是存储所有主题和分区信息的根目录。`brokerZkPath`的默认值是`/brokers`。
- en: 'The `StaticHosts` implementation is used for static partitioning information
    as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: “StaticHosts”实现用于静态分区信息，如：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For creating the `StaticHosts` instance, the first instance of `GlobalPartitionInformation`
    is created as shown in the preceding code. Next, the `KafkaConfig` instance needs
    to be created for constructing the Kafka spout as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建`StaticHosts`实例，首先需要创建`GlobalPartitionInformation`的第一个实例，如前面的代码所示。接下来，需要创建`KafkaConfig`实例来构建Kafka
    spout：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding constructors take the following parameters:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的构造函数需要以下参数：
- en: A list of Kafka brokers
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka经纪人列表
- en: The topic name used to read the message
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于读取消息的主题名称
- en: Client ID, used as a part of the Zookeeper path where the spout as a consumer
    stores the current consumption offset.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端ID，作为Zookeeper路径的一部分，其中spout作为消费者存储当前的消费偏移量。
- en: 'The `KafkaConfig` class also has a bunch of public variables for controlling
    the application''s behavior and how spout fetches messages from the Kafka cluster:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`KafkaConfig`类还有一堆公共变量，用于控制应用程序的行为以及spout如何从Kafka集群中获取消息：'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `Spoutconfig` class extends the `KafkaConfig` class to support two additional
    values as `zkroot` and `id`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`Spoutconfig`类扩展了`KafkaConfig`类，以支持`zkroot`和`id`两个额外的值：'
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding constructor additionally takes the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的构造函数还需要以下内容：
- en: The root path in Zookeeper, where spout stores the consumer offset
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Zookeeper中的根路径，spout存储消费者偏移量。
- en: The unique identity of the spout
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spout的唯一标识
- en: 'The following code sample shows the `KafkaSpout` class instance initialization
    with the previous parameters:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例显示了使用先前参数初始化`KafkaSpout`类实例：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following diagram shows the high-level integration view of what a Kafka
    Storm working model will look like:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Kafka Storm工作模型的高级集成视图：
- en: '![Integrating Storm](img/3090OS_06_02.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![Integrating Storm](img/3090OS_06_02.jpg)'
- en: 'The Kafka spout uses the same Zookeeper instance that is used by Apache Storm,
    to store the states of the message offset and segment consumption tracking if
    it is consumed. These offsets are stored at the root path specified for the Zookeeper.
    The Kafka spout uses these offsets to replay tuples in the event of a downstream
    failure or timeout. Although it also has a provision to rewind to a previous offset
    rather than starting from the last saved offset, Kafka chooses the latest offset
    written around the specified timestamp:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka spout使用与Apache Storm相同的Zookeeper实例来存储消息偏移和段消费跟踪的状态，如果它被消费。这些偏移量存储在Zookeeper指定的根路径下。Kafka
    spout使用这些偏移量在下游故障或超时的情况下重放元组。尽管它也有一个重新回到先前偏移的规定，而不是从最后保存的偏移开始，Kafka会选择在指定时间戳周围写入的最新偏移量：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here the value `-1` forces the Kafka spout to restart from the latest offset
    and `-2` forces the spout to restart from the earliest offset.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的值“-1”强制Kafka spout从最新的偏移重新启动，“-2”强制spout从最早的偏移重新启动。
- en: This storm-kafka spout also has a as it has no support for Kafka 0.7x brokers
    and only supports Kafka 0.8.1.x onwards.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个storm-kafka spout也有一个，因为它不支持Kafka 0.7x经纪人，只支持Kafka 0.8.1.x及以上版本。
- en: Note
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: To run Kafka with Storm, clusters for both Storm and Kafka need to be set up
    and should be running. A Storm cluster setup is beyond the scope of this book.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行Kafka与Storm，需要设置并运行Storm和Kafka的集群。Storm集群设置超出了本书的范围。
- en: Kafka integration with Hadoop
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka与Hadoop集成
- en: Resource sharing, stability, availability, and scalability are a few of the
    many challenges of distributed computing. Nowadays, an additional challenge is
    to process extremely large volumes of data in TBs or PBs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 资源共享、稳定性、可用性和可扩展性是分布式计算的许多挑战之一。如今，另一个挑战是处理TB或PB级别的极大数据量。
- en: Introducing Hadoop
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Hadoop
- en: Hadoop is a large-scale distributed batch-processing framework that parallelizes
    data processing across many nodes and addresses the challenges for distributed
    computing, including big data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop是一个大规模分布式批处理框架，可以在许多节点上并行处理数据，并解决了分布式计算，包括大数据的挑战。
- en: Hadoop works on the principle of the MapReduce framework (introduced by Google),
    which provides a simple interface for the parallelization and distribution of
    large-scale computations. Hadoop has its own distributed data filesystem called
    **Hadoop Distributed File System** (**HDFS**). In any typical Hadoop cluster,
    HDFS splits the data into small pieces (called **blocks**) and distributes it
    to all the nodes. HDFS also replicates these small pieces of data and stores them
    to make sure that, if any node is down, the data is available from another node.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop基于MapReduce框架的原则（由Google引入），为大规模计算的并行化和分布提供了一个简单的接口。Hadoop有自己的分布式数据文件系统称为**Hadoop分布式文件系统**（**HDFS**）。在任何典型的Hadoop集群中，HDFS将数据分割成小块（称为**块**）并将其分发到所有节点。HDFS还复制这些小数据块并存储它们，以确保如果任何节点宕机，数据可以从另一个节点获取。
- en: 'The following diagram shows the high-level view of a multi-node Hadoop cluster:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了多节点Hadoop集群的高级视图：
- en: '![Introducing Hadoop](img/3090OS_06_03.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![介绍Hadoop](img/3090OS_06_03.jpg)'
- en: 'Hadoop has the following main components:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop有以下主要组件：
- en: '**Name node**: This is a single point of interaction for HDFS. A name node
    stores information about the small pieces (blocks) of data that are distributed
    across the node.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**名称节点**：这是HDFS的交互单点。名称节点存储有关分布在节点上的数据小块（块）的信息。'
- en: '**Secondary name node**: This node stores edit logs, which are helpful to for
    restoring the latest updated state of HDFS in the case of a name node failure.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**辅助名称节点**：此节点存储编辑日志，有助于在名称节点故障的情况下恢复HDFS的最新更新状态。'
- en: '**Data node**: These nodes store the actual data distributed by the name node
    in blocks and also store the replicated copy of data from other nodes.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据节点**：这些节点存储名称节点分发的实际数据块，并存储来自其他节点的复制数据。'
- en: '**Job tracker**: This is responsible for splitting the MapReduce jobs into
    smaller tasks.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作业跟踪器**：这负责将MapReduce作业拆分为较小的任务。'
- en: '**Task tracker**: The task tracker is responsible for the execution of tasks
    split by the job tracker.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务跟踪器**：任务跟踪器负责执行由作业跟踪器拆分的任务。'
- en: The data nodes and the task tracker share the same machines and the MapReduce
    job split; execution of tasks is done based on the data store location information
    provided by the name node.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 数据节点和任务跟踪器共享相同的机器和MapReduce作业拆分；任务的执行是基于名称节点提供的数据存储位置信息完成的。
- en: Now before we discuss the Kafka integration with Hadoop let's quickly set up
    a single node Hadoop cluster in pseudo distributed mode.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在讨论Kafka与Hadoop集成之前，让我们快速在伪分布式模式下设置单节点Hadoop集群。
- en: Note
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Hadoop clusters can be set up in three different modes:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop集群可以在三种不同的模式下设置：
- en: Local mode
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地模式
- en: Pseudo distributed mode
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伪分布式模式
- en: Fully distributed mode
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全分布式模式
- en: Local mode and pseudo distributed mode work on single-node cluster. In local
    mode, all the Hadoop main components run in the single JVM instance; whereas,
    in pseudo distributed mode, each component runs in a separate JVM instance on
    the single node. Pseudo distributed mode is primarily used as a development environment
    by developers. In fully distributed mode, all the components run on separate nodes
    and are used in test and production environments.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本地模式和伪分布式模式在单节点集群上运行。在本地模式下，所有Hadoop主要组件在单个JVM实例中运行；而在伪分布式模式下，每个组件在单个节点上的单独JVM实例中运行。伪分布式模式主要由开发人员用作开发环境。在完全分布式模式下，所有组件都在单独的节点上运行，并且用于测试和生产环境。
- en: 'The following are the steps used for creating pseudo distributed mode cluster:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于创建伪分布式模式集群的步骤：
- en: Install and configure Java. Refer to the *Installing Java 1.7 or higher* section
    in [Chapter 1](ch01.html "Chapter 1. Introducing Kafka"), *Introducing Kafka*.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装和配置Java。参考[第1章](ch01.html "第1章。介绍Kafka")中的*安装Java 1.7或更高版本*部分，*介绍Kafka*。
- en: Download the current stable Hadoop distribution from [http://www.apache.org/dyn/closer.cgi/hadoop/common/](http://www.apache.org/dyn/closer.cgi/hadoop/common/).
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[http://www.apache.org/dyn/closer.cgi/hadoop/common/](http://www.apache.org/dyn/closer.cgi/hadoop/common/)下载当前稳定的Hadoop分发包。
- en: 'Unpack the downloaded Hadoop distribution in `/opt` and add Hadoop''s `bin`
    directory to the path as:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`/opt`中解压下载的Hadoop分发包，并将Hadoop的`bin`目录添加到路径中，如下所示：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Add the following configurations:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下配置：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Set up ssh to the localhost without a passphrase:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地主机上设置ssh，无需密码短语：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If ssh-to-localhost does not work without a passphrase, execute the following
    commands:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果ssh-to-localhost在没有密码短语的情况下无法工作，请执行以下命令：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Format the filesystem:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 格式化文件系统：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Start the NameNode daemon and DataNode daemon:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动NameNode守护程序和DataNode守护程序：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once the Hadoop cluster is set up successfully, browse the web interface for
    the NameNode at `http://localhost:50070/`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦成功设置了Hadoop集群，请在`http://localhost:50070/`上浏览NameNode的Web界面。
- en: Integrating Hadoop
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成Hadoop
- en: This section is useful if you have worked with Hadoop or have a working knowledge
    of Hadoop.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经使用过Hadoop或对Hadoop有工作经验，本节将非常有用。
- en: For real-time publish-subscribe use cases, Kafka is used to build a pipeline
    that is available for real-time processing or monitoring and to load the data
    into Hadoop, NoSQL, or data warehousing systems for offline processing and reporting.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实时发布-订阅用例，Kafka用于构建可用于实时处理或监控的管道，并将数据加载到Hadoop、NoSQL或数据仓库系统中进行离线处理和报告。
- en: Kafka provides the source code for both the Hadoop producer and consumer, under
    its `contrib` directory.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka在其`contrib`目录下提供了Hadoop生产者和消费者的源代码。
- en: Hadoop producers
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop生产者
- en: 'A Hadoop producer provides a bridge for publishing the data from a Hadoop cluster
    to Kafka, as shown in the following diagram:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop生产者提供了从Hadoop集群向Kafka发布数据的桥梁，如下图所示：
- en: '![Hadoop producers](img/3090OS_06_04.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![Hadoop生产者](img/3090OS_06_04.jpg)'
- en: 'For a Kafka producer, Kafka topics are considered as URIs and, to connect to
    a specific Kafka broker, URIs are specified as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Kafka生产者，Kafka主题被视为URI，并且为了连接到特定的Kafka代理，URI被指定如下：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The Hadoop producer code suggests two possible approaches for getting the data
    from Hadoop:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop生产者代码提出了从Hadoop获取数据的两种可能方法：
- en: '**Using the Pig script and writing messages in Avro format**: In this approach,
    Kafka producers use Pig scripts for writing data in a binary Avro format, where
    each row signifies a single message. For pushing the data into the Kafka cluster,
    the `AvroKafkaStorage` class (it extends Pig''s `StoreFunc` class) takes the Avro
    schema as its first argument and connects to the Kafka URI. Using the `AvroKafkaStorage`
    producer, we can also easily write to multiple topics and brokers in the same
    Pig-script-based job. While writing Pig scripts, required Kafka JAR files also
    need to be registered. The following is the sample Pig script:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用Pig脚本并以Avro格式编写消息**：在这种方法中，Kafka生产者使用Pig脚本以二进制Avro格式编写数据，其中每行表示单个消息。对于将数据推送到Kafka集群，`AvroKafkaStorage`类（它扩展了Pig的`StoreFunc`类）将Avro模式作为其第一个参数，并连接到Kafka
    URI。使用`AvroKafkaStorage`生产者，我们还可以在同一个基于Pig脚本的作业中轻松写入多个主题和代理。在编写Pig脚本时，还需要注册所需的Kafka
    JAR文件。以下是示例Pig脚本：'
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding script, the Pig `StoreFunc` class makes use of `AvroStorage`
    in Piggybank to convert from Pig's data model to the specified Avro schema.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述脚本中，Pig的`StoreFunc`类利用Piggybank中的`AvroStorage`将数据从Pig的数据模型转换为指定的Avro模式。
- en: '**Using the Kafka OutputFormat class for jobs**: In this approach, the Kafka
    `OutputFormat` class (it extends Hadoop''s `OutputFormat` class) is used for publishing
    data to the Kafka cluster. Using the 0.20 MapReduce API, this approach publishes
    messages as bytes and provides control over output by using low-level methods
    of publishing. The Kafka `OutputFormat` class uses the `KafkaRecordWriter` class
    (it extends Hadoop''s `RecordWriter` class) for writing a record (message) to
    a Hadoop cluster.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用作业的Kafka OutputFormat类**：在这种方法中，Kafka的`OutputFormat`类（它扩展了Hadoop的`OutputFormat`类）用于将数据发布到Kafka集群。使用0.20
    MapReduce API，这种方法将消息作为字节发布，并通过使用低级别的发布方法来控制输出。Kafka的`OutputFormat`类使用`KafkaRecordWriter`类（它扩展了Hadoop的`RecordWriter`类）来将记录（消息）写入Hadoop集群。'
- en: For Kafka producers, we can also configure Kafka producer parameters by prefixing
    them with `kafka.output` in the job configuration. For example, to change the
    compression codec, add the `kafka.output.compression.codec` parameter (for example,
    `SET kafka.output.compression.codec 0` in Pig script for no compression). Along
    with these values, Kafka broker information (`kafka.metadata.broker.list`), the
    topic (`kafka.output.topic`), and the schema (`kafka.output.schema`) are injected
    into the job's configuration.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Kafka生产者，我们还可以通过在作业配置中加上`kafka.output`前缀来配置Kafka生产者参数。例如，要更改压缩编解码器，添加`kafka.output.compression.codec`参数（例如，在Pig脚本中添加`SET
    kafka.output.compression.codec 0`表示不压缩）。除了这些值，Kafka代理信息（`kafka.metadata.broker.list`）、主题（`kafka.output.topic`）和模式（`kafka.output.schema`）也被注入到作业的配置中。
- en: Hadoop consumers
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop消费者
- en: 'A Hadoop consumer is a Hadoop job that pulls data from the Kafka broker and
    pushes it into HDFS. The following diagram shows the position of a Kafka consumer
    in the architecture pattern:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop消费者是从Kafka代理中拉取数据并将其推送到HDFS的Hadoop作业。以下图表显示了Kafka消费者在架构模式中的位置：
- en: '![Hadoop consumers](img/3090OS_06_05.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![Hadoop消费者](img/3090OS_06_05.jpg)'
- en: A Hadoop job performs parallel loading from Kafka to HDFS, and the number of
    mappers for loading the data depends on the number of files in the input directory.
    The output directory contains data coming from Kafka and the updated topic offsets.
    Individual mappers write the offset of the last consumed message to HDFS at the
    end of the map task. If a job fails and jobs get restarted, each mapper simply
    restarts from the offsets stored in HDFS.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Hadoop作业执行从Kafka到HDFS的并行加载，加载数据的mapper数量取决于输入目录中文件的数量。输出目录包含来自Kafka的数据和更新的主题偏移量。单独的mapper在map任务结束时将最后消费的消息的偏移量写入HDFS。如果作业失败并且作业重新启动，每个mapper都会从HDFS中存储的偏移量重新启动。
- en: 'The ETL example provided in the `Kafka-0.8.1.1-src/contrib/hadoop-consumer`
    directory demonstrates the extraction of Kafka data and loading it to HDFS. It
    requires the following inputs from a configuration file, for example, `test/test.properties`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`Kafka-0.8.1.1-src/contrib/hadoop-consumer`目录中提供的ETL示例演示了从Kafka中提取数据并将其加载到HDFS。例如，它需要来自配置文件的以下输入，例如`test/test.properties`：'
- en: '`kafka.etl.topic`: The topic to be fetched.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kafka.etl.topic`：要获取的主题。'
- en: '`kafka.server.uri`: The Kafka server URI.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kafka.server.uri`：Kafka服务器URI。'
- en: '`input`: Input directory containing topic offsets that can be generated by
    `DataGenerator`. The number of files in this directory determines the number of
    mappers in the Hadoop job.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input`：包含由`DataGenerator`生成的主题偏移量的输入目录。此目录中的文件数量决定了Hadoop作业中的mapper数量。'
- en: '`output`: Output directory containing Kafka data and updated topic offsets.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output`：包含Kafka数据和更新的主题偏移量的输出目录。'
- en: '`kafka.request.limit`: It is used to limit the number events fetched.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kafka.request.limit`：用于限制获取的事件数量。'
- en: In the Kafka consumer, the `KafkaETLRecordReader` instance is a record reader
    associated with `KafkaETLInputFormat`. It fetches Kafka data from the server starting
    from the provided offsets (specified by `input`) and stops when it reaches the
    largest available offsets or the specified limit (specified by `kafka.request.limit`).
    `KafkaETLJob` also contains some helper functions to initialize job configuration
    and `SimpleKafkaETLJob` sets up job properties and submits the Hadoop job. Once
    the job is started `SimpleKafkaETLMapper` dumps Kafka data into HDFS (specified
    by `output`).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在卡夫卡消费者中，`KafkaETLRecordReader`实例是与`KafkaETLInputFormat`相关联的记录读取器。它从服务器获取卡夫卡数据，从提供的偏移量（由`input`指定）开始，并在达到最大可用偏移量或指定限制（由`kafka.request.limit`指定）时停止。`KafkaETLJob`还包含一些辅助函数来初始化作业配置，`SimpleKafkaETLJob`设置作业属性并提交Hadoop作业。一旦作业启动，`SimpleKafkaETLMapper`将卡夫卡数据转储到HDFS（由`output`指定）。
- en: Summary
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have primarily learned how Kafka can be integrated with
    existing open source frameworks in the area of real-time/batch data processing.
    In the real-time data processing area, Kafka is integrated with Storm using the
    existing Storm spout. As for batch data processing, Kafka brings Hadoop-based
    data producers and consumes, so that data can be published onto the HDFS, processed
    using MapReduce, and later consumed.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们主要学习了卡夫卡如何与现有的开源框架在实时/批处理数据处理领域集成。在实时数据处理领域，卡夫卡与使用现有的Storm spout的Storm集成。至于批处理数据处理，卡夫卡带来了基于Hadoop的数据生产者和消费者，使数据可以发布到HDFS，使用MapReduce进行处理，然后消费。
- en: In the next chapter, which is also the last chapter of this book, we will look
    at some of the other important facts about Kafka.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，也是本书的最后一章，我们将看一些关于卡夫卡的其他重要事实。
