- en: 14\. Dimensionality Reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14. 降维
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter introduces dimensionality reduction in data science. You will be
    using the Internet Advertisements dataset to analyze and evaluate different techniques
    in dimensionality reduction. By the end of this chapter, you will be able to analyze
    datasets with high dimensions and deal with the challenges posed by these datasets.
    As well as applying different dimensionality reduction techniques to large datasets,
    you will fit models based on those datasets and analyze their results. By the
    end of this chapter, you will be able to deal with huge datasets in the real world.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了数据科学中的降维。你将使用互联网广告数据集来分析和评估不同的降维技术。在本章结束时，你将能够分析高维数据集并应对这些数据集带来的挑战。同时，你将应用不同的降维技术处理大型数据集，并基于这些数据集拟合模型并分析其结果。到本章结束时，你将能够处理现实世界中的巨型数据集。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter on balancing datasets, we dealt with the Bank Marketing
    dataset, which had 18 variables. We were able to load that dataset very easily,
    fit a model, and get results. But have you considered the scenario when the number
    of variables you have to deal with is large, say around 18 million instead of
    the 18 you dealt with in the last chapter? How do you load such large datasets
    and analyze them? How do you deal with the computing resources required for modeling
    with such large datasets?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章关于数据集平衡的内容中，我们处理了银行营销数据集，该数据集有18个变量。我们能够非常轻松地加载该数据集，拟合模型并获得结果。但你是否考虑过，当你需要处理的变量数量非常大时，会是什么情况？例如，当变量数量达到1800万，而不是上一章中的18个时，你该如何加载如此庞大的数据集并进行分析？如何处理需要如此大量计算资源的建模问题？
- en: 'This is the reality in some modern-day datasets in domains such as:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些现代数据集中在以下领域的现实：
- en: Healthcare, where genetics datasets can have millions of features
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医疗保健领域，其中遗传数据集可能有数百万个特征
- en: High-resolution imaging datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高分辨率成像数据集
- en: Web data related to advertisements, ranking, and crawling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与广告、排名和爬取相关的网页数据
- en: 'When dealing with such huge datasets, many challenges can arise:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理如此庞大的数据集时，可能会出现许多挑战：
- en: 'Storage and computation challenges: Large datasets with high dimensions require
    a lot of storage and expensive computational resources for analysis.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储和计算挑战：高维的大型数据集需要大量的存储空间和昂贵的计算资源来进行分析。
- en: 'Exploration challenges: When trying to explore data and derive insights, high-dimensional
    data can be really cumbersome.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索挑战：在尝试探索数据并提取洞察时，高维数据可能非常繁琐。
- en: 'Algorithm challenges: Many algorithms do not scale well in high-dimensional settings.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法挑战：许多算法在高维设置下扩展性较差。
- en: So, what is the solution when we have to deal with high-dimensional data? This
    is where dimensionality reduction techniques come to the fore, which we will explore
    in this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，当我们必须处理高维数据时，解决方案是什么？这就是降维技术发挥作用的地方，本章将详细探讨这些技术。
- en: 'Dimensionality reduction aims to reduce the dimensions of datasets to get over
    the challenges posed by high-dimensional data. In this chapter, we will examine
    some of the popular dimensionality reduction techniques:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 降维的目的是减少数据集的维度，以克服高维数据所带来的挑战。在本章中，我们将探讨一些常见的降维技术：
- en: Backward feature elimination or recursive feature elimination
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向特征消除或递归特征消除
- en: Forward feature selection
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正向特征选择
- en: '**Principal Component Analysis** (**PCA**)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）'
- en: '**Independent Component Analysis** (**ICA**)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立成分分析**（**ICA**）'
- en: Factor analysis
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子分析
- en: Let's first examine our business context and then apply these techniques to
    the problem statement.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先检查一下我们的商业背景，然后将这些技术应用到问题陈述中。
- en: Business Context
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 商业背景
- en: The marketing head of your company comes to you with a problem she has been
    grappling with. Many customers have been complaining about the browsing experience
    of your company's website because of the number of advertisements that pop up
    during browsing. Your company wants to build an engine on your web server that
    identifies potential advertisements and then eliminates them even before they
    pop up.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你公司市场部的负责人向你提出了一个她一直在处理的问题。许多客户投诉你公司网站在浏览过程中弹出广告太多，影响了他们的浏览体验。你公司希望在网页服务器上建立一个引擎，识别潜在的广告，并在广告弹出之前将其删除。
- en: To help you to achieve this, you have been given a dataset that contains a set
    of possible advertisements on a variety of web pages. The features of the dataset
    represent the geometry of the images in the possible adverts, as well as phrases
    occurring in the URL, image URLs, anchor text, and words occurring near the anchor
    text. This dataset has also been labeled, with each possible ad given a label
    that says whether it is actually an advertisement or not. Using this dataset,
    you have to build a model that predicts whether something is an advertisement
    or not. You may think that this is a relatively simple problem that could be solved
    with any binary classification algorithm. However, there is a challenge in the
    dataset. The dataset has a large number of features. You have set out to solve
    this high-dimensional dataset challenge.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您实现这一目标，我们提供了一个包含一组可能广告的数据集，这些广告出现在各种网页上。数据集的特征代表了可能广告的图像几何形状，以及 URL 中的短语、图像
    URL、锚文本和锚文本附近的词汇。这个数据集已经被标注，每个可能的广告都被标注为是否为广告。使用此数据集，您需要构建一个模型来预测某个内容是否为广告。您可能认为这是一个相对简单的问题，可以使用任何二分类算法来解决。然而，数据集存在一个挑战：特征数量非常庞大。您已经开始着手解决这个高维数据集的挑战。
- en: 'This dataset is uploaded in the GitHub repository for working through all the
    subsequent exercises. The attributes of the dataset are available in the following
    link: [https://packt.live/36rqiCg](https://packt.live/36rqiCg).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集已上传到 GitHub 仓库，用于完成后续所有练习。数据集的属性可以通过以下链接访问：[https://packt.live/36rqiCg](https://packt.live/36rqiCg)。
- en: 'Exercise 14.01: Loading and Cleaning the Dataset'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 14.01：加载和清理数据集
- en: In this exercise, we will download the dataset, load it in our Colab notebook,
    and do some basic explorations, such as printing the dimensions of the dataset
    using the `.shape()` and `.describe()` functions, and also cleaning the dataset.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将下载数据集，加载到 Colab 笔记本中，并进行一些基本的探索，例如使用 `.shape()` 和 `.describe()` 函数打印数据集的维度，同时还要清理数据集。
- en: Note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `internet_ads` dataset has been uploaded to our GitHub repository and can
    be accessed at [https://packt.live/2sPaVF6](https://packt.live/2sPaVF6).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`internet_ads` 数据集已上传到我们的 GitHub 仓库，并可通过 [https://packt.live/2sPaVF6](https://packt.live/2sPaVF6)
    访问。'
- en: 'The following steps will help you complete this exercise:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助您完成此练习：
- en: Open a new Colab notebook file.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Colab 笔记本文件。
- en: 'Now, `import pandas` into your Colab notebook:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将 `import pandas` 导入到您的 Colab 笔记本中：
- en: '[PRE0]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, set the path of the drive where the `ad.Data` file is uploaded, as shown
    in the following code snippet:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，设置已上传 `ad.Data` 文件所在驱动器的路径，如以下代码片段所示：
- en: '[PRE1]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Read the file using the `pd.read_csv()` function from the pandas data frame:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 数据框中的 `pd.read_csv()` 函数读取文件：
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `pd.read_csv()` function's arguments are the filename as a string and the
    limit separator of a CSV file, which is `","`. Please note that as there are no
    headers for the dataset. We specifically mention this using the `header = None`
    command. The last argument, `error_bad_lines=False`, is to skip any errors in
    the format of the file and then load data.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`pd.read_csv()` 函数的参数是文件名字符串和 CSV 文件的分隔符，通常是 `","`。请注意，由于数据集没有标题行，我们特别使用 `header
    = None` 命令来指定这一点。最后一个参数 `error_bad_lines=False` 是跳过文件格式中的任何错误，然后加载数据。'
- en: After reading the file, the data frame is printed using the `.head()` function.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 读取文件后，使用 `.head()` 函数打印数据框。
- en: 'You should get the following output:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该得到以下输出：
- en: '![Figure 14.1: Loading data into the Colab notebook'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.1：将数据加载到 Colab 笔记本中'
- en: '](img/B15019_14_01.jpg)'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_01.jpg)'
- en: 'Figure 14.1: Loading data into the Colab notebook'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.1：将数据加载到 Colab 笔记本中
- en: 'Now, print the shape of the dataset, as shown in the following code snippet:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，打印数据集的形状，如以下代码片段所示：
- en: '[PRE3]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should get the following output:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该得到以下输出：
- en: '[PRE4]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: From the shape, we can see that we have a large number of features, `1559`.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从数据的形状来看，我们可以看到特征数量非常大，共有 `1559` 个特征。
- en: 'Find the summary of the numerical features of the raw data using the `.describe()`
    function in pandas, as shown in the following code snippet:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 的 `.describe()` 函数查找原始数据的数值特征摘要，如以下代码片段所示：
- en: '[PRE5]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should get the following output:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该得到以下输出：
- en: '![Figure 14.2: Loading data into the Colab notebook'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.2：将数据加载到 Colab 笔记本中'
- en: '](img/B15019_14_02.jpg)'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_02.jpg)'
- en: 'Figure 14.2: Loading data into the Colab notebook'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.2：将数据加载到 Colab 笔记本中
- en: As we saw from the shape of the data, the dataset has `3279` examples with `1559`
    variables. The variable set has both categorical and numerical variables. The
    summary statistics are only derived for numerical data.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们从数据的形状中看到的，数据集有`3279`个示例，`1559`个变量。变量集包含分类变量和数值变量。汇总统计仅针对数值数据得出。
- en: 'Separate the dependent and independent variables from our dataset, as shown
    in the following code snippet:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的数据集中分离出因变量和自变量，如以下代码片段所示：
- en: '[PRE6]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You should get the following output:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE7]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As seen earlier, there are `1559` features in the dataset. The first `1558`
    features are independent variables. They are separated from the initial `adData`
    data frame using the `.loc()` function and give the indexes of the corresponding
    features (`0` to `1557`). The independent variables are loaded into a new variable
    called `X`. The dependent variable, which is the label of the dataset, is loaded
    in variable `Y`. The shapes of the dependent and independent variables are also printed.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前所见，数据集中有`1559`个特征。前`1558`个特征是自变量。它们通过`.loc()`函数从初始的`adData`数据框中分离出来，并给出相应特征的索引（`0`到`1557`）。这些自变量被加载到一个新的变量`X`中。因变量，即数据集的标签，被加载到变量`Y`中。因变量和自变量的形状也被打印出来。
- en: 'Print the first `15` examples of the independent variables:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印自变量的前`15`个示例：
- en: '[PRE8]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can print as many rows of the data by defining the number within the `head()`
    function. Here, we have printed out the first `15` rows of the data.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以通过在`head()`函数中定义数字来打印任意数量的行。在这里，我们打印了数据的前`15`行。
- en: 'The output is as follows:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 14.3: First 15 examples of independent variables'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图14.3：自变量的前15个示例'
- en: '](img/B15019_14_03.jpg)'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_03.jpg)'
- en: 'Figure 14.3: First 15 examples of independent variables'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.3：自变量的前15个示例
- en: From the output, we can see that there are many missing values in the dataset,
    which are represented by `?`. For further analysis, we have to remove these special
    characters and then replace those cells with assumed values. One popular method
    of replacing special characters is to impute the mean of the respective feature.
    Let's adopt this strategy. However, before doing that, let's look at the data
    types for this dataset to adopt a suitable replacement strategy.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从输出结果中，我们可以看到数据集存在许多缺失值，这些缺失值用`?`表示。为了进一步分析，我们必须去除这些特殊字符，并将这些单元格替换为假定值。替换特殊字符的一种常见方法是用相应特征的均值进行填充。我们就采用这种策略。然而，在此之前，让我们先查看这个数据集的变量类型，以便采用合适的替换策略。
- en: 'Print the data types of the dataset:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印数据集的变量类型：
- en: '[PRE9]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We should get the following output:'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们应该得到以下输出：
- en: '![Figure 14.4: The data types in our dataset'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图14.4：我们数据集中的变量类型'
- en: '](img/B15019_14_04.jpg)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_04.jpg)'
- en: 'Figure 14.4: The data types in our dataset'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.4：我们数据集中的变量类型
- en: From the output, we can see that the first four columns are of the object type,
    which refers to string data, and the others are integer data. When replacing the
    special characters in the data, we need to be cognizant of the data types.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从输出结果中，我们可以看到前四列是对象类型，表示字符串数据，其他列则是整数数据。在替换数据中的特殊字符时，我们需要注意数据的类型。
- en: Replace special characters with `NaN` values for the first four columns.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前四列中的特殊字符替换为`NaN`值。
- en: Replace the special characters in the first four columns, which are of the object
    type, with `NaN` values. `NaN` is an abbreviation for "not a number." Replacing
    special characters with `NaN` values makes it easy to further impute data.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将前四列（对象类型）的特殊字符替换为`NaN`值。`NaN`是“不是数字”（not a number）的缩写。用`NaN`值替换特殊字符使得后续填充数据变得更加容易。
- en: 'This is achieved through the following code snippet:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是通过以下代码片段实现的：
- en: '[PRE10]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To replace the first three columns, we loop through the columns using the `for()`
    loop and also using the `range()` function. Since the first three columns are
    of the `object` or `string` type, we use the `.str.replace()` function, which
    stands for "string replace". After replacing the special characters, `?`, of the
    data with `nan`, we convert the data type to `float` with the `.values.astype(float)`
    function, which is required for further processing. By printing the first 15 examples,
    we can see that all special characters have been replaced with `nan` or `NaN`
    values
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了替换前面三列，我们使用`for()`循环和`range()`函数来遍历这些列。由于前面三列的数据类型是`object`或`string`类型，我们使用`.str.replace()`函数，即“字符串替换”。在用`nan`替换数据中的特殊字符`?`后，我们通过`.values.astype(float)`函数将数据类型转换为`float`，这是进一步处理所必需的。通过打印前15个示例，我们可以看到所有特殊字符都已被`nan`或`NaN`值替换。
- en: 'You should get the following output:'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该获得以下输出：
- en: '![Figure 14.5: After replacing special characters with NaN'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图14.5：替换特殊字符为NaN之后'
- en: '](img/B15019_14_05.jpg)'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_05.jpg)'
- en: 'Figure 14.5: After replacing special characters with NaN'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.5：替换特殊字符为NaN之后
- en: Now, replace special characters for the integer features.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，替换整数特征中的特殊字符。
- en: 'As in *Step 9*, let''s also replace the special characters from the features
    of the `int64` data type with the following code snippet:'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如*步骤9*所示，让我们使用以下代码片段，替换`int64`数据类型特征中的特殊字符：
- en: '[PRE11]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: For the integer features, we do not have `.str` before the `.replace()` function,
    as these features are integer values and not string values.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于整数特征，我们在`.replace()`函数前没有`.str`，因为这些特征是整数值，而不是字符串值。
- en: Now, impute the mean of each column for the `NaN` values.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为`NaN`值填充每列的均值。
- en: 'Now that we have replaced special characters in the data with `NaN` values,
    we can use the `fillna()` function in pandas to replace the `NaN` values with
    the mean of the column. This is executed using the following code snippet:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经将数据中的特殊字符替换为`NaN`值，我们可以使用pandas中的`fillna()`函数将`NaN`值替换为该列的均值。可以使用以下代码片段来执行此操作：
- en: '[PRE12]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding code snippet, the `.mean()` function calculates the mean of
    each column and then replaces the `nan` values with the mean of the column.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，`.mean()`函数计算每列的均值，然后用该列的均值替换`nan`值。
- en: 'You should get the following output:'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该获得以下输出：
- en: '![Figure 14.6: Mean of the NaN columns'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图14.6：NaN列的均值'
- en: '](img/B15019_14_06.jpg)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_06.jpg)'
- en: 'Figure 14.6: Mean of the NaN columns'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.6：NaN列的均值
- en: Scale the dataset using the `minmaxScaler()` function.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`minmaxScaler()`函数对数据集进行缩放。
- en: As in *Chapter 3*, *Binary Classification*, scaling data is useful in the modeling
    step. Let's scale the dataset using the `minmaxScaler()` function as learned in
    *Chapter 3*, *Binary Classification*.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与*第3章*，*二分类*中一样，缩放数据在建模步骤中是有用的。让我们按照*第3章*，*二分类*中学到的方法，使用`minmaxScaler()`函数对数据集进行缩放。
- en: 'This is shown in the following code snippet:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如下代码片段所示：
- en: '[PRE13]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You should get the following output. Here, we have displayed the first 24 columns:'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该获得以下输出。这里，我们展示了前24列：
- en: '![Figure 14.7: Scaling the dataset using the MinMaxScaler() function'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图14.7：使用MinMaxScaler()函数缩放数据集'
- en: '](img/B15019_14_07.jpg)'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_07.jpg)'
- en: 'Figure 14.7: Scaling the dataset using the MinMaxScaler() function'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7：使用MinMaxScaler()函数缩放数据集
- en: Note
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2Yi7Nym](https://packt.live/2Yi7Nym).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅[https://packt.live/2Yi7Nym](https://packt.live/2Yi7Nym)。
- en: You can also run this example online at [https://packt.live/2Q6l9ZZ](https://packt.live/2Q6l9ZZ).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2Q6l9ZZ](https://packt.live/2Q6l9ZZ)在线运行此示例。
- en: You have come to the end of the first exercise. In this exercise, we loaded
    the dataset, extracted summary statistics, cleaned the data, and also scaled the
    data. You can see that in the final output, all the raw values have been transformed
    into scaled values.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经完成了第一个练习。在本练习中，我们加载了数据集，提取了汇总统计信息，清洗了数据，并且对数据进行了缩放。你可以看到，在最终输出中，所有原始值都已转换为缩放值。
- en: In the next section, let's try to augment this dataset with many more features
    so that this becomes a massive dataset and then fit a simple logistic regression
    model on this dataset as a benchmark model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们尝试通过添加更多的特征来增强这个数据集，使其成为一个庞大的数据集，然后在这个数据集上拟合一个简单的逻辑回归模型，作为基准模型。
- en: Let's first see how data can be augmented with an example.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一下如何通过示例增强数据。
- en: Creating a High-Dimensional Dataset
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建高维数据集
- en: In the earlier section, we worked with a dataset that has around `1,558` features.
    In order to demonstrate the challenges with high-dimensional datasets, let's create
    an extremely high dimensional dataset from the internet dataset that we already
    have.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的部分，我们处理了一个约有`1,558`个特征的数据集。为了演示高维数据集的挑战，我们将从已有的互联网数据集中创建一个极高维度的数据集。
- en: This we will achieve by replicating the existing number of features multiple
    times so that the dataset becomes really large. To replicate the dataset, we will
    use a function called `np.tile()`, which copies a data frame multiple times across
    the axes we want. We will also calculate the time it takes for any activity using
    the `time()` function.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过多次复制现有特征的数量来实现这一点，使数据集变得非常大。为了复制数据集，我们将使用一个名为`np.tile()`的函数，它可以在我们指定的轴上多次复制数据框。我们还将使用`time()`函数来计算活动所需的时间。
- en: Let's look at both these functions in action with a toy example.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来看一下这两个函数的作用。
- en: 'You begin by importing the necessary library functions:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先需要导入必要的库函数：
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, to create a dummy data frame, we will use a small dataset with two rows
    and three columns for this example. We use the `pd.np.array()` function to create
    a data frame:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了创建一个虚拟数据框，我们将使用一个包含两行三列的小数据集作为示例。我们使用`pd.np.array()`函数来创建数据框：
- en: '[PRE15]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should get the following output:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该获得以下输出：
- en: '![Figure 14.8: Array for the sample dummy data frame'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.8：示例虚拟数据框的数组'
- en: '](img/B15019_14_08.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_14_08.jpg)'
- en: 'Figure 14.8: Array for the sample dummy data frame'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8：示例虚拟数据框的数组
- en: 'Next, you replicate the dummy data frame and this replication of the columns
    is done using the `pd.np.tile()` function in the following code snippet:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将复制虚拟数据框，列的复制是通过以下代码片段中的`pd.np.tile()`函数来完成的：
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You should get the following output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该获得以下输出：
- en: '![Figure 14.9: Replication of the data frame'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.9：数据框的复制'
- en: '](img/B15019_14_09.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_14_09.jpg)'
- en: 'Figure 14.9: Replication of the data frame'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9：数据框的复制
- en: As we can see in the snippet, the `pd.np.tile()` function accepts two sets of
    arguments. The first one is the data frame, `df`, that we want to replicate. The
    next argument, `(1,5)`, defines which axes we want to replicate. In this example,
    we define that the rows will remain as is because of the `1` argument, and the
    columns will be replicated `5` times with the `5` argument. We can see from the
    `shape()` function that the original data frame, which was of shape `(2,3)`, has
    been transformed into a data frame with a shape of `(2,15)`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如片段所示，`pd.np.tile()`函数接受两组参数。第一组是我们要复制的数据框`df`，下一组参数`(1,5)`定义了我们要复制的轴。在这个示例中，我们定义行保持不变，因为参数是`1`，而列将被复制`5`次，因为参数是`5`。从`shape()`函数中我们可以看到，原始数据框形状为`(2,3)`，经过转换后变为形状为`(2,15)`的数据框。
- en: Calculating the total time is done using the `time` library. To start the timing,
    we invoke the `time.time()` function. In the example, we store the initial time
    in a variable called `t0` and then subtract this from the end time to find the
    total time it takes for the process. Thus we have augmented and added more data
    frames to our exiting internet ads dataset.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 总时间的计算是通过`time`库来完成的。为了开始计时，我们调用`time.time()`函数。在这个例子中，我们将初始时间存储在一个名为`t0`的变量中，然后从结束时间中减去它，得到该过程所需的总时间。因此，我们已经扩展并向现有的互联网广告数据集添加了更多的数据框。
- en: 'Activity 14.01: Fitting a Logistic Regression Model on a HighDimensional Dataset'
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 14.01：在高维数据集上拟合逻辑回归模型
- en: You want to test the performance of your models when the dataset is large. To
    do this, you are artificially augmenting the internet ads dataset so that the
    dataset is 300 times bigger in dimension than the original dataset. You will be
    fitting a logistic regression model on this new dataset and then observe the results.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望在数据集较大时测试模型的性能。为此，你将人工增大互联网广告数据集，使得数据集的维度比原始数据集大 300 倍。你将拟合一个逻辑回归模型，并观察结果。
- en: '**Hint**: In this activity, we will use a notebook similar to *Exercise 14.01*,
    *Loading and Cleaning the Dataset*, and we will also be fitting a logistic regression
    model as done in *Chapter 3*, *Binary Classification*.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示**：在本次活动中，我们将使用类似于*练习 14.01*、*加载和清理数据集*的笔记本，并且我们还将像*第 3 章*、*二元分类*中一样拟合一个逻辑回归模型。'
- en: Note
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will be using the same ads dataset for this activity.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本次活动中使用相同的广告数据集。
- en: The `internet_ads` dataset has been uploaded to our GitHub repository and can
    be accessed at [https://packt.live/2sPaVF6](https://packt.live/2sPaVF6).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`internet_ads` 数据集已上传到我们的 GitHub 仓库，并可以通过 [https://packt.live/2sPaVF6](https://packt.live/2sPaVF6)
    访问。'
- en: 'The steps to complete this activity are as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动的步骤如下：
- en: Open a new Colab notebook.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Colab 笔记本。
- en: Implement all steps from *Exercise 14.01*, *Loading and Cleaning the Dataset*,
    until the normalization of data. Derive the transformed independent `X_tran` variable.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现 *练习 14.01*，*加载和清理数据集*，直到数据标准化的所有步骤。推导变换后的自变量 `X_tran`。
- en: Create a high-dimensional dataset by replicating the columns 300 times using
    the `pd.np.tile()` function. Print the shape of the new dataset and observe the
    number of features in the new dataset.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pd.np.tile()` 函数将列复制 300 次来创建一个高维数据集。打印新数据集的形状并观察新数据集中的特征数量。
- en: Split the dataset into train and test sets.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集拆分为训练集和测试集。
- en: Fit a logistic regression model on the new dataset and note the time it takes
    to fit the model. Note the color change for the indicator for RAM on your Colab notebook.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新数据集上拟合一个逻辑回归模型，并注意拟合模型所需的时间。注意你 Colab 笔记本中 RAM 指标的颜色变化。
- en: '**Expected Output**:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出**：'
- en: 'You should get output similar to the following after fitting the logistic regression
    model on the new dataset:'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在对新数据集拟合逻辑回归模型后，你应该得到类似以下的输出：
- en: '[PRE17]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Figure 14.10: Google Colab RAM utilization'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.10：Google Colab RAM 使用情况'
- en: '](img/B15019_14_10.jpg)'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_10.jpg)'
- en: 'Figure 14.10: Google Colab RAM utilization'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.10：Google Colab RAM 使用情况
- en: Predict on the test set and print the classification report and confusion matrix.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上进行预测，并打印分类报告和混淆矩阵。
- en: 'You should get the following output:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.11: Confusion matrix and the classification report results'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.11：混淆矩阵和分类报告结果'
- en: '](img/B15019_14_11.jpg)'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_11.jpg)'
- en: 'Figure 14.11: Confusion matrix and the classification report results'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.11：混淆矩阵和分类报告结果
- en: Note
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The solution to the activity can be found here: [https://packt.live/2GbJloz](https://packt.live/2GbJloz).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在此处找到：[https://packt.live/2GbJloz](https://packt.live/2GbJloz)。
- en: In this activity, you will have created a high-dimensional dataset by replicating
    the columns of the existing database and identified that the resource utilization
    is quite high with this high dimensional dataset. The resource utilization indicator
    changed its color to orange because of the large dimensions. The longer time,
    `23.86` seconds, taken for modeling was also noticed on this dataset. You will
    have also predicted on the test set to get an accuracy level of around `97%` using
    the logistic regression model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你将通过复制现有数据库的列来创建一个高维数据集，并识别出该高维数据集的资源利用率非常高。由于维度较大，资源利用率指示器的颜色变为橙色。你还注意到，在该数据集上的建模时间较长，为
    `23.86` 秒。你还会在测试集上进行预测，并使用逻辑回归模型得到大约 `97%` 的准确率。
- en: First, you need to know why the color of RAM utilization on Colab changed to
    orange. Because of the huge dataset we created by replication, Colab had to use
    access RAM, due to which the color changed to orange.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要了解为什么 Colab 上的 RAM 使用情况指示器变成了橙色。由于我们通过复制创建了一个庞大的数据集，Colab 必须使用访问 RAM，这导致颜色变为橙色。
- en: But, out of curiosity, what do you think the impact will be on the RAM utilization
    if you increased the replication from 300 to 500? Let's have a look at the following
    example.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，出于好奇，如果你将复制次数从 300 增加到 500，你认为这会对 RAM 使用情况产生什么影响呢？让我们来看一下以下示例。
- en: Note
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You don't need to perform this on your Colab notebook.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要在你的 Colab 笔记本上执行此操作。
- en: 'We begin by defining the path of the dataset for the GitHub repository to our
    "ads" dataset:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义 GitHub 仓库中“ads”数据集的路径：
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we simply load the data using pandas:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 pandas 加载数据：
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create a high-dimensional dataset with a scaling factor of `500`:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个具有 `500` 缩放因子的高维数据集：
- en: '[PRE20]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You will see the following output:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到以下输出：
- en: '![Figure 14.12: Colab crashing'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.12：Colab 崩溃'
- en: '](img/B15019_14_12.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_14_12.jpg)'
- en: 'Figure 14.12: Colab crashing'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.12：Colab 崩溃
- en: From the output, you can see that the session crashes because all the RAM provided
    by Colab has been used. The session will restart, and you will lose all your variables.
    Hence, it is always good to be mindful of the resources you are provided with,
    along with the dataset. As a data scientist, if you feel that a dataset is huge
    with many features but the resources to process that dataset are limited, you
    need to get in touch with the organization and get the required resources or build
    an appropriate strategy to address these high-dimensional datasets.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，您可以看到会话崩溃，因为 Colab 提供的所有 RAM 已被用尽。会话将重启，您将失去所有变量。因此，始终保持对所提供资源的敏感性是很重要的，尤其是在处理数据集时。作为数据科学家，如果您觉得数据集庞大，特征众多，但处理这些数据集的资源有限，您需要联系组织，获取所需资源或制定相应的策略来处理这些高维数据集。
- en: Strategies for Addressing High-Dimensional Datasets
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理高维数据集的策略
- en: In *Activity 14.01*, *Fitting a Logistic Regression Model on a High-Dimensional
    Dataset*, we witnessed the challenges of high-dimensional datasets. We saw how
    the resources were challenged when the replication factor was 300\. You also saw
    that the notebook crashes when the replication factor is increased to 500\. When
    the replication factor was 500, the number of features was around 750,000\. In
    our case, our resources would fail to scale up even before we hit the 1 million
    mark on the number of features. Some modern-day datasets sometimes have hundreds
    of millions, or in many cases billions, of features. Imagine the kind of resources
    and time it would take to get any actionable insights from the dataset.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在*活动 14.01*，*在高维数据集上拟合逻辑回归模型*中，我们见识了高维数据集带来的挑战。我们看到，当复制因子为 300 时，资源受到了挑战。您还看到了，当复制因子增加到
    500 时，笔记本崩溃了。当复制因子为 500 时，特征数量大约为 750,000。在我们的案例中，在特征数量达到 100 万之前，资源就会失败扩展。现代数据集中有时会有数亿个特征，或在许多情况下达到数十亿个特征。想象一下，要从这些数据集中提取任何可操作的见解需要多少资源和时间。
- en: Luckily, we have many robust methods for addressing high-dimensional datasets.
    Many of these techniques are very effective and have helped to address the challenges
    raised by huge datasets.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们有许多强大的方法来处理高维数据集。这些技术中的许多都非常有效，并且帮助解决了由庞大数据集带来的挑战。
- en: 'Let''s look at some of the techniques for dealing with high-dimensional datasets.
    In *Figure 14.14*, you can see the strategies we will be coming across in this
    chapter to deal with such datasets:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看处理高维数据集的一些技巧。在*图 14.14*中，您可以看到本章中我们将要介绍的处理此类数据集的策略：
- en: '![Figure 14.13: Strategies to address high dimensional datasets'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.13：处理高维数据集的策略'
- en: '](img/B15019_14_13.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_14_13.jpg)'
- en: 'Figure 14.13: Strategies to address high dimensional datasets'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.13：处理高维数据集的策略
- en: Backward Feature Elimination (Recursive Feature Elimination)
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后向特征消除（递归特征消除）
- en: The mechanism behind the backward feature elimination algorithm is the recursive
    elimination of features and building a model on those features that remain after
    all the elimination.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 后向特征消除算法的机制是递归地消除特征，并在消除所有特征后，基于剩余的特征构建模型。
- en: 'Let''s look under the hood of this algorithm step by step:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步地看看这个算法的工作原理：
- en: Initially, at a given iteration, the selected classification algorithm is first
    trained on all the `n` features available. For example, let's take the case of
    the original dataset we had, which had `1,558` features. The algorithm starts
    off with all the `1,558` features in the first iteration.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初，在给定的迭代中，选定的分类算法会首先在所有 `n` 个特征上进行训练。例如，假设我们有一个原始数据集，包含 `1,558` 个特征。在第一次迭代中，算法会使用所有的
    `1,558` 个特征开始。
- en: In the next step, we remove one feature at a time and train a model with the
    remaining `n-1` features. This process is repeated `n` times. For example, we
    first remove feature 1 and then fit a model using all the remaining 1,557 variables.
    In the next iteration, we use feature `1` and instead, we eliminate feature `2`
    and then fit the model. This process is repeated `n` times (`1,558`) times.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，我们一次去除一个特征，并使用剩下的 `n-1` 个特征来训练模型。这个过程重复 `n` 次。例如，首先我们去除特征 1，然后使用剩余的 1,557
    个变量拟合模型。在下一次迭代中，我们保留特征 `1`，而去除特征 `2`，然后再拟合模型。这个过程会重复 `n` 次（即 1,558 次）。
- en: For each of the models fitted, the performance of the model (using measures
    such as accuracy) is calculated.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个拟合的模型，都会计算模型的性能（使用准确率等衡量指标）。
- en: The feature whose replacement has resulted in the smallest change in performance
    is removed permanently and *Step 2* is repeated with `n-1` features.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 替换后导致性能变化最小的特征将被永久移除，并且*步骤2*会在`n-1`个特征下重复。
- en: The process is then repeated with `n-2` features and so on.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，该过程会在`n-2`个特征下重复进行，以此类推。
- en: The algorithm keeps on eliminating features until the threshold number of features
    we require is reached.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该算法不断地消除特征，直到达到我们所需的特征数量阈值。
- en: Let's take a look at the backward feature elimination algorithm in action for
    the augmented ads dataset in the next exercise.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下在下一个练习中，后向特征消除算法如何在扩展的广告数据集上应用。
- en: 'Exercise 14.02: Dimensionality Reduction Using Backward Feature Elimination'
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 14.02：使用后向特征消除进行降维
- en: In this exercise, we will fit a logistic regression model after eliminating
    features using the backward elimination technique to find the accuracy of the
    model. We will be using the same ads dataset as before, and we will be enhancing
    it with additional features for this exercise.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将在消除特征后，使用后向消除技巧拟合一个逻辑回归模型，以找到该模型的准确度。我们将使用之前相同的广告数据集，并且在这个练习中我们将对其进行增强，添加更多特征。
- en: 'The following steps will help you complete this exercise:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个练习：
- en: Open a new Colab notebook file.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Colab笔记本文件。
- en: 'Implement all the initial steps similar to *Exercise 14.01*, *Loading and Cleaning
    the Dataset*, until scaling the dataset using the `minmaxscaler()` function:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现所有初始步骤，类似于*练习 14.01*，*加载和清理数据集*，直到使用`minmaxscaler()`函数对数据集进行缩放：
- en: '[PRE21]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, create a high-dimensional dataset. We''ll augment the dataset artificially
    by a factor of `2`. The process of backward feature elimination is a very compute-intensive
    process, and using higher dimensions will involve a longer processing time. This
    is why the augmenting factor has been kept at `2`. This is implemented using the
    following code snippet:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个高维数据集。我们将通过一个`2`的因子人工扩展数据集。后向特征消除是一个计算密集型过程，使用更高的维度会导致更长的处理时间。这就是为什么扩展因子保持为`2`的原因。该过程通过以下代码片段实现：
- en: '[PRE22]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You should get the following output:'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE23]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the backward elimination model. Backward elimination works by providing
    two arguments to the `RFE()` function, which is the model we want to try (logistic
    regression in our case) and the number of features we want the dataset to be reduced
    to. This is implemented as follows:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义后向消除模型。后向消除通过向`RFE()`函数提供两个参数来工作，这个函数是我们想要尝试的模型（在我们这个例子中是逻辑回归）以及我们希望数据集缩减到的特征数量。具体实现如下：
- en: '[PRE24]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this implementation, the number of features that we have given, `250`, is
    identified through trial and error. The process is to first assume an arbitrary
    number of features and then, based on the final metrics, arrive at the most optimum
    number of features for the model. In this implementation, our first assumption
    of `250` implies that we want the backward elimination model to start eliminating
    features until we get the best `250` features.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个实现中，我们给定的特征数量`250`是通过反复试验确定的。该过程是先假设一个任意数量的特征，然后根据最终的度量结果，确定最适合模型的特征数量。在这个实现中，我们对`250`的初步假设意味着我们希望后向消除模型开始消除特征，直到我们得到最佳的`250`个特征。
- en: Fit the backward elimination method to identify the best `250` features.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 适配后向消除法以识别最佳的`250`个特征。
- en: 'We are now ready to fit the backward elimination method on the higher-dimensional
    dataset. We will also note the time it takes for backward elimination to work.
    This is implemented using the following code snippet:'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们现在准备将后向消除法应用于高维数据集。我们还将记录后向消除法所需的时间。这是通过以下代码片段实现的：
- en: '[PRE25]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Fitting the backward elimination method is done using the `.fit()` function.
    We give the independent and dependent training sets.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`.fit()`函数来适配后向消除法。我们给定独立和依赖的训练集。
- en: Note
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The backward elimination method is a compute-intensive process, and therefore
    this process will take a lot of time to execute. The larger the number of features,
    the longer it will take.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 后向消除法是一个计算密集型过程，因此该过程将花费大量时间来执行。特征数量越大，所需时间就越长。
- en: 'The time for backward elimination is at the end of the notifications:'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 后向消除的时间会在通知的最后显示：
- en: '![Figure 14.14: The time taken for the backward elimination process'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.14：后向消除过程所花费的时间'
- en: '](img/B15019_14_14.jpg)'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_14.jpg)'
- en: 'Figure 14.14: The time taken for the backward elimination process'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.14：向后淘汰过程所需时间
- en: You can see that the backward elimination process to find the best `250` features
    has taken `230.35` seconds to implement.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到，向后淘汰法找到最佳`250`个特征的过程花费了`230.35`秒。
- en: 'Display the features identified using the backward elimination method. We can
    display the `250` features that were identified using the backward elimination
    process using the `get_support()` function. This is implemented as follows:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示通过向后淘汰法识别的特征。我们可以使用`get_support()`函数来显示通过向后淘汰法识别的`250`个特征。实现代码如下：
- en: '[PRE26]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You should get the following output:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.15: The identified features being displayed'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.15：展示已识别特征'
- en: '](img/B15019_14_15.jpg)'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_15.jpg)'
- en: 'Figure 14.15: The identified features being displayed'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.15：展示已识别特征
- en: These are the best `250` features that were finally selected using the backward
    elimination process from the entire dataset.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些是通过向后淘汰法从整个数据集中最终选择出的`250`个最佳特征。
- en: 'Now, split the dataset into training and testing sets for modeling:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将数据集划分为训练集和测试集以进行建模：
- en: '[PRE27]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should get the following output:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE28]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: From the output, you see the shapes of both the training set and testing sets.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从输出中，你可以看到训练集和测试集的形状。
- en: 'Transform the train and test sets. In *step 5*, we identified the top `250`
    features through backward elimination. Now we need to reduce the train and test
    sets to those top `250` features. This is done using the `.transform()` function.
    This is implemented using the following code snippet:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换训练集和测试集。在*步骤 5*中，我们通过向后淘汰法识别了前`250`个特征。现在，我们需要将训练集和测试集减少到这`250`个特征。这可以通过`.transform()`函数来完成。以下代码片段实现了这一点：
- en: '[PRE29]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You should get the following output:'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE30]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We can see that both the training set and test sets have been reduced to the
    `250` best features.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，训练集和测试集都已被缩减为`250`个最佳特征。
- en: 'Fit a logistic regression model on the training set and note the time:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上拟合逻辑回归模型，并记录时间：
- en: '[PRE31]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You should get the following output:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE32]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As expected, the total time it takes to fit a model on a reduced set of features
    is much lower than the time it took for the larger dataset in *Activity 14.01*,
    *Fitting a Logistic Regression Model on a HighDimensional Dataset*, which was
    `23.86` seconds. This is a great improvement.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如预期所示，在减少特征集后拟合模型所需的总时间比*活动 14.01*中对更大数据集进行拟合所需的时间要低，后者需要`23.86`秒。这是一个很大的改进。
- en: 'Now, predict on the test set and print the accuracy metrics, as shown in the
    following code snippet:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，对测试集进行预测并打印准确度指标，如以下代码片段所示：
- en: '[PRE33]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You should get the following output:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.16: The achieved accuracy of the logistic regression model'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.16：逻辑回归模型的准确度'
- en: '](img/B15019_14_16.jpg)'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_16.jpg)'
- en: 'Figure 14.16: The achieved accuracy of the logistic regression model'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.16：逻辑回归模型的准确度
- en: You can see that the accuracy measure for this model has improved compared to
    the one we got for the model with higher dimensionality, which was `0.97` in *Activity
    14.01*, *Fitting a Logistic Regression Model on a HighDimensional Dataset*. This
    increase could be attributed to the identification of non-correlated features
    from the complete feature set, which could have boosted the performance of the
    model.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到，与我们在*活动 14.01*中获得的高维数据集模型的准确率（`0.97`）相比，这个模型的准确度有所提高。这一提高可能归因于从完整特征集识别出的非相关特征，这可能提升了模型的表现。
- en: 'Print the confusion matrix:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印混淆矩阵：
- en: '[PRE34]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You should get the following output:'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.17: Confusion matrix'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.17：混淆矩阵'
- en: '](img/B15019_14_17.jpg)'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_17.jpg)'
- en: 'Figure 14.17: Confusion matrix'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.17：混淆矩阵
- en: 'Printing the classification report:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印分类报告：
- en: '[PRE35]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You should get the following output:'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.18: Classification matrix'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.18：分类矩阵'
- en: '](img/B15019_14_18.jpg)'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_18.jpg)'
- en: 'Figure 14.18: Classification matrix'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.18：分类矩阵
- en: Note
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/31ca5k6](https://packt.live/31ca5k6).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参考[https://packt.live/31ca5k6](https://packt.live/31ca5k6)。
- en: You can also run this example online at [https://packt.live/329yJkF](https://packt.live/329yJkF).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/329yJkF](https://packt.live/329yJkF)上在线运行此示例。
- en: As we can see from the backward elimination process, we were able to get an
    improved accuracy of `98%` with `250` features, compared to the model in *Activity
    14.01*, *Fitting a Logistic Regression Model on a HighDimensional Dataset,* where
    an artificially enhanced dataset was used and got an accuracy of `97%`.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从后向消除过程可以看出，我们能够通过 `250` 个特征将准确率提高到 `98%`，相比于 *活动 14.01* 中的模型，*在高维数据集上拟合逻辑回归模型*，当时使用了一个人为增强的数据集，准确率为
    `97%`。
- en: However, it should be noted that dimensionality reduction techniques should
    not be viewed as a method to improve the performance of any model. Dimensionality
    reduction techniques have to be viewed from the perspective of enabling us to
    fit a model on datasets with large numbers of features. When dimensions increase,
    fitting the model becomes intractable. This can be observed if the scaling factor
    used in *Activity 14.01*, *Fitting a Logistic Regression Model on a HighDimensional
    Dataset,* was to be increased from `300` to `500`. In such cases, fitting a model
    wouldn't happen with the current set of resources.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，应该注意的是，降维技术不应被视为提高任何模型性能的方法。降维技术应该从帮助我们在具有大量特征的数据集上拟合模型的角度来看待。当维度增加时，拟合模型变得不可行。如果将
    *活动 14.01* 中使用的缩放因子从 `300` 增加到 `500`，就可以观察到这种现象。在这种情况下，使用当前资源无法完成模型拟合。
- en: Dimensionality reduction aids in such scenarios by reducing the number of features,
    thereby enabling the fitting of a model on reduced dimensions without a large
    degradation of performance, and can sometimes lead to an improvement in results.
    However, it should also be noted that methods such as backward elimination are
    compute-intensive processes. You would have observed this phenomenon as to the
    time it takes in identifying the top 250 features when the scaling factor was
    just 2\. With much higher scaling factors, it will take far more time and resources
    to identify the top 250 features.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 降维在这种情况下的作用是通过减少特征数量，从而使得模型能够在减少维度的情况下进行拟合，并且不会导致性能的大幅下降，有时甚至会带来结果的改善。然而，也应该注意到，像后向消除这样的算法计算量大。你应该已经注意到，当缩放因子仅为
    2 时，识别前 250 个特征所需的时间。如果缩放因子更高，识别前 250 个特征所需要的时间和资源将大大增加。
- en: Having seen the backward elimination method, let's now look at the next technique,
    which is forward feature selection.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了后向消除方法后，我们来看看下一种技术，即前向特征选择。
- en: Forward Feature Selection
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前向特征选择
- en: 'Forward feature selection works in the reverse order as backward elimination.
    In this process, we start off with an initial feature, and features are added
    one by one until no improvement in performance is achieved. The detailed process
    is as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 前向特征选择的工作原理与后向消除相反。在这个过程中，我们从一个初始特征开始，特征逐个加入，直到性能不再提升为止。详细过程如下：
- en: Start model building with one feature.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个特征开始建立模型。
- en: Iterate the model building process *n* times, each time selecting one feature
    at a time. The feature that gives the highest improvement in performance is selected.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代模型构建过程 *n* 次，每次选择一个特征。选择能带来性能最大提升的特征。
- en: Once the first feature is selected, it is the time to select the second feature.
    The process for selecting the second feature proceeds exactly the same as *step
    2*. The remaining *n-1* features are iterated along with the first feature and
    the performance on the model is observed. The feature that produces the biggest
    improvement in model performance is selected as the second feature.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦选择了第一个特征，就可以开始选择第二个特征。选择第二个特征的过程与 *步骤 2* 完全相同。将剩余的 *n-1* 个特征与第一个特征一起迭代，并观察模型的性能。选择能带来最大性能提升的特征作为第二个特征。
- en: The iteration of features will continue until a threshold number of features
    we have determined is extracted.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征的迭代将继续，直到提取出我们设定的阈值数量的特征。
- en: The set of final features selected will be the ones that give the maximum model performance.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终选择的特征集合将是那些能提供最大模型性能的特征。
- en: Let's now implement this algorithm in the next exercise.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在下一个练习中实现这个算法。
- en: 'Exercise 14.03: Dimensionality Reduction Using Forward Feature Selection'
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 14.03：使用前向特征选择进行降维
- en: In this exercise, we will fit a logistic regression model by selecting the optimum
    features through forward feature selection and observing the performance of the
    model. We will be using the same ads dataset as before, and we will be enhancing
    it with additional features for this exercise.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将通过前向特征选择来选择最佳特征，并观察模型的性能，从而拟合一个逻辑回归模型。我们将使用与之前相同的广告数据集，并且在这个练习中通过增加额外的特征来增强数据集。
- en: 'The following steps will help you complete this exercise:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个练习：
- en: Open a new Colab notebook.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Colab 笔记本。
- en: 'Implement all the initial steps similar to *Exercise 14.01*, *Loading and Cleaning
    the Dataset*, up until scaling the dataset using `MinMaxScaler()`:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现所有与 *练习 14.01*，*加载和清理数据集* 类似的初步步骤，直到使用 `MinMaxScaler()` 进行数据集缩放：
- en: '[PRE36]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Create a high-dimensional dataset. Now, augment the dataset artificially to
    a factor of `50`. Augmenting the dataset to higher factors will result in the
    notebook crashing because of lack of memory. This is implemented using the following
    code snippet:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个高维数据集。现在，将数据集人工扩展到 `50` 的倍数。将数据集扩展到更高倍数会导致笔记本因内存不足而崩溃。可以通过以下代码片段来实现：
- en: '[PRE37]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You should get the following output:'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '[PRE38]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Split the high dimensional dataset into training and testing sets:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将高维数据集拆分为训练集和测试集：
- en: '[PRE39]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now we define the threshold features. Once the train and test sets are created,
    the next step is to import the feature selection function, `SelectKBest`. The
    argument we give to this function is the number of features we want. The features
    are selected through experimentation and, as a first step, we assume a threshold
    value. In this example, we assume a threshold value of `250`. This is implemented
    using the following code snippet:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们定义阈值特征。一旦训练集和测试集创建完成，下一步是导入特征选择函数 `SelectKBest`。我们给这个函数的参数是我们想要的特征数量。特征是通过实验选择的，作为第一步，我们假设一个阈值。在这个例子中，我们假设阈值为
    `250`。可以通过以下代码片段来实现：
- en: '[PRE40]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Iterate and get the best set of threshold features. Based on the threshold
    set of features we defined, we have to fit the training set and get the best set
    of threshold features. Fitting on the training set is done using the `.fit()`
    function. We also note the time it takes to find the best set of features. This
    is executed using the following code snippet:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代并获取最佳的阈值特征集。根据我们定义的阈值特征集，我们必须拟合训练集并获取最佳的阈值特征集。训练集拟合使用 `.fit()` 函数进行。我们还需要记录找到最佳特征集所花费的时间。可以通过以下代码片段来执行：
- en: '[PRE41]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You should get something similar to the following output:'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下类似的输出：
- en: '[PRE42]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We can see that the forward selection method has taken around `2.68` seconds,
    which is much lower than the backward selection method.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，前向选择方法大约花费了 `2.68` 秒，这比反向选择方法的时间要短得多。
- en: 'Create new training and test sets. Once we have identified the best set of
    features, we have to modify our training and test sets so that they have only
    those selected features. This is accomplished using the `.transform()` function:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建新的训练集和测试集。一旦我们确定了最佳特征集，我们需要修改训练集和测试集，使它们只包含这些选定的特征。可以通过 `.transform()` 函数来完成：
- en: '[PRE43]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let''s verify the shapes of the train and test sets before transformation and
    after transformation:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在转换前和转换后验证训练集和测试集的形状：
- en: '[PRE44]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You should get the following output:'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '![Figure 14.19: Shape of the training and testing datasets'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.19：训练集和测试集的数据形状'
- en: '](img/B15019_14_19.jpg)'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_19.jpg)'
- en: 'Figure 14.19: Shape of the training and testing datasets'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.19：训练集和测试集的数据形状
- en: You can see that both the training and test sets are reduced to `250` features each.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到，训练集和测试集的特征数都减少到每个 `250` 个特征。
- en: 'Let''s now fit a logistic regression model on the transformed dataset and note
    the time it takes to fit the model:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们在转换后的数据集上拟合一个逻辑回归模型，并记录拟合模型所需的时间：
- en: '[PRE45]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Print the total time:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印总时间：
- en: '[PRE46]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You should get the following output:'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '[PRE47]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: You can see that the training time is much less than the model that was fit
    in *Activity 14.01*, *Fitting a Logistic Regression Model on a HighDimensional
    Dataset*, which was `23.86` seconds. This shorter time is attributed to the number
    of features in the forward selection model.
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到，训练时间比在 *活动 14.01*，*在高维数据集上拟合逻辑回归模型* 中拟合的模型要少，后者花费了 `23.86` 秒。这较短的时间归因于前向选择模型中的特征数量。
- en: 'Now, perform predictions on the test set and print the accuracy metrics:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，对测试集进行预测并打印准确率指标：
- en: '[PRE48]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You should get the following output:'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE49]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Print the confusion matrix:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印混淆矩阵：
- en: '[PRE50]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You should get something similar to the following output:'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到类似以下输出：
- en: '![Figure 14.20: Resulting confusion matrix'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图14.20：结果混淆矩阵'
- en: '](img/B15019_14_20.jpg)'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_20.jpg)'
- en: 'Figure 14.20: Resulting confusion matrix'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.20：结果混淆矩阵
- en: 'Print the classification report:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印分类报告：
- en: '[PRE51]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You should get something similar to the following output:'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到类似以下输出：
- en: '![Figure 14.21: Resulting classification report'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图14.21：结果分类报告'
- en: '](img/B15019_14_21.jpg)'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_21.jpg)'
- en: 'Figure 14.21: Resulting classification report'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.21：结果分类报告
- en: Note
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2YhQE7X](https://packt.live/2YhQE7X).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2YhQE7X](https://packt.live/2YhQE7X)。
- en: This section does not currently have an online interactive example, but can
    be run as usual on Google Colab.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线交互示例，但可以像往常一样在Google Colab上运行。
- en: As we can see from the forward selection process, we were able to get an accuracy
    score of `94%` with `250` features. This score is lower than the one that was
    achieved with the backward elimination method (`98%`) and also the benchmark model
    (`97%`) built in *Activity 14.01*, *Fitting a Logistic Regression Model on a HighDimensional
    Dataset*. However, the time taken to find the best features (2.68 seconds) was
    substantially less than the backward elimination method (230.35 seconds).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从前向选择过程可以看到的，我们能够在使用`250`个特征时获得`94%`的准确率。这一得分低于使用后向消除法（`98%`）和在*Activity
    14.01*中构建的基准模型（`97%`）所取得的分数，*在高维数据集上拟合逻辑回归模型*。然而，寻找最佳特征所花费的时间（2.68秒）远低于后向消除法（230.35秒）。
- en: In the next section, we will be looking at Principal Component Analysis (PCA).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将介绍主成分分析（PCA）。
- en: Principal Component Analysis (PCA)
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: PCA is a very effective dimensionality reduction technique that achieves dimensionality
    reduction without compromising on the information content of the data. The basic
    idea behind PCA is to first identify correlations among different variables within
    the dataset. Once correlations are identified, the algorithm decides to eliminate
    the variables in such a way that the variability of the data is maintained. In
    other words, PCA aims to find uncorrelated sources of data.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种非常有效的降维技术，它在不损害数据内容的情况下实现降维。PCA的基本思想是首先识别数据集内不同变量之间的相关性。一旦识别出相关性，算法会决定以一种方式去除变量，以保持数据的变异性。换句话说，PCA旨在找到不相关的数据源。
- en: Implementing PCA on raw variables results in transforming them into a completely
    new set of variables called principal components. Each of these components represents
    variability in data along an axes that are orthogonal to each other. This means
    that the first axis is fit in the direction where the maximum variability of data
    is present. After this, the second axis is selected in such a way that the axis
    is orthogonal (perpendicular) to the first selected axis and also covers the next
    highest variability.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始变量上实施PCA会将其转化为一组全新的变量，称为主成分。每个主成分代表数据在与其他主成分正交的轴向上的变异性。这意味着第一个轴会沿着数据变异性最大的位置进行拟合。接下来，第二个轴的选择方式是它与第一个选定的轴正交（垂直），并覆盖下一个最高的变异性。
- en: Let's look at the idea of PCA with an example.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例来看一下PCA的概念。
- en: 'We will create a sample dataset with 2 variables and 100 random data points
    in each variable. Random data points are created using the `rand()` function.
    This is implemented in the following code:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个包含2个变量和每个变量100个随机数据点的样本数据集。使用`rand()`函数生成随机数据点。以下代码实现了这一过程：
- en: '[PRE52]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The resulting output is: `(100, 2)`.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出为：`(100, 2)`。
- en: Note
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A random state is defined using the `RandomState(123)` function. This is defined
    to ensure that anyone who reproduces this example gets the same output.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`RandomState(123)`函数定义一个随机状态。这是为了确保任何复现此示例的人都能得到相同的输出。
- en: 'Let''s visualize this data using `matplotlib`:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`matplotlib`可视化这些数据：
- en: '[PRE53]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'You should get the following output:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE54]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![Figure 14.22: Visualization of the data'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.22：数据可视化'
- en: '](img/B15019_14_22.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_14_22.jpg)'
- en: 'Figure 14.22: Visualization of the data'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.22：数据可视化
- en: In the graph, we can see that the data is evenly spread out.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，我们可以看到数据均匀地分布开来。
- en: Let's now find the principal components for this dataset. We will reduce this
    two-dimensional dataset into a one-dimensional dataset. In other words, we will
    reduce the original dataset into one of its principal components.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们为这个数据集找到主成分。我们将把这个二维数据集降维为一维数据集。换句话说，我们将把原始数据集压缩为它的一个主成分。
- en: 'This is implemented in code as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 代码实现如下：
- en: '[PRE55]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'You should get the following output:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE56]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: As we can see in the code, we first define the number of components using the
    `'n_components' = 1` argument. After this, the PCA algorithm is fit on the input
    dataset. After fitting on the input data, the initial dataset is transformed into
    a new dataset with only one variable, which is its principal component.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如代码中所示，我们首先通过`'n_components' = 1`参数定义主成分的数量。之后，PCA算法会对输入数据集进行拟合。拟合后，初始数据集被转换为一个只有一个变量的新数据集，这个变量就是它的主成分。
- en: The algorithm transforms the original dataset into its first principal component
    by using an axis where the data has the largest variability.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通过使用数据变异性最大化的轴将原始数据集转换为其第一个主成分。
- en: 'To visualize this concept, let''s reverse the transformation of the `X_pca`
    dataset to its original form and then visualize this data along with the original
    data. To reverse the transformation, we use the `.inverse_transform()` function:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这个概念，假设我们将`X_pca`数据集从其原始形式进行逆转化，然后将这个数据与原始数据一起可视化。为了逆转化，我们使用`.inverse_transform()`函数：
- en: '[PRE57]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'You should get the following output:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.23: Plot with reverse transformation'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.23：带有逆转化的图示'
- en: '](img/B15019_14_23.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_14_23.jpg)'
- en: 'Figure 14.23: Plot with reverse transformation'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.23：带有逆转化的图示
- en: As we can see in the plot, the data points in orange represent an axis with
    the highest variability. All the data points were projected to that axis to generate
    the first principal component.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在图中所看到的，橙色的数据点代表了具有最大变异性的轴。所有数据点都被投影到该轴上，从而生成第一个主成分。
- en: The data points that are generated when transforming into various principal
    components will be very different from the original data points before transformation.
    Each principal component will be in an axis that is orthogonal (perpendicular)
    to the other principal component. If a second principal component was generated
    for the preceding example, the second principal component would be along an axis
    indicated by the blue arrow in the graph. The way we pick the number of principal
    components for model building is by selecting the number of components that explains
    a certain threshold of variability.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据在不同的主成分中转换时，生成的数据点会与转换前的原始数据点有很大不同。每个主成分都会位于与其他主成分正交（垂直）的轴上。如果为上述示例生成第二个主成分，那么第二个主成分将在图中蓝色箭头所示的轴上。我们为模型构建选择主成分的方式是选择那些解释了某个方差阈值的主成分数量。
- en: For example, if there were originally 1,000 features and we reduced it to 100
    principal components, and then we find that out of the 100 principal components
    the first 75 components explain 90% of the variability of data, we would pick
    those 75 components to build the model. This process is called picking principal
    components with the percentage of variance explained.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果原本有1,000个特征，我们将其减少到100个主成分，然后发现这100个主成分中，前75个主成分解释了数据90%的变异性，那么我们就选择这75个主成分来构建模型。这个过程叫做根据解释的方差百分比选择主成分。
- en: Let's now see how to use PCA as a tool for dimensionality reduction in our use
    case.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在我们的用例中使用PCA作为降维工具。
- en: 'Exercise 14.04: Dimensionality Reduction Using PCA'
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 14.04：使用PCA进行降维
- en: In this exercise, we will fit a logistic regression model by selecting the principal
    components that explain the maximum variability of the data. We will also observe
    the performance of the feature selection and model building process. We will be
    using the same ads dataset as before, and we will be enhancing it with additional
    features for this exercise.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将通过选择解释最大数据变异性的主成分来拟合一个逻辑回归模型。我们还将观察特征选择和模型构建过程的表现。我们将使用之前的相同广告数据集，并在此练习中增加一些额外的特征。
- en: 'The following steps will help you complete this exercise:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个练习：
- en: Open a new Colab notebook file.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Colab笔记本文件。
- en: 'Implement the initial steps from *Exercise 14.01*, *Loading and Cleaning the
    Dataset*, up until scaling the dataset using the `minmaxscaler()` function:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现*练习 14.01*中的初始步骤，*加载和清理数据集*，直到使用`minmaxscaler()`函数进行数据集缩放：
- en: '[PRE58]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Create a high-dimensional dataset. Let''s now augment the dataset artificially
    to a factor of 50\. Augmenting the dataset to higher factors will result in the
    notebook crashing because of a lack of memory. This is implemented using the following
    code snippet:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个高维数据集。现在，我们将人工增加数据集的大小，倍增到50倍。将数据集增大到更高的倍数将导致笔记本崩溃，因为内存不足。这是通过以下代码片段实现的：
- en: '[PRE59]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: You should get the following output
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE60]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Let''s split the high-dimensional dataset to training and test sets:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将高维数据集拆分为训练集和测试集：
- en: '[PRE61]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Let''s now fit the PCA function on the training set. This is done using the
    `.fit()` function, as shown in the following snippet. We will also note the time
    it takes to fit the PCA model on the dataset:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将在训练集上拟合PCA函数。这是通过`.fit()`函数完成的，代码如下所示。我们还将记录在数据集上拟合PCA模型所需的时间：
- en: '[PRE62]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'You should get the following output:'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE63]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: We can see that the time taken to fit the PCA function on the dataset is less
    than the backward elimination model (230.35 seconds) and higher than the forward
    selection method (2.682 seconds).
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，在数据集上拟合PCA函数所花的时间少于后向淘汰模型（230.35秒），但高于前向选择方法（2.682秒）。
- en: 'We will now determine the number of principal components by plotting the cumulative
    variance explained by all the principal components. The variance explained is
    determined by the `pca.explained_variance_ratio_` method. This is plotted in `matplotlib`
    using the following code snippet:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将通过绘制所有主成分所解释的累计方差来确定主成分的数量。方差的解释由`pca.explained_variance_ratio_`方法确定。这个结果通过以下代码片段在`matplotlib`中绘制：
- en: '[PRE64]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: In the code, the `np.cumsum()` function is used to get the cumulative variance
    of each principal component.
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在代码中，`np.cumsum()`函数用于获取每个主成分的累计方差。
- en: 'You will get the following plot as output:'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将得到以下图形作为输出：
- en: '![Figure 14.24: The variance graph'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图14.24：方差图'
- en: '](img/B15019_14_24.jpg)'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_24.jpg)'
- en: 'Figure 14.24: The variance graph'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.24：方差图
- en: From the plot, we can see that the first `250` principal components explain
    more than `90%` of the variance. Based on this graph, we can decide how many principal
    components we want to have depending on the variability it explains. Let's select
    `250` components for fitting our model.
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从图中，我们可以看到前`250`个主成分解释了超过`90%`的方差。根据这个图，我们可以决定希望保留多少个主成分，取决于它所解释的变异性。我们选择`250`个主成分来拟合我们的模型。
- en: 'Now that we have identified that `250` components explain a lot of the variability,
    let''s refit the training set for `250` components. This is described in the following
    code snippet:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经确认`250`个主成分解释了大量的变异性，让我们用`250`个主成分重新拟合训练集。这在以下代码片段中描述：
- en: '[PRE65]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We now transform the training and test sets with the 200 principal components:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用200个主成分来转换训练集和测试集：
- en: '[PRE66]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Let''s verify the shapes of the train and test sets before transformation and
    after transformation:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在转换之前和转换之后，让我们验证训练集和测试集的形状：
- en: '[PRE67]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'You should get the following output:'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.25: Transformed and the original training and testing sets'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图14.25：转换后的训练集和测试集以及原始训练集和测试集'
- en: '](img/B15019_14_25.jpg)'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_25.jpg)'
- en: 'Figure 14.25: Transformed and the original training and testing sets'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.25：转换后的训练集和测试集以及原始训练集和测试集
- en: You can see that both the training and test sets are reduced to `250` features each.
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到，训练集和测试集都被缩减为各自`250`个特征。
- en: 'Let''s now fit the logistic regression model on the transformed dataset and
    note the time it takes to fit the model:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将在转换后的数据集上拟合逻辑回归模型，并记录拟合模型所需的时间：
- en: '[PRE68]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Print the total time:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印总时间：
- en: '[PRE69]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'You should get the following output:'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE70]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: You can see that the training time is much lower than the model that was fit
    in *Activity 14.01*, *Fitting a Logistic Regression Model on a HighDimensional
    Dataset*, which was 23.86 seconds. The shorter time is attributed to the smaller
    number of features, `250`, selected in PCA.
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到，训练时间远低于在*活动 14.01*中拟合的模型，*在高维数据集上拟合逻辑回归模型*，其时间为 23.86 秒。这个更短的时间归因于在 PCA
    中选择了更少的特征，即`250`个特征。
- en: 'Now, predict on the test set and print the accuracy metrics:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在测试集上进行预测，并打印出准确性指标：
- en: '[PRE71]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'You should get the following output:'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.26: Accuracy of the logistic regression model'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.26：逻辑回归模型的准确性'
- en: '](img/B15019_14_26.jpg)'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_26.jpg)'
- en: 'Figure 14.26: Accuracy of the logistic regression model'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.26：逻辑回归模型的准确性
- en: You can see that the accuracy level is better than the benchmark model with
    all the features (`97%`) and the forward selection model (`94%`).
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到，所有特征的准确性（`97%`）和前向选择模型（`94%`）的准确性均优于基准模型。
- en: 'Print the confusion matrix:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印混淆矩阵：
- en: '[PRE72]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'You should get the following output:'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.27: Resulting confusion matrix'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.27：结果混淆矩阵'
- en: '](img/B15019_14_27.jpg)'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_27.jpg)'
- en: 'Figure 14.27: Resulting confusion matrix'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.27：结果混淆矩阵
- en: 'Print the classification report:'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印分类报告：
- en: '[PRE73]'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'You should get the following output:'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.28: Resulting classification matrix'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.28：结果分类矩阵'
- en: '](img/B15019_14_28.jpg)'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_28.jpg)'
- en: 'Figure 14.28: Resulting classification matrix'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.28：结果分类矩阵
- en: Note
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3iXNVbq](https://packt.live/3iXNVbq).
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本节的源代码，请参考[https://packt.live/3iXNVbq](https://packt.live/3iXNVbq)。
- en: This section does not currently have an online interactive example, but can
    be run as usual on Google Colab.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线交互示例，但可以像往常一样在 Google Colab 上运行。
- en: As is evident from the results, we get a score of 98%, which is better than
    the benchmark model. One reason that could be attributed to the higher performance
    could be the creation of uncorrelated principal components using the PCA method,
    which has boosted the performance.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果可以明显看出，我们得到了 98% 的分数，优于基准模型。其表现较高的一个原因可能是通过 PCA 方法创建了无关的主成分，从而提升了性能。
- en: In the next section, we will be looking at Independent Component Analysis (ICA).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论独立成分分析（ICA）。
- en: Independent Component Analysis (ICA)
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独立成分分析（ICA）
- en: ICA is a technique of dimensionality reduction that conceptually follows a similar
    path as PCA. Both ICA and PCA try to derive new sources of data by linearly combining
    the original data.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: ICA 是一种降维技术，其概念上与 PCA 路径相似。ICA 和 PCA 都试图通过线性组合原始数据来推导新的数据源。
- en: However, the difference between them lies in the method they use to find new
    sources of data. While PCA attempts to find uncorrelated sources of data, ICA
    attempts to find independent sources of data.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们之间的区别在于它们用于寻找新数据源的方法。PCA 尝试寻找无关的数据源，而 ICA 则尝试寻找独立的数据源。
- en: ICA has a very similar implementation for dimensionality reduction as PCA.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: ICA 在降维方面有与 PCA 非常相似的实现。
- en: Let's look at the implementation of ICA for our use case.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 ICA 在我们用例中的实现。
- en: 'Exercise 14.05: Dimensionality Reduction Using Independent Component Analysis'
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 14.05：使用独立成分分析进行降维
- en: In this exercise, we will fit a logistic regression model using the ICA technique
    and observe the performance of the model. We will be using the same ads dataset
    as before, and we will be enhancing it with additional features for this exercise.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用 ICA 技术拟合一个逻辑回归模型，并观察模型的表现。我们将使用之前相同的广告数据集，并在本次练习中增加额外的特征。
- en: 'The following steps will help you complete this exercise:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成本次练习：
- en: Open a new Colab notebook file.
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Colab 笔记本文件。
- en: 'Implement all the steps from *Exercise 14.01*, *Loading and Cleaning the Dataset*,
    up until scaling the dataset using `MinMaxScaler()`:'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现 *练习 14.01*、*加载和清理数据集* 中的所有步骤，直到使用 `MinMaxScaler()` 对数据集进行缩放：
- en: '[PRE74]'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Let''s now augment the dataset artificially to a factor of `50`. Augmenting
    the dataset to factors that are higher than `50` will result in the notebook crashing
    because of a lack of memory. This is implemented using the following code snippet:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们将数据集人工扩展至 `50` 倍。将数据集扩展到超过 `50` 倍的因子将导致笔记本因内存不足而崩溃。以下代码片段实现了这一点：
- en: '[PRE75]'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'You should get the following output:'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE76]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Let''s split the high-dimensional dataset into training and testing sets:'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将高维数据集分割成训练集和测试集：
- en: '[PRE77]'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let''s load the ICA function, `FastICA`, and then define the number of components
    we require. We will use the same number of components that we used for PCA:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载 ICA 函数 `FastICA`，然后定义我们所需的成分数。我们将使用与 PCA 相同数量的成分：
- en: '[PRE78]'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Once the ICA method is defined, we will fit the method on the training set
    and also transform the training set to get a new training set with the required
    number of components. We will also note the time taken for fitting and transforming:'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦ICA方法被定义，我们将在训练集上拟合该方法，并将训练集转换为一个具有所需数量组件的新训练集。我们还会记录拟合和转换所需的时间：
- en: '[PRE79]'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: In the code, the `.fit()` function is used to fit on the training set and the
    `transform()` method is used to get a new training set with the required number
    of features.
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在代码中，`.fit()`函数用于在训练集上拟合，`transform()`方法用于获取具有所需特征数量的新训练集。
- en: 'You should get the following output:'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE80]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: We can see that implementing ICA has taken much more time than PCA (179.54 seconds).
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，实现ICA花费的时间远超过PCA（179.54秒）。
- en: 'We now transform the test set with the `250` components:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在使用`250`个成分对测试集进行转换：
- en: '[PRE81]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Let''s verify the shapes of the train and test sets before transformation and
    after transformation:'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在转换前后验证训练集和测试集的形状：
- en: '[PRE82]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'You should get the following output:'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.29: Shape of the original and transformed datasets'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图14.29：原始数据集和转换后数据集的形状'
- en: '](img/B15019_14_29.jpg)'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_29.jpg)'
- en: 'Figure 14.29: Shape of the original and transformed datasets'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.29：原始数据集和转换后数据集的形状
- en: You can see that both the training and test sets are reduced to `250` features each.
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到训练集和测试集都被减少到每个`250`个特征。
- en: 'Let''s now fit the logistic regression model on the transformed dataset and
    note the time it takes:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们在转换后的数据集上拟合逻辑回归模型，并记录所花费的时间：
- en: '[PRE83]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Print the total time:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印总时间：
- en: '[PRE84]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'You should get the following output:'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE85]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Let''s now predict on the test set and print the accuracy metrics:'
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们对测试集进行预测，并打印准确性指标：
- en: '[PRE86]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'You should get the following output:'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE87]'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: We can see that the ICA model has worse results than other models.
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，ICA模型的结果比其他模型差。
- en: 'Print the confusion matrix:'
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印混淆矩阵：
- en: '[PRE88]'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'You should get the following output:'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.30: Resulting confusion matrix'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图14.30：结果混淆矩阵'
- en: '](img/B15019_14_30.jpg)'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_30.jpg)'
- en: 'Figure 14.30: Resulting confusion matrix'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.30：结果混淆矩阵
- en: We can see that the ICA model has done a poor job in classifying the ads. All
    the examples have been wrongly classified as non-ads. We can conclude that ICA
    is not suitable for this dataset.
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，ICA模型在广告分类方面表现不佳。所有的示例都被错误地分类为非广告。我们可以得出结论，ICA不适用于此数据集。
- en: 'Print the classification report:'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印分类报告：
- en: '[PRE89]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'You should get the following output:'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.31: Resulting classification report'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图14.31：结果分类报告'
- en: '](img/B15019_14_31.jpg)'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_31.jpg)'
- en: 'Figure 14.31: Resulting classification report'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.31：结果分类报告
- en: Note
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/325H88Q](https://packt.live/325H88Q).
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/325H88Q](https://packt.live/325H88Q)。
- en: This section does not currently have an online interactive example, but can
    be run as usual on Google Colab.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线交互示例，但可以在Google Colab上照常运行。
- en: As we can see, transforming the data to its first 250 independent components
    did not capture all the necessary variability in the data. This has resulted in
    the degradation of the classification results for this method. We can conclude
    that ICA is not suitable for this dataset.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，将数据转换为前250个独立成分并未捕获数据中所有必要的变异性。这导致了该方法分类结果的下降。我们可以得出结论，ICA不适用于此数据集。
- en: It was also observed that the time taken to find the best independent features
    was longer than for PCA. However, it should be noted that different methods vary
    in results according to the input data. Even though ICA was not suitable for this
    dataset, it still is a potent method for dimensionality reduction that should
    be in the repertoire of a data scientist.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 还观察到，找到最佳独立特征所需的时间比PCA要长。然而，应该注意的是，不同的方法根据输入数据会有不同的结果。尽管ICA不适合此数据集，但它仍然是一个强大的降维方法，应该是数据科学家工具库中的一部分。
- en: 'From this exercise, you may come up with a few questions:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个练习，你可能会提出几个问题：
- en: How do you think we can improve the classification results using ICA?
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你认为我们如何利用ICA改善分类结果？
- en: Increasing the number of components results in a marginal increase in the accuracy
    metrics.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加成分数量会导致准确性指标的边际提高。
- en: Are there any other side effects because of the strategy adopted to improve
    the results?
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于提高结果的策略是否会产生其他副作用？
- en: Increasing the number of components also results in a longer training time for
    the logistic regression model.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 增加组件数量也会导致逻辑回归模型的训练时间延长。
- en: Factor Analysis
  id: totrans-494
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 因子分析
- en: Factor analysis is a technique that achieves dimensionality reduction by grouping
    variables that are highly correlated. Let's look at an example from our context
    of predicting advertisements.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 因子分析是一种通过将高度相关的变量进行分组来实现降维的技术。让我们来看一个来自广告预测的示例。
- en: In our dataset, there could be many features that describe the geometry (the
    size and shape of an image in the ad) of the images on a web page. These features
    can be correlated because they refer to specific characteristics of an image.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中，可能有许多特征描述了网页上广告图像的几何形状（图像的大小和形状）。这些特征可能是相关的，因为它们指向图像的特定特征。
- en: Similarly, there could be many features that describe the anchor text or phrases
    occurring in a URL, which are highly correlated. Factor analysis looks at correlated
    groups such as these from the data and then groups them into latent factors. Therefore,
    if there are 10 raw features describing the geometry of an image, factor analysis
    will group them into one feature that characterizes the geometry of an image.
    Each of these groups is called factors. As many correlated features are combined
    to form a group, the resulting number of features will be much smaller in comparison
    with the original dimensions of the dataset.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，可能有许多特征描述了 URL 中出现的锚文本或短语，这些特征高度相关。因子分析从数据中查看这些相关的组，并将它们分组为潜在因子。因此，如果有 10
    个原始特征描述图像的几何形状，因子分析将它们分组为一个特征，来描述图像的几何形状。这些组中的每一个都被称为因子。由于许多相关特征被组合成一个组，最终的特征数量将比数据集的原始维度要小得多。
- en: Let's now see how factor analysis can be implemented as a technique for dimensionality
    reduction.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何将因子分析作为一种降维技术来实现。
- en: 'Exercise 14.06: Dimensionality Reduction Using Factor Analysis'
  id: totrans-499
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 14.06：使用因子分析进行降维
- en: In this exercise, we will fit a logistic regression model after reducing the
    original dimensions to some key factors and then observe the performance of the
    model.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将在将原始维度降至一些关键因子后，拟合一个逻辑回归模型，然后观察模型的表现。
- en: 'The following steps will help you complete this exercise:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个练习：
- en: Open a new Colab notebook file.
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Colab 笔记本文件。
- en: 'Implement the same initial steps from *Exercise 14.01*, *Loading and Cleaning
    the Dataset*, up until scaling the dataset using the `minmaxscaler()` function:'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现与*练习 14.01*、*加载和清理数据集*相同的初始步骤，直到使用 `minmaxscaler()` 函数对数据集进行缩放：
- en: '[PRE90]'
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Let''s now augment the dataset artificially to a factor of `50`. Augmenting
    the dataset to factors that are higher than `50` will result in the notebook crashing
    because of a lack of memory. This is implemented using the following code snippet:'
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们将数据集人工增大到 `50` 倍。将数据集增大到超过 `50` 的因子数会导致笔记本崩溃，因为内存不足。实现代码片段如下：
- en: '[PRE91]'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'You should get the following output:'
  id: totrans-507
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE92]'
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Let''s split the high-dimensional dataset into train and test sets:'
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将高维数据集拆分为训练集和测试集：
- en: '[PRE93]'
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'An important step in factor analysis is defining the number of factors in a
    dataset. This step is achieved through experimentation. In our case, we will arbitrarily
    assume that there are `20` factors. This is implemented as follows:'
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因子分析中的一个重要步骤是定义数据集中的因子数量。这个步骤是通过实验来实现的。在我们的案例中，我们将任意假设有 `20` 个因子。实现方式如下：
- en: '[PRE94]'
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: The number of factors is defined through the `n_components` argument. We also
    define a random state for reproducibility.
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因子的数量是通过 `n_components` 参数来定义的。我们还为可重复性定义了随机状态。
- en: 'Once the factor method is defined, we will fit the method on the training set
    and also transform the training set to get a new training set with the required
    number of factors. We will also note the time it takes to fit the required number
    of factors:'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦定义了因子方法，我们将把方法应用于训练集，并将训练集转化为一个新的训练集，得到所需数量的因子。我们还将记录拟合所需因子数量的时间：
- en: '[PRE95]'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: In the code, the `.fit()` function is used to fit on the training set, and the
    `transform()` method is used to get a new training set with the required number
    of factors.
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在代码中，`.fit()` 函数用于拟合训练集，`transform()` 方法用于获取一个新的训练集，该训练集具有所需数量的因子。
- en: 'You should get the following output:'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE96]'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Factor analysis is also a compute-intensive method. This is the reason that
    only 20 factors were selected. We can see that it has taken `130.688` seconds
    for `20` factors.
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因子分析也是一种计算密集型方法。这也是只选择 20 个因子的原因。我们可以看到，它已经花费了 `130.688` 秒来处理 `20` 个因子。
- en: 'We now transform the test set with the same number of factors:'
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用相同数量的因子来转换测试集：
- en: '[PRE97]'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Let''s verify the shapes of the train and test sets before transformation and
    after transformation:'
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在转换前后，我们来验证训练集和测试集的形状：
- en: '[PRE98]'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'You should get the following output:'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.32: Original and transformed dataset values'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.32：原始和转换后的数据集值'
- en: '](img/B15019_14_32.jpg)'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_32.jpg)'
- en: 'Figure 14.32: Original and transformed dataset values'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.32：原始和转换后的数据集值
- en: You can see that both the training and test sets have been reduced to `20` factors each.
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到训练集和测试集都已经缩减为每个`20`个因子。
- en: 'Let''s now fit the logistic regression model on the transformed dataset and
    note the time it takes to fit the model:'
  id: totrans-529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来对转换后的数据集拟合逻辑回归模型，并记录拟合模型所需的时间：
- en: '[PRE99]'
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Print the total time:'
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印总时间：
- en: '[PRE100]'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'You should get the following output:'
  id: totrans-533
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE101]'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: We can see that the time it has taken to fit the logistic regression model is
    comparable with other methods.
  id: totrans-535
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，拟合逻辑回归模型所需的时间与其他方法相当。
- en: 'Let''s now predict on the test set and print the accuracy metrics:'
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来对测试集进行预测并打印准确率指标：
- en: '[PRE102]'
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'You should get the following output:'
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE103]'
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: We can see that the factor model has better results than the ICA model, but
    worse results than the other models.
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，因子模型的结果优于 ICA 模型，但比其他模型的结果差。
- en: 'Print the confusion matrix:'
  id: totrans-541
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印混淆矩阵：
- en: '[PRE104]'
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'You should get the following output:'
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.33: Resulting confusion matrix'
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.33：结果混淆矩阵'
- en: '](img/B15019_14_33.jpg)'
  id: totrans-545
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_33.jpg)'
- en: 'Figure 14.33: Resulting confusion matrix'
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.33：结果混淆矩阵
- en: We can see that the factor model has done a better job at classifying the ads
    than the ICA model. However, there is still a high number of false positives.
  id: totrans-547
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，因子模型在分类广告方面的表现优于 ICA 模型。然而，仍然有大量的假阳性。
- en: 'Print the classification report:'
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印分类报告：
- en: '[PRE105]'
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'You should get the following output:'
  id: totrans-550
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.34: Resulting classification report'
  id: totrans-551
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 14.34：结果分类报告'
- en: '](img/B15019_14_34.jpg)'
  id: totrans-552
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15019_14_34.jpg)'
- en: 'Figure 14.34: Resulting classification report'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.34：结果分类报告
- en: Note
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/32b9SNk](https://packt.live/32b9SNk).
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定章节的源代码，请参考 [https://packt.live/32b9SNk](https://packt.live/32b9SNk)。
- en: This section does not currently have an online interactive example, but can
    be run as usual on Google Colab.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线交互示例，但可以像往常一样在 Google Colab 上运行。
- en: As we can see in the results, by reducing the variables to just 20 factors,
    we were able to get a decent classification result. Even though there is degradation
    on the result, we still have a manageable number of features, which will be able
    to scale well on any algorithm. The balance between the accuracy measures and
    the ability to manage features needs to be explored through greater experimentation.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在结果中看到的，通过将变量减少到仅 20 个因子，我们能够获得不错的分类结果。尽管结果有所退化，但我们仍然有一个可管理的特征数量，这将在任何算法中很好地扩展。准确度度量与特征管理能力之间的平衡需要通过更多实验来探索。
- en: How do you think we can improve the classification results for factor analysis?
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为我们如何改进因子分析的分类结果？
- en: Well, increasing the number of components results in an increase in the accuracy metrics.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，增加组件的数量会导致准确率指标的提高。
- en: Comparing Different Dimensionality Reduction Techniques
  id: totrans-560
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较不同的降维技术
- en: Now that we have learned different dimensionality reduction techniques, let's
    apply all of these techniques to a new dataset that we will create from the existing
    ads dataset.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了不同的降维技术，让我们将所有这些技术应用到一个新的数据集上，这个数据集将从现有的广告数据集中创建。
- en: We will randomly sample some data points from a known distribution and then
    add these random samples to the existing dataset to create a new dataset. Let's
    carry out an experiment to see how a new dataset can be created from an existing
    dataset.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从已知分布中随机抽取一些数据点，然后将这些随机样本添加到现有数据集中，以创建一个新数据集。让我们进行一个实验，看看如何从现有数据集中创建一个新数据集。
- en: 'We import the necessary libraries:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入必要的库：
- en: '[PRE106]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: Next, we create a dummy data frame.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个虚拟数据框。
- en: 'We will use a small dataset with two rows and three columns for this example.
    We use the `pd.np.array()` function to create a data frame:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个包含两行三列的小数据集作为本示例。我们使用`pd.np.array()`函数创建数据框：
- en: '[PRE107]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'You should get the following output:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 14.35: Sample data frame'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.35：示例数据框'
- en: '](img/B15019_14_35.jpg)'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_14_35.jpg)'
- en: 'Figure 14.35: Sample data frame'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.35：示例数据框
- en: What we will do next is sample some data points with the same shape as the data
    frame we created.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将采样一些数据点，数据框的形状与我们创建的数据框相同。
- en: Let's sample some data points from a normal distribution that has mean `0` and
    standard deviation of `0.1`. We touched briefly on normal distributions in *Chapter
    3, Binary Classification.* A normal distribution has two parameters. The first
    one is the mean, which is the average of all the data in the distribution, and
    the second one is standard deviation, which is a measure of how spread out the
    data points are.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 从均值为`0`，标准差为`0.1`的正态分布中采样一些数据点。我们在*第3章，二分类*中简要提到了正态分布。正态分布有两个参数，第一个是均值，它是分布中所有数据的平均值；第二个是标准差，它是衡量数据点分布程度的指标。
- en: By assuming a mean and standard deviation, we will be able to draw samples from
    a normal distribution using the `np.random.normal()` Python function. The arguments
    that we have to give for this function are the mean, the standard deviation, and
    the shape of the new dataset.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 通过假设均值和标准差，我们将能够使用`np.random.normal()` Python函数从正态分布中绘制样本。我们需要为此函数提供的参数是均值、标准差和新数据集的形状。
- en: 'Let''s see how this is implemented in code:'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在代码中实现这一点：
- en: '[PRE108]'
  id: totrans-576
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'You should get the following output:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE109]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: As we can see, we give the mean (`mu`), standard deviation (`sigma`), and the
    shape of the data frame `[2,3]` to generate the new random samples.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们提供了均值（`mu`）、标准差（`sigma`）以及数据框形状`[2,3]`，以生成新的随机样本。
- en: 'Print the sampled data frame:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 打印采样的数据框：
- en: '[PRE110]'
  id: totrans-581
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'You will get something like the following output:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到类似以下的输出：
- en: '[PRE111]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'The next step is to add the original data frame and the sampled data frame
    to get the new dataset:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将原始数据框和采样数据框添加在一起，得到新的数据集：
- en: '[PRE112]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'You should get something like the following output:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到类似以下的输出：
- en: '[PRE113]'
  id: totrans-587
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: Having seen how to create a new dataset, let's use this knowledge in the next
    activity.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解如何创建新数据集后，让我们在接下来的活动中应用这些知识。
- en: 'Activity 14.02: Comparison of Dimensionality Reduction Techniques on the Enhanced
    Ads Dataset'
  id: totrans-589
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 14.02：在增强广告数据集上比较降维技术
- en: You have learned different dimensionality reduction techniques. You want to
    determine which is the best technique among them for a dataset you will create.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了不同的降维技术。现在你想确定哪种技术最适合你将要创建的数据集。
- en: '**Hint**: In this activity, we will use the different techniques that you have
    used in all the exercises so far. You will also create a new dataset as we did
    in the previous section.'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示**：在本活动中，我们将使用你迄今为止在所有练习中使用的不同技术。你还将像前一部分那样创建一个新的数据集。'
- en: 'The steps to complete this activity are as follows:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动的步骤如下：
- en: Open a new Colab notebook.
  id: totrans-593
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Colab 笔记本。
- en: Normalize the original ads data and derive the transformed independent variable,
    `X_tran`.
  id: totrans-594
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对原始广告数据进行归一化，并推导出转化后的自变量`X_tran`。
- en: Create a high-dimensional dataset by replicating the columns twice using the
    `pd.np.tile()` function.
  id: totrans-595
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pd.np.tile()`函数通过复制列两次来创建高维数据集。
- en: Create random samples from a normal distribution with mean = 0 and standard
    deviation = 0.1\. Make the new dataset with the same shape as the high-dimensional
    dataset created in *step 3*.
  id: totrans-596
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从均值=0，标准差=0.1的正态分布中创建随机样本。根据*第3步*创建的数据集形状，制作新的数据集。
- en: Add the high dimensional dataset and the random samples to get the new dataset.
  id: totrans-597
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加高维数据集和随机样本，得到新的数据集。
- en: Split the dataset into train and test sets.
  id: totrans-598
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集分为训练集和测试集。
- en: 'Implement backward elimination with the following steps:'
  id: totrans-599
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下步骤实现向后消除法：
- en: Implement the backward elimination step using the `RFE()` function.
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`RFE()`函数实现向后消除法步骤。
- en: Use logistic regression as the model and select the best `300` features.
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用逻辑回归作为模型，选择最佳的`300`个特征。
- en: Fit the `RFE()` function on the training set and measure the time it takes to
    fit the RFE model on the training set.
  id: totrans-602
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练集上拟合`RFE()`函数，并测量拟合RFE模型所需的时间。
- en: Transform the train and test sets with the RFE model.
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用RFE模型变换训练集和测试集。
- en: Fit a logistic regression model on the transformed training set.
  id: totrans-604
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在变换后的训练集上拟合逻辑回归模型。
- en: Predict on the test set and print the accuracy score, confusion matrix, and
    classification report.
  id: totrans-605
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在测试集上进行预测，并打印准确率、混淆矩阵和分类报告。
- en: 'Implement the forward selection technique with the following steps:'
  id: totrans-606
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下步骤实现前向选择技术：
- en: Define the number of features using the `SelectKBest()` function. Select the
    best `300` features.
  id: totrans-607
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`SelectKBest()`函数定义特征数量。选择最佳的`300`个特征。
- en: Fit the forward selection on the training set using the `.fit()` function and
    note the time taken for the fit.
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`.fit()`函数在训练集上拟合前向选择，并记录拟合所需的时间。
- en: Transform both the training and test sets using the `.transform()` function.
  id: totrans-609
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`.transform()`函数变换训练集和测试集。
- en: Fit a logistic regression model on the transformed training set.
  id: totrans-610
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在变换后的训练集上拟合逻辑回归模型。
- en: Predict on the transformed test set and print the accuracy, confusion matrix,
    and classification report.
  id: totrans-611
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在变换后的测试集上进行预测，并打印准确率、混淆矩阵和分类报告。
- en: 'Implement PCA:'
  id: totrans-612
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现PCA：
- en: Define the principal components using the `PCA()` function. Use 300 components.
  id: totrans-613
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`PCA()`函数定义主成分。使用300个组件。
- en: Fit `PCA()` on the training set. Note the time.
  id: totrans-614
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练集上拟合`PCA()`。注意时间。
- en: Transform both the training set and test set to get the respective number of
    components for these datasets using the `.transform()` function.
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`.transform()`函数变换训练集和测试集，获取这些数据集的相应组件数量。
- en: Fit a logistic regression model on the transformed training set.
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在变换后的训练集上拟合逻辑回归模型。
- en: Predict on the transformed test set and print the accuracy, confusion matrix,
    and classification report.
  id: totrans-617
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在变换后的测试集上进行预测，并打印准确率、混淆矩阵和分类报告。
- en: 'Implement ICA:'
  id: totrans-618
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现ICA：
- en: Define independent components using the `FastICA()` function using `300` components.
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`FastICA()`函数并使用`300`个组件定义独立成分。
- en: Fit the independent components on the training set and transform the training
    set. Note the time for the implementation.
  id: totrans-620
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练集上拟合独立成分并变换训练集。记录实现的时间。
- en: Transform the test set to get the respective number of components for these
    datasets using the `.transform()` function.
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`.transform()`函数将测试集变换，获取这些数据集的相应组件数量。
- en: Fit a logistic regression model on the transformed training set.
  id: totrans-622
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在变换后的训练集上拟合逻辑回归模型。
- en: Predict on the transformed test set and print the accuracy, confusion matrix,
    and classification report.
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在变换后的测试集上进行预测，并打印准确率、混淆矩阵和分类报告。
- en: 'Implement factor analysis:'
  id: totrans-624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现因子分析：
- en: Define the number of factors using the `FactorAnalysis()` function and `30` factors.
  id: totrans-625
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`FactorAnalysis()`函数定义因子数量，并使用`30`个因子。
- en: Fit the factors on the training set and transform the training set. Note the
    time for the implementation.
  id: totrans-626
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练集上拟合因子并变换训练集。记录实现的时间。
- en: Transform the test set to get the respective number of components for these
    datasets using the `.transform()` function.
  id: totrans-627
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`.transform()`函数将测试集变换，获取这些数据集的相应组件数量。
- en: Fit a logistic regression model on the transformed training set.
  id: totrans-628
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在变换后的训练集上拟合逻辑回归模型。
- en: Predict on the transformed test set and print the accuracy, confusion matrix,
    and classification report.
  id: totrans-629
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在变换后的测试集上进行预测，并打印准确率、混淆矩阵和分类报告。
- en: Compare the outputs of all the methods.
  id: totrans-630
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较所有方法的输出结果。
- en: '**Expected Output**:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '**预期输出**：'
- en: 'An example summary table of the results is as follows:'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果的示例汇总表：
- en: '![Figure 14.36: Summary output of all the reduction techniques'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.36：所有降维技术的汇总输出'
- en: '](img/B15019_14_36.jpg)'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15019_14_36.jpg)'
- en: 'Figure 14.36: Summary output of all the reduction techniques'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.36：所有降维技术的汇总输出
- en: Note
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项
- en: 'The solution to the activity can be found here: [https://packt.live/2GbJloz](https://packt.live/2GbJloz).'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在这里找到：[https://packt.live/2GbJloz](https://packt.live/2GbJloz)。
- en: In this activity, we implemented five different methods of dimensionality reduction
    with a new dataset that we created from the internet ads dataset.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们使用从互联网广告数据集创建的新数据集实现了五种不同的降维方法。
- en: From the tabulated results, we can see that three methods (backward elimination,
    forward selection, and PCA) have got the same accuracy scores. Therefore, the
    selection criteria for the best method should be based on the time taken to get
    the reduced dimension. With these criteria, the forward selection method is the
    best method, followed by PCA.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 从表格化的结果中，我们可以看到三种方法（反向淘汰、前向选择和PCA）具有相同的准确度分数。因此，选择最佳方法的标准应该是基于降维所需的时间。根据这一标准，前向选择方法是最好的，其次是PCA。
- en: For the third place, we should strike a balance between accuracy and the time
    taken for dimensionality reduction. We can see that factor analysis and backward
    elimination have very close accuracy scores, 96% and 97% respectively. However,
    the time taken for backward elimination is quite large compared to factor analysis.
    Therefore, we should weigh our considerations toward factor analysis as the third
    best, even though the accuracy is marginally lower than backward elimination.
    The last spot should go to ICA because the accuracy is far lower than all the
    other methods.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第三名，我们应该在准确性和降维所花时间之间找到平衡。我们可以看到因子分析和反向淘汰的准确度非常接近，分别为96%和97%。然而，反向淘汰所需的时间比因子分析长得多。因此，我们应该将因子分析作为第三名，即使它的准确度略低于反向淘汰。最后的位置应归于ICA，因为它的准确度远低于其他所有方法。
- en: Summary
  id: totrans-641
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have learned about various techniques for dimensionality
    reduction. Let's summarize what we have learned in this chapter.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了各种降维技术。让我们总结一下本章所学内容。
- en: At the beginning of the chapter, we were introduced to the challenges inherent
    with some of the modern-day datasets in terms of scalability. To further learn
    about these challenges, we downloaded the Internet Advertisement dataset and did
    an activity where we witnessed the scalability challenges posed by a large dataset.
    In the activity, we artificially created a large dataset and fit a logistic regression
    model to it.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们介绍了现代数据集在可扩展性方面的一些挑战。为了更好地理解这些挑战，我们下载了互联网广告数据集，并进行了一项活动，在其中我们亲眼见证了大数据集所带来的可扩展性问题。在这个活动中，我们人工创建了一个大数据集，并对其进行了逻辑回归建模。
- en: In the subsequent sections, we were introduced to five different methods of
    dimensionality reduction.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续部分，我们介绍了五种不同的降维方法。
- en: '**Backward feature elimination** worked on the principle of eliminating features
    one by one until no major degradation of accuracy measures occurred. This method
    is computationally intensive, but we got better results than the benchmark model.'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向特征淘汰**是基于逐步淘汰特征，直到没有明显的准确度下降为止。这个方法计算量较大，但我们得到了比基准模型更好的结果。'
- en: '**Forward feature selection** goes in the opposite direction as backward elimination
    and selects one feature at a time to get the best set of features we predetermined.
    This method is also computationally intensive. We also found out that this method
    had marginally lower accuracy.'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向特征选择**与反向淘汰方法相反，它一次选择一个特征，以获得我们预定的最佳特征集。这个方法也计算量较大。我们还发现这个方法的准确度稍微低一些。'
- en: '**Principal component analysis** (**PCA**) aims at finding components that
    are orthogonal to each other and that best explain the variability of the data.
    We had better results with PCA than we got from the benchmark model.'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）旨在寻找彼此正交的成分，并最好地解释数据的变异性。我们使用PCA得到了比基准模型更好的结果。'
- en: '**Independent component analysis** (**ICA**) is similar to PCA; however, it
    differs in terms of the approach to the selection of components. ICA looks for
    independent components from the dataset. We saw that ICA achieved one of the worst
    results in our context.'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: '**独立成分分析**（**ICA**）与PCA相似；然而，它在选择成分的方式上有所不同。ICA从数据集中寻找独立的成分。我们看到，ICA在我们的实验中取得了最差的结果之一。'
- en: '**Factor analysis** was all about finding factors or groups of correlated features
    that best described the data. We achieved much better results than ICA with factor analysis.'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: '**因子分析**的核心是寻找最佳描述数据的相关特征组或因子。我们通过因子分析取得的结果远优于ICA。'
- en: The aim of this chapter was to equip you with a set of techniques that help
    in scenarios when the scalability of models was challenging. The key to getting
    good results is to understand which method to use in which scenario. This could
    be achieved with lots of hands-on practice and experimentation with many large datasets.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是为你提供一套技术，帮助应对模型可扩展性面临挑战的场景。获得良好结果的关键是理解在不同场景中应使用哪种方法。这可以通过大量的实践和对许多大规模数据集的实验来实现。
- en: Having learned a set of tools to manage scalability in this chapter, we will
    move on to the next chapter, which addresses the problem of boosting performance.
    In the next chapter, you will be introduced to a technique called ensemble learning.
    This technique will help to boost the performance of your machine learning models.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了一套用于管理可扩展性的工具，接下来我们将进入下一章，讨论提升性能的问题。在下一章中，你将接触到一种叫做集成学习的技术，这种技术将帮助提升你的机器学习模型的性能。
