- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Predictions Don’t Grow on Trees, or Do They?
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测不是从树上长出来的，还是它们真能长出来？
- en: Our goal in this chapter is to see and apply concepts learned from previous
    chapters in order to construct and use modern learning algorithms to glean insights
    and make predictions on real datasets. While we explore the following algorithms,
    we should always remember that we are constantly keeping our metrics in mind.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是看到并应用从前几章学到的概念，构建和使用现代学习算法，从真实数据集中获取洞察力并做出预测。尽管我们在探索以下算法时，始终要记住，我们时刻关注着我们的评估指标。
- en: 'In this chapter, we will be looking at the following ML algorithms:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究以下机器学习算法：
- en: Performing naïve Bayes classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行朴素贝叶斯分类
- en: Understanding decision trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解决策树
- en: Diving deep into **unsupervised** **learning** (**UL**)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入研究**无监督学习**（**UL**）
- en: k-means clustering
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-均值聚类
- en: Feature extraction and **principal component** **analysis** (**PCA**)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取和**主成分分析**（**PCA**）
- en: Performing naïve Bayes classification
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行朴素贝叶斯分类
- en: 'Let’s get right into it! Let’s begin with **naïve Bayes** classification. This
    ML model relies heavily on results from previous chapters, specifically with Bayes’
    theorem:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接开始吧！我们从**朴素贝叶斯**分类开始。这个机器学习模型在很大程度上依赖于前几章的结果，特别是贝叶斯定理：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>H</mi><mo>|</mo><mi>D</mi></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>D</mi><mo>|</mo><mi>H</mi></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mi>H</mi></mfenced></mrow><mrow><mi>P</mi><mfenced open="("
    close=")"><mi>D</mi></mfenced></mrow></mfrac></mrow></mrow></math>](img/164.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>H</mi><mo>|</mo><mi>D</mi></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>D</mi><mo>|</mo><mi>H</mi></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mi>H</mi></mfenced></mrow><mrow><mi>P</mi><mfenced open="("
    close=")"><mi>D</mi></mfenced></mrow></mfrac></mrow></mrow></math>](img/164.png)'
- en: 'Let’s look a little closer at the specific features of this formula:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看一下这个公式的具体特征：
- en: '*P(H)* is the probability of the hypothesis before we observe the data, called
    the *prior probability*, or just *prior*'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(H)*是我们在观察数据之前，假设的概率，称为*先验概率*，或简称*先验*。'
- en: '*P(H|D)* is what we want to compute: the probability of the hypothesis after
    we observe the data, called the *posterior*'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(H|D)*是我们想要计算的内容：在观察到数据后，假设的概率，称为*后验概率*。'
- en: '*P(D|H)* is the probability of the data under the given hypothesis, called
    the *likelihood*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(D|H)*是给定假设下数据的概率，称为*似然函数*。'
- en: '*P(D)* is the probability of the data under any hypothesis, called the *normalizing
    constant*'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(D)*是任何假设下数据的概率，称为*归一化常数*。'
- en: Naïve Bayes classification is a classification model, and therefore a supervised
    model. Given this, what kind of data do we need – labeled or unlabeled data?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类是一种分类模型，因此是监督学习模型。基于这一点，我们需要什么类型的数据——有标签数据还是无标签数据？
- en: (Insert *Jeopardy* music here)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: （在此插入*Jeopardy*音乐）
- en: If you answered *labeled data*, then you’re well on your way to becoming a data
    scientist!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回答的是*有标签数据*，那么你已经在成为数据科学家的路上了！
- en: 'Suppose we have a dataset with *n features*, *(x1, x2, …, xn)*, and a *class
    label*, *C*. For example, let’s take some data involving spam text classification.
    Our data would consist of rows of individual text samples and columns of both
    our features and our class labels. Our features would be words and phrases that
    are contained within the text samples, and our class labels are simply *spam*
    or *not spam*. In this scenario, I will replace the not-spam class with an easier-to-say
    word, ham. Let’s take a look at the following code snippet to better understand
    our spam and ham data:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含*n个特征*，*(x1, x2, …, xn)*，以及一个*类别标签*，*C*的数据集。例如，假设我们有一些涉及垃圾邮件文本分类的数据。我们的数据由每行独立的文本样本和每列的特征以及类别标签组成。我们的特征是文本样本中包含的单词和短语，而类别标签则仅是*垃圾邮件*或*非垃圾邮件*。在这个场景中，我将把非垃圾邮件类别替换为更易于说出的词汇“火腿”。让我们看一下下面的代码片段，帮助我们更好地理解垃圾邮件和火腿数据：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Figure 11**.1* is a sample of text data in a row-column format:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11**.1*是以行列格式表示的文本数据示例：'
- en: '![Figure 11.1 – A sample of our spam versus not spam (ham) messages](img/B19488_11_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 我们的垃圾邮件与非垃圾邮件（火腿）消息样本](img/B19488_11_01.jpg)'
- en: Figure 11.1 – A sample of our spam versus not spam (ham) messages
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 我们的垃圾邮件与非垃圾邮件（火腿）消息样本
- en: 'Let’s do some preliminary statistics to see what we are dealing with. Let’s
    see the difference in the number of ham and spam messages at our disposal:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一些初步统计，看看我们正在处理的数据。让我们看看正常邮件和垃圾邮件的数量差异：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This gives us a bar chart, as shown in *Figure 11**.2*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个条形图，如*图 11.2*所示：
- en: '![Figure 11.2 – The distribution of ham versus spam](img/B19488_11_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 正常邮件与垃圾邮件的分布](img/B19488_11_02.jpg)'
- en: Figure 11.2 – The distribution of ham versus spam
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 正常邮件与垃圾邮件的分布
- en: Because we are dealing with classification, it would help to itemize some of
    the metrics we will be using to evaluate our model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们处理的是分类问题，因此列出我们将用来评估模型的部分指标会很有帮助。
- en: Classification metrics
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类指标
- en: 'When evaluating classification models, different metrics are used compared
    to regression models. These metrics help to understand how well the model is performing,
    especially in terms of correctly predicting different classes. Let’s look at what
    they are:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估分类模型时，使用的指标与回归模型不同。这些指标帮助我们理解模型的表现，特别是在正确预测不同类别方面。我们来看看它们是什么：
- en: '**Accuracy**: This is the most intuitive performance measure, and it is simply
    a ratio of correctly predicted observations to the total observations. It’s suitable
    for binary and multiclass classification problems:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准确率**：这是最直观的性能度量，它只是正确预测的观测值与总观测值的比率。适用于二分类和多分类问题：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Accuracy</mtext><mo>=</mo><mfrac><mrow><mtext>Number</mtext><mtext>of</mtext><mtext>correct</mtext><mtext>predictions</mtext></mrow><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi></mrow></mfrac></mrow></mrow></math>](img/165.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>准确率</mtext><mo>=</mo><mfrac><mrow><mtext>正确预测的</mtext><mtext>数量</mtext></mrow><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi></mrow></mfrac></mrow></mrow></math>](img/165.png)'
- en: '2. **Precision (best for binary classification – with only two classes)**:
    Also known as positive predictive value, this metric helps to answer the question:
    “What proportion of positive identifications was actually correct?”'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 2. **精度（最适合二分类 – 仅有两个类别）**：也称为正预测值，该指标有助于回答这个问题：“正识别的比例实际上有多少是正确的？”
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Precision</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow></mrow></math>](img/166.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>精度</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow></mrow></math>](img/166.png)'
- en: Here, *TP* is the number of true positives (predicted positive and the prediction
    was correct), and *FP* is the number of false positives (predicted positive but
    the prediction was incorrect).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*TP*是正确正例的数量（预测为正且预测正确），而*FP*是假正例的数量（预测为正但预测错误）。
- en: '3. **Recall (Sensitivity) (best for binary classification – with only two classes)**:
    This metric helps to answer the question: “What proportion of actual positives
    was identified correctly?”'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 3. **召回率（敏感性）（最适合二分类 – 仅有两个类别）**：该指标有助于回答这个问题：“实际的正例中有多少被正确识别？”
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Recall</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow></mrow></math>](img/167.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>召回率</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow></mrow></math>](img/167.png)'
- en: Here, *TP* (predicted positive and the prediction was correct) is the number
    of true positives, and *FN* is the number of false negatives (predicted negative
    but the prediction was incorrect).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*TP*（预测为正且预测正确）是正确预测的正例数，而*FN*是假负例数（预测为负但预测错误）。
- en: '4. **F1 Score**: The *F1* Score is the weighted average of precision and recall.
    Therefore, this score takes both false positives and false negatives into account.
    It is a good way to show that a classifier has a good value for both precision
    and recall:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 4. **F1得分**：*F1得分*是精度和召回率的加权平均值。因此，这个得分考虑了虚假正例和虚假负例。它是展示分类器在精度和召回率上都有较好表现的一个好方法：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>F</mi><mn>1</mn><mo>=</mo><mn>2</mn><mo>×</mo><mfrac><mrow><mtext>Precision</mtext><mo>×</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac></mrow></mrow></math>](img/168.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>F</mi><mn>1</mn><mo>=</mo><mn>2</mn><mo>×</mo><mfrac><mrow><mtext>Precision</mtext><mo>×</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac></mrow></mrow></math>](img/168.png)'
- en: These metrics are crucial for understanding the behavior of classification models,
    especially in domains where the costs of false positives and false negatives are
    very different.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些度量对于理解分类模型的行为至关重要，尤其是在虚假正例和虚假负例的成本差异很大的领域。
- en: 'So, we have way more *ham* messages than we do *spam*. Because this is a classification
    problem, it would be very useful to know our `ham`. Here’s how we do that:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们有远远更多的*ham*（火腿）信息而不是*spam*（垃圾邮件）。由于这是一个分类问题，知道我们的`ham`信息非常有用。下面是我们如何做到这一点：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So, if we blindly guessed *ham*, we would be correct about 87% of the time,
    but we can do better than that.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果我们盲目猜测是*ham*（火腿），我们大约有87%的机会是正确的，但我们可以做得更好。
- en: 'If we have a set of classes, C, and features, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/169.png),
    then we can use Bayes’ theorem to predict the probability that a single row belongs
    to class C, using the following formula:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一组类别C和特征![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/169.png)，那么我们可以使用贝叶斯定理来预测一行数据属于类别C的概率，使用以下公式：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>class</mtext><mi>C</mi><mo>|</mo><mrow><mrow><mo>{</mo><msub><mi>x</mi><mi>i</mi></msub><mo>}</mo></mrow></mrow></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mrow><mrow><mo>{</mo><msub><mi>x</mi><mi>i</mi></msub><mo>}</mo></mrow></mrow><mo>|</mo><mtext>class</mtext><mi>C</mi></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>class</mtext><mi>C</mi></mrow></mfenced></mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mo>{</mo><msub><mi>x</mi><mi>i</mi></msub><mo>}</mo></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/170.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>class</mtext><mi>C</mi><mo>|</mo><mrow><mrow><mo>{</mo><msub><mi>x</mi><mi>i</mi></msub><mo>}</mo></mrow></mrow></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mrow><mrow><mo>{</mo><msub><mi>x</mi><mi>i</mi></msub><mo>}</mo></mrow></mrow><mo>|</mo><mtext>class</mtext><mi>C</mi></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>class</mtext><mi>C</mi></mrow></mfenced></mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mo>{</mo><msub><mi>x</mi><mi>i</mi></msub><mo>}</mo></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/170.png)'
- en: 'Let’s look at this formula in a little more detail:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下这个公式：
- en: '*P(class C | {* *![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/171.png)*
    *})*: The posterior probability is the probability that the row belongs to class
    C given the features *{xi}*.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(class C | {* *![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/171.png)*
    *})*: 后验概率是指在给定特征*{xi}*的情况下，这一行数据属于类别C的概率。'
- en: '*P({* *![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/172.png)*
    *} | class C)*: This is the likelihood that we would observe these features given
    that the row was in class *C*.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P({* *![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/172.png)*
    *} | class C)*: 这是在假设这一行属于类*C*的情况下，观察到这些特征的概率。'
- en: '*P(class C)*: This is the prior probability. It is the probability that the
    data point belongs to class *C* before we see any data.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(class C)*：这是先验概率。它是数据点属于类别 *C* 的概率，在我们看到任何数据之前。'
- en: '*P({* *![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/173.png)*
    *})*: This is our normalization constant.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P({* *![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/173.png)*
    *})*: 这是我们的标准化常数。'
- en: 'For example, imagine we have an email with three words: *send cash now*. We’ll
    use naïve Bayes to classify the email as either being spam or ham:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一封包含三个单词的电子邮件：*send cash now*。我们将使用朴素贝叶斯分类器将邮件分类为垃圾邮件或正常邮件：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>spam</mtext><mspace width="0.125em" /><mo>|</mo><mspace
    width="0.125em" /><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mspace
    width="0.125em" /><mo>|</mo><mspace width="0.125em" /><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>spam</mtext></mfenced></mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/174.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>spam</mtext><mspace width="0.125em" /><mo>|</mo><mspace
    width="0.125em" /><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mspace
    width="0.125em" /><mo>|</mo><mspace width="0.125em" /><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>spam</mtext></mfenced></mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/174.png)'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>ham</mtext><mspace width="0.125em" /><mo>|</mo><mspace
    width="0.125em" /><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mspace
    width="0.125em" /><mo>|</mo><mspace width="0.125em" /><mtext>ham</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>ham</mtext></mfenced></mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/175.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>ham</mtext><mspace width="0.125em" /><mo>|</mo><mspace
    width="0.125em" /><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mspace
    width="0.125em" /><mo>|</mo><mspace width="0.125em" /><mtext>ham</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>ham</mtext></mfenced></mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/175.png)'
- en: 'We are concerned with the difference of these two numbers. We can use the following
    criteria to classify any single text sample:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注的是这两个数字之间的差异。我们可以使用以下标准来分类任何单个文本样本：
- en: If *P(spam | send cash now)* is larger than *P(ham | send cash now)*, then we
    will classify the text as spam
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *P(spam | send cash now)* 大于 *P(ham | send cash now)*，那么我们将把该文本分类为垃圾邮件。
- en: If *P(ham | send cash now)* is larger than *P(spam | send cash now)*, then we
    will label the text ham
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *P(ham | send cash now)* 大于 *P(spam | send cash now)*，那么我们将把该文本标记为正常邮件。
- en: 'Because both equations have *P (send money now)* in the denominator, we can
    ignore them. So, now we are concerned with the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因为两个方程式的分母中都有 *P (send money now)*，我们可以忽略它们。所以，现在我们关注以下内容：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mspace
    width="0.125em" /><mo>|</mo><mspace width="0.125em" /><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>spam</mtext></mfenced><mspace width="0.125em" /><mtext>VS</mtext><mspace
    width="0.125em" /><mi>P</mi><mfenced open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mspace
    width="0.125em" /><mo>|</mo><mspace width="0.125em" /><mtext>ham</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>ham</mtext></mfenced></mrow></mrow></math>](img/176.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mspace
    width="0.125em" /><mo>|</mo><mspace width="0.125em" /><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>spam</mtext></mfenced><mspace width="0.125em" /><mtext>VS</mtext><mspace
    width="0.125em" /><mi>P</mi><mfenced open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mspace
    width="0.125em" /><mo>|</mo><mspace width="0.125em" /><mtext>ham</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>ham</mtext></mfenced></mrow></mrow></math>](img/176.png)'
- en: 'Let’s work out the numbers in this equation:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这个等式中计算一下具体的数字：
- en: P(spam) = 0.134063
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(spam) = 0.134063
- en: P(ham) = 0.865937
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(ham) = 0.865937
- en: P(send cash now | spam) = ???
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(send cash now | spam) = ???
- en: P(send cash now | ham) = ???
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(send cash now | ham) = ???
- en: The final two likelihoods might seem like they would not be so difficult to
    calculate. All we have to do is count the number of spam messages that include
    the send money, right?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的两个可能性似乎并不难计算。我们只需要数一数包含“send money”这个短语的垃圾邮件数量，对吧？
- en: 'Now, phrase and divide that by the total number of spam messages:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，把这个短语分开，并除以所有垃圾邮件的总数：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Oh no! There are none! There are literally zero texts with the exact phrase
    *send cash now*. The hidden problem here is that this phrase is very specific,
    and we can’t assume that we will have enough data in the world to have seen this
    exact phrase many times before.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 哦不！一个都没有！确实没有！没有任何文本包含确切的短语*send cash now*。隐藏的问题在于，这个短语非常具体，我们不能假设在世界上有足够多的数据能够多次见到这个确切的短语。
- en: 'Instead, we can make a naïve assumption in our Bayes’ theorem. If we assume
    that the features (words) are conditionally independent (meaning that no word
    affects the existence of another word), then we can rewrite the formula:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 相反地，我们可以在贝叶斯定理中做出一个朴素的假设。如果我们假设特征（单词）是条件独立的（即一个单词不会影响另一个单词的存在），那么我们可以重写公式：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced><mo>=</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>cash</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>now</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced></mrow></mrow></math>](img/177.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced><mo>=</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>cash</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>now</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced></mrow></mrow></math>](img/177.png)'
- en: 'And here’s what it looks like done in Python:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是用Python计算的结果：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Printing out the conditional probabilities yields:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出条件概率结果如下：
- en: P(send|spam) = 0.096
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(send|spam) = 0.096
- en: P(cash|spam) = 0.091
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(cash|spam) = 0.091
- en: P(now|spam) = 0.280
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(now|spam) = 0.280
- en: 'With this, we can calculate the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们可以计算以下内容：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>spam</mtext></mfenced><mo>=</mo><mfenced open="(" close=")"><mrow><mn>0.096</mn><mo>⋅</mo><mn>0.091</mn><mo>⋅</mo><mn>0.280</mn></mrow></mfenced><mo>⋅</mo><mn>0.134</mn><mo>=</mo><mn>0.00032</mn></mrow></mrow></math>](img/178.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>spam</mtext></mfenced><mo>=</mo><mfenced open="(" close=")"><mrow><mn>0.096</mn><mo>⋅</mo><mn>0.091</mn><mo>⋅</mo><mn>0.280</mn></mrow></mfenced><mo>⋅</mo><mn>0.134</mn><mo>=</mo><mn>0.00032</mn></mrow></mrow></math>](img/178.png)'
- en: 'Repeating the same procedure for ham gives us the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对垃圾邮件进行相同的操作会得到以下结果：
- en: P(send|ham) = 0.03
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(send|ham) = 0.03
- en: P(cash|ham) = 0.003
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(cash|ham) = 0.003
- en: P(now|ham) = 0.109
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(now|ham) = 0.109
- en: The fact that these numbers are both very low is not as important as the fact
    that the spam probability is much larger than the ham calculation. If we do the
    calculations, we get that the *send cash now* probability for spam is 38 times
    bigger than for spam! Doing this means that we can classify *send cash now* as
    spam! Simple, right?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字都非常低这一事实不如垃圾邮件的概率远大于正常邮件的计算结果重要。如果我们进行计算，我们会发现 *send cash now* 的垃圾邮件概率是正常邮件的
    38 倍！通过这种方式，我们可以将 *send cash now* 分类为垃圾邮件！很简单，对吧？
- en: Let’s use Python to implement a naïve Bayes classifier without having to do
    all of these calculations ourselves.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Python 来实现朴素贝叶斯分类器，而无需自己进行所有这些计算。
- en: 'First, let’s revisit the count vectorizer in scikit-learn, which turns text
    into numerical data for us. Let’s assume that we will train on three documents
    (sentences), in the following code snippet:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下 scikit-learn 中的计数向量化器，它将文本转换为数值数据。假设我们将对三个文档（句子）进行训练，以下是代码片段：
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Figure 11**.3* demonstrates the feature vectors learned from our dataset:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11**.3* 演示了我们从数据集中学习到的特征向量：'
- en: '![Figure 11.3 – The first five rows of our SMS dataset after breaking up each
    text into a count of unique words](img/B19488_11_03.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – 在将每个文本分解为唯一单词的计数后，我们的 SMS 数据集的前五行](img/B19488_11_03.jpg)'
- en: Figure 11.3 – The first five rows of our SMS dataset after breaking up each
    text into a count of unique words
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 在将每个文本分解为唯一单词的计数后，我们的 SMS 数据集的前五行
- en: Note that each row represents one of the three documents (sentences), each column
    represents one of the words present in the documents, and each cell contains the
    number of times each word appears in each document.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每一行代表三个文档（句子）之一，每一列代表文档中出现的单词之一，每个单元格包含该单词在每个文档中出现的次数。
- en: 'We can then use the count vectorizer to transform new incoming test documents
    to conform with our training set (the three sentences):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用计数向量化器将新的输入测试文档转换为符合我们的训练集（三个句子）：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Figure 11**.4* is shown as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11**.4* 如下所示：'
- en: '![Figure 11.4 – Representation of the “please don’t call me” SMS in the same
    vocabulary as our training data](img/B19488_11_04.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.4 – 以与我们训练数据相同的词汇表示“请不要打电话给我” SMS](img/B19488_11_04.jpg)'
- en: Figure 11.4 – Representation of the “please don’t call me” SMS in the same vocabulary
    as our training data
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – 以与我们训练数据相同的词汇表示“请不要打电话给我” SMS
- en: Note how, in our test sentence, we had a new word – namely, *don’t*. When we
    vectorized it, because we hadn’t seen that word previously in our training data,
    the vectorizer simply ignored it. This is important and incentivizes data scientists
    to obtain as much data as possible for their training sets.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在我们的测试句子中，出现了一个新单词——即 *don’t*。当我们对其进行向量化时，由于在训练数据中没有见过这个词，向量化器简单地忽略了它。这一点非常重要，它促使数据科学家尽可能多地获取数据以用于训练集。
- en: 'Now, let’s do this for our actual data:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对实际数据进行此操作：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following is the output:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that the format is in a sparse matrix, meaning the matrix is large and
    full of zeros. There is a special format to deal with objects such as this. Take
    a look at the number of columns.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，格式是稀疏矩阵，这意味着矩阵很大且充满零。处理此类对象有特殊的格式。看看列的数量。
- en: 'There are 7,456 words. That’s a lot! This means that in our training set, there
    are 7,456 unique words to look at. We can now transform our test data to conform
    to our vocabulary:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 共有 7,456 个单词。真不少！这意味着在我们的训练集中，有 7,456 个唯一的单词需要考虑。现在我们可以将测试数据转换为符合我们的词汇：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that we have the same exact number of columns because it is conforming
    to our test set to be exactly the same vocabulary as before. No more, no less.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们有相同数量的列，因为它符合我们的测试集，确保与之前的词汇完全一致。没有更多，也没有更少。
- en: 'Now, let’s build a naïve Bayes model (similar to the linear regression process):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建一个朴素贝叶斯模型（类似于线性回归过程）：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, the `nb` variable holds our fitted model. The training phase of the model
    involves computing the likelihood function, which is the conditional probability
    of each feature given each class:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`nb` 变量保存了我们的拟合模型。模型的训练阶段涉及计算似然函数，即在给定每个类别的情况下，每个特征的条件概率：
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The prediction phase of the model involves computing the posterior probability
    of each class given the observed features and choosing the class with the highest
    probability.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的预测阶段包括根据观察到的特征计算每个类别的后验概率，并选择具有最高概率的类别。
- en: 'We will use `sklearn`’s built-in accuracy and confusion matrix to look at how
    well our naïve Bayes models are performing:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`sklearn`的内置准确率和混淆矩阵来查看我们的朴素贝叶斯模型的表现：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: First off, our accuracy is great! Compared to our null accuracy, which was 87%,
    99% is a fantastic improvement.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们的准确率非常好！与87%的空准确率相比，99%是一个巨大的提升。
- en: Now to our confusion matrix. From before, we know that each row represents actual
    values while columns represent predicted values, so the top-left value, 1,203,
    represents our true negatives. But what is negative and positive? We gave the
    model the spam and ham strings as our classes, not positive and negative.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看我们的混淆矩阵。从前面我们知道，每一行代表实际值，而每一列代表预测值，因此左上角的值1,203表示我们的真实负类。但什么是负类和正类呢？我们将垃圾邮件和正常邮件作为类别，而不是正类和负类。
- en: 'We can use the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下方法：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can then line up the indices so that 1,203 refers to true ham predictions
    and 174 refers to true spam predictions. There were also five false spam classifications,
    meaning that five messages were predicted as spam but were actually ham, as well
    as 11 false ham classifications. In summary, naïve Bayes classification uses Bayes’
    theorem in order to fit posterior probabilities of classes so that data points
    are correctly labeled as belonging to the proper class.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以排列索引，使得1,203代表真实的正常预测，174代表真实的垃圾邮件预测。还存在五个错误的垃圾邮件分类，意味着五条消息被预测为垃圾邮件，但实际上是正常邮件，以及11个错误的正常邮件分类。总之，朴素贝叶斯分类使用贝叶斯定理来拟合类别的后验概率，从而确保数据点正确地标记为属于相应类别。
- en: Every ML model has its own set of unique properties and advantages or disadvantages
    for use with different types of data. The naïve Bayes classifier, for example,
    is known for its speed and efficiency. It is particularly fast when fitting to
    training data and when making predictions on test data. This is due to its assumption
    of feature independence, which simplifies the calculations involved in probability
    estimation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习模型都有自己的一套独特属性和适用于不同数据类型的优缺点。例如，朴素贝叶斯分类器以其速度和效率著称。它在拟合训练数据和进行测试数据预测时特别快。这是因为它假设特征之间是独立的，从而简化了概率估计中的计算。
- en: However, this same assumption can also be seen as a limitation. In reality,
    features often do exhibit some level of dependency, and the naïve Bayes classifier
    may oversimplify complex relationships in data. Moreover, it is based on the assumption
    that the form of the data distribution (often assumed to be Gaussian) holds true,
    which might not always be the case in real-world scenarios.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这一假设也可以看作是一个局限性。实际上，特征通常会表现出一定程度的依赖性，而朴素贝叶斯分类器可能会过度简化数据中的复杂关系。此外，它基于数据分布形式（通常假设为高斯分布）的假设，这在现实场景中未必总是成立。
- en: Despite these limitations, naïve Bayes can perform exceptionally well with appropriate
    data and is particularly useful for text classification tasks, such as spam filtering
    and **sentiment** **analysis** (**SA**).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些局限性，朴素贝叶斯在适当的数据下表现得非常出色，特别适用于文本分类任务，如垃圾邮件过滤和**情感** **分析**（**SA**）。
- en: Another widely used ML technique is the decision tree. Decision trees are a
    **supervised learning** (**SL**) method used for classification and regression.
    They are intuitive and easy to interpret since they mimic human decision-making
    more closely than other algorithms.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种广泛使用的机器学习技术是决策树。决策树是一种**监督学习**（**SL**）方法，常用于分类和回归。由于决策树模拟了人类决策过程，它比其他算法更直观且易于解释。
- en: Understanding decision trees
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解决策树
- en: '**Decision trees** are supervised models that can either perform regression
    or classification. They are a flowchart-like structure in which each internal
    node represents a test on an attribute, each branch represents the outcome of
    the test, and each leaf node represents a class label (for classification) or
    a value (for regression). One of the primary advantages of decision trees is their
    simplicity; they do not require any complex mathematical formulations, making
    them easier to understand and visualize.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**是监督学习模型，可以执行回归或分类任务。它们是一种类似流程图的结构，其中每个内部节点表示对某个属性的测试，每个分支表示测试的结果，每个叶节点表示一个类别标签（用于分类）或一个值（用于回归）。决策树的主要优点之一是它们的简洁性；它们不需要复杂的数学公式，使得它们更容易理解和可视化。'
- en: The goal of a decision tree is to split the data in a manner that maximizes
    the purity of the nodes resulting from those splits. In the context of a classification
    problem, “purity” refers to how homogeneous the nodes are with respect to the
    target variable. A perfectly pure node would contain instances of only a single
    class.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的目标是以最大化节点纯度的方式来划分数据。对于分类问题，“纯度”指的是节点在目标变量方面的同质性。一个完全纯的节点将只包含单一类别的实例。
- en: Decision trees achieve this by using measures of impurity, such as the Gini
    index or entropy (more on that soon), to evaluate potential splits. A good split
    is one that most effectively separates the data into nodes with high purity, meaning
    that it increases the homogeneity of the nodes with respect to the target variable.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通过使用杂质度量，例如基尼指数或熵（稍后会讲到），来评估潜在的分割。一个好的分割是能够最有效地将数据分隔为具有高纯度的节点，这意味着它增加了节点在目标变量方面的同质性。
- en: 'The process involves the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程包括以下内容：
- en: Selecting the best attribute to split the data based on a specific criterion
    (such as Gini or entropy).
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于特定标准（如基尼或熵）选择最佳的属性来划分数据。
- en: Partitioning the dataset into subsets that contain instances with similar values
    for that attribute.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集划分为包含该属性值相似的实例的子集。
- en: Repeating this process recursively for each derived subset until the stopping
    criteria are met (which could be a maximum depth of the tree, a minimum number
    of instances in a node, or the achievement of a node with high purity).
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个派生的子集重复这一过程，直到满足停止标准（例如树的最大深度、节点中的最小实例数，或节点的纯度已达到较高的水平）。
- en: This recursive partitioning makes decision trees a powerful and interpretable
    modeling technique for classification and regression tasks.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这种递归分割使得决策树成为分类和回归任务中强大且可解释的建模技术。
- en: Measuring purity
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 衡量纯度
- en: 'The **Gini index** is a measure of inequality among values of a frequency distribution
    (for example, levels of income). In the context of ML and decision trees, it measures
    the impurity of a node with the following formula:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**基尼指数**是衡量频率分布中值之间不平等的指标（例如收入水平）。在机器学习和决策树的背景下，它通过以下公式来衡量节点的杂质：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Gini</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math>](img/179.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Gini</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math>](img/179.png)'
- en: Here, *D* is the dataset, *J* is the number of classes, and *pi* is the probability
    of class *i* in the dataset *D*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*D* 是数据集，*J* 是类别数，*pi* 是数据集 *D* 中类别 *i* 的概率。
- en: 'Entropy, on the other hand, is a measure from information theory that quantifies
    the amount of uncertainty or randomness in the data. It’s used in the construction
    of decision trees to represent the impurity of a dataset with the following formula:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 熵（Entropy），另一方面，是信息理论中的一个度量，用来量化数据中的不确定性或随机性。它被用于构建决策树，以表示数据集的纯度，其公式如下：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Entropy</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/180.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>熵</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/180.png)'
- en: Here, *pi* is the probability of class *i* within the dataset *D*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*pi*是数据集*D*中类别*i*的概率。
- en: Both the Gini index and entropy are used to choose where to split the data when
    building a decision tree. The choice between using the Gini index and entropy
    often depends on the specific dataset and the preferences of the modeler, as they
    can lead to slightly different trees. In practice, the difference in the trees
    generated by these two methods is often very small.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼指数和熵都用于在构建决策树时选择数据切分的位置。选择使用基尼指数还是熵通常取决于特定数据集和模型建立者的偏好，因为它们可能会导致略有不同的树形结构。实际上，这两种方法生成的树之间的差异通常非常小。
- en: Exploring the Titanic dataset
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索泰坦尼克号数据集
- en: The *Titanic* dataset is truly a classic in the field of data science, often
    used to illustrate the fundamentals of ML. It details the tragic sinking of the
    RMS Titanic, one of the most infamous shipwrecks in history. This dataset serves
    as a rich source of demographic and travel information about the passengers, which
    can be utilized to model and predict survival outcomes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*泰坦尼克号*数据集是数据科学领域中的经典之作，常用于说明机器学习的基本原理。它详细描述了RMS泰坦尼克号的悲惨沉没事件，这也是历史上最臭名昭著的船难之一。该数据集提供了关于乘客的丰富人口统计和旅行信息，可以用来建模和预测生还结果。'
- en: Through the lens of this dataset, we can apply statistical analysis and predictive
    modeling to understand factors that may have influenced the chances of survival.
    For instance, consider a subset of only 25 passengers from the *Titanic* dataset.
    Out of 25, 10 of these individuals survived the disaster, while 15 did not. By
    examining attributes such as age, gender, class, and fare paid, we can begin to
    construct a predictive model that estimates the likelihood of survival for each
    passenger in similar circumstances.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个数据集，我们可以运用统计分析和预测建模来理解可能影响生还几率的因素。例如，考虑仅从*泰坦尼克号*数据集中抽取的25名乘客。在这25人中，有10人幸存，而15人未能生还。通过分析年龄、性别、舱位等级和票价等属性，我们可以开始构建一个预测模型，以估算在类似情况下每个乘客的生还可能性。
- en: We first calculate the *Gini index* before doing anything.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在开始任何操作之前，首先计算*基尼指数*。
- en: 'In this example, overall classes are `survived` and `died`, illustrated in
    the following formula:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，整体类别是`survived`（生还）和`died`（死亡），其公式如下所示：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>G</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>survived</mml:mtext></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>died</mml:mtext></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>25</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>15</mml:mn></mml:mrow><mml:mrow><mml:mn>25</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.48</mml:mn></mml:math>](img/181.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>G</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>survived</mml:mtext></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>died</mml:mtext></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>25</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>15</mml:mn></mml:mrow><mml:mrow><mml:mn>25</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.48</mml:mn></mml:math>](img/181.png)'
- en: This Gini index of 0.48 indicates the level of impurity in the dataset. The
    value suggests a moderate separation between the classes, with some degree of
    mixture between the *survived* and *died* categories within this group of passengers.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 0.48 的基尼指数表示数据集中的杂质水平。这个值表明类别之间的分离度适中，并且在这组乘客中，*生还*和*死亡*类别之间存在一定程度的混合。
- en: If we were to make a split in the dataset based on a certain feature, we would
    calculate the Gini index for each resulting subset. The goal is to choose a split
    that minimizes the Gini index, thus increasing the purity of the subsets with
    respect to the target variable, which in this case is survival on the Titanic.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们根据某一特征对数据集进行划分，我们将计算每个结果子集的基尼指数。目标是选择一个能够最小化基尼指数的划分，从而增加子集相对于目标变量（在本例中为泰坦尼克号上的生存情况）的纯度。
- en: 'Now, let’s consider a potential split on gender. We first calculate the Gini
    index for each given gender, as seen in *Figure 11**.5*:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑按性别进行的潜在划分。我们首先计算每个性别的基尼指数，如*图 11.5*所示：
- en: '![Figure 11.5 – Calculating impurity of our dataset on gender](img/B19488_11_05.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.5 – 根据性别计算数据集的杂质](img/B19488_11_05.jpg)'
- en: Figure 11.5 – Calculating impurity of our dataset on gender
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – 根据性别计算数据集的杂质
- en: 'The following formula calculates the Gini index for male and female, as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下公式计算男性和女性的基尼指数：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Gini</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>15</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>13</mml:mn></mml:mrow><mml:mrow><mml:mn>15</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.23</mml:mn></mml:math>](img/182.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>基尼</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>15</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>13</mml:mn></mml:mrow><mml:mrow><mml:mn>15</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.23</mml:mn></mml:math>](img/182.png)'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Gini</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>8</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.32</mml:mn></mml:math>](img/183.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>基尼</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>8</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.32</mml:mn></mml:math>](img/183.png)'
- en: 'Once we have the Gini index for each gender, we then calculate the overall
    Gini index for the split on gender, as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们为每个性别计算出基尼指数，我们接着计算基于性别的总体基尼指数，计算公式如下：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Gini</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfenced><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mtext>Gini</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.23</mml:mn><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>15</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>+</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mn>0.32</mml:mn><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>+</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.27</mml:mn></mml:math>](img/184.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Gini</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfenced><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mtext>Gini</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.23</mml:mn><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>15</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>+</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mn>0.32</mml:mn><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>+</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.27</mml:mn></mml:math>](img/184.png)'
- en: 'So, the *Gini coefficient* for splitting on *gender* is *0.27*. We then follow
    this procedure for three potential splits (shown in *Figure 11**.6*):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*性别*的 Gini 系数为 *0.27*。然后我们按照这一过程对三个潜在的拆分进行处理（如 *图 11.6* 所示）：
- en: Gender (male or female)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性别（男性或女性）
- en: Number of siblings on board (0 or 1+)
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 登船的兄弟姐妹人数（0 或 1+）
- en: Class (first and second versus third)
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别（第一类和第二类与第三类对比）
- en: '![Figure 11.6 – Calculating the resulting Gini coefficient for multiple splits
    on our dataset to decide which one to use for our decision tree](img/B19488_11_06.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.6 – 计算在我们的数据集上进行多次拆分后得到的 Gini 系数，以决定我们应选择哪个拆分用于决策树](img/B19488_11_06.jpg)'
- en: Figure 11.6 – Calculating the resulting Gini coefficient for multiple splits
    on our dataset to decide which one to use for our decision tree
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – 计算在我们的数据集上进行多次拆分后得到的 Gini 系数，以决定我们应选择哪个拆分用于决策树
- en: In this example, we would choose the gender to split on as it has the *lowest*
    *Gini index*!
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们会选择性别作为拆分标准，因为它具有 *最低的* *Gini 指数*！
- en: Before we get to some more code, we need to think about how to deal with categorical
    features that are not numerically encoded. ML algorithms require numerical inputs,
    and most datasets will have at least one feature that is not numerical.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写更多代码之前，我们需要思考如何处理那些未被数值编码的分类特征。机器学习算法需要数值输入，且大多数数据集中至少有一个特征不是数值型的。
- en: Dummy variables
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟变量
- en: 'Dummy variables are used when we are hoping to convert a categorical feature
    into a quantitative one. Remember that we have two types of categorical features:
    nominal and ordinal. Ordinal features have natural order among them, while nominal
    data does not.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望将分类特征转换为定量特征时，会使用虚拟变量。记住，我们有两种类型的分类特征：名义型和顺序型。顺序型特征之间有自然的顺序，而名义型数据没有。
- en: Encoding qualitative (nominal) data using separate columns is called making
    dummy variables, and it works by turning each unique category of a nominal column
    into its own column that is either `true` or `false`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单独的列对定性（名义型）数据进行编码称为创建虚拟变量，它通过将名义列中的每个唯一类别转换为自己的列，该列的值为 `true` 或 `false`。
- en: 'For example, if we had a column for someone’s college major and we wished to
    plug that information into linear or logistic regression, we couldn’t because
    they only take in numbers! So, for each row, we had new columns that represent
    the single nominal column. In this case, we have four unique majors: computer
    science, engineering, business, and literature. We end up with three new columns
    (we omit computer science as it is not necessary and can be inferred if all of
    the other three majors are 0). *Figure 11**.7* shows us an example:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个列来记录某人的大学专业，并且希望将这个信息应用到线性回归或逻辑回归中，我们是无法做到的，因为它们只接受数字！所以，对于每一行，我们需要新增代表单一名义变量的列。在这种情况下，我们有四个独特的专业：计算机科学、工程学、商学和文学。我们最终会得到三个新列（我们省略了计算机科学列，因为它不是必需的，并且可以通过其他三个专业全为0来推断）。*图
    11**.7*展示了这个例子：
- en: '![Figure 11.7 – Creating dummy variables for a single feature involves creating
    a new binary feature for each option except for one, which can be inferred by
    having all 0s in the rest of the features](img/B19488_11_07.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.7 – 为单一特征创建虚拟变量意味着为每个选项创建一个新的二进制特征，除了一个选项，其余的特征可以通过全为0来推断](img/B19488_11_07.jpg)'
- en: Figure 11.7 – Creating dummy variables for a single feature involves creating
    a new binary feature for each option except for one, which can be inferred by
    having all 0s in the rest of the features
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 – 为单一特征创建虚拟变量意味着为每个选项创建一个新的二进制特征，除了一个选项，其余的特征可以通过全为0来推断
- en: Note that the first row has a 0 in all of the columns, which means that this
    person did not major in engineering, did not major in business, and did not major
    in literature. The second person has a single 1 in the `Engineering` column as
    that is the major they studied.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一行所有列都是0，这意味着此人没有主修工程学、没有主修商学，也没有主修文学。第二个人在`Engineering`列中有一个1，表示他主修的是工程学。
- en: 'We are going to need to make some dummy variables using pandas as we use scikit-learn’s
    built-in decision tree function in order to build a decision tree:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用pandas来创建一些虚拟变量，因为我们将使用scikit-learn内置的决策树函数来构建决策树：
- en: '[PRE18]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*Figure 11**.8* shows what our dataset looks like after our preceding code
    block. Note that we are going to use class, sex, age, and dummy variables for
    city embarked as our features:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11**.8*展示了我们在前一个代码块执行后数据集的样子。注意，我们将使用类别、性别、年龄和登船城市的虚拟变量作为特征：'
- en: '![Figure 11.8 – Our Titanic dataset after creating dummy variables for Embarked](img/B19488_11_08.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 – 在为登船城市创建虚拟变量后，我们的泰坦尼克号数据集](img/B19488_11_08.jpg)'
- en: Figure 11.8 – Our Titanic dataset after creating dummy variables for Embarked
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – 在为登船城市创建虚拟变量后，我们的泰坦尼克号数据集
- en: 'Now, we can fit our decision tree classifier:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以拟合我们的决策树分类器：
- en: '[PRE19]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`max_depth` is a hyperparameter that limits the depth of our tree. It means
    that, for any data point, our tree is only able to ask up to three questions and
    create three splits. We can output our tree into a visual format, and we will
    obtain the result seen in *Figure 11**.9*:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth`是一个限制树深度的超参数。这意味着，对于任何数据点，我们的树只能最多问三个问题并进行三次分裂。我们可以将我们的树输出为可视化格式，最终得到*图
    11**.9*所示的结果：'
- en: '![Figure 11.9 – The decision tree produced with scikit-learn with the Gini
    coefficient calculated at each node](img/B19488_11_09.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.9 – 使用scikit-learn生成的决策树，且每个节点计算了基尼系数](img/B19488_11_09.jpg)'
- en: Figure 11.9 – The decision tree produced with scikit-learn with the Gini coefficient
    calculated at each node
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 – 使用scikit-learn生成的决策树，且每个节点计算了基尼系数
- en: 'We can notice a few things:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以注意到一些细节：
- en: '**Sex** is the first split, meaning that sex is the most important determining
    factor of whether or not a person survived the crash'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性别**是第一个分裂节点，意味着性别是决定一个人是否在撞击中生还的最重要因素。'
- en: '**Embarked_Q** was never used in any split'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Embarked_Q**在任何分裂中都没有被使用'
- en: 'For either classification or regression trees, we can also do something very
    interesting with decision trees, which is that we can output a number that represents
    each feature’s importance in the prediction of our data points (shown in *Figure
    11**.10*):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类树或回归树，我们还可以用决策树做一些非常有趣的事情，那就是我们可以输出一个数字，表示每个特征在预测数据点时的重要性（如*图 11**.10*所示）：
- en: '[PRE20]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![Figure 11.10 – Features that contributed most to the change in the Gini coefficient
    displayed as percentages adding up to 1; it’s no coincidence that our highest
    value (Sex) is also our first split](img/B19488_11_10.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.10 – 贡献最大变化的特征，按百分比显示，所有百分比相加为1；我们的最高值（性别）也恰好是我们的第一个分裂节点](img/B19488_11_10.jpg)'
- en: Figure 11.10 – Features that contributed most to the change in the Gini coefficient
    displayed as percentages adding up to 1; it’s no coincidence that our highest
    value (Sex) is also our first split
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10 – 显示在基尼系数变化中贡献最大特征的百分比，所有百分比加起来为1；我们的最高值（性别）恰好是我们第一次划分的特征，这并非巧合。
- en: The importance scores are an average Gini index difference for each variable,
    with higher values corresponding to higher importance to the prediction. We can
    use this information to select fewer features in the future. For example, both
    of the embarked variables are very low in comparison to the rest of the features,
    so we may be able to say that they are not important in our prediction of life
    or death.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 重要性评分是每个变量的平均基尼指数差异，较高的值对应着对预测的较高重要性。我们可以利用这些信息在未来选择更少的特征。例如，两个“登船”变量相对于其他特征非常低，因此我们可能可以说它们在预测生死方面不重要。
- en: As we transition from the structured realm of SL, where the outcomes are known
    and the model learns from labeled data, we venture into the domain of UL. Recall
    that UL algorithms uncover hidden patterns and intrinsic structures within data
    that isn’t explicitly labeled. In the upcoming section, we will explore how unsupervised
    techniques can discern underlying relationships in data and provide deeper insights
    without the guidance of a predefined outcome, and how they can complement the
    predictive models we’ve discussed so far.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从结构化的SL领域过渡到UL领域时，在SL领域中结果是已知的，模型通过标签数据学习，而UL算法则揭示数据中隐藏的模式和内在结构，这些数据并没有明确的标签。在接下来的部分，我们将探讨无监督技术如何在没有预定义结果的指导下，识别数据中的潜在关系并提供更深的见解，以及它们如何与我们迄今讨论的预测模型相辅相成。
- en: Diving deep into UL
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨UL
- en: It’s time to see some examples of UL, given that we’ve spent some time on *SL
    algorithms*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经花了一些时间讨论*SL算法*，是时候来看一些UL的例子了。
- en: When to use UL
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用UL
- en: 'There are many times when UL can be appropriate. Some very common examples
    include the following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多情况下UL是合适的。一些非常常见的例子包括：
- en: There is no clear response variable. There is nothing that we are explicitly
    trying to predict or correlate to other variables.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有明确的响应变量。我们没有明确试图预测或与其他变量相关联的内容。
- en: To extract structure from data where no apparent structure or patterns exist
    (can be an SL problem).
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从没有明显结构或模式的数据中提取结构（这可能是一个SL问题）。
- en: When an unsupervised concept called **feature extraction** is used. Feature
    extraction is the process of creating new features from existing ones. These new
    features can be even stronger than the original features.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一种无监督的概念——**特征提取**。特征提取是从现有特征中创建新特征的过程。这些新特征可能比原始特征更强大。
- en: The first tends to be the most common reason that data scientists choose to
    use UL. This case arises frequently when we are working with data and we are not
    explicitly trying to predict any of the columns, and we merely wish to find patterns
    of similar (and dissimilar) groups of points. The second option comes into play
    even if we are explicitly attempting to use a supervised model to predict a response
    variable.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个通常是数据科学家选择使用UL的最常见原因。这种情况通常发生在我们处理数据时，并没有明确试图预测任何列，我们仅仅希望找到相似（和不相似）数据点的模式。第二种情况即使我们明确尝试使用监督模型来预测响应变量时，也会发生。
- en: Sometimes, simple **exploratory data analysis** (**EDA**) might not produce
    any clear patterns in the data in the few dimensions that humans can imagine,
    whereas a machine might pick up on data points behaving similarly to each other
    in greater dimensions.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，简单的**探索性数据分析**（**EDA**）可能无法在几个人类能够想象的维度中产生明确的模式，而机器可能会在更高维度中发现数据点之间的相似行为。
- en: The third common reason to use UL is to extract new features from features that
    already exist. This process (lovingly called feature extraction) might produce
    features that can be used in a future supervised model or that can be used for
    presentation purposes (marketing or otherwise).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用UL的第三个常见原因是从已有特征中提取新特征。这个过程（被亲切地称为特征提取）可能会生成比原始特征更强的特征，这些新特征可以用于未来的监督模型，也可以用于展示目的（如市场营销或其他用途）。
- en: k-means clustering
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k均值聚类
- en: '**k-means clustering** is our first example of an **unsupervised ML** (**UML**)
    model. Remember – this means that we are not making predictions. We are trying
    instead to extract structure from seemingly unstructured data.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**k均值聚类**是我们第一个**无监督机器学习（ML）**（**UML**）模型的示例。请记住——这意味着我们不是在做预测。我们是在试图从看似无结构的数据中提取结构。'
- en: '**Clustering** is a family of UML models that attempt to group data points
    into clusters with centroids. The concept of **similarity** is central to the
    definition of a cluster, and therefore to cluster analysis. In general, greater
    similarity between points leads to better clustering. In most cases, we turn data
    into points in *n*-dimensional space and use the distance between these points
    as a form of similarity. The **centroid** of the cluster is then usually the average
    of each dimension (column) for each data point in each cluster. So, for example,
    the centroid of the red cluster is the result of taking the average value of each
    column of each red data point.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**是尝试将数据点分组到具有质心的簇中的一类UML模型。**相似性**的概念是簇定义的核心，因此也是簇分析的核心。通常来说，点之间的相似性越大，聚类效果越好。在大多数情况下，我们将数据转化为*多维*空间中的点，并使用这些点之间的距离作为相似性的一种形式。然后，簇的**质心**通常是每个簇中每个数据点在每一维度（列）上的平均值。例如，红色簇的质心是通过计算每个红色数据点在各列上的平均值得到的。'
- en: The purpose of cluster analysis is to enhance our understanding of a dataset
    by dividing the data into groups. Clustering provides a layer of abstraction from
    individual data points. The goal is to extract and enhance the natural structure
    of the data. There are many kinds of classification procedures. For our class,
    we will be focusing on k-means clustering, which is one of the most popular clustering
    algorithms.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析的目的是通过将数据分为不同的组来增强我们对数据集的理解。聚类为单个数据点提供了一个抽象层。目标是提取并增强数据的自然结构。分类方法有很多种。对于我们的课程，我们将重点关注k均值聚类，它是最流行的聚类算法之一。
- en: '**k-means** is an iterative method that partitions a dataset into *k* clusters.
    It works in four steps:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**k均值**是一种迭代方法，将数据集划分为*k*个簇。它的四个步骤是：'
- en: Choose *k* initial centroids (note that *k* is an input).
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择*k*个初始质心（注意*k*是一个输入值）。
- en: For each point, assign the point to the nearest centroid.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个点，将该点分配给最近的质心。
- en: Recalculate the centroid positions.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新计算质心位置。
- en: Repeat *steps 2* and *3* until the stopping criteria are met.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤2*和*步骤3*，直到满足停止标准。
- en: An illustrative example of clustering
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个聚类的示例
- en: 'Imagine that we have data points in a two-dimensional space, as seen in *Figure
    11**.11*:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有在二维空间中的数据点，如*图11.11*所示：
- en: '![Figure 11.11 – A mock dataset to be clustered](img/B19488_11_11.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图11.11 – 模拟数据集待聚类](img/B19488_11_11.jpg)'
- en: Figure 11.11 – A mock dataset to be clustered
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11 – 模拟数据集待聚类
- en: 'Each dot is colored gray to assume no prior grouping before applying the k-means
    algorithm. The goal here is to eventually color in each dot and create groupings
    (clusters), as illustrated in *Figure 11**.12*:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 每个点被着色为灰色，假设在应用k均值算法之前没有先前的分组。这里的目标是最终为每个点上色并创建分组（簇），如*图11.12*所示：
- en: '![Figure 11.12 – Step 1: k-means clustering begins by placing random centroids](img/B19488_11_12.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图11.12 – 步骤1：k均值聚类通过随机放置质心开始](img/B19488_11_12.jpg)'
- en: 'Figure 11.12 – Step 1: k-means clustering begins by placing random centroids'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12 – 步骤1：k均值聚类通过随机放置质心开始
- en: We have (randomly) chosen *three centroids* (*red*, *blue*, and *yellow*).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经（随机）选择了*三个质心*（*红色*，*蓝色*和*黄色*）。
- en: Important note
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Most k-means algorithms place random initial centroids, but there exist other
    pre-computed methods to place initial centroids. For now, random is fine.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数k均值算法会随机放置初始质心，但也存在其他预计算方法来放置初始质心。目前，随机方法就可以。
- en: '*Step 2* has been applied in *Figure 11**.13*. For each data point, we found
    the most similar centroid (closest):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤2*已经在*图11.13*中应用。对于每个数据点，我们找到了最相似的质心（即最近的质心）：'
- en: '![Figure 11.13 – Step 2: For each point, assign the point to the nearest centroid](img/B19488_11_13.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图11.13 – 步骤2：对每个点，将该点分配给最近的质心](img/B19488_11_13.jpg)'
- en: 'Figure 11.13 – Step 2: For each point, assign the point to the nearest centroid'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13 – 步骤2：对每个点，将该点分配给最近的质心
- en: 'We then apply *step 3* in *Figure 11**.14*:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在*图11.14*中应用*步骤3*：
- en: '![Figure 11.14 – Step 3: Recalculate the centroid positions](img/B19488_11_14.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图11.14 – 步骤3：重新计算质心位置](img/B19488_11_14.jpg)'
- en: 'Figure 11.14 – Step 3: Recalculate the centroid positions'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14 – 步骤3：重新计算质心位置
- en: 'This is *step 3* and the crux of k-means. Note that we have physically moved
    the centroids to be the actual center of each cluster. We have, for each color,
    computed the average point and made that point the new centroid. For example,
    suppose the three red data points had the following coordinates: *(1, 3)*, *(2,
    5)*, and *(3, 4)*. The *center (red cross)* would be calculated as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*步骤3*，也是k-means的关键。请注意，我们已经将质心物理地移动到每个簇的实际中心。我们已经计算了每个颜色的平均点，并将该点作为新的质心。例如，假设三个红色数据点的坐标分别为：*(1,
    3)*、*(2, 5)*和*(3, 4)*。*中心（红色交叉点）*将按如下方式计算：
- en: '[PRE21]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: That is, the *(2, 4)* point would be the coordinates of the preceding red cross.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，*(2, 4)*点将是之前红色交叉点的坐标。
- en: 'We continue with our algorithm by repeating *step 2*. Here is the first part
    where we find the closest center for each point. Note a big change – the point
    in the bottom left used to be a yellow point but has changed to be a red cluster
    point because the yellow cluster moved closer to its yellow constituents:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过重复*步骤2*继续执行我们的算法。在这里是第一部分，我们为每个点找到最近的中心。请注意一个大变化——左下角的点曾经是黄色的，但现在变成了红色簇点，因为黄色簇移动得更接近其黄色成分：
- en: '![Figure 11.15 – Repeating step 2; note the data point on the lower left was
    yellow in the previous step and is now red](img/B19488_11_15.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图11.15 – 重复步骤2；请注意，左下角的数据点在上一步是黄色的，现在变成了红色](img/B19488_11_15.jpg)'
- en: Figure 11.15 – Repeating step 2; note the data point on the lower left was yellow
    in the previous step and is now red
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15 – 重复步骤2；请注意，左下角的数据点在上一步是黄色的，现在变成了红色
- en: 'If we follow *step 3* again, we get the result shown in *Figure 11**.16*:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次执行*步骤3*，我们将得到*图11.16*所示的结果：
- en: '![Figure 11.16 – Step 3 again](img/B19488_11_16.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图11.16 – 再次进行步骤3](img/B19488_11_16.jpg)'
- en: Figure 11.16 – Step 3 again
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.16 – 再次进行步骤3
- en: Here, we recalculate once more the centroids for each cluster (*step 3*). Note
    that the blue center did not move at all, while the yellow and red centers both
    moved.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们再次重新计算每个簇的质心（*步骤3*）。请注意，蓝色中心没有移动，而黄色和红色中心都有所移动。
- en: Because we have reached a **stopping criterion** (clusters do not move if we
    repeat *steps 2* and *3*), we finalize our algorithm and we have our three clusters,
    which is the final result of the k-means algorithm.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们已经达到了**停止准则**（如果我们重复*步骤2*和*步骤3*，簇不再移动），所以我们完成了算法，并得到了我们的三个簇，这就是k-means算法的最终结果。
- en: An illustrative example – beer!
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个说明性的例子——啤酒！
- en: Let’s run a cluster analysis on a new dataset outlining different beers with
    different characteristics. We know that there are many types of beer, but I wonder
    if we could possibly group beers into different categories based on different
    quantitative features.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对一个新的数据集进行聚类分析，该数据集概述了具有不同特征的啤酒。我们知道啤酒有很多种类型，但我想知道我们是否可以根据不同的定量特征将啤酒分组到不同的类别中。
- en: 'Let’s try! Let’s import a dataset of just a few types of beer and visualize
    a few rows in *Figure 11**.17*:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试！让我们导入一个包含几种啤酒的数据集，并在*图11.17*中可视化几行数据：
- en: '[PRE22]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Figure 11.17 – The first five rows of our beer dataset](img/B19488_11_17.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图11.17 – 我们啤酒数据集的前五行](img/B19488_11_17.jpg)'
- en: Figure 11.17 – The first five rows of our beer dataset
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.17 – 我们啤酒数据集的前五行
- en: 'Our dataset has 20 beers with 5 columns: `name`, `calories`, `sodium`, `alcohol`,
    and `cost`. In clustering (as with almost all ML models), we like quantitative
    features, so we will ignore the name of the beer in our clustering:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集包含20种啤酒，具有5个列：`name`（名称）、`calories`（卡路里）、`sodium`（钠）、`alcohol`（酒精）和`cost`（成本）。在聚类中（如几乎所有的机器学习模型一样），我们更喜欢定量特征，因此我们将在聚类中忽略啤酒的名称：
- en: '[PRE23]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we will perform k-means clustering using `scikit-learn`:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用`scikit-learn`执行k-means聚类：
- en: '[PRE24]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Our k-means algorithm has run the algorithm on our data points and come up
    with three clusters:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的k-means算法已经在我们的数据点上运行，并得出了三个簇：
- en: '[PRE25]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can take a look at the center of each cluster by using `groupby` and `mean`
    statements (visualized in *Figure 11**.18*):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`groupby`和`mean`语句来查看每个簇的中心（如*图11.18*所示）：
- en: '[PRE26]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![Figure 11.18 – Our found clusters for the beer dataset with k=3](img/B19488_11_18.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图11.18 – 我们找到的啤酒数据集的簇，k=3](img/B19488_11_18.jpg)'
- en: Figure 11.18 – Our found clusters for the beer dataset with k=3
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.18 – 我们找到的啤酒数据集的簇，k=3
- en: On inspection, we can see that *cluster 0* has, on average, a higher calorie,
    sodium, and alcohol content and costs more. These might be considered heavier
    beers. *Cluster 2* has on average a very low alcohol content and very few calories.
    These are probably light beers. *Cluster 1* is somewhere in the middle.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查，我们可以看到 *簇 0* 的卡路里、钠含量和酒精含量平均较高，且成本更高。这些可能被视为较重的啤酒。*簇 2* 的酒精含量非常低，卡路里也很少。这些可能是轻型啤酒。*簇
    1* 则处于两者之间。
- en: 'Let’s use Python to make a graph to see this in more detail, as seen in *Figure
    11**.19*:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 Python 绘制一个图表，查看更详细的内容，如 *图 11.19* 所示：
- en: '[PRE27]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Figure 11.19 – Our cluster analysis visualized using two dimensions of our
    dataset](img/B19488_11_19.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.19 – 我们通过数据集的二维展示进行聚类分析](img/B19488_11_19.jpg)'
- en: Figure 11.19 – Our cluster analysis visualized using two dimensions of our dataset
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.19 – 我们通过数据集的二维展示进行聚类分析
- en: Choosing an optimal number for K and cluster validation
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择 K 的最佳数值和聚类验证
- en: A big part of k-means clustering is knowing the optimal number of clusters.
    If we knew this number ahead of time, then that might defeat the purpose of even
    using UL. So, we need a way to evaluate the output of our cluster analysis. The
    problem here is that, because we are not performing any kind of prediction, we
    cannot gauge how right the algorithm is at predictions. Metrics such as accuracy
    and RMSE go right out of the window. Luckily, we do have a pretty useful metric
    to help optimize our cluster analyses, called the Silhouette Coefficient.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 聚类的一个重要部分是确定最优的簇数。如果我们事先知道这个数值，那么使用无监督学习（UL）的目的可能就失去了。因此，我们需要一种方法来评估聚类分析的输出。问题在于，因为我们没有进行任何类型的预测，我们无法衡量算法在预测上的准确性。诸如准确度和均方根误差（RMSE）等指标就不适用了。幸运的是，我们确实有一个相当有用的指标来帮助优化我们的聚类分析，叫做轮廓系数。
- en: The Silhouette Coefficient
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 轮廓系数
- en: 'The **Silhouette Coefficient** is a common metric for evaluating clustering
    performance in situations when the true cluster assignments are not known. The
    Silhouette Coefficient is a measure used to assess the quality of clusters created
    by a clustering algorithm. It quantifies how similar an object is to its own cluster
    (cohesion) compared to other clusters (separation). The Silhouette Coefficient
    for a single data point is calculated using the following formula:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**轮廓系数** 是在真实聚类分配未知的情况下评估聚类性能的常用指标。轮廓系数用于评估聚类算法创建的聚类质量。它量化了一个对象与其所属聚类（凝聚性）相比与其他聚类的相似度（分离度）。单个数据点的轮廓系数通过以下公式计算：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>S</mi><mi>C</mi><mo>=</mo><mfrac><mrow><mi>b</mi><mo>−</mo><mi>a</mi></mrow><mrow><mi>max</mi><mfenced
    open="(" close=")"><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/185.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>S</mi><mi>C</mi><mo>=</mo><mfrac><mrow><mi>b</mi><mo>−</mo><mi>a</mi></mrow><mrow><mi>max</mi><mfenced
    open="(" close=")"><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/185.png)'
- en: 'Here, the following applies:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，以下内容适用：
- en: '*a* is the mean distance between a sample and all other points in the same
    class or cluster. It represents the cohesion of the cluster.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a* 是样本与同一类或同一簇中所有其他点之间的平均距离。它表示聚类的凝聚性。'
- en: '*b* is the mean distance between a sample and all other points in the next
    nearest cluster. It represents the separation from the nearest cluster that the
    sample is not a part of.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b* 是一个样本与下一个最近簇中所有其他点之间的平均距离。它表示样本与最近簇（不属于该簇的簇）的分离程度。'
- en: 'It ranges from *-1 (worst)* to *1 (best)*. A global score is calculated by
    taking the mean score for all observations. The Silhouette Coefficient is particularly
    useful for determining the effectiveness of a clustering algorithm because it
    takes into account both the compactness of the clusters and the separation between
    them. In general, a Silhouette Coefficient of 1 is preferred, while a score of
    -1 is not preferable:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 该系数的范围从 *-1（最差）* 到 *1（最佳）*。通过计算所有观测值的均值得出全局得分。轮廓系数特别适用于确定聚类算法的有效性，因为它考虑了聚类的紧凑性和聚类之间的分离度。通常，轮廓系数为
    1 更为理想，而 -1 的得分则不太可取：
- en: '[PRE28]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE29]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s try calculating the coefficient for multiple values of `K` to find the
    best value:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试计算不同 `K` 值的系数，以找到最佳值：
- en: '[PRE30]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: So, it looks like our optimal number of beer clusters is 4! This means that
    our k-means algorithm has determined that there seem to be four distinct types
    of beer.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，看起来我们的最佳啤酒聚类数是4！这意味着我们的k-means算法确定似乎有四种不同类型的啤酒。
- en: '![Figure 11.20 – The Silhouette Coefficient for a varying number of clusters](img/B19488_11_20.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图11.20 – 不同聚类数下的轮廓系数](img/B19488_11_20.jpg)'
- en: Figure 11.20 – The Silhouette Coefficient for a varying number of clusters
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.20 – 不同聚类数下的轮廓系数
- en: 'k-means is a popular algorithm because of its computational efficiency and
    simple and intuitive nature. k-means, however, is highly scale-dependent and is
    not suitable for data with widely varying shapes and densities. There are ways
    to combat this issue by scaling data using `scikit-learn`’s `StandardScalar`:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: k-means是一个广受欢迎的算法，因为它的计算效率高且直观简单。然而，k-means算法对数据的规模依赖性较强，并不适用于形状和密度差异较大的数据。我们可以通过使用`scikit-learn`的`StandardScalar`来对数据进行缩放，从而解决这个问题：
- en: '[PRE31]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Easy!
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 简单！
- en: 'Now, let’s take a look at the third option in our reasons for using unsupervised
    methods: *feature extraction*.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看我们使用无监督方法的第三个原因：*特征提取*。
- en: Feature extraction and PCA
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取和PCA
- en: A common problem when working with data, particularly when it comes to ML, is
    having an overwhelming number of columns and not enough rows to handle such a
    quantity of columns.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据时，特别是在机器学习中，一个常见的问题是有过多的列，而行数不足以处理如此多的列。
- en: A great example of this is when we were looking at the *send cash now* example
    in our naïve Bayes example earlier. Remember we had literally 0 instances of texts
    with that exact phrase? In that case, we turned to a naïve assumption that allowed
    us to extrapolate a probability for both of our categories.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的例子是我们在之前的朴素贝叶斯示例中看过的*立即汇款*例子。记得我们之前的文本中没有任何包含这个确切短语的实例吗？在那种情况下，我们转向了一个朴素的假设，允许我们为我们的两个类别推断一个概率。
- en: The reason we had this problem in the first place is because of something called
    the **curse of dimensionality** (**COD**). The COD basically says that as we introduce
    new feature columns, we need exponentially more rows (data points) to consider
    the increased number of possibilities.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一开始遇到这个问题的原因是因为有一个叫做**维度灾难**（**COD**）的现象。COD基本上是说，随着我们引入新的特征列，我们需要更多的行（数据点）来考虑增加的可能性数量。
- en: 'Consider an example where we attempt to use a learning model that utilizes
    the distance between points on a corpus of text that has 4,086 pieces of text
    and that the whole thing has been count-vectorized using `scikit-learn`. Now,
    let’s do an experiment. I will first consider a single word as the only dimension
    of our text. Then, I will count how many pieces of text are within 1 unit of each
    other. For example, if 2 sentences both contain that word, they would be 0 units
    away and, similarly, if neither of them contains the word, they would be 0 units
    away from one another:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个例子，我们尝试使用一个学习模型，利用一个包含4,086篇文本的语料库中的点之间的距离，并且这些文本已通过`scikit-learn`进行了计数向量化。现在，让我们做一个实验。我将首先考虑一个单词作为我们文本的唯一维度。然后，我将计算多少篇文本在1个单位的距离内。例如，如果两个句子都包含这个词，它们之间的距离就是0单位；同样地，如果两个句子都没有包含这个词，它们之间的距离也是0单位：
- en: '[PRE32]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Important note
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that we have **16,695,396** (**4086*4086**) distances to scan over.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们有**16,695,396**（**4086*4086**）个距离需要扫描。
- en: 'So, 16.2 million pairs of texts are within a single unit of distance. Now,
    let’s try again with the first two words:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，16.2百万对文本位于单个单位距离内。现在，让我们用前两个词再试一次：
- en: '[PRE33]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'By considering a single new column, we lost about 100,000 pairs of points that
    were within a single unit of distance. This is because we are adding space in
    between them for every dimension that we add. Let’s take this test a step further
    and calculate this number for the first 100 words and then plot the results:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 通过考虑一个新的列，我们丢失了大约100,000对原本在一个单位距离内的点。这是因为我们在每增加一个维度时，实际上是在它们之间添加空间。让我们更进一步进行这个测试，计算前100个词的这个数字，然后绘制结果：
- en: '[PRE34]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, let’s plot the number of points within 1 unit versus the number of dimensions
    we consider in *Figure 11**.21*:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在*图11.21*中绘制1个单位内的数据点数与我们考虑的维度数量的关系：
- en: '![Figure 11.21 – The COD says that as we increase the number of feature columns
    in our dataset, data points become further away from each other due to the increase
    in high-dimensional space](img/B19488_11_21.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![图11.21 – COD表明随着我们在数据集里增加特征列，数据点之间的距离会因为高维空间的增加而变得更远](img/B19488_11_21.jpg)'
- en: Figure 11.21 – The COD says that as we increase the number of feature columns
    in our dataset, data points become further away from each other due to the increase
    in high-dimensional space
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.21 – COD表示，随着我们增加数据集中的特征列数量，数据点之间的距离由于高维空间的增加而变得越来越远。
- en: Put another way, the COD states that as we increase the number of feature columns,
    we need exponentially more data to maintain the same level of model performance.
    This is because, in high-dimensional spaces, even the nearest neighbors can be
    very far away from a given data point, making it difficult to create good predictions.
    High dimensionality also increases the risk of overfitting as the model may start
    to fit to noise in the data rather than the actual signal.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，COD表明，随着特征列数量的增加，我们需要指数级地增加数据量以维持相同水平的模型性能。这是因为在高维空间中，即使是最近邻的数据点也可能离某个给定的数据点非常远，这使得创建良好的预测变得困难。高维度还增加了过拟合的风险，因为模型可能开始拟合数据中的噪声而不是实际信号。
- en: Moreover, with more dimensions, the volume of the space increases so rapidly
    that the available data becomes sparse. This sparsity is problematic for any method
    that requires statistical significance. In order to obtain a reliable result,
    the amount of data needed to support the analysis often grows exponentially with
    the dimensionality.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着维度的增加，空间的体积急剧增加，导致可用数据变得稀疏。这种稀疏性对任何需要统计显著性的方法都是一个问题。为了获得可靠的结果，支持分析所需的数据量往往随着维度的增加而呈指数增长。
- en: We can see clearly that the number of points within a single unit of one another
    goes down dramatically as we introduce more and more columns. And this is only
    the first 100 columns!
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，随着列数的增加，单元内点的数量显著下降。而这仅仅是前100列而已！
- en: All of this space that we add in by considering new columns makes it harder
    for the finite amount of points we have to stay happily within range of each other.
    We would have to add more points in order to fill in this gap. And that, my friends,
    is why we should consider using dimension reduction.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过考虑新列所增加的所有空间使得我们有限的点更难保持在彼此的范围内。我们必须添加更多的点来填补这个空白。这就是为什么我们应该考虑使用降维的原因。
- en: 'The COD is solved by either adding more data points (which is not always possible)
    or implementing dimension reduction. **Dimension reduction** is simply the act
    of reducing the number of columns in our dataset and not the number of rows. There
    are two ways of implementing dimension reduction:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: COD问题可以通过增加更多数据点（虽然并非总是可能）或实施降维来解决。**降维**就是减少数据集中的列数，而不是减少行数。有两种实施降维的方法：
- en: '**Feature selection**: This is the act of creating a subset of our column features
    and only using the best features'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：这是指创建我们的列特征的子集，并仅使用最佳特征。'
- en: '**Feature extraction**: This is the act of mathematically transforming our
    feature set into a new extracted coordinate system'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：这是通过数学变换将我们的特征集转化为一个新的提取坐标系统的行为。'
- en: We are familiar with feature selection as the process of saying the `Embarked_Q`
    column is not helping our decision tree. Let’s get rid of it and see how it performs.
    It is literally when we (or the machine) make the decision to ignore certain columns.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们熟悉特征选择，通常是通过判断`Embarked_Q`列对我们的决策树没有帮助。让我们去掉它，看看模型的表现。这实际上就是我们（或机器）决定忽略某些列的过程。
- en: Feature extraction is a bit trickier.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取要复杂一些。
- en: In *feature extraction*, we are using usually fairly complicated mathematical
    formulas in order to obtain new super columns that are usually better than any
    single original column.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在*特征提取*中，我们通常使用相当复杂的数学公式来获得新的超级列，这些列通常比任何单一的原始列要好。
- en: 'Our primary model for doing so is called **PCA**. PCA will extract a set number
    of super columns in order to represent our original data with much fewer columns.
    Let’s take a concrete example. Previously, I mentioned some text with 4,086 rows
    and over 18,000 columns. That dataset is actually a set of *Yelp* online reviews:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于实现这一目标的主要模型称为**PCA**。PCA将提取一定数量的超级列，以便用更少的列表示原始数据。让我们来看一个具体的例子。之前，我提到过一些包含4,086行和超过18,000列的文本数据集。那个数据集实际上是*Yelp*在线评论数据：
- en: '[PRE35]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Our goal is to predict whether or not a person gave a 5- or 1-star review based
    on the words they used in the review. Let’s set a baseline with logistic regression
    and see how well we can predict this binary category:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是预测一个人是否给出了5星或1星的评价，基于他们在评论中使用的词语。让我们用逻辑回归设定一个基准，看看我们在预测这个二分类问题时能做到什么程度：
- en: '[PRE36]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE37]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: So, by utilizing all of the words in our corpus, our model seems to have over
    a 91% accuracy. Not bad!
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，通过利用我们语料库中的所有词语，我们的模型似乎达到了超过91%的准确率。还不错！
- en: 'Let’s try only using the top 100 used words:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试只使用前100个最常用的词：
- en: '[PRE38]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE39]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note how our training and testing matrices have 100 columns. This is because
    I told our vectorizer to only look at the top 100 words. See also that our performance
    took a hit and is now down to 88% accuracy. This makes sense because we are ignoring
    over 4,700 words in our corpus.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的训练和测试矩阵都有100列。这是因为我让我们的向量化器只关注最常用的100个词。同时也能看到，我们的性能有所下降，现在准确率降到了88%。这也很有道理，因为我们忽略了语料库中超过4,700个词。
- en: 'Now, let’s take a different approach. Let’s import a PCA module and tell it
    to make us 100 new super columns and see how that performs:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们换个方法。我们导入一个PCA模块，让它生成100个新的超级特征列，然后看看效果如何：
- en: '[PRE40]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE41]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Not only do our matrices still have 100 columns, but these columns are no longer
    words in our corpus. They are complex transformations of columns and are 100 new
    columns. Also, note that using 100 of these new columns gives us a better predictive
    performance than using the 100 top words!
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的矩阵不仅仍然有100列，而且这些列不再是我们语料库中的词语。它们是列的复杂变换，形成了100个新的特征列。还要注意，使用这100个新列比使用100个最常用词更能提高预测性能！
- en: Feature extraction is a great way to use mathematical formulas to extract brand-new
    columns that generally perform better than just selecting the best ones beforehand.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取是一种利用数学公式提取全新特征列的好方法，这些特征通常比预先选择最佳特征的方式表现得更好。
- en: 'But how do we visualize these new super columns? Well, I can think of no better
    way than to look at an example using image analysis. Specifically, let’s make
    facial recognition software. OK? OK. Let’s begin by importing some faces given
    to us by `scikit-learn`:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该如何可视化这些新的超级特征列呢？嗯，我想不出比通过图像分析来展示更好的方法了。具体来说，让我们做一个人脸识别软件。好吗？好的。让我们从`scikit-learn`提供的一些人脸数据开始：
- en: '[PRE42]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We have gathered 1,288 images of people’s faces, and each one has 1,850 features
    (pixels) that identify that person. Here’s the code we used – an example of one
    of our faces can be seen in *Figure 11**.22*:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集了1,288张人脸图像，每张图像有1,850个特征（像素）用来识别该人。这是我们使用的代码——其中一张人脸的示例见于*图 11.22*：
- en: '[PRE43]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![Figure 11.22 – A face from our dataset: George W. Bush](img/B19488_11_22.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.22 – 来自我们数据集的人脸：乔治·W·布什](img/B19488_11_22.jpg)'
- en: 'Figure 11.22 – A face from our dataset: George W. Bush'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.22 – 来自我们数据集的人脸：乔治·W·布什
- en: 'Great! To get a glimpse at the type of dataset we are looking at, let’s look
    at a few overall metrics:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！为了大致了解我们正在处理的数据集类型，先来看一些整体指标：
- en: '[PRE44]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: So, we have 1,288 images, 1,850 features, and 7 classes (people) to choose from.
    Our goal is to make a classifier that will assign the person’s face a name based
    on the 1,850 pixels given to us.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有1,288张图像，1,850个特征，以及7个类别（人名）可以选择。我们的目标是创建一个分类器，根据给定的1,850个像素来为人脸分配一个名字。
- en: Let’s take a baseline and see how a logistic regression (a classifier that is
    based on linear regression) performs on our data without doing anything to our
    dataset.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先设定一个基准，看看在没有对数据集做任何处理的情况下，逻辑回归（基于线性回归的分类器）在我们的数据上表现如何。
- en: I know we haven’t formally introduced logistic regressions before, but they
    are a very lightweight classifier that works off of very similar assumptions as
    linear regressions from the last chapter. All we need to know for now is that
    it performs classification!
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道我们之前没有正式介绍过逻辑回归，但它是一个非常轻量级的分类器，基于与上一章中的线性回归非常相似的假设。现在我们只需要知道，它执行的是分类任务！
- en: '[PRE45]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output is as follows:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE46]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: So, within 6.3 seconds, we were able to get 81% on our test set. Not too bad.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在6.3秒内，我们在测试集上获得了81%的准确率。还不错吧。
- en: 'Now, let’s try this with our decomposed faces:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用分解过的人脸数据：
- en: '[PRE47]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The preceding code is collecting `75` extracted columns from our 1,850 unprocessed
    columns. These are our super faces. Now, let’s plug in our newly extracted columns
    into our logistic regression and compare:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码从我们 1,850 个未处理的列中提取了 `75` 个列。这些是我们的超级面孔。现在，让我们把新提取的列输入到我们的逻辑回归模型中进行比较：
- en: '[PRE48]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Wow! Not only was this entire calculation about 30 times faster than the unprocessed
    images, but the predictive performance also got better! This shows us that PCA
    and feature extraction, in general, can help us all around when performing ML
    on complex datasets with many columns. By searching for these patterns in the
    dataset and extracting new feature columns, we can speed up and enhance our learning
    algorithms.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！不仅整个计算比未处理的图像快了大约 30 倍，而且预测性能也变得更好了！这表明，PCA 和特征提取通常可以帮助我们在处理包含许多列的复杂数据集时更加高效。通过在数据集中寻找这些模式并提取新的特征列，我们可以加速并增强我们的学习算法。
- en: 'Let’s look at one more interesting thing. I mentioned before that one of the
    purposes of this example was to examine and visualize our eigenfaces, as they
    are called: our super columns. I will not disappoint. Let’s write some code that
    will show us our super columns as they would look to us humans:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一个有趣的东西。我之前提到过，这个例子的一部分目的是检查并可视化我们的特征脸，正如它们所被称的那样：我们的超级列。我不会让你失望的。让我们写一些代码，展示给我们这些超级列在人类眼中是怎样的：
- en: '[PRE49]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Warning: the faces in *Figure 11**.23* are a bit creepy!'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：*图 11.23* 中的面孔有点吓人！
- en: '![Figure 11.23 – Performing PCA on the pixels of our faces creates what is
    known as “eigenfaces” that represent features that our classifiers look for when
    trying to recognize faces](img/B19488_11_23.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.23 – 对我们面部像素进行 PCA 处理，创建了所谓的“特征脸”，这些特征代表了我们的分类器在尝试识别面孔时会寻找的特征](img/B19488_11_23.jpg)'
- en: Figure 11.23 – Performing PCA on the pixels of our faces creates what is known
    as “eigenfaces” that represent features that our classifiers look for when trying
    to recognize faces
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.23 – 对我们面部像素进行 PCA 处理，创建了所谓的“特征脸”，这些特征代表了我们的分类器在尝试识别面孔时会寻找的特征
- en: Wow! A haunting and yet beautiful representation of what the data believes to
    be the most important features of a face. As we move from the top left (first
    super column) to the bottom, it is actually somewhat easy to see what the image
    is trying to tell us. The first super column looks like a very general face structure
    with eyes and nose and a mouth. It is almost saying “I represent the basic qualities
    of a face that all faces must have.” Our second super column directly to its right
    seems to be telling us about shadows in the image. The next one might be telling
    us that skin tone plays a role in detecting who this is, which might be why the
    third face is much darker than the first two.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！这是一个令人毛骨悚然而又美丽的表现，展现了数据认为最重要的面部特征。当我们从左上角（第一个超级列）向下移动时，实际上很容易看出图像想要告诉我们什么。第一个超级列看起来像是一个非常一般的面部结构，有眼睛、鼻子和嘴巴。它几乎在说：“我代表所有面孔必须具备的基本特征。”第二个超级列就在它的右边，似乎在告诉我们图像中的阴影。接下来的一个可能在告诉我们肤色在识别这个人时起了作用，这也许就是为什么第三张脸比前两张要黑得多。
- en: Using feature extraction UL methods such as PCA can give us a very deep look
    into our data and reveal to us what the data believes to be the most important
    features, not just what we believe them to be. Feature extraction is a great preprocessing
    tool that can speed up our future learning methods, make them more powerful, and
    give us more insight into how the data believes it should be viewed. To sum up
    this section, we will list the pros and cons.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征提取的超大规模方法（如 PCA）可以让我们深入了解数据，并揭示数据认为最重要的特征，而不仅仅是我们认为的特征。特征提取是一个很好的预处理工具，它可以加速我们未来的学习方法，使其更强大，并让我们更深入了解数据如何认为应该被解读。总结这一部分，我们将列出优缺点。
- en: 'Here are some of the advantages of using feature extraction:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用特征提取的一些优点：
- en: Our models become much faster
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的模型变得更快
- en: Our predictive performance can become better
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的预测性能可能会变得更好
- en: It can give us insight into the extracted features (eigenfaces)
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以让我们深入了解提取的特征（特征脸）
- en: 'And here are some of the disadvantages of using feature extraction:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用特征提取的一些缺点：
- en: We lose some of the interpretability of our features as they are new mathematically
    derived columns, not our old ones
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们失去了一些特征的可解释性，因为它们是新的数学推导列，而不是我们原来的列。
- en: We can lose predictive performance because we are losing information as we extract
    fewer columns
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能会失去预测性能，因为我们在提取较少列时丢失了信息。
- en: Let’s move on to the summary next.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们进入总结部分。
- en: Summary
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Our exploration into the world of ML has revealed a vast landscape that extends
    well beyond the foundational techniques of linear and logistic regression. We
    delved into decision trees, which provide intuitive insights into data through
    their hierarchical structure. Naïve Bayes classification offered us a probabilistic
    perspective, showing how to make predictions under the assumption of feature independence.
    We ventured into dimensionality reduction, encountering techniques such as feature
    extraction, which help overcome the COD and reduce computational complexity.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对机器学习世界的探索揭示了一个广阔的领域，远远超出了线性回归和逻辑回归等基础技术。我们深入研究了决策树，通过其层级结构提供了直观的数据洞察。朴素贝叶斯分类则为我们提供了一种概率视角，展示了在特征独立假设下如何进行预测。我们还涉足了降维，接触到了特征提取等技术，帮助克服了维度灾难（COD）并减少了计算复杂度。
- en: k-means clustering introduced us to the realm of UL, where we learned to find
    hidden patterns and groupings in data without pre-labeled outcomes. Across these
    methods, we’ve seen how ML can tackle a plethora of complex problems, from predicting
    categorical outcomes to uncovering latent structures in data.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类将我们引入了无监督学习（UL）的领域，在这里我们学会了如何在没有预标记结果的情况下，发现数据中的隐藏模式和分组。在这些方法中，我们看到了机器学习如何解决一系列复杂问题，从预测分类结果到揭示数据中的潜在结构。
- en: Through practical examples, we’ve compared and contrasted SL, which relies on
    labeled data, with UL, which operates without explicit guidance on the output.
    This journey has equipped us with a deeper understanding of the various techniques
    and their appropriate applications within the broad and dynamic field of data
    science.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实际例子，我们对比了依赖于标记数据的监督学习（SL）与在没有明确输出指导下运行的无监督学习（UL）。这段旅程使我们更深入地理解了各种技术及其在广泛且动态的数据科学领域中的适用场景。
- en: As we continue to harness the power of these algorithms, we are reminded of
    the importance of selecting the right model for the right task—a principle that
    remains central to the practice of effective data science.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续利用这些算法的力量时，我们时刻提醒自己，选择适合任务的模型至关重要——这一原则在有效的数据科学实践中始终占据核心地位。
