- en: Transformations and Actions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换和操作
- en: Transformations and actions are the main building blocks of an Apache Spark
    program. In this chapter, we will look at Spark transformations to defer computations
    and then look at which transformations should be avoided. We will then use the `reduce`
    and `reduceByKey` methods to carry out calculations from a dataset. We will then
    perform actions that trigger actual computations on graphs. By the end of this
    chapter, we will also have learned how to reuse the same `rdd` for different actions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 转换和操作是Apache Spark程序的主要构建模块。在本章中，我们将看一下Spark转换来推迟计算，然后看一下应该避免哪些转换。然后，我们将使用`reduce`和`reduceByKey`方法对数据集进行计算。然后，我们将执行触发实际计算的操作。在本章结束时，我们还将学习如何重用相同的`rdd`进行不同的操作。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Using Spark transformations to defer computations to a later time
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark转换来推迟计算到以后的时间
- en: Avoiding transformations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免转换
- en: Using the `reduce` and `reduceByKey` methods to calculate the result
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`reduce`和`reduceByKey`方法来计算结果
- en: Performing actions that trigger actual computations of our **Directed Acyclic
    Graph** (**DAG**)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行触发实际计算我们的**有向无环图**（**DAG**）的操作
- en: Reusing the same `rdd` for different actions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重用相同的`rdd`进行不同的操作
- en: Using Spark transformations to defer computations to a later time
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark转换来推迟计算到以后的时间
- en: Let's first understand Spark DAG creation. We will be executing DAG by issuing
    the action and also deferring the decision about starting the job until the last
    possible moment to check what this possibility gives us.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解Spark DAG的创建。我们将通过发出操作来执行DAG，并推迟关于启动作业的决定，直到最后一刻来检查这种可能性给我们带来了什么。
- en: Let's have a look at the code we will be using in this section.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们将在本节中使用的代码。
- en: 'First, we need to initialize Spark. Every test we carry out will be the same.
    We need to initialize it before we start using it, as shown in the following example:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要初始化Spark。我们进行的每个测试都是相同的。在开始使用之前，我们需要初始化它，如下例所示：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we will have the actual test. Here, `test` is called `should defer computation`. It
    is simple but shows a very powerful abstraction of Spark. We start by creating
    an `rdd` of `InputRecord`, as shown in the following example:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将进行实际测试。在这里，`test`被称为`should defer computation`。它很简单，但展示了Spark的一个非常强大的抽象。我们首先创建一个`InputRecord`的`rdd`，如下例所示：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`InputRecord` is a case class that has a unique identifier, which is an optional
    argument.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`InputRecord`是一个具有可选参数的唯一标识符的案例类。'
- en: 'It can be a random `uuid` if we are not supplying it and the required argument,
    that is, `userId`. The `InputRecord` will be used throughout this book for testing
    purposes. We have created two records of the `InputRecord` that we will apply
    a transformation on, as shown in the following example:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有提供它和必需的参数`userId`，它可以是一个随机的`uuid`。`InputRecord`将在本书中用于测试目的。我们已经创建了两条`InputRecord`的记录，我们将对其应用转换，如下例所示：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will only filter records that have `A` in the `userId` field. We will then
    transform it to the `keyBy(_.userId)` and then extract `userId` from value and
    map it `toLowerCase`. This is our `rdd`. So, here, we have only created DAG, which
    we have not executed yet. Let's assume that we have a complex program and we are
    creating a lot of those acyclic graphs before the actual logic.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只会过滤`userId`字段中包含`A`的记录。然后我们将其转换为`keyBy(_.userId)`，然后从值中提取`userId`并将其映射为小写。这就是我们的`rdd`。所以，在这里，我们只创建了DAG，但还没有执行。假设我们有一个复杂的程序，在实际逻辑之前创建了许多这样的无环图。
- en: 'The pros of Spark are that this is not executed until action is issued, but
    we can have some conditional logic. For example, we can get a fast path execution.
    Let''s assume that we have `shouldExecutePartOfCode()`, which can check a configuration
    switch, or go to the rest service to calculate if the `rdd` calculation is still
    relevant, as shown in the following example:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的优点是直到发出操作之前不会执行，但我们可以有一些条件逻辑。例如，我们可以得到一个快速路径的执行。假设我们有`shouldExecutePartOfCode()`，它可以检查配置开关，或者去REST服务计算`rdd`计算是否仍然相关，如下例所示：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We have used simple methods for testing purposes that we are just returning
    `true` for, but, in real life, this could be complex logic:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了简单的方法进行测试，我们只是返回`true`，但在现实生活中，这可能是复杂的逻辑：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: After it returns `true`, we can decide if we want to execute the DAG or not.
    If we want to, we can call `rdd.collect().toList` or `saveAsTextFile` to execute
    the `rdd`. Otherwise, we can have a fast path and decide that we are no longer
    interested in the input `rdd`. By doing this, only the graph will be created.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在它返回`true`之后，我们可以决定是否要执行DAG。如果要执行，我们可以调用`rdd.collect().toList`或`saveAsTextFile`来执行`rdd`。否则，我们可以有一个快速路径，并决定我们不再对输入的`rdd`感兴趣。通过这样做，只会创建图。
- en: 'When we start the test, it will take some time to complete and return the following
    output:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始测试时，它将花费一些时间来完成，并返回以下输出：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can see that our test passed and we can conclude that it worked as expected.
    Now, let's look at some transformations that should be avoided.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的测试通过了，我们可以得出它按预期工作的结论。现在，让我们看一些应该避免的转换。
- en: Avoiding transformations
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 避免转换
- en: In this section, we will look at the transformations that should be avoided.
    Here, we will focus on one particular transformation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看一下应该避免的转换。在这里，我们将专注于一个特定的转换。
- en: We will start by understanding the `groupBy` API. Then, we will investigate
    data partitioning when using `groupBy`, and then we will look at what a skew partition
    is and why should we avoid skew partitions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从理解`groupBy`API开始。然后，我们将研究在使用`groupBy`时的数据分区，然后我们将看一下什么是skew分区以及为什么应该避免skew分区。
- en: 'Here, we are creating a list of transactions. `UserTransaction` is another
    model class that includes `userId` and `amount`. The following code block shows
    a typical transaction where we are creating a list of five transactions:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在创建一个交易列表。`UserTransaction`是另一个模型类，包括`userId`和`amount`。以下代码块显示了一个典型的交易，我们正在创建一个包含五个交易的列表：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We have created four transactions for `userId = "A"`, and one for `userId =
    "B"`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为`userId = "A"`创建了四笔交易，为`userId = "B"`创建了一笔交易。
- en: 'Now, let''s consider that we want to coalesce transactions for a specific `userId`
    to have the list of transactions. We have an `input` that we are grouping by `userId`,
    as shown in the following example:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑我们想要合并特定`userId`的交易以获得交易列表。我们有一个`input`，我们正在按`userId`分组，如下例所示：
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For every `x` element, we will create a tuple. The first element of a tuple
    is an ID, while the second element is an iterator of every transaction for that
    specific ID. We will transform it into a list using `toList`. Then, we will collect
    everything and assign it to `toList` to have our result. Let''s assert the result. `rdd`
    should contain the same element as `B`, that is, the key and one transaction,
    and `A`, which has four transactions, as shown in the following code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个`x`元素，我们将创建一个元组。元组的第一个元素是一个ID，而第二个元素是该特定ID的每个交易的迭代器。我们将使用`toList`将其转换为列表。然后，我们将收集所有内容并将其分配给`toList`以获得我们的结果。让我们断言结果。`rdd`应该包含与`B`相同的元素，即键和一个交易，以及`A`，其中有四个交易，如下面的代码所示：
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s start this test and check if this behaves as expected. We get the following
    output:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始这个测试，并检查它是否按预期行为。我们得到以下输出：
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: At first glance, it has passed and it works as expected. But the question arises
    as to why we want to group it. We want to group it to save it to the filesystem
    or do some further operations, such as concatenating all the amounts.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，它已经通过了，并且按预期工作。但是，为什么我们要对它进行分组的问题就出现了。我们想要对它进行分组以将其保存到文件系统或进行一些进一步的操作，例如连接所有金额。
- en: We can see that our input is not a normal distribution, since almost all the
    transactions are for the `userId = "A"`. Because of that, we have a key that is
    skewed. This means that one key has the majority of the data in it and that the other
    keys have a lower amount of data. When we use `groupBy` in Spark, it takes all
    the elements that have the same grouping, which in this example is `userId`, and
    sends those values to exactly the same executors.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的输入不是正常分布的，因为几乎所有的交易都是针对`userId = "A"`。因此，我们有一个偏斜的键。这意味着一个键包含大部分数据，而其他键包含较少的数据。当我们在Spark中使用`groupBy`时，它会获取所有具有相同分组的元素，例如在这个例子中是`userId`，并将这些值发送到完全相同的执行者。
- en: For example, if our executors have 5 GB of memory and we have a really big dataset
    that has hundreds of gigabytes and one key has 90 percent of data, it means that everything
    will go to one executor and the rest of the executors will take a minority of
    the data. So, the data will not be normally distributed and, because of the non-uniform
    distribution, processing will not be as efficient as possible.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的执行者有5GB的内存，我们有一个非常大的数据集，有数百GB，其中一个键有90%的数据，这意味着所有数据都将传输到一个执行者，其余的执行者将获取少数数据。因此，数据将不会正常分布，并且由于非均匀分布，处理效率将不会尽可能高。
- en: So, when we use the `groupBy` key, we must first answer the question of why
    we want to group it. Maybe we can filter it or aggregate it at the lower level
    before `groupBy` and then we will only group the results, or maybe we don't group
    at all. We will be investigating how to solve that problem with Spark API in the
    following sections.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们使用`groupBy`键时，我们必须首先回答为什么要对其进行分组的问题。也许我们可以在`groupBy`之前对其进行过滤或聚合，然后我们只会对结果进行分组，或者根本不进行分组。我们将在以下部分中研究如何使用Spark
    API解决这个问题。
- en: Using the reduce and reduceByKey methods to calculate the results
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用reduce和reduceByKey方法来计算结果
- en: In this section, we will use the `reduce` and `reduceBykey` functions to calculate
    our results and understand the behavior of `reduce`. We will then compare the `reduce`
    and `reduceBykey` functions to check which of the functions should be used in
    a particular use case.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用`reduce`和`reduceBykey`函数来计算我们的结果，并了解`reduce`的行为。然后，我们将比较`reduce`和`reduceBykey`函数，以确定在特定用例中应该使用哪个函数。
- en: 'We will first focus on the `reduce` API. First, we need to create an input
    of `UserTransaction`. We have the user transaction `A` with amount `10`, `B` with
    amount `1`, and `A` with amount `101`. Let''s say that we want to find out the
    global maximum. We are not interested in the data for the specific key, but in
    the global data. We want to scan it, take the maximum, and return it, as shown
    in the following example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先关注`reduce`API。首先，我们需要创建一个`UserTransaction`的输入。我们有用户交易`A`，金额为`10`，`B`的金额为`1`，`A`的金额为`101`。假设我们想找出全局最大值。我们对特定键的数据不感兴趣，而是对全局数据感兴趣。我们想要扫描它，取最大值，并返回它，如下例所示：
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So, this is the reduced use case. Now, let''s see how we can implement it,
    as per the following example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是减少使用情况。现在，让我们看看如何实现它，如下例所示：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For the `input`, we need to first map the field that we're interested in. In
    this case, we are interested in `amount`. We will take `amount` and then take
    the maximum value.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`input`，我们需要首先映射我们感兴趣的字段。在这种情况下，我们对`amount`感兴趣。我们将取`amount`，然后取最大值。
- en: In the preceding code example, `reduce` has two parameters, `a` and `b`. One
    parameter will be the current maximum in the specific Lambda that we are passing,
    and the second one will be our actual value, which we are investigating now. If
    the value was higher than the maximum state until now, we will return `a`; if
    not, it will return `b`. We will go through all the elements and, at the end,
    the result will be just one long number.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，`reduce`有两个参数，`a`和`b`。一个参数将是我们正在传递的特定Lambda中的当前最大值，而第二个参数将是我们现在正在调查的实际值。如果该值高于到目前为止的最大状态，我们将返回`a`；如果不是，它将返回`b`。我们将遍历所有元素，最终结果将只是一个长数字。
- en: 'So, let''s test this and check whether the result is indeed `101`, as shown
    in the following code output. This means our test passed:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们测试一下，检查结果是否确实是`101`，如以下代码输出所示。这意味着我们的测试通过了。
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, let''s consider a different situation. We want to find the maximum transaction
    amount, but this time we want to do it according to users. We not only want to
    find out the maximum transaction for user `A` but also for user `B`, but we want
    those things to be independent. So, for every key that is the same, we want to
    take only the maximum value from our data, as shown in the following example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑一个不同的情况。我们想找到最大的交易金额，但这次我们想根据用户来做。我们不仅想找出用户`A`的最大交易，还想找出用户`B`的最大交易，但我们希望这些事情是独立的。因此，对于相同的每个键，我们只想从我们的数据中取出最大值，如以下示例所示：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To achieve this, `reduce` is not a good choice because it will go through all
    of the values and give us the global maximum. We have key operations in Spark
    but, first, we want to do it for a specific group of elements. We need to use `keyBy`
    to tell Spark which ID should be taken as the unique one and it will execute the `reduce`
    function only within the specific key. So, we use `keyBy(_.userId)` and then we
    get the `reducedByKey` function. The `reduceByKey` function is similar to `reduce`
    but it works key-wise so, inside the Lambda, we''ll only get values for a specific
    key, as shown in the following example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这一点，`reduce`不是一个好选择，因为它将遍历所有的值并给出全局最大值。我们在Spark中有关键操作，但首先，我们要为特定的元素组做这件事。我们需要使用`keyBy`告诉Spark应该将哪个ID作为唯一的，并且它将仅在特定的键内执行`reduce`函数。因此，我们使用`keyBy(_.userId)`，然后得到`reducedByKey`函数。`reduceByKey`函数类似于`reduce`，但它按键工作，因此在Lambda内，我们只会得到特定键的值，如以下示例所示：
- en: '[PRE14]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: By doing this, we get the first transaction and then the second one. The first
    one will be a current maximum and the second one will be the transaction that
    we are investigating right now. We will create a helper function that is taking
    those transactions and call it `higherTransactionAmount`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们得到第一笔交易，然后是第二笔。第一笔将是当前的最大值，第二笔将是我们正在调查的交易。我们将创建一个辅助函数，它接受这些交易并称之为`higherTransactionAmount`。
- en: The `higherTransactionAmount` function is used in taking the `firstTransaction`
    and `secondTransaction`. Please note that for the `UserTransaction` type, we need
    to pass that type. It also needs to return `UserTransaction` that we cannot return
    a different type.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`higherTransactionAmount`函数用于获取`firstTransaction`和`secondTransaction`。请注意，对于`UserTransaction`类型，我们需要传递该类型。它还需要返回`UserTransaction`，我们不能返回不同的类型。'
- en: 'If you are using the `reduceByKey` method from Spark, we need to return the
    same type as that of the `input` arguments. If `firstTransaction.amount` is higher
    than `secondTransaction.amount`, we will just return the `firstTransaction` since
    we are returning the `secondTransaction`, so transaction objects not the total
    amount. This is shown in the following example:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用Spark的`reduceByKey`方法，我们需要返回与`input`参数相同的类型。如果`firstTransaction.amount`高于`secondTransaction.amount`，我们将返回`firstTransaction`，因为我们返回的是`secondTransaction`，所以是交易对象而不是总金额。这在以下示例中显示：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we will collect, add, and test the transaction. After our test, we have
    the output where, for the key `B`, we should get transaction `("B", 1)` and, for
    the key `A`, transaction `("A", 101)`. There will be no transaction `("A", 10)` because
    we filtered it out, but we can see that for every key, we are able to find out
    the maximums. This is shown in the following example:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将收集、添加和测试交易。在我们的测试之后，我们得到了输出，对于键`B`，我们应该得到交易`("B", 1)`，对于键`A`，交易`("A",
    101)`。没有交易`("A", 10)`，因为我们已经过滤掉了它，但我们可以看到对于每个键，我们都能找到最大值。这在以下示例中显示：
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can see that the test passed and everything is as expected, as shown in
    the following output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到测试通过了，一切都如预期的那样，如以下输出所示：
- en: '[PRE17]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the next section, we will perform actions that trigger the computations of
    our data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将执行触发数据计算的操作。
- en: Performing actions that trigger computations
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行触发计算的操作
- en: Spark has a lot more actions that issue DAG, and we should be aware of all of
    them because they are very important. In this section, we'll understand what can
    be an action in Spark, do a walk-through of actions, and test those actions if
    they behave as expected.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有更多触发DAG的操作，我们应该了解所有这些，因为它们非常重要。在本节中，我们将了解Spark中可以成为操作的内容，对操作进行一次遍历，并测试这些操作是否符合预期。
- en: The first action we covered is `collect`. We also covered two actions besides
    that—we covered both `reduce` and `reduceByKey` in the previous section. Both
    methods are actions because they return a single result.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖的第一个操作是`collect`。除此之外，我们还涵盖了两个操作——在上一节中我们都涵盖了`reduce`和`reduceByKey`。这两种方法都是操作，因为它们返回单个结果。
- en: 'First, we will create the `input` of our transactions and then apply some transformations
    just for testing purposes. We will take only the user that contains `A`, using
    `keyBy_.userId`, and then take only the amount of the required transaction, as
    shown in the following example:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建我们的交易的`input`，然后应用一些转换，仅用于测试目的。我们将只取包含`A`的用户，使用`keyBy_.userId`，然后只取所需交易的金额，如以下示例所示：
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The first action that we are already aware of is `rdd.collect().toList`. The
    next one is `count()`, which needs to take all of the values and calculate how
    many values are inside the `rdd`. There is no way to execute `count()` without
    triggering the transformation. Also, there are different methods in Spark, such
    as `countApprox`, `countApproxDistinct`, `countByValue`, and `countByValueApprox`.
    The following example shows us the code for `rdd.collect().toList`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道的第一个操作是`rdd.collect().toList`。接下来是`count()`，它需要获取所有的值并计算`rdd`中有多少值。没有办法在不触发转换的情况下执行`count()`。此外，Spark中还有不同的方法，如`countApprox`、`countApproxDistinct`、`countByValue`和`countByValueApprox`。以下示例显示了`rdd.collect().toList`的代码：
- en: '[PRE19]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If we have a huge dataset and the approximate counter is enough, you can use `countApprox`
    as it will be a lot faster. We then use `rdd.first()`, but this option is a bit
    different because it only needs to take the first element. Sometimes, if you want
    to take the first element and execute everything inside our DAG, we need to be
    focus on that and check it in the following way:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个庞大的数据集，并且近似计数就足够了，你可以使用`countApprox`，因为它会快得多。然后我们使用`rdd.first()`，但这个选项有点不同，因为它只需要取第一个元素。有时，如果你想取第一个元素并执行我们DAG中的所有操作，我们需要专注于这一点，并以以下方式检查它：
- en: '[PRE20]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Also, on the `rdd`, we have `foreach()`, which is a for loop to which we can
    pass any function. A Scala function or a Java function is assumed to be Lambda,
    but to execute elements of our result `rdd`, DAG needs to be calculated because from
    here onwards, it is an action. Another variant of the `foreach()` method is `foreachPartition()`,
    which takes every partition and returns an iterator for the partition. Inside
    that, we have an iterator to carry our iterations again and then print our elements.
    We also have our `max()` and `min()` methods and, as expected, `max()` is taking
    the maximum value and `min()` is taking the minimum value. But these methods are
    taking the implicit ordering.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在`rdd`上，我们有`foreach()`，这是一个循环，我们可以传递任何函数。假定Scala函数或Java函数是Lambda，但要执行我们结果`rdd`的元素，需要计算DAG，因为从这里开始，它就是一个操作。`foreach()`方法的另一个变体是`foreachPartition()`，它获取每个分区并返回分区的迭代器。在其中，我们有一个迭代器再次进行迭代并打印我们的元素。我们还有我们的`max()`和`min()`方法，预期的是，`max()`取最大值，`min()`取最小值。但这些方法都需要隐式排序。
- en: 'If we have an `rdd` of a simple primitive type, like `Long`, we don''t need
    to pass it here. But if we do not use `map()`, we need to define the ordering
    for the `UserTransaction` for Spark to find out which element is `max` and which
    element is `min`. These two things need to execute the DAG and so they are classed
    as actions, as shown in the following example:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个简单的原始类型的`rdd`，比如`Long`，我们不需要在这里传递它。但如果我们不使用`map()`，我们需要为Spark定义`UserTransaction`的排序，以便找出哪个元素是`max`，哪个元素是`min`。这两件事需要执行DAG，因此它们被视为操作，如下面的例子所示：
- en: '[PRE21]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We then have `takeOrdered()`, which is a more time-consuming operation than `first()`
    because `first()` takes a random element. `takeOrdered()` needs to execute DAG
    and sort everything. When everything is sorted, only then does it take the top
    element.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有`takeOrdered()`，这是一个比`first()`更耗时的操作，因为`first()`取一个随机元素。`takeOrdered()`需要执行DAG并对所有内容进行排序。当一切都排序好后，它才取出顶部的元素。
- en: 'In our example, we are taking `num = 1`. But sometimes, for testing or monitoring
    purposes, we need to take only the sample of the data. To take a sample, we use
    the `takeSample()` method and pass a number of elements, as shown in the following
    code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们取`num = 1`。但有时，出于测试或监控的目的，我们需要只取数据的样本。为了取样，我们使用`takeSample()`方法并传递一个元素数量，如下面的代码所示：
- en: '[PRE22]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, let''s start the test and see the output of implementing the previous
    actions, as shown in the following screenshot:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始测试并查看实现前面操作的输出，如下面的屏幕截图所示：
- en: '[PRE23]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The first action returns all values. The second actions return `4` as a count.
    We will consider the first element, `1001`, but this is a random value and it
    is not ordered. We will then print all the elements in the loop, as shown in the following
    output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个操作返回所有值。第二个操作返回`4`作为计数。我们将考虑第一个元素`1001`，但这是一个随机值，它是无序的。然后我们在循环中打印所有的元素，如下面的输出所示：
- en: '[PRE24]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We then get `max` and `min` values like `1001` and `1`, which are similar to
    `first()`. After that, we get an ordered list, `List(1)`, and sample `List(100,
    1``)`, which is random. So, in the sample, we get random values from input data
    and applied transformations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到`max`和`min`值，如`1001`和`1`，这与`first()`类似。之后，我们得到一个有序列表`List(1)`，和一个样本`List(100,
    1)`，这是随机的。因此，在样本中，我们从输入数据和应用的转换中得到随机值。
- en: In the next section, we will learn how to reuse the `rdd` for different actions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何重用`rdd`进行不同的操作。
- en: Reusing the same rdd for different actions
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重用相同的rdd进行不同的操作
- en: In this section, we will reuse the same `rdd` for different actions. First,
    we will minimize the execution time by reusing the `rdd`. We will then look at
    caching and a performance test for our code.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将重用相同的`rdd`进行不同的操作。首先，我们将通过重用`rdd`来最小化执行时间。然后，我们将查看缓存和我们代码的性能测试。
- en: 'The following example is the test from the preceding section but a bit modified,
    as here we take `start` by `currentTimeMillis()` and the `result`. So, we are
    just measuring the `result` of all actions that are executed:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的例子是前面部分的测试，但稍作修改，这里我们通过`currentTimeMillis()`取`start`和`result`。因此，我们只是测量执行的所有操作的`result`：
- en: '[PRE25]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If someone doesn''t know Spark very well, they will assume that all actions
    are cleverly executed. We know that every action count means that we are going
    up to the `rdd` in the chain, which means we are going to all transformations
    to load data. In the production system, loading data will be from an external
    PI system such as HDFS. This means that every action causes the call to the filesystem,
    which retrieves all data and then applies transformations, as shown in the following
    example:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人对Spark不太了解，他们会认为所有操作都被巧妙地执行了。我们知道每个操作都意味着我们要上升到链中的`rdd`，这意味着我们要对所有的转换进行加载数据。在生产系统中，加载数据将来自外部的PI系统，比如HDFS。这意味着每个操作都会导致对文件系统的调用，这将检索所有数据，然后应用转换，如下例所示：
- en: '[PRE26]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This is a very expensive operation as every action is very costly. When we
    start this test, we can see that the time taken without caching will take `632`
    milliseconds, as shown in the following output:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常昂贵的操作，因为每个操作都非常昂贵。当我们开始这个测试时，我们可以看到没有缓存的时间为632毫秒，如下面的输出所示：
- en: '[PRE27]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s compare this with the caching use. Our test, at first glance, looks
    very similar, but this is not the same because you are issuing `cache()` and we
    are returning `rdd`. So, `rdd` will be already cached and every subsequent call
    to the `rdd` will go through the `cache`, as shown in the following example:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这与缓存使用进行比较。乍一看，我们的测试看起来非常相似，但这并不相同，因为您正在使用`cache()`，而我们正在返回`rdd`。因此，`rdd`将已经被缓存，对`rdd`的每个后续调用都将经过`cache`，如下例所示：
- en: '[PRE28]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The first action will execute DAG, save the data into our cache, and then the
    subsequent actions will just retrieve the specific things according to the method
    that was called from memory. There will be no HDFS lookup, so let''s start this
    test, as per the following example, and see how long it takes:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个操作将执行DAG，将数据保存到我们的缓存中，然后后续的操作将根据从内存中调用的方法来检索特定的内容。不会有HDFS查找，所以让我们按照以下示例开始这个测试，看看需要多长时间：
- en: '[PRE29]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The first output will be as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个输出将如下所示：
- en: '[PRE30]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The second output will be as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个输出将如下所示：
- en: '[PRE31]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Without cache, the value is `585` milliseconds and with cache, the value is
    `336`. The difference is not much as we are just creating data in tests. However,
    in real production systems, this will be a big difference because we need to look
    up data from external filesystems.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 没有缓存，值为`585`毫秒，有缓存时，值为`336`。这个差异并不大，因为我们只是在测试中创建数据。然而，在真实的生产系统中，这将是一个很大的差异，因为我们需要从外部文件系统中查找数据。
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: So, let's sum up this chapter. Firstly, we used Spark transformations to defer
    computation to a later time, and then we learned which transformations should
    be avoided. Next, we looked at how to use `reduceByKey` and `reduce` to calculate
    our result globally and per specific key. After that, we performed actions that
    triggered computations then learned that every action means a call to the loading
    data. To alleviate that problem, we learned how to reduce the same `rdd` for different
    actions.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们总结一下这一章节。首先，我们使用Spark转换来推迟计算到以后的时间，然后我们学习了哪些转换应该避免。接下来，我们看了如何使用`reduceByKey`和`reduce`来计算我们的全局结果和特定键的结果。之后，我们执行了触发计算的操作，然后了解到每个操作都意味着加载数据的调用。为了缓解这个问题，我们学习了如何为不同的操作减少相同的`rdd`。
- en: In the next chapter, we'll be looking at the immutable design of the Spark engine.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看一下Spark引擎的不可变设计。
