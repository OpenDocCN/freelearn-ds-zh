- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Going Further with Apache Spark
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步了解Apache Spark
- en: In the previous chapter, we leveraged open source components to bring time series
    analysis to production. This requires significant effort to set up and manage
    the platform. In this chapter, we will answer such challenges by using Databricks
    as a cloud-based managed **platform-as-a-service** (**PaaS**) solution to go further
    with Apache Spark. We will use an end-to-end example of time series analysis built
    on Databricks using advanced features such as Delta Live Tables with a streaming
    pipeline, AutoML, Unity Catalog, and AI/BI dashboards.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们利用开源组件将时间序列分析投入生产。这需要大量的努力来设置和管理平台。在本章中，我们将通过使用Databricks作为基于云的托管**平台即服务**（**PaaS**）解决方案，进一步利用Apache
    Spark。我们将使用一个基于Databricks构建的端到端时间序列分析示例，采用先进的功能，如Delta Live Tables结合流处理管道、AutoML、Unity
    Catalog和AI/BI仪表盘。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍以下主要内容：
- en: Databricks components and setup
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks组件和设置
- en: Workflows
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作流
- en: Monitoring, security, and governance
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控、安全性和治理
- en: The user interface
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户界面
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will use code examples to explore the deployment of a scalable
    end-to-end solution for time series analysis on Databricks, starting with the
    setup of the environment in the following section.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过代码示例，探索如何在Databricks上部署一个可扩展的端到端时间序列分析解决方案，从下一节的环境设置开始。
- en: 'The code for this chapter is at this URL:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于以下网址：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch10](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch10)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch10](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch10)'
- en: Databricks components and setup
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Databricks组件和设置
- en: We will be using a Databricks environment, as in [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151),
    for the platform infrastructure. Follow the instructions in the *Technical requirements*
    section of [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151) on setting up the Databricks
    environment.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Databricks环境，如在[*第8章*](B18568_08.xhtml#_idTextAnchor151)中所述，作为平台基础设施。请按照[*第8章*](B18568_08.xhtml#_idTextAnchor151)中的*技术要求*部分的说明来设置Databricks环境。
- en: Workspace, folders, and notebooks
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作区、文件夹和笔记本
- en: 'Once the environment is set up, follow the instructions in the links provided
    here to import the notebooks:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 环境设置完成后，请按照此处提供的链接中的说明导入笔记本：
- en: 'Navigate the Databricks workspace: [https://docs.databricks.com/en/workspace/index.html](https://docs.databricks.com/en/workspace/index.html)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 浏览Databricks工作区：[https://docs.databricks.com/en/workspace/index.html](https://docs.databricks.com/en/workspace/index.html)
- en: 'Create a folder named `ts_spark` and a sub-folder named `ch10`: [https://docs.databricks.com/en/workspace/workspace-objects.html#folders](https://docs.databricks.com/en/workspace/workspace-objects.html#folders)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`ts_spark`的文件夹，以及一个名为`ch10`的子文件夹：[https://docs.databricks.com/en/workspace/workspace-objects.html#folders](https://docs.databricks.com/en/workspace/workspace-objects.html#folders)
- en: 'Import the notebooks for this example into the `ch10` folder: [https://docs.databricks.com/en/notebooks/notebook-export-import.html#import-a-notebook](https://docs.databricks.com/en/notebooks/notebook-export-import.html#import-a-notebook)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将本示例的笔记本导入到`ch10`文件夹中：[https://docs.databricks.com/en/notebooks/notebook-export-import.html#import-a-notebook](https://docs.databricks.com/en/notebooks/notebook-export-import.html#import-a-notebook)
- en: 'There are eight notebooks in all, and they can be imported from the following
    URLs:'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总共有八个笔记本，它们可以从以下网址导入：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_dlt_features.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_dlt_features.dbc)'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_dlt_features.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_dlt_features.dbc)'
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_evaluate_forecast.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_evaluate_forecast.dbc)'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_evaluate_forecast.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_evaluate_forecast.dbc)'
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_generate_forecast.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_generate_forecast.dbc)'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_generate_forecast.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_generate_forecast.dbc)'
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_model_training.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_model_training.dbc)'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_model_training.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_model_training.dbc)'
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_model_training_automl.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_model_training_automl.dbc)'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_model_training_automl.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_model_training_automl.dbc)'
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_reset.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_reset.dbc)'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_reset.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_reset.dbc)'
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_update_data.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_update_data.dbc)'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_update_data.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_update_data.dbc)'
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_update_model.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_update_model.dbc)'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_update_model.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_update_model.dbc)'
- en: With the notebooks imported, we can next set up the clusters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 导入笔记本后，我们可以接下来设置集群。
- en: Clusters
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群
- en: We can use the Databricks **Machine Learning Runtime** (**MLR**) or serverless
    compute for the clusters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为集群使用 Databricks **机器学习运行时**（**MLR**）或无服务器计算。
- en: The MLR cluster comes preloaded with common libraries used for **machine learning**
    (**ML**). It instantiates virtual machines in your cloud provider account. You
    will be charged for the virtual machine by the cloud provider. Choose a small
    instance with minimal CPU and memory to minimize this cost when creating the cluster.
    This will be sufficient for the example in this chapter. Refer to the *Technical
    requirements* section of [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151) on setting
    up the Databricks cluster.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MLR 集群预加载了用于**机器学习**（**ML**）的常见库。它在你的云提供商账户中实例化虚拟机。云提供商将向你收取虚拟机的费用。在创建集群时，选择一个小型实例，配备最小的
    CPU 和内存，以最小化此费用。对于本章的示例，这样就足够了。请参阅[*第 8 章*](B18568_08.xhtml#_idTextAnchor151)中关于设置
    Databricks 集群的*技术要求*部分。
- en: The MLR cluster has libraries required for the AutoML example, which we will
    cover in a later section. You can skip the code execution for this example if
    you do not want to incur the MLR-related cloud provider cost for the associated
    virtual machines. We will provide an alternative workflow that does not use AutoML.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: MLR 集群具有 AutoML 示例所需的库，我们将在后续部分中讲解。如果你不想为相关虚拟机产生 MLR 相关的云提供商费用，可以跳过该示例的代码执行。我们将提供一个不使用
    AutoML 的替代工作流。
- en: Note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of writing, the cloud provider cost for these virtual machines is
    above the free allowance that you get with the free cloud provider trial account.
    This means that you will have to upgrade to a paid cloud provider account and
    the cost will be charged to the credit card you specified when creating the account.
    The virtual machines and cloud infrastructure costs are not incurred from the
    free Databricks trial account.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，这些虚拟机的云提供商费用已超出免费云提供商试用账户提供的免费额度。这意味着你需要升级到付费云提供商账户，费用将通过你在创建账户时指定的信用卡进行扣费。虚拟机和云基础设施的费用并不包含在免费的
    Databricks 试用账户内。
- en: The serverless cluster is included in your Databricks cost as the underlying
    virtual machines are fully managed by Databricks. This means it does not incur
    separate cloud provider costs. Serverless clusters do, however, require the installation
    of ML libraries at the time of writing, as you will see in the code example. Databricks
    may provide serverless clusters with pre-loaded ML libraries in the future.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器集群已包含在您的Databricks费用中，因为底层虚拟机由Databricks完全管理。这意味着它不会产生额外的云服务商费用。然而，无服务器集群在写作时需要安装ML库，正如您在代码示例中所看到的。未来，Databricks可能会提供预加载ML库的无服务器集群。
- en: Note
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Databricks has started to include serverless in the free trial account at the
    time of writing. This means that your use of serverless clusters to execute the
    code in this chapter will be free if within the time and cost limit of the Databricks
    free trial account. This may be subject to change in the future.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks已开始在写作时将无服务器功能包含在免费试用账户中。这意味着如果在Databricks免费试用账户的时间和费用限制内，您使用无服务器集群执行本章代码将是免费的。未来此政策可能会有所变化。
- en: 'You can find more information on MLR and serverless clusters in the following
    resources:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下资源中找到更多关于MLR和无服务器集群的信息：
- en: '[https://docs.databricks.com/en/machine-learning/databricks-runtime-ml.html](https://docs.databricks.com/en/machine-learning/databricks-runtime-ml.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/en/machine-learning/databricks-runtime-ml.html](https://docs.databricks.com/en/machine-learning/databricks-runtime-ml.html)'
- en: '[https://docs.databricks.com/en/compute/serverless/index.html](https://docs.databricks.com/en/compute/serverless/index.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/en/compute/serverless/index.html](https://docs.databricks.com/en/compute/serverless/index.html)'
- en: With the clusters covered, we will next configure the data pipeline using Delta
    Live Tables.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在涵盖集群之后，我们将接下来使用Delta Live Tables配置数据管道。
- en: Streaming with Delta Live Tables
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Delta Live Tables进行流处理
- en: 'Databricks **Delta Live Tables** (**DLT**) is a low-code declarative solution
    to build data pipelines. In our example, we will use DLT for the feature engineering
    pipeline, getting data from the source files, checking the data quality, and transforming
    it into features that can be used to train the time series model. You can find
    more information on DLT at the following link:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks **Delta Live Tables**（**DLT**）是一个低代码声明式的数据管道构建解决方案。在我们的示例中，我们将使用DLT构建特征工程管道，从源文件获取数据，检查数据质量，并将其转换为可以用于训练时间序列模型的特征。您可以在以下链接中找到更多关于DLT的信息：
- en: '[https://www.databricks.com/discover/pages/getting-started-with-delta-live-tables](https://www.databricks.com/discover/pages/getting-started-with-delta-live-tables)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.databricks.com/discover/pages/getting-started-with-delta-live-tables](https://www.databricks.com/discover/pages/getting-started-with-delta-live-tables)'
- en: We will go into the details of the DLT configuration in the *Implementing* *workflows*
    section.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*实施* *工作流*部分深入探讨DLT配置的细节。
- en: Workflows
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作流
- en: 'Databricks workflows are the equivalent of the Airflow DAGs that we used in
    [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087) and [*Chapter 9*](B18568_09.xhtml#_idTextAnchor169).
    You can find more information on workflows, also referred to as **jobs**, at the
    following link:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks工作流相当于我们在[*第4章*](B18568_04.xhtml#_idTextAnchor087)和[*第9章*](B18568_09.xhtml#_idTextAnchor169)中使用的Airflow
    DAGs。您可以在以下链接中找到更多关于工作流，也称为**任务**的信息：
- en: '[https://docs.databricks.com/en/jobs/index.html](https://docs.databricks.com/en/jobs/index.html)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/en/jobs/index.html](https://docs.databricks.com/en/jobs/index.html)'
- en: We will now go into the details of *jobs* configuration.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将深入探讨*任务*配置的细节。
- en: Implementing workflows
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施工作流
- en: The code example in this chapter includes four workflows. These are implemented
    as jobs in Databricks. The best way to visualize the jobs is from the **Workflows**
    > **Jobs** > **Tasks** views in Databricks, as per *Figures 10.1*, *10.2*, *10.3*,
    and *10.4*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码示例包括四个工作流。这些工作流在Databricks中作为任务实现。查看任务的最佳方式是通过Databricks中的**工作流** > **任务**
    > **任务**视图，参考*图10.1*、*10.2*、*10.3*和*10.4*。
- en: 'The jobs are as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 任务如下：
- en: '`ts-spark_ch10_1a_ingest_and_train` – This job is for data ingestion, feature
    engineering, and model training, and is shown in *Figure 10**.1*. It includes
    the following tasks:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ts-spark_ch10_1a_ingest_and_train` – 该任务用于数据摄取、特征工程和模型训练，显示在*图10.1*中。它包括以下任务：'
- en: '`reset`'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reset`'
- en: '`dlt_features`'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dlt_features`'
- en: '`model_training`'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_training`'
- en: '![](img/B18568_10_01.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_10_01.jpg)'
- en: 'Figure 10.1: Data ingestion, feature engineering, and model training job'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：数据摄取、特征工程和模型训练任务
- en: '`ts-spark_ch10_1b_ingest_and_train_automl` – The second job, shown in *Figure
    10**.2*, is another version of the first job, with the difference being the use
    of AutoML, which will be explained in the *Training with* *AutoML* section.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ts-spark_ch10_1b_ingest_and_train_automl` – 第二个作业，如*图 10**.2*所示，是第一个作业的另一个版本，区别在于使用了AutoML，具体内容将在*使用AutoML训练*部分进行解释。'
- en: '![](img/B18568_10_02.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_10_02.jpg)'
- en: 'Figure 10.2: Data ingestion, feature engineering, and model training (AutoML)
    job'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2：数据摄取、特征工程和模型训练（AutoML）作业
- en: '`ts-spark_ch10_2b_ingest_and_forecast` – This job ingests new data, retrains
    the model, and generates and evaluates forecasts, and is shown in *Figure 10**.3*.
    It includes the following tasks.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ts-spark_ch10_2b_ingest_and_forecast` – 该作业用于摄取新数据、重新训练模型，并生成和评估预测，如*图 10**.3*所示。它包括以下任务。'
- en: '`dlt_features`'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dlt_features`'
- en: '`update_model`'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update_model`'
- en: '`generate_forecast`'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generate_forecast`'
- en: '`update_data`'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update_data`'
- en: '`evaluate_forecast`'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evaluate_forecast`'
- en: '![](img/B18568_10_03.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_10_03.jpg)'
- en: 'Figure 10.3: Ingest new data, retrain model, and generate forecasts job'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3：摄取新数据、重新训练模型并生成预测作业
- en: '`ts-spark_ch10_2a_update_iteration` – This job, shown in *Figure 10**.4*, calls
    the preceding one multiple times to ingest new data. It simulates what happens
    in the real world whereby the previous end-to-end workflow is launched at a regular
    interval, say, daily or weekly, with new data.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ts-spark_ch10_2a_update_iteration` – 如*图 10**.4*所示，该作业多次调用前一个作业以摄取新数据。它模拟了现实世界中的情境，即以固定时间间隔（例如每日或每周）启动前一个端到端工作流，并处理新数据。'
- en: '![](img/B18568_10_04.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_10_04.jpg)'
- en: 'Figure 10.4: Multiple calls to ingest and process new data job'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4：多次调用摄取和处理新数据作业
- en: Modularity and task separation
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化和任务分离
- en: As in [*Chapter 9*](B18568_09.xhtml#_idTextAnchor169), we have broken the jobs
    into multiple tasks to illustrate the best practice of modularization. This facilitates
    independent code change, scaling, and task reruns. The ownership of the tasks
    can be with different teams. The jobs in this example can be further split in
    your own implementation, depending on your requirement to launch the tasks separately.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与[*第9章*](B18568_09.xhtml#_idTextAnchor169)一样，我们将作业拆分为多个任务，以展示模块化的最佳实践。这有助于独立修改代码、扩展和任务重跑。任务的所有权可以由不同的团队负责。根据您的需求，您可以进一步拆分这些作业，以便分别启动任务。
- en: We will explain each of these jobs and related tasks in detail in the upcoming
    sections, starting with the ingestion and training job.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中详细解释每个作业和相关任务，从摄取和训练作业开始。
- en: 'To set up the jobs required for this chapter, follow the instructions on creating
    jobs and configuring tasks at these links:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置本章所需的作业，请按照以下链接中的说明创建作业并配置任务：
- en: '[https://docs.databricks.com/en/jobs/configure-job.html#create-a-new-job](https://docs.databricks.com/en/jobs/configure-job.html#create-a-new-job)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/en/jobs/configure-job.html#create-a-new-job](https://docs.databricks.com/en/jobs/configure-job.html#create-a-new-job)'
- en: '[https://docs.databricks.com/en/jobs/configure-task.html#configure-and-edit-databricks-tasks](https://docs.databricks.com/en/jobs/configure-task.html#configure-and-edit-databricks-tasks)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/en/jobs/configure-task.html#configure-and-edit-databricks-tasks](https://docs.databricks.com/en/jobs/configure-task.html#configure-and-edit-databricks-tasks)'
- en: Refer to the tables in the upcoming sections for the configuration when creating
    the jobs and related tasks, replacing `<USER_LOGIN>` with your own Databricks
    user login.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考接下来的章节中的表格，查看创建作业和相关任务时的配置，并将`<USER_LOGIN>`替换为您自己的Databricks用户登录。
- en: Ingest and train
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摄取和训练
- en: The `ts-spark_ch10_1a_ingest_and_train` job, shown in *Figure 10**.1*, will
    be detailed in this section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`ts-spark_ch10_1a_ingest_and_train`作业，如*图 10**.1*所示，将在本节中详细介绍。'
- en: '*Table 10.1* shows the configuration for the `ts_spark_ch10_1a_ingest_and_train`
    job to use when following the instructions in the previously provided URL. Note
    that for simplicity, we have given each task the same name as the code notebook
    or pipeline that it runs.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 10.1* 显示了`ts_spark_ch10_1a_ingest_and_train`作业的配置，您可以在之前提供的URL中按照说明使用。请注意，为简便起见，我们给每个任务起了与其运行的代码笔记本或管道相同的名称。'
- en: '| **Job** | `ts_spark_ch10_1a_ingest_and_train` |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| **作业** | `ts_spark_ch10_1a_ingest_and_train` |'
- en: '| **Task 1** | Task name | `ts_spark_ch10_reset` |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| **任务 1** | 任务名称 | `ts_spark_ch10_reset` |'
- en: '|  | Type | Notebook |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 笔记本 |'
- en: '|  | Source | Workspace |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | 来源 | 工作空间 |'
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_reset`
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | 路径（笔记本） | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_reset`
    |'
- en: '|  | Compute | Serverless |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | 计算 | 无服务器 |'
- en: '| **Task 2** | Task name | `ts_spark_ch10_dlt_features` |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| **任务 2** | 任务名称 | `ts_spark_ch10_dlt_features` |'
- en: '|  | Type | Pipeline |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 管道 |'
- en: '|  | Pipeline | `ts_spark_ch10_dlt_features` |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 管道 | `ts_spark_ch10_dlt_features` |'
- en: '|  | Trigger a full refresh on the pipeline | R |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 触发管道的完整刷新 | R |'
- en: '|  | Depends on | `ts_spark_ch10_reset` |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | 依赖 | `ts_spark_ch10_reset` |'
- en: '| **Task 3** | Task name | `ts_spark_ch10_model_training` |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **任务 3** | 任务名称 | `ts_spark_ch10_model_training` |'
- en: '|  | Type | Notebook |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 笔记本 |'
- en: '|  | Source | Workspace |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 来源 | 工作区 |'
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_model_training`
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | 路径（笔记本） | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_model_training`
    |'
- en: '|  | Compute | Serverless |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | 计算 | 无服务器 |'
- en: '|  | Depends on | `ts_spark_ch10_dlt_features` |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | 依赖 | `ts_spark_ch10_dlt_features` |'
- en: 'Table 10.1: Job configuration – ts_spark_ch10_1a_ingest_and_train'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.1：作业配置 – ts_spark_ch10_1a_ingest_and_train
- en: reset
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: reset
- en: 'The `reset` task does the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`reset` 任务执行以下操作：'
- en: Resets the Databricks catalog, `ts_spark`, which is used for this example
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重置 Databricks 目录 `ts_spark`，该目录用于本示例
- en: Downloads the data files from the GitHub location for this chapter to the volumes
    created in the `ts_spark` catalog
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 GitHub 下载本章节的数据文件到 `ts_spark` 目录下创建的卷
- en: The code for this task is in the `ts_spark_ch10_reset` notebook.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此任务的代码位于 `ts_spark_ch10_reset` 笔记本中。
- en: Catalog and volume
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 目录和卷
- en: 'Databricks’ Unity Catalog provides data governance and management in Databricks.
    It organizes data into a three-level hierarchy: catalogs, schemas (equivalent
    to databases), and tables, views, or volumes. Tabular data is stored in tables
    and views, while files are stored in volumes. In our code example, we are using
    a separate catalog, `ts_spark`, and volumes to store the data files.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 的 Unity Catalog 提供数据治理和管理功能。它将数据组织成三级层次结构：目录、模式（相当于数据库）以及表、视图或卷。表格数据存储在表和视图中，而文件存储在卷中。在我们的代码示例中，我们使用一个单独的目录
    `ts_spark` 和卷来存储数据文件。
- en: dlt_features
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: dlt_features
- en: This task is used for data ingestion and feature engineering. It is implemented
    as the `ts_spark_ch10_dlt_features` DLT pipeline, as shown in *Figure 10**.5*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此任务用于数据摄取和特征工程。它实现为 `ts_spark_ch10_dlt_features` DLT 管道，如*图 10.5*所示。
- en: '![](img/B18568_10_05.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_10_05.jpg)'
- en: 'Figure 10.5: Feature engineering pipeline'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5：特征工程管道
- en: 'You can find and zoom in on a digital version of *Figure 10**.5* here: [https://packt.link/D9OXb](https://packt.link/D9OXb)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里查看并放大*图 10.5*的数字版本：[https://packt.link/D9OXb](https://packt.link/D9OXb)
- en: 'To set up the DLT pipeline required for this chapter, follow the instructions
    on creating the pipeline at the following link:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置本章节所需的 DLT 管道，请按照以下链接中的指示创建管道：
- en: '[https://docs.databricks.com/en/delta-live-tables/configure-pipeline.html#configure-a-new-delta-live-tables-pipeline](https://docs.databricks.com/en/delta-live-tables/configure-pipeline.html#configure-a-new-delta-live-tables-pipeline)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/en/delta-live-tables/configure-pipeline.html#configure-a-new-delta-live-tables-pipeline](https://docs.databricks.com/en/delta-live-tables/configure-pipeline.html#configure-a-new-delta-live-tables-pipeline)'
- en: 'Note that you will have to create the `ts_spark` catalog before you can set
    up the DLT pipeline. Refer to the following instructions to create the `ts_spark`
    catalog via the Catalog Explorer: [https://docs.databricks.com/aws/en/catalogs/create-catalog?language=Catalog%C2%A0Explorer](https://docs.databricks.com/aws/en/catalogs/create-catalog?language=Catalog%C2%A0Explorer)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您需要先创建 `ts_spark` 目录，才能设置 DLT 管道。请参阅以下说明，通过目录浏览器创建 `ts_spark` 目录：[https://docs.databricks.com/aws/en/catalogs/create-catalog?language=Catalog%C2%A0Explorer](https://docs.databricks.com/aws/en/catalogs/create-catalog?language=Catalog%C2%A0Explorer)
- en: Refer to *Table 10.2* for the configuration when creating the pipeline, replacing
    `<USER_LOGIN>` with your own Databricks user login.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 创建管道时，请参考*表 10.2*中的配置，并将 `<USER_LOGIN>` 替换为您自己的 Databricks 用户登录。
- en: '| **Pipeline** | `ts_spark_ch10_dlt_features` |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| **管道** | `ts_spark_ch10_dlt_features` |'
- en: '| **General** | Pipeline name | `ts_spark_ch10_dlt_features` |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| **通用** | 管道名称 | `ts_spark_ch10_dlt_features` |'
- en: '|  | Serverless | R |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | 无服务器 | R |'
- en: '|  | Pipeline mode | Triggered |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | 管道模式 | 触发式 |'
- en: '| **Source code** | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_dlt_features`
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **源代码** | 路径（笔记本） | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_dlt_features`
    |'
- en: '| **Destination** | Storage options | `Unity Catalog` |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **目标** | 存储选项 | `Unity Catalog` |'
- en: '|  | Default catalog / Default schema | `ts_spark /` `ch10` |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | 默认目录 / 默认模式 | `ts_spark /` `ch10` |'
- en: 'Table 10.2: DLT configuration – ts_spark_ch10_dlt_features'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.2：DLT 配置 – ts_spark_ch10_dlt_features
- en: 'The code for this pipeline task is in the `ts_spark_ch10_dlt_features` notebook.
    It has the following steps:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道任务的代码位于`ts_spark_ch10_dlt_features`笔记本中，包含以下步骤：
- en: Read historical data from files in the `vol01_hist` volume using Auto Loader,
    check the data, and store the data in the `raw_hist_power_consumption` streaming
    table.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Auto Loader从`vol01_hist`卷中的文件读取历史数据，检查数据，并将数据存储在`raw_hist_power_consumption`流表中。
- en: Auto Loader
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Auto Loader
- en: 'Databricks Auto Loader, also referred to as `cloudfiles` in code, enables the
    efficient incremental ingestion of new data files as they arrive in a cloud storage
    location. You can find more information on Auto Loader at the following link:
    [https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html](https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks Auto Loader，也称为代码中的`cloudfiles`，可以高效地增量导入到达云存储位置的新数据文件。你可以通过以下链接了解更多关于Auto
    Loader的信息：[https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html](https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html)。
- en: Data quality checks
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量检查
- en: 'Databricks DLT can include data quality checks to ensure data integrity based
    on quality constraints within data pipelines. You can find more information on
    data quality checks in DLT at the following link: [https://docs.databricks.com/en/delta-live-tables/expectations.html](https://docs.databricks.com/en/delta-live-tables/expectations.html).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks DLT可以包括数据质量检查，以确保数据管道中的数据完整性，基于质量约束进行校验。你可以通过以下链接了解更多关于DLT中数据质量检查的信息：[https://docs.databricks.com/en/delta-live-tables/expectations.html](https://docs.databricks.com/en/delta-live-tables/expectations.html)。
- en: Read update data from files in the `vol01_upd` volume using Auto Loader, check
    the data, and store the data in the `raw_upd_power_consumption` streaming table.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Auto Loader从`vol01_upd`卷中的文件读取更新数据，检查数据，并将数据存储在`raw_upd_power_consumption`流表中。
- en: Read raw historical data from the `raw_hist_power_consumption` streaming table,
    transform the data, and store the result in the `curated_hist_power_consumption`
    streaming table.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`raw_hist_power_consumption`流表中读取原始历史数据，转换数据，并将结果存储在`curated_hist_power_consumption`流表中。
- en: Read raw update data from the `raw_upd_power_consumption` streaming table, transform
    the data, and store the result in the `curated_upd_power_consumption` streaming
    table.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`raw_upd_power_consumption`流表中读取原始更新数据，转换数据，并将结果存储在`curated_upd_power_consumption`流表中。
- en: Append data from the `curated_hist_power_consumption` and `curated_upd_power_consumption`
    streaming tables, storing the combined result in the `curated_all_power_consumption`
    streaming table.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`curated_hist_power_consumption`和`curated_upd_power_consumption`流表中追加数据，并将合并结果存储在`curated_all_power_consumption`流表中。
- en: Read curated data from the `curated_all_power_consumption` streaming table,
    use Tempo to calculate the `features_aggr_power_consumption` materialized view.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`curated_all_power_consumption`流表中读取整理后的数据，使用Tempo计算`features_aggr_power_consumption`物化视图。
- en: Tempo
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Tempo
- en: 'Databricks Tempo is an open source project simplifying time series data manipulation
    within Apache Spark. You can find more information on Tempo at the following link:
    [https://databrickslabs.github.io/tempo/](https://databrickslabs.github.io/tempo/).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks Tempo是一个开源项目，简化了在Apache Spark中处理时间序列数据。你可以通过以下链接了解更多关于Tempo的信息：[https://databrickslabs.github.io/tempo/](https://databrickslabs.github.io/tempo/)。
- en: Read aggregated data from the `features_aggr_power_consumption` materialized
    view, and use Tempo to do an `AsOf` join with the `curated_all_power_consumption`
    streaming table. Then, store the result in the `features_gnlr_power_consumption`
    materialized view.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`features_aggr_power_consumption`物化视图中读取汇总数据，使用Tempo与`curated_all_power_consumption`流表进行`AsOf`连接。然后，将结果存储在`features_gnlr_power_consumption`物化视图中。
- en: These steps correspond to the data transformation stages of the medallion approach,
    which was discussed in the *Data processing and storage* section of [*Chapter
    4*](B18568_04.xhtml#_idTextAnchor087).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤对应于“金银铜”方法中的数据转换阶段，这部分在[*第4章*](B18568_04.xhtml#_idTextAnchor087)的*数据处理和存储*章节中进行了讨论。
- en: model_training
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: model_training
- en: 'This task is used to train a Prophet model using the features calculated in
    the prior `dlt_features` task. The code for `model_training` is in the `ts_spark_ch10_model_training`
    notebook. The steps are as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此任务用于训练一个Prophet模型，使用在之前`dlt_features`任务中计算的特征。`model_training`的代码位于`ts_spark_ch10_model_training`笔记本中。步骤如下：
- en: Read the features from `features_aggr_power_consumption`.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`features_aggr_power_consumption`读取特征。
- en: Rename the `Date` column to `ds` and `hourly_Global_active_power` to `y`. These
    column names are required by Prophet.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Date`列重命名为`ds`，并将`hourly_Global_active_power`重命名为`y`。这些列名是 Prophet 所要求的。
- en: Start an MLflow run to track the training in MLflow.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动一个 MLflow 运行以追踪训练过程。
- en: Fit the Prophet model to the dataset.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Prophet 模型拟合到数据集。
- en: Register the model to Unity Catalog, setting the alias as `Champion`.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型注册到 Unity Catalog，并将别名设置为`Champion`。
- en: Note that this notebook shows a simplified model training, which is sufficient
    to illustrate the training step in the example in this chapter. It does not include
    the full model experimentation process and hyperparameter tuning, which we covered
    in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个笔记本展示的是简化的模型训练，足以说明本章示例中的训练步骤。它没有包括完整的模型实验过程和超参数调整，这部分内容在[*第 7 章*](B18568_07.xhtml#_idTextAnchor133)中有所讨论。
- en: Training with AutoML
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AutoML 进行训练
- en: Another approach to model training is to use Databricks AutoML to find the best
    model for the given dataset.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种模型训练方法是使用 Databricks AutoML 来为给定的数据集找到最佳模型。
- en: AutoML is a feature within Databricks that automates the process of developing
    ML models. It has tasks such as data profiling, feature engineering, model selection,
    and hyperparameter tuning. This enables users to quickly generate baseline models
    for regression, classification, and forecasting problems. With its “glass box”
    approach, AutoML provides the underlying code for each model, which differs from
    “black box” approaches that do not show the code details. AutoML can be used from
    the UI, as shown in *Figure 10**.6*, or programmatically, as in the example provided
    in this chapter.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML 是 Databricks 中的一项功能，自动化了机器学习模型开发的过程。它包含数据分析、特征工程、模型选择和超参数调整等任务。此功能使用户能够快速生成回归、分类和预测问题的基线模型。通过其“玻璃盒”方法，AutoML
    提供每个模型的底层代码，这与不显示代码细节的“黑盒”方法不同。AutoML 可以通过 UI 使用，如*图 10.6*所示，也可以通过编程方式使用，如本章提供的示例所示。
- en: '![](img/B18568_10_06.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_10_06.jpg)'
- en: 'Figure 10.6: Databricks AutoML'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6：Databricks AutoML
- en: 'You can find more information on AutoML here:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此处找到有关 AutoML 的更多信息：
- en: '[https://www.databricks.com/product/automl](https://www.databricks.com/product/automl).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.databricks.com/product/automl](https://www.databricks.com/product/automl)。'
- en: 'The `ts-spark_ch10_1b_ingest_and_train_automl` job is an example of how to
    include AutoML programmatically in the training task. The code for this task is
    in the `ts_spark_ch10_model_training_automl` notebook. The steps are as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`ts-spark_ch10_1b_ingest_and_train_automl`作业是如何在训练任务中以编程方式包括 AutoML 的示例。此任务的代码位于`ts_spark_ch10_model_training_automl`笔记本中。步骤如下：'
- en: Read the features from `features_aggr_power_consumption`.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`features_aggr_power_consumption`读取特征。
- en: Call the `databricks.automl.forecast` function, which takes care of renaming
    the columns, starting an MLflow run to track the training, and finding the best
    model for forecasting based on the specified `primary_metric` (`mdape` is used
    in the example).
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`databricks.automl.forecast`函数，它负责重命名列、启动一个 MLflow 运行以追踪训练过程，并根据指定的`primary_metric`（示例中使用的是`mdape`）找到最佳的预测模型。
- en: Register the model to Unity Catalog, setting the alias as `Champion`.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型注册到 Unity Catalog，并将别名设置为`Champion`。
- en: The configuration for the `ts_spark_ch10_1b_ingest_and_train_automl` job is
    shown in *Table 10.3*.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`ts_spark_ch10_1b_ingest_and_train_automl`作业的配置见*表 10.3*。'
- en: '| **Job (optional)** | `ts_spark_ch10_1b_ingest_and_train_automl` |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| **任务（可选）** | `ts_spark_ch10_1b_ingest_and_train_automl` |'
- en: '| --- | --- |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Task 1** | Task name | `ts_spark_ch10_reset` |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| **任务 1** | 任务名称 | `ts_spark_ch10_reset` |'
- en: '|  | Type | Notebook |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 笔记本 |'
- en: '|  | Source | Workspace |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | 来源 | 工作空间 |'
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_reset`
    |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | 路径（笔记本） | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_reset`
    |'
- en: '|  | Compute | Serverless |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | 计算 | 无服务器 |'
- en: '| **Task 2** | Task name | `ts_spark_ch10_dlt_features` |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| **任务 2** | 任务名称 | `ts_spark_ch10_dlt_features` |'
- en: '|  | Type | Pipeline |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 管道 |'
- en: '|  | Pipeline | `ts_spark_ch10_dlt_features` |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | 管道 | `ts_spark_ch10_dlt_features` |'
- en: '|  | Trigger a full refresh on the pipeline | R |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | 触发管道的完整刷新 | R |'
- en: '|  | Depends on | `ts_spark_ch10_reset` |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | 依赖于 | `ts_spark_ch10_reset` |'
- en: '| **Task 3** | Task name | `ts_spark_ch10_model_training_automl` |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| **任务 3** | 任务名称 | `ts_spark_ch10_model_training_automl` |'
- en: '|  | Type | Notebook |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 笔记本 |'
- en: '|  | Source | Workspace |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | 来源 | 工作空间 |'
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_model_training_automl`
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | 路径（笔记本） | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_model_training_automl`
    |'
- en: '|  | Compute | *Refer to the earlier section on clusters to choose the compute
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | 计算 | *参考之前关于集群的部分以选择计算类型* |'
- en: '|  | Depends on | `ts_spark_ch10_dlt_features` |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | 依赖于 | `ts_spark_ch10_dlt_features` |'
- en: 'Table 10.3: Job configuration – ts_spark_ch10_1b_ingest_and_train_automl'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.3：作业配置 - ts_spark_ch10_1b_ingest_and_train_automl
- en: Note that in addition to simplifying the steps compared to the previous training
    approach without AutoML, we also get to find the best model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了简化与之前不使用 AutoML 的训练方法步骤之外，我们还可以找到最佳模型。
- en: Ingest and forecast
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入和预测
- en: The `ts-spark_ch10_2b_ingest_and_forecast` job, shown in *Figure 10**.3*, will
    be detailed in this section.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`ts-spark_ch10_2b_ingest_and_forecast` 作业，如 *图 10.3* 所示，将在本节中详细介绍。'
- en: The configuration for the `ts_spark_ch10_2b_ingest_and_forecast` job is shown
    in *Table 10.4*.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`ts_spark_ch10_2b_ingest_and_forecast` 作业的配置如 *表 10.4* 所示。'
- en: '| **Job** | `ts_spark_ch10_2b_ingest_and_forecast` |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| **作业** | `ts_spark_ch10_2b_ingest_and_forecast` |'
- en: '| --- | --- |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|  | Job parameters | Key: `upd_iter`Value: `1` |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | 作业参数 | 键：`upd_iter` 值：`1` |'
- en: '| **Task 1** | Task name | `ts_spark_ch10_dlt_features` |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **任务 1** | 任务名称 | `ts_spark_ch10_dlt_features` |'
- en: '|  | Type | Pipeline |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 管道 |'
- en: '|  | Pipeline | `ts_spark_ch10_dlt_features` |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | 管道 | `ts_spark_ch10_dlt_features` |'
- en: '|  | Trigger a full refresh on the pipeline | R |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | 触发管道的完全刷新 | R |'
- en: '| **Task 2** | Task name | `ts_spark_ch10_update_model` |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| **任务 2** | 任务名称 | `ts_spark_ch10_update_model` |'
- en: '|  | Type | Notebook |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 笔记本 |'
- en: '|  | Source | Workspace |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | 来源 | 工作区 |'
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_update_model`
    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | 路径（笔记本） | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_update_model`
    |'
- en: '|  | Compute | Serverless |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | 计算 | 无服务器 |'
- en: '|  | Depends on | `ts_spark_ch10_dlt_features` |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | 依赖于 | `ts_spark_ch10_dlt_features` |'
- en: '| **Task 3** | Task name | `ts_spark_ch10_generate_forecast` |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| **任务 3** | 任务名称 | `ts_spark_ch10_generate_forecast` |'
- en: '|  | Type | Notebook |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 笔记本 |'
- en: '|  | Source | Workspace |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | 来源 | 工作区 |'
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_generate_forecast`
    |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | 路径（笔记本） | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_generate_forecast`
    |'
- en: '|  | Compute | Serverless |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | 计算 | 无服务器 |'
- en: '|  | Depends on | `ts_spark_ch10_update_model` |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | 依赖于 | `ts_spark_ch10_update_model` |'
- en: '| **Task 4** | Task name | `ts_spark_ch10_update_data` |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| **任务 4** | 任务名称 | `ts_spark_ch10_update_data` |'
- en: '|  | Type | Notebook |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 笔记本 |'
- en: '|  | Source | Workspace |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | 来源 | 工作区 |'
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_update_data`
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | 路径（笔记本） | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_update_data`
    |'
- en: '|  | Compute | Serverless |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | 计算 | 无服务器 |'
- en: '|  | Depends on | `ts_spark_ch10_generate_forecast` |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  | 依赖于 | `ts_spark_ch10_generate_forecast` |'
- en: '| **Task 5** | Task name | `ts_spark_ch10_evaluate_forecast` |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| **任务 5** | 任务名称 | `ts_spark_ch10_evaluate_forecast` |'
- en: '|  | Type | Notebook |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 笔记本 |'
- en: '|  | Source | Workspace |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | 来源 | 工作区 |'
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_
    evaluate_forecast` |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | 路径（笔记本） | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_evaluate_forecast`
    |'
- en: '|  | Compute | Serverless |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | 计算 | 无服务器 |'
- en: '|  | Depends on | `ts_spark_ch10_update_data` |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | 依赖于 | `ts_spark_ch10_update_data` |'
- en: 'Table 10.4: Job configuration – ts_spark_ch10_2b_ingest_and_forecast'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.4：作业配置 - ts_spark_ch10_2b_ingest_and_forecast
- en: dlt_features
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: dlt_features
- en: This task is the same `ts_spark_ch10_dlt_features` DLT pipeline, shown in *Figure
    10**.5*, as used in the earlier *Ingest and train* section, where it was used
    to process historical data. The difference is here we will be calling this pipeline
    to process new data files from the `vol01_upd` volume.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 该任务与前面 *导入和训练* 部分中使用的 `ts_spark_ch10_dlt_features` DLT 管道相同，如 *图 10.5* 所示，只不过这次我们将调用该管道来处理来自
    `vol01_upd` 卷的新数据文件。
- en: update_model
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: update_model
- en: 'This task is used to train a Prophet model using the features calculated in
    the prior `dlt_features` task. The code for `update_model` is in the `ts_spark_ch10_update_model`
    notebook. This task is similar to the task discussed in the *model_training* section,
    the difference being that we now have new data to include in the training. The
    steps are as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 该任务用于使用先前 `dlt_features` 任务中计算的特征来训练 Prophet 模型。`update_model` 的代码位于 `ts_spark_ch10_update_model`
    笔记本中。该任务与 *模型训练* 部分中讨论的任务类似，唯一的区别是我们现在有了新数据来包含在训练中。步骤如下：
- en: Read the features from `features_aggr_power_consumption`.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `features_aggr_power_consumption` 中读取特征。
- en: Rename the `Date` column to `ds` and `hourly_Global_active_power` to `y`. These
    column names are required by Prophet.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `Date` 列重命名为 `ds`，并将 `hourly_Global_active_power` 列重命名为 `y`。这些列名是 Prophet
    所要求的。
- en: Fit the Prophet model to the dataset.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Prophet 模型拟合到数据集。
- en: Register the model to Unity Catalog, setting the alias as `Champion`.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型注册到Unity Catalog，并设置别名为`Champion`。
- en: With the latest model updated, we can use it to forecast next.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 更新完最新模型后，我们可以使用它进行下一步的预测。
- en: generate_forecast
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: generate_forecast
- en: 'This task uses the previously trained model to generate and store forecasts.
    The code for `generate_forecast` is in the `ts_spark_ch10_generate_forecast` notebook.
    The steps are as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 该任务使用之前训练好的模型来生成和存储预测结果。`generate_forecast`的代码位于`ts_spark_ch10_generate_forecast`笔记本中。步骤如下：
- en: Load the `Champion` model from Unity Catalog.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Unity Catalog加载`Champion`模型。
- en: Generate a forecast for the next 24 hours.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为接下来的24小时生成预测。
- en: Store the forecasts, together with the model’s name and version, in the `forecast`
    table.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测结果与模型的名称和版本一起存储在`forecast`表中。
- en: With the forecast generated, we can compare the forecasted time period against
    the actuals, which we will get next.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成预测后，我们可以将预测的时间段与实际数据进行比较，实际数据将在接下来获取。
- en: update_data
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: update_data
- en: This task simply copies the data file for the new time period from the `vol01_upd_src`
    volume to `vol01_upd`. The code for `update_data` is in the `ts_spark_ch10_update_data`
    notebook.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 该任务只是将新的时间段的数据文件从`vol01_upd_src`卷复制到`vol01_upd`。`update_data`的代码位于`ts_spark_ch10_update_data`笔记本中。
- en: evaluate_forecast
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: evaluate_forecast
- en: 'This task calculates and stores the forecasting accuracy metrics. The code
    for `evaluate_forecast` is in the `ts_spark_ch10_evaluate_forecast` notebook.
    The steps are as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 该任务计算并存储预测准确度指标。`evaluate_forecast`的代码位于`ts_spark_ch10_evaluate_forecast`笔记本中。步骤如下：
- en: Join the `features_aggr_power_consumption` actuals table to the previously created
    `forecast` table.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`features_aggr_power_consumption`实际数据表与之前创建的`forecast`表连接。
- en: Calculate the `mdape` metrics.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`mdape`指标。
- en: Store the calculated metrics with the model’s name and version in the `forecast_metrics`
    table.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将计算得到的指标与模型的名称和版本一起存储在`forecast_metrics`表中。
- en: Store the data quality check results in the `dq_results` table.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据质量检查结果存储在`dq_results`表中。
- en: With the forecast evaluated, we can report on the outcome and metrics. We will
    cover this in the *User interface* section. Before we get to this, let’s detail
    how we will orchestrate the multiple iterations of new data arrival and corresponding
    processing.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估完预测后，我们可以报告结果和指标。我们将在*用户界面*部分介绍这一部分内容。在进入这部分之前，先详细说明如何协调多个新数据到达并进行相应处理的迭代过程。
- en: Updating iterations
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新迭代
- en: The `ts-spark_ch10_2a_update_iteration` job, shown in *Figure 10**.4*, simulates
    what happens in the real world whereby we have new data to process at a regular
    interval, say, daily or weekly. It calls the `ts-spark_ch10_2b_ingest_and_forecast`
    job seven times, corresponding to one week of daily new data. Every call results
    in the end-to-end processing of a new data file, as described in the previous
    *Ingest and* *forecast* section.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`ts-spark_ch10_2a_update_iteration`作业，如*图10.4*所示，模拟了现实中我们在定期时间间隔（如每天或每周）处理新数据的情况。它调用`ts-spark_ch10_2b_ingest_and_forecast`作业七次，对应一周的每日新数据。每次调用都会触发一个新的数据文件的端到端处理，如前面的*获取和预测*部分所描述。'
- en: The configuration for the `ts_spark_ch10_2a_update_iterations` job is shown
    in *Table 10.5*.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`ts_spark_ch10_2a_update_iterations`作业的配置见*表10.5*。'
- en: '| **Job** | `ts_spark_ch10_2a_update_iterations` |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| **作业** | `ts_spark_ch10_2a_update_iterations` |'
- en: '| --- | --- |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Task 1** | Task name | `ts_spark_ch10_update_iterations` |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| **任务1** | 任务名称 | `ts_spark_ch10_update_iterations` |'
- en: '|  | Type | For each |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 每个 |'
- en: '|  | Inputs | [1,2,3,4,5,6,7] |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入 | [1,2,3,4,5,6,7] |'
- en: '| **Task 2 (Add a task to** **loop over)** | Task name | `ts_spark_ch10_update_iterations_iteration`
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| **任务2（添加一个任务以** **循环遍历）** | 任务名称 | `ts_spark_ch10_update_iterations_iteration`
    |'
- en: '|  | Type | Run Job |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 运行作业 |'
- en: '|  | Job | `ts_spark_ch10_2b_ingest_and_forecast` |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | 作业 | `ts_spark_ch10_2b_ingest_and_forecast` |'
- en: '|  | Job Parameters | Key: `upd_iter`Value: `{{input}}` |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | 作业参数 | 键：`upd_iter` 值：`{{input}}` |'
- en: 'Table 10.5: Job configuration – ts_spark_ch10_2a_update_iterations'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.5：作业配置 – ts_spark_ch10_2a_update_iterations
- en: Starting the jobs
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动作业
- en: 'With the jobs configured and explained, we will now start these jobs, which
    will execute the code for this chapter. You can find more information on running
    jobs here:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 配置并解释完作业后，我们将启动这些作业，这些作业将执行本章的代码。有关运行作业的更多信息，请参见：
- en: '[https://docs.databricks.com/en/jobs/run-now.html](https://docs.databricks.com/en/jobs/run-now.html)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/en/jobs/run-now.html](https://docs.databricks.com/en/jobs/run-now.html)'
- en: 'Proceed in the following order:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下顺序进行操作：
- en: Click on `ts-spark_ch10_1a_ingest_and_train`. Wait for the job to complete.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`ts-spark_ch10_1a_ingest_and_train`。等待任务完成。
- en: Click on `ts-spark_ch10_2a_update_iteration`.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`ts-spark_ch10_2a_update_iteration`。
- en: With the jobs launched and executed, we can review their status, as will be
    explained in the next section.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动并执行作业后，我们可以查看它们的状态，下一节将对此进行详细解释。
- en: Monitoring, security, and governance
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控、安全和治理
- en: As we discussed in [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087) in the *From
    DataOps to ModelOps to DevOps* section and in [*Chapter 9*](B18568_09.xhtml#_idTextAnchor169)
    in the *Governance and security* section, a key requirement for workloads in a
    production environment and with sensitive data is to have proper monitoring, security,
    and governance in place. This is greatly facilitated by leveraging the built-in
    functionalities of a managed platform such as Databricks with Unity Catalog. The
    alternative approach, if we were to develop and test our own custom-built platform,
    requires considerable time and effort to robustly meet these requirements.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第4章*](B18568_04.xhtml#_idTextAnchor087)的*从DataOps到ModelOps再到DevOps*部分和[*第9章*](B18568_09.xhtml#_idTextAnchor169)的*治理与安全*部分中讨论的那样，生产环境和涉及敏感数据的工作负载的关键要求是必须具备适当的监控、安全和治理。这通过利用像Databricks与Unity
    Catalog这样的托管平台的内置功能得到极大的促进。如果我们开发和测试自己定制的平台，替代方法将需要相当多的时间和精力，才能稳健地满足这些要求。
- en: Monitoring
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控
- en: The monitoring of the jobs can be done from the `ts-spark_ch10_2b_ingest_and_forecast`
    job. We can see the different runs, their parameters, durations, and statuses,
    among other information useful for monitoring.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过`ts-spark_ch10_2b_ingest_and_forecast`任务来进行作业监控。我们可以查看不同的运行、它们的参数、持续时间、状态等信息，这些对于监控非常有用。
- en: '![](img/B18568_10_07.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_10_07.jpg)'
- en: 'Figure 10.7: Databricks Workflows – Jobs – Runs'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7：Databricks Workflows – 作业 – 运行
- en: The monitoring of the `ts_spark_ch10_dlt_features` DLT pipeline can be done
    from the **Workflows** > **Pipelines** page, as shown in *Figure 10**.8*. We can
    see the different stages, data checks, durations, and statuses, among other information
    useful for monitoring.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`ts_spark_ch10_dlt_features` DLT管道的监控可以通过**Workflows** > **Pipelines**页面完成，如*图10.8*所示。我们可以看到不同的阶段、数据检查、持续时间和状态等信息，这些对于监控非常有用。'
- en: '![](img/B18568_10_08.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_10_08.jpg)'
- en: 'Figure 10.8: Databricks DLT pipelines'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8：Databricks DLT管道
- en: 'You can find more information on observability, monitoring, and alerting here:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此处找到更多关于可观察性、监控和警报的信息：
- en: '[https://docs.databricks.com/en/delta-live-tables/observability.html](https://docs.databricks.com/en/delta-live-tables/observability.html)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/en/delta-live-tables/observability.html](https://docs.databricks.com/en/delta-live-tables/observability.html)'
- en: '[https://www.databricks.com/blog/lakehouse-monitoring-unified-solution-quality-data-and-ai](https://www.databricks.com/blog/lakehouse-monitoring-unified-solution-quality-data-and-ai)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.databricks.com/blog/lakehouse-monitoring-unified-solution-quality-data-and-ai](https://www.databricks.com/blog/lakehouse-monitoring-unified-solution-quality-data-and-ai)'
- en: '[https://docs.databricks.com/en/lakehouse-monitoring/index.html](https://docs.databricks.com/en/lakehouse-monitoring/index.html)'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/en/lakehouse-monitoring/index.html](https://docs.databricks.com/en/lakehouse-monitoring/index.html)'
- en: '[https://docs.databricks.com/aws/en/lakehouse-monitoring/monitor-alerts](https://docs.databricks.com/aws/en/lakehouse-monitoring/monitor-alerts)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/aws/en/lakehouse-monitoring/monitor-alerts](https://docs.databricks.com/aws/en/lakehouse-monitoring/monitor-alerts)'
- en: Security
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全
- en: As shown in *Figure 10**.9*, setting access permissions to tables and other
    objects requires just a few clicks when using Unity Catalog.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.9*所示，使用Unity Catalog设置表格及其他对象的访问权限只需要几次点击。
- en: '![](img/B18568_10_09.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_10_09.jpg)'
- en: 'Figure 10.9: Databricks Unity Catalog – setting permission'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：Databricks Unity Catalog – 设置权限
- en: 'It is also possible to define fine-grained access control at a more granular
    row or column level within tables, as per the following resources:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以根据以下资源在表格内定义更加细粒度的访问控制，细化到行或列级别：
- en: '[https://www.databricks.com/resources/demos/videos/governance/access-controls-with-unity-catalog](https://www.databricks.com/resources/demos/videos/governance/access-controls-with-unity-catalog)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.databricks.com/resources/demos/videos/governance/access-controls-with-unity-catalog](https://www.databricks.com/resources/demos/videos/governance/access-controls-with-unity-catalog)'
- en: 'You can find more information on security here:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此处找到更多关于安全的信息：
- en: '[https://docs.databricks.com/en/security/index.html](https://docs.databricks.com/en/security/index.html)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/en/security/index.html](https://docs.databricks.com/en/security/index.html)'
- en: Governance
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 治理
- en: An important consideration for governance is being able to track the lineage
    of data assets, as shown in *Figure 10**.10*. We can see here the source of the
    data, the multiple intermediate stages, and the final tables where the data is
    stored. Unity Catalog tracks this automatically in Databricks to give us real-time
    visibility into the data flows.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 治理的重要考虑因素之一是能够追踪数据资产的血缘关系，如*图 10.10*所示。我们可以看到数据的来源、多个中间阶段，以及数据存储的最终表格。Unity
    Catalog 会在 Databricks 中自动跟踪这一过程，让我们能够实时监控数据流。
- en: '![](img/B18568_10_10.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_10_10.jpg)'
- en: 'Figure 10.10: Databricks Unity Catalog – lineage view'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10：Databricks Unity Catalog – 血缘视图
- en: 'You can find and zoom in on a digital version of *Figure* *10**.10* here:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到并放大*图 10.10*的数字版本：
- en: https://packt.link/D6DyC
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: https://packt.link/D6DyC
- en: 'We have touched only briefly on governance and security with Databricks Unity
    Catalog. You can find more information here:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅简要提及了使用 Databricks Unity Catalog 进行治理和安全性。你可以在这里找到更多信息：
- en: '[https://www.databricks.com/product/unity-catalog](https://www.databricks.com/product/unity-catalog)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.databricks.com/product/unity-catalog](https://www.databricks.com/product/unity-catalog)'
- en: With an understanding of how to leverage a platform such as Databricks for monitoring,
    security, and governance, we will next uncover how to present the outcome of time
    series analysis.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 了解如何利用 Databricks 这样的平台进行监控、安全性和治理后，我们将继续揭示如何展示时间序列分析的结果。
- en: Databricks UI — AI/BI dashboards
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Databricks 用户界面 — AI/BI 仪表板
- en: When it comes to presenting the outcome of the time series analysis we have
    done so far, Databricks provides a few options for the user interface with AI/BI
    dashboards, Genie spaces, AI-based chatbots, and Lakehouse Apps. We will cover
    AI/BI dashboards in this section and the other options in the next chapter.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示我们迄今为止进行的时间序列分析结果时，Databricks 提供了多种用户界面的选项，包括 AI/BI 仪表板、Genie 空间、基于 AI 的聊天机器人和
    Lakehouse 应用。我们将在本节中介绍 AI/BI 仪表板，其它选项将在下一章中讨论。
- en: We have been using various graphs extensively throughout this book to represent
    data and the outcome of analysis. This has required us to execute code in notebooks
    to create the graphs. This works well when we are able to write code and have
    an execution environment. When this is not the case, a common way to present the
    data and the outcome of the analysis is with a reporting dashboard. This is possible
    with Databricks AI/BI dashboards, as shown in *Figure 10**.11*.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们广泛使用了各种图表来表示数据和分析结果。这要求我们在笔记本中执行代码来创建图表。当我们能够编写代码并拥有执行环境时，这种方式非常有效。然而，在无法编写代码的情况下，常见的展示数据和分析结果的方式是使用报告仪表板。Databricks
    AI/BI 仪表板便提供了这种功能，如*图 10.11*所示。
- en: The Databricks AI/BI dashboard is a solution integrated into the Databricks
    platform to create reports and dashboards. It has AI-powered capabilities to assist
    with the creation of queries and data visualizations. The dashboards can be published
    and shared for consumption.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks AI/BI 仪表板是一个集成到 Databricks 平台中的解决方案，用于创建报告和仪表板。它具备 AI 驱动的功能，帮助生成查询和数据可视化。仪表板可以发布并共享，供他人使用。
- en: '![](img/B18568_10_11.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_10_11.jpg)'
- en: 'Figure 10.11: Databricks AI/BI dashboard'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11：Databricks AI/BI 仪表板
- en: 'To install this dashboard in your own environment, first, download it from
    the following location:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 要在自己的环境中安装此仪表板，首先，下载它并从以下位置获取：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/blob/main/ch10/ts_spark_ch10.lvdash.json](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/blob/main/ch10/ts_spark_ch10.lvdash.json)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/blob/main/ch10/ts_spark_ch10.lvdash.json](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/blob/main/ch10/ts_spark_ch10.lvdash.json)'
- en: 'The dashboard file can then be imported into your own environment by following
    the instructions here:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以按照这里的说明将仪表板文件导入到自己的环境中：
- en: '[https://docs.databricks.com/en/dashboards/index.html#import-a-dashboard-file](https://docs.databricks.com/en/dashboards/index.html#import-a-dashboard-file)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/en/dashboards/index.html#import-a-dashboard-file](https://docs.databricks.com/en/dashboards/index.html#import-a-dashboard-file)'
- en: Note
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You will need a SQL Warehouse in order to run the dashboard. Refer to the following
    instructions to create a SQL Warehouse:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个 SQL 仓库来运行仪表板。请参考以下说明来创建 SQL 仓库：
- en: '[https://docs.databricks.com/aws/en/compute/sql-warehouse/create](https://docs.databricks.com/aws/en/compute/sql-warehouse/create).'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/aws/en/compute/sql-warehouse/create](https://docs.databricks.com/aws/en/compute/sql-warehouse/create)'
- en: 'In this dashboard, we have brought together, in a combined view, the following:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个仪表板中，我们将以下内容整合在一个视图中：
- en: Graph of the actual and forecasted values
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际值与预测值的图表
- en: Number of records that failed and passed the data quality checks
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过数据质量检查的记录数（失败和通过）
- en: Metrics for different model versions
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同模型版本的指标
- en: 'You can find more information on AI/BI dashboards at the following links:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下链接找到更多关于AI/BI仪表板的信息：
- en: '[https://www.databricks.com/blog/introducing-aibi-intelligent-analytics-real-world-data](https://www.databricks.com/blog/introducing-aibi-intelligent-analytics-real-world-data)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.databricks.com/blog/introducing-aibi-intelligent-analytics-real-world-data](https://www.databricks.com/blog/introducing-aibi-intelligent-analytics-real-world-data)'
- en: '[https://docs.databricks.com/en/dashboards/index.html](https://docs.databricks.com/en/dashboards/index.html)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/en/dashboards/index.html](https://docs.databricks.com/en/dashboards/index.html)'
- en: Summary
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: With the end-to-end example of time series analysis on a managed Spark platform,
    this chapter has shown how to leverage the out-of-the-box features of Databricks
    to go further with Apache Spark. We have gone from data ingestion with a streaming
    pipeline to feature engineering and model training and to inferencing and reporting
    while ensuring that monitoring, security, and governance are in place. By combining
    pre-built features on Databricks with our own custom code, we were able to implement
    a solution that can be extended to further use cases.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在托管的Spark平台上进行时间序列分析的端到端示例，本章展示了如何利用Databricks的开箱即用功能进一步推动Apache Spark的应用。我们从通过流处理管道进行数据摄取开始，到特征工程和模型训练，再到推理和报告，同时确保监控、安全性和治理得到了落实。通过将Databricks上预构建的功能与我们自己的自定义代码相结合，我们实现了一个可以扩展到更多使用场景的解决方案。
- en: This brings us to our last chapter, where we will expand on some of the recent
    developments in time series analysis.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引导我们进入最后一章，在本章中，我们将扩展一些近期在时间序列分析中的发展。
- en: Join our community on Discord
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/ds](https://packt.link/ds)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/ds](https://packt.link/ds)'
- en: '![](img/ds_(1).jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ds_(1).jpg)'
