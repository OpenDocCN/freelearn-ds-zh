- en: Chapter 7.  Extending Spark with SparkR
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。使用SparkR扩展Spark
- en: Statisticians and data scientists have been using R to solve challenging problems
    in almost every field, ranging from bioinformatics to election campaigns. They
    prefer R due to its powerful visualization capabilities, strong community, and
    rich package ecosystem for statistics and machine learning. Many academic institutions
    around the world teach data science and statistics using the R language.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学家和数据科学家一直在几乎所有领域使用R解决具有挑战性的问题，从生物信息学到选举活动。他们喜欢R是因为它具有强大的可视化能力、强大的社区以及丰富的统计和机器学习包生态系统。世界各地的许多学术机构使用R语言教授数据科学和统计学。
- en: R was originally created by and for statisticians in around the mid-1990s with
    a goal to deliver a better and more user-friendly way to perform data analysis.
    R was initially used in academics and research. As businesses became increasingly
    aware of the role of data science in their business growth, the number of data
    analysts using R in the corporate sector started growing as well. The R language
    user base is considered to be more than two million strong, after being in existence
    for two decades.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: R最初是在1990年代中期由统计学家创建的，目标是提供更好、更用户友好的数据分析方式。R最初用于学术和研究。随着企业越来越意识到数据科学在业务增长中的作用，企业部门使用R的数据分析师数量也在增长。在存在了20年后，R语言用户基数被认为超过200万。
- en: One of the driving factors behind all this success is the fact that R is designed
    to make the life of the analyst easier but not that of the computer. R is inherently
    single-threaded and it can only process datasets that completely fit in a single
    machine's memory. But nowadays, R users are working with increasingly larger datasets.
    Seamless integration of modern-day distributed processing power underneath the
    well-established R language allows data scientists to leverage the best of both
    worlds. They can keep up with their ever-increasing business demands and continue
    to benefit from the flexibility of their favorite R language.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这一切成功背后的推动因素之一是，R旨在使分析师的生活更轻松，而不是计算机的生活。R本质上是单线程的，它只能处理完全适合单台计算机内存的数据集。但如今，R用户正在处理越来越大的数据集。现代分布式处理能力与成熟的R语言的无缝集成，使数据科学家能够充分利用两者的优势。他们可以满足不断增长的业务需求，并继续从他们喜爱的R语言的灵活性中受益。
- en: 'This chapter introduces SparkR, an R API to Spark for R programmers so that
    they can harness the power of Spark, without learning a new language. Since prior
    knowledge of R, R Studio, and data analysis skills are already assumed, this chapter
    does not attempt to introduce R. A very brief overview of the Spark compute engine
    is provided as a quick recap. The reader should go through the first three chapters
    of this book to gain a deeper understanding of the Spark programming model and
    DataFrames. This knowledge is extremely important because the developer has to
    understand which part of his code is executing in the local R environment and
    which part is being handled by the Spark compute engine. The topics covered in
    this chapter are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了SparkR，这是一个面向R程序员的R API，使他们可以利用Spark的强大功能，而无需学习一种新语言。由于已经假定具有R、R Studio和数据分析技能的先验知识，因此本章不试图介绍R。提供了Spark计算引擎的非常简要的概述作为快速回顾。读者应该阅读本书的前三章，以更深入地了解Spark编程模型和DataFrames。这些知识非常重要，因为开发人员必须了解他的代码的哪一部分在本地R环境中执行，哪一部分由Spark计算引擎处理。本章涵盖的主题如下：
- en: SparkR basics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkR基础知识
- en: Advantages of R with Spark and its limitations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R与Spark的优势及其局限性
- en: Programming with SparkR
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SparkR进行编程
- en: SparkR DataFrames
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkR DataFrames
- en: Machine learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习
- en: SparkR basics
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkR基础知识
- en: R is a language and environment for statistical computing and graphics. SparkR
    is an R package that provides a lightweight frontend to enable Apache Spark access
    from R. The goal of SparkR is to combine the flexibility and ease of use provided
    by the R environment and the scalability and fault tolerance provided by the Spark
    compute engine. Let us recap the Spark architecture before discussing how SparkR
    realizes its goal.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: R是一种用于统计计算和图形的语言和环境。SparkR是一个R包，提供了一个轻量级的前端，以便从R中访问Apache Spark。SparkR的目标是结合R环境提供的灵活性和易用性，以及Spark计算引擎提供的可伸缩性和容错性。在讨论SparkR如何实现其目标之前，让我们回顾一下Spark的架构。
- en: Apache Spark is a fast, general-purpose, fault-tolerant framework for interactive
    and iterative computations on large, distributed datasets. It supports a wide
    variety of data sources as well as storage layers. It provides unified data access
    to combine different data formats, streaming data and defining complex operations
    using high-level, composable operators. You can develop your applications interactively
    using Scala, Python, or R shell (or Java without a shell). You can deploy it on
    your home desktop or you can run it on large clusters of thousands of nodes crunching
    petabytes of data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个快速、通用、容错的框架，用于大规模分布式数据集的交互式和迭代计算。它支持各种数据源和存储层。它提供统一的数据访问，可以结合不同的数据格式、流数据，并使用高级、可组合的操作定义复杂的操作。您可以使用Scala、Python或R
    shell（或没有shell的Java）交互式地开发应用程序。您可以将其部署在家用台式机上，也可以在成千上万个节点的大型集群上运行，处理PB级数据。
- en: Note
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: SparkR originated in the AMPLab ([https://amplab.cs.berkeley.edu/](https://amplab.cs.berkeley.edu/))
    to explore different techniques to integrate the usability of R with the scalability
    of Spark. It was released as an alpha component in Apache Spark 1.4, which was
    released in June 2015\. The Spark 1.5 release had improved R usability and introduced
    the MLlib machine learning package with **Generalized Linear Models** (**GLMs**).
    The Spark 1.6 release that happened in January 2016 added some more features,
    such as model summary and feature interactions. The Spark 2.0 release that happened
    in July 2016 brought several important features, such as UDF, improved model coverage,
    DataFrames Window functions API, and so on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR起源于AMPLab（[https://amplab.cs.berkeley.edu/](https://amplab.cs.berkeley.edu/)），旨在探索将R的易用性与Spark的可伸缩性相结合的不同技术。它作为Apache
    Spark 1.4中的一个alpha组件发布，该版本于2015年6月发布。Spark 1.5版本改进了R的可用性，并引入了带有**广义线性模型**（**GLMs**）的MLlib机器学习包。2016年1月发布的Spark
    1.6版本增加了一些功能，例如模型摘要和特征交互。2016年7月发布的Spark 2.0版本带来了一些重要功能，例如UDF，改进的模型覆盖范围，DataFrames窗口函数API等。
- en: Accessing SparkR from the R environment
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从R环境访问SparkR
- en: 'You can start SparkR from R shell or R Studio. The entry point to SparkR is
    the SparkSession object, which represents the connection to the Spark cluster.
    The node on which R is running becomes the driver. Any objects created by the
    R program reside on this driver. Any objects created via SparkSession are created
    on the worker nodes in the cluster. The following diagram depicts the runtime
    view of R interaction with Spark running on a cluster. Note that R interpreter
    exists on every worker node in the cluster. The following figure does not show
    the cluster manager and it does not show the storage layer either. You could use
    any cluster manager (for example, Yarn or Mesos) and any storage option, such
    as HDFS, Cassandra, or Amazon S3:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从R shell或R Studio启动SparkR。SparkR的入口点是SparkSession对象，它表示与Spark集群的连接。运行R的节点成为驱动程序。由R程序创建的任何对象都驻留在此驱动程序上。通过SparkSession创建的任何对象都将创建在集群中的工作节点上。以下图表描述了R与运行在集群上的Spark的运行时视图。请注意，R解释器存在于集群中的每个工作节点上。以下图表不显示集群管理器，也不显示存储层。您可以使用任何集群管理器（例如Yarn或Mesos）和任何存储选项，例如HDFS、Cassandra或Amazon
    S3：
- en: '![Accessing SparkR from the R environment](img/image_07_001.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![从R环境访问SparkR](img/image_07_001.jpg)'
- en: 'Source: http://www.slideshare.net/Hadoop_Summit/w-145p210-avenkataraman.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：http://www.slideshare.net/Hadoop_Summit/w-145p210-avenkataraman。
- en: A SparkSession object is created by passing information such as application
    name, memory, number of cores, and the cluster manager to connect to. Any interaction
    with the Spark engine is initiated via this SparkSession object. A SparkSession
    object is already created for you if you use SparkR shell. You have to explicitly
    create it otherwise. This object replaces SparkContext and SQLContext objects
    that existed in Spark 1.x releases. These objects still exist for backward compatibility.
    Even the preceding figure depicts SparkContext, which you should treat as SparkSession
    post Spark 2.0.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递应用程序名称、内存、核心数和要连接的集群管理器等信息来创建SparkSession对象。与Spark引擎的任何交互都是通过此SparkSession对象启动的。如果使用SparkR
    shell，则已为您创建了SparkSession对象。否则，您必须显式创建它。此对象替换了Spark 1.x版本中存在的SparkContext和SQLContext对象。这些对象仍然存在以确保向后兼容性。即使前面的图表显示了SparkContext，您也应该将其视为Spark
    2.0之后的SparkSession。
- en: Now that we have understood how to access Spark from the R environment, let
    us examine the core data abstractions provided by the Spark engine.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何从R环境访问Spark，让我们来看看Spark引擎提供的核心数据抽象。
- en: RDDs and DataFrames
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD和DataFrames
- en: At the core of the Spark engine is its main data abstraction, called a **Resilient
    Distributed Dataset** (**RDD**). An RDD is composed of one or more data sources
    and is defined by the user as a series of transformations (aka lineage) on one
    or more stable (concrete) data sources. Every RDD or RDD partition knows how to
    recreate itself on failure using the lineage graph, thereby providing fault tolerance.
    RDD is an immutable data structure, implying that it is sharable between threads
    without synchronization overheads and hence amenable for parallelization. Operations
    on RDDs are either transformations or actions. Transformations are individual
    steps in the lineage. In other words, they are operations that create RDDs because
    every transformation is getting data from a stable data source or transforming
    an immutable RDD and creating another RDD. Transformations are simply declarations;
    they are not evaluated until an *action* operation is applied on that RDD. Actions
    are the operations that utilize the RDDs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark引擎的核心是其主要数据抽象，称为**弹性分布式数据集**（**RDD**）。RDD由一个或多个数据源组成，并由用户定义为对一个或多个稳定（具体）数据源的一系列转换（也称为血统）。每个RDD或RDD分区都知道如何使用血统图在失败时重新创建自己，从而提供容错性。RDD是不可变的数据结构，这意味着它可以在线程之间共享而无需同步开销，因此适合并行化。RDD上的操作要么是转换，要么是动作。转换是血统中的单个步骤。换句话说，它们是创建RDD的操作，因为每个转换都是从稳定数据源获取数据或转换不可变RDD并创建另一个RDD。转换只是声明；直到对该RDD应用*动作*操作之前，它们才会被评估。动作是利用RDD的操作。
- en: Spark optimizes RDD computation based on the action on hand. For example, if
    the action is to read the first line, only one partition is computed, skipping
    the rest. It automatically performs in-memory computation with graceful degradation
    (spills it to disk when memory is insufficient) and distributes processing across
    all the cores. You may cache an RDD if it is frequently accessed in your program
    logic, thereby avoiding recomputing overhead.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark根据手头的动作优化RDD计算。例如，如果动作是读取第一行，只计算一个分区，跳过其余部分。它会自动执行内存计算，并在内存不足时将其溢出到磁盘，并在所有核心上分布处理。如果在程序逻辑中频繁访问RDD，则可以对其进行缓存，从而避免重新计算开销。
- en: The R language provides a two-dimensional data structure called a *DataFrame*
    which makes data manipulation convenient. Apache Spark comes with its own DataFrames
    that are inspired by the DataFrame in R and Python (through Pandas). A Spark DataFrame
    is a specialized data structure that is built on top of the RDD data structure
    abstraction. It provides distributed DataFrame implementation that looks very
    similar to R DataFrame from the developer perspective and at the same time can
    support very large datasets. The Spark dataset API adds structure to DataFrames
    and this structure provides information for more optimization under the hood.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: R语言提供了一个称为*DataFrame*的二维数据结构，使数据操作变得方便。Apache Spark带有自己的DataFrames，受到了R和Python（通过Pandas）中的DataFrame的启发。Spark
    DataFrame是一种专门的数据结构，建立在RDD数据结构抽象之上。它提供了分布式DataFrame实现，从开发者的角度看，它看起来非常类似于R DataFrame，同时可以支持非常大的数据集。Spark数据集API为DataFrame添加了结构，这种结构在底层提供了更多的优化信息。
- en: Getting started
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入门
- en: 'Now that we have understood the underlying data structures and the runtime
    view, it is time to run a few commands. In this section, we assume that you already
    have R and Spark successfully installed and added to the path. We also assume
    that the `SPARK_HOME` environment variable is set. Let us see how to access SparkR
    from R shell or R Studio:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了底层数据结构和运行时视图，是时候运行一些命令了。在本节中，我们假设您已经成功安装了R和Spark，并将其添加到了路径中。我们还假设`SPARK_HOME`环境变量已设置。让我们看看如何从R
    shell或R Studio访问SparkR：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is all you need to do to access the power of Spark DataFrames from within
    the R environment.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是您从R环境中访问Spark DataFrames的全部内容。
- en: Advantages and limitations
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势和限制
- en: 'The R language has long been the lingua franca of data scientists. Its simple-to-understand
    DataFrame abstraction, expressive APIs, and vibrant package ecosystem are exactly
    what the analysts needed. The main challenge was with the scalability. SparkR
    bridges that gap by providing distributed in-memory DataFrames without leaving
    the R eco-system. Such a symbiotic relationship allows users to gain the following
    benefits:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: R语言长期以来一直是数据科学家的通用语言。它简单易懂的DataFrame抽象、表达丰富的API和充满活力的包生态系统正是分析师所需要的。主要挑战在于可扩展性。SparkR通过提供分布式内存中的DataFrame来弥合这一差距，而不会离开R生态系统。这种共生关系使用户能够获得以下好处：
- en: There is no need for the analyst to learn a new language
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析师无需学习新语言
- en: The SparkR APIs are similar to R APIs
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkR的API与R的API类似
- en: You can access SparkR from R studio, along with the autocomplete feature
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以从R Studio访问SparkR，还可以使用自动完成功能
- en: Performing interactive, exploratory analysis of a very large dataset is no longer
    hindered by memory limitations or long turnaround times
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模数据集的交互式探索性分析不再受内存限制或长时间的等待时间的限制
- en: Accessing data from different types of data sources becomes a lot easier. Most
    of the tasks which were imperative before have become declarative. Check [Chapter
    4](http://Chapter%204), *Unified Data Access*, to learn more
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从不同类型的数据源访问数据变得更加容易。大多数以前必须是命令式的任务现在已经变成了声明式的。查看[第4章](http://Chapter%204)，*统一数据访问*，了解更多信息
- en: You can freely mix dplyr such as Spark functions, SQL, and R libraries that
    are still not available in Spark
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以自由混合dplyr、Spark函数、SQL和仍未在Spark中可用的R库
- en: 'In spite of all the exciting advantages of combining the best of both worlds,
    there are still some limitations with this combination. These limitations may
    not impact every use case, but we need to be aware of them anyway:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结合两者最好的优势令人兴奋，但这种组合仍然存在一些限制。这些限制可能不会影响每种用例，但我们无论如何都需要意识到它们：
- en: The inherent dynamic nature of R limits the information available for the catalyst
    optimizer. We may not get the full advantage of optimizations such as predicate
    pushback when compared to statically typed languages such as Scala.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R的固有动态特性限制了可用于催化剂优化器的信息。与静态类型语言（如Scala）相比，我们可能无法充分利用优化，例如谓词推回。
- en: SparkR does not have support for all the machine learning algorithms that are
    already available in other APIs such as the Scala API.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkR不支持所有已经在其他API（如Scala API）中可用的机器学习算法。
- en: In summary, using Spark for data preprocessing and using R for analysis and
    visualization seems to be the best approach in the near future.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在数据预处理方面使用Spark，而在分析和可视化方面使用R似乎是未来最好的方法。
- en: Programming with SparkR
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SparkR进行编程
- en: 'So far, we have understood the runtime model of SparkR and the basic data abstractions
    that provide the fault tolerance and scalability. We have understood how to access
    the Spark API from R shell or R studio. It''s time to try out some basic and familiar
    operations:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了SparkR的运行时模型和提供容错性和可扩展性的基本数据抽象。我们已经了解了如何从R shell或R Studio访问Spark
    API。现在是时候尝试一些基本和熟悉的操作了：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The operations look very similar to R DataFrame functions because spark DataFrames
    are modeled based on R DataFrames and Python (Pandas) DataFrames. But the similarity
    may create confusion if you are not careful. You may accidentally end up choking
    your local machine by running a compute-intensive function on an R `data.frame`,
    thinking that the load will be distributed. For example, the intersect function
    has the same signature in both packages. You need to pay attention to whether
    the object is of class `SparkDataFrame` (Spark DataFrame) or `data.frame` (R DataFrame).
    You also need to minimize back and forth conversions between local R `data.frame`
    objects and Spark DataFrame objects. Let us get a feel for this distinction by
    trying out some examples:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 操作看起来与R DataFrame函数非常相似，因为Spark DataFrames是基于R DataFrames和Python（Pandas）DataFrames建模的。但是，如果不小心，这种相似性可能会引起混淆。您可能会在R的`data.frame`上运行计算密集型函数，以为负载会被分布，从而意外地使本地机器崩溃。例如，intersect函数在两个包中具有相同的签名。您需要注意对象是`SparkDataFrame`（Spark
    DataFrame）还是`data.frame`（R DataFrame）。您还需要尽量减少本地R `data.frame`对象和Spark DataFrame对象之间的来回转换。让我们通过尝试一些示例来感受这种区别：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Function name masking
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 函数名称屏蔽
- en: 'Now that we have tried some basic operations, let us digress a little bit.
    We have to understand what happens when a loaded library has overlapping function
    names with the base package or some other package that was already loaded. This
    is sometimes referred to as function name overlapping, function masking, or name
    conflict. You might have noticed the messages mentioning the objects masked when
    the SparkR package is loaded. This is common for any package loaded into the R
    environment, and is not specific to SparkR alone. If the R environment already
    contains any function that has the same name as a function in the package being
    loaded, then any subsequent calls to that function exhibit the behavior of the
    function in the latest package loaded. If you want to access the previous function
    instead of the `SparkR` function, you need to explicitly prefix that function
    with its package name, as shown:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经尝试了一些基本操作，让我们稍微偏离一下。我们必须了解当加载的库具有与基本包或已加载的其他包重叠的函数名称时会发生什么。有时这被称为函数名称重叠、函数屏蔽或名称冲突。您可能已经注意到在加载SparkR包时提到了被屏蔽的对象的消息。这对于加载到R环境中的任何包都很常见，不仅仅是SparkR。如果R环境已经包含与要加载的包中的函数同名的函数，那么对该函数的任何后续调用都会表现出最新加载的包中函数的行为。如果您想访问以前的函数而不是`SparkR`函数，您需要显式使用其包名称作为前缀，如下所示：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Subsetting data
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子集数据
- en: 'Subsetting operations on R DataFrames are quite flexible and SparkR tries to
    retain these operations with the same or similar equivalents. We have already
    seen some operations in the preceding examples but this section presents them
    in an ordered fashion:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: R DataFrame上的子集操作非常灵活，SparkR试图保留这些操作或类似的等价操作。我们已经在前面的示例中看到了一些操作，但本节以有序的方式呈现它们：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Tip
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: At the time of writing this book (Apache Spark 2.o release), row index based
    slicing is not available. You will not be able to get a specific row or range
    of rows using the `df[n,]` or `df[m:n,]` syntax.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时（Apache Spark 2.0发布），基于行索引的切片不可用。您将无法使用`df[n,]`或`df[m:n,]`语法获取特定行或行范围。
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Column functions
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列函数
- en: 'You will have already noticed the column functions `between` in the subsetting
    data section. These functions operate on the `Column` class. As the name suggests,
    these functions operate on a single column at a time and are usually used in subsetting
    DataFrames. There are several other handy column functions for common operations
    such as sorting, casting, and formatting. In addition to working on the values
    within a column, you can append columns to a DataFrame or drop one or more columns
    from a DataFrame. Negative column subscripts may be used to omit columns, similar
    to R. The following examples show the use of `Column` class functions in subset
    operations followed by adding and dropping columns:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到在子集数据部分中的列函数`between`。这些函数在`Column`类上操作。正如名称所示，这些函数一次在单个列上操作，并且通常用于子集DataFrame。除了在列内的值上工作之外，您还可以将列附加到DataFrame或从DataFrame中删除一个或多个列。负列下标可以用于省略列，类似于R。以下示例展示了在子集操作中使用`Column`类函数，然后添加和删除列：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Grouped data
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分组数据
- en: 'DataFrame data can be subgrouped using the `group_by` function similar to SQL.
    There are multiple ways of performing such operations. We introduce a slightly
    complex example in this section. Moreover, we use `%>%`, aka the forward pipe
    operator, provided by the `magrittr` library, which provides a mechanism for chaining
    commands:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame数据可以使用`group_by`函数进行分组，类似于SQL。执行此类操作的多种方式。我们在本节中介绍了一个稍微复杂的示例。此外，我们使用了`magrittr`库提供的`%>%`，也称为前向管道运算符，它提供了一个链接命令的机制：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can keep chaining the operations using the forward pipe operator. Look at
    the column renamed part of the code carefully. The column name argument is the
    output of previous operations, which would have completed before commencement
    of this operation and thus you can safely assume that the `avg(sepal_len)` column
    already exists. The `format_number` works as expected, and this is yet another
    handy `Column` operation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以继续使用前向管道运算符链接操作。仔细查看代码中的列重命名部分。列名参数是前面操作的输出，在此操作开始之前已经完成，因此您可以安全地假定`avg(sepal_len)`列已经存在。`format_number`按预期工作，这是另一个方便的`Column`操作。
- en: The next section has another similar example with `GroupedData` and its equivalent
    implementation using `dplyr`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节有一个类似的示例，使用`GroupedData`及其等效的`dplyr`实现。
- en: SparkR DataFrames
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkR DataFrames
- en: 'In this section, we try out some useful, commonly used operations. First, we
    try out the traditional R/`dplyr` operations and then show equivalent operations
    using the SparkR API:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们尝试一些有用的常用操作。首先，我们尝试传统的R/`dplyr`操作，然后展示使用SparkR API的等效操作：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This operation is very similar to the SQL group and is followed by order. Its
    equivalent implementation in SparkR is also very similar to the `dplyr` example.
    Look at the following example. Pay attention to the method names and compare their
    positioning with respect to the preceding `dplyr` example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作与SQL分组非常相似，并且后跟排序。它在SparkR中的等效实现也与`dplyr`示例非常相似。查看以下示例。注意方法名称并将其与前面的`dplyr`示例进行比较：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: SparkR is intended to be as close to the existing R API as possible. So, the
    method names look very similar to `dplyr` methods. For example, look at the example
    which has `groupBy` whereas `dplyr` has `group_by`. SparkR supports redundant
    function names. For example, it has `group_by` as well as `groupBy` to cater to
    developers coming from different programming environments. The method names in
    `dplyr` and SparkR are again very close to the SQL keyword `GROUP BY`. But the
    sequence of these method calls is not the same. The example also showed an additional
    step of converting a Spark DataFrame to an R `data.frame` using `collect`. The
    methods are arranged inside out, in the sense that first the data is grouped,
    then summarized, and then arranged. This is understandable because in SparkR,
    the DataFrame created in the innermost method becomes the argument for its immediate
    predecessor and so on.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR旨在尽可能接近现有的R API。因此，方法名称看起来与`dplyr`方法非常相似。例如，看看具有`groupBy`的示例，而`dplyr`具有`group_by`。SparkR支持冗余函数名称。例如，它有`group_by`以及`groupBy`，以满足来自不同编程环境的开发人员。`dplyr`和SparkR中的方法名称再次非常接近SQL关键字`GROUP
    BY`。但是这些方法调用的顺序不同。示例还显示了将Spark DataFrame转换为R `data.frame`的附加步骤，使用`collect`。这些方法是从内到外排列的，意思是首先对数据进行分组，然后进行汇总，然后进行排列。这是可以理解的，因为在SparkR中，内部方法中创建的DataFrame成为其直接前任的参数，依此类推。
- en: SQL operations
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SQL操作
- en: 'If you are not very happy with the syntax in the preceding example, you may
    want to try writing an SQL string as shown, which does exactly the same as the
    preceding but uses the good old SQL syntax:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对前面示例中的语法不太满意，您可能希望尝试编写一个SQL字符串，如所示，它与前面的示例完全相同，但使用了传统的SQL语法：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding example looks like the most natural way of implementing the operation
    on hand, if you are used to fetching data from RDBMS tables. But how are we doing
    this? The first statement tells Spark to register a temporary table (or, as the
    name suggests, a view, a logical abstraction of a table). This is not exactly
    the same as a database table. It is temporary in the sense that it is destroyed
    when the SparkSession object is destroyed. You are not explicitly writing data
    into any RDBMS datastore (you have to use `SaveAsTable` for that). But when once
    you register a Spark DataFrame as a temporary table, you are free to use SQL syntax
    to operate on that DataFrame. The next statement is a basic `SELECT` statement
    that displays column names followed by five rows, as dictated by the `LIMIT` keyword.
    The next SQL statement created a Spark DataFrame containing a Species column followed
    by two average columns sorted on the average sepal length. This DataFrame is in
    turn collected as an R `data.frame` by using collect. The final result is exactly
    the same as the preceding example. You are free to use either syntax. For more
    information and examples, check out the SQL section in[Chapter 4](http://chapter%204),
    *Unified Data Access*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您习惯从RDBMS表中获取数据，前面的示例看起来像是实现手头操作的最自然方式。但我们是如何做到这一点的呢？第一条语句告诉Spark注册一个临时表（或者，如其名称所示，一个视图，表的逻辑抽象）。这并不完全等同于数据库表。它是临时的，意味着在销毁SparkSession对象时会被销毁。您并没有将数据明确写入任何RDBMS数据存储（您必须使用`SaveAsTable`）。但是一旦您将Spark
    DataFrame注册为临时表，就可以自由地使用SQL语法对该DataFrame进行操作。下一条语句是一个基本的`SELECT`语句，显示列名，然后是五行，由`LIMIT`关键字指定。下一个SQL语句创建了一个包含Species列的Spark
    DataFrame，后跟两个平均列，按平均萼片长度排序。然后，通过使用collect将此DataFrame作为R `data.frame`收集。最终结果与前面的示例完全相同。您可以自由选择使用任何语法。有关更多信息和示例，请查看[第4章](http://chapter%204)中的SQL部分，*统一数据访问*。
- en: Set operations
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集合操作
- en: 'The usual set operations, such as `union`, `intersection`, and `minus`, are
    available out of the box in SparkR. In fact, when SparkR is loaded, the warning
    message shows `intersect` as one of the masked functions. The following examples
    are based on `beaver` datasets:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR中可以直接使用常见的集合操作，如`union`、`intersection`和`minus`。实际上，当加载SparkR时，警告消息显示`intersect`作为其中一个屏蔽函数。以下示例基于`beaver`数据集：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Merging DataFrames
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并DataFrame
- en: 'The next example illustrates the joining of two DataFrames using the `merge`
    command. The first part of the example shows the R implementation and the next
    part shows the SparkR implementation:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例说明了使用`merge`命令连接两个DataFrame。示例的第一部分显示了R的实现，下一部分显示了SparkR的实现：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding piece of code completely relies on R''s base package. We have
    used the same names for join columns in both DataFrames for simplicity. The next
    piece of code demonstrates the same example using SparkR. It looks similar to
    the preceding code so look carefully for the differences:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码完全依赖于R的基本包。为简单起见，我们在两个DataFrame中使用了相同的连接列名称。下一段代码演示了使用SparkR的相同示例。它看起来与前面的代码类似，因此请仔细查看其中的区别：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You may want to play with different types of joins, such as left outer join
    and right outer join, or different column names to get a better understanding
    of this function.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想尝试不同类型的连接，例如左外连接和右外连接，或不同的列名，以更好地理解此函数。
- en: Machine learning
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: SparkR provides wrappers on existing MLLib functions. R formulas are implemented
    as MLLib feature transformers. A transformer is an ML pipeline (`spark.ml`) stage
    that takes a DataFrame as input and produces another DataFrame as output, which
    generally contains some appended columns. Feature transformers are a type of transformers
    that convert input columns to feature vectors and these feature vectors are appended
    to the source DataFrame. For example, in linear regression, string input columns
    are one-hot encoded and numeric values are converted to doubles. A label column
    will be appended (if not there in the data frame already) as a replica of the
    response variable.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR提供了现有MLLib函数的包装器。R公式被实现为MLLib特征转换器。转换器是一个ML管道（`spark.ml`）阶段，它以DataFrame作为输入并产生另一个DataFrame作为输出，通常包含一些附加列。特征转换器是一种将输入列转换为特征向量的转换器，这些特征向量被附加到源DataFrame。例如，在线性回归中，字符串输入列被独热编码，数值被转换为双精度数。标签列将被附加（如果数据框中没有的话）作为响应变量的副本。
- en: In this section, we cover example code for the Naive Bayes and Gaussian GLM
    models. We do not explain the models as such or the summaries they produce. Instead,
    we go straight away to how it can be done using SparkR.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们涵盖了朴素贝叶斯和高斯GLM模型的示例代码。我们不解释模型本身或它们产生的摘要。相反，我们直接讨论如何使用SparkR来完成这些操作。
- en: The Naive Bayes model
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型
- en: The NaÃ¯ve Bayes model is an intuitively simple model that works with categorical
    data. We'll be training a sample dataset using the NaÃ¯ve Bayes model. We will
    not explain how the model works but move straight away to training the model using
    SparkR. If you want more information, please refer to [Chapter 6](ch06.xhtml "Chapter 6. 
    Machine Learning"), *Machine Learning*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型是一个直观简单的模型，适用于分类数据。我们将使用朴素贝叶斯模型训练一个样本数据集。我们不会解释模型的工作原理，而是直接使用SparkR来训练模型。如果您想要更多信息，请参考[第6章](ch06.xhtml
    "第6章 机器学习") *机器学习*。
- en: This example takes a dataset with the average marks and attendance of twenty
    students. In fact, this dataset has already been introduced in [Chapter 6](http://Chapter%206),
    *Machine Learning*, for training ensembles. However, let us revisit its contents.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子使用了一个包含二十名学生的平均分数和出勤情况的数据集。实际上，这个数据集已经在[第6章](http://Chapter%206) *机器学习*中被引入，用于训练集成。然而，让我们重新审视一下它的内容。
- en: 'The students are awarded `Pass` or `Fail` based on a set of well-defined rules.
    Two students with IDs `1009` and `1020` are granted `Pass`, even though they would
    have failed otherwise. Even though we do not provide the actual rules to the model,
    we expect the model to predict these two students'' result as `Fail`. Here are
    the `Pass` / `Fail` criteria:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 学生根据一组明确定义的规则被授予`及格`或`不及格`。两名ID为`1009`和`1020`的学生被授予`及格`，即使在其他情况下他们本来会不及格。尽管我们没有向模型提供实际规则，但我们期望模型预测这两名学生的结果为`不及格`。以下是`及格`/`不及格`的标准：
- en: Marks < 40 => Fail
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分数 < 40 => 不及格
- en: Poor attendance => Fail
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出勤不足 => 不及格
- en: Marks above 40 and attendance Full => Pass
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分数超过40且出勤全 => 及格
- en: 'Marks > 60 and attendance at least Enough => PassThe following is an example
    to train Naive Bayes model:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分数 > 60且至少出勤足够 => 及格以下是训练朴素贝叶斯模型的示例：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The Gaussian GLM model
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯GLM模型
- en: 'In this example, we try to predict temperature based on the values of ozone,
    solar radiation, and wind:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们尝试根据臭氧、太阳辐射和风的值来预测温度：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Summary
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: To date, SparkR does not support all algorithms available in Spark, but active
    development is happening to bridge the gap. The Spark 2.0 release has improved
    algorithm coverage, including NaÃ¯ve Bayes, k-means clustering, and survival regression.
    Check out the latest documentation for the supported algorithms. More work is
    underway in bringing out a CRAN release of SparkR, with better integration with
    R packages and Spark packages, and better RFormula support.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，SparkR不支持Spark中所有可用的算法，但正在积极开发以弥合差距。Spark 2.0版本已经改进了算法覆盖范围，包括朴素贝叶斯、k均值聚类和生存回归。查看最新的支持算法文档。在将来，我们将继续努力推出SparkR的CRAN版本，更好地与R包和Spark包集成，并提供更好的RFormula支持。
- en: References
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '*SparkR: The Past, Present and Future* by *Shivaram Venkataraman: *[http://shivaram.org/talks/sparkr-summit-2015.pdf](http://shivaram.org/talks/sparkr-summit-2015.pdf)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SparkR: 过去、现在和未来* 作者 *Shivaram Venkataraman: *[http://shivaram.org/talks/sparkr-summit-2015.pdf](http://shivaram.org/talks/sparkr-summit-2015.pdf)'
- en: '*Enabling Exploratory Data Science with Spark and R* by *Shivaram Venkataraman*
    and *Hossein Falaki:*[http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r](http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过Spark和R实现探索性数据科学* 作者 *Shivaram Venkataraman* 和 *Hossein Falaki:*[http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r](http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r)'
- en: '*SparkR: Scaling R Programs with Spark* by *Shivaram Venkataraman* and others:
    [http://shivaram.org/publications/sparkr-sigmod.pdf](http://shivaram.org/publications/sparkr-sigmod.pdf)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SparkR: 用Spark扩展R程序* 作者 *Shivaram Venkataraman* 和其他人: [http://shivaram.org/publications/sparkr-sigmod.pdf](http://shivaram.org/publications/sparkr-sigmod.pdf)'
- en: '*Recent Developments in SparkR for Advanced Analytics* by *Xiangrui Meng*:
    [http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf](http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SparkR的最新发展* 作者 *Xiangrui Meng*: [http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf](http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf)'
- en: 'To understand RFormula, try out the following links:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解RFormula，请尝试以下链接：
- en: '[https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html)'
- en: '[http://spark.apache.org/docs/latest/ml-features.html#rformula](http://spark.apache.org/docs/latest/ml-features.html#rformula)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/ml-features.html#rformula](http://spark.apache.org/docs/latest/ml-features.html#rformula)'
