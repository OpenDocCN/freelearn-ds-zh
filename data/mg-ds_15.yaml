- en: Implementing ModelOps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 ModelOps
- en: In this chapter, we will look at ModelOps and its closest cousin—DevOps. We
    will explore how to build development pipelines for data science and make projects
    reliable, experiments reproducible, and deployments fast. To do this, we will
    familiarize ourselves with the general model training pipeline, and see how data
    science projects differ from software projects from the development infrastructure
    perspective. We will see what tools can help to version data, track experiments,
    automate testing, and manage Python environments. Using these tools, you will
    be able to create a complete ModelOps pipeline, which will automate the delivery
    of new model versions, while taking care of reproducibility and code quality.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨 ModelOps 及其最亲近的“表亲”——DevOps。我们将研究如何为数据科学构建开发管道，使项目更加可靠，实验可以复现，部署更加快速。为此，我们将熟悉一般的模型训练管道，看看数据科学项目与软件项目在开发基础设施方面的不同。我们还将看看哪些工具可以帮助版本控制数据、跟踪实验、自动化测试并管理
    Python 环境。通过使用这些工具，你将能够创建一个完整的 ModelOps 管道，自动化新模型版本的交付，同时保证可复现性和代码质量。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding ModelOps
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 ModelOps
- en: Looking into DevOps
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 DevOps
- en: Managing code versions and quality
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理代码版本和质量
- en: Storing data along with code
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据与代码一起存储
- en: Managing environments
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理环境
- en: Tracking experiments
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪实验
- en: The importance of automated testing
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化测试的重要性
- en: Continuous model training
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续模型训练
- en: A power pack for your projects
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的项目提供动力包
- en: Understanding ModelOps
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 ModelOps
- en: 'ModelOps is a set of practices for automating a common set of operations that
    arise in data science projects, which include the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ModelOps 是一组用于自动化数据科学项目中常见操作的实践，包括以下内容：
- en: Model training pipeline
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练管道
- en: Data management
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管理
- en: Version control
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本控制
- en: Experiment tracking
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验跟踪
- en: Testing
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试
- en: Deployment
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署
- en: Without ModelOps, teams are forced to waste time on those repetitive tasks.
    Each task in itself is fairly easy to handle, but a project can suffer from mistakes
    in those steps. ModelOps helps us to create project delivery pipelines that work
    like a precise conveyor belt with automated testing procedures that try to catch
    coding errors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 没有 ModelOps，团队将不得不在这些重复任务上浪费时间。每个任务本身相对容易处理，但项目可能会因为这些步骤中的错误而遭受影响。ModelOps 帮助我们创建像精密传送带一样运作的项目交付管道，并通过自动化测试程序来捕捉代码错误。
- en: Let's start by discussing ModelOps' closest cousin—DevOps.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从讨论 ModelOps 的最亲近“表亲”——DevOps 开始。
- en: Looking into DevOps
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 DevOps
- en: '**DevOps** stands for **development operations**. Software development processes
    include many repetitive and error-prone tasks that should be performed each time
    software makes a journey from the source code to a working product.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**DevOps** 代表 **开发运维**。软件开发过程包含许多重复且容易出错的任务，每次软件从源代码到达工作产品时，都需要执行这些任务。'
- en: 'Let''s examine a set of activities that comprise the software development pipeline:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下组成软件开发管道的一系列活动：
- en: Performing checks for errors, typos, bad coding habits, and formatting mistakes.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行检查，查找错误、拼写错误、不良的编码习惯和格式化问题。
- en: Building the code for one or several target platforms. Many applications should
    work on different operating systems.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为一个或多个目标平台构建代码。许多应用程序应该在不同的操作系统上运行。
- en: Running a set of tests that check that the code works as intended, according
    to the requirements.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行一系列测试，检查代码是否按要求正常工作。
- en: Packaging the code.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打包代码。
- en: Deploying packaged software.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署打包好的软件。
- en: '**Continuous integration and continuous deployment** (**CI/CD**) states that
    all of those steps can and should be automated and run as frequently as possible.
    Smaller updates that are thoroughly tested are more reliable. And if everything
    goes wrong, it is much easier to revert such an update. Before CI/CD, the throughput
    of software engineers who manually executed software delivery pipelines limited
    the deployment cycle speed.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**持续集成与持续部署** (**CI/CD**) 表明所有这些步骤可以并且应该被自动化，并尽可能频繁地运行。经过充分测试的小更新更加可靠。如果一切出现问题，回退这样的更新要容易得多。在
    CI/CD 之前，手动执行软件交付管道的软件工程师的吞吐量限制了部署周期的速度。'
- en: Now, highly customizable CI/CD servers rid us of manual labor, and completely
    automate all necessary activities. They run on top of a source code version control
    system, and monitor for new code changes. Once a new code change is present, a
    CI/CD server can launch the delivery pipeline. To implement DevOps, you need to
    spend time writing automated tests and defining software pipelines, but after
    that, the pipeline just works, every time you need it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，高度可定制的 CI/CD 服务器使我们摆脱了手动操作，完全自动化了所有必要的活动。它们运行在源代码版本控制系统之上，并监控新的代码更改。一旦检测到新的代码更改，CI/CD
    服务器就可以启动交付流水线。要实现 DevOps，您需要花时间编写自动化测试并定义软件流水线，但之后，流水线会每次都按需运行。
- en: DevOps took the software development world by storm, producing many technologies
    that make software engineers more productive. Like any technology ecosystem, an
    expert needs to devote time to learning and integrating all tools together. Over
    time, CI/CD servers became more complicated and feature-rich, and many companies
    felt the need to have a full-time expert capable of managing delivery pipelines
    for their projects. Thus, they came up with the role of DevOps engineer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps 在软件开发领域掀起了革命，推出了许多提高软件工程师生产力的技术。像任何技术生态系统一样，专家需要投入时间来学习并将所有工具整合在一起。随着时间的推移，CI/CD
    服务器变得越来越复杂，功能越来越强大，许多公司意识到需要一位全职专家来管理他们项目的交付流水线。因此，出现了 DevOps 工程师这一角色。
- en: Many tools from the DevOps world are becoming much easier to use, requiring
    only a couple of clicks in a user interface. Some CI/CD solutions such as GitLab
    aid you in creating simple CI/CD pipelines automatically.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 DevOps 世界的许多工具变得更加易于使用，只需要在用户界面中点击几下。一些 CI/CD 解决方案，如 GitLab，帮助您自动创建简单的 CI/CD
    流水线。
- en: Many benefits of CI/CD infrastructure apply to data science projects; however,
    many areas remain uncovered. In the next sections of this chapter, we will look
    at how data science projects can use CI/CD infrastructure, and what tools you
    can use to make the automation of data science project delivery more complete.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: CI/CD 基础设施的许多优势适用于数据科学项目；然而，许多领域仍然没有覆盖。在本章接下来的部分，我们将探讨数据科学项目如何使用 CI/CD 基础设施，以及您可以使用哪些工具来使数据科学项目交付的自动化更加完善。
- en: Exploring the special needs of data science project infrastructure
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据科学项目基础设施的特殊需求
- en: 'A modern software project will likely use the following infrastructure to implement
    CI/CD:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现代的软件项目可能会使用以下基础设施来实现 CI/CD：
- en: Version control—Git
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本控制——Git
- en: Code collaboration platform—GitHub, GitLab
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码协作平台——GitHub、GitLab
- en: Automated testing framework—dependent on the implementation language
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化测试框架——取决于实现语言
- en: CI/CD server—Jenkins, Travis CI, Circle CI, or GitLab CI
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CI/CD 服务器——Jenkins、Travis CI、Circle CI 或 GitLab CI
- en: 'All of these technologies miss several core features that are critical for
    data science projects:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些技术都缺少一些对数据科学项目至关重要的核心功能：
- en: Data management—tools for solving the issue of storing and versioning large
    amounts of data files
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管理——解决存储和版本控制大量数据文件问题的工具
- en: Experiment tracking—tools for tracking experiment results
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验追踪——用于跟踪实验结果的工具
- en: Automated testing—tools and methods for testing data-heavy applications
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化测试——测试数据密集型应用程序的工具和方法
- en: Before covering solutions to the preceding issues, we will familiarize ourselves
    with the data science delivery pipeline.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍解决前述问题的方案之前，我们将先了解数据科学交付流水线。
- en: The data science delivery pipeline
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学交付流水线
- en: 'Data science projects consist of multiple data processing pipelines that are
    dependent on each other. The following diagram displays the general pipeline of
    a data science project:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学项目由多个相互依赖的数据处理流水线组成。以下图显示了数据科学项目的一般流水线：
- en: '![](img/939b8432-da8e-45bc-9227-01a32fe4253b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/939b8432-da8e-45bc-9227-01a32fe4253b.png)'
- en: 'Let''s quickly sum up all of the stages in the preceding diagram:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快速总结一下前述图中的所有阶段：
- en: Each model pipeline starts with the **Raw data**, which is stored in some kind
    of data source.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个模型流水线都以**原始数据**开始，这些数据存储在某种数据源中。
- en: Then, data scientists perform **exploratory data analysis** (**EDA**)and create **EDA
    Reports** to deepen the understanding of the dataset and discover possible issues
    with the data.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，数据科学家执行**探索性数据分析**（**EDA**），并创建**EDA报告**，以加深对数据集的理解并发现数据中的潜在问题。
- en: The **Data processing pipeline**transforms raw data into an intermediate format
    that is more suitable for creating datasets for training, validating, and testing
    models.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据处理管道**将原始数据转换为一种中间格式，这种格式更适合创建用于训练、验证和测试模型的数据集。'
- en: The **Model dataset pipeline** creates ready-to-use datasets for training and
    testing models.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型数据集管道**创建了用于训练和测试模型的现成数据集。'
- en: The **Model training pipeline **uses prepared datasets to train models, assess
    their quality by performing offline testing, and generate **Model quality reports **that
    contain detailed information about model testing results.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型训练管道**使用准备好的数据集来训练模型，通过离线测试评估其质量，并生成包含详细模型测试结果的**模型质量报告**。'
- en: At the end of the pipeline, you get the final artifact—a **Trained model **that
    is saved on a hard disk or a database.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在管道的最后，您将获得最终的成果——一个**训练好的模型**，它被保存在硬盘或数据库中。
- en: Now, we are ready to discuss implementation strategies and example tools for
    ModelOps.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始讨论ModelOps的实施策略和示例工具。
- en: Managing code versions and quality
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理代码版本和质量
- en: Data science projects deal with a lot of code, so data scientists need to use
    **source version control** (**SVC**) systems such as Git as a mandatory component.
    The most obvious way of using Git is to employ a code collaboration platform such
    as GitLab or GitHub. Those platforms provide ready-to-use Git servers, along with
    useful collaboration tools for code reviews and issue management, making working
    on shared projects easier. Such platforms also offer integrations with CI/CD solutions,
    creating a complete and easily configurable software delivery pipeline. GitHub
    and GitLab are free to use, and GitLab is available for on-premises installations,
    so there is no excuse for your team to miss the benefits of using one of those
    platforms.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学项目涉及大量代码，因此数据科学家需要使用**源代码版本控制**（**SVC**）系统，例如Git，作为必不可少的组件。使用Git最直接的方式是采用像GitLab或GitHub这样的代码协作平台。这些平台提供了现成的Git服务器，并配有用于代码审查和问题管理的有用协作工具，使得在共享项目上工作更加便捷。这些平台还与CI/CD解决方案集成，创建了一个完整且易于配置的软件交付管道。GitHub和GitLab是免费使用的，而且GitLab支持本地安装，因此您的团队没有理由错过使用这些平台的好处。
- en: Many teams synonymize Git with one of the popular platform offerings, but it
    is sometimes useful to know that it is not the only option you have. Sometimes,
    you have no internet access or the ability to install additional software on server
    machines but still want the benefits of storing code in a shared repository. You
    can still use Git in those restricted environments. Git has a useful feature called
    **file remotes** that allow you to push your code basically everywhere.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 许多团队将Git与流行的平台产品同义，但有时了解它并不是唯一的选择也很有用。有时，您无法访问互联网或在服务器上安装额外软件，但仍希望在共享仓库中存储代码。即便在这些受限环境中，您仍然可以使用Git。Git有一个非常有用的功能叫做**文件远程**，它允许您将代码推送到几乎任何地方。
- en: 'For example, you can use a USB stick or a shared folder as a remote repository:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以使用USB闪存驱动器或共享文件夹作为远程仓库：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: By changing the `file:///` path to the `ssh:///` path, you can also push code
    to the remote SSH machines on your local network.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将`file:///`路径更改为`ssh:///`路径，您还可以将代码推送到本地网络上的远程SSH机器。
- en: Most data science projects are written in Python, where static code analysis
    and code build systems are not as widespread as in other programming languages.
    Those tools allow you to groom code automatically and check it for critical errors
    and possible bugs each time you try to build a project. Python has such tools
    too—look at pre-commit ([https://pre-commit.com](https://pre-commit.com)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据科学项目都是用Python编写的，而静态代码分析和代码构建系统在其他编程语言中更为普及。这些工具可以在每次尝试构建项目时自动修整代码，并检查关键错误和潜在的漏洞。Python也有这样的工具——例如pre-commit（[https://pre-commit.com](https://pre-commit.com)）。
- en: 'The following screenshot demonstrates the running of pre-commit on a Python
    code repository:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图演示了在Python代码仓库上运行pre-commit的情况：
- en: '![](img/07a4c0aa-e54b-40ab-9d85-eb90b5dc742f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07a4c0aa-e54b-40ab-9d85-eb90b5dc742f.png)'
- en: Having covered the main recommendations for handling code, let's now see how
    we can achieve the same results for data, which is an integral part of any data
    science project.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了处理代码的主要建议之后，接下来我们来看一下如何为数据实现相同的效果，数据是任何数据科学项目的核心部分。
- en: Storing data along with the code
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据与代码一起存储
- en: 'As you have seen previously, we can structure code in data science projects
    into a set of pipelines that produce various artifacts: reports, models, and data.
    Different versions of code produce changing outputs, and data scientists often
    need to reproduce results or use artifacts from past versions of pipelines.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您先前所见，我们可以将数据科学项目中的代码结构化为一组生成各种工件（报告、模型和数据）的管道。不同版本的代码会产生不同的输出，数据科学家经常需要复现结果或使用过去版本管道的工件。
- en: 'This distinguishes data science projects from software projects and creates
    a need for managing data versions along with the code: **Data Version Control**
    (**DVC**). In general, different software versions can be reconstructed by using
    the source code alone, but for data science projects this is not sufficient. Let''s
    see what problems arise when you try to track datasets using Git.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这将数据科学项目与软件项目区分开来，并创建了管理数据版本的需求，以及与代码一起使用的**数据版本控制**（**DVC**）。一般来说，可以仅通过源代码重构不同的软件版本，但对于数据科学项目来说，这是不够的。让我们看看当您尝试使用Git跟踪数据集时会遇到什么问题。
- en: Tracking and versioning data
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪和版本控制数据
- en: To train and switch between every version of your data science pipeline, you
    should track data changes along with the code. Sometimes, a full project pipeline
    can take days to calculate. You should store and document not only incoming but
    also intermediate datasets for your project to save time. It is handy to create
    several model training pipelines from a single dataset without waiting for the
    dataset pipeline to finish each time you need it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练和切换您的数据科学管道的每个版本，您应该跟踪数据变化以及代码。有时，一个完整的项目管道可能需要几天的计算时间。您应该存储和记录项目的不仅是输入，还有中间数据集，以节省时间。从单个数据集创建多个模型训练管道而无需每次都等待数据集管道完成非常方便。
- en: Structuring pipelines and intermediate results is an interesting topic that
    deserves special attention. The pipeline structure of your project determines
    what intermediate results are available for use. Each intermediate result creates
    a branching point, from where several other pipelines can start. This creates
    the flexibility of reusing intermediate results, but at the cost of storage and
    time. Projects with lots of intermediate steps can consume a lot of disk space
    and will take more time to calculate, as disk input/output takes a lot of time.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化管道和中间结果是一个值得特别关注的有趣话题。您项目的管道结构决定了可以用于使用的中间结果。每个中间结果都创建了一个分支点，从中可以启动几个其他管道。这创造了重复使用中间结果的灵活性，但以存储和时间为代价。具有许多中间步骤的项目可能会消耗大量磁盘空间，并且计算时间更长，因为磁盘输入/输出需要大量时间。
- en: Be aware that model training pipelines and production pipelines should be different.
    A model training pipeline might have a lot of intermediate steps for research
    flexibility, but a production pipeline should be highly optimized for performance
    and reliability. Only intermediate steps that are strictly necessary to execute
    the finalized production pipeline need to be executed.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模型训练管道和生产管道应该是不同的。模型训练管道可能有很多中间步骤，以提供研究灵活性，但生产管道应高度优化以提高性能和可靠性。只有执行最终生产管道所需的严格必要的中间步骤才需要执行。
- en: Storing data files is necessary for reproducing results but is not sufficient
    for understanding them. You can save yourself a lot of time by documenting data
    descriptions, along with all reports that contain summaries and conclusions that
    your team draws from data. If you can, store those documents in a simple textual
    format so that they can be easily tracked in your version control system along
    with the corresponding code.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 存储数据文件对于复现结果是必要的，但不足以理解结果。通过记录数据描述以及所有包含团队从数据中得出的总结和结论的报告，您可以节省大量时间。如果可能的话，将这些文档存储为简单的文本格式，以便它们可以与相应的代码一起轻松跟踪在您的版本控制系统中。
- en: 'You can use the following folder structure to store the data in your projects:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下文件夹结构存储项目中的数据：
- en: 'Project root:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目根目录：
- en: 'Data:'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据：
- en: Raw—raw data from your customer
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始——来自您的客户的原始数据
- en: Interim—intermediate data generated by the processing pipeline
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中间——处理管道生成的中间数据
- en: Preprocessed—model datasets or output files
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理——模型数据集或输出文件
- en: Reports—project reports for EDA, model quality, and so on
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告——项目的探索性数据分析报告，模型质量等
- en: References—data dictionaries and data source documentation
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引用——数据字典和数据源文档
- en: Storing data in practice
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储实践中的数据
- en: We have explored why it is important to store and manage data artifacts along
    with the code but did not look at how we can do it in practice. Code version control
    systems such as Git are ill-suited for this use case. Git was developed specifically
    for storing source code changes. Internally, each change in Git is stored as a
    `diff` file that represents changed lines of a source code file.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了为什么将数据工件与代码一起存储和管理很重要，但并没有讨论如何在实践中实现这一点。像 Git 这样的代码版本控制系统并不适合这个用例。Git
    是专门为存储源代码变更而开发的。在 Git 内部，每个变更都作为一个 `diff` 文件存储，表示源代码文件中发生变化的行。
- en: 'You can see a simple example of a `diff` file in the following screenshot:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下截图中看到一个简单的 `diff` 文件示例：
- en: '![](img/186ed2d9-af3c-402d-a2be-b9a513b47f2e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/186ed2d9-af3c-402d-a2be-b9a513b47f2e.png)'
- en: The highlighted lines marked with + represent added lines, while highlighted
    lines marked with – stand for deleted lines. Adding large binary or text files
    in Git is considered bad practice because it results in massive redundant `diff`
    computations, which makes repositories slow to work with and large in size.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 高亮的以 + 标记的行表示添加的行，而以 – 标记的行表示删除的行。在 Git 中添加大型二进制或文本文件被认为是不好的做法，因为它会导致大量冗余的 `diff`
    计算，从而使得仓库变得缓慢且体积庞大。
- en: '`diff` files serve a very specific problem: they allow developers to browse,
    discuss, and switch between sets of changes. `diff` is a line-based format that
    is targeted at text files. On the contrary, small changes in binary data files
    will result in a completely different data file. In such cases, Git will generate
    a massive `diff` for each small data modification.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`diff` 文件解决了一个非常特定的问题：它们允许开发者浏览、讨论并在一组变更之间切换。`diff` 是一种基于行的格式，针对的是文本文件。相反，二进制数据文件中的小改动会导致完全不同的数据文件。在这种情况下，Git
    会为每个小的数据修改生成庞大的 `diff` 文件。'
- en: 'In general, you needn''t browse or discuss changes to a data file in a line-based
    format, so calculating and storing `diff` files for each new data version is unnecessary:
    it is much simpler to store the entire data file each time it changes.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你不需要浏览或讨论数据文件中的行级变更，因此为每个新版本的数据计算并存储 `diff` 文件是没有必要的：每次数据文件发生变化时，存储整个数据文件要简单得多。
- en: A growing desire for data versioning systems produced several technical solutions
    to the problem, the most popular being GitLFS and DVC. GitLFS allows you to store
    large files in Git without generating large diffs, while DVC goes further and
    allows you to store data at various remote locations, such as Amazon S3 storage
    or a remote SSH server. DVC goes beyond just implementing data version control
    and allows you to create automated reproducible pipelines by capturing code along
    with its input data, output files, and metrics. DVC also handles pipeline dependency
    graphs, so that it can automatically find and execute any previous steps of the
    pipeline to generate files that you need as input for your code.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据版本控制系统的需求不断增长，产生了几种技术解决方案，其中最流行的是 GitLFS 和 DVC。GitLFS 允许你在 Git 中存储大文件而不会生成庞大的
    `diff` 文件，而 DVC 则进一步扩展，允许你将数据存储在多个远程位置，例如 Amazon S3 存储或远程 SSH 服务器。DVC 不仅实现了数据版本控制，还允许你通过捕获代码及其输入数据、输出文件和指标来创建自动化的可复现流水线。DVC
    还处理流水线的依赖图，从而可以自动查找并执行流水线的任何先前步骤，以生成你需要的输入文件来支持代码的执行。
- en: Now that we are equipped with the tools to handle data storage and versioning,
    let's look at how to manage Python environments so that your team won't waste
    time with package conflicts on a server.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经配备了处理数据存储和版本控制的工具，接下来让我们看看如何管理 Python 环境，以便你的团队不会在服务器上浪费时间处理包冲突。
- en: Managing environments
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理环境
- en: Data science projects depend on a lot of open source libraries and tools for
    doing data analysis. Many of those tools are constantly updated with new features,
    which sometimes break APIs. It is important to fix all dependencies in a shareable
    format that allows every team member to use the same versions and build libraries.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学项目依赖于许多开源库和工具来进行数据分析。这些工具中的许多都在不断更新新功能，而这些更新有时会破坏 API。固定所有依赖项并以共享格式存储非常重要，这样每个团队成员就能使用相同的版本并构建库。
- en: 'The Python ecosystem has multiple environment management tools that take care
    of different problems. Tools overlap in their use cases and are often confusing
    to choose from, so we will cover each briefly:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Python 生态系统中有多个环境管理工具，用于解决不同的问题。这些工具的使用场景有重叠，因此在选择时常常让人困惑，我们将简要介绍每个工具：
- en: '**pyenv** ([https://github.com/pyenv/pyenv](https://github.com/pyenv/pyenv))
    is a tool for managing Python distributions on a single machine. Different projects
    may use different Python versions, and pyenv allows you to switch between different
    Python versions between projects.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pyenv** ([https://github.com/pyenv/pyenv](https://github.com/pyenv/pyenv))
    是一个用于在单台机器上管理Python发行版的工具。不同的项目可能使用不同的Python版本，pyenv允许你在项目之间切换不同的Python版本。'
- en: '**virtualenv** ([https://virtualenv.pypa.io](https://virtualenv.pypa.io)) is
    a tool for creating virtual environments that contain different sets of Python
    packages. Virtual environments are useful for switching contexts between different
    projects, as they may require the use of conflicting versions of Python packages.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**virtualenv** ([https://virtualenv.pypa.io](https://virtualenv.pypa.io)) 是一个用于创建包含不同Python包集合的虚拟环境的工具。虚拟环境在不同项目之间切换时非常有用，因为这些项目可能需要使用不同版本的Python包，这些包可能存在冲突。'
- en: '**pipenv** ([https://pipenv-searchable.readthedocs.io](https://pipenv-searchable.readthedocs.io))
    is a step above virtualenv. Pipenv cares about automatically creating a sharable
    virtual environment for a project that other developers may easily use.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pipenv** ([https://pipenv-searchable.readthedocs.io](https://pipenv-searchable.readthedocs.io))
    是虚拟环境的一个高级工具。Pipenv的主要功能是自动为项目创建一个可共享的虚拟环境，其他开发人员可以轻松使用。'
- en: '**Conda** ([https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/))
    is another environment manager like pipenv. Conda is popular in the data science
    community for several reasons:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Conda** ([https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/))
    是另一个像pipenv一样的环境管理工具。Conda在数据科学社区中很受欢迎，原因有几个：'
- en: It allows sharing environments with other developers via the `environment.yml`
    file.
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许通过`environment.yml`文件与其他开发人员共享环境。
- en: It provides the Anaconda Python distribution, which contains gigabytes of pre-installed
    popular data science packages.
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了Anaconda Python发行版，其中包含大量预安装的流行数据科学包。
- en: It provides highly optimized builds of popular data analysis and machine learning
    libraries. Scientific Python packages often require building dependencies from
    the source code.
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了对流行的数据分析和机器学习库的高度优化版本。科学Python包通常需要从源代码构建依赖项。
- en: Conda can install the CUDA framework along with your favorite deep learning
    framework. CUDA is a specialized computation library that is required for optimizing
    deep neural networks on a GPU.
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conda可以与您喜爱的深度学习框架一起安装CUDA框架。CUDA是一个专用的计算库，优化深度神经网络在GPU上的性能时是必需的。
- en: 'Consider using conda for managing data science project environments if you
    are not doing so already. It will not only solve your environment management problems
    but also save time by speeding up the computation. The following plot shows the
    performance difference between using the TensorFlow libraries installed by **pip**
    and **conda** (you can find the original article by following this link: [https://www.anaconda.com/tensorflow-in-anaconda/](https://www.anaconda.com/tensorflow-in-anaconda/)):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有使用conda来管理数据科学项目环境，建议考虑使用它。它不仅可以解决环境管理问题，还能通过加速计算节省时间。以下图表显示了使用**pip**和**conda**安装TensorFlow库的性能差异（你可以通过以下链接找到原始文章：[https://www.anaconda.com/tensorflow-in-anaconda/](https://www.anaconda.com/tensorflow-in-anaconda/)）：
- en: '![](img/a1f3b1e7-4599-4dcb-97ae-7c7486879d5d.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1f3b1e7-4599-4dcb-97ae-7c7486879d5d.png)'
- en: Next, we will cover the topic of experiment tracking. Experiments are a natural
    part of every data science project. A single project might contain the results
    of hundreds or even thousands of experiments. It is important to keep a record
    so that you can make correct conclusions about experiment results.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论实验跟踪的话题。实验是每个数据科学项目中的自然组成部分。单个项目可能包含数百甚至数千个实验结果。记录实验结果非常重要，这样你才能得出正确的结论。
- en: Tracking experiments
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验跟踪
- en: Experimentation lies at the core of data science. Data scientists perform many
    experiments to find the best approach to solving the task at hand. In general,
    experiments exist in sets that are tied to data processing pipeline steps.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 实验是数据科学的核心。数据科学家进行许多实验，以找到解决手头任务的最佳方法。通常，实验是与数据处理管道步骤相关的一组操作。
- en: 'For example, your project may comprise the following experiment sets:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你的项目可能包括以下实验集：
- en: Feature engendering experiments
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程实验
- en: Experiments with different machine learning algorithms
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同机器学习算法的实验
- en: Hyperparameter optimization experiments
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数优化实验
- en: Each experiment can affect the results of other experiments, so it is crucial
    to be able to reproduce each experiment in isolation. It is also important to
    track all results so your team can compare pipeline variants and choose the best
    one for your project according to the metric values.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 每个实验可能会影响其他实验的结果，因此能够在隔离环境中复现每个实验至关重要。跟踪所有结果也非常重要，这样你的团队可以比较管道的不同版本，并根据指标值选择最适合你项目的版本。
- en: 'A simple spreadsheet file with links to data files and code versions can be
    used to track all experiments, but reproducing experiments will require lots of
    manual work and is not guaranteed to work as expected. Although tracking experiments
    in a file requires manual work, the approach has its benefits: it is very easy
    to start and pleasant to version. For example, you can store the experiment results
    in a simple CSV file, which is versioned in Git along with your code.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的电子表格文件，包含数据文件和代码版本的链接，可以用来跟踪所有实验，但复现实验将需要大量的手动工作，且不能保证按预期工作。尽管使用文件跟踪实验需要手动操作，但这种方法也有其优点：它非常容易上手，而且版本控制也很方便。例如，你可以将实验结果存储在一个简单的
    CSV 文件中，并将其与代码一起在 Git 中进行版本控制。
- en: 'A recommended minimum set of columns for a metric tracking file is as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一份推荐的指标跟踪文件的最小列集如下：
- en: Experiment date
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验日期
- en: Code version (Git commit hash)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码版本（Git 提交哈希）
- en: Model name
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型名称
- en: Model parameters
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数
- en: Training dataset size
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据集大小
- en: Training dataset link
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据集链接
- en: Validation dataset size (fold number for cross-validation)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证数据集大小（交叉验证的折数）
- en: Validation dataset link (none for cross-validation)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证数据集链接（交叉验证没有）
- en: Test dataset size
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据集大小
- en: Test dataset link
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据集链接
- en: Metric results (one column per metric; one column per dataset)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标结果（每个指标一列；每个数据集一列）
- en: Output file links
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出文件链接
- en: Experiment description
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验描述
- en: 'Files are easy to work with if you have a moderate amount of experiments, but
    if your project uses multiple models, and each requires a large amount of experimentation,
    using files becomes cumbersome. If a team of data scientists performs simultaneous
    experiments, tracking files from each team member will require manual merges,
    and data scientists are better off spending time on carrying out more experiments
    rather than merging other teammates'' results. Special frameworks for tracking
    experiment results exist for more complex research projects. These tools integrate
    into the model training pipeline and allow you to automatically track experiment
    results in a shared database so that each team member can focus on experimentation,
    while all bookkeeping happens automatically. Those tools present a rich user interface
    for searching experiment results, browsing metric plots, and even storing and
    downloading experiment artifacts. Another benefit of using experiment tracking
    tools is that they track a lot of technical information that might become handy
    but is too tedious to collect by hand: server resources, server hostnames, script
    paths, and even environment variables present on the experiment run.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果实验数量适中，文件处理起来相对简单，但如果你的项目使用了多个模型，并且每个模型都需要大量的实验，使用文件就变得非常繁琐。如果一组数据科学家同时进行实验，跟踪每个团队成员的文件就需要手动合并，而数据科学家更应该把时间花在进行更多实验上，而不是合并其他团队成员的结果。对于更复杂的研究项目，存在专门的实验结果跟踪框架。这些工具可以集成到模型训练管道中，允许你自动将实验结果跟踪到共享数据库中，这样每个团队成员都可以专注于实验工作，而所有的记录工作会自动完成。这些工具提供了丰富的用户界面，可以用来搜索实验结果、浏览指标图表，甚至存储和下载实验工件。使用实验跟踪工具的另一个好处是，它们能够跟踪许多可能会用得上的技术信息，但这些信息手动收集起来非常繁琐：服务器资源、服务器主机名、脚本路径，甚至实验运行时的环境变量。
- en: 'The data science community uses three major open source solutions that allow
    the tracking of experiment results. These tools pack much more functionality than
    experiment tracking, and we will briefly cover each:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学社区使用三种主要的开源解决方案来跟踪实验结果。这些工具提供的功能远超实验跟踪，我们将简要介绍每一种：
- en: '**Sacred**: This is an advanced experiment tracking server with modular architecture.
    It has a Python framework for managing and tracking experiments that can be easily
    integrated into the existing code base. Sacred also has several UIs that your
    team can use to browse experiment results. Out of all the other solutions, only
    Sacred focuses fully on experiment tracking. It captures the widest set of information,
    including server information, metrics, artifacts, logs, and even experiment source
    code. Sacred presents the most complete experiment tracking experience, but is
    hard to manage, since it requires you to set up a separate tracking server that
    should always be online. Without access to the tracking server, your team won''t
    be able to track experiment results.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sacred**：这是一个具有模块化架构的高级实验追踪服务器。它提供一个Python框架用于管理和追踪实验，可以轻松集成到现有的代码库中。Sacred还提供多个用户界面，供您的团队浏览实验结果。在所有其他解决方案中，只有Sacred专注于实验追踪。它捕捉到最广泛的信息集，包括服务器信息、指标、工件、日志，甚至是实验源代码。Sacred提供了最完整的实验追踪体验，但它的管理较为困难，因为它需要您设置一个单独的追踪服务器，并且该服务器必须始终在线。如果没有访问追踪服务器的权限，您的团队将无法追踪实验结果。'
- en: '**MLflow**: This is an experimentation framework that allows tracking experiments,
    serving models, and managing data science projects. MLflow is easy to integrate
    and can be used both in a client-server setup or locally. Its tracking features
    lag a bit behind Sacred''s powerhouse but will be sufficient for most data science
    projects. MLflow also provides tools for jumpstarting projects from templates
    and serving trained models as APIs, providing a quick way to publish experiment
    results as a production-ready service.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow**：这是一个实验框架，允许追踪实验、提供模型服务以及管理数据科学项目。MLflow易于集成，可以在客户端-服务器架构或本地环境中使用。它的追踪功能稍微落后于Sacred的强大功能，但对于大多数数据科学项目来说已经足够。MLflow还提供从模板启动项目的工具，并将训练好的模型作为API提供，这为发布实验结果作为生产就绪服务提供了一个快速的途径。'
- en: '**DVC**: This is a toolkit for data versioning and pipeline management. It
    also provides basic file-based experiment tracking functionality, but it is subpar
    in terms of usability compared to MLflow and Sacred. The power of DVC lies in
    experiment management: it allows you to create fully versioned and reproducible
    model training pipelines. With DVC, each team member is able to pull code, data,
    and pipelines from a server and reproduce results with a single command. DVC has
    a rather steep learning curve but is worth learning, as it solves many technical
    problems that arise in collaboration on data science projects. If your metric
    tracking requirements are simple, you can rely on DVC''s built-in solution, but
    if you need something more rich and visual, combine DVC with MLflow or Sacred
    tracking—those tools are not mutually exclusive.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DVC**：这是一个用于数据版本控制和管道管理的工具包。它还提供基本的基于文件的实验追踪功能，但在可用性方面，DVC与MLflow和Sacred相比有所不足。DVC的强大之处在于实验管理：它允许您创建完全版本化和可重现的模型训练管道。使用DVC，每个团队成员都能从服务器拉取代码、数据和管道，并通过一个命令重现结果。DVC有相当陡峭的学习曲线，但值得学习，因为它解决了数据科学项目协作中出现的许多技术问题。如果您的指标追踪需求较简单，可以依赖DVC的内建解决方案，但如果您需要更丰富和可视化的功能，可以将DVC与MLflow或Sacred追踪结合使用——这些工具并不互相排斥。'
- en: Now you should have a complete understanding of what tools can be used to track
    code, data, and experiments as a single entity in your project. Next, we will
    cover the topic of automated testing in data science projects.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该对可以在项目中将代码、数据和实验作为一个整体来追踪的工具有了完整的了解。接下来，我们将讨论数据科学项目中的自动化测试主题。
- en: The importance of automated testing
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化测试的重要性
- en: Automated testing is considered to be mandatory in software engineering projects.
    Slight changes in software code can introduce unintended bugs in other parts,
    so it is important to check that everything works as intended as frequently as
    possible. Automated tests that are written in a programming language allow testing
    the system as many times as you like. The principle of CI advises running tests
    each time a change in code is pushed to a version control system. A multitude
    of testing frameworks exists for all major programming languages. Using them,
    developers can create automated tests for the backend and frontend parts of their
    product. Large software projects can include thousands of automated tests that
    are run each time someone changes the code. Tests can consume significant resources
    and require a lot of time for completion. To solve this problem, CI servers can
    run tests in parallel on multiple machines.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程项目中，自动化测试被认为是强制性的。软件代码的轻微变化可能会在其他部分引入无意的 bug，因此尽可能频繁地检查一切是否按预期工作非常重要。用编程语言编写的自动化测试允许你多次测试系统。CI（持续集成）原则建议在每次代码变更推送到版本控制系统时运行测试。所有主要编程语言都有多种测试框架。开发者可以利用它们为产品的后端和前端部分创建自动化测试。大型软件项目可能包含成千上万的自动化测试，每次有人修改代码时都会运行这些测试。测试可能会消耗大量资源，并且需要很长时间才能完成。为了解决这个问题，CI
    服务器可以在多台机器上并行运行测试。
- en: 'In software engineering, we can divide all tests into a hierarchy:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程中，我们可以将所有的测试分为一个层次结构：
- en: End-to-end tests perform a full check of a major function of a system. In data
    science projects, end-to-end tests can train a model on a full dataset and check
    whether the metrics values suffice minimum model quality requirements.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 端到端测试对系统的一个主要功能进行全面检查。在数据科学项目中，端到端测试可以使用完整的数据集来训练模型，并检查度量值是否满足最低的模型质量要求。
- en: Integration tests check that every component of the system works together as
    intended. In a data science system, an integration test might check that all of
    the steps of the model testing pipeline finish successfully and provide the desired
    result.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集成测试检查系统的每个组件是否按预期一起工作。在数据科学系统中，集成测试可能会检查模型测试管道的所有步骤是否都成功完成，并提供期望的结果。
- en: Unit tests check individual classes and functions. In a data science project,
    a unit test can check the correctness of a single method in a data processing
    pipeline step.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单元测试检查单个类和函数。在数据科学项目中，单元测试可以检查数据处理管道步骤中单个方法的正确性。
- en: If the world of software testing is so technologically developed, can data science
    projects benefit from automated tests? The main difference between data science
    code and software testing code is the reliability of data. A fixed set of test
    data that is generated before a test run is sufficient for most software projects.
    In data science projects, the situation is different. For a complete test of the
    model training pipeline, you may need gigabytes of data. Some pipelines may run
    for hours or even days and require distributed computation clusters, so testing
    them becomes impractical. For this reason, many data science projects avoid automated
    testing. Thus, they suffer from unexpected bugs, ripple effects, and slow change
    integration cycles.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果软件测试的世界如此技术先进，数据科学项目能从自动化测试中获益吗？数据科学代码与软件测试代码的主要区别在于数据的可靠性。对于大多数软件项目，测试运行前生成的固定测试数据集已足够。数据科学项目则不同。为了对模型训练管道进行完整测试，可能需要数GB的数据。一些管道可能需要运行几个小时甚至几天，并且需要分布式计算集群，因此对它们进行测试变得不切实际。因此，许多数据科学项目避免使用自动化测试。这样，它们就会遭遇意外的
    bug、涟漪效应和缓慢的变更集成周期。
- en: A ripple effect is a common software engineering problem when a slight change
    in one part of the system can affect other components in an unexpected way, causing
    bugs. Automated tests are an efficient solution for detecting ripple effects before
    they cause any real damage.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 涟漪效应是软件工程中的常见问题，当系统的某一部分发生轻微变化时，可能会以意想不到的方式影响其他组件，从而引发 bug。自动化测试是一种高效的解决方案，可以在涟漪效应造成实际损害之前发现它们。
- en: Despite the difficulties, the benefits of automated testing are too great to
    ignore. Ignoring tests turns out to be much more costly than building them. This
    is true for data science projects and software projects. The benefits of automated
    testing grow with project size, complexity, and team size. If you lead a complex
    data science project, consider automating testing as a mandatory requirement for
    your project.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在困难，自动化测试的好处是不可忽视的。忽视测试的代价远比构建它们要高。这对于数据科学项目和软件项目都适用。自动化测试的好处随着项目规模、复杂性和团队人数的增加而增长。如果你领导一个复杂的数据科学项目，考虑将自动化测试作为项目的强制性要求。
- en: Let's look at how we can approach testing data science projects. End-to-end
    testing for model training pipelines might be impractical, but what about testing
    individual pipeline steps? Apart from the model training code, each data science
    project will have some business logic code and data processing code. Most of this
    code can be abstracted away from distributed computation frameworks in isolated
    classes and functions that are easy to test.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何处理数据科学项目的测试。模型训练管道的端到端测试可能不切实际，但测试单个管道步骤如何？除了模型训练代码外，每个数据科学项目都会有一些业务逻辑代码和数据处理代码。这些代码大多数可以从分布式计算框架中抽象出来，放入易于测试的独立类和函数中。
- en: If you architect a project's code base with tests in mind from the start, it
    will be much easier to automate testing. Software architects and lead engineers
    on your team should take the testability of the code as one of the main acceptance
    criteria for code reviews. If the code is properly encapsulated and abstracted,
    testing becomes easier.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从一开始就将项目的代码架构与测试考虑在内，那么自动化测试将变得更加容易。团队中的软件架构师和首席工程师应将代码的可测试性作为代码审查的主要验收标准之一。如果代码得到了恰当的封装和抽象，测试就会变得更加简单。
- en: In particular, let's take the model training pipeline. If we separate it into
    a series of steps with clearly defined interfaces, we can then test data preprocessing
    code separately from model training code. And if data preprocessing takes a lot
    of time and requires expensive computation resources, you can at least test individual
    parts of the pipeline. Even basic function-level tests (unit tests) can save you
    a lot of time, and it is much easier to transition to full end-to-end tests from
    the basis of unit tests.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是，让我们看一下模型训练管道。如果我们将其分为一系列具有明确定义接口的步骤，那么我们就可以将数据预处理代码与模型训练代码分开进行测试。如果数据预处理需要大量时间并且需要昂贵的计算资源，至少可以测试管道的各个部分。即使是基本的函数级测试（单元测试）也能为你节省大量时间，而且从单元测试基础上过渡到完整的端到端测试会变得更加容易。
- en: 'To benefit from automated testing in your projects, start from the following
    guidelines:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在项目中受益于自动化测试，请从以下准则开始：
- en: Architect your code for better testability.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为更好的可测试性构建你的代码架构。
- en: Start small; write unit tests.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从小做起；编写单元测试。
- en: Consider building integration and end-to-end tests.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑构建集成测试和端到端测试。
- en: Keep at it. Remember that testing saves time—especially those nights when your
    team has to fix unexpected bugs in freshly deployed code.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坚持下去。记住，测试能节省时间——尤其是当你的团队需要修复新部署的代码中的意外漏洞时。
- en: We have seen how to manage, test, and maintain code quality in data science
    projects. Next, let's look at how we can package code for deployment.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了如何管理、测试和维护数据科学项目中的代码质量。接下来，让我们看看如何打包代码以进行部署。
- en: Packaging code
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打包代码
- en: 'When deploying Python code for data science projects, you have several options:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在为数据科学项目部署 Python 代码时，你有几个选择：
- en: '**Regular Python scripts**: You just deploy a bunch of Python scripts to the
    server and run them. This is the simplest form of deployment, but it requires
    a lot of manual preparation: you need to install all required packages, fill in
    configuration files, and so on.While those actions can be automated by using tools
    such as Ansible ([https://www.ansible.com/)](https://www.ansible.com/), it''s
    not recommended to use this form of deployment for anything but the simplest projects
    with no long-term maintainability goals.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**常规 Python 脚本**：你只是将一堆 Python 脚本部署到服务器并运行。这是最简单的部署方式，但需要大量的手动准备：你需要安装所有必需的包，填写配置文件，等等。虽然可以通过使用像
    Ansible ([https://www.ansible.com/)](https://www.ansible.com/)) 这样的工具来自动化这些操作，但除非是最简单且没有长期维护需求的项目，否则不建议使用这种部署方式。'
- en: '**Python packages**: Creating a Python package using a `setup.py`file is a
    much more convenient way to package Python code. Tools such as PyScaffold provide
    ready-to-use templates for Python packages, so you won''t need to spend much time
    structuring your project. In the case of Python packages, Ansible still remains
    a viable option for automating manual deployment actions.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python包**：使用`setup.py`文件创建Python包是一种更加便捷的打包Python代码的方式。像PyScaffold这样的工具提供了现成的Python包模板，因此你无需花费太多时间在项目结构的搭建上。在Python包的情况下，Ansible仍然是自动化手动部署操作的可行选择。'
- en: '**Docker image**:Docker ([https://www.docker.com/](https://www.docker.com/)) is
    based on a technology called Linux containers. Docker allows packaging your code
    into an isolated portable environment that can be easily deployed and scaled on
    any Linux machine. It''s like packaging, shipping, and running your application
    along with all dependencies, including a Python interpreter, data files, and an
    OS distribution without entering the world of heavyweight virtual machines. Docker
    works by building a **Docker image** from a set of commands specified in a **Dockerfile***. *A
    running instance of a Docker imageis called a **Docker container***.*'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker镜像**：Docker（[https://www.docker.com/](https://www.docker.com/)）基于一种名为Linux容器的技术。Docker允许将你的代码打包成一个隔离的可移植环境，可以轻松地在任何Linux机器上进行部署和扩展。它就像是将你的应用程序连同所有依赖项（包括Python解释器、数据文件和操作系统分发版）一起打包、运输并运行，而无需进入重量级虚拟机的世界。Docker通过从一组在**Dockerfile**中指定的命令构建**Docker镜像**来工作。*Docker镜像的运行实例称为**Docker容器***。'
- en: Now, we are ready to integrate all tools for dealing with code, data, experiments,
    environments, testing, packaging, and deployment into a single coherent process
    for delivering machine learning models.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好将所有用于处理代码、数据、实验、环境、测试、打包和部署的工具，集成到一个单一、连贯的流程中，以交付机器学习模型。
- en: Continuous model training
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持续模型训练
- en: The end goal of applying CI/CD to data science projects is to have a continuous
    learning pipeline that creates new model versions automatically. This level of
    automation will allow your team to examine new experiment results right after
    pushing the changed code. If everything works as expected, automated tests finish,
    and model quality reports show good results, the model can be deployed into an
    online testing environment.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 将CI/CD应用于数据科学项目的最终目标是拥有一个持续学习的流程，能够自动创建新的模型版本。这种自动化的程度将使团队能够在推送代码变更后，立即检查新的实验结果。如果一切如预期工作，自动化测试完成，并且模型质量报告显示良好的结果，则该模型可以部署到在线测试环境中。
- en: 'Let''s describe the steps of continuous model learning:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述持续模型学习的步骤：
- en: 'CI:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CI：
- en: Perform static code analysis.
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行静态代码分析。
- en: Launch automated tests.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动自动化测试。
- en: 'Continuous model learning:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持续模型学习：
- en: Fetch new data.
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取新数据。
- en: Generate EDA reports.
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成EDA报告。
- en: Launch data quality tests.
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动数据质量测试。
- en: Perform data processing and create a training dataset.
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行数据处理并创建训练数据集。
- en: Train a new model.
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练新模型。
- en: Test the model's quality.
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试模型质量。
- en: Fix experiment results in an experiment log.
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修复实验日志中的实验结果。
- en: 'CD:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CD：
- en: Package the new model version.
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打包新的模型版本。
- en: Package the source code.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打包源代码。
- en: Publish the model and code to the target server.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型和代码发布到目标服务器。
- en: Launch a new version of the system.
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动新版本的系统。
- en: CI/CD servers can automate all parts of the preceding pipeline. CI/CD steps
    should be easy to handle, as they are what CI/CD servers were created for. Continuous
    model learning should not be hard either, as long as you structure your pipeline
    so that it can be launched automatically from the command line. Tools such as
    DVC can aid you in creating reproducible pipelines, which makes it an attractive
    solution for the continuous model learning pipeline.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: CI/CD服务器可以自动化处理前述流程中的所有部分。CI/CD步骤应当容易操作，因为它们正是CI/CD服务器创建的目的。持续模型学习也不应当困难，只要你结构化你的流程，以便可以从命令行自动启动。像DVC这样的工具可以帮助你创建可重现的流程，使其成为持续模型学习流程的一个有吸引力的解决方案。
- en: Now, let's look at how we can build a ModelOps pipeline in a data science project.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在数据科学项目中构建一个ModelOps流程。
- en: Case study – building ModelOps for a predictive maintenance system
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 – 为预测性维护系统构建ModelOps
- en: Oliver is a team leader of a data science project for a large manufacturing
    company called MannCo, whose plants can be found in multiple cities around the
    country. Oliver's team developed a predictive maintenance model that can help
    MannCo to forecast and prevent expensive equipment breakages, which result in
    costly repairs and long production line outages. The model takes measurements
    of multiple sensors as input and outputs a package probability that can be used
    to plan a diagnostics and repair session.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Oliver 是一家大型制造公司 MannCo 的数据科学项目团队负责人，MannCo 的工厂遍布全国多个城市。Oliver 的团队开发了一种预测性维护模型，帮助
    MannCo 预测并防止昂贵的设备故障，这些故障会导致高额的维修费用和生产线长时间停工。该模型以多个传感器的测量值为输入，输出一个包概率，可以用来规划诊断和修复会话。
- en: This example contains some technical details. If you are unfamiliar with the
    technologies mentioned in this case study, you may want to follow links to get
    a better understanding of the details.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例包含了一些技术细节。如果你不熟悉案例中提到的技术，可以点击链接以便更好地理解细节。
- en: Each piece of this equipment is unique in its own way because it operates under
    different conditions on each one of MannCo's plants. This meant that Oliver's
    team would need to constantly adapt and retrain separate models for different
    plants. Let's look at how they solved this task by building a ModelOps pipeline.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设备每一台都有其独特性，因为它们在 MannCo 各个工厂的不同环境下运行。这意味着 Oliver 的团队需要不断调整和重新训练针对不同工厂的独立模型。让我们来看一下他们是如何通过构建一个
    ModelOps 流水线来解决这一任务的。
- en: There were several data scientists on the team, so they needed a tool for sharing
    the code with each other. The customer requested that, for security purposes,
    all code should be stored in local company servers, and not in the cloud. Oliver
    decided to use GitLab ([https://about.gitlab.com/](https://about.gitlab.com/)),
    as it was a general practice in the company. In terms of the overall code management
    process, Oliver suggested using GitFlow ([https://danielkummer.github.io/git-flow-cheatsheet/](https://danielkummer.github.io/git-flow-cheatsheet/)).
    It provided a common set of rules for creating new features, releases, and hotfixes
    for every team member.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 团队中有多位数据科学家，因此他们需要一个工具来共享代码。客户要求出于安全考虑，所有代码必须存储在公司本地服务器中，而不是云端。Oliver 决定使用 GitLab（[https://about.gitlab.com/](https://about.gitlab.com/)），因为这是公司的一般做法。在整体代码管理流程方面，Oliver
    建议使用 GitFlow（[https://danielkummer.github.io/git-flow-cheatsheet/](https://danielkummer.github.io/git-flow-cheatsheet/)）。它为每个团队成员提供了一套统一的规则，用于创建新功能、发布版本和修复热修复。
- en: Oliver knew that a reliable project structure would help his team to properly
    organize code, notebooks, data, and documentation, so he advised his team to use
    PyScaffold ([https://pyscaffold.readthedocs.io/](https://pyscaffold.readthedocs.io/))
    along with the plugin for data science projects ([https://github.com/pyscaffold/pyscaffoldext-dsproject](https://github.com/pyscaffold/pyscaffoldext-dsproject)).
    PyScaffold allowed them to bootstrap a project template that ensured a uniform
    way to store and version data science projects. PyScaffold already provided the `environment.yml`file,
    which defined a template Anaconda ([https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/))
    environment, so the team did not forget to lock the package dependencies in a
    versioned file from the start of the project. Oliver also decided to use DVC ([https://dvc.org/](https://dvc.org/))
    to version datasets using the company's internal SFTP server. They also used a `--gitlab`
    flag for the `pyscaffold` command so that they would have a ready-to-use GitLab
    CI/CD template when they needed it.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Oliver 知道，一个可靠的项目结构将帮助他的团队妥善组织代码、笔记本、数据和文档，因此他建议团队使用 PyScaffold（[https://pyscaffold.readthedocs.io/](https://pyscaffold.readthedocs.io/)）以及数据科学项目插件（[https://github.com/pyscaffold/pyscaffoldext-dsproject](https://github.com/pyscaffold/pyscaffoldext-dsproject)）。PyScaffold
    让他们能够引导一个项目模板，确保以统一的方式存储和版本化数据科学项目。PyScaffold 已经提供了`environment.yml`文件，用于定义一个模板
    Anaconda（[https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/)）环境，因此团队从项目开始就没有忘记锁定包的依赖关系。Oliver
    还决定使用 DVC（[https://dvc.org/](https://dvc.org/)）通过公司的内部 SFTP 服务器来版本化数据集。他们还使用了`--gitlab`标志来运行`pyscaffold`命令，这样当他们需要时就可以得到一个现成的
    GitLab CI/CD 模板。
- en: 'The project structure looked like this (taken from the `pyscaffold-dsproject`
    documentation):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 项目结构如下所示（摘自`pyscaffold-dsproject`文档）：
- en: '[PRE1]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The project team quickly discovered that they would need to perform and compare
    many experiments to build models for different manufacturing plants. They evaluated
    DVC''s metric tracking capabilities. It allowed tracking all metrics using a simple
    versioned text file in Git. While the feature was convenient for simple projects,
    it would be hard to use it in a project with multiple datasets and models. In
    the end, they decided to use a more advanced metric tracker—MLflow ([https://mlflow.org](https://mlflow.org)).
    It provided a convenient UI for browsing experiment results and allowed using
    a shared database so that every team member would be able to quickly share their
    results with the team. MLflow was installed and configured as a regular Python
    package, so it easily integrated into the existing technology stack of the project:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 项目团队很快发现，他们需要执行并比较许多实验，以便为不同的制造工厂构建模型。他们评估了 DVC 的度量跟踪功能。该功能允许使用 Git 中的简单版本化文本文件跟踪所有度量。虽然这个功能对于简单的项目来说很方便，但在多个数据集和模型的项目中使用会变得很困难。最终，他们决定使用一个更先进的度量跟踪器——MLflow（[https://mlflow.org](https://mlflow.org)）。它提供了一个方便的
    UI，用于浏览实验结果，并允许使用共享数据库，以便每个团队成员都能快速与团队分享他们的结果。MLflow 作为常规 Python 包安装和配置，因此它可以轻松集成到现有的技术栈中：
- en: '![](img/bcceca06-f47b-4ed5-92d5-5c4213c99aba.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bcceca06-f47b-4ed5-92d5-5c4213c99aba.png)'
- en: 'The team also decided to leverage DVC pipelines to make each experiment easily
    reproducible. The team liked to prototype models using Jupyter notebooks, so they
    decided to use papermill ([https://papermill.readthedocs.io/en/latest/](https://papermill.readthedocs.io/en/latest/))
    to work with notebooks as they were a set of parametrized Python scripts. Papermill
    allows executing Jupyter notebooks from the command line without starting Jupyter''s
    web interface. The team found the functionality very convenient to use along with
    the DVC pipelines, but the command line for running a single notebook started
    to be too long:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 团队还决定利用 DVC 管道，使每个实验都能轻松重现。团队喜欢使用 Jupyter notebooks 来原型化模型，因此他们决定使用 papermill（[https://papermill.readthedocs.io/en/latest/](https://papermill.readthedocs.io/en/latest/)）来处理
    notebooks，因为它们是参数化的 Python 脚本集。Papermill 允许从命令行执行 Jupyter notebooks，而无需启动 Jupyter
    的 Web 界面。团队发现这个功能与 DVC 管道一起使用时非常方便，但运行单个 notebook 的命令行开始变得太长：
- en: '[PRE2]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To solve this problem, they wrote a Bash script to integrate DVC with papermill
    so that the team members could create reproducible experiments with less typing
    in the terminal:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，他们编写了一个 Bash 脚本，将 DVC 与 papermill 集成，以便团队成员能够通过更少的终端输入来创建可重现的实验：
- en: '[PRE3]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When using several open source ModelOps tools in a single project, your team
    might need to spend some time integrating them together. Be prepared, and plan
    accordingly.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个项目中使用多个开源ModelOps工具时，你的团队可能需要花一些时间将它们集成在一起。要做好准备，并合理规划。
- en: 'Over time, some parts of the code started to duplicate inside the notebooks.
    The PyScaffold template provides a way to solve this problem by encapsulating
    repeated code in the project''s package directory—`src`*.* This way, the project
    team could quickly share code between notebooks. To install the project''s package
    locally, they simply used the following command from the project''s root directory:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，代码的一些部分开始在 notebooks 中重复。PyScaffold 模板通过将重复的代码封装在项目的包目录 `src` 中，提供了解决此问题的方法。这样，项目团队可以快速在
    notebooks 之间共享代码。为了在本地安装项目包，他们只需从项目的根目录使用以下命令：
- en: '[PRE4]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Closer to the project release date, all stable code bases migrated to the project's
    `src` and `scripts` directories. The `scripts` directory contained a single entry
    point script for training a new model version that was output into the `models` directory,
    which was tracked by DVC.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 临近项目发布日期时，所有稳定的代码库都迁移到了项目的 `src` 和 `scripts` 目录中。`scripts` 目录包含一个用于训练新模型版本的单一入口脚本，模型输出到
    `models` 目录中，DVC 跟踪该目录。
- en: To be sure that new changes did not break anything important, the team wrote
    a set of automated tests using `pytest` ([https://docs.pytest.org/](https://docs.pytest.org/)) for
    the stable code base. The tests also checked model quality on a special test dataset
    created by the team. Oliver modified a GitLab CI/CD template that was generated
    by PyScaffold so that tests would be run with each new commit that was pushed
    in a Git repository.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保新的更改没有破坏任何重要功能，团队编写了一套使用`pytest`（[https://docs.pytest.org/](https://docs.pytest.org/)）的自动化测试，针对稳定的代码库。测试还检查了团队创建的一个特殊测试数据集上的模型质量。Oliver
    修改了由 PyScaffold 生成的 GitLab CI/CD 模板，以确保每次新的提交推送到 Git 仓库时都会运行测试。
- en: The customer requested a simple model API, so the team decided to use an MLflow
    server ([https://mlflow.org/docs/latest/models.html](https://mlflow.org/docs/latest/models.html)),
    as MLflow was already integrated into the project. To further automate the deployment
    and packaging process, the team decided to use Docker along with GitLab CI/CD.
    To do this, they followed GitLab's guide for building Docker images ([https://docs.gitlab.com/ee/ci/docker/using_docker_build.html)](https://docs.gitlab.com/ee/ci/docker/using_docker_build.html).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 客户要求一个简单的模型API，因此团队决定使用MLflow服务器（[https://mlflow.org/docs/latest/models.html](https://mlflow.org/docs/latest/models.html)），因为MLflow已经集成到项目中。为了进一步自动化部署和打包过程，团队决定将Docker与GitLab
    CI/CD一起使用。为此，他们按照GitLab的指南来构建Docker镜像（[https://docs.gitlab.com/ee/ci/docker/using_docker_build.html](https://docs.gitlab.com/ee/ci/docker/using_docker_build.html)）。
- en: 'The overall ModelOps process for the project contained the following steps:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 项目整体的ModelOps流程包含以下步骤：
- en: Create new changes in the code.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代码中创建新的更改。
- en: Run pre-commit tests for code quality and styling (provided by PyScaffold).
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行由PyScaffold提供的代码质量和风格检查的pre-commit测试。
- en: Run pytest tests in GitLab CI/CD.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GitLab CI/CD中运行pytest测试。
- en: Package code and trained models into a Docker image in GitLab CI/CD.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GitLab CI/CD中将代码和训练好的模型打包成Docker镜像。
- en: Push the Docker image into the Docker registry in GitLab CI/CD.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Docker镜像推送到GitLab CI/CD中的Docker镜像库。
- en: After manual confirmation in the GitLab UI, run the `update` command on the
    customer server. This command simply pushes the new version of the Docker image
    from the registry to the customer's server and runs it instead of the old version.
    If you're wondering how you can do this in GitLab CI/CD, take a look here: [https://docs.gitlab.com/ee/ci/environments.html#configuring-manual-deployments](https://docs.gitlab.com/ee/ci/environments.html#configuring-manual-deployments).
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GitLab UI中手动确认后，运行`update`命令在客户的服务器上。这条命令将新版本的Docker镜像从镜像库推送到客户的服务器，并运行新版本的镜像来替代旧版本。如果你想了解如何在GitLab
    CI/CD中实现这一操作，可以查看这里：[https://docs.gitlab.com/ee/ci/environments.html#configuring-manual-deployments](https://docs.gitlab.com/ee/ci/environments.html#configuring-manual-deployments)。
- en: 'Please note that, in real projects, you may want to split the deployment into
    several stages for at least two different environments: staging and production.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在实际项目中，你可能希望将部署过程拆分成多个阶段，至少包括两个不同的环境：预发布环境和生产环境。
- en: Creating an end-to-end ModelOps pipeline streamlined the deployment process
    and allowed the team to spot bugs before they went into production so that the
    team was able to focus on building models instead of carrying out repetitive actions
    to test and deploy new versions of a model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 创建端到端的ModelOps流水线简化了部署过程，并且让团队能够在代码进入生产环境之前发现BUG，这样团队就能专注于构建模型，而不是执行重复的测试和部署新版本模型的操作。
- en: As a conclusion to this chapter, we'll look at a list of tools that you can
    use to build ModelOps pipelines.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的结论部分，我们将查看一份可以帮助你构建ModelOps流水线的工具清单。
- en: A power pack for your projects
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你的项目的强大工具包
- en: The data science community has a great number of open source tools that can
    help you in building ModelOps pipelines. Sometimes, it is hard to navigate the
    never-ending list of products, tools, and libraries so I thought this list of
    tools would be helpful and beneficial for your projects.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学社区有大量开源工具可以帮助你构建ModelOps流水线。有时，面对无尽的产品、工具和库清单会感到困惑，因此我认为这份工具清单会对你的项目有所帮助并且富有价值。
- en: 'For static code analysis for Python, see these:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Python的静态代码分析，参见以下内容：
- en: Flake8 ([http://flake8.pycqa.org](http://flake8.pycqa.org))—a style checker
    for Python code
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flake8 ([http://flake8.pycqa.org](http://flake8.pycqa.org))—Python代码的风格检查器
- en: MyPy ([http://www.mypy-lang.org](http://www.mypy-lang.org))—static typing for
    Python
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MyPy ([http://www.mypy-lang.org](http://www.mypy-lang.org))—Python的静态类型检查工具
- en: wemake ([https://github.com/wemake-services/wemake-python-styleguide](https://github.com/wemake-services/wemake-python-styleguide))—a
    set of enhancements for Flake8
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: wemake ([https://github.com/wemake-services/wemake-python-styleguide](https://github.com/wemake-services/wemake-python-styleguide))—Flake8的增强工具集
- en: 'here are some useful Python tools:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些有用的Python工具：
- en: PyScaffold ([https://pyscaffold.readthedocs.io/](https://pyscaffold.readthedocs.io/))—a
    project templating engine. PyScaffold can set up a project structure for you.
    The `dsproject` extension ([https://github.com/pyscaffold/pyscaffoldext-dsproject](https://github.com/pyscaffold/pyscaffoldext-dsproject))
    contains a good data science project template.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyScaffold ([https://pyscaffold.readthedocs.io/](https://pyscaffold.readthedocs.io/))—一个项目模板引擎。PyScaffold
    可以为你设置项目结构。`dsproject` 扩展（[https://github.com/pyscaffold/pyscaffoldext-dsproject](https://github.com/pyscaffold/pyscaffoldext-dsproject)）包含一个很好的数据科学项目模板。
- en: pre-commit ([https://pre-commit.com](https://pre-commit.com))—a tool that allows
    you to set up Git hooks that run each time you commit the code. Automatic formatting,
    style cakes, code formatting, and other tools can be integrated into your build
    pipeline even before you decide to use a CI/CD server.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pre-commit ([https://pre-commit.com](https://pre-commit.com))—一个允许你设置 Git 钩子的工具，每次提交代码时都会运行。自动格式化、风格检查、代码格式化以及其他工具可以在你决定使用
    CI/CD 服务器之前集成到构建流水线中。
- en: pytest ([https://docs.pytest.org/](https://docs.pytest.org/))—a Python testing
    framework that allows you to structure your tests using reusable fixtures. It
    comes in handy when testing data science pipelines with many data dependencies.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pytest ([https://docs.pytest.org/](https://docs.pytest.org/))—一个 Python 测试框架，允许你使用可重用的
    fixture 组织测试。在测试有许多数据依赖关系的数据科学流水线时，它非常有用。
- en: Hypothesis ([https://hypothesis.works](https://hypothesis.works))—a fuzz testing
    framework for Python that creates automated tests based on metadata about your
    functions.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hypothesis ([https://hypothesis.works](https://hypothesis.works))—一个 Python
    的模糊测试框架，可以基于你的函数的元数据创建自动化测试。
- en: 'For CI/CD servers, see these:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CI/CD 服务器，参考以下内容：
- en: Jenkins ([https://jenkins.io](https://jenkins.io))—a popular, stable, and old
    CI/CD server solution. It packs lots of features but is a bit cumbersome to use
    compared to more modern tools.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jenkins ([https://jenkins.io](https://jenkins.io))—一个流行、稳定且历史悠久的 CI/CD 服务器解决方案。它包含许多功能，但与一些现代工具相比，使用起来有些笨重。
- en: GitLab CI/CD ([https://docs.gitlab.com/ee/ci/](https://docs.gitlab.com/ee/ci/))—is
    a free CI/CD server with cloud and on-premises options. It is easy to set up and
    easy to use, but forces you to live in the GitLab ecosystem, which might not be
    a bad decision, since GitLab is one of the best collaboration platforms out there.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitLab CI/CD ([https://docs.gitlab.com/ee/ci/](https://docs.gitlab.com/ee/ci/))—一个免费的
    CI/CD 服务器，提供云端和本地部署选项。它易于设置和使用，但迫使你生活在 GitLab 的生态系统中，虽然这可能不是一个坏决定，因为 GitLab 是最好的协作平台之一。
- en: Travis CI ([https://travis-ci.org](https://travis-ci.org)) and Circle CI ([https://circleci.com](https://circleci.com))—cloud
    CI/CD solutions. Useful if you develop in cloud environments.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Travis CI ([https://travis-ci.org](https://travis-ci.org)) 和 Circle CI ([https://circleci.com](https://circleci.com))—云
    CI/CD 解决方案。如果你在云环境中开发，这些非常有用。
- en: 'For experiment tracking tools, see these:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实验跟踪工具，参考以下内容：
- en: MLflow ([https://mlflow.org](https://mlflow.org))—experiment tracking framework
    that can be used both locally and in a shared client-server setup
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLflow ([https://mlflow.org](https://mlflow.org))—一个实验跟踪框架，可以在本地使用，也可以在共享的客户端-服务器环境中使用
- en: Sacred ([https://github.com/IDSIA/sacred](https://github.com/IDSIA/sacred))—a
    feature-packed experiment tracking framework
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sacred ([https://github.com/IDSIA/sacred](https://github.com/IDSIA/sacred))—一个功能强大的实验跟踪框架
- en: DVC ([https://dvc.org/doc/get-started/metrics](https://dvc.org/doc/get-started/metrics))—file-based
    metric tracking solution that uses Git
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DVC ([https://dvc.org/doc/get-started/metrics](https://dvc.org/doc/get-started/metrics))—基于文件的度量追踪解决方案，使用
    Git 进行管理
- en: 'For data version control, see these:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据版本控制，参考以下内容：
- en: DVC ([https://dvc.org/](https://dvc.org/))—data version control for data science
    projects
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DVC ([https://dvc.org/](https://dvc.org/))—用于数据科学项目的数据版本控制工具
- en: GitLFS ([https://git-lfs.github.com](https://git-lfs.github.com))—a general
    solution for storing large files in Git
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitLFS ([https://git-lfs.github.com](https://git-lfs.github.com))—一个通用的解决方案，用于在
    Git 中存储大文件
- en: 'For pipeline tools, see these:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对于流水线工具，参考以下内容：
- en: 'Reproducible pipelines for data science projects:'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可复现的数据科学项目流水线：
- en: DVC ([https://dvc.org/doc/get-started/pipeline](https://dvc.org/doc/get-started/pipeline))
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DVC ([https://dvc.org/doc/get-started/pipeline](https://dvc.org/doc/get-started/pipeline))
- en: MLflow Projects ([https://mlflow.org/docs/latest/projects.html](https://mlflow.org/docs/latest/projects.html))
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLflow Projects ([https://mlflow.org/docs/latest/projects.html](https://mlflow.org/docs/latest/projects.html))
- en: 'For code collaboration platforms, see these:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对于代码协作平台，参考以下内容：
- en: GitHub ([https://github.com/](https://github.com/))—the world's largest open
    source repository, and one of the best code collaboration platforms
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub ([https://github.com/](https://github.com/))—世界上最大的开源代码库，也是最好的代码协作平台之一
- en: GitLab ([https://about.gitlab.com](https://about.gitlab.com))—feature-packed
    code collaboration platforms with cloud and on-premises deployment options
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitLab ([https://about.gitlab.com](https://about.gitlab.com))—功能丰富的代码协作平台，提供云端和本地部署选项
- en: Atlassian Bitbucket ([https://bitbucket.org/](https://bitbucket.org/))—code
    collaboration solution from Atlassian, which integrates well with their other
    products, Jira issue tracker and Confluence wiki
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Atlassian Bitbucket ([https://bitbucket.org/](https://bitbucket.org/))—来自 Atlassian
    的代码协作解决方案，能够与其其他产品如 Jira 问题追踪器和 Confluence Wiki 良好集成
- en: 'For deploying your code, see these:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 关于部署代码，请参考以下内容：
- en: Docker ([https://www.docker.com/](https://www.docker.com/))—a tool for managing
    containers and packaging your code into an isolated portable environment that
    could be easily deployed and scaled on any Linux machine
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker ([https://www.docker.com/](https://www.docker.com/))—一个用于管理容器并将代码打包到一个独立的可移植环境中的工具，这些环境可以轻松地在任何
    Linux 机器上进行部署和扩展
- en: Kubernetes ([https://kubernetes.io/](https://kubernetes.io/))—a container orchestration
    platform that automates deployment, scaling, and the management of containerized
    applications
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes ([https://kubernetes.io/](https://kubernetes.io/))—一个容器编排平台，自动化容器化应用程序的部署、扩展和管理
- en: Ansible ([https://www.ansible.com/](https://www.ansible.com/) )—a configuration
    management and automation tool that's handy to use for deployment automation and
    configuration if you do not use containers in your deployment setup
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ansible ([https://www.ansible.com/](https://www.ansible.com/))—一款配置管理和自动化工具，对于不使用容器的部署环境，部署自动化和配置管理非常实用
- en: Summary
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered ModelOps – a set of practices for automating a common
    set of operations that arise in data science projects. We explored how ModelOps
    relates to DevOps and described major steps in the ModelOps pipeline. We looked
    at strategies for managing code, versioning data, and sharing project environments
    between team members. We also examined the importance of experiment tracking and
    automated testing for data science projects. As a conclusion, we outlined the
    full CI/CD pipeline with continuous model training and explored a set of tools
    that can be used to build such pipelines.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 ModelOps —— 一套用于自动化数据科学项目中常见操作的实践方法。我们探讨了 ModelOps 与 DevOps 的关系，并描述了
    ModelOps 流水线中的主要步骤。我们讨论了管理代码、版本控制数据以及在团队成员之间共享项目环境的策略。我们还研究了实验跟踪和自动化测试在数据科学项目中的重要性。最后，我们概述了完整的
    CI/CD 流水线与持续模型训练，并探索了一套可用于构建此类流水线的工具。
- en: In the next chapter, we will look at how to build and manage a data science
    technology stack.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何构建和管理数据科学技术栈。
