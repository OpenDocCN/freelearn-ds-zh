- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Using CI/CD to Automate Model Retraining and Redeployment
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CI/CD自动化模型再训练和重新部署
- en: Having explored various statistical tests in [*Chapter 9*](B17875_09.xhtml#_idTextAnchor129),
    courtesy of diverse open source libraries on Databricks and their integration
    with MLflow, we will now focus on an integral component of MLOps on Databricks.
    In this chapter, we will look at how Databricks unifies DevOps, DataOps, and ModelOps
    all in a single platform.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第9章*](B17875_09.xhtml#_idTextAnchor129)中，我们通过Databricks上多种开源库及其与MLflow的集成，探讨了多种统计测试，现在我们将重点关注Databricks上MLOps的一个核心组成部分。在本章中，我们将研究Databricks如何将DevOps、DataOps和ModelOps统一到一个平台中。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to MLOps
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLOps简介
- en: Fundamentals of MLOps and deployment patterns
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLOps基础与部署模式
- en: Let’s understand what MLOps is.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来理解一下什么是MLOps。
- en: Introduction to MLOps
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLOps简介
- en: 'MLOps serves as a multidisciplinary approach that merges the principles of
    DevOps, ModelOps, and DataOps to facilitate the end-to-end life cycle of ML projects.
    It aims to streamline the transition from model development to deployment, while
    also ensuring effective monitoring and management. In this framework, we have
    the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps作为一种多学科的方法，融合了DevOps、ModelOps和DataOps的原则，旨在促进机器学习项目的端到端生命周期。它的目标是简化从模型开发到部署的过渡，同时确保有效的监控和管理。在这一框架中，我们有以下内容：
- en: '**DevOps**: This focuses on the continuous integration and deployment of code,
    aiming for quicker releases and more reliable software'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DevOps**：专注于代码的持续集成和部署，旨在实现更快的发布和更可靠的软件。'
- en: '**ModelOps**: This specializes in managing ML models, ensuring they are effectively
    trained, validated, and deployed'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ModelOps**：专注于管理ML模型，确保它们经过有效的训练、验证和部署。'
- en: '**DataOps**: This deals with data management practices, encompassing everything
    from data collection and preprocessing to storage and analytics'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataOps**：涉及数据管理实践，涵盖从数据收集、预处理到存储和分析的各个方面。'
- en: 'MLOps improves the performance, stability, and long-term efficiency of ML systems.
    There are two primary risks that MLOps can help mitigate for your use case and
    industry:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps提高了ML系统的性能、稳定性和长期效率。MLOps可以帮助减轻你的用例和行业中的两大主要风险：
- en: '**Technical risks**: These result from poorly managed models that are not performing
    as expected. Without MLOps implementing your infrastructure and pipeline to train
    new models and redeploy them in the production environment, it may be very fragile.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**技术风险**：这些风险来源于管理不当的模型，这些模型未能按预期表现。若没有MLOps来实施你的基础设施和管道以训练新模型并将其重新部署到生产环境中，它可能非常脆弱。'
- en: '**Compliance risks**: If you are part of a regulated industry and must keep
    track of the new regulations and compliances to ensure you are not violating them,
    MLOps can help mitigate them.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规风险**：如果你属于受监管行业，必须跟踪新的法规和合规要求以确保不违反它们，MLOps可以帮助减轻这些风险。'
- en: Through automation, MLOps can also reduce and catch errors before getting to
    the production environment and reduce the time to market to launch and maintain
    products reliant on the most updated models for your use case.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自动化，MLOps还可以在进入生产环境之前减少并捕捉错误，缩短推出和维护依赖于最新模型的产品的上市时间。
- en: Now, let’s look at Databricks as a platform, which allows you to reduce the
    risks outlined previously and helps improve the long-term efficiency of your teams
    and your ML projects.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下Databricks作为一个平台，它可以帮助减少之前提到的风险，并帮助提高团队和ML项目的长期效率。
- en: One unique part of Databricks is that it is a *data-centric AI platform*. As
    part of this AI platform, Databricks uniquely provides all the necessary components
    needed to manage the data, models, and code that are part of the ML projects.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks的一个独特之处在于它是一个*数据驱动的AI平台*。作为这个AI平台的一部分，Databricks独特地提供了管理ML项目中数据、模型和代码所需的所有必要组件。
- en: 'To elucidate how Databricks facilitates MLOps, the following figure illustrates
    the platform’s integration capabilities with various tools and services on the
    Databricks Lakehouse platform:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了阐明Databricks如何促进MLOps，以下图表展示了该平台与Databricks湖仓平台上的各种工具和服务的集成能力：
- en: '![Figure 10.1 – Databricks’ data-centric platform and its components](img/B17875_10_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – Databricks的数据驱动平台及其组件](img/B17875_10_01.jpg)'
- en: Figure 10.1 – Databricks’ data-centric platform and its components
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – Databricks的数据驱动平台及其组件
- en: Note
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Courtesy of Databricks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由Databricks提供。
- en: Next, we’ll explore how Delta Lake serves as a pivotal technology that bridges
    the gap between robust data storage and ML readiness.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨Delta Lake如何作为一项关键技术，弥合强大数据存储与机器学习准备之间的差距。
- en: Delta Lake – more than just a data lake
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake —— 不仅仅是一个数据湖
- en: When it comes to managing complex data ecosystems, Databricks offers Delta Lake,
    a comprehensive open source storage layer that we discussed briefly in [*Chapter
    1*](B17875_01.xhtml#_idTextAnchor014). For more specialized reading, there are
    other detailed books on this topic, written by my esteemed colleagues, listed
    in the *Further reading* section of this chapter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在管理复杂的数据生态系统时，Databricks提供了Delta Lake，这是一层综合性的开源存储层，我们在[*第1章*](B17875_01.xhtml#_idTextAnchor014)中简要讨论过它。对于更深入的阅读，我的几位尊敬的同事也撰写了关于这一主题的其他详细书籍，它们列在本章的*进一步阅读*部分。
- en: Delta Lake stands out for enhancing the reliability, scalability, and performance
    of big data processing frameworks, particularly Apache Spark. Developed by Databricks,
    it equips data lakes with **Atomicity, Consistency, Isolation,** and **Durability**
    (**ACID**) transactions and robust schema enforcement capabilities. This is particularly
    crucial because clean and reliable data is not merely an advantage but a prerequisite
    for any serious data engineering or ML initiative.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake凭借增强大数据处理框架，特别是Apache Spark的可靠性、可扩展性和性能而脱颖而出。由Databricks开发，它为数据湖提供**原子性、一致性、隔离性**和**持久性**（**ACID**）事务及强大的模式强制执行能力。这一点尤为关键，因为干净且可靠的数据不仅仅是一个优势，更是任何严肃数据工程或机器学习计划的前提条件。
- en: Why the need for cleaner data and robust data engineering pipelines?
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么需要更清洁的数据和强大的数据工程管道？
- en: Having clean data in Delta Lake and robust data engineering pipelines is not
    just a matter of operational efficiency but a strategic imperative. Data quality
    directly impacts ML model accuracy, predictive power, and, ultimately, business
    outcomes. Inconsistent or noisy data can mislead algorithms, leading to incorrect
    insights and poor decisions. By enforcing strict schema and providing ACID transactions,
    Delta Lake elevates data lakes from being simple storage repositories to agile,
    data-ready platforms that can handle the intricacies of ML algorithms effectively.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在Delta Lake中拥有干净的数据和强大的数据工程管道，不仅是运营效率的问题，更是战略上的必要条件。数据质量直接影响机器学习模型的准确性、预测能力，最终影响商业结果。不一致或噪声数据可能误导算法，导致错误的洞察和决策。通过强制执行严格的模式并提供ACID事务，Delta
    Lake将数据湖从单纯的存储库提升为能够有效处理机器学习算法复杂性的敏捷数据平台。
- en: Efficient pipelines are equally important. They accelerate data flow from the
    point of ingestion to insights and model deployment. Slow or broken pipelines
    can bottleneck ML projects, costing organizations both time and money. Delta Lake’s
    transactional capabilities and metadata management help build pipelines that are
    not just efficient but also resilient and future-proof.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的管道同样至关重要。它们加速了从数据采集到洞察和模型部署的流动。缓慢或中断的管道会成为机器学习项目的瓶颈，导致组织在时间和资金上的损失。Delta
    Lake的事务能力和元数据管理有助于构建不仅高效，而且具有韧性和未来适应性的管道。
- en: Role of access control in ML modeling
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 访问控制在机器学习建模中的作用
- en: As ML becomes integral to business processes, the requirement for secure and
    controlled data access intensifies. Delta Lake’s **role-based access controls**
    (**RBACs**) come into play here, integrating seamlessly with organizational identity
    management systems. This ensures that sensitive data is only accessible to authorized
    personnel, thereby adding a security layer that helps in meeting regulatory compliance
    requirements and safeguarding the integrity of ML models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习（ML）成为业务流程的核心，数据访问的安全性和控制需求愈加迫切。在这里，Delta Lake的**基于角色的访问控制**（**RBACs**）发挥了重要作用，与组织身份管理系统无缝集成。这确保了敏感数据仅能被授权人员访问，从而增加了一层安全保障，有助于满足合规性要求并保护机器学习模型的完整性。
- en: 'The key features of Delta Lake include the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake的主要特点包括以下几点：
- en: '**ACID transactions**: Delta Lake ensures atomicity, consistency, isolation,
    and durability for data operations, allowing concurrent reads and writes. It provides
    transactional guarantees, so you can confidently perform complex data manipulations.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ACID事务**：Delta Lake确保数据操作的原子性、一致性、隔离性和持久性，允许并发读取和写入。它提供事务性保证，让你可以放心地执行复杂的数据操作。'
- en: '**Schema evolution**: Delta Lake supports schema enforcement, allowing you
    to specify and evolve a schema for your data. It enforces data quality by rejecting
    writes with incompatible schemas and provides schema evolution capabilities to
    handle schema changes over time.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式演化**：Delta Lake支持模式强制执行，允许你为数据指定并演化模式。它通过拒绝写入不兼容的模式来强制执行数据质量，并提供模式演化功能来处理随时间变化的模式。'
- en: '**Time travel**: Delta Lake maintains full historical versions of data, enabling
    you to query and analyze data at any point in time. You can easily track changes
    and compare different versions of the data, which is valuable for auditing, debugging,
    and reproducing analyses.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间旅行**：Delta Lake保持数据的完整历史版本，允许你在任何时间点查询和分析数据。你可以轻松跟踪变更并比较数据的不同版本，这对于审计、调试和重现分析非常有价值。'
- en: '**Optimized data processing**: Delta Lake leverages advanced indexing and caching
    mechanisms to optimize query performance. It uses statistics and optimizations
    to skip unnecessary data during query execution, resulting in faster response
    times.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化的数据处理**：Delta Lake利用先进的索引和缓存机制来优化查询性能。它通过使用统计信息和优化手段，在查询执行过程中跳过不必要的数据，从而提高响应速度。'
- en: '**Data lake metadata management**: Delta Lake stores metadata in a transaction
    log, enabling automatic schema discovery and efficient management of table metadata.
    It provides data lineage information, making it easier to understand the flow
    and transformation of data.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据湖元数据管理**：Delta Lake将元数据存储在事务日志中，实现自动模式发现和高效的表元数据管理。它提供数据血缘信息，使得理解数据流动和转换过程变得更加容易。'
- en: Delta Lake is highly compatible with Apache Spark, allowing you to leverage
    Spark’s robust analytics capabilities on top of your data lake. It has gained
    popularity in data lake architectures, enabling data engineers and scientists
    to build robust, scalable, and reliable data processing pipelines.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake与Apache Spark高度兼容，允许你在数据湖上利用Spark强大的分析能力。它在数据湖架构中获得了广泛的应用，使得数据工程师和科学家能够构建稳健、可扩展和可靠的数据处理管道。
- en: Next, let’s explore the seamless integration of MLflow within the Databricks
    platform, which offers robust capabilities for end-to-end model management. We’ll
    also delve into the emerging domain of ModelOps.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨MLflow在Databricks平台中的无缝集成，它提供了端到端模型管理的强大功能。我们还将深入了解新兴领域——ModelOps。
- en: Comprehensive model management with Databricks MLflow
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Databricks MLflow进行全面的模型管理
- en: For managing models, Databricks provides managed MLflow, which we have already
    covered in the previous chapters in great depth.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在管理模型方面，Databricks提供了托管的MLflow，我们在之前的章节中已经深入讲解了这一部分内容。
- en: MLflow is an open source platform designed to simplify the ML life cycle. It
    provides a comprehensive set of tools and APIs for managing, tracking, and deploying
    ML models. MLflow was developed by Databricks and has gained significant adoption
    within the ML community.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow是一个开源平台，旨在简化机器学习生命周期。它提供了一整套工具和API，用于管理、追踪和部署机器学习模型。MLflow由Databricks开发，并在机器学习社区中得到了广泛的采用。
- en: 'MLflow consists of four main components:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow包含四个主要组件：
- en: '**Tracking**: MLflow Tracking allows you to log and track experiments, parameters,
    metrics, and artifacts associated with your ML projects. It provides a unified
    interface to record and compare different experiment runs, making it easier to
    reproduce results and iterate on models. Tracking also supports integration with
    ML frameworks, such as TensorFlow, PyTorch, and scikit-learn.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪**：MLflow Tracking允许你记录并跟踪与机器学习项目相关的实验、参数、指标和工件。它提供了一个统一的界面，用于记录和比较不同实验的运行结果，使得重现结果和迭代模型变得更加容易。Tracking还支持与ML框架的集成，如TensorFlow、PyTorch和scikit-learn。'
- en: '**Projects**: MLflow Projects provides a standard format for packaging and
    sharing ML code. With MLflow Projects, you can define your ML code as a reusable
    project, including the code, dependencies, and configuration. This enables reproducibility
    and collaboration by ensuring your code can be easily executed in different environments.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**项目**：MLflow Projects提供了一种标准格式，用于打包和共享机器学习代码。通过MLflow Projects，你可以将机器学习代码定义为可重用的项目，包含代码、依赖项和配置。这确保了代码可以在不同环境中轻松执行，从而实现可重现性和协作。'
- en: '**Models**: MLflow Models enables you to manage and deploy ML models in various
    formats. It provides a simple model format that allows you to package models with
    their associated metadata and dependencies. You can then deploy these models in
    various deployment environments, such as batch scoring, real-time serving, or
    cloud platforms.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：MLflow模型使您能够以多种格式管理和部署机器学习模型。它提供了一种简单的模型格式，允许您将模型及其相关元数据和依赖项打包。然后，您可以将这些模型部署到各种部署环境中，如批量评分、实时服务或云平台。'
- en: '**Model Registry**: MLflow Model Registry is an optional component that adds
    model versioning, stage transitions, and collaboration features to MLflow Models.
    It allows you to keep track of different versions of your models, promotes models
    through different stages (for example, staging to production), and manages access
    control for different team members.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型注册表**：MLflow模型注册表是一个可选组件，它为MLflow模型增加了模型版本控制、阶段转换和协作功能。它使您能够跟踪不同版本的模型，推动模型通过不同的阶段（例如，从预发布到生产），并管理不同团队成员的访问控制。'
- en: MLflow supports multiple programming languages, including Python, R, and Java.
    It can be used both in local development environments and distributed clusters,
    making it suitable for different deployment scenarios.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow支持多种编程语言，包括Python、R和Java。它既可以在本地开发环境中使用，也可以在分布式集群中使用，适合不同的部署场景。
- en: As we transition from discussing model management with Databricks MLflow, let’s
    delve into the synergy between DevOps and MLOps, and how these principles are
    adapted and extended for robust ML pipelines within the Databricks ecosystem.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们从讨论使用Databricks MLflow进行模型管理过渡时，让我们深入探讨DevOps和MLOps之间的协同作用，以及这些原则如何在Databricks生态系统中被适配和扩展，以支持强大的ML管道。
- en: Integrating DevOps and MLOps for robust ML pipelines with Databricks
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Databricks中整合DevOps和MLOps，以构建强大的ML管道
- en: Databricks integrates with well-known Git providers such as GitHub, GitLab,
    and Azure DevOps for managing and executing DevOps workflows for our ML projects.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks与著名的Git服务提供商（如GitHub、GitLab和Azure DevOps）集成，用于管理和执行我们的ML项目的DevOps工作流。
- en: DevOps combines software **development** (**Dev**) and IT **operations** (**Ops**)
    to foster collaboration, automation, and continuous delivery. It aims to streamline
    software systems’ development, deployment, and maintenance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps结合了软件**开发**（**Dev**）和IT**运维**（**Ops**），促进了协作、自动化和持续交付。它旨在简化软件系统的开发、部署和维护。
- en: By incorporating DevOps principles, MLOps brings an added layer of efficiency
    to the life cycle of ML models. It fosters cohesive collaboration across every
    stage of the process – from developing and validating models to their deployment,
    monitoring, retraining, and redeployment.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入DevOps原则，MLOps为机器学习模型的生命周期增添了一层效率。它促进了各个阶段之间的紧密协作——从开发和验证模型到它们的部署、监控、再训练和重新部署。
- en: Within the sphere of MLOps, **continuous integration and continuous delivery**
    (**CI/CD**) emerge as critical elements. They underpin automation and drive continuous
    learning within ML systems. The ultimate goal of CI/CD is to seamlessly integrate
    data with source code versions, execute parallel tasks initiated by pertinent
    events, compile artifacts, and propagate releases to the production stage.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLOps领域，**持续集成和持续交付**（**CI/CD**）成为了关键要素。它们支撑了自动化，并推动了ML系统中的持续学习。CI/CD的最终目标是无缝地将数据与源代码版本集成，执行由相关事件触发的并行任务，编译工件，并将发布传播到生产阶段。
- en: Continuous learning through combining CI and CD principles is essential for
    the success of an ML system. Without it, the system risks stagnation and becoming
    a fruitless **Proof of Concept** (**POC**). Consistent learning and adaptation
    enable an ML model to provide valuable business insights.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合持续集成（CI）和持续交付（CD）原则进行持续学习，对于机器学习（ML）系统的成功至关重要。没有这一点，系统可能会陷入停滞，变成一个毫无成效的**概念验证**（**POC**）。持续的学习和适应使得机器学习模型能够提供有价值的商业洞察。
- en: 'To use ML models that continually improve, you need to understand CI, CD, and
    related methods. They work together and depend on each other, as shown in *Figure
    10**.2*:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用不断改进的机器学习模型，您需要理解CI、CD和相关方法。它们是相互协作且相互依赖的，如*图10.2*所示：
- en: '![Figure 10.2 – The relationship between continuous integration, continuous
    delivery, and continuous deployment](img/B17875_10_02.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 – 持续集成、持续交付和持续部署之间的关系](img/B17875_10_02.jpg)'
- en: Figure 10.2 – The relationship between continuous integration, continuous delivery,
    and continuous deployment
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 持续集成、持续交付和持续部署之间的关系
- en: 'Let''s understand these methods in a bit more detail:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微详细了解这些方法：
- en: '**Continuous integration**: In MLOps, CI is not merely about testing and validating
    code but also extends to testing and validating data, data schemas, and ML models.
    This ensures a more robust and reliable integration process tailored to ML needs.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续集成**：在MLOps中，CI不仅仅是测试和验证代码，还扩展到测试和验证数据、数据架构以及ML模型。这确保了一个更强大、更可靠的集成过程，专为ML需求量身定制。'
- en: '**Continuous delivery**: Beyond deploying a single software package or service,
    CD in an MLOps context is about deploying an entire system, which often includes
    an ML training pipeline and a model prediction service.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续交付**：在MLOps中，CD不仅仅是部署单一的软件包或服务，而是部署整个系统，通常包括ML训练管道和模型预测服务。'
- en: '**Continuous deployment**: Similar to traditional DevOps, CD in MLOps goes
    one step further by fully automating the release process and deploying new changes
    to production without human intervention.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续部署**：与传统的DevOps类似，MLOps中的CD更进一步，通过完全自动化发布过程，将新变化部署到生产环境中，而无需人工干预。'
- en: '**Continuous training**: Unique to ML systems, CT focuses on automatically
    retraining and serving models, ensuring they adapt and improve over time.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续训练**：这是机器学习系统独有的，CT专注于自动化重训练和服务模型，确保模型随着时间的推移不断适应和改进。'
- en: At the time of writing this book, Databricks is working on a new feature called
    the MLOps Stack, which provides a template to structure complex ML projects for
    CI/CD integration with Git providers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写本书时，Databricks正在开发一个名为MLOps Stack的新功能，它提供了一个模板，用于将复杂的ML项目结构化，以便与Git提供者进行CI/CD集成。
- en: For further details regarding the MLOps Stack, you are encouraged to peruse
    *MLOps Stack* on GitHub ([https://github.com/databricks/mlops-stack](https://github.com/databricks/mlops-stack)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 想要了解更多关于MLOps Stack的细节，建议访问*GitHub上的MLOps Stack* ([https://github.com/databricks/mlops-stack](https://github.com/databricks/mlops-stack))。
- en: We will not be covering the MLOps Stack in this chapter; instead, we will cover
    another approach to building your MLOps pipelines on Databricks based on utilizing
    what we have learned so far.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将不会涉及MLOps Stack；相反，我们将介绍另一种基于迄今为止所学知识，在Databricks上构建MLOps管道的方法。
- en: Let’s dive deeper and understand the fundamentals of MLOps and the various deployment
    paradigms.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨并理解MLOps的基本原理和各种部署模式。
- en: Fundamentals of MLOps and deployment patterns
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLOps基础与部署模式
- en: To effectively manage MLOps, it’s essential to first familiarize ourselves with
    its underlying terminology and structure. This includes understanding the roles
    and responsibilities associated with various operational environments – namely,
    **development** (**dev**), staging, and **production** (**prod**). Let’s dissect
    what these environments signify in a practical MLOps framework.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效管理MLOps，首先必须熟悉其基础术语和结构。这包括理解与各种操作环境相关的角色和职责——即**开发**（**dev**）、预生产环境和**生产**（**prod**）。让我们从实际的MLOps框架中解析这些环境的含义。
- en: 'Within any ML project, there are three pivotal assets:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何ML项目中，都有三个关键资产：
- en: '**Code base**: This serves as the project’s blueprint. It contains all the
    source code related to data preprocessing, model training, evaluation, and deployment.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码库**：这作为项目的蓝图，包含与数据预处理、模型训练、评估和部署相关的所有源代码。'
- en: '**Data**: This includes the datasets that are used for training, validating,
    and testing the model. The quality and availability of this data directly influence
    the model’s efficacy.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据**：这包括用于训练、验证和测试模型的数据集。数据的质量和可用性直接影响模型的效果。'
- en: '**Trained model**: This is the culmination of your ML workflow, a model that
    has been trained, evaluated, and prepared for inference.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练模型**：这是你ML工作流的最终成果，一个经过训练、评估并准备好进行推理的模型。'
- en: 'Each of these assets goes through distinct phases – development, testing, and
    deployment – which are often segregated into separate environments:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些资产每个都经历不同的阶段——开发、测试和部署——通常这些阶段被分隔到不同的环境中：
- en: '**Development environment (dev)**: This is where the initial code is written
    and tested. It’s generally the most accessible in terms of code and data but has
    the least stringent quality and testing requirements.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发环境（dev）**：这是初始代码编写和测试的地方。通常在代码和数据方面最为便捷，但质量和测试要求最为宽松。'
- en: '**Staging environment**: This serves as an intermediate space for additional
    testing and quality assurance before the project moves to production.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预发布环境**：这是一个中间空间，用于在项目进入生产阶段之前进行额外的测试和质量保证。'
- en: '**Production environment (prod)**: This is the most restrictive environment
    where the finalized assets are deployed. It has the highest quality and security
    requirements and is the least accessible for direct interactions. The following
    figure provides a visual representation of the key assets in MLOps, as well as
    the organizational structure of different environments. It illustrates the life
    cycle of these assets as they progress through development, testing, and, ultimately,
    production:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生产环境（prod）**：这是最具限制性的环境，最终版资源会部署在此。它有最高的质量和安全要求，且对直接交互的访问最为有限。下图提供了 MLOps
    中关键资源的可视化表示，并展示了不同环境的组织结构。它展示了这些资源从开发、测试到最终生产的生命周期：'
- en: '![Figure 10.3 – The various assets related to an ML project and its environments](img/B17875_10_03.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 – 与机器学习项目及其环境相关的各种资源](img/B17875_10_03.jpg)'
- en: Figure 10.3 – The various assets related to an ML project and its environments
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 与机器学习项目及其环境相关的各种资源
- en: Note
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding figure is courtesy of Databricks.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图由 Databricks 提供。
- en: 'The following figure illustrates the accessibility levels and quality requirements
    across these environments:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这些环境中的可访问性级别和质量要求：
- en: '![Figure 10.4 – The various environments and their openness to accessibility](img/B17875_10_04.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4 – 各种环境及其对可访问性的开放程度](img/B17875_10_04.jpg)'
- en: Figure 10.4 – The various environments and their openness to accessibility
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – 各种环境及其对可访问性的开放程度
- en: Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding figure is courtesy of Databricks.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图由 Databricks 提供。
- en: With Databricks, you have the flexibility to structure these dev, staging, and
    prod environments in various ways to meet your project’s specific needs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Databricks，你可以灵活地以多种方式结构化这些开发、预发布和生产环境，以满足项目的特定需求。
- en: It’s important to note that the theoretical separation of dev, staging, and
    prod environments serves as a guideline for best practices in MLOps. However,
    the real-world implementation can vary significantly based on your organizational
    needs, workflow, and technological capabilities.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，开发、预发布和生产环境的理论分离只是 MLOps 最佳实践的指南。然而，实际的实施可能会根据你的组织需求、工作流程和技术能力有显著差异。
- en: In the following section, we will delve into multiple approaches for deploying
    Databricks workspaces to better align your dev, staging, and prod environments
    with your specific organizational requirements.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将深入探讨多种部署 Databricks 工作空间的方法，以便更好地将你的开发、预发布和生产环境与特定的组织需求对接。
- en: 'The following figure showcases three distinct deployment patterns designed
    to set up your dev, QA, and prod environments effectively:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了三种不同的部署模式，旨在有效地设置你的开发、质量保证和生产环境：
- en: '![Figure 10.5 – Various Databricks environment deployment approaches](img/B17875_10_05.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.5 – 各种 Databricks 环境部署方式](img/B17875_10_05.jpg)'
- en: Figure 10.5 – Various Databricks environment deployment approaches
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – 各种 Databricks 环境部署方式
- en: Note
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This is the source for the preceding figure: *The Big Book* *of MLOps*.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述图的来源：《*MLOps 大全*》。
- en: Let’s understand these patterns one by one.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一了解这些模式。
- en: Navigating environment isolation in Databricks – multiple strategies for MLOps
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Databricks 中导航环境隔离 – 多种 MLOps 策略
- en: To devise a robust MLOps strategy, you need to consider not only the type of
    assets involved but also the environment where they reside – dev, staging, or
    prod. Each environment offers varying levels of accessibility, testing rigor,
    and data security, informed by organizational size, governance policies, and security
    requirements.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了制定一个健壮的 MLOps 策略，你不仅需要考虑涉及的资源类型，还要考虑它们所在的环境——开发、预发布或生产。每个环境提供不同级别的可访问性、测试严格性和数据安全性，这些都受到组织规模、治理政策和安全要求的影响。
- en: Multiple cloud accounts
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多个云账户
- en: For large organizations governed by stringent rules and regulations, separating
    dev, staging, and prod environments across distinct cloud accounts is a common
    practice. In such a setup, each cloud account will host its own Databricks workspace.
    This architecture ensures isolation at both the cloud account and network levels,
    while also potentially increasing costs due to duplicated resources and data storage.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于受严格规则和法规约束的大型组织，通常会将开发、预发布和生产环境分隔到不同的云账户中。在这种配置下，每个云账户将托管自己的 Databricks 工作区。该架构在云账户和网络级别上确保了隔离，但由于资源和数据存储的重复，可能会增加成本。
- en: A single cloud account with multiple Databricks workspaces
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单一云账户与多个 Databricks 工作区
- en: Alternatively, smaller organizations or projects may opt for a single cloud
    account containing multiple Databricks workspaces. Each workspace is deployed
    into its own network and is isolated at that level. While more cost-effective,
    this approach still allows for sufficient isolation and can align well with organizational
    data governance policies.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，较小的组织或项目可能会选择在一个云账户中包含多个 Databricks 工作区。每个工作区都部署在自己的网络中，并在该层级上实现隔离。虽然这种方式更具成本效益，但仍然能保证足够的隔离，并且能与组织的数据治理政策保持一致。
- en: A single cloud account with a single Databricks workspace
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单一云账户与单一 Databricks 工作区
- en: Even within a single cloud account, Databricks provides the capability to enforce
    strict isolation between different roles and projects. Features such as RBAC,
    permissions, and native data governance tools such as Unity Catalog allow for
    effective segregation of access within a single workspace.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 即便是在同一个云账户内，Databricks 也提供了严格隔离不同角色和项目的能力。像 RBAC、权限以及 Unity Catalog 等原生数据治理工具，使得在单一工作区内有效地进行访问隔离成为可能。
- en: 'Having explored various approaches for organizing our dev, staging, and prod
    environments in Databricks, it’s time to turn our attention to another pivotal
    aspect of MLOps: the asynchronous nature of life cycles in ML projects. This stands
    in contrast to traditional software DevOps, where code and application updates
    usually happen in lockstep.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了多种组织 Databricks 开发、预发布和生产环境的方式后，是时候关注 MLOps 另一个关键方面：机器学习项目中生命周期的异步性。这与传统的软件
    DevOps 有所不同，后者中的代码和应用更新通常是同步进行的。
- en: Consider a deployed **large language model** (**LLM**) as a case in point. The
    sheer complexity and size of such models can make retraining a formidable challenge.
    You may find that while the data engineering code sees monthly iterations, the
    training code for the model itself remains static for an extended duration.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以部署的**大型语言模型**（**LLM**）为例。这类模型的复杂性和规模使得重新训练成为一项艰巨的挑战。你可能会发现，尽管数据工程代码每月都会进行迭代，但模型本身的训练代码在长时间内保持静态。
- en: On the flip side, think about a churn prediction model. Here, automatic retraining
    might be scheduled monthly using fresh datasets. If the newly trained model outperforms
    its predecessor, it immediately gets moved to production, all without requiring
    any changes to the existing code base.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，考虑一下一个 churn 预测模型。在这种情况下，可能会使用最新数据集每月安排自动重新训练。如果新训练的模型优于前一个模型，它会立即被迁移到生产环境中，而无需对现有代码库进行任何更改。
- en: Navigating asynchronous life cycles
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应对异步生命周期
- en: Given the incongruent update cycles for ML models and code, adopting strategies
    to manage these asynchronicities becomes imperative. You might employ techniques
    such as canary deployments for safer model rollouts, or opt for blue-green deployments
    to ensure smoother rollbacks. Automated monitoring systems and alert mechanisms
    are equally important, serving as early warning systems for model degradation
    or operational issues, thus allowing for quick remediation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于机器学习模型和代码更新周期的不一致，采取管理这些异步性的方法变得至关重要。你可能会采用如金丝雀发布等技术以确保模型的安全发布，或选择蓝绿部署来确保更顺畅的回滚。自动化监控系统和警报机制同样重要，能够作为模型退化或操作问题的早期预警系统，从而实现快速修复。
- en: Fiscal and regulatory considerations
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 财务和监管考虑事项
- en: Beyond technical aspects, MLOps also encompasses financial and compliance variables.
    Cost considerations can’t be overlooked – both for data storage and computational
    resources. Furthermore, data lineage is essential for keeping tabs on data movement
    through your pipeline, which not only aids in debugging but is invaluable for
    compliance and auditing purposes. Similarly, data versioning is indispensable
    when it comes to model reproducibility, an especially crucial feature for models
    undergoing frequent retraining.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 除了技术方面，MLOps 还涵盖了财务和合规变量。不能忽视成本考虑 – 包括数据存储和计算资源。此外，数据血统对于通过您的流水线跟踪数据移动至关重要，这不仅有助于调试，而且对于合规和审计目的至关重要。同样，数据版本控制在模型可重现性方面至关重要，尤其是对于频繁重新训练的模型而言。
- en: With this nuanced understanding, we are better equipped to manage the complexities
    that arise from asynchronous updates in the ML life cycle, in the context of Databricks
    or any MLOps platform.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种微妙的理解，我们能更好地处理 ML 生命周期中异步更新带来的复杂性，特别是在 Databricks 或任何 MLOps 平台的背景下。
- en: Now, let’s take a look at the various ML deployment paradigms that you can utilize.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看你可以利用的各种 ML 部署范式。
- en: Understanding ML deployment patterns
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 ML 部署模式
- en: 'The ultimate goal of any ML project is to get our ML model into production.
    Depending on what kind of use case we are catering to and how sophisticated our
    ML engineering team is, there are two broad ML deployment approaches:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 任何 ML 项目的最终目标是将我们的 ML 模型投入生产。根据我们正在服务的用例类型和我们的 ML 工程团队的复杂性，有两种广义的 ML 部署方法。
- en: The deploy models approach
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署模型方法
- en: The deploy code approach
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署代码方法
- en: Let’s understand these approaches one by one.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个理解这些方法。
- en: The deploy models approach
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署模型方法
- en: The model deployment workflow adheres to a structured methodology, beginning
    in a development environment where code for training the ML model is both crafted
    and refined. After the model undergoes training and the optimal version is ascertained,
    it is formally registered within a specialized model registry. This is followed
    by a battery of integration tests to evaluate its performance and reliability.
    Upon successfully passing these assessments, the model is first elevated to a
    staging environment for further validation. Once it meets all requisite criteria,
    it is then deployed into the production environment.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署工作流遵循结构化方法论，从开发环境开始，其中包括用于训练 ML 模型的代码的制定和完善。在模型经过训练并确定最佳版本之后，它将正式注册在专用模型注册表中。接着进行一系列集成测试以评估其性能和可靠性。成功通过这些评估后，模型首先升级到暂存环境进行进一步验证。一旦满足所有必要条件，它就会被部署到生产环境中。
- en: 'The following figure offers a graphical depiction of this multi-stage approach:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了这种多阶段方法的图形化描述：
- en: '![Figure 10.6 – The deploy models approach](img/B17875_10_06.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6 – 部署模型方法](img/B17875_10_06.jpg)'
- en: Figure 10.6 – The deploy models approach
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 部署模型方法
- en: Note
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This is the source for the preceding figure: *The Big Book* *of MLOps*.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前述图像的来源：*MLOps 大书*。
- en: Throughout this book, all the notebooks we have utilized so far focus on this
    particular deployment approach. It is a popular choice among companies and teams,
    especially when the ML team comprises individuals with a background in data science
    rather than traditional software engineering. This approach offers simplicity
    and serves as a great starting point for ML projects.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，迄今为止我们使用的所有笔记本都集中在这种特定的部署方法上。这是公司和团队特别是 ML 团队成员中，尤其是数据科学背景而非传统软件工程背景的人的流行选择。这种方法提供了简单性，并且是
    ML 项目的一个很好的起点。
- en: 'The following figure showcases the entire end-to-end MLOps life cycle for the
    deploy models approach:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了部署模型方法的完整端到端 MLOps 生命周期：
- en: '![Figure 10.7 – The reference architecture and workflow for deploying a model
    from development to production using the deploy models approach](img/B17875_10_07.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7 – 使用部署模型方法从开发到生产部署模型的参考架构和工作流程](img/B17875_10_07.jpg)'
- en: Figure 10.7 – The reference architecture and workflow for deploying a model
    from development to production using the deploy models approach
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 使用部署模型方法从开发到生产部署模型的参考架构和工作流程
- en: Note
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding figure is courtesy of Databricks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图像由 Databricks 提供。
- en: 'In this MLOps workflow, data engineers, data scientists, and ML engineers collaborate
    to perform various steps to ensure the successful development, deployment, and
    monitoring of ML models. Here is a breakdown of the responsibilities and tasks
    performed by each role:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个MLOps工作流程中，数据工程师、数据科学家和ML工程师协作完成各种步骤，确保ML模型的成功开发、部署和监控。以下是每个角色的责任和任务分解：
- en: '**Data engineers**:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据工程师**：'
- en: Collect data from diverse sources such as databases, cloud storage, and sensors,
    ensuring reliability and quality
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从各种来源收集数据，如数据库、云存储和传感器，确保数据的可靠性和质量。
- en: Cleanse and preprocess the data, handling tasks such as removing duplicates,
    handling missing values, and transforming data into a format suitable for ML algorithms
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗和预处理数据，处理如删除重复数据、处理缺失值，并将数据转换为适合ML算法的格式。
- en: Store and manage data in a data warehouse or Delta Lake, ensuring accessibility
    and efficient utilization by data scientists and ML engineers
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据仓库或Delta Lake中存储和管理数据，确保数据科学家和ML工程师可以高效利用和访问这些数据。
- en: '**Data scientists**:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据科学家**：'
- en: Explore and analyze the data to gain insights into its characteristics, identifying
    relevant patterns and relationships.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索和分析数据，深入了解其特征，识别相关的模式和关系。
- en: Generate and register features into feature tables for reuse.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成并注册特征到特征表中以供重复使用。
- en: Develop and train ML models, employing various algorithms and techniques to
    achieve accurate predictions and desired outcomes. All the model runs and experiments
    are logged automatically into the MLflow tracking server on Databricks.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发和训练ML模型，采用各种算法和技术实现准确的预测和期望的结果。所有的模型运行和实验会自动记录到Databricks上的MLflow跟踪服务器中。
- en: Evaluate and assess the performance of the trained models using appropriate
    metrics and validation techniques.
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用适当的度量标准和验证技术评估和评估训练模型的性能。
- en: Select the most suitable model for deployment based on performance and business
    requirements. The best model is then registered in the Model Registry as a candidate
    model.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据性能和业务需求选择最合适的模型进行部署。最佳模型将被注册到模型注册表中，作为候选模型。
- en: '**ML engineers**:'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ML工程师**：'
- en: Deploy ML models into production environments, making them available for making
    predictions or decisions in real time
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将ML模型部署到生产环境中，使其能够实时进行预测或决策。
- en: Monitor the performance of deployed models, ensuring they operate optimally
    and detect any anomalies or drift in their behavior
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控已部署模型的性能，确保其正常运行，并检测任何异常或行为漂移。
- en: Update and retrain models as new data becomes available, maintaining model relevance
    and accuracy
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着新数据的到来，更新和重新训练模型，保持模型的相关性和准确性。
- en: All the notebooks that we covered as part of this book show this workflow.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中涵盖的所有笔记本都展示了这一工作流程。
- en: Now that we understand how the deploy models approach for MLOps works, let’s
    take a look at the deploy code approach.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了MLOps中模型部署方法的工作原理，接下来让我们看看部署代码的方法。
- en: The deploy code approach
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署代码的方法
- en: In the deploy code approach, we version control not only the code to train the
    ML models but also the code to create the feature tables. This approach works
    well when you have strict regulations to separate data access in each environment.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署代码的方法中，我们不仅对训练ML模型的代码进行版本控制，还对创建特征表的代码进行版本控制。当每个环境中对数据访问有严格规定时，这种方法非常有效。
- en: 'The data scientists develop code in the dev environment for feature engineering
    and model training. After a good candidate model is found, the dev branch code
    is committed to the staging branch, where automated unit tests are run. Again,
    we train the model in staging and perform our performance benchmark test. Once
    everything else looks good, we push the code to the main branch and the prod environment.
    Here, again, we retrain the model on the data in production:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家在开发环境中开发特征工程和模型训练的代码。在找到合适的候选模型后，开发分支代码被提交到暂存分支，在那里进行自动化单元测试。接着，我们在暂存环境中训练模型并进行性能基准测试。一旦一切正常，我们将代码推送到主分支和生产环境。在这里，我们再次在生产数据上重新训练模型：
- en: '![Figure 10.8 – The reference architecture and workflow for deploying a model
    from development to production using the deploy code approach](img/B17875_10_08.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图10.8 – 从开发到生产使用部署代码方法的模型参考架构和工作流程](img/B17875_10_08.jpg)'
- en: Figure 10.8 – The reference architecture and workflow for deploying a model
    from development to production using the deploy code approach
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 – 从开发到生产部署模型的参考架构和工作流，使用部署代码方法
- en: Note
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: 'This is the source for the preceding figure: *The Big Book* *of MLOps*.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面图表的来源：*《MLOps大书》*。
- en: 'The development process involves several stages, starting with the creation
    of code for the training model and feature engineering in the dev environment.
    The following figure showcases the deploy code workflow step by step in the dev
    environment:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 开发过程涉及几个阶段，从开发环境中为训练模型和特征工程创建代码开始。以下图展示了开发环境中部署代码工作流的逐步过程：
- en: '![Figure 10.9 – The deploy code workflow within the development environment](img/B17875_10_09.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图10.9 – 开发环境中的部署代码工作流](img/B17875_10_09.jpg)'
- en: Figure 10.9 – The deploy code workflow within the development environment
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 – 开发环境中的部署代码工作流
- en: Note
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: Courtesy of Databricks
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 由Databricks提供
- en: 'Let’s understand these steps one by one:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步了解这些步骤：
- en: '**Data access points**: In development settings, data scientists typically
    have read-only permissions for production data. For compliance reasons, access
    may be limited to sanitized or duplicate versions of this data. A separate development
    storage is also available for read-write operations, facilitating experimental
    work.'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据访问点**：在开发环境中，数据科学家通常对生产数据具有只读权限。出于合规性原因，访问可能仅限于这些数据的已清洗或复制版本。还可以为读写操作提供单独的开发存储，以便进行实验性工作。'
- en: '**Preliminary data investigation** (**PDI**): Data scientists use an iterative,
    interactive method for data exploration, leveraging notebooks, visual charts,
    and Databricks SQL. This step is often a standalone process and not usually part
    of deployable pipelines.'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初步数据调查**（**PDI**）：数据科学家使用迭代、交互式方法进行数据探索，利用笔记本、可视化图表和Databricks SQL。此步骤通常是独立的过程，通常不属于可部署的管道的一部分。'
- en: '**Source code management**: All ML system code resides in a version control
    repository. Data scientists work on a development branch within this Git repository.
    Code can be synchronized with the Databricks workspace via Databricks Repos.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**源代码管理**：所有的机器学习系统代码都存储在版本控制库中。数据科学家在此Git库中的开发分支上工作。代码可以通过Databricks Repos与Databricks工作区进行同步。'
- en: '**Enhance feature datasets**: This pipeline ingests data from both raw and
    existing feature tables, outputting it to tables within Feature Store. This step
    includes two main tasks:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**增强特征数据集**：此管道从原始和现有特征表中摄取数据，并将其输出到特征库中的表。这一步包括两个主要任务：'
- en: '**Quality assurance**: Here, the data is validated to ensure it meets quality
    standards.'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**质量保证**：在此，数据会被验证以确保其符合质量标准。'
- en: '**Feature construction**: Code is written or updated by data scientists to
    generate new features. Data may be pulled from Feature Store or other Lakehouse
    tables. These dev feature tables are used to build experimental models, and upon
    promotion to production, they update the corresponding production tables.'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征构建**：数据科学家编写或更新代码以生成新的特征。数据可能来自特征库或其他湖仓表。这些开发特征表用于构建实验模型，推广到生产后，它们会更新相应的生产表。'
- en: Management can be separate for feature pipelines if they are governed by different
    teams.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果特征管道由不同的团队管理，它们的管理可以是分开的。
- en: '**Model training pipeline**: Data scientists build pipelines for model training
    on either read-only production data or development-specific data. These pipelines
    may utilize feature tables from both the dev and prod environments:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型训练管道**：数据科学家在只读生产数据或特定于开发的数据上构建模型训练管道。这些管道可能利用来自开发和生产环境的特征表：'
- en: '**Tuning and training**: The training process sources data from feature stores
    and varying levels of Lakehouse tables while logging parameters, metrics, and
    artifacts in the MLflow tracking system.'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**调优与训练**：训练过程从特征库和不同层次的湖仓表中获取数据，同时在MLflow跟踪系统中记录参数、度量和工件。'
- en: '**Model storing**: After training and tuning have been finalized, the model
    is stored on the MLflow tracking server, capturing its association with the input
    data and the code.'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型存储**：在训练和调优完成后，模型存储在MLflow跟踪服务器上，记录其与输入数据和代码的关联。'
- en: When executed in staging or production, the model can be retrieved and registered
    for ongoing management and testing.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当在暂存或生产环境中执行时，模型可以被检索并注册以便进行持续管理和测试。
- en: '**Code finalization**: Once the development work on pipelines for features,
    training, and inference is complete, either the data scientist or the ML engineer
    commits these changes to the version control system from the development branch.'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**代码定稿**：一旦特征、训练和推理的管道开发工作完成，无论是数据科学家还是 ML 工程师都会将这些更改提交到版本控制系统中的开发分支。'
- en: Let’s move on and understand the workflow in the staging environment. The staging
    environment serves as the final testing ground for ML code before it transitions
    to production. It encompasses comprehensive testing of all pipeline components,
    including model training and feature engineering. ML engineers employ a CI pipeline
    to execute unit and integration tests. Successful completion results in a release
    branch, triggering the CI/CD system to initiate the production stage.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们了解一下预发布环境中的工作流程。预发布环境作为 ML 代码进入生产环境之前的最终测试场地，涵盖了所有管道组件的全面测试，包括模型训练和特征工程。ML
    工程师利用 CI 管道执行单元测试和集成测试。测试成功后，生成发布分支，触发 CI/CD 系统启动生产阶段。
- en: 'The following diagram provides a step-by-step visual guide to the workflow
    within the staging environment:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 下图为您提供了预发布环境工作流程的逐步可视化指南：
- en: '![Figure 10.10 – The deploy code workflow within the staging environment](img/B17875_10_10.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.10 – 预发布环境中的部署代码工作流程](img/B17875_10_10.jpg)'
- en: Figure 10.10 – The deploy code workflow within the staging environment
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 – 预发布环境中的部署代码工作流程
- en: Note
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Courtesy of Databricks
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 Databricks 的提供
- en: 'Let’s delve into each of these steps in detail:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解每个步骤：
- en: '**Initiate merge process**: The journey toward deployment commences when an
    ML engineer submits a merge request to the source control’s staging branch, often
    the “main” branch. This action sets off a **CI** workflow.'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**启动合并过程**：当 ML 工程师提交合并请求到源代码管理的预发布分支（通常是“main”分支）时，部署之旅开始。这一操作触发了 **CI** 工作流程。'
- en: '**Execute unit tests**: Within the CI framework, the source code is automatically
    compiled, and unit tests are initiated. Should these tests not succeed, the merge
    request gets dismissed. Note that unit tests operate in isolation from data or
    external services.'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**执行单元测试**：在 CI 框架中，源代码会自动编译并启动单元测试。如果这些测试未通过，合并请求将被拒绝。请注意，单元测试与数据或外部服务相互独立。'
- en: '**Conduct integration** **tests**: Following the unit tests, the CI mechanism
    proceeds to administer integration tests. These tests validate the compatibility
    and functionality of all pipelines, which encompasses feature engineering, model
    training, inference, and monitoring. The staging environment is designed to mirror
    the production setting as closely as feasible.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**进行集成测试**：在单元测试之后，CI 机制会执行集成测试。这些测试验证所有管道的兼容性和功能，包括特征工程、模型训练、推理和监控。预发布环境的设计尽可能接近生产环境。'
- en: To economize on test duration, concessions may be made between the thoroughness
    of testing and execution speed. For instance, smaller data subsets could be used,
    or fewer training cycles run. Depending on the model’s intended application, comprehensive
    load testing might be conducted at this stage.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了节省测试时间，可以在测试的彻底性与执行速度之间做出妥协。例如，可以使用较小的数据子集，或者减少训练周期。根据模型的预期应用，可能会在此阶段进行全面的负载测试。
- en: After successful completion of integration tests in the staging branch, the
    code becomes eligible for production deployment.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在预发布分支完成集成测试后，代码才有资格进行生产部署。
- en: '**Commit to the staging branch**: Should the tests be successful, the code
    merges into the staging branch. In case of test failure, the CI/CD system alerts
    the relevant parties and updates the merge (or pull) request with the results.'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提交到预发布分支**：如果测试成功，代码会合并到预发布分支。如果测试失败，CI/CD 系统会通知相关人员，并将测试结果更新到合并（或拉取）请求中。'
- en: Periodic integration tests can be scheduled on the staging branch, especially
    if it receives frequent updates from multiple contributors.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以定期安排集成测试，尤其是当预发布分支频繁收到多个贡献者的更新时。
- en: '**Establish a release branch**: Once the code has been validated and is ready
    for production deployment, the ML engineer forms a release branch. This action
    prompts the CI/CD system to refresh the production tasks.'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**建立发布分支**：当代码经过验证并准备好进行生产部署时，ML 工程师创建发布分支。此操作促使 CI/CD 系统刷新生产任务。'
- en: Lastly, let’s understand the production environment’s workflow.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们了解一下生产环境的工作流程。
- en: In the production environment, ML engineers oversee the deployment of ML pipelines
    that handle feature computation, model training, and testing, as well as prediction
    publishing and performance monitoring. A retraining mechanism operates on production
    data to keep the model up to date and optimized. Performance benchmarks are rigorously
    evaluated to ensure that the new model meets or exceeds the set standards. Data
    scientists typically lack write and compute permissions in this environment but
    maintain visibility into test outcomes, logs, model artifacts, and pipeline statuses
    to aid in diagnosing any production issues.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，ML工程师监督处理特征计算、模型训练与测试、预测发布和性能监控的ML管道的部署。一个再训练机制在生产数据上运行，以保持模型的更新和优化。性能基准经过严格评估，以确保新模型符合或超过设定的标准。在这个环境中，数据科学家通常没有写入和计算权限，但可以查看测试结果、日志、模型工件和管道状态，以帮助诊断生产中的问题。
- en: 'The following diagram offers a comprehensive, step-by-step visualization of
    the workflow processes in the production environment:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表提供了生产环境中工作流程的全面逐步可视化：
- en: '![Figure 10.11 – The deploy code workflow within the production environment](img/B17875_10_11.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.11 – 生产环境中部署代码的工作流程](img/B17875_10_11.jpg)'
- en: Figure 10.11 – The deploy code workflow within the production environment
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 – 生产环境中部署代码的工作流程
- en: Note
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Courtesy of Databricks
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢Databricks提供支持
- en: 'Let’s go through this workflow step by step:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地了解这个工作流程：
- en: '**Refresh feature data**: This phase involves ingesting new data from production
    and updating tables in Feature Store. This can be either a batch or real-time
    process and can be invoked by different triggers, such as schedules or continuous
    runs.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**刷新特征数据**：此阶段涉及从生产环境摄取新数据并更新特征存储中的表格。此过程可以是批处理或实时处理，并且可以通过不同的触发器启动，例如定时任务或连续运行。'
- en: '**Model training**:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型训练**：'
- en: '**Tuning and training**: The pipeline trains the production model on complete
    data and logs relevant metrics and parameters through autologging. Unlike the
    development stage, only top-performing algorithms and hyperparameters are considered
    to optimize time and performance.'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调优和训练**：管道在完整数据上训练生产模型，并通过自动记录日志相关的度量和参数。与开发阶段不同，只有表现最佳的算法和超参数才会被考虑，以优化时间和性能。'
- en: '**Model assessment**: The quality of the model is tested against a separate
    dataset from production. Test results and custom metrics are recorded.'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型评估**：模型质量通过与生产环境中的独立数据集进行测试。测试结果和自定义度量被记录。'
- en: '**Model registration**: Upon successful training, the model is registered with
    an initial status of “None” in Model Registry.'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型注册**：在成功训练后，模型以“None”初始状态在模型注册表中注册。'
- en: '**Automated deployment**:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自动化部署**：'
- en: '**Compliance verification**: The pipeline performs mandatory compliance checks,
    which can include human review for complex evaluations. The results are logged.'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规性验证**：管道执行强制的合规性检查，可能包括复杂评估的人为审查。结果将被记录。'
- en: '**Performance validation**: Models in the staging phase are compared against
    those in production to avert performance decay.'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能验证**：在预发布阶段的模型与生产环境中的模型进行比较，以避免性能退化。'
- en: '**Transition to production**: The model is advanced to the production stage,
    either manually or automatically, following satisfactory performance comparisons.'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过渡到生产环境**：模型在性能比较令人满意后，手动或自动地推进到生产阶段。'
- en: '**Real-time serving**: MLflow enables the model to be deployed for low-latency
    use cases. The deployed model fetches features and returns predictions for each
    incoming request.'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实时服务**：MLflow使模型能够部署到低延迟的用例中。部署的模型获取特征并为每个传入的请求返回预测结果。'
- en: '**Batch or stream inference**: For higher throughput or latency requirements,
    batch or stream-based inferences are processed. Predictions can be saved in various
    storage options, including message queues such as Apache Kafka.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**批量或流式推理**：对于更高的吞吐量或延迟要求，处理批量或流式推理。预测结果可以保存在各种存储选项中，包括消息队列，如 Apache Kafka。'
- en: '**Ongoing monitoring**:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**持续监控**：'
- en: '**Data feeding**: Logs from different inference types are ingested'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据喂送**：从不同推理类型中摄取日志'
- en: '**Performance and drift metrics**: Various quality and performance metrics
    are calculated'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能和漂移度量**：计算各种质量和性能度量'
- en: '**Metric reporting**: Metrics are saved for further analysis and alerting purposes'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**度量报告**：度量结果被保存以供进一步分析和警报用途'
- en: '**Retraining triggers**: Models can be automatically retrained based on a schedule
    or triggered by performance degradation.'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重新训练触发器**：模型可以根据时间表自动重新训练，或者根据性能下降触发重新训练。'
- en: Note
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Automating the retraining process can be complex and may require manual intervention
    to resolve issues identified through monitoring, such as data drift or performance
    degradation.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化重新训练过程可能会很复杂，并可能需要手动干预来解决通过监控发现的问题，例如数据漂移或性能下降。
- en: 'The following figure summarizes the various steps that are performed in various
    environments for the deploy code approach to ModelOps:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 下图总结了在不同环境中执行部署代码方法的ModelOps中的各种步骤：
- en: '![Figure 10.12 – The various steps performed in various environments for the
    deploy code approach to ModelOps](img/B17875_10_12.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.12 – 在不同环境中执行部署代码方法的ModelOps中的各种步骤](img/B17875_10_12.jpg)'
- en: Figure 10.12 – The various steps performed in various environments for the deploy
    code approach to ModelOps
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 – 在不同环境中执行部署代码方法的ModelOps中的各种步骤
- en: Note
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding figure is courtesy of Databricks.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图由Databricks提供。
- en: Overall, you have three environments. At the top, you have your Git workflow
    provider, which manages transitioning code from one environment to another. At
    the bottom, you have the data access layer or feature tables with data across
    different environments.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，您有三个环境。在顶部，您有Git工作流提供者，负责管理代码从一个环境到另一个环境的过渡。在底部，您有数据访问层或跨不同环境的特征表的数据。
- en: The important point to keep in mind here is that the trained model itself will
    have its own stages in Model Registry in the production environment. We retrain
    the model again in each environment and hydrate the respective feature tables
    based on the updated code.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要记住的重要一点是，训练好的模型本身将在生产环境的模型注册表中有其自己的阶段。我们会在每个环境中重新训练模型，并基于更新后的代码填充相应的特征表。
- en: Note
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This approach may resonate more with individuals who have a background in traditional
    software engineering and are acquainted with DevOps principles. However, at the
    time of writing this book, there is no officially established method for implementing
    the deploy code approach of MLOps on the Databricks platform using the currently
    generally available tools. Although we discussed the concepts of the deploy code
    approach in this section, we won’t be covering this as part of the provided code.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可能更适合那些具有传统软件工程背景并熟悉DevOps原则的个人。然而，在撰写本书时，尚无正式建立的方法来使用目前普遍可用的工具在Databricks平台上实施MLOps的部署代码方法。虽然我们在本节讨论了部署代码方法的概念，但我们不会将其作为提供的代码的一部分来覆盖。
- en: MLOps Stack is going to address this model deployment paradigm when it becomes
    generally available. We will update this book once the new feature is available.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦MLOps Stack普遍可用，将解决此模型部署范式。一旦新功能可用，我们将更新本书。
- en: Now, let’s wrap up this chapter and summarize our key learnings.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们总结本章并总结我们的关键学习内容。
- en: Summary
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered the basics of MLOps, the different deployment approaches
    on Databricks, and their reference architectures.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了MLOps的基础知识，Databricks上的不同部署方法及其参考架构。
- en: Selecting a model deployment approach should be based on your team’s proficiency
    in implementing DevOps processes for ML projects. It’s important to acknowledge
    that there is no universal solution as each approach we have discussed has its
    own advantages and disadvantages. However, it is possible to create a customized
    hybrid ModelOps architecture within the Databricks environment.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 选择模型部署方法应基于团队在实施机器学习项目的DevOps流程方面的熟练程度。重要的是要认识到，我们讨论的每种方法都有其各自的优缺点，因此不存在通用解决方案。然而，在Databricks环境中可以创建定制的混合ModelOps架构。
- en: By considering your team’s strengths and expertise, you can determine the most
    suitable deployment approach for your project. It’s essential to assess scalability,
    maintainability, ease of deployment, and integration with existing infrastructure.
    Evaluating these aspects will help you make an informed decision and optimize
    the model deployment process.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 通过考虑团队的优势和专业知识，您可以确定最适合您项目的部署方法。评估可扩展性、可维护性、部署便捷性以及与现有基础设施的集成是至关重要的。评估这些方面将帮助您做出明智的决策，并优化模型部署过程。
- en: In Databricks, you have the flexibility to tailor your ModelOps architecture
    to your project’s requirements. Leveraging the capabilities of Databricks, you
    can combine the best elements from different deployment approaches to create a
    customized and efficient workflow. This hybrid approach allows you to leverage
    the strengths of different methodologies while mitigating their limitations.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks 中，你可以根据项目需求灵活定制你的 ModelOps 架构。利用 Databricks 的功能，你可以结合不同部署方法的最佳元素，创造出一个定制化且高效的工作流。这种混合方法让你能够在利用不同方法的优势的同时，减轻它们的局限性。
- en: Remember, the ultimate goal is to establish a robust and streamlined model deployment
    process that aligns with your team’s capabilities and project needs. By carefully
    considering your options and utilizing the resources in the Databricks environment,
    you can create a ModelOps architecture that maximizes efficiency and productivity
    for your ML projects.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，最终目标是建立一个强大且简化的模型部署流程，使其与团队能力和项目需求保持一致。通过仔细考虑你的选择并利用 Databricks 环境中的资源，你可以创建一个最大化效率和生产力的
    ModelOps 架构，以支持你的机器学习项目。
- en: Further reading
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Please go through the following sources and their links to learn more about
    the topics that were covered in the chapter:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 请查阅以下来源及其链接，以了解更多关于本章涵盖的主题：
- en: '*The Big Book of* *MLOps*: [bit.ly/big-book-of-mlops](http://bit.ly/big-book-of-mlops)'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*MLOps 大书*：[bit.ly/big-book-of-mlops](http://bit.ly/big-book-of-mlops)'
- en: '*MLOps Stack on* *GitHub*: [https://github.com/databricks/mlops-stack](https://github.com/databricks/mlops-stack)'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*MLOps 堆栈* 在 *GitHub*：[https://github.com/databricks/mlops-stack](https://github.com/databricks/mlops-stack)'
- en: Damji, J. S., Wenig, B., Das, T., and Lee, D. (2020). *Learning Spark* (2nd
    ed.)
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Damji, J. S., Wenig, B., Das, T., 和 Lee, D. (2020). *Learning Spark*（第二版）
