- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Algorithms for Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理的算法
- en: Language is the most important instrument of thought.
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 语言是思维最重要的工具。
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Marvin Minsky
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —马文·明斯基
- en: This chapter introduces algorithms for **natural language processing** (**NLP**).
    It first introduces the fundamentals of NLP. Then it presents preparing data for
    NLP tasks. After that, it explains the concepts of vectorizing textual data. Next,
    we discuss word embeddings. Finally, we present a detailed use case.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了**自然语言处理**（**NLP**）的算法。首先介绍了NLP的基础知识。然后介绍了为NLP任务准备数据。接下来，解释了文本数据向量化的概念。然后，我们讨论了词嵌入。最后，展示了一个详细的用例。
- en: 'This chapter is made up of the following sections:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章由以下几部分组成：
- en: Introducing NLP
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍NLP
- en: '**Bag-of-words-based** (**BoW-based**) NLP'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于词袋模型**（**BoW-based**）的NLP'
- en: Introduction to word embedding
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入介绍
- en: 'Case study: Restaurant review sentiment analysis'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例研究：餐厅评论情感分析
- en: By the end of this chapter, you will understand the basic techniques that are
    used for NLP. You should also understand how NLP can be used to solve some interesting
    real-world problems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将理解用于自然语言处理（NLP）的基本技术。你还应该理解NLP如何用于解决一些有趣的现实世界问题。
- en: Let’s start with the basic concepts.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基本概念开始。
- en: Introducing NLP
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍NLP
- en: NLP is a branch of machine learning algorithms that deals with the interaction
    between computers and human language. It involves analyzing, processing, and understanding
    human language to enable machines to comprehend and respond to human communication.
    NLP is a comprehensive subject and involves using computer linguistic algorithms
    and human-computer interaction technologies and methodologies to process complex
    unstructured data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: NLP是机器学习算法的一个分支，处理计算机与人类语言之间的互动。它涉及分析、处理和理解人类语言，以使计算机能够理解并回应人类的沟通。NLP是一个综合性的学科，涉及使用计算机语言学算法以及人机交互技术和方法来处理复杂的非结构化数据。
- en: NLP works by processing human language and breaking it down into its constituent
    parts, such as words, phrases, and sentences. The goal is to enable the computer
    to understand the meaning of the text and respond appropriately. NLP algorithms
    utilize various techniques, such as statistical models, machine learning, and
    deep learning, to analyze and process large volumes of natural language data.
    For complex problems, we may need to use a combination of techniques to come up
    with an effective solution.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: NLP通过处理人类语言并将其分解为基本部分，如单词、短语和句子，来工作。目标是使计算机理解文本的含义并做出适当回应。NLP算法利用各种技术，如统计模型、机器学习和深度学习，来分析和处理大量的自然语言数据。对于复杂问题，我们可能需要使用多种技术的组合来找到有效的解决方案。
- en: One of the most significant challenges in NLP is dealing with the complexity
    and ambiguity of human language. Languages are quite diverse with complex grammatical
    structures and idiomatic expressions. Additionally, the meaning of words and phrases
    can vary depending on the context in which they are used. NLP algorithms must
    be able to handle these complexities to achieve effective language processing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: NLP的一个重大挑战是处理人类语言的复杂性和歧义性。语言种类繁多，具有复杂的语法结构和习惯用语。此外，单词和短语的含义可能根据使用的上下文而有所不同。NLP算法必须能够处理这些复杂性，以实现有效的语言处理。
- en: Let’s start by looking at some of the terminology that is used when discussing
    NLP.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些在讨论NLP时使用的术语开始。
- en: Understanding NLP terminology
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解NLP术语
- en: 'NLP is a vast field of study. In this section, we will investigate some of
    the basic terminology related to NLP:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: NLP是一个广泛的研究领域。在这一部分，我们将探讨一些与NLP相关的基本术语：
- en: '**Corpus**: A corpus is a large and structured collection of text or speech
    data that serves as a resource for NLP algorithms. It can consist of various types
    of textual data, such as written text, spoken language, transcribed conversations,
    and social media posts. A corpus is created by intentionally gathering and organizing
    data from various online and offline sources, including the internet. While the
    internet can be a rich source for acquiring data, deciding what data to include
    in a corpus requires a purposeful selection and alignment with the goals of the
    particular study or analysis being conducted.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语料库**：语料库是一个大型且结构化的文本或语音数据集合，作为 NLP 算法的资源。它可以由各种类型的文本数据组成，如书面文本、口语语言、转录对话和社交媒体帖子。语料库通过有意地从各种在线和离线来源收集和组织数据来创建，包括互联网。虽然互联网是获取数据的丰富来源，但决定将哪些数据包含在语料库中，需要根据特定研究或分析的目标进行有目的的选择和对齐。'
- en: Corpora, the plural of corpus, can be annotated, meaning they may contain extra
    details about the texts, such as part-of-speech tags and named entities. These
    annotated corpora offer specific information that enhances the training and evaluation
    of NLP algorithms, making them especially valuable resources in the field.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语料库（corpora）是语料（corpus）的复数形式，可以进行注释，意味着它们可能包含关于文本的额外细节，例如词性标签和命名实体。这些注释语料库提供了特定的信息，能够增强
    NLP 算法的训练和评估，使它们在该领域成为极具价值的资源。
- en: '**Normalization**: This process involves converting text into a standard form,
    such as converting all characters to lowercase or removing punctuation, making
    it more amenable to analysis.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化**：这个过程涉及将文本转换为标准形式，例如将所有字符转换为小写字母或去除标点符号，使其更容易进行分析。'
- en: '**Tokenization**: Tokenization breaks down text into smaller parts called tokens,
    usually words or subwords, enabling a more structured analysis.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**：分词将文本拆分成更小的部分，称为词元，通常是单词或子词，从而实现更结构化的分析。'
- en: '**Named Entity Recognition** (**NER**): NER identifies and classifies named
    entities within the text, such as people’s names, locations, organizations, etc.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体识别**（**NER**）：NER 用于识别和分类文本中的命名实体，例如人名、地点、组织等。'
- en: '**Stop words**: These are commonly used words such as *and*, *the*, and *is*,
    which are often filtered out during text processing as they may not contribute
    significant meaning.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停用词**：这些是常用词，例如 *and*、*the* 和 *is*，它们在文本处理过程中通常会被过滤掉，因为它们可能不会提供显著的意义。'
- en: '**Stemming and lemmatization**: Stemming involves reducing words to their root
    form, while lemmatization involves converting words to their base or dictionary
    form. Both techniques help in analyzing the core meaning of words.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取和词形还原**：词干提取是将单词还原为其词根形式，而词形还原是将单词转换为其基本或词典形式。这两种技术有助于分析单词的核心含义。'
- en: 'Next, let us study different text preprocessing techniques used in NLP:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们研究 NLP 中使用的不同文本预处理技术：
- en: '**Word embeddings**: This is a method used to translate words into numerical
    form, where each word is represented as a vector in a space that may have many
    dimensions. In this context, a “high-dimensional vector” refers to an array of
    numbers where the number of dimensions, or individual components, is quite large—often
    in the hundreds or even thousands. The idea behind using high-dimensional vectors
    is to capture the complex relationships between words, allowing words with similar
    meanings to be positioned closer together in this multi-dimensional space. The
    more dimensions the vector has, the more nuanced the relationships it can capture.
    Therefore, in word embeddings, semantically related words end up being closer
    to each other in this high-dimensional space, making it easier for algorithms
    to understand and process language in a way that reflects human understanding.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词嵌入**：这是一种将单词转换为数值形式的方法，其中每个单词都表示为一个向量，位于一个可能具有多个维度的空间中。在这个背景下，“高维向量”指的是一个数字数组，其中维度数量或单独的成分是相当大的——通常在数百甚至数千维之间。使用高维向量的思想是为了捕捉单词之间复杂的关系，使得具有相似含义的单词在这个多维空间中更接近。向量维度越多，它能够捕捉的关系就越细致。因此，在词嵌入中，语义相关的单词会在这个高维空间中彼此更接近，从而使得算法能够更容易地理解和处理语言，反映出人类的理解方式。'
- en: '**Language modeling**: Language modeling is the process of developing statistical
    models that can predict or generate sequences of words or characters based on
    the patterns and structures found in a given text corpus.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言建模**：语言建模是开发统计模型的过程，这些模型可以根据给定文本语料库中发现的模式和结构，预测或生成单词或字符的序列。'
- en: '**Machine translation**: The process of automatically translating text from
    one language to another using NLP techniques and models.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：使用自然语言处理（NLP）技术和模型自动将文本从一种语言翻译成另一种语言的过程。'
- en: '**Sentiment analysis**: The process of determining the attitude or sentiment
    expressed in a piece of text, often by analyzing the words and phrases used and
    their context.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：通过分析文本中使用的单词、短语及其上下文来确定一段文本中表达的态度或情感的过程。'
- en: Text preprocessing in NLP
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP中的文本预处理
- en: Text preprocessing is a vital stage in NLP, where raw text data undergoes a
    transformation to become suitable for machine learning algorithms. This transformation
    involves converting the unorganized and often messy text into what is known as
    a “structured format.” A structured format means that the data is organized into
    a more systematic and predictable pattern, often involving techniques like tokenization,
    stemming, and removing unwanted characters. These steps help in cleaning the text,
    reducing irrelevant information or “noise,” and arranging the data in a manner
    that makes it easier for the machine learning models to understand. By following
    this approach, the raw text, which may contain inconsistencies and irregularities,
    is molded into a form that enhances the accuracy, performance, and efficiency
    of subsequent NLP tasks. In this section, we will explore various techniques used
    in text preprocessing to achieve this structured format.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 文本预处理是NLP中的一个关键阶段，在这一阶段，原始文本数据会经过转换，变得适合机器学习算法。这个转化过程涉及将无序且通常杂乱无章的文本转化为所谓的“结构化格式”。结构化格式意味着数据被组织成更加系统和可预测的模式，通常涉及分词、词干提取和删除不需要的字符等技术。这些步骤有助于清理文本，减少无关信息或“噪音”，并以一种更便于机器学习模型理解的方式整理数据。通过这种方法，原始文本中的不一致性和不规则性得以转化，形成一种能够提高后续NLP任务准确性、性能和效率的形式。在本节中，我们将探索用于文本预处理的各种技术，以实现这种结构化格式。
- en: Tokenization
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词
- en: As a reminder, tokenization is the crucial process of dividing text into smaller
    units, known as tokens. These tokens can be as small as individual words or even
    subwords. In NLP, tokenization is often considered the first step in preparing
    text data for further analysis. The reason for this foundational role lies in
    the very nature of language, where understanding and processing text requires
    breaking it down into manageable parts. By transforming a continuous stream of
    text into individual tokens, we create a structured format that mirrors the way
    humans naturally read and understand language. This structuring provides the machine
    learning models with a clear and systematic way to analyze the text, allowing
    them to recognize patterns and relationships within the data. As we delve deeper
    into NLP techniques, this tokenized format becomes the basis upon which many other
    preprocessing and analysis steps are built.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，分词是将文本分解成更小单位（即令牌）的关键过程。这些令牌可以是单个词语，甚至是子词。在NLP中，分词通常被视为准备文本数据进行进一步分析的第一步。分词之所以如此重要，源于语言本身的特性，理解和处理文本需要将其分解为可管理的部分。通过将连续的文本流转化为单独的令牌，我们创造了一种结构化格式，类似于人类自然阅读和理解语言的方式。这种结构化使得机器学习模型能够以清晰且系统化的方式分析文本，从而识别数据中的模式和关系。随着我们深入研究NLP技术，这种令牌化的格式成为许多其他预处理和分析步骤的基础。
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output will be a list that looks like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是如下所示的列表：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this example, each token is a word. The granularity of the resulting tokens
    will vary based on the objective—for example, each token can consist of a word,
    a sentence, or a paragraph.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，每个令牌都是一个单词。最终令牌的粒度将根据目标而有所不同——例如，每个令牌可以是一个单词、一句话或一段话。
- en: 'To tokenize text based on sentences, you can use the `sent_tokenize` function
    from the `nltk.tokenize` module:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要基于句子对文本进行分词，可以使用`nltk.tokenize`模块中的`sent_tokenize`函数：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this example, the `corpus` variable contains two sentences. The `sent_tokenize`
    function takes the corpus as input and returns a list of sentences. When you run
    the modified code, you will get the following output:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`corpus`变量包含了两个句子。`sent_tokenize`函数将语料库作为输入，并返回一个句子的列表。当你运行修改后的代码时，将得到以下输出：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Sometimes we may need to break down large texts into paragraph-level chunks.
    `nltk` can help with that task. It’s a feature that could be particularly useful
    in applications such as document summarization, where understanding the structure
    at the paragraph level may be crucial. Tokenizing text into paragraphs might seem
    straightforward, but it can be complex depending on the structure and format of
    the text. A simple approach is to split the text into two newline characters,
    which often separate paragraphs in plain text documents.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们可能需要将较大的文本拆分为按段落划分的块。`nltk`可以帮助完成这项任务。这项功能在文档摘要等应用中尤其有用，因为在这些应用中，理解段落级别的结构可能至关重要。将文本按段落进行分词看似简单，但根据文本的结构和格式，可能会变得复杂。一个简单的方法是通过两个换行符来拆分文本，这通常用于纯文本文档中的段落分隔。
- en: 'Here’s a basic example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个基本示例：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, let us look into how we can clean the data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何清理数据。
- en: Cleaning data
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 清理数据
- en: Cleaning data is an essential step in NLP, as raw text data often contains noise
    and irrelevant information that can hinder the performance of NLP models. The
    goal of cleaning data for NLP is to preprocess the text data to remove noise and
    irrelevant information, and to transform it into a format that is suitable for
    analysis using NLP techniques. Note that data cleaning is done after it is tokenized.
    The reason is that cleaning might involve operations that depend on the structure
    revealed by tokenization. For instance, removing specific words or altering word
    forms might be done more accurately after the text is tokenized into individual
    terms.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 清理数据是NLP中的一个关键步骤，因为原始文本数据通常包含噪音和无关信息，这些信息可能会妨碍NLP模型的性能。清理数据的目标是对文本数据进行预处理，去除噪音和无关信息，并将其转换为适合使用NLP技术分析的格式。请注意，数据清理是在数据被分词之后进行的。原因在于，清理可能涉及到依赖于分词揭示的结构的操作。例如，删除特定单词或修改单词形式可能在文本被分词为独立的词汇后更为准确。
- en: 'Let us study some techniques used to clean data and prepare it for machine
    learning tasks:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们研究一些用于清理数据并为机器学习任务做准备的技术：
- en: Case conversion
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大小写转换
- en: Case conversion is a technique in NLP where text is transformed from one case
    format to another, such as from uppercase to lowercase, or from title case to
    uppercase.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 大小写转换是自然语言处理（NLP）中的一种技术，它将文本从一种大小写格式转换为另一种格式，例如从大写转换为小写，或者从标题式大小写转换为大写。
- en: For example, the text “Natural Language Processing” in title case could be converted
    to lowercase to be “natural language processing.”
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，标题式大小写的“Natural Language Processing”可以转换为小写，即“natural language processing”。
- en: This simple yet effective step helps in standardizing the text, which in turn
    simplifies its processing for various NLP algorithms. By ensuring that the text
    is in a uniform case, it aids in eliminating inconsistencies that might otherwise
    arise from variations in capitalization.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这一简单而有效的步骤有助于标准化文本，从而简化其在各种NLP算法中的处理。通过确保文本处于统一的大小写格式，有助于消除由于大小写变化可能产生的不一致性。
- en: Punctuation removal
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标点符号移除
- en: Punctuation removal in NLP refers to the process of removing punctuation marks
    from raw text data before analysis. Punctuation marks are symbols such as periods
    (`.`), commas (`,`), question marks (`?`), and exclamation marks (`!`) that are
    used in written language to indicate pauses, emphasis, or intonation. While they
    are essential in written language, they can add noise and complexity to raw text
    data, which can hinder the performance of NLP models.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中，标点符号移除是指在分析之前，从原始文本数据中删除标点符号的过程。标点符号是如句号（`.`）、逗号（`,`）、问号（`?`）和感叹号（`!`）等符号，它们在书面语言中用于表示停顿、强调或语调。虽然它们在书面语言中至关重要，但它们会为原始文本数据增加噪音和复杂性，进而妨碍NLP模型的性能。
- en: 'It’s a reasonable concern to wonder how the removal of punctuation might affect
    the meaning of sentences. Consider the following examples:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 合理的疑虑是，删除标点符号可能会影响句子的意义。请考虑以下示例：
- en: '`"She''s a cat."`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`"她是只猫。"`'
- en: '`"She''s a cat??"`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`"她是只猫??"`'
- en: Without punctuation, both lines become “She’s a cat,” potentially losing the
    distinct emphasis conveyed by the question marks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 没有标点符号时，两行文本都变成了“她是只猫”，可能失去了问号所传达的独特强调。
- en: However, it’s worth noting that in many NLP tasks, such as topic classification
    or sentiment analysis, punctuation might not significantly impact the overall
    understanding. Additionally, models can rely on other cues from the text’s structure,
    content, or context to derive meaning. In cases where the nuances of punctuation
    are critical, specialized models and preprocessing techniques may be employed
    to retain the required information.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，在许多NLP任务中，如主题分类或情感分析，标点符号可能不会显著影响整体理解。此外，模型可以依赖于文本结构、内容或上下文中的其他线索来推导含义。在标点符号细微差别至关重要的情况下，可能需要使用专门的模型和预处理技术来保留所需的信息。
- en: Handling numbers in NLP
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理数字在NLP中的应用
- en: Numbers within text data can pose challenges in NLP. Here’s a look at two main
    strategies for handling numbers in text analysis, considering both the traditional
    approach of removal and an alternative option of standardization.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据中的数字可能给NLP带来挑战。下面是两种处理文本中数字的主要策略，既考虑了传统的去除方法，也考虑了标准化的替代选项。
- en: 'In some NLP tasks, numbers may be considered noise, particularly when the focus
    is on aspects likeyWord frequency or sentiment analysis. Here’s why some analysts
    might choose to remove numbers:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些自然语言处理（NLP）任务中，数字可能被视为噪声，特别是当关注点集中在像词频或情感分析等方面时。这就是为什么一些分析师可能选择去除数字的原因：
- en: '**Lack of relevance**: Numeric characters may not carry significant meaning
    in specific text analysis scenarios.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏相关性**：在某些文本分析情境中，数字字符可能不携带重要的含义。'
- en: '**Skewing frequency counts**: Numbers can distort word frequency counts, especially
    in models like topic modeling.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扭曲词频统计**：数字可能会扭曲词频统计，尤其是在像主题建模这样的模型中。'
- en: '**Reducing complexity**: Removing numbers may simplify the text data, potentially
    enhancing the performance of NLP models.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少复杂性**：去除数字可以简化文本数据，可能提升NLP模型的性能。'
- en: However, an alternative approach is to convert all numbers to a standard representation
    rather than discarding them. This method acknowledges that numbers can carry essential
    information and ensures that their value is retained in a consistent format. It
    can be particularly useful in contexts where numerical data plays a vital role
    in the meaning of the text.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一种替代方法是将所有数字转换为标准表示，而不是将其丢弃。这种方法承认数字可以携带重要信息，并确保其在一致格式中保留其值。在数字数据对文本含义起着至关重要作用的语境中，这种方法特别有用。
- en: Deciding whether to remove or retain numbers requires an understanding of the
    problem being solved. An algorithm may need customization to distinguish whether
    a number is significant based on the context of the text and the specific NLP
    task. Analyzing the role of numbers within the domain of the text and the goals
    of the analysis can guide this decision-making process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 决定是否去除或保留数字需要理解所解决的问题。一个算法可能需要定制，以根据文本的上下文和特定的NLP任务来区分数字是否重要。分析数字在文本领域中的作用以及分析的目标，可以引导这一决策过程。
- en: Handling numbers in NLP is not a one-size-fits-all approach. Whether to remove,
    standardize, or carefully analyze numbers depends on the unique requirements of
    the task at hand. Understanding these options and their implications helps in
    making informed decisions that align with the goals of the text analysis.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中处理数字并不是一成不变的方法。是否去除、标准化或仔细分析数字，取决于任务的独特要求。理解这些选项及其影响，有助于做出符合文本分析目标的明智决策。
- en: White space removal
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 去除空格
- en: White space removal in NLP refers to the process of removing unnecessary white
    spaces, such as multiple spaces and tab characters. White space in the context
    of text data is not merely the space between words but includes other “invisible”
    characters that create spacing within text. In NLP, white space removal refers
    to the process of eliminating these unnecessary white space characters. Removing
    unnecessary white spaces can reduce the size of the text data and make it easier
    to process and analyze.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中，空格去除指的是去除不必要的空格字符，如多个空格和制表符字符。在文本数据的语境中，空格不仅是单词之间的空白，还包括其他“看不见”的字符，这些字符在文本中创建了间距。在NLP中，空格去除指的是去除这些不必要的空格字符。去除不必要的空格可以减少文本数据的大小，并使其更易于处理和分析。
- en: 'Here’s a simple example to illustrate white space removal:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个简单的例子来说明空格去除：
- en: 'Input text: `"The quick brown fox \tjumps over the lazy dog."`'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入文本：`"The quick brown fox \tjumps over the lazy dog."`
- en: 'Processed text: `"The quick brown fox jumps over the lazy dog."`'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理过的文本：`"The quick brown fox jumps over the lazy dog."`
- en: In the above example, extra spaces and a tab character (denoted by `\t`) are
    removed to create a cleaner and more standardized text string.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，去除了额外的空格和一个制表符字符（由 `\t` 表示），从而创建了更干净且更标准化的文本字符串。
- en: Stop word removal
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 停用词去除
- en: Stop word removal is the process of eliminating common words, known as stop
    words, from a text corpus. stop words are words that occur frequently in a language
    but do not carry significant meaning or contribute to the overall understanding
    of the text. Examples of stop words in English include *the,* and*, is, in* and
    *for*. Stop word removal helps reduce the dimensionality of the data and improve
    the efficiency of the algorithms. By removing words that don’t contribute meaningfully
    to the analysis, computational resources can be focused on the words that do matter,
    improving the efficiency of various NLP algorithms.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词去除是指从文本语料库中删除常见词汇，称为停用词。停用词是在语言中频繁出现，但不具有重要意义或不有助于理解文本的词汇。英语中的停用词包括 *the,*
    和*, is, in* 和 *for*。停用词去除有助于减少数据的维度，并提高算法的效率。通过去除那些对分析没有重要贡献的词汇，可以将计算资源集中在真正重要的词汇上，从而提高各种自然语言处理算法的效率。
- en: Note that stop word removal is more than a mere reduction in text size; it’s
    about focusing on the words that truly matter for the analysis at hand. While
    stop words play a vital role in language structure, their removal in NLP can enhance
    the efficiency and focus of the analysis, particularly in tasks like sentiment
    analysis where the primary concern is understanding the underlying emotion or
    opinion.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，停用词去除不仅仅是减少文本大小；它是为了专注于分析中真正重要的词汇。虽然停用词在语言结构中扮演着重要角色，但在自然语言处理中的去除，可以提升分析的效率和重点，特别是在情感分析等任务中，主要关心的是理解潜在的情感或观点。
- en: Stemming and lemmatization
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 词干提取和词形还原
- en: In textual data, most words are likely to be present in slightly different forms.
    Reducing each word to its origin or stem in a family of words is called **stemming**.
    It is used to group words based on their similar meanings to reduce the total
    number of words that need to be analyzed. Essentially, stemming reduces the overall
    conditionality of the problem. The most common algorithm for stemming English
    is the Porter algorithm.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本数据中，大多数单词可能以略微不同的形式出现。将每个单词简化为它的原型或词干，这一过程称为**词干提取**。它用于根据单词的相似含义将单词分组，以减少需要分析的单词总数。本质上，词干提取减少了问题的整体条件性。英语中最常用的词干提取算法是
    Porter 算法。
- en: 'For example, let us look into a couple of examples:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看一些例子：
- en: 'Example 1: `{use, used, using, uses} => use`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例 1：`{use, used, using, uses} => use`
- en: 'Example 2: `{easily, easier, easiest} => easi`'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例 2：`{easily, easier, easiest} => easi`
- en: It’s important to note that stemming can sometimes result in misspelled words,
    as seen in example 2 where `easi` was produced.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，词干提取有时会导致拼写错误，如示例 2 中生成的 `easi`。
- en: Stemming is a simple and quick process, but it may not always produce correct
    results. For cases where correct spelling is required, lemmatization is a more
    appropriate method. Lemmatization considers the context and reduces words to their
    base form. The base form of a word, also known as the lemma, is its most simple
    and meaningful version. It represents the way a word would appear in the dictionary,
    devoid of any inflectional endings, which will be a correct English word, resulting
    in more accurate and meaningful word roots.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取是一种简单且快速的处理过程，但它可能并不总是产生正确的结果。在需要正确拼写的情况下，词形还原是一种更合适的方法。词形还原考虑上下文并将单词还原为其基本形式。单词的基本形式，也称为词根，是其最简单且最具意义的版本。它代表了单词在字典中的形式，去除了任何词尾变化，形成一个正确的英语单词，从而产生更准确、更有意义的词根。
- en: The process of guiding algorithms to recognize similarities is a precise and
    thoughtful task. Unlike humans, algorithms need explicit rules and criteria to
    make connections that might seem obvious to us. Understanding this distinction
    and knowing how to provide the necessary guidance is a vital skill in the development
    and tuning of algorithms for various applications.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 引导算法识别相似性是一个精确且深思熟虑的任务。与人类不同，算法需要明确的规则和标准来建立连接，这些连接对我们来说可能看起来是显而易见的。理解这一差异并知道如何提供必要的引导，是开发和调整算法在各种应用中的重要技能。
- en: Cleaning data using Python
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Python 清理数据
- en: Let us look into how we can clean text using Python.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下如何使用 Python 清理文本。
- en: 'First, let’s import the necessary libraries:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入必要的库：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, here is the main function to perform text cleaning:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这是执行文本清理的主函数：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let us test the function `clean_text()`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下`clean_text()`函数：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The result will be:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note the word `becom` in the output. As we are using stemming, not all the words
    in the output are expected to be correct English words.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意输出中的单词`becom`。由于我们使用了词干提取技术，输出中的并非所有单词都是正确的英语单词。
- en: All the preceding processing steps are typically needed; the actual processing
    steps depend on the problem that we want to solve. They will vary from use case
    to use case—for example, if the numbers in the text represent something that may
    have some value in the context of the problem that we are trying to solve, then
    we may not need to remove the numbers from the text in the normalization phase.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所有前面的处理步骤通常是必需的；实际的处理步骤取决于我们要解决的问题。它们会因使用场景而异——例如，如果文本中的数字表示一些在我们尝试解决的问题背景下可能有意义的内容，那么在标准化阶段我们可能不需要去除这些数字。
- en: Once the data is cleaned, we need to store the results in a data structure tailored
    for this purpose. This data structure is called the **Term Document Matrix** (**TDM**)
    and is explained next.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被清理，我们需要将结果存储在一个专门为此目的设计的数据结构中。这个数据结构被称为**术语文档矩阵**（**TDM**），接下来会详细解释。
- en: Understanding the Term Document Matrix
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解术语文档矩阵
- en: A TDM is a mathematical structure used in NLP. It’s a table that counts the
    frequency of terms (words) in a collection of documents. Each row represents a
    unique term, and each column represents a specific document. It’s an essential
    tool for text analysis, where you can see how often each word occurs in various
    texts.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: TDM（术语文档矩阵）是自然语言处理（NLP）中使用的数学结构。它是一个表格，用于统计文档集合中术语（单词）的频率。每一行表示一个独特的术语，每一列表示一个特定的文档。它是文本分析中的一个重要工具，可以让你看到每个单词在不同文本中出现的频率。
- en: 'For documents containing the words `cat` and `dog`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包含单词`cat`和`dog`的文档：
- en: 'Document 1: `cat cat dog`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档 1：`cat cat dog`
- en: 'Document 2: `dog dog cat`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档 2：`dog dog cat`
- en: '|  | **Document 1** | **Document 2** |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | **Document 1** | **Document 2** |'
- en: '| **cat** | 2 | 1 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **cat** | 2 | 1 |'
- en: '| **dog** | 1 | 2 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| **dog** | 1 | 2 |'
- en: 'This matrix structure allows the efficient storage, organization, and analysis
    of large text datasets. In Python, the `CountVectorizer` module from the `sklearn`
    library can be used to create a TDM as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这种矩阵结构允许高效地存储、组织和分析大型文本数据集。在 Python 中，可以使用`sklearn`库中的`CountVectorizer`模块来创建一个
    TDM，方法如下：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output looks as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that corresponding to each document, there is a row, and corresponding
    to each distinct word, there is a column. There are three documents and there
    are six distinct words, resulting in a matrix with dimensions 3x6.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个文档对应一行，每个不同的单词对应一列。这里有三个文档和六个不同的单词，结果是一个3x6的矩阵。
- en: In this matrix, the numbers represent the frequency with which each word (column)
    appears in the corresponding document (row). So, for example, if the number in
    the first row and first column is 1, this means that the first word appears once
    in the first document.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个矩阵中，数字表示每个单词（列）在对应文档（行）中出现的频率。例如，如果第一行第一列的数字是1，这意味着第一个单词在第一个文档中出现了一次。
- en: TDM uses the frequency of each term by default, which is a simple way to quantify
    the importance of each word in the context of each individual document. A more
    sophisticated way to quantify the importance of each word is TF-IDF, which is
    explained in the next section.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，TDM使用每个术语的频率，这是量化每个单词在每个文档中的重要性的一种简单方法。更精细的量化方法是使用TF-IDF，这将在下一节中解释。
- en: Using TF-IDF
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TF-IDF
- en: '**Term Frequency-Inverse Document Frequency** (**TF-IDF**) is a method used
    to calculate the significance of words in a document. It considers two main components
    to determine the weight of each term: the **Term Frequency** (**TF**) and the
    **Inverse Document Frequency** (**IDF**). The TF looks at how often a word appears
    in a specific document, while the IDF examines how rare the word is across a collection
    of documents, known as a corpus. In the context of TF-IDF, the corpus refers to
    the entire set of documents that you are analyzing. If we are working with a collection
    of book reviews, for example, the corpus would include all the reviews:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**词频-逆文档频率**（**TF-IDF**）是一种用于计算单词在文档中重要性的方法。它考虑了两个主要成分来确定每个术语的权重：**词频**（**TF**）和**逆文档频率**（**IDF**）。TF
    关注一个词在特定文档中出现的频率，而 IDF 则检查这个词在整个文档集合（即语料库）中的稀有程度。在 TF-IDF 的上下文中，语料库指的是你正在分析的所有文档。如果我们正在处理一组书评，举例来说，语料库将包括所有的书评：'
- en: '**TF**: TF measures the number of times a term appears in a document. It is
    calculated as the ratio of the number of occurrences of a term in a document to
    the total number of terms in the document. The more frequent the term, the higher
    its TF value.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TF**：TF 衡量一个术语在文档中出现的次数。它的计算方法是术语在文档中出现的次数与文档中术语总数的比值。术语出现得越频繁，TF 值就越高。'
- en: '**IDF**: IDF measures the importance of a term across the entire corpus of
    documents. It is calculated as the logarithm of the ratio of the total number
    of documents in the corpus to the number of documents containing the term. The
    rarer the term across the corpus, the higher its IDF value.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IDF**：IDF 衡量一个术语在整个文档集合中的重要性。它的计算方法是语料库中总文档数与包含该术语的文档数之比的对数。术语在语料库中越稀有，它的
    IDF 值就越高。'
- en: 'To compute TF-IDF using Python, do the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Python 计算 TF-IDF，请执行以下操作：
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This will print:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Each column in the output corresponds to a document, and the rows represent
    the TF-IDF values for the terms across the documents. For example, the term `kids`
    has a non-zero TF-IDF value only in the second document, which is in line with
    our expectations.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中的每一列对应一个文档，行表示各文档中术语的 TF-IDF 值。例如，术语`kids`只有在第二个文档中才有非零的 TF-IDF 值，这与我们的预期一致。
- en: Summary and discussion of results
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果总结与讨论
- en: The TF-IDF method provides a valuable way to weigh the importance of terms within
    individual documents and across an entire corpus. The resulting TF-IDF values
    reveal the relevance of specific terms within each document, taking into account
    both their frequency in a given document and their rarity across the entire collection.
    In the provided example, the varying TF-IDF scores for different terms demonstrate
    the model’s ability to distinguish words that are unique to specific documents
    from those that are more commonly used. This ability can be leveraged in various
    applications, such as text classification, information retrieval, and feature
    selection, to enhance the understanding and processing of text data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 方法提供了一种有价值的方式来衡量术语在单个文档内以及在整个语料库中的重要性。计算得到的 TF-IDF 值揭示了每个术语在每个文档中的相关性，同时考虑了它们在给定文档中的频率以及在整个语料库中的稀有性。在提供的示例中，不同术语的
    TF-IDF 值变化表明该模型能够区分那些在特定文档中独有的词汇和那些使用频率较高的词汇。这种能力可以在多个应用中加以利用，如文本分类、信息检索和特征选择，提升对文本数据的理解和处理。
- en: Introduction to word embedding
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入简介
- en: One of the major advancements in NLP is our ability to create a meaningful numeric
    representation of words in the form of dense vectors. This technique is called
    word embedding. So, what exactly is a dense vector? Imagine you have a word like
    `apple`. In word embedding, `apple` might be represented as a series of numbers,
    such as `[0.5, 0.8, 0.2]`, where each number is a coordinate in a continuous,
    multi-dimensional space. The term “dense” means that most or all of these numbers
    are non-zero, unlike sparse vectors where many elements might be zero. In simple
    terms, word embedding takes each word in a text and turns it into a unique, multi-dimensional
    point in space. This way, words with similar meanings will end up closer to each
    other in this space, allowing algorithms to understand the relationships between
    words. Yoshua Bengio first introduced the term in his paper *A Neural Probabilistic
    Language Model*. Each word in an NLP problem can be thought of as a categorical
    object.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 的一项重大进展是我们能够创建单词的有意义的数字表示形式，采用稠密向量的形式。这种技术称为词嵌入。那么，稠密向量到底是什么呢？假设你有一个单词 `apple`（苹果）。在词嵌入中，`apple`
    可能被表示为一系列数字，例如 `[0.5, 0.8, 0.2]`，其中每个数字都是连续的、多维空间中的一个坐标。“稠密”意味着这些数字大多数或全部都不为零，不像稀疏向量那样许多元素为零。简单来说，词嵌入将文本中的每个单词转化为一个独特的、多维的空间点。这样，含义相似的单词将最终在这个空间中彼此接近，从而使算法能够理解单词之间的关系。Yoshua
    Bengio 在他的论文 *A Neural Probabilistic Language Model* 中首次提出了这个术语。NLP 问题中的每个单词可以被视为一个类别对象。
- en: In word embedding, try to establish the neighborhood of each word and use it
    to quantify its meaning and importance. The neighborhood of a word is the set
    of words that surround a particular word.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在词嵌入中，尝试建立每个单词的邻域，并利用它来量化单词的意义和重要性。一个单词的邻域是指围绕特定单词的一组单词。
- en: 'To truly grasp the concept of word embedding, let’s look at a tangible example
    involving a vocabulary of four familiar fruits: `apple`, `banana`, `orange`, and
    `pear`. The goal here is to represent these words as dense vectors, numerical
    arrays where each number captures a specific characteristic or feature of the
    word.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了真正理解词嵌入的概念，我们来看一个涉及四个常见水果词汇的具体例子：`apple`（苹果）、`banana`（香蕉）、`orange`（橙子）和`pear`（梨）。这里的目标是将这些单词表示为稠密向量，这些向量是数字数组，其中每个数字捕捉单词的特定特征或特性。
- en: Why represent words this way? In NLP, converting words into dense vectors enables
    algorithms to quantify the relationships between different words. Essentially,
    we’re turning abstract language into something that is mathematically measurable.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要以这种方式表示单词呢？在自然语言处理（NLP）中，将单词转换为稠密向量可以使算法量化不同单词之间的关系。本质上，我们是在将抽象的语言转化为可以用数学方法衡量的内容。
- en: 'Consider the features of sweetness, acidity, and juiciness for our fruit words.
    We could rate these features on a scale from 0 to 1 for each fruit, where 0 means
    the feature is entirely absent, and 1 means the feature is strongly present. This
    rating could look like this:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们水果单词的甜度、酸度和多汁度特征。我们可以对每个水果的这些特征进行从 0 到 1 的评分，0 表示该特征完全缺失，1 表示该特征非常明显。评分可能是这样的：
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The numbers are subjective and can be derived from taste tests, expert opinions,
    or other methods, but they serve to transform the words into a format that an
    algorithm can understand and work with.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字是主观的，可以通过味觉测试、专家意见或其他方法得出，但它们的作用是将单词转化为算法可以理解并使用的格式。
- en: Visualizing this, you can imagine a 3D space where each axis represents one
    of the features (sweetness, acidity, or juiciness), and each fruit’s vector places
    it at a specific point in this space. Words (fruits) with similar tastes would
    be closer to each other in this space.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过可视化，你可以想象一个三维空间，其中每个坐标轴代表一个特征（甜度、酸度或多汁度），每个水果的向量将其放置在这个空间中的特定位置。具有相似口味的单词（水果）会在这个空间中彼此更接近。
- en: So, why the choice of dense vectors with a length of 3? This is based on the
    specific features we have chosen to represent. In other applications, the vector
    length might be different, based on the number of features you want to capture.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么选择长度为 3 的稠密向量呢？这是基于我们选择的特征来表示的。在其他应用中，向量的长度可能不同，取决于你希望捕捉的特征数量。
- en: This example illustrates how word embedding takes a word and turns it into a
    numerical vector that holds real-world meaning. It’s a crucial step in enabling
    machines to “understand” and process human language.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了词嵌入是如何将一个单词转化为一个持有实际意义的数字向量的。这是让机器“理解”并处理人类语言的关键步骤。
- en: Implementing word embedding with Word2Vec
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Word2Vec 实现词嵌入
- en: Word2Vec is a prominent method used for obtaining vector representations of
    words, commonly referred to as word embeddings. Rather than “generating words,”
    this algorithm creates numerical vectors that represent the semantic meaning of
    each word in the language.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec是一种用于获取单词向量表示的突出方法，通常称为单词嵌入。该算法并不是“生成单词”，而是创建代表每个单词语义的数值向量。
- en: 'The basic idea behind Word2Vec is to use a neural network to predict the context
    of each word in a given text corpus. The neural network is trained by inputting
    the word and its surrounding context words, and the network learns to output the
    probability distribution of the context words given the input word. The weights
    of the neural network are then used as the word embeddings, which can be used
    for various NLP tasks:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec的基本思想是利用神经网络来预测给定文本语料库中每个单词的上下文。神经网络通过输入单词及其周围的上下文单词进行训练，网络学习输出给定输入单词的上下文单词的概率分布。神经网络的权重随后被用作单词嵌入，这些嵌入可以用于各种自然语言处理任务：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let us break down the important parameters of `Word2Vec()` function:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下`Word2Vec()`函数的重要参数：
- en: '**sentences**: This is the input data for the model. It should be a collection
    of sentences, where each sentence is a list of words. Essentially, it’s a list
    of lists of words that represents your entire text corpus.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sentences**：这是模型的输入数据。它应该是一个句子的集合，每个句子是一个单词列表。实际上，它是一个单词列表的列表，代表了你的整个文本语料库。'
- en: '**size**: This defines the dimensionality of the word embeddings. In other
    words, it sets the number of features or numerical values in the vectors that
    represent the words. A typical value might be `100` or `300`, depending on the
    complexity of the vocabulary.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**size**：定义了单词嵌入的维度。换句话说，它设置了表示单词的向量中的特征或数值的数量。一个典型的值可能是`100`或`300`，具体取决于词汇的复杂性。'
- en: '**window**: This parameter sets the maximum distance between the target word
    and the context words used for prediction within a sentence. For example, if you
    set the window size to `5`, the algorithm will consider the five words immediately
    before and after the target word in the training process.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**window**：该参数设置目标单词与句子中用于预测的上下文单词之间的最大距离。例如，如果将窗口大小设置为`5`，算法将在训练过程中考虑目标单词前后五个立即相邻的单词。'
- en: '**min_count**: Words that appear infrequently in the corpus may be excluded
    from the model by setting this parameter. If you set `min_count` to `2`, for example,
    any word that appears fewer than two times across all the sentences will be ignored
    during training.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_count**：通过设置此参数，可以排除在语料库中出现频率较低的单词。例如，如果将`min_count`设置为`2`，那么在所有句子中出现次数少于两次的单词将在训练过程中被忽略。'
- en: '**workers**: This refers to the number of processing threads used during training.
    Increasing this value can speed up training on multi-core machines by enabling
    parallel processing.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**workers**：指的是训练过程中使用的处理线程数量。增加该值可以通过启用并行处理来加速在多核机器上的训练。'
- en: 'Once the Word2Vec model is trained, one of the powerful ways to use it is to
    measure the similarity or “distance” between words in the embedding space. This
    similarity score can give us insight into how the model perceives relationships
    between different words. Now let us check the model by looking at the distance
    between `car` and `train`:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Word2Vec模型训练完成，使用它的强大方法之一是测量嵌入空间中单词之间的相似性或“距离”。这个相似性得分可以让我们洞察模型如何看待不同单词之间的关系。现在，让我们通过查看`car`和`train`之间的距离来检查模型：
- en: '[PRE17]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now let’s look into the similarity of `car` and `apple`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下`car`和`apple`的相似度：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Thus, the output gives us the similarity score between individual terms based
    on the word embeddings learned by the model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，输出给我们的是基于模型学习到的单词嵌入之间的相似性得分。
- en: Interpreting similarity scores
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释相似性得分
- en: 'The following details help with interpreting similarity scores:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下细节有助于解释相似性得分：
- en: '**Very similar**: Scores close to 1 signify strong similarity. Words with this
    score often share contextual or semantic meanings.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非常相似**：接近1的得分表示强烈的相似性。具有此得分的单词通常共享上下文或语义意义。'
- en: '**Moderately similar**: Scores around 0.5 indicate some level of similarity,
    possibly due to shared attributes or themes.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适度相似**：接近0.5的得分表示某种程度的相似性，可能是由于共享的属性或主题。'
- en: '**Weak or no similarity**: Scores close to 0 or negative imply little to no
    similarity or even contrast in meanings.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似度弱或没有相似性**：接近0或负数的得分表示意义之间几乎没有相似性，甚至存在对比。'
- en: Thus, these similarity scores provide quantitative insights into word relationships.
    By understanding these scores, you can better analyze the semantic structure of
    your text corpus and leverage it for various NLP tasks.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些相似度分数提供了关于单词关系的定量见解。通过理解这些分数，你可以更好地分析文本语料库的语义结构，并将其用于各种 NLP 任务。
- en: Word2Vec provides a powerful and efficient way to represent textual data in
    a way that captures semantic relationships between words, reduces dimensionality,
    and improves accuracy in downstream NLP tasks. Let us look into the advantages
    and disadvantages of Word2Vec.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 提供了一种强大且高效的方式来表示文本数据，能够捕捉单词之间的语义关系、减少维度并提高下游 NLP 任务的准确性。让我们来看看 Word2Vec
    的优缺点。
- en: Advantages and disadvantages of Word2Vec
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2Vec 的优缺点
- en: 'The following are the advantages of using Word2Vec:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 Word2Vec 的优点：
- en: '**Capturing semantic relationships**: Word2Vec’s embeddings are positioned
    in the vector space in such a way that semantically related words are located
    near each other. This spatial arrangement captures syntactic and semantic relationships
    like synonyms, analogies, and more, enabling better performance in tasks like
    information retrieval and semantic analysis.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**捕捉语义关系**：Word2Vec 的嵌入在向量空间中的位置使得语义相关的单词靠得很近。通过这种空间安排，捕捉了语法和语义关系，如同义词、类比等，从而在信息检索和语义分析等任务中取得更好的表现。'
- en: '**Reducing dimensionality**: Traditional one-hot encoding of words can create
    a sparse and high-dimensional space, especially with large vocabularies. Word2Vec
    compresses this into a denser and lower-dimensional continuous vector space (typically
    ranging from 100 to 300 dimensions). This condensed representation preserves essential
    linguistic patterns while being computationally more efficient.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：传统的单热编码（one-hot encoding）会创建一个稀疏且高维的空间，尤其是当词汇表很大时。Word2Vec 将其压缩为一个更加密集且低维的连续向量空间（通常为
    100 到 300 维）。这种压缩表示保留了重要的语言模式，同时在计算上更高效。'
- en: '**Handling out-of-vocabulary words**: Word2Vec can infer embeddings for words
    that didn’t appear in the training corpus by leveraging the surrounding context
    words. This property aids in generalizing better to unseen or new text data, enhancing
    robustness.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理词汇外单词**：Word2Vec 可以通过利用上下文词来推断未出现在训练语料中的单词的嵌入。这个特性有助于更好地泛化到未见过或新的文本数据，增强了模型的鲁棒性。'
- en: 'Now let us look into some of the disadvantages of using Word2Vec:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看使用 Word2Vec 的一些缺点：
- en: '**Training complexity**: Word2Vec models can be computationally demanding to
    train, particularly with vast vocabularies and higher-dimensional vectors. They
    require significant computing resources and may necessitate optimization techniques,
    such as negative sampling or hierarchical softmax, to scale efficiently.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练复杂性**：Word2Vec 模型的训练可能需要大量计算资源，特别是在拥有庞大词汇表和高维向量时。它们需要大量的计算资源，并可能需要优化技术，如负采样或层次化软最大（hierarchical
    softmax），以实现高效扩展。'
- en: '**Lack of interpretability**: The continuous and dense nature of Word2Vec embeddings
    makes them challenging to interpret by humans. Unlike carefully crafted linguistic
    features, the dimensions in Word2Vec don’t correspond to intuitive characteristics,
    making it difficult to understand what specific aspects of the words are being
    captured.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏可解释性**：Word2Vec 嵌入的连续性和密集性使得它们难以被人类理解。与精心设计的语言特征不同，Word2Vec 中的维度不对应直观的特征，这使得理解捕获了单词的哪些具体方面变得困难。'
- en: '**Sensitive to text preprocessing**: The quality and effectiveness of Word2Vec
    embeddings can vary significantly based on the preprocessing steps applied to
    the text data. Factors such as tokenization, stemming, and lemmatization, or the
    removal of stopwords, must be carefully considered. The choice of preprocessing
    can impact the spatial relationships within the vector space, potentially affecting
    the model’s performance on downstream tasks.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对文本预处理敏感**：Word2Vec 嵌入的质量和效果可能会根据应用于文本数据的预处理步骤而显著变化。诸如分词、词干提取、词形还原或去除停用词等因素必须谨慎考虑。预处理的选择可能会影响向量空间中的空间关系，从而可能影响模型在下游任务中的表现。'
- en: Next, let us look into a case study about restaurant reviews that combines all
    the concepts presented in this chapter.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一个关于餐厅评论的案例研究，结合了本章介绍的所有概念。
- en: 'Case study: Restaurant review sentiment analysis'
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：餐厅评论情感分析
- en: We will use the Yelp Reviews dataset, which contains labeled reviews as positive
    (5 stars) or negative (1 star). We will train a model that can classify the reviews
    of a restaurant as negative or positive.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Yelp评论数据集，该数据集包含标记为正面（5星）或负面（1星）的评论。我们将训练一个可以将餐厅评论分类为负面或正面的模型。
- en: Let’s implement this processing pipeline by going through the following steps.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤实现这个处理管道。
- en: Importing required libraries and loading the dataset
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入所需的库并加载数据集
- en: 'First, we import the packages that we need:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的包：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then we import the dataset from a .`csv` file:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们从一个`.csv`文件导入数据集：
- en: '[PRE22]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Building a clean corpus: Preprocessing text data'
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个干净的语料库：文本数据预处理
- en: 'Next, we clean the data by performing text preprocessing on each of the reviews
    of the dataset using stemming and stopword removal techniques:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过对数据集中的每条评论进行词干提取和停用词去除等文本预处理来清洗数据：
- en: '[PRE24]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The code iterates through each review in the dataset (in this case, the `'Review'`
    column) and applies the `clean_text` function to preprocess and clean each review.
    The code creates a new list called `corpus`. The result is a list of cleaned and
    preprocessed reviews stored in the `corpus` variable.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 代码遍历数据集中的每一条评论（在这种情况下是`'Review'`列），并应用`clean_text`函数对每条评论进行预处理和清洗。代码创建了一个名为`corpus`的新列表。结果是一个存储在`corpus`变量中的已清洗和预处理过的评论列表。
- en: Converting text data into numerical features
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将文本数据转换为数值特征
- en: Now let’s define the features (represented by `y`) and the label (represented
    by `X`). Remember that **features** are the independent variables or attributes
    that describe the characteristics of the data, used as input for predictions.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义特征（由`y`表示）和标签（由`X`表示）。记住，**特征**是描述数据特征的自变量或属性，作为预测的输入。
- en: 'And **labels** are the dependent variables or target values that the model
    is trained to predict, representing the outcomes corresponding to the features:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 而**标签**是模型被训练来预测的因变量或目标值，表示与特征对应的结果：
- en: '[PRE25]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s divide the data into testing and training data:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将数据分为测试数据和训练数据：
- en: '[PRE26]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To train the model, we are using the Naive Bayes algorithm that we studied
    in *Chapter 7*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们使用了在*第7章*中学习的朴素贝叶斯算法：
- en: '[PRE27]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s predict the test set results:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们预测测试集的结果：
- en: '[PRE28]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, let us print the confusion matrix. Remember that the confusion matrix
    is a table that helps visualize the performance of the classification model:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们打印混淆矩阵。记住，混淆矩阵是一个帮助可视化分类模型表现的表格：
- en: '[PRE29]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Looking at the confusion matrix, we can estimate the misclassification.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看混淆矩阵，我们可以估算误分类情况。
- en: Analyzing the results
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析结果
- en: 'The confusion matrix gives us a glimpse into the misclassifications made by
    our model. In this context, there are:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵让我们窥见了模型所做的误分类。在这个背景下，有：
- en: 55 true positives (correctly predicted positive reviews)
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 55个真正的正例（正确预测的正面评论）
- en: 42 false positives (incorrectly predicted positive reviews)
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 42个假正例（错误预测为正面的评论）
- en: 12 false negatives (incorrectly predicted negative reviews)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 12个假负例（错误预测为负面的评论）
- en: 91 true negatives (correctly predicted negative reviews)
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 91个真正的负例（正确预测的负面评论）
- en: The 55 true positives and 91 true negatives show that our model has a reasonable
    ability to distinguish between positive and negative reviews. However, the 42
    false positives and 12 false negatives highlight areas for potential improvement.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 55个真正的正例和91个真正的负例表明我们的模型具有合理的能力，能够区分正面和负面评论。然而，42个假正例和12个假负例突显了潜在的改进空间。
- en: In the context of restaurant reviews, understanding these numbers helps business
    owners and customers alike gauge the general sentiment. A high rate of true positives
    and true negatives indicates that the model can be trusted to give an accurate
    sentiment overview. This information could be invaluable for restaurants aiming
    to improve service or for potential customers seeking honest reviews. On the other
    hand, the presence of false positives and negatives suggests areas where the model
    might need fine-tuning to avoid misclassification and provide more accurate insights.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在餐厅评论的背景下，理解这些数字有助于商家和顾客评估整体情感。高比例的真正正例和真正负例表明模型能够被信任，提供准确的情感概述。这些信息对于想要提升服务的餐厅或寻求真实评论的潜在顾客来说，可能非常宝贵。另一方面，假正例和假负例的存在则表明模型可能需要调整，以避免误分类并提供更准确的洞察。
- en: Applications of NLP
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）的应用
- en: 'The continued advancement of NLP technology has revolutionized the way we interact
    with computers and other digital devices. It has made significant progress in
    recent years, with impressive achievements in many tasks, including:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）技术的持续进步彻底改变了我们与计算机和其他数字设备的互动方式。近年来，它在多个任务上取得了显著进展，并取得了令人印象深刻的成就，包括：
- en: '**Topic identification**: To discover topics in a text repository and then
    classify the documents in the repository according to the topics discovered.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题识别**：在文本库中发现主题，并根据发现的主题对库中的文档进行分类。'
- en: '**Sentiment analysis**: To classify the text according to the positive or negative
    sentiments that it contains.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：根据文本中的正面或负面情感对其进行分类。'
- en: '**Machine translation**: To translate between different languages.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：在不同语言之间进行翻译。'
- en: '**Text to speech**: To convert spoken words into text.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音转文本**：将口语转换为文本。'
- en: '**Question answering**: This is a process of understanding and responding to
    a query using the information that is available. It involves intelligently interpreting
    the question and providing a relevant answer based on the existing knowledge or
    data.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答**：这是通过使用可用信息来理解和回应查询的过程。它涉及智能地解读问题，并根据现有知识或数据提供相关的答案。'
- en: '**Entity recognition**: To identify entities (such as a person, place, or thing)
    from text.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实体识别**：从文本中识别实体（如人、地点或事物）。'
- en: '**Fake news detection**: To flag fake news based on the content.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假新闻检测**：根据内容标记假新闻。'
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The chapter discussed the basic terminology related to NLP, such as corpus,
    word embeddings, language modeling, machine translation, and sentiment analysis.
    In addition, the chapter covered various text preprocessing techniques that are
    essential in NLP, including tokenization, which involves breaking down text into
    smaller units called tokens, and other techniques such as stemming and stop word
    removal.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了与NLP相关的基本术语，如语料库、词向量、语言建模、机器翻译和情感分析。此外，本章还介绍了NLP中至关重要的各种文本预处理技术，包括分词，它将文本分解成称为标记的小单位，以及其他技术，如词干提取和去除停用词。
- en: The chapter also discussed word embeddings and then presented a use case on
    restaurant review sentiment analysis. Now, readers should have a better understanding
    of the fundamental techniques used in NLP and their potential applications to
    real-world problems.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还讨论了词向量，并展示了一个餐厅评论情感分析的案例。现在，读者应当对NLP中使用的基本技术及其在现实世界问题中的潜在应用有了更好的理解。
- en: In the next chapter, we will look at training neural networks for sequential
    data. We will also investigate how the use of deep learning can further improve
    NLP techniques and the methodologies discussed in this chapter.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论如何训练处理顺序数据的神经网络。我们还将探讨如何利用深度学习进一步改善自然语言处理（NLP）技术和本章讨论的方法论。
- en: Learn more on Discord
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解更多信息，请访问 Discord
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入本书的 Discord 社区 —— 你可以在这里分享反馈、向作者提问并了解新版本 —— 请扫描下面的二维码：
- en: '[https://packt.link/WHLel](https://packt.link/WHLel)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/WHLel](https://packt.link/WHLel)'
- en: '![](img/QR_Code1955211820597889031.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1955211820597889031.png)'
