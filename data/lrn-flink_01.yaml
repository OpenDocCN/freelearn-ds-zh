- en: Chapter 1. Introduction to Apache Flink
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。介绍Apache Flink
- en: With distributed technologies evolving all the time, engineers are trying to
    push those technologies to their limits. Earlier, people were looking for faster,
    cheaper ways to process data. This need was satisfied when Hadoop was introduced.
    Everyone started using Hadoop, started replacing their ETLs with Hadoop-bound
    ecosystem tools. Now that this need has been satisfied and Hadoop is used in production
    at so many companies, another need arose to process data in a streaming manner,
    which gave rise to technologies such as Apache Spark and Flink. Features, such
    as fast processing engines, the ability to scale in no time, and support for machine
    learning and graph technologies, are popularizing these technologies among the
    developer community.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着分布式技术不断发展，工程师们试图将这些技术推向极限。以前，人们正在寻找更快、更便宜的处理数据的方法。当Hadoop被引入时，这种需求得到了满足。每个人都开始使用Hadoop，开始用Hadoop生态系统工具替换他们的ETL。现在，这种需求已经得到满足，Hadoop在许多公司的生产中被使用，另一个需求出现了，即以流式方式处理数据，这催生了Apache
    Spark和Flink等技术。快速处理引擎、能够在短时间内扩展以及对机器学习和图技术的支持等功能，正在开发者社区中推广这些技术。
- en: Some of you might have been already using Apache Spark in your day-to-day life
    and might have been wondering if I have Spark why I need to use Flink? The question
    is quite expected and the comparison is natural. Let me try to answer that in
    brief. The very first thing we need to understand here is Flink is based on the **streaming
    first principle** which means it is real streaming processing engine and not a
    fast processing engine that collects streams as mini batches. Flink considers
    batch processing as a special case of streaming whereas it is vice-versa in the
    case of Spark. Likewise we will discover more such differences throughout this
    book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你们中的一些人可能已经在日常生活中使用Apache Spark，并且可能一直在想，如果我有Spark，为什么还需要使用Flink？这个问题是可以预料的，比较是自然的。让我试着简要回答一下。我们需要在这里理解的第一件事是，Flink基于**流式优先原则**，这意味着它是真正的流处理引擎，而不是将流作为小批量收集的快速处理引擎。Flink将批处理视为流处理的特例，而在Spark的情况下则相反。同样，我们将在本书中发现更多这样的区别。
- en: 'This book is about one of the most promising technologies--Apache Flink. In
    this chapter we are going to talk about the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是关于最有前途的技术之一--Apache Flink。在本章中，我们将讨论以下主题：
- en: History
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 历史
- en: Architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构
- en: Distributed execution
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式执行
- en: Features
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特性
- en: Quick start setup
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速启动设置
- en: Cluster setup
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群设置
- en: Running a sample application
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行一个示例应用程序
- en: History
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 历史
- en: Flink started as a research project named *Stratosphere* with the goal of building
    a next generation big data analytics platform at universities in the Berlin area.
    It was accepted as an Apache Incubator project on April 16, 2014\. Initial versions
    of Stratosphere were based on a research paper by Nephele at [http://stratosphere.eu/assets/papers/Nephele_09.pdf](http://stratosphere.eu/assets/papers/Nephele_09.pdf).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Flink作为一个名为*Stratosphere*的研究项目开始，旨在在柏林地区的大学建立下一代大数据分析平台。它于2014年4月16日被接受为Apache孵化器项目。Stratosphere的最初版本基于Nephele的研究论文[http://stratosphere.eu/assets/papers/Nephele_09.pdf](http://stratosphere.eu/assets/papers/Nephele_09.pdf)。
- en: 'The following diagram shows how the evolution of Stratosphere happened over
    time:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Stratosphere随时间的演变：
- en: '![History](img/image_01_001.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![历史](img/image_01_001.jpg)'
- en: The very first version of Stratosphere was focused on having a runtime, optimizer,
    and the Java API. Later, as the platform got more mature, it started supporting
    execution on various local environments as well as on **YARN**. From version 0.6,
    Stratosphere was renamed Flink. The latest versions of Flink are focused on supporting
    various features such as batch processing, stream processing, graph processing,
    machine learning, and so on.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Stratosphere的第一个版本主要关注运行时、优化器和Java API。随着平台的成熟，它开始支持在各种本地环境以及**YARN**上的执行。从0.6版本开始，Stratosphere更名为Flink。Flink的最新版本专注于支持各种功能，如批处理、流处理、图处理、机器学习等。
- en: Flink 0.7 introduced the most important feature of Flink that is, Flink's streaming
    API. Initially release only had the Java API. Later releases started supporting
    Scala API as well. Now let's look the current architecture of Flink in the next
    section.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 0.7引入了Flink最重要的功能，即Flink的流式API。最初的版本只有Java API。后来的版本开始支持Scala API。现在让我们在下一节中看一下Flink的当前架构。
- en: Architecture
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: 'Flink 1.X''s architecture consists of various components such as deploy, core
    processing, and APIs. We can easily compare the latest architecture with Stratosphere''s
    architecture and see its evolution. The following diagram shows the components,
    APIs, and libraries:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 1.X的架构包括各种组件，如部署、核心处理和API。我们可以轻松地将最新的架构与Stratosphere的架构进行比较，并看到它的演变。以下图表显示了组件、API和库：
- en: '![Architecture](img/image_01_002.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![架构](img/image_01_002.jpg)'
- en: Flink has a layered architecture where each component is a part of a specific
    layer. Each layer is built on top of the others for clear abstraction. Flink is
    designed to run on local machines, in a YARN cluster, or on the cloud. Runtime
    is Flink's core data processing engine that receives the program through APIs
    in the form of JobGraph. **JobGraph** is a simple parallel data flow with a set
    of tasks that produce and consume data streams.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Flink具有分层架构，其中每个组件都是特定层的一部分。每个层都建立在其他层之上，以清晰的抽象。Flink被设计为在本地机器、YARN集群或云上运行。运行时是Flink的核心数据处理引擎，通过API以JobGraph的形式接收程序。**JobGraph**是一个简单的并行数据流，其中包含一组产生和消费数据流的任务。
- en: The DataStream and DataSet APIs are the interfaces programmers can use for defining
    the Job. JobGraphs are generated by these APIs when the programs are compiled.
    Once compiled, the DataSet API allows the optimizer to generate the optimal execution
    plan while DataStream API uses a stream build for efficient execution plans.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DataStream和DataSet API是程序员用于定义作业的接口。当程序编译时，这些API生成JobGraphs。一旦编译完成，DataSet API允许优化器生成最佳执行计划，而DataStream
    API使用流构建进行高效的执行计划。
- en: The optimized JobGraph is then submitted to the executors according to the deployment
    model. You can choose a local, remote, or YARN mode of deployment. If you have
    a Hadoop cluster already running, it is always better to use a YARN mode of deployment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后根据部署模型将优化后的JobGraph提交给执行器。您可以选择本地、远程或YARN部署模式。如果已经运行了Hadoop集群，最好使用YARN部署模式。
- en: Distributed execution
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式执行
- en: Flink's distributed execution consists of two important processes, master and
    worker. When a Flink program is executed, various processes take part in the execution,
    namely **Job Manager**, **Task Manager**, and **Job Client**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Flink的分布式执行由两个重要的进程组成，即主节点和工作节点。当执行Flink程序时，各种进程参与执行，即作业管理器、任务管理器和作业客户端。
- en: 'The following diagram shows the Flink program execution:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Flink程序的执行：
- en: '![Distributed execution](img/image_01_003.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![分布式执行](img/image_01_003.jpg)'
- en: The Flink program needs to be submitted to a **Job Client**. The **Job Client**
    then submits the job to the **Job Manager**. It's the **Job Manager's** responsibility
    to orchestrate the resource allocation and job execution. The very first thing
    it does is allocate the required resources. Once the resource allocation is done,
    the task is submitted to the respective the **Task Manager**. On receiving the
    task, the **Task Manager** initiates a thread to start the execution. While the
    execution is in place, the Task Managers keep on reporting the change of states
    to the **Job Manager**. There can be various states such as starting the execution,
    in progress, or finished. Once the job execution is complete, the results are
    sent back to the client.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Flink程序需要提交给作业客户端。然后作业客户端将作业提交给作业管理器。作业管理器负责编排资源分配和作业执行。它的第一件事是分配所需的资源。资源分配完成后，任务被提交给相应的任务管理器。收到任务后，任务管理器启动线程开始执行。在执行过程中，任务管理器不断向作业管理器报告状态的变化。可能有各种状态，如执行开始、进行中或已完成。作业执行完成后，结果被发送回客户端。
- en: Job Manager
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作业管理器
- en: The **master** processes, also known as **Job Managers**, coordinate and manage
    the execution of the program. Their main responsibilities include scheduling tasks,
    managing checkpoints, failure recovery, and so on.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 主进程，也称为作业管理器，协调和管理程序的执行。它们的主要职责包括调度任务、管理检查点、故障恢复等。
- en: There can be many Masters running in parallel and sharing these responsibilities.
    This helps in achieving high availability. One of the masters needs to be the
    leader. If the leader node goes down, the master node (standby) will be elected
    as leader.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可以并行运行多个主节点并共享这些责任。这有助于实现高可用性。其中一个主节点需要成为领导者。如果领导节点宕机，备用主节点将被选举为领导者。
- en: 'The Job Manager consists of the following important components:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作业管理器包括以下重要组件：
- en: Actor system
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: actor系统
- en: Scheduler
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度器
- en: Check pointing
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点
- en: Flink internally uses the **Akka** actor system for communication between the
    Job Managers and the Task Managers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Flink在内部使用Akka actor系统在作业管理器和任务管理器之间进行通信。
- en: Actor system
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: actor系统
- en: An actor system is a container of actors with various roles. It provides services
    such as scheduling, configuration, logging, and so on. It also contains a thread
    pool from where all actors are initiated. All actors reside in a hierarchy. Each
    newly created actor would be assigned to a parent. Actors talk to each other using
    a messaging system. Each actor has its own mailbox from where it reads all the
    messages. If the actors are local, the messages are shared through shared memory
    but if the actors are remote then messages are passed thought RPC calls.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: actor系统是具有各种角色的actor的容器。它提供诸如调度、配置、日志记录等服务。它还包含一个线程池，所有actor都是从中初始化的。所有actor都驻留在一个层次结构中。每个新创建的actor都会分配给一个父级。actor之间使用消息系统进行通信。每个actor都有自己的邮箱，从中读取所有消息。如果actor是本地的，消息通过共享内存共享，但如果actor是远程的，消息则通过RPC调用传递。
- en: 'Each parent is responsible for the supervision of its children. If any error
    happens with the children, the parent gets notified. If an actor can solve its
    own problem then it can restart its children. If it cannot solve the problem then
    it can escalate the issue to its own parent:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 每个父级负责监督其子级。如果子级出现任何错误，父级会收到通知。如果actor能够解决自己的问题，它可以重新启动其子级。如果无法解决问题，则可以将问题升级给自己的父级：
- en: '![Actor system](img/image_01_004.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![actor系统](img/image_01_004.jpg)'
- en: In Flink, an actor is a container having state and behavior. An actor's thread
    sequentially keeps on processing the messages it will receive in its mailbox.
    The state and the behavior are determined by the message it has received.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在Flink中，actor是一个具有状态和行为的容器。actor的线程会顺序地处理它在邮箱中接收到的消息。状态和行为由它接收到的消息确定。
- en: Scheduler
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调度器
- en: Executors in Flink are defined as task slots. Each Task Manager needs to manage
    one or more task slots. Internally, Flink decides which tasks needs to share the
    slot and which tasks must be placed into a specific slot. It defines that through
    the **SlotSharingGroup** and **CoLocationGroup**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在Flink中，执行器被定义为任务槽。每个任务管理器需要管理一个或多个任务槽。在内部，Flink决定哪些任务需要共享槽，哪些任务必须放入特定的槽中。它通过SlotSharingGroup和CoLocationGroup来定义。
- en: Check pointing
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查点
- en: 'Check pointing is Flink''s backbone for providing consistent fault tolerance.
    It keeps on taking consistent snapshots for distributed data streams and executor
    states. It is inspired by the Chandy-Lamport algorithm but has been modified for
    Flink''s tailored requirement. The details about the Chandy-Lamport algorithm
    can be found at: [http://research.microsoft.com/en-us/um/people/lamport/pubs/chandy.pdf](http://research.microsoft.com/en-us/um/people/lamport/pubs/chandy.pdf).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点是Flink提供一致性容错的支柱。它不断为分布式数据流和执行器状态进行一致的快照。它受Chandy-Lamport算法的启发，但已经修改以满足Flink的定制要求。有关Chandy-Lamport算法的详细信息可以在以下网址找到：[http://research.microsoft.com/en-us/um/people/lamport/pubs/chandy.pdf](http://research.microsoft.com/en-us/um/people/lamport/pubs/chandy.pdf)。
- en: 'The exact implementation details about snapshotting are provided in the following
    research paper: *Lightweight Asynchronous Snapshots for Distributed Dataflows
    (*[http://arxiv.org/abs/1506.08603](http://arxiv.org/abs/1506.08603)).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有关快照实现细节的详细信息可以在以下研究论文中找到：*Lightweight Asynchronous Snapshots for Distributed
    Dataflows (*[http://arxiv.org/abs/1506.08603](http://arxiv.org/abs/1506.08603))。
- en: The fault-tolerant mechanism keeps on creating lightweight snapshots for the
    data flows. They therefore continue the functionality without any significant
    over-burden. Generally the state of the data flow is kept in a configured place
    such as HDFS.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 容错机制不断为数据流创建轻量级快照。因此，它们在没有显着负担的情况下继续功能。通常，数据流的状态保存在配置的位置，如HDFS。
- en: In case of any failure, Flink stops the executors and resets them and starts
    executing from the latest available checkpoint.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在发生故障时，Flink会停止执行器并重置它们，然后从最新可用的检查点开始执行。
- en: 'Stream barriers are core elements of Flink''s snapshots. They are ingested
    into data streams without affecting the flow. Barriers never overtake the records.
    They group sets of records into a snapshot. Each barrier carries a unique ID.
    The following diagram shows how the barriers are injected into the data stream
    for snapshots:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 流障是Flink快照的核心元素。它们被吸收到数据流中而不影响流程。障碍永远不会超越记录。它们将一组记录分组成一个快照。每个障碍都携带一个唯一的ID。以下图表显示了障碍如何被注入到数据流中进行快照：
- en: '![Check pointing](img/image_01_005.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![检查点](img/image_01_005.jpg)'
- en: Each snapshot state is reported to the Flink **Job Manager's** checkpoint coordinator.
    While drawing snapshots, Flink handles the alignment of records in order to avoid
    re-processing the same records because of any failure. This alignment generally
    takes some milliseconds. But for some intense applications, where even millisecond
    latency is not acceptable, we have an option to choose low latency over exactly
    a single record processing. By default, Flink processes each record exactly once.
    If any application needs low latency and is fine with at least a single delivery,
    we can switch off that trigger. This will skip the alignment and will improve
    the latency.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 每个快照状态都报告给Flink的**作业管理器**的检查点协调器。在绘制快照时，Flink处理记录的对齐，以避免由于任何故障而重新处理相同的记录。这种对齐通常需要一些毫秒。但对于一些强烈的应用程序，即使毫秒级的延迟也是不可接受的，我们可以选择低延迟而不是精确的单个记录处理。默认情况下，Flink会精确处理每个记录一次。如果任何应用程序需要低延迟，并且可以接受至少一次交付，我们可以关闭该触发器。这将跳过对齐并提高延迟。
- en: Task manager
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务管理器
- en: 'Task managers are worker nodes that execute the tasks in one or more threads
    in JVM. Parallelism of task execution is determined by the task slots available
    on each Task Manager. Each task represents a set of resources allocated to the
    task slot. For example, if a Task Manager has four slots then it will allocate
    25% of the memory to each slot. There could be one or more threads running in
    a task slot. Threads in the same slot share the same JVM. Tasks in the same JVM
    share TCP connections and heart beat messages:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 任务管理器是在JVM中以一个或多个线程执行任务的工作节点。任务管理器上的任务执行的并行性由每个任务管理器上可用的任务槽确定。每个任务代表分配给任务槽的一组资源。例如，如果一个任务管理器有四个槽，那么它将为每个槽分配25%的内存。一个任务槽中可能运行一个或多个线程。同一槽中的线程共享相同的JVM。同一JVM中的任务共享TCP连接和心跳消息：
- en: '![Task manager](img/image_01_006.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![任务管理器](img/image_01_006.jpg)'
- en: Job client
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作业客户端
- en: The job client is not an internal part of Flink's program execution but it is
    the starting point of the execution. The job client is responsible for accepting
    the program from the user and then creating a data flow and then submitting the
    data flow to the Job Manager for further execution. Once the execution is completed,
    the job client provides the results back to the user.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 作业客户端不是Flink程序执行的内部部分，而是执行的起点。作业客户端负责接受用户的程序，然后创建数据流，然后将数据流提交给作业管理器进行进一步执行。执行完成后，作业客户端将结果提供给用户。
- en: 'A data flow is a plan of execution. Consider a very simple word count program:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流是执行计划。考虑一个非常简单的单词计数程序：
- en: '![Job client](img/image_01_007.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![作业客户端](img/image_01_007.jpg)'
- en: 'When a client accepts the program from the user, it then transforms it into
    a data flow. The data flow for the aforementioned program may look like this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当客户端接受用户的程序时，然后将其转换为数据流。上述程序的数据流可能如下所示：
- en: '![Job client](img/image_01_008.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![作业客户端](img/image_01_008.jpg)'
- en: The preceding diagram shows how a program gets transformed into a data flow.
    Flink data flows are parallel and distributed by default. For parallel data processing,
    Flink partitions the operators and streams. Operator partitions are called sub-tasks.
    Streams can distribute the data in a one-to-one or a re-distributed manner.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了程序如何转换为数据流。Flink数据流默认是并行和分布式的。对于并行数据处理，Flink对操作符和流进行分区。操作符分区称为子任务。流可以以一对一或重新分布的方式分发数据。
- en: 'The data flows directly from the source to the map operators as there is no
    need to shuffle the data. But for a GroupBy operation Flink may need to redistribute
    the data by keys in order to get the correct results:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据直接从源流向映射操作符，因此无需洗牌数据。但对于GroupBy操作，Flink可能需要按键重新分发数据以获得正确的结果。
- en: '![Job client](img/image_01_009.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![作业客户端](img/image_01_009.jpg)'
- en: Features
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特点
- en: In the earlier sections, we tried to understand the Flink architecture and its
    execution model. Because of its robust architecture, Flink is full of various
    features.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们试图了解Flink的架构和其执行模型。由于其健壮的架构，Flink充满了各种功能。
- en: High performance
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高性能
- en: Flink is designed to achieve high performance and low latency. Unlike other
    streaming frameworks such as Spark, you don't need to do many manual configurations
    to get the best performance. Flink's pipelined data processing gives better performance
    compared to its counterparts.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Flink旨在实现高性能和低延迟。与Spark等其他流处理框架不同，您不需要进行许多手动配置以获得最佳性能。Flink的流水线数据处理与其竞争对手相比具有更好的性能。
- en: Exactly-once stateful computation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精确一次性有状态计算
- en: As we discussed in the previous section, Flink's distributed checkpoint processing
    helps to guarantee processing each record exactly once. In the case of high-throughput
    applications, Flink provides us with a switch to allow at least once processing.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中讨论的，Flink的分布式检查点处理有助于确保每个记录的处理仅一次。在高吞吐量应用程序的情况下，Flink为我们提供了一个开关，允许至少一次处理。
- en: Flexible streaming windows
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 灵活的流式窗口
- en: Flink supports data-driven windows. This means we can design a window based
    on time, counts, or sessions. A window can also be customized which allows us
    to detect specific patterns in event streams.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持数据驱动的窗口。这意味着我们可以基于时间、计数或会话设计窗口。窗口也可以定制，这使我们能够在事件流中检测特定模式。
- en: Fault tolerance
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容错
- en: Flink's distributed, lightweight snapshot mechanism helps in achieving a great
    degree of fault tolerance. It allows Flink to provide high-throughput performance
    with guaranteed delivery.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Flink的分布式、轻量级快照机制有助于实现很高程度的容错。它允许Flink提供高吞吐量性能和可靠的传递。
- en: Memory management
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存管理
- en: Flink is supplied with its own memory management inside a JVM which makes it
    independent of Java's default garbage collector. It efficiently does memory management
    by using hashing, indexing, caching, and sorting.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Flink配备了自己的内存管理，位于JVM内部，这使其独立于Java的默认垃圾收集器。它通过使用哈希、索引、缓存和排序有效地进行内存管理。
- en: Optimizer
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化器
- en: Flink's batch data processing API is optimized in order to avoid memory-consuming
    operations such as shuffle, sort, and so on. It also makes sure that caching is
    used in order to avoid heavy disk IO operations.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免消耗大量内存的操作（如洗牌、排序等），Flink的批处理数据处理API进行了优化。它还确保使用缓存以避免大量的磁盘IO操作。
- en: Stream and batch in one platform
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流和批处理在一个平台上
- en: Flink provides APIs for both batch and stream data processing. So once you set
    up the Flink environment, it can host stream and batch processing applications
    easily. In fact Flink works on Streaming first principle and considers batch processing
    as the special case of streaming.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Flink提供了用于批处理和流处理数据的API。因此，一旦设置了Flink环境，它就可以轻松托管流和批处理应用程序。事实上，Flink首先考虑流处理，并将批处理视为流处理的特例。
- en: Libraries
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 库
- en: Flink has a rich set of libraries to do machine learning, graph processing,
    relational data processing, and so on. Because of its architecture, it is very
    easy to perform complex event processing and alerting. We are going to see more
    about these libraries in subsequent chapters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Flink拥有丰富的库，可用于机器学习、图处理、关系数据处理等。由于其架构，执行复杂事件处理和警报非常容易。我们将在后续章节中更多地了解这些库。
- en: Event time semantics
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事件时间语义
- en: Flink supports event time semantics. This helps in processing streams where
    events arrive out of order. Sometimes events may come delayed. Flink's architecture
    allows us to define windows based on time, counts, and sessions, which helps in
    dealing with such scenarios.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持事件时间语义。这有助于处理事件到达顺序混乱的流。有时事件可能会延迟到达。Flink的架构允许我们基于时间、计数和会话定义窗口，这有助于处理这种情况。
- en: Quick start setup
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速开始设置
- en: Now that we understand the details about Flink's architecture and its process
    model, it's time to get started with a quick setup and try out things on our own.
    Flink works on both Windows and Linux machines.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了Flink的架构和其过程模型的细节，是时候开始快速设置并自己尝试一些东西了。Flink可以在Windows和Linux机器上运行。
- en: 'The very first thing we need to do is to download Flink''s binaries. Flink
    can be downloaded from the Flink download page at: [http://flink.apache.org/downloads.html](http://flink.apache.org/downloads.html).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是下载Flink的二进制文件。Flink可以从Flink下载页面下载：[http://flink.apache.org/downloads.html](http://flink.apache.org/downloads.html)。
- en: 'On the download page, you will see multiple options as shown in the following
    screenshot:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在下载页面上，您将看到多个选项，如下面的截图所示：
- en: '![Quick start setup](img/image_01_010.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![快速开始设置](img/image_01_010.jpg)'
- en: In order to install Flink, you don't need to have Hadoop installed. But in case
    you need to connect to Hadoop using Flink then you need to download the exact
    binary that is compatible with the Hadoop version you have with you.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安装Flink，您不需要安装Hadoop。但是，如果您需要使用Flink连接到Hadoop，那么您需要下载与您拥有的Hadoop版本兼容的确切二进制文件。
- en: As I have latest version of **Hadoop 2.7.0** installed with me, I am going to
    download the Flink binary compatible with Hadoop 2.7.0 and built on Scala 2.11.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我已经安装了最新版本的**Hadoop 2.7.0**，我将下载与Hadoop 2.7.0兼容并基于Scala 2.11构建的Flink二进制文件。
- en: 'Here is direct link to download:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是直接下载链接：
- en: '[http://www-us.apache.org/dist/flink/flink-1.1.4/flink-1.1.4-bin-hadoop27-scala_2.11.tgz](http://www-us.apache.org/dist/flink/flink-1.1.4/flink-1.1.4-bin-hadoop27-scala_2.11.tgz)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www-us.apache.org/dist/flink/flink-1.1.4/flink-1.1.4-bin-hadoop27-scala_2.11.tgz](http://www-us.apache.org/dist/flink/flink-1.1.4/flink-1.1.4-bin-hadoop27-scala_2.11.tgz)'
- en: Pre-requisite
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先决条件
- en: 'Flink needs Java to be installed first. So before you start, please make sure
    Java is installed. I have JDK 1.8 installed on my machine:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Flink需要首先安装Java。因此，在开始之前，请确保已安装Java。我在我的机器上安装了JDK 1.8：
- en: '![Pre-requisite](img/image_01_011.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![先决条件](img/image_01_011.jpg)'
- en: Installing on Windows
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Windows上安装
- en: Flink installation is very easy to install. Just extract the compressed file
    and store it on the desired location.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Flink安装非常容易。只需提取压缩文件并将其存储在所需位置。
- en: 'Once extracted, go to the folder and execute `start-local.bat`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 提取后，转到文件夹并执行`start-local.bat`：
- en: '[PRE0]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And you will see that the local instance of Flink has started.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您会看到Flink的本地实例已经启动。
- en: 'You can also check the web UI on `http://localhost:8081/`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在`http://localhost:8081/`上检查Web UI：
- en: '![Installing on Windows](img/image_01_012.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![在Windows上安装](img/image_01_012.jpg)'
- en: You can stop the Flink process by pressing *Cltr* + *C*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过按下*Cltr* + *C*来停止Flink进程。
- en: Installing on Linux
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Linux上安装
- en: 'Similar to Windows, installing Flink on Linux machines is very easy. We need
    to download the binary, place it in a specific folder, extract, and finish:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与Windows类似，在Linux机器上安装Flink非常容易。我们需要下载二进制文件，将其放在特定文件夹中，然后进行提取和完成：
- en: '[PRE1]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As in Windows, please make sure Java is installed on the machine.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与Windows一样，请确保Java已安装在机器上。
- en: 'Now we are all set to submit a Flink job. To stop the local Flink instance
    on Linux, execute following command:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好提交一个Flink作业。要停止Linux上的本地Flink实例，请执行以下命令：
- en: '[PRE2]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Cluster setup
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群设置
- en: Setting up a Flink cluster is very simple as well. Those who have a background
    of installing a Hadoop cluster will be able to relate to these steps very easily.
    In order to set up the cluster, let's assume we have four Linux machines with
    us, each having a moderate configuration. At least two cores and 4 GB RAM machines
    would be a good option to get started.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 设置Flink集群也非常简单。那些有安装Hadoop集群背景的人将能够非常容易地理解这些步骤。为了设置集群，让我们假设我们有四台Linux机器，每台机器都有适度的配置。至少两个核心和4
    GB RAM的机器将是一个很好的选择来开始。
- en: 'The very first thing we need to do this is to choose the cluster design. As
    we have four machines, we will use one machine as the **Job Manager** and the
    other three machines as the **Task Managers**:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是选择集群设计。由于我们有四台机器，我们将使用一台机器作为**作业管理器**，另外三台机器作为**任务管理器**：
- en: '![Cluster setup](img/image_01_013.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![集群设置](img/image_01_013.jpg)'
- en: SSH configurations
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SSH配置
- en: 'In order to set up the cluster, we first need to do password less connections
    to the Task Manager from the Job Manager machine. The following steps needs to
    be performed on the Job Manager machine which creates an SSH key and copies it
    to `authorized_keys`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置集群，我们首先需要在作业管理器机器上进行无密码连接到任务管理器。需要在创建SSH密钥并将其复制到`authorized_keys`上执行以下步骤：
- en: '[PRE3]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will generate the public and private keys in the `/home/flinkuser/.ssh` folder.
    Now copy the public key to the Task Manager machine and perform the following
    steps on the Task Manager to allow password less connection from the Job Manager:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在`/home/flinkuser/.ssh`文件夹中生成公钥和私钥。现在将公钥复制到任务管理器机器，并在任务管理器上执行以下步骤，以允许从作业管理器进行无密码连接：
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Make sure the keys have restricted access by executing the following commands:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 确保密钥通过执行以下命令具有受限访问权限：
- en: '[PRE5]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now you can test the password less SSH connection from the Job Manager machine:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以从作业管理器机器测试无密码SSH连接：
- en: '[PRE6]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Tip
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'If you are using any cloud service instances for the installations, please
    make sure that the ROOT login is enabled from SSH. In order to do this, you need
    to login to each machine: `open file /etc/ssh/sshd_config`. Then change the value
    to `PermitRootLogin yes`. Once you save the file, restart the SSH service by executing
    the command: `sudo service sshd restart`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用任何云服务实例进行安装，请确保从SSH启用了ROOT登录。为了做到这一点，您需要登录到每台机器：`打开文件/etc/ssh/sshd_config`。然后将值更改为`PermitRootLogin
    yes`。保存文件后，通过执行命令重新启动SSH服务：`sudo service sshd restart`
- en: Java installation
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Java安装
- en: Next we need to install Java on each machine. The following command will help
    you install Java on Redhat/CentOS based UNIX machines.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要在每台机器上安装Java。以下命令将帮助您在基于Redhat/CentOS的UNIX机器上安装Java。
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next we need to set up the `JAVA_HOME` environment variable so that Java is
    available to access from everywhere.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要设置`JAVA_HOME`环境变量，以便Java可以从任何地方访问。
- en: 'Create a `java.sh` file:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个`java.sh`文件：
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And add following content in it and save it:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 并添加以下内容并保存：
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Make the file executable and source it:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使文件可执行并对其进行源操作：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can now check if Java is installed properly:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以检查Java是否已正确安装：
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Repeat these installations steps on the Job Manager and Task Manager machines.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在作业管理器和任务管理器机器上重复这些安装步骤。
- en: Flink installation
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flink安装
- en: Once SSH and Java installation is done, we need to download Flink binaries and
    extract them into a specific folder. Please make a note that the installation
    directory on all nodes should be same.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦SSH和Java安装完成，我们需要下载Flink二进制文件并将其提取到特定文件夹中。请注意，所有节点上的安装目录应该相同。
- en: 'So let''s get started:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们开始吧：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that the binary is ready, we need to do some configurations.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在二进制文件已经准备好，我们需要进行一些配置。
- en: Configurations
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置
- en: Flink's configurations are simple. We need to tune a few parameters and we are
    all set. Most of the configurations are same for the Job Manager node and the
    Task Manager node. All configurations are done in the `conf/flink-conf.yaml` file.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Flink的配置很简单。我们需要调整一些参数，然后就可以了。大多数配置对作业管理器节点和任务管理器节点都是相同的。所有配置都在`conf/flink-conf.yaml`文件中完成。
- en: 'The following is a configuration file for a Job Manager node:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是作业管理器节点的配置文件：
- en: '[PRE13]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You may want to change memory configurations for the Job Manager and Task Manager
    based on your node configurations. For the Task Manager, `jobmanager.rpc.address`
    should be populated with the correct Job Manager hostname or IP address.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望根据节点配置更改作业管理器和任务管理器的内存配置。对于任务管理器，`jobmanager.rpc.address`应填入正确的作业管理器主机名或IP地址。
- en: 'So for all Task Managers, the configuration file should be like the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于所有任务管理器，配置文件应如下所示：
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We need to add the `JAVA_HOME` details in this file so that Flink knows exactly
    where to look for Java binaries:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在此文件中添加`JAVA_HOME`详细信息，以便Flink确切知道从何处查找Java二进制文件：
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We also need to add the slave node details in the `conf/slaves` file, with each
    node on a separate new line.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要在`conf/slaves`文件中添加从节点的详细信息，每个节点占据一个新的单独行。
- en: 'Here is how a sample `conf/slaves` file should look like:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 示例`conf/slaves`文件应如下所示：
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Starting daemons
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动守护程序
- en: 'Now the only thing left is starting the Flink processes. We can start each
    process separately on individual nodes or we can execute the `start-cluster.sh`
    command to start the required processes on each node:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在唯一剩下的就是启动Flink进程。 我们可以在各个节点上分别启动每个进程，也可以执行`start-cluster.sh`命令在每个节点上启动所需的进程：
- en: '[PRE17]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If all the configurations are good, then you would see that the cluster is up
    and running. You can check the web UI at `http://<job-manager-ip>:8081/` .
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有配置都正确，那么您会看到集群正在运行。 您可以在`http://<job-manager-ip>:8081/`上检查Web UI。
- en: 'The following are some snapshots of the Flink Web UI:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Flink Web UI的一些快照：
- en: '![Starting daemons](img/image_01_014.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![启动守护程序](img/image_01_014.jpg)'
- en: 'You can click on the **Job Manager** link to get the following view:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以单击**作业管理器**链接以获取以下视图：
- en: '![Starting daemons](img/image_01_015.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![启动守护程序](img/image_01_015.jpg)'
- en: 'Similarly, you can check out the **Task Managers** view as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以按以下方式查看**任务管理器**视图：
- en: '![Starting daemons](img/image_01_016.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![启动守护程序](img/image_01_016.jpg)'
- en: Adding additional Job/Task Managers
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加额外的作业/任务管理器
- en: Flink provides you with the facility to add additional instances of Job and
    Task Managers to the running cluster.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Flink为您提供了向正在运行的集群添加额外的作业和任务管理器实例的功能。
- en: Before we start the daemon, please make sure that you have followed the steps
    given previously.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动守护程序之前，请确保您已按照先前给出的步骤进行操作。
- en: 'To add an additional Job Manager to the existing cluster, execute the following
    command:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要向现有集群添加额外的作业管理器，请执行以下命令：
- en: '[PRE18]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Similarly, we need to execute the following command to add an additional Task
    Manager:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们需要执行以下命令以添加额外的任务管理器：
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Stopping daemons and cluster
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停止守护程序和集群
- en: Once the job execution is completed, you want to shut down the cluster. The
    following commands are used for that.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 作业执行完成后，您希望关闭集群。 以下命令用于此目的。
- en: 'To stop the complete cluster in one go:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要一次停止整个集群：
- en: '[PRE20]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To stop the individual Job Manager:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止单个作业管理器：
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To stop the individual Task Manager:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止单个任务管理器：
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Running sample application
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行示例应用程序
- en: Flink binaries come with a sample application which can be used as it is. Let's
    start with a very simple application, word count. Here we are going try a streaming
    application which reads data from the netcat server on a specific port.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Flink二进制文件附带了一个示例应用程序，可以直接使用。 让我们从一个非常简单的应用程序开始，单词计数。 在这里，我们将尝试一个从特定端口上的netcat服务器读取数据的流式应用程序。
- en: 'So let''s get started. First start the netcat server on port `9000` by executing
    the following command:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。 首先通过执行以下命令在端口`9000`上启动netcat服务器：
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now the netcat server will be start listening on port 9000 so whatever you type
    on the command prompt will be sent to the Flink processing.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在netcat服务器将开始监听端口9000，所以无论您在命令提示符上输入什么都将被发送到Flink处理中。
- en: 'Next we need to start the Flink sample program to listen to the netcat server.
    The following is the command:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要启动Flink示例程序以侦听netcat服务器。 以下是命令：
- en: '[PRE24]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This will start the Flink job execution. Now you can type something on the netcat
    console and Flink will process it.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动Flink作业执行。 现在您可以在netcat控制台上输入一些内容，Flink将对其进行处理。
- en: 'For example, type the following on the netcat server:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在netcat服务器上键入以下内容：
- en: '[PRE25]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can verify the output in logs:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在日志中验证输出：
- en: '[PRE26]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You can also checkout the Flink Web UI to see how your job is performing. The
    following screenshot shows the data flow plan for the execution:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以查看Flink Web UI，以查看作业的执行情况。 以下屏幕截图显示了执行的数据流计划：
- en: '![Running sample application](img/image_01_017.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![运行示例应用程序](img/image_01_017.jpg)'
- en: Here for the job execution, Flink has two operators. The first is the source
    operator which reads data from the Socket stream. The second operator is the transformation
    operator which aggregates counts of words.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在作业执行中，Flink有两个运算符。 第一个是源运算符，它从Socket流中读取数据。 第二个运算符是转换运算符，它聚合单词的计数。
- en: 'We can also look at the timeline of the job execution:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看作业执行的时间轴：
- en: '![Running sample application](img/image_01_018.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![运行示例应用程序](img/image_01_018.jpg)'
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we talked about how Flink started as a university project and
    then became a full-fledged enterprise-ready data processing platform. We looked
    at the details of Flink's architecture and how its process model works. We also
    learnt how to run Flink in local and cluster modes.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了Flink如何作为大学项目开始，然后成为一款成熟的企业级数据处理平台。 我们查看了Flink架构的细节以及其处理模型的工作原理。 我们还学会了如何在本地和集群模式下运行Flink。
- en: In the next chapter, we are going to learn about Flink's Streaming API and look
    at its details and how can we use that API to solve our data streaming processing
    problems.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习Flink的流式API，并查看其细节以及如何使用该API来解决我们的数据流处理问题。
