- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Ensembling and Stacking
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成和堆叠
- en: In the previous chapter, we looked at a few machine learning algorithms and
    used them to generate forecasts on the London Smart Meters dataset. Now that we
    have multiple forecasts for all the households in the dataset, how do we come
    up with a single forecast by choosing or combining these different forecasts?
    At the end of the day, we can only have one forecast that will be used for planning
    whatever task for which you are forecasting. That is what we will be doing in
    this chapter—we will learn how to leverage combinatorial and mathematical optimization
    to come up with a single forecast.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们查看了几种机器学习算法，并使用它们对伦敦智能电表数据集进行了预测。现在，我们对数据集中的所有家庭生成了多个预测，如何通过选择或结合这些不同的预测来得出一个单一的预测呢？最终，我们只能拥有一个预测，用于规划你正在预测的任务。这就是我们在本章中要做的——我们将学习如何利用组合和数学优化来得出一个单一的预测。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Strategies for combining forecasts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合预测的策略
- en: Stacking or blending
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠或混合
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need to set up the Anaconda environment following the instructions
    in the *Preface* of the book to get a working environment with all the libraries
    and datasets required for the code in this book. Any additional library will be
    installed while running the notebooks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要按照本书*前言*中的说明设置Anaconda环境，以获得一个包含所有代码所需库和数据集的工作环境。任何额外的库将在运行笔记本时安装。
- en: 'You will need to run the following notebooks before using the code in this
    chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在使用本章代码之前运行以下笔记本：
- en: '`02-Preprocessing_London_Smart_Meter_Dataset.ipynb` in `Chapter02`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02-处理伦敦智能电表数据集.ipynb` 在 `Chapter02` 中'
- en: '`01-Setting_up_Experiment_Harness.ipynb` in `Chapter04`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01-设置实验框架.ipynb` 在 `Chapter04` 中'
- en: '`02-Baseline_Forecasts_using_darts.ipynb` in `Chapter04`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02-使用darts的基准预测.ipynb` 在 `Chapter04` 中'
- en: '`01-Feature_Engineering.ipynb` in `Chapter06`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01-特征工程.ipynb` 在 `Chapter06` 中'
- en: '`02-Dealing_with_Non-Stationarity.ipynb` in `Chapter07`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02-处理非平稳性.ipynb` 在 `Chapter07` 中'
- en: '`02a-Dealing_with_Non-Stationarity-Train+Val.ipynb` in `Chapter07`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02a-处理非平稳性-训练+验证.ipynb` 在 `Chapter07` 中'
- en: '`00-Single_Step_Backtesting_Baselines.ipynb` in `Chapter08`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`00-单步回测基准.ipynb` 在 `Chapter08` 中'
- en: '`01-Forecasting_with_ML.ipynb` in `Chapter08`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01-使用机器学习进行预测.ipynb` 在 `Chapter08` 中'
- en: '`01a-Forecasting_with_ML_for_Test_Dataset.ipynb` in `Chapter08`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01a-使用机器学习进行测试数据集预测.ipynb` 在 `Chapter08` 中'
- en: '`02-Forecasting_with_Target_Transformation.ipynb` in `Chapter08`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02-使用目标转换进行预测.ipynb` 在 `Chapter08` 中'
- en: '`02a-Forecasting_with_Target_Transformation(Test).ipynb` in `Chapter08`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02a-使用目标转换进行预测（测试）.ipynb` 在 `Chapter08` 中'
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter09](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter09).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在 [https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter09](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter09)
    找到。
- en: Combining forecasts
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合预测
- en: 'We have generated forecasts using many techniques—some univariate, some machine
    learning, and so on. But at the end of the day, we would need a single forecast,
    and that means choosing a forecast or combining a variety. The most straightforward
    option is to choose the algorithm that does the best in the validation dataset,
    which in our case is `LightGBM`. We can think of this *selection* as another function
    that takes the forecasts that we generated as inputs and combines them into a
    final forecast. Mathematically, this can be represented as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用许多技术生成了预测——有些是单变量的，有些是机器学习的，等等。但最终，我们需要一个单一的预测，这意味着选择一个预测或结合多种预测。最简单的选择是选择在验证数据集中表现最好的算法，在我们的案例中是
    `LightGBM`。我们可以将这种*选择*看作是另一个函数，它接受我们生成的预测作为输入并将它们合并成一个最终的预测。从数学角度来看，可以表示如下：
- en: '*Y* = *F*(*Y*[1], *Y*[2], …, *Y*[N])'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y* = *F*(*Y*[1], *Y*[2], …, *Y*[N])'
- en: Here, *F* is the function that combines *N* forecasts. We can use the *F* function
    to choose the best-performing model in the validation dataset. However, this function
    can be as complex as it wants to be, and choosing the right *F* function while
    balancing bias and variance is a must.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*F* 是一个结合 *N* 个预测的函数。我们可以使用 *F* 函数来选择验证数据集中表现最好的模型。然而，这个函数可以非常复杂，选择一个合适的
    *F* 函数，同时平衡偏差和方差是必须的。
- en: '**Notebook alert:**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提示：**'
- en: To follow along with the code, use the `01-Forecast_Combinations.ipynb` notebook
    in the `Chapter09` folder.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随代码进行操作，请在`Chapter09`文件夹中使用`01-Forecast_Combinations.ipynb`笔记本。
- en: 'We will start by loading all the forecasts (both the validation and test forecasts)
    and the corresponding metrics for all the forecasts we have generated so far and
    combining them into `pred_val_df` and `pred_test_df`. Now, we must reshape the
    DataFrame using `pd.pivot` to get it into the shape we want. Up to this point,
    we have been tracking multiple metrics. But to meet this objective, we will need
    to choose one. For this exercise, we are going to choose the MAE as the metric.
    The validation metrics can be combined and reshaped into `metrics_combined_df`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从加载所有预测（包括验证和测试预测）以及到目前为止生成的所有预测对应的指标开始，并将它们合并到`pred_val_df`和`pred_test_df`中。接下来，我们必须使用`pd.pivot`重塑DataFrame，以便获得我们想要的形状。到目前为止，我们一直在跟踪多个指标。但为了实现这个目标，我们需要选择一个指标。在这个练习中，我们选择MAE作为指标。验证指标可以合并并重塑成`metrics_combined_df`：
- en: '![Figure 9.1 – Reshaped predictions DataFrame ](img/B22389_09_01.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – 重塑后的预测 DataFrame](img/B22389_09_01.png)'
- en: 'Figure 9.1: Reshaped predictions DataFrame'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：重塑后的预测 DataFrame
- en: Now, let’s look at some different strategies for combining the forecasts.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看结合预测的几种不同策略。
- en: Best fit
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳拟合
- en: 'This strategy for choosing the best forecast is by far the most popular and
    is as simple as choosing the best forecast for each time series based on the validation
    metrics. This strategy has been made popular by many automated forecasting software
    tools, which call this the “best fit” forecast. The algorithm is very simple:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种选择最佳预测的策略迄今为止是最流行的，其方法非常简单：根据验证指标为每个时间序列选择最佳预测。这种策略已被许多自动化预测软件工具所采用，称其为“最佳拟合”预测。该算法非常简单：
- en: Find the best-performing forecast for each time series using a validation dataset.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用验证数据集找到每个时间序列的最佳预测。
- en: For each time series, select the forecast from the same model for the test dataset.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个时间序列，选择相同模型的测试数据集中的预测。
- en: 'We can do this easily:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松做到这一点：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will create a new column called `best_fit` with the forecasts that have
    been chosen according to the strategy we discussed. Now, we can evaluate this
    new forecast and get the metrics for the test dataset. The following table shows
    the best individual model (`LightGBM`) and the new strategy—`best_fit`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为`best_fit`的新列，其中包含根据我们讨论的策略选择的预测。现在，我们可以评估这个新的预测，并获得测试数据集的指标。下表显示了最佳单一模型（`LightGBM`）和新策略—`best_fit`：
- en: '![Figure 9.2 – Aggregate metrics for the best fit strategy ](img/B22389_09_02.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 最佳拟合策略的汇总指标](img/B22389_09_02.png)'
- en: 'Figure 9.2: Aggregate metrics for the best-fit strategy'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：最佳拟合策略的汇总指标
- en: Here, we can see that the best-fit strategy is performing better than the best
    individual model overall. However, one drawback of this strategy is the fundamental
    assumption of this strategy—whatever model does best in the validation period
    also performs best in the test period. There is no hedging of bets with other
    forecasting models and so on. Given the dynamic nature of time series, this is
    not always the best strategy. Another drawback of this approach is the instability
    of the final forecast.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到最佳拟合策略整体表现优于单一的最佳模型。然而，这种策略的一个缺点是它的基本假设——在验证期表现最佳的模型也将在测试期表现最佳。它没有考虑其他预测模型等的对冲策略。考虑到时间序列的动态特性，这并不总是最佳策略。这种方法的另一个缺点是最终预测的不稳定性。
- en: When we are using such a rule in a live environment, where we retrain and rerun
    the best fit every week, the forecast for any time series can jump back and forth
    between different forecast models, which can generate wildly different forecasts.
    Therefore, the final forecast shows a lot of week-over-week instability, which
    hampers the downstream actions we use these forecasts for. We can look at a few
    other techniques that don’t have this instability.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在实际环境中使用这种规则时，每周重新训练并重新运行最佳拟合时，任何时间序列的预测可能会在不同的预测模型之间来回跳动，产生截然不同的预测结果。因此，最终的预测表现出很大的周间不稳定性，这会妨碍我们使用这些预测进行的后续操作。我们可以考虑一些没有这种不稳定性的其他技术。
- en: Measures of central tendency
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集中趋势度量
- en: 'Another prominent strategy is to use either an average or median to combine
    the forecasts. This is a function, *F*, that is independent of validation metrics.
    This is both the appeal and angst of this method. It is impossible to overfit
    the validation metrics because we are not using them at all. But on the other
    hand, without any information from the validation metric, we may be including
    some very bad models, which pulls down the ensemble. However, empirically, this
    simple averaging of taking the median has proven to be a very strong combination
    method for forecast and is hard to outperform. Let’s see how this can be done:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种显著的策略是使用均值或中位数来合并预测。这是一个函数，*F*，它不依赖于验证指标。这既是这种方法的吸引力，也是一种困扰。由于我们根本没有使用验证指标，所以不可能过度拟合验证数据。但另一方面，由于没有任何验证指标的信息，我们可能会包括一些非常差的模型，这会拉低整体预测效果。然而，经验表明，这种简单的平均或中位数合并方法已被证明是一种非常强大的预测组合方法，且很难被超越。让我们看看如何实现这一方法：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code will create two new columns called `average_ensemble` and
    `median_ensemble` with the combined forecasts. Now, we can evaluate this new forecast
    and get the metrics for the test dataset. The following table shows the best individual
    model (`LightGBM`) and the new strategies:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将创建两个新列，分别称为`average_ensemble`和`median_ensemble`，用于存储合并后的预测。现在，我们可以评估这个新的预测，并获取测试数据集的指标。下表显示了最佳单个模型（`LightGBM`）和新的策略：
- en: '![Figure 9.3 – Aggregate metrics for the mean and median strategies ](img/B22389_09_03.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – 均值和中位数策略的汇总指标](img/B22389_09_03.png)'
- en: 'Figure 9.3: Aggregate metrics for the mean and median strategies'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：均值和中位数策略的汇总指标
- en: Here, we can see that neither the mean nor median strategy is working better
    than the best individual model overall. This can be because we are including methods
    such as Theta and FFT, which are performing considerably worse than the other
    machine learning methods. But since we are not taking any information from the
    validation dataset, we do not know this information. We can make an exception
    and say that we are going to use the validation metrics to choose which models
    we include in the average or median. But we have to be careful because now, we
    are moving closer to the assumption that what works in the validation period is
    going to work in the test period.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，无论是均值策略还是中位数策略，都没有比最好的单个模型整体表现得更好。这可能是因为我们包含了像 Theta 和 FFT 这样的模型，它们的表现远不如其他机器学习方法。但由于我们没有使用验证数据集中的任何信息，所以我们并不知道这一点。我们可以做一个例外，假设我们使用验证指标来选择哪些模型包含在平均值或中位数中。但我们必须小心，因为现在我们越来越接近于假设在验证期有效的模型也会在测试期有效。
- en: There are a few manual techniques we can use here, such as **trimming** (discarding
    the worst-performing models in the ensemble) and **skimming** (selecting only
    the best few models in the ensemble). While effective, these are a bit subjective,
    and often, they become hard to use, especially when we have scores of models to
    choose from.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几种手动技术可以使用，例如**修剪**（丢弃表现最差的模型）和**筛选**（只选择表现最好的几个模型）。虽然这些方法有效，但有些主观性，尤其是在我们需要从成百上千个模型中选择时，它们变得难以使用。
- en: 'If we think about this problem, it is essentially a combinatorial optimization
    problem where we have to select the best combination of models that optimizes
    our metric. If we consider the average for combining the different forecasts,
    mathematically, it can be thought of as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑这个问题，本质上是一个组合优化问题，我们需要选择最佳的模型组合来优化我们的指标。如果我们考虑通过取平均值来合并不同的预测，从数学角度看，可以表示为：
- en: '![](img/B22389_09_001.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_09_001.png)'
- en: Here, *L* is the loss or metric that we are trying to minimize. In our case,
    we chose that to be the MAE. ![](img/B22389_09_002.png) is the binary weight of
    each of the base forecasts. Finally, ![](img/B22389_09_003.png) is the set of
    *N* base forecasts and *Y* is the real observed values of the time series.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*L* 是我们尝试最小化的损失或指标。在我们的例子中，我们选择的是 MAE。![](img/B22389_09_002.png) 是每个基础预测的二进制权重。最后，![](img/B22389_09_003.png)
    是*N*个基础预测集合，*Y* 是时间序列的实际观测值。
- en: But unlike pure optimization, where there is no concept of bias and variance,
    we need an optimal solution that can be generalized. Therefore, selecting the
    global minima in the training data is not advisable because in that case, we might
    be further overfitting the training dataset, increasing the variance of the resulting
    model. For this minimization, we typically use out-of-sample predictions, which
    can be the forecast during the validation period in this case.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 但与纯粹的优化不同，纯粹优化中没有偏差和方差的概念，我们需要一个能够泛化的最优解。因此，选择训练数据中的全局最小值并不可取，因为那样可能会进一步过拟合训练数据集，增加最终模型的方差。在这种最小化中，我们通常使用样本外预测，在这种情况下可以是验证期间的预测。
- en: 'The most straightforward solution is to find *w*, which minimizes this function
    on validation data. But there are two problems with this approach:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的解决方案是找到* w *，使其在验证数据上最小化此函数。但这种方法有两个问题：
- en: The possible candidates (different combinations of the base forecasts) increase
    exponentially as we increase the number of base forecasts, *N*. This becomes computationally
    intractable very soon.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着基准预测数量*N*的增加，可能的候选（基准预测的不同组合）呈指数增长。这很快就变得计算上难以处理。
- en: Selecting the global minima in the validation period may not be the best strategy
    because of overfitting the validation period.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在验证期间选择全局最小值可能不是最佳策略，因为可能会导致验证期间的过拟合。
- en: Now, let’s take a look at a few heuristics-based solutions to this combinatorial
    optimization problem.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看一些基于启发式的解决方案来解决这个组合优化问题。
- en: Heuristic problem-solving is a strategy that uses rules of thumb or shortcuts
    to find solutions quickly, even if they may not be optimal. Heuristics can be
    useful when exact solutions are computationally expensive or time-consuming to
    find. However, they may lead to suboptimal or even incorrect solutions in some
    cases.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式问题解决是一种策略，利用经验法则或捷径快速找到解决方案，即使这些解决方案可能不是最优的。当精确解法在计算上昂贵或耗时时，启发式方法可以非常有用。然而，在某些情况下，它们可能导致次优甚至错误的解决方案。
- en: Heuristics are often used in conjunction with other problem-solving methods,
    such as metaheuristics, to improve the efficiency and effectiveness of the search
    process. Metaheuristics are high-level, problem-independent strategies used to
    solve optimization problems. They offer a framework for developing heuristic algorithms
    that can efficiently explore complex search spaces and find near-optimal solutions.
    Unlike traditional optimization methods, metaheuristics often draw inspiration
    from natural phenomena or biological processes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式方法通常与其他问题解决方法（如元启发式方法）结合使用，以提高搜索过程的效率和效果。元启发式方法是高层次的、与问题无关的策略，用于解决优化问题。它们提供了一个框架，用于开发启发式算法，能够高效地探索复杂的搜索空间并找到近似最优解。与传统优化方法不同，元启发式方法通常从自然现象或生物过程中汲取灵感。
- en: Common examples of metaheuristic methods include genetic algorithms (inspired
    by natural selection), simulated annealing (inspired by metallurgy), particle
    swarm optimization (inspired by bird flocking), and ant colony optimization (inspired
    by ants foraging for food). These methods employ probabilistic or stochastic approaches
    to balance exploration and exploitation, allowing them to avoid getting stuck
    in local optima and discover potentially better solutions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 元启发式方法的常见例子包括遗传算法（灵感来自自然选择）、模拟退火（灵感来自冶金学）、粒子群优化（灵感来自鸟群聚集）和蚁群优化（灵感来自蚂蚁觅食）。这些方法采用概率或随机方法来平衡探索与开发，使其能够避免陷入局部最优解，发现潜在的更好解决方案。
- en: Simple hill climbing
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单的爬山法
- en: We briefly talked about greedy algorithms while discussing decision trees, as
    well as gradient-boosted trees. Greedy optimization is a heuristic that builds
    up a solution stage by stage, selecting a local optimum at each stage. In both
    of these machine learning models, we adopt a greedy, stagewise approach to finding
    the solution to a computationally infeasible optimization problem. To select the
    best subset that gives us the best combination of forecasts, we can employ a simple
    greedy algorithm called hill climbing. If we consider the objective function surface
    as a hill, to find the maxima, we would need to climb the hill. As its name suggests,
    hill climbing ascends the hill, one step at a time, and in each of those steps,
    it takes the best possible path, which increases the objective function. The illustration
    below can make it clearer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论决策树以及梯度提升树时，我们简要介绍了贪婪算法。贪婪优化是一种启发式方法，通过逐步构建解决方案，在每一步选择一个局部最优解。在这两种机器学习模型中，我们采用了贪婪的、逐步的方式来解决计算上不可行的优化问题。为了选择最佳子集，给我们提供最佳的预测组合，我们可以采用一种简单的贪婪算法，称为爬坡算法。如果我们将目标函数的曲面看作一座山，为了找到最大值，我们需要爬上这座山。顾名思义，爬坡算法逐步上升，在每一步中，它选择最优路径，从而增加目标函数的值。下面的示意图可以帮助更清晰地理解。
- en: '![](img/B22389_09_04.png)Figure 9.4: Hill climbing algorithm illustration for
    a one-dimensional objective'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B22389_09_04.png)图 9.4：一维目标的爬坡算法示意图'
- en: We can see in *Figure 9.4* that the objective function (the function we need
    to optimize) has multiple peaks (hills) and in the hill climbing algorithm, we
    “climb” the hill to reach the peak in a step-by-step manner. What we also need
    to keep in mind is that depending on where we start the climb, we might reach
    different points in the objective. In *Figure 9.4*, if we start at point A, we
    reach the local optima and miss the global optima. Now, let’s see how the algorithm
    works in a more rigorous manner.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从 *图 9.4* 中看到，目标函数（我们需要优化的函数）有多个峰值（山峰），而在爬坡算法中，我们“爬”上山，逐步到达峰顶。我们还需要记住，根据我们开始爬坡的位置不同，可能会达到目标中的不同点。在
    *图 9.4* 中，如果我们从 A 点开始爬坡，我们到达局部最优解，并错过全局最优解。现在，让我们看看该算法是如何以更严格的方式运作的。
- en: 'Here, *C* is a set of candidates (base forecasts) and *O* is the objective
    we want to minimize. The algorithm for the simple hill-climb is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*C* 是一组候选解（基础预测），*O* 是我们希望最小化的目标。简单爬坡算法如下：
- en: Initialize a starting solution, *C*[best], as the candidate that gives the minimum
    value in *O*, *O*[best], and remove *C*[best] from *C*.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化起始解，*C*[best]，作为在 *O* 中给出最小值的候选解，即 *O*[best]，并从 *C* 中移除 *C*[best]。
- en: 'While the length of *C* > 0, do the following:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 *C* 的长度大于 0 时，执行以下操作：
- en: Evaluate all members of *C* by averaging the base forecasts in *C*[best] with
    each element in *C* and select the best member (*C*[stage best]) that was added
    to *C*[best] to minimize the objective function, *O* (*O*[stage best]).
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将 *C*[best] 中的基础预测与 *C* 中的每个元素进行平均，评估 *C* 中的所有成员，并选择最佳成员 (*C*[stage best])，将其添加到
    *C*[best] 中，以最小化目标函数 *O*（即 *O*[stage best]）。
- en: 'If *O*[stage best] > *O*[best], then do the following:'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *O*[stage best] > *O*[best]，则执行以下操作：
- en: '*C*[best] = *C*[best] U *C*[stage best].'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*C*[best] = *C*[best] U *C*[stage best]。'
- en: '*O*[best] = *O*[stage best].'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*O*[best] = *O*[stage best]。'
- en: Remove *C*[stage best] from *C*.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *C* 中移除 *C*[stage best]。
- en: Otherwise, exit.
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，退出。
- en: 'At the end of the run, we have *C*[best], which is the best combination of
    forecasts we got through greedy optimization. We have made an implementation of
    this available in `src.forecasting.ensembling.py` under the `greedy_optimization`
    function. The parameters for this function are as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行结束时，我们得到 *C*[best]，这是通过贪婪优化得到的最佳预测组合。我们在 `src.forecasting.ensembling.py`
    中实现了这个功能，位于 `greedy_optimization` 函数下。该函数的参数如下：
- en: '`objective`: This is a callable that takes in a list of strings as the candidates
    and returns a `float` objective value.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`objective`：这是一个可调用函数，接受一个字符串列表作为候选解，并返回一个 `float` 类型的目标值。'
- en: '`candidates`: This is a list of candidates to be included in the optimization.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`candidates`：这是一个候选解列表，将被包括在优化中。'
- en: '`verbose`: A flag that specifies whether progress is printed or not.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`：一个标志，指定是否打印进度。'
- en: The function returns a tuple of the best solution as a list of strings and the
    best score that was obtained through optimization.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回一个元组，其中包含作为字符串列表的最佳解和通过优化得到的最佳评分。
- en: 'Let’s see how we can use this in our example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在我们的示例中使用这个算法：
- en: 'Import all the required libraries/functions:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必需的库/函数：
- en: '[PRE2]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define the objective function and run greedy optimization:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义目标函数并运行贪心优化：
- en: '[PRE3]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once we have the best solution, we can create the combination forecast in the
    test DataFrame:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们得到了最佳解，我们可以在测试数据框中创建组合预测：
- en: '[PRE4]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once we run this code, we will have the combination forecast under the name
    `greedy_ensemble` in our prediction DataFrame. The candidates that are part of
    the optimal solution are LightGBM, Lasso Regression, and LightGBM_auto_stat. Now,
    let’s evaluate the results and look at the aggregated metrics:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行此代码，我们将在预测数据框（DataFrame）中得到名称为`greedy_ensemble`的组合预测。最优解中的候选模型包括 LightGBM、Lasso
    回归和 LightGBM_auto_stat。接下来，让我们评估结果并查看汇总的度量指标：
- en: '![Figure 9.4 – Aggregate metrics for a simple hill climbing-based ensemble
    ](img/B22389_09_05.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – 基于简单爬山的集成方法汇总指标](img/B22389_09_05.png)'
- en: 'Figure 9.5: Aggregate metrics for a simple hill climbing-based ensemble'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：基于简单爬山的集成方法汇总指标
- en: 'As we can see, the simple hill-climb is performing better than any individual
    models or ensemble techniques we have seen so far. This greedy approach seems
    to be working well in this case. Now, let’s understand a few limitations of hill
    climbing, as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，简单的爬山算法的表现优于我们迄今为止看到的任何单一模型或集成技术。在这种情况下，贪心算法似乎运作良好。现在，让我们了解爬山算法的几个局限性，如下所示：
- en: '**Runtime considerations**: Since a simple hill-climb requires us to evaluate
    all the candidates at any step, this can cause a bottleneck in terms of runtime.
    If the number of candidates is large, this approach can take longer to finish.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行时考虑**：由于简单的爬山算法需要在每一步评估所有候选者，这可能会导致运行时瓶颈。如果候选者数量很大，这种方法可能会花费更多时间才能完成。'
- en: '**Short-sightedness**: Hill climbing optimization is short-sighted. During
    optimization, it always picks the best in each step. Sometimes, by choosing a
    slightly worse solution in a step, we may get to a better overall solution.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**短视性**：爬山优化是短视的。在优化过程中，它每一步总是选择最佳的选项。有时，通过在某一步选择一个稍差的解决方案，我们可能会得到一个更好的整体解决方案。'
- en: '**Forward-only**: Hill climbing is a forward-only algorithm. Once a candidate
    has been admitted into the solution, we can’t go back and remove it.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**只前进**：爬山算法是一个只前进的算法。一旦一个候选者被纳入解决方案，我们就不能回头将其移除。'
- en: The greedy approach may not always get us the best solution, especially when
    there are scores of models to combine. So, let’s look at a small variation of
    hill climbing that tries to get over some of the limitations of the greedy approach.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 贪心算法并不总能为我们找到最优解，尤其是在需要组合多个模型时。因此，让我们来看看一种小的变种——爬山算法，它试图克服贪心算法的一些局限性。
- en: Stochastic hill climbing
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机爬山
- en: The key difference between simple hill climbing and stochastic hill climbing
    is in the evaluation of candidates. In a simple hill climb, we *evaluate all possible
    options* and pick the best among them. However, in a stochastic hill climb, we
    *randomly pick a candidate* and add it to the solution if it is better than the
    current solution. In other words, in hill climbing we always move up the hill
    in steps, but in Stochastic hill climbing, we magically teleport to different
    points in the objective function and check we are higher than we have been before.
    This addition of stochasticity helps the optimization not get the local maxima/minima,
    but also introduces quite a bit of uncertainty in reaching any kind of optima.
    Let’s take a look at the algorithm.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的爬山算法和随机爬山算法的关键区别在于候选者的评估。在简单的爬山中，我们会*评估所有可能的选项*并从中挑选最佳的一个。然而，在随机爬山中，我们会*随机挑选一个候选者*，如果它比当前解更好，就将其添加到解决方案中。换句话说，在爬山算法中，我们总是逐步向上移动，但在随机爬山中，我们会神奇地瞬移到目标函数的不同点，检查自己是否比之前更高。这种加入随机性的做法有助于优化算法避免陷入局部最大值/最小值，但也引入了相当大的不确定性，可能无法达到任何最优解。接下来，我们来看看这个算法。
- en: 'Here, *C* is a set of candidates (base forecasts), *O* is the objective we
    want to minimize, and *N* is the maximum number of iterations we want to run the
    optimization for. The algorithm for stochastic hill climbing is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*C* 是候选集（基础预测），*O* 是我们希望最小化的目标，*N* 是我们希望运行优化的最大迭代次数。随机爬山算法如下：
- en: Initialize a starting solution, *C*[best], as the candidate. This can be done
    by picking a candidate at random or choosing the best-performing model.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化起始解，*C*[best]，作为候选者。可以通过随机挑选一个候选者或选择表现最好的模型来完成。
- en: Set the value of the objective function for *C*[best], *O*, as *O*[best], and
    remove *C*[best] from *C*.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *C*[best] 的目标函数值 *O* 设置为 *O*[best]，并从 *C* 中移除 *C*[best]。
- en: 'Repeat this for *N* iterations:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 *N* 次迭代重复以下步骤：
- en: Draw a random sample from *C*, add it to *C*[best], and store it as *C*[stage].
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *C* 中随机抽取一个样本，将其加入 *C*[best]，并存储为 *C*[stage]。
- en: Evaluate *C*[stage] on the objective function, *O* , and store it as *O*[stage].
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在目标函数 *O* 上评估 *C*[stage]，并将其存储为 *O*[stage]。
- en: 'If *O*[stage] > *O*[best], then do the following:'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *O*[stage] > *O*[best]，则执行以下操作：
- en: '*C*[best] = *C*[best] U *C*[stage]'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*C*[best] = *C*[best] U *C*[stage]'
- en: '*O*[best]= *O*[stage].'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*O*[best] = *O*[stage]。'
- en: Remove *C*[best] from *C*.
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *C* 中移除 *C*[best]。
- en: 'At the end of the run, we have *C*[best], which is the best combination of
    forecasts we got through stochastic hill climbing. We have made an implementation
    of this available in `src.forecasting.ensembling.py` under the `stochastic_hillclimbing`
    function. The parameters for this function are as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行结束时，我们得到的是 *C*[best]，它是通过随机爬山算法获得的最佳预测组合。我们已经在 `src.forecasting.ensembling.py`
    中的 `stochastic_hillclimbing` 函数下实现了这一方法。该函数的参数如下：
- en: '`objective`: This is a callable that takes in a list of strings as the candidates
    and returns a `float` objective value.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`objective`：这是一个可调用的函数，它接收一个包含候选字符串的列表并返回一个 `float` 类型的目标值。'
- en: '`candidates`: This is a list of candidates to be included in the optimization.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`candidates`：这是一个候选列表，将被包含在优化过程中。'
- en: '`n_iterations`: The number of iterations to run the hill-climb for. If this
    is not given, a heuristic (twice the number of candidates) is used to set this.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_iterations`：执行爬山算法的迭代次数。如果未给定该值，则使用启发式方法（候选数量的两倍）来设置该值。'
- en: '`init`: This determines the strategy to be used for the initial solution. This
    can be `random` or `best`.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init`：决定用于初始解的策略，可以是 `random` 或 `best`。'
- en: '`verbose`: A flag that specifies whether progress is printed or not.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`：一个标志，用来指定是否打印进度。'
- en: '`random_state`: A seed that gets repeatable results.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_state`：一个种子，用于获得可重复的结果。'
- en: The function returns a tuple of the best solution as a list of strings and the
    best score obtained through optimization.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回一个元组，包含作为字符串列表的最佳解和通过优化获得的最佳得分。
- en: 'This can be used in a very similar fashion to `greedy_optimization`. We will
    only show the different parts here. The full code is available in the notebook:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以与 `greedy_optimization` 以非常相似的方式使用。我们这里只展示不同的部分，完整代码可在笔记本中查看：
- en: '[PRE5]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once we run this code, we will have the combination forecast called `stochastic_hillclimb__ensemble`
    in our prediction DataFrame. The candidates that are part of the optimal solution
    are LightGBM, Lasso Regression_auto_stat, LightGBM_auto_stat, and Lasso Regression.
    Now, let’s evaluate the results and look at the aggregated metrics:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行这段代码，我们将在预测 DataFrame 中得到一个名为 `stochastic_hillclimb__ensemble` 的组合预测。作为最佳解的一部分的候选模型包括
    LightGBM、Lasso 回归_auto_stat、LightGBM_auto_stat 和 Lasso 回归。现在，让我们评估结果并查看聚合指标：
- en: '![Figure 9.5 – Aggregate metrics for a stochastic hill climbing-based ensemble
    ](img/B22389_09_06.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – 基于随机爬山的集成的聚合指标](img/B22389_09_06.png)'
- en: 'Figure 9.6: Aggregate metrics for a stochastic hill climbing-based ensemble'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：基于随机爬山的集成的聚合指标
- en: The stochastic hill climb is not doing better than the greedy approach but is
    better than the mean, median, and best-fit ensembles. We discussed three disadvantages
    of simple hill climbing earlier—runtime considerations, short-sightedness, and
    forward-only. Stochastic hill climbing solves the runtime consideration because
    we are not evaluating all the combinations and selecting the best. Instead, we
    are randomly evaluating the combinations and adding them to the ensemble as soon
    as we see a solution that performs better. It partly solves the short-sightedness
    purely because the randomness in the algorithm may end up choosing a sub-optimal
    solution for each stage. But it still only chooses solutions that are better than
    the current solution.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 随机爬山算法的效果不比贪婪算法更好，但却优于均值、媒体和最佳拟合集成。我们之前讨论了简单爬山法的三个缺点——运行时考虑、短视性和仅向前搜索。随机爬山解决了运行时考虑的问题，因为我们并没有评估所有的组合并选择最佳，而是通过随机评估组合并一旦找到一个表现更好的解就将其加入集成。它在一定程度上解决了短视性问题，因为算法中的随机性可能会导致每个阶段选择一个次优解，但它仍然只选择比当前解更好的解。
- en: Now, let’s look at another modification of hill climbing that handles this issue
    as well.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下另一个改进版的爬山算法，它也解决了这个问题。
- en: Simulated annealing
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟退火
- en: '**Simulated annealing** is a modification of hill climbing that is inspired
    by a physical phenomenon—annealing solids. Annealing is the process of heating
    a solid to a predetermined temperature (usually above its recrystallization point,
    but below its melting point), holding it for a while, and then cooling it (either
    slowly or quickly by quenching it in water).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**模拟退火**是对爬山算法的一种改进，灵感来源于一种物理现象——退火固体。退火是将固体加热到预定温度（通常高于其再结晶温度，但低于其熔点），保持一段时间，然后冷却（可以慢慢冷却，也可以通过在水中淬火来快速冷却）。'
- en: This is done to ensure that the atoms assume a new global minimum energy state,
    which induces desirable properties in some metals, such as iron.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是为了确保原子达到新的全局最小能量状态，这会在某些金属中引入期望的性质，比如铁。
- en: In 1952, Metropolis proposed simulated annealing as an optimization technique.
    The annealing analogy applies to the optimization context as well. When we say
    we heat the system, we mean that we encourage random perturbations. So, when we
    start an optimization with a high temperature, the algorithm explores the space
    and comes up with an initial structure of the problem. And as we reduce the temperature,
    the structure is refined to arrive at a final solution. This technique helps us
    avoid getting stuck in any local optima. Local optima are extrema in the objective
    function surface that are better than other values nearby but may not be the absolute
    best solution possible. The *Further reading* section contains a resource that
    explains what local and global optima are in concise language.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 1952年，Metropolis提出了模拟退火作为一种优化技术。退火类比也适用于优化背景。当我们说加热系统时，实际上是指我们鼓励随机扰动。因此，当我们以高温开始优化时，算法会探索空间并得出问题的初始结构。随着温度的降低，结构会被细化，从而得到最终解决方案。这种技术有助于避免陷入任何局部最优解。局部最优解是目标函数表面上的极值，它比附近的其他值更好，但可能不是最优解。*进一步阅读*部分包含了简洁解释局部最优解和全局最优解的资源。
- en: Now, let’s look at the algorithm.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看这个算法。
- en: 'Here, *C* is a set of candidates (base forecasts), *O* is the objective we
    want to minimize, *N* is the maximum number of iterations we want to run the optimization
    for, *T*[max] is the maximum temperature, and ![](img/B22389_04_009.png) is the
    temperature decay. The algorithm for simulated annealing is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*C*是候选集（基本预测），*O*是我们要最小化的目标，*N*是我们希望运行优化的最大迭代次数，*T*[max]是最大温度，![](img/B22389_04_009.png)是温度衰减。模拟退火算法如下：
- en: Initialize a starting solution, *C*[best], as the candidate. This can be done
    by picking a candidate at random or choosing the best-performing model.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个起始解，*C*[best]，作为候选解。这可以通过随机选择一个候选解或选择表现最好的模型来完成。
- en: Set the value of the objective function for *C*[best], *O*, as *O*[best], and
    remove *C*[best] from *C*.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为*C*[best]设置目标函数的值，*O*，作为*O*[best]，并从*C*中移除*C*[best]。
- en: Set the current temperature, *t*, to *T*[max].
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将当前温度设置为*t* = *T*[max]。
- en: 'Repeat this for *N* iterations:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对*N*次迭代重复执行此操作：
- en: Draw a random sample from *C*, add it to *C*[best], and store it as *C*[stage].
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*C*中随机抽取一个样本，添加到*C*[best]，并将其存储为*C*[stage]。
- en: Evaluate *C*[stage] on the objective function, *O*, and store it as *O*[stage].
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在目标函数上评估*C*[stage]，*O*，并将其存储为*O*[stage]。
- en: 'If *O*[stage] > *O*[best], then do the following:'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *O*[stage] > *O*[best]，则执行以下操作：
- en: '*C*[best].= *C*[best] U *C*[stage]'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*C*[best].= *C*[best] U *C*[stage]'
- en: '*O*[best] = *O*[stage]'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*O*[best] = *O*[stage]'
- en: Remove *C*[best] from *C*.
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*C*中移除*C*[best]。
- en: 'Otherwise, do the following:'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，执行以下操作：
- en: Calculate the acceptance probability, ![](img/B22389_09_005.png).
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算接受概率，![](img/B22389_09_005.png)。
- en: Draw a random sample between 0 and 1, as p.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从0到1之间随机抽取一个样本，记为p。
- en: 'If *p* < *s*, then do the following:'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *p* < *s*，则执行以下操作：
- en: '*C*[best] = *C*[best] U *C*[stage]'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*C*[best] = *C*[best] U *C*[stage]'
- en: '*O*[best] = *O*[stage].'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*O*[best] = *O*[stage]。'
- en: Remove *C*[best] from *C*.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*C*中移除*C*[best]。
- en: '*t* = *t* - ![](img/B22389_04_009.png) (for linear decay) and *t* = *t*/![](img/B22389_04_009.png)
    (for geometric decay).'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*t* = *t* - ![](img/B22389_04_009.png)（对于线性衰减）和 *t* = *t*/![](img/B22389_04_009.png)（对于几何衰减）。'
- en: Exit when *C* is empty.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当*C*为空时退出。
- en: At the end of the run, we have *C*[best], which is the best combination of forecasts
    we got through simulated annealing. We have provided an implementation of this
    in `src.forecasting.ensembling.py` under the `simulated_annealing` function. Setting
    the temperature to the right value is key for the algorithm to work well and is
    typically the hardest hyperparameter to set. More intuitively, we can think of
    temperature in terms of the probability of accepting a worse solution in the beginning.
    In the implementation, we have also made it possible to input the starting and
    ending probability of accepting a worse solution.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行结束时，我们得到了 *C*[best]，这是通过模拟退火获得的最佳预测组合。我们在 `src.forecasting.ensembling.py`
    文件中的 `simulated_annealing` 函数下提供了该实现。将温度设置为合适的值对于算法的良好运行至关重要，而且通常是最难设置的超参数。更直观地，我们可以将温度视为开始时接受较差解的概率。在实现中，我们还使得可以输入接受较差解的起始和结束概率。
- en: In 1989, D.S. Johnson et al. proposed a procedure for estimating the temperature
    range from the given probability range. This has been implemented in `initialize_temperature_range`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 1989 年，D.S. Johnson 等人提出了一种从给定的概率范围估算温度范围的过程。该过程已经在 `initialize_temperature_range`
    中实现。
- en: To summarize, the algorithm starts with random solutions and evaluates how good
    each is. Then, it keeps trying new solutions, sometimes accepting worse ones to
    avoid getting stuck in a local optima, but over time, it becomes less likely to
    accept bad solutions as it “cools down” (just like how metals cool and harden).
    It repeats this process until it either runs out of options or the temperature
    gets too low, leaving the best solution found so far.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，算法从随机解开始，并评估每个解的好坏。然后，它不断尝试新解，有时接受较差的解以避免陷入局部最优解，但随着时间的推移，它接受较差解的可能性会降低，因为它在“冷却”（就像金属冷却并硬化一样）。它重复这一过程，直到选项用尽或温度变得太低，最终保留找到的最佳解。
- en: '**Reference check:**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查：**'
- en: 'The research paper by D.S. Johnson, titled *Optimization by Simulated Annealing:
    An Experimental Evaluation*; *Part I, Graph Partitioning*, is cited in the *References*
    section as reference *1*.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: D.S. Johnson 的研究论文，标题为 *模拟退火优化：实验评估*；*第一部分，图划分*，在 *参考文献* 部分被引用为参考文献 *1*。
- en: 'The parameters for the `simulated_annealing` function are as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`simulated_annealing` 函数的参数如下：'
- en: '`objective`: This is a callable that takes in a list of strings as the candidates
    and returns a `float` objective value.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`objective`：这是一个可调用的函数，接受一个字符串列表作为候选项，并返回一个 `float` 类型的目标值。'
- en: '`candidates`: This is a list of candidates to be included in the optimization.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`candidates`：这是一个候选列表，用于包含在优化中。'
- en: '`n_iterations`: The number of iterations to run simulated annealing for. This
    is a mandatory parameter.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_iterations`：模拟退火运行的迭代次数。这个参数是必需的。'
- en: '`p_range`: The starting and ending probabilities as a tuple. This is the probability
    with which a worse solution is accepted in simulated annealing. The temperature
    range (`t_range`) is inferred from `p_range` during optimization.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p_range`：起始和结束概率的元组。这是在模拟退火中接受较差解的概率。温度范围（`t_range`）将在优化过程中从 `p_range` 推断得出。'
- en: '`t_range`: We can use this if we want to directly set the temperature range
    as a tuple (start, end). If this is set, `p_range` is ignored.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t_range`：如果我们想直接设置温度范围为一个元组（起始，结束），可以使用这个参数。如果设置了该值，`p_range` 会被忽略。'
- en: '`init`: This determines the strategy that’s used for the initial solution.
    This can be `random` or `best`.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init`：这个参数决定了用于初始解的策略。可以是 `random` 或 `best`。'
- en: '`temperature_decay`: This specifies how to decay the temperature. It can be
    `linear` or `geometric`.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature_decay`：指定温度衰减的方式。可以是 `linear` 或 `geometric`。'
- en: '`verbose`: A flag that specifies whether progress is printed or not.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`：一个标志，指定是否打印进度。'
- en: '`random_state`: The seed for getting repeatable results.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_state`：用于获取可重复结果的种子。'
- en: The function returns a tuple of the best solution as a list of strings and the
    best score that was obtained through optimization.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回一个元组，包含作为字符串列表的最佳解决方案和通过优化获得的最佳得分。
- en: 'This can be used in a very similar fashion to the other ways of combining forecasts.
    We will show just the part that is different here. The full code is available
    in the notebook:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以像其他组合预测的方式一样使用。我们将在这里展示不同之处。完整代码可在笔记本中查看：
- en: '[PRE6]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once we run this code, we will have a combination forecast called `simulated_annealing_ensemble`
    in our prediction DataFrame. The candidates that are part of the optimal solution
    are LightGBM, Lasso Regression_auto_stat, LightGBM_auto_stat, and XGB Random Forest.
    Let’s evaluate the results and look at the aggregated metrics:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行这段代码，我们将在预测 DataFrame 中得到一个名为`simulated_annealing_ensemble`的组合预测。作为最优解的一部分的候选模型包括
    LightGBM、Lasso 回归_auto_stat、LightGBM_auto_stat 和 XGB 随机森林。让我们评估一下结果并查看汇总指标：
- en: '![Figure 9.6 – Aggregate metrics for a simulated annealing-based ensemble ](img/B22389_09_07.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – 基于模拟退火的集成的汇总指标](img/B22389_09_07.png)'
- en: 'Figure 9.7: Aggregate metrics for a simulated annealing-based ensemble'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：基于模拟退火的集成的汇总指标
- en: Simulated annealing seems to be doing better than stochastic hill climbing.
    We discussed three disadvantages of simple hill climbing earlier—runtime considerations,
    short-sightedness, and forward-only. Simulated annealing solves the runtime consideration
    because we are not evaluating all the combinations and selecting the best. Instead,
    we are randomly evaluating the combinations and adding them to the ensemble as
    soon as we see a solution that performs better. It also solves the short-sightedness
    problem because, by using temperature, we are also accepting solutions that are
    slightly worse toward the beginning of the optimization. However, it is still
    a forward-only procedure.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟退火似乎比随机爬山表现得更好。我们之前讨论过简单爬山算法的三个缺点——运行时考虑、目光短浅以及仅向前搜索。模拟退火解决了运行时问题，因为我们不是评估所有组合并选择最优的，而是随机评估组合，并在发现更好的解时立即将其添加到集成中。它也解决了目光短浅的问题，因为通过使用温度，我们在优化的初期也会接受略差的解。然而，它仍然是一个仅向前搜索的过程。
- en: So far, we have looked at combinatorial optimization because we said. But if
    we can relax this constraint and make ![](img/B22389_09_008.png). ![](img/B22389_09_009.png)
    (real numbers), the combinatorial optimization problem can be relaxed to a general
    mathematical optimization problem. Let’s see how we can do that.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看过组合优化问题，因为我们说过。但如果我们可以放宽这个约束，使得 ![](img/B22389_09_008.png)。 ![](img/B22389_09_009.png)（实数），那么组合优化问题可以放宽为一个一般的数学优化问题。让我们看看如何做到这一点。
- en: Optimal weighted ensemble
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最优加权集成
- en: 'Previously, we defined the optimization problem we are trying to solve as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们将我们试图解决的优化问题定义为如下：
- en: '![](img/B22389_09_010.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_09_010.png)'
- en: Here, *L* is the loss or metric that we are trying to minimize. In our case,
    we chose that to be the MAE. ![](img/B22389_09_011.png) is the set of *N* base
    forecasts while *Y* is the real observed values of the time series. Instead of
    defining ![](img/B22389_09_008.png), let’s make ![](img/B22389_09_009.png), the
    continuous weights of each of the base forecasts. With this new relaxation, the
    combination becomes a weighted average between the different base forecasts. Now,
    we are looking at a soft mixing of the different forecasts as opposed to the hard-choice-based
    combinatorial optimization (which was what we had been using up until this point).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*L*是我们试图最小化的损失或指标。在我们的例子中，我们选择了 MAE。 ![](img/B22389_09_011.png) 是 *N* 个基本预测集合，而
    *Y* 是时间序列的真实观测值。我们不再定义 ![](img/B22389_09_008.png)，而是让 ![](img/B22389_09_009.png)
    成为每个基本预测的连续权重。通过这个新放宽的约束，组合变成了不同基本预测之间的加权平均。现在，我们正在看不同预测的软混合，而不是基于硬选择的组合优化（这也是我们一直使用的方法）。
- en: This is an optimization problem that can be solved using off-the-shelf algorithms
    from `scipy`. Let’s see how we can use `scipy.optimize` to solve this problem.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可以使用 `scipy` 中现成算法解决的优化问题。让我们看看如何使用 `scipy.optimize` 来解决这个问题。
- en: 'First, we need to define a loss function that takes in a set of weights as
    a list and returns the metric we need to optimize:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义一个损失函数，该函数接受一组作为列表的权重，并返回我们需要优化的指标：
- en: '[PRE7]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, all we need to do is call `scipy.optimize` with the necessary parameters.
    Let’s learn how to do this:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们所需要做的就是用必要的参数调用`scipy.optimize`。让我们学习如何做这件事：
- en: '[PRE8]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The optimization is usually fast and we will get the weights as a list of floating-point
    numbers. We have wrapped this in a function in `src.forecasting.ensembling.py`
    called under the `find_optimal_combination` function. The parameters for this
    function are as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 优化通常很快，我们会得到作为浮动点数的权重列表。我们将其包装在`src.forecasting.ensembling.py`中的一个名为`find_optimal_combination`的函数中。该函数的参数如下：
- en: '`candidates`: This is a list of candidates to be included in the optimization.
    They are returning in the same order in which the returned weights would.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`candidates`：这是待纳入优化的候选项列表。它们的返回顺序与返回的权重顺序相同。'
- en: '`pred_wide`: This is the prediction DataFrame on which we need to learn the
    weights.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred_wide`：这是我们需要学习权重的预测数据框。'
- en: '`target`: This is the column name of the target.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target`：这是目标列的名称。'
- en: '`metric_fn`: This is any callable with a `metric(actuals, pred)` signature.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric_fn`：这是任何具有`metric(actuals, pred)`签名的可调用对象。'
- en: 'The function returns the optimal weights as a list of floating-point numbers.
    Let’s see what the optimal weights are when we learned them through our validation
    forecast:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回最优权重，作为一个浮动点数列表。让我们看看通过验证预测学习到的最优权重是什么：
- en: '![Figure 9.7 – The optimal weights that were learned through optimization ](img/B22389_09_08.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – 通过优化学习得到的最优权重](img/B22389_09_08.png)'
- en: 'Figure 9.8: The optimal weights that were learned through optimization'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：通过优化学习得到的最优权重
- en: Here, we can see that the optimization automatically learned to ignore `FFT`,
    `Theta`, `XGB Random Forest`, and `XGB Random Forest_auto_stat` because they didn’t
    add much value to the ensemble. It has also learned some non-zero weights for
    each of the forecasts. The weights already resemble the selection we made using
    the techniques we discussed previously. Now, we can use these weights to come
    up with a weighted average and call it `optimal_combination_ensemble`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到优化自动学会忽略`FFT`、`Theta`、`XGB Random Forest`和`XGB Random Forest_auto_stat`，因为它们对集成模型贡献不大。它还学会了为每个预测分配一些非零的权重。这些权重已经与我们之前讨论的技术选择相似。现在，我们可以使用这些权重来计算加权平均值，并将其称为`optimal_combination_ensemble`。
- en: 'The aggregated results should be as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合结果应该如下所示：
- en: '![Figure 9.8 – Aggregate metrics for the optimal combination-based ensemble
    ](img/B22389_09_09.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8 – 基于最优组合的集成的聚合指标](img/B22389_09_09.png)'
- en: 'Figure 9.9: Aggregate metrics for the optimal combination-based ensemble'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：基于最优组合的集成的聚合指标
- en: Here, we can see that this soft mixing of the forecasts is doing much better
    than all of the hard-choice-based ensembles on all three metrics.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，这种软性混合预测的表现远远好于所有基于硬性选择的集成方法，在所有三个指标上都有明显的提升。
- en: In all the techniques we discussed, we were using the MAE as the objective function.
    But we can use any metric, a combination of metrics, or even metrics with regularization
    as the objective function. When we discussed Random Forest, we talked about how
    decorrelated trees were essential to getting better performance. A very similar
    principle applies while choosing ensembles as well. Having decorrelated base forecasts
    adds value to the ensemble. So, we can use any measure of variety to regularize
    our metric as well. For instance, we can use correlation as a measure and create
    a regularized metric to be used in these techniques. The `01-Forecast_Combinations.ipynb`
    notebook in the `Chapter09` folder contains a bonus section that shows how to
    do that.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论的所有技术中，我们使用的是MAE作为目标函数。但我们也可以使用任何指标、指标组合，甚至是带正则化的指标作为目标函数。当我们讨论随机森林时，我们提到去相关树对提高性能至关重要。一个非常相似的原则也适用于选择集成方法。拥有去相关的基础预测为集成模型增值。因此，我们可以使用任何多样性度量来对我们的指标进行正则化。例如，我们可以使用相关性作为度量，并创建一个正则化指标用于这些技术。`Chapter09`文件夹中的`01-Forecast_Combinations.ipynb`笔记本包含一个附加部分，展示了如何做到这一点。
- en: 'We started by discussing combining forecasts with a mathematical formulation:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一开始讨论的是如何通过数学公式来组合预测：
- en: '*Y* = *F*(*Y*[1], *Y*[2], …, *Y*[N])'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y* = *F*(*Y*[1], *Y*[2], …, *Y*[N])'
- en: Here, *F* is the function that combines the *N* forecasts.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*F*是将*N*个预测值组合起来的函数。
- en: We did all this while looking at ways to come up with this function as an optimization
    problem, using something such as a mean or median to combine the metrics. But
    we have also seen another way to learn this function, *F*, from data, haven’t
    we? Let’s see how that can be done.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在寻找将该函数作为优化问题的解决方案时，使用了均值或中位数等方法来组合这些指标。但我们也看到了另一种从数据中学习这个函数*F*的方式，不是吗？让我们看看怎么做。
- en: Stacking and blending
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠与混合
- en: We started this chapter by talking about machine learning algorithms, which
    learn a function from a set of inputs and outputs. While using those machine learning
    algorithms, we learned about the functions that forecast our time series, which
    we’ll call base forecasts now.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 本章一开始我们讨论了机器学习算法，这些算法从一组输入和输出中学习一个函数。在使用这些机器学习算法的过程中，我们学习了预测时间序列的函数，现将其称为基预测。
- en: Why not use the same machine learning paradigm to learn this new function, F,
    that we are trying to learn as well?
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不使用相同的机器学习范式来学习我们也想学习的这个新函数F呢？
- en: This is exactly what we do in stacking (often called stacked generalization),
    where we train another learning algorithm on the predictions of some base learners
    to combine these predictions. This second-level model is often called a **stacked
    model** or a **meta model**. And typically, this meta model performs equal to
    or better than the base learners. This is very similar to blending where the only
    difference being the way we split the data.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是堆叠中所做的（通常称为堆叠泛化），我们在一些基学习器的预测结果上训练另一个学习算法来结合这些预测。这种二级模型通常被称为**堆叠模型**或**元模型**。通常，这种元模型的表现与基学习器相当，甚至更好。这与混合（blending）非常相似，唯一的区别在于我们分割数据的方式。
- en: Although the idea originated with Wolpert in 1992, Leo Breiman formalized this
    idea in the way it is used now in his 1996 paper titled *Stacked Regressions*.
    And in 2007, M. J. Van der Laan et al. established the theoretical underpinnings
    of the technique and provided proof that this meta model will perform at least
    as well as or even better than the base learners.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这一思想最早由沃尔珀特（Wolpert）于1992年提出，但莱奥·布雷曼（Leo Breiman）在1996年的论文《堆叠回归》（*Stacked
    Regressions*）中正式化了这一概念，成为如今的应用方式。并且在2007年，M. J. Van der Laan等人建立了这一技术的理论基础，并提供了证明，表明这种元模型的表现至少与基学习器一样好，甚至更好。
- en: '**Reference check:**'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查：**'
- en: The research papers by Leo Breiman (1996) and Mark J. Van der Laan et al. (2007)
    are cited in the *References* section as *2* and *3*, respectively.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 莱奥·布雷曼（1996年）和马尔科·J·范德兰（2007年）等人的研究论文在*参考文献*部分被标记为*2*和*3*。
- en: This is a very popular technique in machine learning competitions such as Kaggle
    and is considered a secret art among machine learning practitioners. We also discussed
    some other techniques, such as bagging and boosting, which combine base learners
    into something more. But those techniques require the base learner to be a weak
    learner. This is where stacking differs because stacking tries to combine a *diverse*
    set of *strong* learners.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在机器学习竞赛中非常流行的一种技术，比如 Kaggle，被认为是机器学习从业者之间的一种秘密技巧。我们还讨论了其他一些技术，比如袋装法（bagging）和提升法（boosting），它们通过将基学习器组合成更复杂的模型来改进效果。但这些技术要求基学习器是一个弱学习器。而堆叠（stacking）则不同，因为堆叠尝试将一组*多样化*的*强*学习器进行组合。
- en: The intuition behind stacking is that different models or families of functions
    learn the output function slightly differently, capturing different properties
    of the problem. For instance, one model may have captured the seasonality very
    well, whereas the other may have captured any particular interaction with an exogenous
    variable better. The stacking model will be able to combine these base models
    into a model that learns to look toward one model for seasonality and the other
    for interaction. This is done by making the meta model learn the predictions of
    the base models. But to prevent data leakage and thereby avoid overfitting, the
    meta model should be trained on out-of-sample predictions. There are two small
    variations of this technique that are used today—stacking and blending.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠的直觉是，不同的模型或函数族学习输出函数的方式略有不同，捕捉了问题的不同特征。例如，一个模型可能很好地捕捉了季节性，而另一个模型则可能更好地捕捉了与外生变量的某种交互。堆叠模型将能够将这些基模型结合成一个模型，其中一个模型关注季节性，另一个模型关注交互。这是通过让元模型学习基模型的预测结果来实现的。但为了防止数据泄漏并避免过拟合，元模型应在样本外的预测数据上进行训练。如今有两种小变体的技术——堆叠和混合。
- en: '**Stacking** is when the meta model is trained on the entire training dataset,
    but with out-of-sample predictions. The following steps are involved in stacking:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆叠**是指元模型在整个训练数据集上进行训练，但使用的是样本外预测。堆叠过程包括以下步骤：'
- en: Split the training dataset into *k* parts.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据集分割成*k*部分。
- en: Iteratively train the base models on *k-1* parts, predict on the *k*^(th) part,
    and save the predictions. Once this step is done, we have the out-of-sample predictions
    for the training dataset from all base models.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*k-1*部分上迭代训练基本模型，在*k*^(th)部分上进行预测，并保存预测结果。完成此步骤后，我们有了来自所有基本模型的训练数据集的样本外预测。
- en: Train a meta model on these predictions.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这些预测上训练一个元模型。
- en: '**Blending** is similar to this but slightly different in the way we generate
    out-of-sample predictions. The following steps are involved in blending:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合**与此类似，但在生成样本外预测的方式上略有不同。混合涉及以下步骤：'
- en: Split the training dataset into two parts—train and holdout.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据集分为两部分——训练和保留。
- en: Train the base models on the training dataset and predict on the holdout dataset.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据集上训练基本模型并在保留数据集上进行预测。
- en: Train a meta model on the validation dataset with the predictions of the base
    model as the features.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在具有基本模型预测结果的验证数据集上训练元模型。
- en: 'Intuitively, we can see that stacking can work better because it uses a much
    larger dataset (usually all the training data) as the out-of-sample prediction,
    so the meta model may be more generalized. But there is a caveat: we assume that
    the entire training data is **independent and identically distributed** (**iid**).
    This is typically an assumption that is hard to meet in time series since the
    data-generating process can change at any time (either gradually or drastically).
    If we know that the data distribution has changed significantly over time, blending
    the holdout period (which is usually the most recent part of the dataset) is better
    because the meta model is only learning on the latest data, thus paying respect
    to the temporal changes in the distribution of data.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉上，我们可以看到堆叠可以工作得更好，因为它使用一个更大的数据集（通常是所有训练数据）作为样本外预测，所以元模型可能更加泛化。但是有一个警告：我们假设整个训练数据是**独立同分布**（**iid**）。这通常是一个很难在时间序列中满足的假设，因为数据生成过程可以随时改变（逐渐或急剧）。如果我们知道数据分布随时间发生了显著变化，那么混合保留期（通常是数据集的最近部分）更好，因为元模型只学习最新的数据，从而尊重数据分布的时间变化。
- en: 'There is no limit to the number of models we can include as base models, but
    usually, there is a plateau that we reach where additional models do not add much
    to the stacked ensemble. We can also add multiple levels of stacking. For instance,
    let’s assume there are four base learners: *B*[1,] *B*[2,] *B*[3] and *B*[4].
    We have also trained two meta models *M*[1] and *M*[2], on the base models. Now,
    we can train a second-level meta model, *M*, on the outputs of *M*[1] and *M*[2]
    and use that as the final prediction. We can use the `pystacknet` Python library
    ([https://github.com/h2oai/pystacknet](https://github.com/h2oai/pystacknet)),
    which is the Python implementation of an older library, called `stacknet`, to
    make the process of creating multi-level (or single-level) stacked ensembles easy.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以包含作为基本模型的模型数量没有限制，但通常会达到一个平台，额外的模型不会对堆叠集成产生太大的影响。我们还可以添加多个堆叠级别。例如，假设有四个基本学习器：*B*[1,]
    *B*[2,] *B*[3] 和 *B*[4]。我们还训练了两个元模型 *M*[1] 和 *M*[2]，在基本模型上。现在，我们可以在 *M*[1] 和 *M*[2]
    的输出上训练第二级元模型 *M*，并将其用作最终预测。我们可以使用`pystacknet` Python库（[https://github.com/h2oai/pystacknet](https://github.com/h2oai/pystacknet)），这是一个名为`stacknet`的旧库的Python实现，以便轻松创建多级（或单级）堆叠集成的过程。
- en: Another key point to keep in mind is the type of models we usually use as meta
    models. It is assumed that the bulk of the learning has been taken care of by
    the base models, which are the multi-dimensional data for patterns for prediction.
    Therefore, the meta models are usually simple models such as linear regression,
    a decision tree, or even a random forest with much lower depth than the base models.
    Another way to think about this is in terms of bias and variance. Stacking can
    overfit the training or holdout set and by including model families with larger
    flexibility or expressive power, we are enabling this overfitting. The *Further
    reading* section contains a few links that explain different techniques of stacking
    from a general machine learning perspective.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个要牢记的关键点是我们通常用作元模型的模型类型。假设大部分学习已经由基本模型完成，这些基本模型是用于预测的多维数据的模式。因此，元模型通常是简单模型，例如线性回归、决策树，甚至比基本模型低得多的随机森林。另一种思考这个问题的方式是从偏差和方差的角度来看。堆叠可能会过拟合训练或留出集，并通过包含具有更大灵活性或表达能力的模型族，我们正在促使这种过拟合发生。*进一步阅读*部分包含了一些链接，从通用机器学习的角度解释了不同的堆叠技术。
- en: 'Now, let’s quickly see how we can use this in our dataset:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们快速看看如何在我们的数据集中使用这个：
- en: '[PRE9]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This would save the blended prediction for linear regression as `linear_reg_blending`.
    We can use the same code but swap the models to try out other models as well.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为线性回归保存混合预测为`linear_reg_blending`。我们可以使用相同的代码，但交换模型以尝试其他模型。
- en: '**Best practice:**'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践：**'
- en: When there are many base models and we want to do implicit base model selection
    as well, we can opt for one of the regularized linear models, such as Ridge or
    Lasso regression. Breiman, in his original paper, *stacked regressions*, proposed
    to use linear regression with positive coefficients and no intercept as the meta
    model. He argued that this gives a theoretical guarantee that the stacked model
    will be at least as good as any best individual model. But in practice, we can
    relax those assumptions while experimenting. Non-negative regression without intercepts
    is very close to the optimal weighted ensemble we discussed earlier. Finally,
    if we are evaluating multiple stacked models to select which one works well, we
    should resort to either having a separate validation dataset (instead of a *train-validation-test*
    split, we can use a *train-validation-validation_meta-test* split) or using cross-validated
    estimates. If we just pick the stacked model that performs best on the test dataset,
    we are overfitting the test dataset.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在许多基本模型并且我们想要进行隐式基本模型选择时，我们可以选择其中一个正则化线性模型，例如岭回归或套索回归。在他最初的论文中，Breiman提出了*堆叠回归*，建议使用具有正系数且没有截距的线性回归作为元模型。他认为这样可以理论上保证堆叠模型至少与任何最佳个体模型一样好。但在实践中，我们可以在实验中放宽这些假设。没有截距的非负回归与我们之前讨论过的最佳加权集成非常接近。最后，如果我们正在评估多个堆叠模型以选择哪个效果好，我们应该要么使用单独的验证数据集（而不是*训练-验证-测试*分割，我们可以使用*训练-验证-验证元-测试*分割），要么使用交叉验证估计。如果我们只是选择在测试数据集上表现最好的堆叠模型，那么我们就是在测试数据集上过拟合了。
- en: 'Now, let’s see how the blended models are doing on our test data:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看混合模型在我们的测试数据上的表现：
- en: '![Figure 9.9 – Aggregate metrics for blending models ](img/B22389_09_10.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图9.9 – 混合模型的聚合指标](img/B22389_09_10.png)'
- en: 'Figure 9.10: Aggregate metrics for blending models'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10：混合模型的聚合指标
- en: Here, we can see that a simple linear regression has learned a meta model that
    performs much better than any of our average ensemble methods. And the Huber regression
    (which is a way to optimize the MAE directly) performs much better on the MAE
    benchmark. However, keep in mind that this is not universal and has to be evaluated
    for each problem you come across. Choosing the metric to optimize for and the
    model to use to combine makes a lot of difference. And often, the simple average
    ensemble is a very formidable benchmark for combining models.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到简单的线性回归学习了一个比我们任何平均集成方法都要好得多的元模型。而 Huber 回归（这是一种直接优化 MAE 的方法）在 MAE
    基准测试上表现得更好。然而，请记住这并非普遍适用，必须针对遇到的每个问题进行评估。选择要优化的指标和要用于组合的模型会产生很大的差异。通常，简单的平均集成是组合模型的一个非常可观的基准。
- en: Huber regression is another version of linear regression (like Ridge and Lasso)
    where the loss function is a combination of squared loss (used in regular linear
    regression) and absolute loss (used in L1 methods). It behaves like squared loss
    for small residuals and like absolute loss for large residuals. This makes it
    less sensitive to outliers. Scikit-Learn has `HuberRegressor` ([https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html)),
    which implements this.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Huber 回归是线性回归的另一种版本（如岭回归和套索回归），其损失函数是平方损失（用于常规线性回归）和绝对损失（用于L1方法）的组合。对于小残差，它表现得像平方损失，而对于大残差，它表现得像绝对损失。这使得它对异常值不太敏感。Scikit-Learn
    提供了 `HuberRegressor` ([https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html))，用于实现这一方法。
- en: '**Additional reading:**'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**附加阅读：**'
- en: There are other more innovative ways to combine base forecasts. This is an active
    area of research. The *Further reading* section contains links to two such ideas
    that are very similar. **Feature-Based Forecast Model Averaging** (**FFORMA**)
    extracts a set of statistical features from the time series and uses it to train
    a machine learning model that predicts the weights in which the base forecast
    should be combined. Another technique (**self-supervised learning for fast and
    scalable time-series hyper-parameter tuning**), from Facebook (Meta) Research,
    trains a classifier to predict which of the base learners does best, given a set
    of statistical features extracted from the time series.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他更具创新性的方式来结合基础预测。这是一个活跃的研究领域。*进一步阅读*部分包含了两个非常相似的想法的链接。**基于特征的预测模型平均法**（**FFORMA**）从时间序列中提取一组统计特征，并用它来训练一个机器学习模型，预测基础预测应如何加权结合。另一种技术（**用于快速且可扩展的时间序列超参数调整的自监督学习**），来自Facebook（Meta）研究，训练一个分类器，预测给定一组从时间序列中提取的统计特征时，哪个基础学习器表现最好。
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Continuing with the streak of practical lessons in the previous chapter, we
    completed yet another hands-on lesson. In this chapter, we generated forecasts
    from different machine learning models from the previous chapter. We learned how
    to combine these different forecasts into a single forecast that performs better
    than any single model. Then, we explored concepts such as combinatorial optimization
    and stacking/blending to achieve state-of-the-art results.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 延续上一章中的实用课程，我们又完成了一个动手实践的课程。在本章中，我们从上一章的不同机器学习模型中生成了预测结果。我们学会了如何将这些不同的预测结果结合成一个比任何单一模型表现都更好的预测。接着，我们探索了组合优化和堆叠/混合等概念，以实现最先进的结果。
- en: In the next chapter, we will start talking about global models of forecasting
    and explore strategies, feature engineering, and so on to enable such modeling.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始讨论全球预测模型，并探索策略、特征工程等，以便实现这种建模。
- en: References
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'The following references were provided in this chapter:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了以下参考文献：
- en: 'David S. Johnson, Cecilia R. Aragon, Lyle A. McGeoch, and Catherine Schevon
    (1989), *Optimization by Simulated Annealing: An Experimental Evaluation; Part
    I, Graph Partitioning*. Operations Research, 1989, vol. 37, issue 6, 865-892:
    [http://dx.doi.org/10.1287/opre.37.6.865](http://dx.doi.org/10.1287/opre.37.6.865)'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: David S. Johnson，Cecilia R. Aragon，Lyle A. McGeoch，和 Catherine Schevon（1989），*模拟退火优化：实验评估；第一部分，图形划分*。运筹学，1989年，第37卷，第6期，865-892：[http://dx.doi.org/10.1287/opre.37.6.865](http://dx.doi.org/10.1287/opre.37.6.865)
- en: '*L. Breiman* (1996), *Stacked regressions*. Mach Learn 24, 49–64: [https://doi.org/10.1007/BF00117832](https://doi.org/10.1007/BF00117832)'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*L. Breiman*（1996），*堆叠回归*。机器学习 24，49-64：[https://doi.org/10.1007/BF00117832](https://doi.org/10.1007/BF00117832)'
- en: 'Mark J. van der Laan; Eric C.Polley; and Alan E.Hubbard (2007), *Super Learner*.
    U.C. Berkeley Division of Biostatistics Working Paper Series. Working Paper 222:
    [https://biostats.bepress.com/ucbbiostat/paper222](https://biostats.bepress.com/ucbbiostat/paper222)'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mark J. van der Laan；Eric C. Polley；和 Alan E. Hubbard（2007），*超级学习者*。加利福尼亚大学伯克利分校生物统计学系工作论文系列。工作论文222：[https://biostats.bepress.com/ucbbiostat/paper222](https://biostats.bepress.com/ucbbiostat/paper222)
- en: Further reading
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 若想进一步了解本章所涉及的主题，请查阅以下资源：
- en: '*A Kaggler’s Guide to Model Stacking in Practice*, by Ha Nguyen: [https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/](https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kaggler实践中的模型堆叠指南*，由 Ha Nguyen 编写：[https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/](https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)'
- en: 'Kai Ming Ting and Ian H. Witten (1997), *Stacked Generalization: when does
    it work?*: [https://www.ijcai.org/Proceedings/97-2/Papers/011.pdf](https://www.ijcai.org/Proceedings/97-2/Papers/011.pdf
    )'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kai Ming Ting 和 Ian H. Witten (1997)，*堆叠泛化：何时有效？*：[https://www.ijcai.org/Proceedings/97-2/Papers/011.pdf](https://www.ijcai.org/Proceedings/97-2/Papers/011.pdf)
- en: 'Pablo Montero-Manso, George Athanasopoulos, Rob J. Hyndman, Thiyanga S. Talagala
    (2020), *FFORMA: Feature-based forecast model averaging*. International Journal
    of Forecasting, Volume 36, Issue 1: [https://robjhyndman.com/papers/fforma.pdf](https://robjhyndman.com/papers/fforma.pdf)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pablo Montero-Manso, George Athanasopoulos, Rob J. Hyndman, Thiyanga S. Talagala
    (2020)，*FFORMA: 基于特征的预测模型平均*。《国际预测学杂志》，第36卷，第1期：[https://robjhyndman.com/papers/fforma.pdf](https://robjhyndman.com/papers/fforma.pdf)'
- en: 'Peiyi Zhang, et al. (2021), *Self-supervised learning for fast and scalable
    time-series hyper-parameter tuning*: [https://www.ijcai.org/Proceedings/97-2/Papers/011.pdf](https://www.ijcai.org/Proceedings/97-2/Papers/011.pdf)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peiyi Zhang 等人 (2021)，*自监督学习用于快速且可扩展的时间序列超参数调优*：[https://www.ijcai.org/Proceedings/97-2/Papers/011.pdf](https://www.ijcai.org/Proceedings/97-2/Papers/011.pdf)
- en: 'Local versus Global Optima: [https://www.mathworks.com/help/optim/ug/local-vs-global-optima.html](https://www.mathworks.com/help/optim/ug/local-vs-global-optima.html)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部最优与全局最优：[https://www.mathworks.com/help/optim/ug/local-vs-global-optima.html](https://www.mathworks.com/help/optim/ug/local-vs-global-optima.html)
- en: Join our community on Discord
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with authors and other readers:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者及其他读者进行讨论：
- en: '[https://packt.link/mts](https://packt.link/mts)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mts](https://packt.link/mts)'
- en: '![](img/QR_Code15080603222089750.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code15080603222089750.png)'
