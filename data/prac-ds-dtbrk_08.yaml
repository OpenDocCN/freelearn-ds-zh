- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Automating ML Workflows Using Databricks Jobs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Databricks Jobs 自动化 ML 工作流
- en: In the last chapter, we covered the ML deployment life cycle and the various
    model deployment paradigms. We also understood how the response latency, the scalability
    of the solution, and the way we are going to access the predictions play an important
    role in deciding the deployment method.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了 ML 部署生命周期和各种模型部署范式。我们还理解了响应延迟、解决方案的可扩展性以及访问预测的方式在决定部署方法时的重要性。
- en: In this chapter, we are going to take a look at **Databricks Workflows** with
    **Jobs** (previously called **Databricks Jobs**). This functionality can be leveraged
    not only to schedule the retraining of our models at regular intervals but also
    to trigger tests to check our models when transitioning from one **Model Registry**
    stage to another using the webhook integrations we discussed in [*Chapter 6*](B17875_06.xhtml#_idTextAnchor100).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨 **Databricks 工作流** 和 **Jobs**（之前称为 **Databricks Jobs**）。这个功能不仅可以用来定期安排模型的再训练，还可以通过我们在
    [*第六章*](B17875_06.xhtml#_idTextAnchor100) 中讨论的 webhook 集成触发测试，检查模型在从一个 **模型注册表**
    阶段过渡到另一个阶段时的表现。
- en: 'We will be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将覆盖以下主题：
- en: Understanding Databricks Workflows
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Databricks 工作流
- en: Utilizing Databricks Workflows with Jobs to automate model training and testing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 Databricks 工作流与 Jobs 自动化模型训练和测试
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following are the technical requirements for this chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章的技术要求：
- en: Access to the Databricks workspace with **Unrestricted cluster creation** permission
    at a minimum
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少需要拥有 **无限制集群创建**权限的 Databricks 工作区访问权限
- en: All the previous notebooks, executed as described
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照之前描述的执行所有笔记本
- en: Now, let’s take a look at Databricks Workflows.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看 Databricks 工作流。
- en: Understanding Databricks Workflows
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Databricks 工作流
- en: Workflows in the simplest sense are frameworks for developing and running your
    data processing pipelines.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流在最简单的意义上是用于开发和运行数据处理管道的框架。
- en: Databricks Workflows provides a reliable, fully managed orchestration service
    for all your data, analytics, and AI workloads on the **Databricks Lakehouse**
    platform on any cloud. Workflows are designed to ground up with the Databricks
    Lakehouse platform, providing deep monitoring capabilities along with centralized
    observability across all your other workflows. There is no additional cost to
    customers for using Databricks Workflows.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 工作流提供了一项可靠的、完全托管的编排服务，用于处理 **Databricks Lakehouse** 平台上所有数据、分析和 AI
    工作负载，支持任何云平台。工作流从 Databricks Lakehouse 平台开始设计，提供深度监控功能，并提供跨所有其他工作流的集中可观察性。客户使用
    Databricks 工作流无需额外费用。
- en: The key benefit of using workflows is that users don’t need to worry about managing
    orchestration software and infrastructure. Users can simply focus on specifying
    the business logic that needs to be executed as part of the workflows.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用工作流的主要好处是用户不需要担心管理编排软件和基础设施。用户只需要专注于指定需要在工作流中执行的业务逻辑。
- en: 'Within Databricks Workflows, there are two ways you can make use of the managed
    workflows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks 工作流中，有两种方式可以使用托管工作流：
- en: '**Delta Live Tables** (**DLT**): DLT is a declarative ETL framework to develop
    reliable pipelines on the Databricks Lakehouse platform. DLT allows easy monitoring
    of the ETL pipelines while managing the infrastructure needed to run these pipelines.
    It also has built-in expectations to allow validation of incoming data for each
    Delta table and keeps track of data lineage while providing data quality checks.
    DLT provides granular lineage at the table level and provides unified monitoring
    and alerting for all the parts of an ETL pipeline.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Delta Live Tables**（**DLT**）：DLT 是一个声明式的 ETL 框架，用于在 Databricks Lakehouse
    平台上开发可靠的管道。DLT 允许轻松监控 ETL 管道，同时管理运行这些管道所需的基础设施。它还内置了期望功能，以便对每个 Delta 表的输入数据进行验证，并跟踪数据血缘，同时提供数据质量检查。DLT
    提供了表级的详细数据血缘，并为 ETL 管道的所有部分提供统一的监控和警报功能。'
- en: DLT is an advanced topic in itself. Going into a lot of detail about DLT is
    outside the scope of this book. We will provide a link to get started with DLT
    in the *Further* *reading* section.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DLT 本身是一个高级话题。深入讨论 DLT 的细节超出了本书的范围。我们将在 *进一步阅读* 部分提供一个链接，以帮助你开始使用 DLT。
- en: 'The following figure illustrates what capabilities are wrapped inside DLT:'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图展示了 DLT 中包含的功能：
- en: '![Figure 8.1 – All the capabilities that DLT provides](img/B17875_08_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – DLT提供的所有功能](img/B17875_08_01.jpg)'
- en: Figure 8.1 – All the capabilities that DLT provides
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – DLT提供的所有功能
- en: Note
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Delta Pipelines are for pure declarative ETL. You cannot make API calls or send
    an email with them. You should use Delta pipelines for ETL. For everything else,
    use Workflows with Jobs. We will cover Workflows with Jobs in the next section
    from the perspective of triggering automated model retraining at regular intervals
    and performing automated validations on updated models in the Model Registry.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Delta管道用于纯声明式ETL。你不能使用它们进行API调用或发送电子邮件。你应该使用Delta管道进行ETL。对于其他所有任务，使用带有作业的工作流。我们将在下一节中从触发自动化模型重新训练和对模型注册表中更新的模型进行自动化验证的角度来介绍带有作业的工作流。
- en: '**Workflows with Jobs**: A Job is a way we can use to trigger the execution
    of Databricks notebooks, libraries, and more, either immediately or at a fixed
    schedule. We will be covering this in more detail in this chapter from the perspective
    of automating your ML workflow.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带有作业的工作流**：作业是一种触发Databricks笔记本、库等执行的方式，可以立即执行或按照固定计划执行。我们将在本章中更详细地讨论这一点，重点是如何自动化你的ML工作流。'
- en: 'As with almost all Databricks features, you can create Jobs either through
    the UI, **command-line interface** (**CLI**), or API. You can define one or more
    tasks as part of a Workflow with Jobs. A task can entail executing one of the
    following options:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与几乎所有Databricks功能一样，你可以通过UI、**命令行接口**（**CLI**）或API创建作业。你可以定义一个或多个任务作为工作流的一部分。任务可以包括执行以下选项之一：
- en: A **Databricks notebook** that is either in a Git repository that’s accessible
    in your Databricks workspace or in a location in your workspace
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**Databricks笔记本**，该笔记本位于可在Databricks工作区访问的Git存储库中，或者位于工作区中的某个位置
- en: A **Python script** loaded in cloud storage and available through the **Databricks
    file** **system** (**DBFS**)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在云存储中加载的**Python脚本**，可以通过**Databricks文件** **系统**（**DBFS**）访问
- en: '**Java code compiled as a JAR file**, which should be installed on the cluster
    for this option to work'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编译为JAR文件的Java代码**，此选项需要在集群上安装该JAR文件才能工作'
- en: A **DLT** pipeline
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DLT**管道'
- en: A **spark-submit** command, which is a utility that allows the submission of
    Spark or PySpark application programs to the underlying cluster
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**spark-submit**命令，这是一个允许将Spark或PySpark应用程序提交到底层集群的工具'
- en: A **Python wheel**
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python wheel**'
- en: You can chain multiple tasks together as part of a Job and repair and re-run
    a failed or canceled job. Databricks also provides support for monitoring the
    status of Jobs through the UI, CLI, API, and email notifications. Links will be
    provided in the *Further reading* section if you want to learn more about how
    to create and manage Workflows with Jobs using API or CLI. Jobs is a very versatile
    workflow management tool that can be used to develop and chain together tasks
    related to your ETL data pipeline or various steps in your ML workflows.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将多个任务串联在一起作为作业的一部分，并修复和重新运行失败或取消的作业。Databricks还提供了通过UI、CLI、API和电子邮件通知监控作业状态的支持。如果你想了解如何使用API或CLI创建和管理带有作业的工作流，链接将在*进一步阅读*部分提供。Jobs是一个非常灵活的工作流管理工具，可以用于开发和串联与ETL数据管道或ML工作流中各个步骤相关的任务。
- en: Let’s delve into how you can automate the retraining of your machine learning
    models at regular intervals using the *Workflows with Jobs* feature in Databricks.
    Workflows offer fine-grained access control, allowing owners and administrators
    to grant permissions to other users or groups for viewing workflow run results
    and managing the workflow runs themselves. Next, we’ll dive deeper into how to
    utilize Databricks Workflows with Jobs for automating both model training and
    testing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨如何使用Databricks中的*带有作业的工作流*功能，定期自动化机器学习模型的重新训练。工作流提供了精细的访问控制，允许所有者和管理员授予其他用户或小组查看工作流运行结果并管理工作流运行的权限。接下来，我们将更深入地探讨如何利用Databricks工作流与作业来自动化模型训练和测试。
- en: Utilizing Databricks Workflows with Jobs to automate model training and testing
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用Databricks带有作业的工作流自动化模型训练和测试
- en: In this section, we’ll delve into the powerful synergy between Databricks Workflows
    and Jobs to automate the training and testing of machine learning models. Before
    we jump into hands-on examples, it’s essential to understand the significance
    of automation in the ML life cycle and how Databricks uniquely addresses this
    challenge.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨 Databricks 工作流和任务之间的强大协同作用，以自动化机器学习模型的训练和测试。在开始实际操作之前，理解自动化在机器学习生命周期中的重要性，以及
    Databricks 如何独特地解决这一挑战，至关重要。
- en: Automating the training and testing phases in machine learning is not just a
    convenience but a necessity for scalable and efficient ML operations. Manual processes
    are not only time-consuming but also prone to errors, making automation a critical
    aspect of modern MLOps.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，自动化训练和测试阶段不仅仅是一种便利，它是可扩展和高效的机器学习操作的必要条件。手动过程不仅耗时，而且容易出错，使得自动化成为现代 MLOps
    中至关重要的一环。
- en: This is where Databricks Workflows comes in and allows for the orchestration
    of complex ML pipelines.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 Databricks 工作流发挥作用的地方，它允许复杂的机器学习管道的编排。
- en: 'Let’s take a look into an example workflow that we will automate using Workflows
    with Jobs. We will be going through the following logical steps shown in *Figure
    8**.2*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个示例工作流，我们将使用工作流与任务来自动化它。我们将依次执行*图 8.2*中展示的以下逻辑步骤：
- en: '![Figure 8.2 – A sample workflow of automated testing and alerting on new model
    promotions](img/B17875_08_02.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 自动化测试和新模型推广警报的示例工作流](img/B17875_08_02.jpg)'
- en: Figure 8.2 – A sample workflow of automated testing and alerting on new model
    promotions
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 自动化测试和新模型推广警报的示例工作流
- en: 'All the relevant code for this part is in the `Chaper-08` folder. Let’s take
    a look at how we can schedule a Databricks notebook as a Workflow with Jobs:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所有相关的代码都位于`Chaper-08`文件夹中。让我们来看一下如何将 Databricks 笔记本作为工作流与任务一起安排：
- en: 'We first navigate to the ![](img/Icon_1.png) tab in the left navigation bar.
    Here, we can click **Create Job**:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导航到左侧导航栏中的 ![](img/Icon_1.png) 标签。在这里，我们可以点击**创建任务**：
- en: '![Figure 8.3 – The contents of the Databricks Workflows tab](img/B17875_08_03.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – Databricks 工作流标签的内容](img/B17875_08_03.jpg)'
- en: Figure 8.3 – The contents of the Databricks Workflows tab
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – Databricks 工作流标签的内容
- en: 'Here, we can provide a name to the task, and then select **Notebook** under
    **Type**. In **Source**, we have two options – **Workspace** and **Git provider**:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们可以为任务提供一个名称，然后在**类型**下选择**Notebook**。在**来源**中，我们有两个选项 – **工作区**和**Git
    提供者**：
- en: '**Workspace**: Using the file browser, you can navigate to the notebook in
    the workspace you want to schedule as a task:'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作区**：使用文件浏览器，您可以导航到您想要作为任务安排的工作区中的笔记本：'
- en: '![Figure 8.4 – How to browse a notebook for scheduling as a Job through exploring
    the Workspace option](img/B17875_08_04.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – 如何通过浏览工作区选项来浏览笔记本并将其安排为任务](img/B17875_08_04.jpg)'
- en: Figure 8.4 – How to browse a notebook for scheduling as a Job through exploring
    the Workspace option
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 如何通过浏览工作区选项来浏览笔记本并将其安排为任务
- en: 'You can simply navigate to the notebook in the `Repos` folder, as shown in
    the following screenshot, and hit **Confirm**:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以简单地导航到`Repos`文件夹中的笔记本，如下图所示，然后点击**确认**：
- en: '![Figure 8.5 – How to browse a notebook for scheduling as a Job through the
    Repos functionality](img/B17875_08_05.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – 如何通过 Repos 功能浏览笔记本并将其安排为任务](img/B17875_08_05.jpg)'
- en: Figure 8.5 – How to browse a notebook for scheduling as a Job through the Repos
    functionality
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 如何通过 Repos 功能浏览笔记本并将其安排为任务
- en: Note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: One important thing to keep in mind here is that when you use the Repos feature
    of Databricks, it creates a local copy for your repository or a clone. If you
    change code in the repository without performing a Git-pull of the latest version
    in your local repository, your updates will not make their way to the Job that
    you are scheduling with the current version of the repository code.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要记住的一件重要事情是，当您使用 Databricks 的 Repos 功能时，它会为您的代码库创建一个本地副本或克隆。如果您在代码库中更改了代码，但没有在本地代码库中执行
    Git 拉取最新版本，那么您的更新将不会传递到您正在安排的任务中，且任务使用的是当前版本的代码库。
- en: For production deployments, it’s important to make use of the Git provider as
    the source rather than the workspace as the source.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产部署，重要的是使用 Git 提供者作为来源，而不是使用工作区作为来源。
- en: '**Git provider**: This method simplifies the creation and management of Jobs
    during productionizing and automated deployments. The main benefit here is that
    you can version control your data pipelines without managing permissions across
    multiple code repositories. You will also have a single source of truth for your
    model pipelines. Every time the Job executes, it will pull the latest version
    of the notebook/code from the remote repository with the specified branch or tag.
    Databricks supports the following Git providers: GitHub, Bitbucket Cloud, GitLab,
    Azure DevOps (excluding Azure China regions), AWS CodeCommit, and GitHub AE.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Git 提供者**：这种方法简化了在生产化和自动化部署过程中创建和管理作业的过程。这里的主要好处是，你可以在不管理多个代码仓库的权限的情况下对数据管道进行版本控制。你还将拥有一个统一的模型管道来源。每次作业执行时，它都会从远程仓库中拉取指定分支或标签的最新版本的笔记本/代码。Databricks
    支持以下 Git 提供者：GitHub、Bitbucket Cloud、GitLab、Azure DevOps（不包括中国区域的 Azure）、AWS CodeCommit
    和 GitHub AE。'
- en: Note
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: An important thing to keep in mind is that if you use the Git provider option
    as the source of one of your notebooks that will be scheduled as a task, you cannot
    mix and match it with tasks that have notebooks using a workspace as their Source
    as part of the same Job workflow. This limitation is only for using the Databricks
    notebooks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一个需要牢记的重要事项是，如果你使用 Git 提供者选项作为其中一个笔记本的来源，该笔记本将被安排为任务，你不能将其与使用工作区作为来源的笔记本任务混合在同一个作业工作流中。这一限制仅适用于使用
    Databricks 笔记本。
- en: To add the notebook from a Git provider, enter the details of the repository
    you want to access the notebook from. In our case, I will use my own Git repository
    for this book as an example.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 Git 提供者添加笔记本，输入你要访问笔记本的仓库详情。在我们的示例中，我将使用我自己为本书创建的 Git 仓库作为例子。
- en: 'For **Path**, a couple of things need to be kept in mind:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **路径**，有几点需要注意：
- en: You need to enter a path relative to the notebook location
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要输入相对于笔记本位置的路径
- en: Don’t add a `/` or `./` character at the beginning of the notebook path
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要在笔记本路径的开头添加 `/` 或 `./` 字符
- en: Don’t include the file extension, such as `.py`
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要包括文件扩展名，例如`.py`
- en: For adding information about your Git repository, click on **Add a git reference**,
    which will open the following window pictured in *Figure 8.6* where you can select
    your Git provider.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要添加有关 Git 仓库的信息，请点击 **添加 Git 引用**，这将打开如 *图 8.6* 所示的窗口，在这里你可以选择你的 Git 提供者。
- en: 'You can select to execute notebooks from a particular Git branch/tag or commit.
    In my case, I will be using the master branch:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以选择从特定的 Git 分支/标签或提交执行笔记本。在我的例子中，我将使用 master 分支：
- en: '![Figure 8.6 – How to set up a Databricks notebook to execute as a Job from
    the repository](img/B17875_08_06.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.6 – 如何设置一个 Databricks 笔记本从仓库作为作业执行](img/B17875_08_06.jpg)'
- en: Figure 8.6 – How to set up a Databricks notebook to execute as a Job from the
    repository
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 如何设置一个 Databricks 笔记本从仓库作为作业执行
- en: When choosing the cluster, you can either utilize the cluster that is already
    up and running in your workspace or define a new Jobs cluster for all the tasks
    of the workflow. We have covered the difference between the cluster types in [*Chapter
    2*](B17875_02.xhtml#_idTextAnchor036).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 选择集群时，你可以使用工作区中已经启动并运行的集群，也可以为工作流中的所有任务定义一个新的作业集群。我们已经在 [*第 2 章*](B17875_02.xhtml#_idTextAnchor036)
    中讨论了集群类型之间的差异。
- en: Lastly, you can also pass the parameters to your tasks. In notebooks, parameters
    are passed as notebook widgets.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你还可以将参数传递给你的任务。在笔记本中，参数作为笔记本小部件传递。
- en: If you have a use case where you need to set up a JAR, `spark-submit` command,
    Python file, or Python wheel as a task, you can define the input parameters as
    a JSON-formatted array of strings. For Python tasks, the passed parameters can
    be accessed using the `argparse` ([https://docs.python.org/3/library/argparse.html](https://docs.python.org/3/library/argparse.html))
    Python module. For Python wheel tasks, you also have the option to pass in keyword
    arguments as key/value pairs that you can then access using the `argparse` package.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你的使用场景需要设置 JAR、`spark-submit` 命令、Python 文件或 Python wheel 作为任务，你可以将输入参数定义为
    JSON 格式的字符串数组。对于 Python 任务，可以通过 `argparse` ([https://docs.python.org/3/library/argparse.html](https://docs.python.org/3/library/argparse.html))
    Python 模块访问传递的参数。对于 Python wheel 任务，你还可以选择传递关键字参数作为键/值对，然后使用 `argparse` 包访问这些参数。
- en: In the `1`. This setting is important in cases where you may require to have
    overlapping execution of a particular Workflow with Jobs. A request to execute
    a Workflow with Jobs is skipped if the concurrent running instances of the workflow
    that is being requested to be executed have hit the maximum number of concurrent
    runs.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`1`中。这个设置在某些情况下很重要，尤其是当你需要与任务并行执行某个特定工作流时。如果请求执行工作流与任务的并行实例达到了最大并行运行次数，系统将跳过该请求。
- en: In our case, we don’t have any dependent task on our notebook task, so we will
    simply hit **Create**. You can also add multiple interdependent tasks as part
    of a workflow by clicking the ![](img/icon_2.png)icon.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们的笔记本任务没有任何依赖任务，所以我们只需点击**创建**。你还可以通过点击![](img/icon_2.png)图标来添加多个相互依赖的任务，作为工作流的一部分。
- en: Once we have successfully created a task to execute as part of our workflow,
    we can see information about our workflow in the **Jobs** tab of the *Workflows*
    section. Now, we have the option to schedule the running of our workflow at regular
    intervals automatically or interactively using the **Run** **now** button.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们成功创建了一个任务来作为工作流的一部分执行，我们可以在*工作流*部分的**作业**标签中看到关于我们工作流的信息。现在，我们可以选择通过**立即运行**按钮定期自动或交互式地调度工作流的运行。
- en: 'We can see a graph showing the success and failure of the past executions of
    the workflows along with the runtime and the details of our Workflow with Jobs
    on the right-hand side. Take note of **Job ID**, as this will be used to automatically
    trigger our model testing notebook using the webhooks integration with the Model
    Registry:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一个图表，显示过去工作流执行的成功与失败，以及右侧显示的工作流与作业的运行时间和详细信息。请注意**作业 ID**，因为它将用于通过与模型注册表的
    Webhooks 集成自动触发我们的模型测试笔记本：
- en: '![Figure 8.7 – The summary page to monitor the Job run history as well as the
    unique Job ID, Git, Schedule, Compute, and Notifications settings](img/B17875_08_007.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7 – 监控作业运行历史记录以及唯一作业 ID、Git、调度、计算和通知设置的总结页面](img/B17875_08_007.jpg)'
- en: Figure 8.7 – The summary page to monitor the Job run history as well as the
    unique Job ID, Git, Schedule, Compute, and Notifications settings
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 – 监控作业运行历史记录以及唯一作业 ID、Git、调度、计算和通知设置的总结页面
- en: 'Let’s take a look at the contents of the automated testing notebook in the
    `Chapter-08` folder that we just scheduled:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下我们刚刚调度的`Chapter-08`文件夹中自动化测试笔记本的内容：
- en: '`Cmd 2` is simply capturing the value of the `event_message` parameter that
    is sent by the Model Registry webhook. It contains information about the event
    that triggered the execution of this notebook workflow, such as the following:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cmd 2` 只是捕获由模型注册表 Webhook 发送的 `event_message` 参数的值。它包含有关触发此笔记本工作流执行的事件的信息，例如以下内容：'
- en: '`event_timestamp`: Time when the event occurred'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`event_timestamp`：事件发生的时间'
- en: '`event`: Name of the event, as described in the chapter on webhooks'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`event`：事件的名称，如在 Webhooks 章节中描述的那样'
- en: '`text`: Description of the purpose of the webhook that initiated the automated
    test execution'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`：描述启动自动化测试执行的 Webhook 的目的'
- en: '`to_stage`: Target stage for the model to be transitioned to'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`to_stage`：模型要转移到的目标阶段'
- en: '`version`: Model version whose transition triggered this webhook'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`version`：触发此 Webhook 的模型版本'
- en: '`from_stage`: Initial stage of the model version in the Model Registry'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from_stage`：模型版本在模型注册表中的初始阶段'
- en: 'Depending on what type of task we are scheduling, the payload of the webhooks
    changes. There will be a link in the *Further reading* section if you want to
    learn more. The following code snippet demonstrates the process of retrieving
    and parsing the payload from webhooks:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据我们正在调度的任务类型，Webhooks 的有效载荷会有所不同。如果你想了解更多，*进一步阅读*部分会提供一个链接。以下代码片段演示了从 Webhooks
    获取和解析有效载荷的过程：
- en: '[PRE0]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`Cmd 4` is simply running some utility code to interact with the MLflow REST
    API. It is a good practice to write modularized code for writing unit tests for
    your code:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cmd 4` 只是运行一些工具代码来与 MLflow REST API 进行交互。编写模块化代码以编写单元测试是一个很好的实践：'
- en: '[PRE1]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In `Cmd 7`, we are downloading a specific model version from the MLflow Model
    Registry for running our tests:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`Cmd 7`中，我们正在从 MLflow 模型注册表下载一个特定的模型版本来运行我们的测试：
- en: '[PRE2]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The rest of the code displays how you can write arbitrary tests to test your
    model before promoting it to the target stage in the Model Registry. In the sample
    code, we are testing whether the model being tested has the required schema for
    the inputs or not. We are also testing the output data type for the response.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码展示了如何编写任意测试，以在将模型提升到目标阶段之前对其进行测试。在示例代码中，我们测试了正在测试的模型是否具有输入所需的架构。我们还测试了响应的输出数据类型。
- en: 'At the end of successfully running the test, we send a message back to the
    Model Registry reporting whether all the tests passed or failed for the ML engineer
    to review:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试成功运行结束时，我们向模型注册表发送一条消息，报告所有测试是否通过，供机器学习工程师审核：
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With this, we are now ready to register the `automated-test` workflow with our
    model training notebook.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们现在可以准备将`automated-test`工作流与我们的模型训练笔记本进行注册。
- en: Let’s take a look at the model training code. Open the `scheduling-workflow-for-model-retraining`
    notebook.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型训练的代码。打开`scheduling-workflow-for-model-retraining`笔记本。
- en: This notebook has code to first register a Jobs webhook with the `TRANSITION_REQUEST_TO_STAGING_CREATED`
    event trigger for our `Churn Prediction Bank` model in the Model Registry.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个笔记本首先包含注册一个作业Webhook的代码，用于在模型注册表中触发`Churn Prediction Bank`模型的`TRANSITION_REQUEST_TO_STAGING_CREATED`事件。
- en: 'Let’s look at the important cells in the notebook one by one:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一一查看笔记本中的重要单元格：
- en: In `Cmd 2`, we are simply installing a notebook-scoped `databricks-registry-webhooks`
    library from **Python Package Index** (**PyPI**). This is an alternate way to
    interact with the Databricks Model Registry webhooks other than using the Databricks
    REST API we covered in [*Chapter 6*](B17875_06.xhtml#_idTextAnchor100).
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Cmd 2`中，我们仅从**Python软件包索引**(**PyPI**)安装一个笔记本作用域的`databricks-registry-webhooks`库。这是与Databricks模型注册表Webhooks交互的另一种方式，而不是使用我们在[*第6章*](B17875_06.xhtml#_idTextAnchor100)中介绍的Databricks
    REST API。
- en: In `Cmd 3`, we are simply reading our original `raw_data` table from the `bank_churn_analysis`
    table while excluding the features that we will not use to train our model.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Cmd 3`中，我们仅从`bank_churn_analysis`表中读取原始`raw_data`表，并排除我们不会用来训练模型的特征。
- en: '`Cmd 5` is some utility code that is dynamically extracting the token and our
    current workspace URL. This code can be put into its own segregated function to
    make it easy for testing.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Cmd 5`是一些实用代码，动态提取令牌和当前工作区的URL。该代码可以放入独立的函数中，方便测试。'
- en: 'In `Cmd 7`, we are registering the Job workflow we created in *step 1* to be
    triggered by a webhook on the `TRANSITION_REQUEST_TO_STAGING_CREATED` event. In
    the code, replace `<jobid>` with the `Job Id` you noted down in *step 2*:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Cmd 7`中，我们将注册我们在*步骤1*中创建的工作流，以便通过`TRANSITION_REQUEST_TO_STAGING_CREATED`事件触发Webhook。在代码中，将`<jobid>`替换为你在*步骤2*中记录的`作业ID`：
- en: '[PRE4]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we use the `AutoML` Python API to trigger a model retraining job, with
    our primary metric being the `F1` score:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用`AutoML` Python API触发一个模型重新训练任务，主要指标是`F1`得分：
- en: '[PRE5]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we simply use the `MLflowClient` class object to register the best-performing
    model into our Model Registry:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们仅使用`MLflowClient`类对象将表现最佳的模型注册到我们的模型注册表中：
- en: '[PRE6]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We now import some utility code that is just a wrapper on top of the `MLflow
    REST API` using the `%run` magic command. This is how you can modularize your
    code for easy testing and maintainability:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在导入一些实用代码，它只是`MLflow REST API`的一个包装器，使用`%run`魔法命令。这是你如何模块化代码，以便进行方便的测试和维护。
- en: '[PRE7]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In `Cmd 17`, we request transitioning the new model version to Staging. Since
    the new model needs to be tested first before we retire our old model, we are
    not going to archive the existing model version in Staging just yet. The following
    code block demonstrates the same.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Cmd 17`中，我们请求将新模型版本过渡到暂存区。由于新模型在我们淘汰旧模型之前需要先进行测试，因此我们暂时不会将现有模型版本归档到暂存区。以下代码块演示了这一过程。
- en: '[PRE8]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Lastly, we are also going to add a comment for the ML engineer to say that
    the model is ready for testing:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们还将添加一条评论，告知机器学习工程师模型已准备好进行测试：
- en: '[PRE9]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With this, we can see that in the `Churn Prediction Bank` model, there is a
    new version of a registered model with a pending request.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有了这个，我们可以看到，在`Churn Prediction Bank`模型中，注册模型的一个新版本有一个待处理的请求。
- en: 'We can get even more detail about this model by clicking the **Model** version.
    This will show us the request to transition the model and also the comment left
    after the model training:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以通过点击**模型**版本获取更多有关此模型的详细信息。这将显示我们请求模型过渡的情况，并显示训练后留下的评论：
- en: '![Figure 8.8 – The message that we created using API to request model transition
    to Staging](img/B17875_08_08.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.8 – 我们通过 API 创建的请求模型过渡到 Staging 的消息](img/B17875_08_08.jpg)'
- en: Figure 8.8 – The message that we created using API to request model transition
    to Staging
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 – 我们通过 API 创建的请求模型过渡到 Staging 的消息
- en: 'On requesting the transition, we can see that the automated test is now executing
    on our new model version. We can see more details by clicking `automated_test`:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在请求过渡时，我们可以看到自动化测试现在正在执行新模型版本。通过点击 `automated_test`，我们可以查看更多详情：
- en: '![Figure 8.9 – The automated testing Job for any new model that gets a request
    to be transitioned to Staging](img/B17875_08_09.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.9 – 对任何请求过渡到 Staging 的新模型执行的自动化测试作业](img/B17875_08_09.jpg)'
- en: Figure 8.9 – The automated testing Job for any new model that gets a request
    to be transitioned to Staging
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – 对任何请求过渡到 Staging 的新模型执行的自动化测试作业
- en: 'The matrix view shows the current status of our test. We can see the actual
    output of our test:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵视图显示了我们测试的当前状态。我们可以看到测试的实际输出：
- en: '![Figure 8.10 – The matrix view for our automated testing Job run](img/B17875_08_10.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.10 – 我们的自动化测试作业运行的矩阵视图](img/B17875_08_10.jpg)'
- en: Figure 8.10 – The matrix view for our automated testing Job run
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 我们的自动化测试作业运行的矩阵视图
- en: 'On successful completion of the model testing, we can check the status on the
    *model* *version* page:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型测试成功完成后，我们可以在*模型* *版本* 页面查看状态：
- en: '![Figure 8.11 – The successful posting of the message into the Model Registry
    after successful testing of the new model](img/B17875_08_11.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11 – 在成功测试新模型后，消息成功发布到模型注册表](img/B17875_08_11.jpg)'
- en: Figure 8.11 – The successful posting of the message into the Model Registry
    after successful testing of the new model
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 在成功测试新模型后，消息成功发布到模型注册表
- en: Now, the ML engineer or the admin can archive the old model and approve the
    request to transition this model to the Staging environment.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，ML 工程师或管理员可以归档旧模型并批准将此模型过渡到 Staging 环境的请求。
- en: 'In my case, that means transitioning the model version 2 to **Archived**:'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我的情况下，这意味着将模型版本 2 过渡到**归档**：
- en: '![Figure 8.12 – How to transition the existing model in Staging to Archived](img/B17875_08_12.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.12 – 如何将现有模型从 Staging 过渡到归档](img/B17875_08_12.jpg)'
- en: Figure 8.12 – How to transition the existing model in Staging to Archived
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 – 如何将现有模型从 Staging 过渡到归档
- en: 'We can add a comment that can keep track of why this model is being archived,
    which will also be logged in this model version’s activity:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以添加一个评论，记录为什么这个模型被归档，这也将被记录在该模型版本的活动日志中：
- en: '![Figure 8.13 – How to add a message to the model being Archived action](img/B17875_08_13.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.13 – 如何在归档操作中添加消息](img/B17875_08_13.jpg)'
- en: Figure 8.13 – How to add a message to the model being Archived action
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13 – 如何在归档操作中添加消息
- en: 'Now I can approve the model version 3 transition to Staging and add a comment.
    We can approve the transition of the model to Staging and the retiring of the
    old model in Staging when we click on the **Approve** button:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我可以批准模型版本 3 过渡到 Staging 并添加评论。当我们点击**批准**按钮时，我们可以批准模型过渡到 Staging，并将旧模型从 Staging
    中退役：
- en: '![Figure 8.14 – How to approve transitioning of the new model to Staging](img/B17875_08_14.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.14 – 如何批准新模型过渡到 Staging](img/B17875_08_14.jpg)'
- en: Figure 8.14 – How to approve transitioning of the new model to Staging
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 – 如何批准新模型过渡到 Staging
- en: This is useful in cases where you have only one version of a model in Staging
    at a given time. The explicit retiring can be useful if you want to have simultaneous
    candidate models in a stage for A/B testing before selecting the best model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方式在每次 Staging 环境中只有一个版本的模型时非常有用。如果你希望在某一阶段同时存在候选模型进行 A/B 测试，选择最佳模型后再进行退役，明确的退役操作会非常有帮助。
- en: So, now we have executed the end-to-end workflow where we trained a new model
    and also triggered automated testing before promoting the model to Staging.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们已经执行了从训练新模型到在推广到 Staging 之前触发自动化测试的端到端工作流。
- en: The last thing to do here is to schedule the monthly retraining of our model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要做的事情是安排每月的模型再训练。
- en: 'Go back to the `scheduling-workflow-for-model-retraining` notebook and open
    it. On the top right of every Databricks notebook, you have a button called **Schedule**.
    On clicking that, you can specify how often you want to execute this notebook
    and what type of cluster to execute it on. You can also add parameters for the
    notebook and set alerts:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 `scheduling-workflow-for-model-retraining` 笔记本并打开它。在每个 Databricks 笔记本的右上角，您会看到一个名为
    **Schedule** 的按钮。点击该按钮后，您可以指定希望执行此笔记本的频率以及希望在哪种类型的集群上执行它。您还可以为笔记本添加参数并设置警报：
- en: '![Figure 8.15 – How to set up our model for automated retraining](img/B17875_08_15.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.15 – 如何为自动化再训练设置模型](img/B17875_08_15.jpg)'
- en: Figure 8.15 – How to set up our model for automated retraining
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15 – 如何为自动化再训练设置模型
- en: As we’ve explored the intricacies of automating machine learning workflows using
    Databricks Jobs, you should now have a solid understanding of how to set up and
    manage your automated ML retraining workflows. Next, we’ll summarize the key takeaways
    in the *Summary* section to help you consolidate your knowledge.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨如何使用 Databricks Jobs 自动化机器学习工作流的复杂性后，您现在应该已经对如何设置和管理自动化 ML 再训练工作流有了扎实的理解。接下来，我们将在*总结*部分总结关键要点，帮助您巩固知识。
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed the workflow management options available in the
    Databricks environment. We also looked at Workflows with Jobs functionality in
    more detail in relation to its utility in automating your ML workflows.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们讨论了在 Databricks 环境中可用的工作流管理选项。我们还更详细地探讨了带有 Jobs 功能的工作流，重点介绍了它在自动化 ML 工作流中的实用性。
- en: We went through a sample workflow of creating a notebook with tests to perform
    on any new model we want to transition to the Staging stage of the Model Registry.
    We then configured the Model Registry Jobs webhooks feature to be triggered by
    another automated model retraining notebook. Similar workflows can make your model
    tests arbitrarily complex to fit your needs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们演示了一个样本工作流，该工作流创建了一个包含测试的笔记本，以便对任何我们希望过渡到模型注册表 Staging 阶段的新模型进行测试。然后，我们配置了模型注册表
    Jobs Webhook 功能，以便通过另一个自动化的模型再训练笔记本来触发该功能。类似的工作流可以使您的模型测试变得复杂多样，以满足您的需求。
- en: In the last chapter, we will cover the concept of model drift and how to trigger
    a model’s retraining automatically.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一章中，我们将介绍模型漂移的概念，以及如何自动触发模型的再训练。
- en: Further reading
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Here are some links to further your understanding:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些链接可以进一步加深您的理解：
- en: 'Databricks, *What Is Delta Live* *Tables?*: [https://docs.databricks.com/en/delta-live-tables/index.html](https://docs.databricks.com/en/delta-live-tables/index.html)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks, *什么是 Delta Live* *表格*：[https://docs.databricks.com/en/delta-live-tables/index.html](https://docs.databricks.com/en/delta-live-tables/index.html)
- en: 'Databricks, *Introduction to Databricks* *Workflows*: [https://docs.databricks.com/en/workflows/index.html](https://docs.databricks.com/en/workflows/index.html)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks, *Databricks 入门* *工作流*：[https://docs.databricks.com/en/workflows/index.html](https://docs.databricks.com/en/workflows/index.html)
