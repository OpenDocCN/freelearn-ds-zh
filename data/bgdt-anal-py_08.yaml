- en: '*Chapter 8*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*'
- en: Creating a Full Analysis Report
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建完整的分析报告
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，您将能够：
- en: Read data from different sources in Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从不同源读取Spark数据
- en: Perform SQL operations on a Spark DataFrame
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对Spark DataFrame执行SQL操作
- en: Generate statistical measurements in a consistent way
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以一致的方式生成统计度量
- en: Generate graphs and plots using Plotly
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Plotly生成图表
- en: Compile an analysis report incorporating all the previous steps and data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汇总包含所有先前步骤和数据的分析报告
- en: In this chapter, we will read the data using Spark, aggregating it, and extract
    the statistical measurements. We will also use the Pandas to generate graphs from
    aggregated data and form an analysis report.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Spark读取数据、聚合数据并提取统计度量。我们还将使用Pandas从聚合数据生成图表，并形成分析报告。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: If you have been part of the data industry for a while, you will understand
    the challenge of working with different data sources, analyzing them, and presenting
    them in consumable business reports. When using Spark on Python, you may have
    to read data from various sources, such as flat files, REST APIs in JSON format,
    and so on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经在数据行业工作了一段时间，您会理解与不同数据源打交道、分析它们并以可消费的业务报告呈现它们的挑战。在使用Python上的Spark时，您可能需要从各种数据源读取数据，如平面文件、REST
    API中的JSON格式等。
- en: In the real world, getting data in the right format is always a challenge and
    several SQL operations are required to gather data. Thus, it is mandatory for
    any data scientist to know how to handle different file formats and different
    sources, and to carry out basic SQL operations and present them in a consumable
    format.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，获取正确格式的数据始终是一个挑战，需要进行多个SQL操作来收集数据。因此，任何数据科学家都必须知道如何处理不同的文件格式和数据源，进行基本的SQL操作，并将其以可消费的格式呈现。
- en: This chapter provides common methods for reading different types of data, carrying
    out SQL operations on it, doing descriptive statistical analysis, and generating
    a full analysis report. We will start with understanding how to read different
    kinds of data into PySpark and will then generate various analyses and plots on
    it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了读取不同类型数据、对其进行SQL操作、进行描述性统计分析并生成完整分析报告的常用方法。我们将从理解如何将不同类型的数据读取到PySpark中开始，然后对其生成各种分析和图表。
- en: Reading Data in Spark from Different Data Sources
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从不同数据源读取Spark数据
- en: One of the advantages of Spark is the ability to read data from various data
    sources. However, this is not consistent and keeps changing with each Spark version.
    This section of the chapter will explain how to read files in CSV and JSON.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的一个优势是能够从各种数据源读取数据。然而，这种能力并不一致，并且随着每个Spark版本的更新而变化。本章的这一部分将解释如何从CSV和JSON文件中读取数据。
- en: 'Exercise 47: Reading Data from a CSV File Using the PySpark Object'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 47：使用PySpark对象从CSV文件读取数据
- en: To read CSV data, you have to write the `spark.read.csv("the file name with
    .csv")` function. Here, we are reading the bank data that was used in the earlier
    chapters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取CSV数据，您必须编写`spark.read.csv("带有.csv的文件名")`函数。这里，我们读取的是前面章节中使用的银行数据。
- en: Note
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The `sep` function is used here.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用了`sep`函数。
- en: We have to ensure that the right `sep` function is used based on how the data
    is separated in the source data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须确保根据源数据的分隔方式使用正确的`sep`函数。
- en: 'Now let''s perform the following steps to read the data from the `bank.csv`
    file:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行以下步骤从`bank.csv`文件读取数据：
- en: 'First, let''s import the required packages into the Jupyter notebook:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们将所需的包导入到Jupyter笔记本中：
- en: '[PRE0]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, import all the required libraries, as illustrated:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导入所有必需的库，如下所示：
- en: '[PRE1]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, use the `tick` themes, which will make our dataset more visible and provide
    higher contrast:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用`tick`主题，这将使我们的数据集更加可视化，并提供更高的对比度：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, change the working directory using the following command:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令更改工作目录：
- en: '[PRE3]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s import the libraries required for Spark to build the Spark session:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入构建Spark会话所需的库：
- en: '[PRE4]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let''s read the CSV data after creating the `df_csv` Spark object using
    the following command:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们在创建`df_csv` Spark对象后，通过以下命令读取CSV数据：
- en: '[PRE5]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Print the schema using the following command:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令打印模式：
- en: '[PRE6]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.1: Bank schema](img/C12913_08_01.jpg)'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.1：银行模式](img/C12913_08_01.jpg)'
- en: 'Figure 8.1: Bank schema'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.1：银行模式
- en: Reading JSON Data Using the PySpark Object
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PySpark对象读取JSON数据
- en: 'To read JSON data, you have to write the `read.json("the file name with .json")`
    function after setting the SQL context:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取JSON数据，必须在设置SQL上下文后使用`read.json("带有.json的文件名")`函数：
- en: '![Figure 8.2: Reading JSON file in PySpark](img/C12913_08_02.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2：在PySpark中读取JSON文件](img/C12913_08_02.jpg)'
- en: 'Figure 8.2: Reading JSON file in PySpark'
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.2：在PySpark中读取JSON文件
- en: SQL Operations on a Spark DataFrame
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark DataFrame上的SQL操作
- en: A DataFrame in Spark is a distributed collection of rows and columns. It is
    the same as a table in a relational database or an Excel sheet. A Spark RDD/DataFrame
    is efficient at processing large amounts of data and has the ability to handle
    petabytes of data, whether structured or unstructured.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的DataFrame是一个分布式的行列集合。它与关系型数据库中的表或Excel工作表相同。Spark的RDD/DataFrame能够高效处理大量数据，并且可以处理PB级别的数据，无论是结构化数据还是非结构化数据。
- en: Spark optimizes queries on data by organizing the DataFrame into columns, which
    helps Spark understand the schema. Some of the most frequently used SQL operations
    include subsetting the data, merging the data, filtering, selecting specific columns,
    dropping columns, dropping all null values, and adding new columns, among others.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Spark通过将DataFrame组织成列来优化数据查询，这有助于Spark理解数据的模式。最常用的一些SQL操作包括对数据进行子集化、合并数据、过滤、选择特定列、删除列、删除所有空值以及添加新列等。
- en: 'Exercise 48: Reading Data in PySpark and Carrying Out SQL Operations'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习48：在PySpark中读取数据并进行SQL操作
- en: For summary statistics of data, we can use the `spark_df.describe().show()`
    function, which will provide information on `count`, `mean`, `standard deviation`,
    `max`, and `min` for all the columns in the DataFrame.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据的总结统计，我们可以使用` spark_df.describe().show()`函数，它将提供DataFrame中所有列的`count`、`mean`、`standard
    deviation`、`max`和`min`等信息。
- en: 'For example, in the dataset that we have considered—the bank marketing dataset
    ([https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson08/bank.csv](https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson08/bank.csv))—the
    summary statistics data can be obtained as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们所考虑的数据集——银行营销数据集([https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson08/bank.csv](https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson08/bank.csv))——中，可以通过以下方式获得总结统计数据：
- en: 'After creating a new Jupyter notebook, import all the required packages, as
    illustrated here:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Jupyter笔记本后，导入所有必要的包，如下所示：
- en: '[PRE7]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, change the working directory using the following command:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令更改工作目录：
- en: '[PRE8]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Import all the libraries required for Spark to build the Spark session:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有需要的库以构建Spark会话：
- en: '[PRE9]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create and read the data from the CSV file using the Spark object, as illustrated:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Spark对象创建并读取CSV文件中的数据，如下所示：
- en: '[PRE10]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let''s print the first five rows from the Spark object using the following
    command:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下命令打印Spark对象的前五行：
- en: '[PRE11]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.3: Bank data of first five rows (Unstructured)](img/C12913_08_03.jpg)'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.3：银行数据的前五行（非结构化）](img/C12913_08_03.jpg)'
- en: 'Figure 8.3: Bank data of first five rows (Unstructured)'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.3：银行数据的前五行（非结构化）
- en: 'The previous output is unstructured. Let''s first identify the data types to
    proceed to get the structured data. Use the following command to print the datatype
    of each column:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的输出是非结构化的。让我们首先识别数据类型，以便获取结构化数据。使用以下命令打印每列的数据类型：
- en: '[PRE12]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.4: Bank datatypes (Structured)](img/C12913_08_04.jpg)'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.4：银行数据类型（结构化）](img/C12913_08_04.jpg)'
- en: 'Figure 8.4: Bank datatypes (Structured)'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.4：银行数据类型（结构化）
- en: 'Now let''s calculate the total number of rows and columns with names to get
    a clear idea of the data we have:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们计算行和列的总数，并查看数据的概况：
- en: '[PRE13]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE14]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.5: Total number of rows and columns names](img/Image38437.jpg)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.5：行和列名称的总数](img/Image38437.jpg)'
- en: 'Figure 8.5: Total number of rows and columns names'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.5：行和列名称的总数
- en: 'Print the summary statistics for the DataFrame using the following command:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令打印DataFrame的总结统计：
- en: '[PRE15]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.6: Summary statistics of numerical columns](img/C12913_08_06.jpg)'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.6：数值列的总结统计](img/C12913_08_06.jpg)'
- en: 'Figure 8.6: Summary statistics of numerical columns'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.6：数值列的总结统计
- en: 'To select multiple columns from a DataFrame, we can use the `spark_df.select(''col1'',
    ''col2'', ''col3'')` function. For example, let''s select the first five rows
    from the `balance` and `y` columns using the following command:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要从 DataFrame 中选择多个列，可以使用 `spark_df.select('col1', 'col2', 'col3')` 函数。例如，使用以下命令从
    `balance` 和 `y` 列中选择前五行：
- en: '[PRE16]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.7: Data of the balance and y columns](img/C12913_08_07.jpg)'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.7：余额和 y 列的数据](img/C12913_08_07.jpg)'
- en: 'Figure 8.7: Data of the balance and y columns'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.7：余额和 y 列的数据
- en: 'To identify the relation between two variables in terms of their frequency
    of levels, `crosstab` can be used. To derive crosstab between two columns, we
    can use the `spark_df.crosstab(''col1'', col2)` function. Crosstab is carried
    out between two categorical variables and not between numerical variables:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了识别两个变量之间在频率层次上的关系，可以使用 `crosstab`。要得出两个列之间的交叉表，可以使用 `spark_df.crosstab('col1',
    'col2')` 函数。交叉表是针对两个分类变量进行的，而不是数字变量：
- en: '[PRE17]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Figure 8.8: Pair wise frequency of categorical columns](img/C12913_08_08.jpg)'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.8：分类列的配对频率](img/C12913_08_08.jpg)'
- en: 'Figure 8.8: Pair wise frequency of categorical columns'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.8：分类列的配对频率
- en: 'Now, let''s add a new column to the dataset:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们向数据集添加一列新列：
- en: '[PRE18]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.9: Data of newly added column](img/C12913_08_09.jpg)'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.9：新添加列的数据](img/C12913_08_09.jpg)'
- en: 'Figure 8.9: Data of newly added column'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.9：新添加列的数据
- en: 'Drop the newly created column using the following command:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令删除新创建的列：
- en: '[PRE19]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Exercise 49: Creating and Merging Two DataFrames'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 49：创建和合并两个 DataFrame
- en: In this exercise, we will extract and use the bank marketing data ([https://archive.ics.uci.edu/ml/datasets/bank+marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing))
    from the UCI Machine Learning Repository. The objective is to carry out merge
    operations on a Spark DataFrame using PySpark.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将提取并使用银行营销数据（[https://archive.ics.uci.edu/ml/datasets/bank+marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing)）来自
    UCI 机器学习库。目标是使用 PySpark 对 Spark DataFrame 执行合并操作。
- en: The data is related to the direct marketing campaigns of a Portuguese banking
    institution. The marketing campaigns were based on phone calls. Often, more than
    one contact for the same client was required, in order to access whether the product
    (**bank term deposit**) would be (**yes**) or would not be (**no**) subscribed.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 数据与葡萄牙银行机构的直接营销活动相关。营销活动主要通过电话进行。通常，需要对同一客户进行多次联系，以判断该产品（**银行定期存款**）是否会被（**是**）或不会被（**否**）订阅。
- en: 'Now, let''s create two DataFrames from the current bank marketing data and
    merge them on the basis of a primary key:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从当前的银行营销数据创建两个 DataFrame，并基于主键将它们合并：
- en: 'First, let''s import the required header files in the Jupyter notebook:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在 Jupyter notebook 中导入所需的头文件：
- en: '[PRE20]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, change the working directory using the following command:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令更改工作目录：
- en: '[PRE21]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Import all the libraries required for Spark to build the Spark session:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入构建 Spark 会话所需的所有库：
- en: '[PRE22]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Read the data from the CSV files into a Spark object using the following command:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将数据从 CSV 文件读取到 Spark 对象：
- en: '[PRE23]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Print the first five rows from the Spark object:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印 Spark 对象的前五行：
- en: '[PRE24]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/Image38484.jpg)'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/Image38484.jpg)'
- en: 'Figure 8.10: Bank data of first five rows (Unstructured)'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.10：前五行的银行数据（非结构化）
- en: Now, to merge the two DataFrames using the primary key (ID), first, we will
    have to split it into two DataFrames.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了使用主键（ID）合并两个 DataFrame，首先需要将其拆分成两个 DataFrame。
- en: 'First, add a new DataFrame with an `ID` column:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，添加一个包含 `ID` 列的新 DataFrame：
- en: '[PRE25]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, create another column, `ID2`:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建另一个列 `ID2`：
- en: '[PRE26]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Split the DataFrame using the following command:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令拆分 DataFrame：
- en: '[PRE27]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, change the ID column names of `train_with_id2`:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，修改 `train_with_id2` 的 ID 列名：
- en: '[PRE28]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Merge `train_with_id1` and `train_with_id2` using the following command:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令合并 `train_with_id1` 和 `train_with_id2`：
- en: '[PRE29]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Exercise 50: Subsetting the DataFrame'
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 50：子集化 DataFrame
- en: In this exercise, we will extract and use the bank marketing data ([https://archive.ics.uci.edu/ml/datasets/bank+marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing))
    from the UCI Machine Learning Repository. The objective is to carry out filter/subsetting
    operations on the Spark DataFrame using PySpark.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将从UCI机器学习库提取并使用银行营销数据（[https://archive.ics.uci.edu/ml/datasets/bank+marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing)）。目标是使用PySpark对Spark数据框进行过滤/子集操作。
- en: 'Let''s subset the DataFrame where the balance is greater than `0` in the bank
    marketing data:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从银行营销数据中提取出余额大于`0`的子集：
- en: 'First, let''s import the required header files in the Jupyter notebook:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在Jupyter笔记本中导入所需的头文件：
- en: '[PRE30]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, change the working directory using the following command:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令更改工作目录：
- en: '[PRE31]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Import all the libraries required for Spark to build the Spark session:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入构建Spark会话所需的所有库：
- en: '[PRE32]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, read the CSV data as a Spark object using the following command:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令将CSV数据作为Spark对象读取：
- en: '[PRE33]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s run SQL queries to subset and filter the DataFrame:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们运行SQL查询以子集化并过滤数据框：
- en: '[PRE34]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output is as follows:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/Image38496.jpg)'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/Image38496.jpg)'
- en: 'Figure 8.11: Filtered DataFrame'
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.11：过滤后的数据框
- en: Generating Statistical Measurements
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成统计度量
- en: 'Python is a general-purpose language with statistical modules. A lot of statistical
    analysis, such as carrying out descriptive analysis, which includes identifying
    the distribution of data for numeric variables, generating a correlation matrix,
    the frequency of levels in categorical variables with identifying mode and so
    on, can be carried out in Python. The following is an example of correlation:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Python是一种通用语言，具有统计模块。很多统计分析（如描述性分析，包括识别数值变量的数据分布，生成相关矩阵，识别类别变量的各个层次频率及其众数等）可以在Python中完成。以下是一个相关性示例：
- en: '![Figure 8.12: Segment numeric data and correlation matrix output](img/C12913_08_12.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.12：分段数值数据与相关矩阵输出](img/C12913_08_12.jpg)'
- en: 'Figure 8.12: Segment numeric data and correlation matrix output'
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.12：分段数值数据与相关矩阵输出
- en: 'Identifying the distribution of data and normalizing it is important for parametric
    models such as `yeo-johnson` method to normalize the data:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 识别数据分布并对其进行标准化对参数模型（如`yeo-johnson`方法）进行数据标准化非常重要：
- en: '![Figure 8.13: Identifying the distribution of the data—Normality test](img/C12913_08_13.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.13：识别数据分布——正态性检验](img/C12913_08_13.jpg)'
- en: 'Figure 8.13: Identifying the distribution of the data—Normality test'
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.13：识别数据分布——正态性检验
- en: The identified variables are then normalized using `yeo-johnson` or the `box-cox`
    method.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用`yeo-johnson`或`box-cox`方法对识别出的变量进行标准化。
- en: 'Generating the importance of features is important in a data science project
    where predictive techniques are used. This broadly falls under statistical analysis
    as various statistical techniques are used to identify the important variables.
    One of the methods that''s used here is `Boruta`, which is a wrap-around `RandomForest`
    algorithm for variable importance analysis. For this, we will be using the `BorutaPy`
    package:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 生成特征的重要性在数据科学项目中非常重要，尤其是在使用预测技术时。这大致属于统计分析范畴，因为各种统计技术用于识别重要的变量。这里使用的一种方法是`Boruta`，它是一个围绕`RandomForest`算法的变量重要性分析方法。为此，我们将使用`BorutaPy`包：
- en: '![Figure 8.14: Feature importance](img/C12913_08_14.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.14：特征重要性](img/C12913_08_14.jpg)'
- en: 'Figure 8.14: Feature importance'
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.14：特征重要性
- en: 'Activity 15: Generating Visualization Using Plotly'
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 15：使用Plotly生成可视化
- en: In this activity, we will extract and use the bank marketing data from the UCI
    Machine Learning Repository. The objective is to generate visualizations using
    Plotly in Python.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，我们将从UCI机器学习库提取并使用银行营销数据。目标是使用Python中的Plotly生成可视化图表。
- en: Note
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Plotly's Python graphing library makes interactive, publication-quality graphs.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Plotly的Python绘图库可以生成互动式、出版质量的图表。
- en: 'Perform the following steps to generate the visualization using Plotly:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以使用Plotly生成可视化图表：
- en: Import the required libraries and packages into the Jupyter notebook.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所需的库和包导入到Jupyter笔记本中。
- en: 'Import the libraries required for Plotly to visualize the data visualization:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于Plotly的数据可视化所需的库：
- en: '[PRE35]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Read the data from the `bank.csv` file into the Spark DataFrame.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`bank.csv`文件中读取数据到Spark数据框。
- en: 'Check the Plotly version that you are running on your system. Make sure you
    are running the updated version. Use the `pip install plotly --upgrade` command
    and then run the following code:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查您系统上运行的Plotly版本。确保您正在运行更新版本。使用`pip install plotly --upgrade`命令，然后运行以下代码：
- en: '[PRE36]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is as follows:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE37]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now import the required libraries to plot the graphs using Plotly:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在导入所需的库以使用Plotly绘制图表：
- en: '[PRE38]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Set the Plotly credentials in the following command, as illustrated here:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下命令中设置Plotly凭据，如此处所示：
- en: '[PRE39]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To generate an API key for Plotly, sign up for an account and navigate to [https://plot.ly/settings#/](https://plot.ly/settings#/).
    Click on the **API Keys** option and then click on the **Regenerate Key** option.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要为Plotly生成API密钥，请注册帐户并转到[https://plot.ly/settings#/](https://plot.ly/settings#/)。单击**API
    Keys**选项，然后单击**Regenerate Key**选项。
- en: 'Now, plot each of the following graphs using Plotly:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用Plotly绘制以下每个图形：
- en: 'Bar graph:'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 条形图：
- en: '![Figure 8.15: Bar graph of bank data](img/C12913_08_15.jpg)'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.15: 银行数据的条形图](img/C12913_08_15.jpg)'
- en: 'Figure 8.15: Bar graph of bank data'
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 8.15: 银行数据的条形图'
- en: 'Scatter plot:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图：
- en: '![Figure 8.16: Scatter plot of bank data](img/C12913_08_16.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.16: 银行数据的散点图](img/C12913_08_16.jpg)'
- en: 'Figure 8.16: Scatter plot of bank data'
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 8.16: 银行数据的散点图'
- en: 'Boxplot:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图：
- en: '![Figure 8.17: Boxplot of bank data](img/C12913_08_17.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.17: 银行数据的箱线图](img/C12913_08_17.jpg)'
- en: 'Figure 8.17: Boxplot of bank data'
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 8.17: 银行数据的箱线图'
- en: Note
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 248.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可在第 248 页找到。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to import data from various sources into a Spark
    environment as a Spark DataFrame. In addition, we learned how to carry out various
    SQL operations on that DataFrame, and how to generate various statistical measures,
    such as correlation analysis, identifying the distribution of data, building a
    feature importance model, and so on. We also looked into how to generate effective
    graphs using Plotly offline, where you can generate various plots to develop an
    analysis report.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何将数据从各种来源导入Spark环境作为Spark DataFrame。此外，我们还学习了如何对该DataFrame执行各种SQL操作，以及如何生成各种统计措施，如相关性分析，数据分布识别，构建特征重要性模型等。我们还研究了如何使用Plotly离线生成有效的图表，您可以生成各种图表以开发分析报告。
- en: 'This book has hopefully offered a stimulating journey through big data. We
    started with Python and covered several libraries that are part of the Python
    data science stack: NumPy and Pandas, We also looked at home we can use Jupyter
    notebooks. We then saw how to create informative data visualizations, with some
    guiding principles on what is a good graph, and used Matplotlib and Seaborn to
    materialize the figures. Then we made a start with the Big Data tools - Hadoop
    and Spark, thereby understanding the principles and the basic operations.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本书希望为您提供一次关于大数据的激动人心的旅程。我们从Python开始，涵盖了Python数据科学堆栈的几个库：NumPy和Pandas，我们还看到如何使用Jupyter笔记本。然后，我们学习了如何创建信息化的数据可视化图表，介绍了良好图表的一些指导原则，并使用Matplotlib和Seaborn生成图形。接着，我们开始使用大数据工具
    - Hadoop和Spark，从而理解了基本原理和操作。
- en: We have seen how we can use DataFrames in Spark to manipulate data, and have
    about utilize concepts such as correlation and dimensionality reduction to better
    understand our data. The book has also covered reproducibility, so that the analysis
    created can be supported and better replicated when needed, and we finished our
    journey with a final report. We hope that the subjects covered, and the practical
    examples in this book will help you in all areas of your own data journey.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何在Spark中使用DataFrame来操作数据，并且已经掌握了利用诸如相关性和维度缩减等概念来更好地理解我们的数据。本书还涵盖了可复现性，以便在需要时能够支持和更好地复制分析，我们以最终报告结束了我们的旅程。希望本书涵盖的主题和实际示例能帮助您在数据旅程的各个领域中取得成功。
