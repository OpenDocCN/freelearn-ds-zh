- en: 'Chapter 13: Integrating External Tools with Spark SQL'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章：将外部工具与Spark SQL集成
- en: '**Business intelligence** (**BI**) refers to the capabilities that enable organizations
    to make informed, data-driven decisions. BI is a combination of data processing
    capabilities, data visualizations, business analytics, and a set of best practices
    that enable, refine, and streamline organizations'' business processes by helping
    them in both strategic and tactical decision making. Organizations typically rely
    on specialist software called BI tools for their BI needs. BI tools combine strategy
    and technology to gather, analyze, and interpret data from various sources and
    provide business analytics about the past and present state of a business.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**商业智能**（**BI**）指的是使组织能够做出明智的、数据驱动的决策的能力。BI结合了数据处理能力、数据可视化、业务分析和一套最佳实践，帮助组织在战略和战术决策中优化、精炼和简化其业务流程。组织通常依赖专业的软件工具，称为BI工具，以满足其BI需求。BI工具将战略与技术结合，收集、分析和解释来自不同来源的数据，并提供有关企业过去和现在状态的业务分析。'
- en: BI tools have traditionally relied on data warehouses as data sources and data
    processing engines. However, with the advent of big data and real-time data, BI
    tools have branched out to using data lakes and other new data storage and processing
    technologies as data sources. In this chapter, you will explore how Spark SQL
    can be used as a distributed **Structured Query Language** (**SQL**) engine for
    BI and SQL analysis tools via Spark Thrift **Java Database Connectivity/Open Database
    Connectivity** (**JDBC/ODBC**) Server. Spark SQL connectivity requirements with
    SQL analysis and BI tools will be presented, along with detailed configuration
    and setup steps. Finally, the chapter will also present options to connect to
    Spark SQL from arbitrary Python applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: BI工具传统上依赖数据仓库作为数据源和数据处理引擎。然而，随着大数据和实时数据的出现，BI工具已经扩展到使用数据湖和其他新的数据存储和处理技术作为数据源。在本章中，您将探索如何通过Spark
    Thrift **Java数据库连接/开放数据库连接**（**JDBC/ODBC**）服务器，将Spark SQL作为分布式**结构化查询语言**（**SQL**）引擎，用于BI和SQL分析工具。将介绍Spark
    SQL与SQL分析和BI工具的连接要求，以及详细的配置和设置步骤。最后，本章还将介绍从任意Python应用程序连接到Spark SQL的选项。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Apache Spark as a **distributed SQL engine**
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark作为**分布式SQL引擎**
- en: Spark connectivity to SQL analysis tools
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark与SQL分析工具的连接
- en: Spark connectivity to BI tools
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark与BI工具的连接
- en: Connecting Python applications to Spark SQL using `pyodbc`
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pyodbc`将Python应用程序连接到Spark SQL
- en: Some of the skills gained in this chapter are an understanding of Spark Thrift
    JDBC/ODBC Server, how to connect SQL editors and BI tools to Spark SQL via JDBC
    and Spark Thrift Server, and connecting business applications built using Python
    with a Spark SQL engine using Pyodbc.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所获得的技能包括理解Spark Thrift JDBC/ODBC服务器、如何通过JDBC和Spark Thrift服务器将SQL编辑器和BI工具连接到Spark
    SQL，以及使用Pyodbc将基于Python的业务应用程序与Spark SQL引擎连接。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Here is what you''ll need for this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所需内容如下：
- en: In this chapter, we will be using Databricks Community Edition to run our code
    ([https://community.cloud.databricks.com](https://community.cloud.databricks.com)).
    Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Databricks社区版来运行代码（[https://community.cloud.databricks.com](https://community.cloud.databricks.com)）。注册说明请参见[https://databricks.com/try-databricks](https://databricks.com/try-databricks)。
- en: We will be using a free and open source SQL editor tool called **SQL Workbench/J**,
    which can be downloaded from [https://www.sql-workbench.eu/downloads.html](https://www.sql-workbench.eu/downloads.html).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用一个免费的开源SQL编辑器工具**SQL Workbench/J**，可以从[https://www.sql-workbench.eu/downloads.html](https://www.sql-workbench.eu/downloads.html)下载。
- en: You will need to download a JDBC driver for SQL Workbench/J to be able to connect
    with Databricks Community Edition. This can be downloaded from [https://databricks.com/spark/jdbc-drivers-download](https://databricks.com/spark/jdbc-drivers-download).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要下载一个JDBC驱动程序，使SQL Workbench/J能够与Databricks社区版连接。您可以从[https://databricks.com/spark/jdbc-drivers-download](https://databricks.com/spark/jdbc-drivers-download)下载该驱动程序。
- en: We will also be using **Tableau Online** to demonstrate BI tool integration.
    You can request a free 14-day Tableau Online trial at [https://www.tableau.com/products/online/request-trial](https://www.tableau.com/products/online/request-trial).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将使用 **Tableau Online** 来演示 BI 工具的集成。你可以在 [https://www.tableau.com/products/online/request-trial](https://www.tableau.com/products/online/request-trial)
    申请免费的 14 天 Tableau Online 试用。
- en: Apache Spark as a distributed SQL engine
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 作为一个分布式 SQL 引擎
- en: One common application of SQL has been its use with BI and SQL analysis tools.
    These SQL-based tools connect to a **relational database management system** (**RDBMS**)
    using a JDBC or ODBC connection and traditional RDBMS JDBC/ODBC connectivity built
    in. In the previous chapters, you have seen that Spark SQL can be used using notebooks
    and intermixed with PySpark, Scala, Java, or R applications. However, Apache Spark
    can also double up as a powerful and fast distributed SQL engine using a JDBC/OCBC
    connection or via the command line.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 的一个常见应用是与 BI 和 SQL 分析工具的结合。这些基于 SQL 的工具通过 JDBC 或 ODBC 连接以及内置的传统 RDBMS JDBC/ODBC
    连接来连接到 **关系型数据库管理系统** (**RDBMS**)。在前面的章节中，你已经看到，Spark SQL 可以通过笔记本使用，并与 PySpark、Scala、Java
    或 R 应用程序混合使用。然而，Apache Spark 也可以作为一个强大且快速的分布式 SQL 引擎，通过 JDBC/ODBC 连接或命令行使用。
- en: Note
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: JDBC is a SQL-based **application programming interface** (**API**) used by
    Java applications to connect to an RDBMS. Similarly, ODBC is a SQL-based API created
    by Microsoft to provide RDBMS access to Windows-based applications. A JDBC/ODBC
    driver is a client-side software component either developed by the RDBMS vendor
    themselves or by a third party that can be used with external tools to connect
    to an RDBMS via the JDBC/ODBC standard.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: JDBC 是一个基于 SQL 的 **应用程序编程接口** (**API**)，Java 应用程序通过它连接到关系型数据库管理系统 (RDBMS)。类似地，ODBC
    是由 Microsoft 创建的一个 SQL 基础的 API，用于为基于 Windows 的应用程序提供 RDBMS 访问。JDBC/ODBC 驱动程序是一个客户端软件组件，由
    RDBMS 厂商自己开发或由第三方开发，可与外部工具一起使用，通过 JDBC/ODBC 标准连接到 RDBMS。
- en: In the following section, we will explore how to make use of **JDBC/ODBC** serving
    capabilities with Apache Spark.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨如何利用 Apache Spark 的 **JDBC/ODBC** 服务功能。
- en: Introduction to Hive Thrift JDBC/ODBC Server
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hive Thrift JDBC/ODBC 服务器介绍
- en: While the JDBC/ODBC driver gives client-side software such as BI or SQL analytics
    tools the ability to connect to database servers, some server-side components
    are also required for the database server to utilize JDBC/ODBC standards. Most
    RDBMSes come built with these JDBC/ODBC serving capabilities, and Apache Spark
    can also be enabled with this server-side functionality using a Thrift JDBC/ODBC
    server.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 JDBC/ODBC 驱动程序赋予 BI 或 SQL 分析工具等客户端软件连接数据库服务器的能力，但数据库服务器也需要一些服务器端组件才能利用 JDBC/ODBC
    标准。大多数 RDBMS 都内置了这些 JDBC/ODBC 服务功能，Apache Spark 也可以通过 Thrift JDBC/ODBC 服务器启用此服务器端功能。
- en: '**HiveServer2** is a server-side interface developed to enable Hadoop Hive
    clients to execute Hive queries against Apache Hive. HiveServer2 has been developed
    to provide multi-client concurrency with open APIs such as JDBC and ODBC. HiveServer2
    itself is based on Apache Thrift, which is a binary communication protocol used
    for creating services in multiple programming languages. Spark Thrift Server is
    Apache Spark''s implementation of HiveServer2 that allows JDBC/ODBC clients to
    execute Spark SQL queries on Apache Spark.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**HiveServer2** 是一个服务器端接口，旨在使 Hadoop Hive 客户端能够执行针对 Apache Hive 的 Hive 查询。HiveServer2
    已经开发出来，旨在通过 JDBC 和 ODBC 等开放的 API 提供多客户端并发能力。HiveServer2 本身基于 Apache Thrift，这是一种用于在多种编程语言中创建服务的二进制通信协议。Spark
    Thrift Server 是 Apache Spark 对 HiveServer2 的实现，允许 JDBC/ODBC 客户端在 Apache Spark
    上执行 Spark SQL 查询。'
- en: 'Spark Thrift Server comes bundled with the Apache Spark distribution, and most
    Apache Spark vendors enable this service by default on their Spark clusters. In
    the case of Databricks, this service can be accessed from the **Clusters** page,
    as shown in the following screenshot:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Thrift Server 随 Apache Spark 发行版一起捆绑，且大多数 Apache Spark 厂商默认在他们的 Spark
    集群上启用此服务。以 Databricks 为例，可以通过 **集群** 页面访问此服务，如下图所示：
- en: '![Figure 13.1 – Databricks JDBC/ODBC interface'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.1 – Databricks JDBC/ODBC 接口](img/B16736_13_01.jpg)'
- en: '](img/B16736_13_01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_13_01.jpg)'
- en: Figure 13.1 – Databricks JDBC/ODBC interface
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 – Databricks JDBC/ODBC 接口
- en: You can get to the Databricks JDBC/ODBC interface shown in the previous screenshot
    by navigating to the **Clusters** page within the Databricks web interface. Then,
    click on **Advanced Options** and then on the **JDBC/ODBC** tab. The Databricks
    JDBC/ODBC interface provides you with the hostname, port, protocol, the HTTP path,
    and the actual JDBC URL required by external tools for connectivity to the Databricks
    Spark cluster. In the following sections, we will explore how this Spark Thrift
    Server functionality can be used by external SQL-based clients to utilize Apache
    Spark as a distributed SQL engine.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过导航到Databricks Web界面内的**Clusters**页面，然后点击**高级选项**，再点击**JDBC/ODBC**选项卡，来访问前一屏幕截图中显示的Databricks
    JDBC/ODBC接口。Databricks JDBC/ODBC接口提供了外部工具连接Databricks Spark集群所需的主机名、端口、协议、HTTP路径和实际的JDBC
    URL。在接下来的部分中，我们将探讨如何使用Spark Thrift Server功能，使外部基于SQL的客户端能够利用Apache Spark作为分布式SQL引擎。
- en: Spark connectivity to SQL analysis tools
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark与SQL分析工具的连接
- en: 'SQL analysis tools, as the same suggests, are tools with interfaces suited
    for quick and easy SQL analysis. They let you connect to an RDBMS, sometimes even
    multiple RDBMSes, at the same time, and browse through various databases, schemas,
    tables, and columns. They even help you visually analyze tables and their structure.
    They also have interfaces designed to perform SQL analysis quickly with multiple
    windows that let you browse tables and columns on one side, compose a SQL query
    in another window, and look at the results in another window. Once such SQL analysis
    tool, called **SQL Workbench/J**, is shown in the following screenshot:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: SQL分析工具，如其名称所示，是专为快速和简便SQL分析而设计的工具。它们允许您连接到一个或多个关系数据库管理系统（RDBMS），浏览各种数据库、模式、表和列。它们甚至帮助您可视化分析表及其结构。它们还具有设计用于快速SQL分析的界面，具有多个窗口，让您可以在一个窗口浏览表和列，在另一个窗口中编写SQL查询，并在另一个窗口中查看结果。其中一种这样的SQL分析工具，称为**SQL
    Workbench/J**，如下屏幕截图所示：
- en: '![Figure 13.2 – SQL Workbench/J interface'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.2 – SQL Workbench/J 界面'
- en: '](img/B16736_13_02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_13_02.jpg)'
- en: Figure 13.2 – SQL Workbench/J interface
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 – SQL Workbench/J 界面
- en: The previous screenshot depicts the interface of **SQL Workbench/J**, which
    represents a typical SQL editor interface with a database, schema, table, and
    column browser on the left-hand side pane. The top pane has a text interface for
    composing actual SQL queries, and the bottom pane shows the results of executed
    SQL queries and has other tabs to show any error messages, and so on. There is
    also a menu and a toolbar on the top to establish connections to databases, toggle
    between various databases, execute SQL queries, browse databases, save SQL queries,
    browse through query history, and so on. This type of SQL analysis interface is
    very intuitive and comes in very handy for quick SQL analysis, as well as for
    building SQL-based data processing jobs, as databases, tables, and columns can
    easily be browsed. Tables and column names can be easily dragged and dropped into
    the **Query Composer** window, and results can be quickly viewed and analyzed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的屏幕截图展示了**SQL Workbench/J**的界面，它代表了一个典型的SQL编辑器界面，左侧窗格有数据库、模式、表和列浏览器。顶部窗格有一个用于编写实际SQL查询的文本界面，底部窗格显示已执行SQL查询的结果，并有其他选项卡显示任何错误消息等等。顶部还有一个菜单和工具栏，用于建立数据库连接、在各个数据库之间切换、执行SQL查询、浏览数据库、保存SQL查询、浏览查询历史记录等等。这种类型的SQL分析界面非常直观，非常适合快速SQL分析，以及构建基于SQL的数据处理作业，因为可以轻松浏览数据库、表和列。可以轻松地将表和列名拖放到**查询组合器**窗口中，快速查看和分析结果。
- en: There are, additionally, rather more sophisticated SQL analysis tools that also
    let you visualize the results of a query right within the same interface. Some
    open source tools to name are **Redash**, **Metabase**, and **Apache Superset**,
    and some cloud-native tools are **Google Data Studio**, **Amazon QuickSight**,
    and so on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些更复杂的SQL分析工具，可以让你在同一个界面内可视化查询结果。一些开源工具如**Redash**、**Metabase**和**Apache
    Superset**，以及一些云原生工具如**Google Data Studio**、**Amazon QuickSight**等。
- en: Tip
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Redash was recently acquired by Databricks and is available to use with the
    Databricks paid versions; it is not available in Databricks Community Edition
    as of this writing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Redash最近被Databricks收购，并可在Databricks付费版本中使用；截至目前，它在Databricks社区版中不可用。
- en: Now you have an idea of what SQL analysis tools look like and how they work,
    let's look at the steps required to connect a SQL analysis tool such as **SQL
    Workbench/J** to **Databricks Community Edition**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经了解了 SQL 分析工具的外观和工作原理，接下来让我们来看一下将 SQL 分析工具（如 **SQL Workbench/J**）连接到 **Databricks
    Community Edition** 所需的步骤。
- en: '**SQL Workbench/J** is a free, RDBMS-independent SQL analysis tool based on
    Java and can be used with any operating system of your choice. Instructions on
    downloading and running **SQL Workbench/J** can be found here: [https://www.sql-workbench.eu/downloads.html](https://www.sql-workbench.eu/downloads.html).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**SQL Workbench/J** 是一个免费的、独立于 RDBMS 的 SQL 分析工具，基于 Java，可以与任何您选择的操作系统一起使用。有关下载和运行
    **SQL Workbench/J** 的说明，请参阅此处：[https://www.sql-workbench.eu/downloads.html](https://www.sql-workbench.eu/downloads.html)。'
- en: 'Once you have **SQL Workbench/J** set up and running on your local machine,
    the following steps will help you get it connected with **Databricks Community
    Edition**:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您在本地机器上成功设置并运行 **SQL Workbench/J**，接下来的步骤将帮助您将其与 **Databricks Community Edition**
    连接：
- en: Download the Databricks JDBC driver from [https://databricks.com/spark/jdbc-drivers-download](https://databricks.com/spark/jdbc-drivers-download),
    and store at a known location.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 [https://databricks.com/spark/jdbc-drivers-download](https://databricks.com/spark/jdbc-drivers-download)
    下载 Databricks JDBC 驱动程序，并将其存储在已知位置。
- en: Launch **SQL Workbench/J** and open the **File** menu. Then, click on **Connect
    window**, to take you to the following screen:![Figure 13.3 – SQL Workbench/J
    connect window
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 **SQL Workbench/J** 并打开 **文件** 菜单。然后，点击 **连接窗口**，以进入以下屏幕：![图 13.3 – SQL Workbench/J
    连接窗口](img/B16736_13_03.jpg)
- en: '](img/B16736_13_03.jpg)'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16736_13_03.jpg)'
- en: Figure 13.3 – SQL Workbench/J connect window
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.3 – SQL Workbench/J 连接窗口
- en: In the previous window, click on **Manage Drivers** to take you to the following
    screen:![Figure 13.4 – Manage drivers screen
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的窗口中，点击 **管理驱动程序**，以进入以下屏幕：![图 13.4 – 管理驱动程序屏幕](img/B16736_13_04.jpg)
- en: '](img/B16736_13_04.jpg)'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16736_13_04.jpg)'
- en: Figure 13.4 – Manage drivers screen
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.4 – 管理驱动程序屏幕
- en: As shown in the preceding **Manage drivers** window screenshot, click on the
    folder icon and navigate to the folder where you stored your previously downloaded
    Databricks drivers and open it, then click the **OK** button.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述的 **管理驱动程序** 窗口截图所示，点击文件夹图标，导航到您存储已下载 Databricks 驱动程序的文件夹并打开它，然后点击 **OK**
    按钮。
- en: Now, navigate to your Databricks `UID` and `PWD` parts and paste it into the
    **URL** field on the connection window of **SQL Workbench/J**, as shown in the
    following screenshot:![Figure 13.6 – SQL Workbench/J connection parameters
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，导航到您的 Databricks `UID` 和 `PWD` 部分，并将其粘贴到 **URL** 字段中，位于 **SQL Workbench/J**
    连接窗口，如下所示截图所示：![图 13.6 – SQL Workbench/J 连接参数](img/B16736_13_06.jpg)
- en: '](img/B16736_13_06.jpg)'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16736_13_06.jpg)'
- en: Figure 13.6 – SQL Workbench/J connection parameters
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.6 – SQL Workbench/J 连接参数
- en: After entering the required JDBC parameters from the Databricks **Clusters**
    page, enter your Databricks username and password in the **Username** and **Password**
    fields on the **SQL Workbench/J** connection window. Then, click on the **Test**
    button to test connectivity to the Databricks clusters. If all the connection
    parameters have been correctly provided, you should see a **Connection Successful**
    message pop up.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在输入来自 Databricks **集群** 页面所需的 JDBC 参数后，在 **SQL Workbench/J** 连接窗口中的 **用户名**
    和 **密码** 字段中输入您的 Databricks 用户名和密码。然后，点击 **测试** 按钮以测试与 Databricks 集群的连接。如果所有连接参数已正确提供，您应该会看到一个
    **连接成功** 的消息弹出。
- en: Tip
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示
- en: Make sure the Databricks cluster is up and running if you see any connection
    failures or **Host Not Found** types of errors.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您看到任何连接失败或 **Host Not Found** 类型的错误，请确保 Databricks 集群已启动并正在运行。
- en: This way, by following the previous steps, you can successfully connect a SQL
    analysis tool such as **SQL Workbench/J** to Databricks clusters and run Spark
    SQL queries remotely. It is also possible to connect to other Spark clusters running
    on other vendors' clusters—just make sure to procure the appropriate **HiveServer2**
    drivers directly from the vendor. Modern BI tools also recognize the importance
    of connecting to big data technologies and data lakes, and in the following section,
    we will explore how to connect BI tools with Apache Spark via a JDBC connection.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循之前的步骤，您可以成功地将 SQL 分析工具（如 **SQL Workbench/J**）连接到 Databricks 集群，并远程运行 Spark
    SQL 查询。也可以连接到运行在其他供应商集群上的其他 Spark 集群——只需确保直接从供应商处获取适当的 **HiveServer2** 驱动程序。现代
    BI 工具也认识到连接大数据技术和数据湖的重要性，在接下来的部分中，我们将探讨如何通过 JDBC 连接将 BI 工具与 Apache Spark 连接。
- en: Spark connectivity to BI tools
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 连接到 BI 工具
- en: In the era of big data and **artificial intelligence** (**AI**), Hadoop and
    Spark have modernized data warehouses into distributed warehouses that can process
    up to **petabytes** (**PB**) of data. Thus, BI tools have also evolved to utilize
    Hadoop- and Spark-based analytical stores as their data sources, connecting to
    them using JDBC/ODBC. BI tools ranging from Tableau, Looker, Sisense, MicroStrategy,
    Domo, and so on all feature connectivity support and built-in drivers to Apache
    Hive and Spark SQL. In this section, we will explore how you can connect a BI
    tool such as Tableau Online with Databricks Community Edition, via a JDBC connection.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据和**人工智能**（**AI**）时代，Hadoop 和 Spark 将数据仓库现代化为分布式仓库，能够处理高达**拍字节**（**PB**）的数据。因此，BI
    工具也已发展为利用基于 Hadoop 和 Spark 的分析存储作为其数据源，并通过 JDBC/ODBC 连接到这些存储。包括 Tableau、Looker、Sisense、MicroStrategy、Domo
    等在内的 BI 工具都支持与 Apache Hive 和 Spark SQL 的连接，并内置了相应的驱动程序。在本节中，我们将探讨如何通过 JDBC 连接将
    BI 工具，如 Tableau Online，连接到 Databricks Community Edition。
- en: '**Tableau Online** is a BI platform fully hosted in the cloud that lets you
    perform data analytics, publish reports and dashboards, and create interactive
    visualizations, all from a web browser. The following steps describe the process
    of connecting Tableau Online with Databricks Community Edition:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tableau Online** 是一个完全托管在云中的 BI 平台，允许你进行数据分析、发布报告和仪表盘，并创建交互式可视化，所有操作都可以通过网页浏览器完成。以下步骤描述了将
    Tableau Online 与 Databricks Community Edition 连接的过程：'
- en: 'If you already have an existing Tableau Online account, sign in. If not, you
    can request a free trial here: [https://www.tableau.com/products/online/request-trial](https://www.tableau.com/products/online/request-trial).'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你已经拥有一个现有的 Tableau Online 账户，请登录。如果没有，你可以在此处请求免费试用：[https://www.tableau.com/products/online/request-trial](https://www.tableau.com/products/online/request-trial)。
- en: Once you have logged in, click on the **New** button near the top right-hand
    corner, as shown in the following screenshot:![Figure 13.7 – Tableau Online new
    workbook
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，点击右上角的**新建**按钮，如以下截图所示：![图 13.7 – Tableau Online 新工作簿
- en: '](img/B16736_13_07.jpg)'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16736_13_07.jpg)'
- en: Figure 13.7 – Tableau Online new workbook
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.7 – Tableau Online 新工作簿
- en: The newly created workbook will prompt you to **Connect to Data**. Click on
    the **Connectors** tab and choose **Databricks** from the list of available data
    sources, as shown in the following screenshot:![Figure 13.8 – Tableau Online data
    sources
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新创建的工作簿会提示你**连接到数据**。点击**连接器**标签，并从可用数据源列表中选择**Databricks**，如以下截图所示：![图 13.8
    – Tableau Online 数据源
- en: '](img/B16736_13_08.jpg)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16736_13_08.jpg)'
- en: Figure 13.8 – Tableau Online data sources
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.8 – Tableau Online 数据源
- en: Then, provide Databricks cluster details such as **Server Hostname**, **HTTP
    Path**, **Authentication**, **Username**, and **Password**, as shown in the following
    screenshot, and click the **Sign In** button. These details are found on the Databricks
    **Clusters** page:![Figure 13.9 – Tableau Online Databricks connection
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，提供 Databricks 集群的详细信息，如**服务器主机名**、**HTTP 路径**、**身份验证**、**用户名**和**密码**，如以下截图所示，并点击**登录**按钮。这些详细信息可以在
    Databricks **集群**页面找到：![图 13.9 – Tableau Online Databricks 连接
- en: '](img/B16736_13_09.jpg)'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16736_13_09.jpg)'
- en: Figure 13.9 – Tableau Online Databricks connection
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.9 – Tableau Online Databricks 连接
- en: Once your connection is successful, your new workbook will open in the **Data
    Source** tab, where you can browse through your existing databases and tables,
    as shown in the following screenshot:![Figure 13.10 – Tableau Online data sources
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦连接成功，你的新工作簿将在**数据源**标签页中打开，你可以浏览现有的数据库和表格，如以下截图所示：![图 13.10 – Tableau Online
    数据源
- en: '](img/B16736_13_10.jpg)'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16736_13_10.jpg)'
- en: Figure 13.10 – Tableau Online data sources
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.10 – Tableau Online 数据源
- en: 'The **Data Source** tab also lets you drag and drop tables and define relationships
    and joins among tables as well, as shown in the following screenshot:![Figure
    13.11 – Tableau Online: defining table joins'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据源**标签页还允许你拖放表格并定义表之间的关系和连接，如以下截图所示：![图 13.11 – Tableau Online：定义表连接'
- en: '](img/B16736_13_11.jpg)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16736_13_11.jpg)'
- en: 'Figure 13.11 – Tableau Online: defining table joins'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.11 – Tableau Online：定义表连接
- en: Tableau Online data sources also let you create two types of connections with
    the underlying data sources—a live connection that queries the underlying data
    sources with every request and an option to create an **Extract**, which extracts
    data from the data sources and stores it within Tableau. With big data sources
    such as Apache Spark, it is recommended to create a live connection because the
    amount of data being queried could be substantially larger than usual.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tableau Online数据源还允许你与基础数据源创建两种类型的连接——一种是实时连接，它会在每次请求时查询基础数据源，另一种是创建**提取**，它从数据源中提取数据并将其存储在Tableau中。对于像Apache
    Spark这样的庞大数据源，建议创建实时连接，因为查询的数据量可能远大于平常。
- en: Sample data can also be browsed in the **Data Source** tab by clicking on the
    **Update Now** button, as shown in the following screenshot:![Figure 13.12 – Tableau
    Online data source preview
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还可以通过点击**更新现在**按钮在**数据源**标签中浏览示例数据，如下图所示：![图13.12 – Tableau Online数据源预览
- en: '](img/B16736_13_12.jpg)'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16736_13_12.jpg)'
- en: Figure 13.12 – Tableau Online data source preview
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.12 – Tableau Online数据源预览
- en: Once the data source connection is established, you can move on to visualizing
    the data and creating reports and dashboards by clicking on `Sheet1` or by creating
    new additional sheets.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据源连接建立，你可以通过点击`Sheet1`或创建新的附加工作表来继续可视化数据并创建报告和仪表板。
- en: Once you are within a sheet, you can start visualizing data by dragging and
    dropping columns from the **Data** pane onto the blank sheet.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦进入工作表，你可以通过将**数据**窗格中的列拖放到空白工作表上来开始可视化数据。
- en: 'Tableau automatically chooses the appropriate visualization based on the fields
    selected. The visualization can also be changed using the **Visualization** drop-down
    selector, and data filters can be defined using the **Filters** box. Aggregations
    can be defined on metrics in the **Columns** field, and columns can also be defined
    as **Dimension**, **Attribute**, or **Metric** as required. The top menu has additional
    settings to sort and pivot data and has other formatting options. There are also
    advanced analytics options such as defining quartiles, medians, and so on available
    within Tableau Online. This way, using Tableau Online with its built-in Databricks
    connector data can be analyzed at scale using the power and efficiency of Apache
    Spark, along with the ease of use and **graphical user interface** (**GUI**) of
    a prominent BI tool such as Tableau Online, as shown in the following screenshot:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tableau会根据选择的字段自动选择合适的可视化方式。可视化方式也可以通过**可视化**下拉选择器进行更改，数据过滤器可以通过**过滤器**框定义。在**列**字段中可以定义度量的聚合，还可以根据需要将列定义为**维度**、**属性**或**度量**。顶部菜单还提供了排序和透视数据的其他设置，并有其他格式化选项。Tableau
    Online还提供了高级分析选项，如定义四分位数、中位数等。通过这种方式，利用Tableau Online内置的Databricks连接器，数据可以在Apache
    Spark的强大和高效的支持下进行大规模分析，同时还可以享受像Tableau Online这样显著BI工具的**图形用户界面**（**GUI**）的易用性，截图如下所示：
- en: '![Figure 13.13 – Tableau Online worksheet with visualizations'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.13 – Tableau Online工作表与可视化'
- en: '](img/B16736_13_13.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_13_13.jpg)'
- en: Figure 13.13 – Tableau Online worksheet with visualizations
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.13 – Tableau Online工作表与可视化
- en: '**Tableau Online** is one of the many popular BI tools that support native
    Databricks connectivity out of the box. Modern BI tools also offer Spark SQL connectivity
    options for connecting to Apache Spark distributions outside of Databricks.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tableau Online**是许多流行的BI工具之一，支持开箱即用的原生Databricks连接。现代BI工具还提供了Spark SQL连接选项，用于连接Databricks之外的Apache
    Spark发行版。'
- en: Connectivity to Apache Spark is not just limited to SQL analysis and BI tools.
    Since the JDBC protocol is based on Java and was meant to be used by Java-based
    applications, any applications built using **Java Virtual Machine** (**JVM**)-based
    programming languages such as **Java** or **Scala** can also make use of the JDBC
    connectivity options to Apache Spark.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 连接Apache Spark不仅限于SQL分析和BI工具。由于JDBC协议基于Java，并且是为了被基于Java的应用程序使用而设计的，因此任何使用**Java虚拟机**（**JVM**）基础编程语言（如**Java**或**Scala**）构建的应用程序也可以使用JDBC连接选项连接Apache
    Spark。
- en: But what about applications based on popular programming languages such as Python
    that are not based on Java? There is a way to connect these types of Python applications
    to Apache Spark via **Pyodbc**, and we will explore this in the following section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，像Python这样流行的编程语言构建的应用程序怎么办？这些类型的Python应用程序可以通过**Pyodbc**连接到Apache Spark，我们将在接下来的部分中探讨这一点。
- en: Connecting Python applications to Spark SQL using Pyodbc
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Python应用程序连接到Spark SQL，使用Pyodbc
- en: '**Pyodbc** is an open source Python module for connecting Python applications
    to data sources using an ODBC connection. Pyodbc can be used with any of your
    local Python applications to connect to Apache Spark via an ODBC driver and access
    databases and tables defined with Apache Spark SQL. In this section, we will explore
    how you can connect Python running on your local machine to a Databricks cluster
    using **Pyodbc** with the following steps:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pyodbc**是一个开源的Python模块，用于通过ODBC连接将Python应用程序连接到数据源。Pyodbc可以与任何本地Python应用程序一起使用，通过ODBC驱动程序连接到Apache
    Spark，并访问使用Apache Spark SQL定义的数据库和表格。在本节中，我们将探讨如何通过以下步骤使用**Pyodbc**将运行在本地机器上的Python连接到Databricks集群：'
- en: 'Download and install the Simba ODBC driver provided by Databricks on your local
    machine from here: [https://databricks.com/spark/odbc-drivers-download](https://databricks.com/spark/odbc-drivers-download).'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里下载并安装Databricks提供的Simba ODBC驱动程序到本地机器：[https://databricks.com/spark/odbc-drivers-download](https://databricks.com/spark/odbc-drivers-download)。
- en: 'Install Pyodbc on your local machine''s Python using `pip`, as shown in the
    following command:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pip`在本地机器的Python中安装Pyodbc，如下所示的命令：
- en: '[PRE0]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create a new Python file using a text editor of your choice and paste the following
    code into it:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您选择的文本编辑器创建一个新的Python文件，并将以下代码粘贴到其中：
- en: '[PRE1]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The driver paths for the previous code configuration may vary based on your
    operating systems and are given as follows:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前述代码配置的驱动路径可能会根据您的操作系统有所不同，具体如下：
- en: '[PRE2]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The values for hostname, port, and the HTTP path can be obtained from your Databricks
    cluster **JDBC/ODBC** tab.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主机名、端口和HTTP路径的值可以从Databricks集群的**JDBC/ODBC**标签页获取。
- en: Once you have added all the code and put in the appropriate configuration values,
    save the Python code file and name it `pyodbc-databricks.py`.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您添加了所有代码并输入了适当的配置值，保存Python代码文件并命名为`pyodbc-databricks.py`。
- en: 'You can now execute the code from your Python interpreter using the following
    command:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以使用以下命令在Python解释器中执行代码：
- en: '[PRE3]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once the code runs successfully, the first five rows of the table you specified
    in your SQL query will be displayed on the console.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦代码成功运行，您在SQL查询中指定的表格的前五行将显示在控制台上。
- en: Note
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'Instructions on configuring the ODBC driver on a Microsoft Windows machine
    to be used with Pyodbc can be found on the Databricks public documentation page
    here: [https://docs.databricks.com/dev-tools/pyodbc.html#windows](https://docs.databricks.com/dev-tools/pyodbc.html#windows).'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配置Windows机器上的ODBC驱动程序以供Pyodbc使用的说明，请参考Databricks公开文档页面：[https://docs.databricks.com/dev-tools/pyodbc.html#windows](https://docs.databricks.com/dev-tools/pyodbc.html#windows)。
- en: This way, using **Pyodbc**, you can integrate Apache Spark SQL into any of your
    Python applications that may be running locally on your machine or some remote
    machine in the cloud or a data center somewhere, but still take advantage of the
    fast and powerful distributed SQL engine that comes with Apache Spark.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，使用**Pyodbc**，您可以将Apache Spark SQL集成到任何运行在您本地机器或云端、数据中心等远程机器上的Python应用程序中，同时仍能利用Apache
    Spark自带的快速强大的分布式SQL引擎。
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you have explored how you can take advantage of Apache Spark's
    Thrift server to enable JDBC/ODBC connectivity and use Apache Spark as a distributed
    SQL engine. You learned how the HiveServer2 service allows external tools to connect
    to Apache Hive using JDBC/ODBC standards and how Spark Thrift Server extends HiveServer2
    to enable similar functionality on Apache Spark clusters. Steps required for connecting
    SQL analysis tools such as SQL Workbench/J were presented in this chapter, along
    with detailed instructions required for connecting BI tools such as Tableau Online
    with Spark clusters. Finally, steps required for connecting arbitrary Python applications,
    either locally on your machine or on remote servers in the cloud or a data center,
    to Spark clusters using Pyodbc were also presented. In the following and final
    chapter of this book, we will explore the Lakehouse paradigm that can help organizations
    seamlessly cater to all three workloads of data analytics—data engineering, data
    science, and SQL analysis—using a single unified distributed and persistent storage
    layer that combines the best features of both data warehouses and data lakes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经探索了如何利用 Apache Spark 的 Thrift 服务器启用 JDBC/ODBC 连接，并将 Apache Spark 用作分布式
    SQL 引擎。你学习了 HiveServer2 服务如何允许外部工具使用 JDBC/ODBC 标准连接到 Apache Hive，以及 Spark Thrift
    服务器如何扩展 HiveServer2，从而在 Apache Spark 集群上实现类似的功能。本章还介绍了连接 SQL 分析工具（如 SQL Workbench/J）所需的步骤，并详细说明了如何将
    Tableau Online 等 BI 工具与 Spark 集群连接。最后，还介绍了如何使用 Pyodbc 将任意 Python 应用程序（无论是本地运行在你的机器上，还是远程部署在云端或数据中心）连接到
    Spark 集群的步骤。在本书的下一章也是最后一章中，我们将探索 Lakehouse 模式，这一模式可以帮助组织通过一个统一的分布式和持久化存储层，结合数据仓库和数据湖的最佳特性，轻松应对数据分析的三种工作负载——数据工程、数据科学和
    SQL 分析。
