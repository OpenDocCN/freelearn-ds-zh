- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Data Transformation – Merging and Concatenating
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换 – 合并和拼接
- en: Understanding how to transform and manipulate data is crucial for unlocking
    valuable insights. Techniques such as joining, merging, and appending allow us
    to blend information from various sources and organize and analyze subsets of
    data. In this chapter, we’ll learn how to merge multiple datasets into a single
    dataset and explore the various techniques that we can use. We’ll understand how
    to avoid duplicate values while merging datasets and some tricks to improve the
    process of merging datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 理解如何转换和处理数据对于挖掘有价值的洞察至关重要。技术如连接、合并和附加使我们能够将来自不同来源的信息融合在一起，并组织和分析数据的子集。在本章中，我们将学习如何将多个数据集合并成一个单一的数据集，并探索可以使用的各种技术。我们将理解如何在合并数据集时避免重复值，并学习一些提升数据合并过程的技巧。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Joining datasets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接数据集
- en: Handling duplicates when merging datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理数据合并中的重复项
- en: Performance tricks for merging
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并时的性能优化技巧
- en: Concatenating DataFrames
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拼接 DataFrame
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find all the code for the chapter at the following link: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter05](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter05).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下链接找到本章的所有代码：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter05](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter05)。
- en: Each section is followed by a script with a similar naming convention, so feel
    free to execute the scripts and/or follow along by reading the chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每个章节后面都有一个具有类似命名约定的脚本，欢迎你执行脚本或通过阅读本章来跟随学习。
- en: Joining datasets
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接数据集
- en: In data analysis projects, it is common to encounter data that is spread across
    multiple sources or datasets. Each dataset may contain different pieces of information
    related to a common entity or subject. **Data merging**, also known as data joining
    or data concatenation, is the process of combining these separate datasets into
    a single cohesive dataset. In data analysis projects, it’s common to encounter
    situations where information about a particular subject or entity is spread across
    multiple datasets. For instance, imagine you’re analyzing customer data for a
    retail business. You might have one dataset containing customer demographics,
    such as names, ages, and addresses, and another dataset with their purchase history,
    such as transaction dates, items bought, and total spending. Each of these datasets
    provides valuable insights but, individually, they don’t give a complete picture
    of customer behavior. To gain a comprehensive understanding, you need to combine
    these datasets. By merging the customer demographics with their purchase history
    based on a common identifier, such as a customer ID, you create a single dataset
    that allows for richer analysis. For example, you could identify patterns such
    as which age groups are buying specific products or how spending habits vary by
    location.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析项目中，常常会遇到数据分散在多个来源或数据集中的情况。每个数据集可能包含与某个共同实体或主题相关的不同信息片段。**数据合并**，也称为数据连接或数据拼接，是将这些独立的数据集合并成一个统一的数据集的过程。在数据分析项目中，常常会遇到某个特定主题或实体的信息分布在多个数据集中的情况。例如，假设你正在为一个零售企业分析客户数据。你可能有一个数据集包含客户的人口统计信息，如姓名、年龄和地址，另一个数据集包含他们的购买历史，如交易日期、购买的商品和总支出。每个数据集都提供了有价值的见解，但单独来看，它们无法完整展现客户的行为。为了获得全面的理解，你需要将这些数据集合并。通过根据一个共同的标识符（如客户
    ID）将客户的人口统计信息与购买历史合并，你可以创建一个单一的数据集，从而进行更丰富的分析。例如，你可以识别出哪些年龄组购买了特定的产品，或支出习惯如何因地域而异。
- en: Choosing the correct merge strategy
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择正确的合并策略
- en: Choosing the correct join type is crucial as it determines which rows from the
    input DataFrames are included in the joined output. Python’s pandas library provides
    several join types, each with different behaviors. Let’s introduce the use case
    example we are going to work on in this chapter and then expand on the different
    types of joins.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的连接类型至关重要，因为它决定了输入 DataFrame 中哪些行会被包含在连接后的输出中。Python 的 pandas 库提供了几种连接类型，每种类型具有不同的行为。我们将介绍本章将要使用的用例示例，然后深入探讨不同类型的连接。
- en: 'In this chapter, our use case involves employee data and project assignments
    for a company managing its workforce and projects. You can execute the following
    script to see the DataFrames in more detail: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/1.use_case.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/1.use_case.py).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的使用案例涉及员工数据和项目分配，适用于一个管理其员工和项目的公司。你可以执行以下脚本，详细查看数据框：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/1.use_case.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/1.use_case.py)。
- en: 'The `employee_data` DataFrame represents employee details, such as their names
    and departments, as presented here:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`employee_data` 数据框表示员工的详细信息，例如他们的姓名和部门，内容如下所示：'
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `project_data` DataFrame contains information about project assignments,
    including the project names:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`project_data` 数据框包含项目分配的信息，包括项目名称：'
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the following sections, we will discuss the different DataFrame merging options,
    starting with the inner join.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论不同的数据框合并选项，从内连接开始。
- en: Inner merge
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内连接
- en: 'The inner merge returns only the rows that have matching values in both DataFrames
    for the specified join columns. It’s very important to note the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 内连接只返回在指定的连接列中，两个数据框中都有匹配值的行。需要特别注意以下几点：
- en: Rows with non-matching keys in either DataFrame will be excluded from the merged
    output
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何一个数据框中具有不匹配键的行将会被排除在合并结果之外
- en: Rows with missing values in the key columns will be excluded from the merged
    result
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有缺失值的键列中的行将被排除在合并结果之外
- en: 'The result of an inner merge is presented in the following figure:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 内连接的结果展示在下图中：
- en: '![Figure 5.1 – Inner merge](img/B19801_05_1.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 内连接](img/B19801_05_1.jpg)'
- en: Figure 5.1 – Inner merge
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 内连接
- en: 'Let’s have a look at how we can achieve the preceding result using the pandas
    `merge` function, using the example presented in the previous section:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何使用 pandas `merge` 函数实现上述结果，参考前一节中的示例：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As we see in the preceding code snippet, the `pd.merge()` function is used
    to merge the two DataFrames. The `on=''employee_id''` argument specifies that
    the `employee_id` column should be used as the key on which to join the DataFrames.
    The `how=''inner''` argument specifies that an inner join should be performed.
    This type of join returns only the rows that have matching values in both DataFrames,
    which, in this case, are the rows where `employee_id` matches in both `employee_data`
    and `project_data`. In the following table, you can see the output of the inner
    join of the two DataFrames:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的代码片段中看到的，`pd.merge()` 函数用于合并两个数据框。`on='employee_id'` 参数指定应使用 `employee_id`
    列作为连接数据框的键。`how='inner'` 参数指定执行内连接。这种连接类型只返回在两个数据框中都有匹配值的行，在本例中就是 `employee_id`
    在 `employee_data` 和 `project_data` 中匹配的行。以下表格展示了两个数据框内连接的结果：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This approach ensures that the data from both DataFrames is combined based on
    a **common key**, with rows included only when there is a match across both DataFrames,
    adhering to the principles of an inner join.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法确保了两个数据框的数据是基于**公共键**合并的，只有在两个数据框中都有匹配时，才会包含对应的行，从而遵循内连接的原则。
- en: 'If this is still not clear, in the following list, we present specific examples
    from the data world where an inner merge is crucial:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仍然不清楚，以下列表展示了在数据世界中，内连接至关重要的具体示例：
- en: '**Match tables**: Inner joins are ideal when you need to match data from different
    tables. For example, if you have a table of employees and another table of department
    names, you can use an inner join to match each employee with their respective
    department.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**匹配表格**：当你需要匹配来自不同表的数据时，内连接是理想的选择。例如，如果你有一个员工表和一个部门名称表，你可以使用内连接将每个员工与他们相应的部门匹配。'
- en: '**Data filtering**: Inner joins can act as a filter to exclude rows that do
    not have corresponding entries in both tables. This is useful in scenarios where
    you only want to consider records that have complete data across multiple tables.
    For instance, matching customer orders with product details only where both records
    exist.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据过滤**：内连接可以作为过滤器，排除那些在两个表中没有对应条目的行。这在你只希望考虑那些在多个表中有完整数据的记录时非常有用。例如，只有在客户订单和产品详情都有记录的情况下，才匹配这两者。'
- en: '**Efficiency in query execution**: Since inner joins only return rows with
    matching values in both tables, they can be more efficient in terms of query execution
    time compared to outer joins, which need to check for and handle non-matching
    entries as well.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询执行效率**：由于内部连接只返回两个表中具有匹配值的行，因此在查询执行时间方面可能比需要检查并处理非匹配条目的外部连接更有效。'
- en: '**Reducing data duplication**: Inner joins help in reducing data duplication
    by only returning matched rows, thus ensuring that the data in the result set
    is relevant and not redundant.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少数据重复**：内部连接通过仅返回匹配的行来帮助减少数据重复，从而确保结果集中的数据是相关的，而不是冗余的。'
- en: '**Simplifying complex queries**: When dealing with multiple tables, inner joins
    can be used to simplify queries by reducing the number of rows to be examined
    and processed in subsequent query operations. This is particularly useful in complex
    database schemas where multiple tables are interrelated.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化复杂查询**：在处理多个表格时，内部连接可用于通过减少需要检查和处理的行数来简化查询。这在复杂的数据库模式中特别有用，其中多个表格相互关联。'
- en: Moving from an inner join to an outer join expands the scope of the merged data,
    incorporating all available rows from both datasets, even if they don’t have corresponding
    matches.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从内部连接转向外部连接扩展了合并数据的范围，合并所有可用的两个数据集的行，即使它们之间没有对应的匹配项。
- en: Outer merge
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 外部合并
- en: The outer merge (also known as full outer join) returns all the rows from both
    DataFrames, combining the matching rows as well as the non-matching rows. The
    full outer join ensures that no data is lost from either DataFrame, but it can
    introduce NaN values where there are unmatched rows in either DataFrame.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 外部合并（也称为完全外部连接）返回两个数据帧的所有行，结合匹配的行以及不匹配的行。完全外部连接确保不会丢失来自任一数据帧的数据，但在其中一个数据帧中存在不匹配行时，可能会引入
    NaN 值。
- en: 'The result of an outer merge is presented in the following figure:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 外部合并的结果如下图所示：
- en: '![Figure 5.2 – Outer merge](img/B19801_05_2.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 外部合并](img/B19801_05_2.jpg)'
- en: Figure 5.2 – Outer merge
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 外部合并
- en: 'Let’s have a look at how we can achieve the preceding result using the pandas
    `merge` function, using the example presented in the previous section:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用 pandas 的 `merge` 函数来实现前述结果，在上一节中提供的示例中：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As we see in the preceding code snippet, the `pd.merge()` function is used
    to merge the two DataFrames. The `on=''employee_id''` argument specifies that
    the `employee_id` column should be used as the key on which to merge the DataFrames.
    The `how=''outer''` argument specifies that a full outer join should be performed.
    This type of join returns all rows from both DataFrames, filling in `NaN` where
    there is no match. In the following table, you can see the output of the outer
    join of the two DataFrames:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的代码片段中看到的那样，`pd.merge()` 函数用于合并这两个数据帧。参数 `on='employee_id'` 指定了应该使用 `employee_id`
    列作为合并数据帧的键。参数 `how='outer'` 指定执行完全外部连接。这种连接类型返回两个数据帧中的所有行，并在没有匹配项的地方填充 `NaN`。在以下表格中，您可以看到这两个数据帧进行外部连接的输出：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This approach ensures that the data from both DataFrames is combined, allowing
    for a comprehensive view of all available data, even if some of it is incomplete
    due to mismatches between the DataFrames.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法确保合并来自两个数据帧的数据，允许全面查看所有可用数据，即使由于数据帧之间的不匹配导致部分数据不完整。
- en: 'In the following list, we present specific examples from the data world where
    an outer merge is crucial:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们提供了数据领域中外部合并至关重要的具体示例：
- en: '**Including optional data**: Outer joins are ideal when you want to include
    rows that have optional data in another table. For instance, if you have a table
    of users and a separate table of addresses, not all users might have an address.
    An outer join allows you to list all users and show addresses for those who have
    them, without excluding users without addresses.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包含可选数据**：当您希望包含另一个表格中具有可选数据的行时，外部连接是理想的选择。例如，如果您有一个用户表和一个单独的地址表，不是所有用户都可能有地址。外部连接允许您列出所有用户，并显示那些有地址的用户的地址，而不排除没有地址的用户。'
- en: '**Data integrity and completeness**: In scenarios where you need a comprehensive
    dataset that includes records from both tables, regardless of whether there’s
    a matching record in the joined table or not, outer joins are essential. This
    ensures that you have a complete view of the data, which is particularly important
    in reports that need to show all entities, such as a report listing all customers
    and their purchases, including those who have not made any purchases.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据完整性和完整性**：在需要一个包含两张表中所有记录的全面数据集的场景中，无论是否在连接表中有匹配记录，外连接都是必不可少的。这确保了你能全面查看数据，特别是在需要展示所有实体的报告中，比如列出所有客户及其购买情况的报告，其中包括那些没有购买的客户。'
- en: '**Mismatched data analysis**: Outer joins can be used to identify discrepancies
    or mismatches between tables. For example, if you are comparing a list of registered
    users against a list of participants in an event, an outer join can help identify
    users who did not participate and participants who are not registered.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据不匹配分析**：外连接可以用来识别表之间的差异或不匹配。例如，如果你在比较注册用户列表与事件参与者列表，外连接可以帮助识别未参与的用户和未注册的参与者。'
- en: '**Complex data merging**: When merging data from multiple sources that do not
    perfectly align, outer joins can be used to ensure that no data is lost during
    the merging process. This is particularly useful in complex data environments
    where data integrity is critical.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂数据合并**：在合并来自多个来源的数据时，这些数据无法完美对齐，外连接可以确保在合并过程中没有数据丢失。这在数据完整性至关重要的复杂数据环境中尤为有用。'
- en: Transitioning from an outer join to a right join narrows the focus of the merged
    data, emphasizing the inclusion of all rows from the right DataFrame while maintaining
    matches from the left DataFrame.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从外连接过渡到右连接，缩小了合并数据的关注范围，强调包含右侧 DataFrame 中的所有行，同时保持左侧 DataFrame 中的匹配行。
- en: Right merge
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 右连接
- en: 'The right merge (also known as right outer join) returns all the rows from
    the right DataFrame and the matching rows from the left DataFrame. The result
    of a right merge is presented in the following figure:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 右连接（也称为右外连接）返回右侧 DataFrame 中的所有行，以及左侧 DataFrame 中的匹配行。右连接的结果如下图所示：
- en: '![Figure 5.3 – Right merge](img/B19801_05_3.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 右连接](img/B19801_05_3.jpg)'
- en: Figure 5.3 – Right merge
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 右连接
- en: 'Let’s have a look at how we can achieve the preceding result using the pandas
    `merge` function, using the example presented in the previous section:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何使用 pandas 的`merge`函数实现前述结果，参考上一节中提供的示例：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `how=''right''` argument specifies that a right outer join should be performed.
    This type of join returns all rows from the right DataFrame (`project_data`),
    and the matched rows from the left DataFrame (`employee_data`). Where there is
    no match, the result will have `NaN` in the columns of the left DataFrame. In
    the following table, you can see the output of the preceding join of the two DataFrames:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`how=''right''` 参数指定执行右外连接。此类型的连接返回右侧 DataFrame（`project_data`）中的所有行，以及左侧 DataFrame（`employee_data`）中的匹配行。如果没有匹配，则结果中左侧
    DataFrame 的列会显示为 `NaN`。在下表中，你可以看到前述两个 DataFrame 合并的输出结果：'
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the following list, we present specific examples from the data world where
    a right merge is crucial:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们展示了数据领域中右连接至关重要的具体示例：
- en: '**Completing data**: A right merge is useful when you need to ensure that all
    entries from the right DataFrame are retained in the result, which is important
    when the right DataFrame contains essential data that must not be lost'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完成数据**：当你需要确保保留右侧 DataFrame 中的所有条目时，右连接非常有用，这在右侧 DataFrame 包含必须保留的重要数据时尤其重要。'
- en: '**Data enrichment**: This type of join can be used to enrich a dataset (right
    DataFrame) with additional attributes from another dataset (left DataFrame) while
    ensuring that all records from the primary dataset are preserved'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据增强**：这种类型的连接可用于通过从另一个数据集（左侧 DataFrame）中获取附加属性来丰富数据集（右侧 DataFrame），同时确保保留主数据集中的所有记录。'
- en: '**Mismatched data analysis**: Like outer joins, right merges can help identify
    which entries in the right DataFrame do not have corresponding entries in the
    left DataFrame, which can be critical for data cleaning and validation processes'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据不匹配分析**：与外连接类似，右连接可以帮助识别右侧 DataFrame 中哪些条目没有对应的左侧 DataFrame 条目，这对于数据清洗和验证过程至关重要。'
- en: Transitioning from a right to a left merge shifts the perspective of the merged
    data, prioritizing the inclusion of all rows from the left DataFrame while maintaining
    matches from the right DataFrame.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从右连接转为左连接，改变了合并数据的视角，优先考虑包括左侧数据框的所有行，同时保持右侧数据框的匹配行。
- en: Left merge
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 左连接
- en: 'The left merge (also known as left outer join) returns all the rows from the
    left DataFrame and the matching rows from the right DataFrame. The result of a
    left merge is presented in the following figure:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 左连接（也称为左外连接）返回左侧数据框的所有行以及右侧数据框的匹配行。左连接的结果如以下图所示：
- en: '![Figure 5.4 – Left merge](img/B19801_05_4.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 左连接](img/B19801_05_4.jpg)'
- en: Figure 5.4 – Left merge
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 左连接
- en: 'Let’s have a look at how we can achieve the preceding result using the pandas
    `merge` function, using the example presented in the previous section:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用 pandas 的`merge`函数来实现前述结果，使用上一节中提供的示例：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `how=''left''` argument specifies that a left outer join should be performed.
    This type of join returns all rows from the left DataFrame (`employee_data`),
    and the matched rows from the right DataFrame (`project_data`). Where there is
    no match, the result will have `NaN` in the columns of the right DataFrame. In
    the following table, you can see the output of the preceding join of the two DataFrames:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`how=''left''`参数指定应执行左外连接。这种类型的连接返回左侧数据框（`employee_data`）的所有行，以及右侧数据框（`project_data`）的匹配行。如果没有匹配项，结果将会在右侧数据框的列中显示`NaN`。在以下表格中，您可以看到前述两数据框合并的结果：'
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you are wondering when the left merge should be used, then the considerations
    presented in the previous section about the right merge apply in this case too.
    Now that we have discussed merge operations, let’s move on to handling duplicates
    that may arise during the merging process.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道何时使用左连接，那么之前关于右连接的考虑同样适用于左连接。现在我们已经讨论了合并操作，接下来我们来讨论在合并过程中可能出现的重复项如何处理。
- en: Handling duplicates when merging datasets
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合并数据集时处理重复项
- en: Handling duplicate keys before performing merge operations is crucial because
    duplicates can lead to unexpected results, such as Cartesian products, where rows
    are multiplied by the number of matching entries. This can not only distort the
    data analysis but also significantly impact performance due to the increased size
    of the resulting DataFrame.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行合并操作之前处理重复键非常重要，因为重复项可能导致意外结果，例如笛卡尔积，行数会根据匹配条目的数量而增加。这不仅会扭曲数据分析，还会因为结果数据框的大小增加而显著影响性能。
- en: Why handle duplication in rows and columns?
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么要处理行和列中的重复项？
- en: 'Duplicate keys can lead to a range of problems that may compromise the accuracy
    of your results and the efficiency of your data processing. Let’s explore why
    it’s a good idea to handle duplicate keys prior to merging data:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 重复的键可能会导致一系列问题，这些问题可能会影响结果的准确性和数据处理的效率。让我们来探讨一下为什么在合并数据之前处理重复键是一个好主意：
- en: If there are duplicate keys in either table, merging these tables can result
    in a **Cartesian product**, where each duplicate key in one table matches with
    each occurrence of the same key in the other table, leading to an exponential
    increase in the number of rows
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果任一表格中存在重复键，合并这些表格可能会导致**笛卡尔积**，即一个表格中的每个重复键与另一个表格中相同键的每个出现匹配，从而导致行数呈指数增长。
- en: Duplicate keys might represent data errors or inconsistencies, which can lead
    to incorrect analysis or conclusions
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复的键可能表示数据错误或不一致，这可能导致错误的分析或结论。
- en: Reducing the dataset size by removing duplicates can lead to faster processing
    times during the merge operation
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过删除重复项来减少数据集的大小，可以加速合并操作的处理时间。
- en: Having understood the importance of handling duplicate keys, let’s now examine
    various strategies to effectively manage these duplicates before proceeding with
    merge operations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了处理重复键的重要性后，让我们来看看在进行合并操作之前，有哪些有效的策略可以管理这些重复项。
- en: Dropping duplicate rows
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除重复行
- en: 'Dropping duplicate entries in your datasets involves identifying and removing
    any duplicate rows based on specific key columns, which ensures that each entry
    is unique. This step not only simplifies subsequent data merging but also enhances
    the reliability of the analysis by eliminating potential sources of error caused
    by duplicate data. To showcase the dropping of duplicates, we will expand the
    example we have been using to add more duplicated rows in each of the DataFrames.
    As always, you can follow the full code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6a.manage_duplicates.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6a.manage_duplicates.py).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中删除重复项涉及识别并删除基于特定键列的重复行，以确保每个条目都是唯一的。这一步不仅简化了后续的数据合并，还通过消除由重复数据引起的潜在错误来源，提高了分析的可靠性。为了展示删除重复项，我们将扩展我们一直在使用的示例，在每个
    DataFrame 中添加更多的重复行。像往常一样，您可以在此查看完整代码：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6a.manage_duplicates.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6a.manage_duplicates.py)。
- en: 'Let’s first create the sample employee data with some duplicate keys in the
    `employee_id` column:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一些具有重复 `employee_id` 键的示例员工数据：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s also create the sample project data with some duplicate keys in the `employee_id`
    column:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还创建一些具有重复 `employee_id` 键的示例项目数据：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we want to merge these datasets. But first, we’ll drop any duplicates
    so that we can make the merge operation as lightweight as possible. Dropping the
    duplicates before the merge is shown in the following code snippet:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们要合并这些数据集。但首先，我们将删除所有重复项，以使合并操作尽可能轻便。删除重复项后的合并操作在以下代码片段中展示：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As shown in the code, `drop_duplicates()` is used to remove duplicate rows based
    on `employee_id`. The `keep='first'` parameter ensures that only the first occurrence
    is kept while others are removed.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如代码所示，`drop_duplicates()` 用于根据 `employee_id` 删除重复行。`keep='first'` 参数确保仅保留首次出现的记录，其他记录将被删除。
- en: 'After dropping the duplicates, you can proceed with the merge operation, as
    shown in the following code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 删除重复项后，您可以继续进行合并操作，如以下代码所示：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The merged dataset can be seen here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 合并后的数据集如下所示：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `merged_data` DataFrame includes columns from both the `employee_data` and
    `project_data` DataFrames, showing the `employee_id`, `name`, `department`, and
    `project_name` values for each employee that exists in both datasets. The duplicates
    are removed, ensuring each employee appears only once in the final merged dataset.
    The `drop_duplicates` operation is crucial for avoiding data redundancy and potential
    conflicts during the merge operation. Next, we will discuss how we can guarantee
    that the merge operation respects the uniqueness of the keys and adheres to specific
    constraints.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`merged_data` DataFrame 包含了来自 `employee_data` 和 `project_data` 两个 DataFrame
    的列，显示了每个在两个数据集中都存在的员工的 `employee_id`、`name`、`department` 和 `project_name` 的值。重复项被删除，确保每个员工在最终合并的数据集中仅出现一次。`drop_duplicates`
    操作对避免数据冗余和合并过程中可能出现的冲突至关重要。接下来，我们将讨论如何确保合并操作尊重键的唯一性并遵守特定的约束条件。'
- en: Validating data before merging
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并前验证数据
- en: When merging datasets, especially large and complex ones, ensuring the integrity
    and validity of the merge operation is crucial. pandas provides the `validate`
    parameter in the `merge()` function to enforce specific conditions and relationships
    between the keys used in the merge. This helps in identifying and preventing unintended
    duplications or data mismatches that could compromise the analysis.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在合并数据集时，尤其是处理大型和复杂数据集时，确保合并操作的完整性和有效性至关重要。pandas 在 `merge()` 函数中提供了 `validate`
    参数，用于强制执行合并键之间的特定条件和关系。这有助于识别并防止可能影响分析的无意重复或数据不匹配。
- en: 'The following code demonstrates how to use the `validate` parameter to enforce
    `merge()` constraints and handle exceptions when these constraints are not met.
    You can see the full code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6b.manage_duplicates_validate.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6b.manage_duplicates_validate.py):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何使用`validate`参数来强制执行`merge()`约束，并在这些约束未满足时处理异常。你可以在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6b.manage_duplicates_validate.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6b.manage_duplicates_validate.py)查看完整代码：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding code snippet, the merge operation is wrapped in a `try-except`
    block. This is a way to handle exceptions, which are errors occurring during a
    program’s execution. The `try` block contains the code that might raise an exception,
    in this case, the merge operation. If an exception occurs, the code execution
    moves to the `except` block.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，合并操作被包装在`try-except`代码块中。这是一种处理异常的方式，异常是指程序执行过程中发生的错误。`try`代码块包含可能引发异常的代码，在这种情况下是合并操作。如果发生异常，代码执行将跳转到`except`代码块。
- en: If the merge operation fails the validation check (in our case, if there are
    duplicate keys in the left DataFrame when they are expected to be unique), `ValueError`
    exception will be raised, and the `except` block will be executed. The `except`
    block catches the `ValueError` exception and prints a `Merge failed:` message,
    followed by the error message provided by pandas.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果合并操作未通过验证检查（在我们的例子中，如果左侧DataFrame中存在重复的键，而这些键应该是唯一的），将引发`ValueError`异常，并执行`except`代码块。`except`代码块捕获`ValueError`异常并打印`Merge
    failed:`消息，后跟pandas提供的错误信息。
- en: 'After executing the preceding code, you will see the following error message:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码后，你将看到以下错误消息：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `validate='one_to_many'` parameter is included in the merge operation. This
    parameter tells pandas to check that the merge operation is of the specified type.
    In this case, `one_to_many` means that the merge keys should be unique in the
    left DataFrame (`employee_data`) but can have duplicates in the right DataFrame
    (`project_data`). If the validation check fails, pandas will raise a `ValueError`
    exception.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`validate=''one_to_many''`参数包含在合并操作中。该参数告诉pandas检查合并操作是否符合指定类型。在这种情况下，`one_to_many`表示合并键在左侧DataFrame（`employee_data`）中应唯一，但在右侧DataFrame（`project_data`）中可以有重复项。如果验证检查失败，pandas将引发`ValueError`异常。'
- en: When to use which approach
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 何时使用哪种方法
- en: Use **manual duplicate removal** when you need fine control over how duplicates
    are identified and handled, or when duplicates require special processing (e.g.,
    aggregation or transformation based on other column values).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要精细控制重复项的识别和处理方式，或者当重复项需要特殊处理（例如基于其他列值的聚合或转换）时，使用**手动删除重复项**。
- en: Use **merge validation** when you want to ensure the structural integrity of
    your data model directly within the merge operation, especially in straightforward
    cases where the relationship between the tables is well-defined and should not
    include duplicate keys according to the business logic or data model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当你希望直接在合并操作中确保数据模型的结构完整性时，使用**合并验证**，尤其是在表之间的关系明确定义并且根据业务逻辑或数据模型不应包含重复键的简单情况。
- en: If there is a good reason for the existence of duplicates in the data, we can
    consider employing aggregation methods during the merge to consolidate redundant
    information.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据中存在重复项是有充分理由的，我们可以考虑在合并过程中采用聚合方法，以合并冗余信息。
- en: Aggregation
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合
- en: Aggregation is a powerful technique for managing duplicates in datasets, particularly
    when dealing with key columns that should be unique but contain multiple entries.
    By grouping data on these key columns and applying aggregation functions, we can
    consolidate duplicate entries into a single, summarized record. Aggregation functions
    such as sum, average, or maximum can be used to combine or summarize the data
    in a way that aligns with the analytical goals.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合是管理数据集重复项的强大技术，特别是在处理应唯一但包含多个条目的关键列时。通过在这些关键列上分组数据并应用聚合函数，我们可以将重复条目合并为单一的汇总记录。可以使用求和、平均值或最大值等聚合函数，以与分析目标对齐的方式来合并或汇总数据。
- en: 'Let’s see how aggregation can be employed to effectively deal with duplicates
    before merging data. We will extend the dataset a little bit to help us with this
    example, as shown here. You can see the full example at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6c.merge_and_aggregate.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6c.merge_and_aggregate.py):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何利用聚合来有效地处理数据重复问题。为了帮助展示这个例子，我们稍微扩展一下数据集，具体如下所示。你可以在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6c.merge_and_aggregate.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6c.merge_and_aggregate.py)看到完整示例：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, let’s perform the aggregation step:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行聚合步骤：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `groupby()` method is used on `employee_data` with `employee_id` as the
    key. This groups the DataFrame by `employee_id`, which is necessary because of
    the duplicate `employee_id` values.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupby()`方法在`employee_data`上使用，`employee_id`作为键。这将DataFrame按`employee_id`分组，因为存在重复的`employee_id`值。'
- en: 'The `agg()` method is then applied to perform specific aggregations on different
    columns:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`agg()`方法被应用于对不同列进行特定的聚合操作：
- en: '`''name'': ''first''` and `''department'': ''first''` ensure that the first
    encountered values for these columns are retained in the grouped data'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''name'': ''first''`和`''department'': ''first''`确保在分组数据中保留这些列的首次遇到的值。'
- en: '`''salary'': ''sum''` sums up the salaries for each `employee_id` value, which
    is useful if the duplicates represent split records of cumulative data'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''salary'': ''sum''`对每个`employee_id`值的薪资进行求和，如果重复数据表示累计数据的拆分记录，这将非常有用。'
- en: 'In the final step, the `pd.merge()` function is used to combine `aggregated_employee_data`
    with `project_data` using an inner join on the `employee_id` column:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，使用`pd.merge()`函数通过在`employee_id`列上进行内连接，将`aggregated_employee_data`与`project_data`合并：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This ensures that only employees with project assignments are included in the
    result. The result after the merge is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了只有有项目分配的员工会被包含在结果中。合并后的结果如下所示：
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `agg()` method in pandas is highly versatile, offering numerous options
    beyond the simple “keep first” approach demonstrated in the previous example.
    This method can apply a wide range of aggregation functions to consolidate data,
    such as summing numerical values, finding averages, or selecting maximum or minimum
    entries. We will dive deeper into the various capabilities of the `agg()` method
    in the next chapter, exploring how these different options can be applied to enhance
    data preparation and analysis.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: pandas中的`agg()`方法非常灵活，提供了许多超出简单“保留首个”方法的选项。这个方法可以应用各种聚合函数来汇总数据，比如对数值进行求和、求平均值，或选择最大值或最小值。我们将在下一章深入探讨`agg()`方法的多种功能，探索如何运用这些不同的选项来提升数据准备和分析的质量。
- en: Let’s move on from using aggregation as a way to handle duplicates to concatenating
    duplicated rows when dealing with text or categorical data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从使用聚合来处理重复数据过渡到拼接重复行，这在处理文本或类别数据时非常有效。
- en: Concatenation
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拼接
- en: The concatenation of values from duplicate rows into a single row can be a useful
    technique, especially when dealing with categorical or textual data that may have
    multiple valid entries for the same key. This approach allows you to preserve
    all the information across duplicates without losing data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 将重复行的值拼接成一行是一种有用的技巧，特别是在处理可能包含多个有效条目的文本或类别数据时。这种方法允许你保留重复数据中的所有信息，而不会丢失数据。
- en: 'Let’s see how concatenation of rows can be employed to effectively deal with
    duplicates before merging data. To showcase this method, we will use the following
    DataFrame:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何通过拼接行来有效地处理数据重复问题，在合并数据之前。为了展示这一方法，我们将使用以下DataFrame：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let’s perform the concatenation step, as shown in the following code snippet:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行拼接步骤，如下面的代码片段所示：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the concatenation step, the `groupby(''employee_id'')` method groups the
    data by `employee_id`. The `transform(lambda x: '', ''.join(x))` method is then
    applied to the `department` column. The `transform` function is used here with
    a `lambda` function that joins all entries of the column department for each group
    (i.e., `employee_id`) into a single string separated by commas.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '在拼接步骤中，`groupby(''employee_id'')`方法按`employee_id`对数据进行分组。然后，`transform(lambda
    x: '', ''.join(x))`方法应用于`department`列。此时，使用`lambda`函数通过逗号将每个组（即`employee_id`）的`department`列的所有条目合并成一个字符串。'
- en: 'The result of this operation replaces the original `department` column in `employee_data`,
    where each `employee_id` now has a single entry for `department` that includes
    all original department data concatenated into one string, as shown in the following
    table:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作的结果替换了`employee_data`中原始的`department`列，现在每个`employee_id`都有一个包含所有原始部门数据合并为一个字符串的单一`department`条目，如下表所示：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Use concatenation when you need to preserve all categorical or textual data
    across duplicate entries without preferring one entry over another.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要在重复条目中保留所有类别或文本数据，而不偏向某一条目时，可以使用连接。
- en: This method is useful for summarizing textual data in a way that is still readable
    and informative, especially when dealing with attributes that can have multiple
    valid values (e.g., an employee belonging to multiple departments).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有助于以可读且信息丰富的方式总结文本数据，特别是在处理可能具有多个有效值的属性时（例如，一个员工属于多个部门）。
- en: Once duplicate rows in each DataFrame are resolved, attention shifts to identifying
    and resolving duplicate columns across the DataFrames to be merged.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦解决了每个数据框中的重复行，注意力就转向识别和解决跨数据框的重复列问题。
- en: Handling duplication in columns
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理列中的重复
- en: When merging data from different sources, it’s not uncommon to encounter DataFrames
    with overlapping column names. This challenge often arises when combining data
    from similar datasets.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在合并来自不同来源的数据时，遇到列名重叠的数据框并不罕见。这种情况通常发生在合并类似数据集时。
- en: 'Expanding the example data we have been using so far, we will adjust the DataFrames
    to help us showcase the options we have when dealing with common columns across
    DataFrames. The data can be seen here:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展我们迄今为止使用的示例数据，我们将调整数据框（DataFrame）以帮助展示在处理多个数据框中共有列时可用的选项。数据可以在此查看：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Let’s see how we can merge these datasets by applying different techniques without
    breaking the merge operation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何通过应用不同的技巧来合并这些数据集，而不会破坏合并操作。
- en: Handling duplicate columns while merging
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 合并时处理重复列
- en: The columns in the two DataFrames presented previously share identical names
    and may represent the same data. However, we have decided to retain both sets
    of columns in the merged DataFrame. This decision is based on the suspicion that,
    despite having the same column names, the entries are not entirely identical,
    indicating that they may be different representations of the same data. This is
    an issue that we can address later, following the merge operation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 前面展示的两个数据框中的列名相同，可能表示相同的数据。然而，我们决定在合并的数据框中保留两组列。这个决定基于这样的怀疑：尽管列名相同，但条目并不完全相同，这表明它们可能是相同数据的不同表示形式。这个问题我们可以在合并操作后再处理。
- en: 'The best approach to keep both sets of columns is to use the `suffixes` parameter
    in the `merge()` function. This will allow you to differentiate between the columns
    from each DataFrame without losing any data. Here’s how you can implement this
    in Python using pandas:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 保持两个列集的最佳方法是使用`merge()`函数中的`suffixes`参数。这将允许你区分来自每个数据框的列，而不会丢失任何数据。以下是在Python中使用pandas实现这一点的方法：
- en: '[PRE25]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `pd.merge()` function is used to merge the two DataFrames on `employee_id`.
    The `how=''outer''` parameter is used to ensure all records from both DataFrames
    are included in the merged DataFrame, even if there are no matching `employee_id`
    values. The `suffixes=(''_1'', ''_2'')` parameter adds suffixes to the columns
    from each DataFrame to differentiate them in the merged DataFrame. This is crucial
    when columns have the same names but come from different sources. Let’s review
    the output DataFrame:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.merge()`函数用于在`employee_id`上合并两个数据框。`how=''outer''`参数确保包括来自两个数据框的所有记录，即使没有匹配的`employee_id`值。`suffixes=(''_1'',
    ''_2'')`参数为每个数据框的列添加后缀，以便在合并后的数据框中区分它们。当列名相同但来自不同数据源时，这一点尤为重要。让我们回顾一下输出数据框：'
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This approach is particularly useful in scenarios where merging data from different
    sources involves overlapping column names but where it’s also important to retain
    and clearly distinguish these columns in the resulting DataFrame. Another point
    to consider is that suffixes allow for identifying which DataFrame the data originated
    from, which is useful in analyses involving data from multiple sources.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在从不同来源合并数据时尤其有用，尤其是当涉及到列名重叠的情况，但同时也需要保留并清晰地区分这些列。另一个需要考虑的点是，后缀可以帮助识别数据来源的数据框，这在涉及多个来源的数据分析中非常有用。
- en: In the next section, we will explain how to deal with duplicate columns by dropping
    them *before* the merge.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将解释如何通过在合并之前删除列来处理重复列。
- en: Dropping duplicate columns before the merge
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在合并前删除重复列
- en: 'If we find that we have copies of the same column in both DataFrames we want
    to merge and that the column in one of the DataFrames is sufficient or more reliable
    than the other, then it may be more practical to drop one of the duplicate columns
    before a merge operation instead of keeping both. This decision can be driven
    by the need to simplify the dataset, reduce redundancy, or when one of the columns
    does not provide additional value to the analysis. Let’s have a look at the data
    for this example:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们发现要合并的两个 DataFrame 中有相同列的副本，而且其中一个 DataFrame 中的列比另一个更可靠或足够使用，那么在合并操作之前删除其中一个重复列可能更为实际，而不是保留两个副本。做出这一决策的原因可能是简化数据集、减少冗余，或者当某一列对分析没有额外价值时。让我们看一下这个示例的数据：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If we take a closer look at this data, we can see that the `department` column
    in the two DataFrames captures the same information but in different formats.
    For the sake of our example, let’s assume that we know the HR system tracks the
    department of each employee in the format presented in the first DataFrame. That’s
    why we will trust this column more than the one in the second DataFrame. Therefore,
    we will drop the second one before the merge operation. Here’s how you can drop
    the column before the merge:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细查看这些数据，可以发现两个 DataFrame 中的 `department` 列捕获了相同的信息，但格式不同。为了简化我们的示例，假设我们知道
    HR 系统以第一个 DataFrame 中呈现的格式跟踪每个员工的部门。这就是为什么我们会更信任第一个 DataFrame 中的列，而不是第二个 DataFrame
    中的列。因此，我们将在合并操作之前删除第二个列。下面是如何在合并之前删除列的操作：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Before merging, the `department` column from `employee_data_2` is dropped because
    it’s deemed less reliable. This is done using the `drop(columns=[''department''],
    inplace=True)` method. Having dropped the required columns, we can proceed with
    the merge:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在合并之前，`employee_data_2` 中的 `department` 列被删除，因为它被认为不够可靠。这是通过 `drop(columns=['department'],
    inplace=True)` 方法完成的。在删除了不需要的列之后，我们可以继续进行合并：
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The DataFrames are merged using the `employee_id` and `name` columns as keys
    with the `pd.merge()` function. The `how='inner'` parameter is used to perform
    an inner join, which includes only rows that have matching values in both DataFrames.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pd.merge()` 函数，以 `employee_id` 和 `name` 列作为键合并 DataFrame。使用 `how='inner'`
    参数来执行内连接，只包含在两个 DataFrame 中具有匹配值的行。
- en: 'To optimize the merging process and improve performance, it’s often beneficial
    to drop unnecessary columns before performing the merge operation for the following
    reasons:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化合并过程并提高性能，通常在执行合并操作之前删除不必要的列是有益的，原因如下：
- en: It leads to improved performance by significantly reducing the memory footprint
    during the merge operation, as it minimizes the amount of data to be processed
    and combined, thereby expediting the process.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过显著减少合并操作时的内存占用，这种做法可以提高性能，因为它最小化了需要处理和合并的数据量，从而加快了处理速度。
- en: The resulting DataFrame becomes simpler and cleaner, facilitating easier data
    management and subsequent analysis. This reduction in complexity not only streamlines
    the merge operation but also reduces the likelihood of errors.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果 DataFrame 变得更加简洁清晰，便于数据管理和后续分析。这种复杂度的减少不仅简化了合并操作，还减少了出错的可能性。
- en: In resource-constrained environments, such as those with limited computing resources,
    minimizing the dataset before intensive operations such as merging enhances resource
    efficiency and ensures smoother execution.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在资源受限的环境中，例如计算资源有限的情况，减少数据集大小在进行如合并等密集型操作之前，可以提高资源效率，并确保更顺畅的执行。
- en: In the case that we have identical columns across the DataFrames, another option
    is to consider whether we can use them as keys in the merge operation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在 DataFrame 中存在相同的列，另一种选择是考虑是否可以将它们作为合并操作的键。
- en: Duplicate keys
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重复键
- en: 'When encountering identical keys across DataFrames, a smart approach is to
    merge based on these common columns. Let’s revisit the example presented in the
    previous section:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 当遇到跨多个 DataFrame 的相同键时，一种智能的做法是基于这些共同列进行合并。让我们回顾一下前一节中提供的示例：
- en: '[PRE30]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We can see here that we used `['employee_id', 'name']` as keys in the merge.
    If `employee_id` and `name` are reliable identifiers that ensure accurate matching
    of records across DataFrames, they should be used as keys. This ensures that the
    merged data accurately represents combined records from both sources.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这里我们使用了 `['employee_id', 'name']` 作为合并的键。如果 `employee_id` 和 `name` 是可靠的标识符，能够确保在
    DataFrame 之间准确匹配记录，那么它们应该作为合并的键。这确保了合并后的数据准确地代表了两个来源的结合记录。
- en: As the volume and complexity of data continues to grow, it is crucial to efficiently
    combine datasets, as we will learn in the following section.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据量和复杂性的不断增长，高效地合并数据集变得至关重要，正如我们在接下来的部分中将要学习的那样。
- en: Performance tricks for merging
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合并的性能技巧
- en: When working with large datasets, the performance of merge operations can significantly
    impact the overall efficiency of data processing tasks. Merging is a common and
    often necessary step in data analysis, but it can be computationally intensive,
    especially when dealing with big data. Therefore, employing performance optimization
    techniques is crucial to ensure that merges are executed as quickly and efficiently
    as possible.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大型数据集时，合并操作的性能可能会显著影响数据处理任务的整体效率。合并是数据分析中常见且常常必需的步骤，但它可能是计算密集型的，尤其是在处理大数据时。因此，采用性能优化技术对于确保合并操作尽可能快速高效地执行至关重要。
- en: Optimizing merge operations can lead to reduced execution time, lower memory
    consumption, and an overall smoother data-handling experience. In the following
    sections, we will explore various performance tricks that can be applied to merge
    operations in pandas, such as utilizing indexes, sorting indexes, choosing the
    right merge method, and reducing memory usage.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 优化合并操作可以减少执行时间，降低内存消耗，并带来更加流畅的数据处理体验。在接下来的部分，我们将探讨一些可以应用于 pandas 合并操作的性能技巧，如使用索引、排序索引、选择合适的合并方法以及减少内存使用。
- en: Set indexes
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置索引
- en: Utilizing indexes in pandas is a critical aspect of data manipulation and analysis,
    particularly when dealing with large datasets or performing frequent data retrieval
    operations. Indexes serve as a tool for both identification and efficient data
    access, providing several benefits that can significantly enhance performance.
    Specifically, when merging DataFrames, utilizing indexes can lead to performance
    improvements. Merging on indexes, rather than on columns, is generally faster
    because pandas can perform the merge operation using optimized index-based joining
    methods, which is more efficient than column-based merging. Let’s revisit the
    employee example to prove this concept. The full code for this example can be
    found at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/8a.perfomance_benchmark_set_index.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/8a.perfomance_benchmark_set_index.py).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pandas 中使用索引是数据处理和分析中的一个关键方面，尤其是在处理大型数据集或进行频繁的数据检索操作时。索引既是标识工具，也是高效数据访问的工具，提供了多种好处，能够显著提高性能。具体来说，在合并
    DataFrame 时，使用索引能够带来性能上的提升。与基于列的合并相比，基于索引的合并通常更快，因为 pandas 可以使用优化的基于索引的连接方法来执行合并操作，这比基于列的合并更高效。让我们回顾一下员工示例来证明这一概念。此示例的完整代码可以在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/8a.perfomance_benchmark_set_index.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/8a.perfomance_benchmark_set_index.py)中找到。
- en: 'First, let’s import the necessary libraries:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE31]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Select the number of rows for the benchmarking example for each DataFrame:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个 DataFrame 选择基准示例的行数：
- en: '[PRE32]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let’s create the DataFrames for the example, which will have the number of
    rows as defined in the `num_rows` variable. The first employee DataFrame is defined
    here:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建示例所需的 DataFrame，这些 DataFrame 的行数将由 `num_rows` 变量定义。这里定义了第一个员工 DataFrame：
- en: '[PRE33]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The second DataFrame is as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个 DataFrame 如下所示：
- en: '[PRE34]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To demonstrate the effectiveness of the performance tricks we applied, we will
    initially perform the merge *without utilizing the index*. We’ll calculate the
    time taken for this operation. Subsequently, we’ll set the index in both DataFrames
    and repeat the merge operation, recalculating the time. Finally, we will present
    the results. Let’s hope this approach yields the desired outcome! Let’s start
    the clock:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们所应用的性能技巧的效果，我们最初将执行不利用索引的合并操作。我们将计算此操作所需的时间。接着，我们会在两个DataFrame中设置索引并重新执行合并操作，重新计算时间。最后，我们将展示结果。希望这个方法能产生预期的效果！开始计时：
- en: '[PRE35]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s perform the merge operation without using indexes, just by inner joining
    on `[''``employee_id'', ''name'']`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在不使用索引的情况下执行合并操作，只通过`['employee_id', 'name']`进行内连接：
- en: '[PRE36]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let’s calculate the time it took to perform the merge:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算执行合并所花费的时间：
- en: '[PRE37]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The timings may vary depending on the computer used to execute the program.
    The idea is that the optimized version takes less time than the original merge
    operation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 执行程序的计算机可能会导致时间有所不同。这个想法是，优化后的版本比原始合并操作所需的时间更短。
- en: 'By setting `employee_id` as the index for both DataFrames (`employee_data_1`
    and `employee_data_2`), we allow pandas to use optimized index-based joining methods.
    This is particularly effective because indexes in pandas are implemented via hash
    tables or B-trees, depending on the data type and the sortedness of the index,
    which facilitates faster lookups:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将`employee_id`作为两个DataFrame（`employee_data_1`和`employee_data_2`）的索引，我们让pandas使用基于索引的优化连接方法。这尤其有效，因为pandas中的索引是通过哈希表或B树实现的，具体取决于数据类型和索引的排序性，这有助于加速查找：
- en: '[PRE38]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let’s repeat the merge operation after setting the indexes and calculate the
    time once more:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置索引后，我们再执行一次合并操作，并重新计算时间：
- en: '[PRE39]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now, if we calculate the percentage difference from the initial time to the
    final one, we see that we managed to drop the time by around 88.5%, just by setting
    the index. This seems impressive but let’s also discuss some considerations when
    setting indexes.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们计算从初始时间到最终时间的百分比差异，我们发现仅仅通过设置索引，我们就将时间缩短了约88.5%。这看起来很令人印象深刻，但我们也需要讨论一些设置索引时的注意事项。
- en: Index considerations
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 索引注意事项
- en: It’s important to choose the right columns for indexing based on the query patterns.
    Over-indexing can lead to unnecessary use of disk space and can degrade write
    performance due to the overhead of maintaining the indexes.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的列进行索引设置非常重要，应基于查询模式。过度索引可能导致不必要的磁盘空间占用，并且由于维护索引的开销，可能会降低写操作性能。
- en: '**Rebuilding** or **reorganizing indexes** is essential for optimal performance.
    These tasks address index fragmentation and ensure consistent performance over
    time.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**重建**或**重组索引**对于优化性能至关重要。这些任务解决了索引碎片问题，并确保随着时间推移性能的一致性。'
- en: While indexes can significantly improve read performance, they can also impact
    write performance. It’s crucial to find a balance between optimizing for read
    operations (such as searches and joins) and maintaining efficient write operations
    (such as inserts and updates).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然索引可以显著提高读取性能，但它们也可能影响写入性能。在优化读取操作（如搜索和连接）与保持高效的写入操作（如插入和更新）之间找到平衡至关重要。
- en: Multi-column indexes, or concatenated indexes, can be beneficial when multiple
    fields are often used together in queries. However, the order of the fields in
    the index definition is important and should reflect the most common query patterns.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 多列索引或连接索引在多个字段经常一起用于查询时可能是有益的。然而，索引定义中字段的顺序非常重要，应反映出最常见的查询模式。
- en: Having proved the importance of setting indexes, let’s go a step further and
    discuss the option of sorting the index before merging.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在证明了设置索引的重要性后，我们进一步讨论在合并前对索引进行排序的选项。
- en: Sorting indexes
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排序索引
- en: 'Sorting the index in pandas can be particularly beneficial in scenarios where
    you are frequently merging or performing join operations on large DataFrames.
    When indexes are sorted, pandas can take advantage of more efficient algorithms
    to align and join data, which can lead to significant performance improvements.
    Let’s deep dive into this before proceeding to the code example:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas中排序索引在你经常对大规模DataFrame进行合并或连接操作的场景中尤其有利。当索引被排序时，pandas可以利用更高效的算法来对齐和连接数据，这可能会显著提升性能。让我们在继续代码示例之前深入探讨这一点：
- en: When indexes are sorted, pandas can use binary search algorithms to locate the
    matching rows between DataFrames. Binary search has a time complexity of *O(log
    n)*, which is much faster than the linear search required for unsorted indexes,
    especially as the size of the DataFrame grows.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当索引已排序时，pandas 可以使用二分查找算法来定位 DataFrame 之间的匹配行。二分查找的时间复杂度是 *O(log n)*，这比未排序索引所需的线性查找要快得多，特别是当
    DataFrame 的大小增加时。
- en: Sorted indexes facilitate quicker data alignment. This is because pandas can
    make certain assumptions about the order of the data, which streamlines the process
    of finding corresponding rows in each DataFrame during a merge.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排序索引有助于更快地对齐数据。这是因为 pandas 可以对数据的顺序做出一些假设，从而简化在合并时查找每个 DataFrame 中对应行的过程。
- en: With sorted indexes, pandas can avoid unnecessary comparisons that would be
    required if the indexes were unsorted. This reduces the computational overhead
    and speeds up the merging process.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用排序后的索引，pandas 可以避免进行不必要的比较，这些比较是当索引未排序时所必需的。这样可以减少计算开销，加速合并过程。
- en: 'Let’s go back to the code example by adding the sorting of the index. The original
    data remains the same; however, in this experiment, we are comparing the time
    it takes to perform the merge operation after the setting of the index versus
    the time it takes to perform the merge operation after the setting and sorting
    of indexes. The following code shows the main code components but, as always,
    you can follow the full example at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/8b.performance_benchmark_sort_indexes.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/8b.performance_benchmark_sort_indexes.py):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到代码示例，加入索引排序的步骤。原始数据保持不变；但是在本实验中，我们比较的是在设置索引后执行合并操作的时间与在设置并排序索引后执行合并操作的时间。以下代码展示了主要的代码组件，但和往常一样，你可以通过
    [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/8b.performance_benchmark_sort_indexes.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/8b.performance_benchmark_sort_indexes.py)
    跟进完整示例：
- en: '[PRE40]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let’s perform the merge operation without sorting indexes:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在不排序索引的情况下执行合并操作：
- en: '[PRE41]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let’s repeat the merge operation after sorting the indexes and calculate the
    time once more:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在排序索引后重复合并操作，并再次计算时间：
- en: '[PRE42]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now, if we calculate the percentage difference from the initial time to the
    final one, we see that we managed to drop the time by an extra ~22%, by sorting
    the index. This seems impressive but let’s also discuss some considerations when
    setting indexes.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们计算从初始时间到最终时间的百分比差异，我们会发现通过排序索引，我们成功地将时间减少了大约 ~22%。这看起来很不错，但我们也需要讨论设置索引时的一些注意事项。
- en: Sorting index considerations
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排序索引的注意事项
- en: Sorting a DataFrame’s index is not free of computational cost. The initial sorting
    operation itself takes time, so it’s most beneficial when the sorted DataFrame
    will be used in multiple merge or join operations, amortizing the cost of sorting
    over these operations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 排序 DataFrame 的索引并不是没有计算成本的。初始的排序操作本身需要时间，因此当排序后的 DataFrame 在多个合并或连接操作中被使用时，这种做法最为有利，可以通过这些操作摊销排序的成本。
- en: Sorting can sometimes increase the memory overhead, as pandas may create a sorted
    copy of the DataFrame’s index. This should be considered when working with very
    large datasets where memory is a constraint.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 排序有时会增加内存开销，因为 pandas 可能会创建 DataFrame 索引的排序副本。在处理非常大的数据集时，若内存是一个限制因素，应该考虑这一点。
- en: Sorting the index is most beneficial when the key used for merging is not only
    unique but also has some logical order that can be leveraged, such as time-series
    data or ordered categorical data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 排序索引最有利的情况是，合并所用的键不仅是唯一的，而且具有一定的逻辑顺序，例如时间序列数据或有序的分类数据。
- en: Index management and maintenance are crucial aspects you should consider when
    working with pandas DataFrames, especially when dealing with large datasets. Maintaining
    a well-managed index requires careful consideration. For example, regularly updating
    or reindexing a DataFrame can introduce computational costs, similar to sorting
    operations. Each time you modify the index—by sorting, reindexing, or resetting—it
    can result in additional memory usage and processing time, particularly with large
    datasets.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 索引管理和维护是你在处理 pandas DataFrame 时需要考虑的关键因素，尤其是在处理大型数据集时。维护一个良好的索引需要谨慎考虑。例如，定期更新或重新索引
    DataFrame 可能会引入计算成本，类似于排序操作。每次修改索引（通过排序、重新索引或重置）时，可能会导致额外的内存使用和处理时间，尤其是在大型数据集上。
- en: Indexes need to be maintained in a way that balances performance and resource
    usage. For instance, if you frequently merge or join DataFrames, ensuring that
    the index is properly sorted and unique can significantly speed up these operations.
    However, continuously maintaining a sorted index can be resource-intensive, so
    it’s most beneficial when the DataFrame will be involved in multiple operations
    that can leverage the sorted index.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 索引需要以平衡性能和资源使用的方式进行维护。例如，如果你经常合并或连接 DataFrame，确保索引已正确排序并且是唯一的，可以显著加速这些操作。然而，持续维护一个已排序的索引可能会消耗大量资源，因此当
    DataFrame 需要进行多次操作并利用已排序的索引时，这样做最为有利。
- en: Additionally, choosing the right index type—whether it’s a simple integer-based
    index, a datetime index for time-series data, or a multi-level index for hierarchical
    data—can influence how efficiently pandas handles your data. The choice of index
    should align with the structure and access patterns of your data to minimize unnecessary
    overhead.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，选择合适的索引类型——无论是基于整数的简单索引、用于时间序列数据的日期时间索引，还是用于层次数据的多级索引——都可能影响 pandas 处理数据的效率。索引的选择应与数据的结构和访问模式相匹配，以最小化不必要的开销。
- en: In the next section, we will discuss how using the `join` function instead of
    `merge` can impact performance.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将讨论使用 `join` 函数而非 `merge` 如何影响性能。
- en: Merge versus join
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并与连接
- en: 'While merging is a commonly used method to combine datasets based on specific
    conditions or keys, there is another approach: the `join` function. This function
    provides a streamlined way to perform merges primarily based on indexes, offering
    a simpler alternative to the more general merge function. The `join` method in
    pandas is particularly useful when the DataFrames involved have their indexes
    set up as the keys for joining, allowing for efficient and straightforward data
    combinations without the need for specifying complex join conditions.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然合并是根据特定条件或键来合并数据集的常用方法，但还有另一种方法：`join` 函数。这个函数提供了一种简化的方式，主要通过索引执行合并，为更通用的合并函数提供了一个更简单的替代方案。当涉及的
    DataFrame 已经将索引设置为用于连接的键时，pandas 中的 `join` 方法特别有用，它能够高效、直接地进行数据组合，而无需指定复杂的连接条件。
- en: 'Using the `join` function instead of `merge` can impact performance in several
    ways, primarily due to the underlying mechanisms and default behaviors of these
    two functions:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `join` 函数代替 `merge` 可能会以多种方式影响性能，主要是因为这两个函数的底层机制和默认行为：
- en: The `join` function in pandas is optimized for index-based joining, meaning
    it’s designed to be efficient when joining DataFrames on their indexes. If your
    DataFrames are already indexed by the keys you want to join on, using `join` can
    be more performance-efficient because it leverages the optimized index structures
    [2][6][7].
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas 中的 `join` 函数针对基于索引的连接进行了优化，意味着它在通过索引连接 DataFrame 时被设计得更为高效。如果你的 DataFrame
    已经按你想要连接的键进行了索引，那么使用 `join` 可以更高效，因为它利用了优化过的索引结构[2][6][7]。
- en: Join is a simplified version of merge that defaults to joining on indexes. This
    simplicity can translate into performance benefits, especially for straightforward
    joining tasks where the complexity of merge is unnecessary. By avoiding the overhead
    of aligning non-index columns, join can execute more quickly in these scenarios
    [2][6].
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Join 是 merge 的简化版本，默认按索引进行连接。这种简化可能带来性能上的优势，尤其是对于那些连接任务简单、合并复杂性不必要的场景。通过避免对非索引列的对齐开销，在这些情况下，join
    可以更快速地执行[2][6]。
- en: Under the hood, join uses merge [2][6].
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从底层实现来看，join 使用的是 merge[2][6]。
- en: When joining large DataFrames, the way join and merge handle memory can impact
    performance. A join, by focusing on index-based joining, might manage memory usage
    more efficiently in certain scenarios, especially when the DataFrames have indexes
    that pandas can optimize on [1][3][4].
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在连接大型 DataFrame 时，join 和 merge 处理内存的方式会影响性能。通过专注于基于索引的连接，join 可能在某些场景下更高效地管理内存使用，尤其是当
    DataFrame 具有 pandas 可优化的索引时 [1][3][4]。
- en: While merge offers greater flexibility by allowing joins on arbitrary columns,
    this flexibility comes with a performance cost, especially for complex joins involving
    multiple columns or non-index joins. Join offers a performance advantage in simpler,
    index-based joins due to its more specific use case [2][6].
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然 merge 提供了更大的灵活性，允许在任意列上进行连接，但这种灵活性带来了性能上的代价，尤其是在涉及多个列或非索引连接的复杂连接时。由于其更具体的使用场景，join
    在较简单的基于索引的连接中具有性能优势 [2][6]。
- en: In summary, choosing between `join` and `merge` depends on the specific requirements
    of your task. If your joining operation is primarily based on indexes, join can
    offer performance benefits due to its optimization for index-based joining and
    its simpler interface. However, for more complex joining needs that involve specific
    columns or multiple keys, merge provides the necessary flexibility, albeit with
    potential impacts on performance.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，选择 `join` 还是 `merge` 取决于任务的具体需求。如果连接操作主要基于索引，join 可以因其针对基于索引的连接进行优化而提供性能优势，且其接口更为简洁。然而，对于涉及特定列或多个键的更复杂连接需求，merge
    提供了必要的灵活性，尽管这可能会对性能产生影响。
- en: Concatenating DataFrames
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接 DataFrame
- en: When you have datasets spread across multiple DataFrames with similar structures
    (same columns or same rows) and you want to combine them into a single DataFrame,
    this is where concatenating shines. The concatenation process can be along a particular
    axis, either row-wise (`axis=0`) or column-wise (`axis=1`).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有多个结构相似（列相同或行相同）的 DataFrame，且想将它们合并成一个 DataFrame 时，连接操作非常适用。连接过程可以沿特定轴进行，按行（`axis=0`）或按列（`axis=1`）连接。
- en: Let’s deep dive into the row-wise concatenation, also known as append.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解按行连接，也称为附加（append）。
- en: Row-wise concatenation
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 按行连接
- en: 'The row-wise concatenation is used to concatenate one DataFrame to another
    along `axis=0`. To showcase this operation, two DataFrames, `employee_data_1`
    and `employee_data_2`, created with the same structure but different data can
    be seen here:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 按行连接用于沿 `axis=0` 将一个 DataFrame 连接到另一个 DataFrame。为了展示这个操作，可以看到两个结构相同但数据不同的 DataFrame，`employee_data_1`
    和 `employee_data_2`：
- en: '[PRE43]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let’s perform the row-wise concatenation, as shown in the following code snippet:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行按行连接，如以下代码片段所示：
- en: '[PRE44]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The `pd.concat()` function is used to concatenate the two DataFrames. The first
    argument is a list of DataFrames to concatenate, and the `axis=0` parameter specifies
    that the concatenation should be row-wise, stacking the DataFrames on top of each
    other.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.concat()` 函数用于连接两个 DataFrame。第一个参数是要连接的 DataFrame 列表，`axis=0` 参数指定连接应按行进行，将
    DataFrame 堆叠在一起。'
- en: 'The result can be seen here:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以在这里看到：
- en: '[PRE45]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Some things you need to consider when performing row-wise concatenation are
    as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 执行按行连接时，需要考虑的几点如下：
- en: Ensure that the columns you want to concatenate are aligned correctly. pandas
    will automatically align columns by name and fill any missing columns with `NaN`
    values.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你要连接的列对齐正确。pandas 会自动按列名对齐列，并用 `NaN` 填充任何缺失的列。
- en: 'After concatenation, you may want to reset the index of the resulting DataFrame
    to avoid duplicate index values, especially if the original DataFrames had their
    own range of indices. Observe the index in the following example before performing
    a `reset` operation:'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接后，你可能希望重置结果 DataFrame 的索引，以避免重复的索引值，尤其是当原始 DataFrame 各自有自己的索引范围时。请在执行 `reset`
    操作之前，观察以下示例中的索引：
- en: '[PRE46]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let’s see the output once more:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们再次查看输出：
- en: '[PRE47]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Resetting the index creates a new, continuous index for the concatenated DataFrame.
    The `drop=True` parameter is used to avoid adding the old index as a column in
    the new DataFrame. This step is crucial for maintaining a clean DataFrame, especially
    when the index itself does not carry meaningful data. A continuous index is often
    easier to work with, particularly for indexing, slicing, and potential future
    merges or joins.
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重置索引会为连接后的数据框创建一个新的连续索引。使用`drop=True`参数可以避免将旧索引作为列添加到新数据框中。这个步骤对于保持数据框的整洁至关重要，特别是当索引本身不携带有意义的数据时。一个连续的索引通常更容易操作，尤其是在索引、切片以及未来的合并或连接操作中。
- en: Concatenation can increase the memory usage of your program, especially when
    working with large DataFrames. Be mindful of the available memory resources.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接操作可能会增加程序的内存使用，特别是当处理大型数据框时。需要注意可用的内存资源。
- en: In the next section, we will discuss the column-wise concatenation.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论按列连接。
- en: Column-wise concatenation
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 按列连接
- en: 'Concatenating DataFrames column-wise in pandas involves combining two or more
    DataFrames side by side, aligning them by their index. To showcase this operation,
    the two DataFrames we have been using so far, `employee_data_1` and `employee_data_2`,
    will be used, and the operation can be done as shown here:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas中，按列连接数据框涉及将两个或更多的数据框并排组合，通过索引对齐它们。为了展示这个操作，我们将使用之前的两个数据框，`employee_data_1`和`employee_data_2`，操作可以像这样进行：
- en: '[PRE48]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The `pd.concat()` function is used with the `axis=1` parameter to concatenate
    the DataFrames side by side. This aligns the DataFrames by their index, effectively
    adding new columns from `employee_performance` to `employee_data_1`. This will
    display the following output:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.concat()`函数与`axis=1`参数一起使用，用于并排连接数据框。这通过索引对齐数据框，有效地将`employee_performance`中的新列添加到`employee_data_1`中。输出将显示如下：'
- en: '[PRE49]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Some things you need to consider when performing column-wise concatenation
    are as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行按列连接时，你需要考虑的几个事项如下：
- en: The indices of the DataFrames to be concatenated are aligned properly. When
    concatenating DataFrames column-wise, each row in the resulting DataFrame should
    ideally represent data from the same entity (e.g., the same employee). Misaligned
    indexes can lead to a scenario where data from different entities is erroneously
    combined, leading to inaccurate and misleading results. For example, if the index
    represents employee IDs, misalignment could result in an employee’s details being
    incorrectly paired with another employee’s performance data.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要连接的数据框的索引会被正确对齐。在按列连接数据框时，结果数据框中的每一行应理想地代表来自同一实体的数据（例如，同一员工）。索引未对齐可能导致来自不同实体的数据被错误地组合，从而产生不准确和误导性的结果。例如，如果索引表示员工ID，未对齐可能导致某个员工的详细信息与另一个员工的表现数据错误地配对。
- en: If the DataFrames contain columns with the same name but are intended to be
    distinct, consider renaming these columns before concatenation to avoid confusion
    or errors in the resulting DataFrame.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据框中包含相同名称的列，但这些列打算是不同的，考虑在连接之前重命名这些列，以避免在结果数据框中产生混淆或错误。
- en: While column-wise concatenation typically does not increase memory usage as
    significantly as row-wise concatenation, it is still important to monitor memory
    usage, especially with large DataFrames.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然按列连接通常不像按行连接那样显著增加内存使用，但仍然需要监控内存使用，尤其是对于大型数据框。
- en: Join versus concatenation
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 连接与连接操作的比较
- en: '**Concatenation** is primarily used for combining DataFrames along an axis
    (either rows or columns) without considering the values within. It’s ideal for
    situations where you simply want to stack DataFrames together based on their order
    or extend them with additional columns.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**连接**主要用于沿轴（行或列）组合数据框，而不考虑其中的值。它适用于那些你只是想根据顺序将数据框堆叠在一起或通过附加列扩展它们的情况。'
- en: '**Joins** are used to combine DataFrames based on one or more keys (a common
    identifier in each DataFrame). This is more about merging datasets based on shared
    data points, which allows for more complex and conditional combinations of data.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '**连接**用于根据一个或多个键（每个数据框中的公共标识符）组合数据框。这更多是基于共享数据点合并数据集，允许更复杂和有条件的数据组合。'
- en: Having explored the nuances of concatenation in pandas, including its importance
    for aligning indexes and how it contrasts with join operations, let’s now summarize
    the key points discussed to encapsulate our understanding and highlight the critical
    takeaways from our exploration of DataFrame manipulations in pandas.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了pandas中拼接操作的细节之后，包括它在对齐索引方面的重要性，以及它与连接操作的对比，我们现在总结讨论的关键点，概括我们的理解，并突出我们在探索pandas中DataFrame操作时的关键收获。
- en: Summary
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored various aspects of DataFrame operations in pandas,
    focusing on concatenation, merging, and the importance of managing indexes.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了pandas中DataFrame操作的各个方面，重点讨论了拼接、合并以及索引管理的重要性。
- en: We discussed merging, which is suited for complex combinations based on shared
    keys, offering flexibility through various join types such as inner, outer, left,
    and right joins. We also discussed how concatenation is used to combine DataFrames
    along a specific axis (either row-wise or column-wise) and is particularly useful
    for appending datasets or adding new dimensions to data. The performance implications
    of these operations were discussed, highlighting that proper index management
    can significantly enhance the efficiency of these operations, especially in large
    datasets.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了合并操作，它适用于基于共享键的复杂组合，并通过内连接、外连接、左连接和右连接等多种连接类型提供灵活性。我们还讨论了如何使用拼接操作在特定轴上（按行或按列）合并DataFrame，这对于追加数据集或为数据添加新维度尤其有用。我们还讨论了这些操作的性能影响，强调了正确的索引管理可以显著提升这些操作的效率，特别是在处理大数据集时。
- en: In the upcoming chapter, we will deep dive into how the `groupby` function can
    be leveraged alongside various aggregation functions to extract meaningful insights
    from complex data structures.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入探讨如何利用`groupby`函数与各种聚合函数结合，从复杂的数据结构中提取有意义的洞察。
- en: References
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[https://github.com/pandas-dev/pandas/issues/38418](https://github.com/pandas-dev/pandas/issues/38418)'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://github.com/pandas-dev/pandas/issues/38418](https://github.com/pandas-dev/pandas/issues/38418)'
- en: '[https://realpython.com/pandas-merge-join-and-concat/](https://realpython.com/pandas-merge-join-and-concat/)'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://realpython.com/pandas-merge-join-and-concat/](https://realpython.com/pandas-merge-join-and-concat/)'
- en: '[https://datascience.stackexchange.com/questions/44476/merging-dataframes-in-pandas-is-taking-a-surprisingly-long-time](https://datascience.stackexchange.com/questions/44476/merging-dataframes-in-pandas-is-taking-a-surprisingly-long-time)'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://datascience.stackexchange.com/questions/44476/merging-dataframes-in-pandas-is-taking-a-surprisingly-long-time](https://datascience.stackexchange.com/questions/44476/merging-dataframes-in-pandas-is-taking-a-surprisingly-long-time)'
- en: '[https://stackoverflow.com/questions/40860457/improve-pandas-merge-performance](https://stackoverflow.com/questions/40860457/improve-pandas-merge-performance)'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/questions/40860457/improve-pandas-merge-performance](https://stackoverflow.com/questions/40860457/improve-pandas-merge-performance)'
- en: '[https://www.youtube.com/watch?v=P6hSBrxs0Eg](https://www.youtube.com/watch?v=P6hSBrxs0Eg)'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=P6hSBrxs0Eg](https://www.youtube.com/watch?v=P6hSBrxs0Eg)'
- en: '[https://pandas.pydata.org/pandas-docs/version/1.5.1/user_guide/merging.html](https://pandas.pydata.org/pandas-docs/version/1.5.1/user_guide/merging.html)'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://pandas.pydata.org/pandas-docs/version/1.5.1/user_guide/merging.html](https://pandas.pydata.org/pandas-docs/version/1.5.1/user_guide/merging.html)'
- en: '[https://pandas.pydata.org/pandas-docs/version/0.20/merging.html](https://pandas.pydata.org/pandas-docs/version/0.20/merging.html)'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://pandas.pydata.org/pandas-docs/version/0.20/merging.html](https://pandas.pydata.org/pandas-docs/version/0.20/merging.html)'
