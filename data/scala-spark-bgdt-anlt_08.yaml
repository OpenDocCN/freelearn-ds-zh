- en: Introduce a Little Structure - Spark SQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入一点结构 - Spark SQL
- en: '"One machine can do the work of fifty ordinary men. No machine can do the work
    of one extraordinary man."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “一台机器可以完成五十个普通人的工作。没有一台机器可以完成一个非凡人的工作。”
- en: '- Elbert Hubbard'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- Elbert Hubbard'
- en: 'In this chapter, you will learn how to use Spark for the analysis of structured
    data (unstructured data, such as a document containing arbitrary text or some
    other format has to be transformed into a structured form); we will see how DataFrames/datasets
    are the corner stone here, and how Spark SQL''s APIs make querying structured
    data simple yet robust. Moreover, we introduce datasets and see the difference
    between datasets, DataFrames, and RDDs. In a nutshell, the following topics will
    be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何使用Spark分析结构化数据（非结构化数据，例如包含任意文本或其他格式的文档必须转换为结构化形式）；我们将看到DataFrames/datasets在这里是基石，以及Spark
    SQL的API如何使查询结构化数据变得简单而强大。此外，我们将介绍数据集，并看到数据集、DataFrames和RDD之间的区别。简而言之，本章将涵盖以下主题：
- en: Spark SQL and DataFrames
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL和DataFrames
- en: DataFrame and SQL API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame和SQL API
- en: DataFrame schema
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame模式
- en: datasets and encoders
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集和编码器
- en: Loading and saving data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载和保存数据
- en: Aggregations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合
- en: Joins
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接
- en: Spark SQL and DataFrames
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL和DataFrames
- en: Before Apache Spark, Apache Hive was the go-to technology whenever anyone wanted
    to run an SQL-like query on a large amount of data. Apache Hive essentially translated
    SQL queries into MapReduce-like, like logic, automatically making it very easy
    to perform many kinds of analytics on big data without actually learning to write
    complex code in Java and Scala.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在Apache Spark之前，每当有人想在大量数据上运行类似SQL的查询时，Apache Hive都是首选技术。Apache Hive基本上将SQL查询转换为类似MapReduce的逻辑，自动使得在大数据上执行许多种类的分析变得非常容易，而无需实际学习如何用Java和Scala编写复杂的代码。
- en: With the advent of Apache Spark, there was a paradigm shift in how we can perform
    analysis on big data scale. Spark SQL provides an easy-to-use SQL-like layer on
    top of Apache Spark's distributed computation abilities. In fact, Spark SQL can
    be used as an online analytical processing database.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Apache Spark的出现，我们在大数据规模上执行分析的方式发生了范式转变。Spark SQL在Apache Spark的分布式计算能力之上提供了一个易于使用的类似SQL的层。事实上，Spark
    SQL可以用作在线分析处理数据库。
- en: '![](img/00297.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00297.jpeg)'
- en: Spark SQL works by parsing the SQL-like statement into an **Abstract Syntax
    Tree** (**AST**), subsequently converting that plan to a logical plan and then
    optimizing the logical plan into a physical plan that can be executed. The final
    execution uses the underlying DataFrame API, making it very easy for anyone to
    use DataFrame APIs by simply using an SQL-like interface rather than learning
    all the internals. Since this book dives into technical details of various APIs,
    we will primarily cover the DataFrame APIs, showing Spark SQL API in some places
    to contrast the different ways of using the APIs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL通过将类似SQL的语句解析为**抽象语法树**（**AST**）来工作，随后将该计划转换为逻辑计划，然后将逻辑计划优化为可以执行的物理计划。最终的执行使用底层的DataFrame
    API，使任何人都可以通过简单地使用类似SQL的接口而不是学习所有内部细节来使用DataFrame API。由于本书深入探讨了各种API的技术细节，我们将主要涵盖DataFrame
    API，并在某些地方展示Spark SQL API，以对比使用API的不同方式。
- en: Thus, DataFrame API is the underlying layer beneath Spark SQL. In this chapter,
    we will show you how to create DataFrames using various techniques, including
    SQL queries and performing operations on the DataFrames.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，DataFrame API是Spark SQL下面的基础层。在本章中，我们将向您展示如何使用各种技术创建DataFrames，包括SQL查询和对DataFrames执行操作。
- en: A DataFrame is an abstraction of the **Resilient Distributed dataset** (**RDD**),
    dealing with higher level functions optimized using catalyst optimizer and also
    highly performant via the Tungsten Initiative. You can think of a dataset as an
    efficient table of an RDD with heavily optimized binary representation of the
    data. The binary representation is achieved using encoders, which serializes the
    various objects into a binary structure for much better performance than RDD representation.
    Since DataFrames uses the RDD internally anyway, a DataFrame/dataset is also distributed
    exactly like an RDD, and thus is also a distributed dataset. Obviously, this also
    means datasets are immutable.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame是**弹性分布式数据集**（**RDD**）的抽象，处理使用catalyst优化器优化的更高级函数，并且通过Tungsten计划也非常高效。您可以将数据集视为具有经过高度优化的数据的RDD的有效表。使用编码器实现了数据的二进制表示，编码器将各种对象序列化为二进制结构，比RDD表示具有更好的性能。由于DataFrames内部使用RDD，因此DataFrame/数据集也像RDD一样分布，因此也是分布式数据集。显然，这也意味着数据集是不可变的。
- en: 'The following is an illustration of the binary representation of data:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是数据的二进制表示的示例：
- en: '![](img/00037.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00037.jpeg)'
- en: datasets were added in Spark 1.6 and provide the benefits of strong typing on
    top of DataFrames. In fact, since Spark 2.0, the DataFrame is simply an alias
    of a dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在Spark 1.6中添加，并在DataFrames之上提供了强类型的好处。事实上，自Spark 2.0以来，DataFrame只是数据集的别名。
- en: '`org.apache.spark.sql` defines type `DataFrame` as a `dataset[Row]`, which
    means that most of the APIs will work well with both datasets and `DataFrames`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`org.apache.spark.sql`定义类型`DataFrame`为`dataset[Row]`，这意味着大多数API将与数据集和`DataFrames`一起很好地工作'
- en: '**type DataFrame = dataset[Row]**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型DataFrame = dataset[Row]**'
- en: A DataFrame is conceptually similar to a table in a Relational Database. Hence,
    a DataFrame contains rows of data, with each row comprised of several columns.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame在概念上类似于关系数据库中的表。因此，DataFrame包含数据行，每行由多个列组成。
- en: One of the first things we need to keep in mind is that, just like RDDs, DataFrames
    are immutable. This property of DataFrames being immutable means every transformation
    or action creates a new DataFrame.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要牢记的第一件事就是，与RDD一样，DataFrames是不可变的。DataFrames具有不可变性的属性意味着每次转换或操作都会创建一个新的DataFrame。
- en: '![](img/00034.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00034.jpeg)'
- en: Let's start looking more into DataFrames and how they are different from RDDs.
    RDD's, as seen before, represent a low-level API of data manipulation in Apache
    Spark. The DataFrames were created on top of RDDs to abstract the low-level inner
    workings of RDDs and expose high-level APIs, which are easier to use and provide
    a lot of functionality out-of-the-box. DataFrame was created by following similar
    concepts found in the Python pandas package, R language, Julia language, and so
    on.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解DataFrame以及它们与RDD的不同之处。如前所述，RDD代表Apache Spark中数据操作的低级API。DataFrame是在RDD的基础上创建的，以抽象出RDD的低级内部工作，并公开易于使用且提供大量功能的高级API。DataFrame是通过遵循Python
    pandas包、R语言、Julia语言等中发现的类似概念创建的。
- en: As we mentioned before, DataFrames translate the SQL code and domain specific
    language expressions into optimized execution plans to be run on top of Spark
    Core APIs in order for the SQL statements to perform a wide variety of operations.
    DataFrames support many different types of input data sources and many types of
    operations. These includes all types of SQL operations, such as joins, group by,
    aggregations, and window functions, as most of the databases. Spark SQL is also
    quite similar to the Hive query language, and since Spark provides a natural adapter
    to Apache Hive, users who have been working in Apache Hive can easily transfer
    their knowledge, applying it to Spark SQL, thus minimizing the transition time.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，DataFrame将SQL代码和特定领域语言表达式转换为优化的执行计划，以在Spark Core API之上运行SQL语句执行各种操作。DataFrame支持许多不同类型的输入数据源和许多类型的操作。这包括所有类型的SQL操作，例如连接、分组、聚合和窗口函数，就像大多数数据库一样。Spark
    SQL也与Hive查询语言非常相似，由于Spark提供了与Apache Hive的自然适配器，因此在Apache Hive中工作的用户可以轻松将其知识转移到Spark
    SQL中，从而最小化过渡时间。
- en: DataFrames essentially depend on the concept of a table, as seen previously.
    The table can be operated on very similar to how Apache Hive works. In fact, many
    of the operations on the tables in Apache Spark are similar to how Apache Hive
    handles tables and operates on those tables. Once you have a table that is the
    DataFrame, the DataFrame can be registered as a table and you can operate on the
    data using Spark SQL statements in lieu of DataFrame APIs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame基本上依赖于表的概念，如前所述。表可以操作得非常类似于Apache Hive的工作方式。实际上，Apache Spark中表的许多操作与Apache
    Hive处理表和对这些表进行操作的方式非常相似。一旦有了作为DataFrame的表，就可以将DataFrame注册为表，并且可以使用Spark SQL语句操作数据，而不是使用DataFrame
    API。
- en: DataFrames depend on the catalyst optimizer and the Tungsten performance improvements,
    so let's briefly examine how catalyst optimizer works. A catalyst optimizer creates
    a parsed logical plan from the input SQL and then analyzes the logical plan by
    looking at all the various attributes and columns used in the SQL statement. Once
    the analyzed logical plan is created, catalyst optimizer further tries to optimize
    the plan by combining several operations and also rearranging the logic to get
    better performance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame依赖于催化剂优化器和Tungsten性能改进，因此让我们简要地了解一下催化剂优化器的工作原理。催化剂优化器从输入SQL创建解析的逻辑计划，然后通过查看SQL语句中使用的所有各种属性和列来分析逻辑计划。一旦创建了分析的逻辑计划，催化剂优化器进一步尝试通过组合多个操作和重新排列逻辑来优化计划以获得更好的性能。
- en: In order to understand the catalyst optimizer, think about it as a common sense
    logic Optimizer which can reorder operations such as filters and transformations,
    sometimes grouping several operations into one so as to minimize the amount of
    data that is shuffled across the worker nodes. For example, catalyst optimizer
    may decide to broadcast the smaller datasets when performing joint operations
    between different datasets. Use explain to look at the execution plan of any data
    frame. The catalyst optimizer also computes statistics of the DataFrame's columns
    and partitions, improving the speed of execution.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解催化剂优化器，可以将其视为一种常识逻辑优化器，可以重新排序操作，例如过滤和转换，有时将几个操作组合成一个，以便最小化在工作节点之间传输的数据量。例如，催化剂优化器可能决定在执行不同数据集之间的联接操作时广播较小的数据集。使用explain查看任何数据框的执行计划。催化剂优化器还计算DataFrame的列和分区的统计信息，提高执行速度。
- en: For example, if there are transformations and filters on the data partitions,
    then the order in which we filter data and apply transformations matters a lot
    to the overall performance of the operations. As a result of all the optimizations,
    the optimized logical plan is generated, which is then converted into a physical
    plan. Obviously, several physical plans are possibilities to execute the same
    SQL statement and generate the same result. The cost optimization logic determines
    and picks a good physical plan, based on cost optimizations and estimations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果数据分区上有转换和过滤器，那么过滤数据和应用转换的顺序对操作的整体性能非常重要。由于所有优化的结果，生成了优化的逻辑计划，然后将其转换为物理计划。显然，有几种物理计划可以执行相同的SQL语句并生成相同的结果。成本优化逻辑根据成本优化和估算确定并选择一个良好的物理计划。
- en: Tungsten performance improvements are another key ingredient in the secret sauce
    behind the phenomenal performance improvements offered by Spark 2.x compared to
    the previous releases, such as Spark 1.6 and older. Tungsten implements a complete
    overhaul of memory management and other performance improvements. Most important
    memory management improvements use binary encoding of the objects and referencing
    them in both off-heap and on-heap memory. Thus, Tungsten allows the usage of office
    heap memory using the binary encoding mechanism to encode all the objects. Binary
    encoded objects take up much less memory. Project Tungsten also improves shuffle
    performance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 钨性能改进是Spark 2.x背后的秘密酱的另一个关键因素，与之前的版本（如Spark 1.6和更早版本）相比，它提供了卓越的性能改进。钨实现了对内存管理和其他性能改进的彻底改革。最重要的内存管理改进使用对象的二进制编码，并在堆外和堆内存中引用它们。因此，钨允许使用二进制编码机制来编码所有对象的堆外内存。二进制编码的对象占用的内存要少得多。Tungsten项目还改进了洗牌性能。
- en: The data is typically loaded into DataFrames through the `DataFrameReader`,
    and data is saved from DataFrames through `DataFrameWriter`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通常通过`DataFrameReader`加载到DataFrame中，并且数据通过`DataFrameWriter`保存。
- en: DataFrame API and SQL API
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame API和SQL API
- en: 'The creation of a DataFrame can be done in several ways:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过多种方式创建DataFrame：
- en: By executing SQL queries
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过执行SQL查询
- en: Loading external data such as Parquet, JSON, CSV, text, Hive, JDBC, and so on
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载Parquet、JSON、CSV、文本、Hive、JDBC等外部数据
- en: Converting RDDs to data frames
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将RDD转换为数据框
- en: A DataFrame can be created by loading a CSV file. We will look at a CSV `statesPopulation.csv`,
    which is being loaded as a DataFrame.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过加载CSV文件来创建DataFrame。我们将查看一个名为`statesPopulation.csv`的CSV文件，它被加载为DataFrame。
- en: The CSV has the following format of US states populations from years 2010 to
    2016.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: CSV文件具有2010年至2016年美国各州人口的以下格式。
- en: '| **State** | **Year** | **Population** |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **州** | **年份** | **人口** |'
- en: '| Alabama | 2010 | 4785492 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 阿拉巴马州 | 2010 | 4785492 |'
- en: '| Alaska | 2010 | 714031 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 阿拉斯加州 | 2010 | 714031 |'
- en: '| Arizona | 2010 | 6408312 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 亚利桑那州 | 2010 | 6408312 |'
- en: '| Arkansas | 2010 | 2921995 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 阿肯色州 | 2010 | 2921995 |'
- en: '| California | 2010 | 37332685 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 加利福尼亚州 | 2010 | 37332685 |'
- en: Since this CSV has a header, we can use it to quickly load into a DataFrame
    with an implicit schema detection.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此CSV具有标题，因此我们可以使用它快速加载到具有隐式模式检测的DataFrame中。
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the DataFrame is loaded, it can be examined for the schema:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 加载DataFrame后，可以检查其模式：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`option("header", "true").option("inferschema", "true").option("sep", ",")`
    tells Spark that the CSV has a `header`; a comma separator is used to separate
    the fields/columns and also that schema can be inferred implicitly.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`option("header", "true").option("inferschema", "true").option("sep", ",")`
    告诉Spark CSV文件有`header`；逗号分隔符用于分隔字段/列，还可以隐式推断模式。'
- en: DataFrame works by parsing the logical plan, analyzing the logical plan, optimizing
    the plan, and then finally executing the physical plan of execution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame通过解析逻辑计划、分析逻辑计划、优化计划，最后执行执行物理计划来工作。
- en: 'Using explain on DataFrame shows the plan of execution:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DataFrame上的explain显示执行计划：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A DataFrame can also be registered as a table name (shown as follows), which
    will then allow you to type SQL statements like a relational Database.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame还可以注册为表名（如下所示），然后您可以像关系数据库一样输入SQL语句。
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once we have the DataFrame as a structured DataFrame or a table, we can run
    commands to operate on the data:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将DataFrame作为结构化DataFrame或表，我们可以运行命令来操作数据：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you see in the preceding piece of code, we have written an SQL-like statement
    and executed it using `spark.sql` API.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看到上述代码片段，我们已经编写了类似SQL的语句，并使用`spark.sql` API执行了它。
- en: Note that the Spark SQL is simply converted to the DataFrame API for execution
    and the SQL is only a DSL for ease of use.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Spark SQL只是转换为DataFrame API以进行执行，SQL只是用于方便使用的DSL。
- en: Using the `sort` operation on the DataFrame, you can order the rows in the DataFrame
    by any column. We see the effect of descending `sort` using the `Population` column
    as follows. The rows are ordered by the Population in a descending order.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DataFrame上的`sort`操作，可以按任何列对DataFrame中的行进行排序。我们可以看到使用`Population`列进行降序`sort`的效果如下。行按人口数量降序排序。
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Using `groupBy` we can group the DataFrame by any column. The following is the
    code to group the rows by `State` and then add up the `Population` counts for
    each `State`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`groupBy`可以按任何列对DataFrame进行分组。以下是按`State`分组行，然后对每个`State`的`Population`计数进行求和的代码。
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using the `agg` operation, you can perform many different operations on columns
    of the DataFrame, such as finding the `min`, `max`, and `avg` of a column. You
    can also perform the operation and rename the column at the same time to suit
    your use case.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`agg`操作，您可以对DataFrame的列执行许多不同的操作，例如查找列的`min`、`max`和`avg`。您还可以执行操作并同时重命名列，以适应您的用例。
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Naturally, the more complicated the logic gets, the execution plan also gets
    more complicated. Let''s look at the plan for the preceding operation of `groupBy`
    and `agg` API invocations to better understand what is really going on under the
    hood. The following is the code showing the execution plan of the group by and
    summation of population per `State`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，逻辑越复杂，执行计划也越复杂。让我们看看`groupBy`和`agg` API调用的执行计划，以更好地了解底层发生了什么。以下是显示按`State`分组和人口总和的执行计划的代码：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: DataFrame operations can be chained together very well so that the execution
    takes advantage of the cost optimization (Tungsten performance improvements and
    catalyst optimizer working together).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame操作可以很好地链接在一起，以便执行可以利用成本优化（钨性能改进和催化剂优化器共同工作）。
- en: 'We can also chain the operations together in a single statement, as shown as
    follows, where we not only group the data by `State` column and then sum the `Population`
    value, but also sort the DataFrame by the summation column:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将操作链接在一条语句中，如下所示，我们不仅按`State`列对数据进行分组，然后对`Population`值进行求和，还对DataFrame进行排序：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding chained operation consists of multiple transformations and actions,
    which can be visualized using the following diagram:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的链式操作包括多个转换和操作，可以使用以下图表进行可视化：
- en: '![](img/00042.jpeg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00042.jpeg)'
- en: 'It''s also possible to create multiple aggregations at the same time, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以同时创建多个聚合，如下所示：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Pivots
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 旋转
- en: Pivoting is a great way of transforming the table to create a different view,
    more suitable to doing many summarizations and aggregations. This is accomplished
    by taking the values of a column and making each of the values an actual column.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转是将表转换为不同视图的一种很好的方式，更适合进行许多汇总和聚合。这是通过取列的值并使每个值成为实际列来实现的。
- en: To understand this better, let's pivot the rows of the DataFrame by `Year` and
    examine the result, which shows that, now, the column `Year` created several new
    columns by converting each unique value into an actual column. The end result
    of this is that, now, instead of just looking at year columns, we can use the
    per year columns created to summarize and aggregate by `Year`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，让我们通过`Year`来旋转DataFrame的行并检查结果，结果显示，现在，列`Year`通过将每个唯一值转换为实际列创建了几个新列。这样做的最终结果是，现在，我们不仅可以查看年份列，还可以使用按年份创建的列来进行汇总和聚合。
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Filters
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过滤器
- en: DataFrame also supports Filters, which can be used to quickly filter the DataFrame
    rows to generate new DataFrames. The Filters enable very important transformations
    of the data to narrow down the DataFrame to our use case. For example, if all
    you want is to analyze the state of California, then using `filter` API performs
    the elimination of non-matching rows on every partition of data, thus improving
    the performance of the operations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame还支持过滤器，可以用于快速过滤DataFrame行以生成新的DataFrame。过滤器使得数据的重要转换变得非常重要，可以将DataFrame缩小到我们的用例。例如，如果您只想分析加利福尼亚州的情况，那么使用`filter`
    API可以在每个数据分区上消除不匹配的行，从而提高操作的性能。
- en: Let's look at the execution plan for the filtering of the DataFrame to only
    consider the state of California.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看过滤DataFrame以仅考虑加利福尼亚州的执行计划。
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now that we can seen the execution plan, let''s now execute the `filter` command,
    as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到执行计划，让我们执行`filter`命令，如下所示：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: User-Defined Functions (UDFs)
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户定义函数（UDFs）
- en: UDFs define new column-based functions that extend the functionality of Spark
    SQL. Often, the inbuilt functions provided in Spark do not handle the exact need
    we have. In such cases, Apache Spark supports the creation of UDFs, which can
    be used.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: UDFs定义了扩展Spark SQL功能的新基于列的函数。通常，Spark提供的内置函数不能处理我们确切的需求。在这种情况下，Apache Spark支持创建可以使用的UDF。
- en: '`udf()` internally calls a case class User-Defined Function, which itself calls
    ScalaUDF internally.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`udf()`在内部调用一个案例类用户定义函数，它本身在内部调用ScalaUDF。'
- en: Let's go through an example of an UDF which simply converts State column values
    to uppercase.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单将State列值转换为大写的UDF示例来进行说明。
- en: First, we create the function we need in Scala.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在Scala中创建我们需要的函数。
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Then, we have to encapsulate the created function inside the `udf` to create
    the UDF.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须将创建的函数封装在`udf`中以创建UDF。
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now that we have created the `udf`, we can use it to convert the State column
    to uppercase.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了`udf`，我们可以使用它将State列转换为大写。
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Schema   structure of data
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的模式结构
- en: A schema is the description of the structure of your data and can be either
    Implicit or Explicit.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 模式是数据结构的描述，可以是隐式的或显式的。
- en: Since the DataFrames are internally based on the RDD, there are two main methods
    of converting existing RDDs into datasets. An RDD can be converted into a dataset
    by using reflection to infer the schema of the RDD. A second method for creating
    datasets is through a programmatic interface, using which you can take an existing
    RDD and provide a schema to convert the RDD into a dataset with schema.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DataFrame在内部基于RDD，因此有两种将现有RDD转换为数据集的主要方法。可以使用反射将RDD转换为数据集，以推断RDD的模式。创建数据集的第二种方法是通过编程接口，使用该接口可以获取现有RDD并提供模式以将RDD转换为具有模式的数据集。
- en: In order to create a DataFrame from an RDD by inferring the schema using reflection,
    the Scala API for Spark provides case classes which can be used to define the
    schema of the table. The DataFrame is created programmatically from the RDD, because
    the case classes are not easy to use in all cases. For instance, creating a case
    classes on a 1000 column table is time consuming.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过反射推断模式从RDD创建DataFrame，Spark的Scala API提供了可以用来定义表模式的案例类。DataFrame是通过RDD以编程方式创建的，因为在所有情况下都不容易使用案例类。例如，在1000列表上创建案例类是耗时的。
- en: Implicit schema
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐式模式
- en: Let us look at an example of loading a **CSV** (c**omma-separated Values**)
    file into a DataFrame. Whenever a text file contains a header, read API can infer
    the schema by reading the header line. We also have the option to specify the
    separator to be used to split the text file lines.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个将**CSV**（逗号分隔值）文件加载到DataFrame中的示例。每当文本文件包含标题时，读取API可以通过读取标题行来推断模式。我们还可以选择指定用于拆分文本文件行的分隔符。
- en: We read the `csv` inferring the schema from the header line and uses comma (`,`)
    as the separator. We also show use of `schema` command and `printSchema` command
    to verify the schema of the input file.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从标题行推断模式读取`csv`并使用逗号（`,`）作为分隔符。我们还展示了`schema`命令和`printSchema`命令来验证输入文件的模式。
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Explicit schema
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显式模式
- en: A schema is described using `StructType`, which is a collection of `StructField`
    objects.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`StructType`来描述模式，它是`StructField`对象的集合。
- en: '`StructType` and `StructField` belong to the `org.apache.spark.sql.types` package.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`StructType`和`StructField`属于`org.apache.spark.sql.types`包。'
- en: DataTypes such as `IntegerType`, `StringType` also belong to the `org.apache.spark.sql.types`
    package.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如`IntegerType`、`StringType`之类的数据类型也属于`org.apache.spark.sql.types`包。
- en: Using these imports, we can define a custom explicit schema.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些导入，我们可以定义一个自定义的显式模式。
- en: 'First, import the necessary classes:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入必要的类：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Define a schema with two columns/fields-an `Integer` followed by a `String`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个包含两列/字段的模式-一个`Integer`，后面是一个`String`：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It''s easy to print the newly created `schema`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 打印新创建的`schema`很容易：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'There is also an option to print JSON, which is as follows, using `prettyJson`
    function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个选项可以打印JSON，如下所示，使用`prettyJson`函数：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'All the data types of Spark SQL are located in the package `org.apache.spark.sql.types`.
    You can access them by doing:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL的所有数据类型都位于包`org.apache.spark.sql.types`中。您可以通过以下方式访问它们：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Encoders
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Encoders
- en: Spark 2.x supports a different way of defining schema for complex data types.
    First, let's look at a simple example.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.x支持一种不同的方式来定义复杂数据类型的模式。首先，让我们来看一个简单的例子。
- en: 'Encoders must be imported using the import statement in order for you to use
    Encoders:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用Encoders，必须使用import语句导入Encoders：
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s look at a simple example of defining a tuple as a data type to be used
    in the dataset APIs:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个简单的例子，定义一个元组作为数据类型在数据集API中使用：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The preceding code looks complicated to use all the time, so we can also define
    a case class for our need and then use it. We can define a case class `Record`
    with two fields-an `Integer` and a `String`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码看起来在任何时候都很复杂，所以我们也可以为我们的需求定义一个案例类，然后使用它。我们可以定义一个名为`Record`的案例类，有两个字段-一个`Integer`和一个`String`：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Using `Encoders` , we can easily create a `schema` on top of the case class,
    thus allowing us to use the various APIs with ease:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Encoders`，我们可以轻松地在案例类之上创建一个`schema`，从而使我们能够轻松使用各种API：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'All the data types of Spark SQL are located in the package **`org.apache.spark.sql.types`**.
    You can access them by doing:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL的所有数据类型都位于包**`org.apache.spark.sql.types`**中。您可以通过以下方式访问它们：
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should use the `DataTypes` object in your code to create complex Spark
    SQL types such as arrays or maps, as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该在代码中使用`DataTypes`对象来创建复杂的Spark SQL类型，如数组或映射，如下所示：
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following are the data types supported in Spark SQL APIs:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Spark SQL API中支持的数据类型：
- en: '| **Data type** | **Value type in Scala** | **API to access or create a data
    type** |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **数据类型** | **Scala中的值类型** | **访问或创建数据类型的API** |'
- en: '| `ByteType` | `Byte` | `ByteType` |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| `ByteType` | `Byte` | `ByteType` |'
- en: '| `ShortType` | `Short` | `ShortType` |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| `ShortType` | `Short` | `ShortType` |'
- en: '| `IntegerType` | `Int` | `IntegerType` |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| `IntegerType` | `Int` | `IntegerType` |'
- en: '| `LongType` | `Long` | `LongType` |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| `LongType` | `Long` | `LongType` |'
- en: '| `FloatType` | `Float` | `FloatType` |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| `FloatType` | `Float` | `FloatType` |'
- en: '| `DoubleType` | `Double` | `DoubleType` |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| `DoubleType` | `Double` | `DoubleType` |'
- en: '| `DecimalType` | `java.math.BigDecimal` | `DecimalType` |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| `DecimalType` | `java.math.BigDecimal` | `DecimalType` |'
- en: '| `StringType` | `String` | `StringType` |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| `StringType` | `String` | `StringType` |'
- en: '| `BinaryType` | `Array[Byte]` | `BinaryType` |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| `BinaryType` | `Array[Byte]` | `BinaryType` |'
- en: '| `BooleanType` | `Boolean` | `BooleanType` |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| `BooleanType` | `Boolean` | `BooleanType` |'
- en: '| `TimestampType` | `java.sql.Timestamp` | `TimestampType` |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| `TimestampType` | `java.sql.Timestamp` | `TimestampType` |'
- en: '| `DateType` | `java.sql.Date` | `DateType` |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| `DateType` | `java.sql.Date` | `DateType` |'
- en: '| `ArrayType` | `scala.collection.Seq` | `ArrayType(elementType, [containsNull])`
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| `ArrayType` | `scala.collection.Seq` | `ArrayType(elementType, [containsNull])`
    |'
- en: '| `MapType` | `scala.collection.Map` | `MapType(keyType, valueType, [valueContainsNull])`
    Note: The default value of `valueContainsNull` is `true`. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| `MapType` | `scala.collection.Map` | `MapType(keyType, valueType, [valueContainsNull])`
    注意：`valueContainsNull`的默认值为`true`。|'
- en: '| `StructType` | `org.apache.spark.sql.Row` | `StructType(fields)` Note: fields
    is a `Seq` of `StructFields`. Also, two fields with the same name are not allowed.
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| `StructType` | `org.apache.spark.sql.Row` | `StructType(fields)` 注意：fields是`StructFields`的`Seq`。另外，不允许有相同名称的两个字段。|'
- en: Loading and saving datasets
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载和保存数据集
- en: We need to have data read into the cluster as input and output or results written
    back to the storage to do anything practical with our code. Input data can be
    read from a variety of datasets and sources such as Files, Amazon S3 storage,
    Databases, NoSQLs, and Hive, and the output can similarly also be saved to Files,
    S3, Databases, Hive, and so on.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将数据读入集群作为输入和输出，或者将结果写回存储，以便对我们的代码进行任何实际操作。输入数据可以从各种数据集和来源中读取，如文件、Amazon
    S3存储、数据库、NoSQL和Hive，输出也可以类似地保存到文件、S3、数据库、Hive等。
- en: Several systems have support for Spark via a connector, and this number is growing
    day by day as more systems are latching onto the Spark processing framework.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 几个系统通过连接器支持Spark，并且随着更多系统接入Spark处理框架，这个数字正在日益增长。
- en: Loading datasets
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集
- en: Spark SQL can read data from external storage systems such as files, Hive tables,
    and JDBC databases through the `DataFrameReader` interface.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL可以通过`DataFrameReader`接口从外部存储系统，如文件、Hive表和JDBC数据库中读取数据。
- en: The format of the API call is `spark.read.inputtype`
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: API调用的格式是`spark.read.inputtype`
- en: Parquet
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet
- en: CSV
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV
- en: Hive Table
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive表
- en: JDBC
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDBC
- en: ORC
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ORC
- en: Text
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本
- en: JSON
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON
- en: 'Let''s look at a couple of simple examples of reading CSV files into DataFrames:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些简单的例子，将CSV文件读入DataFrame中：
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Saving datasets
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存数据集
- en: Spark SQL can save data to external storage systems such as files, Hive tables
    and JDBC databases through `DataFrameWriter` interface.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL可以将数据保存到外部存储系统，如文件、Hive表和JDBC数据库，通过`DataFrameWriter`接口。
- en: The format of the API call is `dataframe``.write.outputtype`
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: API调用的格式是`dataframe``.write.outputtype`
- en: Parquet
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet
- en: ORC
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ORC
- en: Text
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本
- en: Hive table
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive表
- en: JSON
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON
- en: CSV
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV
- en: JDBC
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDBC
- en: 'Let''s look at a couple of examples of writing or saving a DataFrame to a CSV
    file:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些将DataFrame写入或保存到CSV文件的例子：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Aggregations
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合
- en: Aggregation is the method of collecting data based on a condition and performing
    analytics on the data. Aggregation is very important to make sense of data of
    all sizes, as just having raw records of data is not that useful for most use
    cases.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合是根据条件收集数据并对数据进行分析的方法。聚合对于理解各种规模的数据非常重要，因为仅仅拥有原始数据记录对于大多数用例来说并不那么有用。
- en: For example, if you look at the following table and then the aggregated view,
    it is obvious that just raw records do not help you understand the data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你看下面的表，然后看聚合视图，很明显，仅仅原始记录并不能帮助你理解数据。
- en: Imagine a table containing one temperature measurement per day for every city
    in the world for five years.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个包含世界上每个城市每天的一次温度测量的表，为期五年。
- en: 'Shown in the following is a table containing records of average temperature
    per day per city:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个包含每个城市每天平均温度记录的表：
- en: '| **City** | **Date** | **Temperature** |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '**城市** | **日期** | **温度**'
- en: '| Boston | 12/23/2016 | 32 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Boston | 12/23/2016 | 32 |'
- en: '| New York | 12/24/2016 | 36 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| New York | 12/24/2016 | 36 |'
- en: '| Boston | 12/24/2016 | 30 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Boston | 12/24/2016 | 30 |'
- en: '| Philadelphia | 12/25/2016 | 34 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Philadelphia | 12/25/2016 | 34 |'
- en: '| Boston | 12/25/2016 | 28 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Boston | 12/25/2016 | 28 |'
- en: 'If we want to compute the average temperature per city for all the days we
    have measurements for in the above table, we can see results which look similar
    to the following table:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要计算上表中我们有测量数据的所有天的每个城市的平均温度，我们可以看到类似以下表的结果：
- en: '| **City** | **Average Temperature** |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '**城市** | **平均温度**'
- en: '| Boston | 30 - *(32 + 30 + 28)/3* |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Boston | 30 - *(32 + 30 + 28)/3* |'
- en: '| New York | 36 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| New York | 36 |'
- en: '| Philadelphia | 34 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Philadelphia | 34 |'
- en: Aggregate functions
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合函数
- en: Most aggregations can be done using functions that can be found in the `org.apache.spark.sql.functions`
    package. In addition, custom aggregation functions can also be created, also known
    as **User Defined Aggregation Functions** (**UDAF**).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数聚合可以使用`org.apache.spark.sql.functions`包中的函数来完成。此外，还可以创建自定义聚合函数，也称为**用户定义的聚合函数**（**UDAF**）。
- en: Each grouping operation returns a `RelationalGroupeddataset`, on which you can
    specify aggregations.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分组操作都返回一个`RelationalGroupeddataset`，您可以在其中指定聚合。
- en: 'We will load the sample data to illustrate all the different types of aggregate
    functions in this section:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载示例数据，以说明本节中所有不同类型的聚合函数：
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Count
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计数
- en: Count is the most basic aggregate function, which simply counts the number of
    rows for the column specified. An extension is the `countDistinct`, which also
    eliminates duplicates.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 计数是最基本的聚合函数，它只是计算指定列的行数。扩展是`countDistinct`，它还可以消除重复项。
- en: 'The `count` API has several implementations, as follows. The exact API used
    depends on the specific use case:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`count` API有几种实现，如下所示。使用的确切API取决于特定的用例：'
- en: '[PRE32]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s look at examples of invoking `count` and `countDistinct` on the DataFrame
    to print the row counts:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在DataFrame上调用`count`和`countDistinct`来打印行计数：
- en: '[PRE33]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: First
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 首先
- en: Gets the first record in the `RelationalGroupeddataset.`
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 获取`RelationalGroupeddataset`中的第一条记录。
- en: 'The `first` API has several implementations, as follows. The exact API used
    depends on the specific use case:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`first` API有几种实现，如下所示。使用的确切API取决于特定的用例：'
- en: '[PRE34]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s look at an example of invoking `first` on the DataFrame to output the
    first row:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`first`来输出第一行的例子：
- en: '[PRE35]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Last
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后
- en: Gets the last record in the `RelationalGroupeddataset`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 获取`RelationalGroupeddataset`中的最后一条记录。
- en: 'The `last` API has several implementations, as follows. The exact API used
    depends on the specific use case:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`last` API有几种实现，如下所示。使用的确切API取决于特定的用例：'
- en: '[PRE36]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Let's look at an example of invoking `last` on the DataFrame to output the last
    row.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`last`来输出最后一行的例子。
- en: '[PRE37]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: approx_count_distinct
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: approx_count_distinct
- en: Approximate distinct count is much faster at approximately counting the distinct
    records rather than doing an exact count, which usually needs a lot of shuffles
    and other operations. While the approximate count is not 100% accurate, many use
    cases can perform equally well even without an exact count.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 近似不同计数要快得多，它可以近似计算不同记录的数量，而不是进行精确计数，后者通常需要大量的洗牌和其他操作。虽然近似计数不是100%准确，但许多用例即使没有精确计数也可以表现得同样好。
- en: The `approx_count_distinct` API has several implementations, as follows. The
    exact API used depends on the specific use case.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`approx_count_distinct` API有几种实现，如下所示。使用的确切API取决于特定的用例。'
- en: '[PRE38]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s look at an example of invoking `approx_count_distinct` on the DataFrame
    to print the approximate count of the DataFrame:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`approx_count_distinct`来打印DataFrame的近似计数的例子：
- en: '[PRE39]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Min
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小
- en: The minimum of the column value of one of the columns in the DataFrame. An example
    is if you want to find the minimum temperature of a city.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame中某一列的最小值。例如，如果要查找城市的最低温度。
- en: 'The `min` API has several implementations, as follows. The exact API used depends
    on the specific use case:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`min` API有几种实现，如下所示。使用的确切API取决于特定的用例：'
- en: '[PRE40]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s look at an example of invoking `min` on the DataFrame to print the minimum
    Population:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`min`来打印最小人口的例子：
- en: '[PRE41]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Max
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大
- en: The maximum of the column value of one of the columns in the DataFrame. An example
    is if you want to find the maximum temperature of a city.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame中某一列的最大值。例如，如果要查找城市的最高温度。
- en: The `max` API has several implementations, as follows. The exact API used depends
    on the specific use case.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`max` API有几种实现，如下所示。使用的确切API取决于特定的用例。'
- en: '[PRE42]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s look at an example of invoking `max` on the DataFrame to print the maximum
    Population:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`max`来打印最大人口的例子：
- en: '[PRE43]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Average
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均
- en: The average of the values is calculated by adding the values and dividing by
    the number of values.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 值的平均数是通过将值相加并除以值的数量来计算的。
- en: Average of 1,2,3 is (1 + 2 + 3) / 3 = 6/3 = 2
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 1,2,3的平均值是(1 + 2 + 3) / 3 = 6/3 = 2
- en: 'The `avg` API has several implementations, as follows. The exact API used depends
    on the specific use case:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`avg` API有几种实现，如下所示。使用的确切API取决于特定的用例：'
- en: '[PRE44]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let''s look at an example of invoking `avg` on the DataFrame to print the average
    population:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`avg`来打印平均人口的例子：
- en: '[PRE45]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Sum
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总和
- en: Computes the sum of the values of the column. Optionally, `sumDistinct` can
    be used to only add up distinct values.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 计算列值的总和。可以选择使用`sumDistinct`仅添加不同的值。
- en: 'The `sum` API has several implementations, as follows. The exact API used depends
    on the specific use case:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`sum` API有几种实现，如下所示。使用的确切API取决于特定的用例：'
- en: '[PRE46]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Let's look at an example of invoking `sum` on the DataFrame to print the summation
    (total) `Population`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`sum`的例子，打印`Population`的总和。
- en: '[PRE47]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Kurtosis
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 峰度
- en: Kurtosis is a way of quantifying differences in the shape of distributions,
    which may look very similar in terms of means and variances, yet are actually
    different. In such cases, kurtosis becomes a good measure of the weight of the
    distribution at the tail of the distribution, as compared to the middle of the
    distribution.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 峰度是量化分布形状差异的一种方式，这些分布在均值和方差方面可能看起来非常相似，但实际上是不同的。在这种情况下，峰度成为分布尾部的权重与分布中部的权重相比的一个很好的度量。
- en: The `kurtosis` API has several implementations, as follows. The exact API used
    depends on the specific use case.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`kurtosis` API有几种实现，具体使用的API取决于特定的用例。'
- en: '[PRE48]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let''s look at an example of invoking `kurtosis` on the DataFrame on the `Population`
    column:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame的`Population`列上调用`kurtosis`的例子：
- en: '[PRE49]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Skewness
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Skewness
- en: Skewness measures the asymmetry of the values in your data around the average
    or mean.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Skewness测量数据中值围绕平均值或均值的不对称性。
- en: The `skewness` API has several implementations, as follows. The exact API used
    depends on the specific use case.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`skewness` API有几种实现，具体使用的API取决于特定的用例。'
- en: '[PRE50]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let''s look at an example of invoking `skewness` on the DataFrame on the Population
    column:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在人口列上调用`skewness`的DataFrame的例子：
- en: '[PRE51]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Variance
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方差
- en: Variance is the average of the squared differences of each of the values from
    the mean.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 方差是每个值与均值的差的平方的平均值。
- en: 'The `var` API has several implementations, as follows. The exact API used depends
    on the specific use case:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`var` API有几种实现，具体使用的API取决于特定的用例：'
- en: '[PRE52]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, let''s look at an example of invoking `var_pop` on the DataFrame measuring
    variance of `Population`:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一个在测量`Population`方差的DataFrame上调用`var_pop`的例子：
- en: '[PRE53]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Standard deviation
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准差
- en: Standard deviation is the square root of the variance (see previously).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差是方差的平方根（见前文）。
- en: 'The `stddev` API has several implementations, as follows. The exact API used
    depends on the specific use case:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`stddev` API有几种实现，具体使用的API取决于特定的用例：'
- en: '[PRE54]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let''s look at an example of invoking `stddev` on the DataFrame printing the
    standard deviation of `Population`:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`stddev`的例子，打印`Population`的标准差：
- en: '[PRE55]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Covariance
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协方差
- en: Covariance is a measure of the joint variability of two random variables. If
    the greater values of one variable mainly corresponds with the greater values
    of the other variable, and the same holds for the lesser values, then the variables
    tend to show similar behavior and the covariance is positive. If the opposite
    is true, and the greater values of one variable correspond with the lesser values
    of the other variable, then the covariance is negative.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差是两个随机变量联合变异性的度量。如果一个变量的较大值主要对应于另一个变量的较大值，并且较小值也是如此，那么这些变量倾向于显示相似的行为，协方差是正的。如果相反是真的，并且一个变量的较大值对应于另一个变量的较小值，那么协方差是负的。
- en: The `covar` API has several implementations, as follows. The exact API used
    depends on the specific use case.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`covar` API有几种实现，具体使用的API取决于特定的用例。'
- en: '[PRE56]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s look at an example of invoking `covar_pop` on the DataFrame to calculate
    the covariance between the year and population columns:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在DataFrame上调用`covar_pop`的例子，计算年份和人口列之间的协方差：
- en: '[PRE57]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: groupBy
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: groupBy
- en: A common task seen in data analysis is to group the data into grouped categories
    and then perform calculations on the resultant groups of data.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析中常见的任务是将数据分组为分组类别，然后对结果数据组执行计算。
- en: A quick way to understand grouping is to imagine being asked to assess what
    supplies you need for your office very quickly. You could start looking around
    you and just group different types of items, such as pens, paper, staplers, and
    analyze what you have and what you need.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 理解分组的一种快速方法是想象被要求迅速评估办公室所需的物品。您可以开始四处看看，并将不同类型的物品分组，例如笔、纸、订书机，并分析您拥有的和您需要的。
- en: 'Let''s run `groupBy` function on the `DataFrame` to print aggregate counts
    of each State:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`DataFrame`上运行`groupBy`函数，打印每个州的聚合计数：
- en: '[PRE58]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'You can also `groupBy` and then apply any of the aggregate functions seen previously,
    such as `min`, `max`, `avg`, `stddev`, and so on:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以`groupBy`，然后应用先前看到的任何聚合函数，例如`min`、`max`、`avg`、`stddev`等：
- en: '[PRE59]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Rollup
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rollup
- en: 'Rollup is a multi-dimensional aggregation used to perform hierarchical or nested
    calculations. For example, if we want to show the number of records for each State+Year
    group, as well as for each State (aggregating over all years to give a grand total
    for each `State` irrespective of the `Year`), we can use `rollup` as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Rollup是用于执行分层或嵌套计算的多维聚合。例如，如果我们想显示每个州+年份组的记录数，以及每个州的记录数（聚合所有年份以给出每个`State`的总数，而不考虑`Year`），我们可以使用`rollup`如下：
- en: '[PRE60]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The `rollup` calculates the count for state and year, such as California+2014,
    as well as California state (adding up all years).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`rollup`计算州和年份的计数，例如加利福尼亚+2014，以及加利福尼亚州（所有年份的总和）。'
- en: Cube
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cube
- en: 'Cube is a multi-dimensional aggregation used to perform hierarchical or nested
    calculations just like rollup, but with the difference that cube does the same
    operation for all dimensions. For example, if we want to show the number of records
    for each `State` and `Year` group, as well as for each `State` (aggregating over
    all Years to give a grand total for each State irrespective of the `Year`), we
    can use rollup as follows. In addition, `cube` also shows a grand total for each
    Year (irrespective of the `State`):'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Cube是用于执行分层或嵌套计算的多维聚合，就像rollup一样，但不同之处在于cube对所有维度执行相同的操作。例如，如果我们想显示每个`State`和`Year`组的记录数，以及每个`State`的记录数（聚合所有年份以给出每个`State`的总数，而不考虑`Year`），我们可以使用rollup如下。此外，`cube`还显示每年的总数（不考虑`State`）：
- en: '[PRE61]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Window functions
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口函数
- en: 'Window functions allow you to perform aggregations over a window of data rather
    than entire data or some filtered data. The use cases of such window functions
    are:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口函数允许您在数据窗口上执行聚合，而不是整个数据或一些经过筛选的数据。这些窗口函数的用例包括：
- en: Cumulative sum
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累积总和
- en: Delta from previous value for same key
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与先前相同键的前一个值的增量
- en: Weighted moving average
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权移动平均
- en: 'The best way to understand window functions is to imagine a sliding window
    over the larger dataset universe. You can specify a window looking at three rows
    T-1, T, and T+1, and by performing a simple calculation. You can also specify
    a window of latest/most recent ten values:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 理解窗口函数的最佳方法是想象一个滑动窗口覆盖整个数据集。您可以指定一个窗口，查看三行T-1、T和T+1，并进行简单的计算。您还可以指定最新/最近的十个值的窗口：
- en: '![](img/00047.jpeg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00047.jpeg)'
- en: The API for the window specification requires three properties, the `partitionBy()`,
    `orderBy()`, and the `rowsBetween()`. The `partitionBy` chunks the data into the
    partitions/groups as specified by `partitionBy()`. `orderBy()` is used to order
    the data within each partition of data.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口规范的API需要三个属性，`partitionBy()`，`orderBy()`和`rowsBetween()`。`partitionBy`将数据分成由`partitionBy()`指定的分区/组。`orderBy()`用于对数据进行排序，以便在每个数据分区内进行排序。
- en: The `rowsBetween()` specifies the window frame or the span of the sliding window
    to perform the calculations.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`rowsBetween()`指定了滑动窗口的窗口帧或跨度来执行计算。'
- en: 'To try out the windows function, there are certain packages that are needed.
    You can import the necessary packages using import directives, as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试窗口函数，需要某些包。您可以使用导入指令导入必要的包，如下所示：
- en: '[PRE62]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now, you are ready to write some code to learn about the window functions. Let's
    create a window specification for the partitions sorted by `Population` and partitioned
    by `State`. Also, specify that we want to consider all rows until the current
    row as part of the `Window`.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经准备好编写一些代码来了解窗口函数。让我们为按`Population`排序并按`State`分区的分区创建一个窗口规范。还要指定我们希望将当前行之前的所有行视为`Window`的一部分。
- en: '[PRE63]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Compute the `rank` over the window specification. The result will be a rank
    (row number) added to each row, as long as it falls within the `Window` specified.
    In this example, we chose to partition by `State` and then order the rows of each
    `State` further by descending order. Hence, all State rows have their own rank
    numbers assigned.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 计算窗口规范上的`rank`。结果将是一个排名（行号）添加到每一行，只要它在指定的`Window`内。在这个例子中，我们选择按`State`进行分区，然后进一步按降序对每个`State`的行进行排序。因此，所有州的行都有自己的排名号码分配。
- en: '[PRE64]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: ntiles
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ntiles
- en: The ntiles is a popular aggregation over a window and is commonly used to divide
    input dataset into n parts. For example, in predictive analytics, deciles (10
    parts) are often used to first group the data and then divide it into 10 parts
    to get a fair distribution of data. This is a natural function of the window function
    approach, hence ntiles is a good example of how window functions can help.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ntiles是窗口上的一种常见聚合，通常用于将输入数据集分成n部分。例如，在预测分析中，通常使用十分位数（10部分）首先对数据进行分组，然后将其分成10部分以获得数据的公平分布。这是窗口函数方法的自然功能，因此ntiles是窗口函数如何帮助的一个很好的例子。
- en: 'For example, if we want to partition the `statesPopulationDF` by `State` (window
    specification was shown previously), order by population, and then divide into
    two portions, we can use `ntile` over the `windowspec`:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想要按`statesPopulationDF`按`State`进行分区（窗口规范如前所示），按人口排序，然后分成两部分，我们可以在`windowspec`上使用`ntile`：
- en: '[PRE65]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: As shown previously, we have used `Window` function and `ntile()` together to
    divide the rows of each `State` into two equal portions.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所示，我们已经使用`Window`函数和`ntile()`一起将每个`State`的行分成两个相等的部分。
- en: A popular use of this function is to compute deciles used in data science Models.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的一个常见用途是计算数据科学模型中使用的十分位数。
- en: Joins
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接
- en: In traditional databases, joins are used to join one transaction table with
    another lookup table to generate a more complete view. For example, if you have
    a table of online transactions by customer ID and another table containing the
    customer city and customer ID, you can use join to generate reports on the transactions
    by city.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统数据库中，连接用于将一个交易表与另一个查找表连接，以生成更完整的视图。例如，如果您有一个按客户ID分类的在线交易表，另一个包含客户城市和客户ID的表，您可以使用连接来生成有关按城市分类的交易的报告。
- en: '**Transactions table**: The following table has three columns, the **CustomerID**,
    the **Purchased item,** and how much the customer paid for the item:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**交易表**：以下表有三列，**CustomerID**，**购买的物品**，以及客户为该物品支付了多少钱：'
- en: '| **CustomerID** | **Purchased item** | **Price paid** |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| **CustomerID** | **购买的物品** | **支付的价格** |'
- en: '| 1 | Headphone | 25.00 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 1 | Headphone | 25.00 |'
- en: '| 2 | Watch | 100.00 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 手表 | 100.00 |'
- en: '| 3 | Keyboard | 20.00 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 键盘 | 20.00 |'
- en: '| 1 | Mouse | 10.00 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 鼠标 | 10.00 |'
- en: '| 4 | Cable | 10.00 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 电缆 | 10.00 |'
- en: '| 3 | Headphone | 30.00 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 3 | Headphone | 30.00 |'
- en: '**Customer Info table:** The following table has two columns, the **CustomerID**
    and the **City** the customer lives in:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**客户信息表**：以下表有两列，**CustomerID**和客户居住的**City**：'
- en: '| **CustomerID** | **City** |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| **CustomerID** | **City** |'
- en: '| 1 | Boston |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 波士顿 |'
- en: '| 2 | New York |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 纽约 |'
- en: '| 3 | Philadelphia |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 费城 |'
- en: '| 4 | Boston |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 波士顿 |'
- en: 'Joining the transaction table with the customer info table will generate a
    view as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 将交易表与客户信息表连接将生成以下视图：
- en: '| **CustomerID** | **Purchased item** | **Price paid** | **City** |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| **CustomerID** | **购买的物品** | **支付的价格** | **城市** |'
- en: '| 1 | Headphone | 25.00 | Boston |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 1 | Headphone | 25.00 | 波士顿 |'
- en: '| 2 | Watch | 100.00 | New York |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 手表 | 100.00 | 纽约 |'
- en: '| 3 | Keyboard | 20.00 | Philadelphia |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 键盘 | 20.00 | 费城 |'
- en: '| 1 | Mouse | 10.00 | Boston |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 鼠标 | 10.00 | 波士顿 |'
- en: '| 4 | Cable | 10.00 | Boston |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 电缆 | 10.00 | 波士顿 |'
- en: '| 3 | Headphone | 30.00 | Philadelphia |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 3 | Headphone | 30.00 | Philadelphia |'
- en: 'Now, we can use this joined view to generate a report of **Total sale price**
    by **City**:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个连接的视图来生成**按城市**的**总销售价格**的报告：
- en: '| **City** | **#Items** | **Total sale price** |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| **城市** | **#物品** | **总销售价格** |'
- en: '| Boston | 3 | 45.00 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 波士顿 | 3 | 45.00 |'
- en: '| Philadelphia | 2 | 50.00 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Philadelphia | 2 | 50.00 |'
- en: '| New York | 1 | 100.00 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| New York | 1 | 100.00 |'
- en: Joins are an important function of Spark SQL, as they enable you to bring two
    datasets together, as seen previously. Spark, of course, is not only meant to
    generate reports, but is used to process data on a petabyte scale to handle real-time
    streaming use cases, machine learning algorithms, or plain analytics. In order
    to accomplish these goals, Spark provides the API functions needed.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 连接是Spark SQL的重要功能，因为它使您能够将两个数据集合并在一起，正如之前所见。当然，Spark不仅仅是用来生成报告的，而是用来处理PB级别的数据，处理实时流处理用例，机器学习算法或纯粹的分析。为了实现这些目标，Spark提供了所需的API函数。
- en: A typical join between two datasets takes place using one or more keys of the
    left and right datasets and then evaluates a conditional expression on the sets
    of keys as a Boolean expression. If the result of the Boolean expression returns
    true, then the join is successful, else the joined DataFrame will not contain
    the corresponding join.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 两个数据集之间的典型连接是使用左侧和右侧数据集的一个或多个键进行的，然后对键集合上的条件表达式进行布尔表达式的评估。如果布尔表达式的结果为true，则连接成功，否则连接的DataFrame将不包含相应的连接。
- en: 'The join API has 6 different implementations:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 连接API有6种不同的实现：
- en: '[PRE66]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We will use one of the APIs to understand how to use join APIs ; however, you
    can choose to use other APIs depending on the use case:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用其中一个API来了解如何使用连接API；然而，您可以根据用例选择使用其他API：
- en: '[PRE67]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Note that joins will be covered in detail in the next few sections.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，连接将在接下来的几个部分中详细介绍。
- en: Inner workings of join
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接的内部工作方式
- en: Join works by operating on the partitions of a DataFrame using the multiple
    executors. However, the actual operations and the subsequent performance depends
    on the type of `join` and the nature of the datasets being joined. In the next
    section, we will look at the types of joins.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 连接通过使用多个执行器对DataFrame的分区进行操作。然而，实际操作和随后的性能取决于`join`的类型和被连接的数据集的性质。在下一节中，我们将看看连接的类型。
- en: Shuffle join
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Shuffle连接
- en: 'Join between two big datasets involves shuffle join where partitions of both
    left and right datasets are spread across the executors. Shuffles are expensive
    and it''s important to analyze the logic to make sure the distribution of partitions
    and shuffles is done optimally. The following is an illustration of how shuffle
    join works internally:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 两个大数据集之间的连接涉及到分区连接，其中左侧和右侧数据集的分区被分布到执行器上。Shuffles是昂贵的，重要的是要分析逻辑，以确保分区和Shuffles的分布是最优的。以下是内部展示Shuffle连接的示例：
- en: '![](img/00166.jpeg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00166.jpeg)'
- en: Broadcast join
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广播连接
- en: 'A join between one large dataset and a smaller dataset can be done by broadcasting
    the smaller dataset to all executors where a partition from the left dataset exists.
    The following is an illustration of how a broadcast join works internally:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将较小的数据集广播到所有执行器，可以对一个大数据集和一个小数据集进行连接，其中左侧数据集的分区存在。以下是广播连接内部工作的示例：
- en: '![](img/00194.jpeg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00194.jpeg)'
- en: Join types
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接类型
- en: The following is a table of the different types of joins. This is important,
    as the choice made when joining two datasets makes all the difference in the output,
    and also the performance.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是不同类型连接的表格。这很重要，因为在连接两个数据集时所做的选择在输出和性能上都有很大的区别。
- en: '| **Join type** | **Description** |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| **Join type** | **Description** |'
- en: '| **inner** | The inner join compares each row from *left* to rows from *right*
    and combines matched pair of rows from *left* and *right* datasets only when both
    have non-NULL values. |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| **inner** | 内连接将*left*中的每一行与*right*中的行进行比较，并仅在两者都具有非NULL值时才组合匹配的*left*和*right*数据集的行。
    |'
- en: '| **cross** | The cross join matches every row from *left* with every row from
    *right* generating a Cartesian cross product. |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| **cross** | cross join 将*left*中的每一行与*right*中的每一行匹配，生成笛卡尔积。 |'
- en: '| **outer, full, fullouter** | The full outer Join gives all rows in *left*
    and *right* filling in NULL if only in *right* or *left*. |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| **outer, full, fullouter** | full outer Join 给出*left*和*right*中的所有行，如果只在*right*或*left*中，则填充NULL。
    |'
- en: '| **leftanti** | The leftanti Join gives only rows in *left* based on non-existence
    on *right* side. |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| **leftanti** | leftanti Join 仅基于*right*一侧的不存在给出*left*中的行。 |'
- en: '| **left, leftouter** | The leftouter Join gives all rows in *left* plus common
    rows of *left* and *right* (inner join). Fills in NULL if not in *right*. |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| **left, leftouter** | leftouter Join 给出*left*中的所有行以及*left*和*right*的公共行（内连接）。如果*right*中没有，则填充NULL。
    |'
- en: '| **leftsemi** | The leftsemi Join gives only rows in *left* based on existence
    on *right* side. The does not include *right-*side values. |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| **leftsemi** | leftsemi Join 仅基于*right*一侧的存在给出*left*中的行。不包括*right*一侧的值。 |'
- en: '| **right, rightouter** | The rightouter Join gives all rows in *right* plus
    common rows of *left* and *right* (inner join). Fills in NULL if not in *left*.
    |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| **right, rightouter** | rightouter Join 给出*right*中的所有行以及*left*和*right*的公共行（内连接）。如果*left*中没有，则填充NULL。
    |'
- en: We will examine how the different join types work by using the sample datasets.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用示例数据集来研究不同连接类型的工作方式。
- en: '[PRE68]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Inner join
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内连接
- en: Inner join results in rows from both `statesPopulationDF` and `statesTaxRatesDF`
    when state is non-NULL in both datasets.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 当州在两个数据集中都不为NULL时，内连接会给出`statesPopulationDF`和`statesTaxRatesDF`的行。
- en: '![](img/00095.jpeg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00095.jpeg)'
- en: 'Join the two datasets by the state column as follows:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 通过州列连接两个数据集如下：
- en: '[PRE69]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'You can run the `explain()` on the `joinDF` to look at the execution plan:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在`joinDF`上运行`explain()`来查看执行计划：
- en: '[PRE70]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Left outer join
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Left outer join
- en: Left outer join results in all rows from `statesPopulationDF`, including any
    common in `statesPopulationDF` and `statesTaxRatesDF`.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: Left outer join结果包括`statesPopulationDF`中的所有行，包括`statesPopulationDF`和`statesTaxRatesDF`中的任何公共行。
- en: '![](img/00273.jpeg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00273.jpeg)'
- en: 'Join the two datasets by the state column, shown as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 通过州列连接两个数据集，如下所示：
- en: '[PRE71]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Right outer join
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Right outer join
- en: Right outer join results in all rows from `statesTaxRatesDF`, including any
    common in `statesPopulationDF` and `statesTaxRatesDF`.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: Right outer join结果包括`statesTaxRatesDF`中的所有行，包括`statesPopulationDF`和`statesTaxRatesDF`中的任何公共行。
- en: '![](img/00319.jpeg)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00319.jpeg)'
- en: 'Join the two datasets by the `State` column as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 按照“State”列连接两个数据集如下：
- en: '[PRE72]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Outer join
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外连接
- en: Outer join results in all rows from `statesPopulationDF` and `statesTaxRatesDF`.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 外连接结果包括`statesPopulationDF`和`statesTaxRatesDF`中的所有行。
- en: '![](img/00245.jpeg)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00245.jpeg)'
- en: 'Join the two datasets by the `State` column as follows:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 按照“State”列连接两个数据集如下：
- en: '[PRE73]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Left anti join
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 左反连接
- en: Left anti join results in rows from only `statesPopulationDF` if, and only if,
    there is NO corresponding row in `statesTaxRatesDF`.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 左反连接的结果只包括`statesPopulationDF`中的行，如果且仅如果在`statesTaxRatesDF`中没有相应的行。
- en: '![](img/00072.jpeg)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00072.jpeg)'
- en: 'Join the two datasets by the `State` column as follows:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下方式通过“State”列连接两个数据集：
- en: '[PRE74]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Left semi join
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 左半连接
- en: Left semi join results in rows from only `statesPopulationDF` if, and only if,
    there is a corresponding row in `statesTaxRatesDF`.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 左半连接的结果只包括`statesPopulationDF`中的行，如果且仅如果在`statesTaxRatesDF`中有相应的行。
- en: '![](img/00097.jpeg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00097.jpeg)'
- en: 'Join the two datasets by the state column as follows:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 按照州列连接两个数据集如下：
- en: '[PRE75]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Cross join
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉连接
- en: Cross join matches every row from *left* with every row from *right,* generating
    a Cartesian cross product.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉连接将*left*中的每一行与*right*中的每一行进行匹配，生成笛卡尔乘积。
- en: '![](img/00312.jpeg)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00312.jpeg)'
- en: 'Join the two datasets by the `State` column as follows:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下方式通过“State”列连接两个数据集：
- en: '[PRE76]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: You can also use join with cross jointype instead of calling the cross join
    API. `statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State").isNotNull,
    "cross").count`.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用交叉连接类型的连接，而不是调用交叉连接API。`statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State").isNotNull,
    "cross").count`。
- en: Performance implications of join
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接的性能影响
- en: The join type chosen directly impacts the performance of the join. This is because
    joins require the shuffling of data between executors to execute the tasks, hence
    different joins, and even the order of the joins, need to be considered when using
    join.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的连接类型直接影响连接的性能。这是因为连接需要在执行任务之间对数据进行洗牌，因此在使用连接时需要考虑不同的连接，甚至连接的顺序。
- en: 'The following is a table you could use to refer to when writing `Join` code:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是编写`Join`代码时可以参考的表：
- en: '| **Join type** | **Performance considerations and tips** |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| **连接类型** | **性能考虑和提示** |'
- en: '| **inner** | Inner join requires the left and right tables to have the same
    column. If you have duplicate or multiple copies of the keys on either the left
    or right side, the join will quickly blow up into a sort of a Cartesian join,
    taking a lot longer to complete than if designed correctly to minimize the multiple
    keys. |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| **inner** | 内连接要求左表和右表具有相同的列。如果左侧或右侧的键有重复或多个副本，连接将迅速膨胀成一种笛卡尔连接，完成时间比设计正确以最小化多个键的连接要长得多。'
- en: '| **cross** | Cross Join matches every row from *left* with every row from
    *right,* generating a Cartesian cross product. This is to be used with caution,
    as this is the worst performant join, to be used in specific use cases only. |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| **cross** | Cross Join将*left*中的每一行与*right*中的每一行进行匹配，生成笛卡尔乘积。这需要谨慎使用，因为这是性能最差的连接，只能在特定用例中使用。'
- en: '| **outer, full, fullouter** | Fullouter Join gives all rows in *left* and
    *right* filling in NULL if only in *right* or *left*. If used on tables with little
    in common, can result in very large results and thus slow performance. |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| **outer, full, fullouter** | Fullouter Join给出*left*和*right*中的所有行，如果只在*right*或*left*中，则填充NULL。如果在共同点很少的表上使用，可能导致非常大的结果，从而降低性能。'
- en: '| **leftanti** | Leftanti Join gives only rows in *left* based on non-existence
    on *right* side. Very good performance, as only one table is fully considered
    and the other is only checked for the join condition. |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| **leftanti** | Leftanti Join仅基于*right*一侧的不存在给出*left*中的行。性能非常好，因为只考虑一个表，另一个表只需检查连接条件。'
- en: '| **left, leftouter** | Leftouter Join gives all rows in *left* plus common
    rows of *left* and *right* (inner join). Fills in NULL if not in *right*. If used
    on tables with little in common, can result in very large results and thus slow
    performance. |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| **left, leftouter** | Leftouter Join给出*left*中的所有行以及*left*和*right*中的共同行（内连接）。如果*right*中没有，则填充NULL。如果在共同点很少的表上使用，可能导致非常大的结果，从而降低性能。'
- en: '| **leftsemi** | Leftsemi Join gives only rows in *left* based on existence
    on *right* side. Does not include *right* side values. Very good performance,
    as only one table is fully considered and other is only checked for the join condition.
    |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| **leftsemi** | Leftsemi Join仅基于*right*一侧的存在给出*left*中的行。不包括*right*一侧的值。性能非常好，因为只考虑一个表，另一个表只需检查连接条件。'
- en: '| **right, rightouter** | Rightouter Join gives all rows in *right* plus common
    rows of *left* and *right* (inner join). Fills in NULL if not in *left*. Performance
    is similar to the leftouter join mentioned previously in this table. |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| **right, rightouter** | Rightouter Join给出*right*中的所有行以及*left*和*right*中的共同行（内连接）。如果*left*中没有，则填充NULL。性能与上表中先前提到的leftouter
    join类似。'
- en: Summary
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed the origin of DataFrames and how Spark SQL provides
    the SQL interface on top of DataFrames. The power of DataFrames is such that execution
    times have decreased manyfold over original RDD-based computations. Having such
    a powerful layer with a simple SQL-like interface makes them all the more powerful.
    We also looked at various APIs to create, and manipulate DataFrames, as well as
    digging deeper into the sophisticated features of aggregations, including `groupBy`,
    `Window`, `rollup`, and `cubes`. Finally, we also looked at the concept of joining
    datasets and the various types of joins possible, such as inner, outer, cross,
    and so on.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了DataFrame的起源以及Spark SQL如何在DataFrame之上提供SQL接口。DataFrame的强大之处在于，执行时间比原始基于RDD的计算减少了很多倍。拥有这样一个强大的层和一个简单的类似SQL的接口使它们变得更加强大。我们还研究了各种API来创建和操作DataFrame，并深入挖掘了聚合的复杂特性，包括`groupBy`、`Window`、`rollup`和`cubes`。最后，我们还研究了连接数据集的概念以及可能的各种连接类型，如内连接、外连接、交叉连接等。
- en: In the next chapter, we will explore the exciting world of real-time data processing
    and analytics in the [Chapter 9](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c),
    *Stream Me Up, Scotty - Spark Streaming*.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索实时数据处理和分析的激动人心的世界，即[第9章](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c)，*Stream
    Me Up, Scotty - Spark Streaming*。
