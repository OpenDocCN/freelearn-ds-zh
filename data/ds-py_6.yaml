- en: '*Chapter 7*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第七章*'
- en: Processing Human Language
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理人类语言
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将能够：
- en: Create machine learning models for textual data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为文本数据创建机器学习模型
- en: Use the NLTK library to preprocess text
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NLTK库对文本进行预处理
- en: Utilize regular expressions to clean and analyze strings
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则表达式清洗和分析字符串
- en: Create word embedding using the Word2Vec model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Word2Vec模型创建词向量
- en: This chapter shall cover the concepts on processing human language.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍处理人类语言的相关概念。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: One of the most important goals of artificial intelligence (AI) is to understand
    the human language to perform tasks. Spellcheck, sentiment analysis, question
    answering, chat bots, and virtual assistants (such as Siri and Google Assistant)
    all have a natural language processing (NLP) module. The NLP module enables virtual
    assistants to process human language and perform actions based on it. For example,
    when we say, "OK Google, set an alarm for 7 A.M.", the speech is first converted
    to text and then this text is processed by the NLP module. After this processing,
    the virtual assistant calls the appropriate API of the Alarm/Clock application.
    Processing human language has its own set of challenges because it is ambiguous,
    with words meaning different things depending on the context in which they are
    used. This is the biggest pain point of language for AI.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）最重要的目标之一是理解人类语言，以执行任务。拼写检查、情感分析、问答、聊天机器人和虚拟助手（如Siri和Google助手）都具有自然语言处理（NLP）模块。NLP模块使虚拟助手能够处理人类语言并根据语言执行相应的操作。例如，当我们说“OK
    Google，设定一个早上7点的闹钟”时，语音首先被转换为文本，然后由NLP模块处理。处理完毕后，虚拟助手会调用闹钟/时钟应用程序的适当API。处理人类语言有其自身的挑战，因为语言具有歧义性，词汇的意义取决于其所处的上下文。这是AI语言处理中的最大痛点。
- en: Another big reason is the unavailability of complete information. We tend to
    leave out most of the information while communicating; information that is common
    sense or things that are universally true or false. For example, the sentence
    "I saw a man on a hill with a telescope" can have different meanings depending
    on the contextual information. For example, it could mean that "I saw a man who
    had a telescope on a hill," but it could also mean that "I saw a man on a hill
    through a telescope." It is very difficult for computers to keep track of this
    information as most of it is contextual. Due to the advances in deep learning,
    NLP today works much better than when we used traditional methods such as clustering
    and linear models. This is the reason we will use deep learning on text corpora
    to solve NLP problems. NLP, like any other machine learning problem, has two main
    parts, data processing and model creation. In the next topic, we will learn how
    to process textual data, and later, we will learn how to use this processed data
    to create machine learning models to solve our problems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要原因是完整信息的缺乏。在交流时，我们往往省略大部分信息；这些信息通常是常识，或者是普遍真实或错误的事情。例如，句子“I saw a man on
    a hill with a telescope”根据上下文信息的不同，可以有不同的含义。例如，它可能意味着“我看到一个手持望远镜的男人站在山上”，也可能意味着“我通过望远镜看到一个站在山上的男人”。计算机很难跟踪这些信息，因为大部分是上下文性的。由于深度学习的进步，今天的自然语言处理（NLP）比我们以前使用传统方法（如聚类和线性模型）时更为有效。这也是我们将使用深度学习来处理文本语料库解决NLP问题的原因。像其他任何机器学习问题一样，NLP也有两个主要部分：数据处理和模型创建。在接下来的内容中，我们将学习如何处理文本数据，随后我们将学习如何使用这些处理过的数据来创建机器学习模型，以解决我们的实际问题。
- en: Text Data Processing
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本数据处理
- en: 'Before we start building machine learning models for our textual data, we need
    to process the data. First, we will learn the different ways in which we can understand
    what the data comprises. This helps us get a sense of what the data really is
    and decide on the preprocessing techniques to be used in the next step. Next,
    we will move on to learn the techniques that will help us preprocess the data.
    This step helps reduce the size of the data, thus reducing the training time,
    and also helps us transform the data into a form that would be easier for machine
    learning algorithms to extract information from. Finally, we will learn how to
    convert the textual data to numbers so that machine learning algorithms can actually
    use it to create models. We do this using word embedding, much like the entity
    embedding we performed in *Chapter 5*: *Mastering Structured Data*.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始为文本数据构建机器学习模型之前，我们需要对数据进行处理。首先，我们将学习不同的方法来理解数据的组成。这有助于我们了解数据的真正内容，并决定下一步要使用的预处理技术。接下来，我们将学习有助于预处理数据的技术。这一步有助于减少数据的大小，从而缩短训练时间，并帮助我们将数据转换为机器学习算法更易于提取信息的形式。最后，我们将学习如何将文本数据转换为数字，以便机器学习算法可以实际使用这些数据来创建模型。我们通过词嵌入来实现这一点，就像我们在*第
    5 章*：“*掌握结构化数据*”中进行的实体嵌入一样。
- en: Regular Expressions
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则表达式
- en: Before we start working on textual data, we need to learn about regular expressions
    (RegEx). RegEx is not really a preprocessing technique, but a sequence of characters
    that defines a search pattern in a string. RegEx is a powerful tool when dealing
    with textual data as it helps us find sequences in a collection of text. A RegEx
    consists of metacharacters and regular characters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始处理文本数据之前，我们需要了解正则表达式（RegEx）。正则表达式并不真正属于预处理技术，而是一串定义字符串中搜索模式的字符。正则表达式是处理文本数据时的强大工具，它帮助我们在文本集合中查找特定的序列。正则表达式由元字符和普通字符组成。
- en: '![Figure 7.1: Tables containing metacharacters used in RegEx, and some examples'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.1：包含在正则表达式中使用的元字符的表格，以及一些示例'
- en: '](img/C13322_07_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_01.jpg)'
- en: 'Figure 7.1: Tables containing metacharacters used in RegEx, and some examples'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.1：包含在正则表达式中使用的元字符的表格，以及一些示例
- en: 'Using RegEx, we can search for complex patterns in a text. For example, we
    can use it to remove URLs from a text. We can use the `re` module in Python to
    remove a URL as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则表达式，我们可以在文本中搜索复杂的模式。例如，我们可以用它来从文本中删除 URL。我们可以使用 Python 的`re`模块删除 URL，如下所示：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`re.sub` accepts three parameters: the first is RegEx, the second is the expression
    you want to substitute in place of the matched pattern, and the third is the text
    in which it should search for the pattern.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`re.sub`接受三个参数：第一个是正则表达式，第二个是你想要替换匹配模式的表达式，第三个是它应该搜索该模式的文本。'
- en: 'The output of the command is as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下：
- en: '![Figure 7.2: Output command'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.2：输出命令'
- en: '](img/C13322_07_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_02.jpg)'
- en: 'Figure 7.2: Output command'
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.2：输出命令
- en: Note
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'It is difficult to remember all the RegEx conventions, so when working with
    RegEx, refer to a cheat sheet, such as: (http://www.pyregex.com/).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 记住所有正则表达式的约定很困难，因此在使用正则表达式时，参考备忘单是个不错的选择，例如： (http://www.pyregex.com/)。
- en: 'Exercise 54: Using RegEx for String Cleaning'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 54：使用正则表达式进行字符串清理
- en: In this exercise, we will use the `re` module of Python to modify and analyze
    a string. We will simply learn how to use RegEx in this exercise, and in the following
    section, we will see how we can use RegEx to preprocess our data. We will use
    a single review from the IMDB Movie Review dataset (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter07),
    which we shall also work on later in the chapter to create sentiment analysis
    models. This dataset is already processed, and some words have been removed. This
    will be the case sometimes when dealing with prebuilt datasets, so it is important
    to analyze the dataset you are working on before you start working.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 Python 的`re`模块来修改和分析字符串。在本练习中，我们将简单地学习如何使用正则表达式，接下来的部分我们将展示如何使用正则表达式来预处理数据。我们将使用
    IMDB 电影评论数据集中的一条评论（https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter07），稍后我们将在本章中利用它来创建情感分析模型。该数据集已经经过处理，一些单词已被删除。处理预构建数据集时，通常会出现这种情况，因此在开始工作之前，了解你所使用的数据集是非常重要的。
- en: 'In this exercise, we will use a movie review from IMDB. Save the review text
    into a variable, as in the following code. You can use any other paragraph of
    text for this exercise:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用来自IMDB的电影评论。将评论文本保存到一个变量中，如以下代码所示。你也可以使用任何其他段落的文本进行这个练习：
- en: '[PRE1]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Calculate the length of the review to know by how much we should reduce the
    size. We will use `len(string)` and get the output, as shown in the following
    code:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论的长度，以确定我们需要减少多大的大小。我们将使用`len(string)`并获得输出，如以下代码所示：
- en: '[PRE2]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output length is as follows:'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出长度如下：
- en: '![Figure 7.3: Length of the string'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.3：字符串长度'
- en: '](img/C13322_07_03.jpg)'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_03.jpg)'
- en: 'Figure 7.3: Length of the string'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.3：字符串长度
- en: 'Sometimes, when you scrape data from websites, hyperlinks get recorded as well.
    Most of the times, hyperlinks do not provide us any information. Remove any hyperlink
    from the data using a complex regex string, as in "`https?\://\S+`". This selects
    any substring with `https://` in it:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时，当你从网站抓取数据时，超链接也会被记录下来。大多数情况下，超链接不会为我们提供任何有用信息。通过使用复杂的正则表达式字符串（如"`https?\://\S+`"），删除数据中的任何超链接。这将选择任何包含`https://`的子字符串：
- en: '[PRE3]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The string with hyperlinks is removed as follows:'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 去除超链接后的字符串如下：
- en: '![Figure 7.4: The string with hyperlinks removed'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.4：去除超链接后的字符串'
- en: '](img/C13322_07_04.jpg)'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_04.jpg)'
- en: 'Figure 7.4: The string with hyperlinks removed'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.4：去除超链接后的字符串
- en: 'Next, we will remove the `br` HTML tags from the text, which we observed while
    reading the string. Sometimes, these HTML tags get added to the scrapped data:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将从文本中删除`br` HTML标签，这些标签是我们在读取字符串时观察到的。有时，这些HTML标签会被添加到抓取的数据中：
- en: '[PRE4]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The string without the `br` tags is as follows:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 去除`br`标签后的字符串如下：
- en: '![Figure 7.5: The string without br tags'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.5：去除br标签后的字符串'
- en: '](img/C13322_07_05.jpg)'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_05.jpg)'
- en: 'Figure 7.5: The string without br tags'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.5：去除br标签后的字符串
- en: 'Now, we will remove all the digits from the text. This helps us reduce the
    size of the dataset when digits are of no significance to us:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将从文本中删除所有数字。当数字对我们没有意义时，这有助于减少数据集的大小：
- en: '[PRE5]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The string without digits is shown here:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 去除数字后的字符串如下所示：
- en: '![Figure 7.6: The string without digits'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.6：去除数字后的字符串'
- en: '](img/C13322_07_06.jpg)'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_06.jpg)'
- en: 'Figure 7.6: The string without digits'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.6：去除数字后的字符串
- en: 'Next, we will remove all special characters and punctuations. Depending on
    your problem, these could just be taking up space and not providing relevant information
    for the machine learning algorithms. So, we remove them with the following regex
    pattern:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将删除所有特殊字符和标点符号。根据你的问题，这些可能只是占用空间，且不会为机器学习算法提供相关信息。所以，我们使用以下正则表达式模式将它们移除：
- en: '[PRE6]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The string without special characters and punctuations is shown here:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 去除特殊字符和标点符号后的字符串如下所示：
- en: '![Figure 7.7: The string without special characters'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.7：没有特殊字符的字符串'
- en: '](img/C13322_07_07.jpg)'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_07.jpg)'
- en: 'Figure 7.7: The string without special characters'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.7：没有特殊字符的字符串
- en: 'Now, we will substitute `can''t` with `cannot` and `it''s` with `it is`. This
    helps us reduce the training time as the number of unique words reduces:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将把`can't`替换为`cannot`，并将`it's`替换为`it is`。这有助于减少训练时间，因为唯一单词的数量减少了：
- en: '[PRE7]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The final string is as follows:'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终的字符串如下：
- en: '![Figure 7.8: The final string'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.8：最终字符串'
- en: '](img/C13322_07_08.jpg)'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_08.jpg)'
- en: 'Figure 7.8: The final string'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.8：最终字符串
- en: 'Finally, we will calculate the length of the cleaned string:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将计算清理后字符串的长度：
- en: '[PRE8]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output size of the string is as follows:'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 字符串的输出大小如下：
- en: '![Figure 7.9: The length of cleaned string'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.9：清理后的字符串长度'
- en: '](img/C13322_07_09.jpg)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_09.jpg)'
- en: 'Figure 7.9: The length of cleaned string'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.9：清理后的字符串长度
- en: We reduced the size of the review by 14%.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将评论的大小减少了14%。
- en: 'Now, we will use RegEx to analyze the data and get all the words that start
    with a capital letter:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用正则表达式分析数据，并获取所有以大写字母开头的单词：
- en: Note
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE9]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The words are as follows:'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单词如下：
- en: '![Figure 7.10: Words starting with capital letters'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.10：以大写字母开头的单词'
- en: '](img/C13322_07_10.jpg)'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_10.jpg)'
- en: 'Figure 7.10: Words starting with capital letters'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.10：以大写字母开头的单词
- en: 'To find all the one- and two-letter words in the text, use the following:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在文本中查找所有一字母和二字母的单词，请使用以下方法：
- en: '[PRE10]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.11: One and two letter words'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.11：一字母和二字母单词'
- en: '](img/C13322_07_11.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_11.jpg)'
- en: 'Figure 7.11: One and two letter words'
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.11：一字母和二字母单词
- en: Congratulations! You successfully modified and analyzed a review string using
    RegEx with the `re` module.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你已经成功使用 `re` 模块和正则表达式修改并分析了评论字符串。
- en: Basic Feature Extraction
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本特征提取
- en: 'Basic feature extraction helps us understand what our data consists of. This
    helps us select the steps to take to preprocess the dataset. Basic feature extraction
    consists of actions such as calculation of the average number of words and count
    of special characters. We will make use of the IMDB movie review dataset in this
    section as an example:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基本特征提取帮助我们了解数据的组成。这有助于我们选择进行数据预处理的步骤。基本特征提取包括计算平均词数和特殊字符计数等操作。我们将在本节中使用 IMDB
    电影评论数据集作为示例：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s see what our dataset consists of:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据集包含了什么：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.12: SentimentText data'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.12：SentimentText 数据'
- en: '](img/C13322_07_12.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_12.jpg)'
- en: 'Figure 7.12: SentimentText data'
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.12：SentimentText 数据
- en: 'The `SentimentText` variable contains the actual review and the `Sentiment`
    variable contains the sentiment of the review. `1` represents a positive sentiment
    and `0` represents a negative sentiment. Let''s print the first review to get
    a sense of the data we are dealing with:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`SentimentText` 变量包含实际评论，`Sentiment` 变量包含评论的情绪。`1` 表示正面情绪，`0` 表示负面情绪。让我们打印出第一条评论，以便了解我们处理的数据：'
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The first review is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条评论如下：
- en: '![Figure 7.13: First review'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.13：第一条评论'
- en: '](img/C13322_07_13.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_13.jpg)'
- en: 'Figure 7.13: First review'
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.13：第一条评论
- en: Now, we will try to understand the kind of data we are working with by getting
    the key statistics of the dataset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将尝试通过获取数据集的关键统计数据来了解我们正在处理的数据类型。
- en: '**Number of words**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**词数**'
- en: 'We can get the number of words in each review with the following code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码获取每条评论中的词数：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now, the `word_count` variable in the DataFrame contains the total number of
    words in the review. The `apply` function applies the `split` function to each
    row of the dataset iteratively. Now, we can get the average number of words for
    each class to see if positive reviews have more words than negative reviews.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，DataFrame 中的 `word_count` 变量包含评论中的总词数。`apply` 函数会将 `split` 函数逐行应用于数据集。现在，我们可以获取每一类评论的平均词数，看看正面评论是否比负面评论有更多的词汇。
- en: 'The `mean()` function calculates the average of a column in pandas. For negative
    reviews, use the following code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`mean()` 函数计算 pandas 中某列的平均值。对于负面评论，使用以下代码：'
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The average number of words for a negative sentiment is as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 负面情绪的平均词数如下：
- en: '![Figure 7.14: Total number of words for negative sentiment'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.14：负面情绪的总词数'
- en: '](img/C13322_07_14.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_14.jpg)'
- en: 'Figure 7.14: Total number of words for negative sentiment'
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.14：负面情绪的总词数
- en: 'For positive reviews, use the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正面评论，使用以下代码：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The average number of words for a positive sentiment is as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正面情绪的平均词数如下：
- en: '![Figure 7.15: Total number of words for positive sentiment'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.15：正面情绪的总词数'
- en: '](img/C13322_07_15.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_15.jpg)'
- en: 'Figure 7.15: Total number of words for positive sentiment'
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.15：正面情绪的总词数
- en: We can see that there isn't much difference in the average number of words for
    either class.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，正负情绪的平均词数差异不大。
- en: '**Stop words**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**停用词**'
- en: Stop words are the most common words in a language – for example, "I", "me",
    "my", "yours", and "the." Most of the time, these words provide no real information
    about the sentence, so we remove these words from our dataset to reduce the size.
    The `nltk` library has a list of stop words for the English language that we can
    access.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是语言中最常见的词汇——例如，“I”、“me”、“my”、“yours”和“the”。大多数时候，这些词汇不会提供句子的实际信息，因此我们将它们从数据集中移除，以减少数据的大小。`nltk`
    库提供了一个可以访问的英文停用词列表。
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To get the count of these stop words, we can use the following code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取这些停用词的计数，我们可以使用以下代码：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we can see the average number of stop words for each class, by using
    the following code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下代码查看每个类别的平均停用词数量：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The average number of stop words for a negative sentiment is shown here:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 负面情绪的平均停用词数量如下：
- en: '![](img/C13322_07_16.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C13322_07_16.jpg)'
- en: 'Figure 7.16: Average number of stop words for a negative sentiment'
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.16：负面情绪的平均停用词数
- en: 'Now, to get the number of stop words for a positive sentiment, we use the following
    code:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了获取正面情绪的停用词数量，我们使用以下代码：
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output average number of stop words for a positive sentiment is shown here:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示的是正面情感的停用词平均数量：
- en: '![Figure 7.17: Average number of stop words for a positive sentiment'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.17：正面情感的停用词平均数量'
- en: '](img/C13322_07_17.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_17.jpg)'
- en: 'Figure 7.17: Average number of stop words for a positive sentiment'
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.17：正面情感的停用词平均数量
- en: '**Number of special characters**'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**特殊字符数量**'
- en: 'Depending on the kind of problem you are dealing with, you will want to either
    keep special characters such as `@`, `#`, `$`, and `*`, or remove them. To be
    able to do that, you first must figure out how many special characters occur in
    your dataset. To get the count of `^`, `&`, `*`, `$`, `@`, and `#` in your dataset,
    use the following code:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你处理的问题类型，你可能需要保留诸如 `@`、`#`、`$` 和 `*` 等特殊字符，或者将它们移除。要做到这一点，你首先需要弄清楚数据集中出现了多少特殊字符。要获取数据集中
    `^`、`&`、`*`、`$`、`@` 和 `#` 的数量，可以使用以下代码：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Text Preprocessing
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本预处理
- en: Now that we know what our data comprises, we need to preprocess it so that our
    machine learning algorithms can easily find patterns in the text. In this section,
    we will go over some of the techniques used to clean and reduce the dimensionality
    of the data we feed into our machine learning algorithm.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了数据的组成部分，我们需要对其进行预处理，以便机器学习算法能够轻松地在文本中找到模式。在本节中，我们将介绍一些用于清理和减少我们输入机器学习算法的数据维度的技术。
- en: '**Lowercase**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**小写化**'
- en: 'The first preprocessing step we perform is converting all the data into lowercase.
    This prevents multiple copies of the same word. You can easily convert all text
    to lowercase using the following code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行的第一个预处理步骤是将所有数据转换为小写字母。这可以防止出现同一个词的多个副本。你可以使用以下代码轻松将所有文本转换为小写：
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The apply function applies the `lower` function to each row of the dataset iteratively.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: apply 函数会将 `lower` 函数迭代地应用于数据集的每一行。
- en: '**Stop word removal**'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**停用词移除**'
- en: 'As discussed previously, stop words should be removed from the dataset as they
    add very little useful information. Stop words do not affect the sentiments of
    a sentence. We perform this step to remove any bias that stop words might introduce:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，停用词应从数据集中移除，因为它们提供的信息非常有限。停用词不会影响句子的情感。我们执行此步骤是为了去除停用词可能引入的偏差：
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Frequent word removal**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**常见词移除**'
- en: 'Stop words are more general words such as ''a'', ''an,'' and ''the.'' However,
    in this step, you will remove the most frequent word from the dataset you are
    working with. For example, the words that can be removed from a tweet dataset
    are `RT`, `@username`, and `DM`. First, find the most frequent words:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是一些常见的词汇，如 'a'、'an' 和 'the'。然而，在这一步，你将移除数据集中最常见的词。例如，可以从推文数据集中移除的词包括 `RT`、`@username`
    和 `DM`。首先，找出最常见的词汇：
- en: '[PRE24]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The most frequent words are:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的词汇是：
- en: '![Figure 7.18: Most frequent words in tweet dataset'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.18：推文数据集中最常见的词汇'
- en: '](img/C13322_07_18.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_18.jpg)'
- en: 'Figure 7.18: Most frequent words in tweet dataset'
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.18：推文数据集中最常见的词汇
- en: 'From the output, we get a hint: the text contains HTML tags, which can be removed
    to considerably reduce the dataset size. So, let''s first remove all `<br />`
    HTML tags and then remove words such as ''movie'' and ''film,'' which will not
    have much impact on the sentiment detector:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出结果中，我们得到一个提示：文本包含HTML标签，这些标签可以去除，从而大大减少数据集的大小。因此，我们首先去除所有 `<br />` HTML 标签，然后去除诸如
    'movie' 和 'film' 这样的词，这些词对情感分析器的影响不大：
- en: '[PRE25]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Punctuation and special character removal**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**标点符号和特殊字符移除**'
- en: 'Next, we remove all the punctuations and special characters from the text as
    they add very little information to the text. To remove punctuations and special
    characters, use this regex:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从文本中移除所有标点符号和特殊字符，因为它们对文本提供的信息很少。要移除标点符号和特殊字符，可以使用以下正则表达式：
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The regex selects all alphanumerical characters and spaces.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式选择所有字母数字字符和空格。
- en: '**Spellcheck**'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**拼写检查**'
- en: 'Sometimes, incorrect spellings of the same word causes us to have copies of
    the same word. This can be corrected by performing a spellcheck using the autocorrect
    library:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，同一个词的拼写错误会导致我们拥有相同词汇的多个副本。通过使用自动纠正库进行拼写检查，可以纠正这种问题：
- en: '[PRE27]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**Stemming**'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**词干提取**'
- en: '`nltk` library:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk` 库：'
- en: '[PRE28]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Spellcheck, stemming, and lemmatization can take a lot of time to complete depending
    on the size of the dataset, so make sure that you do need to perform this step
    by looking into the dataset.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写检查、词干提取和词形还原的处理时间可能会很长，具体取决于数据集的大小，因此，在执行这些步骤之前，请先检查数据集，确保需要进行这些操作。
- en: '**Lemmatization**'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**词形还原**'
- en: Tip
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示
- en: You should prefer lemmatization to stemming as it is more effective.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该更倾向于使用词形还原（lemmatization）而非词干提取（stemming），因为它更为有效。
- en: '`nltk` library:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk` 库：'
- en: '[PRE29]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: We are working to reduce the dimensionality of the dataset because of the "curse
    of dimensionality." Datasets become sparse as their dimensionality (dependent
    variables) increases. This causes data science techniques to fail. This is due
    to the difficulty in modelling the high number of features (dependent variables)
    to get the correct output. As the number of features of the dataset increases,
    we need more data points to model them. So, to get around the curse of high-dimensional
    data, we need to obtain a lot more data, which in turn would increase the time
    required to process it.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在减少数据集的维度，原因是“维度灾难”。随着维度（因变量）增加，数据集会变得稀疏。这会导致数据科学技术失效，因为很难对高维特征进行建模以得到正确的输出。随着数据集特征数量的增加，我们需要更多的数据点来进行建模。因此，为了克服高维数据的灾难，我们需要获取更多的数据，这样会增加处理这些数据所需的时间。
- en: '**Tokenization**'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**分词**'
- en: '`nltk` library:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk` 库：'
- en: '[PRE30]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The tokenized list is as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 标记后的列表如下：
- en: '![Figure 7.19: Tokenized list'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.19：标记后的列表'
- en: '](img/C13322_07_19.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_19.jpg)'
- en: 'Figure 7.19: Tokenized list'
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.19：分词后的列表
- en: As you can see, it separates punctuations from words and detects complex words
    such as "Dr."
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它将标点符号与单词分开，并检测像“Dr.”这样的复杂词语。
- en: 'Exercise 55: Preprocessing the IMDB Movie Review Dataset'
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 55：预处理 IMDB 电影评论数据集
- en: In this exercise, we will preprocess the IMDB Movie Review dataset to make it
    ready for any machine learning algorithm. The dataset consists of 25,000 movie
    reviews along with the sentiment (positive or negative) of the review. We want
    to predict sentiments using the review, so we need to keep that in mind while
    performing preprocessing.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将预处理 IMDB 电影评论数据集，使其适用于任何机器学习算法。该数据集包含25,000条电影评论以及评论的情感（正面或负面）。我们希望通过评论预测情感，因此在进行预处理时需要考虑这一点。
- en: 'Load the IMDB movie review dataset using pandas:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 加载 IMDB 电影评论数据集：
- en: '[PRE31]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'First, we will convert all characters in the dataset into lowercase:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将数据集中的所有字符转换为小写字母：
- en: '[PRE32]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we will write a `clean_str` function, in which we will clean the reviews
    using the `re` module:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将编写一个 `clean_str` 函数，在其中使用 `re` 模块清理评论：
- en: '[PRE33]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE34]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Use the apply function of pandas to perform review cleaning on the complete
    dataset.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 pandas 的 apply 函数对整个数据集进行评论清理。
- en: 'Next, check the word distribution in the dataset using the following code:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下代码检查数据集中的词语分布：
- en: '[PRE35]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The occurrence of the top 10 words is as follows:'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 排名前10的词汇出现频率如下：
- en: '![Figure 7.20: Top 10 words'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.20：排名前10的词汇'
- en: '](img/C13322_07_20.jpg)'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_20.jpg)'
- en: 'Figure 7.20: Top 10 words'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.20：排名前10的词语
- en: 'Remove stop words from the reviews:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从评论中移除停用词：
- en: Note
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This will be done by first tokenizing the reviews and then removing the stop
    word loaded from the `nltk` library.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将通过首先对评论进行分词，然后移除从 `nltk` 库加载的停用词来完成。
- en: 'We add ''`movie`,'' ''`film`,'' and ''`time`'' to the stop words as they occur
    very frequently in the reviews and don''t really contribute much to understanding
    what the review sentiment is:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将'`movie`'、'`film`' 和 '`time`' 加入停用词列表，因为它们在评论中出现频率很高，且对理解评论情感没有太大帮助：
- en: '[PRE36]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we convert the tokens back into sentences and drop the reviews where
    all the text was stop words:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将分词后的内容转回为句子，并去除那些全部由停用词组成的评论：
- en: '[PRE37]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The next step is to convert the text into tokens and then numbers. We will
    be using the Keras Tokenizer as it performs both the steps for us:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是将文本转换为词汇表中的标记，再转换为数字。我们将使用 Keras Tokenizer，因为它能同时完成这两个步骤：
- en: '[PRE38]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'To get the size of the vocabulary, use the following:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取词汇表的大小，使用以下代码：
- en: '[PRE39]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The number of unique tokens is as follows:'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 唯一标记的数量如下：
- en: '![](img/C13322_07_21.jpg)'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C13322_07_21.jpg)'
- en: 'Figure 7.21: Number of unique tokens'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.21：唯一词汇的数量
- en: To reduce the training time of our model, we will cap the length of our reviews
    at 200 words. You can play around with this number to find out what gives you
    the best accuracy.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了减少模型的训练时间，我们将把评论的长度限制在200个单词以内。你可以调整这个数字，以找到最适合的准确率。
- en: Note
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE40]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You should save the tokenizer so that you can convert the reviews back to text:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该保存分词器，以便在之后将评论转回为文本：
- en: '[PRE41]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'To preview a cleaned review, run the following command:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要预览清理过的评论，运行以下命令：
- en: '[PRE42]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'A cleaned review looks like this:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个清理过的评论如下所示：
- en: '![Figure 7.22: A cleaned review'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.22：清理后的评论'
- en: '](img/C13322_07_22.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_22.jpg)'
- en: 'Figure 7.22: A cleaned review'
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.22：已清理的评论
- en: 'To get the actual input to the next step of the process, run the following
    command:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取下一步骤的实际输入，请运行以下命令：
- en: '[PRE43]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The input to the next step for the `reviews` command will look something like
    this:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`reviews` 命令的下一步骤输入大致如下所示：'
- en: '![Figure 7.23: Input for next step to the cleaned review'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.23：下一步骤的输入，已清理的评论'
- en: '](img/C13322_07_23.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_23.jpg)'
- en: 'Figure 7.23: Input for next step to the cleaned review'
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.23：下一步骤的输入，已清理的评论
- en: Congratulations! You have successfully preprocessed your first text dataset.
    The review data is now a matrix of 25,000 rows, or reviews, and 200 columns, or
    words. Next, we will learn how we can convert this data into embedding to make
    it easier to predict the sentiment.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经成功预处理了你的第一个文本数据集。评论数据现在是一个包含 25,000 行（即评论）和 200 列（即单词）的矩阵。接下来，我们将学习如何将这些数据转换为嵌入，以便更容易预测情感。
- en: Text Processing
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本处理
- en: Now that we have cleaned our dataset, we will convert it into a form that machine
    learning models can work with. Recall *Chapter 5*, *Mastering Structured Data*,
    where we discussed how neural networks cannot process words, so we need to represent
    words as numbers to be able to process them. Therefore, to be able to perform
    tasks such as sentiment analysis, we need to convert text into numbers.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清理好了数据集，将其转化为机器学习模型可以使用的形式。回顾一下*第 5 章*，*结构化数据的掌握*，我们讨论了神经网络无法处理单词，因此我们需要将单词表示为数字才能处理它们。因此，为了执行情感分析等任务，我们需要将文本转换为数字。
- en: 'So, the very first method we discussed was one-hot encoding, which performs
    poorly in the case of words, because words have certain relationships between
    them and one-hot encoding makes it so that the words are computed as if they were
    independent of each other. For example, let us assume we have three words: ''car,''
    ''truck,'' and ''ship.'' Now, ''car'' is closer to ''truck'' in terms of similarity,
    but it still has some similarity to ''ship.'' One-hot encoding fails to capture
    that relationship.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们首先讨论的方法是独热编码（one-hot encoding），它在处理单词时表现较差，因为单词之间存在某些关系，而独热编码却将单词当作彼此独立的来计算。例如，假设我们有三个单词：‘car’（车），‘truck’（卡车），和‘ship’（船）。现在，‘car’在相似度上更接近‘truck’，但它与‘ship’仍然有一定相似性。独热编码未能捕捉到这种关系。
- en: Word embeddings too are vector representations of words, but they capture the
    relationship of each word with another word. The different ways of getting word
    embedding are explained in the following section.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入也是单词的向量表示，但它们捕捉了每个单词与其他单词之间的关系。获取词嵌入的不同方法将在以下部分中解释。
- en: '**Count Embedding**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**计数嵌入（Count Embedding）**'
- en: '**Count embedding** is a simple vector representation of the word depending
    on the amount of times it appears in a piece of text. Assume a dataset where you
    have *n* unique words and *M* different records. To get the count embedding, you
    create an *N x M* matrix, where each row is a word and each column is a record.
    The values of any *(n,m)* location in the matrix will contain a count of the number
    of times a word *n* occurs in a record *m*.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**计数嵌入（Count embedding）** 是一种简单的词向量表示，依据单词在文本中出现的次数来构建。假设有一个数据集，其中包含 *n* 个唯一单词和
    *M* 个不同记录。要获得计数嵌入，你需要创建一个 *N x M* 矩阵，其中每一行代表一个单词，每一列代表一个记录。矩阵中任何 *(n,m)* 位置的值将包含单词
    *n* 在记录 *m* 中出现的次数。'
- en: '**TF-IDF Embedding**'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**TF-IDF 嵌入（TF-IDF Embedding）**'
- en: '**TF-IDF** is a way to obtain the importance of each word in a collection of
    words or document. It stands for term frequency-inverse document frequency. In
    TF-IDF, the importance of a word increases proportionally to the frequency of
    the word, but this importance is offset by the number of documents that have that
    word, thus helping to adjust for certain words that are used more frequently.
    In other words, the importance of a word is calculated using the frequency of
    the word in one data point of the training set. This importance is increased or
    decreased depending on the occurrence of the word in other data points of the
    training set.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**TF-IDF** 是一种衡量每个单词在一组单词或文档中重要性的方法。它代表词频-逆文档频率（term frequency-inverse document
    frequency）。在 TF-IDF 中，单词的重要性随着该单词的频率而增加，但这一重要性会被包含该单词的文档数量所抵消，从而有助于调整某些使用频率较高的单词。换句话说，单词的重要性是通过计算该单词在训练集中的一个数据点的频率来得出的。这一重要性会根据单词在训练集中的其他数据点的出现情况而增加或减少。'
- en: 'Weights generated by TF-IDF consist of two terms:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 生成的权重由两个术语组成：
- en: '**Term Frequency** (**TF**): The frequency of a word in the document, as shown
    in the following figure:'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词频**（**TF**）：单词在文档中出现的频率，如下图所示：'
- en: '![Figure 7.24: The term frequency equation'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.24：词频公式'
- en: '](img/C13322_07_24.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_24.jpg)'
- en: 'Figure 7.24: The term frequency equation'
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.24：词频公式
- en: where w is the word.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 w 是单词。
- en: '**Inverse Document Frequency** (**IDF**): The amount of information the word
    provides, as shown in the following figure:'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逆文档频率**（**IDF**）：单词提供的信息量，如下图所示：'
- en: '![Figure 7.25: The inverse document frequency equation'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.25：逆文档频率公式'
- en: '](img/C13322_07_25.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_25.jpg)'
- en: 'Figure 7.25: The inverse document frequency equation'
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.25：逆文档频率公式
- en: The weight is the product of these two terms. In case of TF-IDF, we replace
    the count of the word with this weight in the *N x M* matrix that we used in the
    count embedding section.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 权重是这两个项的乘积。在 TF-IDF 的情况下，我们用这个权重替代词频，在我们之前用于计数嵌入部分的 *N x M* 矩阵中。
- en: '**Continuous bag-of-words embedding**'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**连续词袋模型嵌入**'
- en: '**Continuous bag-of-words** (**CBOW**) works by using neural networks. It predicts
    a word when the input is its surrounding words. The input to the neural network
    is the one-hot vector of the surrounding words. The count of input words is selected
    using the window parameter. The network has only one hidden layer and the output
    layer of the network is activated using the softmax activation function to get
    the probability. The activation function between the layers is linear, but the
    method of updating the gradients is the same as normal neural networks.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**连续词袋模型**（**CBOW**）通过使用神经网络进行工作。当输入是某个单词的上下文单词时，它预测该单词。神经网络的输入是上下文单词的独热向量。输入单词的数量由窗口参数决定。网络只有一个隐藏层，输出层通过
    softmax 激活函数来获取概率。层与层之间的激活函数是线性的，但更新梯度的方法与常规神经网络相同。'
- en: The embedding matrix of the corpus is the weight between the hidden layer and
    the output layer. Thus, this embedding matrix will be of the *N x H* dimension,
    where *N* is the number of unique words in the corpus and *H* is the number of
    hidden layer nodes. CBOW works better than the two methods discussed previously
    due to its probabilistic nature and low memory requirements.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库的嵌入矩阵是隐藏层和输出层之间的权重。因此，这个嵌入矩阵的维度将是 *N x H*，其中 *N* 是语料库中唯一单词的数量，*H* 是隐藏层节点的数量。由于其概率性质和低内存需求，CBOW
    比之前讨论的两种方法表现更好。
- en: '![Figure 7.26: A representation of CBOW network'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.26：CBOW 网络的表示'
- en: '](img/C13322_07_26.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_26.jpg)'
- en: 'Figure 7.26: A representation of CBOW network'
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.26：CBOW 网络的表示
- en: '**Skip-gram embedding**'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**Skip-gram 嵌入**'
- en: 'Using a neural network, skip-gram predicts the surrounding words given an input
    word. The input here is the one-hot vector of the word and the output is the probability
    of the surrounding words. The number of output words is decided by the window
    parameter. Much like CBOW, this method uses a neural network with a single hidden
    layer and the activations are all linear except for the output layer, where we
    use the softmax function. One big difference though is how the error gets calculated:
    different errors are calculated for the different words being predicted and then
    all are added together to get the final error. The error for each individual word
    is calculated by subtracting the output probability vector with the target one-hot
    vector.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络，skip-gram 根据输入单词预测其周围的单词。这里的输入是单词的独热向量，输出是周围单词的概率。输出单词的数量由窗口参数决定。与 CBOW
    类似，这种方法使用一个只有单层隐藏层的神经网络，且所有激活函数均为线性，除了输出层，我们使用 softmax 函数。一个重要的区别是误差的计算方式：为每个被预测的单词计算不同的误差，然后将所有误差加起来得到最终的误差。每个单词的误差是通过将输出概率向量与目标独热向量相减来计算的。
- en: 'The embedding matrix here is the weight matrix between the input layer and
    the hidden layer. Thus, this embedding matrix will be of the *H x N* dimension,
    where *N* is the number of unique words in the corpus and *H* is the number of
    hidden layer nodes. Skip-gram works much better than CBOW for less frequent words,
    but is generally slower:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的嵌入矩阵是输入层和隐藏层之间的权重矩阵。因此，这个嵌入矩阵的维度将是 *H x N*，其中 *N* 是语料库中唯一单词的数量，*H* 是隐藏层节点的数量。对于较少频繁出现的单词，skip-gram
    的表现远好于 CBOW，但通常较慢：
- en: '![Figure 7.27: A representation of skip-gram network'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.27：skip-gram 网络的表示'
- en: '](img/C13322_07_27.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_27.jpg)'
- en: 'Figure 7.27: A representation of skip-gram network'
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.27：跳字模型的表示
- en: Tip
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示
- en: Use CBOW for datasets with less words but a high number of samples. Use skip-gram
    when working with a dataset with more words and a low number of samples.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对于词汇较少但样本量大的数据集，使用CBOW；对于词汇量较大且样本量较小的数据集，使用skip-gram。
- en: '**Word2Vec**'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**Word2Vec**'
- en: 'The `gensim` library:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim`库：'
- en: '[PRE44]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: To train the model, we need to pass the tokenized sentences as arguments to
    the `Word2Vec` class of `gensim`. `iter` is the number of epochs to train for,
    and `size` refers to the number of nodes in the hidden layer and decides the size
    of the embedding layer. `window` refers to the number of surrounding words that
    are considered when training the neural network. `min_count` refers to the minimum
    frequency required for a word to be considered. `workers` is the number of threads
    to use while training and `sg` refers to the training algorithm to be used, *0*
    for CBOW and *1* for skip-gram.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们需要将令牌化的句子作为参数传递给`gensim`的`Word2Vec`类。`iter`是训练的轮数，`size`指的是隐藏层节点数，也决定了嵌入层的大小。`window`是指在训练神经网络时，考虑的上下文单词数。`min_count`是指某个单词至少出现多少次才能被考虑。`workers`是训练时使用的线程数，`sg`是指使用的训练算法，*0*代表CBOW，*1*代表skip-gram。
- en: 'To get the number of unique words in the trained embedding, you can use the
    following:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取训练好的词向量中的唯一词汇数量，可以使用以下代码：
- en: '[PRE45]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Before we use these embeddings, we need to make sure that they are correct.
    To do that, we find the similar words:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这些词向量之前，我们需要确保它们是正确的。为此，我们通过查找相似的单词来验证：
- en: '[PRE46]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.28: Similar words'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.28：相似单词'
- en: '](img/C13322_07_28.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_28.jpg)'
- en: 'Figure 7.28: Similar words'
  id: totrans-279
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.28：相似单词
- en: 'To save your embeddings to a file, use the following code:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 要将你的词向量保存到文件中，请使用以下代码：
- en: '[PRE47]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To load a pretrained embedding, you can use this function:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载预训练的词向量，可以使用这个函数：
- en: '[PRE48]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The function first reads the embedding file `filename` and gets all the embedding
    vectors present in the file. Then, it creates an embedding matrix that stacks
    the embedding vectors together. The `num_words` parameter limits the size of the
    vocabulary and can be helpful in cases where the training time of the NLP algorithm
    is too high. `word_index` is a dictionary with the key as unique words of the
    corpus and the value as the index of the word. `embedding_dim` is the size of
    the embedding vectors as specified while training.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数首先读取嵌入文件`filename`，并获取文件中所有的嵌入向量。然后，它会创建一个嵌入矩阵，将这些嵌入向量堆叠在一起。`num_words`参数限制了词汇表的大小，在NLP算法训练时间过长的情况下非常有用。`word_index`是一个字典，键是语料库中的唯一单词，值是该单词的索引。`embedding_dim`是训练时指定的嵌入向量的大小。
- en: Tip
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示
- en: 'There are a lot of really good pretrained embeddings available. Some of the
    popular ones are GloVe: https://nlp.stanford.edu/projects/glove/ and fastText:
    https://fasttext.cc/docs/en/english-vectors.html'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '有很多非常好的预训练词向量可供使用。一些流行的包括GloVe: https://nlp.stanford.edu/projects/glove/ 和
    fastText: https://fasttext.cc/docs/en/english-vectors.html'
- en: 'Exercise 56: Creating Word Embeddings Using Gensim'
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习56：使用Gensim创建词向量
- en: In this exercise, we will create our own Word2Vec embedding using the `gensim`
    library. The word embedding will be created for the IMDB movie review dataset
    that we have been working with. We will take off from where we left in *Exercise
    55*.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用`gensim`库创建我们自己的Word2Vec词向量。这个词向量将为我们正在使用的IMDB电影评论数据集创建。我们将从*练习55*的结束部分继续。
- en: The reviews variable has reviews in the token form but they have been converted
    into numbers. The `gensim` Word2Vec requires tokens in the string form, so we
    backtrack to where we converted the tokens back to sentences in step 6 of *Exercise
    55*.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评审变量包含的是以令牌形式表示的评论，但它们已经被转换成数字。`gensim`的Word2Vec要求令牌为字符串形式，因此我们回溯到步骤6中将令牌重新转换为句子的部分，即*练习55*。
- en: '[PRE49]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The tokens of the first review are as follows:'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一条评论的令牌如下：
- en: '![Figure 7.29: Tokens of first review'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.29：第一条评论的令牌'
- en: '](img/C13322_07_29.jpg)'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_29.jpg)'
- en: 'Figure 7.29: Tokens of first review'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.29：第一条评论的令牌
- en: 'Now, we convert the lists in each row into a single list using the `apply`
    function of pandas, using the following code:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用`pandas`的`apply`函数将每一行的列表转换为单个列表，使用如下代码：
- en: '[PRE50]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, we feed this preprocessed data into Word2Vec to create the word embedding:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将这些预处理过的数据输入到Word2Vec中，以创建词向量：
- en: '[PRE51]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let us check how well the model performs by viewing some similar words:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过查看一些相似的单词来检查模型的表现：
- en: '[PRE52]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The most similar words to ''`insight`'' in the dataset are:'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集中与'`insight`'最相似的词语是：
- en: '![Figure 7.30: Similar words to ''insight'''
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.30：与''insight''相似的词语'
- en: '](img/C13322_07_30.jpg)'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_30.jpg)'
- en: 'Figure 7.30: Similar words to ''insight'''
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.30：与'insight'相似的词语
- en: 'To obtain the similarity between two words, use:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获得两个词之间的相似度，请使用：
- en: '[PRE53]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output similarity is shown here:![Figure 7.31: Similarity output'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里显示了相似度的输出结果：![图7.31：相似度输出
- en: '](img/C13322_07_31.jpg)'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_31.jpg)'
- en: 'Figure 7.31: Similarity output'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.31：相似度输出
- en: The similarity score ranges from `0` to `1`, where `1` means that both words
    are the same, and `0` means that both words are completely different and not related
    in any way.
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相似度分数的范围是从`0`到`1`，其中`1`表示两个词完全相同，`0`表示两个词完全不同，毫无关联。
- en: Plot the embedding on a 2D space to understand what words are found to be similar
    to each other.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将嵌入结果绘制在二维空间中，以理解哪些词语被发现彼此相似。
- en: First, convert the embedding into two dimensions using PCA. We will plot only
    the first 200 words. (You can plot more if you like.)
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，使用PCA将嵌入转换为二维。我们将仅绘制前200个词。（如果愿意，你可以绘制更多的词。）
- en: '[PRE54]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, plot the result on a scatter plot using `matplotlib`:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用`matplotlib`将结果绘制成散点图：
- en: '[PRE55]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Your output should look like the following:'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你的输出结果应该如下所示：
- en: '![Figure 7.32: Representation of embedding of first 200 words using PCA'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.32：使用PCA表示前200个词的嵌入'
- en: '](img/C13322_07_32.jpg)'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_32.jpg)'
- en: 'Figure 7.32: Representation of embedding of first 200 words using PCA'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.32：使用PCA表示前200个词的嵌入
- en: Note
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The axes do not mean anything in the representation of word embedding. The representation
    simply shows the closeness of different words.
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在词嵌入的表示中，坐标轴没有任何特定含义。表示仅显示不同词语之间的相似度。
- en: 'Save the embedding to a file so that you can retrieve it later:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将嵌入保存到文件中，以便稍后检索：
- en: '[PRE56]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Congratulations! You just created your first word embedding. You can play around
    with the embedding and view the similarity between different words.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚创建了你的第一个词嵌入。你可以玩转这些嵌入，查看不同词语之间的相似性。
- en: 'Activity 19: Predicting Sentiments of Movie Reviews'
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动19：预测电影评论的情感
- en: 'In this activity, we will attempt to predict sentiments of movie reviews. The
    dataset (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter07)
    comprises 25,000 movie reviews sourced from IMDB with their sentiment (positive
    or negative). Let''s look at the following scenario: You work at a DVD rental
    company, which has to predict the number of DVDs to create of a certain movie
    depending on how it is being perceived by the reviewers. To do this, you create
    a machine learning model that can analyze reviews to figure out how the movie
    is being perceived.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将尝试预测电影评论的情感。数据集（https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter07）包含25,000条来自IMDB的电影评论及其情感（正面或负面）。让我们看看以下情景：你在一家DVD租赁公司工作，必须根据评论者的评价预测某部电影需要制作的DVD数量。为此，你创建一个机器学习模型，能够分析评论并判断电影的受欢迎程度。
- en: Read and preprocess the movie reviews.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取并预处理电影评论。
- en: Create the word embedding of the reviews.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建评论的词嵌入。
- en: 'Create a fully connected neural network to predict sentiments, much like the
    neural network models we created in *Chapter 5*: *Mastering Structured Data*.
    The input will be the word embedding of the reviews and the output of the model
    will be either `1` (positive sentiment) or `0` (negative sentiment).'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个完全连接的神经网络来预测情感，这与我们在*第5章*中创建的神经网络模型类似：*掌握结构化数据*。输入将是评论的词嵌入，而模型的输出将是`1`（正面情感）或`0`（负面情感）。
- en: Note
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 378.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第378页找到。
- en: The output is a little cryptic because stop words and punctuations have been
    removed, but you can still understand the general sense of the review.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果有些晦涩，因为停用词和标点符号已经被移除，但你仍然可以理解评论的大致意思。
- en: Congratulations! You just created your first NLP module. You should find that
    the model gives an accuracy of of around 76% which is quite low. This is because
    it is predicting sentiments based on individual words; it has no way of figuring
    out the context of the review. For example, it will predict "not good" as a positive
    sentiment as it sees the word 'good.' If it could look at multiple words, it would
    understand that this is a negative sentiment. In the next section, we will learn
    how to create neural networks that can retain information of the past.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你刚刚创建了你的第一个NLP模块。你应该会发现该模型的准确率大约是76%，这是比较低的。这是因为它是基于单个词来预测情感的；它无法理解评论的上下文。例如，它会将“not
    good”预测为积极情感，因为它看到了“good”这个词。如果它能看到多个词，就能理解这是负面情感。在接下来的章节中，我们将学习如何创建能够保留过去信息的神经网络。
- en: Recurrent Neural Networks (RNNs)
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 递归神经网络（RNN）
- en: Until now, none of the problems we discussed had a temporal dependence, which
    means that the prediction depends not only on the current input but also on the
    past inputs. For example, in the case of the dog vs. cat classifier, we only needed
    the picture of the dog to classify it as a dog. No other information or images
    were required. Instead, if you want to make a classifier that predicts if a dog
    is walking or standing, you will require multiple images in a sequence or a video
    to figure out what the dog is doing. RNNs are like the fully connected networks
    that we talked about. The only change is that an RNN has memory that stores information
    about the previous inputs as states. The outputs of the hidden layers are fed
    in as inputs for the next input.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的所有问题都没有时间依赖性，这意味着预测不仅依赖于当前输入，还依赖于过去的输入。例如，在狗与猫分类器的案例中，我们只需要一张狗的图片就能将其分类为狗。不需要其他信息或图片。而如果你想创建一个分类器，用于预测狗是走路还是站着，你将需要一系列的图片或视频，以确定狗的行为。RNNs就像我们之前讨论的完全连接的网络。唯一的区别是，RNN具有存储关于之前输入的信息作为状态的记忆。隐藏层的输出作为下一个输入的输入。
- en: '![Figure 7.33: Representation of recurrent neural network'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.33：递归神经网络的表示'
- en: '](img/C13322_07_33.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_33.jpg)'
- en: 'Figure 7.33: Representation of recurrent neural network'
  id: totrans-338
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.33：递归神经网络的表示
- en: From the image, you can understand how the outputs of the hidden layers are
    used as inputs for the next input. This acts as a memory element in the neural
    network. Another thing to keep in mind is that the output of a normal neural network
    is a function of the input and weights of the network.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像中，你可以理解隐藏层的输出是如何作为下一个输入的输入。这在神经网络中充当记忆元素。另一个需要注意的事项是，普通神经网络的输出是输入和网络权重的函数。
- en: his allows us to randomly input any data point to get the right output. However,
    this is not the case with RNNs. In the case of RNNs, our output depends on the
    previous inputs, so we need to feed in the input in the correct sequence.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得我们能够随机输入任何数据点以获得正确的输出。然而，RNNs（递归神经网络）却不是这样。在RNN的情况下，我们的输出取决于之前的输入，因此我们需要按照正确的顺序输入数据。
- en: '![Figure 7.34: Representation of recurrent layer'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.34：递归层的表示'
- en: '](img/C13322_07_34.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_34.jpg)'
- en: 'Figure 7.34: Representation of recurrent layer'
  id: totrans-343
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.34：递归层的表示
- en: 'In the preceding image, you can see a single RNN layer on the left in the "folded"
    model. U is the input weight, V is the output weight, and W is the weight associated
    with the memory input. The memory of the RNN is also referred to as state. The
    "unfolded" model on the right shows how the RNN works for the input sequence [xt-1,
    xt, xt+1]. The model differs based on the kind of application. For example, in
    case of sentiment analysis, the input sequence will require only one output in
    the end. The unfolded model for this problem is shown here:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图像中，你可以看到左边“折叠”模型中的单个RNN层。U是输入权重，V是输出权重，W是与记忆输入相关的权重。RNN的记忆也被称为状态。右边的“展开”模型展示了RNN如何处理输入序列[xt-1,
    xt, xt+1]。该模型会根据应用的不同而有所变化。例如，在情感分析中，输入序列最终只需要一个输出。这个问题的展开模型如下所示：
- en: '![Figure 7.35: Unfolded representation of a recurrent layer used to perform
    sentiment analysis'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.35：用于情感分析的递归层展开表示'
- en: '](img/C13322_07_35.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_35.jpg)'
- en: 'Figure 7.35: Unfolded representation of a recurrent layer used to perform sentiment
    analysis'
  id: totrans-347
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.35：用于情感分析的递归层展开表示
- en: LSTMs
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSTM（长短期记忆网络）
- en: '**Long short-term memory** (**LSTM**) cell is a special kind of RNN cell, capable
    of retaining information over long-term periods of time. Hochreiter and Schmidhuber
    introduced LSTMs in 1997\. RNNs suffer from the vanishing gradient problem. They
    lose information detected over long periods of time. For example, if we are performing
    sentiment analysis on a text and the first sentence says "I am happy today" and
    then the rest of the text is devoid of any sentiments, the RNN will not do a good
    job of detecting that the sentiment of the text is happy. Long short-term memory
    (LSTM) cells overcome this issue by storing certain inputs for a longer time without
    forgetting them. Most real-world recurrent machine learning implementations are
    done using LSTMs. The only difference between RNN cells and LSTM cells is the
    memory states. Every RNN layer takes an input of the memory state and outputs
    a memory state, whereas every LSTM layer takes a long-term and a short-term memory
    as the input and outputs both the long and the short-term memories. The long-term
    memory allows the network to retain information for a longer time.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTM**）单元是一种特殊的 RNN 单元，能够在长时间段内保持信息。Hochreiter 和 Schmidhuber 在
    1997 年引入了 LSTM。RNN 遭遇了梯度消失问题。它们在长时间段内会丢失所检测到的信息。例如，如果我们在对一篇文本进行情感分析，第一句话说“我今天很高兴”，然后接下来的文本没有任何情感，RNN
    就无法有效地检测到文本的情感是高兴的。长短期记忆（LSTM）单元通过长时间存储某些输入而不忘记它们，克服了这个问题。大多数现实世界的递归机器学习实现都使用
    LSTM。RNN 单元和 LSTM 单元的唯一区别在于记忆状态。每个 RNN 层接受一个记忆状态作为输入，并输出一个记忆状态，而每个 LSTM 层则接受长期记忆和短期记忆作为输入，并输出这两者。长期记忆使得网络能够保留信息更长时间。'
- en: 'LSTM cells are implemented in Keras, and you easily can add an LSTM layer into
    your model:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 单元在 Keras 中已经实现，你可以轻松地将 LSTM 层添加到模型中：
- en: '[PRE57]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Here, `units` is the number of nodes in the layer, `activation` is the activation
    function to use for the layer. `recurrent_dropout` and `dropout` are the dropout
    probability for the recurrent state and input respectively. `return_sequences`
    specifies if the output should contain the sequence or not; this is made `True`
    when you plan to use another recurrent layer after the current layer.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`units` 是层中节点的数量，`activation` 是该层使用的激活函数。`recurrent_dropout` 和 `dropout`
    分别是递归状态和输入的丢弃概率。`return_sequences` 指定输出是否应包含序列；当你计划在当前层后面使用另一个递归层时，这一选项设置为 `True`。
- en: Note
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: LSTMs almost always work better than RNNs.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 通常比 RNN 更有效。
- en: 'Exercise 57: Performing Sentiment Analysis Using LSTM'
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 57：使用 LSTM 执行情感分析
- en: In this exercise, we will modify the model we created for the previous activity,
    to make it use an LSTM cell. We will use the same IMDB movie review dataset that
    we have been working with. Most of the preprocessing steps are like those in *Activity
    19*.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将修改之前为前一个活动创建的模型，使其使用 LSTM 单元。我们将继续使用之前处理过的 IMDB 电影评论数据集。大多数预处理步骤与*活动
    19*中的步骤相似。
- en: 'Read the IMDB movie review dataset using pandas in Python:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 在 Python 中读取 IMDB 电影评论数据集：
- en: '[PRE58]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Convert the tweets to lowercase to reduce the number of unique words:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将推文转换为小写，以减少唯一词汇的数量：
- en: '[PRE59]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Clean the reviews using RegEx with the `clean_str` function:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 RegEx 和 `clean_str` 函数清理评论：
- en: '[PRE60]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Next, remove stop words and other frequently occurring unnecessary words from
    the reviews. This step converts strings into tokens (which will be helpful in
    the next step):'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，去除评论中的停用词和其他频繁出现的不必要的词语。此步骤将字符串转换为标记（这在下一步中会有所帮助）：
- en: '[PRE61]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Combine the tokens to get a string and then drop any review that does not have
    anything in it after the stop-word removal:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些标记组合成一个字符串，然后删除任何在去除停用词后内容为空的评论：
- en: '[PRE62]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Tokenize the reviews using the Keras Tokenizer and convert them into numbers:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Keras Tokenizer 对评论进行标记化，并将它们转换为数字：
- en: '[PRE63]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Finally, pad the tweets to have a maximum of 100 words. This will remove any
    words after the 100-word limit and add 0s if the number of words is less than
    100:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将推文填充为最多 100 个单词。如果单词数少于 100，将补充 0，超过 100 则会删除多余的单词：
- en: '[PRE64]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Load the previously created embedding to get the embedding matrix using the
    `load_embedding` function discussed in the *Text Processing* section, by using
    the following code:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `load_embedding` 函数加载先前创建的嵌入，获取嵌入矩阵，该函数在*文本处理*部分中有讨论，使用以下代码：
- en: '[PRE65]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Split the data into training and testing sets with an 80:20 split. This can
    be modified to find the best split:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集，按 80:20 的比例划分。此比例可以调整，以找到最佳的划分方式：
- en: '[PRE66]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Create and compile the Keras model with one LSTM layer. You can experiment
    with different layers and hyperparameters:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建并编译包含一个LSTM层的Keras模型。你可以尝试不同的层和超参数：
- en: '[PRE67]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Train the model on the data for 10 epochs to see if it performs better than
    the one in *Activity 1*, by using the following code:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码训练模型10个epoch，以查看其表现是否优于*活动1*中的模型：
- en: '[PRE68]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Check the accuracy of the model:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查模型的准确度：
- en: '[PRE69]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The accuracy of the LSTM model is:'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LSTM模型的准确度为：
- en: '![Figure 7.36: LSTM model accuracy'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.36：LSTM模型准确度'
- en: '](img/C13322_07_36.jpg)'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_36.jpg)'
- en: 'Figure 7.36: LSTM model accuracy'
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.36：LSTM模型准确度
- en: 'Plot the confusion matrix of the model to get a proper sense of the model''s
    prediction:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制模型的混淆矩阵，以便更好地理解模型的预测：
- en: '[PRE70]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '![Figure 7.37: Confusion matrix of the model (0 = negative sentiment, 1 = positive
    sentiment)'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.37：模型的混淆矩阵（0 = 负面情感，1 = 正面情感）'
- en: '](img/C13322_07_37.jpg)'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_07_37.jpg)'
- en: 'Figure 7.37: Confusion matrix of the model (0 = negative sentiment, 1 = positive
    sentiment)'
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.37：模型的混淆矩阵（0 = 负面情感，1 = 正面情感）
- en: 'Check the performance of the model by seeing the sentiment predictions on random
    reviews using the following code:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码检查模型的表现，通过查看随机评论的情感预测结果：
- en: '[PRE71]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output is as follows:'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.38: A negative review from the IMDB dataset'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.38：来自IMDB数据集的负面评论'
- en: '](img/C13322_07_38.jpg)'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_07_38.jpg)'
- en: 'Figure 7.38: A negative review from the IMDB dataset'
  id: totrans-395
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.38：来自IMDB数据集的负面评论
- en: Congratulations! You just implemented an RNN to predict sentiments of a movie
    review. This network works a little better than the previous network we created.
    Play around with the architecture and hyperparameters of the network to improve
    the accuracy of the model. You can also try using pretrained word embedding from
    either fastText or GloVe to improve the accuracy of the model.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚实现了一个RNN来预测电影评论的情感。这个网络比我们之前创建的网络表现得稍微好一点。可以通过调整网络架构和超参数来提高模型的准确度。你还可以尝试使用来自fastText或GloVe的预训练词嵌入来提高模型的准确度。
- en: 'Activity 20: Predicting Sentiments from Tweets'
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动20：从推文中预测情感
- en: 'In this activity, we will attempt to predict sentiments of a tweet. The provided
    dataset (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter07)
    contains 1.5 million tweets and their sentiments (positive or negative). Let''s
    look at the following scenario: You work at a big consumer organization, which
    recently created a Twitter account. Some of the customers who have had a bad experience
    with your company are taking to Twitter to express their sentiments, which is
    causing a decline in the reputation of the company. You have been tasked to identify
    these tweets so that the company can get in touch with them to provide better
    support. You do this by creating a sentiment predictor, which can determine whether
    the sentiment of a tweet is positive or negative. Before using your new sentiment
    predictor on actual tweets about your company, you will test it on the provided
    tweets dataset.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将尝试预测推文的情感。提供的数据集（https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter07）包含150万条推文及其情感（正面或负面）。让我们来看以下情境：你在一家大型消费者组织工作，该公司最近创建了一个Twitter账户。部分对公司有不满经历的顾客开始在Twitter上表达他们的情感，导致公司声誉下降。你被指派识别这些推文，以便公司可以与这些顾客取得联系，提供更好的支持。你通过创建一个情感预测器来完成这项任务，预测器可以判断推文的情感是正面还是负面。在将你的情感预测器应用到关于公司实际的推文之前，你将先在提供的推文数据集上进行测试。
- en: Read the data and remove all unnecessary information.
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据并去除所有不必要的信息。
- en: Clean the tweets, tokenize them, and finally convert them into numbers.
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理推文，进行分词，最后将其转换为数字。
- en: Load GloVe Twitter embedding and create the embedding matrix (https://nlp.stanford.edu/projects/glove/).
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载GloVe Twitter嵌入并创建嵌入矩阵（https://nlp.stanford.edu/projects/glove/）。
- en: Create an LSTM model to predict the sentiment.
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个LSTM模型来预测情感。
- en: Note
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 383.
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第383页找到。
- en: Congratulations! You just created a machine learning module to predict sentiments
    from tweets. You can now deploy this using Twitter API to perform real-time sentiment
    analysis on tweets. You can play around with different embeddings from GloVe and
    fastText and see how much improvement you can get on your model.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚创建了一个机器学习模块，用于从推文中预测情感。现在你可以使用Twitter API将其部署，用于实时推文情感分析。你可以尝试不同的GloVe和fastText嵌入，并查看模型能提高多少准确度。
- en: Summary
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we learned how computers understand human language. We first
    learned what RegEx is and how it helps data scientists analyze and clean text
    data. Next, we learned about stop words, what they are, and why they are removed
    from the data to reduce the dimensionality. Next, we next learned about sentence
    tokenization and its importance, followed by word embedding. Embedding is a topic
    that we covered in *Chapter 5*: *Mastering Structured Data*; here, we learned
    how to create word embedding to boost our NLP model''s performance. To create
    better models, we looked at a RNNs, a special type of neural network that retains
    memory of past inputs. Finally, we learned about LSTM cells and how they are better
    than normal RNN cells.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了计算机如何理解人类语言。我们首先了解了什么是正则表达式（RegEx），以及它如何帮助数据科学家分析和清洗文本数据。接下来，我们了解了停用词，它们是什么，以及为什么要从数据中去除停用词以减少维度。接着，我们学习了句子切分及其重要性，然后是词嵌入。词嵌入是我们在*第5章*《*掌握结构化数据*》中讲解的主题；在这里，我们学习了如何创建词嵌入以提升我们的自然语言处理（NLP）模型的性能。为了创建更好的模型，我们研究了循环神经网络（RNN），这是一种特殊类型的神经网络，能够保留过去输入的记忆。最后，我们学习了LSTM单元及其为何优于普通RNN单元。
- en: Now that you have completed this chapter, you are capable of handling textual
    data and creating machine learning models for NLP. In the next chapter, you will
    learn how to make models faster using transfer learning and a few tricks of the
    craft.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经完成了本章的学习，你已经能够处理文本数据并为自然语言处理创建机器学习模型。在下一章中，你将学习如何通过迁移学习和一些技巧加速模型的训练。
