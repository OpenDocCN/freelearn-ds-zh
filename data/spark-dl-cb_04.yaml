- en: Pain Points of Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络的痛点
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Introduction to feedforward networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络简介
- en: Sequential workings of RNNs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN的顺序工作
- en: 'Paint point #1 – the vanishing gradient problem'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 痛点＃1 - 梯度消失问题
- en: 'Pain point #2 – the exploding gradient problem'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 痛点＃2 - 梯度爆炸问题
- en: Sequential workings of LSTMs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM的顺序工作
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Recurrent neural networks have proven to be incredibly efficient at tasks involving
    the learning and prediction of sequential data. However, when it comes to natural
    language, the question of long-term dependencies comes into play, which is basically
    remembering the context of a particular conversation, paragraph, or sentence in
    order to make better predictions in the future. For example, consider a sentence
    that says:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络已被证明在涉及学习和预测序列数据的任务中非常高效。然而，当涉及自然语言时，长期依赖性的问题就出现了，这基本上是记住特定对话、段落或句子的上下文，以便在未来做出更好的预测。例如，考虑一个句子，说：
- en: '*Last year, I happened to visit China. Not only was Chinese food different
    from the Chinese food available everywhere else in the world, but the people were
    extremely warm and hospitable too. In my three years of stay in this beautiful
    country, I managed to pick up and speak very good....*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*去年，我碰巧访问了中国。中国的食物不仅与世界上其他地方提供的中国食物不同，而且人们也非常热情好客。在这个美丽的国家呆了三年，我学会了说一口很好的......*'
- en: If the preceding sentence were fed into a recurrent neural network to predict
    the next word in the sentence (such as Chinese), the network would find it difficult
    since it has no memory of the context of the sentence. This is what we mean by
    long-term dependencies. In order to predict the word Chinese correctly, the network
    needs to know the context of the sentence as well as remember the fact that I
    happened to visit China last year. Recurrent neural networks therefore become
    inefficient at performing such tasks. However, this problem is overcome by **Long
    Short-Term Memory Units** (**LSTMs**), which are capable of remembering long-term
    dependencies and storing information in the cell state. LSTMs will be discussed
    later on, but the bulk of this chapter will focus on a basic introduction to Neural
    Networks, activation functions, Recurrent Networks, some of the main pain points
    or drawbacks of Recurrent Networks, and finally how these drawbacks may be overcome
    by the use of LSTMs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将前面的句子输入到循环神经网络中以预测句子中的下一个单词（比如中国），网络会发现很难，因为它没有句子上下文的记忆。这就是我们所说的长期依赖性。为了正确预测单词“中国”，网络需要知道句子的上下文，还需要记住我碰巧去年访问中国的事实。因此，循环神经网络在执行此类任务时效率低下。然而，**长短期记忆单元**（**LSTM**）可以克服这个问题，它能够记住长期依赖性并将信息存储在细胞状态中。稍后将讨论LSTM，但本章的大部分内容将重点介绍神经网络、激活函数、循环网络、循环网络的一些主要痛点或缺点，以及如何通过使用LSTM来克服这些缺点。
- en: Introduction to feedforward networks
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈网络简介
- en: To understand recurrent networks, first you have to understand the basics of
    feedforward networks. Both of these networks are named after the way they move
    information through a series of mathematical operations performed at the nodes
    of the network. One feeds information in only one direction through every node
    (never touching a given node twice), while the other cycles it through a loop
    and feeds it back to the same node (kind of like a feedback loop). It is easily
    understood how the first kind is called a **feedforward network,** while the latter
    is recurrent.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解循环网络，首先必须了解前馈网络的基础知识。这两种网络都是根据它们通过网络节点执行的一系列数学运算的方式命名的。一种只通过每个节点向一个方向传递信息（永远不会两次触及给定节点），而另一种则通过循环将信息传递并将其反馈到同一节点（有点像反馈循环）。很容易理解第一种称为**前馈网络**，而后者是循环的。
- en: Getting ready
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: The most important concept while understanding any neural network diagram is
    the concept of computational graphs. Computational graphs are nothing but the
    nodes of the neural network connected to each other, and each node performs a
    particular mathematical function.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 理解任何神经网络图表的最重要概念是计算图的概念。计算图实际上就是相互连接的神经网络节点，每个节点执行特定的数学函数。
- en: How to do it...
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: 'Feedforward neural networks channel the inputs (to the input layer) through
    a set of computational nodes which are nothing but mathematical operators and
    activation functions arranged in layers to calculate the network outputs. The output
    layer is the final layer of the neural network and usually contains linear functions.
    The layers between the input layer and the output layer are called **hidden layers** and
    usually contain nonlinear elements or functions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络通过一组计算节点（即数学运算符和激活函数）将输入（到输入层）传递到计算网络输出的层。输出层是神经网络的最终层，通常包含线性函数。输入层和输出层之间的层称为**隐藏层**，通常包含非线性元素或函数：
- en: 'The following diagram (a) shows how nodes are interconnected in feedforward
    neural networks with many layers:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下图（a）显示了前馈神经网络中节点如何相互连接：
- en: '![](img/00095.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00095.jpeg)'
- en: FeedForward Neural Network
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络
- en: Feedforward neural networks mainly differ from each other by the type of functions
    (activation functions) that are used in the hidden-layer nodes. They also differ
    from each other by the algorithms that are used to optimize the other parameters
    of the network during training.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前馈神经网络主要通过隐藏层节点中使用的函数（激活函数）的类型来区分彼此。它们还通过在训练期间用于优化网络的其他参数的算法来区分彼此。
- en: The relationships between nodes shown in the preceding diagram need not be fully
    populated for every node; optimization strategies usually start with a large number
    of hidden nodes and tune the network by eliminating connections, and possibly
    nodes, as training progresses. It may not be necessary to utilize every node during
    the training process.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的图中显示的节点之间的关系不需要对每个节点进行完全填充；优化策略通常从大量的隐藏节点开始，并通过消除连接和可能的节点来调整网络，随着训练的进行。在训练过程中可能不需要利用每个节点。
- en: How it works...
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The neuron is the basic structural element of any neural network. A neuron
    can be thought of as a simple mathematical function or operator that operates
    on the input flowing through it to produce an output flowing out of it. The inputs
    to a neuron are multiplied by the node''s weight matrix, summed over all the inputs,
    translated, and passed through an activation function. These are basically matrix
    operations in mathematics as described here:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是任何神经网络的基本结构元素。神经元可以被看作是一个简单的数学函数或运算符，它对通过它流动的输入进行操作，以产生从它流出的输出。神经元的输入与节点的权重矩阵相乘，对所有输入求和，进行平移，并通过激活函数传递。这基本上是数学中的矩阵运算，如下所述：
- en: The computational graph representation of a neuron is shown in the preceding
    diagram (b).
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经元的计算图表示如前图(b)所示。
- en: 'The transfer function for a single neuron or node is written as follows:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单个神经元或节点的传递函数如下所示：
- en: '![](img/00096.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00096.jpeg)'
- en: Here, *x*[ *i* ]is the input to the ith node, *w*[ *i* ]is the weight term associated
    with the *i*^(th) node, *b* is the bias which is generally added to prevent overfitting, *f*(⋅)
    is the activation function operating over the inputs flowing into the node, and *y* is
    the output from the node.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*[ *i* ]是第i个节点的输入，*w*[ *i* ]是与第i个节点相关的权重项，*b*是通常添加的偏差，以防止过拟合，*f*(⋅)是作用于流入节点的输入的激活函数，*y*是节点的输出。
- en: Neurons with sigmoidal activation functions are commonly used in the hidden
    layer(s) of the neural network, and the identity function is usually used in the
    output layer.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有S形激活函数的神经元通常用于神经网络的隐藏层，并且恒等函数通常用于输出层。
- en: The activation functions are generally chosen in a manner to ensure the outputs
    from the node are strictly increasing, smooth (continuous first derivative), or
    asymptotic.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活函数通常被选择为确保节点的输出严格增加、平滑（连续的一阶导数）或渐近的方式。
- en: 'The following logistic function  is used as a sigmoidal activation function:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下的逻辑函数被用作S形激活函数：
- en: '![](img/00097.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00097.jpeg)'
- en: A neural trained using backpropagation algorithm may learn faster if the activation
    function is antisymmetric, that is, *f*(-*x*) = -*f*(*x*) as in the case of the
    sigmoidal activation function. The backpropagation algorithm will be discussed
    in detail in the following sections of this chapter.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反向传播算法训练的神经元，如果激活函数是反对称的，即*f*(-*x*) = -*f*(*x*)，可能会学习得更快，就像S形激活函数的情况一样。反向传播算法将在本章的后续部分中详细讨论。
- en: 'The logistic function, however, is not antisymmetric, but can be made antisymmetric
    by a simple scaling and shift, resulting in the hyperbolic tangent function which
    has  a first derivative described by *f *''(*x*) = 1 - *f *²(*x*),  as shown in
    the following mathematical function:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逻辑函数不是反对称的，但可以通过简单的缩放和移位来使其成为反对称，从而得到具有由*f*(*x*) = 1 - *f*²(*x*)描述的一阶导数的双曲正切函数，如下数学函数所示：
- en: '![](img/00098.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00098.jpeg)'
- en: The simple form of the sigmoidal function and its derivative allows for the
    quick and accurate calculation of the gradients needed to optimize the selection
    of the weights and biases and carry out second-order error analysis.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: S形函数及其导数的简单形式允许快速准确地计算梯度，以优化权重和偏差的选择，并进行二阶误差分析。
- en: There's more...
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'At every neuron/node in the layers of a neural network, a series of matrix
    operations are performed. A more mathematical way of visualizing the feedforward
    network is given in the following diagram, which will help you to better understand
    the operations at each node/neuron:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的各层中的每个神经元/节点上执行一系列矩阵运算。下图以更数学化的方式展示了前馈网络，这将帮助您更好地理解每个节点/神经元的操作：
- en: '![](img/00099.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00099.jpeg)'
- en: Intuitively, we can see that the inputs (which are vectors or matrices) are
    first multiplied by weight matrices. A bias is added to this term and then activated
    using an activation function (such as ReLU, tanh, sigmoid, threshold, and so on)
    to produce the output. Activation functions are key in ensuring that the network
    is able to learn linear as well as non-linear functions.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '直观地，我们可以看到输入（向量或矩阵）首先被权重矩阵相乘。然后添加一个偏差项，然后使用激活函数（如ReLU、tanh、sigmoid、阈值等）激活以产生输出。激活函数是确保网络能够学习线性和非线性函数的关键。 '
- en: 'This output then flows into the next neuron as its input, and the same set
    of operations are performed all over again. A number of such neurons combine together
    to form a layer (which performs a certain function or learns a certain feature
    of the input vector), and many such layers combine together to form a feedforward
    neural network that can learn to recognize inputs completely, as shown in the
    following diagram:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，这个输出作为下一个神经元的输入，然后再次执行相同的一系列操作。许多这样的神经元组合在一起形成一个层（执行输入向量的某个功能或学习某个特征），许多这样的层组合在一起形成一个前馈神经网络，可以完全学会识别输入，如下图所示：
- en: '![](img/00100.gif)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.gif)'
- en: 'Let''s suppose our feedforward network has been trained to classify images
    of dogs and images of cats. Once the network is trained, as shown in the following
    diagram, it will learn to label images as dog or cat when presented with new images:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们的前馈网络已经训练好，可以对狗和猫的图像进行分类。一旦网络训练好，如下图所示，它将学会在呈现新图像时将图像标记为狗或猫：
- en: '![](img/00101.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00101.jpeg)'
- en: In such networks, there is no relation between the present output and the previous
    or future outputs.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这样的网络中，当前输出与先前或未来的输出之间没有关系。
- en: This means the feedforward network can basically be exposed to any random collection
    of images and the first image it is exposed to will not necessarily alter how
    it classifies the second or third images. Therefore, we can say that the output
    at time step *t* is independent of the output at time step  *t - 1*.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这意味着前馈网络基本上可以暴露给任何随机的图像集合，它暴露给的第一张图像不一定会改变它对第二张或第三张图像的分类方式。因此，我们可以说在时间步*t*的输出与时间步*t-1*的输出是独立的。
- en: Feedforward networks work well in such cases as image classification, where
    the data is not sequential. Feedforward networks also perform well when used on
    two related variables such as temperature and location, height and weight, car
    speed and brand, and so on.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前馈网络在图像分类等情况下效果很好，其中数据不是顺序的。前馈网络在使用两个相关变量时也表现良好，比如温度和位置、身高和体重、汽车速度和品牌等。
- en: However, there may be cases where the current output is dependent on the outputs
    at previous time steps (the ordering of data is important).
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，可能存在当前输出依赖于先前时间步的输出的情况（数据的顺序很重要）。
- en: Consider the scenario of reading a book. Your understanding of the sentences
    in the book is based on your understanding of all the words in the sentence. It
    wouldn't be possible to use a feedforward network to predict the next word in
    a sentence, as the output in such a case would depend on the previous outputs.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑阅读一本书的情景。你对书中句子的理解基于你对句子中所有单词的理解。使用前馈网络来预测句子中的下一个单词是不可能的，因为在这种情况下输出取决于先前的输出。
- en: 'Similarly, there are many cases where the output requires the previous output
    or some information from the previous outputs (for example, stock market data,
    NLP, voice recognition, and so on). The feedforward network may be modified as
    in the following diagram to capture information from previous outputs:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，有许多情况下，输出需要先前的输出或一些先前输出的信息（例如，股市数据、自然语言处理、语音识别等）。前馈网络可以被修改如下图所示，以捕获先前输出的信息：
- en: '![](img/00102.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00102.jpeg)'
- en: At time step *t*, the input at *t* as well as the information from *t-1* is
    both provided to the network to obtain the output at time *t*.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在时间步*t*，输入*t*以及*t-1*的信息都提供给网络，以获得时间*t*的输出。
- en: Similarly, the information from *t* as well as the new input is fed into the
    network at time step *t+1* to produce the output at *t+1*. The right-hand side
    of the preceding diagram is a generalized way of representing such a network where
    the output of the network flows back in as input for future time steps. Such a
    network is called a **recurrent neural network** (**RNN**).
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，从时间步*t*以及新输入都被输入到网络中的时间步*t+1*，以产生*t+1*的输出。前面图表的右侧是表示这样一个网络的一般方式，其中网络的输出会作为未来时间步的输入。这样的网络被称为**循环神经网络**（**RNN**）。
- en: See also
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: '**Activation Function**: In artificial neural networks, the activation function of
    a node decides the kind of output that node produces, given an input or set of
    inputs. The output *y[k]* is given by the input *u[k]* and bias *b[k]*, which
    are passed through the activation function *φ(.)*  as shown in the following expression:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数**：在人工神经网络中，节点的激活函数决定了节点在给定输入或一组输入时产生的输出类型。输出*y[k]*由输入*u[k]*和偏置*b[k]*通过激活函数*φ(.)*得到，如下式所示：'
- en: '![](img/00103.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00103.jpeg)'
- en: 'There are various types of activation functions. The following are the commonly
    used ones:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种类型的激活函数。以下是常用的几种：
- en: '**Threshold function**:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**阈值函数**：'
- en: '![](img/00104.gif)![](img/00105.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00104.gif)![](img/00105.jpeg)'
- en: It is clear from the preceding diagram that this kind of  function restricts
    the output values of neurons to between 0 and 1\. This may be useful in many cases.
    However, this function is non-differentiable, which means it cannot be used to
    learn non-linearities, which is vital when using the backpropagation algorithm.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表可以清楚地看出，这种函数限制了神经元的输出值在0和1之间。在许多情况下，这可能是有用的。然而，这个函数是不可微的，这意味着它不能用于学习非线性，而在使用反向传播算法时，这是至关重要的。
- en: '**Sigmoid function**:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Sigmoid函数**：'
- en: '![](img/00106.jpeg)![](img/00107.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00106.jpeg)![](img/00107.jpeg)'
- en: The sigmoid function is a logistic function with a lower limit of 0 and an upper
    limit of 1, as with the threshold function. This activation function is continuous
    and therefore, also differentiable. In the sigmoid function, the slope parameter
    of the preceding function is given by α. The function is nonlinear in nature,
    which is critical in increasing the performance since it is able to accommodate
    non linearities in the input data unlike regular linear functions. Having non
    linear capabilities ensure that small changes in the weights and bias causes significant
    changes in the output of the neuron.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数是一个具有下限为0和上限为1的逻辑函数，与阈值函数一样。这个激活函数是连续的，因此也是可微的。在Sigmoid函数中，前面函数的斜率参数由α给出。这个函数是非线性的，这对于提高性能至关重要，因为它能够容纳输入数据中的非线性，而常规线性函数不能。具有非线性能力确保权重和偏置的微小变化会导致神经元输出的显著变化。
- en: '**Hyperbolic Tangent function (tanh)**:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**双曲正切函数（tanh）**：'
- en: '![](img/00108.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00108.jpeg)'
- en: This function enables activation functions to range from -1 to +1 instead of
    between 0 and 1 as in the previous cases.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数使激活函数的范围从0到1变为-1到+1。
- en: '**Rectified Linear Unit (ReLU) function**: ReLUs are the smooth approximation
    to the sum of many logistic units, and produce sparse activity vectors. The following
    is the equation of the function:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**修正线性单元（ReLU）函数**：ReLU是许多逻辑单元的平滑近似，产生稀疏的活动向量。以下是该函数的方程：'
- en: '![](img/00109.jpeg)![](img/00110.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00109.jpeg)![](img/00110.jpeg)'
- en: ReLU function graph
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU函数图
- en: In the preceding diagram, softplus ![](img/00111.jpeg)(x) = log ( 1 + e^x) is
    the smooth approximation to the rectifier.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，softplus ![](img/00111.jpeg)(x) = log ( 1 + e^x)是整流器的平滑近似。
- en: '**Maxout function**: This function utilizes a technique known as **"dropout"** and
    improves the accuracy of the dropout technique''s fast approximate model averaging
    in order to facilitate optimization.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Maxout函数**：该函数利用一种称为**“dropout”**的技术，并改进了快速近似模型平均的准确性，以便促进优化。'
- en: 'Maxout networks learn not just the relationship between hidden units, but also
    the activation function of each hidden unit. By actively dropping out hidden units,
    the network is forced to find other paths to get to the output from a given input
    during the training process. The following diagram is the graphical depiction
    of how this works:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Maxout网络不仅学习隐藏单元之间的关系，还学习每个隐藏单元的激活函数。通过主动丢弃隐藏单元，网络被迫在训练过程中找到其他路径以从给定输入到输出。以下图表是这个过程如何工作的图形描述：
- en: '![](img/00112.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00112.jpeg)'
- en: Maxout Network
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Maxout网络
- en: 'The preceding diagram shows the Maxout network with five visible units, three
    hidden units, and two neurons for each hidden unit. The Maxout function is given
    by the following equations:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了具有五个可见单元、三个隐藏单元和每个隐藏单元两个神经元的Maxout网络。Maxout函数由以下方程给出：
- en: '![](img/00113.jpeg)![](img/00114.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00113.jpeg)![](img/00114.jpeg)'
- en: 'Here  W..[ij ] is the mean vector of the size of the input obtained by accessing
    the matrix W ∈  ![](img/00115.jpeg) at the second coordinate *i* and third coordinate *j*.
    The number of intermediate units (*k) *is called the number of pieces used by
    the Maxout nets. The following diagram shows how the Maxout function compares
    to the ReLU and **Parametric Rectified Linear Unit** (**PReLU**) functions:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 W..[ij ]是通过访问矩阵W ∈  ![](img/00115.jpeg)的第二坐标 *i* 和第三坐标 *j*获得的输入的大小的均值向量。中间单元的数量（*k）*称为Maxout网络使用的片数。以下图表显示了Maxout函数与ReLU和**参数修正线性单元**（**PReLU**）函数的比较：
- en: '![](img/00116.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00116.jpeg)'
- en: Graphical comparison of Maxout, ReLU and PReLU function
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Maxout、ReLU和PReLU函数的图形比较
- en: Sequential workings of RNNs
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN的顺序工作
- en: 'Recurrent neural networks are a type of artificial neural network designed
    to recognize and learn patterns in sequences of data. Some of the examples of
    such sequential data are:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络是一种人工神经网络，旨在识别和学习数据序列中的模式。此类序列数据的一些示例包括：
- en: Handwriting
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手写
- en: Text such as customer reviews, books, source code, and so on
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诸如客户评论、书籍、源代码等文本
- en: Spoken word / Natural Language
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 口语/自然语言
- en: Numerical time series / sensor data
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数值时间序列/传感器数据
- en: Stock price variation data
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 股价变动数据
- en: Getting ready
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In recurrent neural networks, the hidden state from the previous time step
    is fed back into the network at the next time step, as shown in the following
    diagram:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归神经网络中，来自上一个时间步的隐藏状态被反馈到下一个时间步的网络中，如下图所示：
- en: '![](img/00117.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00117.jpeg)'
- en: Basically, the upward facing arrows going into the network represent the inputs
    (matrices/vectors) to the RNN at each time step, while the upward-facing arrows
    coming out of the network represent the output of each RNN unit. The horizontal
    arrows indicate the transfer of information learned in a particular time step
    (by a particular neuron) onto the next time step.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，进入网络的朝上箭头代表RNN在每个时间步的输入（矩阵/向量），而从网络中出来的朝上箭头代表每个RNN单元的输出。水平箭头表示在特定时间步（由特定神经元）学习的信息传递到下一个时间步。
- en: 'More information about using RNNs can be found at :'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用RNN的更多信息，请访问：
- en: '[https://deeplearning4j.org/usingrnns](https://deeplearning4j.org/usingrnns)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://deeplearning4j.org/usingrnns](https://deeplearning4j.org/usingrnns)'
- en: How to do it...
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'At every node/neuron of a recurrent network, a series of matrix multiplication
    steps are carried out. The input vector/matrix is multiplied by a weight vector/matrix
    first, a bias is added to this term, and this is finally passed through an activation
    function to produce the output (just as in the case of feedforward networks):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归网络的每个节点/神经元上，进行一系列矩阵乘法步骤。首先将输入向量/矩阵乘以权重向量/矩阵，然后添加偏差项，最后通过激活函数产生输出（就像前馈网络的情况一样）：
- en: 'The following diagram shows an intuitive and mathematical way of visualizing
    RNNs in the form of a computational graph:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下图表显示了一种直观和数学化的方式来可视化RNNs，以计算图的形式：
- en: '![](img/00118.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00118.jpeg)'
- en: At the first time step (which is *t=0*), *h*[*0* ]is calculated using the first
    formula on the right-hand side of the preceding diagram. Since *h*^(*-1* )does
    not exist, the middle term becomes zero.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一个时间步骤（即*t=0*），使用前面图表右侧的第一个公式计算*h*[*0*]。由于*h*^(*-1*)不存在，中间项变为零。
- en: The input matrix *x*[*0* ]is multiplied by the weight matrix *w[i]* and a bias
    *b[h]* is added to this term.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入矩阵*x*[*0*]乘以权重矩阵*w[i]*，并且将偏差*b[h]*添加到这个项。
- en: The two preceding matrices are added and then passed through an activation function
    *g[h]* to obtain *h[0]*.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将前面的两个矩阵相加，然后通过激活函数*g[h]*获得*h[0]*。
- en: Similarly, *y[0]* is calculated using the second equation on the right-hand
    side of the preceding diagram by multiplying *h[0]* with the weight matrix *w[y]*,
    adding a bias *b[y]* to it, and passing it through an activation function *g[y]*.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，*y[0]*使用前面图表右侧的第二个方程计算，方法是将*h[0]*与权重矩阵*w[y]*相乘，加上偏差*b[y]*，并通过激活函数*g[y]*传递。
- en: At the next time step (which is *t=1*), *h^((t-1))* does exist. It is nothing
    but *h[0]*. This term, multiplied with the weight matrix *w[R]*, is also provided
    as the input to the network along with the new input matrix *x[1]*.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个时间步（即*t=1*），*h^((t-1))*存在。它就是*h[0]*。这个项乘以权重矩阵*w[R]*，也作为网络的输入与新的输入矩阵*x[1]*一起提供。
- en: This process is repeated over a number of time steps, and the weights, matrices,
    and biases flow through the entire network over different time steps.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程在多个时间步骤中重复进行，权重、矩阵和偏差在不同的时间步骤中通过整个网络流动。
- en: This entire process is executed over one single iteration, which constitutes
    the forward pass of the network.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整个过程在一个迭代中执行，这构成了网络的前向传递。
- en: How it works...
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'To train feedforward neural networks the most commonly used technique is backpropagation
    through time. It is a supervised learning method used to reduce the loss function
    by updating weights and biases in the network after every time step. A number
    of training cycles (also known as epochs) are executed where the error determined
    by the loss function is backward propagated by a technique called gradient descent.
    At the end of each training cycle, the network updates its weights and biases
    to produce an output which is closer to the desired output, until a sufficiently
    small error is achieved :'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 训练前馈神经网络最常用的技术是通过时间的反向传播。这是一种监督学习方法，用于通过在每个时间步之后更新网络中的权重和偏差来减少损失函数。执行多个训练周期（也称为时代），其中由损失函数确定的误差通过梯度下降的技术进行反向传播。在每个训练周期结束时，网络更新其权重和偏差，以产生接近期望输出的输出，直到达到足够小的误差：
- en: 'The backpropagation algorithm basically implements the following three fundamental
    steps during every iteration:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次迭代期间，反向传播算法基本上实现以下三个基本步骤：
- en: The forward pass of the input data and calculating the loss function
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据的前向传递和计算损失函数
- en: The computation of gradients and errors
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度和误差的计算
- en: Backpropagation through time and adjustment of weights and biases accordingly
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过时间的反向传播和相应地调整权重和偏差
- en: After the weighted sum of inputs (passed through an activation function after
    adding a bias) is fed into the network and an output is obtained, the network
    immediately compares how different the predicted output is from the actual case
    (correct output).
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在通过激活函数加上偏差后的输入的加权和被馈送到网络中并获得输出后，网络立即比较预测输出与实际情况（正确输出）的差异有多大。
- en: Next, the error is calculated by the network. This is nothing but the network
    output subtracted from the actual/correct output.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，网络计算误差。这实际上就是网络输出减去实际/正确的输出。
- en: The next step involves backpropagation through the entire network based on the
    calculated error. The weights and biases are then updated to notice whether the
    error increases or decreases.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步涉及根据计算的误差在整个网络中进行反向传播。然后更新权重和偏差以观察误差是增加还是减少。
- en: The network also remembers whether the error increases by increasing the weights
    and biases or by decreasing the weights and biases.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络还记得，增加权重和偏差会增加误差，或者减少权重和偏差会减少误差。
- en: Based on the preceding inferences, the network continues to update the weights
    and biases during every iteration in a manner such that the error becomes minimal.
    The following example will make things clearer.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据前述推论，网络在每次迭代期间继续以使误差最小的方式更新权重和偏差。下面的例子将使事情更清楚。
- en: 'Consider a simple case of teaching a machine how to double a number, as shown
    in the following table:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑一个简单的情况，教会机器如何将一个数字加倍，如下表所示：
- en: '![](img/00119.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00119.jpeg)'
- en: As you can see, by initializing the weights randomly (*W = 3*), we obtain outputs
    of 0, 3, 6, and 9.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如你所看到的，通过随机初始化权重（*W = 3*），我们得到了0、3、6和9的输出。
- en: The error is calculated by subtracting the column of correct outputs from the
    column of model outputs. The square error is nothing but each error term multiplied
    by itself. It is usually a better practice to use square error as it eliminates
    negative values from error terms.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 误差是通过将正确输出的列减去模型输出的列来计算的。平方误差实际上就是每个误差项与自身相乘。通常最好使用平方误差，因为它消除了误差项中的负值。
- en: The model then realizes that in order to minimize the error, the weight needs
    to be updated.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型随后意识到，为了最小化误差，需要更新权重。
- en: 'Let''s suppose the model updates its weight to *W = 4* during the next iteration.
    This would result in the following output:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设在下一次迭代中，模型将其权重更新为*W = 4*。这将导致以下输出：
- en: '![](img/00120.jpeg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00120.jpeg)'
- en: The model now realizes that the error actually increased by increasing the weight
    to *W = 4*. Therefore, the model updates its weight by reducing it to *W = 2*
    in its next iteration, which results in the actual/correct output.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型现在意识到，通过增加权重到*W = 4*，实际上误差增加了。因此，在下一次迭代中，模型通过将权重减小到*W = 2*来更新权重，从而得到实际/正确的输出。
- en: 'Note that, in this simple case, the error increases when the weight is increased
    and reduces when the weight is decreased, as follows:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，在这个简单的情况下，当增加权重时，误差增加，当减少权重时，误差减少，如下所示：
- en: '![](img/00121.jpeg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00121.jpeg)'
- en: In an actual neural network, a number of such weight updates are performed during
    every iteration until the model converges with the actual/correct output.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实际的神经网络中，每次迭代期间都会执行多次这样的权重更新，直到模型收敛到实际/正确的输出。
- en: There's more...
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'As seen in the preceding case, the error increased when the weight was increased
    but decreased when the weight was decreased. But this may not always be the case.
    The network uses the following graph to determine how to update weights and when
    to stop updating them:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的情况所示，当增加权重时，误差增加，但当减少权重时，误差减少。但这并不总是成立。网络使用以下图表来确定如何更新权重以及何时停止更新它们：
- en: '![](img/00122.jpeg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00122.jpeg)'
- en: Let the weights be initialized to zero at the beginning of the first iteration.
    As the network updates its weights by increasing them from point A to B, the error
    rate begins to decrease.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让权重在第一次迭代开始时初始化为零。当网络通过从点A到B增加权重时，误差率开始减少。
- en: Once the weights reach point B, the error rate becomes minimal. The network
    constantly keeps track of the error rate.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦权重达到B点，误差率就变得最小。网络不断跟踪误差率。
- en: On further increasing the weights from point B towards point C, the network
    realizes that the error rate begins to increase again. Thus, the network stops
    updating its weights and reverts back to the weights at point B, as they are optimal.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进一步增加从点B到点C的权重后，网络意识到错误率再次开始增加。因此，网络停止更新其权重，并恢复到点B的权重，因为它们是最佳的。
- en: 'In the next scenario, consider a case where the weights are randomly initialized
    to some value (let''s say, point C), as shown in the following graph:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下一个场景中，考虑一种情况，即权重被随机初始化为某个值（比如说，点C），如下图所示：
- en: '![](img/00123.jpeg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00123.jpeg)'
- en: On further increasing these random weights, the error also increases ( starting
    at point C and moving away from point B, indicated by the small arrow in the graph).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进一步增加这些随机权重后，错误也增加了（从点C开始并远离点B，图中的小箭头表示）。
- en: The network realizes that the error increased and begins to decrease the weights
    from point C so that the error decreases (indicated by the long arrow from point
    C moving towards point B in the graph). This decrease of weights happens until
    the error reaches a minimal value (point B on the graph).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络意识到错误增加，并开始从点C减小权重，以使错误减少（在图中从点C向点B移动的长箭头表示）。这种权重减少会一直持续，直到错误达到最小值（图中的点B）。
- en: The network continues to further update its weights even after reaching point
    B (indicated by the arrow moving away from point B and towards point A on the
    graph). It then realizes that the error is again increasing. As a result, it stops
    the weight update and reverts back to the weights that gave the minimal error
    value (which are the weights at point B).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络继续在达到点B后进一步更新其权重（在图中从点B远离并向点A移动的箭头表示）。然后它意识到错误再次增加。因此，它停止权重更新，并恢复到给出最小错误值的权重（即点B处的权重）。
- en: 'This is how neural networks perform weight updates after backpropagation. This
    kind of weight update is momentum-based. It relies on the computed gradients at
    each neuron of the network during every iteration, as shown in the following diagram:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是神经网络在反向传播后执行权重更新的方式。这种权重更新是基于动量的。它依赖于在每次迭代期间网络中每个神经元计算的梯度，如下图所示：
- en: '![](img/00124.jpeg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00124.jpeg)'
- en: Basically, the gradients are computed for each input with respect to the output
    every time an input flows into a neuron. The chain rule is used to compute the
    gradients during the backward pass of backpropagation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，每次输入流入神经元时，都会针对输出计算每个输入的梯度。链式法则用于在反向传播的后向传递期间计算梯度。
- en: See also
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'A detailed explanation of the math behind backpropagation can be found at the
    following links:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在以下链接找到反向传播背后的数学详细解释：
- en: '[https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)'
- en: '[https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5](https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5](https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c)'
- en: 'Andrej Karpathy''s blog has tons of useful information about recurrent neural
    networks. Here is a link explaining their unreasonable effectiveness:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Andrej Karpathy的博客中有大量关于递归神经网络的有用信息。以下是一个解释它们不合理有效性的链接：
- en: '[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
- en: 'Pain point #1 – The vanishing gradient problem'
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 痛点＃1 - 梯度消失问题
- en: Recurrent neural networks are great for tasks involving sequential data. However,
    they do come with their drawbacks. This section will highlight and discuss one
    such drawback, known as the **vanishing gradient problem**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络非常适用于涉及序列数据的任务。然而，它们也有缺点。本节将重点讨论其中一个缺点，即**梯度消失问题**。
- en: Getting ready
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The name vanishing gradient problem stems from the fact that, during the backpropagation
    step, some of the gradients vanish or become zero. Technically, this means that
    there is no error term being propagated backward during the backward pass of the
    network. This becomes a problem when the network gets deeper and more complex.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失问题的名称源于在反向传播步骤中，一些梯度消失或变为零。从技术上讲，这意味着在网络的反向传播过程中没有错误项被向后传播。当网络变得更深更复杂时，这就成为了一个问题。
- en: How to do it...
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'This section will describe how the vanishing gradient problem occurs in recurrent
    neural networks:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述递归神经网络中梯度消失问题的发生方式：
- en: While using backpropagation, the network first calculates the error, which is
    nothing but the model output subtracted from the actual output squared (such as
    the square error).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用反向传播时，网络首先计算错误，这只是模型输出减去实际输出的平方（如平方误差）。
- en: Using this error, the model then computes the change in error with respect to
    the change in weights (de/dw).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这个错误，模型然后计算错误相对于权重变化的变化（de/dw）。
- en: The computed derivative multiplied by the learning rate ![](img/00125.jpeg) gives ![](img/00126.jpeg)w,
    which is nothing but the change in weights. The term ![](img/00127.jpeg)w is added
    to the original weights to update them to the new weights.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算得到的导数乘以学习率 ![](img/00125.jpeg) 得到 ![](img/00126.jpeg)w，这就是权重的变化。术语 ![](img/00127.jpeg)w
    被添加到原始权重上，以将它们更新为新的权重。
- en: Suppose the value of de/dw (the gradient or rate of change of error with respect
    to weights) is much less than 1, then that term multiplied by the learning rate ![](img/00128.jpeg) (which
    is always much less than 1) gives a very small, negligible, number which is negligible.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设 de/dw（错误相对于权重的梯度或变化率）的值远小于1，那么该术语乘以学习率 ![](img/00128.jpeg) （始终远小于1）得到一个非常小的可忽略的数字。
- en: This happens because the weight updates during backpropagation are only accurate
    for the most recent time step, and this accuracy reduces while backpropagating
    through the previous time steps and becomes almost insignificant when the weight
    updates flow through many steps back in time.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是因为在反向传播过程中，权重更新仅对最近的时间步准确，而在通过以前的时间步进行反向传播时，准确性会降低，并且当权重更新通过许多时间步回溯时，这种准确性几乎变得微不足道。
- en: There may be certain cases where sentences may be extremely long and the neural
    network is trying to predict the next word in a sentence. It does so based on
    the context of the sentence, for which it needs information from many previous
    time steps (these are called **long-term dependencies**). The number of previous
    times steps the network needs to backpropagate through increases with the increasing
    length of sentences. In such cases, the recurrent networks become incapable of
    remembering information from many time steps in the past and therefore are unable
    to make accurate predictions.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些情况下，句子可能非常长，神经网络试图预测句子中的下一个单词。它基于句子的上下文进行预测，因此需要来自许多先前时间步的信息（这些被称为长期依赖）。网络需要通过的先前时间步数随着句子长度的增加而增加。在这种情况下，循环网络无法记住过去许多时间步的信息，因此无法进行准确的预测。
- en: When such a scenario occurs, the network requires many more complex calculations,
    as a result of which the number of iterations increases substantially and during
    which the change in error term vanishes (by reducing over time) and changes in
    weight (![](img/00129.jpeg)w) become negligibly small. As a result, the new or
    updated weight is almost equal to the previous weight.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当出现这种情况时，网络需要进行更多复杂的计算，因此迭代次数大大增加，同时误差项的变化减少（随着时间的推移）并且权重的变化变得微不足道。因此，新的或更新的权重几乎等于先前的权重。
- en: Since there is no weight update occurring, the network stops learning or being
    able to update its weights, which is a problem as this will cause the model to
    overfit the data.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于没有发生权重更新，网络停止学习或无法更新其权重，这是一个问题，因为这将导致模型过度拟合数据。
- en: 'This entire process is illustrated in the following diagram:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个过程如下图所示：
- en: '![](img/00130.jpeg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00130.jpeg)'
- en: How it works...
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'This section will describe some of the repercussions of the vanishing gradient
    problem:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述梯度消失问题的一些后果：
- en: This problem occurs when we train a neural network model using some sort of
    optimization techniques which are gradient based.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们使用一些基于梯度的优化技术训练神经网络模型时，就会出现这个问题。
- en: Generally, adding more hidden layers tends to make the network able to learn
    more complex arbitrary functions, and thus do a better job in predicting future
    outcomes. Deep Learning makes a big difference due to the large number of hidden
    layers it has, ranging from 10 to 200\. It is now possible to make sense of complicated
    sequential data, and perform tasks such as Speech Recognition, Image Classification,
    Image Captioning, and more.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常，增加更多的隐藏层倾向于使网络能够学习更复杂的任意函数，从而在预测未来结果方面做得更好。深度学习由于具有大量的隐藏层（从10到200个），因此产生了很大的影响。现在可以理解复杂的序列数据，并执行诸如语音识别、图像分类、图像字幕等任务。
- en: The problem caused by the preceding steps is that, in some cases, the gradients
    become so small that they almost vanish, which in turn prevents the weights from
    updating their values during future time steps.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由前述步骤引起的问题是，在某些情况下，梯度变得非常小，几乎消失，这反过来阻止权重在未来时间步骤中更新其值。
- en: In the worst case, it could result in the training process of the network being
    stopped, which means that the network stops learning the different features it
    was intended to learn through the training steps.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最坏的情况下，这可能导致网络的训练过程停止，这意味着网络停止通过训练步骤学习不同的特征。
- en: The main idea behind backpropagation is that it allows us, as researchers, to
    monitor and understand how machine learning algorithms process and learn various
    features. When the gradients vanish, it becomes impossible to interpret what is
    going on with the network, and hence identifying and debugging errors becomes
    even more of a challenge.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播的主要思想是，它允许我们作为研究人员监视和理解机器学习算法如何处理和学习各种特征。当梯度消失时，就不可能解释网络中发生了什么，因此识别和调试错误变得更加具有挑战性。
- en: There's more...
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The following are some of the ways in which the problem of vanishing gradients
    can be solved:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是解决梯度消失问题的一些方法：
- en: One method to overcome this problem to some extent by using the ReLU activation
    function. It computes the function *f(x)=max(0,x) (i.e., t*he activation function
    simply thresholds the lower level of outputs at zero) and prevents the network
    from producing negative gradients.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种方法在一定程度上克服这个问题是使用ReLU激活函数。它计算函数*f(x)=max(0,x)（即，激活函数简单地将输出的较低级别阈值设为零），并防止网络产生负梯度。
- en: Another way to overcome this problem is to perform unsupervised training on
    each layer separately and then fine-tune the entire network through backpropagation,
    as done by Jürgen Schmidhuber in his study of multi-level hierarchy in neural
    networks. The link to this paper is provided in the following section.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种克服这个问题的方法是对每个层进行无监督训练，然后通过反向传播对整个网络进行微调，就像Jürgen Schmidhuber在他对神经网络中多层次层次结构的研究中所做的那样。该论文的链接在下一节中提供。
- en: A third solution to this problem is the use of **LSTM** (**Long Short-Term Memory**)
    units or **GRUs (Gated Recurrent Units)**, which are special types of RNNs.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决这个问题的第三种方法是使用LSTM（长短期记忆）单元或GRUs（门控循环单元），这些是特殊类型的RNN。
- en: See also
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'The following links provide a more in-depth description of the vanishing gradient
    problem and also some ways to tackle the issue:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接提供了对梯度消失问题的更深入描述，以及一些解决该问题的方法：
- en: '[https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)'
- en: '[http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf)'
- en: '[http://people.idsia.ch/~juergen/cvpr2012.pdf](http://people.idsia.ch/~juergen/cvpr2012.pdf)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://people.idsia.ch/~juergen/cvpr2012.pdf](http://people.idsia.ch/~juergen/cvpr2012.pdf)'
- en: 'Pain point #2 – The exploding gradient problem'
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 痛点＃2 - 爆炸梯度问题
- en: Another drawback of recurrent neural networks is the problem of exploding gradients.
    This is similar to the vanishing gradient problem but the exact opposite. Sometimes,
    during backpropagation, the gradients explode to extraordinarily large values.
    As with the vanishing gradient problem, the problem of exploding gradients occurs
    when network architectures get deeper.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络的另一个缺点是爆炸梯度问题。这与梯度消失问题类似，但完全相反。有时在反向传播过程中，梯度会爆炸成异常大的值。与梯度消失问题一样，爆炸梯度问题发生在网络架构变得更深时。
- en: Getting ready
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The name exploding gradient problem stems from the fact that, during the backpropagation
    step, some of the gradients vanish or become zero. Technically, this means that
    there is no error term being propagated backward during the backward pass of the
    network. This becomes a problem when the network gets deeper and more complex.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 爆炸梯度问题的名称源于反向传播步骤中一些梯度消失或变为零的事实。从技术上讲，这意味着在网络的反向传播过程中没有误差项向后传播。当网络变得更深更复杂时，这就成为了一个问题。
- en: How to do it...
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'This section will describe the exploding gradient problem in recurrent neural
    networks:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述递归神经网络中的爆炸梯度问题：
- en: The exploding gradient problem is very similar to the vanishing gradient problem,
    but just the opposite.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爆炸梯度问题与梯度消失问题非常相似，但完全相反。
- en: When long-term dependencies arise in recurrent neural networks, the error term
    is propagated backward through the network sometimes explodes or becomes very
    large.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当递归神经网络中出现长期依赖时，误差项向后传播时有时会爆炸或变得非常大。
- en: This error term multiplied by the learning rate results in an extremely large ![](img/00131.jpeg)w.
    This gives rise to new weights that look very different from the previous weights.
    It is called the exploding gradient problem because the value of the gradient
    becomes too large.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个误差项乘以学习率的结果是一个极端大的![](img/00131.jpeg)w。这导致产生的新权重看起来与以前的权重非常不同。这被称为爆炸梯度问题，因为梯度的值变得太大。
- en: 'The problem of exploding gradients is illustrated in an algorithmic fashion,
    in the following diagram:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爆炸梯度问题以算法方式在以下图表中进行了说明：
- en: '![](img/00132.jpeg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00132.jpeg)'
- en: How it works...
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'Since neural networks use a gradient-based optimization technique to learn
    features present in data, it is essential that these gradients are preserved in
    order for the network to calculate an error based on the change in gradients.
    This section will describe how the exploding gradient problem occurs in recurrent
    neural networks:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络使用基于梯度的优化技术来学习数据中存在的特征，因此必须保留这些梯度，以便网络根据梯度的变化计算误差。本节将描述爆炸梯度问题在递归神经网络中是如何发生的：
- en: While using backpropagation, the network first calculates the error, which is
    nothing but the model output subtracted from the actual output squared (such as
    the square error).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用反向传播时，网络首先计算误差，这只是模型输出减去实际输出的平方（如平方误差）。
- en: Using this error, the model then computes the change in error with respect to
    the change in weights (de/dw).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这个误差，模型然后计算了相对于权重变化的误差变化（de/dw）。
- en: The computed derivative multiplied by the learning rate ![](img/00133.jpeg) gives ![](img/00134.jpeg)w,
    which is nothing but the change in weights. The term ![](img/00135.jpeg)w is added
    to the original weights to update them to the new weights.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算得到的导数乘以学习率![](img/00133.jpeg)得到![](img/00134.jpeg)w，这只是权重的变化。项![](img/00135.jpeg)w被添加到原始权重上，以将它们更新为新的权重。
- en: Suppose the value of de/dw (the gradient or rate of change of error with respect
    to weights) is greater than 1, then that term multiplied by the learning rate ![](img/00136.jpeg) gives
    a very, very large number that is of no use to the network while trying to optimize
    weights further, since the weights are no longer in the same range.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设de/dw（误差相对于权重的梯度或变化率）的值大于1，那么该项乘以学习率![](img/00136.jpeg)将得到一个非常非常大的数字，对于网络在进一步优化权重时是毫无用处的，因为权重已不再处于相同的范围内。
- en: This happens because the weight updates during backpropagation are only accurate
    for the most recent time step, and this accuracy reduces while backpropagating
    through the previous time steps and becomes almost insignificant when the weight
    updates flow through many steps back in time.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是因为在反向传播过程中，权重更新仅对最近的时间步准确，而在通过以前的时间步进行反向传播时，这种准确性会降低，并且当权重更新通过许多时间步回溯时几乎变得无关紧要。
- en: The number of previous times steps the network needs to backpropagate through
    increases with the increase in the number of sequences in the input data. In such
    cases, the recurrent networks become incapable of remembering information from
    many time steps in the past and therefore are unable to make accurate predictions
    of future time steps.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络需要通过的以前时间步数随着输入数据中序列数量的增加而增加。在这种情况下，递归网络无法记住过去许多时间步的信息，因此无法准确预测未来时间步。
- en: When such a scenario occurs, the network requires many more complex calculations,
    as a result of which the number of iterations increases substantially and during
    which the change in error term increases beyond 1 and changes in weight (![](img/00137.jpeg)w)
    explode. As a result, the new or updated weight is completely out of range when
    compared to the previous weight.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当出现这种情况时，网络需要进行更多复杂的计算，因此迭代次数大大增加，错误项的变化超过1，权重（w）的变化激增。结果，与先前的权重相比，新的或更新的权重完全超出范围。
- en: Since there is no weight update occurring, the network stops learning or being
    able to update its weights within a specified range, which is a problem as this
    will cause the model to overfit the data.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于没有发生权重更新，网络停止学习或无法在指定范围内更新其权重，这是一个问题，因为这将导致模型过度拟合数据。
- en: There's more...
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The following are some of the ways in which the problem of exploding gradients
    can be solved:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是解决梯度爆炸问题的一些方法：
- en: Certain gradient clipping techniques can be applied to solve this issue of exploding
    gradients.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以应用某些梯度裁剪技术来解决梯度爆炸的问题。
- en: Another way to prevent this is by using truncated Backpropagation Through Time,
    where instead of starting the backpropagation at the last time step (or output
    layer), we can choose a smaller time step (say, 15) to start backpropagating.
    This means that the network will backpropagate through only the last 15 time steps
    at one instance and learn information related to those 15-time steps only. This
    is similar to feeding in mini batches of data to the network as it would become
    far too computationally expensive to compute the gradient over every single element
    of the dataset in the case of very large datasets.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种预防方法是使用截断的时间反向传播，而不是从最后一个时间步（或输出层）开始反向传播，我们可以选择一个较小的时间步（比如15）开始反向传播。这意味着网络将一次只反向传播最后的15个时间步，并且只学习与这15个时间步相关的信息。这类似于将小批量数据馈送到网络中，因为在大型数据集的情况下，计算每个数据集元素的梯度将变得过于昂贵。
- en: The final option to prevent the explosion of gradients is by monitoring them
    and adjusting the learning rate accordingly.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止梯度爆炸的最后一种选择是监控它们并相应地调整学习率。
- en: See also
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'A more detailed explanation of the vanishing and exploding gradient problems
    can be found at the following links:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在以下链接找到有关消失和爆炸梯度问题的更详细解释：
- en: '[http://neuralnetworksanddeeplearning.com/chap5.html](http://neuralnetworksanddeeplearning.com/chap5.html)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://neuralnetworksanddeeplearning.com/chap5.html](http://neuralnetworksanddeeplearning.com/chap5.html)'
- en: '[https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/](https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/](https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/)'
- en: '[https://machinelearningmastery.com/exploding-gradients-in-neural-networks/](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/exploding-gradients-in-neural-networks/](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)'
- en: Sequential working of LSTMs
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTMs的顺序工作
- en: '**Long Short-Term Memory Unit** (**LSTM**) cells are nothing but slightly more
    advanced architectures compared to Recurrent Networks. LSTMs can be thought of
    as a special kind of Recurrent Neural Networks with the capabilities of learning
    long-term dependencies that exist in sequential data. The main reason behind this
    is the fact that LSTMs contain memory and are able to store and update information
    within their cells unlike Recurrent Neural Networks.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**长短期记忆单元**（**LSTM**）单元只是相对于循环网络而言稍微更先进的架构。LSTMs可以被认为是一种具有学习顺序数据中存在的长期依赖关系能力的特殊类型的循环神经网络。其主要原因是LSTMs包含内存，并且能够存储和更新其单元内的信息，而不像循环神经网络那样。'
- en: Getting ready
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备好了
- en: 'The main components of a Long Short-Term Memory unit are as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆单元的主要组成部分如下：
- en: The input gate
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门
- en: The forget gate
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遗忘门
- en: The update gate
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新门
- en: Each of these gates is made up of a sigmoid layer followed by a pointwise multiplication
    operation. The sigmoid layer outputs numbers between zero and one. These values
    describe  how much information of each component is allowed to pass through the
    respective gate. A value of zero means the gate will allow nothing to pass through
    it, while a value of one means the gate allows all the information to pass through.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这些门中的每一个都由一个S形层和一个逐点乘法操作组成。S形层输出介于零和一之间的数字。这些值描述了每个组件的信息有多少被允许通过相应的门。值为零意味着门不允许任何信息通过，而值为一意味着门允许所有信息通过。
- en: The best way to understand LSTM cells is through computational graphs, just
    like in the case of recurrent neural networks.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 了解LSTM单元的最佳方法是通过计算图，就像循环神经网络的情况一样。
- en: 'LSTMs were originally developed by Sepp Hochreiter and Jurgen Schmidhuber in
    1997\. The following is, link to their published paper:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: LSTMs最初是由Sepp Hochreiter和Jurgen Schmidhuber于1997年开发的。以下是他们发表的论文链接：
- en: '[http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)'
- en: How to do it...
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'This section will describe the inner components of a single LSTM cell, primarily,
    the three different gates present inside the cell. A number of such cells stacked
    together form an LSTM network:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述单个LSTM单元的内部组件，主要是单元内部存在的三个不同门。一系列这样的单元堆叠在一起形成一个LSTM网络：
- en: LSTMs also have a chain-like structure like RNNs. Standard RNNs are basically
    modules of repeating units like a simple function (for example, tanh).
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTMs也像RNNs一样具有链式结构。标准RNN基本上是重复单元的模块，如简单函数（例如tanh）。
- en: LSTMs have the capability to retain information for long periods of time as
    compared to RNNs owing to the presence of memory in each unit. This allows them
    to learn important information during the early stages in a sequence of inputs
    and also gives it the ability to have a significant impact on the decisions made
    by the model at the end of each time step.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与RNN相比，由于每个单元中存在内存，LSTM具有比RNN更长时间地保留信息的能力。这使它们能够在输入序列的早期阶段学习重要信息，并且还赋予了它们在每个时间步的决策中产生重要影响的能力。
- en: By being able to store information right from the early stages of an input sequence, LSTMs
    are actively able to preserve the error that can be backpropagated through time
    and layers instead of letting that error vanish or explode.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过能够从输入序列的早期阶段存储信息，LSTM能够积极地保留可以通过时间和层进行反向传播的错误，而不是让该错误消失或爆炸。
- en: LSTMs are capable of learning information over many time steps and thus have
    denser layer architectures by preserving the error which is backpropagated through
    those layers.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM能够在许多时间步长上学习信息，因此通过保留通过这些层进行反向传播的错误，具有更密集的层架构。
- en: The cell structures called **"gates"** give the LSTM the ability to retain information,
    add information or remove information from the **cell state**.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 细胞结构称为**“门”**赋予了LSTM保留信息、添加信息或从**细胞状态**中删除信息的能力。
- en: 'The following diagram illustrates the structure of an LSTM. The key feature
    while trying to  understand LSTMs is in understanding the LSTM network architecture
    and cell state, which can be visualized here:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下图示了LSTM的结构。在尝试理解LSTM时的关键特征在于理解LSTM网络架构和细胞状态，可以在这里进行可视化：
- en: '![](img/00138.gif)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00138.gif)'
- en: In the preceding diagram, *x[t]* and *h[t-1]* are the two inputs to the cell.
    *x*[*t* ]is the input from the current time step, while h[t-1 ]is the input from
    the previous time step (which is the output of the preceding cell during the previous
    time step). Besides these two inputs, we also have *h*[, ]which is the current
    output (i.e., time step t) from the LSTM cell after performing its operations
    on the two inputs through its gates.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的图中，*x[t]*和*h[t-1]*是细胞的两个输入。*x*[*t*]是当前时间步的输入，而*h[t-1]*是上一个时间步的输入（即上一个时间步的细胞的输出）。除了这两个输入，我们还有*h*[,
    ]，它是经过门控循环单元（LSTM）细胞对这两个输入进行操作后的当前输出（即时间步t）。
- en: In the preceding diagram, r[t ]represents the output emerging from the input
    gate, which takes in inputs *h*[*t-1* ]and *x[t]*, performs multiplication of
    these inputs with its weight matrix *W[z]*, and passes them through a sigmoid
    activation function.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的图中，r[t]表示从输入门中出现的输出，它接受*h*[*t-1*]和*x[t]*的输入，将这些输入与其权重矩阵*W[z]*相乘，并通过S形激活函数传递。
- en: Similarly, the term *z[t]* represents the output emerging from the forget gate.
    This gate has a set of weight matrices (represented by *W[r]*) which are specific
    to this particular gate and govern how the gate functions.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，术语*z[t]*表示从遗忘门中出现的输出。这个门有一组权重矩阵（由*W[r]*表示），这些权重矩阵特定于这个特定的门，并控制门的功能。
- en: Finally, there is ![](img/00139.jpeg)[t], which is the output emerging from
    the update gate. In this case, there are two parts. The first part is a sigmoid
    layer which is also called the **input gate layer**, and its primary function
    is deciding which values to update. The next layer is a tanh layer . The primary
    function of this layer is to create a vector or array containing new values that  could
    be added to the cell state.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，还有![](img/00139.jpeg)[t]，它是从更新门中出现的输出。在这种情况下，有两个部分。第一部分是一个称为**输入门层**的S形层，其主要功能是决定要更新哪些值。下一层是一个tanh层。这一层的主要功能是创建一个包含可以添加到细胞状态中的新值的向量或数组。
- en: How it works...
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'A combination of a number of LSTM cells/units forms an LSTM network. The architecture
    of such a network is shown in the following diagram:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列LSTM细胞/单元的组合形成了LSTM网络。这种网络的架构如下图所示：
- en: '![](img/00140.jpeg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00140.jpeg)'
- en: In the preceding diagram, the full LSTM cell is represented by ***"A"***. The
    cell takes the current input (*x**[i]*) of a sequence of inputs, and produces
    (*h**[i]*) which is nothing but the output of the current hidden state. This output
    is then sent to the next LSTM cell as its input.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的图中，完整的LSTM细胞由***“A”***表示。细胞接受输入序列的当前输入（*x**[i]*），并产生（*h**[i]*），这实际上就是当前隐藏状态的输出。然后将此输出作为下一个LSTM细胞的输入。
- en: An LSTM cell is slightly more complicated than an RNN cell. While the RNN cell
    has just one function/layer acting on a current input, the LSTM cell has three
    layers which are the three gates controlling the information flowing through the
    cell at any given instance in time.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM细胞比RNN细胞稍微复杂一些。RNN细胞只有一个作用于当前输入的功能/层，而LSTM细胞有三个层，即控制细胞在任何给定时间点流动的三个门。
- en: The cell behaves a lot like the hard disk memory in a computer. The cell, therefore,
    has the capability to allow the writing, reading and storing of information within
    its cell state. The cell also makes decisions about what information to store,
    and when to allow reading, writing, and erasing information. This is facilitated
    by the gates that open or close accordingly.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 细胞的行为很像计算机中的硬盘内存。因此，细胞具有允许在其细胞状态内写入、读取和存储信息的能力。细胞还会决定存储哪些信息，以及何时允许读取、写入和擦除信息。这是通过相应地打开或关闭门来实现的。
- en: The gates present in LSTM cells are analog in contrast to the digital storage
    systems in today's computers. This means that the gates can only be controlled
    through an element-wise multiplication through sigmoids, yielding probability
    values between 0 and 1\. A high value will cause the gate to remain open while
    a low value will cause the gate to remain shut.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM细胞中的门是模拟的，与当今计算机中的数字存储系统形成对比。这意味着门只能通过S形函数的逐元素乘法来控制，产生介于0和1之间的概率值。高值将导致门保持打开，而低值将导致门保持关闭。
- en: Analog systems have an edge over digital systems when it comes to neural network
    operations since they are differentiable. This makes analog systems more suitable
    for tasks like backpropagation which primarily rely on the gradients.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模拟系统在神经网络操作方面比数字系统更具优势，因为它们是可微分的。这使得模拟系统更适合像反向传播这样主要依赖于梯度的任务。
- en: The gates pass on information or block information or let only a part of the
    information flow through them based on its strength and importance. The information
    is filtered at every time step through the sets of weight matrices specific to
    each gate. Therefore, each gate has complete control over how to act on the information
    it receives.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 门传递信息或阻止信息，或者只让部分信息根据其强度和重要性流过它们。每一次时间步骤，信息都会通过特定于每个门的权重矩阵集合进行过滤。因此，每个门都完全控制如何对接收到的信息进行操作。
- en: The weight matrices associated with each gate, like the weights that modulate
    input and hidden states, are adjusted based on the recurrent network's learning
    process and gradient descent.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与每个门相关的权重矩阵，如调制输入和隐藏状态的权重，都是根据递归网络的学习过程和梯度下降进行调整的。
- en: The first gate is called the **forget gate** and it controls what information
    is maintained from the previous state. This gate takes the previous cell output
    (*h**[t]** - 1*) as its input along with the current input (*x**[t]*), and applies
    a sigmoid activation (![](img/00141.jpeg)) in order to produce and output value
    between 0 and 1 for each hidden unit. This is followed by the element-wise multiplication
    with the current state (illustrated by the first operation in the preceding diagram).
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个门被称为“遗忘门”，它控制从上一个状态中保留哪些信息。该门将上一个细胞的输出（*h**[t]** - 1*）作为其输入，以及当前输入（*x**[t]*），并应用sigmoid激活（![](img/00141.jpeg)）以产生每个隐藏单元的0到1之间的输出值。然后进行与当前状态的逐元素乘法（在前面图表中的第一个操作中说明）。
- en: The second gate is called the **update gate** and its primary function is to
    update the cell state based on the current input. This gate passes the same input
    as the forget gate's inputs (*h**[t-1]* and *x**[t]*) into a sigmoid activation
    layer (![](img/00141.jpeg)) followed by a tanh activation layer and then performs
    an element-wise multiplication between these two results. Next, element-wise addition
    is performed with the result and the current state (illustrated by the second
    operation in the preceding diagram).
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个门被称为“更新门”，其主要功能是根据当前输入更新细胞状态。该门将与遗忘门的输入相同的输入（*h**[t-1]*和*x**[t]*）传递到一个sigmoid激活层（![](img/00141.jpeg)），然后经过tanh激活层，并对这两个结果进行逐元素乘法。接下来，将结果与当前状态进行逐元素加法（在前面图表中的第二个操作中说明）。
- en: Finally, there is an output gate which controls what information and how much
    information gets transferred to the adjoining cell to act as its inputs during
    the next time step. The current cell state is passed through a tanh activation
    layer and multiplied element-wise with the cell input (*h**[t-1]* and *x**[t]*)
    after being passed through a sigmoid layer (![](img/00141.jpeg)) for this operation.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，有一个输出门，它控制传递到相邻细胞的信息和信息量，以作为下一个时间步骤的输入。当前细胞状态通过tanh激活层传递，并在通过sigmoid层（![](img/00141.jpeg)）进行此操作后，与细胞输入（*h**[t-1]*和*x**[t]*）进行逐元素乘法。
- en: The update gate behaves as the filter on what the cell decides to output to
    the next cell. This output, h[t], is then passed on to the next LSTM cell as its
    input, and also to the above layers if many LSTM cells are stacked on top of each
    other.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新门的行为就像细胞决定输出到下一个细胞的过滤器。这个输出h[t]然后传递给下一个LSTM细胞作为它的输入，并且如果许多LSTM细胞堆叠在一起，也传递给上面的层。
- en: There's more...
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: LSTMs were a big leap forward when compared to what could be accomplished with
    feedforward networks and Recurrent Neural Networks. One might wonder what the
    next big step in the near future is, or even what that step might be. A lot researchers
    do believe "attention" is the next big step when it comes to the field of artificial
    intelligence. With the amount of data growing vastly with each day it becomes
    impossible to process every single bit of that data. This is where attention could
    be a potential game-changer, causing the networks to give their attention only
    to data or areas which are of high priority or interest and disregard useless
    information. For example, if an RNN is being used to create an image captioning
    engine, it will only pick a part of the image to to give its attention to for
    every word it outputs.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 与前馈网络和循环神经网络相比，LSTM是一个重大的飞跃。人们可能会想知道未来的下一个重大进步是什么，甚至可能是什么。许多研究人员认为，“注意力”是人工智能领域的下一个重大进步。随着每天数据量的急剧增长，处理每一位数据变得不可能。这就是注意力可能成为潜在的游戏改变者的地方，使网络只关注高优先级或感兴趣的数据或区域，并忽略无用的信息。例如，如果一个RNN被用来创建图像字幕引擎，它将只选择图像的一部分来关注，以便输出每个单词。
- en: 'The recent (2015) paper by Xu, et al. does exactly this. They explore adding
    attention to LSTM cells. Reading this paper can be a good place to start learning
    about the use of attention in neural networks. There have been some good results
    with using attention for various tasks, and more research is currently being conducted
    on the subject. The paper by Xu, et al. can be found using the following link:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 徐等人在2015年的最新论文正是这样做的。他们探索了在LSTM细胞中添加注意力。阅读这篇论文可以是学习神经网络中使用注意力的好起点。在各种任务中使用注意力已经取得了一些良好的结果，目前正在对该主题进行更多的研究。徐等人的论文可以通过以下链接找到：
- en: '[https://arxiv.org/pdf/1502.03044v2.pdf](https://arxiv.org/pdf/1502.03044v2.pdf)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1502.03044v2.pdf](https://arxiv.org/pdf/1502.03044v2.pdf)'
- en: Attention isn't the only variant to LSTMs. Some of the other active research
    is based on the utilization of grid LSTMs, as used in the paper by Kalchbrenner,
    et al., for which the link is at: [https://arxiv.org/pdf/1507.01526v1.pdf](https://arxiv.org/pdf/1507.01526v1.pdf).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力并不是LSTM的唯一变体。一些其他活跃的研究是基于格子LSTM的利用，正如Kalchbrenner等人在其论文中使用的那样，链接在：[https://arxiv.org/pdf/1507.01526v1.pdf](https://arxiv.org/pdf/1507.01526v1.pdf)。
- en: See also
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Some other useful information and papers related to RNNs and LSTMs in generative
    networks can be found by visiting the following links:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 关于生成网络中的RNN和LSTM的其他有用信息和论文可以通过访问以下链接找到：
- en: '[http://www.deeplearningbook.org/contents/rnn.html](http://www.deeplearningbook.org/contents/rnn.html)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.deeplearningbook.org/contents/rnn.html](http://www.deeplearningbook.org/contents/rnn.html)'
- en: '[https://arxiv.org/pdf/1502.04623.pdf](https://arxiv.org/pdf/1502.04623.pdf)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1502.04623.pdf](https://arxiv.org/pdf/1502.04623.pdf)'
- en: '[https://arxiv.org/pdf/1411.7610v3.pdf](https://arxiv.org/pdf/1411.7610v3.pdf)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1411.7610v3.pdf](https://arxiv.org/pdf/1411.7610v3.pdf)'
- en: '[https://arxiv.org/pdf/1506.02216v3.pdf](https://arxiv.org/pdf/1506.02216v3.pdf)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1506.02216v3.pdf](https://arxiv.org/pdf/1506.02216v3.pdf)'
