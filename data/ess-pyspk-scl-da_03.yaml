- en: 'Chapter 2: Data Ingestion'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章：数据摄取
- en: '**Data ingestion** is the process of moving data from disparate operational
    systems to a central location such as a data warehouse or a data lake to be processed
    and made conducive for data analytics. It is the first step of the data analytics
    process and is necessary for creating centrally accessible, persistent storage,
    where data engineers, data scientists, and data analysts can access, process,
    and analyze data to generate business analytics.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据摄取**是将数据从不同的操作系统迁移到中央位置（如数据仓库或数据湖）以便进行处理，并使其适用于数据分析的过程。这是数据分析过程的第一步，对于创建可以集中访问的持久存储至关重要，在这里，数据工程师、数据科学家和数据分析师可以访问、处理和分析数据，以生成业务分析。'
- en: You will be introduced to the capabilities of Apache Spark as a data ingestion
    engine for both batch and real-time processing. Various data sources supported
    by Apache Spark and how to access them using Spark's DataFrame interface will
    be presented.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你将了解Apache Spark作为批量和实时处理的数据摄取引擎的功能。将介绍Apache Spark支持的各种数据源以及如何使用Spark的DataFrame接口访问它们。
- en: Additionally, you will learn how to use Apache Spark's built-in functions to
    access data from external data sources, such as a **Relational Database Management
    System** (**RDBMS**), and message queues such as Apache Kafka, and ingest them
    into data lakes. The different data storage formats, such as structured, unstructured,
    and semi-structured file formats, along with the key differences between them,
    will also be explored. Spark's real-time streams processing engine called **Structured
    Streaming** will also be introduced. You will learn to create end-to-end data
    ingestion pipelines using batch processing as well as real-time stream processing.
    Finally, will explore a technique to unify batch and streams processing, called
    **Lambda Architecture**, and its implementation using Apache Spark.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还将学习如何使用Apache Spark的内置函数，从外部数据源访问数据，例如**关系数据库管理系统**（**RDBMS**）以及消息队列，如Apache
    Kafka，并将其摄取到数据湖中。还将探讨不同的数据存储格式，如结构化、非结构化和半结构化文件格式，以及它们之间的主要差异。Spark的实时流处理引擎——**结构化流**（Structured
    Streaming）也将被介绍。你将学习如何使用批处理和实时流处理创建端到端的数据摄取管道。最后，我们将探索一种将批处理和流处理统一的技术——**Lambda架构**，并介绍如何使用Apache
    Spark实现它。
- en: In this chapter, you will learn about the essential skills that are required
    to perform both batch and real-time ingestion using Apache Spark. Additionally,
    you will acquire the knowledge and tools required for building end-to-end scalable
    and performant big data ingestion pipelines.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习执行批量和实时数据摄取所需的基本技能，使用Apache Spark。此外，你将掌握构建端到端可扩展且高效的大数据摄取管道所需的知识和工具。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要主题：
- en: Introduction to Enterprise Decision Support Systems
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 企业决策支持系统简介
- en: Ingesting data from data sources
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据源摄取数据
- en: Ingesting data into data sinks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向数据接收端摄取数据
- en: Using file formats for data storage in data lakes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文件格式进行数据湖中的数据存储
- en: Building data ingestion pipelines in batches and real time
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建批量和实时数据摄取管道
- en: Unifying batch and real-time data ingestion using Lambda architecture
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Lambda架构统一批量和实时数据摄取
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will be using the Databricks Community Edition to run our
    code. This can be found at [https://community.cloud.databricks.com](https://community.cloud.databricks.com).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Databricks社区版来运行我们的代码。你可以在[https://community.cloud.databricks.com](https://community.cloud.databricks.com)找到它。
- en: Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注册说明请参见[https://databricks.com/try-databricks](https://databricks.com/try-databricks)。
- en: The code used in this chapter can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter02](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter02).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的代码可以从[https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter02](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter02)下载。
- en: The datasets used for this chapter can be found at [](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data)
    .
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的数据集可以在[https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data)找到。
- en: Introduction to Enterprise Decision Support Systems
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 企业决策支持系统简介
- en: An **Enterprise Decision Support System** (**Enterprise DSS**) is an end-to-end
    data processing system that takes operational and transactional data generated
    by a business organization and converts them into actionable insights. Every Enterprise
    DSS has a few standard components, such as data sources, data sinks, and data
    processing frameworks. An Enterprise DSS takes raw transactional data as its input
    and converts this into actionable insights such as operational reports, enterprise
    performance dashboards, and predictive analytics.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**企业决策支持系统**（**Enterprise DSS**）是一个端到端的数据处理系统，它将企业组织生成的运营和交易数据转换为可操作的洞察。每个企业决策支持系统都有一些标准组件，例如数据源、数据接收器和数据处理框架。企业决策支持系统以原始交易数据为输入，并将其转化为可操作的洞察，如运营报告、企业绩效仪表板和预测分析。'
- en: 'The following diagram illustrates the components of a typical Enterprise DSS
    in a big data context:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了在大数据环境中典型的企业决策支持系统的组成部分：
- en: '![Figure 2.1 – The Enterprise DSS architecture'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1 – 企业决策支持系统架构'
- en: '](img/B16736_02_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_02_01.jpg)'
- en: Figure 2.1 – The Enterprise DSS architecture
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 企业决策支持系统架构
- en: A big data analytics system is also an Enterprise DSS operating at much larger
    *Volumes*, with more *Variety* of data, and arriving at much faster *Velocity*.
    Being a type of Enterprise DSS, a big data analytics system has components that
    are similar to that of a traditional Enterprise DSS. The first step of building
    an Enterprise DSS is data ingestion from data sources into data sinks. You will
    learn about this process throughout this chapter. Let's elaborate on the different
    components of a big data analytics system, starting with data sources.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据分析系统也是一种企业决策支持系统，但其处理的规模要大得多，涉及的数据种类更多，且数据到达速度更快。作为企业决策支持系统的一种类型，大数据分析系统的组件与传统企业决策支持系统相似。构建企业决策支持系统的第一步是从数据源摄取数据并将其传送到数据接收器。您将在本章中学习这一过程。让我们从数据源开始，详细讨论大数据分析系统的各个组件。
- en: Ingesting data from data sources
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据源摄取数据
- en: In this section, we will learn about various data sources that a big data analytics
    system uses as a source of data. Typical data sources include transactional systems
    such as RDBMSes, file-based data sources such as **data lakes**, and **message
    queues** such as **Apache Kafka**. Additionally, you will learn about Apache Spark's
    built-in connectors to ingest data from these data sources and also write code
    so that you can view these connectors in action.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解大数据分析系统使用的各种数据源。典型的数据源包括事务系统（如RDBMS）、基于文件的数据源（如**数据湖**）以及**消息队列**（如**Apache
    Kafka**）。此外，您将学习Apache Spark内置的连接器，以便从这些数据源摄取数据，并编写代码以查看这些连接器的实际操作。
- en: Ingesting from relational data sources
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从关系型数据源摄取数据
- en: A **Transactional System**, or an **Operational System**, is a data processing
    system that helps an organization carry out its day-to-day business functions.
    These transactional systems deal with individual business transactions, such as
    a point-of-service transaction at a retail kiosk, an order placed on an online
    retail portal, an airline ticket booked, or a banking transaction. A historical
    aggregate of these transactions forms the basis of data analytics, and analytics
    systems ingest, store, and process these transactions over long periods. Therefore,
    such Transactional Systems form the source of data of analytics systems and a
    starting point for data analytics.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**事务系统**，或称**操作系统**，是一种数据处理系统，帮助组织执行日常的业务功能。这些事务系统处理单个业务交易，例如零售自助服务亭的交易、在线零售门户下单、航空票务预订或银行交易。这些交易的历史汇总构成了数据分析的基础，分析系统摄取、存储并处理这些交易数据。因此，这类事务系统构成了分析系统的数据源，并且是数据分析的起点。'
- en: Transactional systems come in many forms; however, the most common ones are
    RDBMSes. In the following section, we will learn how to ingest data from an RDBMS.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 交易系统有多种形式；然而，最常见的是关系型数据库管理系统（RDBMS）。在接下来的章节中，我们将学习如何从RDBMS中摄取数据。
- en: Relational data sources are a collection of relational databases and relational
    tables that consist of rows and named columns. The primary programming abstraction
    used to communicate with and query an RDBMS is called **Structured Query Language**
    (**SQL**). External systems can communicate with an RDBMS via communication protocols
    such as JDBC and ODBC. Apache Spark comes with a built-in JDBC data source that
    can be used to communicate with and query data stored in RDBMS tables.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据源是一组关系型数据库和关系型表，这些表由行和命名列组成。用于与 RDBMS 进行通信和查询的主要编程抽象称为 **结构化查询语言** (**SQL**)。外部系统可以通过
    JDBC 和 ODBC 等通信协议与 RDBMS 进行通信。Apache Spark 配备了一个内置的 JDBC 数据源，可以用来与存储在 RDBMS 表中的数据进行通信和查询。
- en: 'Let''s take a look at the code required to ingest data from an RDBMS table
    using PySpark, as shown in the following code snippet:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下使用 PySpark 从 RDBMS 表中摄取数据所需的代码，如以下代码片段所示：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the previous code snippet, we use the `spark.read()` method to load data
    from a JDBC data source by specifying the format to be `jdbc`. Here, we connect
    to a popular open source RDBMS called `url` that specifies the `jdbc url` for
    the MySQL server along with its `hostname`, `port number`, and `database name`.
    The `driver` option specifies the JDBC driver to be used by Spark to connect and
    communicate with the RDBMS. The `dtable`, `user`, and `password` options specify
    the table name to be queried and the credentials that are needed to authenticate
    with the RDBMS. Finally, the `show()` function reads sample data from the RDBMS
    table and displays it onto the console.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用 `spark.read()` 方法通过指定格式为 `jdbc` 来加载来自 JDBC 数据源的数据。在这里，我们连接到一个流行的开源
    RDBMS，名为 `url`，该 URL 指定了 MySQL 服务器的 `jdbc url`，并包含其 `hostname`、`port number` 和
    `database name`。`driver` 选项指定了 Spark 用来连接并与 RDBMS 通信的 JDBC 驱动程序。`dtable`、`user`
    和 `password` 选项指定了要查询的表名以及进行身份验证所需的凭证。最后，`show()` 函数从 RDBMS 表中读取示例数据并显示在控制台上。
- en: Important note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The previous code snippet, which uses dummy database credentials, shows them
    in plain text. This poses a huge data security risk and is not a recommended practice.
    The appropriate best practices to handle sensitive information such as using config
    files or other mechanisms provided by big data software vendors such as obscuring
    or hiding sensitive information should be followed.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段使用了虚拟的数据库凭证，并以纯文本形式显示。这会带来巨大的数据安全风险，并且不推荐这种做法。处理敏感信息时，应遵循适当的最佳实践，比如使用配置文件或其他大数据软件供应商提供的机制，如隐藏或模糊化敏感信息。
- en: To run this code, you can either use your own MySQL server and configure it
    with your Spark cluster, or you can use the sample code provided with this chapter
    to set up a simple MySQL server. The required code can be found at [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/blob/main/Chapter02/utils/mysql-setup.ipynb](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/blob/main/Chapter02/utils/mysql-setup.ipynb).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此代码，您可以使用自己的 MySQL 服务器并将其配置到您的 Spark 集群中，或者可以使用本章提供的示例代码来设置一个简单的 MySQL 服务器。所需的代码可以在
    [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/blob/main/Chapter02/utils/mysql-setup.ipynb](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/blob/main/Chapter02/utils/mysql-setup.ipynb)
    中找到。
- en: Note
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Apache Spark provides a JDBC data source and is capable of connecting to virtually
    any RDBMS that supports JDBC connections and has a JDBC driver available. However,
    it doesn't come bundled with any drivers; they need to be procured from the respective
    RDBMS provider, and the driver needs to be configured to your Spark cluster to
    be available to your Spark application.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 提供了一个 JDBC 数据源，能够连接几乎所有支持 JDBC 连接并具有 JDBC 驱动程序的关系型数据库管理系统（RDBMS）。然而，它并不自带任何驱动程序；需要从相应的
    RDBMS 提供商处获取，并且需要将驱动程序配置到 Spark 集群中，以便 Spark 应用程序可以使用。
- en: Ingesting from file-based data sources
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从基于文件的数据源进行数据摄取
- en: File-based data sources are very common when data is exchanged between different
    data processing systems. Let's consider an example of a retailer who wants to
    enrich their internal data sources with external data such as Zip Code data, as
    provided by a postal service provider. This data between the two organizations
    is usually exchanged via file-based data formats such as XML or JSON or more commonly
    using a delimited plain-text or CSV formats.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 文件型数据源在不同数据处理系统之间交换数据时非常常见。举个例子，假设一个零售商想要用外部数据（如邮政服务提供的邮政编码数据）来丰富他们的内部数据源。这两个组织之间的数据通常通过文件型数据格式进行交换，如
    XML 或 JSON，或者更常见的方式是使用分隔符的纯文本或 CSV 格式。
- en: Apache Spark supports various file formats, such as plain-text, CSV, JSON as
    well as binary file formats such as Apache Parquet and ORC. These files need to
    be located on a distributed filesystem such as **Hadoop Distributed File System**
    (**HDFS**), or a cloud-based data lake such as **AWS S3**, **Azure Blob**, or
    **ADLS** storage.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 支持多种文件格式，如纯文本、CSV、JSON 以及二进制文件格式，如 Apache Parquet 和 ORC。这些文件需要存储在分布式文件系统上，如**Hadoop
    分布式文件系统**（**HDFS**），或者是基于云的数据湖，如**AWS S3**、**Azure Blob**或**ADLS**存储。
- en: 'Let''s take a look at how to ingest data from CSV files using PySpark, as shown
    in the following block of code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何使用 PySpark 从 CSV 文件中读取数据，如下面的代码块所示：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the previous code snippet, we use the `spark.read()` function to read a CSV
    file. We specify the `inferSchema` and `header` options to be `true`. This helps
    Spark infer the column names and data type information by reading a sample set
    of data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用 `spark.read()` 函数来读取 CSV 文件。我们将 `inferSchema` 和 `header` 选项设置为
    `true`，这有助于 Spark 通过读取数据样本来推断列名和数据类型信息。
- en: Important note
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The file-based data source needs to be on a distributed filesystem. The Spark
    Framework leverages data parallel processing, and each Spark Executor tries to
    read a subset of the data into its own local memory. Therefore, it is essential
    that the file be located on a distributed filesystem and accessible by all the
    Executors and the Driver. HDFS and cloud-based data lakes, such as AWS S3, Azure
    Blob, and ADLS storage, are all distributed data storage layers that are good
    candidates to be used as file-based data sources with Apache Spark.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 文件型数据源需要存储在分布式文件系统中。Spark 框架利用数据并行处理，每个 Spark Executor 尝试将数据的一个子集读取到其本地内存中。因此，文件必须存储在分布式文件系统中，并且所有
    Executor 和 Driver 都能够访问该文件。HDFS 和基于云的数据湖，如 AWS S3、Azure Blob 和 ADLS 存储，都是分布式数据存储层，是与
    Apache Spark 一起使用的理想文件型数据源。
- en: Here, we read the CSV files from a `dbfs/` location, which is Databricks' proprietary
    filesystem called **Databricks Filesystem** (**DBFS**). DBFS is an abstraction
    layer that actually utilizes either AWS S3 or Azure Blob or ADLS storage underneath.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从`dbfs/`位置读取 CSV 文件，这是 Databricks 的专有文件系统，称为**Databricks 文件系统**（**DBFS**）。DBFS
    是一个抽象层，实际上使用的是 AWS S3、Azure Blob 或 ADLS 存储。
- en: Tip
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Given that each Executor tries to read only a subset of data, it is important
    that the file type being used is splittable. If the file cannot be split, an Executor
    might try to read a file larger than its available memory, run out of memory,
    and throw an "Out of Memory" error. One example of such an unsplittable file is
    a `gzipped` CSV or a text file.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个 Executor 只尝试读取数据的一个子集，因此文件类型必须是可拆分的。如果文件不能拆分，Executor 可能会尝试读取比其可用内存更大的文件，从而导致内存溢出并抛出“内存不足”错误。一个不可拆分文件的例子是`gzipped`的
    CSV 文件或文本文件。
- en: Ingesting from message queues
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从消息队列中读取数据
- en: Another type of data source commonly used in real-time streaming analytics is
    a **message queue**. A message queue offers a publish-subscribe pattern of data
    consumption, where a publisher publishes data to a queue while multiple subscribers
    could consume the data asynchronously. In a **Distributed Computing** context,
    a message queue needs to be distributed, fault-tolerant, and scalable, in order
    to serve as a data source for a distributed data processing system.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在实时流处理分析中常用的数据源是**消息队列**。消息队列提供了一种发布-订阅模式的数据消费方式，其中发布者将数据发布到队列，而多个订阅者可以异步地消费数据。在**分布式计算**环境中，消息队列需要是分布式的、容错的，并且可扩展的，以便作为分布式数据处理系统的数据源。
- en: One such message queue is Apache Kafka, which is quite prominent in real-time
    streaming workloads with Apache Spark. Apache Kafka is more than just a message
    queue; it is an end-to-end distributed streams processing platform in itself.
    However, for our discussion, we will consider Kafka to be just a distributed,
    scalable, and fault-tolerant message queue.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个消息队列是 Apache Kafka，它在与 Apache Spark 一起处理实时流式工作负载时非常突出。Apache Kafka 不仅仅是一个消息队列；它本身是一个端到端的分布式流处理平台。然而，在我们的讨论中，我们将
    Kafka 视为一个分布式、可扩展且容错的消息队列。
- en: 'Let''s take a look at what the code to ingest from Kafka using PySpark looks
    like, as shown in the following block of code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下使用 PySpark 从 Kafka 导入数据的代码，如下面的代码块所示：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the previous code example, we use `spark.read()` to load data from a Kafka
    server by providing its *hostname* and *port number*, a *topic* named `wordcount`*.*
    We also specify that Spark should start reading *events* from the very beginning
    of the queue, using the `StartingOffsets` option. Even though Kafka is more commonly
    used for streaming use cases with Apache Spark, this preceding code example makes
    use of Kafka as a data source for batch processing of data. You will learn to
    use Kafka with Apache Spark for processing streams in the *Data ingestion in real
    time using Structured Streaming* section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用`spark.read()`通过提供*主机名*和*端口号*，从 Kafka 服务器加载数据，主题名为`wordcount`。我们还指定了
    Spark 应该从队列的最开始处读取*事件*，使用`StartingOffsets`选项。尽管 Kafka 通常用于与 Apache Spark 一起处理流式用例，但这个前面的代码示例将
    Kafka 作为批量处理数据的数据源。你将在*使用结构化流实时导入数据*部分学习如何使用 Kafka 和 Apache Spark 处理流。
- en: Tip
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: In Kafka terminology, an individual queue is called a *topic*, and each event
    is called an *offset*. Kafka is a queue, so it serves events in the same order
    in which they were published onto a topic, and individual consumers can choose
    their own starting and ending offsets.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kafka 术语中，单个队列称为*主题*，而每个事件称为*偏移量*。Kafka 是一个队列，因此它按照事件发布到主题的顺序处理事件，个别消费者可以选择自己的起始和结束偏移量。
- en: Now that you are familiar with ingesting data from a few different types of
    **data sources** using Apache Spark, in the following section, let's learn how
    to ingest data into **d****ata sinks**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经熟悉了使用 Apache Spark 从不同类型的**数据源**导入数据，在接下来的部分中，让我们学习如何将数据导入到**数据汇**中。
- en: Ingesting data into data sinks
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入数据到数据汇
- en: A data sink, as its name suggests, is a storage layer for storing raw or processed
    data either for short-term staging or long-term persistent storage. Though the
    term of *data sink* is commonly used in real-time data processing, there is no
    specific harm in calling any storage layer where ingested data lands a data sink.
    Just like data sources, there are also different types of data sinks. You will
    learn about a few of the most common ones in the following sections.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 数据汇，顾名思义，是用于存储原始或处理过的数据的存储层，既可以用于短期暂存，也可以用于长期持久存储。尽管*数据汇*一词通常用于实时数据处理，但没有特定的限制，任何存储层中存放导入数据的地方都可以称为数据汇。就像数据源一样，数据汇也有不同的类型。你将在接下来的章节中学习一些最常见的类型。
- en: Ingesting into data warehouses
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入到数据仓库
- en: '**Data warehouses** are a specific type of persistent data storage most prominent
    in Business Intelligence type workloads. There is an entire field of study dedicated
    to Business Intelligence and data warehousing. Typically, a data warehouse uses
    an RDBMS as its data store. However, a data warehouse is different from a traditional
    database in that it follows a specific type of data modeling technique, called
    **dimensional modeling**. Dimensional models are very intuitive for representing
    real-world business attributes and are conducive for Business Intelligence types
    of queries used in building business reports and dashboards. A data warehouse
    could be built on any commodity RDBMS or using specialist hardware and software.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据仓库**是一种特定类型的持久数据存储，最常用于商业智能类型的工作负载。商业智能和数据仓库是一个完整的研究领域。通常，数据仓库使用 RDBMS
    作为其数据存储。然而，数据仓库与传统数据库不同，它遵循一种特定的数据建模技术，称为**维度建模**。维度模型非常直观，适合表示现实世界的业务属性，并有利于用于构建商业报告和仪表板的商业智能类型查询。数据仓库可以建立在任何通用
    RDBMS 上，或者使用专业的硬件和软件。'
- en: 'Let''s use PySpark to save a DataFrame to an RDBMS table, as shown in the following
    code block:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 PySpark 将 DataFrame 保存到 RDBMS 表中，如下面的代码块所示：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the previous block of code, we programmatically create a DataFrame with
    two columns from a Python `List` object. Then, we save the Spark DataFrame to
    a MySQL table using the `spark.write()` function, as shown in the following code
    snippet:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们通过编程方式从一个 Python `List` 对象创建了一个包含两列的 DataFrame。然后，我们使用 `spark.write()`
    函数将 Spark DataFrame 保存到 MySQL 表中，如下所示的代码片段所示：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding snippet of code to write data to an RDBMS is almost the same as
    the one to read data from an RDBMS. We still need to use the MySQL JDBC driver
    and specify the *hostname*, *port number*, *database name*, and *database credentials*.
    The only difference is that here, we need to use the `spark.write()` function
    instead of `spark.read()`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段写入数据到 RDBMS 与读取数据的代码几乎相同。我们仍然需要使用 MySQL JDBC 驱动程序，并指定*主机名*、*端口号*、*数据库名*和*数据库凭据*。唯一的区别是，在这里，我们需要使用
    `spark.write()` 函数，而不是 `spark.read()`。
- en: Ingesting into data lakes
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向数据湖中注入数据
- en: Data warehouses are excellent for intuitively representing real-world business
    data and storing highly structured relational data in a way that is conducive
    for Business Intelligence types of workloads. However, data warehouses fall short
    when handling unstructured data that is required by data science and machine learning
    types of workloads. Data warehouses are not good at handling the high *Volume*
    and *Velocity* of big data. That's where data lakes step in to fill the gap left
    by data warehouses.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库非常适合直观地表示现实世界的业务数据，并以有利于商业智能类型工作负载的方式存储高度结构化的关系型数据。然而，当处理数据科学和机器学习类型工作负载所需的非结构化数据时，数据仓库就显得不足。数据仓库不擅长处理大数据的高*体积*和*速度*。这时，数据湖就填补了数据仓库留下的空白。
- en: By design, data lakes are highly scalable and flexible when it comes to storing
    various types of data, including highly structured relational data and unstructured
    data such as images, text, social media, videos, and audio. Data lakes are also
    adept at handling data in batches as well as in streams. With the emergence of
    the cloud, data lakes have become very common these days, and they seem to be
    the future of persistent storage for all big data analytics workloads. A few examples
    of data lakes include Hadoop HDFS, AWS S3, Azure Blob or ADLS storage, and Google
    Cloud Storage.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从设计上讲，数据湖在存储各种类型的数据时具有高度的可扩展性和灵活性，包括高度结构化的关系型数据以及非结构化数据，如图像、文本、社交媒体、视频和音频。数据湖也擅长处理批量数据和流数据。随着云计算的兴起，数据湖如今变得非常普遍，并且似乎是所有大数据分析工作负载的持久存储的未来。数据湖的一些例子包括
    Hadoop HDFS、AWS S3、Azure Blob 或 ADLS 存储以及 Google Cloud 存储。
- en: 'Cloud-based data lakes have a few advantages over their on-premises counterparts:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 基于云的数据湖相较于本地部署的版本有一些优势：
- en: They are on-demand and infinitely scalable.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是按需的，并且具有无限的可扩展性。
- en: They are pay-per-use, thus saving you on upfront investments.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是按使用量计费的，从而节省了前期投资。
- en: They are completely independent of computing resources; so, storage can scale
    independently of computing resources.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们与计算资源完全独立；因此，存储可以独立于计算资源进行扩展。
- en: They support both structured and unstructured data, along with simultaneous
    batch and streaming, allowing the same storage layer to be used by multiple workloads.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们支持结构化数据和非结构化数据，并同时支持批处理和流式处理，使得同一存储层可以用于多个工作负载。
- en: Because of the preceding advantages, cloud-based data lakes have become prominent
    over the past few years. Apache Spark treats these data lakes as yet another file-based
    data storage. Therefore, working with data lakes using Spark is as simple as working
    with any other file-based data storage layer.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述优势，基于云的数据湖在过去几年变得越来越流行。Apache Spark 将这些数据湖视为另一种基于文件的数据存储。因此，使用 Spark 操作数据湖就像操作任何其他基于文件的数据存储层一样简单。
- en: 'Let''s take a look at how easy it is to save data to a data lake using PySpark,
    as shown in the following code example:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下使用 PySpark 将数据保存到数据湖是多么简单，如下所示的代码示例：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code block, we take the `wordcount_df` DataFrame that we created
    in the previous section and save it to the data lake in CSV format using the DataFrame's
    `write()` function. The `mode` option instructs `DataFrameWriter` to overwrite
    any existing data in the specified file location; note that you could also use
    `append` mode.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们将前一节中创建的 `wordcount_df` DataFrame 保存到数据湖中的 CSV 格式，使用的是 DataFrame
    的 `write()` 函数。`mode` 选项指示 `DataFrameWriter` 替换指定文件位置中任何现有的数据；请注意，你也可以使用 `append`
    模式。
- en: Ingesting into NoSQL and in-memory data stores
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向 NoSQL 和内存数据存储中注入数据
- en: Data warehouses have always been the traditional persistent storage layer of
    choice for data analytics use cases, and data lakes are emerging as the new choice
    to cater to a wider range of workloads. However, there are other big data analytics
    use cases involving ultra-low latency query response times that require special
    types of storage layers. Two such types of storage layers are NoSQL databases
    and in-memory databases, which we will explore in this section.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库一直以来都是数据分析用例的传统持久存储层，而数据湖作为新的选择，正在崛起，旨在满足更广泛的工作负载。然而，还有其他涉及超低延迟查询响应时间的大数据分析用例，这些用例需要特定类型的存储层。两种这样的存储层是
    NoSQL 数据库和内存数据库，本节将详细探讨这两种存储层。
- en: NoSQL databases for operational analytics at scale
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于大规模操作分析的 NoSQL 数据库
- en: NoSQL databases are an alternative to traditional relational databases, where
    there is a requirement for handling messy and unstructured data. NoSQL databases
    are very good at storing large amounts of unstructured data in the form of **Key-Value**
    pairs and very efficient at retrieving the **Value** for any given **Key** in
    constant time, at high concurrency.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: NoSQL 数据库是传统关系型数据库的替代方案，主要用于处理杂乱且非结构化的数据。NoSQL 数据库在以 **键值** 对的形式存储大量非结构化数据方面表现非常出色，并且能够在高并发情况下，以常数时间高效地检索任何给定
    **键** 对应的 **值**。
- en: Let's consider a use case where a business wants to provide precalculated, hyper-personalized
    content to their individual customers using millisecond query response times in
    a highly concurrent manner. A NoSQL database such as Apache Cassandra or MongoDB
    would be an ideal candidate for the use case.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个业务场景，其中一家企业希望通过毫秒级的查询响应时间，以高度并发的方式向单个客户提供预计算的、超个性化的内容。像 Apache Cassandra
    或 MongoDB 这样的 NoSQL 数据库将是这个用例的理想选择。
- en: Note
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Apache Spark doesn't come out of the box with any connectors for NoSQL databases.
    However, they are built and maintained by the respective database provider and
    can be downloaded for the respective provider and then configured with Apache
    Spark.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 并不自带针对 NoSQL 数据库的连接器。然而，这些连接器由各自的数据库提供商构建和维护，可以从相应的提供商处下载，并与 Apache
    Spark 配置使用。
- en: In-memory database for ultra-low latency analytics
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存数据库用于超低延迟分析
- en: In-memory databases store data purely in memory only, and persistent storage
    such as disks are not involved. This property of in-memory databases makes them
    faster in terms of data access speeds compared to their disk-based counterparts.
    A few examples of in-memory databases include **Redis** and **Memcached**. Since
    system memory is limited and data stored in memory is not durable over power cycles,
    in-memory databases are not suitable for the persistent storage of large amounts
    of historical data, which is typical for big data analytics systems. They do have
    their use in real-time analytics involving ultra-low latency response times.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 内存数据库仅将数据存储在内存中，不涉及磁盘等持久存储。正是由于这一特点，内存数据库在数据访问速度上比基于磁盘的数据库更快。一些内存数据库的例子包括 **Redis**
    和 **Memcached**。由于系统内存有限且存储在内存中的数据在断电后无法持久保存，因此内存数据库不适合用于存储大量历史数据，这在大数据分析系统中是典型需求。然而，它们在涉及超低延迟响应时间的实时分析中有其应用。
- en: Let's consider the example of an online retailer wanting to show the estimated
    shipment delivery time of a product to a customer at the time of checkout on their
    online portal. Most of the parameters that are needed to estimate delivery lead
    time can be precalculated. However, certain parameters, such as customer Zip Code
    and location, are only available when the customer provides them during checkout.
    Here, data needs to be instantly collected from the web portal, processed using
    an ultra-fast event processing system, and the results need to be calculated and
    stored in an ultra-low latency storage layer to be accessed and served back to
    the customer via the web app. All this processing should happen in a matter of
    seconds, and an in-memory database such as Redis or Memcached would serve the
    purpose of an ultra-low latency data storage layer.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一家在线零售商希望在顾客结账时，在其在线门户上展示产品的预计运输交付时间。大多数需要估算交货时间的参数可以预先计算。然而，某些参数，如客户邮政编码和位置，只有在客户结账时提供时才能获得。在这种情况下，需要立即从
    Web 门户收集数据，利用超快的事件处理系统进行处理，然后将结果计算并存储在超低延迟的存储层中，以便通过 Web 应用程序访问并返回给客户。所有这些处理应该在几秒钟内完成，而像
    Redis 或 Memcached 这样的内存数据库将充当超低延迟数据存储层的角色。
- en: So far, you have learned about accessing data from a few different data sources
    and ingesting them into various data sinks. Additionally, you have learned that
    you do not have much control over the data source. However, you do have complete
    control of your data sinks. Choosing the right data storage layer for certain
    high concurrency, ultra-low latency use cases is important. However, for most
    big data analytics use cases, data lakes are becoming the de facto standard as
    the preferred persistent data storage layer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了如何访问来自不同数据源的数据并将其导入到各种数据接收端。此外，你已经了解到你对数据源的控制有限，但你对数据接收端有完全的控制。为某些高并发、超低延迟的用例选择正确的数据存储层非常重要。然而，对于大多数大数据分析用例，数据湖已成为首选的持久数据存储层，几乎成了事实上的标准。
- en: Another key factor for optimal data storage is the actual format of the data.
    In the following section, we will explore a few data storage formats and their
    relative merits.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个优化数据存储的关键因素是数据的实际格式。在接下来的部分，我们将探讨几种数据存储格式及其相对优点。
- en: Using file formats for data storage in data lakes
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用文件格式在数据湖中存储数据
- en: The file format you choose to store data in a data lake is key in determining
    the ease of data storage and retrieval, query performance, and storage space.
    So, it is vital that you choose the optimal data format that can balance these
    factors. Data storage formats can be broadly classified into structured, unstructured,
    and semi-structured formats. In this section, we will explore each of these types
    with the help of code examples.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你选择用于在数据湖中存储数据的文件格式对数据存储和检索的便捷性、查询性能以及存储空间有关键影响。因此，选择一个可以平衡这些因素的最佳数据格式至关重要。数据存储格式大致可以分为结构化、非结构化和半结构化格式。在本节中，我们将通过代码示例探讨这几种类型。
- en: Unstructured data storage formats
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非结构化数据存储格式
- en: Unstructured data is any data that is not represented by a predefined data model
    and can be either human or machine-generated. For instance, unstructured data
    could be data stored in plain text documents, PDF documents, sensor data, log
    files, video files, images, audio files, social media feeds, and more.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据是指没有预定义数据模型表示的数据，可以是人工或机器生成的。例如，非结构化数据可以是存储在纯文本文件、PDF文件、传感器数据、日志文件、视频文件、图像、音频文件、社交媒体流等中的数据。
- en: 'Unstructured data might contain important patterns, and extracting these patterns
    could lead to valuable insights. However, storing data in unstructured format
    is not very useful due to the following reasons:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据可能包含重要的模式，提取这些模式可能带来有价值的见解。然而，由于以下原因，以非结构化格式存储数据并不十分有用：
- en: Unstructured data might not always have an inherent compression mechanism and
    can take up large amounts of storage space.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非结构化数据可能并不总是具有固有的压缩机制，并且可能占用大量存储空间。
- en: Externally compressing unstructured files saves space but expends the processing
    power for the compression and decompression of files.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对非结构化文件进行外部压缩可以节省空间，但会消耗用于文件压缩和解压的处理能力。
- en: Storing and accessing unstructured files is somewhat difficult because they
    inherently lack any schema information.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储和访问非结构化文件比较困难，因为它们本身缺乏任何模式信息。
- en: 'Given the preceding reasons, it makes sense to ingest unstructured data and
    convert it into a structured format before storing it inside the data lake. This
    makes the downstream processing of data easier and more efficient. Let''s take
    a look at an example where we take a set of unstructured image files and convert
    them into a DataFrame of image attributes. Then, we store them using the CSV file
    format, as shown in the following code snippet:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于上述原因，摄取非结构化数据并在将其存储到数据湖之前将其转换为结构化格式是合理的。这样可以使后续的数据处理更加轻松和高效。让我们看一个例子，我们将一组非结构化的图像文件转换为图像属性的DataFrame，然后使用CSV文件格式存储它们，如下所示的代码片段所示：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the previous code block, the following occurs:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码块中，发生了以下情况：
- en: We load a set of images files using Spark's built-in `image` format; the result
    is a Spark DataFrame of image attributes.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用Spark内置的`image`格式加载一组图像文件，结果是一个包含图像属性的Spark DataFrame。
- en: We use the `printSchema()` function to take a look at the DataFrame's schema
    and discover that the DataFrame has a single nested column named `image` with
    `origin`, `height`, `width`, `nChannels`, and more, as its inner attributes.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`printSchema()`函数查看DataFrame的模式，并发现DataFrame有一个名为`image`的单一嵌套列，其中包含`origin`、`height`、`width`、`nChannels`等作为其内部属性。
- en: Then, we bring up the inner attributes to the top level using the `image` prefix
    with each inner attribute, such as `image.origin`, and create a new DataFrame
    named `image_df` with all of the image's individual attributes as top-level columns.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们使用`image`前缀将内部属性提升到顶层，例如`image.origin`，并创建一个新的DataFrame，命名为`image_df`，其中包含图像的所有单独属性作为顶层列。
- en: Now that we have our final DataFrame, we write it out to the data lake using
    the CSV format.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们已经得到了最终的DataFrame，我们将其以CSV格式写入数据湖。
- en: Upon browsing the data lake, you can see that the process writes a few CSV files
    to the data lake with file sizes of, approximately, 127 bytes.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在浏览数据湖时，你可以看到该过程向数据湖写入了几个CSV文件，文件大小大约为127字节。
- en: Tip
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示
- en: The number of files written out to storage depends on the number of partitions
    of the DataFrame. The number of DataFrame partitions depends on the number of
    executors cores and the `spark.sql.shuffle.partitions` Spark configuration. This
    number also changes every time the DataFrame undergoes a shuffle operation. In
    Spark 3.0, **Adaptive Query Execution** automatically manages the optimal number
    of shuffle partitions.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 写入存储的文件数量取决于DataFrame的分区数量。DataFrame的分区数量取决于执行器核心的数量以及`spark.sql.shuffle.partitions`的Spark配置。每当DataFrame进行洗牌操作时，这个数量也会发生变化。在Spark
    3.0中，**自适应查询执行**会自动管理最优的洗牌分区数量。
- en: 'Along with file sizes, query performance is also an important factor when considering
    the file format. So, let''s run a quick test where we perform a moderately complex
    operation on the DataFrame, as shown in the following block of code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 文件大小和查询性能是考虑文件格式时的两个重要因素。因此，我们进行一个快速测试，在DataFrame上执行一个适度复杂的操作，如以下代码块所示：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The previous block of code, first, creates a new column with every row value
    as the maximum width among all rows. Then, it filters out the row that has this
    maximum value for the `width` column. The query is moderately complex and typical
    of the kind of queries used in data analytics. In our sample test case, the query
    running on an unstructured binary file took, approximately, *5.03 seconds*. In
    the following sections, we will look at the same query on other file formats and
    compare query performances.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块首先创建了一个新列，其中每一行的值为所有行中最大的宽度。然后，它过滤掉具有`width`列最大值的行。这个查询是适度复杂的，典型的数据分析查询类型。在我们的示例测试中，在一个非结构化二进制文件上运行的查询大约花费了*5.03秒*。在接下来的章节中，我们将查看其他文件格式上的相同查询，并比较查询性能。
- en: Semi-structured data storage formats
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半结构化数据存储格式
- en: In the preceding example, we were able to take a binary image file, extract
    its attributes, and store them in CSV format, which makes the data structured
    but still keeps it in a human-readable format. CSV format is another type of data
    storage format called semi-structured data format. Semi-structured data formats,
    like unstructured data formats, do not have a predefined data model. However,
    they organize data in a way that makes it easier to infer schema information from
    the files themselves, without any external metadata being supplied. They are a
    popular data format for the exchange of data between distinct data processing
    systems. Examples of semi-structured data formats include CSV, XML, and JSON.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们能够获取一个二进制图像文件，提取其属性，并将其存储为CSV格式，这使得数据结构化，但仍保持人类可读格式。CSV格式是另一种数据存储格式，称为半结构化数据格式。半结构化数据格式与非结构化数据格式类似，没有预定义的数据模型。然而，它们以一种方式组织数据，使得从文件本身推断模式信息变得更加容易，而无需提供外部元数据。它们是不同数据处理系统之间交换数据的流行数据格式。半结构化数据格式的示例包括CSV、XML和JSON。
- en: 'Let''s take a look an example of how we can use PySpark to handle semi-structured
    data, as shown in the following code block:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用PySpark处理半结构化数据的示例，如以下代码块所示：
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The previous code example takes the CSV files generated during the previous
    image processing example and loads them as a Spark DataFrame. We have enabled
    options to infer column names and data types from the actual data. The `printSchema()`
    function shows that Spark was able to infer column data types correctly for all
    columns except for the binary data column from the semi-structured files. The
    `show()` function shows that a DataFrame was correctly reconstructed from the
    CSV files along with column names.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码示例使用在先前图像处理示例中生成的CSV文件，并将其加载为Spark数据框。我们启用了从实际数据推断列名和数据类型的选项。`printSchema()`函数显示Spark能够正确推断所有列的数据类型，除了来自半结构化文件的二进制数据列。`show()`函数显示数据框已经从CSV文件中正确重建，并且包含列名。
- en: 'Let''s run a moderately complex query on the `csv_df` DataFrame, as shown in
    the following block of code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在`csv_df`数据框上运行一个适度复杂的查询，如下所示代码块：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding block of code, we perform a few DataFrame operations to get
    the row with the maximum value for the `width` column. The code took *1.24 seconds*
    to execute using CSV data format, compared to the similar code that we executed
    in the *Unstructured Data Storage Formats* section, which took, approximately,
    *5 seconds*. Thus, seemingly, semi-structured file formats are better than unstructured
    files for data storage, as it is relatively easier to infer schema information
    from this data storage format.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们执行了一些数据框操作，以获取`width`列最大值的行。使用CSV数据格式执行的代码花费了*1.24秒*，而我们在*非结构化数据存储格式*部分执行的类似代码大约花费了*5秒*。因此，显然，半结构化文件格式比非结构化文件更适合数据存储，因为从这种数据存储格式中推断模式信息相对更容易。
- en: However, pay attention to the results of the `show()` function in the preceding
    code snippet. The data column containing binary data is inferred incorrectly as
    string type, and the column data is truncated. Therefore, it should be noted that
    semi-structured formats are not suitable for representing all data types, and
    they could also lose information with certain data types during the conversion
    from one data format into another.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意前面代码片段中`show()`函数的结果。包含二进制数据的数据列被错误地推断为字符串类型，并且列数据被截断。因此，需要注意的是，半结构化格式并不适合表示所有数据类型，并且在从一种数据格式转换到另一种数据格式时，某些数据类型可能会丢失信息。
- en: Structured data storage formats
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化数据存储格式
- en: Structured data follows a predefined data model and has a tabular format with
    well-defined rows and named columns along with defined data types. A few examples
    of structured data formats are relational database tables and data generated by
    transactional systems. Note that there are also file formats that are fully structured
    data along with their data models, such as Apache Parquet, Apache Avro, and ORC
    files, that can be easily stored on data lakes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据遵循预定义的数据模型，具有表格格式，具有明确定义的行和命名列以及定义的数据类型。结构化数据格式的一些示例包括关系数据库表和事务系统生成的数据。请注意，还有一些完全结构化数据及其数据模型的文件格式，如Apache
    Parquet、Apache Avro和ORC文件，它们可以轻松存储在数据湖中。
- en: '**Apache Parquet** is a binary, compressed, and columnar storage format that
    was designed to be efficient at data storage as well as query performance. Parquet
    is a first-class citizen of the Apache Spark framework, and Spark''s in-memory
    storage format, called **Tungsten**, was designed to take full advantage of the
    Parquet format. Therefore, you will get the best performance and efficiency out
    of Spark when your data is stored in Parquet format.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Parquet**是一种二进制、压缩的列式存储格式，旨在提高数据存储效率和查询性能。Parquet是Apache Spark框架的一级公民，Spark的内存存储格式**Tungsten**旨在充分利用Parquet格式。因此，当你的数据存储在Parquet格式中时，你将从Spark中获得最佳的性能和效率。'
- en: Note
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A Parquet file is a binary file format, meaning that the contents of the file
    have a binary encoding. Therefore, they are not human-readable, unlike text-based
    file formats such as JSON or CSV. However, one advantage of this is that they
    are easily interpreted by machines without losing any time during the encoding
    and decoding processes.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet文件是一种二进制文件格式，意味着文件的内容经过二进制编码。因此，它们不可供人类阅读，不像基于文本的文件格式，如JSON或CSV。然而，这种格式的一个优点是，机器可以轻松解析这些文件，并且在编码和解码过程中不会浪费时间。
- en: 'Let''s convert the `image_df` DataFrame, containing image attribute data from
    the *Unstructured data storage formats* section, into Parquet format, as shown
    in the following code block:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将`image_df` DataFrame（包含来自*未结构化数据存储格式*部分的图像属性数据）转换为Parquet格式，如下所示的代码块所示：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The previous block of code loads binary image files into a Spark DataFrame and
    writes the data back into the data lake in Parquet format. The result of the `show()`
    function reveals that the binary data in the *data* column was not truncated and
    has been preserved from the source image files as-is.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个代码块将二进制图像文件加载到Spark DataFrame中，并将数据以Parquet格式写回数据湖。`show()`函数的结果显示，*data*列中的二进制数据并未被截断，并且已经从源图像文件中如实保留。
- en: 'Let''s perform a moderately complex operation, as shown in the following block
    of code:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行一个中等复杂度的操作，如下所示的代码块：
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The preceding code block extracts the row with the maximum value for the column
    named `width`. The query takes, approximately, *4.86 seconds* to execute, as compared
    to over *5 seconds* with the original unstructured image data. Therefore, this
    makes the structured Parquet file format the optimal format to be used to store
    data in data lakes with Apache Spark. Seemingly, the semi-structured CSV files
    took less time to execute the query, but they also truncated the data, making
    it not the right fit for every use case. As a general rule of thumb, the Parquet
    data format is recommended for almost all Apache Spark use cases, that is, unless
    a specific use case calls for another type of data storage format.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块提取了列名为`width`的最大值所在的行。该查询大约需要*4.86秒*来执行，而使用原始未结构化的图像数据时则需要超过*5秒*。因此，这使得结构化的Parquet文件格式成为在Apache
    Spark数据湖中存储数据的最佳格式。表面上看，半结构化的CSV文件执行查询所需时间较短，但它们也截断了数据，导致它们并不适合所有使用场景。作为一个通用的经验法则，几乎所有Apache
    Spark的使用场景都推荐使用Parquet数据格式，除非某个特定的使用场景需要其他类型的数据存储格式。
- en: So far, you have seen that choosing the right data format can affect the correctness,
    ease of use, storage efficiency, query performance, and scalability of the data.
    Additionally, there is another factor that needs to be considered when storing
    data into data lakes no matter which data format you use. This technique is called
    **data partitioning** and can really make or break your downstream query performance.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到选择合适的数据格式会影响数据的正确性、易用性、存储效率、查询性能和可扩展性。此外，无论你使用哪种数据格式，将数据存储到数据湖中时，还有另一个需要考虑的因素。这个技术叫做**数据分区**，它可以真正决定你的下游查询性能是成功还是失败。
- en: Put simply, data partitioning is the process of physically dividing your data
    across multiple folders or partitions. Apache Spark uses this partition information
    to only read the relevant data files required by the query into memory. This mechanism
    is called **partition pruning**. You will learn more about data partitioning in
    [*Chapter 3*](B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056), *Data Cleansing
    and Integration*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，数据分区是将数据物理地划分到多个文件夹或分区中的过程。Apache Spark利用这些分区信息，只将查询所需的相关数据文件加载到内存中。这一机制称为**分区剪枝**。你将会在[*第3章*](B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056)，*数据清理与整合*中了解更多关于数据分区的内容。
- en: So far, you have learned about the individual components of an Enterprise DSS,
    namely, data sources, data sinks, and data storage formats. Additionally, you
    gained a certain level of familiarity with the Apache Spark Framework as a big
    data processing engine in the previous chapter. Now, let's put this knowledge
    to use and build an end-to-end data ingestion pipeline in the following section.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经了解了企业决策支持系统（DSS）的各个组成部分，即数据源、数据目标和数据存储格式。此外，在上一章中，你对Apache Spark框架作为大数据处理引擎也有了一定的了解。现在，让我们运用这些知识，构建一个端到端的数据摄取管道。
- en: Building data ingestion pipelines in batch and real time
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建批处理和实时数据摄取管道
- en: An end-to-end data ingestion pipeline involves reading data from data sources
    and ingesting it into a data sink. In the context of big data and data lakes,
    data ingestion involves a large number of data sources and, thus, requires a data
    processing engine that is highly scalable. There are specialist tools available
    in the market that are purpose-built for handling data ingestion at scale, such
    as StreamSets, Qlik, Fivetran, Infoworks, and more, from third-party vendors.
    In addition to this, cloud providers have their own native offerings such as AWS
    Data Migration Service, Microsoft Azure Data Factory, and Google Dataflow. There
    are also free and open source data ingestion tools available that you could consider
    such as Apache Sqoop, Apache Flume, Apache Nifi, to name a few.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一个端到端的数据摄取管道涉及从数据源读取数据，并将其摄取到数据目标中。在大数据和数据湖的背景下，数据摄取通常涉及大量数据源，因此需要一个高可扩展性的数据处理引擎。市场上有一些专门的工具，旨在处理大规模数据摄取，例如
    StreamSets、Qlik、Fivetran、Infoworks 等第三方供应商提供的工具。此外，云服务提供商也有其自有的本地工具，例如 AWS 数据迁移服务、Microsoft
    Azure 数据工厂和 Google Dataflow。还有一些免费的开源数据摄取工具可以考虑使用，例如 Apache Sqoop、Apache Flume、Apache
    Nifi 等。
- en: Tip
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Apache Spark is good enough for ad hoc data ingestion, but it is not a common
    industry practice to use Apache Spark as a dedicated data ingestion engine. Instead,
    you should consider a dedicated, purpose-built data ingestion tool for your dedicated
    data ingestion needs. You can either choose from third-party vendors or choose
    to manage one of the open source offerings by yourself.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 足够适合用于临时数据摄取，但将 Apache Spark 作为专门的数据摄取引擎并不是行业中的常见做法。相反，您应该考虑选择一个专门的、为数据摄取需求量身定制的工具。您可以选择第三方供应商提供的工具，或者选择自己管理一个开源工具。
- en: In this section, we will explore Apache Spark's capabilities for data ingestion
    in both a batch and streams processing manner.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨 Apache Spark 在批量处理和流处理方式下的数据摄取能力。
- en: Data ingestion using batch processing
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量处理的数据摄取
- en: Batch processing refers to processing one group or batch of data at a time.
    Batch processes are scheduled to run at specified intervals without any user intervention.
    Customarily, batch processes are run at night, after business hours. The simple
    reason for this is that batch processes tend to read a large number of transactions
    from the operational systems, which adds a lot of load to the operational systems.
    This is undesirable because operational systems are critical for the day-to-day
    operations of a business, and we do not want to unnecessarily burden the transactional
    system with workloads that are non-critical to daily business operations.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 批量处理是指一次处理一组或一批数据。批量处理通常是在预定的时间间隔内运行的，且不需要用户干预。通常情况下，批量处理会安排在夜间、业务时间之外运行。其简单的原因在于，批量处理通常需要从操作系统中读取大量的事务数据，这会给操作系统带来很大的负担。这是不可取的，因为操作系统对于企业的日常运营至关重要，我们不希望给事务系统带来那些对日常业务操作没有关键影响的工作负载。
- en: Additionally, batch processing jobs tend to be repetitive as they run at regular
    intervals, bringing in the new set of data generated after the last successful
    batch process has been run. Batch processing can be of two types, namely, **full
    data load** and **incremental data load**.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，批量处理任务通常是重复性的，因为它们会在固定的时间间隔内运行，每次都会引入自上次成功的批处理以来生成的新数据。批量处理可以分为两种类型，分别是 **完整数据加载**
    和 **增量数据加载**。
- en: Full data loads
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完整数据加载
- en: A full data load involves completely overwriting an existing dataset. This is
    useful for datasets that are relatively small in size and that do not change often.
    It is also an easier process to implement, as we just have to scan the entire
    source dataset and completely overwrite the destination dataset. There is no need
    to maintain any state information regarding the previous data ingestion job. Let's
    take an example of a dimensional table from a data warehouse, such as a calendar
    table or a table containing the data of all the physical stores of a retailer.
    These tables do not change often and are relatively small, making them ideal candidates
    for full data loads. Though easy to implement, full data loads have their drawbacks
    when it comes to dealing with very large source datasets that change regularly.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 完全数据加载涉及完全覆盖现有数据集。这对于数据量相对较小且变化不频繁的数据集非常有用。它也是一个更容易实现的过程，因为我们只需要扫描整个源数据集并完全覆盖目标数据集。无需维护任何关于上次数据导入作业的状态信息。以数据仓库中的维度表为例，例如日历表或包含所有零售商实体店数据的表。这些表变化不大且相对较小，非常适合进行完全数据加载。虽然实现简单，但在处理非常大的源数据集并且数据经常变化时，完全数据加载有其缺点。
- en: Let's consider the transactional data of a large retailer with more than a thousand
    stores all over the country, generating about 500 transactions per month per store.
    This translates to, approximately, 15,000 transactions ingested into the data
    lake per day. This number quickly adds up when we also consider historical data.
    Let's say that we just started building our data lake, and so far, we have only
    about 6 months of transactional data ingested. Even at this scale, we already
    have 3 million transactional records in our dataset, and completely truncating
    and loading the dataset is not a trivial task.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们考虑一个大型零售商的交易数据，该零售商在全国拥有超过一千家门店，每家门店每月产生大约500笔交易。换算下来，大约每天有15,000笔交易被导入数据湖。考虑到历史数据，这个数字会迅速增加。假设我们刚开始构建数据湖，目前只导入了大约6个月的交易数据。即使在这种规模下，我们的数据集中已经有了300万条交易记录，完全清空并重新加载数据集并非一项轻松的任务。
- en: Another important factor to consider here is that typically, operational systems
    only retain historical data for small time intervals. Here, a full load means
    losing history from the data lake as well. At this point, you should consider
    an incremental load for data ingestion.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的重要因素是，通常操作系统只保留小时间间隔的历史数据。在这里，完全加载意味着也会丢失数据湖中的历史数据。此时，您应考虑增量加载来进行数据导入。
- en: Incremental data loads
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增量数据加载
- en: During an incremental data load, we only ingest the new set of data that was
    created in the data source after the previous case of successful data ingestion.
    This incremental dataset is generally referred to as the delta. An incremental
    load ingests datasets that are smaller in size compared to a full load, and since
    we already maintain the full historical data in our delta lake, incremental doesn't
    need to depend on the data source maintaining a full history.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在增量数据加载过程中，我们只导入在上次成功的数据导入后，数据源中新创建的一组数据。这个增量数据集通常被称为delta（增量集）。与完全加载相比，增量加载导入的数据集更小，并且由于我们已经在delta湖中维护了完整的历史数据，因此增量加载不需要依赖数据源来维护完整的历史记录。
- en: Building on the same retailer example from earlier, let's assume that we run
    our incremental batch load once per night. In this scenario, we only need to ingest
    15,000 transactions into the data lake per day, which is pretty easy to manage.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面提到的零售商示例，假设我们每晚运行一次增量批量加载。在这种情况下，我们每天只需要将15,000笔交易导入数据湖，这相对容易管理。
- en: 'Designing an incremental data ingestion pipeline is not as simple compared
    to a full load pipeline. State information about the previous run of the incremental
    job needs to be maintained so that we can identify all of the new records from
    the data source that have not already been ingested into the data lake. This state
    information is stored in a special data structure, called a *watermark* table.
    This watermark table needs to be updated and maintained by the data ingestion
    job. A typical data ingestion pipeline is illustrated as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 设计增量数据导入管道并不像设计完全加载管道那么简单。需要维护增量作业上次运行的状态信息，以便我们能够识别所有来自数据源的尚未导入到数据湖中的新记录。这个状态信息被存储在一个特殊的数据结构中，称为*水印*表。这个水印表需要由数据导入作业来更新和维护。一个典型的数据导入管道如下图所示：
- en: '![Figure 2.2 – Data ingestion'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.2 – 数据摄取'
- en: '](img/B16736_02_02.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_02_02.jpg)'
- en: Figure 2.2 – Data ingestion
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 数据摄取
- en: The preceding diagram shows a typical data ingestion pipeline using Apache Spark,
    along with the watermark table for incremental loads. Here, we ingest raw transactional
    data from source systems using Spark's built-in data sources, process them using
    DataFrame operations and then send the data back to a data lake.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图示展示了一个典型的使用Apache Spark的数据摄取管道，并且包含了用于增量加载的水印表。在这里，我们使用Spark内建的数据源从源系统摄取原始交易数据，使用DataFrame操作进行处理，然后将数据发送回数据湖。
- en: Expanding on the retail example from the previous section, let's build an end-to-end
    data ingestion pipeline with batch processing using PySpark. One of the prerequisites
    for building a data pipeline is, of course, data, and for this example, we will
    make use of the *Online Retail* dataset made available by *UC Irvine Machine Learning
    Repository*. The dataset is available in CSV format in the GitHub repository mentioned
    in the *Technical requirements* section of this chapter. The *Online Retail* dataset
    contains transactional data for an online retailer.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展上一节的零售示例时，让我们使用PySpark构建一个端到端的数据摄取管道，采用批处理方式。构建数据管道的一个前提条件当然是数据，对于这个示例，我们将使用*UC
    Irvine机器学习库*提供的*在线零售*数据集。该数据集以CSV格式存放在本章*技术要求*部分提到的GitHub仓库中。*在线零售*数据集包含了一个在线零售商的交易数据。
- en: We will download the dataset, consisting of two CSV files, and upload them to
    the *Databricks Community Edition* notebook environment via the upload interface
    that is present within the notebook's file menu. Once the dataset has been uploaded,
    we will make a note of the file location.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将下载包含两个CSV文件的数据集，并通过笔记本文件菜单中的上传接口将它们上传到*Databricks Community Edition*笔记本环境。一旦数据集上传完成，我们将记录文件位置。
- en: Note
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you are using your own Spark environment, make sure that you have the datasets
    available at a location that is accessible for your Spark cluster.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是自己的Spark环境，请确保数据集存放在Spark集群可以访问的位置。
- en: 'Now we can get started with the actual code for the data ingestion pipeline,
    as shown in the following code example:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始实际的代码部分，构建数据摄取管道，如以下代码示例所示：
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the preceding code block, we loaded the CSV files with the `header` and
    `inferSchema` options enabled. This creates a Spark DataFrame with eight columns
    along with their respective data types and column names. Now, let''s ingest this
    data into the data lake in Parquet format, as shown in the following code block:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们启用了`header`和`inferSchema`选项来加载CSV文件。这会创建一个包含八个列及其相应数据类型和列名的Spark
    DataFrame。现在，让我们将这些数据以Parquet格式摄取到数据湖中，如以下代码块所示：
- en: '[PRE13]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we save the `retail_df` Spark DataFrame, containing raw retail transactions,
    to the data lake in Parquet format using the DataFrameWriter's `write()` function.
    We also specify the `mode` option to `overwrite` and, essentially, implement a
    full data load.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将包含原始零售交易数据的`retail_df` Spark DataFrame使用DataFrameWriter的`write()`函数保存到数据湖中，以Parquet格式存储。我们还将`mode`选项设置为`overwrite`，基本上执行的是全量数据加载。
- en: One thing to note here is that the entire data ingestion job is a mere **10**
    lines of code, and this can easily be scaled up to tens of millions of records,
    processing up to many petabytes of data. This is the power and simplicity of Apache
    Spark, which has made it the de facto standard for big data processing in a very
    short period of time. Now, how would you actually scale the preceding data ingestion
    batch job and then, eventually, productionize it?
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，整个数据摄取作业仅仅是**10**行代码，而且它可以轻松扩展到数千万条记录，处理多达数个PB的数据。这就是Apache Spark的强大与简洁，它使得Apache
    Spark在极短的时间内成为大数据处理的事实标准。那么，你将如何扩展前述的数据摄取批处理作业，并最终将其投入生产环境呢？
- en: Apache Spark was built from the ground up to be scalable, and its scalability
    is entirely dependent on the number of cores available to a job on the cluster.
    So, to scale your Spark job, all you need to do is to allocate more processing
    cores to the job. Most commercially available Spark-as-a-managed-service offerings
    provide a nifty **Autoscaling** functionality. With this autoscaling functionality,
    you just need to specify the minimum and the maximum number of nodes for your
    cluster, and **Cluster Manager** dynamically figures out the optimal number of
    cores for your job.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是从头开始构建的，旨在具备可扩展性，其可扩展性完全依赖于集群上作业可用的核心数量。因此，要扩展你的 Spark 作业，你只需为作业分配更多的处理核心。大多数商业化的
    Spark 作为托管服务的提供方案都提供了便捷的**自动扩展**功能。通过此自动扩展功能，你只需指定集群的最小和最大节点数，**集群管理器**就能动态计算出为你的作业分配的最优核心数。
- en: Most commercial Spark offerings also come with a built-in **Job Scheduler**
    and support directly scheduling notebooks as jobs. External schedulers, ranging
    from the rudimentary **crontab** to sophisticated job orchestrators such as **Apache
    Airflow**, can also be used to productionize your Spark jobs. This really makes
    the process of cluster capacity planning easier for you, freeing up your time
    to focus on actual data analytics rather than spending your time and energy on
    capacity planning, tuning, and maintaining Spark clusters.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数商业化的 Spark 提供方案也配备了内置的**作业调度器**，并支持将笔记本直接作为作业进行调度。外部调度器，从简单的**crontab**到复杂的作业协调器如**Apache
    Airflow**，也可以用来将 Spark 作业生产化。这大大简化了集群容量规划的过程，帮助你腾出时间，专注于实际的数据分析，而不是在容量规划、调优和维护
    Spark 集群上耗费时间和精力。
- en: So far, in this section, you have viewed an example of a full load batch ingestion
    job that loads the entire data from the data source and then overwrites the dataset
    in the data lake. You would need to add a little more business logic to maintain
    the ingestion job's state in a watermark data structure and then calculate the
    delta to perform an incremental load. You could build all this logic by yourself,
    or, alternatively, you could simply use Spark's structured streaming engine to
    do the heavy lifting for you, as discussed in the following section.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本节中，你已经查看了一个完整加载批处理摄取作业的示例，该作业从数据源加载整个数据集，并覆盖数据湖中的数据集。你需要添加一些业务逻辑，以在水印数据结构中维护摄取作业的状态，然后计算增量进行增量加载。你可以自己构建所有这些逻辑，或者，也可以简单地使用
    Spark 的结构化流处理引擎来为你完成繁重的工作，正如接下来的部分将讨论的那样。
- en: Data ingestion in real time using structured streaming
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用结构化流处理进行实时数据摄取
- en: Often, businesses need to make tactical decisions in real time along with strategic
    decision-making in order to stay competitive. Therefore, the need to ingest data
    into a data lake arises in real time. However, keeping up with the fast data *Velocity*
    of big data requires a robust and scalable streams processing engine. Apache Spark
    has one such streams processing engine, called **Structured Streaming**, which
    we will explore next.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 企业通常需要在实时做出战术决策的同时进行战略决策，以保持竞争力。因此，实时将数据摄取到数据湖的需求应运而生。然而，跟上大数据的快速数据*速度*需要一个强大且可扩展的流处理引擎。Apache
    Spark 就有这样一个流处理引擎，叫做**结构化流处理**，我们接下来将探讨它。
- en: Structured Streaming primer
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化流处理入门
- en: '**Structured Streaming** is a Spark streams processing engine based on the
    Spark SQL engine. Just like all other components of Spark, Structured Streaming
    is also scalable and fault-tolerant. Since Structured Streaming is based on the
    Spark SQL engine, you can use the same Spark DataFrame API that you have already
    been using for batch processing for streams processing, too. Structured Streaming
    supports all of the functions and constructs supported by the DataFrame API.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**结构化流处理**是一个基于 Spark SQL 引擎的 Spark 流处理引擎。与 Spark 的其他所有组件一样，结构化流处理也具备可扩展性和容错性。由于结构化流处理基于
    Spark SQL 引擎，你可以使用与批处理一样的 Spark DataFrame API 来进行流处理。结构化流处理支持 DataFrame API 所支持的所有函数和构造。'
- en: Structured Streaming treats each incoming stream of data just like tiny a batch
    of data, called a *micro-batch,* and keeps appending each micro-batch to the target
    dataset. Structured Streaming's programming model continuously processes micro-batches,
    treating each micro-batch just like a batch job. So, an existing Spark batch job
    can be easily converted into a streaming job with a few minor changes. Structured
    Streaming is designed to provide maximum throughput, which means that a Structured
    Streaming job can scale out to multiple nodes on a cluster and process very large
    amounts of incoming data in a distributed fashion.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理将每个传入的数据流视为一批小数据，称为*微批次（micro-batch）*，并不断将每个微批次附加到目标数据集。结构化流处理的编程模型持续处理微批次，将每个微批次视为一个批处理作业。因此，现有的Spark批处理作业可以通过少量的修改轻松转换为流处理作业。结构化流处理旨在提供最大吞吐量，这意味着结构化流处理作业可以扩展到集群中的多个节点，并以分布式方式处理大量传入数据。
- en: Structured Streaming comes with additional fault tolerance to failures and guarantees
    exactly once semantics. To achieve this, Structured Streaming keeps track of the
    data processing progress. It keeps track of the offsets or events processed at
    any point in time using concepts such as **checkpointing** and **write-ahead logs**.
    Write-ahead logging is a concept from relational databases and is used to provide
    atomicity and durability to databases. In this technique, records are first written
    to the log before any changes are written to the final database. Checkpointing
    is another technique in Structured Streaming, where the position of the current
    offset being read is recorded on a persistent storage system.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理还具备额外的故障容忍能力，保证精确一次语义（exactly-once semantics）。为了实现这一点，结构化流处理跟踪数据处理进度。它通过**检查点（checkpointing）**和**预写日志（write-ahead
    logs）**等概念，跟踪任何时刻处理的偏移量或事件。预写日志是关系型数据库中的一个概念，用来保证数据库的原子性和持久性。在此技术中，记录会先写入日志，然后再写入最终的数据库。检查点是结构化流处理中另一种技术，它将当前读取的偏移量位置记录在持久化存储系统中。
- en: Together, these techniques enable Structured Streaming to keep a record of the
    position of the last offset processed within the stream, giving it the ability
    to resume processing the stream exactly where it left off, just in case the streaming
    job fails.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些技术，结构化流处理能够记录流中最后一个处理过的偏移量的位置，使其具备在流处理作业失败时从中断点恢复处理的能力。
- en: Note
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We recommended that checkpoints are stored in persistent storage with high availability
    and partition tolerance support, such as a cloud-based data lake.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议将检查点存储在具有高可用性和分区容忍支持的持久化存储中，例如基于云的数据湖。
- en: These techniques of checkpointing, write-ahead logs, and repayable streaming
    data sources, along with streaming data sinks that support the reprocessing of
    data, enable Structured Streaming to guarantee that every event of the stream
    is processed exactly once.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术（检查点、预写日志和可重放的流式数据源）以及支持重新处理数据的流式数据接收器，使得结构化流处理能够保证每个流事件都被处理一次且仅处理一次。
- en: Note
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Structured Streaming's micro-batch processing model is not suitable for processing
    an event as soon as it occurs at the source. There are other streams processing
    engines such as Apache Flink or Kafka Streams that are more suitable for ultra-low
    latency streams processing.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理的微批处理模型不适合处理源端事件发生时立即进行处理。像Apache Flink或Kafka Streams这样的其他流处理引擎，更适合超低延迟的流处理。
- en: Loading data incrementally
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增量加载数据
- en: Since Structured Streaming has built-in mechanisms to help you to easily maintain
    the state information that is required for an incremental load, you can simply
    choose Structured Streaming for all your incremental loads and really simplify
    your architectural complexity. Let's build a pipeline to perform incremental loads
    in a real-time streaming fashion.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于结构化流处理（Structured Streaming）内置了机制，帮助你轻松维护增量加载所需的状态信息，因此你可以简单地选择结构化流处理来处理所有增量加载，真正简化你的架构复杂性。让我们构建一个管道，以实时流式方式执行增量加载。
- en: Typically, our data ingestion starts with data already loaded onto a data source
    such as a data lake or a message queue such as Kafka. Here, we, first, need to
    load some data into a Kafka topic. You can start with an existing Kafka cluster
    with some data already in a topic, or you can set up a quick Kafka server and
    load the *Online Retail* dataset using the code provided at [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/blob/main/Chapter02/utils/kafka-setup.ipynb](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/blob/main/Chapter02/utils/kafka-setup.ipynb).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们的数据摄取从已经加载到数据源的数据开始，例如数据湖或像 Kafka 这样的消息队列。在这里，我们首先需要将一些数据加载到 Kafka 主题中。你可以从一个已经在主题中包含数据的现有
    Kafka 集群开始，或者你可以设置一个快速的 Kafka 服务器，并使用[https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/blob/main/Chapter02/utils/kafka-setup.ipynb](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/blob/main/Chapter02/utils/kafka-setup.ipynb)中提供的代码加载*在线零售*数据集。
- en: 'Let''s take a look at how to perform real-time data ingestion from Kafka into
    a data lake using Structured Streaming, as shown in the following snippets of
    code:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何使用结构化流处理从 Kafka 实时摄取数据到数据湖，以下是相关的代码片段：
- en: '[PRE14]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding block of code, we declare all the columns that we intend to
    read from a Kafka event along with their data types. Structured Streaming requires
    that the data schema be declared upfront. Once the schema has been defined, we
    can start reading data from a Kafka topic and load it into a Spark DataFrame,
    as shown in the following block of code:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们声明了所有我们打算从 Kafka 事件中读取的列及其数据类型。结构化流处理要求数据模式必须提前声明。一旦定义了模式，我们就可以开始从
    Kafka 主题中读取数据，并将其加载到 Spark DataFrame 中，如以下代码块所示：
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding code block, we start reading a stream of events from a Kafka
    topic, called `retail_events`, and tell Kafka that we want to start loading the
    events from the beginning of the stream using the `startingOffsets` option. The
    events in a Kafka topic follow a key-value pattern. This means that our actual
    data is encoded within a JSON object in the `value` column that we need to extract,
    as shown in the following code block:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们开始从一个 Kafka 主题 `retail_events` 中读取事件流，并告知 Kafka 我们希望从流的开始处加载事件，使用
    `startingOffsets` 选项。Kafka 主题中的事件遵循键值对模式。这意味着我们的实际数据被编码在 `value` 列中的 JSON 对象内，我们需要提取这些数据，如下代码块所示：
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the preceding code block, we extract the data using the `from_json()` function
    by passing in the data schema object that we defined earlier. This results in
    a `retail_df` DataFrame that has all of the columns of the event that we require.
    Additionally, we append an `EventTime` column from the Kafka topic, which shows
    when the event actually arrived in Kafka. This could be of some use later, during
    further data processing. Since this DataFrame was created using the `readStream()`
    function, Spark inherently knows this is a Streaming DataFrame and makes Structured
    Streaming APIs available to this DataFrame.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们通过传递之前定义的数据模式对象，使用 `from_json()` 函数提取数据。这样会得到一个 `retail_df` DataFrame，其中包含我们需要的事件所有列。此外，我们从
    Kafka 主题中附加了一个 `EventTime` 列，它显示了事件实际到达 Kafka 的时间。这个信息在之后的进一步数据处理过程中可能会有所帮助。由于这个
    DataFrame 是通过 `readStream()` 函数创建的，Spark 本身就知道这是一个流式 DataFrame，并为该 DataFrame 提供了结构化流处理
    API。
- en: 'Once we have extracted the raw event data from the Kafka stream, we can persist
    it to the data lake, as shown in the following block of code:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从 Kafka 流中提取了原始事件数据，就可以将其持久化到数据湖中，如以下代码块所示：
- en: '[PRE17]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the preceding code block, we make use of the `writeStream()` function that
    is available to Streaming DataFrames to save data to the data lake in a streaming
    fashion. Here, we write data in Parquet format, and the resultant data on the
    data lake will be a set of `.parquet` files. Once saved, these Parquet files are
    no different from any other Parquet files, whether created by batch processing
    or streams processing.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们利用了 `writeStream()` 函数，这是流式 DataFrame 提供的功能，可以以流式方式将数据保存到数据湖中。在这里，我们以
    Parquet 格式写入数据，结果数据湖中的数据将是一组 `.parquet` 文件。保存后，这些 Parquet 文件与任何其他由批处理或流处理创建的 Parquet
    文件没有区别。
- en: Additionally, we use `outputMode` as `append` to indicate that we will treat
    this as an unbounded dataset and will keep appending new Parquet files. The `checkpointLocation`
    option stores the Structured Streaming write-ahead log and other checkpointing
    information. This makes it an incremental data load job as the stream only picks
    up new and unprocessed events based on the offset information stored at the checkpoint
    location.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将`outputMode`设置为`append`，以表明我们将其视为一个无界数据集，并将继续追加新的Parquet文件。`checkpointLocation`选项存储结构化流处理的写前日志及其他检查点信息。这使其成为一个增量数据加载作业，因为流处理仅基于存储在检查点位置的偏移信息处理新的和未处理的事件。
- en: Note
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Structured Streaming supports `complete` and `update` modes in addition to `append`
    mode. A description of these modes and when to use them can be found in Apache
    Spark's official documentation at [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理支持`complete`和`update`模式，除了`append`模式外。关于这些模式的描述以及何时使用它们，可以在Apache Spark的官方文档中找到，网址为[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes)。
- en: But what if you need to run the incremental data load job as a less frequent
    batch process instead of running it in a continuous streaming manner?
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 那如果你需要将增量数据加载作业作为较少频繁的批处理作业运行，而不是以持续流处理方式运行呢？
- en: Well, Structured Streaming supports this too via the `trigger` option. We can
    use `once=True` for this option, and the streaming job will process all new and
    unprocessed events when the job is externally triggered and then stop the stream
    when there are no new events to be processed. We can schedule this job to run
    periodically based on a time interval and it will just behave like a batch job
    but with all the benefits of an incremental load.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理也支持这种情况，方法是通过`trigger`选项。我们可以将`once=True`用于该选项，流处理作业将在外部触发时处理所有新的和未处理的事件，然后在没有新的事件需要处理时停止流处理。我们可以根据时间间隔安排该作业定期运行，它的行为就像一个批处理作业，但具备增量加载的所有优势。
- en: In summary, the Spark SQL engine's DataFrame API is both powerful and easy to
    use for batch data processing and streams processing. There are slight differences
    between the functions and utilities provided between a static DataFrame and streaming
    DataFrame. However, for the most part, the programming models between batch processing
    and streams processing that use DataFrames are very similar. This minimizes the
    learning curve and helps to unify batch and streams processing using Apache Spark's
    unified analytics engine.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Spark SQL引擎的DataFrame API在批量数据处理和流处理方面都非常强大且易于使用。静态DataFrame和流式DataFrame在功能和工具方面有一些微小的差别。然而，在大多数情况下，使用DataFrame的批处理和流处理编程模型非常相似。这减少了学习曲线，有助于使用Apache
    Spark的统一分析引擎来统一批处理和流处理。
- en: Now, in the next section, let's examine how to implement a unified data processing
    architecture with Apache Spark using a concept called **Lambda Architecture**.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在下一节中，让我们探讨如何使用Apache Spark实现一个统一的数据处理架构，采用的概念就是**Lambda架构**。
- en: Unifying batch and real time using Lambda Architecture
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Lambda架构统一批量数据和实时数据
- en: Both batch and real-time data processing are important elements of any modern
    Enterprise DSS, and an architecture that seamlessly implements both these data
    processing techniques can help increase throughput, minimize latency, and allow
    you to get to fresh data much more quickly. One such architecture is called **Lambda
    Architecture**, which we will examine next.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 批量数据处理和实时数据处理是现代企业决策支持系统（DSS）中的重要组成部分，能够无缝实现这两种数据处理技术的架构有助于提高吞吐量、减少延迟，并使您能够更快速地获得最新数据。这样的架构被称为**Lambda架构**，我们接下来将详细探讨。
- en: Lambda Architecture
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lambda架构
- en: 'Lambda Architecture is a data processing technique that is used to ingest,
    process, and query both historical and real-time data with a single architecture.
    Here, the goal is to increase throughput, data freshness, and fault tolerance
    while maintaining a single view of both historical and real-time data for end
    users. The following diagram illustrates a typical Lambda Architecture:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda架构是一种数据处理技术，用于以单一架构摄取、处理和查询历史数据与实时数据。在这里，目标是提高吞吐量、数据新鲜度和容错性，同时为最终用户提供历史数据和实时数据的统一视图。以下图示展示了一个典型的Lambda架构：
- en: '![Figure 2.3 – Lambda Architecture'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.3 – Lambda架构'
- en: '](img/B16736_02_03.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_02_03.jpg)'
- en: Figure 2.3 – Lambda Architecture
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – Lambda 架构
- en: As shown in the preceding diagram, a Lambda Architecture consists of three main
    components, namely, the **Batch Layer**, the **Speed Layer**, and the **Serving
    Layer**. We will discuss each of these layers in the following sections.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，Lambda 架构由三个主要组件组成，即 **批处理层**、**速度层** 和 **服务层**。我们将在接下来的章节中讨论这几个层。
- en: The Batch layer
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理层
- en: The Batch layer is like any typical ETL layer involving the batch processing
    of data from the source system. This layer usually involves scheduled jobs that
    run periodically, typically, at night.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理层就像任何典型的 ETL 层，涉及从源系统批量处理数据。这个层通常涉及定期运行的调度作业，通常在晚上进行。
- en: Apache Spark can be used to build batch processing jobs or Structured Streaming
    jobs that get triggered on a schedule, and it can also be used for the batch layer
    to build historical data in the data lake.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 可用于构建批处理作业或按计划触发的结构化流式作业，也可用于批处理层构建数据湖中的历史数据。
- en: The Speed layer
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速度层
- en: The Speed layer continuously ingests data from the same data source as the Batch
    layer from the data lake into real-time views. The Speed layer continuously delivers
    the latest data that the Batch layer cannot provide yet due to its inherent latency.
    Spark Structured Steaming can be used to implement low latency streaming jobs
    to continuously ingest the latest data from the source system.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 速度层持续从数据湖中与批处理层相同的数据源中摄取数据，生成实时视图。速度层不断提供批处理层尚未提供的最新数据，这是由于批处理层固有的延迟。Spark Structured
    Streaming 可用于实现低延迟的流式作业，持续从源系统中摄取最新数据。
- en: The Serving layer
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务层
- en: The Serving layer combines the historical data from the Batch layer and the
    latest data from the Speed layer into a single view to support ad hoc queries
    by end users. Spark SQL makes a good candidate for the Serving layer as it can
    help users query historical data from the Batch layer, as well as the latest data
    from the Speed layer, and present the user with a unified view of data for low-latency,
    ad hoc queries.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 服务层将批处理层中的历史数据和速度层中的最新数据合并为一个视图，以支持终端用户的临时查询。Spark SQL 是服务层的一个优秀候选，它可以帮助用户查询批处理层中的历史数据以及速度层中的最新数据，并为用户呈现一个统一的数据视图，以便进行低延迟的临时查询。
- en: 'In the previous sections, you implemented data ingestion jobs for batch as
    well as streaming using Apache Spark. Now, let''s explore how you can combine
    the two views to give users a single unified view, as shown in the following code
    snippet:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你实现了批处理和流式处理的数据摄取作业。现在，让我们探讨如何将这两种视图结合起来，向用户提供一个统一的视图，如下所示的代码片段所示：
- en: '[PRE18]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding code block, we create two DataFrames, one from the `union`
    function to unite these two DataFrames and then create a **Spark Global Temp View**
    using the combined DataFrame. The result is a view that is accessible across all
    **Spark Sessions** across the cluster, which gives you a unified view of data
    across both the batch and speed layers, as shown in the following line of code:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们创建了两个 DataFrame，一个是通过`union`函数将这两个 DataFrame 合并，然后使用合并后的 DataFrame
    创建一个 **Spark Global Temp View**。结果是一个可以在集群中所有 **Spark Sessions** 中访问的视图，它为你提供了跨批处理层和速度层的数据统一视图，如下代码所示：
- en: '[PRE19]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The preceding line of code is a SQL query that queries data from the Spark global
    view, which acts as the **Serving Layer** and can be presented to end users for
    ad hoc queries across both the latest data and historical data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码行是一个 SQL 查询，它查询来自 Spark 全局视图的数据，Spark 全局视图充当 **服务层**，并可以呈现给终端用户，以便跨最新数据和历史数据进行临时查询。
- en: In this way, you can make use of Apache Spark's SQL engine's DataFrame, Structured
    Streaming, and SQL APIs to build a Lambda Architecture that improves data freshness,
    throughput, and provides a unified view of data. However, the Lambda Architecture
    is somewhat complex to maintain because there are two separate data ingestion
    pipelines for batch and real-time processing along with two separate data sinks.
    Indeed, there is an easier way to unify the batch and speed layers using an open
    source storage layer called **Delta Lake**. You will learn about this in [*Chapter
    3*](B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056), *Data Cleansing and Integration*.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，您可以利用Apache Spark SQL引擎的DataFrame、结构化流处理和SQL API来构建一个Lambda架构，从而提高数据的新鲜度、吞吐量，并提供统一的数据视图。然而，Lambda架构的维护较为复杂，因为它有两个独立的数据导入管道，分别用于批处理和实时处理，并且有两个独立的数据接收端。实际上，有一种更简单的方式可以使用开源存储层Delta
    Lake来统一批处理和实时层。您将在[*第3章*](B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056)，*数据清洗与整合*中学习到这一点。
- en: Summary
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about Enterprise DSS in the context of big data
    analytics and its components. You learned about various types of data sources
    such as RDBMS-based operational systems, message queues, and file sources, and
    data sinks, such as data warehouses and data lakes, and their relative merits.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了在大数据分析背景下的企业决策支持系统（DSS）及其组成部分。您学习了各种类型的数据源，如基于RDBMS的操作系统、消息队列、文件源，以及数据接收端，如数据仓库和数据湖，并了解了它们的相对优缺点。
- en: Additionally, you explored different types of data storage formats such as unstructured,
    structured, and semistructured and learned about the benefits of using structured
    formats such as Apache Parquet with Spark. You were introduced to data ingestion
    in a batch and real-time manner and learned how to implement them using Spark
    DataFrame APIs. We also introduced Spark's Structured Streaming framework for
    real-time streams processing, and you learned how to use Structured Streaming
    to implement incremental data loads using minimal programming overheads. Finally,
    you explored the Lambda Architecture to unify batch and real-time data processing
    and its implementation using Apache Spark. The skills learned in this chapter
    will help you to implement scalable and performant distributed data ingestion
    pipelines via Apache Spark using both batch and streams processing models.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还探索了不同类型的数据存储格式，如非结构化、结构化和半结构化数据，并了解了使用结构化格式（如Apache Parquet与Spark）带来的好处。您还了解了数据批量导入和实时导入的方式，并学习了如何使用Spark
    DataFrame API来实现它们。我们还介绍了Spark的结构化流处理框架，用于实时流数据处理，您学习了如何使用结构化流处理来实现增量数据加载，同时减少编程负担。最后，您探索了Lambda架构，将批处理和实时数据处理进行统一，并了解了如何使用Apache
    Spark来实现它。您在本章中学到的技能将帮助您通过Apache Spark实施可扩展且高性能的分布式数据导入管道，支持批处理和流处理模式。
- en: In the next chapter, you will learn about the techniques to process, cleanse,
    and integrate the raw data that was ingested into a data lake in this chapter
    into clean, consolidated, and meaningful datasets that are ready for end users
    to perform business analytics on and generate meaningful insights.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习如何处理、清洗和整合在本章中导入数据湖的原始数据，将其转化为干净、整合且有意义的数据集，供最终用户进行业务分析并生成有价值的洞察。
