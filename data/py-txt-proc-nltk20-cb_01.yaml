- en: Chapter 1. Tokenizing Text and WordNet Basics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章. 分词文本和WordNet基础知识
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将介绍：
- en: Tokenizing text into sentences
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本分词成句子
- en: Tokenizing sentences into words
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将句子分词成单词
- en: Tokenizing sentences using regular expressions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则表达式进行句子分词
- en: Filtering stopwords in a tokenized sentence
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分词后的句子中过滤停用词
- en: Looking up synsets for a word in WordNet
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在WordNet中查找一个单词的synset
- en: Looking up lemmas and synonyms in WordNet
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在WordNet中查找词元和同义词
- en: Calculating WordNet synset similarity
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算WordNet synset相似度
- en: Discovering word collocations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现词组
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: '**NLTK** is the **Natural Language Toolkit**, a comprehensive Python library
    for natural language processing and text analytics. Originally designed for teaching,
    it has been adopted in the industry for research and development due to its usefulness
    and breadth of coverage.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**NLTK**是**自然语言工具包**，是一个用于自然语言处理和文本分析的综合性Python库。最初是为教学设计的，由于其有用性和广泛的应用范围，它已被工业界用于研究和开发。'
- en: This chapter will cover the basics of tokenizing text and using WordNet. **Tokenization**
    is a method of breaking up a piece of text into many pieces, and is an essential
    first step for recipes in later chapters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍分词文本和使用WordNet的基础知识。**分词**是将一段文本分解成许多片段的方法，是后续章节中食谱的必要第一步。
- en: '**WordNet** is a dictionary designed for programmatic access by natural language
    processing systems. NLTK includes a WordNet corpus reader, which we will use to
    access and explore WordNet. We''ll be using WordNet again in later chapters, so
    it''s important to familiarize yourself with the basics first.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**WordNet**是为自然语言处理系统程序化访问而设计的词典。NLTK包括一个WordNet语料库读取器，我们将使用它来访问和探索WordNet。我们将在后续章节中再次使用WordNet，因此首先熟悉其基础知识很重要。'
- en: Tokenizing text into sentences
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本分词成句子
- en: Tokenization is the process of splitting a string into a list of pieces, or
    *tokens*. We'll start by splitting a paragraph into a list of sentences.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是将字符串分割成一系列片段或*标记*的过程。我们将从将一个段落分割成句子列表开始。
- en: Getting ready
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Installation instructions for NLTK are available at [http://www.nltk.org/download](http://www.nltk.org/download)
    and the latest version as of this writing is 2.0b9\. NLTK requires Python 2.4
    or higher, but is **not compatible with Python 3.0**. The **recommended Python
    version is 2.6**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK的安装说明可在[http://www.nltk.org/download](http://www.nltk.org/download)找到，截至本文写作的最新版本是2.0b9。NLTK需要Python
    2.4或更高版本，但**不兼容Python 3.0**。**推荐的Python版本是2.6**。
- en: Once you've installed NLTK, you'll also need to install the data by following
    the instructions at [http://www.nltk.org/data](http://www.nltk.org/data). We recommend
    installing everything, as we'll be using a number of corpora and pickled objects.
    The data is installed in a data directory, which on Mac and Linux/Unix is usually
    `/usr/share/nltk_data`, or on Windows is `C:\nltk_data`. Make sure that `tokenizers/punkt.zip`
    is in the data directory and has been unpacked so that there's a file at `tokenizers/punkt/english.pickle`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了NLTK，你还需要按照[http://www.nltk.org/data](http://www.nltk.org/data)上的说明安装数据。我们建议安装所有内容，因为我们将在后续章节中使用多个语料库和pickle对象。数据安装在数据目录中，在Mac和Linux/Unix系统中通常是`/usr/share/nltk_data`，在Windows系统中是`C:\nltk_data`。请确保`tokenizers/punkt.zip`在数据目录中，并且已经解压，以便在`tokenizers/punkt/english.pickle`中有一个文件。
- en: Finally, to run the code examples, you'll need to start a Python console. Instructions
    on how to do so are available at [http://www.nltk.org/getting-started](http://www.nltk.org/getting-started).
    For Mac with Linux/Unix users, you can open a terminal and type **python**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了运行代码示例，你需要启动一个Python控制台。有关如何操作的说明可在[http://www.nltk.org/getting-started](http://www.nltk.org/getting-started)找到。对于Mac和Linux/Unix用户，你可以打开一个终端并输入**python**。
- en: How to do it...
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Once NLTK is installed and you have a Python console running, we can start
    by creating a paragraph of text:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了NLTK并且你有一个Python控制台正在运行，我们可以从创建一段文本开始：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we want to split `para` into sentences. First we need to import the sentence
    tokenization function, and then we can call it with the paragraph as an argument.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想将`para`分割成句子。首先我们需要导入句子分词函数，然后我们可以用段落作为参数调用它。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So now we have a list of sentences that we can use for further processing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在有一系列句子可以用于进一步处理。
- en: How it works...
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: '`sent_tokenize` uses an instance of `PunktSentenceTokenizer` from the `nltk.tokenize.punkt`
    module. This instance has already been trained on and works well for many European
    languages. So it knows what punctuation and characters mark the end of a sentence
    and the beginning of a new sentence.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`sent_tokenize` 使用来自 `nltk.tokenize.punkt` 模块的 `PunktSentenceTokenizer` 实例。这个实例已经在许多欧洲语言上进行了训练，并且效果良好。因此，它知道哪些标点符号和字符标志着句子的结束和新一行的开始。'
- en: There's more...
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The instance used in `sent_tokenize()` is actually loaded on demand from a pickle
    file. So if you're going to be tokenizing a lot of sentences, it's more efficient
    to load the `PunktSentenceTokenizer` once, and call its `tokenize()` method instead.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`sent_tokenize()` 中使用的实例实际上是从 pickle 文件中按需加载的。所以如果你要分词大量句子，一次性加载 `PunktSentenceTokenizer`
    并调用其 `tokenize()` 方法会更有效率。'
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Other languages
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他语言
- en: 'If you want to tokenize sentences in languages other than English, you can
    load one of the other pickle files in `tokenizers/punkt` and use it just like
    the English sentence tokenizer. Here''s an example for Spanish:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要对非英语语言的句子进行分词，你可以加载 `tokenizers/punkt` 中的其他 pickle 文件，并像使用英语句子分词器一样使用它。以下是一个西班牙语的例子：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: See also
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the next recipe, we'll learn how to split sentences into individual words.
    After that, we'll cover how to use regular expressions for tokenizing text.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个菜谱中，我们将学习如何将句子分割成单个单词。之后，我们将介绍如何使用正则表达式进行文本分词。
- en: Tokenizing sentences into words
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将句子分词成单词
- en: In this recipe, we'll split a sentence into individual words. The simple task
    of creating a list of words from a string is an essential part of all text processing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将一个句子分割成单个单词。从字符串中创建单词列表的简单任务是所有文本处理的基本部分。
- en: How to do it...
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Basic word tokenization is very simple: use the `word_tokenize()` function:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基本单词分词非常简单：使用 `word_tokenize()` 函数：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works...
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`word_tokenize()` is a wrapper function that calls `tokenize()` on an instance
    of the `TreebankWordTokenizer` . It''s equivalent to the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`word_tokenize()` 是一个包装函数，它在一个 `TreebankWordTokenizer` 实例上调用 `tokenize()`。它等同于以下代码：'
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It works by separating words using spaces and punctuation. And as you can see,
    it does not discard the punctuation, allowing you to decide what to do with it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过使用空格和标点符号来分隔单词。正如你所见，它不会丢弃标点符号，这让你可以决定如何处理它们。
- en: There's more...
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Ignoring the obviously named `WhitespaceTokenizer` and `SpaceTokenizer` , there
    are two other word tokenizers worth looking at: `PunktWordTokenizer` and `WordPunctTokenizer`.
    These differ from the `TreebankWordTokenizer` by how they handle punctuation and
    contractions, but they all inherit from `TokenizerI`. The inheritance tree looks
    like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略显然命名的 `WhitespaceTokenizer` 和 `SpaceTokenizer`，还有两个其他值得关注的单词分词器：`PunktWordTokenizer`
    和 `WordPunctTokenizer`。它们与 `TreebankWordTokenizer` 的不同之处在于它们处理标点和缩写的方式，但它们都继承自
    `TokenizerI`。继承关系如下：
- en: '![There''s more...](img/3609OS_01_01.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多...](img/3609OS_01_01.jpg)'
- en: Contractions
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缩写
- en: '`TreebankWordTokenizer` uses conventions found in the Penn Treebank corpus,
    which we''ll be using for training in [Chapter 4](ch04.html "Chapter 4. Part-of-Speech
    Tagging"), *Part-of-Speech Tagging* and [Chapter 5](ch05.html "Chapter 5. Extracting
    Chunks"), *Extracting Chunks*. One of these conventions is to separate contractions.
    For example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`TreebankWordTokenizer` 使用在宾州树库语料库中找到的约定，我们将在第 4 章（[ch04.html](ch04.html "Chapter 4. Part-of-Speech
    Tagging")）*词性标注*和第 5 章（[ch05.html](ch05.html "Chapter 5. Extracting Chunks")）*提取词组*中用于训练。这些约定之一是分隔缩写。例如：'
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you find this convention unacceptable, then read on for alternatives, and
    see the next recipe for tokenizing with regular expressions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这个约定不可接受，那么请继续阅读以了解替代方案，并查看下一个菜谱，了解如何使用正则表达式进行分词。
- en: PunktWordTokenizer
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PunktWordTokenizer
- en: An alternative word tokenizer is the `PunktWordTokenizer`. It splits on punctuation,
    but keeps it with the word instead of creating separate tokens.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可选的单词分词器是 `PunktWordTokenizer`。它会在标点符号处分割，但会将标点符号与单词一起保留，而不是创建单独的标记。
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: WordPunctTokenizer
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: WordPunctTokenizer
- en: Another alternative word tokenizer is `WordPunctTokenizer`. It splits all punctuations
    into separate tokens.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可选的单词分词器是 `WordPunctTokenizer`。它将所有标点符号分割成单独的标记。
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: See also
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: For more control over word tokenization, you'll want to read the next recipe
    to learn how to use regular expressions and the `RegexpTokenizer` for tokenization.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地控制单词分词，你可能需要阅读下一个菜谱，了解如何使用正则表达式和 `RegexpTokenizer` 进行分词。
- en: Tokenizing sentences using regular expressions
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则表达式进行句子分词
- en: Regular expression can be used if you want complete control over how to tokenize
    text. As regular expressions can get complicated very quickly, we only recommend
    using them if the word tokenizers covered in the previous recipe are unacceptable.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要完全控制如何标记化文本，可以使用正则表达式。由于正则表达式可能会很快变得复杂，我们只建议在之前的配方中提到的单词标记化器不可接受时使用它们。
- en: Getting ready
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'First you need to decide how you want to tokenize a piece of text, as this
    will determine how you construct your regular expression. The choices are:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要决定你想要如何标记化一段文本，因为这将决定你如何构建你的正则表达式。选择包括：
- en: Match on the tokens
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 匹配标记
- en: Match on the separators, or gaps
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 匹配分隔符，或间隔
- en: We'll start with an example of the first, matching alphanumeric tokens plus
    single quotes so that we don't split up contractions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从第一个示例开始，匹配字母数字标记和单引号，这样我们就不需要分割缩略语。
- en: How to do it...
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: We'll create an instance of the `RegexpTokenizer`, giving it a regular expression
    string to use for matching tokens.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个`RegexpTokenizer`的实例，给它一个用于匹配标记的正则表达式字符串。
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There's also a simple helper function you can use in case you don't want to
    instantiate the class.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想实例化类，你也可以使用一个简单的辅助函数。
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now we finally have something that can treat contractions as whole words, instead
    of splitting them into tokens.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于有一种可以处理缩略语作为整个单词的方法，而不是将它们分割成标记。
- en: How it works...
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The `RegexpTokenizer` works by compiling your pattern, then calling `re.findall()`
    on your text. You could do all this yourself using the `re` module, but the `RegexpTokenizer`
    implements the `TokenizerI` interface, just like all the word tokenizers from
    the previous recipe. This means it can be used by other parts of the NLTK package,
    such as corpus readers, which we'll cover in detail in [Chapter 3](ch03.html "Chapter 3. Creating
    Custom Corpora"), *Creating Custom Corpora*. Many corpus readers need a way to
    tokenize the text they're reading, and can take optional keyword arguments specifying
    an instance of a `TokenizerI` subclass. This way, you have the ability to provide
    your own tokenizer instance if the default tokenizer is unsuitable.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`RegexpTokenizer`通过编译你的模式，然后在你的文本上调用`re.findall()`来工作。你可以使用`re`模块自己完成所有这些操作，但`RegexpTokenizer`实现了`TokenizerI`接口，就像之前配方中的所有单词标记化器一样。这意味着它可以被NLTK包的其他部分使用，例如语料库读取器，我们将在第3章[创建自定义语料库](ch03.html
    "第3章。创建自定义语料库")中详细讨论。许多语料库读取器需要一个方法来标记化他们正在读取的文本，并且可以接受可选的关键字参数来指定一个`TokenizerI`子类的实例。这样，如果你觉得默认标记化器不合适，你可以提供自己的标记化器实例。'
- en: There's more...
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多...
- en: '`RegexpTokenizer` can also work by matching the gaps, instead of the tokens.
    Instead of using `re.findall()`, the `RegexpTokenizer` will use `re.split()`.
    This is how the `BlanklineTokenizer` in `nltk.tokenize` is implemented.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`RegexpTokenizer`也可以通过匹配间隔来工作，而不是匹配标记。它不会使用`re.findall()`，而是使用`re.split()`。这就是`nltk.tokenize`中的`BlanklineTokenizer`是如何实现的。'
- en: Simple whitespace tokenizer
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简单空白标记化器
- en: 'Here''s a simple example of using the `RegexpTokenizer` to tokenize on whitespace:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用`RegexpTokenizer`在空白处进行标记化的简单示例：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Notice that punctuation still remains in the tokens.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，标点符号仍然保留在标记中。
- en: See also
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: For simpler word tokenization, see the previous recipe.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更简单的单词标记化，请参阅之前的配方。
- en: Filtering stopwords in a tokenized sentence
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在标记化句子中过滤停用词
- en: '**Stopwords** are common words that generally do not contribute to the meaning
    of a sentence, at least for the purposes of information retrieval and natural
    language processing. Most search engines will filter stopwords out of search queries
    and documents in order to save space in their index.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**停用词**是通常不贡献于句子意义的常见单词，至少对于信息检索和自然语言处理的目的来说是这样。大多数搜索引擎都会从搜索查询和文档中过滤掉停用词，以节省索引空间。'
- en: Getting ready
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: NLTK comes with a stopwords corpus that contains word lists for many languages.
    Be sure to unzip the datafile so NLTK can find these word lists in `nltk_data/corpora/stopwords/`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK附带了一个包含许多语言单词列表的停用词语料库。请确保解压缩数据文件，以便NLTK可以在`nltk_data/corpora/stopwords/`中找到这些单词列表。
- en: How to do it...
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: We're going to create a set of all English stopwords, then use it to filter
    stopwords from a sentence.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个包含所有英语停用词的集合，然后使用它来过滤句子中的停用词。
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How it works...
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The stopwords corpus is an instance of `nltk.corpus.reader.WordListCorpusReader`.
    As such, it has a `words()` method that can take a single argument for the file
    ID, which in this case is `'english'`, referring to a file containing a list of
    English stopwords. You could also call `stopwords.words()` with no argument to
    get a list of all stopwords in every language available.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词语料库是 `nltk.corpus.reader.WordListCorpusReader` 的一个实例。因此，它有一个 `words()` 方法，可以接受单个参数作为文件
    ID，在这种情况下是 `'english'`，指的是包含英语停用词列表的文件。您也可以不带参数调用 `stopwords.words()`，以获取所有可用的语言的停用词列表。
- en: There's more...
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You can see the list of all English stopwords using `stopwords.words(''english'')`
    or by examining the word list file at `nltk_data/corpora/stopwords/english`. There
    are also stopword lists for many other languages. You can see the complete list
    of languages using the `fileids()` method:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `stopwords.words('english')` 或通过检查 `nltk_data/corpora/stopwords/english`
    中的单词列表文件来查看所有英语停用词。还有许多其他语言的停用词列表。您可以使用 `fileids()` 方法查看完整的语言列表：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Any of these `fileids` can be used as an argument to the `words()` method to
    get a list of stopwords for that language.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 `fileids` 中的任何一个都可以用作 `words()` 方法的参数，以获取该语言的停用词列表。
- en: See also
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: If you'd like to create your own stopwords corpus, see the *Creating a word
    list corpus* recipe in [Chapter 3](ch03.html "Chapter 3. Creating Custom Corpora"),
    *Creating Custom Corpora*, to learn how to use the `WordListCorpusReader`. We'll
    also be using stopwords in the *Discovering word collocations* recipe, later in
    this chapter.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想创建自己的停用词语料库，请参阅 [第 3 章](ch03.html "第 3 章。创建自定义语料库") 中的 *Creating a word
    list corpus* 菜谱，*Creating Custom Corpora*，了解如何使用 `WordListCorpusReader`。我们还将在此章后面的
    *Discovering word collocations* 菜谱中使用停用词。
- en: Looking up synsets for a word in WordNet
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 WordNet 中查找单词的同义词集
- en: WordNet is a lexical database for the English language. In other words, it's
    a dictionary designed specifically for natural language processing.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: WordNet 是英语的词汇数据库。换句话说，它是一个专门为自然语言处理设计的字典。
- en: NLTK comes with a simple interface for looking up words in WordNet. What you
    get is a list of **synset** instances, which are groupings of synonymous words
    that express the same concept. Many words have only one synset, but some have
    several. We'll now explore a single synset, and in the next recipe, we'll look
    at several in more detail.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 提供了一个简单的接口来查找 WordNet 中的单词。您得到的是 **synset** 实例的列表，这些实例是表达相同概念的同类词的分组。许多单词只有一个同义词集，但有些有几个。我们现在将探索一个同义词集，在下一道菜谱中，我们将更详细地查看几个同义词集。
- en: Getting ready
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Be sure you've unzipped the `wordnet` corpus in `nltk_data/corpora/wordnet`.
    This will allow the `WordNetCorpusReader` to access it.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已将 `wordnet` 语料库解压缩到 `nltk_data/corpora/wordnet`。这将允许 `WordNetCorpusReader`
    访问它。
- en: How to do it...
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Now we're going to lookup the `synset` for `cookbook`, and explore some of the
    properties and methods of a synset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将查找 `cookbook` 的 `synset`，并探索同义词集的一些属性和方法。
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How it works...
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: You can look up any word in WordNet using `wordnet.synsets(word)` to get a list
    of synsets. The list may be empty if the word is not found. The list may also
    have quite a few elements, as some words can have many possible meanings and therefore
    many synsets.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `wordnet.synsets(word)` 在 WordNet 中查找任何单词，以获取同义词集列表。如果找不到单词，列表可能为空。列表也可能包含很多元素，因为一些单词可能有多种可能的含义，因此有多个同义词集。
- en: There's more...
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Each synset in the list has a number of attributes you can use to learn more
    about it. The `name` attribute will give you a unique name for the synset, which
    you can use to get the synset directly.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的每个同义词集都有一些属性，您可以使用这些属性来了解更多关于它的信息。`name` 属性将为您提供同义词集的唯一名称，您可以使用它直接获取同义词集。
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `definition` attribute should be self-explanatory. Some synsets also have
    an `examples` attribute, which contains a list of phrases that use the word in
    context.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`definition` 属性应该是自解释的。一些同义词集（synsets）也具有 `examples` 属性，其中包含使用该词的短语列表。'
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Hypernyms
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上位词
- en: Synsets are organized in a kind of inheritance tree. More abstract terms are
    known as **hypernyms** and more specific terms are **hyponyms** . This tree can
    be traced all the way up to a root hypernym.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 同义词集以某种继承树的形式组织。更抽象的术语称为 **上位词**，更具体的术语称为 **下位词**。这棵树可以追溯到根上位词。
- en: Hypernyms provide a way to categorize and group words based on their similarity
    to each other. The synset similarity recipe details the functions used to calculate
    similarity based on the distance between two words in the hypernym tree.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 上义词提供了一种根据词之间的相似性对词进行分类和分组的方法。上义词树中两个词之间的距离计算的相似度菜谱详细说明了用于计算相似度的函数。
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see, `reference book` is a *hypernym* of `cookbook`, but `cookbook`
    is only one of many *hyponyms* of `reference book`. All these types of books have
    the same root hypernym, `entity`, one of the most abstract terms in the English
    language. You can trace the entire path from `entity` down to `cookbook` using
    the `hypernym_paths()` method.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`参考书`是`食谱`的*上义词*，但`食谱`只是`参考书`众多*下义词*中的一个。所有这些类型的书籍都有相同的根上义词，`实体`，这是英语中最抽象的术语之一。您可以使用`hypernym_paths()`方法从`实体`追踪到`食谱`的整个路径。
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This method returns a list of lists, where each list starts at the root hypernym
    and ends with the original `Synset`. Most of the time you'll only get one nested
    list of synsets.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法返回一个列表的列表，其中每个列表从根上义词开始，以原始`sense`结束。大多数情况下，您只会得到一个嵌套的`sense`列表。
- en: Part-of-speech (POS)
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词性（POS）
- en: You can also look up a simplified part-of-speech tag.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以查找简化的词性标签。
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: There are four common POS found in WordNet.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: WordNet中有四种常见的词性。
- en: '| Part-of-speech | Tag |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 词性 | 标签 |'
- en: '| --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Noun | n |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 名词 | n |'
- en: '| Adjective | a |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 形容词 | a |'
- en: '| Adverb | r |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 副词 | r |'
- en: '| Verb | v |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 动词 | v |'
- en: These POS tags can be used for looking up specific `synsets` for a word. For
    example, the word `great` can be used as a noun or an adjective. In WordNet, `great`
    has one noun synset and six adjective synsets.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些POS标签可用于查找一个词的特定`sense`。例如，单词`great`可以用作名词或形容词。在WordNet中，`great`有一个名词`sense`和六个形容词`sense`。
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: These POS tags will be referenced more in the *Using WordNet for Tagging* recipe
    of [Chapter 4](ch04.html "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech
    Tagging*.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些POS标签将在[第4章](ch04.html "第4章。词性标注")的*使用WordNet进行词性标注*菜谱中更多地进行参考。
- en: See also
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the next two recipes, we'll explore lemmas and how to calculate synset similarity.
    In [Chapter 2](ch02.html "Chapter 2. Replacing and Correcting Words"), *Replacing
    and Correcting Words*, we'll use WordNet for lemmatization, synonym replacement,
    and then explore the use of antonyms.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个菜谱中，我们将探讨词元和如何计算`sense`相似度。在[第2章](ch02.html "第2章。替换和纠正单词")中，*替换和纠正单词*，我们将使用WordNet进行词元化、同义词替换，然后探讨反义词的使用。
- en: Looking up lemmas and synonyms in WordNet
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在WordNet中查找词元和同义词
- en: Building on the previous recipe, we can also look up lemmas in WordNet to find
    **synonyms** of a word. A **lemma** (in linguistics) is the canonical form, or
    morphological form, of a word.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个菜谱的基础上，我们还可以在WordNet中查找词元，以找到一个词的**同义词**。在语言学中，**词元**是一个词的规范形式或形态形式。
- en: How to do it...
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In the following block of code, we''ll find that there are two lemmas for the
    `cookbook synset` by using the `lemmas` attribute:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们将通过使用`lemmas`属性找到`cookbook sense`的两个词元：
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How it works...
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As you can see, `cookery_book` and `cookbook` are two distinct `lemmas` in the
    same `synset`. In fact, a lemma can only belong to a single synset. In this way,
    a synset represents a group of lemmas that all have the same meaning, while a
    lemma represents a distinct word form.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`cookery_book`和`cookbook`是同一`sense`中的两个不同的词元。事实上，一个词元只能属于一个`sense`。这样，一个`sense`代表了一组具有相同意义的词元，而一个词元代表了一个独特的单词形式。
- en: There's more...
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Since lemmas in a synset all have the same meaning, they can be treated as
    synonyms. So if you wanted to get all synonyms for a `synset`, you could do:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一个`sense`中的词元都具有相同的意义，因此它们可以被视为同义词。所以如果您想获取一个`sense`的所有同义词，您可以这样做：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: All possible synonyms
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 所有可能的同义词
- en: As mentioned before, many words have multiple `synsets` because the word can
    have different meanings depending on the context. But let's say you didn't care
    about the context, and wanted to get all possible synonyms for a word.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，许多词有多个`sense`，因为这个词可以根据上下文有不同的含义。但假设您不关心上下文，只想为一个词找到所有可能的同义词。
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see, there appears to be 38 possible synonyms for the word `book`.
    But in fact, some are verb forms, and many are just different usages of `book`.
    Instead, if we take the set of synonyms, there are fewer unique words.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，似乎有38个可能的同义词用于单词`book`。但实际上，有些是动词形式，许多只是`book`的不同用法。相反，如果我们取同义词集，那么独特的单词就少多了。
- en: '[PRE24]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Antonyms
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反义词
- en: Some lemmas also have **antonyms**. The word `good`, for example, has 27 `synsets`,
    five of which have `lemmas` with antonyms.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一些词元也有 **反义词**。例如，单词 `good` 有 27 个 `synset`，其中 5 个有带反义词的 `lemmas`。
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `antonyms()` method returns a list of `lemmas`. In the first case here,
    we see that the second `synset` for `good` as a noun is defined as `moral excellence`,
    and its first antonym is `evil`, defined as `morally wrong`. In the second case,
    when `good` is used as an adjective to describe positive qualities, the first
    antonym is `bad`, which describes negative qualities.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`antonyms()` 方法返回一个 `lemmas` 列表。在这里的第一个例子中，我们看到 `good` 作为名词的第二个 `synset` 被定义为
    `道德上的优点`，其第一个反义词是 `evil`，定义为 `道德上的错误`。在第二个例子中，当 `good` 被用作形容词来描述积极的品质时，第一个反义词是
    `bad`，它描述的是消极的品质。'
- en: See also
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the next recipe, we'll learn how to calculate `synset` similarity. Then in
    [Chapter 2](ch02.html "Chapter 2. Replacing and Correcting Words"), *Replacing
    and Correcting Words*, we'll revisit lemmas for lemmatization, synonym replacement,
    and antonym replacement.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个菜谱中，我们将学习如何计算 `synset` 相似度。然后在 [第2章](ch02.html "第2章。替换和修正单词")，*替换和修正单词*
    中，我们将重新审视词元化、同义词替换和反义词替换。
- en: Calculating WordNet synset similarity
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算WordNet synset相似度
- en: Synsets are organized in a *hypernym* tree. This tree can be used for reasoning
    about the similarity between the synsets it contains. Two synsets are more similar,
    the closer they are in the tree.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`Synsets` 是按 *hypernym* 树组织起来的。这棵树可以用来推理它包含的 `synset` 之间的相似度。两个 `synset` 越接近树中的位置，它们就越相似。'
- en: How to do it...
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: If you were to look at all the hyponyms of `reference book` (which is the hypernym
    of `cookbook`) you'd see that one of them is `instruction_book`. These seem intuitively
    very similar to `cookbook`, so let's see what WordNet similarity has to say about
    it.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看 `reference book`（它是 `cookbook` 的超类）的所有下位词，你会看到其中之一是 `instruction_book`。这些看起来直观上与
    `cookbook` 非常相似，所以让我们看看 WordNet 相似度对此有什么看法。
- en: '[PRE26]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: So they are over 91% similar!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它们的相似度超过 91%！
- en: How it works...
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`wup_similarity` is short for *Wu-Palmer Similarity*, which is a scoring method
    based on how similar the word senses are and where the synsets occur relative
    to each other in the hypernym tree. One of the core metrics used to calculate
    similarity is the shortest path distance between the two synsets and their common
    hypernym.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`wup_similarity` 是指 *Wu-Palmer Similarity*，这是一种基于词义相似性和在超类树中相对位置进行评分的方法。用于计算相似度的核心指标之一是两个
    `synset` 及其共同超类之间的最短路径距离。'
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: So `cookbook` and `instruction book` must be very similar, because they are
    only one step away from the same hypernym, `reference book`, and therefore only
    two steps away from each other.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`cookbook` 和 `instruction book` 必定非常相似，因为它们只相差一步就能到达同一个超类 `reference book`，因此彼此之间只相差两步。
- en: There's more...
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Let's look at two dissimilar words to see what kind of score we get. We'll compare
    `dog` with `cookbook`, two seemingly very different words.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看两个不同的词，看看我们会得到什么样的分数。我们将比较 `dog` 和 `cookbook`，这两个词看起来非常不同。
- en: '[PRE28]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Wow, `dog` and `cookbook` are apparently 38% similar! This is because they share
    common hypernyms farther up the tree.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，`dog` 和 `cookbook` 看起来有 38% 的相似度！这是因为它们在树的上层共享共同的超类。
- en: '[PRE29]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Comparing verbs
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较动词
- en: The previous comparisons were all between nouns, but the same can be done for
    verbs as well.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的比较都是名词之间的，但同样的方法也可以用于动词。
- en: '[PRE30]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The previous synsets were obviously handpicked for demonstration, and the reason
    is that the hypernym tree for verbs has a lot more breadth and a lot less depth.
    While most nouns can be traced up to `object`, thereby providing a basis for similarity,
    many verbs do not share common hypernyms, making WordNet unable to calculate similarity.
    For example, if you were to use the `synset` for `bake.v.01` here, instead of
    `bake.v.02`, the return value would be `None`. This is because the root hypernyms
    of the two synsets are different, with no overlapping paths. For this reason,
    you also cannot calculate similarity between words with different parts of speech.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的 `synset` 明显是特意挑选出来进行演示的，原因是动词的超类树有更多的广度而深度较少。虽然大多数名词可以追溯到 `object`，从而提供相似性的基础，但许多动词没有共享的共同超类，这使得
    WordNet 无法计算相似度。例如，如果你在这里使用 `bake.v.01` 的 `synset`，而不是 `bake.v.02`，返回值将是 `None`。这是因为这两个
    `synset` 的根超类不同，没有重叠的路径。因此，你也不能计算不同词性的单词之间的相似度。
- en: Path and LCH similarity
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 路径和 LCH 相似度
- en: Two other similarity comparisons are the path similarity and **Leacock Chodorow
    (LCH)** similarity.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两种相似度比较是路径相似度和**Leacock Chodorow (LCH**)相似度。
- en: '[PRE31]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As you can see, the number ranges are very different for these scoring methods,
    which is why we prefer the `wup_similarity()` method.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这些评分方法的数值范围差异很大，这就是为什么我们更喜欢`wup_similarity()`方法。
- en: See also
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考内容
- en: The recipe on *Looking up synsets for a word in WordNet,* discussed earlier
    in this chapter, has more details about hypernyms and the hypernym tree.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面讨论的*在WordNet中查找单词的synsets*的食谱中，有更多关于上位词和上位词树的信息。
- en: Discovering word collocations
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现单词搭配
- en: '**Collocations** are two or more words that tend to appear frequently together,
    such as "United States". Of course, there are many other words that can come after
    "United", for example "United Kingdom", "United Airlines", and so on. As with
    many aspects of natural language processing, context is very important, and for
    collocations, context is everything!'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**搭配词**是指两个或更多经常一起出现的单词，例如“United States”。当然，还有许多其他单词可以跟在“United”后面，例如“United
    Kingdom”，“United Airlines”等等。与自然语言处理的许多方面一样，上下文非常重要，对于搭配词来说，上下文就是一切！'
- en: In the case of collocations, the context will be a document in the form of a
    list of words. Discovering collocations in this list of words means that we'll
    find common phrases that occur frequently throughout the text. For fun, we'll
    start with the script for *Monty Python and the Holy Grail*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在搭配词的情况下，上下文将是一个单词列表形式的文档。在这个单词列表中寻找搭配词意味着我们将找到在整个文本中频繁出现的常见短语。为了好玩，我们将从*《蒙提·派森与圣杯》*的剧本开始。
- en: Getting ready
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: The script for *Monty Python and the Holy Grail* is found in the `webtext` corpus,
    so be sure that it's unzipped in `nltk_data/corpora/webtext/`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*《蒙提·派森与圣杯》*的剧本可以在`webtext`语料库中找到，所以请确保它在`nltk_data/corpora/webtext/`中已解压。'
- en: How to do it...
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: We're going to create a list of all lowercased words in the text, and then produce
    a `BigramCollocationFinder`, which we can use to find **bigrams**, which are pairs
    of words. These bigrams are found using association measurement functions found
    in the `nltk.metrics` package.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个包含文本中所有小写单词的列表，然后生成一个`BigramCollocationFinder`，我们可以使用它来查找**双词组合**，即单词对。这些双词组合是通过在`nltk.metrics`包中找到的关联测量函数找到的。
- en: '[PRE32]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Well that's not very useful! Let's refine it a bit by adding a word filter to
    remove punctuation and stopwords.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这并不很有用！让我们通过添加一个单词过滤器来去除标点符号和停用词来稍微改进一下。
- en: '[PRE33]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Much better—we can clearly see four of the most common bigrams in *Monty Python
    and the Holy Grail*. If you'd like to see more than four, simply increase the
    number to whatever you want, and the collocation finder will do its best.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 更好——我们可以清楚地看到*《蒙提·派森与圣杯》*中最常见的四个双词组合。如果你想要看到超过四个，只需将数字增加到你想要的任何值，搭配词查找器将尽力而为。
- en: How it works...
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The `BigramCollocationFinder` constructs two frequency distributions: one for
    each word, and another for bigrams. A **frequency distribution**, or `FreqDist`
    in NLTK, is basically an enhanced dictionary where the keys are what''s being
    counted, and the values are the counts. Any filtering functions that are applied,
    reduce the size of these two `FreqDists` by eliminating any words that don''t
    pass the filter. By using a filtering function to eliminate all words that are
    one or two characters, and all English stopwords, we can get a much cleaner result.
    After filtering, the collocation finder is ready to accept a generic scoring function
    for finding collocations. Additional scoring functions are covered in the *Scoring
    functions* section further in this chapter.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`BigramCollocationFinder`构建了两个频率分布：一个用于每个单词，另一个用于双词组合。**频率分布**，在NLTK中称为`FreqDist`，基本上是一个增强的字典，其中键是正在计数的项，值是计数。任何应用到的过滤函数都会通过消除任何未通过过滤器的单词来减少这两个`FreqDist`的大小。通过使用过滤函数消除所有一或两个字符的单词以及所有英语停用词，我们可以得到一个更干净的结果。过滤后，搭配词查找器就准备好接受一个通用的评分函数来查找搭配词。额外的评分函数将在本章后面的*评分函数*部分进行讨论。'
- en: There's more...
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In addition to `BigramCollocationFinder`, there's also `TrigramCollocationFinder`,
    for finding triples instead of pairs. This time, we'll look for **trigrams** in
    Australian singles ads.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`BigramCollocationFinder`，还有`TrigramCollocationFinder`，用于查找三元组而不是成对的三元组。这次，我们将寻找澳大利亚单身广告中的**三元组**。
- en: '[PRE34]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now, we don't know whether people are looking for a long-term relationship or
    not, but clearly it's an important topic. In addition to the stopword filter,
    we also applied a frequency filter which removed any trigrams that occurred less
    than three times. This is why only one result was returned when we asked for four—because
    there was only one result that occurred more than twice.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们不知道人们是否在寻找长期关系，但显然这是一个重要的话题。除了停用词过滤器外，我们还应用了一个频率过滤器，该过滤器移除了出现次数少于三次的所有三元组。这就是为什么当我们要求四个结果时只返回一个结果的原因——因为只有一个结果出现了两次以上。
- en: Scoring functions
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评分函数
- en: There are many more scoring functions available besides `likelihood_ratio()`.
    But other than `raw_freq()`, you may need a bit of a statistics background to
    understand how they work. Consult the NLTK API documentation for `NgramAssocMeasures`
    in the `nltk.metrics` package, to see all the possible scoring functions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `likelihood_ratio()` 之外，还有许多其他的评分函数可用。但除了 `raw_freq()` 之外，你可能需要一点统计学背景才能理解它们是如何工作的。请参考
    `nltk.metrics` 包中 `NgramAssocMeasures` 的 NLTK API 文档，以查看所有可能的评分函数。
- en: Scoring ngrams
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评分 n-gram
- en: In addition to the `nbest()` method, there are two other ways to get **ngrams**
    (a generic term for describing *bigrams* and *trigrams*) from a collocation finder.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `nbest()` 方法之外，还有两种从搭配查找器中获取 **n-gram**（描述 *bigram* 和 *trigram* 的通用术语）的方法。
- en: '`above_score(score_fn, min_score)` can be used to get all ngrams with scores
    that are at least `min_score`. The `min_score` that you choose will depend heavily
    on the `score_fn` you use.'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`above_score(score_fn, min_score)` 可以用来获取所有评分至少为 `min_score` 的 n-gram。你选择的
    `min_score` 将在很大程度上取决于你使用的 `score_fn`。'
- en: '`score_ngrams(score_fn)` will return a list with tuple pairs of `(ngram, score)`.
    This can be used to inform your choice for `min_score` in the previous step.'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`score_ngrams(score_fn)` 将返回一个包含 `(ngram, score)` 元组的列表。这可以用来告知你之前步骤中 `min_score`
    的选择。'
- en: See also
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The `nltk.metrics` module will be used again in [Chapter 7](ch07.html "Chapter 7. Text
    Classification"), *Text Classification*.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk.metrics` 模块将在 [第7章](ch07.html "第7章。文本分类") *文本分类* 中再次使用。'
