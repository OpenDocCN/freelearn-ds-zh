- en: Chapter 5.  Supervised and Unsupervised Learning by Examples
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。通过示例进行监督和无监督学习
- en: 'In [Chapter 2](part0023_split_000.html#LTSU2-5afe140a04e845e0842b44be7971e11a
    "Chapter 2. Machine Learning Best Practices"), *Machine Learning Best Practices* readers,
    learned some theoretical underpinnings of basic machine learning techniques. Whereas,
    [Chapter 3](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d "Chapter 3. Understanding
    the Problem by Understanding the Data"), *Understanding the Problem by Understanding
    the Data,* describes the basic data manipulation using Spark''s APIs such as RDD,
    DataFrame, and Datasets. [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Knowledge
    through Feature Engineering*, on the other hand, describes feature engineering
    from both the theoretical and practical point of view. However, in this chapter,
    the reader will learn the practical know-how needed quickly and powerfully to
    apply supervised and unsupervised techniques on the available data to the new
    problems through some widely used examples based on the understandings from the
    previous chapters. These examples we are talking about will be demonstrated from
    the Spark perspective. In a nutshell, the following topics will be covered throughout
    this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](part0023_split_000.html#LTSU2-5afe140a04e845e0842b44be7971e11a "第2章。机器学习最佳实践")中，《机器学习最佳实践》读者学习了一些基本机器学习技术的理论基础。而[第3章](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "第3章。通过了解数据来理解问题")中，《通过了解数据来理解问题》，描述了使用Spark的API（如RDD、DataFrame和Datasets）进行基本数据操作。另一方面，[第4章](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "第4章。通过特征工程提取知识")描述了特征工程的理论和实践。然而，在本章中，读者将学习到快速而强大地在可用数据上应用监督和无监督技术以解决新问题所需的实际知识，这些知识是基于前几章的理解，并且将从Spark的角度演示这些例子。简而言之，本章将涵盖以下主题：
- en: Machine learning classes
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习课程
- en: Supervised learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Recommender system
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统
- en: Advanced learning and generalization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级学习和泛化
- en: Machine learning classes
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习课程
- en: 'As stated in [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark"), *Introduction to Data
    Analytics with Spark* and [Chapter 2](part0023_split_000.html#LTSU2-5afe140a04e845e0842b44be7971e11a
    "Chapter 2. Machine Learning Best Practices"), *Machine* *Learning Best Practices*,
    machine learning techniques can be categorized further into three major classes
    of algorithms: supervised learning, unsupervised learning, and the recommender
    system. Where classification and regression algorithms are widely used in the
    supervised learning application development, clustering, on the other hand, falls
    in the category of unsupervised learning. In this section, we will describe some
    examples of the supervised learning technique.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第1章](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d "第1章。使用Spark进行数据分析简介")中所述，《使用Spark进行数据分析简介》和[第2章](part0023_split_000.html#LTSU2-5afe140a04e845e0842b44be7971e11a
    "第2章。机器学习最佳实践")中，《机器学习最佳实践》，机器学习技术可以进一步分为三类主要算法：监督学习、无监督学习和推荐系统。分类和回归算法在监督学习应用开发中被广泛使用，而聚类则属于无监督学习的范畴。在本节中，我们将描述一些监督学习技术的示例。
- en: 'Then we will provide some example of the same example presented using Spark.
    On the other hand, an example of the clustering technique will be discussed in
    the section: *Unsupervised learning*, where a regression technique often models
    the past relationship between variables to predict their future changes (up or
    down). Here we show two real-life examples of classification and regression algorithms
    respectively. In contrast, a classification technique takes a set of data with
    known labels and learns how to label new records based on that information:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将提供一些使用Spark呈现的相同示例的例子。另一方面，聚类技术的示例将在“无监督学习”部分中讨论，回归技术经常模拟变量之间的过去关系，以预测它们的未来变化（上升或下降）。在这里，我们分别展示了分类和回归算法的两个现实生活中的例子。相比之下，分类技术接受一组带有已知标签的数据，并学习如何基于该信息为新记录打上标签：
- en: '**Example (classification)**: Gmail uses a machine learning technique called
    classification to designate if an e-mail is spam or not, based on the data of
    an e-mail.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例（分类）**：Gmail使用一种称为分类的机器学习技术，根据电子邮件的数据来确定电子邮件是否为垃圾邮件。'
- en: '**Example (regression)**: As an example, suppose you are an online currency
    trader and you work on Forex or Fortrade. Right now you have two currency pairs
    in mind to buy or sell say: GBP/USD and USD/JPY. If you look at these two pairs
    carefully, USD is a common in these two pairs. Now if you look at the historical
    prices of USD, GBP, or JPY you can predict the future outcome of whether you should
    open the trade in buy or sell. These types of problems can be resolved with supervised
    learning techniques using regression analysis:![Machine learning classes](img/00164.jpeg)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例（回归）**：举个例子，假设你是一名在线货币交易员，你在外汇或Fortrade上工作。现在你心里有两个货币对要买入或卖出，比如：GBP/USD和USD/JPY。如果你仔细观察这两对货币，USD是这两对货币的共同点。现在，如果你观察USD、GBP或JPY的历史价格，你可以预测未来的结果，即你应该开仓买入还是卖出。这些问题可以通过使用回归分析的监督学习技术来解决：![机器学习课程](img/00164.jpeg)'
- en: 'Figure 1: Classification, clustering, and collaborative filtering-the big picture'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：分类、聚类和协同过滤-大局观
- en: 'On the other hand, clustering and dimensionality reduction are commonly used
    for unsupervised learning. Here are some examples:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，聚类和降维常用于无监督学习。以下是一些示例：
- en: '**Example (clustering)**: Google News uses a technique called clustering to
    group news articles into different categories, based on title and content. Clustering
    algorithms discover groupings that occur in collections of data.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例（聚类）**：Google新闻使用一种称为聚类的技术，根据标题和内容将新闻文章分成不同的类别。聚类算法发现数据集中出现的分组。'
- en: '**Example (collaborative filtering)**: The collaborative filtering algorithm
    is often used in the recommendation system development. Renowned companies such
    as Amazon and Netflix use a machine learning technique called collaborative filtering,
    to determine which products users will like based on their history and similarity
    to other users.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例（协同过滤）**：协同过滤算法经常用于推荐系统的开发。像亚马逊和Netflix这样的知名公司使用一种称为协同过滤的机器学习技术，根据用户的历史和与其他用户的相似性来确定用户会喜欢哪些产品。'
- en: '**Example (dimensionality reduction)**: Dimensionality reduction is often used
    to make the available dataset that is high dimensional. For example, suppose you
    have an image of size 2048x1920, and you would like to reduce the dimension to
    1080x720 without sacrificing the quality much. In this case, popular algorithms
    such as **Principal Component Analysis** (**PCA**) or **Singular Value Decomposition**
    (**SVD**) can be used although you can also implement the SVD to implement the
    PCA. This is why SVD is more widely used.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例（降维）**：降维通常用于使高维数据集变得更加可用。例如，假设您有一张尺寸为2048x1920的图像，并且希望将其降维到1080x720，而不会牺牲太多质量。在这种情况下，可以使用流行的算法，如**主成分分析**（**PCA**）或**奇异值分解**（**SVD**），尽管您也可以实现SVD来实现PCA。这就是为什么SVD更广泛使用的原因。'
- en: Supervised learning
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'As already stated, a supervised learning application makes predictions based
    on a set of examples and the goal is to learn general rules that map inputs to
    outputs aligning with the real world. For example, a dataset for spam filtering
    usually contains spam messages as well as non-spam messages. Consequently, we
    could know which messages in the training set are spams or non-spam. Therefore,
    supervised learning is the machine learning technique of inferring a function
    from the labeled training data. The following steps are involved in supervised
    learning tasks:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如已经说明的，监督学习应用程序基于一组示例进行预测，其目标是学习将输入与与现实世界相一致的输出相映射的一般规则。例如，垃圾邮件过滤的数据集通常包含垃圾邮件和非垃圾邮件。因此，我们可以知道训练集中哪些消息是垃圾邮件或非垃圾邮件。因此，监督学习是从标记的训练数据中推断函数的机器学习技术。监督学习任务涉及以下步骤：
- en: Train the ML model with the training dataset
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练数据集训练ML模型
- en: Use the test dataset to test the model performance
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用测试数据集测试模型性能
- en: Therefore, the dataset for training the ML model, in this case, is labeled with
    the value of interest and a supervised learning algorithm looks for patterns in
    those value labels. After the algorithm has found the required patterns, those
    patterns can be used to make predictions for unlabeled test data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，用于训练ML模型的数据集被标记为感兴趣的值，监督学习算法会寻找这些值标签中的模式。算法找到所需的模式后，这些模式可以用于对未标记的测试数据进行预测。
- en: A typical use of the supervised learning is diverse and commonly used in the
    bioinformatics, cheminformatics, database marketing, handwriting recognition,
    information retrieval, object recognition in computer vision, optical character
    recognition, spam detection, pattern recognition, speech recognition, and so on,
    and in these applications mostly the classification technique is used. On the
    other hand, supervised learning is a special case of downward causation in biological
    systems.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的典型用途多种多样，通常用于生物信息学、化学信息学、数据库营销、手写识别、信息检索、计算机视觉中的对象识别、光学字符识别、垃圾邮件检测、模式识别、语音识别等应用中，这些应用中主要使用分类技术。另一方面，监督学习是生物系统中向下因果关系的特例。
- en: Tip
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'More on how the supervised learning technique works from the theoretical perspective
    can be found on these books: Vapnik, V. N. *The Nature of Statistical Learning
    Theory (2nd Ed.)*, Springer Verlag, 2000; and Mehryar M., Afshin R. Ameet T. (2012)
    Foundations of Machine Learning, The MIT Press ISBN 9780262018258.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有关监督学习技术如何从理论角度工作的更多信息可以在以下书籍中找到：Vapnik, V. N. *统计学习理论的本质（第二版）*，Springer Verlag，2000年；以及Mehryar
    M.，Afshin R. Ameet T.（2012）机器学习基础，麻省理工学院出版社ISBN 9780262018258。
- en: Supervised learning example
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督学习示例
- en: 'Classification is a family of supervised machine learning algorithms that designate
    input as belonging to one of the several pre-defined classes. Some common use
    cases for classification include:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是一类监督机器学习算法，将输入指定为预定义类别之一。分类的一些常见用例包括：
- en: Credit card fraud detection
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信用卡欺诈检测
- en: E-mail spam detection
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件垃圾邮件检测
- en: Classification data is labeled, for example, as spam/non-spam or fraud/non-fraud.
    Machine learning assigns a label or class to new data. You classify something
    based on pre-determined features. Features are the *if questions* that you ask.
    The label is the answer to those questions. For example, if an object walks, swims,
    and quacks like a duck, then the label would be *duck*. Or suppose for a flight
    is delayed on to be a departure or arrival by more than say 1 hour, it would be
    a delay; otherwise not a delay.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 分类数据被标记，例如垃圾邮件/非垃圾邮件或欺诈/非欺诈。机器学习为新数据分配标签或类别。您根据预先确定的特征对某物进行分类。特征是您提出的“如果问题”。标签是这些问题的答案。例如，如果一个对象像鸭子一样走路，游泳和呱呱叫，那么标签将是*鸭子*。或者假设航班延误超过1小时，那么它将是延误；否则不是延误。
- en: Supervised learning with Spark - an example
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行监督学习-一个例子
- en: We will demonstrate an example by analyzing an air-flight delay. The dataset
    named `On_Time_Performance_2016_1.csv` from the United Department of Transportation
    website at [http://www.transtats.bts.gov/](http://www.transtats.bts.gov/) will
    be used.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过分析航班延误来演示一个示例。将使用美国交通部网站上的名为`On_Time_Performance_2016_1.csv`的数据集[http://www.transtats.bts.gov/](http://www.transtats.bts.gov/)。
- en: Air-flight delay analysis using Spark
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark进行航班延误分析
- en: 'We are using flight information for 2016\. For each flight, we have the following
    information presented in *Table 1* (we have presented only a few fields out of
    444,827 rows and 110 columns as of May 17, 2016):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用2016年的航班信息。对于每次航班，我们在*表1*中提供了以下信息（截至2016年5月17日，共444,827行和110列）：
- en: '| **Data field** | **Description** | **Example value** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **数据字段** | **描述** | **示例值** |'
- en: '| `DayofMonth` | Day of month | 2 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| `DayofMonth` | 月份 | 2 |'
- en: '| `DayOfWeek` | Day of week | 5 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| `DayOfWeek` | 星期几 | 5 |'
- en: '| `TailNum` | Tail number for the plane | N505NK |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| `TailNum` | 飞机尾号 | N505NK |'
- en: '| `FlightNum` | Flight number | 48 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| `FlightNum` | 航班号 | 48 |'
- en: '| `AirlineID` | Airline ID | 19805 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| `AirlineID` | 航空公司ID | 19805 |'
- en: '| `OriginAirportID` | Origin airport ID | JFK |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| `OriginAirportID` | 起飞机场ID | JFK |'
- en: '| `DestAirportID` | Destination airport ID | LAX |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| `DestAirportID` | 目的地机场ID | LAX |'
- en: '| `Dest` | Destination airport code | 1424 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| `Dest` | 目的地机场代码 | 1424 |'
- en: '| `CRSDepTime` | Schedule departure time | 10:00 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| `CRSDepTime` | 计划起飞时间 | 10:00 |'
- en: '| `DepTime` | Actual departure time | 10:30 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| `DepTime` | 实际起飞时间 | 10:30 |'
- en: '| `DepDelayMinutes` | Departure delay in minutes | 30 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| `DepDelayMinutes` | 起飞延误时间 | 30 |'
- en: '| `CRSArrTime` | Schedule arrival time | 22:45 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| `CRSArrTime` | 计划到达时间 | 22:45 |'
- en: '| `ArrTime` | Actual arrival time | 23:45 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| `ArrTime` | 实际到达时间 | 23:45 |'
- en: '| `ArrDelayMinutes` | Arrival delay in minutes | 60 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| `ArrDelayMinutes` | 到达延误时间 | 60 |'
- en: '| `CRSElapsedTime` | Elapsed time | 825 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| `CRSElapsedTime` | 飞行时间 | 825 |'
- en: '| `Distance` | Total distance | 6200 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| `Distance` | 总距离 | 6200 |'
- en: 'Table 1: Sample data from the "On Time On Time Performance 2016_1" dataset'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：来自“准时表现2016_1”数据集的样本数据
- en: 'In this scenario, we will build a tree to predict the label of delayed or not
    delayed based on the following features shown in the figure, which is small snapshot
    of an air flight dataset. Here `ArrDelayMinutes` is 113 which should be classified
    in delayed (1.0) and other rows are less than 60 minutes so the label should be
    0.0 (not delayed). From this dataset, we will do some operation such as feature
    extraction, transformation, and selection. *Table 2* shows the top five rows related
    to the features we will be considering for this example as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将构建一棵树，根据图中显示的以下特征来预测延误或未延误的标签，这是航班数据集的一个小快照。这里`ArrDelayMinutes`为113，应该被分类为延误（1.0），其他行的延误时间少于60分钟，因此标签应为0.0（未延误）。从这个数据集中，我们将进行一些操作，如特征提取、转换和选择。*表2*显示了我们将在此示例中考虑的与特征相关的前五行：
- en: '**Label**: Delayed and not delayed - delayed if delay >60 minutes'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签**：延误和未延误 - 如果延误>60分钟，则为延误'
- en: '**Features**: {`DayOfMonth`, `WeekOfday`, `CRSdeptime`, `CRSarrtime`, `Carrier`,
    `CRSelapsedtime`, `Origin`, `Dest`, `ArrDelayMinutes`}![Air-flight delay analysis
    using Spark](img/00004.jpeg)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征**：{`DayOfMonth`, `WeekOfday`, `CRSdeptime`, `CRSarrtime`, `Carrier`, `CRSelapsedtime`,
    `Origin`, `Dest`, `ArrDelayMinutes`}![使用Spark进行航班延误分析](img/00004.jpeg)'
- en: 'Figure 2: Selected feature for air-flight delay prediction'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：用于航班延误预测的选定特征
- en: Loading and parsing the Dataset
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载和解析数据集
- en: 'Before performing the feature extraction, we need to load and parse the dataset.
    This step also includes: loading packages and related dependencies, reading the
    dataset as a DataFrame, making the POJO or Bean class, and adding the new label
    column based on requirements.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行特征提取之前，我们需要加载和解析数据集。这一步还包括：加载包和相关依赖项，将数据集读取为DataFrame，创建POJO或Bean类，并根据要求添加新的标签列。
- en: '**Step 1: Load required packages and dependencies**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：加载所需的包和依赖项**'
- en: 'For reading csv files, we used the csv reader provided by the Databricks:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了读取csv文件，我们使用了Databricks提供的csv读取器：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2: Create the Spark session**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：创建Spark会话**'
- en: 'Here is the code to create a Spark session:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建Spark会话的代码：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step 3: Read and parse the csv file using Dataset**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：使用数据集读取和解析csv文件**'
- en: 'This dataset contains many columns that we will not include as a feature in
    this example. So we will select from the DataFrame only the features that we have
    mentioned previously. This DataFrame output has already been shown in *Figure
    2*:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含许多列，我们在这个示例中不会将其作为特征。因此，我们将从DataFrame中仅选择我们之前提到的特征。这个DataFrame的输出已经在*图2*中显示过了：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is a the output of top 5 rows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前5行的输出：
- en: '![Loading and parsing the Dataset](img/00081.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![加载和解析数据集](img/00081.jpeg)'
- en: '**Step 4: Making a POJO or Bean class**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：创建POJO或Bean类**'
- en: The POJO class we have developed is called `Flight` where the required features
    and label field will be defined with the corresponding setter and getter.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发的POJO类名为`Flight`，其中将使用相应的setter和getter定义所需的特征和标签字段。
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We believe the preceding class is self-explanatory, which is used for setting
    and getting the feature values from the original dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信前面的类是不言自明的，它用于从原始数据集中设置和获取特征值。
- en: '**Step 5: Adding the new label column based on the delay column**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：根据延误列添加新的标签列**'
- en: 'If the delay is greater than 40 then the label should be 1 otherwise it should
    be 0\. Create a new Dataset using the Flight bean class. This dataset can contain
    an empty string for the `ArrDelayMinutes` column. So before mapping we filtered
    the rows containing the empty string from the dataset:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果延误超过40分钟，则标签应为1，否则应为0。使用Flight bean类创建一个新的数据集。这个数据集可以在`ArrDelayMinutes`列中包含空字符串。因此，在映射之前，我们从数据集中过滤掉包含空字符串的行：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now create a new Dataest from the RDD we created above as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从上面创建的RDD中创建一个新的数据集：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now show the top 5 rows from the data frame `flightDelayData` in following
    *Figure 3*:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在下面的*图3*中显示数据帧`flightDelayData`的前5行：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Output:]'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[输出：]'
- en: '![Loading and parsing the Dataset](img/00030.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![加载和解析数据集](img/00030.jpeg)'
- en: Figure 3:The DataFrame showing the new label column
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：显示新标签列的DataFrame
- en: Feature extraction
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征提取
- en: For extracting the feature, we have to make numerical values and if there are
    any text values, then we have to make a labeled vector for applying a machine
    learning algorithm.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取特征，我们必须制作数值，并且如果有任何文本值，那么我们必须制作一个标记向量，以应用机器学习算法。
- en: '**Step 1 :Transformation towards feature extraction**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步：转向特征提取
- en: 'Here we will transform the columns containing text into double values columns.
    Here we use `StringIndexer` for making a unique index for each unique text:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将把包含文本的列转换为双值列。在这里，我们使用`StringIndexer`为每个唯一的文本制作一个唯一的索引：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Output]:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[输出]：'
- en: '![Feature extraction](img/00043.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![特征提取](img/00043.jpeg)'
- en: 'Figure 4: Uunique indices for each unique text'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：每个唯一文本的唯一索引
- en: '**Step 2: Making the feature vectors using the vector assembler**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步：使用向量组装器制作特征向量
- en: 'Make the feature vector with the vector assembler and transform it to the labelled
    vector for applying the machine learning algorithm (decision tree). Note, here
    we used the decision tree to show just an example since it shows better classification
    accuracies. Based on the algorithm and model selection and tuning, you will be
    further able to explore and use other classifiers:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用向量组装器制作特征向量，并将其转换为标记向量，以应用机器学习算法（决策树）。请注意，这里我们使用决策树只是举个例子，因为它显示了更好的分类准确性。根据算法和模型的选择和调整，您将能够进一步探索和使用其他分类器：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now transform the assembler into a Dataset of row as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将组装器转换为行数据集，如下所示：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now convert the Dataset into `JavaRDD` for making the feature vectors as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将数据集转换为`JavaRDD`，以制作特征向量，如下所示：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Map the RDD for `LabeledPoint` as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式将RDD映射为`LabeledPoint`：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now print the first five values as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在按如下方式打印前五个值：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Output]:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[输出]：'
- en: '![Feature extraction](img/00025.jpeg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![特征提取](img/00025.jpeg)'
- en: 'Figure 5: The corresponding assembled vectors'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：相应的组装向量
- en: Preparing the training and testing set
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备训练和测试集
- en: Here we will prepare the training dataset from the dataset of the labeled vector.
    Initially, we will make a training set where 15% of records will be non-delayed
    and 85% will be delayed records. Finally, the training and testing dataset will
    be prepared as 70% and 30% respectively.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从标记向量的数据集中准备训练数据集。最初，我们将制作一个训练集，其中15%的记录将是非延迟记录，85%将是延迟记录。最后，训练和测试数据集将分别准备为70%和30%。
- en: '**Step 1: Make training and test set from the whole Dataset**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步：从整个数据集中制作训练和测试集
- en: 'First, create a new RDD by filtering the RDD based on the labels (that is,
    1 and 0) we created previously as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，根据之前创建的标签（即1和0）来过滤RDD，创建一个新的RDD，如下所示：
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now union the two RDDs using the `union()` method as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用`union()`方法将两个RDD联合起来，如下所示：
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now further convert the combined RDD into Dataset of Row as follows (max categories
    is set to be 4):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将合并的RDD进一步转换为行数据集，如下所示（最大类别设置为4）：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we need to do the vector indexer for the categorical variables as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要对分类变量进行向量索引，如下所示：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that we have the feature indexer using the `VectorIndexerModel` estimator.
    Now the next task is to do the string indexing using the `StringIndexerModel`
    estimator as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用`VectorIndexerModel`估计器进行了特征索引。现在下一个任务是使用`StringIndexerModel`估计器进行字符串索引，如下所示：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, split the Dataset of Row into training and test (70% and 30% respectively
    but you should adjust the values based on your requirements) set as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将行数据集分割为训练和测试集（分别为70%和30%，但您应根据您的需求调整值），如下所示：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Well done! Now our dataset is ready to train the model, right? For the time
    being, we will naively select a classifier to say let's use the decision tree
    classifier to solve our purpose. You can try this with other multiclass classifiers
    based on examples provided in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*, [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*,
    and [Chapter 8](part0067_split_000.html#1VSLM1-5afe140a04e845e0842b44be7971e11a
    "Chapter 8.  Adapting Your Machine Learning Models"), *Adapting Your Machine Learning
    Models*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！现在我们的数据集已经准备好训练模型了，对吧？暂时，我们会天真地选择一个分类器，比如说让我们使用决策树分类器来解决我们的目的。您可以根据[第6章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "第6章。构建可扩展的机器学习管道"), *构建可扩展的机器学习管道*，[第7章](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "第7章。调整机器学习模型"), *调整机器学习模型*，和 [第8章](part0067_split_000.html#1VSLM1-5afe140a04e845e0842b44be7971e11a
    "第8章。调整您的机器学习模型"), *调整您的机器学习模型*中提供的示例尝试其他多类分类器。
- en: Training the model
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'As shown in *Figure 2*, training and test data will be collected from the raw
    data. After the feature engineering process has been done, the RDD of feature
    vectors with labels or ratings will be used next to be processed by the classification
    algorithm before building the predictive model (as shown in *Figure 6*) and at
    the end the test data will be used for testing the model performance:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图2*所示，训练和测试数据将从原始数据中收集。在特征工程过程完成后，带有标签或评级的特征向量的RDD将在构建预测模型之前通过分类算法进行处理（如*图6*所示），最后测试数据将用于测试模型的性能：
- en: '![Training the model](img/00070.jpeg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![训练模型](img/00070.jpeg)'
- en: 'Figure 6: Supervised learning using Spark'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：使用Spark进行监督学习
- en: 'Next, we prepare the values for the parameters that will be required for the
    decision tree. You might have wondered why we are talking about the decision tree.
    The reason is simple since using the decision tree (that is, the **binary decision
    tree**)we observed better prediction accuracy compared to the Naive Bayes approaches.
    Refer to *Table 2* which describes the categorical features and their significance
    as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为决策树所需的参数值做准备。也许你会想为什么我们要谈论决策树。原因很简单，因为我们观察到使用决策树（即**二叉决策树**）相比朴素贝叶斯方法有更好的预测准确度。参考*表2*，描述了分类特征及其重要性如下：
- en: '| **Categorical features** | **Mapping** | **Significant** |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **分类特征** | **映射** | **重要性** |'
- en: '| categoricalFeaturesInfo | 0 -> 31 | Specifies that the feature index 0 (which
    represents the day of the month) has 31 categories [values {0, ..., 31}] |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| categoricalFeaturesInfo | 0 -> 31 | 指定特征索引0（代表月份的天数）有31个类别[值{0，...，31}] |'
- en: '| categoricalFeaturesInfo | 1 -> 7 | Represents days of the week, and specifies
    that the feature index 1 has seven categories |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| categoricalFeaturesInfo | 1 -> 7 | 表示一周的天数，并指定特征索引1有七个类别 |'
- en: '| Carrier | 0 -> N | N signifies the numbers from 0 to up to the number of
    distinct carriers |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Carrier | 0 -> N | N表示从0到不同航空公司的数量 |'
- en: 'Table 2: Categorical features and their significance'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：分类特征及其重要性
- en: Now we will describe the approach of the decision tree construction in brief.
    We will use the CategoricalFeaturesInfo that specifies which features are categorical
    and how many categorical values each of those features can take during the tree
    construction process. This is given as a map from the feature index to the number
    of categories for that feature.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将简要描述决策树构建的方法。我们将使用CategoricalFeaturesInfo来指定哪些特征是分类的，以及在树构建过程中每个特征可以取多少个分类值。这是一个从特征索引到该特征的类别数的映射。
- en: 'However, the model is trained by making associations between the input features
    and the labeled output associated with those features. We train the model using
    the `DecisionTreeClassifier` method which returns a `DecisionTreeModel` eventually
    as shown in *Figure 7*. The detailed source code for constructing the tree will
    be shown later in this section:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，该模型是通过将输入特征与与这些特征相关联的标记输出进行关联来训练的。我们使用`DecisionTreeClassifier`方法训练模型，最终返回一个`DecisionTreeModel`，如*图7*所示。构建树的详细源代码将在本节后面显示。
- en: '![Training the model](img/00084.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![训练模型](img/00084.jpeg)'
- en: 'Figure 7: The binary decision tree generated for the air-flight delay analysis
    (partially shown)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：为航班延误分析生成的二叉决策树（部分显示）
- en: '**Step 1: Train the decision tree model**'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：训练决策树模型**'
- en: 'To train the decision tree classifier model, we need to have the necessary
    labels and features:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练决策树分类器模型，我们需要有必要的标签和特征：
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Step 2: Convert the indexed labels back to original labels**'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：将索引标签转换回原始标签**'
- en: 'To create a decision tree pipeline, we need to have the original labels apart
    from the indexed labels. So, let''s do it as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建决策树管道，我们需要除了索引标签之外的原始标签。因此，让我们按照以下方式进行：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Step 3: Chain the indexer and tree in a single pipeline**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：将索引器和树链接成一个单一管道**'
- en: 'Create a new pipeline where the stages are as follows: `labelIndexer`, `featureIndexer`,
    `dt`, `labelConverter` as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的管道，其中阶段如下：`labelIndexer`，`featureIndexer`，`dt`，`labelConverter`如下：
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now fit the pipeline using the training set we created in *Step 8* as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用我们在*步骤8*中创建的训练集来拟合管道如下：
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Testing the model
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试模型
- en: 'Here we will test the models as shown in the following steps:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将测试模型：
- en: '**Step 1: Make the prediction on the test dataset**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：对测试数据集进行预测**'
- en: 'Make the prediction on the test set by transforming the `PipelineModel` and
    show the performance parameters as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 通过转换`PipelineModel`对测试集进行预测，并显示性能参数如下：
- en: '[PRE23]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Step 2: Evaluate the model**'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：评估模型**'
- en: 'Evaluate the model by the multiclass classification evaluator and print the
    accuracy and test error as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多类分类评估器评估模型，并打印准确度和测试错误如下：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The preceding code segment produces the classification accuracy and test error
    as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码段生成了分类准确度和测试错误如下：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Please note that since we randomly split the dataset into training and testing,
    you might get a different result. The classification accuracy is 75.40%, which
    is not good, we believe.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于我们随机将数据集分成训练集和测试集，你可能会得到不同的结果。分类准确度为75.40%，这并不好，我们认为。
- en: However, now it's your turn to use a different classifier and tune before deploying
    the model. A more details discussion will be carried out in [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models,* regarding
    tuning the ML models.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在轮到你使用不同的分类器和调整模型了。有关调整ML模型的更多详细讨论将在[第7章](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "第7章。调整机器学习模型")中进行，*调整机器学习模型*。
- en: '**Step 3: Print the decision tree**'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：打印决策树**'
- en: 'Here is the code to print the decision tree:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是打印决策树的代码：
- en: '[PRE26]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This code segment produces a decision tree as shown in *Figure 7*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码段生成一个决策树，如*图7*所示。
- en: '**Step 4: Stop the Spark session**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：停止Spark会话**'
- en: 'Stop the Spark session using the `stop()` method of Spark as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark的`stop()`方法停止Spark会话如下：
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This is a good practice that you initiate a Spark session and close or stop
    it properly to avoid the memory leak in your applications.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的做法，你要正确启动和关闭或停止Spark会话，以避免应用程序中的内存泄漏。
- en: Unsupervised learning
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In unsupervised learning, data points have no labels related to them; therefore,
    we need to put labels on them algorithmically. In other words, the correct classes
    of the training dataset in unsupervised learning are unknown.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，数据点没有与之相关的标签；因此，我们需要通过算法给它们贴上标签。换句话说，在无监督学习中，训练数据集的正确类别是未知的。
- en: Consequently, classes have to be inferred from the unstructured datasets which
    implies that the goal of an unsupervised learning algorithm is to pre-process
    the data in some structured ways by describing its structure. The main objective
    of the unsupervised learning algorithms or techniques is to explore the unknown
    patterns of the input data that are mostly unlabeled. In this way, it is closely
    related to the problem of density estimation used in theoretical and applied statistics.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，必须从非结构化数据集中推断出类别，这意味着无监督学习算法的目标是通过描述其结构以某种结构化方式预处理数据。无监督学习算法或技术的主要目标是探索大多数未标记的输入数据的未知模式。这样，它与理论和应用统计学中使用的密度估计问题密切相关。
- en: Unsupervised learning, however, also comprehends many other techniques to summarize
    and explain the key features of the data including exploratory data analysis for
    finding these hidden patterns, even grouping the data points or features and applying
    the unsupervised learning technique based on data mining methods for the data
    pre-processing.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无监督学习还包括许多其他技术，以总结和解释数据的关键特征，包括用于发现这些隐藏模式的探索性数据分析，甚至对数据点或特征进行分组，并根据数据挖掘方法应用无监督学习技术进行数据预处理。
- en: To overcome this obstacle in unsupervised learning, clustering techniques are
    used typically to group the unlabeled samples based on certain similarity measures,
    mining hidden patterns towards feature learning.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服无监督学习中的这一障碍，通常使用聚类技术根据某些相似性度量对未标记的样本进行分组，挖掘隐藏的模式以进行特征学习。
- en: Tip
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'For in-depth theoretical knowledge, how the unsupervised algorithms work, please
    refer to these three books: Bousquet, O.; von Luxburg, U.; Raetsch, G., eds. (2004).
    *Advanced Lectures on Machine Learning*. Springer-Verlag. ISBN 978-3540231226\.
    Or Duda, Richard O.; Hart, Peter E.; Stork, David G. (2001). *Unsupervised Learning
    and Clustering*. *Pattern Classification (2nd Ed.)*. Wiley. ISBN 0-471-05669-3
    and Jordan, Michael I.; Bishop, Christopher M. (2004). *Neural Networks*. In Allen
    B. Tucker. Computer *Science Handbook, Second Edition (Section VII: Intelligent
    Systems)*. Boca Raton, FL: Chapman & Hall/CRC Press LLC. ISBN 1-58488-360-X.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '要深入了解理论知识，了解无监督算法的工作原理，请参考以下三本书：Bousquet, O.; von Luxburg, U.; Raetsch, G.,
    eds. (2004). *Advanced Lectures on Machine Learning*. Springer-Verlag. ISBN 978-3540231226。或者Duda,
    Richard O.; Hart, Peter E.; Stork, David G. (2001). *Unsupervised Learning and
    Clustering*. *Pattern Classification (2nd Ed.)*. Wiley. ISBN 0-471-05669-3和Jordan,
    Michael I.; Bishop, Christopher M. (2004). *Neural Networks*. In Allen B. Tucker.
    Computer *Science Handbook, Second Edition (Section VII: Intelligent Systems)*.
    Boca Raton, FL: Chapman & Hall/CRC Press LLC. ISBN 1-58488-360-X。'
- en: Unsupervised learning example
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习示例
- en: In clustering, an algorithm groups objects into categories by analyzing similarities
    between input examples where similar objects or features are clustered and marked
    using circles around them.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类中，算法通过分析输入示例之间的相似性将对象分组到类别中，其中相似的对象或特征被聚类并用圆圈标记。
- en: 'Clustering uses include: **Search results grouping** such as grouping of customers,
    **anomaly detection** for suspicious pattern finding, **text categorization**
    for finding useful patterns in the tests, **social network analysis** for finding
    coherent groups, **data center computing clusters** for finding a way of putting
    related computers together to improve performance, **astronomic data analysis**
    for galaxy formation, and **real estate data analysis** for identifying neighborhoods
    based on similar features. Moreover, clustering uses unsupervised algorithms,
    which do not have the outputs in advance.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的用途包括：**搜索结果分组**，如客户分组，**异常检测**用于发现可疑模式，**文本分类**用于在测试中发现有用的模式，**社交网络分析**用于找到连贯的群体，**数据中心计算集群**用于找到将相关计算机放在一起以提高性能的方法，**天文数据分析**用于星系形成，以及**房地产数据分析**用于基于相似特征识别社区。此外，聚类使用无监督算法，事先没有输出。
- en: Clustering using the K-means algorithm begins by initializing all the coordinates
    to centroids. Note that Spark also supports other clustering algorithms such as Gaussian
    mixture, **Power Iteration Clustering** (**PIC**), **Latent Dirichlet Allocation**
    (**LDA**), Bisecting k-means, and Streaming k-means. Whereas, the Gaussian mixture
    is mainly used for expectation minimization as an optimization algorithm, the
    LDA, on the other hand, is used for the document classification and clustering.
    PIC is used for the clustering vertices of a graph given pairwise similarities
    as edge properties. Bisecting K-means is faster than the regular K-means, but
    it will generally produce a different clustering. Therefore, to keep the discussion
    simpler we will use the K-means algorithm for our purposes.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用K均值算法进行聚类是通过将所有坐标初始化为质心开始的。请注意，Spark还支持其他聚类算法，如**高斯混合**，**幂迭代聚类**（**PIC**），**潜在狄利克雷分配**（**LDA**），二分K均值和流式K均值。而高斯混合主要用于期望最小化作为优化算法，另一方面，LDA用于文档分类和聚类。PIC用于根据边属性的成对相似性对图的顶点进行聚类。二分K均值比常规K均值更快，但通常会产生不同的聚类。因此，为了使讨论更简单，我们将在我们的目的中使用K均值算法。
- en: Interested readers should refer the Spark ML and Spark MLlib based clustering
    techniques at [https://spark.apache.org/docs/latest/ml-clustering.html](https://spark.apache.org/docs/latest/ml-clustering.html)
    and [https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html)
    web pages respectively to get more insights. With every pass of the algorithm,
    each point is assigned to its nearest centroid based on some distance metric,
    usually the **Euclidean distance**.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣的读者应该参考Spark ML和基于Spark MLlib的聚类技术，分别在[https://spark.apache.org/docs/latest/ml-clustering.html](https://spark.apache.org/docs/latest/ml-clustering.html)和[https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html)网页上获取更多见解。在算法的每次迭代中，根据某种距离度量，通常是**欧几里得距离**，每个点都被分配到其最近的质心。
- en: Note that there are other ways to calculate the distance, for example, the **Chebyshev
    distance** is used to measure the distance by considering only the most significant
    dimensions. The **Hamming distance algorithm** is used to identify the difference
    bit by bit of two strings. The **Mahalanobis distance** is used to normalize the
    covariance matrix to make the distance metric scale-invariant.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，还有其他计算距离的方法，例如，**切比雪夫距离**用于仅考虑最重要的维度来测量距离。**汉明距离算法**用于逐位识别两个字符串的不同。**马哈alanobis距离**用于将协方差矩阵标准化，使距离度量在尺度上不变。
- en: The **Manhattan distance** is used to measure the distance following only axis-aligned
    directions. The **Minkowski distance algorithm** is used to make the Euclidean
    distance, Manhattan distance, and Chebyshev distance generalize. The **Haversine
    distance** is used to measure the great-circle distances between two points on
    a sphere from their longitudes and latitudes. Considering these distance measuring
    algorithms, it is clear that the Euclidean distance algorithm would be the most
    appropriate to solve our problem.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**曼哈顿距离**用于仅遵循轴对齐方向的距离。**闵可夫斯基距离算法**用于使欧几里德距离、曼哈顿距离和切比雪夫距离泛化。**Haversine距离**用于测量球面上两点之间的大圆距离，根据它们的经度和纬度。考虑这些距离测量算法，很明显，欧几里得距离算法将是解决我们问题最合适的方法。'
- en: 'The centroids are then updated to be the centers of all the points assigned
    to it in that pass. This repeats until there is a minimum change in the centers.
    The k-means algorithm is an iterative algorithm and works in two steps:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后更新质心为该通行中分配给它的所有点的中心。这一过程重复，直到中心发生最小变化。K均值算法是一个迭代算法，分为两步：
- en: '**Cluster assignment step**: This algorithm will go through each data point
    and, depending upon which centroid it is nearer to, it will be assigned that centroid
    and, in turn, the cluster it represents'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**簇分配步骤**：该算法将遍历每个数据点，并根据它离哪个质心更近来分配该质心，进而分配它代表的簇。'
- en: '**Move centroid step**: This algorithm will take each centroid and move it
    to the mean of the data points in the cluster'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移动质心步骤**：该算法将取每个质心并将其移动到簇中数据点的平均值'
- en: Unsupervised learning with Spark - an example
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spark进行无监督学习-一个例子
- en: 'We will use the *Saratoga NY Homes* downloaded from the URL [http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html](http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html) to
    demonstrate an example of clustering as an unsupervised learning technique using
    Spark in Java. The dataset contains several features as follows: Price, Lot Size,
    Waterfront, Age, Land Value, New Construct, Central Air, Fuel Type, Heat Type,
    Sewer Type, Living Area, Pct.College, Bedrooms, Fireplaces, Bathrooms, and the
    number of Rooms. However, among those columns, we have shown only some selected
    columns in *Table 3*. Note that the original dataset was downloaded and later
    on converted into a corresponding text file as a tab delimiter:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用从URL [http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html](http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html)
    下载的*Saratoga NY Homes*来演示使用Java中的Spark作为无监督学习技术的聚类的一个例子。数据集包含以下几个特征：价格、地块大小、水边、年龄、土地价值、新建、中央空调、燃料类型、加热类型、下水道类型、居住面积、大学百分比、卧室、壁炉、浴室和房间数量。然而，在这些列中，我们只在*表3*中显示了一些选择的列。请注意，原始数据集是下载的，后来转换为相应的文本文件作为制表符分隔符：
- en: '| **Price** | **Lot Size** | **Water Front** | **Age** | **Land Value** | **Rooms**
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| **价格** | **地块大小** | **水边** | **年龄** | **土地价值** | **房间** |'
- en: '| 132500 | 0.09 | 0 | 42 | 5000 | 5 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 132500 | 0.09 | 0 | 42 | 5000 | 5 |'
- en: '| 181115 | 0.92 | 0 | 0 | 22300 | 6 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 181115 | 0.92 | 0 | 0 | 22300 | 6 |'
- en: '| 109000 | 0.19 | 0 | 133 | 7300 | 8 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 109000 | 0.19 | 0 | 133 | 7300 | 8 |'
- en: '| 155000 | 0.41 | 0 | 13 | 18700 | 5 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 155000 | 0.41 | 0 | 13 | 18700 | 5 |'
- en: '| 86060 | 0.11 | 0 | 0 | 15000 | 3 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 86060 | 0.11 | 0 | 0 | 15000 | 3 |'
- en: '| 120000 | 0.68 | 0 | 31 | 14000 | 8 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 120000 | 0.68 | 0 | 31 | 14000 | 8 |'
- en: '| 153000 | 0.4 | 0 | 33 | 23300 | 8 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 153000 | 0.4 | 0 | 33 | 23300 | 8 |'
- en: '| 170000 | 1.21 | 0 | 23 | 146000 | 9 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 170000 | 1.21 | 0 | 23 | 146000 | 9 |'
- en: '| 90000 | 0.83 | 0 | 36 | 222000 | 8 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 90000 | 0.83 | 0 | 36 | 222000 | 8 |'
- en: '| 122900 | 1.94 | 0 | 4 | 212000 | 6 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 122900 | 1.94 | 0 | 4 | 212000 | 6 |'
- en: '| 325000 | 2.29 | 0 | 123 | 126000 | 12 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 325000 | 2.29 | 0 | 123 | 126000 | 12 |'
- en: 'Table 3: Sample data from the "Saratoga NY Homes" dataset'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：来自“Saratoga NY Homes”数据集的样本数据
- en: 'We further took only the first two features (that is, Price and Lot Size) using
    the Spark feature learning algorithm presented in the previous chapter for simplicity.
    Our target is to show an exploratory analysis based on these two features for
    possible neighborhoods of the house located in the same area. First, look at the
    basic scatter plot diagram based on the value in the dataset:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步仅使用前两个特征（即价格和地块大小），以简化前一章中介绍的Spark特征学习算法。我们的目标是基于这两个特征对位于同一区域的房屋可能的邻域进行探索性分析。首先，看一下基于数据集中的值的基本散点图：
- en: '![Unsupervised learning with Spark - an example](img/00014.jpeg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark进行无监督学习-一个例子](img/00014.jpeg)'
- en: 'Figure 8: Cluster of the neighborhoods'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：邻域的簇
- en: 'It''s clearly seen that there are four cluster on the plot marked as circles
    in *Figure 8*. However, finding a number of clusters is a tricky task. Here, we
    have the advantage of visual inspection, which is not available for data on hyperplanes
    or multidimensional data. Now we need to find the same result using Spark. For
    simplicity, we will use the K-means clustering API of Spark. The use of raw data
    and finding feature vectors is shown in *Figure 9*:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，在*图8*中标有圆圈的图中有四个簇。然而，确定簇的数量是一项棘手的任务。在这里，我们有视觉检查的优势，这对于超平面或多维数据上的数据是不可用的。现在我们需要使用Spark找到相同的结果。为简单起见，我们将使用Spark的K均值聚类API。原始数据的使用和查找特征向量如*图9*所示：
- en: '![Unsupervised learning with Spark - an example](img/00015.jpeg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark进行无监督学习-一个例子](img/00015.jpeg)'
- en: 'Figure 9: Unsupervised learning using Spark'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：使用Spark进行无监督学习
- en: K-means clustering of the neighborhood
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 邻域的K均值聚类
- en: 'Before performing the feature extraction, we need to load and parse the Saratoga
    NY Homes dataset. This step also includes: loading packages and related dependencies,
    reading the dataset as RDD, model training and prediction, collecting the local
    parsed data, and comparing clustering.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行特征提取之前，我们需要加载和解析Saratoga NY Homes数据集。这一步还包括：加载包和相关依赖项，将数据集读取为RDD，模型训练和预测，收集本地解析的数据，并进行聚类比较。
- en: '**Step 1: Import statistics and related classes**'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：导入统计和相关类**'
- en: 'Here is the code to import statistics and related classes:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是导入统计和相关类的代码：
- en: '[PRE28]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Step 2: Create the Spark session**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：创建Spark会话**'
- en: 'Here is the code to create a Spark session:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建Spark会话的代码：
- en: '[PRE29]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Step 3: Load the Saratoga NY Homes.txt**'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：加载Saratoga NY Homes.txt**'
- en: 'Read, parse, and create RDDs from the dataset:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集中读取、解析和创建RDD：
- en: '[PRE30]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**Step 4:Transform the data into an RDD of dense vectors**'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：将数据转换为密集向量的RDD**'
- en: 'If you follow the preceding step carefully, actually we have created the normal
    RDD. Therefore, that RDD has to be converted into the corresponding `JavaRDD`
    before mapping into a dense vector using Vector:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细遵循前面的步骤，实际上我们已经创建了普通的RDD。因此，在将其映射为密集向量之前，该RDD必须转换为相应的`JavaRDD`：
- en: '[PRE31]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Step 5: Train the model**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：训练模型**'
- en: 'Train the model by specifying four clusters and five iterations. Just refer
    the following code for doing that:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定四个聚类和五次迭代来训练模型。只需参考以下代码来执行：
- en: '[PRE32]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You should receive the results as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该收到以下结果：
- en: '[PRE33]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**Step 6: Show the cluster centers**'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：显示聚类中心**'
- en: '[PRE34]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding code should produce the center of the clusters as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码应该产生如下的聚类中心：
- en: '[PRE35]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '**Step 7: Evaluate the model error rate**'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：评估模型错误率**'
- en: '[PRE36]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This should produce the result as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该产生如下结果：
- en: '[PRE37]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '**Step 8: Predict the cluster for the second element**'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8：预测第二个元素的聚类**'
- en: '[PRE38]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Output prediction: 0'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 输出预测：0
- en: '**Step 9: Stop the Spark session**'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9：停止Spark会话**'
- en: 'Stop the Spark session using the `stop()` method as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`stop()`方法停止Spark会话如下：
- en: '[PRE39]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '**Step 10: Cluster comparing**'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤10：聚类比较**'
- en: 'Now let''s compare the cluster assignments by k-means versus the ones we have
    done individually. The k-means algorithm gives the cluster IDs starting from 0\.
    Once you inspect the data, you find out the following mapping between the A to
    D cluster IDs we gave versus K-means in Table 4:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们比较k-means与我们单独完成的聚类分配。k-means算法从0开始给出聚类ID。一旦您检查数据，您会发现我们在表4中给出的A到D聚类ID与k-means之间的以下映射：
- en: '| **Cluster name** | **Cluster number** | **Cluster assignment** |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| **聚类名称** | **聚类编号** | **聚类分配** |'
- en: '| A | 3 | A=>3 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| A | 3 | A=>3 |'
- en: '| B | 1 | B=>1 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| B | 1 | B=>1 |'
- en: '| C | 0 | C=>0 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| C | 0 | C=>0 |'
- en: '| D | 2 | D=>2 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| D | 2 | D=>2 |'
- en: 'Table 4: Cluster assignment for the neighbourhood K-means clustering example'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：邻域k-means聚类示例的聚类分配
- en: 'Now, let''s pick some of the data from different parts of the chart and predict
    which cluster it belongs to. Let''s look at the house (say 1 as an example) data,
    which has a lot size of 876 square feet and is priced at $665K:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从图表的不同部分挑选一些数据，并预测它属于哪个聚类。让我们看一下房屋（以1为例）的数据，它的占地面积为876平方英尺，售价为65.5万美元：
- en: '[PRE40]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[Output] Prediction: 2'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[输出] 预测：2'
- en: That means the house with the preceding properties falls in the cluster 2\.
    You can test the prediction capability with more data of course. Let's do some
    neighborhood analysis to see what meaning these clusters carry. We can assume
    that most of the houses in cluster 3 are near downtown. The cluster 2 houses are
    on hilly terrain for example.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着具有前述属性的房屋属于聚类2。当然，您可以通过更多数据测试预测能力。让我们进行一些邻域分析，看看这些聚类承载着什么含义。我们可以假设聚类3中的大多数房屋都靠近市中心。例如，聚类2中的房屋位于多山的地形上。
- en: In this example, we dealt with a very small set of features; common sense and
    visual inspection would also lead us to the same conclusions. However, if you
    want to acquire more accuracy, of course, you should construct more meaningful
    features by not only considering only the lot size and the house price but other
    features like the number of rooms, house age, land value, heating type, and so
    on.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们处理了一组非常少的特征；常识和视觉检查也会导致我们得出相同的结论。然而，如果您想获得更准确的结果，当然，您应该构建更有意义的特征，不仅考虑占地面积和房价，还要考虑其他特征，如房间数量、房龄、土地价值、供暖类型等。
- en: However, it would not be wise to include the *Waterfront* as a meaningful feature
    since no house has a water garden in front of the house in this example. We will
    provide a detailed analysis towards the better accuracy of meaningful a prediction
    in next chapter, where we will demonstrate these considerations.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将*滨水*作为一个有意义的特征是不明智的，因为在这个例子中没有房子的前面有水上花园。我们将在下一章节中对更准确地预测的更有意义的特征的准确性进行详细分析。
- en: The beauty of the k-means algorithm is that it does the clustering on the data
    with an unlimited number of features. It is a great tool to use when you have
    a raw data and would like to know the patterns in that data. However, deciding
    pn the number of clusters prior to doing the experiment might not be successful
    but sometimes may lead to an overfitting problem or an underfitting problem.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: k-means算法的美妙之处在于它可以对具有无限特征的数据进行聚类。当您有原始数据并想了解数据中的模式时，这是一个很好的工具。然而，在进行实验之前决定聚类的数量可能不成功，有时可能会导致过拟合问题或欠拟合问题。
- en: Tip
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: To overcome the aformentioned limitation of the K-means, we have some more robust
    algorithms like **Markov Chain Monte Carlo** (**MCMC** , see also [https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo))
    presented in Tribble, Seth D., *Markov chain Monte Carlo algorithms using completely
    uniformly distributed driving sequences*, Diss. Stanford University, 2007\. Moreover,
    a more technical discussion can be found at the URL [http://www.autonlab.org/tutorials/kmeans11.pdf](http://www.autonlab.org/tutorials/kmeans11.pdf)
    too.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服K均值的上述局限性，我们有一些更健壮的算法，如**马尔可夫链蒙特卡洛**（MCMC，也见[https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)）在Tribble,
    Seth D.，*Markov chain Monte Carlo algorithms using completely uniformly distributed
    driving sequences*，2007年斯坦福大学博士论文中提出。此外，更多技术讨论可以在[http://www.autonlab.org/tutorials/kmeans11.pdf](http://www.autonlab.org/tutorials/kmeans11.pdf)的网址中找到。
- en: Recommender system
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统
- en: 'A recommender system is an original killer application which is a subclass
    of an information filtering system that looks to predict the rating or preference
    from the users that they usually provide to an item. The concept of recommender
    systems has become very common in recent years and has been subsequently applied
    in different applications. The most popular ones are probably products (for example,
    movies, music, books, research articles), news, search queries, social tags, and
    so on). Recommender systems can be typed into four categories as stated in [Chapter
    2](part0023_split_000.html#LTSU2-5afe140a04e845e0842b44be7971e11a "Chapter 2. Machine
    Learning Best Practices"), *Machine Learning Best Practices.* These are shown
    in *Figure 10*:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是一种原始的杀手级应用程序，是信息过滤系统的一个子类，旨在预测用户通常对项目提供的评分或偏好。推荐系统的概念近年来变得非常普遍，并随后被应用于不同的应用程序。最流行的可能是产品（例如电影、音乐、书籍、研究文章）、新闻、搜索查询、社交标签等。推荐系统可以分为四类，如[第2章](part0023_split_000.html#LTSU2-5afe140a04e845e0842b44be7971e11a
    "第2章。机器学习最佳实践")中所述，*机器学习最佳实践*。这些显示在*图10*中：
- en: '**The collaborative filtering system**: This is the accumulation of a consumer''s
    preferences and recommendations to other users based on likeness in behavioral
    patterns **Content-based systems**: Here the supervised machine learning is used
    to persuade a classifier to distinguish between interesting and uninteresting
    items for the users'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同过滤系统**：这是根据行为模式的相似性累积消费者的偏好和对其他用户的推荐**基于内容的系统**：这里使用监督机器学习来说服分类器区分用户感兴趣和不感兴趣的项目'
- en: '**Hybrid recommender systems**: This is a recent research and hybrid approach
    (that is, combining collaborative filtering and content-based filtering)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合推荐系统**：这是最近的研究和混合方法（即，结合协同过滤和基于内容的过滤）'
- en: '**Knowledge-based systems**: Here knowledge about users and products are used
    to understand what fulfils a user''s requirements, using a perception tree, decision
    support systems, and case-based reasoning:![Recommender system](img/00028.jpeg)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于知识的系统**：这里使用关于用户和产品的知识来理解用户的需求，使用感知树、决策支持系统和基于案例的推理：![推荐系统](img/00028.jpeg)'
- en: 'Figure 10: Hierarchy of the recommendation systems'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：推荐系统的层次结构
- en: 'From the technical viewpoint, we can further categorize them as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，我们可以进一步将它们分类如下：
- en: The item **hierarchy** is the weakest one where it is naively assuming that
    one item is correlated to another, for example, if you buy a printer, it is more
    likely that you will buy the ink. Previously this approach was used by **BestBuy**
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物品**层次结构**是最弱的，它天真地假设一个物品与另一个物品相关，例如，如果你买了打印机，你更有可能购买墨水。之前**BestBuy**使用过这种方法
- en: '**Attribute-based recommendation**: Assumes that you like action movies starring
    Sylvester Stallone, therefore, you might be like the Rambo series. **Netflix**
    used to use this approach'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于属性的推荐**：假设你喜欢史泰龙主演的动作电影，因此你可能会喜欢兰博系列。Netflix曾经使用过这种方法'
- en: '**Collaborative filtering** (U**ser-user similarity):** This assumes and exemplifies
    those people like you who brought baby milk also bought diapers. Target use this
    approach'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同过滤**（用户-用户相似性）：假设并且举例说，那些像你一样购买了婴儿奶粉的人也购买了尿布。Target使用这种方法'
- en: '**Collaborative filtering** (**Item-item similarity)**: This assumes and exemplifies
    that people who like Godfather series also like Scarface. Netflix currently uses
    this approach'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同过滤**（物品-物品相似性）：假设并且举例说，喜欢教父系列的人也喜欢《疤面煞星》。Netflix目前使用这种方法'
- en: '**Social, interest and graph-based approach**: This assumes for example that
    your friend who likes Michel Jackson will also like *Just Beat It*. The tech giant
    like **LinkedIn** and **Facebook** use this approach'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社交、兴趣和基于图的方法**：例如，假设喜欢迈克尔·杰克逊的朋友也会喜欢《Just Beat It》。像**LinkedIn**和**Facebook**这样的科技巨头使用这种方法'
- en: '**Model-based approach**: This uses an advanced algorithm such as **SVM**,
    **LDA**, and **SVD** based on the implicit features'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型的方法**：这使用高级算法，如**SVM**、**LDA**和**SVD**基于隐含特征'
- en: 'As shown in *Figure 11*, the model-based recommender system that widely used
    advanced algorithms such as SVM, LDA, or SVD is the most robust approach in the
    recommender system class:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图11*所示，基于模型的推荐系统广泛使用高级算法，如SVM、LDA或SVD，是推荐系统类中最健壮的方法：
- en: '![Recommender system](img/00029.jpeg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![推荐系统](img/00029.jpeg)'
- en: 'Figure 11: The recommender system from the technical point of view'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：从技术角度看的推荐系统
- en: Collaborative filtering in Spark
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark中的协同过滤
- en: 'As already mentioned, the collaborative filtering techniques are commonly used
    for recommender systems. However, Spark MLlib currently supports model-based collaborative
    filtering only. Here, users and products are described by a small set of latent
    factors. The latent factors are later used for making the prediction of the missing
    entries. According to the Spark API reference for the collaborative filtering
    on [http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html):
    the **Alternating Least Squares** (**ALS**) (also known as non-linear least square,that
    is, NLS; see more at [https://en.wikipedia.org/wiki/Non-linear_least_squares](https://en.wikipedia.org/wiki/Non-linear_least_squares))
    algorithm is used to learn these latent factors by considering the following parameters:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，协同过滤技术通常用于推荐系统。然而，Spark MLlib目前仅支持基于模型的协同过滤。在这里，用户和产品由一小组潜在因素描述。这些潜在因素后来用于预测缺失的条目。根据Spark
    API参考协同过滤[http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html)：**交替最小二乘法**（**ALS**）（也称为非线性最小二乘法，即NLS；更多信息请参见[https://en.wikipedia.org/wiki/Non-linear_least_squares](https://en.wikipedia.org/wiki/Non-linear_least_squares)）算法用于通过考虑以下参数来学习这些潜在因素：
- en: '`numBlocks` is the number of blocks used for the parallelized computation using
    the native LAPACK'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numBlocks`是使用本机LAPACK进行并行计算的块数'
- en: '`rank` is the number of latent factors during the machine learning model building'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank`是在构建机器学习模型期间的潜在因素的数量'
- en: '`iterations` are the number of iterations needed to gain more accurate predictions'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iterations`是需要进行更准确预测的迭代次数'
- en: '`lambda` signifies the regularization parameter for the ALS algorithm'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lambda`表示ALS算法的正则化参数'
- en: '`implicitPrefs` specifies which feedback to be used (explicit feedback ALS
    variant or one adapted for implicit feedback data)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`implicitPrefs`指定要使用的反馈（显式反馈ALS变体或适用于隐式反馈数据的变体）'
- en: '`alpha` specifies the baseline confidence in preference observations for the
    ALS algorithm'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`指定ALS算法中偏好观察的基线置信度'
- en: At first, the ALS, which is an iterative algorithm, is used to model the rating
    matrix as the multiplication of low-ranked users and product factors. After that,
    the learning task is done by using these factors by minimizing the reconstruction
    error of the observed ratings.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，ALS是一种迭代算法，用于将评分矩阵建模为低秩用户和产品因子的乘积。之后，通过最小化观察到的评分的重建误差来使用这些因子进行学习任务。
- en: However, the unknown ratings can successively be calculated by multiplying these
    factors together. The approach for the move recommendation or any other recommendation
    based on the collaborative filtering technique used in the Spark MLlib has been
    proven a high performer with high prediction accuracy and is scalable for the
    billions of ratings on commodity clusters used by companies such as Netflix. In
    following this way, a company such as Netflix can recommend movies to its subscribers
    based on the predicted ratings. The ultimate target is to increase the sales and
    of course the customer satisfaction.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，未知的评分可以通过将这些因素相乘来逐步计算。在Spark MLlib中使用的协同过滤技术进行的移动推荐或其他推荐的方法已被证明是一个高性能的方法，具有高预测准确性，并且可扩展到像Netflix这样的公司使用的商品集群上的数十亿个评分。按照这种方式，Netflix这样的公司可以根据预测的评分向其订阅者推荐电影。最终目标是增加销售额，当然也包括客户满意度。
- en: For brevity and page limitation, we will not show the movie recommendations
    using the collaborative filtering approach in this chapter. However, a step-by-step
    example using Spark will be shown in [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced*
    *Machine Learning with Streaming and Graph Data*.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁和页面限制，我们将不在本章中展示使用协同过滤方法进行电影推荐。但是，将在[第9章](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "第9章。流式数据和图数据的高级机器学习")中展示使用Spark的逐步示例，*流式数据和图数据的高级机器学习*。
- en: Tip
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'For the time being, interested readers are advised to visit the Spark website
    for the latest API and codes for the same at this URL: [http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html),
    where an example has been presented to show the sample movie recommendations using
    the ALS algorithm.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，建议感兴趣的读者访问Spark网站获取最新的API和相同代码，网址为：[http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html)，其中提供了一个示例，展示了使用ALS算法进行样本电影推荐。
- en: Advanced learning and generalizations
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级学习和泛化
- en: In this section, we will discuss some advanced aspects of learning, for example
    how we can generalize the supervised learning techniques for semi-supervised learning,
    active learning, structured prediction, and reinforcement learning. Moreover,
    reinforcement and semi-supervised learning will be discussed in brief.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些学习的高级方面，例如如何将监督学习技术泛化为半监督学习、主动学习、结构化预测和强化学习。此外，将简要讨论强化学习和半监督学习。
- en: Generalizations of supervised learning
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习的泛化
- en: 'There are several ways in which the standard supervised learning problem can
    be generalized:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 标准监督学习问题可以泛化的几种方式：
- en: '**Semi-supervised learning**: In this generalization technique, only the required
    output values for selected features are provided for a subset of the training
    data to build and evaluate the machine learning model. On the other hand, the
    remaining data is kept unchanged or unlabeled.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半监督学习**：在这种泛化技术中，仅为所选特征的一部分训练数据提供所需的输出值，以构建和评估机器学习模型。另一方面，其余数据保持不变或未标记。'
- en: '**Active learning**: In contrast, in active learning, algorithms typically
    interactively collect new features by making queries to a human user instead of
    assuming all the training features are given. Consequently, the queries used here
    are based on unlabeled data. Interestingly, it is also an example that combines
    semi-supervised learning with active learning.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主动学习**：相比之下，在主动学习中，算法通常通过向人类用户提出查询来交互地收集新特征，而不是假设所有训练特征都已给出。因此，这里使用的查询是基于未标记数据的。有趣的是，这也是将半监督学习与主动学习相结合的一个例子。'
- en: '**Structured prediction**: Sometimes the desired features need to be extracted
    or selected from complex objects like a parse tree or a labelled graph, and then
    the standard supervised or unsupervised methods must be improved towards adaptability
    for making it generalized. To be more precise, for example when a supervised machine
    learning technique tries to predict structured or unstructured texts such as translating
    NLP sentences into syntactic representation, structured prediction evolves that
    need to handle a large-scale parse tree. To make this task easier, often the structured
    SVMs or Markov logic networks or constrained conditional models are used that
    technically extend and update the classical supervised learning algorithms.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化预测**：有时需要从复杂对象（如解析树或带标签的图）中提取或选择所需的特征，然后必须改进标准监督或无监督方法以使其适应于泛化。更准确地说，例如，当监督机器学习技术尝试预测结构化或非结构化文本时，如将自然语言处理句子翻译成句法表示时，需要处理大规模解析树的结构化预测。为了简化这个任务，通常使用结构化SVM或马尔可夫逻辑网络或受限条件模型，这些技术上扩展和更新了经典的监督学习算法。'
- en: '**Learning to rank**: Machine-learned ranking is required when the input itself
    is a subset of objects and the desired output is a ranking of those objects, then
    the standard methods must be extended or improved similarly to the structure prediction
    technique.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习排名**：当输入本身是对象的子集并且期望的输出是这些对象的排名时，必须类似于结构预测技术来扩展或改进标准方法。'
- en: Tip
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Interested readers can refer to these two URLs: [https://en.wikipedia.org/wiki/Learning_to_rank](https://en.wikipedia.org/wiki/Learning_to_rank)
    and [https://en.wikipedia.org/wiki/Structured_prediction](https://en.wikipedia.org/wiki/Structured_prediction), where
    a more details discussion can be found.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的读者可以参考以下两个网址：[https://en.wikipedia.org/wiki/Learning_to_rank](https://en.wikipedia.org/wiki/Learning_to_rank)
    和 [https://en.wikipedia.org/wiki/Structured_prediction](https://en.wikipedia.org/wiki/Structured_prediction)，在这里可以找到更详细的讨论。
- en: Summary
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We have discussed some supervised, unsupervised, and recommender systems from
    a theoretical and Spark's perspective. However, there are numerous examples for
    the supervised, unsupervised, reinforcement or recommendation systems too. Nevertheless,
    we have tried to present some simple examples for the sake of simplicity.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从理论和Spark的角度讨论了一些监督、无监督和推荐系统。然而，监督、无监督、强化或推荐系统也有许多例子。尽管如此，我们已经尽力提供一些简单的例子以求简单。
- en: We will provide more insights on these examples in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*. More feature incorporation, extraction, selection
    using Spark ML and Spark MLlib pipelines, model scaling, and tuning will be discussed
    too. We also intend to provide some examples including data collection to model
    building and prediction.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第6章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d "第6章。构建可扩展的机器学习管道")*构建可扩展的机器学习管道*中提供更多关于这些示例的见解。还将讨论使用Spark
    ML和Spark MLlib管道进行更多特征合并、提取、选择、模型扩展和调整。我们还打算提供一些包括数据收集到模型构建和预测的示例。
