- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Understanding Sequential Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解顺序模型
- en: A sequence works in a way a collection never can.
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个序列的工作方式是一个集合无法做到的。
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —George Murray
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —George Murray
- en: This chapter covers an important class of machine learning models, the sequential
    models. A defining characteristic of such models is that the processing layers
    are arranged in such a way that the output of one layer is the input to the other.
    This architecture makes them perfect to process sequential data. Sequential data
    is the type of data that consists of ordered series of elements such as a sentence
    in a document or a time series of stock market prices.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了机器学习模型中的一个重要类别——顺序模型。此类模型的一个定义特征是，处理层以这样的方式排列：一层的输出是另一层的输入。这种架构使它们非常适合处理顺序数据。顺序数据是由有序元素组成的数据类型，例如文档中的一句话或股市价格的时间序列。
- en: In this chapter, we will start with understanding the characteristics of sequential
    data. Then, we will present the working of RNNs and how they can be used to process
    sequential data. Next, we will learn how we can address the limitations of RNN
    through GRU without scarifying accuracy. Then, we will discuss the architecture
    of LSTM. Finally, we will compare different sequential modeling architectures
    with a recommendation on when to use which one.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从理解顺序数据的特征开始。然后，我们将介绍RNN的工作原理以及如何使用它们处理顺序数据。接下来，我们将学习如何通过GRU解决RNN的局限性而不牺牲准确性。然后，我们将讨论LSTM的架构。最后，我们将比较不同的顺序建模架构，并推荐在何时使用哪种架构。
- en: 'In this chapter, we will go through the following concepts:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下概念：
- en: Understanding sequential data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解顺序数据
- en: How RNNs can process sequential data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN如何处理顺序数据
- en: Addressing the limitations of RNNs through GRUs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过GRU解决RNN的局限性
- en: Understanding LSTM
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解LSTM
- en: Let us start by first looking into the characteristics of sequential data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解顺序数据的特征。
- en: Understanding sequential data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解顺序数据
- en: Sequential data is a specific type of data structure where the order of the
    elements matters, and each element has a relational dependency on its predecessors.
    This “sequential behavior” is distinct because it conveys information not just
    in the individual elements but also in the pattern or sequence in which they occur.
    In sequential data, the current observation is not only influenced by external
    factors but also by previous observations in the sequence. This dependency forms
    the core characteristic of sequential data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序数据是一种特殊的数据结构，其中元素的顺序至关重要，每个元素与其前面的元素之间存在关联依赖性。这种“顺序行为”非常独特，因为它不仅在单独的元素中传递信息，还在它们发生的模式或顺序中传递信息。在顺序数据中，当前的观察不仅受到外部因素的影响，还受到序列中之前观察值的影响。这种依赖性构成了顺序数据的核心特征。
- en: 'Understanding the different types of sequential data is essential to appreciate
    its broad applications. Here are the primary categories:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 理解不同类型的顺序数据对于理解其广泛应用至关重要。以下是主要的分类：
- en: '**Time series data**: This is a series of data points indexed or listed in
    time order. The value at any point in time is dependent on the past values. Time
    series data is widely used in various fields, including economics, finance, and
    healthcare.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列数据**：这是按时间顺序索引或列出的数据点序列。任何时间点的值都依赖于过去的值。时间序列数据广泛应用于经济学、金融和医疗等各个领域。'
- en: '**Textual data**: Text data is also sequential in nature, where the order of
    words, sentences, or paragraphs can convey meaning. **Natural language processing**
    (**NLP**) leverages this sequential property to analyze and interpret human languages.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本数据**：文本数据本质上也是顺序的，其中单词、句子或段落的顺序可以传递意义。**自然语言处理**（**NLP**）利用这一顺序特性来分析和解释人类语言。'
- en: '**Spatial-temporal data**: This involves data that captures both spatial and
    temporal relationships, such as weather patterns or traffic flow over time in
    a specific geographical area.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时空数据**：这类数据捕捉了空间和时间之间的关系，例如特定地理区域内的天气模式或交通流量随时间变化的情况。'
- en: 'Here’s how these types of sequential data manifest in real-world scenarios:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这些类型的顺序数据在现实世界场景中的表现方式：
- en: '**Time series data**: This type of data is clearly illustrated through financial
    market trends, where stock prices constantly vary in response to ongoing market
    dynamics. Similarly, sociological studies might analyze birth rates, reflecting
    year-to-year changes influenced by factors like economic conditions and social
    policies.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列数据**：这种数据类型通过金融市场趋势得到清晰体现，股票价格随着持续的市场动态不断变化。类似地，社会学研究可以分析出生率，反映出受经济条件和社会政策等因素影响的年度变化。'
- en: '**Textual data**: The sequential nature of text is paramount in literary and
    journalistic works. In novels, news articles, or essays, the specific ordering
    of words, sentences, and paragraphs constructs narratives and arguments, giving
    the text meaning beyond individual words.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本数据**：文本的顺序性在文学和新闻作品中至关重要。在小说、新闻文章或论文中，单词、句子和段落的特定排列构建了叙事和论证，赋予文本超越单词本身的意义。'
- en: '**Spatial-temporal data**: Areas in which this data type is vital are urban
    development and environmental studies. For instance, housing prices across different
    regions might be tracked over time to identify economic trends, while meteorological
    studies might monitor weather changes at specific geographical locations to forecast
    patterns and natural events.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时空数据**：这种数据类型在城市发展和环境研究中至关重要。例如，可以跟踪不同地区的房价随时间的变化，以识别经济趋势，而气象研究可以监测特定地理位置的天气变化，预测模式和自然事件。'
- en: These real-world examples demonstrate how the inherent sequential behavior in
    different types of data can be leveraged to provide insights and drive decisions
    across various domains.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些现实世界的例子展示了不同类型数据中固有的顺序行为如何被利用以提供见解并推动各个领域的决策。
- en: In deep learning, handling sequential data requires specialized neural network
    architectures like sequential models. These models are designed to capture and
    exploit the temporal dependencies that inherently exist among the elements of
    sequential data. By recognizing these dependencies, sequential models provide
    a robust framework for creating more nuanced and effective machine learning models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，处理序列数据需要像序列模型这样的专门神经网络架构。这些模型旨在捕获和利用序列数据元素之间固有的时间依赖关系。通过识别这些依赖关系，序列模型为创建更为细致和有效的机器学习模型提供了坚实的框架。
- en: In summary, sequential data is a rich and complex type of data that finds applications
    across diverse domains. Recognizing its sequential nature, understanding its types,
    and leveraging specialized models enable data scientists to draw deeper insights
    and build more powerful predictive tools. Before we study the technical details,
    let us start by looking at the history of sequential modeling techniques.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，序列数据是一种在各个领域中应用广泛的丰富而复杂的数据类型。认识其顺序性、理解其类型，并利用专业模型，使数据科学家能够深入洞察并构建更强大的预测工具。在我们研究技术细节之前，让我们先来看一看序列建模技术的历史。
- en: Let us study different types of sequential models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来研究不同类型的序列模型。
- en: Types of sequence models
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列模型的类型
- en: Sequential models are classified into various categories by examining the kind
    of data they handle, both in terms of input and output. This classification takes
    into account the specific nature of the data being used (like textual information,
    numerical data, or time-based patterns), and also how this data evolves or transforms
    from the beginning of the process to the end. By delving into these characteristics,
    we can identify three principal types of sequence models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查它们处理的数据类型（如文本信息、数值数据或基于时间的模式），以及这些数据从过程开始到结束如何演变或转换，将序列模型分类为各种类别。通过深入了解这些特征，我们可以确定三种主要类型的序列模型。
- en: One-to-many
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一对多
- en: In one-to-many sequence models, a singular event or input can initiate the generation
    of an entire sequence. This unique attribute opens doors to a wide range of applications,
    but it also leads to complexities in training and implementation. The one-to-many
    sequence models offer exciting opportunities but come with inherent complexities
    in training and execution. As generative AI continues to advance, these models
    are likely to play a pivotal role in shaping creative and customized solutions
    across various domains.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在一对多序列模型中，单一事件或输入可以启动整个序列的生成。这个独特的特性为广泛的应用领域打开了大门，但也带来了训练和实施的复杂性。一对多序列模型提供了令人兴奋的机会，但也伴随着训练和执行中的固有复杂性。随着生成性AI的不断进步，这些模型很可能在塑造各个领域的创造性和定制化解决方案中发挥关键作用。
- en: 'The key to harnessing their potential lies in understanding their capabilities
    and recognizing the intricacies of training and implementation. The one-to-many
    sequence model is shown in *Figure 10.1*:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 发挥其潜力的关键在于理解其能力，并识别训练和实施中的复杂性。一对多序列模型如*图10.1*所示：
- en: '![A diagram of a flowchart  Description automatically generated](img/B18046_10_01.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![A diagram of a flowchart  Description automatically generated](img/B18046_10_01.png)'
- en: 'Figure 10.1: One-to-many sequential model'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：一对多序列模型
- en: 'Let’s delve into the characteristics, capabilities, and challenges of one-to-many
    models:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨一对多模型的特性、能力和挑战：
- en: '**Wide range of applications**: The ability to translate a single input into
    a meaningful sequence makes one-to-many models versatile and powerful. They can
    be employed to write poetry, create art such as drawings and paintings, and even
    craft personalized cover letters for job applications.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广泛的应用范围**：将单一输入转化为有意义的序列的能力使得一对多模型既多才多艺又强大。它们可以用于写诗、创作艺术作品如绘画和图画，甚至为求职申请编写个性化的求职信。'
- en: '**Part of generative AI**: These models fall under the umbrella of generative
    AI, a burgeoning field that aims to create new content that is both coherent and
    contextually relevant. This is what allows them to perform such varied tasks as
    mentioned above.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成性AI的一部分**：这些模型属于生成性AI的范畴，这是一个新兴领域，旨在创建既连贯又符合上下文的新内容。这使得它们能够执行上述提到的各种任务。'
- en: '**Intensive training process**: Training one-to-many models is typically more
    time-consuming and computationally expensive compared to other sequence models.
    The reason for this lies in the complexity of translating a single input into
    a wide array of potential outputs. The model must learn not only the relationship
    between the input and the output but also the intricate patterns and structures
    inherent in the generated sequence.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化训练过程**：与其他序列模型相比，训练一对多模型通常更加耗时且计算开销更大。原因在于将单一输入转化为多种潜在输出的复杂性。模型不仅需要学习输入与输出之间的关系，还需要掌握生成序列中固有的复杂模式和结构。'
- en: Note that unlike one-to-one models, where a single input corresponds to a single
    output, or many-to-many models, where a sequence of inputs is mapped to a sequence
    of outputs, the one-to-many paradigm must learn to extrapolate a rich and structured
    sequence from a singular starting point. This requires a deeper understanding
    of the underlying patterns and can often necessitate more sophisticated training
    algorithms.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与一对一模型（一个输入对应一个输出）或多对多模型（一个输入序列对应一个输出序列）不同，一对多范式必须从单一的起点中推导出丰富且结构化的序列。这需要对底层模式有更深入的理解，并且通常需要更复杂的训练算法。
- en: The one-to-many approach isn’t without its challenges. Ensuring that the generated
    sequence maintains coherence, relevance, and creativity requires careful design
    and fine-tuning. It often demands a more extensive dataset and expert knowledge
    in the specific domain to guide the model’s training.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一对多方法并非没有挑战。确保生成的序列保持一致性、相关性和创造性需要精心设计和细致调优。它通常需要更大规模的数据集，并依赖于特定领域的专家知识来指导模型的训练。
- en: Many-to-one
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多对一
- en: Many-to-one sequential models are specialized tools in data analysis that take
    a sequence of inputs and convert them into a single output. This process of synthesizing
    multiple inputs into one concise output forms the core of the many-to-one model,
    allowing it to distill the essential characteristics of the data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 多对一序列模型是数据分析中的专业工具，它们将一系列输入转化为一个单一输出。将多个输入合成为一个简明输出的过程构成了多对一模型的核心，使其能够提炼数据的本质特征。
- en: 'These models have diverse applications, such as in sentiment analysis, where
    a sequence of words like a review or a post is analyzed to determine an overall
    sentiment such as positive, negative, or neutral. The many-to-one sequential model
    is shown in *Figure 10.2*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型有广泛的应用，例如情感分析，在这种应用中，像评论或帖子这样的词语序列被分析，以确定整体情感，例如正面、负面或中立。多对一的序列模型如*图10.2*所示：
- en: '![A diagram of a flowchart  Description automatically generated](img/B18046_10_02.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![A diagram of a flowchart  Description automatically generated](img/B18046_10_02.png)'
- en: 'Figure 10.2: Many-to-one sequential model'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：多对一序列模型
- en: The training process of many-to-one models is a complex yet integral part of
    their functionality. It distinguishes them from one-to-many models, whose focus
    is on creating a sequence from a single input. In contrast, many-to-one models
    must efficiently compress information, demanding careful selection of algorithms
    and precise tuning of parameters.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 多对一模型的训练过程是其功能中复杂但至关重要的一部分。它使得这些模型与一对多模型有所不同，后者的重点是从单一输入创建一个序列。相比之下，多对一模型必须高效地压缩信息，因此需要仔细选择算法并精确调节参数。
- en: Training a many-to-one model involves teaching it to identify the vital features
    of the input sequence and to represent them accurately in the output. This involves
    discarding irrelevant information, a task that requires intricate balancing. The
    training process also often necessitates specialized pre-processing and feature
    engineering, tailored to the specific nature of the input data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 训练多对一模型的过程包括教会它识别输入序列的关键特征，并准确地在输出中表示这些特征。这需要舍弃不相关的信息，这项任务需要精细的平衡。训练过程通常还需要针对输入数据的特定性质进行专业的预处理和特征工程。
- en: As discussed in the prior subsection, the training of many-to-one models may
    be more challenging than other types, requiring a deeper understanding of the
    underlying relationships in the data. Continuous monitoring of the model’s performance
    during training, along with a methodical selection of data and hyperparameters,
    is essential for the success of the model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一小节所讨论的，训练多对一模型可能比其他类型的模型更具挑战性，因为它需要更深入地理解数据中的潜在关系。在训练过程中持续监控模型的表现，以及系统地选择数据和超参数，对于模型的成功至关重要。
- en: Many-to-one models are noteworthy for their ability to simplify complex data
    into understandable insights, finding applications in various industries for tasks
    such as summarization, classification, and prediction. Although their design and
    training can be intricate, their unique ability to interpret sequential data provides
    inventive solutions to complex data analysis challenges.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 多对一模型因其能够将复杂数据简化为易于理解的洞察力而值得注意，广泛应用于各行各业的任务，如摘要、分类和预测。尽管它们的设计和训练可能非常复杂，但它们解读序列数据的独特能力为解决复杂数据分析挑战提供了创新的解决方案。
- en: Thus, many-to-one sequential models are vital instruments in contemporary data
    analysis, and understanding their particular training process is crucial for leveraging
    their capabilities fully. The training process, characterized by meticulous algorithm
    selection, parameter tuning, and domain expertise, sets these models apart. As
    the field progresses, many-to-one models will continue to offer valuable contributions
    to data interpretation and application.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，多对一序列模型是当代数据分析中至关重要的工具，理解其特定的训练过程对于充分利用其能力至关重要。训练过程的特点是精确的算法选择、参数调优和领域专业知识，这使得这些模型与众不同。随着该领域的发展，多对一模型将继续为数据解读和应用做出宝贵贡献。
- en: Many-to-many
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多对多
- en: 'This is a type of sequential model that takes sequential data as the input,
    processes it in some way, and then generates sequential data as the output. An
    example of many-to-many models is machine translation, where a sequence of words
    in one language is translated into a corresponding sequence in another language.
    An illustrative example of this would be the translation of English text into
    French. While there are numerous machine translation models that fall into this
    category, a prominent approach is the use of **Sequence-to-Sequence** (**Seq2Seq**)
    models, particularly with STM networks. Seq2Seq models with LSTM have become a
    standard method for tasks such as English-to-French translation and have been
    implemented in various NLP frameworks and tools. The many-to-many sequential model
    is shown in *Figure 10.3*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种序列模型，它将序列数据作为输入，经过某种方式处理后，生成序列数据作为输出。多对多模型的一个例子是机器翻译，其中一种语言的词序列被翻译成另一种语言中的对应序列。例如，英语文本翻译成法语就是一个典型的例子。虽然有许多机器翻译模型属于这一类别，但一种突出的做法是使用**序列到序列（Seq2Seq）**模型，特别是与LSTM网络配合使用。使用LSTM的Seq2Seq模型已经成为处理英语到法语翻译等任务的标准方法，并已在多个NLP框架和工具中实现。多对多序列模型如*图
    10.3*所示：
- en: '![A diagram of a flowchart  Description automatically generated](img/B18046_10_03.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的流程图示意图](img/B18046_10_03.png)'
- en: 'Figure 10.3: Many-to-many sequential model'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3：多对多序列模型
- en: Over the years, many algorithms have been developed to process and train machine
    learning models using sequential data. Let us start with studying how to represent
    sequential data with 3-dimensional data structures.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，已经开发出许多算法，用于处理和训练使用序列数据的机器学习模型。我们先从学习如何使用三维数据结构表示序列数据开始。
- en: Data representation for sequential models
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列模型的数据表示
- en: 'Timesteps add depth to the data, making it a 3D structure. In the context of
    sequential data, each “unit” or instance of this dimension is termed a “timestep.”
    This is crucial to remember: while the dimension is called “timesteps,” each individual
    data point in this dimension is a “timestep.” *Figure 10.4* illustrates the three
    dimensions in data used for training RNNs, emphasizing the addition of timesteps:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 时间步（Timesteps）为数据增添了深度，使其成为一个三维结构。在序列数据的上下文中，这一维度的每个“单元”或实例被称为“时间步”。需要记住的是：虽然这一维度被称为“时间步”，但这一维度中的每个数据点都是一个“时间步”。*图
    10.4* 展示了用于训练RNN的三维数据结构，强调了时间步的添加：
- en: '![A picture containing text, linedrawing  Description automatically generated](img/B18046_10_04.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图像，包含文字和线条](img/B18046_10_04.png)'
- en: 'Figure 10.4: The 3D data structures used in RNN training'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4：RNN训练中使用的三维数据结构
- en: Given that the concept of timesteps is a new addition to our exploration, a
    special notation is introduced to represent it effectively. A superscript enclosing
    a timestep in angle brackets is paired with the variable in question. For example,
    using this notation, ![](img/B18046_10_001.png) and ![](img/B18046_10_002.png)
    represent the value of the variable `stock_price` at timestep *t1* and timestep
    *t2*, respectively.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于时间步的概念是我们探索中的新内容，特引入了一种特殊的符号表示法来有效地表示它。该符号是在尖括号中括住时间步，并与相关变量配对。例如，使用此符号表示法，![](img/B18046_10_001.png)和![](img/B18046_10_002.png)分别表示变量`stock_price`在时间步*t1*和*t2*的值。
- en: The choice of dividing data into batches, essentially deciding the “length,”
    can be both an intentional design decision and influenced by external tools and
    libraries. Often, machine learning frameworks provide utilities to automatically
    batch data, but choosing an optimal batch size can be a combination of experimentation
    and domain knowledge.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据划分为批次的选择，本质上是决定“长度”，既可以是有意的设计决策，也可能受到外部工具和库的影响。通常，机器学习框架提供了自动批处理数据的工具，但选择最佳的批次大小可能需要结合实验和领域知识。
- en: Let us start the discussion on sequential modeling techniques with RNNs first.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从RNN开始讨论序列建模技术。
- en: Introducing RNNs
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍RNN
- en: RNNs, are a special breed of neural networks designed specifically for sequential
    data. Here’s a breakdown of their key attributes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: RNN（循环神经网络）是一种专门为处理序列数据而设计的神经网络。以下是其关键特性的解析。
- en: The term “recurrent” stems from the unique feedback loop RNNs possess. Unlike
    traditional neural networks, which are essentially stateless and produce outputs
    solely based on the current inputs, RNNs carry forward a “state” from one step
    in the sequence to the next.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: “递归”一词源于RNN所具备的独特反馈回路。与传统的神经网络不同，传统神经网络本质上是无状态的，并且仅根据当前输入生成输出，而RNN则将一个“状态”从序列中的一步传递到下一步。
- en: When we talk about a “run” in the context of RNNs, we’re referring to a single
    pass or processing of an element in the sequence. So, as the RNN processes each
    element, or each “run,” it retains some information from the previous steps.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论RNN中的“运行”时，我们指的是序列中某个元素的单次传递或处理。因此，随着RNN处理每个元素或每次“运行”，它会保留一些来自前一步的信息。
- en: The magic of RNNs lies in their ability to maintain a memory of previous runs
    or steps. They achieve this by incorporating an additional input, which is essentially
    the state or memory from the previous run. This mechanism allows RNNs to recognize
    and learn the dependencies between elements in a sequence, such as the relationships
    between consecutive words in a sentence.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的魔力在于它们能够保持对先前运行或步骤的记忆。它们通过结合一个额外的输入——即来自前一步的状态或记忆——来实现这一点。这种机制使RNN能够识别和学习序列中元素之间的依赖关系，例如句子中连续单词之间的关系。
- en: Let us study the architecture of RNNs in detail.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细研究RNN的架构。
- en: Understanding the architecture of RNNs
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解RNN的架构
- en: 'First, let us define some variables:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义一些变量：
- en: '![](img/B18046_10_003.png) : the input at timestep *t*'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18046_10_003.png)：时间步* t *的输入'
- en: '![](img/B18046_10_004.png) : actual output (ground truth) at timestep *t*'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18046_10_004.png)：时间步* t *的实际输出（真实值）'
- en: '![](img/B18046_10_005.png) : predicted output at timestep *t*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18046_10_005.png)：时间步* t *的预测输出'
- en: Understanding the memory cell and hidden state
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解记忆单元和隐藏状态
- en: '**RNNs** stand out because of their inherent ability to remember and maintain
    context as they progress through different timesteps. This state at a certain
    timestep *t* is represented by ![](img/B18046_10_006.png), where *h* stands for
    hidden. It is the summary of the information learned up to a particular timestep.
    As shown in *Figure 10.5*, the RNN keeps on learning by updating its hidden state
    at each timestep. The RNN uses this hidden state at each timestep to keep a context.
    At its core, “context” refers to the collective information or knowledge an RNN
    retains from previous timesteps. It allows RNNs to memorize the state at each
    timestep and pass this information to the next timestep as it progresses along
    through the sequence. This hidden state makes the RNN stateful:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**RNNs**之所以突出，是因为它们天生具有记住并维持上下文的能力，随着时间步的推进保持这种能力。某个时间步* t *的状态通过 ![](img/B18046_10_006.png)表示，其中*h*表示隐藏状态。这是截至某一特定时间步所学到的信息的总结。如*图10.5*所示，RNN通过在每个时间步更新其隐藏状态不断学习。RNN在每个时间步使用这个隐藏状态来保持上下文。从本质上讲，“上下文”是指RNN从之前的时间步中保留的集体信息或知识。它使RNN能够在每个时间步记住状态，并将这些信息传递到下一个时间步，随着序列的推进。这个隐藏状态使得RNN是有状态的：'
- en: '![Diagram  Description automatically generated](img/B18046_10_05.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B18046_10_05.png)'
- en: 'Figure 10.5: Hidden state in RNN'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5：RNN中的隐藏状态
- en: For example, if we use an RNN to translate a sentence from English to French,
    each input is a sentence that needs to be defined as sequential data. To get it
    right, the RNN cannot translate each word in isolation. It needs to capture the
    context of the words that have been translated so far, allowing the RNN to correctly
    translate the entire sentence. This is achieved through the hidden state that
    is calculated and stored at each timestep and passed on to the later ones.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们使用RNN将一句话从英语翻译成法语，那么每个输入就是需要定义为序列数据的句子。为了准确翻译，RNN不能单独翻译每个单词。它需要捕捉到已经翻译的单词的上下文，从而使RNN能够正确翻译整个句子。这是通过在每个时间步计算并存储隐藏状态来实现的，并将其传递给后续时间步。
- en: The RNN’s strategy of memorizing the state with the intention of using it for
    future timesteps brings new research questions that need to be addressed. For
    example, *what* to remember and *what* to forget. And, perhaps the trickiest one,
    *when* to forget. The variants of RNN, like GRUs and LSTM, attempt to answer these
    questions in different ways.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: RNN通过记住状态并打算将其用于未来时间步的策略带来了新的研究问题，需要解决。例如，*记住*什么以及*忘记*什么。而且，也许最棘手的问题是，*何时*忘记。RNN的变种，如GRU和LSTM，尝试以不同的方式回答这些问题。
- en: Understanding the characteristics of the input variable
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解输入变量的特性
- en: 'Let’s get a deeper understanding of the input variable, ![](img/B18046_10_007.png),
    and the methodology behind encoding it when working with RNNs. One of the pivotal
    applications for RNNs lies in the realm of NLP. Here, the sequential data we deal
    with comprises sentences. Think of each sentence as a sequence of words, such
    that a sentence can be delineated as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地理解输入变量![](img/B18046_10_007.png)及其在处理RNN时的编码方法。RNN的一个关键应用领域是在NLP中。在这里，我们处理的序列数据是句子。可以把每个句子看作是一个单词的序列，因此一个句子可以被描述为：
- en: '![](img/B18046_10_008.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_008.png)'
- en: 'In this representation, ![](img/B18046_10_009.png) denotes an individual word
    within the sentence. To avoid confusion: each ![](img/B18046_10_010.png) is not
    an entire sentence but rather an individual word within it.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表示中，![](img/B18046_10_009.png)表示句子中的一个单独单词。为了避免混淆：每个![](img/B18046_10_010.png)并不是整个句子，而是其中的一个单独单词。
- en: 'Each word, ![](img/B18046_10_011.png), is encoded using a one-hot vector. The
    length of this vector is defined by |V|, where:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词![](img/B18046_10_011.png)都使用一个独热编码向量进行编码。该向量的长度由|V|定义，其中：
- en: V signifies our vocabulary set, which is a collection of distinct words.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V表示我们的词汇集合，它是一个包含不同单词的集合。
- en: '|V| quantifies the total number of entries in V.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '|V|量化了V中条目的总数。'
- en: In the context of widely-used applications, one could envision V as comprising
    the entire set of words found in a standard English dictionary, which may contain
    roughly 150,000 words. However, for specific NLP tasks, only a subset of this
    vast vocabulary is necessary.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在广泛应用的背景下，可以将V视为包含标准英语词典中所有单词的集合，通常包含大约150,000个单词。然而，对于特定的NLP任务，只需要这个庞大词汇表的一部分。
- en: '**Note**: It’s essential to differentiate between V and |V|. While V stands
    for the vocabulary itself, |V| represents the size of this vocabulary.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：区分V和|V|非常重要。V代表词汇表本身，而|V|表示该词汇表的大小。'
- en: When referring to the “dictionary,” we’re drawing from a general notion of standard
    English dictionaries. However, there are more exhaustive corpora available, like
    the Common Crawl, which can contain word sets stretching into the tens of millions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当提到“词典”时，我们指的是标准英语词典的一般概念。然而，也有更为详尽的语料库可用，如Common Crawl，其中包含的词集可以达到数千万个单词。
- en: For many applications, a subset of this vocabulary should be enough. Formally,
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用程序来说，这个词汇表的子集就足够了。形式化地说，
- en: '![](img/B18046_10_012.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_012.png)'
- en: To understand the working of RNNs, let us examine the first timestep, *t1*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解RNN的工作原理，让我们先看看第一个时间步*t1*。
- en: Training the RNN at the first timestep
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在第一个时间步训练RNN
- en: RNNs operate by analyzing sequences one timestep at a time. Let’s dive into
    the initial phase of this process. For the timestep *t1*, the network receives
    an input represented as ![](img/B18046_10_013.png). Based on this input, the RNN
    makes an initial prediction, which we denote as ![](img/B18046_10_014.png). At
    every timestep, *tt*, the RNN leverages the hidden state from the previous timestep,
    ![](img/B18046_10_015.png), to provide contextual information.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: RNN通过一次分析一个时间步的序列来操作。让我们深入了解这一过程的初始阶段。在时间步*t1*，网络接收表示为![](img/B18046_10_013.png)的输入。基于这个输入，RNN做出初步预测，我们将其表示为![](img/B18046_10_014.png)。在每个时间步*tt*，RNN利用来自前一个时间步的隐藏状态![](img/B18046_10_015.png)，以提供上下文信息。
- en: However, at *t1*, since we’re just beginning, there’s no prior hidden state
    to reference. Therefore, the hidden state ![](img/B18046_10_016.png) is initialized
    to zero.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在*t1*时刻，由于我们刚刚开始，因此没有前一个隐藏状态可以引用。因此，隐藏状态![](img/B18046_10_016.png)初始化为零。
- en: The activation function in action
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数的作用
- en: Referencing *Figure 10.6*, you’ll notice an element marked by **A**. This represents
    the activation function, a crucial component in neural networks. Essentially,
    the activation function determines how much signal to pass onto the next layer.
    For this timestep, the activation function receives both the input ![](img/B18046_10_017.png)
    and the previous hidden state ![](img/B18046_10_018.png).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 参考*图10.6*，你会注意到一个标记为**A**的元素。它代表激活函数，这是神经网络中的关键组件。本质上，激活函数决定了多少信号传递到下一层。在这个时间步，激活函数接收输入![](img/B18046_10_017.png)和前一个隐藏状态![](img/B18046_10_018.png)。
- en: As discussed in *Chapter 8*, an activation function in neural networks is a
    mathematical equation that determines the output of a neuron based on its input.
    Its primary role is to introduce non-linearity into the network, enabling it to
    learn from errors and make adjustments, which is essential for learning complex
    patterns.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如*第8章*所述，神经网络中的激活函数是一个数学方程式，根据输入决定神经元的输出。它的主要作用是将非线性引入网络，从而使其能够通过误差进行学习和调整，这对于学习复杂的模式至关重要。
- en: A recurring choice for the activation function in many neural networks is “`tanh`.”
    But what’s the reasoning behind this preference?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 许多神经网络中常用的激活函数是“`tanh`”。那么，这种偏好的背后有什么原因呢？
- en: 'The world of neural networks isn’t without its challenges, and one such obstacle
    is the vanishing gradient problem. To put it plainly, as we keep training our
    model, occasionally, the gradient values, which guide our weight adjustments,
    diminish to tiny numbers. This drop means the changes we make to our network’s
    weights become almost negligible. Such minute tweaks result in an excruciatingly
    slow learning process, sometimes even coming to a standstill. Here’s where the
    “`tanh`" function shines. It’s chosen because it acts as a buffer against this
    vanishing gradient issue, steering the training process toward consistency and
    efficiency:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的世界并非没有挑战，其中一个难题就是梯度消失问题。简单来说，当我们持续训练模型时，梯度值——指导我们调整权重的数值——有时会变得非常小。这种下降意味着我们对网络权重的调整几乎可以忽略不计。这些微小的调整导致学习过程变得极其缓慢，有时甚至会停滞不前。这时，“`tanh`”函数就发挥了作用。之所以选择它，是因为它在对抗梯度消失问题时充当了缓冲作用，推动训练过程朝着一致性和效率的方向发展：
- en: '![Diagram  Description automatically generated with medium confidence](img/B18046_10_06.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图示描述，自动生成，信心中等](img/B18046_10_06.png)'
- en: 'Figure 10.6: RNN training at timestep t1'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：RNN在时间步t1的训练
- en: 'As we zero in on the outcome of the activation function, we arrive at the value
    for the hidden state, ![](img/B18046_10_019.png). In mathematical terms, this
    relationship can be expressed as:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们聚焦于激活函数的结果时，我们得到了隐藏状态的值，![](img/B18046_10_019.png)。在数学上，这种关系可以表示为：
- en: '![](img/B18046_10_020.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_020.png)'
- en: This hidden state is not just a passing phase. It holds value as we step into
    the next timestep, *t2*. Think of it as a relay racer passing on the baton, or
    in this case, context, from one timestep to its successor, ensuring continuity
    in the sequence.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个隐藏状态不仅仅是一个过渡阶段。当我们进入下一个时间步，*t2*时，它依然保持重要的价值。可以把它想象成接力赛选手传递接力棒，或者在这个例子中，是从一个时间步到下一个时间步的上下文传递，确保序列的连续性。
- en: 'The second activation function (represented by **B** in *Figure 10.7*) is used
    to generate the predicted output ![](img/B18046_10_021.png) at timestep *t1*.
    The choice of this activation function will depend on the type of the output variable.
    For instance, if an RNN is employed to predict stock market prices, the ReLU function
    can be adopted as the output variable is continuous. On the other hand, if we
    are doing sentiment analysis on a bunch of posts, it can be a sigmoid activation
    function. In *Figure 10.7*, assuming that it is a multiclass output variable,
    we are using the softmax activation function. Remember that a multiclass output
    variable refers to a situation where the output or the prediction can fall into
    one of several distinct classes. In machine learning, this is common in classification
    problems where the aim is to categorize an input into one of several predefined
    categories. For example, if we are categorizing objects as a car, bike, or bus,
    the output variable has multiple classes, thus is termed as “multiclass.” Mathematically,
    we can represent it as:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个激活函数（在*图10.7*中由**B**表示）用于生成时间步*t1*的预测输出 ![](img/B18046_10_021.png)。选择这个激活函数将取决于输出变量的类型。例如，如果RNN用于预测股市价格，可以采用ReLU函数，因为输出变量是连续的。另一方面，如果我们对一堆帖子进行情感分析，可能会使用sigmoid激活函数。在*图10.7*中，假设它是一个多类输出变量，我们使用的是softmax激活函数。请记住，多类输出变量指的是输出或预测可以落入多个不同类别中的情况。在机器学习中，这种情况通常出现在分类问题中，目标是将输入归类为几个预定义类别之一。例如，如果我们将物体分类为汽车、自行车或公交车，则输出变量有多个类别，因此称为“多类”。在数学上，我们可以将其表示为：
- en: '![](img/B18046_10_022.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_022.png)'
- en: From *Eq. 10.1* and *Eq. 10.2*, It should be obvious that the objective of training
    the RNN is to find the optimal values of three sets of weight matrices (*W*[hx],
    *W*[hh], and *W*[yh]) and two sets of biases (*b*[h] and *b*[y]). As we progress,
    it becomes evident that these weights and biases maintain consistency across all
    timesteps.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从*方程 10.1*和*方程 10.2*可以明显看出，训练RNN的目标是找到三组权重矩阵（*W*[hx]、*W*[hh]和*W*[yh]）和两组偏置（*b*[h]和*b*[y]）的最优值。随着训练的进展，显而易见，这些权重和偏置在所有时间步中保持一致。
- en: Training the RNN for a whole sequence
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练整个序列的RNN
- en: 'Previously, we developed the mathematical formulation for the hidden state
    for the first timestep, *t1*. Let us now study the working of the RNN through
    more than one timestep to train a complete sequence, as shown in *Figure 10.7*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们为第一个时间步*t1*推导了隐藏状态的数学公式。现在，我们通过多个时间步来研究RNN的工作原理，以训练完整的序列，如*图 10.7*所示：
- en: '![Diagram  Description automatically generated](img/B18046_10_07.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图示描述自动生成](img/B18046_10_07.png)'
- en: 'Figure 10.7: Sequential processing in RNN'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7：RNN中的顺序处理
- en: '**Info**: In *Figure 10.7*, it can be observed that the hidden state travels
    from left to right carrying the context forward shown by the arrow **A**. The
    ability of RNNs and their variants to create this “information highway” propagating
    through time is the defining feature of RNNs.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**信息**：在*图 10.7*中，可以观察到隐藏状态从左到右传播，并通过箭头**A**将上下文信息向前传递。RNN及其变体能够创建这种“信息高速公路”并在时间上传播，是RNN的定义特征。'
- en: 'We calculated *Eq. 10.1* for the timestep *t1*. For any timestep t, we can
    generalize *Eq. 10.1* as:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为时间步*t1*计算了*方程 10.1*。对于任何时间步t，我们可以将*方程 10.1*推广为：
- en: '![](img/B18046_10_023.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_023.png)'
- en: For NLP applications, ![](img/B18046_10_024.png)is encoded as a one-hot vector.
    In this case, the dimension of ![](img/B18046_10_025.png) will be equal to |V|,
    where V is the vector representing the vocabulary. The hidden variable ![](img/B18046_10_026.png)
    will be a lower-dimensional representation of the original input, ![](img/B18046_10_025.png).
    By lowering the dimension of the input variable ![](img/B18046_10_028.png) by
    many folds, we intend the hidden layer to capture only the important information
    of the input variable ![](img/B18046_10_025.png). The dimension of ![](img/B18046_10_006.png)
    is represented by *D*[h].
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NLP应用，![](img/B18046_10_024.png)被编码为一个独热向量。在这种情况下，![](img/B18046_10_025.png)的维度将等于|V|，其中V是表示词汇表的向量。隐藏变量![](img/B18046_10_026.png)将是原始输入![](img/B18046_10_025.png)的低维表示。通过将输入变量![](img/B18046_10_028.png)的维度降低多个倍数，我们希望隐藏层仅捕捉输入变量![](img/B18046_10_025.png)的重要信息。![](img/B18046_10_006.png)的维度由*D*[h]表示。
- en: It is not unusual for ![](img/B18046_10_006.png) to have dimensions 500 times
    lower than ![](img/B18046_10_032.png).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于![](img/B18046_10_006.png)的维度比![](img/B18046_10_032.png)低500倍，这并不罕见。
- en: 'So, typically:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，通常情况下：
- en: '![](img/B18046_10_033.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_033.png)'
- en: Because of the lower dimensions of ![](img/B18046_10_034.png) , weight matrix
    *W*[hh] is a comparatively small data structure as ![](img/B18046_10_035.png).
    *W*[hx] on the other hand, will be as wide as ![](img/B18046_10_036.png).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于![](img/B18046_10_034.png)的维度较低，权重矩阵*W*[hh]相对较小，类似于![](img/B18046_10_035.png)。另一方面，*W*[hx]的宽度将与![](img/B18046_10_036.png)一样宽。
- en: Combining weight matrices
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 合并权重矩阵
- en: In *Eq. 10.3*, both *W*[hh] and *W*[hx] are used in the calculation of ![](img/B18046_10_037.png).
    To simplify the analysis, it helps to combine *W*[hh] and *W*[hx] into one weight
    parameter matrix, ![](img/B18046_10_038.png). This simplified representation will
    be quite useful for the discussion of more complex variants of RNNs that are discussed
    later in this chapter.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在*方程 10.3*中，*W*[hh]和*W*[hx]都用于计算![](img/B18046_10_037.png)。为了简化分析，有助于将*W*[hh]和*W*[hx]合并为一个权重参数矩阵，![](img/B18046_10_038.png)。这种简化表示对于后续章节中讨论的更复杂的RNN变体非常有用。
- en: 'To create one combined weight matrix, *W*[h], we simply horizontally concatenate
    *W*[hh] and *W*[hx] horizontally to create a combined weight matrix, *W*[h]:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个合并的权重矩阵*W*[h]，我们只需将*W*[hh]和*W*[hx]水平拼接，形成一个合并的权重矩阵*W*[h]：
- en: '![](img/B18046_10_039.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_039.png)'
- en: As we are simply horizontally concatenating, the dimensions of the *W*[h] will
    have the same number of rows and total number of columns, i.e.,
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只是进行水平拼接，*W*[h]的维度将有相同数量的行和总列数，即：
- en: '![](img/B18046_10_040.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_040.png)'
- en: 'Using *W*[h] in *Eq. 10.3*:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在*方程 10.3*中使用*W*[h]：
- en: '![](img/B18046_10_041.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_041.png)'
- en: Where ![](img/B18046_10_042.png) indicates the vertical stacking of two vectors
    together.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B18046_10_042.png) 表示将两个向量垂直堆叠在一起。
- en: '![](img/B18046_10_043.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_043.png)'
- en: Where ![](img/B18046_10_044.png) and ![](img/B18046_10_045.png) are the respective
    transposed vectors.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B18046_10_044.png) 和 ![](img/B18046_10_045.png) 是相应的转置向量。
- en: Let us look at a specific example.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个具体的例子。
- en: Let us assume that we are using RNNs for an NLP application. The size of the
    vocabulary is 50,000 words. It means that each input ![](img/B18046_10_025.png)
    will be encoded as a hot vector having a dimension of 50,000\. Let assume that
    ![](img/B18046_10_047.png) has a dimension of 50\. It will be the lower-dimension
    representation of ![](img/B18046_10_025.png).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在为自然语言处理（NLP）应用使用RNN。词汇表的大小是50,000个单词。这意味着每个输入 ![](img/B18046_10_025.png)
    将被编码为一个维度为50,000的热编码向量。假设 ![](img/B18046_10_047.png) 的维度为50。它将是 ![](img/B18046_10_025.png)
    的低维表示。
- en: Now, it should be obvious that *W*[hh] will have dimensions of (50![](img/B18046_10_049.png)50).
    *W*[hx] will have dimensions of (50![](img/B18046_10_049.png)50,000).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，应该显而易见，*W*[hh] 的维度将是 (50![](img/B18046_10_049.png)50)。*W*[hx] 的维度将是 (50![](img/B18046_10_049.png)50,000)。
- en: 'Going back to the above example, *W*[h] will have dimensions of (50x50,000+50)
    = 50![](img/B18046_10_051.png)50,050, i.e.,:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 回到上面的例子，*W*[h] 的维度将是 (50x50,000+50) = 50![](img/B18046_10_051.png)50,050，即：
- en: '![](img/B18046_10_052.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_052.png)'
- en: Calculating the output for each timestep
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算每个时间步的输出
- en: 'In our model, the output generated for a given timestep, such as *t1*, is denoted
    by ![](img/B18046_10_053.png). Since we are employing the softmax function for
    normalization in our model, the output for any timestep, *tt*, can be generalized
    using the following equation:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型中，给定时间步（如 *t1*）生成的输出由 ![](img/B18046_10_053.png) 表示。由于我们在模型中使用了softmax函数进行归一化，因此任何时间步
    *tt* 的输出可以通过以下方程式进行概括：
- en: '![](img/B18046_10_054.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_054.png)'
- en: Understanding how the output is calculated at each timestep lays the foundation
    for the subsequent stage of training, where we need to evaluate how well the model
    is performing.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 理解在每个时间步如何计算输出，为随后的训练阶段奠定基础，在这一阶段，我们需要评估模型的表现。
- en: Now that we have a grasp of how the outputs are generated at each timestep,
    it becomes essential to determine the discrepancy between these predicted outputs
    and the actual target values. This discrepancy, referred to as “loss,” gives us
    a measure of the model’s error. In the next section, we will delve into the methods
    of computing RNN loss, allowing us to gauge the model’s accuracy and make necessary
    adjustments to the weights and biases. This process is vital in training the model
    to make more accurate predictions, thereby enhancing its overall performance.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了如何在每个时间步生成输出，接下来需要确定这些预测输出与实际目标值之间的差异。这种差异被称为“损失”，它为我们提供了模型误差的衡量标准。在接下来的部分，我们将深入探讨计算RNN损失的方法，帮助我们评估模型的准确性，并对权重和偏差进行必要的调整。这个过程对于训练模型使其做出更准确的预测至关重要，从而提高整体性能。
- en: Computing RNN loss
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算RNN损失
- en: As mentioned, the objective of training RNNs is to find the right values of
    three sets of weights (*W*[hx], *W*[hh], and *W*[yh]) and two sets of biases (*b*[h]
    and *b*[y]). Initially, at timestep *t1*, these values are initialized randomly.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，训练RNN的目标是找到三组权重（*W*[hx]、*W*[hh] 和 *W*[yh]）以及两组偏差（*b*[h] 和 *b*[y]）的正确值。最初，在时间步
    *t1*，这些值会随机初始化。
- en: 'As the training process progresses, these values are changed as the gradient
    descent algorithm kicks in. We need to compute loss at each timestep of the forward
    propagation in RNNs. Let us break down the process of computing the loss:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练过程的推进，这些值会随着梯度下降算法的应用而发生变化。我们需要在RNN的前向传播过程中计算每个时间步的损失。让我们分解计算损失的过程：
- en: '**Compute loss for individual timestep**:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算单个时间步的损失**：'
- en: 'At timestep *t1*, the predicted output is ![](img/B18046_10_055.png). The expected
    output is ![](img/B18046_10_056.png). The actual loss function used will depend
    on the type of model we are training. For example, if we are training a classifier,
    then this loss at timestep *t1* will be:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在时间步 *t1*，预测输出为 ![](img/B18046_10_055.png)。期望输出为 ![](img/B18046_10_056.png)。实际使用的损失函数将取决于我们训练的模型类型。例如，如果我们正在训练分类器，那么在时间步
    *t1* 的损失将是：
- en: '![](img/B18046_10_057.png)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B18046_10_057.png)'
- en: '**Aggregate loss for complete sequence**:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**完整序列的聚合损失**：'
- en: 'For a complete sequence consisting of multiple timesteps, we will compute the
    individual losses for each of the timesteps, {*t*[1],*t*[2],…*t*[T]). The loss
    for one sequence with *T* timesteps will be the aggregate of the loss of each
    timestep, as calculated by the following equation:'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于由多个时间步组成的完整序列，我们将计算每个时间步的单独损失， {*t*[1],*t*[2],…*t*[T]}。一个包含 *T* 个时间步的序列的损失，将是每个时间步损失的汇总，按照以下公式计算：
- en: '![](img/B18046_10_058.png)'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B18046_10_058.png)'
- en: '**Compute loss for multiple sequences in a batch**:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算一批次中多个序列的损失**：'
- en: If there is more than one sequence in the batch, then, first, the loss is calculated
    for each individual sequence. We then compute the cost across all the sequences
    in a particular batch and use it for backpropagation.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果一个批次中有多个序列，首先会为每个单独的序列计算损失。然后，我们计算一个批次中所有序列的总体损失，并将其用于反向传播。
- en: By calculating the loss in this structured manner, we guide the model in adjusting
    its weights and biases to better align with the desired output. This iterative
    process, repeated over many batches and epochs, allows the model to learn from
    the data and make more accurate predictions.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过以这种结构化的方式计算损失，我们引导模型调整其权重和偏差，以更好地与期望的输出对齐。这个迭代过程，在多个批次和时期中反复进行，使得模型能够从数据中学习并做出更准确的预测。
- en: Backpropagation through time
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间反向传播
- en: Backpropagation, as explained in *Chapter 8*, is used in neural networks to
    progressively learn from the examples of training datasets. RNNs add another dimension
    to the training data, that is, the timesteps. **Backpropagation through time**
    (**BPTT**) is designed to handle the sequential data as the training process is
    going through the timesteps.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如*第8章*所述，反向传播用于神经网络中，从训练数据集的示例中逐步学习。RNN 在训练数据中增加了另一个维度，即时间步。**时间反向传播**（**BPTT**）旨在处理序列数据，因为训练过程是通过时间步进行的。
- en: Backpropagation is triggered when the forward feed process calculates the loss
    of the last timestep of a batch. We then apply this derivative to adjust the weights
    and biases for the RNN model. RNNs have three sets of weights, *W*[hh] , *W*[hx]
    and *W*[hy], and two sets of biases (*b*[h] and *b*[y]). Once the weights and
    biases are adjusted, we will continue with gradient descent for model training.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当前馈过程计算完一个批次的最后一个时间步的损失时，触发反向传播。然后我们应用这个导数来调整 RNN 模型的权重和偏差。RNN 有三组权重，*W*[hh]，*W*[hx]
    和 *W*[hy]，以及两组偏差（*b*[h] 和 *b*[y]）。一旦调整了权重和偏差，我们将继续进行梯度下降来训练模型。
- en: The name of this section, *Backpropagation through time*, does not hint toward
    any time machine that takes us back to some medieval era. Instead, it stems from
    the fact that once the cost has been calculated through forward-feed, it needs
    to run backward through each of the timesteps and update weights and biases.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的名称 *时间反向传播* 并不暗示任何让我们回到中世纪时代的时间机器。它的来源在于一旦通过前馈计算了成本，必须通过每个时间步倒推并更新权重和偏差。
- en: The backpropagation process is crucial for tuning the model’s parameters, but
    once the model is trained, what’s next? After we’ve used backpropagation to minimize
    the loss, we have a model that’s ready to make predictions. In the next section,
    we’ll explore how to use the trained RNN model to make predictions on new data.
    We’ll find that predicting with RNNs is similar to the process used with fully
    connected neural networks, where the input data is processed by the trained RNN
    to produce the predictions. This shift from training to prediction forms a natural
    progression in understanding how RNNs can be applied to real-world problems.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播过程对于调优模型的参数至关重要，但一旦模型训练完成，接下来该做什么呢？在我们使用反向传播来最小化损失之后，模型已经准备好进行预测。在接下来的部分，我们将探讨如何使用训练好的
    RNN 模型对新数据进行预测。我们会发现，使用 RNN 进行预测与使用全连接神经网络的过程类似，输入数据经过训练好的 RNN 处理后，生成预测结果。这种从训练到预测的转变，形成了理解
    RNN 如何应用于现实问题的自然进展。
- en: Predicting with RNNs
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 RNN 进行预测
- en: 'Once the model is trained, predicting with RNNs is similar to with fully connected
    neural networks. The input data is given as input to the trained RNN model and
    predictions are obtained. Here’s how it functions:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，使用 RNN 进行预测与使用全连接神经网络进行预测类似。将输入数据作为输入传递给训练好的 RNN 模型，得到预测结果。其工作原理如下：
- en: '**Input preparation**: Just like in a standard neural network, you begin by
    preparing the input data. In the case of an RNN, this input data is typically
    sequential, representing timesteps in a process or series.'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入准备**：与标准神经网络一样，首先要准备输入数据。对于RNN而言，这些输入数据通常是序列化的，代表过程或系列中的时间步。'
- en: '**Model utilization**: You then feed this input data into the trained RNN model.
    The model’s learned weights and biases, optimized during the training phase, are
    used to process the input through each layer of the network. In an RNN, this includes
    passing the data through the recurrent connections that handle the sequential
    aspects of the data.'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型利用**：接着将输入数据输入到已训练的RNN模型中。模型在训练阶段优化的学习权重和偏置将用于通过网络的每一层处理输入数据。在RNN中，这包括通过处理数据的循环连接来处理数据的序列特性。'
- en: '**Activation functions**: As in other neural networks, activation functions
    within the RNN transform the data as it moves through the layers. Depending on
    the specific design of the RNN, different activation functions might be used at
    different stages.'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**激活函数**：与其他神经网络一样，RNN中的激活函数会在数据通过各层时对其进行转换。根据RNN的具体设计，可能在不同阶段使用不同的激活函数。'
- en: '**Generating predictions**: The penultimate step is generating the predictions.
    The output of the RNN is processed through a final layer, often using a softmax
    activation function for classification tasks, to produce the final prediction
    for each input sequence.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成预测**：倒数第二步是生成预测。RNN的输出会通过最后一层进行处理，通常在分类任务中使用softmax激活函数，生成每个输入序列的最终预测结果。'
- en: '**Interpretation**: The predictions are then interpreted based on the specific
    task at hand. This could be classifying a sequence of text, predicting the next
    value in a time series, or any other task that relies on sequential data.'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**解释**：预测结果将根据具体任务进行解释。这可能是分类一段文本序列、预测时间序列中的下一个值，或者任何依赖于序列数据的其他任务。'
- en: Thus, predicting with an RNN follows a process similar to that of fully connected
    neural networks, with the main distinction being the handling of sequential data.
    The RNN’s ability to capture temporal relationships within the data allows it
    to provide unique insights and predictions that other neural network architectures
    might struggle with.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RNN的预测过程与全连接神经网络相似，主要区别在于对序列数据的处理。RNN捕捉数据中时间关系的能力使其能够提供其他神经网络架构难以处理的独特洞察和预测。
- en: Limitations of basic RNNs
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本RNN的局限性
- en: 'Earlier in the chapter, we introduced basic RNNs. Sometimes we refer to basic
    RNNs as “plain vanilla” RNNs. This term refers to their fundamental, unadorned
    structure. While they serve as a solid introduction to recurrent neural networks,
    these basic RNNs do have notable limitations:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面，我们介绍了基本RNN。有时我们将基本RNN称为“纯粹的香草”RNN。这个术语指的是它们的基本、朴素结构。虽然它们作为递归神经网络的一个良好入门，但这些基本RNN确实有显著的局限性：
- en: '**Vanishing gradient problem**: This issue makes it challenging for the RNN
    to learn and retain long-term dependencies in the data.'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**梯度消失问题**：这个问题使得RNN很难学习和保持数据中的长期依赖关系。'
- en: '**Inability to look ahead in the sequence**: Traditional RNNs process sequences
    from the beginning to the end, which limits their capability to understand the
    future context in a sequence.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**无法预见序列中的未来**：传统的RNN从头到尾处理序列，这限制了它们理解序列中未来上下文的能力。'
- en: Let us investigate them one by one.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一探讨这些问题。
- en: Vanishing gradient problem
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度消失问题
- en: RNNs iteratively process the input data one timestep at a time. This means that
    as the input sequences become longer, RNNs find it hard to capture long-term dependencies.
    Long-term dependencies refer to relationships between elements in a sequence that
    are far apart from each other. Imagine analyzing a lengthy piece of text, such
    as a novel. If a character’s actions in the first chapter influence events in
    the last chapter, that’s a long-term dependency. The information from the beginning
    of the text has to be “remembered” all the way to the end for full understanding.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: RNN逐步处理输入数据，一次一个时间步。这意味着随着输入序列变长，RNN很难捕捉长期依赖关系。长期依赖关系指的是序列中相距较远的元素之间的关系。想象一下分析一段长篇文本，比如一部小说。如果一个角色在第一章的行为影响了最后一章的事件，那就是一个长期依赖关系。文本开头的信息必须“记住”直到结尾，才能充分理解。
- en: RNNs often struggle with such long-range connections. The hidden state mechanism
    of RNNs, designed to retain information from previous timesteps, can be too simplistic
    to capture these intricate relationships. As the distance between related elements
    grows, the RNN may lose track of the connection. There is not much intelligence
    on when and what to keep in memory and when and what to forget.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: RNN通常在处理这样的长程依赖时会遇到困难。RNN的隐藏状态机制旨在保留来自前一个时间步的信息，但它过于简单，无法捕捉这些复杂的关系。随着相关元素之间距离的增加，RNN可能会丧失对连接的跟踪。它没有智能来判断何时保存记忆、何时忘记信息。
- en: For many use cases in sequential data, only the most recent information is important.
    For example, consider a predictive text application trying to assist a person
    typing an email by suggesting the next word to type.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多顺序数据的应用场景，只有最新的信息是重要的。例如，考虑一个预测文本应用，它试图通过建议下一个要输入的单词来协助一个人写邮件。
- en: 'As we know, such functionality is now standard in modern word processors. If
    the user is typing:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，这种功能现在在现代文字处理软件中是标准配置。如果用户正在输入：
- en: '![](img/B18046_10_08.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_08.png)'
- en: 'Figure 10.8: Predictive text example'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8：预测文本示例
- en: the predictive text application can easily suggest the next word “hard”. It
    does not need to bring the context from the prior sentences to predict the next
    word. For such applications, where long-term memory is not required, RNNs are
    the best choice. RNNs will not over-complicate the architecture without compromising
    on accuracy.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 预测文本应用可以轻松地建议下一个单词“hard”。它不需要带入前一句话的上下文来预测下一个单词。对于这种不需要长时记忆的应用，RNN是最佳选择。RNN在不牺牲准确性的情况下，不会使架构过于复杂。
- en: 'But for other applications, keeping the long-term dependencies is important.
    RNNs struggle with managing long-term dependencies. Let us look at an example:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于其他应用，保留长时依赖关系是重要的。RNN在管理长时依赖关系时遇到困难。让我们来看一个例子：
- en: '![](img/B18046_10_09.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_09.png)'
- en: 'Figure 10.9: Predictive text example with a long-term dependency'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：带有长时依赖的预测文本示例
- en: As we read this sentence from left to right, we can observe that “was” (used
    later in the sentence) is referring to the “man.” RNNs in their original form
    will struggle to carry the hidden state forward for multiple timesteps. The reason
    is that, in RNNs, the hidden state is calculated for each timestep and carried
    forward.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从左到右阅读这句话时，可以观察到“was”（稍后在句子中使用）指的是“man”。原始形式的RNN在多时间步长中会很难保持隐藏状态的传递。原因在于，在RNN中，隐藏状态是针对每个时间步计算的，并且被传递到下一个时间步。
- en: Due to the recursive nature of this operation, we are always concerned about
    the signal prematurely fading while progressing from element to element in different
    timesteps. This behavior of RNNs is identified as the vanishing gradient problem.
    To combat this vanishing gradient problem, we prefer to choose tanh as the activation
    function. As the second derivative of tanh decays very slowly to zero, the choice
    of tanh helps manage the vanishing gradient problem to some extent. But we need
    more sophisticated architecture, like GRUs and LSTM, to better manage the vanishing
    gradient problem, which we will discuss in the next section.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此操作的递归特性，我们总是担心在不同时间步从一个元素到另一个元素的过程中，信号会提前衰减。RNN的这种行为被称为梯度消失问题。为了应对梯度消失问题，我们通常选择tanh作为激活函数。由于tanh的二阶导数衰减到零的速度非常慢，选择tanh有助于在一定程度上管理梯度消失问题。但是我们需要更复杂的架构，如GRU和LSTM，以更好地管理梯度消失问题，下一节将详细讨论这一点。
- en: Inability to look ahead in the sequence
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无法在序列中向前看
- en: RNNs can be categorized based on the direction of information flow through the
    sequence. The two primary types are unidirectional RNNs and bidirectional RNNs.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: RNN可以根据信息流动的方向分类。主要有两种类型：单向RNN和双向RNN。
- en: '**Unidirectional RNNs**: These networks process the input data in one direction,
    usually from the beginning of the sequence to the end. They carry the context
    forward, building understanding step by step as they iterate through the elements
    of a sequence, such as words in a sentence. Here’s the limitation: unidirectional
    RNNs cannot “look ahead” in the sequence.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单向RNN**：这些网络以单一方向处理输入数据，通常是从序列的开始到结束。它们将上下文信息逐步传递，随着序列元素的迭代（如句子中的单词），逐步建立理解。其局限性在于：单向RNN无法在序列中“向前看”。'
- en: They only have access to the information they’ve seen so far, meaning they can’t
    incorporate future elements to build a more accurate or nuanced context. Imagine
    reading a complex sentence one word at a time, without being able to glance ahead
    and see what’s coming. You might miss subtleties or misunderstand the overall
    meaning.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它们只能访问到迄今为止所看到的信息，这意味着它们无法结合未来的元素来构建更准确或更细致的上下文。想象一下，逐字阅读一篇复杂的句子，而无法提前预览即将出现的内容。你可能会错过一些细微之处或误解整体意义。
- en: '**Bidirectional RNNs**: In contrast, bidirectional RNNs process the sequence
    in both directions simultaneously. They combine insights from both the past and
    the future elements, allowing for a richer understanding of the context.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双向RNN**：相反，双向RNN同时处理序列的两个方向。它们结合了过去和未来元素的信息，使得对上下文有更丰富的理解。'
- en: 'Let us consider the following two sentences:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下两句话：
- en: '![](img/B18046_10_10.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_10.png)'
- en: 'Figure 10.10: Examples where an RNN must look ahead in the sentence'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10：RNN必须在句子中提前查看的示例
- en: Both of these sentences use the word “cricket.” If the context is built only
    from left to right, as done in unidirectional RNNs, we cannot contextualize “cricket”
    properly as its relevant information will be in a future timestep. To solve this
    problem, we will look into bidirectional RNNs, which are discussed in *Chapter
    11**.11*
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这两句话都使用了“cricket”这个词。如果上下文仅从左到右构建，就像单向RNN那样，我们就无法正确地理解“cricket”，因为其相关信息将在未来的时间步中出现。为了解决这个问题，我们将研究双向RNN，它们在*第11章*中有详细讨论。
- en: Now let us study GRUs and their detailed working and architecture.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们研究GRU及其详细的工作原理和架构。
- en: GRU
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GRU
- en: 'GRUs represent an evolution of the basic RNN structure, specifically designed
    to address some of the challenges encountered with traditional RNNs, such as the
    vanishing gradient problem. The architecture of a GRU is illustrated in *Figure
    10.8*:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: GRU代表了基本RNN结构的一种演变，特别设计用来解决传统RNN遇到的一些挑战，例如梯度消失问题。GRU的架构如*图10.8*所示：
- en: '![Diagram, schematic  Description automatically generated](img/B18046_10_11.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图示，示意图  描述自动生成](img/B18046_10_11.png)'
- en: 'Figure 10.11: GRU'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11：GRU
- en: 'Let us start discussing GRU with the first activation function, annotated as
    **A**. At each timestep t, GRU first calculates the hidden state using the tanh
    activation function and utilizing ![](img/B18046_10_059.png) and ![](img/B18046_10_060.png)
    as inputs. The calculation is no different than how the hidden state is determined
    in the original RNNs presented in the previous section. But there is an important
    difference. The output is a *candidate* hidden state, which is calculated using
    *Eq. 10.6*:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从讨论第一种激活函数开始，该函数标注为**A**。在每个时间步t，GRU首先使用tanh激活函数计算隐藏状态，并利用 ![](img/B18046_10_059.png)
    和 ![](img/B18046_10_060.png) 作为输入。这个计算与上一节中介绍的原始RNN中隐藏状态的确定方法没有什么不同。但是有一个重要的区别。输出是一个*候选*隐藏状态，它是通过*公式10.6*计算得出的：
- en: '![](img/B18046_10_061.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_061.png)'
- en: where ![](img/B18046_10_062.png) is the candidate value of the hidden layer.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B18046_10_062.png) 是隐藏层的候选值。
- en: Now, instead of using the candidate hidden state straight away, the GRU takes
    a moment to decide whether to use it. Imagine it like someone pausing to think
    before making a decision. This pause-and-think step is what we call the **gating
    mechanism**. It checks out the information and then selects what details to remember
    and what to forget for the next step. It’s kind of like filtering out the noise
    and focusing on the important stuff. By blending the old information (from the
    previous hidden state) and the new draft (the candidate), GRUs are better at following
    long stories or sequences without getting lost. By introducing a candidate hidden
    state, GRUs bring an added layer of flexibility. They can judiciously decide the
    portion of the candidate state to incorporate. This distinction equips GRUs to
    adeptly tackle challenges, such as the vanishing gradient, with a finesse that
    traditional RNNs often lack. In simpler terms, while the classic RNNs might struggle
    to remember long stories, GRUs, with their special features, are better listeners
    and retainers.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，GRU不会立即使用候选隐藏状态，而是花时间决定是否使用它。可以想象成某人做决定之前停下来思考。这一停顿思考的过程就是我们所说的**门控机制**。它检查信息，然后选择接下来要记住的细节和要遗忘的部分。它有点像过滤掉噪音，集中注意力在重要的东西上。通过将旧信息（来自之前的隐藏状态）和新草案（候选状态）结合起来，GRU能够更好地跟随长篇故事或序列，而不会迷失方向。通过引入候选隐藏状态，GRU增加了额外的灵活性。它们可以谨慎地决定将候选状态的哪一部分纳入。这个区别使得GRU能够巧妙地应对诸如梯度消失之类的挑战，而传统RNN通常缺乏这种能力。简单来说，经典的RNN可能难以记住长篇故事，而GRU凭借其独特的特点，更像是优秀的听众和记忆者。
- en: LSTM was proposed in 1997 and GRUs in 2014\. Most books on this topic prefer
    the chronological order and present LSTMs first. I have chosen to present these
    algorithms ordered by complexity. As the motivation behind GRUs was to simplify
    LSTMs, it may be useful to study the simpler algorithm first.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是在1997年提出的，而GRU则是在2014年提出的。大多数关于这一主题的书籍倾向于按时间顺序呈现，首先介绍LSTM。我选择按复杂度顺序呈现这些算法。由于GRU的提出动机是简化LSTM，因此从学习较简单的算法开始可能会更有帮助。
- en: Introducing the update gate
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入更新门
- en: In a standard RNN, the hidden value at each timestep is calculated and automatically
    becomes the new state of the memory cell. In contrast, GRUs introduce a more nuanced
    approach. The GRU model brings more flexibility to the process by allowing control
    over when to update the state of the memory cell. This added flexibility is implemented
    through a mechanism called the “update gate,” sometimes referred to as the “reset
    gate.”
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的RNN中，每个时间步的隐藏值都会被计算并自动成为记忆单元的新状态。相比之下，GRU引入了一个更细致的方法。GRU模型通过允许控制何时更新记忆单元的状态，为这个过程带来了更多的灵活性。这个增加的灵活性是通过一个叫做“更新门”的机制实现的，有时也被称为“重置门”。
- en: The update gate’s function is to evaluate whether the information in the candidate
    hidden state, ![](img/B18046_10_063.png), is significant enough to update the
    memory cell’s hidden state or if the memory cell should retain the old hidden
    value from previous timesteps.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 更新门的作用是评估候选隐藏状态中的信息，![](img/B18046_10_063.png)，是否足够重要以更新记忆单元的隐藏状态，或者记忆单元是否应该保留之前时间步的旧隐藏值。
- en: In mathematical terms, this decision-making process helps the model to manage
    information more selectively, determining whether to integrate new insights or
    continue relying on previously acquired knowledge. If the model deems that the
    candidate hidden state’s information is not significant enough to alter the memory
    cell’s existing state, the previous hidden value will be retained. Conversely,
    if the new information is considered relevant, it can overwrite the memory cell’s
    state, thus adjusting the model’s internal representation as it processes the
    sequence.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，这个决策过程帮助模型更加有选择性地管理信息，决定是整合新的见解，还是继续依赖之前获得的知识。如果模型认为候选隐藏状态的信息不足以改变记忆单元当前的状态，那么就会保留之前的隐藏值。相反，如果新的信息被认为相关，它就会覆盖记忆单元的状态，从而在处理序列时调整模型的内部表示。
- en: This unique gating mechanism sets GRUs apart from traditional RNNs and allows
    for more effective learning from sequential data with complex temporal relationships.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这种独特的门控机制使得GRU区别于传统的RNN，并且使得它能在处理具有复杂时间关系的序列数据时，进行更有效的学习。
- en: Implementing the update gate
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现更新门
- en: This intelligence that we add to how the state is updated in the memory cell
    is the defining feature of a GRU. The decision will be taken soon of whether we
    should update the current hidden state with the candidate hidden state. To make
    this decision, we use the second activation function shown in *Figure 10.**11*,
    annotated as **B**. This activation function is implementing the update gate.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在记忆单元中如何更新状态所加入的智能是GRU的定义特征。很快我们将决定是否应该用候选隐藏状态更新当前的隐藏状态。为了做出这个决定，我们使用*图10.11*中显示的第二个激活函数，标注为**B**。这个激活函数实现了更新门。
- en: 'It is implemented as a sigmoid layer that takes as input the current input
    and the previous hidden state. The output of the sigmoid layer is a value between
    0 and 1 represented by the variable ![](img/B18046_10_064.png) The output of the
    update gate is the variable ![](img/B18046_10_065.png), which is governed by the
    following sigmoid function:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 它作为一个sigmoid层实现，该层以当前输入和先前的隐藏状态为输入。sigmoid层的输出是一个介于0和1之间的值，由变量![](img/B18046_10_064.png)表示。更新门的输出是变量![](img/B18046_10_065.png)，它由以下sigmoid函数控制：
- en: '![](img/B18046_10_066.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_066.png)'
- en: As ![](img/B18046_10_067.png) is the output of a sigmoid function, it is close
    to either 1 or 0, which determines whether the update gate is open or closed.
    If the update gate is open, ![](img/B18046_10_068.png) will be chosen as the new
    hiddenstate. In the training process, the GRU will learn when to open the gate
    and when to close it.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于![](img/B18046_10_067.png)是sigmoid函数的输出，它接近于1或0，这决定了更新门是否开启。如果更新门开启，![](img/B18046_10_068.png)将被选为新的隐藏状态。在训练过程中，GRU将学习何时开启门，何时关闭门。
- en: Updating the hidden cell
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新隐藏单元
- en: 'For a certain timestep, the next hidden state is determined using the calculation
    from the following equation:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个时间步，下一隐藏状态是通过以下方程的计算得出的：
- en: '![](img/B18046_10_069.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_069.png)'
- en: '*Eq. 10.8* consists of two terms, annotated as **1** and **2**. Being an output
    of a sigmoid function, ![](img/B18046_10_070.png) can either be 0 or 1\. It means:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '*方程 10.8* 包含两个项，标注为**1**和**2**。作为sigmoid函数的输出，![](img/B18046_10_070.png)可以是0或1。这意味着：'
- en: '![](img/B18046_10_071.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_071.png)'
- en: '![](img/B18046_10_072.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_072.png)'
- en: In other words, if the gate is open, update the value of ![](img/B18046_10_073.png).
    Otherwise, just retain the old state.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果门是开启的，更新![](img/B18046_10_073.png)的值。否则，只需保留旧状态。
- en: Let us now look into how we can run GRUs for multiple timesteps.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看如何在多个时间步上运行GRU。
- en: Running GRUs for multiple timesteps
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在多个时间步上运行GRU
- en: When deploying GRUs across several timesteps, we can visualize this process
    as depicted in *Figure 10.12*. Much like the foundational RNNs we discussed in
    the prior segment, GRUs create what can be thought of as an “information highway.”
    This pathway effectively transfers context from the beginning to the end of a
    sequence, visualized as ![](img/B18046_10_074.png) in *Figure 10.12* and annotated
    as **A**.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 当在多个时间步上部署GRU时，我们可以像*图10.12*所示那样可视化这个过程。就像我们在前一部分讨论的基础RNN一样，GRU创建了可以看作“信息高速公路”的东西。这条路径有效地将上下文从序列的开始传递到结束，在*图10.12*中可视化为![](img/B18046_10_074.png)，并标注为**A**。
- en: What differentiates GRUs from traditional RNNs is the decision-making process
    about how information flows on this highway. Instead of transferring information
    blindly at each timestep, a GRU pauses to evaluate its relevance.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: GRU与传统RNN的区别在于它关于信息如何在这条高速公路上传输的决策过程。与每个时间步盲目地传递信息不同，GRU会暂停并评估其相关性。
- en: Let’s illustrate this with a basic example. Imagine reading a book where each
    sentence is a piece of information. However, instead of remembering every detail
    about every sentence, your mind (acting like a GRU) selectively recalls the most
    impactful or emotional sentences. This selective memory is akin to how the update
    gate in a GRU works.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个基本的例子来说明。假设你正在阅读一本书，每个句子都是一条信息。然而，与你记住每个句子的每个细节不同，你的大脑（像一个GRU）会选择性地回忆起那些最有影响力或最有情感的句子。这种选择性记忆类似于GRU中更新门的工作方式。
- en: The update gate serves a crucial role here. It’s a mechanism that determines
    which portions of the prior information, or the prior “hidden state,” should be
    retained or discarded. Essentially, the gate helps the network zoom in on and
    retain the most pertinent details, ensuring that the carried context remains as
    relevant as possible.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 更新门在这里发挥着至关重要的作用。它是一个机制，决定哪些先前的信息，或先前的“隐藏状态”，应该保留或丢弃。本质上，更新门帮助网络聚焦并保留最相关的细节，确保传递的上下文保持尽可能相关。
- en: '![Diagram  Description automatically generated](img/B18046_10_12.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图示  描述自动生成](img/B18046_10_12.png)'
- en: 'Figure 10.12: Sequential processing in RNN'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12：RNN中的顺序处理
- en: Introducing LSTM
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入LSTM
- en: RNNs are widely used for sequence modeling tasks, but they suffer from limitations
    in capturing long-term dependencies in the data. An advanced version of RNNs,
    known as LSTM, was developed to address these limitations. Unlike simple RNNs,
    LSTMs have a more complex mechanism to manage context, enabling them to better
    capture patterns in sequences.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: RNN广泛应用于序列建模任务，但它们在捕捉数据中的长期依赖性方面存在局限性。为了克服这些局限性，开发了RNN的高级版本——LSTM。与简单的RNN不同，LSTM具有更复杂的机制来管理上下文，使其能够更好地捕捉序列中的模式。
- en: 'In the previous section, we discussed GRUs, where hidden state ![](img/B18046_10_006.png)
    is used to carry the context from timestep to timestep. LSTM has a much more complex
    mechanism for managing the context. It has two variables that carry the context
    from timestep to timestep: the cell state and the hidden state. They are explained
    as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了GRU，其中隐藏状态 ![](img/B18046_10_006.png) 用于将上下文从一个时间步传递到下一个时间步。LSTM拥有更为复杂的机制来管理上下文。它有两个变量来携带上下文信息：细胞状态和隐藏状态。它们的解释如下：
- en: '**The cell state** (represented as ![](img/B18046_10_076.png)): This is responsible
    for maintaining the long-term dependencies of the input data. It is passed from
    one timestep to the next and is used to maintain information across a longer period.
    As we will learn later in this section, it is carefully determined by the forget
    gate and the update gate what should be included in the cell state. It can be
    considered as the “persistence layer” or “memory” of the LSTM as it maintains
    the information over a long period of time.'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**细胞状态**（表示为 ![](img/B18046_10_076.png)）：它负责维护输入数据的长期依赖性。它从一个时间步传递到下一个时间步，用于在更长时间内保持信息。正如我们在本节稍后将学到的，细胞状态的内容是由遗忘门和更新门精确决定的。它可以被视为LSTM的“持久层”或“记忆”，因为它在较长时间内保持信息。'
- en: '**The hidden state** (represented as ![](img/B18046_10_077.png)): This context
    is focused on the current timestep, which may or may not be important for the
    long-term dependencies. It is the output of the LSTM unit for a particular timestep
    and is passed as input to the next time step. As indicated in *Figure 10.23*,
    the hidden state, ![](img/B18046_10_078.png), is used to generate the output ![](img/B18046_10_079.png)at
    timestep *t*.'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**隐藏状态**（表示为 ![](img/B18046_10_077.png)）：该上下文关注于当前时间步，它可能对长期依赖性重要，也可能不重要。它是LSTM单元在特定时间步的输出，并作为输入传递到下一个时间步。如*图10.23*所示，隐藏状态
    ![](img/B18046_10_078.png) 用于生成时间步 *t* 的输出 ![](img/B18046_10_079.png)。'
- en: Let us now study these mechanisms in more detail, starting with how the current
    cell state is updated.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更详细地研究这些机制，从当前细胞状态如何更新开始。
- en: Introducing the forget gate
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入遗忘门
- en: The forget gate in an LSTM network is responsible for determining which information
    to discard from the previous state, and which information to keep. It is annotated
    as **A** in *Figure 10.3*. It is implemented as a sigmoid layer that takes as
    input the current input and the previous hidden state. The output of the sigmoid
    layer is a vector of values between 0 and 1, where each value corresponds to a
    single cell in the LSTM’s memory.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM网络中的遗忘门负责确定从先前状态中丢弃哪些信息，保留哪些信息。它在*图10.3*中标注为**A**。它实现为一个sigmoid层，输入为当前输入和先前的隐藏状态。sigmoid层的输出是一个在0到1之间的值向量，每个值对应于LSTM记忆中一个单元的状态。
- en: '![](img/B18046_10_080.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_080.png)'
- en: As it is a sigmoid function, it means that ![](img/B18046_10_081.png) can be
    either close to 0 or 1.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它是一个sigmoid函数，这意味着 ![](img/B18046_10_081.png) 可以接近0或接近1。
- en: If ![](img/B18046_10_082.png) is 1, then it means that the value from the previous
    state ![](img/B18046_10_083.png) should be used to calculate ![](img/B18046_10_084.png).
    If ![](img/B18046_10_085.png) is 0, then it means that the value from the previous
    state ![](img/B18046_10_086.png)should be forgotten.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ![](img/B18046_10_082.png) 为 1，则意味着应使用来自前一状态 ![](img/B18046_10_083.png) 的值来计算
    ![](img/B18046_10_084.png)。如果 ![](img/B18046_10_085.png) 为 0，则意味着应忘记来自前一状态 ![](img/B18046_10_086.png)
    的值。
- en: '**Info**: Usually, binary variables are considered active when their logic
    is 1\. It may feel counter-intuitive that the “forget gate” forgets the previous
    state when ![](img/B18046_10_087.png) = 0, but this is how logic was presented
    in the original paper and is followed by the researchers for consistency.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**信息**：通常，二进制变量在其逻辑为 1 时被认为是激活的。当 ![](img/B18046_10_087.png) = 0 时，"遗忘门" 忘记前一状态的行为可能显得不直观，但这是原始论文中提出的逻辑，研究人员为了保持一致性遵循了这一点。'
- en: '![Diagram, schematic  Description automatically generated](img/B18046_10_13.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图示，示意图  描述自动生成](img/B18046_10_13.png)'
- en: 'Figure 10.13: LSTM architecture'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13：LSTM 架构
- en: The candidate cell state
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 候选单元状态
- en: 'In LSTM, at each timestep, a candidate cell state, ![](img/B18046_10_088.png),
    is calculated, which is annotated as **Y** in *Figure 10.13*, and is the proposed
    new state for the memory cell. It is calculated using the current input ![](img/B18046_10_025.png)
    and the previous hidden state ![](img/B18046_10_090.png) as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LSTM 中，在每个时间步，计算出一个候选单元状态，![](img/B18046_10_088.png)，它在*图 10.13*中标注为 **Y**，并作为提议的新状态用于记忆单元。它通过当前输入
    ![](img/B18046_10_025.png) 和前一隐藏状态 ![](img/B18046_10_090.png) 来计算，公式如下：
- en: '![](img/B18046_10_091.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_091.png)'
- en: The update gate
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新门
- en: The update gate is also called the input gate. The update gate in LSTM networks
    is a mechanism that allows the network to selectively incorporate new information
    into the current state so that the memory can focus on the most relevant information.
    It is annotated as **B** in *Figure 10.13*.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 更新门也叫做输入门。LSTM 网络中的更新门是一种机制，它允许网络有选择地将新信息融入当前状态，使得记忆能够集中关注最相关的信息。它在*图 10.13*中标注为
    **B**。
- en: 'It is responsible for determining whether the candidate cell state ![](img/B18046_10_088.png)should
    be added to ![](img/B18046_10_093.png). It is implemented as a sigmoid layer that
    takes as input the current input ![](img/B18046_10_025.png) and the previous hidden
    state:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 它负责判断候选单元状态 ![](img/B18046_10_088.png) 是否应添加到 ![](img/B18046_10_093.png) 中。它作为一个
    sigmoid 层实现，输入为当前输入 ![](img/B18046_10_025.png) 和前一时刻的隐藏状态：
- en: '![](img/B18046_10_095.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_095.png)'
- en: The output of the sigmoid layer, ![](img/B18046_10_096.png), is a vector of
    values between 0 and 1, where each value corresponds to a single cell in the LSTM’s
    memory. A value of 0 indicates that the calculated ![](img/B18046_10_097.png)
    should be ignored, while a value of 1 indicates that ![](img/B18046_10_098.png)
    is significant enough to be incorporated in ![](img/B18046_10_099.png). Being
    a sigmoid function, it can have any value between 0 and 1, which indicates that
    some of the information from ![](img/B18046_10_097.png) should be incorporated
    in ![](img/B18046_10_093.png), but not all.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid 层的输出，![](img/B18046_10_096.png)，是一个值介于 0 和 1 之间的向量，每个值对应 LSTM 记忆中的一个单元。值为
    0 表示计算出的 ![](img/B18046_10_097.png) 应该被忽略，而值为 1 表示 ![](img/B18046_10_098.png)
    足够重要，应该被纳入 ![](img/B18046_10_099.png) 中。作为一个 sigmoid 函数，它的值可以介于 0 和 1 之间，表示来自
    ![](img/B18046_10_097.png) 的部分信息应该被融入 ![](img/B18046_10_093.png)，但不是全部。
- en: The update gate allows the LSTM to selectively incorporate new information into
    the current state and prevent the memory from becoming flooded with irrelevant
    data. By controlling the amount of new information that is added to the memory
    state, the update gate helps the LSTM to maintain a balance between preserving
    the previous state and incorporating new information.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 更新门允许 LSTM 有选择地将新信息融入当前状态，防止记忆被无关数据淹没。通过控制新信息加入记忆状态的量，更新门帮助 LSTM 保持在保留前一状态和融入新信息之间的平衡。
- en: Calculating memory state
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算记忆状态
- en: 'As compared to GRU, the main difference in LSTM is that instead of having a
    single update gate (as we have in GRU), we have separate gates for the update
    and forget mechanisms for hidden state management. Each gate determines what is
    the right mix of various states to optimally calculate both the long-term memory
    ![](img/B18046_10_093.png) current cell state and the current hidden state, ![](img/B18046_10_078.png).
    The memory state is calculated by:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GRU 相比，LSTM 的主要区别在于，LSTM 不仅有一个更新门（如 GRU 中的那样），还为隐藏状态管理提供了独立的更新和遗忘门。每个门决定了各种状态的正确混合，以最优地计算长时记忆
    ![](img/B18046_10_093.png)、当前单元状态和当前隐藏状态 ![](img/B18046_10_078.png)。记忆状态通过以下方式计算：
- en: '![](img/B18046_10_104.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_104.png)'
- en: '*Eq. 10.12* consists of two terms annotated as **1** and **2**. Being an output
    of a sigmoid function, ![](img/B18046_10_105.png)and ![](img/B18046_10_106.png)
    can either be 0 or 1\. It means:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '*方程 10.12* 由标注为 **1** 和 **2** 的两个项组成。作为 sigmoid 函数的输出，![](img/B18046_10_105.png)
    和 ![](img/B18046_10_106.png) 的值可以是 0 或 1。意味着：'
- en: '![](img/B18046_10_071.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_071.png)'
- en: '![](img/B18046_10_072.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_072.png)'
- en: In other words, if the gate is open, update the value of ![](img/B18046_10_037.png).
    Otherwise, just retain the old state.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果门是开启的，则更新 ![](img/B18046_10_037.png) 的值。否则，保留旧状态。
- en: Thus, the update gate in a GRU is a mechanism that allows the network to selectively
    discard information from the previous hidden state so that the hidden state can
    focus on the most relevant information. This is shown in *Figure 10.13*, which
    shows how the state travels from left to right.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，GRU 中的更新门是一种机制，允许网络有选择地丢弃之前隐藏状态中的信息，以便隐藏状态可以专注于最相关的信息。如*图 10.13*所示，展示了状态如何从左向右传递。
- en: The output gate
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出门
- en: The output gate in an LSTM network is annotated as **C** in *Figure 10.13*.
    It is responsible for determining which information from the current memory state
    should be passed on as the output of the LSTM. It is implemented as a sigmoid
    layer that takes as input the current input and the previous hidden state. The
    output of the sigmoid layer is a vector of values between 0 and 1, where each
    value corresponds to a single cell in the LSTM’s memory.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 网络中的输出门在*图 10.13*中标注为 **C**。它负责确定当前记忆状态中的哪些信息应作为 LSTM 的输出传递。它作为一个 sigmoid
    层实现，输入为当前输入和前一个隐藏状态。sigmoid 层的输出是一个值在 0 和 1 之间的向量，其中每个值对应于 LSTM 内存中的一个单独的单元。
- en: As it is a sigmoid function, it means that ![](img/B18046_10_110.png) can be
    either close to 0 or 1.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它是一个 sigmoid 函数，这意味着 ![](img/B18046_10_110.png) 可以接近 0 或 1。
- en: If ![](img/B18046_10_111.png) is 1, then it means that the value from the previous
    state ![](img/B18046_10_099.png) should be used to calculate. ![](img/B18046_10_076.png)
    If ![](img/B18046_10_081.png) is 0, then it means that the value from the previous
    state ![](img/B18046_10_099.png)should be forgotten.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ![](img/B18046_10_111.png) 为 1，则表示应该使用之前状态的值 ![](img/B18046_10_099.png) 来进行计算。若
    ![](img/B18046_10_081.png) 为 0，则表示应该忘记之前状态的值 ![](img/B18046_10_099.png)。
- en: '![](img/B18046_10_116.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_116.png)'
- en: A value of 0 indicates that the corresponding cell should not contribute to
    the output, while a value of 1 indicates that the cell should fully contribute
    to the output. Values between 0 and 1 indicate that the cell should contribute
    some, but not all, of its value to the output.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 值为 0 表示对应的单元不应对输出作出贡献，而值为 1 则表示该单元应完全贡献于输出。介于 0 和 1 之间的值表示该单元应部分贡献其值给输出。
- en: In LSTMs, after processing the output gate, the current state is passed through
    a `tanh` function. This function adjusts the values such that they fall within
    a range between -1 and 1\. Why is this scaling necessary? The `tanh` function
    ensures that the LSTM’s output remains normalized and prevents values from becoming
    too large, which can be problematic during training due to potential issues like
    exploding gradients.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LSTM 中，经过输出门处理后，当前状态会通过一个 `tanh` 函数。该函数调整值，使其落在 -1 到 1 的范围内。为什么需要这种缩放？`tanh`
    函数确保 LSTM 的输出保持归一化，并防止值变得过大，这在训练过程中可能会导致梯度爆炸等问题。
- en: After scaling, the result from the output gate is multiplied by this normalized
    state. This combined value represents the final output of the LSTM at that specific
    timestep.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 经缩放后，输出门的结果与此归一化状态相乘。这个组合值表示 LSTM 在特定时间步的最终输出。
- en: 'To provide a simple analogy: imagine adjusting the volume of your music so
    it’s neither too loud nor too soft, but just right for your environment. The `tanh`
    function acts similarly, ensuring the output is optimized and suitable for further
    processing.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个简单的类比：想象调整音乐的音量，使其既不太大也不太小，而是刚好适合你的环境。`tanh`函数的作用类似，确保输出是优化的，适合进一步处理。
- en: The output gate is important because it allows the LSTM to selectively pass
    on relevant information from the current memory state as the output. It also helps
    to prevent irrelevant information from being passed on as the output.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 输出门非常重要，因为它允许LSTM从当前的记忆状态中选择性地传递相关信息作为输出。它还帮助防止无关信息被作为输出传递。
- en: 'This output gate generates the variable ![](img/B18046_10_117.png)which determines
    that the contribution of the cell state is output to the hidden state:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出门生成变量 ![](img/B18046_10_117.png)，决定细胞状态对隐藏状态的贡献是否输出：
- en: '![](img/B18046_10_118.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_10_118.png)'
- en: In LSTM, ![](img/B18046_10_119.png) is used as input to the gates, whereas ![](img/B18046_10_093.png)
    is the hidden state.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在LSTM中，![](img/B18046_10_119.png) 被用作输入到门中，而 ![](img/B18046_10_093.png) 是隐藏状态。
- en: In summary, the output gate in LSTM networks is a mechanism that allows the
    network to selectively pass on relevant information from the current memory state
    as the output so that the LSTM can generate appropriate output based on the relevant
    information it has stored in its memory.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，LSTM网络中的输出门是一种机制，它允许网络从当前记忆状态中选择性地传递相关信息作为输出，这样LSTM就可以根据其存储的相关信息生成适当的输出。
- en: Putting everything together
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容汇总
- en: Let’s delve into the workings of the LSTM across multiple timesteps, as depicted
    by **A** in *Figure 10.14*.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解LSTM在多个时间步中的工作原理，如*图10.14*中的**A**所示。
- en: Just like GRUs, LSTMs create a conduit – often referred to as an “information
    highway” – which helps ferry context across successive timesteps. This is illustrated
    in *Figure 10.14*. What’s fascinating about LSTMs is their ability to use long-term
    memory to transport this context.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 就像GRU一样，LSTM创建了一条通道——通常称为“信息高速公路”——帮助将上下文传递到后续的时间步。这个过程在*图10.14*中有所展示。LSTM的魅力在于它能够利用长期记忆来传递这些上下文。
- en: As we traverse from one timestep to the next, the LSTM learns what should be
    retained in its long-term memory, denoted as ![](img/B18046_10_076.png). At the
    start of every timestep, ![](img/B18046_10_093.png) interacts with the “forget
    gate,” allowing some pieces of information to be discarded. Subsequently, it encounters
    the “update gate,” where new data is infused. This allows ![](img/B18046_10_093.png)
    to transition between timesteps, continually gaining and shedding information
    as dictated by the two gates.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从一个时间步推进到下一个时间步时，LSTM会学习哪些信息应该保留在其长期记忆中，表示为 ![](img/B18046_10_076.png)。在每个时间步的开始，![](img/B18046_10_093.png)
    与“忘记门”进行交互，允许一些信息被丢弃。接着，它遇到“更新门”，新的数据被注入其中。这使得 ![](img/B18046_10_093.png) 能够在时间步之间转换，按照两个门的指示，不断地获得和舍弃信息。
- en: 'Now, here’s where it gets intricate. At the close of every timestep, a copy
    of the long-term memory, ![](img/B18046_10_093.png), undergoes transformation
    via the tanh function. This processed data is then sieved by the output gate,
    culminating in what we term short-term memory, ![](img/B18046_10_077.png). This
    short-term memory serves a dual purpose: it determines the output at that specific
    timestep and lays the foundation for the subsequent timestep, as portrayed in
    *Figure 10.14*:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，事情变得复杂了。在每个时间步结束时，长期记忆的副本 ![](img/B18046_10_093.png) 通过tanh函数进行转换。处理后的数据经过输出门筛选，最终得到我们称之为短期记忆的结果
    ![](img/B18046_10_077.png)。这个短期记忆有双重作用：它决定了特定时间步的输出，并为随后的时间步奠定基础，如*图10.14*所示：
- en: '![Diagram  Description automatically generated](img/B18046_10_14.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图示 说明自动生成](img/B18046_10_14.png)'
- en: 'Figure 10.14: LSTM with multiple timesteps'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14：带有多个时间步的LSTM
- en: Let us now look into how we can code RNNs.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何编写RNN的代码。
- en: Coding sequential models
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写顺序模型
- en: For our exploration into LSTM, we’ll be diving into sentiment analysis using
    the well-known IMDb movie reviews dataset. Here, every review is tagged with a
    sentiment, positive or negative, encoded as binary values (`True` for positive,
    and `False` for negative). Our aim is to craft a binary classifier capable of
    predicting these sentiments based solely on the text content of the review.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的LSTM探索中，我们将深入研究使用著名的IMDb电影评论数据集进行情感分析。在这里，每条评论都被标记为一个情感，正面或负面，使用二进制值进行编码（`True`表示正面，`False`表示负面）。我们的目标是创建一个二分类器，能够仅根据评论的文本内容预测这些情感。
- en: 'In total, the dataset boasts 50,000 movie reviews. For our purposes, we’ll
    be dividing this equally: 25,000 reviews for training our model, and the remaining
    25,000 for evaluating its performance.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，该数据集包含50,000条电影评论。为了我们的目的，我们将其平分：25,000条用于训练模型，剩下的25,000条用于评估模型的性能。
- en: For those seeking a deeper dive into the dataset, more information is available
    at Stanford’s IMDB Dataset.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些希望深入了解数据集的人，更多信息可以在斯坦福的IMDB数据集中找到。
- en: Loading the dataset
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'First, we need to load the dataset. We will import this dataset through `keras.datasets`.
    The advantage of importing this dataset through `keras.datasets` is that it has
    been processed to be used for machine learning. For example, the reviews have
    been individually encoded as a list of word indexes. The overall frequency of
    a particular word has been chosen as the index. So, if the index of the word is
    “7,” it means that it is the 7^(th) most frequent word. The use of pre-prepared
    data allows us to focus on the RNN algorithm instead of data preparation:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要加载数据集。我们将通过`keras.datasets`导入该数据集。通过`keras.datasets`导入的优势在于，它已经被处理成可以用于机器学习的格式。例如，评论已被单独编码为单词索引的列表。特定单词的总体频率被选为索引。因此，如果一个单词的索引是“7”，这意味着它是第7个最常见的单词。使用预处理好的数据让我们能够专注于RNN算法，而不是数据准备：
- en: '[PRE0]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that the argument `num_words=50000` is used to select only the top 50000
    words. As the frequency of a word is used as the index, it means all the words
    with indexes less than 50000 are filtered out:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，参数`num_words=50000`用于仅选择前50000个词汇。由于单词的频率被用作索引，这意味着所有索引小于50000的单词都会被过滤掉：
- en: '[PRE1]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When working with sequences of varying lengths, it’s often beneficial to ensure
    that they all have a uniform length. This is particularly crucial when feeding
    them into neural networks, which often expect consistent input sizes. To achieve
    this, we use padding—adding zeros at the beginning or end of sequences until they
    reach a specified length.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理长度不等的序列时，通常需要确保它们都具有相同的长度。这一点在将序列输入神经网络时尤为重要，因为神经网络通常期望输入的大小一致。为此，我们使用填充——在序列的开头或末尾添加零，直到它们达到指定的长度。
- en: 'Here’s how you can implement this with TensorFlow:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何使用TensorFlow实现这一点：
- en: '[PRE2]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Indexes are great for the consumption of algorithms. For human readability,
    we can convert these indexes back to words:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 索引非常适合算法处理。为了便于人类阅读，我们可以将这些索引转换回单词：
- en: '[PRE3]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that word indexes start from 3 instead of 0 or 1\. The reason is that the
    first three indexes are reserved.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，单词索引从3开始，而不是从0或1\. 这是因为前三个索引是保留的。
- en: Next, let us look into how we can prepare the data.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何准备数据。
- en: Preparing the data
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'In our example, we are considering a vocabulary of 50,000 words. This means
    that each word in the input sequence ![](img/B18046_10_025.png) will be encoded
    using a one-hot vector representation, where the dimension of each vector is 50,000\.
    A one-hot vector is a binary vector that has 0s in all positions except for the
    index corresponding to the word, where it has a 1\. Here’s we can load the IMDb
    dataset in TensorFlow, specifying the vocabulary size:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们考虑的词汇量为50,000个单词。这意味着输入序列中的每个单词![](img/B18046_10_025.png)都将使用一个one-hot向量表示，其中每个向量的维度为50,000\.
    One-hot向量是一个二进制向量，除了与单词对应的索引位置为1外，其他位置都是0。下面是我们如何在TensorFlow中加载IMDb数据集，并指定词汇大小：
- en: '[PRE4]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that as `vocab_size` is set to `50,000`, so the data will be loaded with
    the `50,000` most frequently occurring words. The remaining words will be discarded
    or replaced with a special token (often denoted as `<UNK>` for “unknown”). This
    ensures that our input data is manageable and only includes the most relevant
    information for our model. The variables `x_train` and `x_test` will contain the
    training and testing input data, respectively, while `y_train` and `y_test` will
    contain the corresponding labels.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于`vocab_size`被设置为`50,000`，因此数据将加载前`50,000`个最常出现的单词，其余单词将被丢弃或替换为一个特殊的符号（通常用`<UNK>`表示“未知”）。这确保了我们的输入数据是可管理的，并且只包含对模型最相关的信息。变量`x_train`和`x_test`分别包含训练和测试输入数据，而`y_train`和`y_test`则包含相应的标签。
- en: Creating the model
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建模型
- en: 'We begin by defining an empty stack. We’ll use this for building our network,
    layer by layer:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个空的堆栈。我们将使用这个堆栈逐层构建我们的网络：
- en: '[PRE5]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Next, we’ll add an `Embedding` layer to our model. If you recall our discussion
    about word embeddings in *Chapter 9*, we used them to represent words in a continuous
    vector space. The `Embedding` layer serves a similar purpose but within the neural
    network. It provides a way to map each word in our vocabulary to a continuous
    vector. Words that are close to one another in this vector space are likely to
    share context or meaning.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向模型中添加一个`Embedding`层。如果你还记得我们在*第9章*中讨论的词嵌入，我们使用它们来表示词汇在连续向量空间中的位置。`Embedding`层也起到类似的作用，但它是在神经网络中进行的。它提供了一种将词汇表中的每个单词映射到连续向量的方式。彼此接近的词在这个向量空间中可能会共享上下文或意义。
- en: 'Let us define the `Embedding` layer, considering the vocabulary size we chose
    earlier and mapping each word to a 50-dimensional vector, corresponding to the
    dimension of ![](img/B18046_10_026.png):'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义`Embedding`层，考虑到我们之前选择的词汇表大小，并将每个词映射到一个50维的向量，对应于！[](img/B18046_10_026.png)的维度：
- en: '[PRE6]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`Dropout` layers prevents overfitting and force the model to learn multiple
    representations of the same data by randomly disabling neurons in the learning
    phase. Let us randomly disable 25% of the neurons to deal with overfitting:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dropout`层可以防止过拟合，并通过在学习阶段随机禁用神经元，迫使模型学习同一数据的多种表示。让我们随机禁用25%的神经元以应对过拟合：'
- en: '[PRE7]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we’ll add an LSTM layer, which is a specialized form of RNN. While basic
    RNNs have issues in learning long-term dependencies, LSTMs are designed to remember
    such dependencies, making them suitable for our task. This LSTM layer will analyze
    the sequence of words in the review along with their embeddings, using this information
    to determine the sentiment of a given review. We’ll use 32 units in this layer:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加一个LSTM层，它是RNN的一种专门形式。虽然基础RNN在学习长期依赖关系时存在问题，LSTM旨在记住这些依赖关系，使其适用于我们的任务。这个LSTM层将分析评论中词语的顺序及其嵌入，利用这些信息来确定给定评论的情感。我们将在这一层中使用32个单元：
- en: '[PRE8]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Add a second `Dropout` layer to drop 25% of neurons to reduce overfitting:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 添加第二个`Dropout`层，随机丢弃25%的神经元，以减少过拟合：
- en: '[PRE9]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'All LSTM units are connected to a single node in the `Dense` layer. A sigmoid
    activation function determines the output from this node – a value between 0 and
    1\. Closer to 0 indicates a negative review. Closer to 1 indicates a positive
    review:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 所有LSTM单元都连接到`Dense`层中的一个节点。一个sigmoid激活函数决定了这个节点的输出——一个介于0和1之间的值。接近0表示负面评论，接近1表示正面评论：
- en: '[PRE10]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let us compile the model. We will use `binary_crossentropy` as the loss
    function and `Adam` as the optimizer:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编译模型。我们将使用`binary_crossentropy`作为损失函数，`Adam`作为优化器：
- en: '[PRE11]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Display a summary of the model’s structure:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 显示模型结构的摘要：
- en: '[PRE12]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Training the model
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'We’ll now train the LSTM model on our training data. Training the model involves
    several key components, each of which is described below:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将基于训练数据训练LSTM模型。训练模型涉及几个关键组件，每个组件如下所述：
- en: '**Training Data**: These are the features (reviews) and labels (positive or
    negative sentiments) that our model will learn from.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据**：这些是模型将从中学习的特征（评论）和标签（正面或负面情感）。'
- en: '**Batch Size**: This determines the number of samples that will be used in
    each update of the model parameters. A higher batch size might require more memory.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批次大小**：这决定了每次更新模型参数时将使用的样本数量。较大的批次大小可能需要更多的内存。'
- en: '**Epochs**: An epoch is a complete iteration over the entire training data.
    The more epochs, the more times the learning algorithm will work through the entire
    training dataset.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Epochs**：一个epoch是对整个训练数据集的一次完整迭代。epoch越多，学习算法就会越多次地遍历整个训练数据集。'
- en: '**Validation Split**: This fraction of the training data will be set aside
    for validation and not be used for training. It helps us evaluate how well the
    model is performing.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Validation Split**：这部分训练数据将被用于验证，不用于训练。它帮助我们评估模型的表现。'
- en: '**Verbose**: This parameter controls how much output the model will produce
    during training. A value of 1 means that progress bars will be displayed:'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Verbose**：这个参数控制模型在训练过程中会输出多少内容。值为1时，进度条会显示出来：'
- en: '[PRE14]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Viewing some incorrect predictions
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查看一些错误的预测
- en: 'Let’s have a look at some of the incorrectly classified reviews:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下其中一些错误分类的评论：
- en: '[PRE16]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We select the first 20 incorrectly classified reviews:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了前20条被错误分类的评论：
- en: '[PRE17]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Summary
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The foundational concepts of sequential models were explained in this chapter,
    which aimed to give you a basic understanding of the techniques and methodologies
    of such techniques. In this chapter, we presented RNNs, which are great for handling
    sequential data. A GRU is a type of RNN that was introduced by Cho et al. in 2014
    as a simpler alternative to LSTM networks.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 本章解释了顺序模型的基础概念，旨在让你对这些技术及其方法有一个基本的了解。在这一章中，我们介绍了适合处理顺序数据的RNN。GRU是一种RNN类型，由Cho等人于2014年提出，作为LSTM网络的简化替代方案。
- en: Like LSTMs, GRUs are designed to learn long-term dependencies in sequential
    data, but they do so using a different approach. GRUs use a single gating mechanism
    to control the flow of information into and out of the hidden state, rather than
    the three gates used by LSTMs. This makes them easier to train and requires fewer
    parameters, making them more efficient to use.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 和LSTM一样，GRU被设计用来学习顺序数据中的长期依赖关系，但它们采用了不同的方法。GRU使用单一的门控机制来控制信息在隐藏状态中流入和流出的过程，而LSTM使用三个门控机制。这使得GRU更容易训练，参数更少，使用起来更高效。
- en: The next chapter introduces some advanced techniques related to sequential models.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章介绍了一些与顺序模型相关的高级技术。
- en: Learn more on Discord
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解更多内容，请访问Discord
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入这本书的Discord社区，在这里你可以分享反馈、向作者提问并了解新版本，请扫描下面的二维码：
- en: '[https://packt.link/WHLel](https://packt.link/WHLel)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/WHLel](https://packt.link/WHLel)'
- en: '![](img/QR_Code1955211820597889031.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1955211820597889031.png)'
