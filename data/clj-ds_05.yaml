- en: Chapter 5. Big Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 大数据
- en: '|   | *"More is different."* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *"更多即不同。"* |   |'
- en: '|   | --*Philip Warren Anderson* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*Philip Warren Anderson* |'
- en: In the previous chapters, we've used regression techniques to fit models to
    the data. In [Chapter 3](ch03.xhtml "Chapter 3. Correlation"), *Correlation*,
    for example, we built a linear model that used ordinary least squares and the
    normal equation to fit a straight line through the athletes' heights and log weights.
    In [Chapter 4](ch04.xhtml "Chapter 4. Classification"), *Classification*, we used
    Incanter's optimize namespace to minimize the logistic cost function and build
    a classifier of Titanic's passengers. In this chapter, we'll apply similar analysis
    in a way that's suitable for much larger quantities of data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们使用回归技术将模型拟合到数据中。例如，在[第3章](ch03.xhtml "第3章. 相关性")，*相关性*中，我们构建了一个线性模型，利用普通最小二乘法和正态方程，通过运动员的身高和对数体重拟合了一条直线。在[第4章](ch04.xhtml
    "第4章. 分类")，*分类*中，我们使用Incanter的优化命名空间，最小化逻辑代价函数，并构建了一个泰坦尼克号乘客的分类器。在本章中，我们将以适合更大数据量的方式应用类似的分析。
- en: We'll be working with a relatively modest dataset of only 100,000 records. This
    isn't big data (at 100 MB, it will fit comfortably in the memory of one machine),
    but it's large enough to demonstrate the common techniques of large-scale data
    processing. Using Hadoop (the popular framework for distributed computation) as
    its case study, this chapter will focus on how to scale algorithms to very large
    volumes of data through parallelism. We'll cover two libraries that Clojure offers
    to work with Hadoop—**Tesser** and **Parkour**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个相对简单的数据集，只有100,000条记录。这不是大数据（数据大小为100 MB，足以在一台机器的内存中舒适地存储），但它足够大，可以展示大规模数据处理的常用技术。本章将以Hadoop（分布式计算的流行框架）为案例，重点讨论如何通过并行化将算法扩展到非常大的数据量。我们将介绍Clojure提供的两个与Hadoop一起使用的库——**Tesser**和**Parkour**。
- en: Before we get to Hadoop and distributed data processing though, we'll see how
    some of the same principles that enable Hadoop to be effective at a very large
    scale can also be applied to data processing on a single machine, by taking advantage
    of the parallel capacity available in all modern computers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在深入讨论Hadoop和分布式数据处理之前，我们将首先看看一些相同的原则，这些原则使得Hadoop在大规模上有效，并且这些原则也可以应用于单机的数据处理，借助现代计算机的并行能力。
- en: Downloading the code and data
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载代码和数据
- en: This chapter makes use of data on individual income by the zip code provided
    by the U.S. Internal Revenue Service (IRS). The data contains selected income
    and tax items classified by state, zip code, and income classes.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的数据来自美国国税局（IRS）按邮政编码划分的个人收入数据。数据包含按州、邮政编码和收入等级分类的收入和税务项目。
- en: It's 100 MB in size and can be downloaded from [http://www.irs.gov/pub/irs-soi/12zpallagi.csv](http://www.irs.gov/pub/irs-soi/12zpallagi.csv)
    to the example code's data directory. Since the file contains the IRS Statistics
    of Income (SoI), we've renamed the file to `soi.csv` for the examples.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 文件大小为100 MB，可以从[http://www.irs.gov/pub/irs-soi/12zpallagi.csv](http://www.irs.gov/pub/irs-soi/12zpallagi.csv)下载到示例代码的数据目录中。由于该文件包含美国国税局的所得税统计（SoI），我们已将文件重命名为`soi.csv`以供示例使用。
- en: Note
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The example code for this chapter is available from the Packt Publishing's website
    or [https://github.com/clojuredatascience/ch5-big-data](https://github.com/clojuredatascience/ch5-big-data).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例代码可以从Packt Publishing的网站或[https://github.com/clojuredatascience/ch5-big-data](https://github.com/clojuredatascience/ch5-big-data)获取。
- en: 'As usual, a script has been provided to download and rename the data for you.
    It can be run on the command line from within the project directory with:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，提供了一个脚本，可以为你下载并重命名数据。可以在项目目录内的命令行中运行：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you run this, the file will be downloaded and renamed automatically.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行此操作，文件将自动下载并重命名。
- en: Inspecting the data
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查数据
- en: 'Once you''ve downloaded the data, take a look at the column headings in the
    first line of the file. One way to access the first line of the file is to load
    the file into memory, split on newline characters, and take the first result.
    The Clojure core library''s function `slurp` will return the whole file as a string:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下载数据后，查看文件第一行的列标题。访问文件第一行的一种方法是将文件加载到内存中，按换行符分割，并取第一个结果。Clojure核心库中的函数`slurp`将返回整个文件作为字符串：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The file is around 100 MB in size on disk. When loaded into memory and converted
    into object representations, the data will occupy more space in memory. This is
    particularly wasteful when we're only interested in the first row.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件大约 100 MB 大小。当加载到内存并转换为对象表示时，数据在内存中将占用更多空间。当我们只对第一行感兴趣时，这样的做法尤其浪费。
- en: 'Fortunately, we don''t have to load the whole file into memory if we take advantage
    of Clojure''s lazy sequences. Instead of returning a string representation of
    the contents of the whole file, we could return a reference to the file and then
    step through it one line at a time:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，如果我们利用 Clojure 的懒序列，我们不必将整个文件加载到内存中。我们可以返回文件的引用，然后逐行读取：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we're using `clojure.java.io/reader` to return a reference
    to the file. Also, we're using the `clojure.core` function `line-seq` to return
    a lazy sequence of lines from the file. In this way, we can read files even larger
    than the available memory.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用 `clojure.java.io/reader` 返回文件的引用。同时，我们使用 `clojure.core` 函数 `line-seq`
    返回文件的懒序列。通过这种方式，我们可以读取比可用内存更大的文件。
- en: 'The result of the previous function is as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个函数的结果如下：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'There are 77 fields in the file, so we won''t identify them all. The first
    four fields are:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 文件中有 77 个字段，因此我们不会全部列出。前四个字段是：
- en: '`STATEFIPS`: This is the Federal Information Processing System (FIPS) code.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`STATEFIPS`：这是联邦信息处理系统（FIPS）代码。'
- en: '`STATE`: This is the two-letter code for the State.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`STATE`：这是州的两字母代码。'
- en: '`zipcode`: This is the 5-digit zip code.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zipcode`：这是 5 位数的邮政编码。'
- en: '`AGI_STUB`: This is the side of the adjusted gross income, binned in the following
    way:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AGI_STUB`：这是调整后总收入的一部分，按以下方式分箱：'
- en: $1 under $25,000
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: $1 至 $25,000
- en: $25,000 under $50,000
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: $25,000 至 $50,000
- en: $50,000 under $75,000
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: $50,000 至 $75,000
- en: $75,000 under $100,000
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: $75,000 至 $100,000
- en: $100,000 under $200,000
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: $100,000 至 $200,000
- en: $200,000 or more
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: $200,000 或更多
- en: 'The other fields that we''re interested in are as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的其他字段如下：
- en: '`N1`: The number of returns submitted'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N1`：提交的报税表数量'
- en: '`MARS2`: The number of joint returns submitted'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MARS2`：提交的联合报税表数量'
- en: '`NUMDEP`: The number of dependents'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NUMDEP`：受抚养人数量'
- en: '`N00200`: The number of returns with salaries and wages'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N00200`：包含薪水和工资的报税表数量'
- en: '`N02300`: The number of returns with unemployment compensation'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N02300`：包含失业补偿的报税表数量'
- en: If you're curious, the full list of column definitions is available in the IRS
    data definition document at [http://www.irs.gov/pub/irs-soi/12zpdoc.doc](http://www.irs.gov/pub/irs-soi/12zpdoc.doc).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣，可以在 [http://www.irs.gov/pub/irs-soi/12zpdoc.doc](http://www.irs.gov/pub/irs-soi/12zpdoc.doc)
    查阅完整的列定义列表。
- en: Counting the records
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计数记录
- en: 'Our file is certainly wide, but is it tall? We''d like to determine the total
    number of rows in the file. Having created a lazy sequence, this is simply a matter
    of counting the length of the sequence:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的文件当然很宽，但它高吗？我们想要确定文件中的总行数。创建了懒序列后，这只是计算序列长度的问题：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding example returns 166,905, including the header row, so we know
    there are actually 166,904 rows in the file.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例返回 166,905 行，包括标题行，因此我们知道文件中实际上有 166,904 行。
- en: The `count` function is the simplest way to count the number of elements in
    a sequence. For vectors (and other types implementing the counted interface),
    this is also the most efficient one, since the collection already knows how many
    elements it contains and therefore it doesn't need to recalculate it. For a lazy
    sequence however, the only way to determine how many elements are contained in
    the sequence is to step through it from the beginning to the end.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`count` 函数是计算序列中元素数量的最简单方法。对于向量（以及实现计数接口的其他类型），这是最有效的方法，因为集合已经知道包含多少个元素，因此无需重新计算。然而，对于懒序列，唯一确定序列中包含多少元素的方法是从头到尾逐步遍历它。'
- en: 'Clojure''s implementation of `count` is written in Java, but the Clojure equivalent
    would be a reduce over the sequence like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure 中的 `count` 实现是用 Java 编写的，但等效的 Clojure 版本是对序列进行 reduce 操作，如下所示：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding function we pass to `reduce` accepts a counter `i` and the next
    element from the sequence `x`. For each `x`, we simply increment the counter `i`.
    The reduce function accepts an initial value of zero, which represents the concept
    of nothing. If there are no lines to reduce over, zero will be returned.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递给`reduce`的前一个函数接受一个计数器`i`和来自序列的下一个元素`x`。对于每个`x`，我们简单地增加计数器`i`。`reduce`函数接受一个初始值零，代表“无”的概念。如果没有要合并的行，则会返回零。
- en: As of version 1.5, Clojure offers the reducers library ([http://clojure.org/reducers](http://clojure.org/reducers)),
    which provides an alternative way to perform reductions that trades memory efficiency
    for speed.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从版本1.5开始，Clojure提供了`reducers`库（[http://clojure.org/reducers](http://clojure.org/reducers)），它提供了一种通过牺牲内存效率来换取速度的替代方式来执行合并操作。
- en: The reducers library
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: reducers库
- en: The `count` operation we implemented previously is a sequential algorithm. Each
    line is processed one at a time until the sequence is exhausted. But there is
    nothing about the operation that demands that it must be done in this way.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前实现的`count`操作是一个顺序算法。每一行按顺序处理，直到序列耗尽。但是，这个操作并没有要求它必须按这种方式进行。
- en: 'We could split the number of lines into two sequences (ideally of roughly equal
    length) and reduce over each sequence independently. When we''re done, we would
    just add together the total number of lines from each sequence to get the total
    number of lines in the file:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将行数分成两个序列（理想情况下长度大致相等），然后独立地对每个序列进行合并操作。当我们完成时，只需将每个序列的总行数相加，即可得到文件中的总行数：
- en: '![The reducers library](img/7180OS_05_100.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![Reducers库](img/7180OS_05_100.jpg)'
- en: If each **Reduce** ran on its own processing unit, then the two count operations
    would run in parallel. All the other things being equal, the algorithm would run
    twice as fast. This is one of the aims of the `clojure.core.reducers` library—to
    bring the benefit of parallelism to algorithms implemented on a single machine
    by taking advantage of multiple cores.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每个**Reduce**在自己的处理单元上运行，那么这两个计数操作将会并行执行。其他条件相同的情况下，算法将运行得更快，是原来的两倍。这就是`clojure.core.reducers`库的目标之一——通过利用多核处理器，让算法能够在单台机器上实现并行化。
- en: Parallel folds with reducers
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Reducers的并行折叠
- en: The parallel implementation of reduce implemented by the reducers library is
    called **fold**. To make use of a fold, we have to supply a combiner function
    that will take the results of our reduced sequences (the partial row counts) and
    return the final result. Since our row counts are numbers, the combiner function
    is simply `+`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由reducers库实现的`reduce`的并行版本称为**fold**。为了使用`fold`，我们必须提供一个合并函数，它将接收我们减少过的序列结果（部分行计数）并返回最终结果。由于我们的行计数是数字，所以合并函数就是`+`。
- en: Note
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Reducers are a part of Clojure's standard library, they do not need to be added
    as an external dependency.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Reducers是Clojure标准库的一部分，无需作为外部依赖添加。
- en: 'The adjusted example, using `clojure.core.reducers` as `r`, looks like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的示例，使用`clojure.core.reducers`作为`r`，如下所示：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The combiner function, `+`, has been included as the first argument to fold
    and our unchanged reduce function is supplied as the second argument. We no longer
    need to pass the initial value of zero—`fold` will get the initial value by calling
    the combiner function with no arguments. Our preceding example works because `+`,
    called with no arguments, already returns zero:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 合并函数`+`已作为第一个参数传递给`fold`，我们未更改的`reduce`函数则作为第二个参数传入。我们不再需要传递初始值零——`fold`会通过调用没有参数的合并函数来获取初始值。我们之前的示例之所以有效，是因为`+`在没有参数的情况下已经返回零：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To participate in folding then, it''s important that the combiner function
    have two implementations: one with zero arguments that returns the identity value
    and another with two arguments that *combines* the arguments. Different folds
    will, of course, require different combiner functions and identity values. For
    example, the identity value for multiplication is `1`.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要参与折叠，合并函数必须有两个实现：一个没有参数，返回标识值，另一个有两个参数，用于*合并*这两个参数。当然，不同的折叠操作将需要不同的合并函数和标识值。例如，乘法的标识值是`1`。
- en: 'We can visualize the process of seeding the computation with an identity value,
    iteratively reducing over the sequence of `xs` and combining the reductions into
    an output value as a tree:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将用标识值初始化计算的过程，迭代地在`xs`序列上执行合并，并将所有合并结果作为一棵树的输出值：
- en: '![Parallel folds with reducers](img/7180OS_05_110.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![使用Reducers的并行折叠](img/7180OS_05_110.jpg)'
- en: There may be more than two reductions to combine, of course. The default implementation
    of `fold` will split the input collection into chunks of 512 elements. Our 166,000-element
    sequence will therefore generate 325 reductions to be combined. We're going to
    run out of page real estate quite quickly with a tree representation diagram,
    so let's visualize the process more schematically instead—as a two-step reduce
    and combine process.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，可能有超过两个的归约需要合并。`fold` 的默认实现会将输入集合拆分成 512 个元素的块。我们的 166,000 元素的序列因此会生成 325
    次归约以供合并。由于树形表示图占用的空间较大，我们将很快用尽页面的可用空间，因此让我们改为以更简洁的方式来可视化这个过程——作为一个两步的归约和合并过程。
- en: 'The first step performs a parallel reduce across all the chunks in the collection.
    The second step performs a serial reduce over the intermediate results to arrive
    at the final result:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步在集合中的所有块上执行并行归约。第二步则对中间结果执行串行归约，以得到最终结果：
- en: '![Parallel folds with reducers](img/7180OS_05_120.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![使用 reducers 的并行折叠](img/7180OS_05_120.jpg)'
- en: The preceding representation shows reduce over several sequences of `xs`, represented
    here as circles, into a series of outputs, represented here as squares. The squares
    are combined serially to produce the final result, represented by a star.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上述表示展示了如何对多个 `xs` 序列进行归约，这里以圆圈表示，最终输出以方块表示。这些方块将串行合并，以产生最终结果，用星形表示。
- en: Loading large files with iota
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 iota 加载大文件
- en: Calling `fold` on a lazy sequence requires Clojure to realize the sequence into
    memory and then chunk the sequence into groups for parallel execution. For situations
    where the calculation performed on each row is small, the overhead involved in
    coordination outweighs the benefit of parallelism. We can improve the situation
    slightly by using a library called `iota` ([https://github.com/thebusby/iota](https://github.com/thebusby/iota)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在懒序列上调用 `fold` 需要 Clojure 将序列加载到内存中，然后将序列分块以进行并行执行。对于每行计算较小的情况，协调开销会超过并行化带来的好处。我们可以通过使用一个名为
    `iota` 的库来稍微改善这种情况 ([https://github.com/thebusby/iota](https://github.com/thebusby/iota))。
- en: Note
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `iota` library loads files directly into the data structures suitable for
    folding over with reducers that can handle files larger than available memory
    by making use of memory-mapped files.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`iota` 库通过使用内存映射文件将文件直接加载到适合使用 reducers 进行折叠的数据结构中，可以处理比可用内存更大的文件。'
- en: 'With `iota` in the place of our `line-seq` function, our line count simply
    becomes:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `iota` 代替我们的 `line-seq` 函数后，我们的行数统计变成了：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So far, we've just been working with the sequences of unformatted lines, but
    if we're going to do anything more than counting the rows, we'll want to parse
    them into a more useful data structure. This is another area in which Clojure's
    reducers can help make our code more efficient.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只处理了未格式化的行序列，但如果我们要做的不仅仅是计算行数，我们希望将其解析为更有用的数据结构。这是 Clojure 的 reducers
    可以帮助我们提高代码效率的另一个领域。
- en: Creating a reducers processing pipeline
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个 reducers 处理管道
- en: 'We already know that the file is comma-separated, so let''s first create a
    function to turn each row into a vector of fields. All fields except the first
    two contain numeric data, so let''s parse them into doubles while we''re at it:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道文件是以逗号分隔的，因此让我们首先创建一个函数，将每行转换为一个字段向量。除了前两个字段，其他字段都包含数值数据，因此我们在此过程中将其解析为
    double 类型：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We''re using the reducers version of `map` to apply our `parse-line` function
    to each of the lines from the file in turn:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `map` 的 reducers 版本，将我们的 `parse-line` 函数依次应用到文件中的每一行：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The final `into` function call converts the reducers' internal representation
    (a reducible collection) into a Clojure vector. The previous example should return
    a sequence of 77 fields, representing the first row of the file after the header.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的 `into` 函数调用将 reducers 的内部表示（一个可归约的集合）转换为一个 Clojure 向量。上面的示例应该返回一个包含 77 个字段的序列，表示文件中第一行（头部后）的内容。
- en: 'We''re just dropping the column names at the moment, but it would be great
    if we could make use of these to return a map representation of each record, associating
    the column name with the field value. The keys of the map would be the column
    headings and the values would be the parsed fields. The `clojure.core` function
    `zipmap` will create a map out of two sequences—one for the keys and one for the
    values:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们只是去掉了列名，但如果我们能利用这些列名返回每条记录的映射表示，将列名与字段值关联起来，那就太好了。映射的键将是列名，值将是解析后的字段。`clojure.core`
    函数 `zipmap` 将从两个序列（一个用于键，一个用于值）创建一个映射：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This function returns a map representation of each row, a much more user-friendly
    data structure:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数返回每行的映射表示，这是一种更加用户友好的数据结构：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A great thing about Clojure's reducers is that in the preceding computation,
    calls to `r/map`, `r/drop` and `r/take` are composed into a reduction that will
    be performed in a single pass over the data. This becomes particularly valuable
    as the number of operations increases.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure的reducers的一个很棒的功能是，在上述计算中， `r/map`, `r/drop` 和 `r/take` 的调用被组合成一个减少，将在数据上进行单次遍历。随着操作数量的增加，这变得尤为重要。
- en: 'Let''s assume that we''d like to filter out zero ZIP codes. We could extend
    the reducers pipeline like this:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要过滤掉零邮政编码。我们可以扩展reducer管道如下：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `r/remove` step is now also being run together with the `r/map`, `r/drop`
    and `r/take` calls. As the size of the data increases, it becomes increasingly
    important to avoid making multiple iterations over the data unnecessarily. Using
    Clojure's reducers ensures that our calculations are compiled into a single pass.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`r/remove` 步骤现在也与 `r/map`, `r/drop` 和 `r/take` 调用一起运行。随着数据量的增加，避免不必要地多次迭代数据变得越来越重要。使用Clojure的reducers确保我们的计算编译成单次遍历。'
- en: Curried reductions with reducers
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用reducers的柯里化减少
- en: 'To make the process clearer, we can create a **curried** version of each of
    our previous steps. To parse the lines, create a record from the fields and filter
    zero ZIP codes. The curried version of the function is a reduction waiting for
    a collection:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使过程更加清晰，我们可以为之前的每个步骤创建一个 **柯里化** 版本。解析行，从字段创建记录并过滤掉零邮政编码。函数的柯里化版本是一个等待集合的减少：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In each case, we''re calling one of reducers'' functions, but without providing
    a collection. The response is a curried version of the function that can be applied
    to the collection at a later time. The curried functions can be composed together
    into a single `parse-file` function using `comp`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，我们调用reducers的函数之一，但没有提供集合。响应是函数的柯里化版本，稍后可以应用于集合。这些柯里化函数可以使用 `comp` 组合成单个
    `parse-file` 函数：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It's only when the `parse-file` function is called with a sequence that the
    pipeline is actually executed.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当 `parse-file` 函数与序列一起调用时，管道才会实际执行。
- en: Statistical folds with reducers
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用reducers进行统计折叠
- en: 'With the data parsed, it''s time to perform some descriptive statistics. Let''s
    assume that we''d like to know the mean *number of returns* (column `N1`) submitted
    to the IRS by ZIP code. One way of doing this—the way we''ve done several times
    throughout the book—is by adding up the values and dividing it by the count. Our
    first attempt might look like this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 数据解析完成后，是时候进行一些描述性统计了。假设我们想知道IRS按邮政编码提交的平均*退回数量*（列 `N1`）。有一种方法可以做到这一点——这是本书中多次尝试过的方法——通过将值加起来并除以计数来实现。我们的第一次尝试可能如下所示：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: While this works, it's comparatively slow. We iterate over the data once to
    create `xs`, a second time to calculate the sum, and a third time to calculate
    the count. The bigger our dataset gets, the larger the time penalty we'll pay.
    Ideally, we would be able to calculate the mean value in a single pass over the
    data, just like our `parse-file` function previously. It would be even better
    if we can perform it in parallel too.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这样做可以工作，但速度相对较慢。我们需要对数据进行三次迭代：一次创建 `xs`，一次计算总和，一次计算数量。数据集越大，我们支付的时间成本越大。理想情况下，我们希望能够在单次数据遍历中计算均值，就像我们之前的
    `parse-file` 函数一样。如果能并行执行更好。
- en: Associativity
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合性质
- en: 'Before we proceed, it''s useful to take a moment to reflect on why the following
    code wouldn''t do what we want:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，花点时间反思以下代码为什么不能达到我们想要的效果：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Our `mean` function is a function of two arities. Without arguments, it returns
    zero, the identity for the `mean` computation. With two arguments, it returns
    their mean:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `mean` 函数是两个参数的函数。没有参数时，它返回零，是 `mean` 计算的单位元。有两个参数时，它返回它们的平均值：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding example folds over the `N1` data with our `mean` function and
    produces a different result from the one we obtained previously. If we could expand
    out the computation for the first three `xs`, we might see something like the
    following code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例对 `N1` 数据使用我们的 `mean` 函数进行了折叠，并产生了与之前不同的结果。如果我们能够扩展出前三个 `xs` 的计算，我们可能会看到类似以下代码的内容：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is a bad idea, because the `mean` function is not associative. For an
    associative function, the following holds true:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不好的主意，因为 `mean` 函数不是关联的。对于关联函数，以下条件成立：
- en: '![Associativity](img/7180OS_05_01.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![结合性](img/7180OS_05_01.jpg)'
- en: 'Addition is associative, but multiplication and division are not. So the `mean`
    function is not associative either. Contrast the `mean` function with the following
    simple addition:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 加法是结合律的，但乘法和除法不是。所以`mean`函数也不是结合的。将`mean`函数与以下简单加法进行对比：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This yields an identical result to:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这与以下结果相同：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: It doesn't matter how the arguments to `+` are partitioned. Associativity is
    an important property of functions used to reduce over a set of data because,
    by definition, the results of a previous calculation are treated as inputs to
    the next.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`+`的参数如何分区并不重要。结合性是用于对数据集进行归约的函数的一个重要特性，因为根据定义，先前计算的结果会作为下一个计算的输入。'
- en: The easiest way of converting the `mean` function into an associative function
    is to calculate the sum and the count separately. Since the sum and the count
    are associative, they can be calculated in parallel over the data. The `mean`
    function can be calculated simply by dividing one by the other.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 将`mean`函数转化为结合函数的最简单方法是分别计算和与计数。由于和与计数是结合的，它们可以并行地计算数据。`mean`函数可以通过将两者相除来简单计算。
- en: Calculating the mean using fold
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用fold计算均值
- en: 'We''ll create a fold using two custom functions, `mean-combiner` and `mean-reducer`.
    This requires defining three entities:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用两个自定义函数`mean-combiner`和`mean-reducer`来创建一个fold。这需要定义三个实体：
- en: The identity value for the fold
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fold的身份值
- en: The reduce function
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: reduce函数
- en: The combine function
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: combine函数
- en: 'We discovered the benefits of associativity in the previous section, and so
    we''ll want to update our intermediate `mean` by using associative operations
    only and calculating the sum and count separately. One way of representing the
    two values is a map of two keys, `:count` and `:sum`. The value that represents
    zero for our mean would be a sum of zero and a count of zero, or a map such as
    the following: `{:count 0 :sum 0}`.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一节中发现了结合律的好处，因此我们希望只使用结合操作来更新中间的`mean`，并分别计算和与计数。表示这两个值的一种方式是一个包含`：count`和`：sum`两个键的映射。表示零的值就是和为零，计数为零，或者如下所示的映射：`{:count
    0 :sum 0}`。
- en: 'The combine function, `mean-combiner`, provides the seed value when it''s called
    without arguments. The two-argument combiner needs to add together the `:count`
    and the `:sum` for each of the two arguments. We can achieve this by merging the
    maps with `+`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: combine函数`mean-combiner`在没有参数时会提供初始值。两个参数的combiner需要将每个参数的`:count`和`:sum`相加。我们可以通过使用`+`来合并这些映射：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `mean-reducer` function needs to accept an accumulated value (either an
    identity value or the results of a previous reduction) and incorporate the new
    `x`. We do this simply by incrementing the `:count` and adding `x` to the accumulated
    `:sum`:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`mean-reducer`函数需要接受一个累积值（无论是身份值还是先前计算结果），并将新的`x`纳入其中。我们只需通过递增`:count`并将`x`加到累积的`:sum`中来实现：'
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding two functions are enough to completely specify our `mean` fold:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 前述两个函数已经足够完全定义我们的`mean` fold：
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The result gives us all we need to calculate the mean of `N1`, which is calculated
    in only one pass over the data. The final step of the calculation can be performed
    with the following `mean-post-combiner` function:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 结果给出了我们计算`N1`均值所需的所有信息，而这个计算只需一次遍历数据。计算的最后一步可以通过以下`mean-post-combiner`函数来执行：
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Happily, the values agree with the mean we calculated previously.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，结果与我们之前计算的均值一致。
- en: Calculating the variance using fold
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用fold计算方差
- en: 'Next, we''d like to calculate the variance of the `N1` values. Remember that
    the variance is the mean squared difference from the mean:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们希望计算`N1`值的方差。记住，方差是均值的平方差：
- en: '![Calculating the variance using fold](img/7180OS_05_02.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![使用fold计算方差](img/7180OS_05_02.jpg)'
- en: 'To implement this as a fold, we might write something as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将其实现为fold，我们可能会编写如下代码：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: First, we calculate the `mean` value of the series using the fold we constructed
    just now. Then, we define a function of `x` and `sq-diff`, which calculates the
    squared difference of `x` from the `mean` value. We map it over the squared differences
    and call our `mean` fold a second time to arrive at the final variance result.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用刚才构建的fold计算该系列的`mean`值。然后，我们定义一个关于`x`和`sq-diff`的函数，计算`x`与`mean`值之间的平方差。我们将其映射到平方差上，并再次调用`mean`
    fold，最终得到方差结果。
- en: 'Thus, we make two complete passes over the data, firstly to calculate the mean,
    and secondly to calculate the difference of each `x` from the `mean` value. It
    might seem that calculating the variance is necessarily a sequential algorithm:
    it may not seem possible to reduce the number of steps further and calculate the
    variance in only a single fold over the data.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们对数据进行了两次完整的遍历，首先计算均值，然后计算每个 `x` 与 `mean` 值的差异。看起来计算方差必然是一个顺序算法：似乎无法进一步减少步骤，只能通过单次遍历数据来计算方差。
- en: 'In fact, it is possible to express the variance calculation as a single fold.
    To do so, we need to keep track of three things: the count, the (current) mean,
    and the sum of squared differences:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，方差计算可以用一个折叠表达。为此，我们需要追踪三项内容：计数、（当前）均值和平方差之和：
- en: '[PRE27]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Our combiner function is shown in the preceding code. The identity value is
    a map with all three values set to zero. The zero-arity combiner returns this
    value.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的合并函数在前面的代码中展示。单位值是一个映射，所有三项的值都设为零。零元合并器返回这个值。
- en: 'The two-arity combiner needs to combine the counts, means, and sums-of-squares
    for both of the supplied values. Combining the counts is easy—we simply add them
    together. The means is only marginally trickier: we need to calculate the weighted
    mean of the two means. If one mean is based on fewer records, then it should count
    for less in the combined mean:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 二元合并器需要合并两个传入值的计数、均值和平方和。合并计数很简单——我们只需要将它们相加。均值则稍微复杂一些：我们需要计算两个均值的加权平均。如果一个均值是基于较少的记录计算的，那么它在合并均值时应该占较少的份额：
- en: '![Calculating the variance using fold](img/7180OS_05_03.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![使用折叠计算方差](img/7180OS_05_03.jpg)'
- en: 'Combining the sums of squares is the most complicated calculation. While adding
    the sums of squares, we also need to add a factor to account for the fact that
    the sum of squares from `a` and `b` were likely calculated from differing means:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 合并平方和是最复杂的计算。合并平方和时，我们还需要添加一个因子，以考虑到 `a` 和 `b` 的平方和可能是基于不同的均值计算得出的：
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The reducer is much simpler and contains the explanation on how the variance
    fold is able to calculate the variance in one pass over the data. For each new
    record, the `:mean` value is recalculated from the previous `mean` and current
    `count`. We then add to the sum of squares the product of the difference between
    the means before and after taking account of this new record.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 缩减器要简单得多，并解释了方差折叠如何通过一次数据扫描计算方差。对于每个新记录，`mean` 值是根据之前的 `mean` 和当前的 `count` 重新计算的。然后，我们将平方和增加为均值前后差异的乘积，考虑到这条新记录。
- en: 'The final result is a map containing the `count`, `mean` and total `sum-of-squares`.
    Since the variance is just the `sum-of-squares` divided by the `count`, our `variance-post-combiner`
    function is a relatively simple one:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是一个包含 `count`、`mean` 和总 `sum-of-squares` 的映射。由于方差只是 `sum-of-squares` 除以
    `count`，因此我们的 `variance-post-combiner` 函数是相对简单的：
- en: '[PRE29]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Putting the three functions together yields the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 将这三个函数结合起来，得到如下结果：
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Since the standard deviation is simply the square root of the variance, we only
    need a slightly modified `variance-post-combiner` function to calculate it as
    well.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于标准差只是方差的平方根，因此我们只需要稍微修改过的 `variance-post-combiner` 函数，也可以计算标准差。
- en: Mathematical folds with Tesser
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Tesser 进行数学折叠
- en: We should now understand how to use folds to calculate parallel implementations
    of simple algorithms. Hopefully, we should also have some appreciation for the
    ingenuity required to find efficient solutions that will perform the minimum number
    of iterations over the data.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在应该理解如何使用折叠来计算简单算法的并行实现。希望我们也能理解找到高效解决方案所需的巧思，这样能够在数据上进行最少次数的迭代。
- en: 'Fortunately, the Clojure library Tesser ([https://github.com/aphyr/tesser](https://github.com/aphyr/tesser))
    includes implementations for common mathematical folds, including the mean, standard
    deviation, and covariance. To see how to use Tesser, let''s consider the covariance
    of two fields from the IRS dataset: the salaries and wages, `A00200`, the unemployment
    compensation, `A02300`.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Clojure 库 Tesser ([https://github.com/aphyr/tesser](https://github.com/aphyr/tesser))
    包含了常见数学折叠的实现，包括均值、标准差和协方差。为了了解如何使用 Tesser，我们考虑一下 IRS 数据集中两个字段的协方差：工资和薪金 `A00200`，以及失业补偿
    `A02300`。
- en: Calculating covariance with Tesser
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Tesser 计算协方差
- en: 'We encountered covariance in [Chapter 3](ch03.xhtml "Chapter 3. Correlation"),
    *Correlation*, as a measure of how two sequences of data vary together. The formula
    is reproduced as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](ch03.xhtml "第3章. 相关性")，*相关性*中遇到了协方差，它是用来衡量两组数据如何一起变化的指标。公式如下所示：
- en: '![Calculating covariance with Tesser](img/7180OS_05_04.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![使用Tesser计算协方差](img/7180OS_05_04.jpg)'
- en: 'A covariance fold is included in `tesser.math`. In the following code, we''ll
    include `tesser.math` as `m` and `tesser.core` as `t`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`tesser.math`中包含了协方差fold。在下面的代码中，我们将`tesser.math`作为`m`，`tesser.core`作为`t`来引用：'
- en: '[PRE31]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `m/covariance` function expects to receive two arguments: a function to
    return the `x` value and another to return the `y` value. Since keywords act as
    functions to extract their corresponding values from a map, we simply pass the
    keywords as arguments.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`m/covariance`函数期望接收两个参数：一个返回`x`值的函数，另一个返回`y`值的函数。由于关键字作为提取相应值的函数，我们只需将关键字作为参数传递。'
- en: 'Tesser works in a similar way to Clojure''s reducers, but with some minor differences.
    Clojure''s `fold` takes care of splitting our data into subsequences for parallel
    execution. With Tesser however, we must divide our data into chunks explicitly.
    Since this is something we''re going to do repeatedly, let''s create a little
    helper function called `chunks`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Tesser的工作方式类似于Clojure的reducers，但有一些小差异。Clojure的`fold`负责将我们的数据分割为子序列进行并行执行。而在Tesser中，我们必须显式地将数据分成小块。由于这是我们将要反复做的事情，让我们创建一个名为`chunks`的辅助函数：
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: For the most of the rest of this chapter, we'll be using the `chunks` function
    to split our input data into groups of `1024` records.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的大部分内容中，我们将使用`chunks`函数将输入数据拆分为`1024`条记录的小组。
- en: Commutativity
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交换律
- en: 'Another difference between Clojure''s reducers and Tesser''s folds is that
    Tesser doesn''t guarantee that the input order will be preserved. Along with being
    associative, as we discussed previously, Tesser''s functions must be commutative.
    A commutative function is the one whose result is the same if its arguments are
    provided in a different order:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure的reducers与Tesser的folds之间的另一个区别是，Tesser不保证输入顺序会被保持。除了是结合律的，如我们之前讨论的，Tesser的函数还必须满足交换律。交换律函数是指无论其参数的顺序如何变化，结果都相同的函数：
- en: '![Commutativity](img/7180OS_05_05.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![交换律](img/7180OS_05_05.jpg)'
- en: Addition and multiplication are commutative, but subtraction and division are
    not. Commutativity is a useful property of functions intended for distributed
    data processing, because it lowers the amount of coordination required between
    subtasks. When Tesser executes a combine function, it's free to do so on whichever
    reducer functions return their values first. If the order doesn't matter, it doesn't
    need to wait for the first to complete.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 加法和乘法是交换律的，但减法和除法不是。交换律是分布式数据处理函数的一个有用属性，因为它减少了子任务之间协调的需求。当Tesser执行合并函数时，它可以在任何先返回结果的reducer函数上进行，而不需要等到第一个完成。
- en: 'Let''s rewrite our `load-data` function into a `prepare-data` function that
    will return a commutative Tesser fold. It performs the same steps (parsing a line
    of the text file, formatting the record as a map and removing zero ZIP codes)
    that our previous reducers-based function did, but it no longer assumes that the
    column headers will be the first row in the file—*first* is a concept that explicitly
    requires ordered data:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将`load-data`函数改写为`prepare-data`函数，它将返回一个交换律的Tesser fold。它执行与我们之前基于reducer的函数相同的步骤（解析文本文件中的一行，格式化记录为map并删除零ZIP代码），但它不再假设列头是文件中的第一行——*first*是一个明确要求有序数据的概念：
- en: '[PRE33]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now that all the preparation is being done in Tesser, we can pass the result
    of `iota/seq` directly as input. This will be particularly useful when we come
    to run our code distributed on Hadoop later in the chapter:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有的准备工作都已在Tesser中完成，我们可以直接将`iota/seq`的结果作为输入。这在本章后面我们将代码分布式运行在Hadoop时尤其有用：
- en: '[PRE34]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In [Chapter 3](ch03.xhtml "Chapter 3. Correlation"), *Correlation*, we saw
    how in the case of simple linear regression with one feature and one response
    variable, the correlation coefficient is the covariance over the product of standard
    deviations:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.xhtml "第3章. 相关性")，*相关性*中，我们看到在简单线性回归中，当只有一个特征和一个响应变量时，相关系数是协方差与标准差乘积的比值：
- en: '![Commutativity](img/7180OS_05_06.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![交换律](img/7180OS_05_06.jpg)'
- en: 'Tesser includes functions to calculate the correlation of a pair of attributes
    as a fold too:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Tesser还包括计算一对属性相关性的函数，作为一个fold来实现：
- en: '[PRE35]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: There's a modest, positive correlation between these two variables. Let's build
    a linear model that predicts the value of unemployment compensation, `A02300`,
    using salaries and wages, `A00200`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个变量之间存在适度的正相关。让我们构建一个线性模型，用工资和薪金（`A00200`）来预测失业补偿（`A02300`）的值。
- en: Simple linear regression with Tesser
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Tesser 进行简单线性回归
- en: 'Tesser doesn''t currently provide a linear regression fold, but it does give
    us the tools we need to implement one. We saw in [Chapter 3](ch03.xhtml "Chapter 3. Correlation"),
    *Correlation*, how the coefficients for a simple linear regression model, the
    slope and the intercept, can be calculated as a simple function of the variance,
    covariance, and means of the two inputs:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Tesser 当前没有提供线性回归的 fold，但它提供了我们实现线性回归所需的工具。在[第3章](ch03.xhtml "第3章. 相关性")*相关性*中，我们看到如何通过输入的方差、协方差和均值来计算简单线性回归模型的系数——斜率和截距：
- en: '![Simple linear regression with Tesser](img/7180OS_05_07.jpg)![Simple linear
    regression with Tesser](img/7180OS_05_08.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Tesser 进行简单线性回归](img/7180OS_05_07.jpg)![使用 Tesser 进行简单线性回归](img/7180OS_05_08.jpg)'
- en: The slope *b* is the covariance divided by the variance in *X*. The intercept
    is the value that ensures the regression line passes through the means of both
    the series. Ideally, therefore, we'd be able to calculate each of these four variables
    in a single fold over the data. Tesser provides two fold combinators, `t/fuse`
    and `t/facet`, to build more sophisticated folds out of more basic folds.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率 *b* 是协方差除以 *X* 的方差。截距是确保回归线穿过两个序列均值的值。因此，理想情况下，我们能够在数据的单个 fold 中计算出这四个变量。Tesser
    提供了两个 fold 组合器，`t/fuse` 和 `t/facet`，用于将更基本的 folds 组合成更复杂的 folds。
- en: 'In cases where we have one input record and multiple calculations to be run
    in parallel, we should use `t/fuse`. For example, in the following example, we''re
    fusing the mean and the standard deviation folds into a single fold that will
    calculate both values at once:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们有一个输入记录并且需要并行运行多个计算的情况下，我们应该使用 `t/fuse`。例如，在以下示例中，我们将均值和标准差的 fold 融合为一个单独的
    fold，同时计算这两个值：
- en: '[PRE36]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here, we have the same calculation to run on all the fields in the map; therefore,
    we should use `t/facet`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要对映射中的所有字段进行相同的计算；因此，我们应该使用 `t/facet`：
- en: '[PRE37]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In the preceding code, we selected only two values from the record (`A00200`
    and `A02300`) and calculated the `mean` value for both of them simultaneously.
    Returning to the challenge of performing simple linear regression—we have four
    numbers to calculate, so let''s `fuse` them together:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们只选择了记录中的两个值（`A00200` 和 `A02300`），并同时计算了它们的 `mean` 值。回到执行简单线性回归的挑战——我们有四个数值需要计算，因此我们将它们
    `fuse`（结合）起来：
- en: '[PRE38]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`fuse` very succinctly binds together the calculations we want to perform.
    In addition, it allows us to specify a `post-combine` step to be included as part
    of the fuse. Rather than handing the result off to another function to finalize
    the output, we can specify it directly as an integral part of the fold. The `post-combine`
    step receives the four results and calculates the slope and intercept from them,
    returning the two coefficients as a vector.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`fuse` 非常简洁地将我们想要执行的计算结合在一起。此外，它允许我们指定一个 `post-combine` 步骤，作为 fuse 的一部分。我们可以直接将其作为
    fold 的一个整体来指定，而不是将结果交给另一个函数来完成输出。`post-combine` 步骤接收四个结果，并从中计算出斜率和截距，返回两个系数作为一个向量。'
- en: Calculating a correlation matrix
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算相关性矩阵
- en: 'We''ve only compared two features to see how they are correlated, but Tesser
    makes it very simple to look at the inter-correlation of a large number of target
    features. We supply the target features as a map of the feature name to some function
    of the input record that returns the desired feature. In [Chapter 3](ch03.xhtml
    "Chapter 3. Correlation"), *Correlation*, for example, we would have taken the
    logarithm of the height. Here, we will simply extract each of the features as
    it is and provide human-readable names for each of them:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只比较了两个特征，看看它们之间的相关性，但 Tesser 使得查看多个目标特征之间的相互关联变得非常简单。我们将目标特征提供为一个映射，其中特征名称对应于输入记录中某个函数，这个函数返回所需的特征。例如，在[第3章](ch03.xhtml
    "第3章. 相关性")*相关性*中，我们本来会取身高的对数。在这里，我们将简单地提取每个特征，并为它们提供易于理解的名称：
- en: '[PRE39]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Tesser will calculate the correlation between each pair of features and return
    the results in a map. The map is keyed by tuples (vectors of two elements) containing
    the names of each pair of features, and the associated value is the correlation
    between them.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Tesser将计算每对特征之间的相关性，并将结果以映射的形式返回。映射以包含每对特征名称的元组（两个元素的向量）为键，相关值为它们之间的相关性。
- en: If you run the preceding example now, you'll find that there are a high correlations
    between some of the variables. For example, the correlation between `:dependents`
    and `:unemployment-compensation` is `0.821`. Let's build a linear regression model
    that uses all of these variables as inputs.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在运行前面的示例，你会发现某些变量之间有较高的相关性。例如，`:dependents`和`:unemployment-compensation`之间的相关性为`0.821`。让我们建立一个线性回归模型，将所有这些变量作为输入。
- en: Multiple regression with gradient descent
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降的多元回归
- en: 'When we ran multiple linear regression in [Chapter 3](ch03.xhtml "Chapter 3. Correlation"),
    *Correlation*, we used the normal equation and matrices to quickly arrive at the
    coefficients for a multiple linear regression model. The normal equation is repeated
    as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[第3章](ch03.xhtml "第3章. 相关性")，*相关性*中运行多元线性回归时，我们使用了正规方程和矩阵来快速得出多元线性回归模型的系数。正规方程如下重复：
- en: '![Multiple regression with gradient descent](img/7180OS_05_09.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降的多元回归](img/7180OS_05_09.jpg)'
- en: The normal equation uses matrix algebra to very quickly and efficiently arrive
    at the least squares estimates. Where all data fits in memory, this is a very
    convenient and concise equation. Where the data exceeds the memory available to
    a single machine however, the calculation becomes unwieldy. The reason for this
    is matrix inversion. The calculation of ![Multiple regression with gradient descent](img/7180OS_05_10.jpg)
    is not something that can be accomplished on a fold over the data—each cell in
    the output matrix depends on many others in the input matrix. These complex relationships
    require that the matrix be processed in a nonsequential way.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 正规方程使用矩阵代数来非常快速且高效地得出最小二乘估计。当所有数据都能装入内存时，这是一个非常方便且简洁的方程。然而，当数据超出单台机器可用的内存时，计算变得笨重。原因在于矩阵求逆。计算![梯度下降的多元回归](img/7180OS_05_10.jpg)并不是可以在数据上折叠一次完成的——输出矩阵中的每个单元格都依赖于输入矩阵中的许多其他单元格。这些复杂的关系要求矩阵必须以非顺序的方式处理。
- en: An alternative approach to solve linear regression problems, and many other
    related machine learning problems, is a technique called **gradient descent**.
    Gradient descent reframes the problem as the solution to an iterative algorithm—one
    that does not calculate the answer in one very computationally intensive step,
    but rather converges towards the correct answer over a series of much smaller
    steps.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 解决线性回归问题以及许多其他相关机器学习问题的另一种方法是**梯度下降**技术。梯度下降将问题重新构造为一个迭代算法的解决方案——这个算法并不是在一个计算量极大的步骤中计算答案，而是通过一系列更小的步骤逐渐接近正确答案。
- en: We encountered gradient descent in the previous chapter, when we used Incanter's
    `minimize` function to calculate the parameters that produced the lowest cost
    for our logistic regression classifier. As the volume of data increases, Incanter
    no longer remains a viable solution to run gradient descent. In the next section,
    we'll see how we can run gradient descent for ourselves using Tesser.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们遇到了梯度下降，当时我们使用了Incanter的`minimize`函数来计算出使我们逻辑回归分类器成本最低的参数。随着数据量的增加，Incanter
    不再是执行梯度下降的可行方案。在下一节中，我们将看到如何使用Tesser自行运行梯度下降。
- en: The gradient descent update rule
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降更新规则
- en: Gradient descent works by the iterative application of a function that moves
    the parameters in the direction of their optimum values. To apply this function,
    we need to know the gradient of the cost function with the current parameters.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降通过反复应用一个函数来工作，这个函数将参数朝着其最优值的方向移动。为了应用这个函数，我们需要知道当前参数下的成本函数的梯度。
- en: 'Calculating the formula for the gradient involves calculus that''s beyond the
    scope of this book. Fortunately, the resulting formula isn''t terribly difficult
    to interpret:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 计算梯度公式涉及微积分内容，超出了本书的范围。幸运的是，最终的公式并不难理解：
- en: '![The gradient descent update rule](img/7180OS_05_11.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降更新规则](img/7180OS_05_11.jpg)'
- en: '![The gradient descent update rule](img/7180OS_05_12.jpg) is the partial derivative,
    or the gradient, of our cost function *J(β)* for the parameter at index *j*. Therefore,
    we can see that the gradient of the cost function with respect to the parameter
    at index *j* is equal to the difference between our prediction and the true value
    of *y* multiplied by the value of *x* at index *j*.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '![梯度下降更新规则](img/7180OS_05_12.jpg)是我们代价函数*J(β)*相对于索引*j*的参数的偏导数或梯度。因此，我们可以看到，代价函数相对于索引*j*的参数的梯度等于我们预测值与真实值*y*之间的差乘以索引*j*的特征值*x*。'
- en: 'Since we''re seeking to descend the gradient, we want to subtract some proportion
    of the gradient from the current parameter values. Thus, at each step of gradient
    descent, we perform the following update:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们希望沿着梯度下降，所以我们希望从当前的参数值中减去梯度的某个比例。因此，在每一步梯度下降中，我们会执行以下更新：
- en: '![The gradient descent update rule](img/7180OS_05_13.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降更新规则](img/7180OS_05_13.jpg)'
- en: Here, `:=` is the assigment operator and *α* is a factor called the **learning
    rate**. The learning rate controls how large an adjustment we wish make to the
    parameters at each iteration as a fraction of the gradient. If our prediction
    *ŷ* nearly matches the actual value of *y*, then there would be little need to
    change the parameters. In contrast, a larger error will result in a larger adjustment
    to the parameters. This rule is called the **Widrow-Hoff learning rule** or the
    **Delta rule**.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`:=`是赋值操作符，*α*是一个叫做**学习率**的因子。学习率控制我们希望在每次迭代中根据梯度对参数进行调整的大小。如果我们的预测*ŷ*几乎与实际值*y*相符，那么对参数的调整就不大。相反，较大的误差会导致对参数进行更大的调整。这个规则被称为**Widrow-Hoff学习规则**或**Delta规则**。
- en: The gradient descent learning rate
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降学习率
- en: 'As we''ve seen, gradient descent is an iterative algorithm. The learning rate,
    usually represented by *α*, dictates the speed at which the gradient descent converges
    to the final answer. If the learning rate is too small, convergence will happen
    very slowly. If it is too large, gradient descent will not find values close to
    the optimum and may even diverge from the correct answer:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，梯度下降是一个迭代算法。学习率，通常用*α*表示，决定了梯度下降收敛到最终答案的速度。如果学习率太小，收敛过程会非常缓慢。如果学习率过大，梯度下降就无法找到接近最优值的结果，甚至可能会从正确答案发散出去：
- en: '![The gradient descent learning rate](img/7180OS_05_130.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降学习率](img/7180OS_05_130.jpg)'
- en: 'In the preceding chart, a small learning rate leads to a show convergence over
    many iterations of the algorithm. While the algorithm does reach the minimum,
    it does so over many more steps than is ideal and, therefore, may take considerable
    time. By contrast, in following diagram, we can see the effect of a learning rate
    that is too large. The parameter estimates are changed so significantly between
    iterations that they actually overshoot the optimum values and diverge from the
    minimum value:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，一个较小的学习率会导致算法在多次迭代中收敛得非常慢。虽然算法最终能够达到最小值，但它需要经过比理想情况更多的步骤，因此可能会花费相当长的时间。相反，在下图中，我们可以看到一个过大学习率的效果。参数估计在每次迭代之间变化如此剧烈，以至于它们实际上超出了最优值，并且从最小值开始发散：
- en: '![The gradient descent learning rate](img/7180OS_05_140.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降学习率](img/7180OS_05_140.jpg)'
- en: The gradient descent algorithm requires us to iterate repeatedly over our dataset.
    With the correct version of alpha, each iteration should successively yield better
    approximations of the ideal parameters. We can choose to terminate the algorithm
    when either the change between iterations is very small or after a predetermined
    number of iterations.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法要求我们在数据集上反复迭代。在选择正确的α值后，每次迭代应该会逐步提供更接近理想参数的估计值。当迭代之间的变化非常小，或者达到了预定的迭代次数时，我们可以选择终止算法。
- en: Feature scaling
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征缩放
- en: As more features are added to the linear model, it is important to scale features
    appropriately. Gradient descent will not perform very well if the features have
    radically different scales, since it won't be possible to pick a learning rate
    to suit them all.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 随着更多特征被添加到线性模型中，适当缩放特征变得非常重要。如果特征的尺度差异非常大，梯度下降的表现会很差，因为无法为所有特征选择一个合适的学习率。
- en: 'A simple scaling we can perform is to subtract the mean value from each of
    the values and divide it by the standard-deviation. This will tend to produce
    values with zero mean that generally vary between `-3` and `3`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以执行的简单缩放是从每个值中减去均值并除以标准差。这将使得值趋于零均值，通常在`-3`和`3`之间波动：
- en: '[PRE40]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The feature-factors function in the preceding code uses `t/facet` to calculate
    the `mean` value and standard deviation of all the input features:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码中的`feature-factors`函数使用`t/facet`来计算所有输入特征的`mean`值和标准差：
- en: '[PRE41]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'If you run the preceding example, you''ll see the different means and standard
    deviations returned by the `feature-scales` function. Since our feature scales
    and input records are represented as maps, we can perform the scale across all
    the features at once using Clojure''s `merge-with` function:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行上述示例，你会看到`feature-scales`函数返回的不同均值和标准差。由于我们的特征缩放和输入记录表示为 maps，我们可以使用 Clojure
    的`merge-with`函数一次性对所有特征进行缩放：
- en: '[PRE42]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Likewise, we can perform the all-important reversal with `unscale-features`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以使用`unscale-features`进行至关重要的反转操作：
- en: '[PRE43]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let''s scale our features and take a look at the very first feature. Tesser
    won''t allow us to execute a fold without a reduce, so we''ll temporarily revert
    to using Clojure''s reducers:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对特征进行缩放并查看第一个特征。Tesser 不允许我们在没有 reducer 的情况下执行折叠操作，因此我们将暂时恢复使用 Clojure 的
    reducers：
- en: '[PRE44]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This simple step will help gradient descent perform optimally on our data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步骤将帮助梯度下降在我们的数据上进行优化。
- en: Feature extraction
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征提取
- en: Although we've used maps to represent our input data in this chapter, it's going
    to be more convenient when running gradient descent to represent our features
    as a matrix. Let's write a function to transform our input data into a map of
    `xs` and `y`. The `y` axis will be a scalar response value and `xs` will be a
    matrix of scaled feature values.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在本章中使用 maps 来表示输入数据，但在运行梯度下降时，将特征表示为矩阵会更加方便。让我们编写一个函数，将输入数据转换为一个包含`xs`和`y`的
    map。`y`轴将是标量响应值，`xs`将是缩放后的特征值矩阵。
- en: 'As in the previous chapters, we''re adding a bias term to the returned matrix
    of features:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所示，我们将偏置项添加到返回的特征矩阵中：
- en: '[PRE45]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Our `feature-matrix` function simply accepts an input of a record and the features
    to convert into a matrix. We call this from within `extract-features`, which returns
    a function that we can call on each input record:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`feature-matrix`函数仅接受一个记录输入和要转换为矩阵的特征。我们在`extract-features`中调用此函数，该函数返回一个可以应用于每个输入记录的函数：
- en: '[PRE46]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The preceding example shows the data converted into a format suitable to perform
    gradient descent: a map containing the `y` response variable and a matrix of values,
    including the bias term.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例展示了数据被转换为适合进行梯度下降的格式：一个包含`y`响应变量的 map 和包含偏置项的值矩阵。
- en: Creating a custom Tesser fold
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建自定义 Tesser 折叠
- en: Each iteration of gradient descent adjusts the coefficients by an amount determined
    by the cost function. The cost function is calculated by summing over the errors
    for each parameter in the dataset, so it will be useful to have a fold that sums
    the values of the matrices element-wise.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的每次迭代都会根据成本函数确定的值调整系数。成本函数是通过对数据集中每个参数的误差求和计算得出的，因此，拥有一个对矩阵元素进行逐项求和的折叠函数将非常有用。
- en: 'Whereas Clojure represents a fold with a reducer, a combiner, and an identity
    value obtained from the combiner, Tesser folds are expressed as six collaborative
    functions. The implementation of Tesser''s `m/mean` fold is as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 而 Clojure 通过 reducer、combiner 以及从 combiner 获得的身份值来表示折叠，Tesser 的折叠则通过六个协作函数来表达。Tesser
    的`m/mean`折叠的实现如下：
- en: '[PRE47]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Tesser chooses to represent the `reducer` identity separately from the `combiner`
    function, and includes three other functions as well; the `combiner-identity`,
    `post-reducer`, and `post-combiner` functions. Tesser's `mean` fold represents
    the pair of numbers (the count and the accumulated sum) as a vector of two numbers
    but, in other respects, it's similar to our own.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Tesser 选择将`reducer`身份与`combiner`函数分开表示，并且还包含另外三个函数：`combiner-identity`、`post-reducer`和`post-combiner`函数。Tesser
    的`mean`折叠将一对数字（计数和累积和）表示为两个数字的向量，但在其他方面，它与我们自己的折叠类似。
- en: '![Creating a custom Tesser fold](img/7180OS_05_150.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![创建自定义 Tesser 折叠](img/7180OS_05_150.jpg)'
- en: We've already seen how to make use of a `post-combiner` function with our `mean-post-combiner`
    and `variance-post-combiner` functions earlier in the chapter.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何使用`post-combiner`函数，与我们在本章前面提到的`mean-post-combiner`和`variance-post-combiner`函数配合使用。
- en: Creating a matrix-sum fold
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个矩阵求和折叠
- en: To create a custom `matrix-sum` fold, we'll need an identity value. We encountered
    the identity matrix in [Chapter 3](ch03.xhtml "Chapter 3. Correlation"), *Correlation*,
    but this is the identity for matrix multiplication not addition. If the identity
    value for `+` is zero (because adding zero to a number doesn't change it), it
    follows that the identity matrix for matrix addition is simply a matrix of zeros.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个自定义的`matrix-sum`折叠，我们需要一个单位值。我们在[第三章](ch03.xhtml "第3章 相关性")《相关性》中遇到过单位矩阵，但这是矩阵乘法的单位矩阵，而不是加法的单位矩阵。如果`+`的单位值是零（因为将零加到一个数字上不会改变它），那么矩阵加法的单位矩阵就是一个全零矩阵。
- en: 'We have to make sure that the matrix is the same size as the matrices we want
    to add. So, let''s parameterize our `matrix-sum` fold with the number rows and
    columns for the matrix. We can''t know in advance how large this needs to be,
    because the identity function is called before anything else in the fold:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须确保矩阵的大小与我们想要相加的矩阵相同。所以，让我们使用矩阵的行数和列数来参数化我们的`matrix-sum`折叠。我们无法提前知道需要多大，因为单位函数会在折叠中的任何操作之前被调用：
- en: '[PRE48]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The preceding example is the completed `matrix-sum` fold definition. We don''t
    provide the `post-combiner` and `post-reducer` functions; since, if omitted, these
    are assumed to be the identity function, which is what we want. We can use our
    new fold to calculate a sum of all the features in our input like this:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例是完整的`matrix-sum`折叠定义。我们没有提供`post-combiner`和`post-reducer`函数；因为如果省略这些，默认它们是单位函数，这正是我们想要的。我们可以使用新的折叠来计算输入中所有特征的总和，如下所示：
- en: '[PRE49]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Calculating the sum of a matrix gets us closer to being able to perform gradient
    descent. Let's use our new fold to calculate the total model error, given some
    initial coefficients.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 计算矩阵的总和让我们更接近执行梯度下降的目标。让我们使用新的折叠计算总模型误差，前提是我们有一些初始系数。
- en: Calculating the total model error
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算总模型误差
- en: 'Let''s take a look again at the delta rule for gradient descent:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一看梯度下降的delta规则：
- en: '![Calculating the total model error](img/7180OS_05_14.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![计算总模型误差](img/7180OS_05_14.jpg)'
- en: 'For each parameter *j*, we adjust the parameter by some proportion of the overall
    prediction error, *ŷ - y*, multiplied by the feature. Larger features, therefore,
    get a larger share of the cost than smaller features and are adjusted by a correspondingly
    larger amount. To implement this in the code, we need to calculate:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个参数* j *，我们根据总体预测误差* ŷ - y *的某个比例来调整该参数，并乘以特征。因此，较大的特征比较小的特征承担更多的成本，并相应地调整更大的量。为了在代码中实现这一点，我们需要计算：
- en: '![Calculating the total model error](img/7180OS_05_15.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![计算总模型误差](img/7180OS_05_15.jpg)'
- en: 'This is the sum of the prediction error multiplied by the feature across all
    the input records. As we did earlier, our predicted value of *y* will be calculated
    using the following formula for each input record *x*:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这是所有输入记录中特征与预测误差乘积的总和。正如我们之前所做的那样，我们的预测值* y *将使用以下公式为每个输入记录* x *计算：
- en: '![Calculating the total model error](img/7180OS_05_16.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![计算总模型误差](img/7180OS_05_16.jpg)'
- en: 'The coefficients *β* will be the same across all our input records, so let''s
    create a `calculate-error` function. Given the transposed coefficients *β*^T,
    we return a function that will calculate ![Calculating the total model error](img/7180OS_05_17.jpg).
    Since *x* is a matrix and *ŷ - y* is a scalar, the result will be a matrix:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 系数*β*在所有输入记录中都是相同的，因此让我们创建一个`calculate-error`函数。给定转置的系数*β*^T，我们返回一个函数来计算！[计算总模型误差](img/7180OS_05_17.jpg)。由于*
    x *是一个矩阵，* ŷ - y *是一个标量，结果将是一个矩阵：
- en: '[PRE50]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'To calculate the sum of the error for the entire dataset, we can simply chain
    our previously defined `matrix-sum` function after the `calculate-error` step:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算整个数据集的误差总和，我们可以简单地在`calculate-error`步骤后连接我们之前定义的`matrix-sum`函数：
- en: '[PRE51]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Notice how the gradient is negative for all the features. This means that in
    order to descend the gradient and produce better estimates of the model coefficients,
    parameters must be increased.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有特征的梯度都是负的。这意味着，为了下降梯度并生成更好的模型系数估计，必须增加参数。
- en: Creating a matrix-mean fold
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个矩阵均值折叠
- en: 'The update rule defined in the previous code actually calls for the mean of
    the cost to be assigned to each of the features. This means that we need both
    `sum` and `count` to be calculated. We don''t want to perform two separate passes
    over the data. So, as we did previously, we `fuse` the two folds into one:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码中定义的更新规则实际上要求将代价的平均值分配给每一个特征。这意味着我们需要计算`sum`和`count`。我们不想对数据执行两次单独的遍历。因此，正如我们之前做的那样，我们将这两个折叠合并成一个：
- en: '[PRE52]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The `fuse` function will return a map of `:sum` and `:count`, so we'll call
    `post-combine` on the result. The `post-combine` function specifies a function
    to be run at the end of our fold which simply divides the sum by the count.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`fuse`函数将返回`：sum`和`：count`的映射，因此我们将在结果上调用`post-combine`。`post-combine`函数指定了一个在折叠结束时运行的函数，该函数简单地将总和除以计数。'
- en: 'Alternatively, we could create another custom fold to return the mean instead
    of the sum of a sequence of matrices. It has a lot in common with the `matrix-sum`
    fold defined previously but, like the `mean` fold we calculated earlier in the
    chapter, we will also keep track of the count of records processed:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以创建另一个自定义折叠，返回一个矩阵序列的均值，而不是总和。它与之前定义的`matrix-sum`折叠有很多相似之处，但与我们在本章前面计算的`mean`折叠一样，我们还需要跟踪处理过的记录数：
- en: '[PRE53]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The reducer identity is a vector containing `[zeros-matrix 0]`. Each reduction
    adds to the matrix total and increments the counter by one. Each combine step
    sums the two matrices—and both the counts—to yield a total sum and count over
    all the partitions. Finally, in the `post-combiner` step, the mean is calculated
    as the ratio of `sum` and `count`.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 减少器的恒等式是一个包含`[zeros-matrix 0]`的向量。每次减少都会将值添加到矩阵总和中，并将计数器加一。每个合并步骤都会将两个矩阵以及它们的计数相加，以得出所有分区的总和和总计数。最后，在`post-combiner`步骤中，均值被计算为`sum`与`count`的比值。
- en: 'Although the code for the custom fold is more lengthy than our fused `sum`
    and `count` solution, we now have a general way of computing the means of matrices.
    It leads to more concise and readable examples and we can use it in our error-calculating
    code like this:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自定义折叠的代码比我们合并的`sum`和`count`解决方案要更长，但我们现在有了一种计算矩阵均值的一般方法。这使得示例更加简洁易读，而且我们可以像这样在误差计算代码中使用它：
- en: '[PRE54]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The small extra effort of creating a custom fold has made the intention of the
    calling code a little easier to follow.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自定义折叠的小额额外努力，使得调用代码的意图变得更容易理解。
- en: Applying a single step of gradient descent
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用梯度下降的单步
- en: 'The objective of calculating the cost is to determine the amount by which to
    adjust each of the coefficients. Once we''ve calculated the average cost, as we
    did previously, we need to update the estimate of our coefficients *β*. Together,
    these steps represent a single iteration of gradient descent:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 计算代价的目标是确定调整每个系数的幅度。一旦我们计算出平均代价，正如我们之前所做的那样，我们需要更新对系数*β*的估计。这些步骤合起来代表梯度下降的单次迭代：
- en: '![Applying a single step of gradient descent](img/7180OS_05_14.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![应用梯度下降的单步](img/7180OS_05_14.jpg)'
- en: 'We can return the updated coefficients in a `post-combiner` step that makes
    use of the average cost, the value of alpha, and the previous coefficients. Let''s
    create a utility function `update-coefficients`, which will receive the coefficients
    and alpha and return a function that will calculate the new coefficients, given
    a total model cost:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在`post-combiner`步骤中返回更新后的系数，该步骤利用平均代价、alpha的值和之前的系数。我们来创建一个实用函数`update-coefficients`，它将接收系数和alpha，并返回一个函数，该函数将在给定总模型代价的情况下计算新的系数：
- en: '[PRE55]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'With the preceding function in place, we have everything we need to package
    up a batch gradient descent update rule:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的函数到位后，我们就有了将批量梯度下降更新规则打包的所有必要工具：
- en: '[PRE56]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The resulting matrix represents the values of the coefficients after the first
    iteration of gradient descent.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 结果矩阵表示梯度下降第一次迭代后的系数值。
- en: Running iterative gradient descent
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行迭代梯度下降
- en: Gradient descent is an iterative algorithm, and we will usually need to run
    it many times to convergence. With a large dataset, this can be very time-consuming.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一个迭代算法，通常需要运行多次才能收敛。对于一个大型数据集，这可能非常耗时。
- en: 'To save time, we''ve included a random sample of `soi.csv` in the data directory
    called `soi-sample.csv`. The smaller size allows us to run iterative gradient
    descent in a reasonable timescale. The following code runs gradient descent for
    100 iterations, plotting the values of the parameters between each iteration on
    an `xy-plot`:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省时间，我们在数据目录中包含了一个名为`soi-sample.csv`的`soi.csv`随机样本。较小的文件大小使得我们能够在合理的时间尺度内运行迭代梯度下降。以下代码执行100次梯度下降迭代，并绘制每次迭代之间的参数值在`xy-plot`上的变化：
- en: '[PRE57]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'If you run the example, you should see a chart similar to the following:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个示例，你应该会看到一个类似于下图的图表：
- en: '![Running iterative gradient descent](img/7180OS_05_160.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![Running iterative gradient descent](img/7180OS_05_160.jpg)'
- en: In the preceding chart, you can see how the parameters converge to relatively
    stable the values over the course of 100 iterations.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图表中，你可以看到在100次迭代过程中，参数是如何趋向相对稳定的值的。
- en: Scaling gradient descent with Hadoop
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Hadoop扩展梯度下降
- en: The length of time each iteration of batch gradient descent takes to run is
    determined by the size of your data and by how many processors your computer has.
    Although several chunks of data are processed in parallel, the dataset is large
    and the processors are finite. We've achieved a speed gain by performing calculations
    in parallel, but if we double the size of the dataset, the runtime will double
    as well.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降每次迭代运行所需的时间由数据大小和计算机的处理器数量决定。尽管多个数据块是并行处理的，但数据集很大，且处理器是有限的。通过并行计算，我们已经实现了速度提升，但如果我们将数据集的大小翻倍，运行时间也会翻倍。
- en: Hadoop is one of several systems that has emerged in the last decade which aims
    to parallelize work that exceeds the capabilities of a single machine. Rather
    than running code across multiple processors, Hadoop takes care of running a calculation
    across many servers. In fact, Hadoop clusters can, and some do, consist of many
    thousands of servers.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop是过去十年中出现的多个系统之一，旨在并行化超出单台机器能力的工作。Hadoop并不是将代码分发到多个处理器上执行，而是负责在多个服务器上运行计算。实际上，Hadoop集群可以，也确实有很多，包含成千上万的服务器。
- en: Hadoop consists of two primary subsystems— the **Hadoop Distributed File System**
    (**HDFS**)—and the job processing system, **MapReduce**. HDFS stores files in
    chunks. A given file may be composed of many chunks and chunks are often replicated
    across many servers. In this way, Hadoop can store quantities of data much too
    large for any single server and, through replication, ensure that the data is
    stored reliably in the event of hardware failure too. As the name implies, the
    MapReduce programming model is built around the concept of map and reduce steps.
    Each job is composed of at least one map step and may optionally specify a reduce
    step. An entire job may consist of several map and reduce steps chained together.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop由两个主要子系统组成——**Hadoop分布式文件系统**（**HDFS**）——和作业处理系统**MapReduce**。HDFS将文件存储为块。一个文件可能由许多块组成，且这些块通常会在多个服务器之间进行复制。通过这种方式，Hadoop能够存储任何单一服务器无法处理的庞大数据量，并且通过复制确保数据在硬件故障时能够可靠存储。正如其名所示，MapReduce编程模型围绕map和reduce步骤构建。每个作业至少包含一个map步骤，并可以选择性地指定一个reduce步骤。一个完整的作业可能由多个map和reduce步骤串联而成。
- en: '![Scaling gradient descent with Hadoop](img/7180OS_05_170.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![Scaling gradient descent with Hadoop](img/7180OS_05_170.jpg)'
- en: In the respect that reduce steps are optional, Hadoop has a slightly more flexible
    approach to distributed calculation than Tesser. Later in this chapter and in
    the future chapters, we'll explore more of the capabilities that Hadoop has to
    offer. Tesser does enable us to convert our folds into Hadoop jobs, so let's do
    this next.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在reduce步骤是可选的这一点上，Hadoop相比Tesser在分布式计算方面有一个稍微更灵活的方法。在本章以及未来章节中，我们将进一步探讨Hadoop提供的更多功能。Tesser确实使我们能够将折叠转换为Hadoop作业，接下来我们就来做这个。
- en: Gradient descent on Hadoop with Tesser and Parkour
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Tesser和Parkour在Hadoop上运行梯度下降
- en: Tesser's Hadoop capabilities are available in the `tesser.hadoop` namespace,
    which we're including as `h`. The primary public API function in the Hadoop namespace
    is `h/fold`.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Tesser的Hadoop功能可以在`tesser.hadoop`命名空间中找到，我们将其包含为`h`。Hadoop命名空间中的主要公共API函数是`h/fold`。
- en: The `fold` function expects to receive at least four arguments, representing
    the configuration of the Hadoop job, the input file we want to process, a working
    directory for Hadoop to store its intermediate files, and the fold we want to
    run, referenced as a Clojure var. Any additional arguments supplied will be passed
    as arguments to the fold when it is executed.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '`fold` 函数期望接收至少四个参数，分别代表 Hadoop 作业的配置、我们想要处理的输入文件、Hadoop 用于存储临时文件的工作目录，以及我们希望执行的
    `fold`，它被引用为 Clojure `var`。任何额外的参数将在执行时作为参数传递给 `fold`。'
- en: The reason for using a var to represent our fold is that the function call initiating
    the fold may happen on a completely different computer than the one that actually
    executes it. In a distributed setting, the var and arguments must entirely specify
    the behavior of the function. We can't, in general, rely on other mutable local
    state (for example, the value of an atom, or the value of variables closing over
    the function) to provide any additional context.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `var` 来表示我们的 `fold` 的原因是，启动 `fold` 的函数调用可能发生在与实际执行它的计算机完全不同的计算机上。在分布式环境中，`var`
    和参数必须完全指定函数的行为。我们通常不能依赖其他可变的局部状态（例如，一个原子变量的值，或闭包内的变量值）来提供任何额外的上下文。
- en: Parkour distributed sources and sinks
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Parkour 分布式源和接收器
- en: The data which we want our Hadoop job to process may exist on multiple machines
    too, stored distributed in chunks on HDFS. Tesser makes use of a library called
    **Parkour** ([https://github.com/damballa/parkour/](https://github.com/damballa/parkour/))
    to handle accessing potentially distributed data sources. We'll study Parkour
    in more detail later this and the next chapter but, for now, we'll just be using
    the `parkour.io.text` namespace to reference input and output text files.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望 Hadoop 作业处理的数据可能也存在于多台机器上，并且在 HDFS 上分块分布。Tesser 使用一个名为 **Parkour** 的库（[https://github.com/damballa/parkour/](https://github.com/damballa/parkour/)）来处理访问可能分布式的数据源。我们将在本章和下一章更详细地学习
    Parkour，但目前，我们只会使用 `parkour.io.text` 命名空间来引用输入和输出文本文件。
- en: 'Although Hadoop is designed to be run and distributed across many servers,
    it can also run in *local mode*. Local mode is suitable for testing and enables
    us to interact with the local filesystem as if it were HDFS. Another namespace
    we''ll be using from Parkour is the `parkour.conf` namespace. This will allow
    us to create a default Hadoop configuration and operate it in local mode:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Hadoop 设计为在多个服务器上运行和分布式执行，但它也可以在 *本地模式* 下运行。本地模式适用于测试，并使我们能够像操作 HDFS 一样与本地文件系统交互。我们将从
    Parkour 中使用的另一个命名空间是 `parkour.conf`。这将允许我们创建一个默认的 Hadoop 配置，并在本地模式下运行它：
- en: '[PRE58]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: In the preceding example, we use Parkour's `text/dseq` function to create a
    representation of the IRS input data. The return value implements Clojure's reducers
    protocol, so we can use `r/take` on the result.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们使用 Parkour 的 `text/dseq` 函数创建了一个 IRS 输入数据的表示。返回值实现了 Clojure 的 reducers
    协议，因此我们可以对结果使用 `r/take`。
- en: Running a feature scale fold with Hadoop
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Hadoop 运行功能规模的 `fold`
- en: Hadoop needs a location to write its temporary files while working on a task,
    and will complain if we try to overwrite an existing directory. Since we'll be
    executing several jobs over the course of the next few examples, let's create
    a little utility function that returns a new file with a randomly-generated name.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 在执行任务时需要一个位置来写入其临时文件，如果我们尝试覆盖现有目录，它会发出警告。由于在接下来的几个示例中我们将执行多个作业，让我们创建一个小的工具函数，返回一个带有随机生成名称的新文件。
- en: '[PRE59]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Parkour provides a default Hadoop configuration object with the shorthand (`conf/ig`).
    This will return an empty configuration. The default value is enough, we don't
    need to supply any custom configuration.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: Parkour 提供了一个默认的 Hadoop 配置对象，使用简写（`conf/ig`）。它将返回一个空的配置。默认值已经足够，我们不需要提供任何自定义配置。
- en: Note
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: All of our Hadoop jobs will write their temporary files to a random directory
    inside the project's `tmp` directory. Remember to delete this folder later, if
    you're concerned about preserving disk space.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的所有 Hadoop 作业将把临时文件写入项目 `tmp` 目录中的一个随机目录。如果你担心磁盘空间，可以稍后删除这个文件夹。
- en: 'If you run the preceding example now, you should get an output similar to the
    following:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在运行前面的示例，你应该会得到类似以下的输出：
- en: '[PRE60]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Although the return value is identical to the values we got previously, we're
    now making use of Hadoop behind the scenes to process our data. In spite of this,
    notice that Tesser will return the response from our fold as a single Clojure
    data structure.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管返回值与我们之前获得的值相同，但我们现在在幕后使用Hadoop来处理我们的数据。尽管如此，注意Tesser将从我们的折叠操作返回一个单一的Clojure数据结构作为响应。
- en: Running gradient descent with Hadoop
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Hadoop运行梯度下降
- en: 'Since `tesser.hadoop` folds return Clojure data structures just like `tesser.core`
    folds, defining a gradient descent function that makes use of our scaled features
    is very simple:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`tesser.hadoop`折叠返回的Clojure数据结构与`tesser.core`折叠类似，定义一个利用我们缩放后的特征的梯度下降函数非常简单：
- en: '[PRE61]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The preceding code defines a `hadoop-gradient-descent` function that iterates
    a `descend` function `5` times. Each iteration of descend calculates the improved
    coefficients based on the `gradient-descent-fold` function. The final return value
    is a vector of coefficients after `5` iterations of a gradient descent.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码定义了一个`hadoop-gradient-descent`函数，该函数迭代执行`descend`函数`5`次。每次`descend`迭代都会基于`gradient-descent-fold`函数计算改进后的系数。最终返回值是梯度下降`5`次后的系数向量。
- en: 'We run the job on the full IRS data in the following example:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下示例中运行整个IRS数据的作业：
- en: '[PRE62]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'After several iterations, you should see an output similar to the following:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过几次迭代后，你应该看到类似以下的输出：
- en: '[PRE63]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: We've seen how we're able to calculate gradient descent using distributed techniques
    locally. Now, let's see how we can run this on a cluster of our own.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何使用本地的分布式技术计算梯度下降。现在，让我们看看如何在我们自己的集群上运行这个过程。
- en: Preparing our code for a Hadoop cluster
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为Hadoop集群准备我们的代码
- en: Hadoop's Java API defines `Tool` and the associated `ToolRunner` classes that
    are intended to help execute jobs on a Hadoop cluster. A `Tool` class is Hadoop's
    name for a generic command-line application that interacts with the Hadoop framework.
    By creating our own tool, we create a command-line application that can be submitted
    to a Hadoop cluster.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的Java API定义了`Tool`和相关的`ToolRunner`类，用于帮助在Hadoop集群上执行任务。`Tool`类是Hadoop为通用命令行应用程序定义的名称，它与Hadoop框架进行交互。通过创建我们自己的工具，我们就创建了一个可以提交到Hadoop集群的命令行应用程序。
- en: 'Since it''s a Java framework, Hadoop expects to interact with class representations
    of our code. So, the namespace defining our tool needs to contain the `:gen-class`
    declaration, which instructs the Clojure compiler to create a class from our namespace:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个Java框架，Hadoop期望与我们代码的类表示进行交互。因此，定义我们工具的命名空间需要包含`:gen-class`声明，指示Clojure编译器从我们的命名空间创建一个类：
- en: '[PRE64]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'By default, `:gen-class` will expect the namespace to define a main function
    called `-main`. This will be the function that Hadoop will call with our arguments,
    so we can simply delegate the call to a function that will actually execute our
    job:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`:gen-class`将期望命名空间定义一个名为`-main`的主函数。这将是Hadoop用我们的参数调用的函数，因此我们可以简单地将调用委托给一个实际执行我们任务的函数：
- en: '[PRE65]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Parkour provides a Clojure interface to many of Hadoop''s classes. In this
    case, `parkour.tool/run` contains all we need to run our distributed gradient
    descent function on Hadoop. With the preceding example in place, we need to instruct
    the Clojure compiler to ahead-of-time (AOT) compile our namespace and specify
    the class we''d like our project''s main class to be. We can achieve it by adding
    the `:aot` and `:main` declarations to the `project.clj` function like this:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Parkour提供了一个Clojure接口，用于与Hadoop的许多类进行交互。在这种情况下，`parkour.tool/run`包含了我们在Hadoop上运行分布式梯度下降函数所需的一切。通过上述示例，我们需要指示Clojure编译器提前（AOT）编译我们的命名空间，并指定我们希望将项目的主类定义为哪个类。我们可以通过将`:aot`和`:main`声明添加到`project.clj`函数中来实现：
- en: '[PRE66]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: In the example code, we have specified these as a part of the `:uberjar` profile,
    since our last step, before sending the job to the cluster, would be to package
    it up as an uberjar file.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例代码中，我们将这些设置为`：uberjar`配置的一部分，因为在将作业发送到集群之前，我们的最后一步是将其打包为uberjar文件。
- en: Building an uberjar
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建uberjar
- en: 'A JAR contains executable java code. An uberjar contains executable java code,
    plus all the dependencies required to run it. An uberjar provides a convenient
    way to package up code to be run in a distributed environment, because the job
    can be sent from machine to machine while carrying its dependencies with it. Although
    it makes for large job payloads, it avoids the need to ensure that job-specific
    dependencies are preinstalled on all the machines in the cluster. To create an
    uberjar file with **Leiningen**, execute the following command line within the
    project directory:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: JAR文件包含可执行的Java代码。一个uberjar文件包含可执行的Java代码，以及运行所需的所有依赖项。uberjar提供了一种方便的方式来打包代码，以便在分布式环境中运行，因为作业可以在机器之间传递，同时携带它的依赖项。虽然这会导致较大的作业负载，但它避免了确保所有集群中的机器预先安装特定作业依赖项的需要。要使用**Leiningen**创建uberjar文件，在项目目录下执行以下命令行：
- en: '[PRE67]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Once you do this, two files will be created in the target directory. One file,
    `ch5-0.1.0.jar`, contains the project's compiled code. This is the same file as
    the one that would be generated with `lein jar`. In addition, uberjar generates
    the `ch5-0.1.0-standalone.jar` file. This contains the AOT-compiled project code
    in addition to the project's dependencies. The resulting file is large, but it
    contains everything the Hadoop job will need in order to run.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，目标目录中将创建两个文件。一个文件`ch5-0.1.0.jar`包含项目的编译代码。这与使用`lein jar`生成的文件相同。此外，uberjar会生成`ch5-0.1.0-standalone.jar`文件。这个文件包含了项目代码的AOT编译版本以及项目的依赖项。生成的文件虽然较大，但它包含了Hadoop作业运行所需的所有内容。
- en: Submitting the uberjar to Hadoop
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将uberjar提交到Hadoop
- en: Once we've created an uberjar file, we're ready to submit it to Hadoop. Having
    a working local Hadoop installation is not a prerequisite to follow along with
    the examples in this chapter, and we won't describe the steps required to install
    it here.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了一个uberjar文件，就可以将其提交给Hadoop。拥有一个本地的Hadoop安装并不是跟随本章示例的前提条件，我们也不会在此描述安装步骤。
- en: Note
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Links to Hadoop installation guides are provided on this book's wiki at [http://wiki.clojuredatascience.com](http://wiki.clojuredatascience.com).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop安装指南的链接已提供在本书的wiki页面：[http://wiki.clojuredatascience.com](http://wiki.clojuredatascience.com)。
- en: 'However, if you already have Hadoop installed and configured in local mode,
    you can run the example job on the command line now. Since the tool specified
    by the main class also accepts two arguments—the work directory and the input
    file—these will need to be provided too:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你已经在本地模式下安装并配置了Hadoop，现在就可以在命令行上运行示例作业。由于主类指定的工具也接受两个参数——工作目录和输入文件——因此这些参数也需要提供：
- en: '[PRE68]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: If the command runs successfully, you may see logging messages as an output
    by the Hadoop process. After some time, you should see the final coefficients
    output by the job.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令成功运行，你可能会看到Hadoop进程输出的日志信息。经过一段时间，你应该能看到作业输出的最终系数。
- en: Although it takes more time to execute at the moment, our Hadoop job has the
    advantage that it can be distributed on a cluster that can scale indefinitely
    with the size of the data we have.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管目前执行需要更多时间，但我们的Hadoop作业有一个优势，那就是它可以分布在一个可以随着数据量不断扩展的集群上。
- en: Stochastic gradient descent
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: The method we've just seen of calculating gradient descent is often called **batch
    gradient descent**, because each update to the coefficients happens inside an
    iteration over all the data in a *single batch*. With very large amounts of data,
    each iteration can be time-consuming and waiting for convergence could take a
    very long time.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才看到的梯度下降计算方法通常被称为**批量梯度下降**，因为每次对系数的更新都是在对所有数据进行*单次批量*迭代的过程中完成的。对于大量数据来说，每次迭代可能会非常耗时，等待收敛可能需要很长时间。
- en: 'An alternative method of gradient descent is called **stochastic gradient descent**
    or **SGD**. In this method, the estimates of the coefficients are continually
    updated as the input data is processed. The update method for stochastic gradient
    descent looks like this:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的另一种方法叫做**随机梯度下降**或**SGD**。在这种方法中，系数的估算会随着输入数据的处理不断更新。随机梯度下降的更新方法如下：
- en: '![Stochastic gradient descent](img/7180OS_05_14.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![随机梯度下降](img/7180OS_05_14.jpg)'
- en: In fact, this is identical to batch gradient descent. The difference in application
    is purely that expression ![Stochastic gradient descent](img/7180OS_05_17.jpg)
    is calculated over a *mini-batch*—a random smaller subset of the overall data.
    The mini-batch size should be large enough to represent a fair sample of the input
    records—for our data, a reasonable mini-batch size might be about 250.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这与批量梯度下降是完全相同的。应用的不同之处纯粹在于表达式 ![随机梯度下降](img/7180OS_05_17.jpg) 是在 *小批次*——即数据的随机子集——上计算的。小批次的大小应该足够大，以便代表输入记录的公平样本——对于我们的数据，一个合理的小批次大小可能是
    250。
- en: Stochastic gradient descent arrives at the best estimates by splitting the entire
    dataset into mini-batches and processing each of them in turn. Since the output
    of each mini-batch is the coefficient we would like to use for the next mini-batch
    (in order to incrementally improve the estimates), the algorithm is inherently
    sequential.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降通过将整个数据集分成小批次并逐个处理它们来获得最佳估计。由于每个小批次的输出就是我们希望用于下一个小批次的系数（以便逐步改进估计），因此该算法本质上是顺序的。
- en: The advantage stochastic gradient descent offers over batch gradient descent
    is that it can arrive at good estimates in just one iteration over the dataset.
    For very large datasets, it may not even be necessary to process all the mini-batches
    before good convergence has been achieved.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降相较于批量梯度下降的优势在于，它可以在对数据集进行一次迭代后就获得良好的估计。对于非常大的数据集，甚至可能不需要处理所有小批次，就能达到良好的收敛效果。
- en: '![Stochastic gradient descent](img/7180OS_05_180.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![随机梯度下降](img/7180OS_05_180.jpg)'
- en: We could implement SGD with Tesser by taking advantage of the fact that the
    combiner is applied serially, and treat each chunk as a mini-batch from which
    the coefficients could be calculated. This would mean that our reduce step was
    the identity function—we have no reduction to perform.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过利用组合器串行应用的事实，使用 Tesser 实现 SGD，并将每个块视为一个小批次，从中可以计算出系数。这意味着我们的归约步骤就是恒等函数——我们不需要进行任何归约操作。
- en: 'Instead, let''s use this as an opportunity to learn more on how to construct
    a Hadoop job in Parkour. Before delving more into Parkour, let''s see how stochastic
    gradient descent could be implemented using what we already know:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，让我们利用这个机会来学习如何在 Parkour 中构建一个 Hadoop 作业。在深入了解 Parkour 之前，让我们先看看如何使用我们已经掌握的知识实现随机梯度下降：
- en: '[PRE69]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The preceding code groups the input collection into smaller groups of 250 elements.
    Gradient descent is run on each of these mini-batches and the coefficients are
    updated. The next iteration of gradient descent will use the new coefficients
    on the next batch and, for an appropriate value of alpha, produce improved recommendations.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将输入集合分成较小的 250 元素组。对每个小批次运行梯度下降并更新系数。梯度下降的下一次迭代将在下一个批次上使用新的系数，并且对于适当的 alpha
    值，生成改进的推荐结果。
- en: 'The following code will chart the output over many hundreds of batches:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将在数百个批次中绘制输出：
- en: '[PRE70]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: We're supplying a learning rate over 100 times smaller than the value for batch
    gradient descent. This will help ensure that mini-batches containing outliers
    don't pull the parameters too far away from their optimal values. Because of the
    variance inherent in each of the mini-batches, the output of stochastic gradient
    descent will not converge exactly to the most optimal parameters, but will instead
    oscillate around the minimum.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供的学习率比批量梯度下降的值小 100 倍。这有助于确保包含离群点的小批次不会使参数偏离其最优值。由于每个小批次固有的方差，随机梯度下降的输出将不会完全收敛到最优参数，而是会围绕最小值波动。
- en: '![Stochastic gradient descent](img/7180OS_05_190.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![随机梯度下降](img/7180OS_05_190.jpg)'
- en: The preceding image shows the more random effect of stochastic gradient descent;
    in particular, the effect of variance among the mini-batches on the parameter
    estimates. In spite of the much lower learning rate, we can see spikes corresponding
    to the batches with the data containing outliers.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图片展示了随机梯度下降更为随机的效果；特别是小批次之间方差对参数估计的影响。尽管学习率大大降低，我们仍能看到与包含离群点的数据批次相对应的尖峰。
- en: Stochastic gradient descent with Parkour
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Parkour 的随机梯度下降
- en: For the rest of this chapter, we're going to build a Hadoop job directly with
    Parkour. Parkour exposes more of Hadoop's underlying capabilities than Tesser
    does, and this is a mixed blessing. While Tesser makes it very easy to write folds
    and apply them to large datasets in Hadoop, Parkour will require us to understand
    more about Hadoop's computation model.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将直接使用 Parkour 构建 Hadoop 作业。Parkour 比 Tesser 更加暴露 Hadoop 的底层能力，这既是优点也是缺点。尽管
    Tesser 使得在 Hadoop 中编写 fold 操作并应用于大型数据集变得非常容易，但 Parkour 需要我们更多地理解 Hadoop 的计算模型。
- en: 'Although Hadoop''s approach to MapReduce embodies many of the principles we''ve
    encountered so far this chapter, it differs from Tesser''s abstractions in several
    critical ways:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Hadoop 的 MapReduce 方法体现了我们在本章中遇到的许多原则，但它与 Tesser 的抽象在几个关键方面有所不同：
- en: Hadoop assumes that the data to be processed are key/value pairs
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 假定待处理的数据是键/值对
- en: Hadoop does not require a reduce stage following a map
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 不要求在 map 之后进行 reduce 阶段
- en: Tesser folds over the whole sequence of inputs, Hadoop reduces over groups
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tesser 在整个输入序列上进行折叠，Hadoop 在各组数据上进行归约
- en: Hadoop's groups of values are defined by a partitioner
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 的值组由分区器定义
- en: Tesser's combine phase happens *after* reduce, Hadoop's combine stage happens
    *before* reduce
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tesser 的 combine 阶段发生在 *reduce* 之后，Hadoop 的 combine 阶段发生在 *reduce* 之前
- en: 'The last of these is particularly unfortunate. The terminology we''ve learned
    for Clojure reducers and Tesser is reversed for Hadoop: for Hadoop, the combiners
    aggregate the output from the mappers before the data is sent to the reducers.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一项特别令人遗憾。我们为 Clojure reducer 和 Tesser 学到的术语，在 Hadoop 中被反转了：在 Hadoop 中，combiner
    会在数据被发送到 reducer 之前聚合 mapper 的输出。
- en: 'We can see the broad flow represented in the following diagram with the output
    of the mappers combined into intermediate representations and sorted before being
    sent to the reducers. Each reducer reduces over a subset of the entire data. The
    combine step is optional and, in fact, we won''t need one for our stochastic gradient
    descent jobs:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图示中看到广泛的流程，其中 mapper 的输出被合并成中间表示并在发送到 reducers 之前进行排序。每个 reducer 对整个数据的子集进行归约。combine
    步骤是可选的，事实上，在我们的随机梯度下降作业中我们不需要一个：
- en: '![Stochastic gradient descent with Parkour](img/7180OS_05_200.jpg)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![带有 Parkour 的随机梯度下降](img/7180OS_05_200.jpg)'
- en: With or without a combining step, the data is sorted into groups before being
    sent to the reducers and the grouping strategy is defined by a partitioner. The
    default partitioning scheme is to partition by the key of your key/value pair
    (different keys are represented by different shades of gray in the preceding diagram).
    In fact, any custom partitioning scheme can be used.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是否有 combine 步骤，数据都会在发送到 reducers 之前按组进行排序，分组策略由分区器定义。默认的分区方案是按键/值对的键进行分区（不同的键在前面的图示中由不同的灰度表示）。事实上，也可以使用任何自定义的分区方案。
- en: As you can see, Parkour and Hadoop do not assume that the output is a single
    result. Since the groups that Hadoop reduces over are by default defined by the
    grouping key, the result of a reduce can be a dataset of many records. In the
    preceding diagram, we illustrated the case for three different results, one for
    each of the keys in our data.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Parkour 和 Hadoop 都不假定输出是单一结果。由于 Hadoop 的 reduce 操作默认通过分组键定义的组进行，reduce
    的结果可以是一个包含多个记录的数据集。在前面的图示中，我们为数据中的每个键展示了三个不同结果的情况。
- en: Defining a mapper
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义 mapper
- en: The first component of the Hadoop job we'll define is the **mapper**. The mapper's
    role is usually to take a chunk of input records and transform them in some way.
    It's possible to specify a Hadoop job with no reducers; in this case, the output
    of the mappers is also the output of the whole job.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义的 Hadoop 作业的第一个组件是 **mapper**。mapper 的角色通常是接受一块输入记录并以某种方式转换它们。可以指定一个没有
    reducer 的 Hadoop 作业；在这种情况下，mapper 的输出就是整个作业的输出。
- en: 'Parkour allows us to define the action of a mapper as a Clojure function. The
    only requirement of the function is that it accepts the input data (either from
    a source file or the output of a previous MapReduce step) as the final argument.
    Additional arguments can be provided if necessary, so long as the input is the
    final argument:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: Parkour 允许我们将映射器的操作定义为 Clojure 函数。该函数的唯一要求是，它需要将输入数据（无论是来自源文件，还是来自前一个 MapReduce
    步骤的输出）作为最后一个参数传入。如果需要，还可以提供额外的参数，只要输入数据是最后一个参数：
- en: '[PRE71]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The `map` function in the preceding example, `parse-m` (by convention, Parkour
    mappers have the suffix `-m`), is responsible for taking a single line of the
    input and converting it into a feature representation. We''re reusing many of
    the functions we defined earlier in the chapter: `parse-line`, `format-record`,
    `scale-features`, and `extract-features`. Parkour will provide input to the mapper
    function as a reducible collection, so we will chain the functions together with
    `r/map`.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子中，`map` 函数中的 `parse-m`（按照惯例，Parkour 映射器的后缀是 `-m`）负责将输入的单行数据转换成特征表示。我们重新使用了本章前面定义的许多函数：`parse-line`、`format-record`、`scale-features`
    和 `extract-features`。Parkour 将以可简化集合的形式向映射器函数提供输入数据，因此我们将函数通过 `r/map` 进行链式调用。
- en: Stochastic gradient descent expects to process data in mini-batches, so our
    mapper is responsible for partitioning the data into groups of 250 rows. We `shuffle`
    before calling `partition` to ensure that the ordering of the data is random.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降需要按小批量处理数据，因此我们的映射器负责将数据划分为 250 行的小组。在调用 `partition` 之前，我们进行 `shuffle`，以确保数据的顺序是随机的。
- en: Parkour shaping functions
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Parkour 造型函数
- en: We're also supplying metadata to the `parse-m` function in the form of the `{::mr/source-as
    :vals ::mr/sink-as :vals}` map. These are two namespaced keywords referencing
    `parkour.mapreduce/source-as` and `parkour.mapreduce/sink-as`, and are instructions
    to Parkour on how the data should be shaped before providing it to the function
    and what shape of data it can expect in return.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还向 `parse-m` 函数提供了元数据，形式为 `{::mr/source-as :vals ::mr/sink-as :vals}` 的映射。这两个命名空间关键字分别引用
    `parkour.mapreduce/source-as` 和 `parkour.mapreduce/sink-as`，它们是指示 Parkour 在将数据提供给函数之前应该如何构造数据以及它可以期望返回的数据的结构。
- en: '![Parkour shaping functions](img/7180OS_05_210.jpg)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![Parkour 造型函数](img/7180OS_05_210.jpg)'
- en: Valid options for a Parkour mapper are `:keyvals`, `:keys`, and `:vals`. The
    preceding diagram shows the effect for a short sequence of three key/value pairs.
    By requesting to source our data as `:vals`, we get a sequence containing only
    the value portion of the key/value pair.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: Parkour 映射器的有效选项有 `:keyvals`、`:keys` 和 `:vals`。前面的图示展示了三个键/值对的短序列的效果。通过请求将数据源作为
    `:vals`，我们得到一个仅包含键/值对中值部分的序列。
- en: Defining a reducer
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义一个归约器
- en: 'Defining a reducer in Parkour is the same as defining a mapper. Again, the
    last argument must be the input (now, the input from a prior map step), but additional
    arguments can be provided. Our Parkour reducer for stochastic gradient descent
    looks like this:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Parkour 中定义一个归约器与定义映射器相同。最后一个参数必须是输入（现在是来自先前映射步骤的输入），但可以提供额外的参数。我们的随机梯度下降的
    Parkour 归约器如下所示：
- en: '[PRE72]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Our input is provided as a reducible collection like before, so we use the Clojure's
    reducers library to iterate over it. We're using `r/reduce` rather than `r/fold`,
    because we don't want to perform our reduction in parallel over the data. In fact,
    the reason for using Hadoop is that we can control the parallelism of each of
    the map and reduce phases independently. Now that we have our map and reduce steps
    defined, we can combine them into a single job by using the functions in the `parkour.graph`
    namespace.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入数据与之前一样作为可简化集合提供，因此我们使用 Clojure 的 reducers 库来进行迭代。我们使用 `r/reduce` 而不是 `r/fold`，因为我们不想对数据进行并行的归约处理。实际上，使用
    Hadoop 的原因是我们可以独立控制映射阶段和归约阶段的并行度。现在，我们已经定义了映射和归约步骤，可以通过使用 `parkour.graph` 命名空间中的函数将它们组合成一个作业。
- en: Specifying Hadoop jobs with Parkour graph
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Parkour 图定义 Hadoop 作业
- en: 'The `graph` namespace is Parkour''s main API to define Hadoop jobs. Each job
    must have at a minimum an input, a mapper, and an output, and we can chain these
    specifications with Clojure''s `->` macro. Let''s first define a very simple job,
    which takes the output from our mappers and writes them immediately to disk:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '`graph` 命名空间是 Parkour 用来定义 Hadoop 作业的主要 API。每个作业至少需要一个输入、一个映射器和一个输出，我们可以通过
    Clojure 的 `->` 宏将这些规格链式连接。首先定义一个非常简单的作业，它从我们的映射器获取输出并立即写入磁盘：'
- en: '[PRE73]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The response from the preceding example should be a directory within the project's
    `tmp` directory, where Hadoop will have written its files. If you navigate to
    the directory, you should see several files. On my computer, I see four files—`_SUCCESS`,
    `part-m-00000`, `part-m-00001`, and `part-m-00002`. The presence of the `_SUCCESS`
    file indicates that our job is completed successfully. The `part-m-xxxxx` files
    are chunks of our input file.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例返回的响应应该是项目 `tmp` 目录中的一个子目录，Hadoop 会将文件写入该目录。如果你进入该目录，你应该会看到几个文件。在我的电脑上，我看到了四个文件——`_SUCCESS`、`part-m-00000`、`part-m-00001`
    和 `part-m-00002`。`_SUCCESS` 文件的存在表示我们的作业已成功完成。`part-m-xxxxx` 文件是输入文件的块。
- en: The fact that there are three files indicates that Hadoop created three mappers
    to process our input data. If we were running in distributed mode, these could
    have been created in parallel. If you open one of the files, you should see a
    long sequence of `clojure.lang.LazySeq@657d118e`. Since we wrote to a text file,
    it is a text representation of the output of our mapper data.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 文件数量为三，说明 Hadoop 创建了三个映射器来处理我们的输入数据。如果我们以分布式模式运行，可能会并行创建这些文件。如果你打开其中一个文件，你应该看到一长串
    `clojure.lang.LazySeq@657d118e`。由于我们写入的是文本文件，所以这是我们映射器输出的文本表示。
- en: Chaining mappers and reducers with Parkour graph
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Parkour 图链式连接映射器和 reducers
- en: What we really want to do is to its chain our map and reduce steps to happen
    one after the other. For this, we will have to insert an intermediate step, the
    **partitioner**, and tell the partitioner how to serialize our `clojure.lang.LazySeqs`.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正想做的是将映射和减少步骤链式连接，使它们一个接一个地执行。为此，我们必须插入一个中间步骤，**分区器**，并告诉分区器如何序列化我们的 `clojure.lang.LazySeqs`。
- en: The latter can be accomplished by borrowing from Tesser, which implements the
    serialization and deserialization of arbitrary Clojure data structures using **Fressian**.
    In the next chapter, we'll look closer, at the support Parkour provides to create
    well-defined schemas for our partitioned data but, for now, it's simply enough
    for the partitioner to pass the encoded data through.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 后者可以通过借用 Tesser 来实现，Tesser 实现了使用 **Fressian** 对任意 Clojure 数据结构进行序列化和反序列化的功能。在下一章中，我们将更深入地了解
    Parkour 如何提供支持，帮助我们为分区数据创建良定义的架构，但现在仅仅让分区器通过编码后的数据就足够了。
- en: Note
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Fressian is an extensible binary data format. You can learn more about it from
    the documentation at [https://github.com/clojure/data.fressian](https://github.com/clojure/data.fressian).
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: Fressian 是一种可扩展的二进制数据格式。你可以通过 [https://github.com/clojure/data.fressian](https://github.com/clojure/data.fressian)
    的文档了解更多信息。
- en: 'Our keys will be encoded as `FressianWritable`, while our keys are not specified
    at all (we sink our map data just as `vals`). Hadoop''s representation of nil
    is a `NullWritable` type. We import both in our namespace with:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的键将编码为 `FressianWritable`，而我们的键没有明确指定（我们像处理 `vals` 一样处理我们的映射数据）。Hadoop 对 `nil`
    的表示是 `NullWritable` 类型。我们通过以下方式将两者导入到我们的命名空间中：
- en: '[PRE74]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'With the import in place, we can specify our job in its entirety:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入完成后，我们可以完全指定我们的作业：
- en: '[PRE75]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: We need to ensure that we have only one reducer processing our mini-batches
    (although there are variations of SGD that would permit us to average the results
    of several stochastic gradient descent runs, we want to arrive at a single set
    of near-optimal coefficients). We will use Parkour's `conf` namespace to `assoc!
    mapred.reduce.tasks` to `1`.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保只有一个 reducer 来处理我们的迷你批次（尽管 SGD 有一些变种允许我们平均多个随机梯度下降的结果，但我们希望得到一组接近最优的系数）。我们将使用
    Parkour 的 `conf` 命名空间，通过 `assoc! mapred.reduce.tasks` 设置为 `1`。
- en: 'Between the map and reduce steps, we specify the partitioner and pass the `kv-classes`
    function defined at the top of the function. The final example simply runs this
    job:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在映射和减少步骤之间，我们指定分区器并传递在函数顶部定义的 `kv-classes` 函数。最后的示例仅仅运行这个作业：
- en: '[PRE76]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: If you navigate to the directory returned by the job, you should now see a directory
    containing just two files—`_SUCCESS` and `part-r-00000`. One file is the output
    per reducer, so with one reducer, we ended up with one `part-r-xxxxx` file. Inside
    this file will be the coefficients of the linear model calculated with stochastic
    gradient descent.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你进入作业返回的目录，你应该现在看到一个只包含两个文件的目录——`_SUCCESS` 和 `part-r-00000`。每个文件是一个 reducer
    的输出，因此在只有一个 reducer 的情况下，我们最终得到了一个 `part-r-xxxxx` 文件。这个文件内部包含了使用随机梯度下降计算的线性模型系数。
- en: Summary
  id: totrans-399
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned some of the fundamental techniques of distributed
    data processing and saw how the functions used locally for data processing, map
    and reduce, are powerful ways of processing even very large quantities of data.
    We learned how Hadoop can scale unbounded by the capabilities of any single server
    by running functions on smaller subsets of the data whose outputs are themselves
    combined to finally produce a result. Once you understand the tradeoffs, this
    "divide and conquer" approach toward processing data is a simple and very general
    way of analyzing data on a large scale.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了一些分布式数据处理的基本技术，并了解了本地用于数据处理的函数——映射（map）和归约（reduce），它们是处理即使是非常大量数据的强大方法。我们了解了Hadoop如何通过在数据的较小子集上运行函数，来扩展超越任何单一服务器的能力，这些子集的输出最终会组合起来生成结果。一旦你理解了其中的权衡，这种“分而治之”的数据处理方法，就是一种简单且非常通用的大规模数据分析方式。
- en: We saw both the power and limitations of simple folds to process data using
    both Clojure's reducers and Tesser. We've also begun exploring how Parkour exposes
    more of Hadoop's underlying capabilities.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了简单折叠操作在使用Clojure的reducers和Tesser时处理数据的强大功能和局限性。我们还开始探索Parkour如何暴露Hadoop更深层的能力。
- en: In the next chapter, we'll see how to use Hadoop and Parkour to address a particular
    machine learning challenge—clustering a large volume of text documents.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将看到如何使用Hadoop和Parkour来解决一个特定的机器学习挑战——对大量文本文档进行聚类。
