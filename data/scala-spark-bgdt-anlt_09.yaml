- en: Stream Me Up, Scotty - Spark Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Stream Me Up, Scotty - Spark Streaming
- en: '*"I really like streaming services. It''s a great way for people to find your
    music"*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “我真的很喜欢流媒体服务。这是人们发现你的音乐的好方法。”
- en: '- Kygo'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- Kygo'
- en: 'In this chapter, we will learn about Spark Streaming and find out how we can
    take advantage of it to process streams of data using the Spark API. Moreover,
    in this chapter, we will learn various ways of processing real-time streams of
    data using a practical example to consume and process tweets from Twitter. In
    a nutshell, the following topics will be covered throughout this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习Spark Streaming，并了解如何利用它来使用Spark API处理数据流。此外，在本章中，我们将通过一个实际的例子学习处理实时数据流的各种方法，以消费和处理来自Twitter的推文。简而言之，本章将涵盖以下主题：
- en: A brief introduction to streaming
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流媒体的简要介绍
- en: Spark Streaming
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: Discretized streams
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散流
- en: Stateful/stateless transformations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有状态/无状态转换
- en: Checkpointing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点
- en: Interoperability with streaming platforms (Apache Kafka)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与流媒体平台的互操作性（Apache Kafka）
- en: Structured streaming
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流
- en: A Brief introduction to streaming
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流媒体的简要介绍
- en: In today's world of interconnected devices and services, it is hard to even
    spend a few hours a day without our smartphone to check Facebook, or order an
    Uber ride, or tweet something about the burger you just bought, or check the latest
    news or sports updates on your favorite team. We depend on our phones and Internet,
    for a lot of things, whether it is to get work done, or just browse, or e-mail
    your friend. There is simply no way around this phenomenon, and the number and
    variety of applications and services will only grow over time.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今互联设备和服务的世界中，很难一天中甚至只有几个小时不使用我们的智能手机来检查Facebook，或者预订Uber出行，或者发推文关于你刚买的汉堡，或者查看你最喜欢的球队的最新新闻或体育更新。我们依赖手机和互联网，无论是完成工作，浏览，还是给朋友发电子邮件，都需要它们。这种现象是无法避免的，应用程序和服务的数量和种类只会随着时间的推移而增长。
- en: As a result, the smart devices are everywhere, and they generate a lot of data
    all the time. This phenomenon, also broadly referred to as the Internet of Things,
    has changed the dynamics of data processing forever. Whenever you use any of the
    services or apps on your iPhone, or Droid or Windows phone, in some shape or form,
    real-time data processing is at work. Since so much is depending on the quality
    and value of the apps, there is a lot of emphasis on how the various startups
    and established companies are tackling the complex challenges of **SLAs** (**Service
    Level Agreements**), and usefulness and also the timeliness of the data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，智能设备随处可见，它们一直在产生大量数据。这种现象，也广泛称为物联网，已经永久改变了数据处理的动态。每当你在iPhone、Droid或Windows手机上使用任何服务或应用时，实时数据处理都在发挥作用。由于很多东西都取决于应用的质量和价值，各种初创公司和成熟公司如何应对**SLA**（**服务级别协议**）的复杂挑战，以及数据的有用性和及时性都受到了很多关注。
- en: One of the paradigms being researched and adopted by organisations and service
    providers is the building of very scalable, near real-time or real-time processing
    frameworks on a very cutting-edge platform or infrastructure. Everything must
    be fast and also reactive to changes and failures. You would not like it if your
    Facebook updated once every hour or if you received email only once a day; so,
    it is imperative that data flow, processing, and the usage are all as close to
    real time as possible. Many of the systems we are interested in monitoring or
    implementing generate a lot of data as an indefinite continuous stream of events.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 组织和服务提供商正在研究和采用的范式之一是在非常尖端的平台或基础设施上构建非常可扩展的、接近实时或实时的处理框架。一切都必须快速，并且对变化和故障也要有反应。如果你的Facebook每小时只更新一次，或者你一天只收到一次电子邮件，你肯定不会喜欢；因此，数据流、处理和使用都尽可能接近实时是至关重要的。我们感兴趣监控或实施的许多系统产生了大量数据，作为一个无限持续的事件流。
- en: As in any other data processing system, we have the same fundamental challenges
    of a collection of data, storage, and processing of data. However, the additional
    complexity is due to the real-time needs of the platform. In order to collect
    such indefinite streams of events and then subsequently process all such events
    in order to generate actionable insights, we need to use highly scalable specialized
    architectures to deal with tremendous rates of events. As such, many systems have
    been built over the decades starting from AMQ, RabbitMQ, Storm, Kafka, Spark,
    Flink, Gearpump, Apex, and so on.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他数据处理系统一样，我们面临着数据的收集、存储和处理的基本挑战。然而，额外的复杂性是由于平台的实时需求。为了收集这种无限的事件流，并随后处理所有这些事件以生成可操作的见解，我们需要使用高度可扩展的专门架构来处理巨大的事件速率。因此，多年来已经建立了许多系统，从AMQ、RabbitMQ、Storm、Kafka、Spark、Flink、Gearpump、Apex等等。
- en: Modern systems built to deal with such large amounts of streaming data come
    with very flexible and scalable technologies that are not only very efficient
    but also help realize the business goals much better than before. Using such technologies,
    it is possible to consume data from a variety of data sources and then use it
    in a variety of use cases almost immediately or at a later time as needed.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理如此大量的流数据，现代系统采用了非常灵活和可扩展的技术，这些技术不仅非常高效，而且比以前更好地实现了业务目标。使用这些技术，可以从各种数据源中获取数据，然后几乎立即或在需要时在各种用例中使用它。
- en: Let us talk about what happens when you take out your smartphone and book an
    Uber ride to go to the airport. With a few touches on the smartphone screen, you're
    able to select a point, choose the credit card, make the payment, and book the
    ride. Once you're done with your transaction, you then get to monitor the progress
    of your car real-time on a map on your phone. As the car is making its way toward
    you, you're able to monitor exactly where the car is and you can also make a decision
    to pick up coffee at the local Starbucks while you're waiting for the car to pick
    you up.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来谈谈当你拿出手机预订Uber去机场的时候会发生什么。通过几次触摸屏幕，你可以选择一个地点，选择信用卡，付款，预订车辆。一旦交易完成，你就可以实时在手机地图上监控车辆的进度。当车辆向你靠近时，你可以准确地知道车辆的位置，也可以决定在等车的时候去当地的星巴克买咖啡。
- en: You could also make informed decisions regarding the car and the subsequent
    trip to the airport by looking at the expected time of arrival of the car. If
    it looks like the car is going to take quite a bit of time picking you up, and
    if this poses a risk to the flight you are about to catch, you could cancel the
    ride and hop in a taxi that just happens to be nearby. Alternatively, if it so
    happens that the traffic situation is not going to let you reach the airport on
    time, thus posing a risk to the flight you are due to catch, then you also get
    to make a decision regarding rescheduling or canceling your flight.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过查看车辆的预计到达时间来对车辆和随后的机场行程做出明智的决定。如果看起来车辆要花很长时间来接你，而且这可能对你即将要赶的航班构成风险，你可以取消预订并搭乘附近的出租车。另外，如果交通状况不允许你按时到达机场，从而对你即将要赶的航班构成风险，你也可以决定重新安排或取消你的航班。
- en: Now in order to understand how such real-time streaming architectures work to
    provide such invaluable information, we need to understand the basic tenets of
    streaming architectures. On the one hand, it is very important for a real-time
    streaming architecture to be able to consume extreme amounts of data at very high
    rates while , on the other hand, also ensuring reasonable guarantees that the
    data that is getting ingested is also processed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了理解这样的实时流架构是如何提供如此宝贵的信息的，我们需要了解流架构的基本原则。一方面，实时流架构能够以非常高的速率消耗极大量的数据，另一方面，还要确保数据被摄入后也得到合理的处理。
- en: 'The following images diagram shows a generic stream processing system with
    a producer putting events into a messaging system while a consumer is reading
    from the messaging system:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个带有生产者将事件放入消息系统的通用流处理系统，而消费者正在从消息系统中读取事件：
- en: '![](img/00125.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00125.jpeg)'
- en: 'Processing of real-time streaming data can be categorized into the following
    three essential paradigms:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实时流数据的处理可以分为以下三种基本范式：
- en: At least once processing
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少一次处理
- en: At most once processing
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至多一次处理
- en: Exactly once processing
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确一次处理
- en: Let's look at what these three stream processing paradigms mean to our business
    use cases.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这三种流处理范式对我们的业务用例意味着什么。
- en: While exactly once processing of real-time events is the ultimate nirvana for
    us, it is very difficult to always achieve this goal in different scenarios. We
    have to compromise on the property of exactly once processing in cases where the
    benefit of such a guarantee is outweighed by the complexity of the implementation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对于我们来说，实时事件的精确一次处理是最终的理想境界，但在不同的场景中总是实现这一目标非常困难。在那些保证的好处被实现的复杂性所压倒的情况下，我们不得不在精确一次处理的属性上做出妥协。
- en: At least once processing
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 至少一次处理
- en: The at least once processing paradigm involves a mechanism to save the position
    of the last event received **only after** the event is actually processed and
    results persisted somewhere so that, if there is a failure and the consumer restarts,
    the consumer will read the old events again and process them. However, since there
    is no guarantee that the received events were not processed at all or partially
    processed, this causes a potential duplication of events as they are fetched again.
    This results in the behavior that events ate processed at least once.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 至少一次处理范式涉及一种机制，即**只有在**事件实际处理并且结果被持久化之后才保存最后接收到的事件的位置，以便在发生故障并且消费者重新启动时，消费者将再次读取旧事件并处理它们。然而，由于无法保证接收到的事件根本没有被处理或部分处理，这会导致事件的潜在重复，因此事件至少被处理一次。
- en: At least once is ideally suitable for any application that involves updating
    some instantaneous ticker or gauge to show current values. Any cumulative sum,
    counter, or dependency on the accuracy of aggregations (`sum`, `groupBy`, and
    so on) does not fit the use case for such processing simply because duplicate
    events will cause incorrect results.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 至少一次处理理想地适用于任何涉及更新瞬时标记或表盘以显示当前值的应用程序。任何累积总和、计数器或依赖于聚合的准确性（`sum`、`groupBy`等）都不适用于这种处理的用例，因为重复的事件会导致不正确的结果。
- en: 'The sequence of operations for the consumer are as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者的操作顺序如下：
- en: Save results
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存结果
- en: Save offsets
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存偏移量
- en: 'Shown in the following is an illustration of what happens if there are a failure
    and **consumer** restarts. Since the events have already been processed but the
    offsets have not saved, the consumer will read from the previous offsets saved,
    thus causing duplicates. Event 0 is processed twice in the following figure:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，说明了如果出现故障并且**消费者**重新启动会发生什么。由于事件已经被处理，但偏移量没有保存，消费者将从之前保存的偏移量读取，从而导致重复。在下图中，事件0被处理了两次：
- en: '![](img/00277.jpeg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00277.jpeg)'
- en: At most once processing
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 至多一次处理
- en: The At most once processing paradigm involves a mechanism to save the position
    of the last event received before the event is actually processed and results
    persisted somewhere so that, if there is a failure and the consumer restarts,
    the consumer will not try to read the old events again. However, since there is
    no guarantee that the received events were all processed, this causes potential
    loss of events as they are never fetched again. This results in the behavior that
    the events are processed at most once or not processed at all.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 至多一次处理范式涉及一种机制，在事件实际被处理并结果被持久化到某个地方之前，保存最后接收到的事件的位置，以便在发生故障并且消费者重新启动时，消费者不会尝试再次读取旧事件。然而，由于无法保证接收到的事件是否全部被处理，这可能导致事件的潜在丢失，因为它们永远不会再次被获取。这导致事件最多被处理一次或根本不被处理。
- en: At most once is ideally suitable for any application that involves updating
    some instantaneous ticker or gauge to show current values, as well as any cumulative
    sum, counter, or other aggregation, provided accuracy is not mandatory or the
    application needs absolutely all events. Any events lost will cause incorrect
    results or missing results.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 至多一次理想适用于任何需要更新一些即时标记或计量器以显示当前值的应用程序，以及任何累积总和、计数器或其他聚合，只要准确性不是必需的或应用程序绝对需要所有事件。任何丢失的事件都将导致不正确的结果或缺失的结果。
- en: 'The sequence of operations for the consumer are as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者的操作顺序如下：
- en: Save offsets
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存偏移量
- en: Save results
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存结果
- en: 'Shown in the following is an illustration of what happens if there are a failure
    and the **consumer** restarts. Since the events have not been processed but offsets
    are saved, the consumer will read from the saved offsets, causing a gap in events
    consumed. Event 0 is never processed in the following figure:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如果发生故障并且**消费者**重新启动时会发生的情况的示例。由于事件尚未被处理但偏移量已保存，消费者将从保存的偏移量读取，导致事件被消费时出现间隙。在以下图中，事件0从未被处理：
- en: '![](img/00340.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00340.jpeg)'
- en: Exactly once processing
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一次性处理
- en: The Exactly once processing paradigm is similar to the at least once paradigm,
    and involves a mechanism to save the position of the last event received only
    after the event has actually been processed and the results persisted somewhere
    so that, if there is a failure and the consumer restarts, the consumer will read
    the old events again and process them. However, since there is no guarantee that
    the received events were not processed at all or were partially processed, this
    causes a potential duplication of events as they are fetched again. However, unlike
    the at least once paradigm, the duplicate events are not processed and are dropped,
    thus resulting in the exactly once paradigm.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性处理范式类似于至少一次处理范式，并涉及一种机制，只有在事件实际被处理并且结果被持久化到某个地方后，才保存最后接收到的事件的位置，以便在发生故障并且消费者重新启动时，消费者将再次读取旧事件并处理它们。然而，由于无法保证接收到的事件是否根本未被处理或部分处理，这可能导致事件的潜在重复，因为它们会再次被获取。然而，与至少一次处理范式不同，重复的事件不会被处理，而是被丢弃，从而导致一次性处理范式。
- en: Exactly once processing paradigm is suitable for any application that involves
    accurate counters, aggregations, or which in general needs every event processed
    only once and also definitely once (without loss).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性处理范式适用于任何需要准确计数器、聚合或一般需要每个事件仅被处理一次且绝对一次（无损失）的应用程序。
- en: 'The sequence of operations for the consumer are as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者的操作顺序如下：
- en: Save results
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存结果
- en: Save offsets
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存偏移量
- en: 'The following is illustration shows what happens if there are a failure and
    the **consumer** restarts. Since the events have already been processed but offsets
    have not saved, the consumer will read from the previous offsets saved, thus causing
    duplicates. Event 0 is processed only once in the following figure because the
    **consumer** drops the duplicate event 0:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是示例显示了如果发生故障并且**消费者**重新启动时会发生的情况。由于事件已经被处理但偏移量尚未保存，消费者将从先前保存的偏移量读取，从而导致重复。在以下图中，事件0仅被处理一次，因为**消费者**丢弃了重复的事件0：
- en: '![](img/00105.jpeg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00105.jpeg)'
- en: 'How does the Exactly once paradigm drop duplicates? There are two techniques
    which can help here:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性范式如何丢弃重复项？这里有两种技术可以帮助：
- en: Idempotent updates
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 幂等更新
- en: Transactional updates
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 事务更新
- en: Spark Streaming also implements structured streaming in Spark 2.0+, which support
    Exactly once processing out of the box. We will look at structured streaming later
    in this chapter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming还在Spark 2.0+中实现了结构化流处理，支持一次性处理。我们将在本章后面讨论结构化流处理。
- en: Idempotent updates involve saving results based on some unique ID/key generated
    so that, if there is a duplicate, the generated unique ID/key will already be
    in the results (for instance, a database) so that the consumer can drop the duplicate
    without updating the results. This is complicated as it's not always possible
    or easy to generate unique keys. It also requires additional processing on the
    consumer end. Another point is that the database can be separate for results and
    offsets.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 幂等更新涉及基于生成的某个唯一ID/键保存结果，以便如果有重复，生成的唯一ID/键已经存在于结果中（例如，数据库），因此消费者可以丢弃重复项而不更新结果。这很复杂，因为并非总是可能或容易生成唯一键。它还需要在消费者端进行额外的处理。另一点是，数据库可以分开用于结果和偏移量。
- en: Transactional updates save results in batches that have a transaction beginning
    and a transaction commit phase within so that, when the commit occurs, we know
    that the events were processed successfully. Hence, when duplicate events are
    received, they can be dropped without updating results. This technique is much
    more complicated than the idempotent updates as now we need some transactional
    data store. Another point is that the database must be the same for results and
    offsets.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 事务更新以批量方式保存结果，其中包括事务开始和事务提交阶段，因此在提交发生时，我们知道事件已成功处理。因此，当接收到重复事件时，可以在不更新结果的情况下丢弃它们。这种技术比幂等更新复杂得多，因为现在我们需要一些事务性数据存储。另一点是，数据库必须用于结果和偏移量。
- en: You should look into the use case you're trying to build and see if at least
    once processing, or At most once processing, can be reasonably wide and still
    achieve an acceptable level of performance and accuracy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该研究您正在构建的用例，并查看至少一次处理或最多一次处理是否可以合理地广泛应用，并且仍然可以实现可接受的性能和准确性。
- en: We will be looking at the paradigms closely when we learn about Spark Streaming
    and how to use Spark Streaming and consume events from Apache Kafka in the following
    sections.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将仔细研究Spark Streaming的范例，以及如何使用Spark Streaming并从Apache Kafka中消费事件。
- en: Spark Streaming
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: Spark Streaming is not the first streaming architecture to come into existence.
    Several technologies have existenced over time to deal with the real-time processing
    needs of various business use cases. Twitter Storm was one of the first popular
    stream processing technologies out there and was in used by many organizations
    fulfilling the needs of many businesses.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming并不是第一个出现的流处理架构。 随着时间的推移，出现了几种技术来处理各种业务用例的实时处理需求。 Twitter Storm是最早流行的流处理技术之一，并被许多组织使用，满足了许多企业的需求。
- en: Apache Spark comes with a streaming library, which has rapidly evolved to be
    the most widely used technology. Spark Streaming has some distinct advantages
    over the other technologies, the first and foremost being the tight integration
    between Spark Streaming APIs and the Spark core APIs making building a dual purpose
    real-time and batch analytical platform feasible and efficient than otherwise.
    Spark Streaming also integrates with Spark ML and Spark SQL, as well as GraphX,
    making it the most powerful stream processing technology that can serve many unique
    and complex use cases. In this section, we will look deeper into what Spark Streaming
    is all about.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark配备了一个流处理库，它迅速发展成为最广泛使用的技术。 Spark Streaming相对于其他技术具有一些明显的优势，首先是Spark
    Streaming API与Spark核心API之间的紧密集成，使得构建双重用途的实时和批量分析平台比以往更可行和高效。 Spark Streaming还与Spark
    ML和Spark SQL以及GraphX集成，使其成为可以满足许多独特和复杂用例的最强大的流处理技术。 在本节中，我们将更深入地了解Spark Streaming的全部内容。
- en: For more information on Spark Streaming, you can refer to [https://spark.apache.org/docs/2.1.0/streaming-programming-guide.html](https://spark.apache.org/docs/2.1.0/streaming-programming-guide.html).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Spark Streaming的更多信息，您可以参考[https://spark.apache.org/docs/2.1.0/streaming-programming-guide.html](https://spark.apache.org/docs/2.1.0/streaming-programming-guide.html)。
- en: Spark Streaming supports several input sources and can write results to several
    sinks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming支持多种输入源，并可以将结果写入多个接收器。
- en: '![](img/00004.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00004.jpeg)'
- en: While Flink, Heron (successor to Twitter Storm), Samza, and so on all handle
    events as they are collected with minimal latency, Spark Streaming consumes continuous
    streams of data and then processes the collected data in the form of micro-batches.
    The size of the micro-batch can be as low as 500 milliseconds but usually cannot
    go lower than that.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Flink、Heron（Twitter Storm的继任者）、Samza等都可以在收集事件时以最小的延迟处理事件，但Spark Streaming会消耗连续的数据流，然后以微批次的形式处理收集到的数据。
    微批次的大小可以低至500毫秒，但通常不会低于这个值。
- en: Apache Apex, Gear pump, Flink, Samza, Heron, or other upcoming technologies
    compete with Spark Streaming in some use cases. If you need true event-by-event
    processing, then Spark Streaming is not the right fit for your use case.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Apex、Gear pump、Flink、Samza、Heron或其他即将推出的技术在某些用例中与Spark Streaming竞争。 如果您需要真正的事件处理，那么Spark
    Streaming不适合您的用例。
- en: The way streaming works are by creating batches of events at regular time intervals
    as per configuration and delivering the micro-batches of data at every specified
    interval for further processing.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 流媒体的工作方式是根据配置定期创建事件批次，并在每个指定的时间间隔交付数据的微批次以进行进一步处理。
- en: '![](img/00011.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00011.jpeg)'
- en: Just like `SparkContext,` Spark Streaming has a `StreamingContext`, which is
    the main entry point for the streaming job/application. `StreamingContext` is
    dependent on `SparkContext`. In fact, the `SparkContext` can be directly used
    in the streaming job. The `StreamingContext` is similar to the `SparkContext`,
    except that `StreamingContext` also requires the program to specify the time interval
    or duration of the batching interval, which can be in milliseconds or minutes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`SparkContext`一样，Spark Streaming也有一个`StreamingContext`，它是流作业/应用程序的主要入口点。 `StreamingContext`依赖于`SparkContext`。
    实际上，`SparkContext`可以直接在流作业中使用。 `StreamingContext`类似于`SparkContext`，只是`StreamingContext`还需要程序指定批处理间隔的时间间隔或持续时间，可以是毫秒或分钟。
- en: Remember that `SparkContext` is the main point of entry, and the task scheduling
    and resource management is part of `SparkContext`, so `StreamingContext` reuses
    the logic.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`SparkContext`是入口点，任务调度和资源管理是`SparkContext`的一部分，因此`StreamingContext`重用了这一逻辑。
- en: StreamingContext
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StreamingContext
- en: '`StreamingContext` is the main entry point for streaming and essentially takes
    care of the streaming application, including checkpointing, transformations, and
    actions on DStreams of RDDs.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamingContext`是流处理的主要入口点，基本上负责流处理应用程序，包括DStreams的检查点、转换和操作。'
- en: Creating StreamingContext
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建StreamingContext
- en: 'A new StreamingContext can be created in two ways:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过两种方式创建新的StreamingContext：
- en: 'Create a `StreamingContext` using an existing `SparkContext` as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用现有的`SparkContext`创建`StreamingContext`如下：
- en: '[PRE0]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create a `StreamingContext` by providing the configuration necessary for a
    new `SparkContext` as follows:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过提供新的`SparkContext`所需的配置来创建`StreamingContext`如下：
- en: '[PRE1]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A third method is to use `getOrCreate()`, which is used to either recreate
    a `StreamingContext` from checkpoint data or to create a new `StreamingContext`.
    If checkpoint data exists in the provided `checkpointPath`, then `StreamingContext`
    will be recreated from the checkpoint data. If the data does not exist, then the
    `StreamingContext` will be created by calling the provided `creatingFunc`:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三种方法是使用`getOrCreate()`，它用于从检查点数据重新创建`StreamingContext`，或创建一个新的`StreamingContext`。如果检查点数据存在于提供的`checkpointPath`中，则将从检查点数据重新创建`StreamingContext`。如果数据不存在，则将通过调用提供的`creatingFunc`创建`StreamingContext`：
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Starting StreamingContext
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始StreamingContext
- en: 'The `start()` method starts the execution of the streams defined using the
    `StreamingContext`. This essentially starts the entire streaming application:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`start()`方法启动使用`StreamingContext`定义的流的执行。这实质上启动了整个流应用程序：'
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Stopping StreamingContext
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停止StreamingContext
- en: Stopping the `StreamingContext` stops all processing and you will have to recreate
    a new `StreamingContext` and invoke `start()` on it to restart the application.
    There are two APIs useful to stop a stream processing application.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 停止`StreamingContext`将停止所有处理，您将需要重新创建一个新的`StreamingContext`并在其上调用`start()`来重新启动应用程序。有两个有用的API用于停止流处理应用程序。
- en: 'Stop the execution of the streams immediately (do not wait for all received
    data to be processed):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 立即停止流的执行（不等待所有接收到的数据被处理）：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Stop the execution of the streams, with the option of ensuring that all received
    data has been processed:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 停止流的执行，并确保所有接收到的数据都已被处理：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Input streams
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入流
- en: 'There are several types of input streams such as `receiverStream` and `fileStream`
    that can be created using the `StreamingContext` as shown in the following subsections:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种类型的输入流，如`receiverStream`和`fileStream`，可以使用`StreamingContext`创建，如下面的子节所示：
- en: receiverStream
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: receiverStream
- en: Create an input stream with any arbitrary user implemented receiver. It can
    be customized to meet the use cases.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任意用户实现的接收器创建一个输入流。它可以定制以满足用例。
- en: Find more details at [http://spark.apache.org/docs/latest/streaming-custom-receivers.html](http://spark.apache.org/docs/latest/streaming-custom-receivers.html).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在[http://spark.apache.org/docs/latest/streaming-custom-receivers.html](http://spark.apache.org/docs/latest/streaming-custom-receivers.html)找到更多细节。
- en: 'Following is the API declaration for the `receiverStream`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`receiverStream`的API声明：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: socketTextStream
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: socketTextStream
- en: 'This creates an input stream from TCP source `hostname:port`. Data is received
    using a TCP socket and the received bytes are interpreted as UTF8 encoded `\n`
    delimited lines:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从TCP源`hostname:port`创建一个输入流。使用TCP套接字接收数据，并将接收到的字节解释为UTF8编码的`\n`分隔行：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: rawSocketStream
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: rawSocketStream
- en: Create an input stream from network source `hostname:port`, where data is received
    as serialized blocks (serialized using the Spark's serializer) that can be directly
    pushed into the block manager without deserializing them. This is the most efficient
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络源`hostname:port`创建一个输入流，其中数据作为序列化块（使用Spark的序列化器进行序列化）接收，可以直接推送到块管理器而无需对其进行反序列化。这是最有效的
- en: way to receive data.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接收数据的方法。
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: fileStream
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: fileStream
- en: 'Create an input stream that monitors a Hadoop-compatible filesystem for new
    files and reads them using the given key-value types and input format. Files must
    be written to the monitored directory by moving them from another location within
    the same filesystem. File names starting with a dot (`.`) are ignored, so this
    is an obvious choice for the moved file names in the monitored directory. Using
    an atomic file rename function call, the filename which starts with `.` can be
    now renamed to an actual usable filename so that `fileStream` can pick it up and
    let us process the file content:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个输入流，监视Hadoop兼容文件系统以获取新文件，并使用给定的键值类型和输入格式进行读取。文件必须通过将它们从同一文件系统中的另一个位置移动到监视目录中来写入。以点（`.`）开头的文件名将被忽略，因此这是在监视目录中移动文件名的明显选择。使用原子文件重命名函数调用，以`.`开头的文件名现在可以重命名为实际可用的文件名，以便`fileStream`可以捡起它并让我们处理文件内容：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: textFileStream
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: textFileStream
- en: 'Create an input stream that monitors a Hadoop-compatible filesystem for new
    files and reads them as text files (using a key as `LongWritable`, value as Text,
    and input format as `TextInputFormat`). Files must be written to the monitored
    directory by moving them from another location within the same filesystem. File
    names starting with . are ignored:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个输入流，监视Hadoop兼容文件系统以获取新文件，并将它们作为文本文件读取（使用`LongWritable`作为键，Text作为值，`TextInputFormat`作为输入格式）。文件必须通过将它们从同一文件系统中的另一个位置移动到监视目录中来写入。以`.`开头的文件名将被忽略：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: binaryRecordsStream
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: binaryRecordsStream
- en: 'Create an input stream that monitors a Hadoop-compatible filesystem for new
    files and reads them as flat binary files, assuming a fixed length per record,
    generating one byte array per record. Files must be written to the monitored directory
    by moving them from another location within the same filesystem. File names starting
    with `.` are ignored:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个输入流，监视Hadoop兼容文件系统以获取新文件，并将它们作为固定长度的二进制文件读取，生成每个记录的一个字节数组。文件必须通过将它们从同一文件系统中的另一个位置移动到监视目录中来写入。以`.`开头的文件名将被忽略：
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: queueStream
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: queueStream
- en: 'Create an input stream from a queue of RDDs. In each batch, it will process
    either one or all of the RDDs returned by the queue:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从RDD队列创建一个输入流。在每个批处理中，它将处理队列返回的一个或所有RDD：
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: textFileStream example
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: textFileStream示例
- en: 'Shown in the following is a simple example of Spark Streaming using `textFileStream`.
    In this example, we create a `StreamingContext` from the spark-shell `SparkContext`
    (`sc`) and an interval of 10 seconds. This starts the `textFileStream`, which
    monitors the directory named **streamfiles** and processes any new file found
    in the directory. In this example, we are simply printing the number of elements
    in the RDD:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用`textFileStream`的Spark Streaming的简单示例。在这个例子中，我们从spark-shell的`SparkContext`（`sc`）和一个间隔为10秒的时间间隔创建了一个`StreamingContext`。这将启动`textFileStream`，监视名为**streamfiles**的目录，并处理在目录中找到的任何新文件。在这个例子中，我们只是打印RDD中的元素数量：
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: twitterStream example
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: twitterStream示例
- en: 'Let us look at another example of how we can process tweets from Twitter using
    Spark Streaming:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看另一个示例，说明我们如何使用Spark Streaming处理来自Twitter的推文：
- en: First, open a terminal and change the directory to `spark-2.1.1-bin-hadoop2.7`.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，打开一个终端并将目录更改为`spark-2.1.1-bin-hadoop2.7`。
- en: Create a folder `streamouts` under the `spark-2.1.1-bin-hadoop2.7` folder where
    you have spark installed. When the application runs, `streamouts` folder will
    have collected tweets to text files.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您安装了spark的`spark-2.1.1-bin-hadoop2.7`文件夹下创建一个`streamouts`文件夹。当应用程序运行时，`streamouts`文件夹将收集推文到文本文件中。
- en: 'Download the following jars into the directory:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下jar文件下载到目录中：
- en: '[http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar](http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar](http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar)'
- en: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar)'
- en: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar)'
- en: 'Launch spark-shell with the jars needed for Twitter integration specified:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指定的Twitter集成所需的jar启动spark-shell：
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we can write a sample code. Shown in the following is the code to test
    Twitter event processing:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以编写一个示例代码。以下是用于测试Twitter事件处理的代码：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You will see the `streamouts` folder contains several `tweets` output in text
    files. You can now open the directory `streamouts` and check that the files contain
    `tweets`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到`streamouts`文件夹中包含几个文本文件中的`tweets`输出。您现在可以打开`streamouts`目录并检查文件是否包含`tweets`。
- en: Discretized streams
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离散流
- en: Spark Streaming is built on an abstraction called **Discretized Streams** referred,
    to as **DStreams**. A DStream is represented as a sequence of RDDs, with each
    RDD created at each time interval. The DStream can be processed in a similar fashion
    to regular RDDs using similar concepts such as a directed cyclic graph-based execution
    plan (Directed Acyclic Graph). Just like a regular RDD processing, the transformations
    and actions that are part of the execution plan are handled for the DStreams.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming是建立在一个称为**离散流**的抽象上的，称为**DStreams**。DStream被表示为一系列RDD，每个RDD在每个时间间隔创建。DStream可以以类似于常规RDD的方式进行处理，使用类似的概念，如基于有向无环图的执行计划（有向无环图）。就像常规RDD处理一样，执行计划中的转换和操作也适用于DStreams。
- en: DStream essentially divides a never ending stream of data into smaller chunks
    known as micro-batches based on a time interval, materializing each individual
    micro-batch as a RDD which can then processed as a regular RDD. Each such micro-batch
    is processed independently and no state is maintained between micro-batches thus
    making the processing stateless by nature. Let's say the batch interval is 5 seconds,
    then while events are being consumed, real-time and a micro-batch are created
    at every 5-second interval and the micro-batch is handed over for further processing
    as an RDD. One of the main advantages of Spark Streaming is that the API calls
    used to process the micro-batch of events are very tightly integrated into the
    spark for APIs to provide seamless integration with the rest of the architecture.
    When a micro-batch is created, it gets turned into an RDD, which makes it a seamless
    process using spark APIs.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: DStream基本上将一个永无止境的数据流分成较小的块，称为微批处理，基于时间间隔，将每个单独的微批处理实现为一个RDD，然后可以像常规RDD一样进行处理。每个这样的微批处理都是独立处理的，微批处理之间不保留状态，因此本质上是无状态的处理。假设批处理间隔为5秒，那么在事件被消耗时，每5秒间隔都会创建一个实时和微批处理，并将微批处理作为RDD交给进一步处理。Spark
    Streaming的一个主要优势是用于处理事件微批处理的API调用与spark的API紧密集成，以提供与架构的其余部分无缝集成。当创建一个微批处理时，它会转换为一个RDD，这使得使用spark
    API进行无缝处理成为可能。
- en: 'The `DStream` class looks like the following in the source code showing the
    most important variable, a `HashMap[Time, RDD]` pairs:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`DStream`类在源代码中如下所示，显示了最重要的变量，即`HashMap[Time, RDD]`对：'
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Shown in the following is an illustration of a DStream comprising an RDD created
    every **T** seconds:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个由每**T**秒创建的RDD组成的DStream的示例：
- en: '![](img/00076.jpeg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00076.jpeg)'
- en: In the following example, a streaming context is created to create micro-batches
    every 5 seconds and to create an RDD, which is just like a Spark core API RDD.
    The RDDs in the DStream can be processed just like any other RDD.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，创建了一个流上下文，以便每5秒创建一个微批处理，并创建一个RDD，它就像Spark核心API RDD一样。DStream中的RDD可以像任何其他RDD一样进行处理。
- en: 'The steps involved in building a streaming application are as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 构建流应用程序涉及的步骤如下：
- en: Create a `StreamingContext` from the `SparkContext`.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`SparkContext`创建一个`StreamingContext`。
- en: Create a `DStream` from `StreamingContext`.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`StreamingContext`创建一个`DStream`。
- en: Provide transformations and actions that can be applied to each RDD.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供可以应用于每个RDD的转换和操作。
- en: Finally, the streaming application is started by calling `start()` on the `StreamingContext`.
    This starts the entire process of consuming and processing real-time events.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过在`StreamingContext`上调用`start()`来启动流应用程序。这将启动消费和处理实时事件的整个过程。
- en: Once the Spark Streaming application has started, no further operations can
    be added. A stopped context cannot be restarted and you have to create a new streaming
    context if such a need arises.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Spark Streaming应用程序启动，就不能再添加其他操作了。停止的上下文无法重新启动，如果有这样的需要，您必须创建一个新的流上下文。
- en: 'Shown in the following is an example of how to create a simple streaming job
    accessing Twitter:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个访问Twitter的简单流作业的示例：
- en: 'Create a `StreamingContext` from the `SparkContext`:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`SparkContext`创建`StreamingContext`：
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Create a `DStream` from `StreamingContext`:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`StreamingContext`创建`DStream`：
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Provide transformations and actions that can be applied to each RDD:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供可应用于每个RDD的转换和操作：
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, the streaming application is started by calling `start()` on the `StreamingContext`.
    This starts the entire process of consuming and processing real-time events:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过在`StreamingContext`上调用`start()`来启动流应用程序。这将启动整个实时事件的消费和处理过程：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Created a `DStream` of type `ReceiverInputDStream`, which is defined as an
    abstract class for defining any `InputDStream` that has to start a receiver on
    worker nodes to receive external data. Here, we are receiving from Twitter stream:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了一个`ReceiverInputDStream`类型的`DStream`，它被定义为定义任何必须在工作节点上启动接收器以接收外部数据的`InputDStream`的抽象类。在这里，我们从Twitter流接收：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If you run a transformation `flatMap()` on the `twitterStream`, you get a `FlatMappedDStream`,
    as shown in the following:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在`twitterStream`上运行`flatMap()`转换，将得到一个`FlatMappedDStream`，如下所示：
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Transformations
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换
- en: Transformations on a DStream are similar to the transformations applicable to
    a Spark core RDD. Since DStream consists of RDDs, a transformation also applies
    to each RDD to generate a transformed RDD for the RDD, and then a transformed
    DStream is created. Every transformation creates a specific `DStream` derived
    class.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: DStream上的转换类似于适用于Spark核心RDD的转换。由于DStream由RDD组成，因此转换也适用于每个RDD，以生成转换后的RDD，然后创建转换后的DStream。每个转换都创建一个特定的`DStream`派生类。
- en: 'The following diagram shows the hierarchy of `DStream` classes starting from
    the parent `DStream` class. We can also see the different classes inheriting from
    the parent class:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了从父`DStream`类开始的`DStream`类的层次结构。我们还可以看到从父类继承的不同类：
- en: '![](img/00019.jpeg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00019.jpeg)'
- en: There are a lot of `DStream` classes purposely built for the functionality.
    Map transformations, window functions, reduce actions, and different types of
    input streams are all implemented using different class derived from `DStream`
    class.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多`DStream`类是专门为功能而构建的。映射转换、窗口函数、减少操作和不同类型的输入流都是使用从`DStream`类派生的不同类来实现的。
- en: 'Shown in the following is an illustration of a transformation on a base DStream
    to generate a filtered DStream. Similarly, any transformation is applicable to
    a DStream:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对基本DStream进行转换以生成过滤DStream的示例。同样，任何转换都适用于DStream：
- en: '![](img/00382.jpeg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00382.jpeg)'
- en: Refer to the following table for the types of transformations possible.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下表格，了解可能的转换类型。
- en: '| Transformation | Meaning |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 转换 | 意义 |'
- en: '| `map(func)` | This applies the transformation function to each element of
    the DStream and returns a new DStream. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| `map(func)` | 将转换函数应用于DStream的每个元素，并返回一个新的DStream。 |'
- en: '| `flatMap(func)` | This is similar to map; however, just like RDD''s `flatMap`
    versus map, using `flatMap` operates on each element and applies `flatMap`, producing
    multiple output items per each input. |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| `flatMap(func)` | 这类似于map；然而，就像RDD的`flatMap`与map一样，使用`flatMap`对每个元素进行操作并应用`flatMap`，从而为每个输入产生多个输出项。
    |'
- en: '| `filter(func)` | This filters out the records of the DStream to return a
    new DStream. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| `filter(func)` | 这将过滤掉DStream的记录，返回一个新的DStream。 |'
- en: '| `repartition(numPartitions)` | This creates more or fewer partitions to redistribute
    the data to change the parallelism. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| `repartition(numPartitions)` | 这将创建更多或更少的分区以重新分发数据以更改并行性。 |'
- en: '| `union(otherStream)` | This combines the elements in two source DStreams
    and returns a new DStream. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| `union(otherStream)` | 这将合并两个源DStream中的元素，并返回一个新的DStream。 |'
- en: '| `count()` | This returns a new DStream by counting the number of elements
    in each RDD of the source DStream. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| `count()` | 通过计算源DStream的每个RDD中的元素数量，返回一个新的DStream。 |'
- en: '| `reduce(func)` | This returns a new DStream by applying the `reduce` function
    on each element of the source DStream. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| `reduce(func)` | 通过在源DStream的每个元素上应用`reduce`函数，返回一个新的DStream。 |'
- en: '| `countByValue()` | This computes the frequency of each key and returns a
    new DStream of (key, long) pairs. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| `countByValue()` | 这计算每个键的频率，并返回一个新的(key, long)对的DStream。 |'
- en: '| `reduceByKey(func, [numTasks])` | This aggregates the data by key in the
    source DStream''s RDDs and returns a new DStream of (key, value) pairs. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKey(func, [numTasks])` | 这将按键聚合源DStream的RDD，并返回一个新的(key, value)对的DStream。
    |'
- en: '| `join(otherStream, [numTasks])` | This joins two DStreams of *(K, V)* and
    *(K, W)* pairs and returns a new DStream of *(K, (V, W))* pairs combining the
    values from both DStreams. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| `join(otherStream, [numTasks])` | 这将连接两个*(K, V)*和*(K, W)*对的DStream，并返回一个新的*(K,
    (V, W))*对的DStream，结合了两个DStream的值。 |'
- en: '| `cogroup(otherStream, [numTasks])` | `cogroup()`, when called on a DStream
    of *(K, V)* and *(K, W)* pairs, will return a new DStream of *(K, Seq[V], Seq[W])*
    tuples. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| `cogroup(otherStream, [numTasks])` | `cogroup()`在对*(K, V)*和*(K, W)*对的DStream调用时，将返回一个新的*(K,
    Seq[V], Seq[W])*元组的DStream。 |'
- en: '| `transform(func)` | This applies a transformation function on each RDD of
    the source DStream and returns a new DStream. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| `transform(func)` | 这在源DStream的每个RDD上应用转换函数，并返回一个新的DStream。 |'
- en: '| `updateStateByKey(func)` | This updates the state for each key by applying
    the given function on the previous state of the key and the new values for the
    key. Typically, it used to maintain a state machine. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| `updateStateByKey(func)` | 这通过在键的先前状态和键的新值上应用给定的函数来更新每个键的状态。通常用于维护状态机。 |'
- en: Window operations
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口操作
- en: 'Spark Streaming provides windowed processing, which allows you to apply transformations
    over a sliding window of events. The sliding window is created over an interval
    specified. Every time the window slides over a source DStream, the source RDDs,
    which fall within the window specification, are combined and operated upon to
    generate the windowed DStream. There are two parameters that need to be specified
    for the window:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming提供了窗口处理，允许您在事件的滑动窗口上应用转换。滑动窗口是在指定的间隔内创建的。每当窗口在源DStream上滑动时，窗口规范内的源RDD将被组合并操作以生成窗口化的DStream。窗口需要指定两个参数：
- en: '**Window length: This specifies the length in interval considered as the window**'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**窗口长度：指定为窗口考虑的间隔长度**'
- en: 'Sliding interval: This is the interval at which the window is created'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滑动间隔：这是创建窗口的间隔
- en: The window length and the sliding interval must both be a multiple of the block
    interval.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口长度和滑动间隔都必须是块间隔的倍数。
- en: 'Shown in the following is an illustration shows a DStream with a sliding window
    operation showing how the old window (dotted line rectangle) slides by one interval
    to the right into the new window (solid line rectangle):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，显示了具有滑动窗口操作的DStream，显示了旧窗口（虚线矩形）如何在一个间隔内向右滑动到新窗口（实线矩形）：
- en: '![](img/00028.jpeg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00028.jpeg)'
- en: Some of the common window operation are as follows.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的窗口操作如下。
- en: '| Transformation | Meaning |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 转换 | 意义 |'
- en: '| `window(windowLength, slideInterval)` | This creates a window on the source
    DStream and returns the same as a new DStream. |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| `window(windowLength, slideInterval)` | 在源DStream上创建窗口，并返回一个新的DStream。 |'
- en: '| `countByWindow(windowLength, slideInterval)` | This returns count of elements
    in the DStream by applying a sliding window. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| `countByWindow(windowLength, slideInterval)` | 通过应用滑动窗口返回DStream中元素的计数。 |'
- en: '| `reduceByWindow(func, windowLength, slideInterval)` | This returns a new
    DStream by applying the reduce function on each element of the source DStream
    after creating a sliding window of length `windowLength`. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByWindow(func, windowLength, slideInterval)` | 创建一个新的DStream，通过在创建长度为`windowLength`的滑动窗口后，对源DStream的每个元素应用reduce函数来实现。
    |'
- en: '| `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])` | This
    aggregates the data by key in the window applied to the source DStream''s RDDs
    and returns a new DStream of (key, value) pairs. The computation is provided by
    function `func`. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])` | 在应用于源DStream的RDD的窗口中按键聚合数据，并返回新的（键，值）对的DStream。计算由函数`func`提供。
    |'
- en: '| `reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])`
    | This aggregates the data by key in the window applied to the source DStream''s
    RDDs and returns a new DStream of (key, value) pairs. The key difference between
    the preceding function and this one is the `invFunc`, which provides the computation
    to be done at the beginning of the sliding window. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])`
    | 在应用于源DStream的RDD的窗口中按键聚合数据，并返回新的（键，值）对的DStream。与前一个函数的关键区别在于`invFunc`，它提供了在滑动窗口开始时要执行的计算。
    |'
- en: '| `countByValueAndWindow(windowLength, slideInterval, [numTasks])` | This computes
    the frequency of each key and returns a new DStream of (key, long) pairs within
    the sliding window as specified. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| `countByValueAndWindow(windowLength, slideInterval, [numTasks])` | 这计算每个键的频率，并返回指定滑动窗口内的新DStream的（键，长）对。
    |'
- en: Let us look at the Twitter stream example in more detail. Our goal is to print
    the top five words used in tweets streamed every five seconds, using a window
    of length 15 seconds, sliding every 10 seconds. Hence, we can get the top five
    words in 15 seconds.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下Twitter流示例。我们的目标是每五秒打印流式传输的推文中使用的前五个单词，使用长度为15秒的窗口，每10秒滑动一次。因此，我们可以在15秒内获得前五个单词。
- en: 'To run this code, follow these steps:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此代码，请按照以下步骤操作：
- en: First, open a terminal and change directory to `spark-2.1.1-bin-hadoop2.7`.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，打开终端并切换到`spark-2.1.1-bin-hadoop2.7`目录。
- en: Create a folder `streamouts` under the `spark-2.1.1-bin-hadoop2.7` folder where
    you have spark installed. When the application runs, the `streamouts` folder will
    have collected tweets to text files.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在安装了spark的`spark-2.1.1-bin-hadoop2.7`文件夹下创建一个名为`streamouts`的文件夹。当应用程序运行时，`streamouts`文件夹将收集推文到文本文件中。
- en: 'Download the following jars into the directory:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下jar包下载到目录中：
- en: '[http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar](http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar](http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar)'
- en: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar)'
- en: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar)'
- en: 'Launch spark-shell with the jars needed for Twitter integration specified:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指定的Twitter集成所需的jar启动spark-shell：
- en: '[PRE23]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we can write the code. Shown in the following is the code used to test
    Twitter event processing:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以编写代码。以下是用于测试Twitter事件处理的代码：
- en: '[PRE24]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is displayed on the console every 15 seconds and looks something
    like the following:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出每15秒在控制台上显示，并且看起来像下面这样：
- en: '[PRE25]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Stateful/stateless transformations
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有状态/无状态转换
- en: 'As seen previously, Spark Streaming uses a concept of DStreams, which are essentially
    micro-batches of data created as RDDs. We also saw types of transformations that
    are possible on DStreams. The transformations on DStreams can be grouped into
    two types: **Stateless transformations** and **Stateful transformations.**'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Spark Streaming使用DStreams的概念，这些DStreams实质上是作为RDDs创建的微批数据。我们还看到了在DStreams上可能的转换类型。DStreams上的转换可以分为两种类型：**无状态转换**和**有状态转换**。
- en: In Stateless transformations, the processing of each micro-batch of data does
    not depend on the previous batches of data. Thus, this is a stateless transformation,
    with each batch doing its own processing independently of anything that occurred
    prior to this batch.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在无状态转换中，每个微批处理的处理不依赖于先前的数据批处理。因此，这是一个无状态的转换，每个批处理都独立于此批处理之前发生的任何事情进行处理。
- en: In Stateful transformations, the processing of each micro-batch of data depends
    on the previous batches of data either fully or partially. Thus, this is a stateful
    transformation, with each batch considering what happened prior to this batch
    and then using the information while computing the data in this batch.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在有状态转换中，每个微批处理的处理取决于先前的数据批处理，完全或部分地。因此，这是一个有状态的转换，每个批处理都考虑了此批处理之前发生的事情，并在计算此批处理中的数据时使用这些信息。
- en: Stateless transformations
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无状态转换
- en: Stateless transformations transform one DStream to another by applying transformations
    to each of the RDDs within the DStream. Transformations such as `map()`, `flatMap()`,
    `union()`, `join()`, and `reduceByKey` are all examples of stateless transformations.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态转换通过对DStream中的每个RDD应用转换来将一个DStream转换为另一个DStream。诸如`map()`、`flatMap()`、`union()`、`join()`和`reduceByKey`等转换都是无状态转换的示例。
- en: 'Shown in the following is an illustration showing a `map()` transformation
    on `inputDStream` to generate a new `mapDstream`:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例显示了对`inputDStream`进行`map()`转换以生成新的`mapDstream`：
- en: '![](img/00210.jpeg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00210.jpeg)'
- en: Stateful transformations
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有状态转换
- en: Stateful transformations operate on a DStream, but the computations depend on
    the previous state of processing. Operations such as `countByValueAndWindow`,
    `reduceByKeyAndWindow` , `mapWithState`, and `updateStateByKey` are all examples
    of stateful transformations. In fact, all window-based transformations are all
    stateful because, by the definition of window operations, we need to keep track
    of the window length and sliding interval of DStream.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 有状态转换在DStream上进行操作，但计算取决于先前的处理状态。诸如`countByValueAndWindow`、`reduceByKeyAndWindow`、`mapWithState`和`updateStateByKey`等操作都是有状态转换的示例。实际上，所有基于窗口的转换都是有状态的，因为根据窗口操作的定义，我们需要跟踪DStream的窗口长度和滑动间隔。
- en: Checkpointing
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查点
- en: Real-time streaming applications are meant to be long running and resilient
    to failures of all sorts. Spark Streaming implements a checkpointing mechanism
    that maintains enough information to recover from failures.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 实时流应用程序旨在长时间运行并对各种故障具有弹性。Spark Streaming实现了一个检查点机制，可以维护足够的信息以从故障中恢复。
- en: 'There are two types of data that needs to be checkpointed:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 需要检查点的两种数据类型：
- en: Metadata checkpointing
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据检查点
- en: Data checkpointing
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据检查点
- en: 'Checkpointing can be enabled by calling `checkpoint()` function on the `StreamingContext`
    as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在`StreamingContext`上调用`checkpoint()`函数来启用检查点，如下所示：
- en: '[PRE26]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Specifies the directory where the checkpoint data will be reliably stored.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 指定可靠存储检查点数据的目录。
- en: Note that this must be a fault-tolerant file system like HDFS.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这必须是像HDFS这样的容错文件系统。
- en: 'Once checkpoint directory is set, any DStream can be checkpointed into the
    directory based on a specified interval. Looking at the Twitter example, we can
    checkpoint each DStream every 10 seconds into the directory `checkpoints`:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了检查点目录，任何DStream都可以根据指定的间隔检查点到该目录中。看看Twitter的例子，我们可以每10秒将每个DStream检查点到`checkpoints`目录中：
- en: '[PRE27]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `checkpoints` directory looks something like the following after few seconds,
    showing the metadata as well as the RDDs and the `logfiles` are maintained as
    part of the checkpointing:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，`checkpoints`目录看起来像下面这样，显示了元数据以及RDDs，`logfiles`也作为检查点的一部分进行维护：
- en: '![](img/00246.jpeg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00246.jpeg)'
- en: Metadata checkpointing
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元数据检查点
- en: '**Metadata checkpointing** saves information defining the streaming operations,
    which are represented by a **Directed Acyclic Graph** (**DAG**) to the HDFS. This
    can be used to recover the DAG, if there is a failure and the application is restarted.
    The driver restarts and reads the metadata from HDFS, and rebuilds the DAG and
    recovers all the operational state before the crash.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**元数据检查点**保存定义流操作的信息，这些信息由**有向无环图**（**DAG**）表示到HDFS。这可以用于在发生故障并且应用程序重新启动时恢复DAG。驱动程序重新启动并从HDFS读取元数据，并重建DAG并恢复崩溃之前的所有操作状态。'
- en: 'Metadata includes the following:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据包括以下内容：
- en: '**Configuration**: the configuration that was used to create the streaming
    application'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置**：用于创建流应用程序的配置'
- en: '**DStream operations**: the set of DStream operations that define the streaming
    application'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DStream操作**：定义流应用程序的DStream操作集'
- en: '**Incomplete batches**: batches whose jobs are queued but have not completed
    yet'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不完整的批处理**：作业已排队但尚未完成的批处理'
- en: Data checkpointing
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据检查点
- en: Data checkpointing saves the actual RDDs to HDFS so that, if there is a failure
    of the Streaming application, the application can recover the checkpointed RDDs
    and continue from where it left off. While streaming application recovery is a
    good use case for the data checkpointing, checkpointing also helps in achieving
    better performance whenever some RDDs are lost because of cache cleanup or loss
    of an executor by instantiating the generated RDDs without a need to wait for
    all the parent RDDs in the lineage (DAG) to be recomputed.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 数据检查点将实际的RDD保存到HDFS，以便如果流应用程序发生故障，应用程序可以恢复检查点的RDD并从中断的地方继续。虽然流应用程序恢复是数据检查点的一个很好的用例，但检查点还有助于在某些RDD由于缓存清理或执行器丢失而丢失时实例化生成的RDD，而无需等待所有父RDD在血统（DAG）中重新计算。
- en: 'Checkpointing must be enabled for applications with any of the following requirements:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有以下任何要求的应用程序，必须启用检查点：
- en: '**Usage of stateful transformations**: If either `updateStateByKey` or `reduceByKeyAndWindow`
    (with inverse function) is used in the application, then the checkpoint directory
    must be provided to allow for periodic RDD checkpointing.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用有状态转换**：如果应用程序中使用了`updateStateByKey`或`reduceByKeyAndWindow`（带有逆函数），则必须提供检查点目录以允许定期RDD检查点。'
- en: '**Recovering from failures of the driver running the application**: Metadata
    checkpoints are used to recover with progress information.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从运行应用程序的驱动程序的故障中恢复**：元数据检查点用于恢复进度信息。'
- en: If your streaming application does not have the stateful transformations, then
    the application can be run without enabling checkpointing.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的流应用程序没有有状态的转换，则可以在不启用检查点的情况下运行应用程序。
- en: There might be loss of data received but not processed yet in your streaming
    application.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 您的流应用程序中可能会丢失已接收但尚未处理的数据。
- en: Note that checkpointing of RDDs incurs the cost of saving each RDD to storage.
    This may cause an increase in the processing time of those batches where RDDs
    get checkpointed. Hence, the interval of checkpointing needs to be set carefully
    so as not to cause performance issues. At tiny batch sizes (say 1 second), checkpointing
    too frequently every tiny batch may significantly reduce operation throughput.
    Conversely, checkpointing too infrequently causes the lineage and task sizes to
    grow, which may cause processing delays as the amount of data to be persisted
    is large.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，RDD的检查点会产生将每个RDD保存到存储的成本。这可能会导致RDD检查点的批次处理时间增加。因此，检查点的间隔需要谨慎设置，以免引起性能问题。在小批量大小（比如1秒）的情况下，每个小批量频繁检查点可能会显著降低操作吞吐量。相反，检查点太不频繁会导致血统和任务大小增长，这可能会导致处理延迟，因为要持久化的数据量很大。
- en: For stateful transformations that require RDD checkpointing, the default interval
    is a multiple of the batch interval that is at least 10 seconds.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要RDD检查点的有状态转换，默认间隔是批处理间隔的倍数，至少为10秒。
- en: A checkpoint interval of 5 to 10 sliding intervals of a DStream is a good setting
    to start with.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 一个5到10个滑动间隔的DStream的检查点间隔是一个很好的起点设置。
- en: Driver failure recovery
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 驱动程序故障恢复
- en: Driver failure recovery can be accomplished by using `StreamingContext.getOrCreate()`
    to either initialize `StreamingContext` from an existing checkpoint or to create
    a new StreamingContext.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`StreamingContext.getOrCreate()`可以实现驱动程序故障恢复，以初始化`StreamingContext`从现有检查点或创建新的StreamingContext。
- en: 'The two conditions for a streaming application when started are as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 流应用程序启动时的两个条件如下：
- en: When the program is being started for the first time, it needs to create a new
    `StreamingContext`, set up all the streams, and then call `start()`
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当程序第一次启动时，需要从检查点目录中的检查点数据初始化一个新的`StreamingContext`，设置所有流，然后调用`start()`
- en: When the program is being restarted after failure, it needs to initialize a
    `StreamingContext` from the checkpoint data in the checkpoint directory and then
    call `start()`
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在故障后重新启动程序时，需要从检查点目录中的检查点数据初始化一个`StreamingContext`，然后调用`start()`
- en: 'We will implement a function `createStreamContext()`, which creates the `StreamingContext`
    and sets up the various DStreams to parse the tweets and generate the top five
    tweet hashtags every 15 seconds using a window. But instead of calling `createStreamContext(`)
    and then calling `ssc.start()` , we will call `getOrCreate()` so that if the `checkpointDirectory`
    exists, then the context will be recreated from the checkpoint data. If the directory
    does not exist (the application is running for the first time), then the function
    `createStreamContext()` will be called to create a new context and set up the
    DStreams:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个名为`createStreamContext()`的函数，它创建`StreamingContext`并设置各种DStreams来解析推文，并使用窗口每15秒生成前五个推文标签。但是，我们将调用`getOrCreate()`而不是调用`createStreamContext()`然后调用`ssc.start()`，这样如果`checkpointDirectory`存在，那么上下文将从检查点数据中重新创建。如果目录不存在（应用程序第一次运行），那么将调用函数`createStreamContext()`来创建一个新的上下文并设置DStreams：
- en: '[PRE28]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Shown in the following is the code showing the definition of the function and
    how `getOrCreate()` can be called:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是显示函数定义以及如何调用`getOrCreate()`的代码：
- en: '[PRE29]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Interoperability with streaming platforms (Apache Kafka)
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与流平台（Apache Kafka）的互操作性
- en: Spark Streaming has very good integration with Apache Kafka, which is the most
    popular messaging platform currently. Kafka integration has several approaches,
    and the mechanism has evolved over time to improve the performance and reliability.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming与Apache Kafka有非常好的集成，这是当前最流行的消息平台。Kafka集成有几种方法，并且该机制随着时间的推移而不断发展，以提高性能和可靠性。
- en: 'There are three main approaches for integrating Spark Streaming with Kafka:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 将Spark Streaming与Kafka集成有三种主要方法：
- en: Receiver-based approach
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于接收器的方法
- en: Direct stream approach
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接流方法
- en: Structured streaming
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流
- en: Receiver-based approach
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于接收器的方法
- en: The receiver-based approach was the first integration between Spark and Kafka.
    In this approach, the driver starts receivers on the executors that pull data
    using high-level APIs, from Kafka brokers. Since receivers are pulling events
    from Kafka brokers, receivers update the offsets into Zookeeper, which is also
    used by Kafka cluster. The key aspect is the usage of a **WAL** (**Write Ahead
    Log**), which the receiver keeps writing to as it consumes data from Kafka. So,
    when there is a problem and executors or receivers are lost or restarted, the
    WAL can be used to recover the events and process them. Hence, this log-based
    design provides both durability and consistency.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 基于接收器的方法是Spark和Kafka之间的第一个集成。在这种方法中，驱动程序在执行程序上启动接收器，使用高级API从Kafka代理中拉取数据。由于接收器从Kafka代理中拉取事件，接收器会将偏移量更新到Zookeeper中，这也被Kafka集群使用。关键之处在于使用**WAL**（预写式日志），接收器在从Kafka消耗数据时会不断写入。因此，当出现问题并且执行程序或接收器丢失或重新启动时，可以使用WAL来恢复事件并处理它们。因此，这种基于日志的设计既提供了耐用性又提供了一致性。
- en: Each receiver creates an input DStream of events from a Kafka topic while querying
    Zookeeper for the Kafka topics, brokers, offsets, and so on. After this, the discussion
    we had about DStreams in previous sections comes into play.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 每个接收器都会从Kafka主题创建一个输入DStream，同时查询Zookeeper以获取Kafka主题、代理、偏移量等。在此之后，我们在前几节中讨论过的DStreams就会发挥作用。
- en: Long-running receivers make parallelism complicated as the workload is not going
    to be properly distributed as we scale the application. Dependence on HDFS is
    also a problem along with the duplication of write operations. As for the reliability
    needed for exactly once paradigm of processing, only the idempotent approach will
    work. The reason why a transactional approach, will not work in the receiver-based
    approach is that there is no way to access the offset ranges from the HDFS location
    or Zookeeper.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 长时间运行的接收器使并行性变得复杂，因为随着应用程序的扩展，工作负载不会得到适当的分布。依赖HDFS也是一个问题，还有写操作的重复。至于一次性处理所需的可靠性，只有幂等方法才能起作用。接收器基于事务的方法无法起作用的原因是，无法从HDFS位置或Zookeeper访问偏移量范围。
- en: The receiver-based approach works with any messaging system, so it's more general
    purpose.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 基于接收器的方法适用于任何消息系统，因此更通用。
- en: 'You can create a receiver-based stream by invoking the `createStream()` API
    as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过调用`createStream()` API创建基于接收器的流，如下所示：
- en: '[PRE30]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Shown in the following is an example of creating a receiver-based stream that
    pulls messages from Kafka brokers:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建基于接收器的流的示例，从Kafka代理中拉取消息：
- en: '[PRE31]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Shown in the following is an illustration of how the driver launches receivers
    on executors to pull data from Kafka using the high-level API. The receivers pull
    the topic offset ranges from the Kafka Zookeeper cluster and then also update
    Zookeeper as they pull events from the brokers:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是驱动程序如何在执行程序上启动接收器，使用高级API从Kafka中拉取数据的示例。接收器从Kafka Zookeeper集群中拉取主题偏移量范围，然后在从代理中拉取事件时也更新Zookeeper：
- en: '![](img/00078.jpeg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00078.jpeg)'
- en: Direct stream
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直接流
- en: The direct stream based approach is the newer approach with respect to Kafka
    integration and works by using the driver to connect to the brokers directly and
    pull events. The key aspect is that using direct stream API, Spark tasks work
    on a 1:1 ratio when looking at spark partition to Kafka topic/partition. No dependency
    on HDFS or WAL makes it flexible. Also, since now we can have direct access to
    offsets, we can use idempotent or transactional approach for exactly once processing.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 基于直接流的方法是相对于Kafka集成的较新方法，通过使用驱动程序直接连接到代理并拉取事件。关键之处在于使用直接流API，Spark任务在处理Spark分区到Kafka主题/分区时是一对一的比例。不依赖于HDFS或WAL使其灵活。此外，由于现在我们可以直接访问偏移量，我们可以使用幂等或事务性方法进行一次性处理。
- en: Create an input stream that directly pulls messages from Kafka brokers without
    using any receiver. This stream can guarantee that each message from Kafka is
    included in transformations exactly once.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个直接从Kafka代理中拉取消息而不使用任何接收器的输入流。此流可以保证每条来自Kafka的消息在转换中被包含一次。
- en: 'Properties of a direct stream are as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 直接流的属性如下：
- en: '**No receivers**: This stream does not use any receiver, but rather directly
    queries Kafka.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**没有接收器**：此流不使用任何接收器，而是直接查询Kafka。'
- en: '**Offsets**: This does not use Zookeeper to store offsets, and the consumed
    offsets are tracked by the stream itself. You can access the offsets used in each
    batch from the generated RDDs.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏移量**：这不使用Zookeeper来存储偏移量，而是由流本身跟踪消耗的偏移量。您可以从生成的RDD中访问每个批次使用的偏移量。'
- en: '**Failure recovery**: To recover from driver failures, you have to enable checkpointing
    in the `StreamingContext`.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障恢复**：要从驱动程序故障中恢复，必须在`StreamingContext`中启用检查点。'
- en: '**End-to-end semantics**: This stream ensures that every record is effectively
    received and transformed exactly once, but gives no guarantees on whether the
    transformed data are outputted exactly once.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端语义**：此流确保每条记录被有效接收和转换一次，但不能保证转换后的数据是否被输出一次。'
- en: 'You can create a direct stream by using KafkaUtils, `createDirectStream()`
    API as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用KafkaUtils的`createDirectStream()` API创建直接流，如下所示：
- en: '[PRE32]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Shown in the following is an example of a direct stream created to pull data
    from Kafka topics and create a DStream:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建直接流的示例，从Kafka主题中拉取数据并创建DStream：
- en: '[PRE33]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The direct stream API can only be used with Kafka, so this is not a general
    purpose approach.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 直接流API只能与Kafka一起使用，因此这不是一种通用方法。
- en: 'Shown in the following is an illustration of how the driver pulls offset information
    from Zookeeper and directs the executors to launch tasks to pull events from brokers
    based on the offset ranges prescribed by the driver:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是驱动程序如何从Zookeeper中拉取偏移量信息，并指示执行程序根据驱动程序指定的偏移量范围启动任务从代理中拉取事件的示例：
- en: '![](img/00118.jpeg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00118.jpeg)'
- en: Structured streaming
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流
- en: Structured streaming is new in Apache Spark 2.0+ and is now in GA from Spark
    2.2 release. You will see details in the next section along with examples of how
    to use structured streaming.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流是Apache Spark 2.0+中的新功能，从Spark 2.2版本开始已经是GA。您将在下一节中看到详细信息，以及如何使用结构化流的示例。
- en: For more details on the Kafka integration in structured streaming, refer to
    [https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 有关结构化流中Kafka集成的更多详细信息，请参阅[https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)。
- en: 'An example of how to use Kafka source stream in structured streaming is as
    follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 使用结构化流中的Kafka源流的示例如下：
- en: '[PRE34]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'An example of how to use Kafka source instead of source stream (in case you
    want more batch analytics approach) is as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kafka源而不是源流的示例（如果您想要更多的批量分析方法）如下：
- en: '[PRE35]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Structured streaming
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流
- en: Structured streaming is a scalable and fault-tolerant stream processing engine
    built on top of Spark SQL engine. This brings stream processing and computations
    closer to batch processing, rather than the DStream paradigm and challenges involved
    with Spark streaming APIs at this time. The structured streaming engine takes
    care of several challenges like exactly-once stream processing, incremental updates
    to results of processing, aggregations, and so on.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流是建立在Spark SQL引擎之上的可伸缩和容错的流处理引擎。这将流处理和计算更接近批处理，而不是DStream范式和当前时刻涉及的Spark流处理API的挑战。结构化流引擎解决了诸多挑战，如精确一次的流处理、处理结果的增量更新、聚合等。
- en: The structured streaming API also provides the means to tackle a big challenge
    of Spark streaming, that is, Spark streaming processes incoming data in micro-batches
    and uses the received time as a means of splitting the data, thus not considering
    the actual event time of the data. The structured streaming allows you to specify
    such an event time in the data being received so that any late coming data is
    automatically handled.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流API还提供了解决Spark流的一个重大挑战的手段，即，Spark流以微批处理方式处理传入数据，并使用接收时间作为数据分割的手段，因此不考虑数据的实际事件时间。结构化流允许您在接收的数据中指定这样一个事件时间，以便自动处理任何延迟的数据。
- en: The structured streaming is GA in Spark 2.2, and the APIs are marked GA. Refer
    to [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流在Spark 2.2中是GA的，API已标记为GA。请参阅[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)。
- en: 'The key idea behind structured streaming is to treat a live data stream as
    an unbounded table being appended to continuously as events are processed from
    the stream. You can then run computations and SQL queries on this unbounded table
    as you normally do on batch data. A Spark SQL query for instance will process
    the unbounded table:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流的关键思想是将实时数据流视为不断追加到的无界表，随着事件从流中处理，可以运行计算和SQL查询。例如，Spark SQL查询将处理无界表：
- en: '![](img/00348.jpeg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00348.jpeg)'
- en: As the DStream keeps changing with time, more and more data will be processed
    to generate the results. Hence, the unbounded input table is used to generate
    a result table. The output or results table can be written to an external sink
    known as **Output**.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 随着DStream随时间的变化，将处理更多的数据以生成结果。因此，无界输入表用于生成结果表。输出或结果表可以写入称为**输出**的外部接收器。
- en: 'The **Output** is what gets written out and can be defined in a different mode:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**是写出的内容，可以以不同的模式定义：'
- en: '**Complete mode**: The entire updated result table will be written to the external
    storage. It is up to the storage connector to decide how to handle the writing
    of the entire table.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整模式**：整个更新后的结果表将写入外部存储。由存储连接器决定如何处理整个表的写入。'
- en: '**Append mode**: Only any new rows appended to the result table since the last
    trigger will be written to the external storage. This is applicable only on the
    queries where existing rows in the result table are not expected to change.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**追加模式**：自上次触发以来附加到结果表的任何新行都将写入外部存储。这仅适用于查询，其中不希望更改结果表中的现有行。'
- en: '**Update mode**: Only the rows that were updated in the result table since
    the last trigger will be written to the external storage. Note that this is different
    from the complete mode in that this mode only outputs the rows that have changed
    since the last trigger. If the query doesn''t contain aggregations, it will be
    equivalent to Append mode.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新模式**：自上次触发以来更新的行将写入外部存储。请注意，这与完整模式不同，因为此模式仅输出自上次触发以来发生更改的行。如果查询不包含聚合，它将等同于追加模式。'
- en: 'Shown in the following is an illustration of the output from the unbounded
    table:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是从无界表输出的示例：
- en: '![](img/00001.jpeg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00001.jpeg)'
- en: We will show an example of creating a Structured streaming query by listening
    to input on localhost port 9999.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示一个示例，通过监听本地端口9999来创建一个结构化流查询。
- en: 'If using a Linux or Mac, it''s easy to start a simple server on port 9999:
    nc -lk 9999.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用Linux或Mac，在端口9999上启动一个简单的服务器很容易：nc -lk 9999。
- en: 'Shown in the following is an example where we start by creating an `inputStream`
    calling SparkSession''s `readStream` API and then extracting the words from the
    lines. Then we group the words and count the occurrences before finally writing
    the results to the output stream:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，我们首先通过调用SparkSession的`readStream` API创建一个`inputStream`，然后从行中提取单词。然后我们对单词进行分组和计数，最后将结果写入输出流：
- en: '[PRE36]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'As you keep typing words in the terminal, the query keeps updating and generating
    results which are printed on the console:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在终端中不断输入单词时，查询会不断更新并生成结果，这些结果将打印在控制台上：
- en: '[PRE37]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Handling Event-time and late data
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理事件时间和延迟数据
- en: '**Event time** is the time inside the data itself. Traditional Spark Streaming
    only handled time as the received time for the DStream purposes, but this is not
    enough for many applications where we need the event time. For example, if you
    want to get the number of times hashtag appears in a tweet every minute, then
    you should want to use the time when the data was generated, not when Spark receives
    the event. To get event time into the mix, it is very easy to do so in structured
    streaming by considering the event time as a column in the row/event. This allows
    window-based aggregations to be run using the event time rather than the received
    time. Furthermore, this model naturally handles data that has arrived later than
    expected based on its event time. Since Spark is updating the result table, it
    has full control over updating old aggregates when there is late data as well
    as cleaning up old aggregates to limit the size of intermediate state data. There
    is also support for watermarking event streams, which allows the user to specify
    the threshold of late data and allows the engine to accordingly clean up the old
    state.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**事件时间**是数据本身的时间。传统的Spark流处理只处理DStream目的的接收时间，但这对于许多需要事件时间的应用程序来说是不够的。例如，如果要每分钟获取推文中特定标签出现的次数，则应该使用生成数据时的时间，而不是Spark接收事件时的时间。通过将事件时间作为行/事件中的列来将事件时间纳入结构化流中是非常容易的。这允许基于窗口的聚合使用事件时间而不是接收时间运行。此外，该模型自然地处理了根据其事件时间到达的数据。由于Spark正在更新结果表，因此它可以完全控制在出现延迟数据时更新旧的聚合，以及清理旧的聚合以限制中间状态数据的大小。还支持为事件流设置水印，允许用户指定延迟数据的阈值，并允许引擎相应地清理旧状态。'
- en: Watermarks enable the engine to track the current event times and determine
    whether the event needs to be processed or has been already processed by checking
    the threshold of how late data can be received. For instance, if the event time
    is denoted by `eventTime` and the threshold interval of late arriving data is
    `lateThreshold`, then by checking the difference between the `max(eventTime) -
    lateThreshold` and comparing with the specific window starting at time T, the
    engine can determine if the event can be considered for processing in this window
    or not.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 水印使引擎能够跟踪当前事件时间，并通过检查数据的延迟阈值来确定是否需要处理事件或已经通过处理。例如，如果事件时间由`eventTime`表示，延迟到达数据的阈值间隔为`lateThreshold`，则通过检查`max(eventTime)
    - lateThreshold`的差异，并与从时间T开始的特定窗口进行比较，引擎可以确定是否可以在此窗口中考虑处理事件。
- en: 'Shown in the following is an extension of the preceding example on structured
    streaming listening on port 9999\. Here we are enabling `Timestamp` as part of
    the input data so that we can do Window operations on the unbounded table to generate
    results:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是对结构化流的前面示例的扩展，监听端口9999。在这里，我们启用`Timestamp`作为输入数据的一部分，以便我们可以对无界表执行窗口操作以生成结果：
- en: '[PRE38]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Fault tolerance semantics
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容错语义
- en: Delivering **end-to-end exactly once semantics** was one of the key goals behind
    the design of Structured streaming, which implements the Structured streaming
    sources, the output sinks, and the execution engine to reliably track the exact
    progress of the processing so that it can handle any kind of failure by restarting
    and/or reprocessing. Every streaming source is assumed to have offsets (similar
    to Kafka offsets) to track the read position in the stream. The engine uses checkpointing
    and write ahead logs to record the offset range of the data being processed in
    each trigger. The streaming sinks are designed to be idempotent for handling reprocessing.
    Together, using replayable sources and idempotent sinks, Structured streaming
    can ensure end-to-end exactly once semantics under any failure.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 实现“端到端精确一次语义”是结构化流设计的关键目标之一，它实现了结构化流源、输出接收器和执行引擎，可可靠地跟踪处理的确切进度，以便能够通过重新启动和/或重新处理来处理任何类型的故障。假定每个流式源都有偏移量（类似于Kafka偏移量）来跟踪流中的读取位置。引擎使用检查点和预写日志来记录每个触发器中正在处理的数据的偏移量范围。流式输出接收器设计为幂等，以处理重新处理。通过使用可重放的源和幂等的接收器，结构化流可以确保在任何故障情况下实现端到端的精确一次语义。
- en: Remember that exactly once the paradigm is more complicated in traditional streaming
    using some external database or store to maintain the offsets.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，传统流式处理中的范式更加复杂，需要使用一些外部数据库或存储来维护偏移量。
- en: 'The structured streaming is still evolving and has several challenges to overcome
    before it can be widely used. Some of them are as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流仍在不断发展，并且在被广泛使用之前需要克服一些挑战。其中一些挑战如下：
- en: Multiple streaming aggregations are not yet supported on streaming datasets
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式数据集上尚不支持多个流式聚合
- en: Limiting and taking first *N* rows is not supported on streaming datasets
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式数据集上不支持限制和获取前*N*行
- en: Distinct operations on streaming datasets are not supported
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式数据集上不支持不同的操作
- en: Sorting operations are supported on streaming datasets only after an aggregation
    step is performed and that too exclusively when in complete output mode
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在执行聚合步骤之后，流式数据集上仅支持排序操作，而且仅在完整输出模式下才支持
- en: Any kind of join operations between two streaming datasets are not yet supported
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前还不支持任何两个流式数据集之间的连接操作。
- en: Only a few types of sinks - file sink and for each sink are supported
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只支持少数类型的接收器 - 文件接收器和每个接收器
- en: Summary
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed the concepts of the stream processing systems,
    Spark streaming, DStreams of Apache Spark, what DStreams are, DAGs and lineages
    of DStreams, Transformations, and Actions. We also looked at window concept of
    stream processing. We also looked at a practical examples of consuming tweets
    from Twitter using Spark Streaming.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了流处理系统、Spark流处理、Apache Spark的DStreams概念、DStreams是什么、DStreams的DAG和血统、转换和操作。我们还研究了流处理的窗口概念。我们还看了使用Spark流处理从Twitter消费推文的实际示例。
- en: In addition, we looked at receiver-based and direct stream approaches of consuming
    data from Kafka. In the end, we also looked at the new structured streaming, which
    promises to solve many of the challenges such as fault tolerance and exactly once
    semantics on the stream. We also discussed how structured streaming also simplifies
    the integration with messaging systems such as Kafka or other messaging systems.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还研究了从Kafka消费数据的基于接收者和直接流的方法。最后，我们还研究了新的结构化流处理，它承诺解决许多挑战，如流上的容错和精确一次语义。我们还讨论了结构化流处理如何简化与Kafka或其他消息系统的集成。
- en: In the next chapter, we will look at graph processing and how it all works.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看一下图形处理以及它是如何运作的。
