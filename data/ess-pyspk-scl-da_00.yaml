- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: Apache Spark is a unified data analytics engine designed to process huge volumes
    of data in a fast and efficient way. PySpark is the Python language API of Apache
    Spark that provides Python developers an easy-to-use scalable data analytics framework.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个统一的数据分析引擎，旨在以快速高效的方式处理海量数据。PySpark 是 Apache Spark 的 Python 语言
    API，为 Python 开发人员提供了一个易于使用且可扩展的数据分析框架。
- en: '*Essential PySpark for Scalable Data Analytics* starts by exploring the distributed
    computing paradigm and provides a high-level overview of Apache Spark. You''ll
    then begin your data analytics journey with the data engineering process, learning
    to perform data ingestion, data cleansing, and integration at scale.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*可扩展数据分析的必备 PySpark* 通过探索分布式计算范式开始，并提供 Apache Spark 的高层次概述。然后，你将开始数据分析之旅，学习执行数据摄取、数据清洗和大规模数据集成的过程。'
- en: This book will also help you build real-time analytics pipelines that enable
    you to gain insights much faster. Techniques for building cloud-based data lakes
    are presented along with Delta Lake, which brings reliability and performance
    to data lakes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本书还将帮助你构建实时分析管道，使你能够更快速地获得洞察。书中介绍了构建基于云的数据湖的技术，并讨论了 Delta Lake，它为数据湖带来了可靠性和性能。
- en: A newly emerging paradigm called the Data Lakehouse is presented, which combines
    the structure and performance of a data warehouse with the scalability of cloud-based
    data lakes. You'll learn how to perform scalable data science and machine learning
    using PySpark, including data preparation, feature engineering, model training,
    and model productionization techniques. Techniques to scale out standard Python
    machine learning libraries are also presented, along with a new pandas-like API
    on top of PySpark called Koalas.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本书介绍了一个新兴的范式——数据湖仓（Data Lakehouse），它将数据仓库的结构和性能与基于云的数据湖的可扩展性结合起来。你将学习如何使用 PySpark
    执行可扩展的数据科学和机器学习，包括数据准备、特征工程、模型训练和模型生产化技术。还介绍了如何扩展标准 Python 机器学习库的技术，以及在 PySpark
    上提供的类似 pandas 的新 API，名为 Koalas。
- en: Who this book is for
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适合人群
- en: This book is intended for practicing data engineers, data scientists, data analysts,
    citizen data analysts, and data enthusiasts who are already using data analytics
    to delve into the world of distributed and scalable data analytics. It's recommended
    that you have knowledge of the field of data analytics and data manipulation to
    gain actionable insights.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本书面向那些已经在使用数据分析探索分布式和可扩展数据分析世界的实践数据工程师、数据科学家、数据分析师、公民数据分析师和数据爱好者。建议你具有数据分析和数据处理领域的知识，以便获得可操作的见解。
- en: What this book covers
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容概述
- en: '[*Chapter 1*](B16736_01_Final_JM_ePub.xhtml#_idTextAnchor014), *Distributed
    Computing Primer*, introduces the distributed computing paradigm. It also talks
    about how distributed computing became a necessity with the ever-increasing data
    sizes over the last decade and ends with the in-memory data-parallel processing
    concept with the Map Reduce paradigm, and finally, contains introduction to the
    latest features in Apache Spark 3.0 engine.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第一章*](B16736_01_Final_JM_ePub.xhtml#_idTextAnchor014)，*分布式计算概述*，介绍了分布式计算范式。它还讲述了随着过去十年数据量的不断增长，分布式计算为何成为一种必需，并最终介绍了基于内存的数据并行处理概念，包括
    Map Reduce 范式，并介绍了 Apache Spark 3.0 引擎的最新功能。'
- en: '[*Chapter 2*](B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032), *Data Ingestion*,
    covers various data sources, such as databases, data lakes, message queues, and
    how to ingest data from these data sources. You will also learn about the uses,
    differences, and efficiency of various data storage formats at storing and processing
    data.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第二章*](B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032)，*数据摄取*，涵盖了各种数据源，如数据库、数据湖、消息队列，以及如何从这些数据源中摄取数据。你还将了解各种数据存储格式在存储和处理数据方面的用途、差异和效率。'
- en: '[*Chapter 3*](B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056), *Data Cleansing
    and Integration*, discusses various data cleansing techniques, how to handle bad
    incoming data, data reliability challenges and how to cope with them, and data
    integration techniques to build a single integrated view of the data.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第三章*](B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056)，*数据清洗与集成*，讨论了各种数据清洗技术，如何处理不良输入数据、数据可靠性挑战以及如何应对这些挑战，以及数据集成技术，以构建单一的集成数据视图。'
- en: '[*Chapter 4*](B16736_04_Final_JM_ePub.xhtml#_idTextAnchor075), *Real-time Data
    Analytics*, explains how to perform real-time data ingestion and processing, discusses
    the unique challenges that real-time data integration presents and how to overcome,
    and also the benefits it provides.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第四章*](B16736_04_Final_JM_ePub.xhtml#_idTextAnchor075)，*实时数据分析*，解释了如何进行实时数据的获取和处理，讨论了实时数据集成所面临的独特挑战及其解决方法，以及它所带来的好处。'
- en: '[*Chapter 5*](B16736_05_Final_JM_ePub.xhtml#_idTextAnchor094), *Scalable Machine
    Learning with PySpark*, briefly talks about the need to scale out machine learning
    and discusses various techniques available to achieve this from using natively
    distributed machine learning algorithms to embarrassingly parallel processing
    to distributed hyperparameter search. It also provides an introduction to PySpark
    MLlib library and an overview of its various distributed machine learning algorithms.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第五章*](B16736_05_Final_JM_ePub.xhtml#_idTextAnchor094)，*使用PySpark进行可扩展的机器学习*，简要讲解了扩展机器学习的需求，并讨论了实现这一目标的各种技术，从使用原生分布式机器学习算法，到令人尴尬的并行处理，再到分布式超参数搜索。它还介绍了PySpark
    MLlib库，并概述了其各种分布式机器学习算法。'
- en: '[*Chapter 6*](B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107), *Feature Engineering
    – Extraction, Transformation, and Selection*, explores various techniques for
    converting raw data into features that are suitable to be consumed by machine
    learning models, including techniques for scaling, transforming features.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第六章*](B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107)，*特征工程——提取、转换与选择*，探讨了将原始数据转化为适合机器学习模型使用的特征的各种技术，包括特征缩放和转换技术。'
- en: '[*Chapter 7*](B16736_07_Final_JM_ePub.xhtml#_idTextAnchor128), *Supervised
    Machine Learning*, explores supervised learning techniques for machine learning
    classification and regression problems including linear regression, logistic regression,
    and gradient boosted trees.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第七章*](B16736_07_Final_JM_ePub.xhtml#_idTextAnchor128)，*监督式机器学习*，探讨了用于机器学习分类和回归问题的监督学习技术，包括线性回归、逻辑回归和梯度提升树。'
- en: '[*Chapter 8*](B16736_08_Final_JM_ePub.xhtml#_idTextAnchor150), *Unsupervised
    Machine Learning*, covers unsupervised learning techniques such as clustering,
    collaborative filtering, and dimensionality reduction to reduce the number of
    features prior to applying supervised learning.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第八章*](B16736_08_Final_JM_ePub.xhtml#_idTextAnchor150)，*无监督式机器学习*，介绍了无监督学习技术，如聚类、协同过滤和降维，以减少应用监督学习前的特征数量。'
- en: '[*Chapter 9*](B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164), *Machine Learning
    Life Cycle Management*, explains that it is not just sufficient to just build
    and train models, but in the real world, multiple versions of the same model are
    built and different versions are suitable for different applications. Thus, it
    is necessary to track various experiments, their hyperparameters, metrics, and
    also the version of the data they were trained on. It is also necessary to track
    and store the various models in a centrally accessible repository so models can
    be easily productionized and shared; and finally, mechanisms are needed to automate
    this repeatedly occurring process. This chapter introduces these techniques using
    an end-to-end open source machine learning life cycle management library called
    MLflow.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第九章*](B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164)，*机器学习生命周期管理*，解释了仅仅构建和训练模型是不够的，在现实世界中，同一个模型会构建多个版本，并且不同版本适用于不同的应用。因此，有必要跟踪各种实验、它们的超参数、指标，以及它们训练所用的数据版本。还需要在一个集中可访问的库中跟踪和存储各种模型，以便能够轻松地将模型投入生产并进行共享；最后，还需要机制来自动化这一重复出现的过程。本章通过一个端到端的开源机器学习生命周期管理库MLflow介绍了这些技术。'
- en: '[*Chapter 10*](B16736_10_Final_JM_ePub.xhtml#_idTextAnchor176)*, Scaling Out
    Single-Node Machine Learning Using PySpark*, explains that in [*Chapter 5*](B16736_05_Final_JM_ePub.xhtml#_idTextAnchor094)*,
    Scalable Machine Learning with PySpark*, you learned how to use the power of Apache
    Spark''s distributed computing framework to train and score machine learning models
    at scale. Spark''s native machine learning library provides good coverage of standard
    tasks that data scientists typically perform; however, there is a wide variety
    of functionality provided by standard single-node Python libraries that were not
    designed to work in a distributed manner. This chapter deals with techniques for
    horizontally scaling out standard Python data processing and machine learning
    libraries such as pandas, scikit-learn, and XGBoost. This chapter covers scaling
    out typical data science tasks such as exploratory data analysis, model training,
    model inference, and finally also covers a scalable Python library named Koalas
    that lets you effortlessly write PySpark code using very familiar and easy-to-use
    pandas-like syntax.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第10章*](B16736_10_Final_JM_ePub.xhtml#_idTextAnchor176)*, 使用PySpark进行单节点机器学习的横向扩展*，解释了在[*第5章*](B16736_05_Final_JM_ePub.xhtml#_idTextAnchor094)*,
    使用PySpark进行可扩展机器学习*中，您学习了如何利用Apache Spark的分布式计算框架在大规模上训练和评分机器学习模型。Spark的本地机器学习库很好地覆盖了数据科学家通常执行的标准任务；然而，标准的单节点Python库提供了多种功能，但这些库并非为分布式方式而设计。本章介绍了如何将标准Python数据处理和机器学习库（如pandas、scikit-learn和XGBoost）进行横向扩展。本章将涵盖常见数据科学任务的横向扩展，如探索性数据分析、模型训练、模型推理，最后还将介绍一个可扩展的Python库——Koalas，它使您能够使用非常熟悉且易于使用的类似pandas的语法轻松编写PySpark代码。'
- en: '[*Chapter 11*](B16736_11_Final_JM_ePub.xhtml#_idTextAnchor188)*,* *Data Visualization
    with PySpark*, covers data visualizations, which are an important aspect of conveying
    meaning from data and gleaning insights into it. This chapter covers how the most
    popular Python visualization libraries can be used along with PySpark.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第11章*](B16736_11_Final_JM_ePub.xhtml#_idTextAnchor188)*,* *使用PySpark进行数据可视化*，介绍了数据可视化，这是从数据中传递意义并获得洞察力的重要方面。本章将介绍如何使用最流行的Python可视化库与PySpark结合。'
- en: '[*Chapter 12*](B16736_12_Final_JM_ePub.xhtml#_idTextAnchor199)*, Spark SQL
    Primer*, covers SQL, which is an expressive language for ad hoc querying and data
    analysis. This chapter will introduce Spark SQL for data analysis and also show
    how to interchangeably use PySpark with data analysis.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第12章*](B16736_12_Final_JM_ePub.xhtml#_idTextAnchor199)*, Spark SQL入门*，介绍了SQL，这是一种用于临时查询和数据分析的表达语言。本章将介绍Spark
    SQL用于数据分析，并展示如何交替使用PySpark进行数据分析。'
- en: '[*Chapter 13*](B16736_13_Final_JM_ePub.xhtml#_idTextAnchor214)*, Integrating
    External Tools with Spark SQL*, explains that once we have clean, curated, and
    reliable data in our performant data lake, it would be a missed opportunity to
    not democratize this data across the organization to citizen analysts. The most
    popular way of doing this is via various existing **Business Intelligence** (**BI**)
    tools. This chapter deals with requirements for BI tool integration.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第13章*](B16736_13_Final_JM_ePub.xhtml#_idTextAnchor214)*, 将外部工具与Spark SQL集成*，解释了当我们在高效的数据湖中拥有干净、整理过且可靠的数据时，未能将这些数据普及到组织中的普通分析师将是一个错失的机会。最流行的方式是通过各种现有的**商业智能**（**BI**）工具。本章将讨论BI工具集成的需求。'
- en: '[*Chapter 14*](B16736_14_Final_JM_ePub.xhtml#_idTextAnchor222)*, The Data Lakehouse*,
    explains that traditional descriptive analytics tools such as BI tools are designed
    around data warehouses and expect data to be presented in a certain way and modern
    advanced analytics and data science tools are geared toward working with large
    amounts of data that''s easily accessible in data lakes. It is also not practical
    or cost-effective to store redundant data in separate storage locations to be
    able to cater to these individual use cases. This chapter will present a new paradigm
    called Data Lakehouse that tries to overcome the limitations of data warehouses
    and data lakes and bridge the gap by combining the best elements of both.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第14章*](B16736_14_Final_JM_ePub.xhtml#_idTextAnchor222)*, 数据湖屋*，解释了传统的描述性分析工具，如商业智能（BI）工具，是围绕数据仓库设计的，并期望数据以特定方式呈现，而现代的高级分析和数据科学工具则旨在处理可以轻松访问的大量数据，这些数据通常存储在数据湖中。将冗余数据存储在单独的存储位置以应对这些独立的用例既不实际也不具成本效益。本章将介绍一种新的范式——数据湖屋，它试图克服数据仓库和数据湖的局限性，通过结合两者的最佳元素来弥合差距。'
- en: To get the most out of this book
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要最大限度地利用本书
- en: Basic to intermediate knowledge of the disciplines of data engineering, data
    science, and SQL analytics is expected. A general level of proficiency using any
    programming language, especially Python, and a working knowledge of performing
    data analytics using frameworks such as pandas and SQL will help you to get the
    most out of this book.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 预计读者具备数据工程、数据科学和 SQL 分析的基础至中级知识。能够使用任何编程语言，特别是 Python，并且具备使用 pandas 和 SQL 等框架进行数据分析的基本知识，将有助于你从本书中获得最大收益。
- en: '![](img/B16736_Preface_Table_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16736_Preface_Table_01.jpg)'
- en: 'The book makes use of Databricks Community Edition to run all code: [https://community.cloud.databricks.com](https://community.cloud.databricks.com).
    Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用 Databricks Community Edition 来运行所有代码：[https://community.cloud.databricks.com](https://community.cloud.databricks.com)。注册说明可在[https://databricks.com/try-databricks](https://databricks.com/try-databricks)找到。
- en: The entire code base used in this book can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/all_chapters/ess_pyspark.dbc](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/all_chapters/ess_pyspark.dbc).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用的整个代码库可以从[https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/all_chapters/ess_pyspark.dbc](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/all_chapters/ess_pyspark.dbc)下载。
- en: The datasets used for this chapter can be found at [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的数据集可以在[https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data)找到。
- en: '**If you are using the digital version of this book, we advise you to type
    the code yourself or access the code from the book''s GitHub repository (a link
    is available in the next section). Doing so will help you avoid any potential
    errors related to the copying and pasting of code.**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你正在使用本书的数字版本，我们建议你自己输入代码，或者从本书的 GitHub 仓库获取代码（链接将在下一个章节提供）。这样做有助于避免由于复制和粘贴代码而导致的潜在错误。**'
- en: Download the example code files
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: You can download the example code files for this book from GitHub at [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics).
    If there's an update to the code, it will be updated in the GitHub repository.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 GitHub 上下载本书的示例代码文件：[https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics)。如果代码有更新，GitHub
    仓库会进行更新。
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的书籍和视频丰富目录中还提供了其他代码包，可以在[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)查看！
- en: Download the color images
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载彩色图片
- en: 'We also provide a PDF file that has color images of the screenshots and diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781800568877_ColorImages.pdf](_ColorImages.pdf)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了一个包含本书中使用的截图和图表的彩色图片的 PDF 文件。你可以在这里下载：[https://static.packt-cdn.com/downloads/9781800568877_ColorImages.pdf](_ColorImages.pdf)
- en: Conventions used
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的约定
- en: There are a number of text conventions used throughout this book.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了多种文本约定。
- en: '`Code in text`: Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: "The `readStream()` method of the DataStreamReader
    object is used to create the streaming DataFrame."'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`文本中的代码`：表示文本中的代码词汇、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟 URL、用户输入和 Twitter 账户名。例如：“DataStreamReader
    对象的 `readStream()` 方法用于创建流式 DataFrame。”'
- en: 'A block of code is set as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块按如下方式设置：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出都按如下方式书写：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For instance, words in menus or dialog boxes appear in **bold**. Here is an example:
    "There can be multiple **Map** stages followed by multiple **Reduce** stages."'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示一个新术语、一个重要词汇或屏幕上看到的词汇。例如，菜单或对话框中的词汇通常是**粗体**。例如：“可以有多个**Map**阶段，接着是多个**Reduce**阶段。”'
- en: Tips or important notes
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 提示或重要注意事项
- en: Appear like this.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式显示。
- en: Get in touch
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系我们
- en: Feedback from our readers is always welcome.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终欢迎读者的反馈。
- en: '`customercare@packtpub.com` and mention the book title in the subject of your
    message.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`customercare@packtpub.com` 并在邮件主题中注明书名。'
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)
    and fill in the form.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**勘误**：尽管我们已尽力确保内容的准确性，但错误难免发生。如果您在本书中发现错误，我们将非常感激您向我们报告。请访问 [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)
    并填写表格。'
- en: '`copyright@packt.com` with a link to the material.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`copyright@packt.com` 并附上相关材料的链接。'
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果您有兴趣成为作者**：如果您在某个主题上具有专业知识，并且有兴趣撰写或为书籍贡献内容，请访问 [authors.packtpub.com](http://authors.packtpub.com)。'
- en: Share your thoughts
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享您的想法
- en: Once you've read *Essential PySpark for Scalable Data Analytics*, we'd love
    to hear your thoughts! Please [https://packt.link/r/1-800-56887-8](https://packt.link/r/1-800-56887-8)
    for this book and share your feedback.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完 *Essential PySpark for Scalable Data Analytics* 后，我们非常期待听到您的想法！请访问 [https://packt.link/r/1-800-56887-8](https://packt.link/r/1-800-56887-8)
    为本书留下评价并分享您的反馈。
- en: Your review is important to us and the tech community and will help us make
    sure we're delivering excellent quality content.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您的评价对我们以及技术社区都至关重要，它将帮助我们确保提供优质的内容。
