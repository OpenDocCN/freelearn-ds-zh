- en: Jupyter and Big Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jupyter与大数据
- en: Big data is the topic on everyone's mind. I thought it would be good to see
    what can be done with big data in Jupyter. One up-and-coming language for dealing
    with large datasets is Spark. Spark is an open source toolset. We can use Spark
    coding in Jupyter much like the other languages we have seen.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据是每个人都在关注的话题。我认为在Jupyter中看看大数据能做什么会很有意思。一个处理大数据集的新兴语言是Spark。Spark是一个开源工具集，我们可以像使用其他语言一样在Jupyter中使用Spark代码。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Installing Spark for use in Jupyter
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Jupyter中安装Spark
- en: Using Spark's features
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark的功能
- en: Apache Spark
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark
- en: One of the tools we will be using is Apache Spark. Spark is an open source toolset
    for cluster computing. While we will not be using a cluster, typical usage for
    Spark is a larger set of machines or clusters that operate in parallel to analyze
    a big dataset. Installation instructions are available at [https://www.dataquest.io/blog/pyspark-installation-guide](https://www.dataquest.io/blog/pyspark-installation-guide).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的工具之一是Apache Spark。Spark是一个用于集群计算的开源工具集。虽然我们不会使用集群，但Spark的典型用途是将多个机器或集群并行操作，用于分析大数据集。安装说明可以在[https://www.dataquest.io/blog/pyspark-installation-guide](https://www.dataquest.io/blog/pyspark-installation-guide)找到。
- en: Installing Spark on macOS
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在macOS上安装Spark
- en: 'Up-to-date instructions for installing Spark are available at [https://medium.freecodecamp.org/installing-scala-and-apache-spark-on-mac-os-837ae57d283f](https://medium.freecodecamp.org/installing-scala-and-apache-spark-on-mac-os-837ae57d283f).
    The main steps are:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Spark的最新说明可以在[https://medium.freecodecamp.org/installing-scala-and-apache-spark-on-mac-os-837ae57d283f](https://medium.freecodecamp.org/installing-scala-and-apache-spark-on-mac-os-837ae57d283f)找到。主要步骤如下：
- en: Get Homebrew from [http://brew.sh](http://brew.sh). If you are doing software
    development on macOS, you will likely already have Homebrew.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[http://brew.sh](http://brew.sh)获取Homebrew。如果你在macOS上进行软件开发，你很可能已经安装了Homebrew。
- en: 'Install `xcode-select`: `xcode-select` is used for different languages. For
    Spark we use Java, Scala and, of course, Spark as follows:'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装`xcode-select`：`xcode-select`用于不同的编程语言。在Spark中，我们使用Java、Scala，当然还有Spark，具体如下：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Again, it is likely that you will already have this for other software development
    tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，很可能你已经为其他软件开发任务安装了此项工具。
- en: 'Use Homebrew to install Java:'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Homebrew安装Java：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Use Homebrew to install Scala:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Homebrew安装Scala：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Use Homebrew to install Apache Spark:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Homebrew安装Apache Spark：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should test whether this is working using the Spark shell, as in this command:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该使用Spark shell测试此功能是否正常，像这样运行命令：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will result in the familiar logo display:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将显示熟悉的Logo：
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The site continues on to talk about setting up exports and the like, but I did
    not need to do this.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该网站继续讨论设置导出等，但我并没有需要做这些。
- en: At this point, we can bring up a Python 3 Notebook and start using Spark in
    Jupyter.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以启动一个Python 3笔记本并开始在Jupyter中使用Spark。
- en: You can type `quit()` to exit.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以输入`quit()`退出。
- en: Now, when we run our Notebook while using a Python kernel, we can access Spark.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们在使用Python内核时运行笔记本时，我们可以访问Spark。
- en: Windows install
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Windows安装
- en: I had run Spark in Python 2 in previous of Jupyter. I could not get the installs
    to work correctly for Windows.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我以前在Jupyter中使用Python 2运行过Spark。对于Windows，我无法正确安装。
- en: First Spark script
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一个Spark脚本
- en: 'Our first script reads in a text file and sees how much the line lengths add
    up to, as shown next. Note that we are reading in the Notebook file we are running;
    the Notebook is named `Spark File Lengths`, and is stored in the `Spark File Lengths.ipynb` file:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个脚本读取一个文本文件，并查看每行的长度总和，如下所示。请注意，我们读取的是当前运行的笔记本文件；该笔记本名为`Spark File Lengths`，并存储在`Spark
    File Lengths.ipynb`文件中：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the `print(totalLengths)` script, we first initialize Spark, but only if
    we have not done so already. Spark will complain if you try to initialize it more
    than once, so all Spark scripts should have this `if` statement prefix.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在`print(totalLengths)`脚本中，我们首先初始化Spark，但仅当我们尚未初始化它时才会进行此操作。如果你尝试多次初始化Spark，它会报错，因此所有Spark脚本都应该有这个`if`语句前缀。
- en: The script reads in a text file (the source of this script), takes every line
    and computes its length, and then adds all the lengths together.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本读取一个文本文件（此脚本的源文件），处理每一行，计算其长度，然后将所有长度相加。
- en: A `lambda` function is an anonymous (not named) function that takes arguments
    and returns a value. In the first case, given a `s` string return its length.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`lambda`函数是一个匿名（无名）函数，它接受参数并返回一个值。在第一个例子中，给定一个`s`字符串，返回其长度。'
- en: A `reduce` function takes each value as an argument, applies the second argument
    to it, replaces the first value with the result, and then proceeds with the rest
    of the list. In our case, it walks through the line lengths and adds them all
    up.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce`函数将每个值作为参数，应用第二个参数，并用结果替换第一个值，然后继续处理剩余的列表。在我们的案例中，它遍历行的长度并将它们加总。'
- en: Then, running this in a Notebook, we see the following screenshot. Note that
    the size of your file may be slightly different.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在Notebook中运行时，我们看到如下截图。请注意，你的文件大小可能略有不同。
- en: 'Also, the first time you begin the Spark engine (using the line `sc = pyspark.SparkContext()`),
    it may take a while and your script may not complete successfully. If that happens,
    just try it again:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当你第一次启动Spark引擎（使用`sc = pyspark.SparkContext()`这一行代码）时，可能需要一些时间，且脚本可能无法成功完成。如果发生这种情况，请再试一次：
- en: '![](img/25b44c28-0313-4131-8024-e3d8b4e3a4f5.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25b44c28-0313-4131-8024-e3d8b4e3a4f5.png)'
- en: Spark word count
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 单词计数
- en: 'Now that we have seen some of the functionality, let''s explore further. We
    can use a script similar to the following to count the word occurrences in a file:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到了一些功能，接下来让我们进一步探索。我们可以使用类似以下的脚本来统计文件中单词的出现次数：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We have the same preamble to the coding. Then, we load the text file into memory.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对编码有相同的引言。然后，我们将文本文件加载到内存中。
- en: Once the file is loaded, we split each line into words and use a `lambda` function
    to tick off each occurrence of a word. The code is truly creating a new record
    for each word occurrence, such as at appears one. The idea is that this process
    could be split over multiple processors, where each processor generates these
    low-level information bits. We are not concerned with optimizing this process
    at all.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文件加载完成，我们将每行拆分成单词，并使用`lambda`函数统计每个单词的出现次数。代码实际上为每个单词出现创建了一个新记录，比如"at"出现一次。这个过程可以分配到多个处理器，每个处理器生成这些低级别的信息位。我们并不关注优化这个过程。
- en: Once we have all of these records, we reduce/summarize the record set according
    to the word occurrences mentioned.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了所有这些记录，我们就根据单词出现的次数来减少/总结记录集。
- en: The `counts` object is called a **Resilient Distributed Dataset** (**RDD**)
    in Spark. It is resilient since care is taken to persist the dataset. The RDD
    is distributed as it can be manipulated by all nodes in the operating cluster.
    And, of course, it is a dataset consisting of a variety of data items.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`counts`对象在Spark中称为**弹性分布式数据集**（**RDD**）。它是弹性的，因为我们在处理数据集时会确保持久化。RDD是分布式的，因为它可以被操作集群中的所有节点所处理。当然，它是由各种数据项组成的数据集。'
- en: The last `for` loop runs `collect()` against the RDD. As mentioned, this RDD
    could be distributed among many nodes. The `collect()` function pulls all copies
    of the RDD into one location. Then, we loop through each record.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的`for`循环对RDD执行`collect()`操作。如前所述，RDD可能分布在多个节点上。`collect()`函数将RDD的所有副本拉取到一个位置。然后，我们遍历每条记录。
- en: 'When we run this in Jupyter, we see something akin to this displayed:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在Jupyter中运行时，我们看到类似于以下的输出：
- en: '![](img/6a7cdef0-9bef-45ad-a289-bcfdba125d25.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a7cdef0-9bef-45ad-a289-bcfdba125d25.png)'
- en: The listing is abbreviated as the list of words continues for some time. What
    is curious is that the word splitting logic in Spark does not appear to work very
    well; some of the results are not words, such as the first entry an empty string.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表略有缩略，因为单词列表会继续一段时间。值得注意的是，Spark中的单词拆分逻辑似乎并不十分有效；一些结果并不是单词，比如第一个条目是空字符串。
- en: Sorted word count
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排序后的单词计数
- en: 'Using the same script with a minor modification, we can make one more call
    and sort the results. The script now looks as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的脚本，稍微修改一下，我们可以再进行一次调用并对结果进行排序。现在脚本如下所示：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we have added another function call to RDD creation, `sortByKey()`. So,
    after we have mapped/reduced, and arrived at a list of words and occurrences,
    we can then easily sort the results.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们添加了另一个RDD创建函数调用，`sortByKey()`。因此，在我们完成映射/减少操作并得到了单词和出现次数的列表之后，我们可以轻松地对结果进行排序。
- en: 'The resultning output looks like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出如下所示：
- en: '![](img/93cdf7dd-0eff-4bd5-bfd0-2eb6b597200e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93cdf7dd-0eff-4bd5-bfd0-2eb6b597200e.png)'
- en: Estimate pi
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估算π值
- en: 'We can use `map` or `reduce` to estimate pi if we have code like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有如下代码，我们可以使用`map`或`reduce`来估算π值：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This code has the same preamble. We are using the Python `random` package. There
    is a constant for the number of samples to attempt.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码有相同的引言。我们正在使用Python的`random`包，并设定了样本的数量常量。
- en: We are building an RDD called `count`. We call the `parallelize` function to
    split this process between the nodes available. The code just maps the result
    of the `sample` function call. Finally, we reduce the generated map set by adding
    all the samples.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在构建一个称为`count`的RDD。我们调用`parallelize`函数来在可用的节点之间分配这个过程。代码只是映射`sample`函数调用的结果。最后，我们通过将生成的映射集合减少来得到总数。
- en: The `sample` function gets two random numbers and returns a one or a zero depending
    on where the two numbers end up in size. We are looking for random numbers in
    a small range and then checking whether they occur within a circle of the same
    diameter. With a large enough sample, we would end up with `PI(3.141...)`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample`函数获取两个随机数，并根据它们的大小返回1或0。我们正在寻找一个小范围内的随机数，然后检查它们是否落在同一直径的圆内。足够大的样本，我们将得到`PI(3.141...)`。'
- en: 'If we run this in Jupyter, we see the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在Jupyter中运行，我们会看到以下内容：
- en: '![](img/23444f8f-7695-43d8-b936-47962275bb0a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23444f8f-7695-43d8-b936-47962275bb0a.png)'
- en: When I ran this with `NUM_SAMPLES = 100000`, I ended up with `PI = 3.126400`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当我使用`NUM_SAMPLES = 100000`运行时，最终得到了`PI = 3.126400`。
- en: Log file examination
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志文件检查
- en: 'I downloaded one of the `access_log` files from `monitorware.com`. Like any
    other web access log, we have one line per entry, like this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我从`monitorware.com`下载了一个`access_log`文件。和任何其他的网页访问日志一样，我们每条记录一行，就像这样：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The first part is the IP address of the caller, followed by a timestamp, the
    type of HTTP access, the URL referenced, the HTTP type, the resulting HTTP response
    code, and finally the number of bytes in the response.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分是调用者的IP地址，然后是时间戳，HTTP访问类型，引用的URL，HTTP类型，生成的HTTP响应代码，最后是响应中的字节数。
- en: 'We can use Spark to load in and parse out some statistics of the log entries,
    as in this script:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Spark加载和解析日志条目的一些统计信息，就像这个脚本一样：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This script has the same preamble as the others. We read in the `access_log`
    file. Then, we print the `count` record.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本与其他脚本有相同的序言。我们读取`access_log`文件。然后，我们打印`count`记录。
- en: Similarly, we find out how many log entries were `GET` and `POST` operations.
    `GET` is assumed to be the most prevalent.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们了解到有多少日志条目是`GET`和`POST`操作。`GET`被认为是最普遍的。
- en: When I first did this, I really didn't expect anything else, so I removed `gets`
    and `posts` from the set and printed out the outliers to see what they were.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当我第一次这样做时，我真的不希望有任何其他情况发生，所以我从集合中移除了`gets`和`posts`，并打印出离群值，看看它们是什么。
- en: 'When we run this in Jupyter, we see the expected output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在Jupyter中运行时，我们看到了预期的输出：
- en: '![](img/6fb013de-6368-4a87-8871-32ed6e0fc998.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6fb013de-6368-4a87-8871-32ed6e0fc998.png)'
- en: The text processing was not very fast (especially for so few records).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 文本处理速度不是很快（特别是对于如此少量的记录）。
- en: I liked being able to work with the data frames in such a way. There is something
    pleasing about being able to do basic algebra with sets in a programmatic way,
    without having to be concerned about edge cases.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢能够以这样的方式处理数据框架。以程序化的方式进行基本代数运算，而不必担心边界情况，这确实令人愉悦。
- en: By the way, a `HEAD` request works just like  `GET`, but does not return the
    `HTTP` body. This allows a caller to determine what kind of response would have
    come back and respond appropriately.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，`HEAD`请求的工作方式与`GET`完全相同，但不返回`HTTP`正文。这允许调用者确定应该返回什么类型的响应，并做出相应的响应。
- en: Spark primes
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark质数
- en: 'We can run a series of numbers through a filter to determine whether each number
    is prime or not. We can use this script:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以运行一系列数字通过一个过滤器来确定每个数字是否是质数或不是。我们可以使用这个脚本：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The script generates numbers up to `100000`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本生成了最多`100000`的数字。
- en: We then loop over each of the numbers and pass it to our filter. If the filter
    returns `True`, we get a record. Then, we just count how many results we found.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们循环遍历每个数字，并将其传递给我们的过滤器。如果过滤器返回`True`，我们就得到一条记录。然后，我们只需计算找到的结果数量。
- en: 'Running this in Jupyter, we see the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter中运行时，我们看到以下内容：
- en: '![](img/17a3cee1-e1b9-4f85-90d9-12257698f786.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17a3cee1-e1b9-4f85-90d9-12257698f786.png)'
- en: This was very fast. I was waiting and didn't notice that it went so quickly.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个速度非常快。我在等待中没有注意到它如此迅速。
- en: Spark text file analysis
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark文本文件分析
- en: In this example, we will look through a news article to determine some basic
    information from the article.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将浏览一篇新闻文章，以确定文章的一些基本信息。
- en: 'We will be using the following script against the 2600 raid news article from
    [https://www.newsitem.com](https://www.newsitem.com):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将会使用以下脚本对来自[https://www.newsitem.com](https://www.newsitem.com)的2600条新闻文章进行处理：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The code reads in the article and splits it into `sentences`, as determined
    by the appearance of a period. From there, the code maps out the `bigrams` present.
    A bigram is a pair of words that appear next to each other. We then sort the list
    and print out the top ten most prevalent.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 代码读取文章并根据句点的出现将其拆分成`sentences`。然后，代码映射出出现的`bigrams`。bigram是一对相邻的单词。接着，我们对列表进行排序，打印出前十个最常见的词对。
- en: 'When we run this in a Notebook, we see these results:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在Notebook中运行这个时，看到以下结果：
- en: '![](img/043463c5-8d7b-4b08-a66f-d04458eb481e.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/043463c5-8d7b-4b08-a66f-d04458eb481e.png)'
- en: I really had no idea what to expect from the output. It's curious that you can
    glean some insight into the article as `the` and `mall` appear `15` times and
    `the` and `guards` appear `11` times—a raid must have occurred in a mall and included
    the security guards in some manner.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我完全不知道从输出中该期待什么。很奇怪的是，你可以从文章中获取一些洞察力，因为`the`和`mall`出现了`15`次，而`the`和`guards`出现了`11`次——应该是发生了一次袭击，地点是在商场，且某种程度上涉及了保安。
- en: Spark evaluating history data
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark正在评估历史数据
- en: In this example, we combine the previous sections to look at some historical
    data and determine a number of useful attributes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们结合前面几个部分，查看一些历史数据并确定一些有用的属性。
- en: 'The historical data we are using is the guest list for the Jon Stewart television
    show. A typical record from the data looks as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的历史数据是Jon Stewart电视节目的嘉宾名单。数据中的典型记录如下所示：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This contains the year, the occupation of the guest, the date of appearance,
    a logical grouping of the occupations, and the name of the guest.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这包含了年份、嘉宾的职业、出现日期、职业的逻辑分组和嘉宾的姓名。
- en: For our analysis, we will be looking at the number of appearances per year,
    the occupation that appears most frequently, and the personality who appears most
    frequently.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的分析，我们将查看每年出现的次数、最常出现的职业以及最常出现的人物。
- en: 'We will be using this script:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个脚本：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The script has a number of features:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本有一些特点：
- en: We are using several packages.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了几个包。
- en: It has the familiar context preamble.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有熟悉的上下文序言。
- en: We start dictionaries for `years`, `occupations`, and `guests`. A dictionary
    contains `key` and `value`. For this use, the key will be the raw value from the
    CSV. The value will be the number of occurrences in the dataset.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为`years`、`occupations`和`guests`启动字典。字典包含`key`和值。对于此用例，键将是来自CSV的原始值，值将是数据集中该项出现的次数。
- en: We open the file and start reading line by line, using a `reader` object. We
    are using ignore errors as there are a couple of nulls in the file.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们打开文件并开始逐行读取，使用`reader`对象。由于文件中有一些空值，我们在读取时忽略错误。
- en: On each line, we take the value of interest (`year`, `occupation`, `name`).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一行中，我们获取感兴趣的值（`year`、`occupation`、`name`）。
- en: We see if the value is present in the appropriate dictionary.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们查看该值是否存在于相应的字典中。
- en: If it is there, increment the value (counter).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果存在，递增该值（计数器）。
- en: Otherwise, initialize an entry in the dictionary.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，初始化字典中的一个条目。
- en: We then sort each of the dictionaries in reverse order of the number of appearances
    of the item.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们按照每个项目出现次数的逆序对字典进行排序。
- en: Finally, we display the top five values for each dictionary.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们展示每个字典的前五个值。
- en: 'If we run this in a Notebook, we have the following output:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在Notebook中运行这个，我们会得到以下输出：
- en: '![](img/db77a6e1-123b-46a5-bf85-43732af2ed73.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db77a6e1-123b-46a5-bf85-43732af2ed73.png)'
- en: We show the tail of the script and the preceding output.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示脚本的尾部和之前的输出。
- en: There may be a smarter way to do all of this, but I am not aware of it.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有更聪明的方法来做这些事情，但我并不清楚。
- en: The build-up of the accumulators is pretty standard, regardless of what language
    you are using. I think there is an opportunity to use a `map()` function here.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 累加器的构建方式相当标准，无论你使用什么语言。我认为这里有机会使用`map()`函数。
- en: I really liked just trimming off the lists/arrays so easily instead of having
    to call a function.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我真的很喜欢只需要轻松去除列表/数组，而不需要调用函数。
- en: The number of guests per year is very consistent. Actors are prevalent—probably
    the group of people of most interest to the audience. The guest list was a little
    surprising. The guests are mostly actors, but I think all have strong political
    directions.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 每年嘉宾的数量非常一致。演员占据主导地位——他们大概是观众最感兴趣的群体。嘉宾名单有点令人惊讶。嘉宾大多数是演员，但我认为所有人都有强烈的政治倾向。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we used Spark functionality via Python coding for Jupyter.
    First, we installed the Spark additions to Jupyter. We wrote an initial script
    that just read lines from a text file. We went further and determined the word
    counts in that file. We added sorting to the results. We wrote was a script to
    estimate pi. We evaluated web log files for anomalies. We determined a set of
    prime numbers, and we evaluated a text stream for certain characteristics.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过 Python 编程在 Jupyter 中使用了 Spark 功能。首先，我们安装了 Spark 插件到 Jupyter。我们编写了一个初始脚本，它只是从文本文件中读取行。我们进一步分析了该文件中的单词计数，并对结果进行了排序。我们编写了一个脚本来估算
    pi 值。我们评估了网页日志文件中的异常。我们确定了一组素数，并对文本流进行了某些特征的评估。
