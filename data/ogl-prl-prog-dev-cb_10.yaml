- en: Chapter 10. Developing the Radix Sort with OpenCL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章. 使用OpenCL开发Radix排序
- en: 'In this chapter, we are going to explore the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下食谱：
- en: Understanding the Radix sort
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Radix排序
- en: Understanding the MSD and LSD Radix sorts
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解MSD和LSD Radix排序
- en: Understanding reduction
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解归约
- en: Developing the Radix sort in OpenCL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在OpenCL中开发Radix排序
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: In the previous chapter, we learned about developing the Bitonic sort using
    OpenCL. In this chapter, we are going to explore how to develop the Radix sort
    with OpenCL. Radix sorting is also known as **bucket sorting**, and we'll see
    why later on.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何使用OpenCL开发Bitonic排序。在本章中，我们将探讨如何使用OpenCL开发Radix排序。Radix排序也被称为**桶排序**，我们将在稍后看到原因。
- en: Note
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The first Radix sort algorithms came from a machine called the **Hollerith machine**
    that was used in 1890 to tabulate the United States census, and though it may
    not be quite as famous as the machine created by *Charles Babbage*, it does have
    its place in computing history.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个Radix排序算法来自一台名为**Hollerith machine**的机器，它在1890年用于编制美国人口普查，尽管它可能没有像*查尔斯·巴贝奇*创造的机器那样出名，但它确实在计算机历史中占有一席之地。
- en: Understanding the Radix sort
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Radix排序
- en: The Radix sort is not a comparison-based sorting algorithm, and it has a few
    qualities that make it more suitable to parallel computation, especially on vector
    processors such as GPU and modern CPUs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Radix排序不是基于比较的排序算法，它有一些特性使其更适合并行计算，尤其是在像GPU和现代CPU这样的向量处理器上。
- en: Tip
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: I am somewhat reluctant to use the term *modern* since processor technology
    has evolved so quickly over time that the use of this word somehow seems dated.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我有些不愿意使用“现代”这个词，因为处理器技术随着时间的推移发展得如此之快，这个词的使用似乎有些过时。
- en: The way the Radix sort works is rather interesting when you compare it with
    the comparison-based sorting algorithms such as quicksort; the main difference
    between them is how they process the keys of the input data. The Radix sort does
    this by breaking down a key into smaller sequences of sub-keys, if you will, and
    sorts these sub-keys one by one.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Radix排序的工作方式与比较排序算法（如快速排序）相比非常有趣；它们之间的主要区别在于它们如何处理输入数据的关键字。Radix排序通过将关键字分解成更小的子关键字序列（如果可以这样说的話），然后逐个对这些子关键字进行排序来实现这一点。
- en: Numbers can be translated in binary and can be viewed as a sequence of bits;
    the same analogy can be drawn from strings where they are sequences of characters.
    The Radix sort, when applied to such keys, does not compare the individual keys,
    but rather it works on processing and comparing pieces of those keys.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数字可以转换为二进制，可以看作是位序列；同样的类比也可以从字符串中得出，它们是字符序列。当应用于此类关键字时，Radix排序不比较单个关键字，而是处理和比较这些关键字的片段。
- en: 'Radix sort algorithms treat the keys like numbers in a base-R number system.
    *R* is known as the radix, hence the given name of this algorithm. Different values
    of *R* can be applied to different types of sorting. Examples could be:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Radix排序算法将关键字视为基-R数系统中的数字。*R*被称为基数，因此该算法的名称由此而来。不同的*R*值可以应用于不同的排序类型。例如：
- en: '*R = 256* would be sorting strings where each character is an 8-bit ASCII value'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R = 256* 将是对每个字符都是8位ASCII值的字符串进行排序'
- en: '*R = 65536* would be sorting Unicode strings where each character is a 16-bit
    Unicode value'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R = 65536* 将是对每个字符都是16位Unicode值的Unicode字符串进行排序'
- en: '*R = 2* would be sorting binary numbers'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R = 2* 将是对二进制数进行排序'
- en: How to do it…
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: At this point, let's examine an example to see how the Radix sort would sort
    the numbers 44565, 23441, 16482, 98789, and 56732, assuming that each number is
    a five-digit number laid out in memory in contiguous locations
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，让我们通过一个例子来看看Radix排序如何对数字44565、23441、16482、98789和56732进行排序，假设每个数字都是内存中连续位置排列的五位数
- en: '| 44565 | 23441 | 16482 | 98789 | 56732 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 44565 | 23441 | 16482 | 98789 | 56732 |'
- en: 'We are going to extract each digit in a right-to-left fashion examining the
    least significant digit first. Therefore, we have the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按从右到左的顺序提取每个数字，首先检查最低有效位。因此，我们有以下：
- en: '| 5 | 1 | 2 | 9 | 2 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1 | 2 | 9 | 2 |'
- en: 'Let''s assume we apply counting sort to this array of numbers and it becomes
    the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将计数排序应用于这个数字数组，它将变成以下样子：
- en: '| 1 | 2 | 2 | 5 | 9 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 2 | 5 | 9 |'
- en: 'This translates to the following order. Take note that the sorting is stable:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着以下顺序。请注意，排序是稳定的：
- en: '| 23441 | 16482 | 56732 | 44565 | 98789 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 23441 | 16482 | 56732 | 44565 | 98789 |'
- en: 'Next, we shift to the left by one digit. Notice that now the array of numbers
    is:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向左移动一位。注意，现在数字数组如下：
- en: '| 4 | 8 | 3 | 6 | 8 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 8 | 3 | 6 | 8 |'
- en: 'Applying the counting sort again and translating it back to the order of the
    numbers, we have:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 再次应用计数排序并将其转换回数字顺序，我们有：
- en: '| 56732 | 23441 | 16482 | 98789 | 44565 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 56732 | 23441 | 16482 | 98789 | 44565 |'
- en: 'For the 1000^(th) digit we have:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第1,000位，我们有：
- en: '| 23441 | 16482 | 56732 | 98789 | 44565 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 23441 | 16482 | 56732 | 98789 | 44565 |'
- en: 'For the 10,000^(th) digit we have:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第10,000位，我们有：
- en: '| 23441 | 44565 | 16482 | 56732 | 98789 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 23441 | 44565 | 16482 | 56732 | 98789 |'
- en: 'For the 100,000^(th) digit we have:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第100,000位，我们有：
- en: '| 16482 | 23441 | 44565 | 56732 | 98789 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 16482 | 23441 | 44565 | 56732 | 98789 |'
- en: Voila! Radix sorting sorted the array of five-digit numbers. We should note
    that the sort is *stable*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！基数排序已经对五位数的数组进行了排序。我们应该注意，这种排序是**稳定的**。
- en: Note
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: '**Stable sorting** refers to the capability of the algorithm to be able to
    maintain the relative order between any two elements with equal keys. Let us assume
    that an array, `int a[5]`, of the values `1`, `2`, `3`, `4`, `9`, and `2`, through
    some sorting algorithm, X, will sort the elements to `1`, `2`, `2`, `3`, `4`,
    and `9`. The point here is that the two equal values we saw, which are both the
    number `2`, occur at positions `1` and `5` (assuming arrays are zero indexed).
    Then, through X, the sorted list will be such that `a[1]` is always before `a[5]`.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**稳定排序**指的是算法能够保持具有相等键的任意两个元素之间的相对顺序的能力。让我们假设有一个整型数组`int a[5]`，包含值`1`、`2`、`3`、`4`、`9`和`2`，通过某种排序算法X，将元素排序为`1`、`2`、`2`、`3`、`4`和`9`。这里的要点是，我们看到的两个相等的值，即数字`2`，分别位于位置`1`和`5`（假设数组是零索引）。然后，通过X，排序后的列表将使得`a[1]`始终在`a[5]`之前。'
- en: There are actually two basic approaches to Radix sorting. We have seen one approach
    in which we examine the least-significant digit and sort it. This is commonly
    referred to as **LSD Radix sorting** since we work our way from right to left.
    The other approach would be to work from left to right.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，基数排序有两种基本方法。我们已经看到了一种方法，即我们检查最低有效位并对其进行排序。这通常被称为**LSD基数排序**，因为我们从右到左进行操作。另一种方法是从左到右进行操作。
- en: The key consideration in Radix sorting is the concept of the *key*. Depending
    on the context, a key may be a word or a string, and each of them would be of
    fixed length or variable length.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基数排序的关键考虑因素是**键**的概念。根据上下文，键可能是一个单词或字符串，并且它们的长度可以是固定的或可变的。
- en: Understanding the MSD and LSD Radix sorts
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解MSD和LSD基数排序
- en: Let us take some time to understand how the MSD Radix sort and the LSD Radix
    sort work before we start working on developing the equivalent on OpenCL.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始开发OpenCL上的等效程序之前，让我们花些时间来理解MSD基数排序和LSD基数排序是如何工作的。
- en: How to do it…
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: The Radix sort assumes that we wish to sort Radix-R numbers by considering the
    most significant digit first. For this to happen, we can partition the input into
    *R* rather than just two, and we have actually seen this done before. This is
    data binning, but it extends that with the counting sort. A Radix sort can be
    run on ASCII characters, Unicode characters, integer numbers (32-bit / 64-bit),
    or floating-point numbers (sorting floating-point numbers is tricky). You need
    to figure out what constitutes a key. Keys can be thought of as 8-bit keys, 16-bit
    keys, and so on, and we know by now that Radix sorts require repeated iterations
    to extract the keys and sort and bin them based on base *R*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 基数排序假设我们希望首先考虑最高有效位来对基数-R的数字进行排序。为了实现这一点，我们可以将输入分成*R*而不是仅仅两个部分，我们之前确实看到过这样做。这是数据分箱，但它通过计数排序进行了扩展。基数排序可以在ASCII字符、Unicode字符、整数（32位/64位）或浮点数（排序浮点数是棘手的）上运行。你需要确定什么构成了键。键可以被认为是8位键、16位键等等，现在我们知道基数排序需要重复迭代以提取键，并根据基数*R*对它们进行排序和分箱。
- en: 'In the following code snippet, we have an MSD Radix sort that sorts the characters
    in a given string in the programming language C, and the radix we use is 256 (the
    maximum value of an unsigned 8-bit number, otherwise a signed 8-bit would be -128
    to 127):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们有一个MSD基数排序，它使用编程语言C对给定字符串中的字符进行排序，我们使用的基数是256（无符号8位数的最大值，否则有符号8位数的范围是-128到127）：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The second approach in Radix sorting scans the input from right to left and
    examines each element by applying a similar operation as in an MSD Radix sort.
    This is known as the **Least Significant Digit** (**LSD) Radix sort**. LSD Radix
    sorting works because when any two elements differ, the sorting will place them
    in the proper relative order, and even when these two elements differ, the fact
    that LSD exhibits stable sorting means that their relative order is still maintained.
    Let''s take a look at how it would work for sorting three character strings:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 基数排序的第二种方法是从右向左扫描输入，并通过对MSD基数排序应用类似操作来检查每个元素。这被称为**最低有效位**（**LSD**）基数排序。LSD基数排序之所以有效，是因为当任何两个元素不同时，排序将它们放置在正确的相对顺序中，即使这两个元素不同，LSD表现出稳定的排序特性，这意味着它们的相对顺序仍然保持不变。让我们看看它是如何对排序三个字符字符串起作用的：
- en: '![How to do it…](img/4520OT_10_01.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![如何做…](img/4520OT_10_01.jpg)'
- en: 'A typical LSD Radix sort for sorting characters in a given string might look
    like the following code (assuming all keys have a fixed width; let''s call it
    `W`):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对给定字符串中的字符进行排序的典型LSD基数排序可能看起来像以下代码（假设所有键都有固定的宽度；让我们称它为`W`）：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How it works…
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Both approaches are similar as they both bin the characters into *R* bins, that
    is, 256 bins, and they also use the idea of the counting sort to work out where
    the final sorting arrangement is going to be using a temporary storage, `temp`,
    and then use that temporary storage and move the data to their sorted places.
    The nice thing about MSD over LSD Radix sorts is that MSD may not examine all
    of the keys and works for variable-length keys; although, in that lies another
    problem—MSD can experience sub-linear sorts; in practice LSD is generally preferred
    when the size of the key is fixed.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都很相似，因为它们都将字符分入 *R* 个桶中，即256个桶，并且它们还使用了计数排序的思想来确定最终的排序排列，使用临时存储`temp`，然后使用这个临时存储并将数据移动到它们的排序位置。与LSD基数排序相比，MSD基数排序的优点是MSD可能不会检查所有的键，并且适用于可变长度的键；尽管如此，这也带来了另一个问题——MSD可能会遇到次线性排序；在实践中，当键的大小固定时，通常更倾向于使用LSD。
- en: The runtime of an LSD Radix sort is ![How it works…](img/4520OT_10_16.jpg) when
    compared to the runtimes of other sorting algorithms that are based on the divide-conquer
    approach, which generally have a runtime of ![How it works…](img/4520OT_10_17.jpg)
    you might be tempted to conclude that Radix sorting would be faster than comparison-based
    sorts like quicksort, and you could be right. But, in practice, a well-tuned quicksort
    can outperform a Radix sort by 24 percent by applying more advanced techniques
    to improve cache friendliness during the execution. However, technology is constantly
    evolving, and researchers and engineers will find opportunities to maximize the
    performance.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于划分-征服方法的其它排序算法的运行时间相比，LSD基数排序的运行时间是![它是如何工作的…](img/4520OT_10_16.jpg)，你可能倾向于得出结论，基数排序会比基于比较的排序算法（如快速排序）更快，你可能是对的。但是，在实践中，经过良好调优的快速排序可以通过应用更高级的技术来提高缓存友好性，从而比基数排序快24%。然而，技术不断进步，研究人员和工程师将找到机会最大化性能。
- en: Tip
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: You may wish to read the papers *The influence of cache on sorting* by *LaMarca*
    and *Adapting Radix Sort to the memory hierarchy* by *Rahman and Raman* for more
    algorithmic improvements that they have worked on.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想阅读LaMarca的论文《缓存对排序的影响》和Rahman和Raman的论文《将基数排序适应内存层次结构》，以了解更多他们所工作的算法改进。
- en: Understanding reduction
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解归约
- en: 'Radix sorting employs two techniques: **reduction** and **scan**. These are
    classified as data collection patterns as they occur frequently in parallel computing.
    This recipe will focus on reduction, which allows data to be condensed to a single
    element using associative binary operators. The scan pattern can be easily mistaken
    for the reduction pattern and the key difference is that this pattern reduces
    every subsequence of a collection up to every position in the input. We''ll defer
    the discussion of scans until we get to the next section.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基数排序采用两种技术：**归约**和**扫描**。这些被归类为数据收集模式，因为它们在并行计算中经常出现。这个配方将专注于归约，它允许使用关联的二进制运算符将数据压缩为单个元素。扫描模式很容易与归约模式混淆，关键区别在于这种模式将集合的每个子序列减少到输入的每个位置。我们将推迟对扫描的讨论，直到我们到达下一节。
- en: 'In the reduction pattern, we typically have an associative binary operator,
    ![Understanding reduction](img/4520OT_10_18.jpg)that we use to collate all elements
    in a container in a pair-wise fashion. The fact that we need an associative binary
    operator is an important one, because it implies that the developer can reorganize
    the combination function to check if it performs efficiently; we''ll go into that
    a little later. Let''s take a look at a serial algorithm for conducting reduction
    in the following code snippet:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在归约模式中，我们通常有一个结合二进制运算符![理解归约](img/4520OT_10_18.jpg)，我们用它以成对的方式收集容器中的所有元素。我们需要一个结合二进制运算符是一个重要的因素，因为它意味着开发者可以重新组织组合函数以检查其效率；我们稍后会详细讨论这一点。让我们看一下以下代码片段中执行归约的串行算法：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The algorithm basically takes an associative binary operator, `f` (that is,
    a pointer to a function), and an array `a`, of length `n` and computes the operation
    ![Understanding reduction](img/4520OT_10_19.jpg)over the array with an initial
    value identified by `identity`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法基本上接受一个结合二进制运算符`f`（即函数的指针）和一个长度为`n`的数组`a`，并计算数组上的操作![理解归约](img/4520OT_10_19.jpg)，初始值由`identity`标识。
- en: 'An associative binary operator can allow the developer to extract parallelism
    from it because associativity means that the operator would produce the same result
    regardless of the order in which it is applied to the elements. That is to say:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 结合二进制运算符可以允许开发者从中提取并行性，因为结合性意味着运算符无论应用于元素的顺序如何，都会产生相同的结果。也就是说：
- en: '![Understanding reduction](img/4520OT_10_20.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![理解归约](img/4520OT_10_20.jpg)'
- en: 'The previous expression is equivalent to:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 上述表达式等同于：
- en: '![Understanding reduction](img/4520OT_10_21.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![理解归约](img/4520OT_10_21.jpg)'
- en: 'Putting on the many core hat, we can actually imagine a tree of computations
    in which the sub-trees represent the computation of the form![Understanding reduction](img/4520OT_10_22.jpg).
    The first sweep would compute the result of this sub-tree while the second sweep
    would collate the results of the other sub-trees. This will be evident once you
    have had a chance to examine them visually in the next two diagrams:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 穿上多核帽子，我们实际上可以想象一个计算树，其中子树代表形式![理解归约](img/4520OT_10_22.jpg)的计算。第一次遍历将计算这个子树的结果，而第二次遍历将收集其他子树的结果。一旦你有机会在接下来的两个图中直观地检查它们，这将会很明显：
- en: '![Understanding reduction](img/4520OT_10_02.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![理解归约](img/4520OT_10_02.jpg)'
- en: 'It will be very useful for you to contrast the manner in which these diagrams
    differ. One of the ways is that the former implies a sequence of operations in
    traversal order, and this is very different from the latter (as shown in the following
    diagram):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对比这些图示的不同方式对你来说将非常有用。其中一种方式是前者暗示了一个按遍历顺序的操作序列，这与后者（如下图中所示）非常不同：
- en: '![Understanding reduction](img/4520OT_10_03.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![理解归约](img/4520OT_10_03.jpg)'
- en: 'It''s great news to know that associative operators allow the reduction to
    be parallelized, but it''s not the entire story, because associativity only allows
    us to group the operations and does not reveal to us whether these groups of binary
    operations need to occur in a specific order. If you are wondering whether we
    are talking about commutativity, you are spot on! Commutativity gives us the important
    property of changing the order of application. We know that some operations exhibit
    one of these while others exhibit both; for example, we know that addition and
    multiplication of numbers is both associative and commutative. The following is
    what a commutative parallel reduction might look like:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 了解结合运算符允许归约并行化是个好消息，但这并不是全部，因为结合性只允许我们分组操作，并不能告诉我们这些二进制操作组是否需要按特定顺序发生。如果你想知道我们是否在谈论交换律，你完全正确！交换律给我们提供了改变应用顺序的重要属性。我们知道一些操作表现出其中之一，而另一些则表现出两者；例如，我们知道数字的加法和乘法既是结合的又是交换的。以下是一个交换律并行归约可能的样子：
- en: '![Understanding reduction](img/4520OT_10_04.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![理解归约](img/4520OT_10_04.jpg)'
- en: Now, seeing this information, you might wonder how this can be translated into
    OpenCL. We are going to demonstrate a few reductions kernels in this recipe where
    each one will provide you with an improvement over the previous one.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看到这些信息，你可能会想知道这如何翻译成OpenCL。我们将在这个菜谱中演示几个归约内核，每个内核都会在之前的基础上提供改进。
- en: How to do it…
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: For this recipe, we are going to assume that we have a large array of a few
    million elements and that we like to apply the reduction algorithm to compute
    the sum of all elements. The first thing to do is produce a parallel algorithm
    for the serial version we saw earlier. All the kernels we are demonstrating are
    in `Ch10/Reduction/reduction.cl`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，我们将假设我们有一个包含几百万个元素的数组，并且我们希望应用归约算法来计算所有元素的总和。首先要做的是为之前看到的串行版本生成一个并行算法。我们展示的所有内核都在`Ch10/Reduction/reduction.cl`。
- en: In the serial version of the algorithm, you would have noticed that we simply
    pass the accumulator into the binary function to perform the operation. However,
    we cannot use this method in the GPU since it cannot support tens of thousands
    of executing threads and also the device can contain many more processors than
    an x86 CPU has. The only solution is to partition the data across the processors
    so that each block processes a portion of the input, and when all of the processors
    are executing in parallel, we should expect the work to be completed in a short
    span of time.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法的串行版本中，您可能会注意到我们只是简单地将累加器传递给二进制函数以执行操作。然而，在GPU上我们不能使用这种方法，因为它不能支持成千上万的执行线程，而且设备可以包含比x86
    CPU更多的处理器。唯一的解决方案是将数据分区到处理器上，以便每个块处理输入的一部分，当所有处理器并行执行时，我们应该期望工作在短时间内完成。
- en: 'Assuming that a block has computed its summed value, we still need a way to
    collate all those partial sums from all blocks, and considering that OpenCL does
    not have a global synchronization primitive or API, we have two options: have
    OpenCL collate the partial sums or have the host code collate the partial sums;
    for our examples, the second option is chosen.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个块已经计算出了其总和值，我们仍然需要一种方式来汇总所有块的所有部分总和，考虑到OpenCL没有全局同步原语或API，我们有两种选择：让OpenCL汇总部分总和，或者让主机代码汇总部分总和；在我们的示例中，选择了第二种选项。
- en: 'The first kernel, `reduce0`, is a direct translation of the serial algorithm:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个内核`reduce0`是串行算法的直接翻译：
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How it works…
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'This kernel block would load the elements to its shared memory, `sdata`, and
    we conduct the reduction in `sdata` in various stages governed by the `for` loop,
    allowing work items with IDs that are multiples of two to perform the pair-wise
    reduction. Therefore, in the first iteration of the loop, work items with IDs
    *{0, 2, 4, 6, 8, 10, 12, 14, ..., 254}* would execute, in the second iteration,
    only work items with IDs *{0, 4, 8, 12, 252}* would execute, and so on. Following
    the reduction algorithm, the partial sum would be deposited into `sdata[0]`, and
    finally this value would be copied out by one thread which happens to have an
    ID value equal to `0`. Admittedly, this kernel is pretty good but it suffers from
    two problems: the modulus operator takes a longer time to execute and wavefronts
    are diverged. The larger issue here is the problem of wavefront divergence since
    it means that some work items in the wavefronts are executing while some are not,
    and in this case, the work items with odd IDs are not executing while those with
    even IDs are, GPUs deal with this problem by implementing predication, and this
    means that all work items in the following code snippet actually get executed.
    However, the predication unit on the GPU will apply a mask so that only those
    work items whose IDs matched the condition, `if(tid % (2*s) == 0)`, will execute
    the statement in the `if` statement, while those work items who fail the condition,
    `false`, would invalidate their results. Obviously, this is a waste of computing
    resources:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内核块会将元素加载到其共享内存`sdata`中，我们在`sdata`中进行各种阶段的归约，这些阶段由`for`循环控制，允许具有ID为2的倍数的作业项执行成对归约。因此，在循环的第一迭代中，ID为*{0,
    2, 4, 6, 8, 10, 12, 14, ..., 254}*的作业项将执行，在第二迭代中，只有ID为*{0, 4, 8, 12, 252}*的作业项将执行，依此类推。按照归约算法，部分总和将被存入`sdata[0]`，最后这个值将由一个ID值恰好等于`0`的线程复制出来。诚然，这个内核相当不错，但它有两个问题：取模运算符执行时间较长，波前发散。这里更大的问题是波前发散问题，因为它意味着波前中的某些作业项正在执行，而有些则没有，在这种情况下，具有奇数ID的作业项没有执行，而具有偶数ID的作业项则执行，GPU通过实现预测来解决此问题，这意味着以下代码片段中的所有作业项实际上都会执行。然而，GPU上的预测单元将应用一个掩码，以便只有那些ID与条件匹配的作业项，即`if(tid
    % (2*s) == 0)`，将执行`if`语句中的语句，而那些未通过条件的作业项，即`false`，将使他们的结果无效。显然，这是计算资源的浪费：
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Fortunately, this can be solved with little effort, and the next kernel code
    demonstrates this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这可以通过很少的努力来解决，接下来的内核代码展示了这一点：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We replaced the conditional evaluation after the modulus operator has been applied
    to something more palatable. The appetizing portion is the fact that we no longer
    have diverging wavefronts, and we have also made strided accesses to the shared
    memory.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模运算符应用后的条件评估替换为更易于接受的内容。令人垂涎的部分是，我们不再有发散的前沿，我们还对共享内存进行了步进访问。
- en: There's more…
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'So far, we have seen how we can apply our understanding of associativity to
    build the reduction kernel and also how to make use of our new understanding of
    commutativity in the reduction process. The commutative reduction tree is actually
    better than the associative reduction tree because it makes better use of the
    shared memory by compacting the reduced values and hence raising efficiency; the
    following kernel, `reduce2`, reflects this:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何将我们对结合律的理解应用于构建归约内核，以及如何利用我们对交换律的新理解来优化归约过程。交换律归约树实际上比结合律归约树更好，因为它通过压缩归约值更好地利用了共享内存，从而提高了效率；下面的内核，`reduce2`，反映了这一点：
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'However, this isn''t very good because now during the first iteration, we have
    already made half of those work items idle and efficiency is definitely affected.
    Fortunately, however, the remedy is simple. We reduce half the number of blocks
    and during the hydration of the shared memory, we load two elements and store
    the sum of these two elements instead of just loading values from global memory
    and storing them into shared memory. The kernel, `reduce3`, reflects this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不理想，因为在第一次迭代中，我们已经使一半的工作项闲置，效率肯定受到了影响。幸运的是，然而，补救措施很简单。我们减少了一半的块数，在共享内存的活化过程中，我们加载两个元素并将这两个元素的和存储起来，而不是仅仅从全局内存中加载值并将它们存储到共享内存中。内核，`reduce3`，反映了这一点：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, things are starting to look much better and we''ve used what we call **reversed
    loop** (which is basically counting backwards) to get rid of the problem of divergent
    wavefronts; in the meantime, we have also not reduced our capacity to reduce elements
    because we''ve performed that while hydrating the shared memory. The question
    is whether there''s more we can do? Actually, there is another idea we can qualify
    and that is to take advantage of atomicity of wavefronts or warps executing on
    GPUs. The next kernel, `reduce4`, demonstrates how we utilized wavefront programming
    to reduce blocks atomically:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，事情开始看起来要好得多，我们使用了所谓的**逆序循环**（这基本上是反向计数）来消除发散前沿的问题；同时，我们也没有减少我们的元素归约能力，因为我们是在活化共享内存的同时执行这一操作的。问题是是否还有更多我们可以做的？实际上，我们还有一个可以验证的想法，那就是利用在GPU上执行的前沿或warps的原子性。下一个内核，`reduce4`，展示了我们如何利用前沿编程来原子性地减少块：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the code block demarcated by the statement `if (tid < 64)`, we no longer
    need to place the memory barriers because the code block only hosts one wavefront
    which executes atomically in the lock step.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在由语句`if (tid < 64)`定义的代码块中，我们不再需要放置内存屏障，因为该代码块只包含一个原子性地同步执行的前沿。
- en: Developing the Radix sort in OpenCL
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在OpenCL中开发基数排序
- en: 'From this section onwards, we are going to develop this sorting method for
    OpenCL. We are going to do two things: implement the parallel Radix sort described
    in the paper that *Marco Zagha* and *Guy E. Blelloch* wrote in 1991 titled *Radix
    Sort for Vector Multiprocessors*. The former algorithm was crafted for the CRAY
    Y-MP computer (which, in turn, was adapted from the parallel Radix sort algorithm
    that worked on the **Connection Machine (CM-2)**).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 从本节开始，我们将为OpenCL开发这种排序方法。我们将做两件事：实现论文中描述的并行基数排序，这篇论文是由*Marco Zagha*和*Guy E.
    Blelloch*在1991年撰写的，题为*向量多处理器的基数排序*。前者算法是为CRAY Y-MP计算机设计的（该计算机反过来又从在**连接机（CM-2）**上运行的并行基数排序算法中改编而来）。
- en: Getting ready
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Radix sorting attempts to treat keys as multi-digit numbers, where each digit
    is an integer depending on the size of the Radix, *R*. An example would be sorting
    a large array of 32-bit numbers. We can see that each such number is made up of
    four bytes (each byte is 8-bits on today's CPU and GPU processors), and if we
    decide to assume that each digit would be 8-bits, we naturally would treat a 32-bit
    number as comprised of four digits. This notion is most natural when you apply
    the concept back to a string of words, treating each word as comprising of more
    than one character.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 基数排序试图将键视为多数字数，其中每个数字是一个依赖于基数大小 *R* 的整数。一个例子就是对一个大型32位数字数组进行排序。我们可以看到，这样的每个数字由四个字节组成（在今天的CPU和GPU处理器上，每个字节是8位），如果我们决定假设每个数字是8位，那么我们自然会把这个32位数字视为由四个数字组成。当你将这个概念应用到由多个字符组成的单词字符串时，这个概念最为自然。
- en: 'The original algorithm worded in the 1999 paper basically uses the counting
    sort algorithm and it has three main components which will in turn sort the input
    by iterating all three components until the job is done. The pseudo code, which
    is a serial algorithm, is presented as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 1999年论文中提出的原始算法基本上使用了计数排序算法，并且有三个主要组件，通过迭代这三个组件直到任务完成来对输入进行排序。伪代码，这是一个串行算法，如下所示：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The algorithm `HISTOGRAM-KEYS` is something that we have already encountered
    a few chapters ago, and it is really the histogram. This algorithm computes the
    distribution of the keys that it encounters during the sort. This algorithm is
    expressed in a serial fashion, that is, it is supposed to run on a single executing
    thread; we have already learned how to parallelize that and you can apply those
    techniques here. However, what we are going to do now deviates from what you have
    seen in that previous chapter, and we'll reveal that soon enough.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 `HISTOGRAM-KEYS` 是我们在几章之前已经遇到过的，它实际上就是直方图。这个算法计算它在排序过程中遇到的键的分布。这个算法以串行方式表达，也就是说，它应该在单个执行线程上运行；我们已经学过如何并行化它，并且你可以在这里应用这些技术。然而，我们现在要做的将偏离你在上一章中看到的内容，我们很快就会揭示这一点。
- en: The next algorithm is `SCAN-BUCKETS`, and it is named as such because it actually
    scans the entire histogram to compute the prefix sums (we'll examine prefix sums
    in fair detail later). In this scan operation, `Bucket[i]` contains the number
    of digits with a value, `j`, such that `j` is greater than `i`, and this value
    is also the position, that is, the array index in the output.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个算法是 `SCAN-BUCKETS`，它之所以被命名为这样，是因为它实际上扫描整个直方图来计算前缀和（我们将在后面详细检查前缀和）。在这个扫描操作中，`Bucket[i]`
    包含具有值 `j` 的数字的数量，其中 `j` 大于 `i`，这个值也是位置，即输出中的数组索引。
- en: The final algorithm is `RANK-AND-PERMUTE`, and each key with a digit of value
    of `i` is placed in its final location by getting the offset from `Bucket[i]`
    and incrementing the bucket so that the next key with the same value `i` gets
    placed in the next location. You should also notice that `COUNTING SORT` is stable.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的算法是 `RANK-AND-PERMUTE`，每个具有值 `i` 的键通过从 `Bucket[i]` 获取偏移量并增加桶，以便具有相同值 `i`
    的下一个键放置在下一个位置来放置在其最终位置。你也应该注意到 `COUNTING SORT` 是稳定的。
- en: Before we dive into parallelization of the algorithms and how they work in a
    cohesive manner, it's important to take the next few paragraphs to understand
    what prefix sums are; the next paragraph highlights why they matter in Radix sorts.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨算法的并行化以及它们如何协同工作之前，重要的是花接下来的几段文字来理解前缀和是什么；下一段文字将强调它们在基数排序中的重要性。
- en: 'In the previous sections, we introduced MSD and LSD Radix sorts and the prefix
    sums computation is embedded in the code. However, we didn''t flag it out for
    you then. So, now''s the time and the following is the code (taken from the previous
    `lsd_sort` and `msd_sort` sections):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们介绍了MSD和LSD基数排序，以及前缀和的计算被嵌入到代码中。然而，当时我们没有特别指出这一点。所以，现在是时候了，以下就是代码（取自之前的
    `lsd_sort` 和 `msd_sort` 部分）：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you recall how MSD/LSD works, we basically create a histogram of the values
    we have encountered and, at each stage of the sorting, we compute the prefix sums
    so that the algorithm can know where to place the output in a sorted order. If
    you are still doubtful, you should stop now and flip back to that section and
    work through the LSD sorting for strings of three characters.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得MSD/LSD是如何工作的，我们基本上创建了一个我们遇到值的直方图，并且在排序的每个阶段，我们计算前缀和，以便算法知道在排序顺序中放置输出在哪里。如果你仍然怀疑，你现在应该停下来，翻回到那个部分，并处理三个字符字符串的LSD排序。
- en: Note
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The prefix sums is actually a generalization of the global sum, and its original
    formulation goes something like the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀和实际上是全局和的推广，其原始公式大致如下：
- en: The prefix sum operation takes a binary associative operator ![Getting ready](img/4520OT_10_23.jpg),
    and an ordered set of n elements, ![Getting ready](img/4520OT_10_24.jpg), and
    returns the ordered set ![Getting ready](img/4520OT_10_25.jpg).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀和操作接受一个二元结合运算符![准备中](img/4520OT_10_23.jpg)，一个有序的n个元素的集合![准备中](img/4520OT_10_24.jpg)，并返回一个有序的集合![准备中](img/4520OT_10_25.jpg)。
- en: We use a concrete example like taking a summation over an arbitrary array like
    `[39, 23, 44, 15, 86]`. Using the addition operator, the output would be `[39,
    62, 108, 125, 211]`, and it is not obvious why this sort of computation is important
    or is even needed. In fact it is not even clear whether there is a direct way
    to parallelize this algorithm because of dependencies that each subsequent computation
    relies on the previous.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用一个具体的例子来说明，比如对一个任意数组如`[39, 23, 44, 15, 86]`进行求和。使用加法运算符，输出将是`[39, 62, 108,
    125, 211]`，这种计算为什么重要或者为什么需要并不明显。实际上，由于后续的每个计算都依赖于前一个计算，甚至不清楚是否有直接并行化此算法的方法。
- en: 'A sequential version of the prefix sums which has a runtime of ![Getting ready](img/4520OT_10_16.jpg)
    can be expressed as follows, assuming there are two arrays `in_arr` and `out_arr`,
    and `out_arr` is designed to contain the prefix sums for `in_arr`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀和的顺序版本具有![准备中](img/4520OT_10_16.jpg)的运行时间，可以表示如下，假设有两个数组`in_arr`和`out_arr`，并且`out_arr`被设计用来包含`in_arr`的前缀和：
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To extract parallelism from this, we need to adjust the way we view the arbitrary
    array of input values, and the adjustment we are talking about is actually imagining
    the array to be consumed by a tree of computations. Let's go on a little further
    to see why.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从这中提取并行性，我们需要调整我们看待任意输入值数组的观点，而我们正在讨论的调整实际上是将数组想象成被一个计算树消耗。让我们进一步探讨一下为什么。
- en: 'At this point, we think it''s important to step back into history and see who
    came up with the original prefix sum computation. As far as I am aware, two researchers
    in 1986, *Daniel Hillis* and *Guy L. Steele*, presented a version of the prefix
    sum as part of an article titled *Data Parallel Algorithms* in the *ACM (Association
    for Computing Machinery)* magazine, and the algorithm they presented worked as
    follows (cited as such in that article):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们认为回顾历史并了解谁提出了原始的前缀和计算方法是很重要的。据我所知，1986年，两位研究人员*丹尼尔·希尔利斯*和*盖·L·斯蒂尔*在*ACM（计算机机械协会）*杂志上发表的一篇题为*数据并行算法*的文章中提出了前缀和的一个版本，他们提出的算法如下（在该文章中如此引用）：
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The following diagram (courtesy of *Mark Harris* from the NVIDIA Corporation),
    pictorially illustrates what the Hillis and Steele algorithm does. It starts at
    the level where all eight elements are looked upon as leaves of the binary tree
    and proceeds to work its way through computing the partial sums. Each level of
    the computation, `d`, will compute partial sums based on the previous level's
    computation. An assumption found in the algorithm is that it assumes that there
    are as many processors as there are elements and this is demonstrated by the conditional
    statement in the algorithm, `if (k >= 2j)`. Another problem it has got is that
    it's not very efficient; it has a runtime complexity of ![Getting ready](img/4520OT_10_26.jpg),
    and you will recall that our sequential scan runs at ![Getting ready](img/4520OT_10_16.jpg),
    so it is definitely slower.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表（由NVIDIA公司的*Mark Harris*提供），直观地说明了Hillis和Steele算法的工作原理。它从所有八个元素都被视为二叉树叶子的那一层开始，然后通过计算部分和逐步工作。计算的每一层，`d`，将基于上一层的计算计算部分和。算法中的一个假设是它假设有与元素一样多的处理器，这通过算法中的条件语句`if
    (k >= 2j)`得到证明。它还有一个问题就是效率不高；它的运行时间复杂度为![准备中](img/4520OT_10_26.jpg)，你将记得我们的顺序扫描以![准备中](img/4520OT_10_16.jpg)的速度运行，所以它肯定更慢。
- en: '![Getting ready](img/4520OT_10_05.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![准备中](img/4520OT_10_05.jpg)'
- en: 'However, *Guy Blelloch* found ways to improve this, and they are based on the
    idea of building a balanced binary tree and building out that tree by performing
    addition on each node (conceptually speaking). Because such a tree with *n* leaves
    (which is corresponding to the number of elements in the array) would have ![Getting
    ready](img/4520OT_10_27.jpg) levels and each level has *2^d* nodes, the runtime
    complexity is ![Getting ready](img/4520OT_10_16.jpg). The following diagram is
    an illustration of how a balanced binary tree can compute the array of arbitrary
    values:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，*Guy Blelloch*找到了改进这种方法的方法，它们基于构建平衡二叉树并在每个节点上进行加法操作（从概念上讲）的想法。因为这样的树有*n*个叶子（这对应于数组中的元素数量），将有![准备中](img/4520OT_10_27.jpg)层，每层有*2^d*个节点，运行时间复杂度为![准备中](img/4520OT_10_16.jpg)。以下图表展示了平衡二叉树如何计算任意值的数组：
- en: '![Getting ready](img/4520OT_10_06.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![准备中](img/4520OT_10_06.jpg)'
- en: The previous diagram created juxtaposition, and it alters the way the same piece
    of data you saw, that is, one dimensional flat array containing arbitrary values.
    Imagine a tree of computations that scans and operates on two values. One way
    of storing those partial sums is to write the value in place to the array and
    another way is to use shared memory on the device.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的图表创建了并列关系，并改变了你看同一份数据的方式，即包含任意值的一维扁平数组。想象一个计算树，它扫描并操作两个值。存储这些部分和的一种方法是将值就地写入数组，另一种方法是在设备上使用共享内存。
- en: 'The astute reader in you would notice that we can probably parallelize the
    computation at each level of the tree by allowing one thread to read two elements,
    sum them up, and write them back into the array, and then you just read off the
    last element of that array for the final sum. This algorithm that we just described
    is known as a **reduction** kernel or an **up-sweep** kernel (since we are sweeping
    values up to the root of the tree), and we have seen how it works in the chapter
    where we discussed about sparse matrix computations in OpenCL. The following is
    the more formal definition of the reduction phase by *Guy Blelloch* when it''s
    applied to a balanced binary tree with depth ![Getting ready](img/4520OT_10_28.jpg):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你敏锐的读者直觉会注意到，我们可能通过允许一个线程读取两个元素，将它们相加，并将结果写回数组，然后在数组中读取最后一个元素以获得最终和，来并行化树中每一层的计算。我们刚才描述的算法被称为**归约**内核或**上推**内核（因为我们正在将值向上推到树的根），我们在讨论OpenCL中稀疏矩阵计算的章节中看到了它是如何工作的。以下是由*Guy
    Blelloch*提出的归约阶段的更正式定义，当它应用于深度![准备中](img/4520OT_10_28.jpg)的平衡二叉树时：
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You might think that this up-sweep kernel still doesn't compute the prefix sums,
    but we do appear to have found a solution to solving summation in parallel; at
    this point, the following diagram will help us learn what actually goes on during
    a run of the up-sweep, and we find it helpful to flatten the loop a little to
    examine its memory access pattern.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为这个上推内核仍然没有计算前缀和，但似乎我们已经找到了并行解决求和问题的解决方案；在这个时候，以下图表将帮助我们了解上推运行期间实际上发生了什么，我们发现稍微展开循环以检查其内存访问模式是有帮助的。
- en: 'Assuming we have eight elements in our array (that is, `n = 8`), our tree would
    have a depth of `3` and `d` would range from `0` to `2`. Imagining that we are
    at `d = 0`, through to `2` we would have the following expressions:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的数组中有八个元素（即，`n = 8`），我们的树将有一个深度为`3`，`d`的范围从`0`到`2`。想象一下，当我们处于`d = 0`时，通过到`2`，我们会得到以下表达式：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The next diagram best explains the evaluation of the preceding expressions,
    and a picture does reveal more about the story than plain equations:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下一张图表最好地解释了前面表达式的评估，而图片确实比简单的方程更能揭示故事：
- en: '![Getting ready](img/4520OT_10_07.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![准备就绪](img/4520OT_10_07.jpg)'
- en: From this diagram, we can observe that partial sums are built up at each level
    of the tree and one of the efficiencies introduced here is not repeating any addition,
    that is, no redundancies. Let's demonstrate how this would work for an array of
    eight elements, and we will also employ the up-sweep algorithm.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张图中，我们可以观察到在树的每一层构建了部分和，这里引入的一个效率是避免了任何重复的加法，也就是说，没有冗余。让我们演示这对于一个包含八个元素的数组是如何工作的，我们还将使用up-sweep算法。
- en: 'The following diagram illustrates the writes that occurred at each level of
    the tree we''re scanning; in that diagram, the boxes colored blue represent the
    partial sums that were built up at each level of the tree, and the red box represents
    the final summed value:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了我们在扫描的树的每一层发生的写入操作；在该图表中，蓝色框表示在树的每一层构建的部分和，红色框表示最终的总和值：
- en: '![Getting ready](img/4520OT_10_08.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![准备就绪](img/4520OT_10_08.jpg)'
- en: 'To be able to compute the prefix sums from the up-sweep phase we need to proceed
    from the root of this tree and perform a *down-sweep* using this algorithm by
    *Guy Blelloch*:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够从up-sweep阶段计算前缀和，我们需要从树的根开始，并使用*Guy Blelloch*的此算法进行*down-sweep*：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This down-sweep works its way down from the top (or root) of the tree after
    the reduce phase and builds the prefix sums. Let's flatten the loop to examine
    its memory access pattern.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个down-sweep在reduce阶段之后从树的顶部（或根）开始向下进行，并构建前缀和。让我们简化循环以检查其内存访问模式。
- en: 'As before with the up-sweep, let''s assume that we have eight elements (that
    is, `n = 8`); we would have a depth of `3`, and that implies `d` would range from
    `0` to `2`. The following are the flattened expressions:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如同之前的up-sweep一样，让我们假设我们有八个元素（即，`n = 8`）；我们会有一个深度为`3`，这意味着`d`的范围从`0`到`2`。以下是一些简化后的表达式：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following diagram best expresses how the prefix sums are computed from
    the reduce/up-sweep phase:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表最好地表达了从reduce/up-sweep阶段计算前缀和的方式：
- en: '![Getting ready](img/4520OT_10_09.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![准备就绪](img/4520OT_10_09.jpg)'
- en: 'Let us concretize these ideas by looking at how the down-sweep phase would
    proceed after the reduce/up-sweep phase using the following diagram; the `input`
    array is the original array, and we have kept it there for you to verify that
    the prefix sum computation according to the previous algorithm is correct. The
    lower portion of the diagram illustrates how memory is accessed. Keep in mind
    that updates are done in place, and when you combine the diagrams of the up-sweep
    and down-sweep phases, you''ll notice that we make two passes over the original
    input array to arrive at the solution of prefix sums, which is what we wanted:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下图表具体化这些想法，看看在reduce/up-sweep阶段之后，down-sweep阶段将如何进行；`input`数组是原始数组，我们保留它以便您验证根据之前的算法计算的前缀和是否正确。图表的下部说明了内存的访问方式。请注意，更新是在原地进行的，当您结合up-sweep和down-sweep阶段的图表时，您会注意到我们对原始输入数组进行了两次遍历，以得到前缀和的解，这正是我们想要的：
- en: '![Getting ready](img/4520OT_10_10.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![准备就绪](img/4520OT_10_10.jpg)'
- en: How to do it …
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点 …
- en: The kernel we present here is found in `Ch10/RadixSort_GPU/RadixSort.cl`, and
    the implementation drew inspiration from the academic paper entitled *Radix Sort
    for Vector Multiprocessors* by *Mark Zagha* and *Guy E. Blelloch* for 32-bit integers.
    The algorithm is based on the LSD Radix sort, and it iterates all the keys while
    shifting the keys based on the chosen Radix and executing OpenCL kernels in sequence;
    this is best described in the previous diagram.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里提出的内核可以在`Ch10/RadixSort_GPU/RadixSort.cl`中找到，其实施受到了由*Mark Zagha*和*Guy E.
    Blelloch*撰写的学术论文《向量多处理器的基数排序》（*Radix Sort for Vector Multiprocessors*）的启发，该论文针对32位整数。该算法基于LSD基数排序，并迭代所有键，根据选择的基数移动键，并按顺序执行OpenCL内核；这在前面的图表中描述得最好。
- en: 'As before, we present the sequential version of the Radix sort that was translated
    based on *Zagha* and *Blelloch*, and like what we have done previously, this is
    the golden reference which we will use to determine the correctness of the data
    calculated by the OpenCL equivalent. We won''t spend too much time discussing
    about this implementation here, but rather it serves as a reference point where
    you can draw the similarities and contrasts when we demonstrate how the parallel
    and sequential code differs:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们展示了基于*Zagha*和*Blelloch*的Radix排序的顺序版本，就像我们之前所做的那样，这是我们的黄金参考，我们将用它来确定OpenCL等效数据计算的准确性。我们不会在这里过多地讨论这个实现，但它作为参考点，当我们展示并行和顺序代码的差异时，你可以从中找到相似之处和不同之处：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This sequential code is akin to the `lsd_sort` code we showed earlier, and it
    essentially builds a histogram of the examined keys that uses the counting sort
    to sort them, and it keeps doing this until all data is acted upon.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这段顺序代码类似于我们之前展示的`lsd_sort`代码，它本质上构建了一个使用计数排序对它们进行排序的键的直方图，并且它会一直这样做，直到所有数据都被处理。
- en: 'The following kernels are taken from `Ch10/RadixSort_GPU/RadixSort.cl`, and
    we''ll refer to the appropriate code when we explain the internal workings of
    the algorithm:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内核来自`Ch10/RadixSort_GPU/RadixSort.cl`，当我们解释算法的内部工作原理时，我们将引用适当的代码：
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works…
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The strategy we present here is to break keys, that is, break 32-bit integers
    into 8-bit digits, and then sort them one at a time starting from the least significant
    digit. Based on this idea, we are going to loop four times and at each loop number
    *i*, we are going to examine the *i* numbered 8-bit digit.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里提出的方法是分解键，即把32位整数分解成8位数字，然后逐个从最低有效位开始排序。基于这个想法，我们将循环四次，在每次循环编号*i*时，我们将检查*i*编号的8位数字。
- en: 'The general looping structure based on the previous description is given in
    the following code:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的描述，以下代码给出了基于一般循环结构的代码：
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The three invocations in the loop are the work horses of this implementation
    and they invoke the kernels to compute the histogram from the input based on the
    current byte we are looking at. The algorithm will basically compute the histogram
    of the keys that it has examined; the next phase is to compute the prefix sums
    (we'll be using the Hillis and Steele algorithm for this), and finally we will
    update the data structures and write out the values in a sorted order. Let's go
    into detail about how this works.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 循环中的三个调用是这个实现的“工作马”，它们调用内核根据当前我们正在查看的字节从输入中计算直方图。该算法基本上会计算它检查过的键的直方图；下一阶段是计算前缀和（我们将使用Hillis和Steele算法来完成这项工作），最后我们将更新数据结构并以排序顺序写出值。让我们详细了解一下它是如何工作的。
- en: 'In the host code, you will need to prepare the data structures slightly differently
    than what we have shown you so far, because these structures need to be shared
    across various kernels while we swing between host code and kernel code. The following
    diagram illustrates this general idea for `runKernels()`, and this situation is
    because we created a single command queue which all kernels will latch on to in
    program order; this applies to their execution as well:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在主机代码中，你需要以与我们之前展示的不同方式准备数据结构，因为这些结构需要在主机代码和内核代码之间切换时共享。以下图示说明了`runKernels()`的一般概念，这种情况是因为我们创建了一个单个命令队列，所有内核都将按照程序顺序附加到它；这也适用于它们的执行：
- en: '![How it works…](img/4520OT_10_11.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/4520OT_10_11.jpg)'
- en: For this implementation, the data structure that holds the unsorted data (that
    is, `unsortedData_d`) needs to be read and shared across the kernels. Therefore,
    you need to create the device buffer with the flag `CL_MEM_USE_HOST_PTR` since
    the OpenCL specification guarantees that the implementations cached it across
    multiple kernel invocations. Next, we will look at how the histogram is computed
    on the GPU.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实现，需要读取并共享存储未排序数据的数据结构（即`unsortedData_d`）。因此，你需要使用带有标志`CL_MEM_USE_HOST_PTR`的设备缓冲区，因为OpenCL规范保证它在多个内核调用之间缓存了它。接下来，我们将看看如何在GPU上计算直方图。
- en: 'The computation of the histogram is based on the threaded histogram we introduced
    in a previous chapter, but this time around, we decided to show you another implementation
    which is based on using atomic functions in OpenCL, and in particular using `atomic_inc()`.
    The `atomic_inc` function will update the value pointed by the location by one.
    The histogram works on the OpenCL-supported GPU because we have chosen to use
    the shared memory and CPU doesn''t support that yet. The strategy is to divide
    our input array into blocks of *N x R* elements where *R* is the radix (in our
    case *R = 8* since each digit is 8-bits wide and *2⁸=256*) and *N* is the number
    of threads executing the block. This strategy is based on the assumption that
    our problem sizes are always going to be much larger than the amount of threads
    available, and we configure it programmatically on the host code prior to launching
    the kernel as shown in the following code:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图的计算基于我们在前一章中引入的线程化直方图，但这次我们决定向您展示另一种实现，该实现基于在OpenCL中使用原子函数，特别是使用`atomic_inc()`。`atomic_inc`函数将更新指向该位置的值加一。由于我们选择了使用共享内存，而CPU目前还不支持这一点，因此直方图在OpenCL支持的GPU上工作。策略是将我们的输入数组划分为*N
    x R*元素的块，其中*R*是基数（在我们的情况下*R = 8*，因为每个数字是8位宽，且*2⁸=256*），而*N*是执行该块的线程数。这种策略基于我们的问题大小总是会比可用的线程数大得多的假设，我们在启动内核之前在主机代码中程序化地配置它，如下面的代码所示：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: By setting up the OpenCL thread block to be equal to `BIN_SIZE`, that is, 256,
    the kernel waits for the computation to complete by polling the OpenCL device
    for its execution status; this poll-release mechanism is encapsulated by `waitAndReleaseDevice()`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将OpenCL线程块设置为等于`BIN_SIZE`，即256，内核通过轮询OpenCL设备以获取其执行状态来等待计算完成；这种轮询-释放机制由`waitAndReleaseDevice()`封装。
- en: Note
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: When you have multiple kernel invocations and one kernel waits on the other,
    you need synchronization, and OpenCL provides this via `clGetEventInfo` and `clReleaseEvent`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有多个内核调用，其中一个内核等待另一个内核时，你需要同步，OpenCL通过`clGetEventInfo`和`clReleaseEvent`提供这种同步。
- en: 'In the histogram kernel, we built up the histogram by reading the inputs into
    shared memory (after initializing it to zero), and to prevent any threads from
    executing kernel code that reads from shared memory before all data is loaded
    into it, we placed a memory barrier as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在直方图内核中，我们通过将输入读取到共享内存中（在将其初始化为零之后）来构建直方图，为了防止任何线程在所有数据加载到共享内存之前执行从共享内存中读取的内核代码，我们放置了一个内存屏障，如下所示：
- en: '[PRE21]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: It's debatable whether we should initialize the shared memory, but it's best
    practice to initialize data structures, just like you would do in other programming
    languages. The trade off, in this case, is program correctness versus wasting
    processor cycles.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否应该初始化共享内存是有争议的，但最佳实践是初始化数据结构，就像在其他编程语言中做的那样。在这种情况下，权衡的是程序正确性与浪费处理器周期。
- en: 'Next, we shift the data value (residing in shared memory) by a number, `shiftBy`,
    which is the key we are sorting, extract the byte, and then update the local histogram
    atomically. We placed a memory barrier thereafter. Finally, we write out the binned
    values to their appropriate location in the global histogram, and you will notice
    that this implementation performs what we call *scattered writes*:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据值（位于共享内存中）通过一个数字`shiftBy`进行位移，这个数字是我们排序的关键，提取字节，然后原子性地更新局部直方图。之后，我们放置了一个内存屏障。最后，我们将分箱值写入全局直方图中的适当位置，你会注意到这种实现执行了我们所说的*分散写入*：
- en: '[PRE22]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Once the histogram is established, the next task that `runKernels()` performs
    is to execute the computations of prefix sums in the kernels `blockScan`, `blockPrefixSum`,
    `blockAdd`, `unifiedBlockScan`, and `mergePrefixSums` in turn. We'll explain what
    each kernel does in the following sections.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦建立了直方图，`runKernels()`函数接下来的任务就是依次执行内核`blockScan`、`blockPrefixSum`、`blockAdd`、`unifiedBlockScan`和`mergePrefixSums`中的前缀和计算。我们将在接下来的章节中解释每个内核的功能。
- en: 'The general strategy for this phase (encapsulated in `computeBlockScans()`)
    is to pre-scan the histogram bins so that we generate the prefix sums for each
    bin. We then write out that value to an auxiliary data structure, `sum_in_d`,
    and write out all intermediary sums into another auxiliary data structure, `scannedHistogram_d`.
    The following is the configuration we sent to the `blockScan` kernel:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此阶段的通用策略（封装在`computeBlockScans()`中）是在预扫描直方图桶，以便为每个桶生成累加和。然后我们将该值写入辅助数据结构`sum_in_d`，并将所有中间和写入另一个辅助数据结构`scannedHistogram_d`。以下是我们发送给`blockScan`内核的配置：
- en: '[PRE23]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The general strategy behind scanning is illustrated in the following diagram,
    where the input is divided into separate blocks and each block will be submitted
    for a block scan. The generated results are prefix sums, but we need to collate
    these results across all blocks to obtain a cohesive view. After which, the histogram
    bins are updated with these prefix sum values, and then finally we can use the
    updated histogram bins to sort the input array.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描背后的通用策略在以下图中得到说明，其中输入被分成单独的块，每个块将提交进行块扫描。生成的结果是累加和，但我们需要整理所有块的结果以获得一个连贯的视图。之后，使用这些累加和值更新直方图桶，然后最终我们可以使用更新的直方图桶对输入数组进行排序。
- en: '![How it works…](img/4520OT_10_12.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作…](img/4520OT_10_12.jpg)'
- en: 'Let''s look at how the block scan is done by examining `blockScan`. First,
    we load the values from the previously computed histogram bin into its shared
    memory as shown in the following code:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过检查`blockScan`来看看块扫描是如何进行的。首先，我们将之前计算出的直方图桶的值加载到其共享内存中，如下面的代码所示：
- en: '[PRE24]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we perform the Hillis and Steele prefix sum algorithm locally, and build
    the summed values for the current block:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在本地执行Hillis和Steele累加和算法，并为当前块构建累加值：
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, we write out a prefix sum for this block to `sum_in_d`, represented
    in the following code by `sumBuffer`, and the intermediary prefix sums to the
    `scannedHistogram_d` object, represented here by `output`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将此块的累加和写入`sum_in_d`，在以下代码中由`sumBuffer`表示，并将中间累加和写入`scannedHistogram_d`对象，在此处由`output`表示：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following diagram illustrates this concept for two parallel block scans
    (assuming we have a shared memory that holds eight elements) and shows how it''s
    stored into the output:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图说明了两个并行块扫描的概念（假设我们有一个包含八个元素的共享内存）以及它是如何存储到输出中的：
- en: '![How it works…](img/4520OT_10_13.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作…](img/4520OT_10_13.jpg)'
- en: 'At this phase of the computation, we have managed to compute the prefix sums
    for all the individual blocks. We need to collate them through the next phase,
    which is in the kernel `blockPrefixSum` where the individual block''s summed value
    is accumulated by each work item. The work done by each thread will compute the
    sum across different blocks. Depending on the thread with ID, `i`, will gather
    all sums from block number `0` to `(i – 1)`. The following code in `blockPrefixSum`
    illustrates this process:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个计算阶段，我们已经成功计算了所有单个块的累加和。我们需要在下一个阶段整理它们，该阶段在`blockPrefixSum`内核中，每个工作项将累加单个块的累加值。每个线程的工作将计算不同块的总和。根据线程ID，`i`将收集从块号`0`到`(i
    – 1)`的所有总和。以下`blockPrefixSum`中的代码说明了这个过程：
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The astute reader will notice that we have left out the prefix sum for one
    block, and the following remedies are obtained by computing the final accumulated
    prefix sums for this block:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 聪明的读者会注意到我们遗漏了一个块的累加和，以下是通过计算此块的最终累积累加和来获得的补救措施：
- en: '[PRE28]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The following diagram best represents what computation goes on in the previous
    kernel code. It assumes that we have a block scan for 16 elements that has been
    completed in `blockScanKernel`, and each element contains the prefixed sum. To
    collate these sums, we configure our kernel to run eight threads with a striding
    factor of `8` (assuming a block size of eight), and the diagram expresses what
    each of the eight threads are working on. The threads collate the sums by working
    out the summation of the entire input, progressively computing ![How it works…](img/4520OT_10_28_1.jpg)
    and writing them out to `sum_out_d` and `summary_in_d`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图最好地表示了上一个内核代码中的计算过程。它假设我们有一个16个元素的块扫描，已经在`blockScanKernel`中完成，并且每个元素都包含前缀和。为了整理这些和，我们配置内核运行八个线程，步进因子为`8`（假设块大小为八个），图表达了八个线程各自在做什么。线程通过计算整个输入的求和，逐步计算![如何工作…](img/4520OT_10_28_1.jpg)并将它们写入`sum_out_d`和`summary_in_d`来整理这些和。
- en: 'The following is a diagram that illustrates the process where given an input,
    all elements of that input are the summed values of the block scan for all blocks;
    the algorithm basically sums everything and writes to the output array:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个图示，说明了给定输入时，该输入的所有元素都是所有块块扫描的求和值；算法基本上将所有内容相加并写入输出数组：
- en: '![How it works…](img/4520OT_10_14.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作…](img/4520OT_10_14.jpg)'
- en: 'At this point, we have to collate the intermediary prefix sums computed, that
    is, ![How it works…](img/4520OT_10_29.jpg) inside `sum_out_d`, and with that from
    `scannedHistogram_d`. We basically add the two intermediary sums together using
    `blockAddKernel`. The following is how we prepare the kernel prior to launch:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们必须汇总计算出的中间前缀和，即在`sum_out_d`中的![如何工作…](img/4520OT_10_29.jpg)，以及从`scannedHistogram_d`中的值。我们基本上使用`blockAddKernel`将这两个中间求和值相加。以下是我们如何准备在启动内核之前：
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We then basically collate them back to `scannedHistogram_d` with `blockAddKernel`
    whose code is shown as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们基本上使用`blockAddKernel`将这些值汇总回`scannedHistogram_d`，其代码如下所示：
- en: '[PRE30]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Finally, we perform another prefix sum to collate the values in `summary_in_d`,
    as all elements inside that array contains each individual block's prefix sum.
    Because our chosen Radix value is `256`, we need to work out the prefix sums computation
    for blocks `0` to `y` using ![How it works…](img/4520OT_10_30.jpg)through to ![How
    it works…](img/4520OT_10_31.jpg). This is illustrated in the following diagram,
    and it is encapsulated in the `unifiedBlockScan` kernel. We won't show the kernel
    code as it's similar to the `blockPrefixSum` kernel.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们执行另一个前缀和操作，以汇总`summary_in_d`中的值，因为该数组中的所有元素都包含每个单独块的求和值。由于我们选择的基数值是`256`，我们需要计算出从块`0`到`y`的前缀和计算，通过![如何工作…](img/4520OT_10_30.jpg)到![如何工作…](img/4520OT_10_31.jpg)。这在一个以下图中展示，并且封装在`unifiedBlockScan`内核中。我们不会展示内核代码，因为它与`blockPrefixSum`内核类似。
- en: '![How it works…](img/4520OT_10_15.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作…](img/4520OT_10_15.jpg)'
- en: 'At this point in time, we are left with writing the collated prefix sums we
    have just performed previously into `scannedHistogram_d`. This collation exercise
    is different from the previous one where we gather the intermediary prefix sums
    across the blocks, but nonetheless, it''s still a collation exercise, and we need
    to push in the values from `summary_in_d`. We accomplished this with `mergePrefixSumsKernel`
    with the inputs reflected in the following host code:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在此时刻，我们剩下将之前执行的前缀和写入`scannedHistogram_d`。这种汇总练习与之前的汇总不同，我们是在块之间收集中间前缀和，但尽管如此，它仍然是一个汇总练习，我们需要将`summary_in_d`中的值推入。我们通过`mergePrefixSumsKernel`完成了这项工作，以下是其主机代码的反映：
- en: '[PRE31]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `mergePrefixSumsKernel` exercise is a relatively simple exercise to shift
    the values to their proper positions with the following kernel code:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`mergePrefixSumsKernel`练习是一个相对简单的练习，使用以下内核代码将值移到它们正确的位置：'
- en: '[PRE32]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'With this, the prefix sums are properly computed. The next phase of the algorithm
    will be to rank and permute the keys using each work item / thread to permute
    its 256 elements via the prescanned histogram bins, encapsulated in `computeRankNPermutations()`.
    The following is the host code for the kernel launch:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，前缀和被正确计算。算法的下一阶段将是使用每个工作项/线程对键进行排序和排列，每个工作项/线程通过预先扫描的直方图桶排列其256个元素，封装在`computeRankNPermutations()`中。以下是为内核启动的主机代码：
- en: '[PRE33]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Once the kernel has completed successfully, the data values will be in a sorted
    order and will be held in the device memory by `sortedData_d`. We need to copy
    those data into `unsortedData_d` again, and we will continue to do this until
    we have not completed the iteration of the keys.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦内核成功完成，数据值将是有序的，并将由`sortedData_d`保存在设备内存中。我们需要将这些数据再次复制到`unsortedData_d`中，我们将继续这样做，直到我们完成键的迭代。
- en: 'In the `rankNPermute` kernel, we will again make use of shared memory. The
    data into shared memory, and the data is organized as `GROUP_SIZE * RADIX` where
    the `GROUP_SIZE = 64` and `RADIX = 256` expressions hold true, and because each
    work group is configured to execute with 64 threads, we basically have one thread
    hydrating 256 elements of its shared memory (which the following code snippet
    demonstrates):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在`rankNPermute`内核中，我们再次利用共享内存。将数据放入共享内存中，数据组织为`GROUP_SIZE * RADIX`，其中`GROUP_SIZE
    = 64`和`RADIX = 256`表达式成立，并且由于每个工作组被配置为使用64个线程执行，我们基本上有一个线程填充其共享内存中的256个元素（以下代码片段演示了这一点）：
- en: '[PRE34]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, it ranks the elements based on the same idea as in the sequential algorithm,
    and you should refer back to that now. The difference is that we are pulling data
    values from `unsortedData` in global device memory, processing them in device
    memory, figuring out where the values should be, and writing them out to `sortedData`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，它根据与顺序算法中相同的思想对元素进行排序，你现在应该回顾一下那个算法。不同之处在于，我们从全局设备内存中的`unsortedData`中提取数据值，在设备内存中处理它们，确定值应该放在哪里，并将它们写入到`sortedData`中：
- en: '[PRE35]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: After the ranking and permutation is done, the data values in the `sortedData_d`
    object are sorted based on the current examined key. The algorithm will copy the
    data in `sortedData_d` into `unsortedData_d` so that the entire process can be
    repeated for a total of four times.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 排序和排列完成后，`sortedData_d`对象中的数据值将根据当前检查的键进行排序。算法会将`sortedData_d`中的数据复制到`unsortedData_d`中，以便整个过程可以重复进行，总共四次。
