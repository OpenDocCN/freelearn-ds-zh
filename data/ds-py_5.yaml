- en: '*Chapter 6*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*'
- en: Decoding Images
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码图像
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Create models that can classify images into different categories
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建能够将图像分类为不同类别的模型
- en: Use the Keras library to train neural network models for images
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras库训练图像的神经网络模型
- en: Utilize concepts of image augmentation in different business scenarios
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同的商业场景中利用图像增强的概念
- en: Extract meaningful information from images
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图像中提取有意义的信息
- en: This chapter will cover various concepts on how to read and process images.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖如何读取和处理图像的各种概念。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: So far, we have only been working with numbers and text. In this chapter, we
    will learn how to use machine learning to decode images and extract meaningful
    information, such as the type of object present in an image, or the number written
    in an image. Have you ever stopped to think about how our brains interpret the
    images they receive from our eyes? After millions of years of evolution, our brains
    have become highly efficient and accurate at recognizing objects and patterns
    from the images they get from our eyes. We have been able to replicate the function
    of our eyes using cameras, but making computers recognize patterns and objects
    in images is a really tough job. The field associated with understanding what
    is present in images is known as computer vision. The field of computer vision
    has witnessed tremendous research and advancements in the past few years. The
    introduction of Convoluted Neural Networks (CNNs) and the ability to train neural
    networks on GPUs were the biggest of these breakthroughs. Today, CNNs are used
    anywhere we have a computer vision problem, for example, self-driving cars, facial
    recognition, object detection, object tracking, and creating fully autonomous
    robots. In this chapter, we will learn how these CNNs work and how big an improvement
    they are compared to traditional methods.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只处理过数字和文本。在本章中，我们将学习如何使用机器学习解码图像并提取有意义的信息，比如图像中存在的物体类型或图像中写的数字。你有没有停下来思考过我们的大脑是如何解读它们从眼睛接收到的图像的？经过数百万年的进化，我们的大脑已经变得非常高效和准确，能够从眼睛接收到的图像中识别物体和模式。我们已经能够通过相机复制眼睛的功能，但让计算机识别图像中的模式和物体却是一项非常艰巨的任务。与理解图像中存在的内容相关的领域被称为计算机视觉。计算机视觉领域在过去几年里经历了巨大的研究和进展。卷积神经网络（CNN）和在GPU上训练神经网络的能力是其中最重大的突破之一。如今，CNN被广泛应用于任何计算机视觉问题中，例如自动驾驶汽车、人脸识别、物体检测、物体追踪以及创建完全自主的机器人。在本章中，我们将学习这些CNN是如何工作的，并了解它们相比传统方法有哪些显著的改进。
- en: Images
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像
- en: The digital cameras that we have today store images as a big matrix of numbers.
    These are what we call digital images. A single number on this matrix refers to
    a single pixel in the image. Individual numbers refer to the intensity of the
    color at that pixel. For a grayscale image, these values vary from 0 to 255, where
    0 is black and 255 is white. For a colored image, this matrix is three-dimensional,
    where each dimension has values for red, green, and blue. The values in the matrices
    refer to the intensities of the respective colors. We use these values as input
    to our computer vision programs or data science models to perform predictions
    and recognitions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天使用的数码相机将图像存储为一个巨大的数字矩阵，这些就是我们所说的数字图像。矩阵中的每个数字代表图像中的一个像素。每个数字代表该像素的颜色强度。对于灰度图像，这些值的范围是0到255，其中0是黑色，255是白色。对于彩色图像，这个矩阵是三维的，每个维度对应红色、绿色和蓝色的值。矩阵中的值代表各自颜色的强度。我们将这些值作为输入，用于我们的计算机视觉程序或数据科学模型，以进行预测和识别。
- en: 'Now, there are two ways for us to create machine learning models using these
    pixels:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有两种方法可以让我们使用这些像素来创建机器学习模型：
- en: Input individual pixels as different input variables to the neural network
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将单独的像素作为不同的输入变量输入神经网络
- en: Use a convolutional neural network
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积神经网络
- en: Creating a fully connected neural network that takes individual pixel values
    as input variables is the easiest and the most intuitive way for us right now,
    so we will start by creating this model. In the next section, we will learn about
    CNNs and see how much better they are at dealing with images.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个将单个像素值作为输入变量的全连接神经网络是目前最简单、最直观的方法，因此我们将从创建这个模型开始。在下一节中，我们将学习CNN，并了解它们在处理图像时的优势。
- en: 'Exercise 50: Classify MNIST Using a Fully Connected Neural Network'
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习50：使用全连接神经网络分类MNIST
- en: In this exercise, we will perform classification on the **Modified National
    Institute of Standards and Technology database** (**MNIST**) dataset. MNIST is
    a dataset of handwritten digits that have been normalized to fit into a 28 x 28
    pixel bounding box. There are 60,000 training images and 10,000 testing images
    in this dataset. In case of the fully connected network, we feed the individual
    pixels as features to the network, and then train it as a normal neural network,
    much like the first neural network we trained in *Chapter 5*, *Mastering Structured
    Data*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将对 **修改后的国家标准与技术研究所数据库**（**MNIST**）数据集进行分类。MNIST 是一个手写数字数据集，已被规范化以适应
    28 x 28 像素的边界框。该数据集包含 60,000 张训练图像和 10,000 张测试图像。在完全连接的网络中，我们将单个像素作为特征输入到网络中，然后像训练第一个神经网络一样训练它，就像在
    *第 5 章* *掌握结构化数据* 中训练的第一个神经网络一样。
- en: 'To complete this exercise, complete the following steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此练习，请执行以下步骤：
- en: 'Load the required libraries, as illustrated here:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所需的库，如下所示：
- en: '[PRE0]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the MNIST dataset using the Keras library:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Keras 库加载 MNIST 数据集：
- en: '[PRE1]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'From the shape of the dataset, you can figure out that the data is available
    in 2D format. The first element is the number of images available, whereas the
    next two elements are the width and height of the images:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据集的形状，你可以推测数据是以 2D 格式呈现的。第一个元素是可用图像的数量，接下来的两个元素是图像的宽度和高度：
- en: '[PRE2]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows:'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.1: Width and height of images'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.1：图像的宽度和高度'
- en: '](img/C13322_06_01.jpg)'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_06_01.jpg)'
- en: 'Figure 6.1: Width and height of images'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.1：图像的宽度和高度
- en: 'Plot the first image to see what kind of data you are dealing with:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制第一张图像，查看你正在处理的数据类型：
- en: '[PRE3]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Figure 6.2: Sample image of the MNIST dataset'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.2：MNIST 数据集的样本图像'
- en: '](img/C13322_06_02.jpg)'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_06_02.jpg)'
- en: 'Figure 6.2: Sample image of the MNIST dataset'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.2：MNIST 数据集的样本图像
- en: 'Convert the 2D data into 1D data so that our neural network can take it as
    input (28 x 28 pixels = 784):'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 2D 数据转换为 1D 数据，以便我们的神经网络可以将其作为输入（28 x 28 像素 = 784）：
- en: '[PRE4]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Convert the target variable to a one-hot vector so that our network does not
    form unnecessary connections between the different target variables:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目标变量转换为 one-hot 向量，这样我们的网络就不会在不同的目标变量之间形成不必要的连接：
- en: '[PRE5]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create the model. Make a small two-layer network; you can experiment with other
    architectures. You will learn more about cross-entropy loss in the following section:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建模型。建立一个小型的两层网络；你可以尝试其他架构。接下来你将学习更多关于交叉熵损失的内容：
- en: '[PRE6]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Figure 6.3: Model architecture of the dense network'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.3：稠密网络的模型架构'
- en: '](img/C13322_06_03.jpg)'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_06_03.jpg)'
- en: 'Figure 6.3: Model architecture of the dense network'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.3：稠密网络的模型架构
- en: 'Train the model and check the final accuracy:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型并检查最终准确率：
- en: '[PRE7]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.4: Model accuracy'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4：模型准确率'
- en: '](img/C13322_06_04.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_04.jpg)'
- en: 'Figure 6.4: Model accuracy'
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.4：模型准确率
- en: 'Congratulations! You have now created a model that can predict the number on
    an image with 93.57% accuracy. You can plot different test images and see your
    network''s result using the following code. Change the value of the image variable
    to get different images:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你现在已经创建了一个能够以 93.57% 的准确率预测图像上的数字的模型。你可以使用以下代码绘制不同的测试图像，并查看网络的结果。更改图像变量的值来获取不同的图像：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Figure 6.5: An MNIST image with prediction from dense network'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5：带有稠密网络预测的 MNIST 图像'
- en: '](img/C13322_06_05.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_05.jpg)'
- en: 'Figure 6.5: An MNIST image with prediction from dense network'
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.5：带有稠密网络预测的 MNIST 图像
- en: 'You can visualize only the incorrect predictions to understand where your model
    fails:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以仅可视化错误的预测，以了解你的模型在哪些地方失败：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Figure 6.6: Incorrectly classified example from the dense network'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.6：稠密网络错误分类的示例'
- en: '](img/C13322_06_06.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_06.jpg)'
- en: 'Figure 6.6: Incorrectly classified example from the dense network'
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.6：稠密网络错误分类的示例
- en: As you can see in the previous screenshot, the model failed because we predicted
    the class to be 2 whereas the correct class was 3.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在之前的截图中所见，模型失败了，因为我们预测的类别是 2，而正确的类别是 3。
- en: Convolutional Neural Networks
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '**Convolutional Neural Network** (**CNN**) is the name given to a neural network
    that has convolutional layers. These convolutional layers handle the high dimensionality
    of raw images efficiently with the help of convolutional filters. CNNs allow us
    to recognize highly complex patterns in images, which would be impossible with
    a simple neural network. CNNs can also be used for natural language processing.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNN**）是指具有卷积层的神经网络。这些卷积层借助卷积滤波器高效地处理原始图像的高维度。CNN使我们能够识别图像中的复杂模式，这是简单神经网络无法做到的。CNN还可以用于自然语言处理。'
- en: The first few layers of a CNN are convolutional, where the network applies different
    filters to the image to find useful patterns in the image; then there's the pooling
    layers, which help down-sample the output of the convolutional layers. The activation
    layer controls which signal flows from one layer to the next, emulating the neurons
    in our brain. The last few layers in the network are dense layers; these are the
    same layers we used for the previous exercise.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的前几层是卷积层，网络在这些层中应用不同的滤波器来寻找图像中的有用模式；接着是池化层，它们有助于下采样卷积层的输出。激活层控制信号从一层流向下一层，模拟我们大脑中的神经元。网络中的最后几层是全连接层；这些层与我们在之前练习中使用的层相同。
- en: Convolutional Layer
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积层
- en: The convolutional layer consists of multiple filters that learn to activate
    when they see a certain feature, edge, or color in the initial layers, and eventually
    faces, honeycombs, and wheels. These filters are exactly like the Instagram filters
    that we are all so used to. Filters change the appearance of the image by altering
    the pixels in a certain manner. Let's take a filter that detects horizontal edges
    as an example.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层由多个滤波器组成，它们在看到初始层中的特征、边缘或颜色时会激活，最终能够识别出面孔、蜂窝图案和车轮等。这些滤波器就像我们常用的Instagram滤镜一样。滤镜通过以某种方式改变像素来改变图像的外观。以一个检测水平边缘的滤波器为例。
- en: '![Figure 6.7: Horizontal edge detection filter'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.7：水平边缘检测滤波器'
- en: '](img/C13322_06_07.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_07.jpg)'
- en: 'Figure 6.7: Horizontal edge detection filter'
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.7：水平边缘检测滤波器
- en: As you can see in the preceding screenshot, the filter transforms the image
    into another image that has the horizontal line highlighted. To get the transformation,
    we multiply parts of the image by the filter one by one. First, we take the top-left
    3 x 3 cross section of the image and perform matrix multiplication with the filter
    to get the first top-left pixel of the transformation. Then we move the filter
    one pixel to the right and get the second pixel of the transformation, and so
    on. The transformation is a new image that has only the horizontal line section
    of the image highlighted. The values of the filter parameters, 9 in this case,
    are the weights or parameters that a convolutional layer learns while training.
    Some filters might learn to detect horizontal lines, some vertical lines, and
    some lines at a 45-degree angle. The subsequent layers learn more complex structures,
    such as the pattern of a wheel or a human face.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面截图所示，滤波器将图像转换成另一幅图像，其中水平线被突出显示。为了得到这种转换，我们将图像的部分区域与滤波器逐一相乘。首先，我们取图像的左上角3x3区域，并与滤波器进行矩阵乘法，得到转换后的第一个左上角像素。然后，我们将滤波器向右移动一个像素，得到转换后的第二个像素，依此类推。转换后的图像是一幅只突出显示水平线部分的图像。滤波器参数的值（此处为9）是卷积层在训练过程中学习的权重或参数。有些滤波器可能学会检测水平线，有些则是垂直线，或者是45度角的线。随后的层将学习到更复杂的结构，比如车轮或人脸的模式。
- en: 'Some hyperparameters of the convolutional layer are listed here:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出了一些卷积层的超参数：
- en: '**Filters**: This is the count of filters in each layer of the network. This
    number also reflects the dimension of the transformation, because each filter
    will result in one dimension of the output.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滤波器**：这是网络中每层的滤波器数量。这个数字也反映了转换的维度，因为每个滤波器将导致输出的一个维度。'
- en: '**Filter size**: This is the size of the convolutional filter that the network
    will learn. This hyperparameter will determine the size of the output transformation.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滤波器大小**：这是网络将学习的卷积滤波器的大小。这个超参数将决定输出转换的大小。'
- en: '**Stride**: In the preceding horizontal edge example, we moved the filter by
    one pixel every pass. This is the stride. It refers to how much the filter will
    move every pass. This hyperparameter also determines the size of the output transformation.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步长**：在前述水平边缘示例中，我们每次通过都将滤波器移动一个像素。这就是步长。它指的是滤波器每次通过时移动的量。这个超参数还决定了输出转换的大小。'
- en: '**Padding**: This hyperparameter makes the network pad the image with zeros
    on all the sides. This helps preserve edge information in some cases and helps
    us keep the input and output of the same size.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**填充**：这是一个超参数，使网络在图像的所有边缘填充零。在某些情况下，这有助于保留边缘信息，并确保输入和输出的大小相同。'
- en: Note
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: If you perform padding, then you get an image of the same or larger size as
    the output of the convolution operation. If you do not perform padding, then the
    image will decrease in size.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果进行填充，则得到的图像大小与卷积操作的输出相同或更大。如果不进行填充，则图像大小将会减小。
- en: Pooling Layer
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化层
- en: '**Pooling layers** reduce the size of the input image to reduce the amount
    of computation and parameters in the network. Pooling layers are inserted periodically
    between convolutional layers to control overfitting. The most common variant of
    pooling is 2 x 2 max pooling with a stride of 2\. This variant performs down-sampling
    of the input to keep only the maximum value of the four pixels in the output.
    The depth dimension remains unchanged.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**池化层** 将输入图像的大小减小，以减少网络中的计算量和参数。池化层周期性地插入到卷积层之间，以控制过拟合。最常见的池化变体是 2 x 2 最大池化，步长为
    2。此变体通过下采样输入，保留输出中四个像素的最大值。深度维度保持不变。'
- en: '![Figure 6.8: Max pooling operation'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.8：最大池化操作'
- en: '](img/C13322_06_08.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_08.jpg)'
- en: 'Figure 6.8: Max pooling operation'
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.8：最大池化操作
- en: in the past, we used to perform average pooling as well, but max pooling is
    used more often nowadays because it has proven to work better in practice. Many
    data scientists do not like using pooling layers, simply due to the information
    loss that accompanies the pooling operation. There has been some research on this
    topic, and it has been found that simple architectures without pooling layers
    outperform state-of-the-art models at times. To reduce the size of the input,
    it is suggested to use larger strides in the convolutional layer every once in
    a while.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，我们也进行过平均池化，但是现在更常见的是使用最大池化，因为在实践中已经证明其效果更好。许多数据科学家不喜欢使用池化层，仅仅是因为池化操作会伴随信息的丢失。关于这个主题已经有一些研究，发现在某些时候，简单的没有池化层的架构能够超越最先进的模型。为了减少输入的大小，建议偶尔在卷积层中使用更大的步长。
- en: Note
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The research paper *Striving for Simplicity: The All Convolutional Net* evaluates
    models with pooling layers to find that pooling layers do not always improve the
    performance of the network, mostly when enough data is available. For more information,
    read the *Striving for Simplicity: The All Convolutional Net* paper: https://arxiv.org/abs/1412.6806'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '研究论文 *Striving for Simplicity: The All Convolutional Net* 评估具有池化层的模型，发现在有足够数据可用时，池化层并不总是能够提高网络的性能。有关更多信息，请阅读
    *Striving for Simplicity: The All Convolutional Net* 论文：https://arxiv.org/abs/1412.6806'
- en: Adam Optimizer
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Adam 优化器
- en: Optimizers update weights with the help of loss functions. Selecting the wrong
    optimizer or the wrong hyperparameter for the optimizer can lead to a delay in
    finding the optimal solution for the problem.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器通过损失函数更新权重。选择错误的优化器或优化器的错误超参数可能会导致在找到问题的最优解时延迟。
- en: 'The name Adam is derived from adaptive moment estimation. Adam has been designed
    specifically for training deep neural networks. The use of Adam is widespread
    in the data science community due to its speed in getting close to the optimal
    solution. Thus, if you want fast convergence, use the **Adam optimizer**. Adam
    does not always lead to the optimal solution; in such cases, SGD with momentum
    helps achieve state-of-the-art results. The following would be the parameters:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 的名称源自自适应矩估计。Adam 是专门设计用于训练深度神经网络的优化器。由于其快速接近最优解的速度，Adam 在数据科学社区中被广泛使用。因此，如果您想要快速收敛，请使用
    **Adam 优化器**。但是，Adam 并不总是导致最优解；在这种情况下，带有动量的 SGD 有助于实现最先进的结果。以下是参数：
- en: '**Learning rate**: This is the step size for the optimizer. Larger values (0.2)
    result in faster initial learning, whereas smaller values (0.00001) slow the learning
    down during training.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：这是优化器的步长。较大的值（0.2）会导致更快的初始学习速度，而较小的值（0.00001）会在训练过程中减慢学习速度。'
- en: '**Beta 1**: This is the exponential decay rate for the mean estimates of the
    gradient.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Beta 1**：这是梯度均值估计的指数衰减率。'
- en: '**Beta 2**: This is the exponential decay rate for the uncentered variance
    estimates of the gradient.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Beta 2**：这是梯度的未中心化方差估计的指数衰减率。'
- en: '**Epsilon**: This is a very small number to prevent division by zero.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Epsilon**：这是一个非常小的数值，用于防止除零错误。'
- en: A good starting point for deep learning problems are learning rate = 0.001,
    beta 1 = 0.9, beta 2 = 0.999, and epsilon = 10-8.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习问题，一个好的起始点是学习率 = 0.001，Beta 1 = 0.9，Beta 2 = 0.999，Epsilon = 10^-8。
- en: Note
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information, read the Adam paper: https://arxiv.org/abs/1412.6980v8'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请阅读Adam论文：[https://arxiv.org/abs/1412.6980v8](https://arxiv.org/abs/1412.6980v8)
- en: Cross-entropy Loss
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: '**Cross-entropy loss** is used when we are working with a classification problem
    where the output of each class is a probability value between 0 and 1\. The loss
    here increases as the model deviates from the actual value; it follows a negative
    log graph. This helps when the model predicts probabilities that are far from
    the actual value. For example, if the probability of the true label is 0.05, we
    penalize the model with a huge loss. On the other hand, if the probability of
    the true label is 0.40, we penalize it with a smaller loss.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉熵损失**用于分类问题中，其中每个类别的输出是介于0和1之间的概率值。这里的损失随着模型偏离实际值而增加；它遵循一个负对数图形。当模型预测的概率远离实际值时，这种损失尤为有效。例如，如果真实标签的概率是0.05，我们会给模型一个很大的惩罚损失。另一方面，如果真实标签的概率是0.40，我们则给予它较小的惩罚损失。'
- en: '![Figure 6.9: Graph of log loss versus probability'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.9：对数损失与概率的关系图](img/C13322_06_09.jpg)'
- en: '](img/C13322_06_09.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_09.jpg)'
- en: 'Figure 6.9: Graph of log loss versus probability'
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.9：对数损失与概率的关系图
- en: 'The preceding graph shows that the loss increases exponentially as the predictions
    get further from the true label. The formula that the cross-entropy loss follows
    is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表显示，当预测值远离真实标签时，损失会呈指数增长。交叉熵损失遵循的公式如下：
- en: '![Figure 6.10: Cross entropy loss formula'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.10：交叉熵损失公式](img/C13322_06_10.jpg)'
- en: '](img/C13322_06_10.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_12.jpg)'
- en: 'Figure 6.10: Cross entropy loss formula'
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.10：交叉熵损失公式
- en: '*M* is number of classes in the dataset (10 in the case of MNIST), *y* is the
    true label, and *p* is the predicted probability of the class. We prefer cross-entropy
    loss for classification since the weight update becomes smaller as we get closer
    to the ground truth. Cross-entropy loss penalizes the probability of the correct
    class only.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*M*是数据集中的类别数（对于MNIST来说是10），*y*是真实标签，*p*是该类别的预测概率。我们偏好使用交叉熵损失来进行分类，因为随着我们接近真实值，权重更新会变得越来越小。交叉熵损失只会惩罚正确类别的概率。'
- en: 'Exercise 51: Classify MNIST Using a CNN'
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习51：使用CNN对MNIST进行分类
- en: 'In this exercise, we will perform classification on the **Modified National
    Institute of Standards and Technology (MNIST**) dataset using a CNN instead of
    the fully connected layers used in *Exercise 50*. We feed the network the complete
    image as input and get the number on the image as output:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用CNN对**修改后的国家标准与技术研究院（MNIST）**数据集进行分类，而不是像*练习50*中那样使用全连接层。我们将完整的图像作为输入，得到图像上的数字作为输出：
- en: 'Load the MNIST dataset using the Keras library:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Keras库加载MNIST数据集：
- en: '[PRE10]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Convert the 2D data into 3D data with the third dimension having only one layer,
    which is how Keras requires the input:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将二维数据转换为三维数据，第三维只有一层，这是Keras要求的输入格式：
- en: '[PRE11]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Convert the target variable to a one-hot vector so that our network does not
    form an unnecessary connection between the different target variables:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目标变量转换为一个独热编码向量，这样我们的网络就不会在不同的目标变量之间形成不必要的连接：
- en: '[PRE12]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create the model. Here, we make a small CNN. You can experiment with other
    architectures:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建模型。这里，我们构建了一个小型CNN。你可以尝试其他架构：
- en: '[PRE13]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Add the convolutional layers:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加卷积层：
- en: '[PRE14]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Add the pooling layer:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加池化层：
- en: '[PRE15]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Flatten the 2D matrices into 1D vectors:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将二维矩阵展平成一维向量：
- en: '[PRE16]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Use dense layers as the final layers for the model:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用全连接层作为模型的最后几层：
- en: '[PRE17]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To understand this fully, look at the output of the model in the following
    screenshot:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了完全理解这一点，请查看模型输出的以下截图：
- en: '![Figure 6.11: Model architecture of the CNN'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.11：CNN的模型架构](img/C13322_06_11.jpg)'
- en: '](img/C13322_06_11.jpg)'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_06_10.jpg)'
- en: 'Figure 6.11: Model architecture of the CNN'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.11：CNN的模型架构
- en: 'Train the model and check the final accuracy:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型并检查最终的准确度：
- en: '[PRE18]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.12: Final model accuracy'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.12：最终模型准确度](img/C13322_06_12.jpg)'
- en: '](img/C13322_06_12.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_12.jpg)'
- en: 'Figure 6.12: Final model accuracy'
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.12：最终模型准确率
- en: 'Congratulations! You have now created a model that can predict the number on
    an image with 98.62% accuracy. You can plot different test images and see your
    network''s result using the code given in *Exercise 50*. Also, plot the incorrect
    predictions to see where the model went wrong:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！现在你已经创建了一个能够以 98.62% 的准确率预测图像上数字的模型。你可以使用*练习 50*中提供的代码绘制不同的测试图像，并查看你的网络结果。还可以绘制错误预测，看看模型哪里出错：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Figure 6.13: Incorrect prediction of the model; the true label is 2'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.13：模型的错误预测；真实标签为 2'
- en: '](img/C13322_06_13.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_13.jpg)'
- en: 'Figure 6.13: Incorrect prediction of the model; the true label is 2'
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.13：模型的错误预测；真实标签为 2
- en: As you can see, the model is having difficulty predicting images that are ambiguous.
    You can play around with the layers and hyperparameters to see if you can get
    a better accuracy. Try substituting the pooling layers with convolutional layers
    with a higher stride, as suggested in the previous section.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，模型在预测模糊的图像时遇到了困难。你可以尝试调整层和超参数，看看是否能获得更好的准确率。尝试用更高步幅的卷积层替代池化层，正如前面一节中建议的那样。
- en: Regularization
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: '**Regularization** is a technique that helps machine learning models generalize
    better by making modifications in the learning algorithm. This helps prevent overfitting
    and helps our model work better on data that it hasn''t seen during training.
    In this section, we will learn about the different regularizers available to us.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化**是一种通过修改学习算法帮助机器学习模型更好地泛化的技术。它有助于防止过拟合，并使我们的模型在训练过程中未见过的数据上表现得更好。在本节中，我们将学习可用的不同正则化方法。'
- en: Dropout Layer
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 丢弃层
- en: '**Dropout** is a regularization technique that we use to prevent overfitting
    in our neural network models. We ignore randomly selected neurons from the network
    while training. This prevents the activations of those neurons continuing down
    the line, and the weight updates are not applied to them during back propagation.
    The weights of neurons are tuned to identify specific features; neurons that neighbor
    them become dependent on this, which can lead to overfitting because these neurons
    can get specialized to the training data. When neurons are randomly dropped, the
    neighboring neurons step in and learn the representation, leading to multiple
    different representations being learned by the network. This make the network
    generalize better and prevents the model from overfitting. One import thing to
    keep in mind is that dropout layers should not be used when you are performing
    predictions or testing your model. This would make the model lose valuable information
    and would lead to a loss in performance. Keras takes care of this by itself.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**丢弃（Dropout）**是一种我们用来防止神经网络模型过拟合的正则化技术。在训练过程中，我们随机忽略网络中的神经元。这样可以防止这些神经元的激活信号继续传播下去，且在反向传播时这些神经元的权重更新不会被应用。神经元的权重被调节来识别特定的特征，而与它们相邻的神经元则变得依赖于这些特征，这可能会导致过拟合，因为这些神经元可能会过于专门化于训练数据。当神经元被随机丢弃时，相邻的神经元会介入并学习这些表示，从而使网络学习到多种不同的表示。这使得网络能更好地进行泛化，并防止模型过拟合。一个需要注意的重要事项是，当你进行预测或测试模型时，不应使用丢弃层。这会使模型失去宝贵的信息，并导致性能下降。Keras
    会自动处理这个问题。'
- en: When using the dropout layer, it is recommended to create larger networks because
    it gives the model more opportunities to learn. We generally use a dropout probability
    between 0.2 and 0.5\. This probability refers to the probability by which a neuron
    will be dropped from training. A dropout layer after every layer is found to give
    good results, so you can start by placing dropout layers with a probability of
    0.2 after every layer and then fine-tune from there.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用丢弃层时，建议创建更大的网络，因为这能为模型提供更多学习的机会。我们通常使用 0.2 到 0.5 之间的丢弃概率。该概率指的是神经元在训练过程中被丢弃的概率。每层之后使用丢弃层通常能得到较好的效果，因此你可以从每层之后放置一个丢弃层，概率设置为
    0.2，然后从那里进行微调。
- en: 'To create a dropout layer in Keras with a probability of 0.5, you can use the
    following function:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Keras 中创建一个丢弃层（dropout layer），且其概率为 0.5，你可以使用以下函数：
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![Figure 6.14: Visualizing dropout in a dense neural network'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.14：在密集神经网络中可视化丢弃'
- en: '](img/C13322_06_14.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_14.jpg)'
- en: 'Figure 6.14: Visualizing dropout in a dense neural network'
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.14：在密集神经网络中可视化丢弃
- en: L1 and L2 Regularization
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L1 和 L2 正则化
- en: '**L2** is the most common type of regularization, followed by **L1**. These
    regularizers work by adding a term to the loss of the model to get the final cost
    function. This added term leads to a decrease in the weights of the model. This
    in turn leads to a model that generalizes well.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2** 是最常见的正则化类型，其次是 **L1**。这些正则化器通过向模型的损失中添加一个项来工作，以获得最终的代价函数。这一额外的项会导致模型的权重减少，从而使模型具有良好的泛化能力。'
- en: 'The cost function of L1 regularization looks like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化的代价函数如下所示：
- en: '![Figure 6.15: Cost function of L1 regularization'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.15：L1 正则化的代价函数](img/C13322_06_17.jpg)'
- en: '](img/C13322_06_15.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_15.jpg)'
- en: 'Figure 6.15: Cost function of L1 regularization'
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.15：L1 正则化的代价函数
- en: Here, λ is the regularization parameter. L1 regularization leads to weights
    that are very close to zero. This makes the neurons with L1 regularization become
    dependent only on the most important inputs and ignore the noisy inputs.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，λ 是正则化参数。L1 正则化会导致权重非常接近零。这使得应用 L1 正则化的神经元仅依赖于最重要的输入，并忽略噪声输入。
- en: 'The cost function of L2 regularization looks like this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化的代价函数如下所示：
- en: '![Figure 6.16: Cost function of L2 regularization'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.16：L2 正则化的代价函数'
- en: '](img/C13322_06_16.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_16.jpg)'
- en: 'Figure 6.16: Cost function of L2 regularization'
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.16：L2 正则化的代价函数
- en: 'L2 regularization heavily penalizes high-weight vectors and prefers weights
    that are diffused. L2 regularization is also known as weight decay because it
    forces the weights of a network to decay towards zero but, unlike L1 regularization,
    not exactly to zero. We can combine L1 and L2 and implement them together. To
    implement these regularizers, you can use the following functions in Keras:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化对高权重向量进行重罚，并偏好扩散的权重。L2 正则化也被称为 权重衰减，因为它迫使网络的权重衰减到接近零，但与 L1 正则化不同，L2 正则化并不会完全将权重压缩到零。我们可以将
    L1 和 L2 正则化结合使用。要实现这些正则化器，您可以在 Keras 中使用以下函数：
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Batch Normalization
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量归一化
- en: In *Chapter 1*, *Introduction to Data Science and Data Pre processing* we learned
    how to perform normalization and how it helped speed up the training of our machine
    learning models. Here, we will extend that same normalization to the individual
    layers of the neural network. **Batch normalization** allows layers to learn independently
    of other layers. It does this by normalizing the inputs to a layer to have a fixed
    mean and variance; this prevents the changes in parameters of previous layers
    from affecting the input of the layer too much. It also has a slight regularization
    effect; much like dropout, it prevents overfitting, but it does that by introducing
    noise into the values of the mini batches. When using batch normalization, make
    sure to use a lower dropout, which is better because dropout leads to a loss of
    information. However, do not remove dropout and rely completely on batch normalization,
    because a combination of the two has been seen to work better. While using batch
    normalization, a higher learning rate can be used because it makes sure that no
    action is too high or too low.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第一章*，*数据科学与数据预处理简介*中，我们学习了如何进行归一化，以及它如何帮助加速我们机器学习模型的训练。在这里，我们将对神经网络的每一层应用相同的归一化方法。**批量归一化**允许各层独立学习，而不受其他层的影响。它通过将层的输入标准化，使其具有固定的均值和方差来实现这一点；这可以防止前一层的参数变化对当前层的输入产生过大影响。它还有一定的正则化作用；类似于
    dropout，它防止过拟合，但它是通过在小批量的值中引入噪声来实现的。在使用批量归一化时，请确保使用较低的 dropout，这样更好，因为 dropout
    会导致信息丢失。然而，不要完全依赖批量归一化而去除 dropout，因为两者结合使用效果更好。使用批量归一化时，可以使用较高的学习率，因为它确保了不会有动作过大或过小。
- en: '![Figure 6.17: Batch normalization equation'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.17：批量归一化方程'
- en: '](img/C13322_06_17.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_16.jpg)'
- en: 'Figure 6.17: Batch normalization equation'
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.17：批量归一化方程
- en: Here, (xi) is the input to the layer and y is the normalized input. μ is the
    batch mean and σ2 is the batch's standard deviation. Batch normalization introduces
    two new (x_i ) ̂the loss.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，(xi) 是层的输入，y 是标准化后的输入。μ 是批量均值，σ2 是批量的标准差。批量归一化引入了两个新的（x_i）̂损失。
- en: 'To create a batch normalization layer in Keras, you can use the following function:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Keras 中创建批量归一化层，您可以使用以下函数：
- en: '[PRE22]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Exercise 52: Improving Image Classification Using Regularization Using CIFAR-10
    images'
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 52：使用正则化改进图像分类，使用 CIFAR-10 图像
- en: 'In this exercise, we will perform classification on the Canadian Institute
    for Advanced Research (CIFAR-10) dataset. It consists of 60,000 32 x 32 color
    images in 10 classes. The 10 different classes represent birds, airplanes, cats,
    cars, frogs, deer, dogs, trucks, ships, and horses. It is one of the most widely
    used datasets for machine learning research, mainly in the field of CNNs. Due
    to the low resolution of the images, models can be trained much quicker on these
    images. We will use this dataset to implement some of the regularization techniques
    we learned in the previous section:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将对加拿大高级研究院（CIFAR-10）数据集进行分类。该数据集包含 60,000 张 32 x 32 的彩色图像，分为 10 类。这
    10 类分别是：鸟类、飞机、猫、汽车、青蛙、鹿、狗、卡车、船和马。它是机器学习研究中最广泛使用的数据集之一，主要用于卷积神经网络（CNN）领域。由于图像的分辨率较低，模型可以在这些图像上更快速地训练。我们将使用该数据集实现我们在上一节中学到的一些正则化技术：
- en: Note
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: To get the raw CIFAR-10 files and CIFAR-100 dataset, visit https://www.cs.toronto.edu/~kriz/cifar.html.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 若要获取原始 CIFAR-10 文件和 CIFAR-100 数据集，请访问 https://www.cs.toronto.edu/~kriz/cifar.html。
- en: 'Load the CIFAR-10 dataset using the Keras library:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Keras 库加载 CIFAR-10 数据集：
- en: '[PRE23]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Check the dimensions of the data:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据的维度：
- en: '[PRE24]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.18: Dimensions of x'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.18：x 的维度'
- en: '](img/C13322_06_18.jpg)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_06_18.jpg)'
- en: 'Figure 6.18: Dimensions of x'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.18：x 的维度
- en: 'Similar dimensions, for `y`:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相似的维度，针对 `y`：
- en: '[PRE25]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows:'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.19: Dimensions of y'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.19：y 的维度'
- en: '](img/C13322_06_19.jpg)'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_06_19.jpg)'
- en: 'Figure 6.19: Dimensions of y'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.19：y 的维度
- en: As these are color images, they have three channels.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于这些是彩色图像，它们有三个通道。
- en: 'Convert the data to the format that Keras requires:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据转换为 Keras 所需的格式：
- en: '[PRE26]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Convert the target variable to a one-hot vector so that our network does not
    form unnecessary connections between the different target variables:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目标变量转换为 one-hot 向量，以确保网络在不同的目标变量之间不会形成不必要的连接：
- en: '[PRE27]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Create the model. Here, we make a small CNN without regularization first:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建模型。在这里，我们首先创建一个不带正则化的小型 CNN：
- en: '[PRE28]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Add the convolutional layers:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加卷积层：
- en: '[PRE29]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Add the pooling layer:'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加池化层：
- en: '[PRE30]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Flatten the 2D matrices into 1D vectors:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 2D 矩阵展平为 1D 向量：
- en: '[PRE31]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Use dense layers as the final layers for the model and compile the model:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用密集层作为模型的最终层并编译模型：
- en: '[PRE32]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Train the model and check the final accuracy:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型并检查最终准确度：
- en: '[PRE33]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now check the accuracy of the model:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在检查模型的准确度：
- en: '[PRE34]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output is as follows:'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.20: Accuracy of model'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.20：模型的准确度'
- en: '](img/C13322_06_20.jpg)'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_06_20.jpg)'
- en: 'Figure 6.20: Accuracy of model'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.20：模型的准确度
- en: 'Now create the same model, but with regularization. You can experiment with
    other architectures as well:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在创建相同的模型，但加入正则化。你也可以尝试其他架构：
- en: '[PRE35]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Add the convolutional layers:'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加卷积层：
- en: '[PRE36]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Add the pooling layer:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加池化层：
- en: '[PRE37]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Add the batch normalization layer along with a dropout layer:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加批量归一化层和 Dropout 层：
- en: '[PRE38]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Flatten the 2D matrices into 1D vectors:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 2D 矩阵展平为 1D 向量：
- en: '[PRE39]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Use dense layers as the final layers for the model and compile the model:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用密集层作为模型的最终层并编译模型：
- en: '[PRE40]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![Figure 6.21: Architecture of the CNN with regularization'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.21：带正则化的 CNN 架构'
- en: '](img/C13322_06_21.jpg)'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_06_21.jpg)'
- en: 'Figure 6.21: Architecture of the CNN with regularization'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.21：带正则化的 CNN 架构
- en: 'Train the model and check the final accuracy:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型并检查最终准确度：
- en: '[PRE41]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.22: Final accuracy output'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.22：最终准确度输出'
- en: '](img/C13322_06_22.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_22.jpg)'
- en: 'Figure 6.22: Final accuracy output'
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.22：最终准确度输出
- en: Congratulations! You made use of regularization to make your model work better
    than before. If you do not see an improvement in your model, train it for longer,
    so set it for more epochs. You will also see that you can train for a lot more
    epochs without worrying about overfitting.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你通过使用正则化使得模型比以前表现得更好。如果你的模型没有看到改善，尝试将训练时间延长，增加更多的训练轮数。你也会发现，可以训练更多的轮次而不必担心过拟合。
- en: 'You can plot different test images and see your network''s result using the
    code given in *Exercise 50*. Also, plot the incorrect predictions to see where
    the model went wrong:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以绘制不同的测试图像，并使用*练习 50* 中给出的代码查看网络的结果。同时，绘制错误预测，看看模型哪里出错：
- en: '[PRE42]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![Figure 6.23: Incorrect prediction of the model'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.23：模型的错误预测'
- en: '](img/C13322_06_23.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_23.jpg)'
- en: 'Figure 6.23: Incorrect prediction of the model'
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.23：模型的错误预测
- en: As you can see, the model is having difficulty in predicting images that are
    ambiguous. The true label is *horse*. You can play around with the layers and
    hyperparameters to see if you can get a better accuracy. Try creating more complex
    models with regularization and train them for longer.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，模型在预测模糊图像时遇到了困难。真实标签是*马*。你可以尝试调整层和超参数，看看是否能提高准确率。尝试创建更复杂的模型并进行正则化，训练更长时间。
- en: Image Data Preprocessing
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像数据预处理
- en: In this section, we go over a few techniques that you can use as a data scientist
    to preprocess images. First, we look at image normalization, and then we learn
    how we can convert a color image into a greyscale image. Finally, we look at ways
    in which we can bring all images in a dataset to the same dimensions. Preprocessing
    images is needed because datasets do not contain images that are the same size;
    we need to convert them into a standard size to train machine learning models
    on them. Some image preprocessing techniques help by reducing the model's training
    time by either making the important features easier to identify for the model
    or by reducing the dimensions as in the case of a greyscale image.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍数据科学家可以用来预处理图像的一些技术。首先，我们将介绍图像归一化，然后学习如何将彩色图像转换为灰度图像。最后，我们将探讨如何将数据集中的所有图像调整为相同尺寸。预处理图像是必要的，因为数据集中的图像大小不同，我们需要将它们转换为标准大小，以便在其上训练机器学习模型。一些图像预处理技术通过简化模型识别重要特征或通过减少维度（如灰度图像的情况）来帮助减少模型的训练时间。
- en: Normalization
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 归一化
- en: In the case of images, the scale of the pixels is of the same order and in the
    range 0 to 255\. Therefore, this normalization step is optional, but it might
    help speed up the learning process. To reiterate, centering the data and scaling
    it to the same order helps the network by ensuring that the gradients do not go
    out of control. A neural network shares parameters (neurons). If inputs are not
    scaled to the same order, it would make it difficult for the network to learn.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像而言，像素的规模在同一量级，范围是0到255。因此，这一步归一化是可选的，但它可能有助于加速学习过程。再重申一下，数据中心化并将其缩放到相同的量级，有助于确保梯度不会失控。神经网络共享参数（神经元）。如果输入数据没有缩放到相同的量级，那么网络学习将变得困难。
- en: Converting to Grayscale
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换为灰度图像
- en: 'Depending on the kind of dataset and problem you have, you can convert your
    images from RGB to greyscale. This helps the network work much more quickly because
    it has a lot fewer parameters to learn. Depending on the type of problem, you
    might not want to do this because it leads to a loss in information provided by
    the colors of the image. To convert an RGB image to a grayscale image, use the
    **Pillow** library:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据集和问题的不同，你可以将图像从RGB转换为灰度图像。这有助于网络更快地工作，因为它需要学习的参数要少得多。根据问题类型，你可能不希望这样做，因为这会导致丢失图像颜色所提供的信息。要将RGB图像转换为灰度图像，可以使用**Pillow**库：
- en: '[PRE43]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![Figure 6.24: Image of a car converted to grayscale'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.24：转换为灰度的汽车图像'
- en: '](img/C13322_06_24.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_24.jpg)'
- en: 'Figure 6.24: Image of a car converted to grayscale'
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.24：转换为灰度的汽车图像
- en: Getting All Images to the Same Size
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有图像调整为相同大小
- en: 'When working with real-life datasets, you will often come across a major challenge
    in that not all the images in your dataset will be the same size. You can perform
    one of the following steps depending on the situation to get around the issue:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理现实生活中的数据集时，你会经常遇到一个主要的挑战，那就是数据集中的所有图像大小可能不相同。你可以根据情况执行以下步骤来解决这个问题：
- en: '`resize` function is the algorithm that will be used to get new pixels of the
    resized image. The bicubic algorithm is fast and is one of the best pixel resampling
    algorithms for upsampling.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resize`函数是用于获取调整大小后新像素的算法。双三次插值算法速度较快，是上采样时最好的像素重采样算法之一。'
- en: '![Figure 7.25: Upsampled image of a car'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.25：上采样的汽车图像'
- en: '](img/C13322_06_25.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_25.jpg)'
- en: 'Figure 6.25: Upsampled image of a car'
  id: totrans-254
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.25：上采样的汽车图像
- en: '`resize` function is the algorithm that will be used to get the new pixels
    of the resized image, as mentioned previously. The antialiasing algorithm helps
    smoothen out the pixelated images. It works better than bicubic but is much slower.
    Antialiasing is one of the best pixel resampling algorithms for downsampling.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resize`函数是用于获取调整大小后新像素的算法，如前所述。抗锯齿算法有助于平滑像素化的图像。它比双三次插值算法效果更好，但速度较慢。抗锯齿是最适合下采样的像素重采样算法之一。'
- en: '![Figure 6.26: Down sampled image of a car'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.26：下采样的汽车图像'
- en: '](img/C13322_06_26.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_26.jpg)'
- en: 'Figure 6.26: Down sampled image of a car'
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.26：下采样的汽车图像
- en: '**Crop**: This is another method to make all images of the same size is to
    crop them. As mentioned before, you can use different centers to prevent the loss
    of information. You can use the following code to crop your images:'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**裁剪**：将所有图像裁剪为相同大小的另一种方法是裁剪它们。如前所述，可以使用不同的中心来防止信息丢失。你可以使用以下代码裁剪图像：'
- en: '[PRE44]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![Figure 6.27: Cropped image of a car'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.27：裁剪后的汽车图像'
- en: '](img/C13322_06_27.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_27.jpg)'
- en: 'Figure 6.27: Cropped image of a car'
  id: totrans-263
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.27：裁剪后的汽车图像
- en: '**Padding**: Padding adds a layer of zeros or ones around the image to increase
    the size of the image. To perform padding, use the following code:'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**填充**：填充是指在图像周围添加一层零或一的边界，以增加图像的大小。执行填充时，请使用以下代码：'
- en: '[PRE45]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![Figure 6.28: Padded image of a cropped car'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.28：裁剪汽车的填充图像'
- en: '](img/C13322_06_28.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_28.jpg)'
- en: 'Figure 6.28: Padded image of a cropped car'
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.28：裁剪后的填充汽车图像
- en: Other Useful Image Operations
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他有用的图像操作
- en: The **Pillow** library has many functions for modifying and creating new images.
    These will be helpful for creating new images from our existing training data.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pillow** 库提供了许多用于修改和创建新图像的功能。这些功能将帮助我们从现有的训练数据中创建新图像。'
- en: 'To flip an image, we can use the following code:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 要翻转图像，可以使用以下代码：
- en: '[PRE46]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![Figure 6.29: Flipped image of a cropped car'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.29：翻转后的裁剪汽车图像'
- en: '](img/C13322_06_29.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_29.jpg)'
- en: 'Figure 6.29: Flipped image of a cropped car'
  id: totrans-275
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.29：翻转后的裁剪汽车图像
- en: 'To rotate an image by 45 degrees, we can use the following code:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 要将图像旋转 45 度，可以使用以下代码：
- en: '[PRE47]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![Figure 6.30: The cropped car image rotated 45 degrees'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.30：旋转 45 度后的裁剪汽车图像'
- en: '](img/C13322_06_30.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_30.jpg)'
- en: 'Figure 6.30: The cropped car image rotated 45 degrees'
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.30：旋转 45 度后的裁剪汽车图像
- en: 'To shift an image by 1,000 pixels, we can use the following code:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 要将图像平移 1,000 像素，可以使用以下代码：
- en: '[PRE48]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![Figure 6.31: Rotated image of the cropped car'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.31：旋转后的裁剪汽车图像'
- en: '](img/C13322_06_31.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_31.jpg)'
- en: 'Figure 6.31: Rotated image of the cropped car'
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.31：旋转后的裁剪汽车图像
- en: 'Activity 17: Predict if an Image Is of a Cat or a Dog'
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 17：预测图像是猫还是狗
- en: 'In this activity, we will attempt to predict if the provided image is of a
    cat or a dog. The cats and dogs dataset (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter06)
    from Microsoft contains 25,000 color images of cats and dogs. Let''s look at the
    following scenario: You work at a veterinary clinic with two vets, one that specializes
    in dogs and one in cats. You want to automate the appointments of the doctors
    by figuring out if the next client is a dog or a cat. To do this, you create a
    CNN model:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将尝试预测提供的图像是猫还是狗。微软提供的猫狗数据集（https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter06）包含了
    25,000 张猫和狗的彩色图像。假设你在一家兽医诊所工作，诊所里有两位兽医，一位专门治疗狗，另一位专门治疗猫。你希望通过判断下一位客户是狗还是猫，来自动安排兽医的预约。为此，你创建了一个
    CNN 模型：
- en: Load the dog versus cat dataset and preprocess the images.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载狗与猫数据集并预处理图像。
- en: 'Use the image filenames to find the cat or dog label for each image. The first
    images should look like this:![Figure 6.32: First images of the dog and cat class'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用图像文件名找到每个图像的猫或狗标签。第一张图像应该是这样的：![图 6.32：狗与猫类别的第一张图像
- en: '](img/C13322_06_32.jpg)'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_06_32.jpg)'
- en: 'Figure 6.32: First images of the dog and cat class'
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.32：狗与猫类别的第一张图像
- en: Get the images in the correct shape to be trained.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取形状正确的图像以进行训练。
- en: Create a CNN that makes use of regularization.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个使用正则化的 CNN。
- en: Note
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 369.
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 369 页找到。
- en: 'You should find that the test set accuracy for this model is 70.4%. The training
    set accuracy is really high, around 96\. This means that the model has started
    to overfit. Improving the model to get the best possible accuracy is left for
    you as an exercise. You can plot the incorrectly predicted images using the code
    from previous exercises to get a sense of how well the model performs:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该发现该模型的测试集准确率为 70.4%。训练集准确率非常高，约为 96%。这意味着模型已经开始出现过拟合。改进模型以获得最佳准确率的任务留给你作为练习。你可以使用前面练习中的代码绘制错误预测的图像，从而了解模型的表现：
- en: '[PRE49]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![Figure 6.33: Incorrect prediction of a dog by the regularized CNN model'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.33：正则化的 CNN 模型错误预测的狗图像'
- en: '](img/C13322_06_33.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_33.jpg)'
- en: 'Figure 6.33: Incorrect prediction of a dog by the regularized CNN model'
  id: totrans-300
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.33：常规CNN模型错误地预测为狗
- en: Data Augmentation
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: While training machine learning models, we data scientists often run into the
    problem of imbalanced classes and a lack of training data. This leads to sub-par
    models that perform poorly when deployed in real-life scenarios. One easy way
    to deal with these problems is data augmentation. There are multiple ways of performing
    data augmentation, such as rotating the image, shifting the object, cropping an
    image, shearing to distort the image, and zooming in to a part of the image, as
    well as more complex methods such as using Generative Adversarial Networks (GANs)
    to generate new images. GANs are simply two neural networks that are competing
    with each other. A generator network tries to make images that are similar to
    the already existing images, while a discriminator network tries to determine
    if the image was generated or was part of the original data. After the training
    is complete, the generator network is able to create images that are not a part
    of the original data but are so similar that they can be mistaken for images that
    were actually captured by a camera.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练机器学习模型时，我们数据科学家经常遇到类别不平衡和训练数据不足的问题。这导致模型性能不佳，在实际应用中表现差强人意。应对这些问题的一种简单方法是数据增强。数据增强有多种方式，例如旋转图像、平移物体、裁剪图像、剪切扭曲图像、放大图像的某部分，以及更复杂的方法，如使用生成对抗网络（GANs）生成新图像。GAN只是两个相互竞争的神经网络。生成器网络试图生成与已有图像相似的图像，而判别器网络则尝试判断图像是生成的还是原始数据的一部分。训练完成后，生成器网络能够创造出并非原始数据的一部分，但与真实拍摄的图像相似，几乎可以误认为是摄像机拍摄的图像。
- en: Note
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can learn more about GANs in this paper: https://arxiv.org/abs/1406.2661.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这篇论文中了解更多关于GAN的信息：https://arxiv.org/abs/1406.2661。
- en: '![Figure 6.34: On the left is a fake image generated by a GAN, whereas the
    one on the right is an image of a real person'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.34：左侧是由GAN生成的假图像，而右侧是一个真实人物的图像'
- en: '](img/C13322_06_34.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_34.jpg)'
- en: 'Figure 6.34: On the left is a fake image generated by a GAN, whereas the one
    on the right is an image of a real person'
  id: totrans-307
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.34：左侧是一个由生成对抗网络（GAN）生成的假图像，而右侧是一个真实人物的图像
- en: Note
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Credits: http://www.whichfaceisreal.com'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：http://www.whichfaceisreal.com
- en: 'Coming back to the traditional methods of performing image augmentation, we
    perform the operations mentioned previously, such as flipping images, and then
    train our model on both the original and the transformed image. Let''s say we
    have the following flipped image of a cat on the left:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 回到传统的图像增强方法，我们执行之前提到的操作，如翻转图像，然后在原始图像和变换后的图像上训练我们的模型。假设我们有以下左侧的翻转猫图像：
- en: '![Figure 6.35: Normal picture of the cat on the right and flipped image on
    the left'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.35：右侧是猫的正常图像，左侧是翻转后的图像'
- en: '](img/C13322_06_35.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_35.jpg)'
- en: 'Figure 6.35: Normal picture of the cat on the right and flipped image on the
    left'
  id: totrans-313
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.35：右侧是猫的正常图像，左侧是翻转后的图像
- en: Now, a machine learning model trained on this left image would have a hard time
    recognizing the flipped image on the right as that of a cat because it is facing
    the other way. This is because the convolutional layers are trained to detect
    images of cats looking to the left only. It has created rules about the position
    of the different features of a body.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一个在左侧图像上训练的机器学习模型可能会很难将右侧翻转后的图像识别为猫的图像，因为它朝向相反。这是因为卷积层被训练成只检测朝左看的猫图像。它已经对身体的不同特征位置建立了规则。
- en: Thus, we train our model on all the augmented images. Data augmentation is the
    key to getting the best results from a CNN model. We make use of the `ImageDataGenerator`
    class in Keras to perform image augmentations easily. You will learn more about
    generators in the next section.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在所有增强后的图像上训练我们的模型。数据增强是获得CNN模型最佳结果的关键。我们利用Keras中的`ImageDataGenerator`类轻松执行图像增强。你将在下一节中了解更多关于生成器的内容。
- en: Generators
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成器
- en: In the previous chapter, we discussed how big datasets could lead to problems
    in training due to the limitations in RAM. This problem is a bigger issue when
    working with images. Keras has implemented generators that help us get batches
    of input images and their corresponding labels while training on the fly. These
    generators also help us perform data augmentation on images before using them
    for training. First, we will see how we can make use of the `ImageDataGenerator`
    class to generate augmented images for our model.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了大数据集如何由于 RAM 的限制而导致训练问题。当处理图像时，这个问题会更严重。Keras 实现了生成器，帮助我们在训练时动态获取输入图像及其相应标签。这些生成器还帮助我们在训练前对图像进行数据增强。首先，我们将看看如何利用`ImageDataGenerator`类为我们的模型生成增强后的图像。
- en: 'To implement data augmentation, we just need to change our *Exercise 3* code
    a little bit. We will substitute `model.fit()` with the following:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现数据增强，我们只需要稍微修改我们的*练习 3* 代码。我们将用以下代码替代`model.fit()`：
- en: '[PRE50]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let''s now look at what `ImageDataGenerator` is actually doing:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下`ImageDataGenerator`实际在做什么：
- en: '`rotation_range`: This parameter defines the maximum degrees by which the image
    can be rotated. This rotation is random and can be of any value less than the
    amount mentioned. This ensures that no two images are the same.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rotation_range`：此参数定义图像可以旋转的最大角度。旋转是随机的，可以小于该值的任何数值。这确保了没有两张图像是相同的。'
- en: '`width_shift_range`/`height_shift_range`: This value defines the amount by
    which the image can be shifted. If the value is less than 1, then the value is
    assumed to be a fraction of the total width. If it is more than 1, it is taken
    as pixel. The range will be in the interval (`-shift_range`, `+ shift_range`).'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width_shift_range`/`height_shift_range`：该值定义了图像可以移动的范围。如果值小于 1，则认为它是总宽度的一个比例；如果大于
    1，则表示像素数。范围将在（`-shift_range`，`+ shift_range`）区间内。'
- en: '`shear_range`: This is the shearing angle in degrees (counter-clockwise direction).'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shear_range`：这是剪切角度，单位为度（逆时针方向）。'
- en: '`zoom_range`: The value here can either be [`lower_range`, `upper_range`] or
    be a float, in which case the range would be [`1-zoom_range`, `1+zoom_range`].
    This is the range for the random zooming.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zoom_range`：这里的值可以是[`lower_range`, `upper_range`]，或者是浮动值，表示[`1-zoom_range`,
    `1+zoom_range`]，这是随机缩放的范围。'
- en: '`horizontal_flip` / `vertical_flip`: A true value here makes the generator
    randomly flip the image horizontally or vertically.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`horizontal_flip` / `vertical_flip`：此处的布尔值为真时，生成器会随机水平或垂直翻转图像。'
- en: '`fill_mode`: This helps us decide what to put in the whitespaces created by
    the rotation and searing process.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fill_mode`：这帮助我们决定在旋转和剪切过程中产生的空白区域应填充什么内容。'
- en: '`constant`: This fills the white space with a constant value that has to be
    defined using the `cval` parameter.'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`constant`：此选项将用常数值填充空白区域，常数值需要通过`cval`参数定义。'
- en: '`nearest`: This fills the whitespace with the nearest pixel.'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`nearest`：这会用最近的像素填充空白区域。'
- en: '`reflect`: This causes a reflection effect, much like a mirror.'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`reflect`：这会产生反射效果，就像镜子一样。'
- en: '`wrap`: This causes the image to wrap around and fill the whitespace.'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`wrap`：这会使图像环绕并填充空白区域。'
- en: The generator is applying the preceding operations randomly on all the images
    it encounters. This ensures that the model does not see the same image twice and
    mitigates overfitting. We have to use the `fit_generator()` function instead of
    the `fit()` function when working with generators. We pass a suitable batch size
    to the generator depending on the amount of free RAM we have for training.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器会随机应用前述操作到它遇到的所有图像上。这确保了模型不会看到相同的图像两次，从而减轻过拟合问题。在使用生成器时，我们需要使用`fit_generator()`函数，而不是`fit()`函数。我们根据训练时可用的内存大小，向生成器传递合适的批处理大小。
- en: 'The default Keras generator has a bit of memory overhead; to remove this, you
    can create your own generator. To do this, you will have to make sure you implement
    these four parts of the generator:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 Keras 生成器有一些内存开销；为了去除这些开销，你可以创建自己的生成器。为此，你需要确保实现生成器的以下四个部分：
- en: Read the input image (or any other data).
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取输入图像（或任何其他数据）。
- en: Read or generate the label.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取或生成标签。
- en: Preprocess or augment the image.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对图像进行预处理或增强。
- en: Note
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure to augment the images randomly.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确保随机增强图像。
- en: Generate output in the form that Keras expects.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以 Keras 所期望的形式生成输出。
- en: 'An example code to help you create your own generator is given here:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供了一个示例代码，帮助你创建自己的生成器：
- en: '[PRE51]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Implementing `get_input`, `get_output`, and `preprocess_image` is left as an
    exercise.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 `get_input`、`get_output` 和 `preprocess_image` 被留作练习。
- en: 'Exercise 53: Classify CIFAR-10 Images with Image Augmentation'
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 53：使用图像增强对 CIFAR-10 图像进行分类
- en: 'In this exercise, we will perform classification on the CIFAR-10 (Canadian
    Institute for Advanced Research) dataset, similar to *Exercise 52*. Here, we will
    make use of generators to augment the training data. We will rotate, shift, and
    flip the images randomly:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将对 CIFAR-10（加拿大高级研究院）数据集进行分类，类似于*练习 52*。在这里，我们将使用生成器来增强训练数据。我们将随机旋转、平移和翻转图像：
- en: 'Load the CIFAR-10 dataset using the Keras library:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Keras 库加载 CIFAR-10 数据集：
- en: '[PRE52]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Convert the data to the format that Keras requires:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据转换为 Keras 所需的格式：
- en: '[PRE53]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Convert the target variable to a one-hot vector so that our network does not
    form unnecessary connections between the different target variables:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目标变量转换为 one-hot 向量，以便我们的网络不会在不同的目标变量之间形成不必要的连接：
- en: '[PRE54]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Create the model. We will use the network from *Exercise 3*:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建模型。我们将使用*练习 3*中的网络：
- en: '[PRE55]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Add the convolutional layers:'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加卷积层：
- en: '[PRE56]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Add the pooling layer:'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加池化层：
- en: '[PRE57]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Add the batch normalization layer, along with a dropout layer:'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加批归一化层，并附加一个丢弃层：
- en: '[PRE58]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Flatten the 2D matrices into 1D vectors:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 2D 矩阵展平为 1D 向量：
- en: '[PRE59]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Use dense layers as the final layers for the model:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用全连接层作为模型的最后一层：
- en: '[PRE60]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Compile the model using the following code:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码编译模型：
- en: '[PRE61]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Create the data generator and pass it the augmentations you want on the data:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据生成器并传递所需的增强方式：
- en: '[PRE62]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Train the model:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE63]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Check the final accuracy of the model:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查模型的最终准确度：
- en: '[PRE64]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The output is shown as follows:'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![](img/C13322_06_36.jpg)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C13322_06_36.jpg)'
- en: 'Figure 6.36: Model accuracy output'
  id: totrans-372
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.36：模型准确度输出
- en: Congratulations! You have made use of data augmentation to make your model recognize
    a wider range of images. You must have noticed that the accuracy of your model
    decreased. This is due to the low number of epochs we trained the model on. Models
    in which we use data augmentatio n need to be trained for more epochs. You will
    also see that you can train for a lot more epochs without worrying about overfitting.
    This is because every epoch, the model is seeing a new image from the dataset.
    Images are rarely repeated, if ever. You will definitely see an improvement if
    you run the model for more epochs. Experiment with more architectures and augmentations.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经使用数据增强让你的模型识别更广泛的图像。你一定注意到模型的准确率下降了。这是因为我们训练模型的 epochs 数量较少。使用数据增强的模型需要更多的
    epochs 来训练。你还会看到，即使训练更多 epochs 也不用担心过拟合。这是因为每个 epoch，模型看到的数据都是新图像，数据集中的图像很少重复，甚至几乎不重复。如果你训练更多
    epochs，一定会看到进展。试着尝试更多的架构和增强方式。
- en: Here you can see an incorrectly classified image. By checking the incorrectly
    identified images, you can gauge the performance of the model and can figure out
    where it is performing poorly.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这里你可以看到一个错误分类的图像。通过检查错误识别的图像，你可以评估模型的表现，并找出其表现不佳的地方。
- en: '[PRE65]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'See the following screenshot to check the incorrect prediction:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下截图以检查错误预测：
- en: '![Figure 6.37: Incorrect prediction from the CNN model that was trained on
    augmented data'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.37：基于增强数据训练的 CNN 模型的错误预测'
- en: '](img/C13322_06_37.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_37.jpg)'
- en: 'Figure 6.37: Incorrect prediction from the CNN model that was trained on augmented
    data'
  id: totrans-379
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.37：基于增强数据训练的 CNN 模型的错误预测
- en: 'Activity 18: Identifying and Augmenting an Image'
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 18：识别和增强图像
- en: 'In this activity, we will attempt to predict if an image is of a cat or a dog,
    like in *Activity 17*. However, this time we will make use of generators to handle
    images and perform data augmentation on them to get better results:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将尝试预测图像是猫还是狗，类似于*活动 17*。不过这次我们将使用生成器来处理图像，并对它们进行数据增强，以获得更好的结果：
- en: Create functions to get each image and each image label. Then, create a function
    to preprocess the loaded images and augment them. Finally, create a data generator
    (as shown in the **Generators** section) to make use of the aforementioned functions
    to feed data to Keras during training.
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建函数以获取每个图像和每个图像标签。然后，创建一个函数来预处理加载的图像并对其进行增强。最后，创建一个数据生成器（如**生成器**部分所示），利用上述函数在训练期间将数据传递给
    Keras。
- en: Load the test dataset that will not be augmented. Use the functions from *Activity
    17.*
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载未增强的测试数据集。使用*活动 17*中的函数。
- en: Create a CNN that will identify if the image provided is of a cat or a dog.
    Make sure to make use of regularization.
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个CNN模型，用于识别给定的图像是猫还是狗。确保使用正则化。
- en: Note
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 373.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第373页找到。
- en: You should find that the test set accuracy for this model is around 72%, which
    is an improvement on the model in *Activity 17*. You will observe that the training
    accuracy is really high, at around 98%. This means that this model has started
    to overfit, much like the one in *Activity 17*. This could be due to a lack of
    data augmentation. Try changing the data augmentation parameters to see if there
    is any change in accuracy. Alternatively, you can modify the architecture of the
    neural network to get better results. You can plot the incorrectly predicted images
    to get a sense of how well the model performs.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会发现该模型的测试集准确度大约为72%，相比*活动17*中的模型有所提升。你还会观察到训练集的准确度非常高，约为98%。这意味着该模型开始出现过拟合，就像*活动17*中的模型一样。这可能是由于数据增强不足造成的。尝试更改数据增强参数，看看准确度是否有所变化。或者，你可以修改神经网络的架构，以获得更好的结果。你可以绘制出错误预测的图像，了解模型的表现如何。
- en: '[PRE66]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'An example is shown in the following screenshot:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个例子：
- en: '![Figure 6.38: Incorrect prediction of a cat by the data augmentation CNN model'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.38：数据增强CNN模型错误预测为猫'
- en: '](img/C13322_06_38.jpg)'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_06_38.jpg)'
- en: 'Figure 6.38: Incorrect prediction of a cat by the data augmentation CNN model'
  id: totrans-392
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.38：数据增强CNN模型错误预测为猫
- en: Summary
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned what digital images are and how to create machine
    learning models with them. We then covered how to use the Keras library to train
    neural network models for images. We also covered what regularization is, how
    to use it with neural networks, what image augmentation is, and how to use it
    was our focus. We covered what CNNs are and how to implement them. Lastly, we
    discussed various image preprocessing techniques.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们学习了数字图像是什么以及如何使用它们创建机器学习模型。然后，我们讲解了如何使用Keras库训练图像的神经网络模型。我们还介绍了什么是正则化，如何在神经网络中使用正则化，什么是图像增强，以及如何使用它。我们探讨了CNN是什么以及如何实现CNN。最后，我们讨论了各种图像预处理技术。
- en: Now that you have completed this chapter, you will be able to handle any kind
    of data to create machine learning models. In the next chapter, we shall learn
    how to process human language.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经完成了本章内容，你将能够处理任何类型的数据来创建机器学习模型。在下一章中，我们将学习如何处理人类语言。
