- en: Understanding AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解人工智能
- en: You now have a good understanding of what data science can do and how we can
    check whether it works. We have covered the main domains of data science, including
    machine learning and deep learning, but still, the inner workings of the algorithms
    are difficult to discern through the fog. In this chapter, we will look at algorithms.
    You will get an intuitive understanding of how the learning process is defined
    using mathematics and statistics. Deep neural networks won't be so mystical anymore,
    and common machine learning jargon will not scare you but provide understanding
    and ideas to complete the ever-growing list of potential projects.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对数据科学能做什么以及我们如何检查其效果有了很好的理解。我们已经介绍了数据科学的主要领域，包括机器学习和深度学习，但算法的内部机制仍然难以通过雾霾般的视角看清。在本章中，我们将讨论算法。你将直观地理解如何通过数学和统计学来定义学习过程。深度神经网络将不再是那么神秘，常见的机器学习术语也不会让你害怕，而是提供理解和启发，帮助你完成不断增长的潜在项目清单。
- en: 'You are not the only one who will benefit from reading this chapter. Your new
    knowledge will streamline communication with colleagues, making meetings short
    and purposeful and teamwork more efficient. We will start at the heart of every
    machine learning problem: defining the learning process. To do this, we will start
    with the two subjects that lie at the root of data science: mathematics and statistics.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章不仅对你有益，你的新知识还将简化与同事的沟通，使会议更加简短且有目的，团队合作更高效。我们将从每个机器学习问题的核心开始：定义学习过程。为了做到这一点，我们将从数据科学的两大根基——数学和统计学开始。
- en: 'In this chapter, we will be covering the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding mathematical optimization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数学优化
- en: Thinking with statistics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用统计学思维
- en: How do machines learn?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器是如何学习的？
- en: Exploring machine learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索机器学习
- en: Exploring deep learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索深度学习
- en: Understanding mathematical optimization
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数学优化
- en: First, we will explore the concept of mathematical optimization. Optimization
    is the central component of machine learning problem. It turns out that the learning
    process is nothing more than a mere mathematical optimization problem. The trick
    is to define it properly. To come up with a good definition, we first need to
    understand how mathematical optimization works and which problems it can solve.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探讨数学优化的概念。优化是机器学习问题的核心组成部分。事实证明，学习过程不过是一个简单的数学优化问题。关键在于如何正确地定义它。为了给出一个好的定义，我们首先需要理解数学优化的工作原理，以及它能解决哪些问题。
- en: If you work in the business sector, I bet that you hear the word optimization
    several times a day. To optimize something means to make it more efficient, cut
    costs, increase revenues, and minimize risks. Optimization involves taking a number
    of actions, measuring results, and deciding whether you have ended up in a better
    place.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在商业领域工作，我敢打赌你每天都会听到“优化”这个词。优化意味着提高效率、削减成本、增加收入并最小化风险。优化涉及采取一系列行动、衡量结果，并决定是否达到了更好的状态。
- en: For example, to optimize your daily route to work, you can minimize the total
    time you spend driving from home to the office. Let's suppose that in your case
    the only thing that matters is time. Thus, optimization means minimization of
    the time. You may try different options such as using another road or going by
    public transport instead of driving your car. To choose the best, you will evaluate
    all routes using the same quantity, that is, the total time to make it from home
    to the office.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了优化你每天上班的路线，你可以最小化从家到办公室的总驾车时间。假设对你而言，唯一重要的是时间。因此，优化就是时间的最小化。你可以尝试不同的方案，比如走另一条路，或者改用公共交通而不是开车。为了选择最佳路线，你将使用相同的量度来评估所有路线，即从家到办公室的总时间。
- en: 'To get a better idea of defining optimization problems, let''s consider another
    example. Our friend Jonathan was tired of his day-to-day job at a bank, so he
    has started a rabbit farm. It turns out that rabbits breed fast. To start, he
    bought four rabbits, and after a while he had 16\. A month later, there were 256
    of them. All of those new rabbits causing additional expense. Jonathan''s rabbit
    sale rates fell lower than the rate at which the rabbits bred. Jonathan''s smart
    farmer friend Aron was impressed with his rabbit production rates, so he proposed
    to buy all excess rabbits for a discounted price. Now, Jonathan needs to find
    out how many rabbits to sell to Aron so that he can keep within the following
    boundaries:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解优化问题的定义，我们来看看另一个例子。我们的朋友乔纳森厌倦了银行的日常工作，于是他开始了一个兔子农场。结果发现兔子繁殖得很快。一开始，他买了四只兔子，不久后就有了16只。一个月后，它们的数量达到了256只。所有这些新兔子带来了额外的费用。乔纳森的兔子销售率低于兔子繁殖的速度。乔纳森聪明的农场朋友阿隆对他的兔子繁殖速度印象深刻，于是提议以折扣价购买所有多余的兔子。现在，乔纳森需要找出应该卖给阿隆多少只兔子，以便保持在以下边界内：
- en: He won't get in a situation where he can't sell a rabbit to someone who desperately
    wants one. The rabbit breeding rate should not fall below the rabbit selling forecasts.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他不会陷入不能将兔子卖给迫切需要兔子的人这一境地。兔子的繁殖率不应低于兔子销售预期。
- en: His total expenses for rabbit care stay within the budget.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他在兔子照料方面的总支出保持在预算范围内。
- en: As you can see, we have defined another optimization problem, and the rabbit
    farm started to remind the bank job that Jonathan left before. This optimization
    task is quite different though, it looks harder. In the first problem, we tried
    to minimize commuting time. In this problem, we need to seek the minimum amount
    of rabbits to sell so it does not violate other conditions. We call problems like
    these **constrained optimization**. Additional constraints allow us to model more
    realistic scenarios in complex environments. To name a few, constrained optimization
    can solve planning, budgeting, and routing problems. In the end, Jonathan was
    disappointed with his rabbit farm and sold it to Aron. He then continued his path
    of finding a perfect occupation that wouldn't end up similar to his banking job.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们定义了另一个优化问题，兔子农场开始让人想起了乔纳森之前离开的银行工作。这个优化任务虽然也很复杂，但它与第一个问题不同，看起来更加困难。在第一个问题中，我们尝试最小化通勤时间；而在这个问题中，我们需要寻找一个最小的兔子销售数量，以确保不违反其他条件。我们将这种类型的问题称为**约束优化**。额外的约束让我们能够在复杂的环境中模拟更为现实的场景。举几个例子，约束优化可以解决规划、预算和路由问题。最后，乔纳森对他的兔子农场感到失望，并将其卖给了阿隆。他随后继续寻找一个完美的职业，希望它不会像银行工作那样结束。
- en: There is one place where profits and losses cease making you mad; that is the mathematics
    department at a technical university. To get a position there, they ask you to
    pass an exam. The first task is to find a minimum of a function, ![](img/bddf2c16-f3e3-4ee5-a413-152095024615.png).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个地方，盈亏不再让你抓狂；那就是技术大学的数学系。为了得到一个职位，他们要求你通过一项考试。第一个任务是找到一个函数的最小值，![](img/bddf2c16-f3e3-4ee5-a413-152095024615.png)。
- en: 'The following is the plot of this function:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该函数的图表：
- en: '![](img/911bbd48-87ee-4d8d-b9b9-d7fb2b90ba24.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/911bbd48-87ee-4d8d-b9b9-d7fb2b90ba24.png)'
- en: While examining the plot, you notice that this function takes values no less
    than 0, so the answer is obviously 0\. The next question looks like the previous
    one but with a twist—*Find a minimum of the function* ![](img/d94e8a03-ff14-400a-8131-1bd7086194e4.png),
    *where ![](img/1def254f-e730-4c92-aaf4-e592208ef0ec.png) is an arbitrary number*.
    To solve it, you draw a bunch of plots and find out that the minimum value is
    always *a*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查图表时，你会注意到这个函数的值不小于 0，所以答案显然是 0。下一个问题看起来像是之前的问题，但有一些变化——*找到该函数的最小值* ![](img/d94e8a03-ff14-400a-8131-1bd7086194e4.png)，*其中
    ![](img/1def254f-e730-4c92-aaf4-e592208ef0ec.png) 是一个任意数值*。为了解决这个问题，你绘制了一些图表，并发现最小值始终是
    *a*。
- en: The last question takes it to the extreme. It says that you won't be given a
    formula for ![](img/65cb02ad-8d7e-4ab8-ae3d-3a0fb57d2388.png), but you can go
    to your teacher and ask for values of ![](img/93434734-953b-4790-86f1-da915f2d31de.png) for
    some ![](img/d9035d65-10fa-4f72-ace4-b66d9f566858.png) as many times as you want.
    It is impossible to draw a plot. In other plots, the minimum was always the lowest
    point. How can we find that point without looking at the plot? To tackle this
    problem, we will first imagine that we have a plot of this function.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的问题将问题推向极限。它说你不会得到关于 ![](img/65cb02ad-8d7e-4ab8-ae3d-3a0fb57d2388.png) 的公式，但你可以去找老师，要求获取一些
    ![](img/93434734-953b-4790-86f1-da915f2d31de.png) 在某些 ![](img/d9035d65-10fa-4f72-ace4-b66d9f566858.png)
    下的值，并且可以重复多次。这样无法绘制图像。在其他图像中，最小值总是最低点。那么我们如何在不查看图像的情况下找到这个点呢？为了解决这个问题，我们首先假设我们有该函数的图像。
- en: 'First, we will draw a line between two arbitrary points of the function, as
    seen in the following plot:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将在函数的两个任意点之间画一条直线，如下图所示：
- en: '![](img/ffd06268-af3c-47f9-aca4-fa7b27087a62.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ffd06268-af3c-47f9-aca4-fa7b27087a62.png)'
- en: 'We will call the distance between those points ![](img/15e681ce-db67-4f73-8d4b-505deea6e04e.png).
    If we make ![](img/6a7e6175-b05f-4c10-9d92-f13938a7ba93.png) smaller and smaller,
    two points will be so close that they will visually converge to a single point:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这两个点之间的距离称为 ![](img/15e681ce-db67-4f73-8d4b-505deea6e04e.png)。如果我们将 ![](img/6a7e6175-b05f-4c10-9d92-f13938a7ba93.png)
    越变越小，这两个点会越来越接近，直到它们在视觉上合并成一个点：
- en: '![](img/d40fa9e2-4e3f-49a0-a572-563935a309c9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d40fa9e2-4e3f-49a0-a572-563935a309c9.png)'
- en: The line in the preceding plot is called a tangent. It has a very convenient
    property, and the slope of this line can help us find the minimum or maximum of
    a function. If the line is flat, then we have found either the minimum or maximum
    of a function. If all nearby points are higher, then it should be a maximum. If
    all nearby points are lower, then this is the minimum.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上面图中的直线被称为切线。它有一个非常方便的特性，切线的斜率可以帮助我们找到函数的最小值或最大值。如果这条线是平的，那么我们就找到了函数的最小值或最大值。如果附近的点都比较高，那么应该是最大值。如果附近的点都比较低，那么这就是最小值。
- en: 'The following plot shows a function (drawn in blue) and its maximum, along
    with the tangent (drawn in orange):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了一个函数（用蓝色表示）及其最大值，并附有切线（用橙色表示）：
- en: '![](img/8205186f-79a2-4ac6-8a63-695d7ce6c7ae.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8205186f-79a2-4ac6-8a63-695d7ce6c7ae.png)'
- en: 'Drawing a bunch of lines and points becomes mundane after a while. Thankfully,
    there is a simple way to compute a slope of this line between ![](img/b41a95fb-d14c-4d8a-bdbc-4740c20405e3.png) and ![](img/da500071-60c5-4e9f-a9f1-bcbff0c8c5c0.png).
    If you recall the Pythagorean theorem, you will quickly find an answer: [![](img/429f9755-80d6-431a-b87d-9a7467ec2461.png)].
    We can easily find the slope using this formula.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制一堆线条和点很快会变得单调无趣。幸运的是，有一种简单的方法可以计算这条线的斜率，方法是计算从 ![](img/b41a95fb-d14c-4d8a-bdbc-4740c20405e3.png)
    到 ![](img/da500071-60c5-4e9f-a9f1-bcbff0c8c5c0.png) 之间的斜率。如果你回忆一下勾股定理，你很快就能找到答案：[![](img/429f9755-80d6-431a-b87d-9a7467ec2461.png)]。我们可以通过这个公式轻松找到斜率。
- en: Congratulations, we have just invented our first mathematical optimization algorithm,
    a gradient descent. As always, the name is scary, but the intuition is simple.
    To have a good understanding of function optimization, imagine that you are standing
    on a large hill. You need to descend from it with your eyes closed. You will probably
    test the area around you by moving your feet around. When you feel a descending
    direction, you will take a step there and repeat. In mathematical terminology,
    the hill would be a function, ![](img/05f59972-aef1-4530-89de-6725bf754977.png).
    Each time you evaluate a slope, you calculate the gradient of the function, ![](img/e17eff6c-1b96-4465-8ce1-da2bc478e56a.png).
    You can follow this gradient to find the minimum or maximum of a function. That's
    why it is called gradient descent.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，我们刚刚发明了第一个数学优化算法——梯度下降。像往常一样，名字听起来吓人，但直观理解很简单。为了更好地理解函数优化，假设你正站在一座大山丘上，眼睛闭着，你需要从山丘上走下来。你可能会通过移动脚步来测试周围的区域。当你感受到某个方向在下降时，你就会朝着那个方向迈步，然后继续重复这个过程。用数学术语来说，这座山丘就是一个函数，![](img/05f59972-aef1-4530-89de-6725bf754977.png)。每次你评估斜率时，实际上是在计算函数的梯度，![](img/e17eff6c-1b96-4465-8ce1-da2bc478e56a.png)。你可以沿着这个梯度找到函数的最小值或最大值。这就是为什么它叫做梯度下降。
- en: You can solve the final task by using gradient descent. You can choose a starting
    point, [![](img/c10f822a-4c2a-4f53-be28-925894d94bf8.png)], ask for the value
    of [![](img/20764d2d-3746-443c-a6f9-55212c8b176d.png)], and calculate the slope
    using the small number, ![](img/212858db-b721-4543-bc1c-b54c9fb0ec9f.png). By
    looking at the slope, you can decide whether your next pick, [![](img/77706a78-632d-40df-87dc-d753b3fa43b4.png)], should
    be greater or less than [![](img/c8400b52-92cc-463f-94a7-17ff7f8bf387.png)]. When
    the slope becomes zero, you can test whether your current value of [![](img/51db79c8-9fd4-4570-97ca-bcf1fa28f884.png)] is
    the minimum or maximum by looking at several nearby values. If every value is
    less than [![](img/afff9b66-3b6c-46b2-abb6-4fb90cc6c18e.png)], then [![](img/8e4a7548-7761-46f4-9756-099ce8f2a4d7.png)] is
    the maximum. Otherwise, it is a minimum.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用梯度下降法来解决最终任务。你可以选择一个起始点，[![](img/c10f822a-4c2a-4f53-be28-925894d94bf8.png)]，请求获取[![](img/20764d2d-3746-443c-a6f9-55212c8b176d.png)]的值，并使用小数字[![](img/212858db-b721-4543-bc1c-b54c9fb0ec9f.png)]计算斜率。通过观察斜率，你可以决定你的下一个选择[![](img/77706a78-632d-40df-87dc-d753b3fa43b4.png)]应该大于还是小于[![](img/c8400b52-92cc-463f-94a7-17ff7f8bf387.png)]。当斜率变为零时，你可以通过查看附近的几个值来测试当前的[![](img/51db79c8-9fd4-4570-97ca-bcf1fa28f884.png)]值是否为最小值或最大值。如果每个值都小于[![](img/afff9b66-3b6c-46b2-abb6-4fb90cc6c18e.png)]，那么[![](img/8e4a7548-7761-46f4-9756-099ce8f2a4d7.png)]就是最大值。否则，它就是最小值。
- en: 'As always, there is a caveat. Let''s examine this function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，有一个警告。让我们来分析一下这个函数：
- en: '![](img/cf871086-e906-4a3e-9e7e-59c38b9ca861.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf871086-e906-4a3e-9e7e-59c38b9ca861.png)'
- en: If we start gradient descent at point **A**, we will end up with a real minimum.
    But if we start at point **B**, we will be stuck at the local minimum. When you
    use gradient descent, you can never actually know whether you are in a local or
    global minimum. One way to check is to repeat the descent from various points
    that are far away from each other. The other way to avoid local minima is to increase
    the step size, ![](img/04a9240d-d9ff-430a-818d-c2b77eb5ee44.png). But be careful;
    if ![](img/60e0e937-c4e9-4c72-b04b-5af0ef22adf6.png) is too large, you will just
    jump over the minima again and again, never reaching your true goal, the global
    minimum.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从点**A**开始梯度下降，我们将最终找到一个真实的最小值。但如果我们从点**B**开始，我们将停留在局部最小值。使用梯度下降时，你永远无法确切知道自己是否处于局部最小值或全局最小值。检查的一种方法是从相隔较远的不同点重复下降。避免局部最小值的另一种方法是增加步长，[![](img/04a9240d-d9ff-430a-818d-c2b77eb5ee44.png)]。但要小心；如果[![](img/60e0e937-c4e9-4c72-b04b-5af0ef22adf6.png)]太大，你将一次次跳过最小值，永远无法到达真正的目标——全局最小值。
- en: Like in machine learning, there are many mathematical optimization algorithms
    with different trade-offs. Gradient descent is one of the simplest and easiest
    to get started with. Despite being simple, gradient descent is commonly used to
    train machine learning models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在机器学习中一样，有许多数学优化算法，它们各有优缺点。梯度下降是最简单且最容易入门的一种。尽管它很简单，但梯度下降在训练机器学习模型时被广泛使用。
- en: 'Let''s review a few key points before moving on:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们回顾一下几个关键点：
- en: Mathematical optimization is the central component of machine learning.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数学优化是机器学习的核心组成部分。
- en: 'There are two kinds of optimization problems: constrained and unconstrained.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化问题有两种类型：有约束和无约束。
- en: Gradient descent is a simple and widely applied optimization algorithm. To understand
    the intuition behind gradient descent, recall the hill descent analogy.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降是一种简单且广泛应用的优化算法。为了理解梯度下降背后的直觉，可以回想一下下坡比喻。
- en: Now you have a good grip on the main principles of mathematical optimization
    in your tool belt, we can research the field of statistics—the grandfather of
    machine learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经掌握了数学优化的主要原理，我们可以研究统计学领域——机器学习的祖父学科。
- en: Thinking with statistics
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用统计学思维
- en: Statistics deal with all things about data, namely, collection, analysis, interpretation,
    inference, and presentation. It is a vast field, incorporating many methods for
    analyzing data. Covering it all is out of the scope of this book, but we will
    look into one concept that lies at the heart of machine learning, that is, **maximum
    likelihood estimation** (**MLE**). As always, do not fear the terminology, as
    the underlying concepts are simple and intuitive. To understand MLE, we will need
    to dive into probability theory, the cornerstone of statistics.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学处理关于数据的一切，具体来说，包括数据的收集、分析、解释、推断和展示。它是一个广阔的领域，包含了许多分析数据的方法。全面覆盖这个领域超出了本书的范围，但我们将深入探讨一个在机器学习中占据核心地位的概念——**最大似然估计**（**MLE**）。一如既往，别怕术语，因为背后的概念既简单又直观。为了理解
    MLE，我们需要深入了解概率论，这也是统计学的基石。
- en: To start, let's look at why we need probabilities when we already are equipped
    with such great mathematical tooling. We use calculus to work with functions on
    an infinitesimal scale and to measure how they change. We developed algebra to
    solve equations, and we have dozens of other areas of mathematics that help us
    to tackle almost any kind of hard problem we can think of. We even came up with
    category theory that provides a universal language for all mathematics that almost
    no one can understand (Haskell programmers included).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看为什么在我们已经拥有如此强大的数学工具时，还需要概率。我们使用微积分来处理无穷小尺度上的函数，并衡量它们的变化。我们发展了代数来解方程，而且我们有其他许多数学领域来帮助我们解决几乎任何我们能想到的难题。我们甚至提出了范畴论，它为所有数学提供了一个通用的语言，但几乎没有人能理解它（包括
    Haskell 程序员）。
- en: The difficult part is that we all live in a chaotic universe where things can't
    be measured exactly. When we study real-world processes we want to learn about
    many random events that distort our experiments. Uncertainty is everywhere, and
    we must tame and use it for our needs. That is when probability theory and statistics
    come into play. Probabilities allow us to quantify and measure uncertain events
    so we can make sound decisions. Daniel Kahneman showed in his widely known book
    *Thinking, Fast and Slow*, that our intuition is notoriously bad in solving statistical
    problems. Probabilistic thinking helps us to avoid biases and act rationally.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 难点在于我们生活在一个混乱的宇宙中，事物无法精确测量。当我们研究现实世界中的过程时，我们需要了解许多随机事件，这些事件会扭曲我们的实验结果。 不确定性无处不在，我们必须驾驭并利用它满足我们的需求。这时，概率论和统计学就派上了用场。概率让我们能够量化和衡量不确定事件，从而做出明智的决策。丹尼尔·卡尼曼在他广为人知的著作《*思考，快与慢*》中指出，我们在解决统计问题时的直觉往往非常糟糕。概率思维帮助我们避免偏见，理性地行动。
- en: Frequentist probabilities
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 频率主义概率
- en: 'Imagine that a stranger suggested you play a game: he gives you a coin. You
    toss it. If it comes up heads, you get $100\. If it is tails, you lose $75\. Before
    playing a game, you will surely want to check whether or not it is fair. If the
    coin is biased toward tails, you can lose money pretty quickly. How can we approach
    this? Let''s conduct an experiment, wherein we will record 1 if heads come up
    and 0 if we see tails. The fun part is that we will need to make 1,000 tosses
    to be sure that our calculations are right. Imagine we got the following results:
    600 heads (1s) and 400 tails (0s). If we then count how frequent heads or tails
    came up in the past, we will get 60% and 40%, respectively. We can interpret those
    frequencies as probabilities of a coin coming up heads or tails. We call this
    a frequentist view on the probabilities. It turns out that our coin is actually
    biased toward heads. The expected value of this game can be calculated by multiplying
    probabilities with their values and summing everything up (the value in the following
    formula is negative because $40 is a potential loss, not gain):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，有个陌生人建议你玩一个游戏：他给你一枚硬币，你抛掷它。如果是正面，你赢得$100；如果是反面，你输了$75。在玩这个游戏之前，你肯定会想检查一下它是否公平。如果硬币偏向反面，你可能很快就会亏钱。我们该如何处理这个问题呢？让我们做一个实验，其中我们记录1表示正面，0表示反面。好玩的地方在于，我们需要抛掷1,000次，才能确保我们的计算是正确的。假设我们得到了以下结果：600次正面（1），400次反面（0）。如果我们计算正面和反面出现的频率，我们会得到60%和40%。我们可以将这些频率解读为硬币正面或反面出现的概率。这就是频率主义的概率观。事实证明，我们的硬币实际上偏向正面。这个游戏的期望值可以通过将概率与其对应的数值相乘并加总得出（下列公式中的数值为负，因为$40是潜在的损失，而不是收益）：
- en: '![](img/c9ef427c-e4dd-4b65-ac08-e900e07fa5b0.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9ef427c-e4dd-4b65-ac08-e900e07fa5b0.png)'
- en: The more you play, the more you get. Even after having several consecutive unlucky
    throws in a row, you can be sure that the returns will average out soon. Thus,
    a frequentist probability measures a proportion of some event to all other possible
    events.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 玩得越多，得到的越多。即使连续几次掷骰子不幸运，你也可以确定，回报很快会趋于平均。因此，频率派概率衡量某个事件与所有其他可能事件的比例。
- en: Conditional probabilities
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件概率
- en: 'It is handy to know the probability of an event given some other event has
    occurred. We write the conditional probability of an event ![](img/9f4edf0c-02f4-4f5d-b327-e42b6dcc3a16.png) given
    event ![](img/0550de15-ba34-43f1-89cb-7092ac03c448.png) as ![](img/a2f7111c-68ab-4eff-80fb-67c0fb20a134.png).
    Take rain, for example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 了解某个事件发生的条件概率是非常有用的，特别是当另一个事件已经发生时。我们将事件 ![](img/9f4edf0c-02f4-4f5d-b327-e42b6dcc3a16.png)
    给定事件 ![](img/0550de15-ba34-43f1-89cb-7092ac03c448.png) 的条件概率表示为 ![](img/a2f7111c-68ab-4eff-80fb-67c0fb20a134.png)。以下雨为例：
- en: What is the probability of rain given we hear thunder?
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定我们听到雷声，下雨的概率是多少？
- en: What is the probability of rain given it is sunny?
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定晴天，下雨的概率是多少？
- en: 'In the following diagram, you can see the probabilities of different events
    occurring together:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，您可以看到不同事件同时发生的概率：
- en: '![](img/481bc35d-6f6e-46b7-897d-ad0651b4d5d5.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/481bc35d-6f6e-46b7-897d-ad0651b4d5d5.png)'
- en: From this Euler diagram, we can see that ![](img/28e92085-79d8-4c5a-872b-cc0c512fb639.png),
    meaning that there is always rain when we hear thunder (yes, it is not exactly
    true, but we'll take this as true for the sake of simplicity).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个欧拉图中，我们可以看到 ![](img/28e92085-79d8-4c5a-872b-cc0c512fb639.png)，这意味着每当我们听到雷声时，必定下雨（是的，这并不完全准确，但为了简化问题，我们假设它是正确的）。
- en: 'What about ![](img/43ee27c6-2ae9-48ce-a87b-33bfbef3869d.png)? Visually, this
    probability is small, but how can we formulate this mathematically to do the exact
    calculations? Conditional probability is defined as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 ![](img/43ee27c6-2ae9-48ce-a87b-33bfbef3869d.png) 呢？从视觉上看，这个概率很小，但我们如何用数学公式来精确计算呢？条件概率的定义如下：
- en: '![](img/a4c54506-0805-410b-aad6-05bd8cf1501f.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4c54506-0805-410b-aad6-05bd8cf1501f.png)'
- en: In words, we divide the joint probability of both *Rain* and *Sunny* by the
    probability of sunny weather.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 用语言来说，我们将*下雨*和*晴天*的联合概率除以晴天的概率。
- en: Dependent and independent events
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依赖事件与独立事件
- en: 'We call a pair of events independent, if the probability of one event does
    not influence the other. For example, take the probability of rolling a die and
    getting a 2 twice in a row. Those events are independent. We can state this as
    follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个事件的发生概率不影响另一个事件的发生概率，我们就称这对事件是独立的。例如，掷骰子连续两次得到2的概率。这两个事件是独立的。我们可以这样表述：
- en: '![](img/4da71ad8-0de3-4f03-84b2-5f4d26d8111e.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4da71ad8-0de3-4f03-84b2-5f4d26d8111e.png)'
- en: 'But why does this formula work? First, let''s rename events for the first and
    second tosses as A and B to remove notational clutter and then rewrite the probability
    of a roll explicitly as a joint probability of both rolls we have seen so far:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么这个公式有效呢？首先，我们将第一次和第二次投掷的事件重命名为A和B，以去除符号混乱，然后明确地将掷骰子的概率重写为我们到目前为止所看到的两个投掷的联合概率：
- en: '![](img/50559135-6ee9-4325-b8dc-956b27439e7b.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50559135-6ee9-4325-b8dc-956b27439e7b.png)'
- en: 'And now multiply and divide ![](img/44c56de8-4513-4459-a57c-53b0054b58a1.png)
    by ![](img/1d94f705-b256-4be6-97c3-906f2cd98549.png) (nothing changes, it can
    be canceled out), and recall the definition of conditional probability:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将 ![](img/44c56de8-4513-4459-a57c-53b0054b58a1.png) 与 ![](img/1d94f705-b256-4be6-97c3-906f2cd98549.png)
    相乘和相除（没有变化，它们可以被约去），并回顾条件概率的定义：
- en: '![](img/b54ec1b2-12c0-4a51-8eb7-4e828a1b8480.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b54ec1b2-12c0-4a51-8eb7-4e828a1b8480.png)'
- en: If we read the previous expression from right to left, we find that ![](img/f9030a42-ae3c-4599-8d4b-b2fcf30b9b2c.png).
    Basically, this means that A is independent of B! The same argument goes for ![](img/0213653a-f971-423a-9b01-2ec9716cb496.png).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从右到左读出前面的表达式，我们发现 ![](img/f9030a42-ae3c-4599-8d4b-b2fcf30b9b2c.png)。基本上，这意味着A与B是独立的！同样的推理适用于 ![](img/0213653a-f971-423a-9b01-2ec9716cb496.png)。
- en: Bayesian view on probability
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯概率观点
- en: 'Before this point, we always measured probabilities as frequencies. The frequentist
    approach is not the only way to define probabilities. While frequentists think
    about probabilities as proportions, the Bayesian approach takes prior information
    into account. Bayes'' theory is centered around a simple theorem that allows us
    to compute conditional probabilities based on prior knowledge:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点之前，我们总是将概率衡量为频率。频率主义方法并不是定义概率的唯一方式。虽然频率主义者将概率视为比例，但贝叶斯方法考虑了先验信息。贝叶斯理论基于一个简单的定理，让我们可以基于先前的知识计算条件概率：
- en: '![](img/044af8c0-e62f-4e60-9fa5-d9d23981ca19.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/044af8c0-e62f-4e60-9fa5-d9d23981ca19.png)'
- en: In this example, the prior value is ![](img/77bcd102-059d-4827-8e48-ebb7ebe62e33.png).
    If we do not know the real prior value, we can substitute an estimate that is
    based on our experience to make an approximate calculation. This is the beauty
    of Bayes' theorem. You can calculate complex conditional probabilities with simple
    components.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，先验值是 ![](img/77bcd102-059d-4827-8e48-ebb7ebe62e33.png)。如果我们不知道真实的先验值，我们可以用基于经验的估计值来进行近似计算。这就是贝叶斯定理的魅力所在。你可以用简单的组件计算复杂的条件概率。
- en: Bayes' theorem has immense value and a vast area of application. The Bayesian
    theory even has its own branch of statistics and inference methods. Many people
    think that the Bayesian view is a lot closer to how we humans understand uncertainties,
    in particular, how prior experience affects decisions we make.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理具有巨大的价值和广泛的应用领域。贝叶斯理论甚至有自己的一支统计学分支和推理方法。许多人认为，贝叶斯观点更接近我们人类对不确定性的理解，特别是先验经验如何影响我们的决策。
- en: Distributions
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布
- en: 'Probabilities work with sets of outcomes or events. Many problems we describe
    with probabilities share common properties. In the following plot, you can see
    the bell curve:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 概率与一组结果或事件有关。我们用概率描述的许多问题都有共同的属性。在下图中，你可以看到钟形曲线：
- en: '![](img/e75f1b9f-bef0-4888-96d2-2268f20267be.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e75f1b9f-bef0-4888-96d2-2268f20267be.png)'
- en: The bell curve, or Gaussian distribution, is centered around the most probable
    set of outcomes, and the tails on both ends represent the least likely outcomes.
    Because of its mathematical properties, the bell curve appears everywhere in our
    world. Measure the height of lots of random people, and you will see a bell curve;
    look at the height of all grass blades in your lawn, and you will see it again.
    Calculate the probability of people in your city having a certain income, and
    here it is again.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 钟形曲线或高斯分布集中在最可能的结果集周围，两端的尾部表示最不可能的结果。由于其数学特性，钟形曲线在我们的世界中无处不在。测量许多随机人的身高，你会看到钟形曲线；观察你草坪上所有草的高度，你会再次看到它。计算你所在城市的居民具有某种收入的概率，你又会看到它。
- en: The Gaussian distribution is one of the most common distributions, but there
    are many more. A probability distribution is a mathematical law that tells us
    the probabilities of different possible outcomes of events formulated as a mathematical
    function.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布是最常见的分布之一，但还有许多其他分布。概率分布是一种数学法则，它告诉我们不同可能结果的概率，这些结果通过数学函数表达。
- en: When we measured relative frequencies of a coin-toss event, we calculated the
    so-called empirical probability distribution. Coin tosses also can be formulated
    as a Bernoulli distribution. And if we wanted to calculate the probability of
    heads after *n* trials, we may use a binomial distribution.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们测量硬币投掷事件的相对频率时，我们计算了所谓的经验概率分布。硬币投掷也可以被表述为伯努利分布。如果我们想要计算在*n*次试验后得到正面的概率，我们可以使用二项分布。
- en: 'It is convenient to introduce a concept analogous to a variable that may be
    used in probabilistic environments—a random variable. Random variables are the
    basic building blocks of statistics. Each random variable has a distribution assigned
    to it. Random variables are written in uppercase by convention, and we use the
    ~ symbol to specify a distribution assigned to a variable:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 引入一个与变量类似的概念，在概率环境中可能会使用——随机变量。随机变量是统计学的基本构件。每个随机变量都有一个分布。随机变量通常用大写字母表示，我们使用~符号来指定分配给变量的分布：
- en: '![](img/32946440-4404-446d-bd28-815b1df0db25.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32946440-4404-446d-bd28-815b1df0db25.png)'
- en: This means that the random variable *X* is distributed according to a Bernoulli
    law with the probability of success (heads) equal to 0.6.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着随机变量*X*根据伯努利定律分布，成功（正面）的概率为0.6。
- en: Calculating statistics from data samples
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据样本计算统计量
- en: 'Suppose you are doing research on human height and are eager to publish a mind-blowing
    scientific paper. To complete your research, you need to measure the average person''s
    height in your area. You can do this in two ways:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在研究人类身高，并且渴望发表一篇令人惊叹的科学论文。为了完成你的研究，你需要测量你所在地区普通人的平均身高。你可以通过两种方式完成这个任务：
- en: Collect the heights of every person in your city and calculate average
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集你所在城市每个人的身高并计算平均值
- en: Apply statistics
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用统计学
- en: 'Statistics allows us to reason about different properties of the population
    without collecting a full dataset for each person in the population. The process
    of selecting a random subset of data from the true population is called sampling.
    A statistic is any function that is used to summarize the data using values from
    the sample. The ubiquitous statistic that is used by almost everyone on a daily
    basis is the sample mean or arithmetic average:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学允许我们在不为每个人收集完整数据集的情况下推理出种群的不同属性。从真实种群中选择随机数据子集的过程称为抽样。统计量是使用样本中的值来总结数据的任何函数。几乎每个人每天都使用的无处不在的统计量是样本均值或算术平均数：
- en: '![](img/6cfed050-984c-4092-b7ca-c64ae9932998.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6cfed050-984c-4092-b7ca-c64ae9932998.png)'
- en: 'We have collected a random sample of 16 people to calculate an average height.
    In the following table, we can see the heights over the course of four days:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经收集了一组随机样本共16人的平均身高。在下表中，我们可以看到四天内的身高情况：
- en: '| **Day** | **Heights** | **Average** |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| **日期** | **身高** | **平均值** |'
- en: '| Monday | 162 cm, 155 cm, 160 cm, 171 cm | 162.00 cm |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 星期一 | 162 cm, 155 cm, 160 cm, 171 cm | 162.00 cm |'
- en: '| Tuesday | 180 cm, 200 cm, 210 cm, 179 cm | 192.25 cm |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 星期二 | 180 cm, 200 cm, 210 cm, 179 cm | 192.25 cm |'
- en: '| Wednesday | 160 cm, 170 cm, 158 cm, 176 cm | 166.00 cm |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 星期三 | 160 cm, 170 cm, 158 cm, 176 cm | 166.00 cm |'
- en: '| Thursday | 178 cm, 169 cm, 157 cm, 165 cm | 167.25 cm |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 星期四 | 178 cm, 169 cm, 157 cm, 165 cm | 167.25 cm |'
- en: '| Total |  | 171.88 cm |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 总计 |  | 171.88 cm |'
- en: We collected a sample of four heights for each day, a total of 16 heights. Your
    statistician friend Fred told you on Friday that he had already collected a sample
    of 2,000 people and the average height in the area was about 170 cm.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集了每天四个身高的样本，总共16个身高。你的统计学朋友弗雷德告诉你，他已经收集了2000人的样本，该地区的平均身高约为170厘米。
- en: 'To investigate, we can look at how your sample average changed with each new
    data point:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要调查这个问题，我们可以看看每个新数据点对你的样本平均值造成了怎样的影响：
- en: '![](img/00996332-e5a5-406b-96ab-6d4f476833d9.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00996332-e5a5-406b-96ab-6d4f476833d9.png)'
- en: Notice, that on day 2, the average value was unexpectedly high. It may just
    have happened that we had stumbled upon four tall people. The random fluctuations
    in the data are called variance.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第2天的平均值出乎意料地高。可能我们碰巧遇到了四个高个子。数据中的随机波动称为方差。
- en: 'We can measure sample variance using the following formula:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下公式来测量样本方差：
- en: '![](img/634c6613-06d9-452d-959d-2541f3dc820c.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/634c6613-06d9-452d-959d-2541f3dc820c.png)'
- en: Sample variance summarizes our data, so we can consider it as another statistic.
    The larger the variance is, the more sample size you need to collect before calculating
    the accurate average value, which will be close to the real one. This phenomenon
    has a name—the law of large numbers. The more measurements you make, the better
    your estimate will be.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 样本方差总结了我们的数据，因此我们可以将其视为另一个统计量。方差越大，计算准确平均值之前需要收集的样本量就越多，这个平均值将接近真实值。这种现象有个名字——大数定律。你做的测量越多，你的估计就越准确。
- en: Statistical modeling
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计建模
- en: 'Statistics is more than simply calculating summary numbers. One of the most
    interesting aspects of statistics is modeling. Statistical modeling studies mathematical
    models that make a set of statistical assumptions about data. To be more clear,
    let''s return to our weather example. We have collected a dataset with random
    variables that describe the current weather:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学不仅仅是计算总结性数字。统计学中最有趣的一个方面是建模。统计建模研究了对数据进行一组统计假设的数学模型。更清楚地说，让我们回到我们的天气例子。我们收集了一个包含描述当前天气的随机变量的数据集：
- en: Average speed of the wind
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均风速
- en: Air humidity
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空气湿度
- en: Air temperature
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空气温度
- en: Total number of birds seen in the sky in a local area
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当地某区域天空中看到的鸟类总数
- en: Statistician's mood
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计学家的心情
- en: Using this data, we want to infer which variables are related to rain. To do
    this, we will build a statistical model. Besides the previous data, we have recorded
    a binary rain variable that takes the value 1 if it rained, and 0 otherwise.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些数据，我们希望推断哪些变量与降雨相关。为此，我们将建立一个统计模型。除了之前的数据，我们还记录了一个二元降雨变量，如果下雨则取值1，反之取值0。
- en: 'Now, we pose a set of assumptions in relation to the data:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对数据做出一组假设：
- en: Rain probability has a Bernoulli distribution.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降雨概率遵循伯努利分布。
- en: Rain probability depends on data we have collected. In other words, there is
    a relationship between the data and rain probability.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降雨概率取决于我们收集到的数据。换句话说，数据与降雨概率之间存在某种关系。
- en: 'You may find it strange thinking about rain in terms of probability. What does
    it mean to say that last Wednesday, the probability of rain was 45%? Last Wednesday
    is a past date, so we can examine the data and check whether there was rain. The
    trick is to understand that in our dataset, there are many days similar to Wednesday.
    Let''s suppose that we have collected the following values:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能觉得以概率的方式思考降雨很奇怪。那么，如何理解说上周三降雨的概率是45%呢？上周三是过去的日期，所以我们可以检查数据，看看是否下雨。关键在于理解，在我们的数据集中，有许多类似周三的天数。假设我们收集到了以下值：
- en: '| **Day of week** | **Speed of wind** | **Humidity** | **Temperature** | **Outcome**
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| **星期几** | **风速** | **湿度** | **温度** | **结果** |'
- en: '| Monday | 5 m/s | 50% | 30 C | no rain |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 周一 | 5 m/s | 50% | 30°C | 不下雨 |'
- en: '| Tuesday | 10 m/s | 80% | 25 C | rain |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 周二 | 10 m/s | 80% | 25°C | 下雨 |'
- en: '| Wednesday | 5 m/s | 52% | 28 C | rain |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 周三 | 5 m/s | 52% | 28°C | 下雨 |'
- en: '| Thursday | 3 m/s | 30% | 23 C | no rain |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 周四 | 3 m/s | 30% | 23°C | 不下雨 |'
- en: '| Friday | 8 m/s | 35% | 27 C | no rain |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 周五 | 8 m/s | 35% | 27°C | 不下雨 |'
- en: In this example, Monday and Wednesday are very similar, but their rain outcomes
    are different. In a sufficiently large dataset, we could find two rows that match
    exactly but have different outcomes. Why is this happening? First, our dataset
    does not include all the possible variables that can describe rain. It is impossible
    to collect such a dataset, so we make an assumption that our data is related to
    rain, but does not describe it fully. Measurement errors, the randomness of events,
    and incomplete data make rain probabilistic. You may wonder if rain is probabilistic
    in nature? Or is every period of rain predetermined? To check whether rain events
    are deterministic, we must collect a daily snapshot of the complete state of the
    universe, which is impossible. Statistics and probability theory help us to understand
    our world even if we have imperfect information. For example, imagine that we
    have 10 days similar to last Wednesday in our dataset. By similar, I mean that
    all variables we have collected differ only by a small amount. Out of those 10
    days, 8 were rainy and 2 were sunny. We may say that on a day typical to last
    Wednesday there is an 80% probability of rain. That is the most accurate answer
    we can give using this data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，周一和周三非常相似，但它们的降雨结果不同。在足够大的数据集中，我们可能会找到两行数据完全相同但结果不同。为什么会这样？首先，我们的数据集并没有包括所有可以描述降雨的变量。收集到这样一个数据集是不可能的，因此我们假设我们的数据与降雨相关，但并没有完全描述降雨。测量误差、事件的随机性以及不完全的数据使得降雨具有概率性。你可能会想，降雨本质上是概率性的？还是每一场降雨都是预定的？为了验证降雨事件是否是确定性的，我们必须收集一个完整的宇宙状态的每日快照，这显然是不可能的。统计学和概率论帮助我们理解这个世界，即使我们只有不完全的信息。例如，假设我们在数据集中有10天和上周三相似。所谓相似，意味着我们收集到的所有变量只有很小的差异。在这10天中，有8天下雨，2天晴天。我们可以说，在类似上周三的日子里，降雨的概率是80%。这是我们使用这些数据能够给出的最准确的答案。
- en: Having assumptions about data in place, we can proceed to modeling. We can make
    another assumption that there exists some mathematical model **M**, that uses
    data to estimate rain probability. That is, model **M** uses data **d** to learn
    the relationship between the data and rain probability. The model will infer this
    relationship by assigning rain probabilities that are closest to real outcomes
    in our dataset.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在做出数据假设后，我们可以进行建模。我们可以假设存在一个数学模型**M**，它使用数据来估计降雨概率。也就是说，模型**M**使用数据**d**来学习数据与降雨概率之间的关系。该模型将通过给出最接近我们数据集中实际结果的降雨概率来推断这种关系。
- en: The main goal of model **M** is not to make accurate predictions, but to find
    and explain relationships. This is where we can draw a line between statistics
    and machine learning. Machine learning seeks to find accurate predictive models,
    while statistics uses models to find explanations and interpretations. Goals differ,
    but the underlying concepts that allow models to learn from data are the same.
    Now, we can finally uncover how this model **M** can learn from data. We will
    disentangle the magic, leaving a straightforward understanding of the mathematics
    behind machine learning.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 模型**M**的主要目标不是做出准确的预测，而是找到并解释关系。这就是我们可以区分统计学和机器学习的地方。机器学习旨在找到准确的预测模型，而统计学则使用模型来找到解释和理解。目标不同，但让模型从数据中学习的基本概念是相同的。现在，我们终于可以揭示模型**M**是如何从数据中学习的。我们将解开其中的“魔法”，并留下机器学习背后数学的直观理解。
- en: How do machines learn?
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器是如何学习的？
- en: How do algorithms learn? How can we define learning? As humans, we learn a lot
    throughout our lives. It is a natural task for us. In the first few years of our
    lives, we learn how to control our body, walk, speak, and recognize different
    objects. We constantly get new experiences, and these experiences change the way
    we think, behave, and act. Can a piece of computer code learn like we do? To approach
    machine learning, we first need to come up with a way to transmit experience directly
    to the algorithm.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 算法是如何学习的？我们如何定义学习？作为人类，我们一生中学习了很多东西。这是我们的一项自然任务。在生命的最初几年，我们学会了如何控制自己的身体、走路、说话以及识别不同的物体。我们不断获得新的经验，这些经验改变了我们的思维、行为和行动方式。计算机代码能像我们一样学习吗？为了接近机器学习，我们首先需要找到一种将经验直接传递给算法的方法。
- en: 'In practical cases, we are interested in teaching algorithms to perform all
    kinds of specific tasks faster, better, and more reliably that we can do ourselves.
    For now, we will focus on prediction and recognition tasks. Thus, we want to build
    algorithms that are able to recognize patterns and predict future outcomes. The
    following table shows some examples of recognition and prediction tasks:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际情况下，我们希望教算法执行各种特定任务，比我们自己做得更快、更好、更可靠。目前，我们将专注于预测和识别任务。因此，我们希望构建能够识别模式并预测未来结果的算法。下表展示了预测和识别任务的一些例子：
- en: '| Recognition tasks | Is this a high-paying customer?How much does this house
    cost in the current market?What are those objects in an image? |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 识别任务 | 这是不是一个高收入的客户？这座房子在当前市场上的价格是多少？图像中的那些物体是什么？ |'
- en: '| Prediction tasks | Is this customer likely to return his debt in the next
    6 months?How much will we sell in the next quarter?How risky is this investment?
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 预测任务 | 这个客户在未来6个月内可能会偿还债务吗？我们下个季度将销售多少？这个投资有多大风险？ |'
- en: The first idea may be to approach learning as humans do, and provide explanations
    and examples through speech, images, and sets of examples. Unfortunately, while
    learning in this way, we perform many complex cognitive tasks, such as listening,
    writing, and speaking. A computer algorithm by itself cannot collect new experiences
    the way we do. What if, instead, we take a simplified model of our world in the
    form of digital data? For example, predicting customer churn for Acme Co could
    be done only using data about customer purchases and product ratings. The more
    complete and full the dataset is, the more accurate the model of customer churn
    is likely to be.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个想法可能是像人类一样进行学习，并通过语言、图像和示例集提供解释和示例。不幸的是，在这种方式下学习时，我们需要执行许多复杂的认知任务，例如听、写和说。单靠计算机算法无法像我们一样收集新的经验。那么，如果我们用数字数据的简化模型来表示我们的世界呢？例如，预测Acme公司客户流失的任务可以仅使用关于客户购买和产品评级的数据来完成。数据集越完整，客户流失模型的准确性就越高。
- en: 'Let''s look at another example. We will build a machine learning project cost
    estimator. This model will use the attributes of a project to calculate the cost
    estimate. Suppose that we have collected the following data attributes for each
    project in our company:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看另一个例子。我们将建立一个机器学习项目成本估算器。这个模型将使用项目的属性来计算成本估算。假设我们已经收集了公司中每个项目的以下数据属性：
- en: '| **Attribute name** | **Attribute type** | **Attribute description** | **Possible
    values** |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| **属性名称** | **属性类型** | **属性描述** | **可能的值** |'
- en: '| Number of attributes | Integer | Number of data attributes in the project
    dataset | 0 to ∞ |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 属性数量 | 整数 | 项目数据集中数据属性的数量 | 0 到 ∞ |'
- en: '| Number of data scientists | Integer | Number of data scientists requested
    by the customer for project implementation | 0 to ∞ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 数据科学家数量 | 整数 | 客户为项目实施要求的数据科学家数量 | 0 到 ∞ |'
- en: '| Integration | Integer | Integration with customer''s software systems requested
    by the customer | 0 for no integration in project scope1 for integration in project
    scope |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 集成 | 整数 | 客户要求的与客户软件系统的集成 | 0 表示项目范围内没有集成，1 表示项目范围内有集成 |'
- en: '| Is a large company | Integer | Indicates if the customer has a large number
    of employees | 0 = customer''s company employee number greater than 1001 = customer''s
    company employee number less or equal to 100 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 是否为大型公司 | 整数 | 表示客户是否拥有大量员工 | 0 = 客户公司员工人数大于1001，1 = 客户公司员工人数少于或等于100 |'
- en: '| Total project cost | Integer | Total cost in USD | 0 to ∞ |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 总项目成本 | 整数 | 总成本（单位：美元） | 0 到 ∞ |'
- en: 'The example dataset containing these attributes is provided in the following
    table:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 含有这些属性的示例数据集如下表所示：
- en: '| **Number of attributes** | **Number of data scientists** | **Integration**
    | **Is a large company** | **Total project cost** |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **属性数量** | **数据科学家数量** | **集成** | **是否为大型公司** | **总项目成本** |'
- en: '| 10 | 1 | 1 | 0 | 135,000 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 1 | 1 | 0 | 135,000 |'
- en: '| 20 | 1 | 0 | 1 | 140,000 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 1 | 0 | 1 | 140,000 |'
- en: '| 5 | 2 | 1 | 0 | 173,200 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 2 | 1 | 0 | 173,200 |'
- en: '| 100 | 3 | 1 | 1 | 300,000 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 3 | 1 | 1 | 300,000 |'
- en: 'The simplest model we can imagine is a so-called linear model. It sums data
    attributes multiplied by variable coefficients to calculate the project cost estimate:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能想象的最简单模型是所谓的线性模型。它将数据属性与变量系数相乘后求和，以计算项目成本估算值：
- en: '![](img/4e5bca27-1c7f-4532-ac51-06629d71190b.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e5bca27-1c7f-4532-ac51-06629d71190b.png)'
- en: 'In this simplified scenario, we do not know the real values of cost variables.
    However, we can use statistical methods and estimate them from data. Let''s start
    with a random set of parameters:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简化的场景中，我们并不知道成本变量的真实值。然而，我们可以使用统计方法，并从数据中进行估算。让我们从一组随机参数开始：
- en: Base cost = 50,000
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本成本 = 50,000
- en: Cost per data attribute = 115
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个数据属性的成本 = 115
- en: Cost per data scientist = 40,000
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个数据科学家的成本 = 40,000
- en: Integration cost = 50,000
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成成本 = 50,000
- en: Customer relation complexity cost = 5,000
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户关系复杂度成本 = 5,000
- en: 'If we use the parameters for every project we have in our dataset, we will
    get the following results:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用数据集中每个项目的参数，我们将得到以下结果：
- en: '***Total project 1 cost = 50,000 + 115 x 10 + 40,000 x 1 + 50,000 x 1 + 50,000
    x 0 = 141,150***'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '***总项目1成本 = 50,000 + 115 x 10 + 40,000 x 1 + 50,000 x 1 + 50,000 x 0 = 141,150***'
- en: '***Total project 2 cost = 50,000 + 115 x 20 + 40,000 x 1 + 50,000 x 0 + 50,000
    x 1 = 142,300***'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '***总项目2成本 = 50,000 + 115 x 20 + 40,000 x 1 + 50,000 x 0 + 50,000 x 1 = 142,300***'
- en: '***Total project 3 cost = 50,000 + 115 x 5 + 40,000 x 2 + 50,000 x 1 + 50,000
    x 0 = 180,575***'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '***总项目3成本 = 50,000 + 115 x 5 + 40,000 x 2 + 50,000 x 1 + 50,000 x 0 = 180,575***'
- en: '***Total project 4 cost = 50,000 + 115 x 100 + 40,000 x 3 + 50,000 x 1 + 50,000
    x 1 = 281,500***'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '***总项目4成本 = 50,000 + 115 x 100 + 40,000 x 3 + 50,000 x 1 + 50,000 x 1 = 281,500***'
- en: 'You have probably noticed that the values differ from the real project costs
    in our dataset. This means that if we use this model on any real project our estimates
    will be erroneous. We can measure this error in multiple ways, but let''s stick
    with one of the most popular choices:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，这些值与我们数据集中真实项目的成本有所不同。这意味着，如果我们将此模型应用于任何真实项目，我们的估算将会有误。我们可以通过多种方式来衡量这个误差，但让我们选择其中最受欢迎的一种方法：
- en: '![](img/8a0ef489-b139-450a-b4af-da5d63b5b94c.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a0ef489-b139-450a-b4af-da5d63b5b94c.png)'
- en: There are many ways to quantify prediction errors. All of them introduce different
    trade-offs and limitations. Error measure selection is one of the most important
    technical aspects of building a machine learning model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多方法可以量化预测误差。它们每种方法都引入了不同的权衡和限制。误差度量的选择是构建机器学习模型时最重要的技术之一。
- en: For the overall error, let's take an arithmetic average of individual errors
    of all projects. The number that we calculated is called the **root mean squared
    error** (**RMSE**).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对于整体误差，我们可以取所有项目的单个误差的算术平均值。我们计算出的这个数字叫做**均方根误差**（**RMSE**）。
- en: The exact mathematical form of this measure is not a consequence. RMSE has straightforward
    logic behind it. While we can derive the RMSE formula using several technical
    constraints posed on a linear model, mathematical proofs are out of the scope
    of this book.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 该度量的确切数学形式并不重要。RMSE背后有直接的逻辑。虽然我们可以通过对线性模型施加若干技术约束来推导RMSE公式，但数学证明不在本书的范围内。
- en: It turns out that we can use optimization algorithms to tweak our cost parameters
    so that RMSE will be minimized. In other words, we can find the best fit for cost
    parameters that minimize the error for all rows in the dataset. We call this procedure
    MLE.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，我们可以使用优化算法调整我们的成本参数，以使得 RMSE 最小化。换句话说，我们可以找到最适合的成本参数，使得数据集中的所有行的误差最小。我们称这个过程为最大似然估计（MLE）。
- en: 'MLE gives a way to estimate the parameters of a statistical model given data.
    It seeks to maximize the probability of parameters given data. It may sound difficult,
    but the concept becomes very intuitive if we rephrase the definition as a question:
    what parameters should we set so the results we get will be closest to the data?
    MLE helps us to find the answer to this question.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最大似然估计（MLE）提供了一种估计统计模型参数的方法，给定数据。它的目标是最大化给定数据下参数的概率。虽然听起来可能有些困难，但如果我们将定义重新表述为一个问题：我们应该设置什么参数，使得我们得到的结果最接近数据？MLE帮助我们找到这个问题的答案。
- en: 'Let''s focus on another example to get a more general approach. Imagine that
    we have started a coffee subscription service. A customer chooses her favorite
    flavors of coffee in our mobile app and fills in an address and payment information.
    After that, our courier delivers a hot cup of coffee every morning. There is a
    feedback system built in the app. We promote seasonal offerings and discounts
    to clients via push notifications. There was a big growth in subscribers last
    year: almost 2,000 people are already using the service and 100 more are subscribing
    each month. However, our customer churn percentage is growing disturbingly fast.
    Marketing offers do not seem to make a big difference. To solve this problem,
    we have decided to build a machine learning model that predicts customer churn
    in advance. Knowing that a customer will churn, we can tailor an individual offer
    that will turn them into active user again.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们专注于另一个例子，来获得一个更普遍的方法。假设我们开始了一个咖啡订阅服务。客户在我们的手机应用程序中选择她最喜欢的咖啡口味，并填写地址和支付信息。之后，我们的快递员每天早晨为她送上一杯热咖啡。应用程序中内置了反馈系统。我们通过推送通知向客户推广季节性优惠和折扣。去年的订阅用户增长了很多：现在有将近
    2,000 人在使用该服务，每月还有 100 多人订阅。然而，我们的客户流失率正在快速增长，令人担忧。营销优惠似乎没有产生太大的效果。为了解决这个问题，我们决定建立一个机器学习模型，提前预测客户流失。通过知道客户将流失，我们可以定制个性化的优惠，将他们重新转化为活跃用户。
- en: This time, we will be more rigorous and abstract in our definitions. We will
    define a model **M** that takes customer data **X** and historical churn outcomes
    **Y**. We will call **Y** the target variable.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们将在定义上更加严谨和抽象。我们将定义一个模型**M**，该模型接受客户数据**X**和历史流失结果**Y**。我们将**Y**称为目标变量。
- en: 'The following table describes the attributes of our dataset:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格描述了我们数据集的属性：
- en: '| **Attribute name** | **Attribute type** | **Attribute description** | **Possible
    values** |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| **属性名称** | **属性类型** | **属性描述** | **可能的值** |'
- en: '| Months subscribed | Integer | The number of months a user has been subscribed
    to our service | 0 to ∞ |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 订阅月数 | 整数 | 用户已订阅我们服务的月数 | 0 到 ∞ |'
- en: '| Special offerings activated | Integer | The number of special offers the
    user activated last month | 0 to ∞ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 启动的特殊优惠 | 整数 | 用户上个月激活的特殊优惠数量 | 0 到 ∞ |'
- en: '| Number of cups on weekdays | Float | The average number of cups the user
    orders on weekdays, last month | 1.0 to 5.0 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 工作日订购杯数 | 浮动数 | 用户上个月在工作日平均订购的杯数 | 1.0 到 5.0 |'
- en: '| Number of cups on the weekend | Float | The average number of cups the user
    orders on weekends, last month | 1.0 to 2.0 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 周末订购杯数 | 浮动数 | 用户上个月在周末平均订购的杯数 | 1.0 到 2.0 |'
- en: This kind of table is called a data dictionary. We can use it to understand
    the data coming in and out of the model, without looking into the code or databases.
    Every data science project must have an up-to-date data dictionary. More complete
    examples of data dictionaries will be shown later in this book.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表格被称为数据字典。我们可以利用它来了解模型中输入输出的数据，而无需查看代码或数据库。每个数据科学项目都必须有一个最新的数据字典。本书稍后将展示更完整的数据字典示例。
- en: 'Our target variable, **Y**, can be described in the following way:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标变量**Y**可以通过以下方式描述：
- en: '| **Target variable name** | **Target variable type** | **Target variable description**
    | **Target variable possible values** |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| **目标变量名称** | **目标变量类型** | **目标变量描述** | **目标变量可能的值** |'
- en: '| Churn | Integer | Indicates whether the user stopped using our service last
    month | 0 or 1 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 用户流失 | 整数 | 指示用户是否在上个月停止使用我们的服务 | 0 或 1 |'
- en: 'Given the customer description, ![](img/47db3c92-704a-4938-a88d-aededf2bbb93.png),
    the model outputs churn probability, ![](img/0afcd35b-e59e-4f4d-93c6-8d4a7542ad7f.png).
    A hat over ![](img/e52dbafb-bbd9-49d6-b4b2-c433dcdc1282.png) means that ![](img/0afcd35b-e59e-4f4d-93c6-8d4a7542ad7f.png)
    is not a real churn probability, but only an estimate that can contain errors.
    This value will not be strictly zero or one. Instead, the model will output a
    probability between 0% and 100%. For example, for some customer ![](img/47db3c92-704a-4938-a88d-aededf2bbb93.png),
    we got a ![](img/0afcd35b-e59e-4f4d-93c6-8d4a7542ad7f.png) of 76%. We may interpret
    this value like this: based on historical data, the expectancy of this customer
    to churn is 76%. Or, out of 100 customers like customer ![](img/30287e69-02b1-4fbb-9a6d-527793dd6d66.png), 76
    will churn.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 给定客户描述 ![](img/47db3c92-704a-4938-a88d-aededf2bbb93.png)，模型输出流失概率 ![](img/0afcd35b-e59e-4f4d-93c6-8d4a7542ad7f.png)。在
    ![](img/e52dbafb-bbd9-49d6-b4b2-c433dcdc1282.png) 上加帽子表示 ![](img/0afcd35b-e59e-4f4d-93c6-8d4a7542ad7f.png)
    不是一个真实的流失概率，而只是一个可能包含误差的估计值。这个值不会严格等于零或一。相反，模型会输出一个介于 0% 和 100% 之间的概率。例如，对于某个客户
    ![](img/47db3c92-704a-4938-a88d-aededf2bbb93.png)，我们得到了一个 76% 的流失概率 ![](img/0afcd35b-e59e-4f4d-93c6-8d4a7542ad7f.png)。我们可以这样解读这个值：根据历史数据，该客户的流失预期为
    76%。或者，100 个像客户 ![](img/30287e69-02b1-4fbb-9a6d-527793dd6d66.png) 的客户中，将有 76 个客户流失。
- en: A machine learning model must have some variable parameters that can change
    to better match churn outcomes. Now that we have used formulas, we can't go on
    without introducing at least one Greek letter. All the parameters of our model
    will be represented by ![](img/3457eb94-12b2-4edd-ab4e-622dec683370.png).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器学习模型必须具有一些可变的参数，这些参数可以调整以更好地匹配流失结果。现在我们已经使用了公式，我们不能继续进行而不引入至少一个希腊字母。我们模型的所有参数将由
    ![](img/3457eb94-12b2-4edd-ab4e-622dec683370.png) 表示。
- en: 'Now, we have everything in place:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们一切准备就绪：
- en: Historical customer data ![](img/e5c22cd2-7bee-4f0f-9ab5-4a14b5f72c4a.png) and
    churn outcomes ![](img/b9742446-b92d-4048-988c-bd56185c6a35.png), which we will
    call the training dataset ![](img/03798ff0-0bc9-462d-acdc-7444803b369b.png)
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 历史客户数据 ![](img/e5c22cd2-7bee-4f0f-9ab5-4a14b5f72c4a.png) 和流失结果 ![](img/b9742446-b92d-4048-988c-bd56185c6a35.png)，我们将其称为训练数据集
    ![](img/03798ff0-0bc9-462d-acdc-7444803b369b.png)。
- en: Machine learning algorithm ![](img/5b08d2dd-01fb-4d28-90dd-af668b42920d.png) that
    accepts customer description ![](img/747c1c74-c079-4d1c-b27c-f584b284b30c.png) and
    outputs churn probability ![](img/80513bf6-2166-4fb5-b16d-ba7c898db5cb.png)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习算法 ![](img/5b08d2dd-01fb-4d28-90dd-af668b42920d.png)，接受客户描述 ![](img/747c1c74-c079-4d1c-b27c-f584b284b30c.png)，输出流失概率
    ![](img/80513bf6-2166-4fb5-b16d-ba7c898db5cb.png)。
- en: Model parameters ![](img/667c9f2f-7ffa-4be7-9f9e-9b705773a02e.png) that can
    be tuned using MLE
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数 ![](img/667c9f2f-7ffa-4be7-9f9e-9b705773a02e.png)，可以通过最大似然估计（MLE）来调整。
- en: We will estimate the parameters ![](img/25e1af35-cbdd-413b-9e89-de80eadc420a.png) for
    our model ![](img/0c1721ef-3faa-45cd-98a4-77a06950ed10.png) using MLE on the training
    dataset ![](img/8d37f0bc-1c35-40e5-9b5c-e2533fdc2bcd.png). I have placed a hat
    on top of ![](img/2ceb9466-e565-4ecc-8ac6-eeef6dc17921.png) to indicate that in
    theory there may be the best parameter set ![](img/2ceb9466-e565-4ecc-8ac6-eeef6dc17921.png),
    but in reality we have limited data. Thus, the best parameter set we can get is
    only an estimate of true that may contain errors.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用MLE在训练数据集 ![](img/8d37f0bc-1c35-40e5-9b5c-e2533fdc2bcd.png) 上估计我们模型的参数
    ![](img/25e1af35-cbdd-413b-9e89-de80eadc420a.png) 。我在 ![](img/2ceb9466-e565-4ecc-8ac6-eeef6dc17921.png)
    上加了一个帽子，表示理论上可能存在最优的参数集 ![](img/2ceb9466-e565-4ecc-8ac6-eeef6dc17921.png)，但实际上我们有数据限制。因此，我们能得到的最佳参数集只是一个可能包含误差的真实估计。
- en: Now, we can finally use our model to make predictions about customer churn
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们终于可以使用我们的模型来预测客户流失。
- en: 'probability ![](img/b8b4a596-2252-4c58-9284-940b53d5864b.png):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 概率 ![](img/b8b4a596-2252-4c58-9284-940b53d5864b.png)：
- en: '![](img/af320444-5ea3-465c-b4ac-ac4cefe58fe4.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af320444-5ea3-465c-b4ac-ac4cefe58fe4.png)'
- en: The exact interpretation of probability depends heavily on the model **M** we
    used to estimate this probability. Some models can be used to give probabilistic
    interpretations, while others do not have such qualities.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 概率的确切解释在很大程度上依赖于我们用来估计该概率的模型 **M**。一些模型可以用来给出概率解释，而其他模型则没有这样的特性。
- en: Note that we had not explicitly defined the kind of machine learning model **M** we
    use. We have defined an abstract framework for learning from data that does not
    depend on the specific data or concrete algorithms it uses. This is the beauty
    of mathematics that opens up limitless practical applications. With this abstract
    framework, we can come up with many models **M** that have different trade-offs
    and capabilities. This is how machines learn.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们没有明确地定义我们使用的机器学习模型**M**的类型。我们定义了一个从数据中学习的抽象框架，它不依赖于特定的数据或具体的算法。这就是数学的魅力，它打开了无限的实际应用。通过这个抽象框架，我们可以提出许多具有不同权衡和能力的模型**M**。这就是机器如何学习的方式。
- en: '**How to choose a model**'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何选择模型**'
- en: There are many different types of machine learning models and many estimation
    methods. Linear regression and MLE are among the simplest examples that show the
    underlying principles that lie beneath many machine learning models. A theorem
    called the **no free lunch theorem** says that there is no model that will give
    you the best results for each task for every dataset. Our machine learning framework
    is abstract, but this does not mean it can yield a perfect algorithm. Some models
    are best for one task, but are terrible for another. One model may classify images
    better than humans do, but it will fail at credit scoring. The process of choosing
    the best model for a given task requires deep knowledge of several disciplines,
    such as machine learning, statistics, and software engineering. It depends on
    many factors, such as statistical data properties, the type of task we are trying
    to solve, business constraints, and risks. That is why only a professional data
    scientist can handle the selection and training machine of learning models. This
    process has many intricacies and explaining all of them is beyond the scope of
    this book. An interested reader may refer to the book list at the end of this
    book. There you can find free books that explain the technical side of machine
    learning in great depth.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型有很多种类型，估计方法也各不相同。线性回归和最大似然估计（MLE）是最简单的例子之一，它们展示了许多机器学习模型背后的基本原理。有一个叫做**无免费午餐定理**的定理，指出没有任何一个模型能在每个任务和每个数据集上都给出最佳结果。我们的机器学习框架是抽象的，但这并不意味着它能产生完美的算法。有些模型适合某些任务，但在其他任务中表现糟糕。一个模型可能在图像分类上比人类做得更好，但它在信用评分上却会失败。选择最适合某个任务的最佳模型的过程需要深厚的知识，涉及多个学科，如机器学习、统计学和软件工程。它依赖于许多因素，如统计数据特性、我们试图解决的任务类型、商业约束和风险。这就是为什么只有专业的数据科学家才能处理机器学习模型的选择和训练。这个过程涉及很多复杂细节，解释所有这些内容超出了本书的范围。有兴趣的读者可以参考本书末尾的书单，在那里你可以找到一些免费的书籍，详细解释机器学习的技术细节。
- en: Exploring machine learning
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索机器学习
- en: Now that you understand the general flow of thought on how to define learning
    processes using mathematics and statistics, we can explore the inner workings
    of machine learning. Machine learning studies algorithms and statistical models
    that are able to learn and perform specific tasks without explicit instruction.
    As every software development manager should have some expertise in computer programming,
    the data science project manager should understand machine learning. Grasping
    the underlying concepts between any machine learning algorithm will allow you
    to understand better the limitations and requirements for your project. It will
    ease communication and improve understanding between you and the data scientists
    on your team. Knowledge of basic machine learning terminology will make you speak
    in the language of data science.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了如何使用数学和统计学来定义学习过程的一般思路，我们可以深入探索机器学习的内部工作原理。机器学习研究的是能够在没有明确指令的情况下学习并执行特定任务的算法和统计模型。正如每个软件开发经理应该具备一定的计算机编程专业知识一样，数据科学项目经理也应该理解机器学习。掌握任何机器学习算法背后的基本概念将帮助你更好地理解项目的局限性和需求。这将有助于你与团队中的数据科学家之间的沟通与理解。了解基本的机器学习术语将使你能够用数据科学的语言进行交流。
- en: We will now dive into the main intuitions behind popular machine learning algorithms,
    leaving out technical details for the sake of seeing the wood for the trees.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将深入探讨流行机器学习算法背后的主要直觉，为了突出重点，我们将省略技术细节。
- en: Defining goals of machine learning
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义机器学习的目标
- en: When we speak about machine learning we speak about accurate predictions and
    recognition. Statisticians often use simple but interpretable models with a rigorous
    mathematical base to explain data and prove points. Machine learning specialists
    build more complex models that are harder to interpret and often work like black
    boxes. Thus, many machine learning algorithms are more suited to prediction quality
    than model interpretability. Trends change slowly, and while more researchers
    look into topics of model interpretation and prediction explanation, the prime
    goal of machine learning continues to be the creation of faster and more accurate
    models.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论机器学习时，我们谈论的是准确的预测和识别。统计学家通常使用简单但可解释的模型，并以严格的数学基础来解释数据并证明观点。机器学习专家则构建更复杂的模型，这些模型较难解释，通常像黑盒一样工作。因此，许多机器学习算法更注重预测质量而非模型可解释性。趋势变化缓慢，尽管越来越多的研究人员开始研究模型解释和预测说明的主题，机器学习的主要目标仍然是创建更快、更准确的模型。
- en: Using a life cycle to build machine learning models
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用生命周期来构建机器学习模型
- en: 'When creating machine learning models, we typically follow a fixed set of stages:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建机器学习模型时，我们通常遵循一组固定的阶段：
- en: '**Exploratory data analysts**:In this stage, a data scientist uses a set of
    statistical and visualization techniques to have a better understanding of data.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索性数据分析**：在这一阶段，数据科学家使用一系列统计和可视化技术，以更好地理解数据。'
- en: '**Data preparation**: In this part, a data scientist transforms data into a
    format suitable for applying a machine learning algorithm.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据准备**：在此部分，数据科学家将数据转换为适合应用机器学习算法的格式。'
- en: '**Data preprocessing**: Here, we clean prepared data and transform it so that
    a machine learning algorithm can properly use every part of the data.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据预处理**：在此阶段，我们清洗已准备好的数据并进行转换，以便机器学习算法能够正确使用数据的每一部分。'
- en: '**Modeling**: In this part, a data scientist trains machine learning models.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建模**：在这一部分，数据科学家训练机器学习模型。'
- en: '**Testing**: In this stage, we evaluate the model using a set of metrics that
    measure its performance.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试**：在此阶段，我们使用一组衡量模型性能的指标来评估模型。'
- en: This process repeats many times before we achieve sufficiently good results.
    You can apply the life cycle to train many kinds of machine learning models, which
    we will explore next.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程会重复多次，直到我们达到足够好的结果。你可以将生命周期应用于训练多种机器学习模型，接下来我们将探索这些模型。
- en: Linear models
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性模型
- en: The most basic type of machine learning models is a linear model. We have already
    seen a detailed example of a linear model in the previous section. Predictions
    of a linear model can be interpreted by looking at coefficients of a model. The
    greater the coefficient, the more its contribution to the final prediction. While
    simple, those models are often not the most accurate. Linear models are fast and
    computationally efficient, which makes them valuable in settings with lots of
    data and limited computational resources.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的机器学习模型类型是线性模型。我们在上一部分已经详细介绍了线性模型的示例。线性模型的预测可以通过查看模型的系数来解释。系数越大，它对最终预测的贡献越大。虽然简单，但这些模型往往不是最准确的。线性模型快速且计算效率高，这使得它们在数据量大且计算资源有限的环境中非常有价值。
- en: Linear models are fast, efficient, simple, and interpretable. They can solve
    both classification and regression problems.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型快速、高效、简单且可解释。它们可以解决分类和回归问题。
- en: Classification and regression trees
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类和回归树
- en: '**Classification and regression tree** (**CART**) take a very intuitive approach
    for making predictions. CART build a decision tree based on the training data.
    If we use CART for a credit default risk task, we may see a model like this:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类与回归树** (**CART**) 采用一种非常直观的预测方法。CART根据训练数据构建决策树。如果我们将CART用于信用违约风险任务，我们可能会看到这样的模型：'
- en: '![](img/5bd3570c-9d49-4528-8331-e3d564709153.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bd3570c-9d49-4528-8331-e3d564709153.png)'
- en: To make a prediction, an algorithm starts from the top of the tree and makes
    consecutive choices based on values in the data. For binary classification, at
    the bottom of the tree you will get a proportion of positive cases for similar
    customers.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行预测，算法从树的顶部开始，并根据数据中的值做出连续的选择。对于二元分类，树的底部会显示类似客户的正例比例。
- en: 'While simple, CART models suffer from two disadvantages:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管简单，CART模型也有两个缺点：
- en: Low prediction accuracy.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低预测准确度。
- en: There are many possible trees for a single dataset. One tree may have significantly
    better prediction accuracy than the other.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一个数据集，可能有许多不同的树。某一棵树的预测精度可能远远优于其他树。
- en: 'But how does CART choose columns and values for splits? We will explore the
    general logic of CART on binary classification:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 但CART如何选择列和数值进行分裂呢？我们将探索CART在二分类中的一般逻辑：
- en: At first, it takes a single column and divides the data into two parts for each
    value of this column.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，它选取一列数据，并根据该列的每个值将数据分为两部分。
- en: Then it computes a proportion of positive cases for each split.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它计算每次分裂中正例的比例。
- en: '*Step 1* and *step 2* are repeated for each column in the data.'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤1*和*步骤2*对数据中的每一列都会重复进行。'
- en: We rank each split by how well it divides the data. If the split divides the
    dataset perfectly, then positive cases would be for all values, lower than a threshold
    and negative cases would be on the other side. To illustrate, if **Age > 25** is
    a perfect split, then all customers younger than 25 will have credit defaults
    and all customers older than 25 will have a perfect credit history.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据每次分裂对数据的划分效果对其进行排名。如果分裂能够完美划分数据，那么正例将完全出现在某一侧，且低于某个阈值，负例则出现在另一侧。举例来说，如果**年龄
    > 25**是一个完美的分裂，那么所有年龄小于25的客户将会有信用违约，而所有年龄大于25的客户将拥有完美的信用历史。
- en: According to *step 4*, the best split is chosen for the current level of the
    tree. The dataset is divided into two parts according to the split value.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据*步骤4*，为当前树的层级选择最佳的分裂。数据集根据分裂值被分为两部分。
- en: '*Steps 1* to *5* are repeated for each new dataset part.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一个新的数据集部分，*步骤1*到*步骤5*都会重复执行。
- en: The procedure continues before the algorithm meets a stopping criterion. For
    example, we can stop building a tree by looking at the depth of the decision tree
    or the minimum number of data points available for the next split.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该过程会持续进行，直到算法达到停止标准。例如，我们可以通过查看决策树的深度或用于下一次分裂的数据点最小数量来停止建树。
- en: We can also apply CART to regression problems, although the algorithm would
    be slightly more complicated. CART is simple and interpretable, but it produces
    very weak models that are rarely applied in the practice. However, the properties
    of the algorithm and implementation tricks allow us to use their weakest point
    as their main strength. We will learn how to exploit these properties in the following
    section.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将CART应用于回归问题，尽管算法会稍微复杂一些。CART简单且易于解释，但它生成的模型非常弱，很少在实际中应用。然而，该算法的特性和实现技巧使得我们能够将其最弱的部分转化为其主要优势。我们将在接下来的部分学习如何利用这些特性。
- en: Ensemble models
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成模型
- en: Suppose that you own a retail store franchise. Business is growing and you are
    ready to build a new store. The question is, where should you build it? Selection
    of a building location is extremely important as it is permanent and it defines
    the local customer base that will go into your shop.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你拥有一家零售店的特许经营。业务正在增长，你准备开设一家新店。问题是，应该在哪个地方开设新店呢？选择店铺位置极为重要，因为它是永久性的，并且决定了将进入你店铺的当地顾客群体。
- en: 'You have several options to make this decision:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你有几种选择来做出这个决策：
- en: Decide yourself.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自己决定。
- en: Ask for the advice of the most competent employee.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向最有能力的员工请教建议。
- en: Ask the opinion of many slightly less experienced employees.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向许多稍微经验不足的员工请教意见。
- en: '*Options 1* and *2* encompass one and two persons that make a decision. *Option
    3* encompasses opinions of several of experts. Statistically, *option 3* is likely
    to yield a better decision. Even world-class experts can make a mistake. Several
    professionals, sharing information between each other, are much more likely to
    succeed. That is the reason why living in big communities and working in large
    organizations leads to great results.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*选项1*和*2*分别代表一个或两个人作出决策。*选项3*则包括几位专家的意见。从统计学角度看，*选项3*更有可能做出更好的决策。即使是世界级的专家也可能犯错。多个专业人员之间相互共享信息，更有可能成功。这就是为什么生活在大社区中和在大型组织中工作能够取得更好成果的原因。'
- en: In machine learning, this principle works too. Many models can contribute to
    making a single decision in an ensemble. Model ensembles tend to be more accurate
    than single models, including the most advanced ones. Be wary, though; you need
    to build many models to create an ensemble. A large number of models increase
    computational resource requirements quite rapidly, making a tradeoff between prediction
    accuracy and speed.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，这一原则同样适用。许多模型可以共同做出一个决策，从而形成集成模型。与单一模型相比，模型集成往往更为准确，包括最先进的模型。然而需要小心的是，你需要构建多个模型来创建集成模型。大量模型会迅速增加计算资源需求，因此需要在预测准确性和速度之间做出权衡。
- en: Tree-based ensembles
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于树的集成方法
- en: A particularly useful model for ensembles is a decision tree. There is an entire
    class of machine learning models devoted to different ways of creating tree ensembles.
    This type of model is the most frequent winner of Kaggle competitions on structured
    data, so it is important to understand how it works.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一个特别有用的集成模型是决策树。实际上有一个完整的机器学习模型类别，专门用于创建不同的树集成方法。这类模型在Kaggle结构化数据竞赛中是最常获胜的，因此理解它是如何工作的非常重要。
- en: 'Trees are good candidates to build ensembles because they have high variance.
    Because of the randomness in a tree-building algorithm, every decision tree differs
    from the previous one, even if the dataset does not change. Each time we build
    a decision tree, we may come up with something that''s different from before.
    Thus, each tree will make different errors. Recall the following diagram:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是构建集成模型的好候选者，因为它们具有较高的方差。由于树构建算法中的随机性，每棵决策树与前一棵树都会有所不同，即使数据集没有发生变化。每次我们构建决策树时，可能会得出与之前不同的结果。因此，每棵树会犯不同的错误。回想以下图示：
- en: '![](img/d148d81a-041f-4c10-ac00-aee5b322721b.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d148d81a-041f-4c10-ac00-aee5b322721b.png)'
- en: It turns out that decision trees have extremely low bias and high variance.
    Imagine that many different trees make hundreds of predictions for each individual,
    creating an ensemble. What would happen if we average all predictions? We will
    be a lot closer to the real answer. When used in an ensemble, decision trees can
    handle complex datasets with high prediction accuracy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，决策树具有极低的偏差和较高的方差。想象一下，许多不同的树为每个个体做出数百个预测，从而创建了一个集成。假设我们对所有预测结果进行平均，会发生什么呢？我们将会更接近真实答案。当决策树用于集成时，它们能够处理复杂的数据集并具有较高的预测准确性。
- en: 'In the following diagram, you can see how multiple trees can create an ensemble:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，你可以看到多个树如何创建一个集成：
- en: '![](img/9686fcd0-4f5a-4686-89e7-53a94528923b.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9686fcd0-4f5a-4686-89e7-53a94528923b.png)'
- en: If you are working with structured data, be sure to try decision tree ensembles
    before jumping into other areas of machine learning, including deep learning.
    Nine times out of ten, the results will satisfy both you and your customer. Media
    often overlooks the value of this algorithm. Praise for ensemble models is rare
    to find, yet it is arguably the most commonly used algorithm family for solving
    practical applied machine learning problems. Be sure to give tree ensembles a
    chance.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在处理结构化数据，务必在进入其他机器学习领域（包括深度学习）之前尝试决策树集成模型。十有八九，结果会让你和你的客户都感到满意。媒体通常忽视这种算法的价值。集成模型鲜有赞誉，但它无疑是解决实际应用机器学习问题时最常用的算法家族。务必给树集成模型一个机会。
- en: Clustering models
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类模型
- en: Another useful application of machine learning is clustering. In contrast to
    other machine learning problems we have studied in this section, clustering is
    an unsupervised learning problem. This means that clustering algorithms can work
    with unlabeled data. To illustrate, let's look at a task that's central to marketing
    departments—customer segmentation. Coming up with marketing offers for each individual
    client may be impossible. For example, if you own a large retail store network,
    you want to apply different discounts at stores depending on customer interests
    to boost sales. To do this, marketing departments create customer segments and
    tailor marketing companies to each specific segment.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的另一个有用应用是聚类。与我们在本节中研究的其他机器学习问题不同，聚类是一个无监督学习问题。这意味着聚类算法可以处理没有标签的数据。为了说明这一点，我们来看一个营销部门的核心任务——客户细分。为每个客户制定营销策略可能是不可能的。例如，如果你经营一个大型零售店网络，你可能希望根据客户的兴趣在不同的商店应用不同的折扣，以提升销量。为此，营销部门创建客户细分，并根据每个细分市场量身定制营销活动。
- en: 'In the following diagram, you can see six customers assigned to two different
    segments:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，你可以看到六个客户被分配到两个不同的细分市场：
- en: '![](img/455c6dc0-8c82-42f8-b509-634317dc759e.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/455c6dc0-8c82-42f8-b509-634317dc759e.png)'
- en: We may automate customer segmentation by taking all purchase histories for all
    customers and applying a clustering algorithm to group similar customers together.
    The algorithm will assign each customer to a single segment, allowing you to further
    analyze those segments. While exploring data inside each segment, you may find
    interesting patterns that will give insights for new marketing offers targeted
    at this specific segment of customers.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将所有客户的购买历史记录并应用聚类算法将相似的客户分组，从而自动化客户细分。该算法会将每个客户分配到一个单独的细分市场，从而进一步分析这些细分市场。在探索每个细分市场中的数据时，您可能会发现一些有趣的模式，这些模式将为针对特定客户群体的新营销方案提供洞察。
- en: We can apply clustering algorithms to data in an ad hoc manner because they
    don't require prior labeling. However, the situation can get complicated, as many
    of the algorithms suffer from the curse of dimensionality and can't work with
    many columns in the data.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以临时的方式将聚类算法应用于数据，因为它们不需要预先标记。然而，情况可能会变得复杂，因为许多算法受到维度灾难的困扰，无法处理数据中的许多列。
- en: 'The most popular clustering algorithm is K-means. In its simplest form, the
    algorithm has only one parameter: the number of clusters to find in the data.
    K-means approaches clustering from a geometrical standpoint. Imagine each data
    row as a point in space. For us, this idea is easy for datasets with two or three
    points, but it works well beyond three dimensions. Having laid out our dataset
    in a geometric space, we can now see that some points will be closer to each other.
    K-means finds center points around which other points cluster.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的聚类算法是K-means。其最简单的形式，算法只有一个参数：要在数据中找到的簇数。K-means从几何角度进行聚类。想象每一行数据都是空间中的一个点。对于我们来说，这个概念在具有两个或三个点的数据集上非常容易理解，但它在超过三维空间时也能很好地工作。将数据集布置在几何空间中后，我们可以看到一些点会彼此更接近。K-means寻找中心点，其他点围绕这些中心点聚集。
- en: 'It does this iteratively, as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过以下迭代过程完成此任务：
- en: It takes the current cluster centers (for the first iteration, it takes random
    points).
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它采用当前的簇中心（对于第一次迭代，它选择随机点）。
- en: It goes through all data rows and assigns them to the closest cluster's center
    point.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它遍历所有数据行，并将它们分配给最接近的簇中心点。
- en: It updates cluster centers by averaging the locations of all points from *step
    2*.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它通过对*步骤 2*中的所有点的位置进行平均来更新簇中心。
- en: 'The algorithm is explained in the following diagram:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在以下图示中进行了说明：
- en: '![](img/47842a77-e1b1-48d7-a5e7-af9423a38a22.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47842a77-e1b1-48d7-a5e7-af9423a38a22.png)'
- en: At this point, we conclude the introduction to machine learning. While there
    are many more machine learning algorithms to study, describing them is beyond
    the scope of this book. I am sure you will find that knowledge about regression,
    decision trees, ensemble models, and clustering covers a surprisingly large portion
    of practical applications and will serve you well. Now we are ready to move on
    to deep learning.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经完成了机器学习的介绍。虽然还有许多其他机器学习算法需要研究，但描述它们超出了本书的范围。我相信您会发现，回归、决策树、集成模型和聚类的知识涵盖了令人惊讶的大部分实际应用，并且将对您大有帮助。现在，我们准备进入深度学习的部分。
- en: Exploring deep learning
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索深度学习
- en: Deep neural networks that classify images and play Go better than we do create
    an impression of extremely complex models whose internals are inspired by our
    own brain's structure. In fact, the central ideas behind neural networks are easy
    to grasp. While first neural networks were indeed inspired by the physical structure
    of our brain, the analogy no longer holds and the relation to physical processes
    inside the human brain is mostly historical.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络通过对图像分类和围棋游戏的表现优于我们，给人一种极其复杂模型的印象，这些模型的内部结构灵感来源于我们大脑的结构。事实上，神经网络背后的核心思想是容易理解的。虽然最初的神经网络确实受到了我们大脑物理结构的启发，但这种类比如今已经不再成立，神经网络与人类大脑内部的物理过程的关系主要是历史性的。
- en: 'To demystify neural networks, we will start with the basic building blocks:
    artificial neurons. An artificial neuron is nothing more than two mathematical
    functions. The first takes a bunch of numbers as input and combines them by using
    its internal state—weights. The second, an activation function, takes the output
    of the first and applies special transformations. The activation function tells
    us how active this neuron is to a particular input combination. In the following
    diagram, you can see how an artificial neuron converts the input to output:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 为了揭开神经网络的神秘面纱，我们将从基本的构建块开始：人工神经元。一个人工神经元不过是由两个数学函数组成。第一个函数接受一堆数字作为输入，并通过它的内部状态——权重，来组合这些数字。第二个函数是激活函数，它接受第一个函数的输出并应用特定的变换。激活函数告诉我们这个神经元对于某一特定输入组合的活跃程度。在下面的图中，你可以看到一个人工神经元是如何将输入转换为输出的：
- en: '![](img/9c3f5af2-af84-4bb6-9cc6-68693be66f4a.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c3f5af2-af84-4bb6-9cc6-68693be66f4a.png)'
- en: 'In the following diagram, we can see the plot of the most popular activation
    function:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到最流行的激活函数的图像：
- en: '![](img/5077ba7a-3bcb-48b0-9387-9512feaad043.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5077ba7a-3bcb-48b0-9387-9512feaad043.png)'
- en: If the output is less than 0, the function will output 0\. If it is greater,
    it will echo its input. Simple, isn't it? Try to come up with the name for this
    function. I know, naming is hard. Names should be simple, while conveying deep
    insights about the core concept of the thing you are naming. Of course, mathematicians
    knew this and, as we have witnessed many times before, came up with a perfect
    and crystal-clear name—**rectified linear unit** (**ReLU**). An interesting fact
    is that ReLU does not conform to basic requirements for an activation function,
    but still gives better results than other alternatives. Other activation functions
    may be better in specific situations, but none of them beat ReLU in being a sensible
    default.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出小于0，函数将输出0。如果输出大于0，它将回显其输入。简单吧？试着为这个函数起个名字。我知道，命名是困难的。名字应该简单，同时传达出事物核心概念的深刻洞察。当然，数学家们早就知道这一点，正如我们之前看到过的，提出了一个完美而清晰的名字——**修正线性单元**（**ReLU**）。一个有趣的事实是，ReLU并不符合激活函数的基本要求，但它仍然比其他替代品给出了更好的结果。其他激活函数在特定情况下可能更好，但没有一个能比ReLU更合理地作为默认选择。
- en: 'Another important activation function you need to know about is sigmoid. You
    can see it in the following screenshot:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个你需要了解的重要激活函数是sigmoid。你可以在下面的截图中看到它：
- en: '![](img/262339b8-8361-415e-9196-6a9a8132fd08.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/262339b8-8361-415e-9196-6a9a8132fd08.png)'
- en: Before ReLU came to the throne, sigmoid was a popular choice as an activation
    function. While its value as an activation function has faded, the sigmoid is
    still important for another reason. It often comes up in binary classification
    problems. If you look at the plot closely, you will find it can map any number
    to a range between 0 and 1\. This property makes sigmoid useful when modeling
    binary classification problems.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在ReLU登上王座之前，sigmoid曾是一个受欢迎的激活函数选择。尽管它作为激活函数的价值已经减弱，但sigmoid仍然因为另一个原因而重要。它经常出现在二分类问题中。如果你仔细观察图表，你会发现它能将任何数字映射到0和1之间的范围。这一特性使得sigmoid在建模二分类问题时非常有用。
- en: 'Note that we use sigmoid in binary classification problems not because it conveniently
    maps any number to something between 0 and 1\. The reason behind this useful property
    is that sigmoid, also called the logistic function, is tightly related to the
    Bernoulli probability distribution. This distribution describes events that can
    happen with probability ***p*** between 0 and 1\. For example, a Bernoulli distribution
    can describe a coin toss with ***p = 0.5*** or 50%. As you can see, any binary
    classification problem can be naturally described by the Bernoulli distribution.
    To see how, look at the following questions: *What is the probability of a client
    clicking on an ad?* *What is the probability of a client stating a default while
    being in debt?* We can model these cases as Bernoulli distributions.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在二分类问题中使用sigmoid，并不是因为它方便地将任何数字映射到0和1之间的某个值。这个有用特性的背后原因是sigmoid，也称为逻辑函数，与伯努利概率分布紧密相关。该分布描述了在概率***p***介于0和1之间的事件。例如，伯努利分布可以描述一个抛硬币事件，其***p
    = 0.5***，即50%。正如你所看到的，任何二分类问题都可以自然地通过伯努利分布来描述。要理解这一点，可以看看下面这些问题：*客户点击广告的概率是多少？*
    *客户在欠债时违约的概率是多少？* 我们可以将这些情况建模为伯努利分布。
- en: 'Now, we know the main components of an artificial neuron: weights and activation
    function. To make a neuron work, we need to take its input and combine it with
    neuron weights. To do this, we can recall linear regression. Linear regression
    models combine data attributes by multiplying each attribute to a weight and then
    summing them up. Then, apply an activation function, and you will get an artificial
    neuron. If our data row had two columns named *a* and *b*, the neuron would have
    two weights, ![](img/99978480-9cf3-4eea-aaaf-8ea1d21db412.png) and ![](img/9fa78070-be91-46b4-8591-9efde6594188.png).
    A formula for a neuron with ReLU activation is shown as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道人工神经元的主要组成部分：权重和激活函数。为了让神经元工作，我们需要将它的输入与神经元的权重结合起来。为此，我们可以回想线性回归。线性回归模型通过将每个属性与一个权重相乘并将它们相加来组合数据属性。然后，应用激活函数，你就会得到一个人工神经元。如果我们的数据行有两个列，分别叫做
    *a* 和 *b*，那么神经元将有两个权重，![](img/99978480-9cf3-4eea-aaaf-8ea1d21db412.png) 和 ![](img/9fa78070-be91-46b4-8591-9efde6594188.png)。一个具有
    ReLU 激活函数的神经元公式如下所示：
- en: '![](img/126417e1-835a-47ee-9965-2ca8eae45ded.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/126417e1-835a-47ee-9965-2ca8eae45ded.png)'
- en: Note that ![](img/ccfa7b45-6039-45f6-a898-d338ee0a57aa.png) is a special weight
    called a bias that is not tied to any input.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，![](img/ccfa7b45-6039-45f6-a898-d338ee0a57aa.png) 是一个特殊的权重，称为偏置，它与任何输入无关。
- en: 'So, an artificial neuron is just a bunch of multiplications and additions:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个人工神经元只是一些乘法和加法：
- en: '![](img/e7aa98f4-39cf-4175-8f6a-edebbe2efc87.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7aa98f4-39cf-4175-8f6a-edebbe2efc87.png)'
- en: 'Or, to give you a more concrete example of an actual calculation, take a look
    at the following:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，为了给你一个更具体的实际计算例子，看看下面的内容：
- en: '![](img/4b19e3f6-cd57-4c7f-93e9-0ea1b7d8ec5c.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b19e3f6-cd57-4c7f-93e9-0ea1b7d8ec5c.png)'
- en: The operation of combining numbers by multiplying each term by a constant and
    adding the results is omnipresent in machine learning and statistics. It is called
    a linear combination of two vectors. You can think of a vector as a fixed set
    of numbers. In our example, the first vector would be a data row and the second
    vector would contain the weights for each data attribute.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 将数字结合的操作，即将每个项与常数相乘并将结果相加，在机器学习和统计学中无处不在。它被称为两个向量的线性组合。你可以把一个向量看作是一个固定的数字集合。在我们的例子中，第一个向量将是数据行，第二个向量将包含每个数据属性的权重。
- en: Building neural networks
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建神经网络
- en: 'We are ready to build our first neural network. Let''s start with an example:
    our company is struggling with customer retention. We know a lot about our customers,
    and we can create an offer that would make them want to stay. The problem is,
    we cannot identify which customers will churn. So, our boss, Jane, asks us to
    build a churn prediction model. This model will take customer data and predict
    the probability of churning in the next month. With this probability estimate,
    Jane can decide if she needs to create a personalized marketing offer for this
    client.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备构建我们的第一个神经网络。让我们从一个例子开始：我们的公司在客户留存方面遇到了困难。我们知道很多关于客户的信息，并且可以创建一个能让他们想要留下的优惠。问题是，我们无法识别哪些客户会流失。因此，我们的老板，简，要求我们构建一个流失预测模型。这个模型将接受客户数据，并预测下个月流失的概率。有了这个概率估计，简就可以决定是否需要为该客户创建个性化的营销优惠。
- en: 'We have decided to use a neural network to solve this churn prediction problem.
    Our network will comprise multiple layers of neurons. Neurons in each layer will
    be connected to neurons in the next layer:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定使用神经网络来解决这个客户流失预测问题。我们的网络将由多个神经元层组成。每一层的神经元将与下一层的神经元相连接：
- en: '![](img/f6042a5e-ca74-4211-8ab6-71fc811bb9e3.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6042a5e-ca74-4211-8ab6-71fc811bb9e3.png)'
- en: That is a lot of arrows, isn't it? A connection between two neurons means that
    a neuron will pass its output to the next neuron. If a neuron receives multiple
    inputs, they are all summed up. This type of network is called a **fully connected
    neural network** (**FCNN**).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这可真是很多箭头，不是吗？两个神经元之间的连接意味着一个神经元将把它的输出传递给下一个神经元。如果一个神经元接收多个输入，它们会被全部加起来。这种类型的网络被称为**全连接神经网络**（**FCNN**）。
- en: 'We see how we can make predictions using neural networks, but how can we learn
    what predictions to make? If you look closer, a neural network is nothing more
    than a large function with lots of weights. The model prediction is determined
    by using weights and information incoming through the neuron inputs. Thus, to
    have an accurate neural network, you must set the right weights. We already know
    that we can use mathematical optimization and statistics to minimize prediction
    error by changing the parameters of a function. A neural network is nothing more
    than a large and complex mathematical function with variable weights. Therefore,
    we can use MLE and gradient descent to do the optimization. I will give the formal
    names of each stage in bold, followed by intuitive explanations of each stage:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了如何使用神经网络进行预测，但我们如何学习应该做出什么样的预测呢？仔细观察，神经网络不过是一个具有大量权重的大型函数。模型的预测是通过使用权重和通过神经元输入传入的信息来决定的。因此，要拥有一个准确的神经网络，你必须设置正确的权重。我们已经知道，可以通过数学优化和统计学来最小化预测误差，方法是改变函数的参数。神经网络不过是一个带有可变权重的大型复杂数学函数。因此，我们可以使用最大似然估计（MLE）和梯度下降来进行优化。我将以粗体字给出每个阶段的正式名称，随后是每个阶段的直观解释：
- en: '**Network initialization**: At first, we can initialize our weights with random
    values.'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**网络初始化**：一开始，我们可以用随机值来初始化权重。'
- en: '**Forward pass**: We can take an example from our training dataset and make
    a prediction using our current set of weights.'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**前向传播**：我们可以从训练数据集中选取一个例子，并使用当前的权重集进行预测。'
- en: '**Loss function calculation**: We measure the difference between our prediction
    and ground truth. We want to make this difference as closely to 0 as possible.
    That is, we want to minimize the loss function.'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**损失函数计算**：我们衡量预测值与真实值之间的差异。我们希望这个差异尽可能接近 0。也就是说，我们希望最小化损失函数。'
- en: '**Backward pass**: We can use an optimization algorithm to adjust the weights
    so that the prediction will be more accurate. A special algorithm called backpropagation
    can calculate updates to each layer of neurons, going from the last layer to the
    first.'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**反向传播**：我们可以使用优化算法调整权重，使得预测更加准确。一个叫做反向传播的特殊算法可以计算每一层神经元的更新，从最后一层到第一层。'
- en: '*Steps 1* to *4* are repeated until the desired accuracy level is achieved,
    or until the network stops learning:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤 1* 到 *4* 会重复进行，直到达到所需的准确度水平，或直到网络停止学习：'
- en: '![](img/831809c3-879a-425f-817f-e9794f8d8d9a.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/831809c3-879a-425f-817f-e9794f8d8d9a.png)'
- en: 'Backpropagation is the most widely used learning algorithm for training neural
    networks. It takes a prediction error and calculates how much we should change
    each weight in the network to make predictions closer to the ground truth. The
    name backpropagation comes from the specific way of how the algorithm updates
    the weights: it starts from the last layer, propagating changes to every neuron
    until it reaches the network input. When inputs go through the network to calculate
    an output prediction, we call it a forward pass. When we change the weights by
    propagating the error, we call it a backward pass.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是训练神经网络时最广泛使用的学习算法。它通过计算预测误差来确定我们应该如何调整网络中每个权重，以使预测值更接近真实值。反向传播的名称来源于算法更新权重的特定方式：它从最后一层开始，逐层将变化传播到每个神经元，直到到达网络输入层。当输入经过网络计算输出预测时，我们称之为前向传播。当我们通过传播误差来改变权重时，我们称之为反向传播。
- en: Nowadays, there are many different types of building blocks that you can use
    to compose a neural network. Some specific neuron types work better with image
    data, while others can utilize the sequential nature of text. Many specialized
    layers were invented to improve the speed of training and fight overfitting. A
    specific composition of layers in a neural network devoted to solving a specific
    task is called a neural network architecture. All neural network architectures,
    no matter how complex and deep, still conform to basic laws of backpropagation.
    Next, we will explore the domain-specific applications of deep learning.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，有许多不同类型的构建模块可以用来组成神经网络。有些特定的神经元类型在处理图像数据时效果更好，而其他类型则能利用文本的序列性质。许多专门的层被发明出来，以提高训练速度并抵抗过拟合。专门用于解决特定任务的神经网络层组合被称为神经网络架构。所有神经网络架构，无论它们有多复杂或多深，仍然遵循反向传播的基本规律。接下来，我们将探讨深度学习在特定领域的应用。
- en: Introduction to computer vision
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉简介
- en: 'First, we will look at computer vision. Let''s start with an example. Our client,
    Joe, likes animals. He is the happy owner of six cats and three dogs. Being a
    happy owner, he also likes to take pictures of his pets. Large photo archives
    have accumulated on his computer over the years. Joe has decided that he needs
    to bring order into his dreaded photo folder, containing 50,000 pet photos. To
    help Joe, we have decided to create a neural network that takes an image and decides
    whether a cat or dog is present on the photo. The following diagram shows how
    a neural network classifier works with a cat photo:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看看计算机视觉。让我们从一个例子开始。我们的客户乔（Joe）喜欢动物。他是六只猫和三只狗的快乐主人。作为一个快乐的主人，他也喜欢拍摄宠物的照片。多年来，乔的电脑上积累了大量的照片档案。乔决定需要整理他那个包含50,000张宠物照片的可怕照片文件夹。为了帮助乔，我们决定创建一个神经网络，输入一张图片，然后判断照片中是否有猫或狗。下面的图示展示了神经网络分类器如何处理一张猫的照片：
- en: '![](img/9b1832d8-40c9-4ec3-9fc2-139563cc4f82.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b1832d8-40c9-4ec3-9fc2-139563cc4f82.png)'
- en: 'At first, we transform an image into three tables of numbers, one each for
    the red, green and blue channel of every pixel. If we try to use a plain FCNN
    as before, we will see unimpressive results. Deep neural networks shine at computer
    vision tasks because of a specific neuron type called a convolutional filter or
    a convolution. **Convolutional neural networks** (**CNNs**) were invented by a
    French machine learning researcher Yann LeCun. In CNNs, a single neuron can look
    at a small patch of an image, say, 16x16 pixels, instead of taking the entire
    set of pixels as an input. This neuron can go through each 16x16 region of an
    image, detecting some feature of an image it had learned through backpropagation.
    Then, this neuron can pass information to further layers. In the following illustration,
    you can see a single convolutional neuron going through small image patches and
    trying to detect a fur-like pattern:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将图像转换为三张数字表格，分别表示每个像素的红色、绿色和蓝色通道。如果我们尝试像以前那样使用普通的全连接神经网络（FCNN），结果会让人失望。深度神经网络在计算机视觉任务中表现出色，因为它有一种特定类型的神经元，叫做卷积滤波器或卷积。**卷积神经网络**（**CNN**）是由法国机器学习研究员Yann
    LeCun发明的。在CNN中，单个神经元可以查看图像的一个小区域，比如16x16个像素，而不是将整个像素集作为输入。这个神经元可以遍历图像的每个16x16的区域，检测它通过反向传播学习到的图像特征。然后，这个神经元可以将信息传递给后续的层。在下面的插图中，您可以看到一个卷积神经元遍历小的图像块并尝试检测类似毛发的图案：
- en: '![](img/f2936bd0-cc7e-4adb-b9ed-c5b1c69afbd5.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2936bd0-cc7e-4adb-b9ed-c5b1c69afbd5.png)'
- en: The remarkable achievement of CNNs is that a single neuron can reuse a small
    number of weights and still cover the entire image. This feature makes CNNs a
    lot faster and lighter than regular neural networks. The idea found implementation
    only in the 2010s, when a CNN surpassed all other computer vision methods in an
    ImageNet competition, where an algorithm had to learn to classify photos between
    21,000 possible categories. The development of CNNs took so long because we lacked
    the computational capabilities to train deep neural networks with a large number
    of parameters on big datasets. To achieve good accuracy, CNNs require significant
    amounts of data. For example, the ImageNet competition includes 1,200,000 training
    images.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）显著的成就之一是，单个神经元可以重用少量的权重，依然能够覆盖整个图像。这一特性使得CNN比传统神经网络更加快速和轻量化。这个理念直到2010年代才得以实现，那时CNN在ImageNet比赛中超越了所有其他计算机视觉方法。在这场比赛中，算法必须学习如何将照片分类到21,000个可能的类别中。CNN的发展历程较长，因为我们缺乏足够的计算能力来训练包含大量参数的大型数据集上的深度神经网络。为了获得良好的准确率，CNN需要大量的数据。例如，ImageNet比赛包括1,200,000张训练图像。
- en: At first layers, CNNs tend to detect simple patterns, such as edges and contours
    in an image. As the layer depth progresses, convolutional filters become more
    complex, detecting features such as eyes, noses, and so on.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的层次，卷积神经网络（CNN）通常会检测图像中的简单模式，如边缘和轮廓。随着层次的加深，卷积滤波器变得更加复杂，能够检测出眼睛、鼻子等特征。
- en: 'In the following visualizations, you can see an example visualization of convolutional
    filters at different layers of the neural network:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的可视化中，您可以看到神经网络不同层次上卷积滤波器的一个示例：
- en: '![](img/2a479028-5b00-4e7d-a173-4cc0daff5cdf.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a479028-5b00-4e7d-a173-4cc0daff5cdf.png)'
- en: 'Many neurons learn to recognize simple patterns that are useful for any computer
    vision task. This observation leads us to a very important idea: a neural network
    that was trained to perform one task well can be retrained to perform another
    task. Moreover, you will need much less training data for the second task, as
    the network has already learned many useful features from the previous training
    dataset. In particular, if you want to train a CNN classifier for two classes
    from scratch, you will need to label tens of thousands of images to reach a good
    performance level. However, if you use a network that was pretrained on ImageNet,
    you will probably get good results with only 1,000 to 2,000 images. This approach
    is called transfer learning. Transfer learning is not limited to computer vision
    tasks. In recent years, researchers made significant progress in using it for
    other domains: natural language processing, reinforcement learning, and sound
    processing.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 许多神经元学习识别对任何计算机视觉任务有用的简单模式。这一观察引导我们得出一个非常重要的思想：一个训练良好以执行一项任务的神经网络可以重新训练以执行另一项任务。此外，对于第二项任务，您将需要更少的训练数据，因为网络已经从先前的训练数据集中学到了许多有用的特征。特别是，如果您想从头开始训练一个CNN分类器以处理两类图像，您将需要标记成千上万张图像以达到良好的性能水平。然而，如果您使用在ImageNet上预训练过的网络，您可能只需使用1,000到2,000张图像就能获得良好的结果。这种方法称为迁移学习。迁移学习不仅限于计算机视觉任务。近年来，研究人员在其他领域如自然语言处理、强化学习和声音处理中也取得了显著进展。
- en: Now that you have an understanding of how deep CNNs work, we will proceed to
    the language domain, where deep learning has changed everything.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了深度卷积神经网络的工作原理，我们将继续到语言领域，深度学习已经改变了一切。
- en: Introduction to natural language processing
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理简介
- en: Before the deep learning revolution, **natural language processing** (**NLP**)
    systems were almost fully rule based. Linguists created intricate parsing rules
    and tried to define our language's grammar to automate tasks such as part of speech
    tagging or named entity recognition. Human-level translation between different
    languages and free-form question answering were in the domain of science fiction.
    NLP systems were hard to maintain and took a long time to develop.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习革命之前，自然语言处理（NLP）系统几乎完全基于规则。语言学家们创建了复杂的解析规则，试图定义我们语言的语法，以自动化诸如词性标注或命名实体识别之类的任务。不同语言之间的人类级别翻译和自由形式问题回答在科幻领域。NLP系统难以维护且开发周期长。
- en: As with computer vision, deep learning took the NLP world by storm. Deep-learning-based
    NLP algorithms successfully perform near-human-level translation between different
    languages, can measure the emotional sentiment of a text, can learn to retrieve
    information from a text, and can generate answers on free-form questions. Another
    great benefit of deep learning is a unified approach. A single part-of-speech
    tagging model architecture will work for French, English, Russian, German, and
    other languages. You will need training data for all those languages, but the
    underlying model will be the same. With deep learning, we need not try to hardcode
    the rules of our ambiguous language. While many tasks, such as long-form writing
    and human-level dialogue, are yet unconquerable for deep learning, NLP algorithms
    are a great help in business and daily life.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 就像计算机视觉一样，深度学习席卷了自然语言处理（NLP）领域。基于深度学习的NLP算法成功实现了不同语言之间接近人类水平的翻译，可以测量文本的情感倾向，能够学习从文本中检索信息，并能生成自由形式的问题答案。深度学习的另一个重大好处是统一的方法。一个词性标注模型架构可以适用于法语、英语、俄语、德语等多种语言。虽然需要为所有这些语言准备训练数据，但基础模型是相同的。使用深度学习，我们无需试图硬编码我们复杂的语言规则。尽管许多任务，如长篇写作和人类水平的对话，对深度学习仍然是不可征服的，但NLP算法在商业和日常生活中提供了巨大帮助。
- en: 'For NLP deep learning, everything began with an idea: the meaning of a word
    is defined by its neighbors. That is, to learn a language and the meaning of words,
    all you need is to understand the context for each word in the text. This idea
    may seem to be too simple to be true. To check its validity, we can create a neural
    network that will predict a word by receiving surrounding words as an input. To
    create a training dataset, we may use any text in any language.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NLP深度学习而言，一切始于一个想法：单词的含义由其邻居定义。也就是说，要学习一门语言和单词的含义，你只需要理解文本中每个单词的上下文。这个想法可能看起来过于简单以至于不真实。为了验证其有效性，我们可以创建一个神经网络，通过接收周围单词作为输入来预测一个单词。为了创建训练数据集，我们可以使用任何语言的任何文本。
- en: 'If we take a context window of two words, then we can generate the following
    training samples for this sentence:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们采用两个单词的上下文窗口，那么我们可以为这个句子生成以下训练样本：
- en: If, we, take, a → will
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 如果，我们，取，a → will
- en: We, can, following, training → generate
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过训练→生成
- en: Following, training, for, this → samples
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Following, training, for, this → samples
- en: And so on…
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推…
- en: 'Next, we need to come up with a way to convert all words to numbers, because
    neural networks only understand numbers. One approach would be to take all unique
    words in a text and assign them to a number:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要想出一种方法将所有单词转换为数字，因为神经网络只能理解数字。一种方法是将文本中所有独特的单词分配给一个数字：
- en: Following → 0
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Following → 0
- en: Training → 1
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Training → 1
- en: Samples → 2
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Samples → 2
- en: For → 3
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: For → 3
- en: This → 4
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: This → 4
- en: …
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: Then, we represent each word by a set of weights inside a neural network. In
    particular, we start with two random numbers between 0 and 1 for each word.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过神经网络中的一组权重来表示每个词。具体来说，我们从每个词的两个介于0和1之间的随机数字开始。
- en: 'We place all the numbers into a table as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有数字放入如下表格中：
- en: '| **Word identifier** | **Word vector** |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| **词标识符** | **词向量** |'
- en: '| 0 | 0.63, 0.26 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.63, 0.26 |'
- en: '| 1 | 0.52, 0.51 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.52, 0.51 |'
- en: '| 2 | 0.72, 0.16 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.72, 0.16 |'
- en: '| 3 | 0.28, 0.93 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.28, 0.93 |'
- en: '| 4 | 0.27, 0.71 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.27, 0.71 |'
- en: '| … | … |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| … | … |'
- en: '| N | 0.37, 0.34 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| N | 0.37, 0.34 |'
- en: Now we have a way to convert every word in the text into a pair of numbers.
    We will take all numbers that we have generated as weights for our neural network.
    It will get four words as input, convert them into eight numbers, and use them
    to predict the identifier for the word in the middle.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种方法，将文本中的每个词转换为一对数字。我们将把我们生成的所有数字作为神经网络的权重。它将接收四个词作为输入，将它们转换为八个数字，并使用它们来预测中间词的标识符。
- en: 'For example, for the training sample **Following**, **Training**, **For**,
    **This** → **Samples**:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于训练样本**Following**，**Training**，**For**，**This** → **Samples**：
- en: 'Input:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: Following → 0 → 0.63, 0.26
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Following → 0 → 0.63, 0.26
- en: Training → 1 → 0.52, 0.51
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 训练→1→0.52, 0.51
- en: For → 3 → 0.28, 0.93
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: For → 3 → 0.28, 0.93
- en: This → 4 → 0.27, 0.71
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: This → 4 → 0.27, 0.71
- en: 'Output:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: 2 → Samples
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 2 → Samples
- en: We call each pair of numbers associated with a word, a word vector. Our neural
    network will output a vector of probabilities from zero to one. The length of
    this vector will match the total number of unique words in our dataset. Then,
    the number with the largest probability will represent the word that is the most
    likely completion of our input according to the model.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将与一个词相关的每一对数字称为词向量。我们的神经网络将输出一个从零到一的概率向量。这个向量的长度将与我们数据集中的独特词汇总数匹配。然后，具有最大概率的数字将代表最有可能根据模型完成输入的词。
- en: In this setup, we can apply the backpropagation algorithm to adjust word vectors
    until the model matches right words to their contexts. In our example, you can
    imagine each word lives on a coordinate grid. The elements of the word vector
    could represent *X* and *Y* coordinates. If you think about words in this geometric
    fashion, you may conclude that you can add or subtract word vectors to get another
    word vector. In the real world, such vectors contain not two but 100 to 300 elements,
    but the intuition remains the same. After many training iterations, you will see
    remarkable results.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，我们可以应用反向传播算法调整词向量，直到模型将正确的词与它们的上下文匹配。在我们的例子中，你可以想象每个词都位于一个坐标网格上。词向量的元素可能代表*X*和*Y*坐标。如果你从几何的角度看待单词，你可能会得出结论，你可以通过加或减词向量得到另一个词向量。在现实世界中，这样的向量包含的不仅仅是两个元素，而是100到300个元素，但直觉是一样的。经过多次训练迭代后，你会看到显著的结果。
- en: 'Try to calculate the following using word vectors:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用词向量计算以下内容：
- en: King - Man + Woman = ?
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: King - Man + Woman = ?
- en: You will get a vector for the word Queen. By learning to put words into their
    surrounding contexts, the model learns how different words relate to each other.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到一个词向量表示“Queen”一词。通过学习将词放入它们的上下文中，模型学会了不同词之间的关系。
- en: 'The model we have built is called Word2Vec. We can train Word2Vec models in
    two ways:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建的模型叫做Word2Vec。我们可以通过两种方式训练Word2Vec模型：
- en: Predict a word by using its surrounding context. This setup is called a **continuous
    bag of words** (**CBOW**).
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其周围的上下文预测一个词。这个设置叫做**连续词袋模型**（**CBOW**）。
- en: Predict the surrounding context by word. This setup is called **Skipgram**.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词预测周围的上下文。这个设置叫做**Skipgram**。
- en: The two approaches do not differ in anything, except model input and output
    specifications.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法除了模型的输入和输出规范外，没有任何区别。
- en: 'Word vectors are also referred to as word embeddings. Embeddings contain much
    more information about words than simple numeric identifiers, and NLP models can
    use them to achieve better accuracy. For example, you can train a sentiments classification
    model by following these steps:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量也被称为词嵌入。词嵌入比简单的数字标识符包含更多关于单词的信息，NLP模型可以利用这些信息来提高准确性。例如，你可以通过以下步骤训练一个情感分类模型：
- en: Create a training dataset that contains user reviews and their sentiment, labeled
    0 as negative and 1 as positive.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含用户评论及其情感的训练数据集，情感标记为负面（0）和正面（1）。
- en: Embed the user reviews into sets of word vectors.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将用户评论嵌入到词向量的集合中。
- en: Train deep learning classifier using this dataset.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用该数据集训练深度学习分类器。
- en: Current state-of-the-art models rarely use word embeddings created by training
    a separate model. Newer architectures allow learning task-specific word embeddings
    on the fly, without the need to use Word2Vec. Nonetheless, we have covered word
    embeddings in this chapter as they give us an idea of how computers can understand
    the meaning of a text. While modern models are more complex and robust, this idea
    remains intact.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最先进的模型很少使用通过训练单独模型创建的词嵌入。较新的架构允许在任务执行过程中动态学习任务特定的词嵌入，无需使用Word2Vec。然而，我们在这一章中讨论了词嵌入，因为它们能帮助我们理解计算机如何理解文本的含义。尽管现代模型更复杂且更强大，但这个思想仍然保持不变。
- en: The concept of embeddings originated in NLP, but now it has found applications
    in recommended systems, face recognition, classification problems with lots of
    categorical data, and many other domains.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入的概念起源于NLP，但现在已被广泛应用于推荐系统、人脸识别、包含大量类别数据的分类问题以及许多其他领域。
- en: To train a classifier that uses word embeddings, you can use a CNN. In a CNN,
    each neuron progressively scans the input text in windows of words. Convolutional
    neurons learn weights that combine word vectors of nearby words into more compact
    representations that are used by the output layer to estimate sentence sentiment.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个使用词嵌入的分类器，你可以使用卷积神经网络（CNN）。在CNN中，每个神经元都会逐步扫描输入文本的单词窗口。卷积神经元学习权重，将附近单词的词向量结合成更紧凑的表示形式，这些表示会被输出层用来估计句子的情感。
- en: 'You can see how a single convolutional neuron works on a single sentence in
    the following screenshot:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下截图中看到单个卷积神经元如何处理一个句子：
- en: '![](img/646d01d6-47ca-4bfb-90a4-4fe58f5377c2.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![](img/646d01d6-47ca-4bfb-90a4-4fe58f5377c2.png)'
- en: 'CNNs process text in a fixed window, which is an oversimplification. In reality,
    a word at the beginning of the sentence can affect its ending, and vice versa.
    Another architecture called **recurrent neural networks** (**RNNs**) can process
    sequences of any length, passing information from start to end. This is possible
    because all recurrent neurons are connected to themselves:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: CNN在固定窗口中处理文本，这是一个过于简化的观点。实际上，句子开头的单词可能会影响句子的结尾，反之亦然。另一种架构叫做**递归神经网络**（**RNNs**），它可以处理任意长度的序列，将信息从开始传递到结束。这是可能的，因为所有的递归神经元都与自身连接：
- en: '![](img/22883415-c01e-4a97-a129-683dfaea54c0.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22883415-c01e-4a97-a129-683dfaea54c0.png)'
- en: 'Self-connection allows a neuron to cycle through its input, pulling its internal
    state through each iteration:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 自连接允许神经元在每次迭代中将其输入反馈到自身，拉动其内部状态：
- en: '![](img/4d0fd715-8483-4a27-bf57-9285aa83c472.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d0fd715-8483-4a27-bf57-9285aa83c472.png)'
- en: The preceding screenshot depicts a single recurrent neuron as it unfolds. With
    each new word, a recurrent neuron changes its previous state. When the final word
    is processed, it returns its internal state as the output. This is the most basic
    recurrent architecture. Neural networks that are used in practice have a more
    complex internal structure, but the idea of recurrent connections holds. When
    speaking about recurrent networks, you will probably hear about **long short-term
    memory networks** (**LSTMs**). While they differ in the details, the flow of thought
    is the same for both RNNs and LSTMs.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图展示了一个单一的递归神经元在展开的过程。随着每个新单词的到来，递归神经元会改变它的前一个状态。当最后一个单词被处理时，它会返回其内部状态作为输出。这是最基本的递归架构。实际应用中使用的神经网络具有更复杂的内部结构，但递归连接的思想依然存在。当谈到递归网络时，你可能会听到关于**长短期记忆网络**（**LSTMs**）的讨论。虽然它们在细节上有所不同，但RNN和LSTM的思路是相同的。
- en: Summary
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we uncovered the inner workings of machine learning and deep
    learning. We learned the main concepts of mathematical optimization and statistics.
    We connected them to machine learning and, finally, learned how machines learn
    and how we can use optimization algorithms to define learning. Lastly, we covered
    popular machine learning and deep learning algorithms, including linear regression,
    tree ensembles, CNNs, word embeddings, and recurrent neural networks. This chapter
    concludes our introduction to data science.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们揭示了机器学习和深度学习的内部工作原理。我们学习了数学优化和统计学的主要概念。我们将这些概念与机器学习联系起来，最终学习了机器是如何学习的，以及我们如何使用优化算法来定义学习。最后，我们介绍了流行的机器学习和深度学习算法，包括线性回归、树集成、卷积神经网络（CNN）、词嵌入和递归神经网络。本章结束了我们对数据科学的介绍。
- en: In the next chapter, we will learn how to build and sustain a data science team
    capable of delivering complex cross-functional projects.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习如何建立和维持一个能够交付复杂跨职能项目的数据科学团队。
