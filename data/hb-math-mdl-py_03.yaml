- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Principal Component Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: A well-known algorithm to extract features from high-dimensional data for consumption
    in **machine learning** (**ML**) models is **Principal Component Analysis** (**PCA**).
    In mathematical terms, *dimension* is the minimum number of coordinates required
    to specify a vector in space. A lot of computational power is needed to find the
    distance between two vectors in high-dimensional space and in such cases, dimension
    is considered a curse. An increase in dimension will result in high performance
    of the algorithm only to a certain extent and will drop beyond that. This is the
    curse of dimensionality, as shown in *Figure 3**.1*, which impedes the achievement
    of efficiency for most ML algorithms. The variable columns or features in data
    represent dimensions of space and the rows represent the coordinates in that space.
    With the increasing dimension of data, sparsity increases and there is an exponentially
    increasing computational effort required to calculate distance and density. Theoretically
    speaking, an increase in dimension practically increases noise and redundancy
    in large datasets. Arguably, PCA is the most popular technique to tackle this
    complexity of dimensionality in high-dimensional problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一个广为人知的算法，用于从高维数据中提取特征，供**机器学习**（**ML**）模型使用，便是**主成分分析**（**PCA**）。用数学术语来说，*维度*是指指定空间中一个向量所需的最小坐标数。要在高维空间中找到两个向量之间的距离，需要大量的计算能力，在这种情况下，维度被视为一种诅咒。维度的增加会在一定程度上提升算法的性能，但超过这个范围后，性能将下降。这就是维度诅咒，正如*图3.1*所示，它妨碍了大多数ML算法效率的提升。数据中的变量列或特征代表空间的维度，而行代表该空间中的坐标。随着数据维度的增加，稀疏性增加，并且计算距离和密度所需的计算量呈指数增长。从理论上讲，维度的增加实际上会增加大数据集中的噪声和冗余。可以说，PCA是应对高维问题中维度复杂性最流行的技术。
- en: '![Figure 3.1: Curse of dimensionality](img/Figure_03_01_B18943.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1：维度诅咒](img/Figure_03_01_B18943.jpg)'
- en: 'Figure 3.1: Curse of dimensionality'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：维度诅咒
- en: PCA comes from the field of linear algebra and is essentially a data preparation
    method that projects the data in a subspace before fitting the ML model to the
    newly created low-dimensional dataset. PCA is a data projection technique useful
    in visualizing high-dimensional data and improving data classification. It was
    invented in the 1900s following the principal axis theorem. The main objectives
    of PCA are to find an orthonormal basis for the data, sort dimensions in the order
    of importance or variance, discard dimensions of low importance, and focus only
    on uncorrelated Gaussian components.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: PCA源自线性代数领域，本质上是一种数据准备方法，在将ML模型拟合到新创建的低维数据集之前，先将数据投影到一个子空间。PCA是一种数据投影技术，能有效地可视化高维数据并改善数据分类。它是在1900年代根据主轴定理发明的。PCA的主要目标是为数据找到一个正交归一基，按重要性或方差的顺序排序维度，丢弃不重要的维度，并只关注不相关的高斯成分。
- en: 'This chapter covers the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下内容：
- en: Linear algebra for PCA
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA的线性代数
- en: Linear discriminant analysis – the difference from PCA
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性判别分析 – 与PCA的区别
- en: Applications of PCA
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA的应用
- en: The following section talks about linear algebra, the subject of mathematics
    on which PCA is based.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分将讨论线性代数，这是PCA所基于的数学学科。
- en: Linear algebra for PCA
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA的线性代数
- en: PCA is an unsupervised method used to reduce the number of features of a high-dimensional
    dataset. An unlabeled dataset is reduced into its constituent parts by matrix
    factorization (or decomposition) followed by ranking of these parts in accordance
    with variances. The projected data representative of the original data becomes
    the input to train ML models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种无监督方法，用于减少高维数据集的特征数量。通过矩阵因式分解（或分解），然后根据方差对这些部分进行排序，将一个未标记的数据集缩减为其组成部分。表示原始数据的投影数据将成为训练ML模型的输入。
- en: 'PCA is defined as the orthogonal projection of data onto a lower dimensional
    linear space called the principal subspace, done by finding new axes (or basis
    vectors) that preserve the maximum variance of projected data; the new axes or
    vectors are known as principal components. PCA preserves the information by considering
    the variance of projection vectors: the highest variance lies on the first axis,
    the second highest on the second axis, and so forth. The working principle of
    the linear transformation called PCA is shown in *Figure 3**.2*. It compresses
    the feature space by identifying a subspace that captures the essence of the complete
    feature matrix.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 被定义为数据向下投影到一个称为主子空间的低维线性空间，这一过程是通过找到新的坐标轴（或基向量）来实现的，这些新坐标轴最大程度地保留了投影数据的方差；这些新坐标轴或向量被称为主成分。PCA
    通过考虑投影向量的方差来保持信息：最大方差位于第一坐标轴，第二大方差位于第二坐标轴，依此类推。线性变换的工作原理，即 PCA，如*图 3.2* 所示。它通过识别一个子空间，捕捉完整特征矩阵的本质，从而压缩了特征空间。
- en: '![Figure 3.2: PCA working principle](img/Figure_03_02_B18943.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2：PCA 工作原理](img/Figure_03_02_B18943.jpg)'
- en: 'Figure 3.2: PCA working principle'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：PCA 工作原理
- en: There are other approaches to reducing data dimensionality such as feature selection
    methods (wrapper and filter), non-linear methods such as manifold learning (t-SNE),
    and deep learning (autoencoders) networks; however, the widest and most popular
    exploratory approach is PCA. Typically, linear algebraic methods assume that all
    inputs have the same distribution, and hence, it is a good practice to (either
    normalize or standardize) scale data before using PCA if the input features have
    different units.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 PCA，还有其他减少数据维度的方法，如特征选择方法（包装法和过滤法）、非线性方法如流形学习（t-SNE），以及深度学习（自编码器）网络；然而，最广泛且最流行的探索性方法是
    PCA。通常，线性代数方法假设所有输入具有相同的分布，因此，如果输入特征具有不同的单位，使用 PCA 之前最好对数据进行归一化或标准化处理。
- en: Covariance matrix – eigenvalues and eigenvectors
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协方差矩阵 – 特征值和特征向量
- en: The constraint in PCA is all the principal axes should be mutually orthogonal.
    The covariance of data is a measure of how much any pair of features in the data
    vary from each other. A covariance matrix checks the correlations between features
    in data and the directions of these relationships are obtained depending on whether
    the covariance is less than, equal to, or greater than zero. *Figure 3**.3* displays
    the covariance matrix formula. Each element in the matrix represents the correlation
    between two features in the data where *j* and *k* run over *p* variates, *N*
    is the number of observations (rows), and the ![](img/Formula_03_001.png) bar
    and ![](img/Formula_03_002.png) bar in the formula denote the expected values
    (averages).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 的约束是所有主轴应互相正交。数据的协方差是衡量数据中任意一对特征之间变化程度的度量。协方差矩阵检查数据中特征之间的相关性，这些关系的方向取决于协方差是小于、等于还是大于零。*图
    3.3* 显示了协方差矩阵的公式。矩阵中的每个元素表示数据中两个特征之间的相关性，其中*j* 和 *k* 遍历*p*个变量，*N* 是观察值的数量（行数），公式中的
    ![](img/Formula_03_001.png) 条和 ![](img/Formula_03_002.png) 条表示期望值（均值）。
- en: '![Figure 3.3: Covariance matrix](img/Formula_03_003.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3：协方差矩阵](img/Formula_03_003.jpg)'
- en: 'Figure 3.3: Covariance matrix'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：协方差矩阵
- en: The objective of PCA is to explain most of the data variability with far fewer
    variables or features than that in the original dataset. Each of the *N* observations
    or records resides in *p*-dimensional space. Not all the dimensions are equally
    relevant. PCA seeks a smaller number of important dimensions, where importance
    is quantified by the amount of variation of the observations along each dimension.
    The dimensions figured out by PCA are a linear combination of the p features.
    We can take the linear combinations and reduce the number of plots required for
    visual analysis of the feature space while retaining the essence of the original
    data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 的目标是通过比原始数据集中的变量或特征少得多的变量来解释大部分的数据变化性。每个*N*个观察值或记录都位于*p*维空间中。并非所有维度都是同等重要的。PCA
    寻求少数几个重要的维度，其中重要性通过每个维度上观察值的变化量来量化。PCA 确定的维度是*p*个特征的线性组合。我们可以利用这些线性组合来减少用于可视化分析特征空间的图表数量，同时保留原始数据的本质。
- en: The eigenvalues and eigenvectors of a covariance matrix computed by eigendecomposition
    determine the magnitude and direction of the new subspace, respectively. In linear
    algebra, an **eigenvector** (associated with a set of linear equations) of a linear
    transformation is a non-zero vector that changes by a magnitude (scalar) when
    the transformation is applied to it. The corresponding **eigenvalue** is the magnitude
    or factor by which the eigenvector is scaled, and **eigendecomposition** is the
    factorization of a matrix into its eigenvectors and eigenvalues. The principal
    components are the eigenvectors. The top eigenvalues of a covariance matrix after
    sorting in descending order yield the principal components of the dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由特征分解计算出的协方差矩阵的特征值和特征向量分别决定了新子空间的大小和方向。在线性代数中，**特征向量**（与一组线性方程相关联）是一个非零向量，当对其进行线性变换时，该向量会按某个大小（标量）发生变化。对应的**特征值**是特征向量被缩放的大小或因子，**特征分解**是将矩阵分解为其特征向量和特征值。主成分就是特征向量。对协方差矩阵按降序排序后，最顶端的特征值对应数据集的主成分。
- en: The first **principal component** (**PC**) of data is the linear combination
    of the features that have the highest variance. The second PC is the linear combination
    of the features that have maximum variance out of all linear combinations uncorrelated
    with the first PC. The first two PCs are shown in *Figure 3**.4*, and this computational
    process proceeds until all the PCs of the dataset are found. These PCs are essentially
    the eigenvectors, and linear algebraic techniques show that the eigenvector corresponding
    to the highest eigenvalue of the covariance matrix explains the greatest proportion
    of data variability. Each PC vector defines a direction in feature space and all
    of them are uncorrelated – that is, mutually orthogonal. The PCs form the basis
    of the new space.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的第一个**主成分**（**PC**）是具有最大方差的特征的线性组合。第二个主成分是所有与第一个主成分不相关的线性组合中具有最大方差的特征的线性组合。前两个主成分在*图
    3.4*中显示，这一计算过程会一直进行，直到找到数据集的所有主成分。这些主成分本质上就是特征向量，线性代数方法表明，协方差矩阵中与最大特征值对应的特征向量解释了数据变异性的最大比例。每个主成分向量在特征空间中定义了一个方向，而且它们彼此之间没有相关性——即相互正交。主成分构成了新空间的基础。
- en: '![Figure 3.4: Principal components in feature space (x = ￼, y = ￼)](img/Figure_03_03_B18943.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4：特征空间中的主成分（x = ￼，y = ￼）](img/Figure_03_03_B18943.jpg)'
- en: 'Figure 3.4: Principal components in feature space (x = ![](img/Formula_03_004.png),
    y = ![](img/Formula_03_005.png))'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：特征空间中的主成分（x = ![](img/Formula_03_004.png)，y = ![](img/Formula_03_005.png))
- en: Number of PCs – how to select for a dataset
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分数量 – 如何为数据集选择合适的数量
- en: The question is how to determine how well a dataset is explained by a certain
    number of PCs. The answer lies in the percentage of variance retained by the number
    of PCs. We would ideally like to have the smallest number of PCs possible explaining
    most of the variability. There is no robust method to determine the number of
    usable components. As the number of observations and variables in the dataset
    vary, different levels of accuracy and amounts of reduction are desirable.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是如何确定一个数据集被某个数量的主成分（PC）解释的程度。答案在于由主成分数量保留的方差百分比。我们理想情况下希望能够用最少数量的主成分来解释大部分的变异性。没有一个健壮的方法来确定可用的成分数量。随着数据集中观测值和变量的变化，所需的准确度和降维量也不同。
- en: The **proportion of variance explained** (**PVE**) by the *m*th PC is computed
    by considering the *m*th eigenvalue. PVE is the ratio of the mth eigenvalue represented
    by ![](img/Formula_03_006.png) (of *j*th variate in *Figure 3**.5*) and the sum
    of the eigenvalues of all the PCs or eigenvectors. To put it simply, PVE is the
    variance explained by each PC divided by the total variance of all PCs in the
    dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分方差解释比例**（**PVE**）是通过考虑第*m*个特征值来计算的。PVE是由第*m*个特征值表示的比例，即 ![](img/Formula_03_006.png)（来自*图
    3.5*中的*j*个变异量），与所有主成分或特征向量的特征值之和的比率。简单来说，PVE是每个主成分解释的方差与数据集中所有主成分总方差之比。'
- en: '![Figure 3.5: PVE calculation](img/Formula_03_007.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5：PVE 计算](img/Formula_03_007.jpg)'
- en: 'Figure 3.5: PVE calculation'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5：PVE 计算
- en: In general, we look for the “elbow point” where the PVE significantly drops
    off to determine the number of usable PCs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会寻找PVE显著下降的“拐点”，以确定可用的主成分数量。
- en: '![Figure 3.6a: PVE versus PC](img/Figure_03_04_B18943.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6a：PVE 与主成分的关系](img/Figure_03_04_B18943.jpg)'
- en: 'Figure 3.6a: PVE versus PC'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6a：PVE 与主成分的关系
- en: The first PC in the example shown in *Figure 3**.6a* explains 62.5% of data
    variability, and the second PC explains 25%. In a cumulative manner, the first
    two PCs explain 87.5% of the variability, as shown in *Figure 3**.6b*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 3.6a* 中显示的第一个主成分解释了 62.5% 的数据变异性，第二个主成分解释了 25%。以累积的方式，前两个主成分解释了 87.5% 的变异性，如
    *图 3.6b* 所示。
- en: '![Figure 3.6b: Cumulative PVE (y axis) versus PC](img/Figure_03_05_B18943.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6b：累积 PVE（纵轴）与主成分（PC）](img/Figure_03_05_B18943.jpg)'
- en: 'Figure 3.6b: Cumulative PVE (y axis) versus PC'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6b：累积 PVE（纵轴）与主成分（PC）
- en: Another widely used matrix decomposition method to identify the number of PCs
    is **singular value decomposition** (**SVD**). It is a reduced-rank approximation
    method that provides a simple means to do the same – that is, compute the principal
    components corresponding to the singular values. SVD is just another way to factorize
    a matrix and allows us to unravel similar information as eigendecomposition does.
    The singular values of the data matrix obtained via SVD are essentially the square
    roots of the eigenvalues of the covariance matrix in PCA. SVD is the same as PCA
    of a raw data matrix, which is mean-centered.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种广泛使用的矩阵分解方法用于确定主成分（PC）的数量是 **奇异值分解** (**SVD**)。它是一种降秩逼近方法，提供了一种简便的方式来实现相同的目标——即计算与奇异值对应的主成分。SVD
    只是另一种矩阵分解方法，它让我们解开与特征分解类似的信息。通过 SVD 得到的数据矩阵的奇异值，本质上是 PCA 中协方差矩阵的特征值的平方根。SVD 就是对原始数据矩阵（均值中心化的矩阵）进行
    PCA。
- en: 'A mathematical demonstration of SVD can be found on the Wolfram pages: [https://mathworld.wolfram.com/SingularValueDecomposition.xhtml](https://mathworld.wolfram.com/SingularValueDecomposition.xhtml).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: SVD 的数学演示可以在 Wolfram 页面上找到：[https://mathworld.wolfram.com/SingularValueDecomposition.xhtml](https://mathworld.wolfram.com/SingularValueDecomposition.xhtml)。
- en: 'SVD is an iterative numerical method. Every rectangular matrix has SVD, although,
    for a few complex problems, it may fail to decompose some matrices neatly. You
    can perform SVD using the linear algebra class of the Python library, `scipy`.
    The `scikit-learn` library provides functions for SVD: [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.xhtml).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: SVD 是一种迭代的数值方法。每个矩阵都有 SVD，尽管对于一些复杂问题，它可能无法整齐地分解某些矩阵。你可以使用 Python 库中的线性代数类 `scipy`
    来执行 SVD。`scikit-learn` 库提供了 SVD 的函数：[https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.xhtml)。
- en: High-dimensional data can be reduced to a subset of dimensions (columns) that
    are most relevant to the problem being solved. The data matrix (rows by columns)
    results in a matrix with a lower rank that approximates the original matrix and
    best captures its salient features.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 高维数据可以被简化为一组与所解决问题最相关的维度（列）。数据矩阵（行与列）会得到一个秩较低的矩阵，该矩阵近似原始矩阵，并且最佳地捕捉其显著特征。
- en: Feature extraction methods
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征提取方法
- en: 'There are two ways in which dimensionality reduction can be done: one is feature
    selection and the other is feature extraction. A subset of original features is
    selected in the former approach by filtering based on some criteria true to the
    particular use case and corresponding data. On the other hand, a set of new features
    is found in the feature extraction approach.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 降维可以通过两种方式进行：一种是特征选择，另一种是特征提取。在特征选择方法中，通过根据某些符合特定使用案例和相应数据的标准进行过滤，选出原始特征的一个子集。而在特征提取方法中，找到一组新的特征。
- en: 'Feature extraction is done using linear mapping from the original features,
    which no longer exist upon implementation of the method. In essence, new features
    constructed from available data do not have column names as in the original data.
    There are two feature extraction methods: PCA and LDA. A nonlinear mapping may
    also be used depending on the data but the method no longer remains PCA or LDA.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取是通过从原始特征进行线性映射来完成的，实施该方法后原始特征将不再存在。从本质上讲，通过可用数据构建的新特征没有像原始数据那样的列名。有两种特征提取方法：PCA
    和 LDA。根据数据的不同，也可以使用非线性映射，但该方法就不再是 PCA 或 LDA。
- en: Now that we have explored PCA for the reduction of features (and hence, the
    reduction of the high-dimensional space), we shall learn about a supervised method
    of linear feature extraction called **linear discriminant** **analysis** (**LDA**).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了用于特征降维的 PCA（因此，也降维了高维空间），接下来我们将学习一种监督式的线性特征提取方法，称为 **线性判别分析** (**LDA**)。
- en: LDA – the difference from PCA
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LDA – 与 PCA 的区别
- en: LDA and PCA are linear transformation methods; the latter yields directions
    or PCs that maximize data variance and the former yields directions that maximize
    the separation between data classes. The way in which the PCA algorithm works
    disregards class labels.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LDA和PCA都是线性变换方法；后者通过求解最大化数据方差的方向或主成分（PCs），前者则通过求解最大化数据类别间分离的方向。PCA算法的工作方式忽略了类别标签。
- en: LDA is a supervised method to reduce dimensionality that projects the data onto
    a subspace in a way that maximizes the separability between (groups) classes;
    hence, it is used for pattern classification problems. LDA works well for data
    with multiple classes; however, it makes assumptions of normally distributed classes
    and equal class covariances. PCA tends to work well if the number of samples in
    each class is relatively small. In both cases, though, observations ought to be
    much higher relative to the dimensions for meaningful results.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一种监督方法，用于将数据投影到一个子空间，以最大化（组）类别之间的可分性；因此，它用于模式分类问题。LDA对于具有多个类别的数据效果较好；然而，它假设类别是正态分布的且类别协方差相等。PCA通常在每个类别的样本数相对较少时表现较好。然而，无论如何，观测值应该相对于维度要高得多，以便获得有意义的结果。
- en: LDA seeks a projection that discriminates data in the best possible way, unlike
    PCA, which seeks a projection that preserves maximum information in the data.
    When regularization of the estimate of covariance is introduced to moderate the
    influence of different variables on LDA, it is called **regularized** **discriminant
    analysis**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LDA寻找一种最能区分数据的投影，而PCA则寻找一种能够保持数据中最大信息量的投影。当引入协方差估计的正则化，以调节不同变量对LDA的影响时，这被称为**正则化**
    **判别分析**。
- en: '![Figure 3.7: Linear discriminants](img/Figure_03_06_B18943.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7：线性判别](img/Figure_03_06_B18943.jpg)'
- en: 'Figure 3.7: Linear discriminants'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7：线性判别
- en: 'LDA involves developing a probabilistic model per class based on the distribution
    of each input variable (*Figure 3**.7*). It may be considered as an application
    of the Bayes'' theorem for classification and assumes that the input variables
    are uncorrelated; if they are correlated, the PCA transform may aid in removing
    the linear dependence. The scikit-learn library provides functions for LDA. Example
    code with a synthetic dataset is given here:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: LDA涉及为每个类别基于每个输入变量的分布开发一个概率模型（*图 3.7*）。它可以视为应用贝叶斯定理进行分类，并假设输入变量是无关的；如果它们是相关的，PCA变换可以帮助去除线性依赖。scikit-learn库提供了LDA的函数。这里给出了一个使用合成数据集的示例代码：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding example, the hyperparameter (`solver`) in the grid search is
    set to `'svd'` (default) but other `solver` values can also be used. This example
    only introduces us to using LDA with scikit-learn; there is a whole lot of customization
    that can be done depending on the problem being solved.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，网格搜索中的超参数（`solver`）设置为`'svd'`（默认值），但也可以使用其他`solver`值。此示例只是介绍了如何在scikit-learn中使用LDA；根据解决的问题，实际上可以进行大量定制。
- en: We have explored the linear algebraic methods for dimensionality reduction;
    we shall learn about the most important applications of PCA in the next section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了用于降维的线性代数方法；接下来我们将学习PCA的最重要应用。
- en: Applications of PCA
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA的应用
- en: PCA is one fundamental algorithm and forms the foundation of ML. It finds use
    in diverse areas such as noise reduction in images, classification of data in
    general, anomaly detection, and other applications in medical data correlation.
    We will explore a couple of widely used applications of PCA in this section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一个基础算法，是机器学习的基石。它广泛应用于图像噪声减少、数据分类、异常检测以及医学数据相关性等领域。在本节中，我们将探讨PCA的几个广泛应用。
- en: 'The scikit-learn library in Python provides functions for PCA. The following
    example code shows how to leverage PCA for dimensionality reduction while developing
    a predictive model that uses a PCA projection as input. We will be using PCA on
    a synthetic dataset while fitting a logistic regression model for classification:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Python中的scikit-learn库提供了PCA的函数。以下示例代码展示了如何在开发预测模型时，利用PCA进行降维，并将PCA投影作为输入。我们将在一个合成数据集上使用PCA，同时拟合一个用于分类的逻辑回归模型：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will plot the principal components using the following code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码绘制主成分：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding example, we do not see improvement in the model accuracy beyond
    eight components (*Figure 3**.8*). It is evident that the first eight components
    contain maximum information about the class and the remaining ones are redundant.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们没有看到模型准确率在超过八个主成分后有所提升（*图3.8*）。显然，前八个主成分包含了关于类别的最大信息，其余的则是冗余的。
- en: '![Figure 3.8: Classification accuracy versus the number of PCs for a synthetic
    dataset](img/Figure_03_07_B18943.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图3.8：合成数据集上分类准确率与主成分数量的关系](img/Figure_03_07_B18943.jpg)'
- en: 'Figure 3.8: Classification accuracy versus the number of PCs for a synthetic
    dataset'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8：合成数据集上分类准确率与主成分数量的关系
- en: The number of components after a PCA transform of features that results in the
    best average performance of the model is chosen and fed to the ML model for predictions.
    In the following subsection, we will learn about denoising and the detection of
    outliers using PCA.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在PCA转换特征后的主成分数量，是选择能够提供最佳平均模型性能的数量，然后将其输入到机器学习模型中进行预测。在接下来的小节中，我们将学习如何使用PCA进行去噪和异常点检测。
- en: Noise reduction
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 噪声减少
- en: 'PCA finds use in the reduction of noise in data, especially images. PCA reconstruction
    of an image by denoising can be achieved with the decomposition method of the
    scikit-learn Python library. Details of the library function with examples can
    be found here: [https://scikit-learn.org/stable/auto_examples/applications/plot_digits_denoising.xhtml](https://scikit-learn.org/stable/auto_examples/applications/plot_digits_denoising.xhtml)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: PCA用于减少数据中的噪声，尤其是在图像处理中。PCA通过去噪对图像进行重建可以通过使用scikit-learn Python库的分解方法实现。该库函数的详细信息和示例可以在此找到：[https://scikit-learn.org/stable/auto_examples/applications/plot_digits_denoising.xhtml](https://scikit-learn.org/stable/auto_examples/applications/plot_digits_denoising.xhtml)
- en: A good exercise would be to reconstruct images obtained from a video sequence
    exploring linear PCA as well as kernel PCA and check which one provides smoother
    images.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的练习是重建从视频序列中获得的图像，探索线性PCA和核PCA，并检查哪种方法提供更平滑的图像。
- en: Image compression (*Figure 3**.9*) is another important application of PCA.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图像压缩（*图3.9*）是PCA的另一个重要应用。
- en: '![Figure 3.9: Image compression with PCA](img/Figure_03_08_B18943.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9：PCA图像压缩](img/Figure_03_08_B18943.jpg)'
- en: 'Figure 3.9: Image compression with PCA'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9：PCA图像压缩
- en: The percentage of variance expressed by the PCs determines how many features
    should be the input to deep learning models (neural networks) for image classification
    so that the computing performance is not affected while dealing with huge and
    high-dimensional datasets.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分所表示的方差百分比决定了应该有多少特征作为输入传递到深度学习模型（神经网络）中进行图像分类，以便在处理庞大和高维数据集时，不影响计算性能。
- en: Anomaly detection
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常检测
- en: 'Detecting anomalies is common in fraud detection, fault detection, and system
    health monitoring in sensor networks. PCA makes use of the cluster method for
    detecting an outlier, typically collective and unordered outliers. Cluster-based
    anomaly detection assumes that the inlying (normal) data points belong to large
    and dense clusters, and outlying (anomalous) ones belong to small or sparse clusters
    or do not belong to any of them. Example code with sample telemetry data can be
    found in the following repository: [https://github.com/ranja-sarkar/mm](https://github.com/ranja-sarkar/mm).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测在欺诈检测、故障检测和传感器网络中的系统健康监控中非常常见。PCA利用聚类方法来检测异常点，通常是集体性的和无序的异常点。基于聚类的异常检测假设，正常数据点属于大而密集的簇，而异常数据点则属于小的或稀疏的簇，或者根本不属于任何簇。带有示例遥测数据的代码可以在以下仓库找到：[https://github.com/ranja-sarkar/mm](https://github.com/ranja-sarkar/mm)。
- en: The PCs apply distance metrics to identify anomalies. PCA, in this case, determines
    what constitutes a normal class. As an exercise, you can use unsupervised learning
    methods such as K-means and Isolation Forest to detect outliers in the same dataset
    for a comparison of the results and gain more meaningful insights.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分应用距离度量来识别异常。在这种情况下，PCA确定什么构成正常类别。作为一个练习，你可以使用无监督学习方法，如K均值和隔离森林，在相同数据集上检测异常点，并比较结果，获取更多有意义的洞察。
- en: Summary
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we learned about two linear algebraic methods used to reduce
    the dimensionality of data: namely, principal component analysis and linear discriminant
    analysis. The focus was on PCA, which is an unsupervised method to reduce the
    feature space of high-dimensional data and to know why this reduction is necessary
    for solving business problems. We did a detailed study of the mathematics behind
    the algorithm and how it works in ML models. We also learned about a couple of
    important applications of PCA along with the Python code.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章，我们学习了两种用于减少数据维度的线性代数方法：即主成分分析（PCA）和线性判别分析（LDA）。重点是PCA，它是一种无监督方法，用于减少高维数据的特征空间，并了解为什么这种降维对于解决业务问题至关重要。我们详细研究了该算法背后的数学原理及其在机器学习模型中的工作原理。我们还了解了PCA的一些重要应用，并附带了Python代码。
- en: In the next chapter, we will learn about an optimization method called Gradient
    Descent, which is arguably the most common (and popular) algorithm to optimize
    neural networks. It is a learning algorithm that works by minimizing a given cost
    function. As the name suggests, it uses a gradient (derivative) iteratively to
    minimize the function.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习一种优化方法——梯度下降法，它可以说是优化神经网络中最常见（也是最流行）的算法。这是一种通过最小化给定的成本函数来工作的学习算法。顾名思义，它通过迭代使用梯度（导数）来最小化该函数。
