- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: Education is not the learning of facts,
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 教育不是学习事实，
- en: but the training of the mind to think.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 但是训练思维。
- en: '- Albert Einstein'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '- 阿尔伯特·爱因斯坦'
- en: Data is the new silicon of our age, and machine learning, coupled with biologically
    inspired cognitive systems, serves as the core foundation to not only enable but
    also accelerate the birth of the fourth industrial revolution. This book is dedicated
    to our parents, who through extreme hardship and sacrifice, made our education
    possible and taught us to always practice kindness.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是我们时代的新硅，而机器学习与生物启发式认知系统结合，不仅能够实现，还能够加速第四次工业革命的诞生。这本书献给我们的父母，他们通过极大的艰辛和牺牲，使我们的教育成为可能，并教导我们始终要行善。
- en: The *Apache Spark 2.x Machine Learning Cookbook* is crafted by four friends
    with diverse background, who bring in a vast experience across multiple industries
    and academic disciplines. The team has immense experience in the subject matter
    at hand. The book is as much about friendship as it is about the science underpinning
    Spark and Machine Learning. We wanted to put our thoughts together and write a
    book for the community that not only combines Spark’s ML code and real-world data
    sets but also provides context-relevant explanation, references, and readings
    for a deeper understanding and promoting further research. This book is a reflection
    of what our team would have wished to have when we got started with Apache Spark.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*Apache Spark 2.x机器学习食谱*由四位具有不同背景的朋友共同创作，他们在多个行业和学术学科中拥有丰富的经验。团队在手头的主题上拥有丰富的经验。这本书不仅关乎友谊，也关乎支撑Spark和机器学习的科学。我们希望将我们的想法汇集起来，为社区撰写一本书，不仅结合了Spark的ML代码和真实数据集，还提供了相关的解释、参考和阅读，以便更深入地理解并促进进一步的研究。这本书反映了我们团队在开始使用Apache
    Spark时希望拥有的东西。'
- en: My own interest in machine learning and artificial intelligence started in the
    mid eighties when I had the opportunity to read two significant artifacts that
    happened to be listed back to back in *Artificial Intelligence, An International
    Journal*, Volume 28, Number 1, February 1986\. While it has been a long journey
    for engineers and scientists of my generation, fortunately, the advancements in
    resilient distributed computing, cloud computing, GPUs, cognitive computing, optimization,
    and advanced machine learning have made the dream of long decades come true. All
    these advancements have become accessible for the current generation of ML enthusiasts
    and data scientists alike.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我对机器学习和人工智能的兴趣始于八十年代中期，当时我有机会阅读两篇重要的文章，恰好在1986年2月的*人工智能国际期刊*第28卷第1期中依次列出。对于我这一代工程师和科学家来说，这是一个漫长的旅程，幸运的是，弹性分布式计算、云计算、GPU、认知计算、优化和先进的机器学习的进步使长达数十年的梦想成真。所有这些进步对当前一代机器学习爱好者和数据科学家都变得可及。
- en: We live in one of the rarest periods in history--a time when multiple technological
    and sociological trends have merged at the same point in time. The elasticity
    of cloud computing with built-in access to ML and deep learning nets will provide
    a whole new set of opportunities to create and capture new markets. The emergence
    of Apache Spark as the *lingua franca* or the *common language* of near real-time
    resilient distributed computing and data virtualization has provided smart companies
    the opportunity to employ ML techniques at a scale without a heavy investment
    in specialized data centers or hardware.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生活在历史上最罕见的时期之一——多种技术和社会趋势在同一时间点融合。具有内置ML和深度学习网络访问权限的云计算的弹性将提供一整套新的机会，以创造和占领新的市场。Apache
    Spark作为近实时弹性分布式计算和数据虚拟化的*通用语言*的出现，为聪明的公司提供了在不需要大量投资于专门的数据中心或硬件的情况下应用ML技术的机会。
- en: The *Apache Spark 2.x Machine Learning Cookbook* is one of the most comprehensive
    treatments of the Apache Spark machine learning API, with selected subcomponents
    of Spark to give you the foundation you need before you can master a high-end
    career in machine learning and Apache Spark. The book is written with the goal
    of providing clarity and accessibility, and it reflects our own experience (including
    reading the source code) and learning curve with Apache Spark, which started with
    Spark 1.0.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*Apache Spark 2.x机器学习食谱*是对Apache Spark机器学习API最全面的处理之一，选择了Spark的子组件，为您提供在掌握机器学习和Apache
    Spark的高端职业之前所需的基础。本书的目标是提供清晰和易懂的内容，反映了我们自己的经验（包括阅读源代码）和学习曲线，我们从Spark 1.0开始。'
- en: The *Apache Spark 2.x Machine Learning Cookbook* lives at the intersection of
    Apache Spark, machine learning, and Scala for developers, and data scientists
    through a practitioner’s lens who not only has to understand the code but also
    the details, theory, and inner workings of a given Spark ML algorithm or API to
    establish a successful career in the new economy.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*Apache Spark 2.x机器学习食谱*处于Apache Spark、机器学习和Scala的交汇处，面向开发人员和数据科学家，通过实践者的视角，他们不仅需要理解代码，还需要了解给定Spark
    ML算法或API的细节、理论和内部工作，以在新经济中建立成功的职业。'
- en: The book takes the cookbook format to a whole new level by blending downloadable
    ready-to-run Apache Spark ML code recipes with background, actionable theory,
    references, research, and real-life data sets to help the reader understand the
    *what*, *how* and the *why* behind the extensive facilities offered by Spark for
    the machine learning library. The book starts by laying the foundations needed
    to succeed and then rapidly evolves to cover all the meaningful ML algorithms
    available in Apache Spark.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书采用食谱格式，将可下载的即时运行的Apache Spark ML代码配方与背景、可操作的理论、参考、研究和真实数据集相结合，以帮助读者理解Spark为机器学习库提供的广泛功能背后的*什么*、*如何*和*为什么*。本书从奠定成功所需的基础开始，然后迅速发展到涵盖Apache
    Spark中所有有意义的ML算法。
- en: What this book covers
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容
- en: '[Chapter 1](part0027.html#PNV60-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Spark Using Scala*, covers installing and configuring a
    real-life development environment with machine learning and programming with Apache
    Spark. Using screenshots, it walks you through downloading, installing, and configuring
    Apache Spark and IntelliJ IDEA along with the necessary libraries that would reflect
    a developer’s desktop in a real-world setting. It then proceeds to identify and
    list over 40 data repositories with real-world data sets that can help the reader
    in experimenting and advancing even further with the code recipes. In the final
    step, we run our first ML program on Spark and then provide directions on how
    to add graphics to your machine learning programs, which are used in the subsequent
    chapters.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第1章，《使用Scala实现Spark的实用机器学习》，涵盖了在实际开发环境中安装和配置机器学习和Apache Spark的内容。通过屏幕截图，它引导您下载、安装和配置Apache
    Spark和IntelliJ IDEA，以及必要的库，这些都反映了开发者在真实世界环境中的桌面。然后，它继续识别和列出了40多个真实数据集的数据存储库，这些数据集可以帮助读者通过实验和进一步的代码配方。最后，我们在Spark上运行我们的第一个ML程序，然后提供如何将图形添加到您的机器学习程序的指导，这些图形在后续章节中使用。
- en: '[Chapter 2](part0064.html#1T1400-4d291c9fed174a6992fd24938c2f9c77), *Just Enough
    Linear Algebra for Machine Learning with Spark*, covers the use of linear algebra
    (vector and matrix), which is the foundation of some of the most monumental works
    in machine learning. It provides a comprehensive treatment of the DenseVector,
    SparseVector, and matrix facilities available in Apache Spark, with the recipes
    in the chapter. It provides recipes for both local and distributed matrices, including
    RowMatrix, IndexedRowMatrix, CoordinateMatrix, and BlockMatrix to provide a detailed
    explanation of this topic. We included this chapter because mastery of the Spark
    and ML/MLlib was only possible by reading most of the source code line by line
    and understanding how the matrix decomposition and vector/matrix arithmetic work
    underneath the more course-grain algorithm in Spark.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 第2章，《Spark中机器学习的线性代数基础》，涵盖了线性代数（向量和矩阵）的使用，这是机器学习中一些最重要的工作的基础。它通过该章的配方全面介绍了Apache
    Spark中可用的DenseVector、SparseVector和矩阵功能。它提供了本地和分布式矩阵的配方，包括RowMatrix、IndexedRowMatrix、CoordinateMatrix和BlockMatrix，以提供对这个主题的详细解释。我们包括了这一章，因为只有通过逐行阅读大部分源代码，并理解矩阵分解和向量/矩阵算术在Spark中更粗粒度算法下的工作方式，才能掌握Spark和ML/MLlib。
- en: '[Chapter 3](part0116.html#3EK180-4d291c9fed174a6992fd24938c2f9c77), *Spark’s
    Three Data Musketeers for Machine Learning - Perfect Together,* provides an end-to-end
    treatment of the three pillars of resilient distributed data manipulation and
    wrangling in Apache spark. The chapter comprises detailed recipes covering RDDs,
    DataFrame, and Dataset facilities from a practitioner’s point of view. Through
    an exhaustive list of 17 recipes, examples, references, and explanation, it lays
    out the foundation to build a successful career in machine learning sciences.
    The chapter provides both functional (code) as well as non-functional (SQL interface)
    programming approaches to solidify the knowledge base reflecting the real demands
    of a successful Spark ML engineer at tier 1 companies.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章，《Spark的三大数据武士用于机器学习-完美结合》，提供了Apache Spark中具有弹性分布式数据处理和整理功能的三大支柱的端到端处理。该章节包括了从实践者的角度详细介绍RDDs、DataFrame和Dataset功能的详细配方。通过详尽的17个配方、示例、参考和解释，它奠定了在机器学习科学领域建立成功职业的基础。该章节提供了功能性（代码）和非功能性（SQL接口）的编程方法，以巩固知识基础，反映了一名成功的Spark
    ML工程师在一线公司的真实需求。
- en: '[Chapter 4](part0200.html#5UNGG0-4d291c9fed174a6992fd24938c2f9c77), *Common
    Recipes for Implementing a Robust Machine Learning System*, covers and factors
    out the tasks that are common in most machine learning systems through 16 short
    but to-the-point code recipes that the reader can use in their own real-world
    systems. It covers a gamut of techniques, ranging from normalizing data to evaluating
    the model output, using best practice metrics via Spark’s ML/MLlib facilities
    that might not be readily visible to the reader. It is a combination of recipes
    that we use in our day-to-day jobs in most situations but are listed separately
    to save on space and complexity of other recipes.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第4章，《实现强大机器学习系统的常见配方》，通过16个简短但直截了当的代码配方，涵盖了大多数机器学习系统中常见的任务，并将这些任务因素化，读者可以在自己的真实世界系统中使用。它涵盖了一系列技术，从数据归一化到评估模型输出，使用了Spark的ML/MLlib功能的最佳实践指标，这些可能不会立即对读者可见。这是我们在日常工作中大多数情况下使用的配方的组合，但单独列出以节省空间和其他配方的复杂性。
- en: '[Chapter 5](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part I*, is
    the first of two chapters exploring classification and regression in Apache Spark.
    This chapter starts with Generalized Linear Regression (GLM) extending it to Lasso,
    Ridge with different types of optimization available in Spark. The chapter then
    proceeds to cover Isotonic regression, Survival regression with multi-layer perceptron
    (neural networks) and One-vs-Rest classifier.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章，《Spark 2.0中回归和分类的实用机器学习-第一部分》，是探索Apache Spark中分类和回归的两章中的第一章。该章从广义线性回归（GLM）开始，扩展到具有不同类型优化的Lasso、Ridge。然后，该章继续涵盖等温回归、生存回归、多层感知器（神经网络）和一对多分类器。
- en: '[Chapter 6](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part II*, is
    the second of the two regression and classification chapters. This chapter covers
    RDD-based regression systems, ranging from Linear, Logistic, and Ridge to Lasso,
    using Stochastic Gradient Decent and L_BFGS optimization in Spark. The last three
    recipes cover Support Vector Machine (SVM) and Naïve Bayes, ending with a detailed
    recipe for ML pipelines that are gaining a prominent position in the Spark ML
    ecosystem.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第6章，“Spark 2.0中的回归和分类实用机器学习-第二部分”，是两个回归和分类章节中的第二部分。该章节涵盖了基于RDD的回归系统，从线性、逻辑和岭到套索，使用Spark中的随机梯度下降和L_BFGS优化。最后三个配方涵盖了支持向量机（SVM）和朴素贝叶斯，最后详细介绍了在Spark
    ML生态系统中占据重要位置的ML管道的配方。
- en: '[Chapter 7](part0374.html#B4LIC0-4d291c9fed174a6992fd24938c2f9c77), *Recommendation
    Engine that Scales with Spark*, covers how to explore your data set and build
    a movie recommendation engine using Spark’s ML library facilities. It uses a large
    dataset and some recipes in addition to figures and write-ups to explore the various
    methods of recommenders before going deep into collaborative filtering techniques
    in Spark.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第7章，“使用Spark扩展的推荐引擎”，介绍了如何探索数据集并利用Spark的ML库设施构建电影推荐引擎。它使用了大型数据集和一些配方，以及图表和解释，探索了推荐系统的各种方法，然后深入研究了Spark中的协同过滤技术。
- en: '[Chapter 8](part0401.html#BUDHI0-4d291c9fed174a6992fd24938c2f9c77), *Unsupervised
    Clustering with Apache Spark 2.0*, covers the techniques used in unsupervised
    learning, such as KMeans, Mixture, and Expectation (EM), Power Iteration Clustering
    (PIC), and Latent Dirichlet Allocation (LDA), while also covering the why and
    how to help the reader to understand the core concepts. Using Spark Streaming,
    the chapter commences with a real-time KMeans clustering recipe to classify the
    input stream into labeled classes via unsupervised means.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章，“Apache Spark 2.0中的无监督聚类”，涵盖了无监督学习中使用的技术，如KMeans、混合和期望（EM）、幂迭代聚类（PIC）和潜在狄利克雷分布（LDA），同时也涵盖了为了帮助读者理解核心概念而介绍的原因和方法。利用Spark
    Streaming，该章节以实时KMeans聚类配方开始，通过无监督手段将输入流分类为标记类。
- en: '[Chapter 9](part0437.html#D0O5Q0-4d291c9fed174a6992fd24938c2f9c77), *Optimization
    - Going Down the Hill with Gradient Descent*, is a unique chapter that walks you
    through optimization as it applies to machine learning. It starts from a closed
    form formula and quadratic function optimization (for example, cost function),
    to using Gradient Descent (GD) in order to solve a regression problem from scratch.
    The chapter helps to look underneath the hood by developing the reader’s skill
    set using Scala code while providing in-depth explanation of how to code and understand
    Stochastic Descent (GD) from scratch. The chapter concludes with one of Spark’s
    ML API to achieve the same concepts that we code from scratch.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第9章，“优化-使用梯度下降下山”，是一章独特的章节，它带领读者了解优化在机器学习中的应用。它从闭合形式公式和二次函数优化（例如成本函数）开始，到使用梯度下降（GD）来从头解决回归问题。该章节通过使用Scala代码来培养读者的技能，并深入解释如何编写和理解从头开始的随机下降（GD）。该章节以Spark的ML
    API结束，以实现我们从头开始编码的相同概念。
- en: '[Chapter 10](part0460.html#DMM2O0-4d291c9fed174a6992fd24938c2f9c77), *Building
    Machine Learning Systems with Decision Tree and Ensemble Models*, covers the Tree
    and Ensemble models for classification and regression in depth using Spark’s machine
    library. We use three real-world data sets to explore the classification and regression
    problems using Decision Tree, Random Forest Tree, and Gradient Boosted Tree. The
    chapter provides an in-depth explanation of these methods in addition to plug-and-play
    code recipes that explore Apache Spark’s machine library step by step.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第10章，“使用决策树和集成模型构建机器学习系统”，深入介绍了Spark的机器学习库中用于分类和回归的树和集成模型。我们使用三个真实世界的数据集，使用决策树、随机森林树和梯度提升树来探索分类和回归问题。该章节提供了这些方法的深入解释，以及逐步探索Apache
    Spark的机器学习库的即插即用代码配方。
- en: '[Chapter 11](part0494.html#EN3LS0-4d291c9fed174a6992fd24938c2f9c77), *The Curse
    of High-Dimensionality in Big Data*, demystifies the art and science of dimensionality
    reduction and provides a complete coverage of Spark’s ML/MLlib library, which
    facilitates this important concept in machine learning at scale. The chapter provides
    sufficient and in-depth coverage of the theory (the what and why) and then proceeds
    to cover two fundamental techniques available (the how) in Spark for the readers
    to use. The chapter covers Single Value Decomposition (SVD), which relates well
    with the second chapter and then proceeds to examine the Principal Component Analysis
    (PCA) in depth with code and write ups.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第11章，“大数据中的高维度诅咒”，揭开了降维的艺术和科学之谜，并全面介绍了Spark的ML/MLlib库，该库在大规模机器学习中促进了这一重要概念。该章节充分深入地介绍了理论（什么和为什么），然后继续介绍了Spark中供读者使用的两种基本技术（如何）。该章节涵盖了与第二章相关的奇异值分解（SVD），然后深入研究了主成分分析（PCA），并附有代码和解释。
- en: '[Chapter 12](part0512.html#F89000-4d291c9fed174a6992fd24938c2f9c77), *Implementing
    Text Analytics with Spark 2.0 ML Library*, covers the various techniques available
    in Spark for implementing text analytics at scale. It provides a comprehensive
    treatment by starting from the basics, such as Term Frequency (TF) and similarity
    techniques, such as Word2Vec, and moves on to analyzing a complete dump of Wikipedia
    for a real-life Spark ML project. The chapter concludes with an in-depth discussion
    and code for implementing Latent Semantic Analysis (LSA) and Topic Modeling with
    Latent Dirichlet Allocation (LDA) in Spark.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[第12章](part0512.html#F89000-4d291c9fed174a6992fd24938c2f9c77)，*使用Spark 2.0
    ML库实现文本分析*，介绍了Spark中用于实现大规模文本分析的各种技术。它从基础知识开始，如词频（TF）和相似性技术，如Word2Vec，然后继续分析完整的维基百科转储，用于实际的Spark
    ML项目。本章最后深入讨论并提供了在Spark中实现潜在语义分析（LSA）和使用潜在狄利克雷分配（LDA）进行主题建模的代码。'
- en: '[Chapter 13](part0538.html#G12EK0-4d291c9fed174a6992fd24938c2f9c77), *Spark
    Streaming and Machine Learning Library*, starts by providing an introduction to
    and the future direction of Spark streaming, and then proceeds to provide recipes
    for both RDD-based (DStream) and structured streaming to establish a baseline.
    The chapter then proceeds to cover all the available ML streaming algorithms in
    Spark at the time of writing this book. The chapter provides code and shows how
    to implement streaming DataFrame and streaming data sets, and then proceeds to
    cover queueStream for debugging before it goes into Streaming KMeans (unsupervised
    learning) and streaming linear models such as Linear and Logistic regression using
    real-world datasets.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[第13章](part0538.html#G12EK0-4d291c9fed174a6992fd24938c2f9c77)，*Spark Streaming和机器学习库*，首先介绍了Spark流处理的概念和未来发展方向，然后提供了基于RDD（DStream）和结构化流的食谱，以建立基线。本章然后继续介绍了在撰写本书时Spark中所有可用的ML流算法。本章提供了代码，并展示了如何实现流DataFrame和流数据集，然后继续介绍了用于调试的queueStream，然后进入了Streaming
    KMeans（无监督学习）和使用真实世界数据集的流线性模型，如线性和逻辑回归。'
- en: What you need for this book
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书所需的内容
- en: Please use the details from the software list document.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请使用软件清单文档中的详细信息。
- en: 'To execute the recipes in this book, you need a system running Windows 7 and
    above, or Mac 10, with the following software installed:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行本书中的食谱，您需要运行Windows 7及以上版本或Mac 10的系统，并安装以下软件：
- en: Apache Spark 2.x
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 2.x
- en: Oracle JDK SE 1.8.x
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oracle JDK SE 1.8.x
- en: JetBrain IntelliJ Community Edition 2016.2.X or later version
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JetBrain IntelliJ Community Edition 2016.2.X或更高版本
- en: Scala plug-in for IntelliJ 2016.2.x
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala插件适用于IntelliJ 2016.2.x
- en: Jfreechart 1.0.19
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jfreechart 1.0.19
- en: breeze-core 0.12
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: breeze-core 0.12
- en: Cloud9 1.5.0 JAR
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud9 1.5.0 JAR
- en: Bliki-core 3.0.19
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bliki-core 3.0.19
- en: hadoop-streaming 2.2.0
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: hadoop-streaming 2.2.0
- en: Jcommon 1.0.23
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jcommon 1.0.23
- en: Lucene-analyzers-common 6.0.0
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lucene-analyzers-common 6.0.0
- en: Lucene-core-6.0.0
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lucene-core-6.0.0
- en: Spark-streaming-flume-assembly 2.0.0
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark-streaming-flume-assembly 2.0.0
- en: Spark-streaming-kafka-assembly 2.0.0
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark-streaming-kafka-assembly 2.0.0
- en: The hardware requirements for this software are mentioned in the software list
    provided with the code bundle of this book.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此软件的硬件要求在本书的代码包中提供的软件清单中有所提及。
- en: Who this book is for
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适合谁
- en: This book is for Scala developers with a fairly good exposure to and understanding
    of machine learning techniques, but who lack practical implementations with Spark.
    A solid knowledge of machine learning algorithms is assumed, as well as some hands-on
    experience of implementing ML algorithms with Scala. However, you do not need
    to be acquainted with the Spark ML libraries and the ecosystem.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本书适用于Scala开发人员，他们对机器学习技术有相当丰富的经验和理解，但缺乏Spark的实际实现。假定您具有扎实的机器学习算法知识，以及一些使用Scala实现ML算法的实际经验。但是，您不需要熟悉Spark
    ML库和生态系统。
- en: Sections
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部分
- en: 'In this book, you will find several headings that appear frequently (Getting
    ready, How to do it…, How it works…, There''s more…, and See also). To give clear
    instructions on how to complete a recipe, we use these sections as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，您将经常看到几个标题（准备工作、如何做、工作原理、更多内容和参见）。为了清晰地说明如何完成食谱，我们使用以下部分：
- en: Getting ready
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: This section tells you what to expect in the recipe, and describes how to set
    up any software or any preliminary settings required for the recipe.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本节告诉您在食谱中可以期待什么，并描述了为食谱设置任何软件或任何先决设置所需的步骤。
- en: How to do it…
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: This section contains the steps required to follow the recipe.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含了遵循食谱所需的步骤。
- en: How it works…
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理…
- en: This section usually consists of a detailed explanation of what happened in
    the previous section.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通常包括对前一节中发生的事情的详细解释。
- en: There's more…
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容…
- en: This section consists of additional information about the recipe in order to
    make the reader more knowledgeable about the recipe.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包括有关食谱的额外信息，以使读者更加了解食谱。
- en: See also
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: This section provides helpful links to other useful information for the recipe.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了有关食谱的其他有用信息的链接。
- en: Conventions
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 约定
- en: 'In this book, you will find a number of text styles that distinguish between
    different kinds of information. Here are some examples of these styles and an
    explanation of their meaning. Code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles are shown as follows: "Mac users note that we installed Spark 2.0 in the
    `/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/` directory on a Mac machine."'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，您将找到许多文本样式，用于区分不同类型的信息。以下是一些样式的示例及其含义的解释。文本中的代码词、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟URL、用户输入和Twitter句柄显示如下："Mac用户请注意，我们在Mac机器上的`/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/`目录中安装了Spark
    2.0。"
- en: 'A block of code is set as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块设置如下：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出都按以下方式编写：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**New terms** and **important words** are shown in bold. Words that you see
    on the screen, for example, in menus or dialog boxes, appear in the text like
    this: "Configure Global Libraries. Select Scala SDK as your global library."'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**新术语**和**重要单词**以粗体显示。您在屏幕上看到的单词，例如菜单或对话框中的单词，会以这种方式出现在文本中："配置全局库。选择 Scala
    SDK 作为您的全局库。"'
- en: Warnings or important notes appear like this.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 警告或重要提示会显示为这样。
- en: Tips and tricks appear like this.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 提示和技巧会显示为这样。
- en: Reader feedback
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读者反馈
- en: Feedback from our readers is always welcome. Let us know what you think about
    this book-what you liked or disliked. Reader feedback is important for us as it
    helps us develop titles that you will really get the most out of. To send us general
    feedback, simply e-mail `feedback@packtpub.com`, and mention the book's title
    in the subject of your message. If there is a topic that you have expertise in
    and you are interested in either writing or contributing to a book, see our author
    guide at [www.packtpub.com/authors](http://www.packtpub.com/authors).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的读者反馈总是受欢迎的。让我们知道你对这本书的想法-你喜欢或不喜欢什么。读者的反馈对我们很重要，因为它帮助我们开发您真正能从中获益的标题。要发送一般反馈，只需发送电子邮件至`feedback@packtpub.com`，并在消息主题中提及书名。如果您在某个专题上有专业知识，并且有兴趣撰写或为一本书做出贡献，请查看我们的作者指南[www.packtpub.com/authors](http://www.packtpub.com/authors)。
- en: Customer support
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户支持
- en: Now that you are the proud owner of a Packt book, we have a number of things
    to help you to get the most from your purchase.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您是 Packt 图书的自豪所有者，我们有一些东西可以帮助您充分利用您的购买。
- en: Downloading the example code
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码
- en: 'You can download the example code files for this book from your account at
    [http://www.packtpub.com](http://www.packtpub.com). If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you. You can download the
    code files by following these steps:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[http://www.packtpub.com](http://www.packtpub.com)的帐户中下载本书的示例代码文件。如果您在其他地方购买了这本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便文件直接发送到您的电子邮件。您可以按照以下步骤下载代码文件：
- en: Log in or register to our website using your e-mail address and password.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录或注册到我们的网站，使用您的电子邮件地址和密码。
- en: Hover the mouse pointer on the SUPPORT tab at the top.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将鼠标指针悬停在顶部的“支持”标签上。
- en: Click on Code Downloads & Errata.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击代码下载和勘误。
- en: Enter the name of the book in the Search box.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索框中输入书名。
- en: Select the book for which you're looking to download the code files.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您要下载代码文件的书籍。
- en: Choose from the drop-down menu where you purchased this book from.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从下拉菜单中选择您购买这本书的地点。
- en: Click on Code Download.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击代码下载。
- en: 'You can also download the code files by clicking on the Code Files button on
    the book''s webpage at the Packt Publishing website. This page can be accessed
    by entering the book''s name in the Search box. Please note that you need to be
    logged in to your Packt account. Once the file is downloaded, please make sure
    that you unzip or extract the folder using the latest version of:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过在 Packt Publishing 网站上的书籍网页上点击“代码文件”按钮来下载代码文件。可以通过在搜索框中输入书名来访问该页面。请注意，您需要登录到您的
    Packt 帐户。文件下载后，请确保您使用最新版本的解压缩或提取文件夹：
- en: WinRAR / 7-Zip for Windows
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WinRAR / 7-Zip 适用于 Windows
- en: Zipeg / iZip / UnRarX for Mac
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zipeg / iZip / UnRarX 适用于 Mac
- en: 7-Zip / PeaZip for Linux
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 7-Zip / PeaZip 适用于 Linux
- en: The code bundle for the book is also hosted on GitHub at [https://github.com/PacktPublishing/Apache-Spark-2x-Machine-Learning-Cookbook](https://github.com/PacktPublishing/Apache-Spark-2x-Machine-Learning-Cookbook).
    We also have other code bundles from our rich catalog of books and videos available
    at **[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)**.
    Check them out!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 该书的代码包也托管在 GitHub 上，网址为[https://github.com/PacktPublishing/Apache-Spark-2x-Machine-Learning-Cookbook](https://github.com/PacktPublishing/Apache-Spark-2x-Machine-Learning-Cookbook)。我们还有其他代码包，来自我们丰富的图书和视频目录，可在[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)上找到。去看看吧！
- en: Errata
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 勘误
- en: Although we have taken every care to ensure the accuracy of our content, mistakes
    do happen. If you find a mistake in one of our books-maybe a mistake in the text
    or the code-we would be grateful if you could report this to us. By doing so,
    you can save other readers from frustration and help us improve subsequent versions
    of this book. If you find any errata, please report them by visiting [http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details of your errata. Once your errata are verified, your submission will
    be accepted and the errata will be uploaded to our website or added to any list
    of existing errata under the Errata section of that title. To view the previously
    submitted errata, go to [https://www.packtpub.com/books/content/support](https://www.packtpub.com/books/content/support)
    and enter the name of the book in the search field. The required information will
    appear under the Errata section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经尽一切努力确保内容的准确性，但错误确实会发生。如果您在我们的书中发现错误-可能是文本或代码中的错误-我们将不胜感激，如果您能向我们报告。通过这样做，您可以帮助其他读者避免挫折，并帮助我们改进本书的后续版本。如果您发现任何勘误，请访问[http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)，选择您的书，点击勘误提交表格链接，并输入您的勘误详情。一旦您的勘误经过验证，您的提交将被接受，并且勘误将被上传到我们的网站，或者添加到该书的勘误列表的现有勘误部分下。要查看以前提交的勘误，请转到[https://www.packtpub.com/books/content/support](https://www.packtpub.com/books/content/support)，并在搜索框中输入书名。所需的信息将出现在勘误部分下。
- en: Piracy
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 盗版
- en: Piracy of copyrighted material on the Internet is an ongoing problem across
    all media. At Packt, we take the protection of our copyright and licenses very
    seriously. If you come across any illegal copies of our works in any form on the
    Internet, please provide us with the location address or website name immediately
    so that we can pursue a remedy. Please contact us at `copyright@packtpub.com`
    with a link to the suspected pirated material. We appreciate your help in protecting
    our authors and our ability to bring you valuable content.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网上侵犯版权材料的盗版是跨媒体持续存在的问题。在Packt，我们非常重视版权和许可的保护。如果您在互联网上发现我们作品的任何形式的非法副本，请立即向我们提供位置地址或网站名称，以便我们采取补救措施。请通过`copyright@packtpub.com`与我们联系，并附上涉嫌盗版材料的链接。我们感谢您帮助保护我们的作者和我们为您提供有价值内容的能力。
- en: Questions
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: If you have a problem with any aspect of this book, you can contact us at `questions@packtpub.com`,
    and we will do our best to address the problem.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对本书的任何方面有问题，可以通过`questions@packtpub.com`与我们联系，我们将尽力解决问题。
