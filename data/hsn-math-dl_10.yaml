- en: Regularization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: In the previous chapter, we learned about (deep) feedforward neural networks
    and how they are structured. We learned how these architectures can leverage their
    hidden layers and non-linear activations to learn to perform well on some very
    challenging tasks, which linear models aren't able to do. We also saw that neural
    networks tend to overfit to the training data by learning noise in the dataset,
    which leads to errors in the testing data. Naturally, since our goal is to create
    models that generalize well, we want to close the gap so that our models perform
    just as well on both datasets. This is the goal of regularization—to reduce test
    error, sometimes at the expense of greater training error.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们学习了（深度）前馈神经网络及其结构。我们了解了这些架构如何利用隐藏层和非线性激活函数来学习如何在一些非常具有挑战性的任务上表现出色，而线性模型则无法做到这一点。我们还看到神经网络倾向于通过学习数据集中的噪声来过拟合训练数据，从而导致测试数据上的误差。自然地，由于我们的目标是创建能够很好地泛化的模型，我们希望缩小差距，使我们的模型在两个数据集上表现得同样好。这正是正则化的目标——减少测试误差，有时可能会牺牲一定的训练误差。
- en: In this chapter, we will cover a variety of methods used in regularization,
    how they work, and why certain techniques are preferred over others. This includes
    limiting the capacity of a neural network, applying norm penalties and dataset
    augmentation, and more.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍多种用于正则化的方法，讲解它们的工作原理，并解释为何某些技术比其他技术更受偏好。这包括限制神经网络的能力、应用范数惩罚和数据集扩增等内容。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: The need for regularization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化的必要性
- en: Norm penalties
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 范数惩罚
- en: Early stopping
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 早停法
- en: Parameter typing and sharing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数类型和共享
- en: Dataset augmentation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集扩增
- en: Dropout
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout
- en: Adversarial training
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗训练
- en: The need for regularization
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化的必要性
- en: In previous chapters, we learned how feedforward neural networks are basically
    a complex function that maps an input to a corresponding target/label by learning
    the underlying distribution using the training data. We can recall that during
    training, after an error has been calculated during the forward pass, backpropagation
    is used to update the parameters in order to reduce the loss and better approximate
    the data distribution. We also learned about the capacity of neural networks,
    the bias-variance trade-off, and how neural networks can underfit or overfit to
    the training data, which prevents it from being able to perform well on unseen
    data or test data (that is, a generalization error occurs).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了前馈神经网络如何通过使用训练数据学习底层分布，将输入映射到相应的目标/标签，从而基本上成为一个复杂的函数。我们回想一下，在训练过程中，前向传播计算出误差后，反向传播用于更新参数，以减少损失，更好地逼近数据分布。我们还学习了神经网络的能力、偏差-方差权衡以及神经网络如何过拟合或欠拟合训练数据，导致它在未见数据或测试数据上表现不佳（即发生了泛化误差）。
- en: Before we get into what exactly regularization is, let's revisit overfitting
    and underfitting. Neural networks, as we know, are universal function approximators.
    Deep neural networks have many hidden layers, which means there are a lot of parameters
    that need to be trained. As a general rule, the more parameters a model has, the
    more complex it is, which means there's a greater risk of it overfitting to the
    training data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨正则化的具体内容之前，让我们先回顾一下过拟合和欠拟合。正如我们所知，神经网络是通用的函数逼近器。深度神经网络具有多个隐藏层，这意味着有很多需要训练的参数。一般来说，模型的参数越多，模型越复杂，也就意味着它过拟合训练数据的风险更大。
- en: 'This means that our model has perfectly learned all the patterns that exist
    in the data, including the noise, and has zero loss on the training data, but
    has a high loss on the test data. Additionally, overfitted models, in general,
    have a lower bias and a very high variance. Conversely, models with fewer parameters
    tend to be simpler, which means they are more likely to underfit to the training
    data because they observe a small portion of the data that doesn''t differ much.
    Therefore, they tend to have a much greater bias, which also leads to high variance.
    The following diagram illustrates the preceding explanation:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的模型已经完美地学习了数据中所有的模式，包括噪声，在训练数据上的损失为零，但在测试数据上的损失很高。此外，过拟合的模型通常具有较低的偏差和非常高的方差。相反，参数较少的模型往往较为简单，这意味着它们更可能欠拟合训练数据，因为它们观察到的数据量较小，且这些数据之间差异不大。因此，它们往往具有更大的偏差，这也导致了高方差。以下图表说明了上述解释：
- en: '![](img/165391af-7a2f-46ab-ae26-0993b3440a6d.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/165391af-7a2f-46ab-ae26-0993b3440a6d.png)'
- en: Somewhere in between overfitting and underfitting is a sweet spot where we have
    the optimal capacity; that is, the model hyperparameters that are perfectly suited
    to the task and data at hand—this is what we are aiming for. This tells us that
    the goal of regularization is to prevent our model from overfitting and that we
    prefer simpler models over vastly complex ones. However, the best model is one
    that is large and properly regularized.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在过拟合和欠拟合之间有一个甜蜜点，我们在这个位置上有最优的容量；也就是说，模型的超参数是完全适应当前任务和数据的——这就是我们的目标。它告诉我们，正则化的目标是防止模型过拟合，我们更倾向于选择较简单的模型，而不是复杂的模型。然而，最好的模型是一个大型并且正则化得当的模型。
- en: Now that we know the purpose of regularization, let's explore some of the many
    ways that we can regularize our deep neural networks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了正则化的目的，接下来让我们探讨一些我们可以用来正则化深度神经网络的方法。
- en: Norm penalties
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 范数惩罚
- en: 'Adding a parameter norm penalty to the objective function is the most classic
    of the regularization methods. What this does is limit the capacity of the model.
    This method has been around for several decades and predates the advent of deep
    learning. We can write this as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标函数中添加一个参数范数惩罚是最经典的正则化方法。这么做的目的是限制模型的容量。这个方法已经存在了几十年，甚至早于深度学习的出现。我们可以将其写为如下形式：
- en: '![](img/e8bc159d-b7cf-44a3-824e-064daec6d725.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8bc159d-b7cf-44a3-824e-064daec6d725.png)'
- en: Here, [![](img/535e6a8d-0d2d-47bc-8ad3-07bcbb01091e.png)]. The *α* value, in
    the preceding equation, is a hyperparameter that determines how large a regularizing
    effect the regularizer will have on the regularized cost function. The greater the
    value of *α* is, the more regularization is applied, and the smaller it is, the
    less of an effect regularization has on the cost function.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，[![](img/535e6a8d-0d2d-47bc-8ad3-07bcbb01091e.png)]。前述方程中的 *α* 值是一个超参数，决定了正则化项对正则化成本函数的影响有多大。*α*
    值越大，正则化作用越强，越小则正则化对成本函数的影响越小。
- en: In the case of neural networks, we only apply the parameter norm penalties to
    the weights since they control the interaction or relationship between two nodes
    in successive layers, and we leave the biases as they are since they need less
    data in comparison to the weights.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的情况下，我们仅对权重应用参数范数惩罚，因为它们控制着连续层之间两个节点的交互或关系，而我们保持偏置不变，因为它们相比权重需要的数据较少。
- en: There are a few different choices we can make when it comes to what kind of
    parameter norm to use, and each has a different effect on the solution.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择使用哪种参数范数时，我们有几种不同的选择，每一种都有不同的解决效果。
- en: L2 regularization
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L2 正则化
- en: 'The L2 regularization method is often referred to as **ridge regression** (but
    more commonly known as **weight decay**). It forces the weights of the network
    in the direction of the origin through the following regularization term to the
    objective function:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化方法通常被称为 **岭回归**（但更常见的名字是 **权重衰减**）。它通过以下正则化项，迫使网络的权重朝向原点收缩：
- en: '![](img/ec96d3c7-c42a-4279-9a24-3d58349d85e2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec96d3c7-c42a-4279-9a24-3d58349d85e2.png)'
- en: For simplicity, we will assume that *θ* = *w* and that all the letters are matrices.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们假设 *θ* = *w*，且所有字母都代表矩阵。
- en: 'The regularized objective function, in this case, will be as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，正则化后的目标函数如下：
- en: '![](img/aab6dd33-f3d6-4e56-8f1d-de0206215020.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aab6dd33-f3d6-4e56-8f1d-de0206215020.png)'
- en: 'If we take its gradient, then it becomes the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计算它的梯度，那么就变成了以下形式：
- en: '![](img/ad2c2931-b5b2-4137-ae9b-3346f20ea6df.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad2c2931-b5b2-4137-ae9b-3346f20ea6df.png)'
- en: 'Using the preceding gradient, we can calculate the update for the weights at
    each gradient step, as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述的梯度，我们可以计算每一步梯度更新的权重更新，如下所示：
- en: '![](img/48b66bb6-9b56-4820-ac10-bdac9cae0707.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48b66bb6-9b56-4820-ac10-bdac9cae0707.png)'
- en: 'We can expand and rewrite the right-hand side of the preceding update as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以展开并重写前述更新式的右侧如下：
- en: '![](img/b96ed906-1d3e-4008-a0a9-d1f1bed34247.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b96ed906-1d3e-4008-a0a9-d1f1bed34247.png)'
- en: 'From this equation, we can clearly see that the modified learning rule causes
    our weight to shrink by [![](img/436ce41e-e6f7-4bc4-a7b1-a2221ec84250.png)] at
    every step, as in the following diagram:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个方程中，我们可以清晰地看到，修改后的学习规则使得我们的权重在每一步都按 [![](img/436ce41e-e6f7-4bc4-a7b1-a2221ec84250.png)] 收缩，就像下面的图示一样：
- en: '![](img/09e2fc82-bb08-45e9-a9eb-18ec80d4362d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09e2fc82-bb08-45e9-a9eb-18ec80d4362d.png)'
- en: In the preceding diagram, we can see the effect that L2 regularization has on
    our weights. The solid circles toward the top-right side represent contours of
    equal value of the original object function, [![](img/78025785-3209-4d95-8579-e7deeb0a9c6f.png)],
    which we have not yet applied our regularizer to. The dotted circles, on the other
    hand, represent the contours of the regularizer term, ![](img/b633427d-d029-4083-99b0-bdcd65bd2202.png).
    Finally, ![](img/33f695a7-3bd0-4db4-a59b-d0a340606de8.png), the point where both
    the contours meet, represents when competing objectives reach equilibrium.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到L2正则化对我们权重的影响。右上方的实心圆表示原始目标函数的等值线，[![](img/78025785-3209-4d95-8579-e7deeb0a9c6f.png)]，我们尚未对其应用正则化项。另一方面，虚线圆圈表示正则化项的等值线，![](img/b633427d-d029-4083-99b0-bdcd65bd2202.png)。最后，![](img/33f695a7-3bd0-4db4-a59b-d0a340606de8.png)，两个等值线交汇的点代表两个目标达到平衡时的情况。
- en: L1 regularization
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L1正则化
- en: 'Another form of norm penalty is to use L1 regularization, which is sometimes
    referred to as **least absolute shrinkage and selection operator** (**lasso**)
    regression. In this case, the regularization term is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种范数惩罚形式是使用L1正则化，有时也称为**最小绝对收缩和选择算子**（**lasso**）回归。在这种情况下，正则化项如下：
- en: '![](img/c3058fed-c4ca-4434-888a-e2ea1c42cb6d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3058fed-c4ca-4434-888a-e2ea1c42cb6d.png)'
- en: What this does is it sums together the absolute values of the parameters. The
    effect that this has is that it introduces sparsity to our model by zeroing out
    some of the values, telling us that they aren't very important. This can be thought
    of as a form of feature selection.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它将参数的绝对值求和。其效果是通过将一些值归零，给我们的模型引入稀疏性，告诉我们这些值并不重要。这可以被看作是特征选择的一种形式。
- en: 'Similar to the preceding L2 regularization, in L1 regularization, the *α* hyperparameter controls
    how much of an effect the regularization has on the objective function:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于前面的L2正则化，在L1正则化中，*α*超参数控制正则化对目标函数的影响程度：
- en: '![](img/187af489-baa7-4607-afea-0ac2a430fd27.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/187af489-baa7-4607-afea-0ac2a430fd27.png)'
- en: 'This is illustrated as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 具体如下图所示：
- en: '![](img/ae903d9e-0c3c-47d3-b14f-b793ae1cb04c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae903d9e-0c3c-47d3-b14f-b793ae1cb04c.png)'
- en: As you can see in the preceding diagram, the contours of the objective function
    now meet at the axes instead of at a point away from it, as was the case in L2
    regularization, which is where the sparsity in this method comes from.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的图示中看到的，目标函数的轮廓现在与坐标轴相交，而不是像在L2正则化中那样与某一点相交，这也是该方法中稀疏性产生的原因。
- en: Now that we have learned how we can regularize our deep neural networks, let's
    have a look at what early stopping is in the following section.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何对深度神经网络进行正则化，接下来让我们看看什么是早停法。
- en: Early stopping
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 早停法
- en: During training, we know that our neural networks (which have sufficient capacity
    to learn the training data) have a tendency to overfit to the training data over
    many iterations, and then they are unable to generalize what they have learned
    to perform well on the test set. One way of overcoming this problem is to plot
    the error on the training and test sets at each iteration and analytically look
    for the iteration where the error from the training and test sets is the closest.
    Then, we choose those parameters for our model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们知道我们的神经网络（它有足够的容量来学习训练数据）有过拟合训练数据的倾向，经过多次迭代后，它们无法将所学知识推广到测试集上，表现不佳。克服这个问题的一种方法是在每次迭代时绘制训练集和测试集的误差，并通过分析寻找训练集和测试集误差最接近的迭代次数。然后，我们选择这些参数作为我们的模型。
- en: 'Another advantage of this method is that this in no way alters the objective
    function in the way that parameter norms do, which makes it easy to use and means
    it doesn''t interfere with the network''s learning dynamics, which is shown in
    the following diagram:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的另一个优点是，它并不会像参数范数那样改变目标函数，这使得它易于使用，并且不会干扰网络的学习动态，正如下图所示：
- en: '![](img/28316bfe-c660-4d02-90b0-ce7936d2fdaf.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28316bfe-c660-4d02-90b0-ce7936d2fdaf.png)'
- en: However, this approach isn't perfect—it does have a downside. It is computationally
    expensive because we have to train the network longer than is needed and collect
    more data for it, and then observe the point where the performance started to
    degrade.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法并不完美——它确实存在一些缺点。它计算开销较大，因为我们必须训练网络更长时间并收集更多数据，然后观察性能开始下降的点。
- en: Parameter tying and sharing
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数绑定与共享
- en: The preceding parameter norm penalties work by penalizing the model parameters
    when they deviate from 0 (a fixed value). But sometimes, we may want to express
    prior knowledge about which parameters would be better suited to the model. Although
    we may not know what those parameters are, thanks to domain knowledge and the
    architecture of the model, we know that there are likely to be some dependencies
    between the parameters of the model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上述参数范数惩罚通过惩罚模型参数的偏离（从 0，即固定值）来起作用。但有时，我们可能希望表达一些关于哪些参数更适合模型的先验知识。虽然我们可能不知道这些参数是什么，但得益于领域知识和模型的架构，我们知道模型的参数之间可能存在一些依赖关系。
- en: 'These dependencies could be some specific parameters that are closer to some
    than to others. Let''s suppose we have two different models for a classification
    task and detect the same number of classes. Their input distributions, however,
    are not the same. Let''s name the first model *A* with *θ^((A))* parameters and
    the second model *B* with *θ^((B))* parameters. Both of these models map their
    respective inputs to the outputs:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些依赖关系可能是一些特定的参数，它们与某些参数比与其他参数更接近。假设我们有两个不同的分类任务模型，并且检测相同数量的类别。然而，它们的输入分布并不相同。我们将第一个模型命名为
    *A*，其参数为 *θ^((A))*，第二个模型命名为 *B*，其参数为 *θ^((B))*。这两个模型分别将各自的输入映射到输出：
- en: '![](img/03ecf77a-5b1e-43e1-84ab-d086d87e5109.png) and ![](img/4f0c1b9e-d87a-4b67-8bd6-e4385eaadc24.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03ecf77a-5b1e-43e1-84ab-d086d87e5109.png) 和 ![](img/4f0c1b9e-d87a-4b67-8bd6-e4385eaadc24.png)'
- en: Naturally, since both of these models are working on relatively similar (maybe
    even the same) task(s) and so likely have similar (or the same) input distributions,
    both model *A* and model *B*'s parameters should be close to each other.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两种模型都在执行相对类似（甚至可能是相同）任务，并且很可能具有相似（或相同）输入分布，因此模型 *A* 和模型 *B* 的参数应该彼此接近。
- en: 'We can use a parameter norm penalty, such as the L2 penalty, to determine the
    closeness of the *θ^((A))* and *θ^((B))* parameters, as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用参数范数惩罚，例如 L2 惩罚，来确定 *θ^((A))* 和 *θ^((B))* 参数的接近度，方法如下：
- en: '![](img/589248e9-de5a-45ad-9edb-660777b80a8e.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/589248e9-de5a-45ad-9edb-660777b80a8e.png)'
- en: We can use other metrics besides the L2 norm to measure the distance.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 L2 范数，我们还可以使用其他度量来衡量距离。
- en: This method of forcing parameters to be close to each other is referred to as
    **parameter sharing**. The reason for this is that this can be interpreted as
    the different models sharing a set of parameters. This approach is preferred to
    parameter norm penalties because it requires less memory since we only have to
    store a unique set of shared parameters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种强制使参数彼此接近的方法称为**参数共享**。其原因在于，这可以解释为不同模型共享一组参数。与参数范数惩罚相比，这种方法更为优选，因为它需要更少的内存，因为我们只需存储一组独特的共享参数。
- en: Dataset augmentation
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集增强
- en: Deep feedforward networks, as we have learned, are very data-hungry and they
    use all this data to learn the underlying data distribution so that they can use
    their gained knowledge to make predictions on unseen data. This is because the
    more data they see, the more likely it is that what they encounter in the test
    set will be an interpolation of the distribution they have already learned. But
    getting a large enough dataset with good-quality labeled data is by no means a
    simple task (especially for certain problems where gathering data could end up
    being very costly). A method to circumvent this issue is using data augmentation;
    that is, generating synthetic data and using it to train our deep neural network.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所学，深度前馈网络非常依赖数据，它们利用所有这些数据学习潜在的数据分布，以便能够使用所获得的知识对未见数据进行预测。这是因为它们看到的数据越多，它们在测试集上遇到的内容就越有可能是它们已经学习过的分布的插值。但获得一个足够大且具有良好质量标签的数据集绝非易事（尤其是对于某些问题，收集数据可能非常昂贵）。绕过这个问题的一种方法是使用数据增强；也就是说，生成合成数据并利用这些数据训练我们的深度神经网络。
- en: The way synthetic data generation works is that we use a generative model (more
    on this in [Chapter 12](916c9cb2-14fa-44d9-a899-90948a342c52.xhtml), *Generative
    Models*) to learn the underlying distribution of the dataset that we will use
    to train our network for the task at hand, and then use the generative model to
    create synthetic data that is similar to the ground-truth data so that it appears
    to have come from the same dataset. We can also add small variations to the synthetic
    data to make the model more robust to noise.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据生成的方式是我们使用生成模型（在[第12章](916c9cb2-14fa-44d9-a899-90948a342c52.xhtml)《生成模型》中会进一步讲解）来学习数据集的潜在分布，然后用生成模型创建类似于真实数据的合成数据，使其看起来像是来自同一数据集。我们还可以对合成数据添加一些小的变异，使模型对噪声更加鲁棒。
- en: This method has proven to be very effective in the case of computer vision—particularly
    object detection/classification—which we make use of in convolutional neural networks
    (which we will learn about in [Chapter 9](2c830a26-9964-47fb-8d69-904e4f087b95.xhtml),
    *Convolutional Neural Networks*).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在计算机视觉中已经被证明非常有效——特别是在目标检测/分类中——我们在卷积神经网络中使用了它（我们将在[第9章](2c830a26-9964-47fb-8d69-904e4f087b95.xhtml)《卷积神经网络》中学习到更多内容）。
- en: Another type of data augmentation that is often used in image recognition is
    image cropping and image rotation, where we either crop a large segment of the
    input image or rotate it by some angle. These methods have also been proven to
    increase robustness and improve generalization on unseen data. We could also corrupt,
    blur, or add in some Gaussian noise to the images to make the network more robust
    since a lot of real-world data tends to be noisy.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常用于图像识别的数据增强方法是图像裁剪和图像旋转，我们可以对输入图像进行裁剪，或者将其旋转一定的角度。这些方法也被证明能增加鲁棒性，并提高在未见数据上的泛化能力。我们还可以对图像进行损坏、模糊处理，或加入一些高斯噪声，以使网络更具鲁棒性，因为许多真实世界的数据往往是噪声较大的。
- en: However, there are limitations to this. For example, in the case of optical
    character recognition (where we want to recognize letters and numbers), horizontal
    flips and 180-degree rotations can affect the class. After a transformation, a
    *b* can turn into a *d* and a *6* can turn into a *9*. There are also some problems
    where data augmentation simply isn't an option; an example of this is in the medical
    domain where we could be trying to work with MRI and CT scans. However, what we
    could do, in this case, is apply affine transformations, such as rotations and
    translations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这也存在一些局限性。例如，在光学字符识别（我们希望识别字母和数字）的情况下，水平翻转和180度旋转可能会影响类别。经过变换后，*b* 可能变成 *d*，*6*
    可能变成 *9*。在一些情况下，数据增强根本就不是一个选项；例如，在医学领域，我们可能需要处理MRI和CT扫描数据。然而，在这种情况下，我们可以应用仿射变换，比如旋转和平移。
- en: Let's focus for a moment on noise injection. There are two ways we can do this—the
    first is to inject noise to the input data and the second is to inject noise into
    the hidden units. In fact, it has been found that the addition of noise to hidden
    units can be a much better regularizer than parameter shrinking because it encourages
    stability.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时聚焦于噪声注入。有两种方法可以做到这一点——第一种是将噪声注入到输入数据中，第二种是将噪声注入到隐藏单元中。事实上，研究发现，将噪声添加到隐藏单元中，比参数收缩更能起到更好的正则化作用，因为它有助于提高稳定性。
- en: Dropout
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 丢弃法（Dropout）
- en: In the preceding section, we learned about applying penalties to the norm of
    the weights to regularize them, as well as other approaches, such as dataset augmentation
    and early stopping. However, there is another effective approach that is widely used
    in practice, known as dropout.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何通过对权重的范数应用惩罚来进行正则化，以及其他方法，例如数据集增强和早停。然而，还有一种在实际中广泛使用的有效方法，称为丢弃法（dropout）。
- en: So far, when training neural networks, all the weights have been learned together.
    However, dropout alters this idea by having the network only learn a fraction
    of the weights during each iteration. The reason for this is to avoid co-adaptation.
    This occurs when we train the entire network over all the training data and some
    connections end up stronger than others, thereby contributing more toward the
    network's predictive capabilities because the stronger connections overpower the
    weaker connections, effectively ignoring them. As we train the network with more
    iterations, some of the weaker connections essentially die out and are no longer
    trainable, so only a subnetwork ends up being trained, putting part of the network
    to waste. This is something that the preceding norm penalties are unable to address.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在训练神经网络时，所有权重都是一起学习的。然而，dropout 改变了这一观点，在每次迭代中，网络只学习一部分权重。这样做的原因是为了避免共同适应（co-adaptation）。当我们在所有训练数据上训练整个网络时，某些连接会比其他连接更强，从而对网络的预测能力贡献更大，因为强连接压倒了弱连接，实际上忽略了它们。随着我们进行更多次迭代训练，一些较弱的连接会逐渐“死掉”，不再可训练，因此最终只会训练出一个子网络，导致部分网络被浪费掉。前述的正规化惩罚方法是无法解决这个问题的。
- en: 'The way that dropout overcomes overfitting is by randomly (according to some
    predefined probability) removing (dropping out) neurons from a hidden layer; that
    is, we temporarily zero out some of the incoming and outgoing edges of a node
    so that it does not have an impact on the network during that training iteration.
    For example, if we have a **multilayer perceptron** (**MLP**) with one hidden
    layer that consists of 10 neurons and we have dropout with *p *= 0.5, then half
    the neurons are set to 0\. If *p *= 0.2, then 20 percent of the neurons are dropped:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: dropout 克服过拟合的方式是通过随机（根据预设的概率）从隐藏层中移除（drop out）神经元；也就是说，我们临时将某些节点的输入和输出边缘归零，使其在该次训练迭代中不对网络产生影响。例如，如果我们有一个**多层感知器**（**MLP**），它有一个包含
    10 个神经元的隐藏层，并且我们使用 dropout，*p* = 0.5，那么一半的神经元将被置为 0。如果 *p* = 0.2，则 20% 的神经元会被丢弃：
- en: '![](img/a5a6f686-7382-40c4-8525-104420cec101.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5a6f686-7382-40c4-8525-104420cec101.png)'
- en: 'Let''s consider an MLP with *L* hidden layers, such that [![](img/114466dd-a33d-42f4-88b2-9b7b9e0e84fe.png)],
    where the vector input to each layer is *z^((l))* and the output vector of each
    layer is *y^((l))* (for the sake of simplicity, *y^((0) )*= *x*). The weights
    and biases of each layer are denoted by *W^((l))* and *b^((l))*, respectively.
    Then, we get the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个具有 *L* 个隐藏层的 MLP，其中 [![](img/114466dd-a33d-42f4-88b2-9b7b9e0e84fe.png)]，每层的输入向量为
    *z^((l))*，每层的输出向量为 *y^((l))*（为了简便起见，*y^((0))* = *x*）。每层的权重和偏置分别用 *W^((l))* 和 *b^((l))*
    表示。然后，我们得到如下公式：
- en: '![](img/5623174a-02fd-42c4-9fc8-e11f03bd3582.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5623174a-02fd-42c4-9fc8-e11f03bd3582.png)'
- en: Here, *f* is any activation function.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*f* 是任意激活函数。
- en: 'Now, with dropout, the feedforward operations become the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用 dropout 后，前向传播操作变为以下形式：
- en: '![](img/0333e648-262e-4b49-91fb-8b0655e27aa6.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0333e648-262e-4b49-91fb-8b0655e27aa6.png)'
- en: So, we find that *p *= 0.5 has the best regularizing effect during training.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们发现 *p* = 0.5 在训练过程中具有最佳的正则化效果。
- en: Adversarial training
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗训练
- en: Nowadays, neural networks have started to reach human-level accuracy on a number
    of tasks, and in some, they can be seen to have even surpassed humans. But have
    they really surpassed humans or does it just seem this way? In production environments,
    we often have to deal with noisy data, which can cause our model to make incorrect
    predictions. So, we will now learn about another very important method of regularization—**adversarial
    training**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，神经网络在许多任务上已经达到了人类水平的准确性，在某些任务上甚至超越了人类。但它们真的超越了人类，还是仅仅看起来如此？在生产环境中，我们常常需要处理噪声数据，这可能会导致我们的模型做出错误的预测。因此，我们接下来将学习另一种非常重要的正则化方法——**对抗训练**。
- en: 'Before we get into the what and the how of adversarial training, let''s take
    a look at the following diagram:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论对抗训练的“是什么”和“如何做”之前，让我们先看一下下面的图示：
- en: '![](img/e3badced-6166-43aa-b615-25a0bf7b13b8.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3badced-6166-43aa-b615-25a0bf7b13b8.png)'
- en: What we have done, in the preceding diagram, is added in negligible Gaussian
    noise to the pixels of the original image. To us, the image looks exactly the
    same, but to a convolutional neural network, it looks entirely different. This
    is a problem, and it occurs even when our models are perfectly trained and have
    almost no error.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们所做的是在原始图像的像素中加入了微小的高斯噪声。对我们来说，图像看起来完全一样，但对卷积神经网络来说，图像看起来完全不同。这是一个问题，即使我们的模型训练得非常完美，几乎没有错误，依然会发生这种情况。
- en: 'What we do is find a data point, *x''*, which is near to *x*, but the model
    predicts *x''* to be part of a different class. Now, to add noise to our image,
    we can do so as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的是找到一个数据点，*x'*，它接近*x*，但模型预测*x'*属于不同的类别。现在，为了向我们的图像添加噪声，我们可以按如下方式进行：
- en: '![](img/06453a6a-bb69-4ffc-af11-cbbae42e4790.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06453a6a-bb69-4ffc-af11-cbbae42e4790.png)'
- en: The reason this interests us is that adding adversarially perturbed data samples
    to our training dataset can help reduce the error on our test set.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以关注这个问题，是因为向训练数据集中添加对抗性扰动的数据样本可以帮助减少测试集上的误差。
- en: Summary
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we covered a variety of methods that are used to regularize
    the parameters of a neural network. These methods are very important when it comes
    to training our models because they help ensure that they can generalize to unseen
    data by preventing overfitting, thereby performing well on the tasks we want to
    use them for. In the following chapters, we will learn about different types of
    neural networks and how each one is best suited for certain types of problems.
    Each neural network has a form of regularization that it can use to help improve
    performance.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们讨论了多种用于正则化神经网络参数的方法。这些方法在训练模型时非常重要，因为它们帮助确保模型能够推广到未见过的数据，防止过拟合，从而在我们希望使用它们的任务中表现良好。在接下来的章节中，我们将学习不同类型的神经网络，以及每种网络如何更好地解决某些类型的问题。每种神经网络都有一种正则化方法，可以用来帮助提升性能。
- en: In the next chapter, we will learn about convolutional neural networks, which
    are used for computer vision.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习卷积神经网络，它们用于计算机视觉。
