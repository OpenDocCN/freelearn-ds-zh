- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Monitoring the Health and Performance of a Data Science System
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控数据科学系统的健康状况和性能
- en: In this chapter, we will cover the extra mile so you are able to build robust,
    production-ready systems. One of the most important aspects to achieve this is
    to have all the data we need to ensure the system is operating correctly and detect
    as soon as possible when something goes wrong so we can take corrective actions.
    In this chapter, we’ll see how to set up a proper logging facility and how we
    can monitor the performance and health of our software in real time.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨，以便你能够构建稳健、适用于生产的系统。实现这一目标的最重要方面之一是拥有所有必要的数据，以确保系统正确运行，并尽早检测到问题，以便采取纠正措施。在本章中，我们将展示如何设置适当的日志设施，以及如何实时监控我们软件的性能和健康状况。
- en: 'We’re near the end of our journey into FastAPI for data science. Until now,
    we’ve mainly focused on the functionality of the programs we implemented. However,
    there is another aspect that is often overlooked by developers but is actually
    very important: *assessing whether the system is functioning correctly and reliably
    in production* and being warned as soon as possible when that’s not the case.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将结束 FastAPI 数据科学之旅。到目前为止，我们主要关注的是我们实现的程序的功能。然而，还有一个方面常常被开发者忽视，但实际上非常重要：*评估系统是否在生产环境中正确且可靠地运行*，并在系统出现问题时尽早收到警告。
- en: For this, lot of tools and techniques exist so we can gather the maximum amount
    of data about how our program is performing. That’s what we’ll review in this
    chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，存在许多工具和技术，我们可以收集尽可能多的数据，了解我们的程序如何运行。这就是我们在本章中要回顾的内容。
- en: 'We’re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主要主题：
- en: Configuring and using a logging facility with Loguru
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置并使用 Loguru 日志设施
- en: Configuring Prometheus metrics and monitoring them in Grafana
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 Prometheus 指标并在 Grafana 中监控它们
- en: Configuring Sentry for reporting errors
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 Sentry 用于报告错误
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, you’ll require a Python virtual environment, just as we set
    up in [*Chapter 1*](B19528_01.xhtml#_idTextAnchor024), *Python Development* *Environment
    Setup*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，你将需要一个 Python 虚拟环境，就像我们在[*第一章*](B19528_01.xhtml#_idTextAnchor024)中设置的那样，*Python
    开发环境设置*。
- en: 'To run a Dramatiq worker, you’ll need a running Redis server on your local
    computer. The easiest way is to run it as a Docker container. If you’ve never
    used Docker before, we recommend you read the *Getting started* tutorial in the
    official documentation at [https://docs.docker.com/get-started/](https://docs.docker.com/get-started/).
    Once done, you’ll be able to run a Redis server with this simple command:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 Dramatiq 工作程序，你需要在本地计算机上运行 Redis 服务器。最简单的方式是将其作为 Docker 容器运行。如果你以前从未使用过
    Docker，我们建议你阅读官方文档中的*入门教程*，[https://docs.docker.com/get-started/](https://docs.docker.com/get-started/)。完成后，你可以通过以下简单命令运行
    Redis 服务器：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You’ll find all the code examples of this chapter in the dedicated GitHub repository
    at [https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在专门的 GitHub 仓库中找到本章的所有代码示例：[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15)。
- en: A note about the screenshots
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 关于截图的说明
- en: 'In the course of this chapter, we’ll present several screenshots, in particular
    of the Grafana interface. Their goal is to show you the general layout of the
    UI to help you identify its different parts. Don’t worry if you struggle to read
    the actual content: the explanations around them will explain where to look at
    and what to interact with.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将展示一些截图，特别是 Grafana 界面的截图。它们的目的是帮助你了解界面的整体布局，帮助你识别不同的部分。如果你在阅读实际内容时遇到困难，不用担心：周围的解释将帮助你找到需要关注的地方并了解该与哪些部分交互。
- en: Configuring and using a logging facility with Loguru
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置并使用 Loguru 日志设施
- en: 'In software development, logs are probably the simplest but most powerful way
    to control the behavior of a system. They usually consist of lines of plain text
    that are printed at specific points of a program. By reading them chronologically,
    we are able to trace the behavior of the program and check that everything goes
    well. Actually, we’ve already seen log lines in this book. When you run a FastAPI
    app with Uvicorn and make some requests, you’ll see these lines in the console
    output:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件开发中，日志可能是控制系统行为最简单但最强大的方式。它们通常由程序中特定位置打印的纯文本行组成。通过按时间顺序阅读这些日志，我们可以追踪程序的行为，确保一切顺利进行。实际上，在本书中我们已经看到过日志行。当你使用
    Uvicorn 运行 FastAPI 应用并发出一些请求时，你会在控制台输出中看到这些日志行：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Those are the logs generated by Uvicorn, which tell us when it has started and
    when it has handled a request. As you can see, logs can help us to know what happened
    in our program and what actions it performed. They can also tell us when something
    goes wrong, which could be a bug that needs to be solved.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是 Uvicorn 生成的日志，告诉我们它何时启动，以及何时处理了一个请求。正如你所见，日志可以帮助我们了解程序发生了什么以及执行了哪些操作。它们还可以告诉我们何时出现问题，这可能是一个需要解决的
    bug。
- en: Understanding log levels
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解日志级别
- en: 'Notice that before each log line, we have the `INFO` keyword. This is what
    we call the **log level**. It’s a way to classify the importance of this log.
    In general, the following levels are defined:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在每个日志行之前，我们都有`INFO`关键字。这就是我们所说的**日志级别**。它是分类日志重要性的方式。一般来说，定义了以下几种级别：
- en: '`DEBUG`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEBUG`'
- en: '`INFO`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`INFO`'
- en: '`WARNING`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WARNING`'
- en: '`ERROR`'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ERROR`'
- en: 'You can consider this the *level of importance*: `DEBUG` is really specific
    information about what the program does, which could help you to debug the code,
    while `ERROR` means that something bad happened in your program, which probably
    requires action on your part. The good thing about those levels is that we can
    *configure the minimum level* that should be output by the logger. The actual
    call to the log function is still there in the code, but it’s ignored by the logger
    if it doesn’t match the minimum level.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将其视为*重要性等级*：`DEBUG`是关于程序执行的非常具体的信息，这有助于调试代码，而`ERROR`意味着程序中发生了问题，可能需要你采取行动。关于这些级别的好处是，我们可以*配置日志记录器应输出的最小级别*。即使日志函数调用仍然存在于代码中，如果它不符合最小级别，日志记录器也会忽略它。
- en: Typically, we can set the `DEBUG` level in local development so we have all
    the information to help us develop and fix our program. On the other hand, we
    can set the level to `INFO` or `WARNING` in production so we have only the most
    important messages.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们可以在本地开发中设置`DEBUG`级别，这样可以获取所有信息以帮助我们开发和修复程序。另一方面，我们可以在生产环境中将级别设置为`INFO`或`WARNING`，以便只获取最重要的消息。
- en: Adding logs with Loguru
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Loguru 添加日志
- en: 'Adding your own logs to a Python program can be fairly easy using the `logging`
    module available in the standard library. You could do something like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准库中提供的`logging`模块，向 Python 程序添加日志非常容易。你可以像这样做：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, it’s just a function call with a string in the argument. Typically,
    logging modules expose the different levels as methods, as you see here with `warning`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这只是一个带有字符串参数的函数调用。通常，日志模块将不同的级别作为方法暴露，就像这里的`warning`一样。
- en: The standard `logging` module is really powerful and allows you to finely customize
    how your logs are handled, printed, and formatted. If you go through the logging
    tutorials in the official documentation, [https://docs.python.org/3/howto/logging.html](https://docs.python.org/3/howto/logging.html),
    you’ll see it can quickly become really complex, even for simple cases.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的`logging`模块非常强大，允许你精细定制日志的处理、打印和格式。如果你浏览官方文档中的日志教程，[https://docs.python.org/3/howto/logging.html](https://docs.python.org/3/howto/logging.html)，你会发现它很快会变得非常复杂，甚至对于简单的情况也是如此。
- en: That’s why Python developers usually use libraries wrapping the `logging` module
    and exposing much more friendly functions and interfaces. In this chapter, we’ll
    review how to use and configure **Loguru**, a modern yet simple approach to logging.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么 Python 开发者通常使用封装了`logging`模块并提供更友好函数和接口的库。在本章中，我们将回顾如何使用和配置**Loguru**，一种现代而简单的日志处理方法。
- en: 'As always, the first thing to do is to install it in our Python environment:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，首先需要在我们的 Python 环境中安装它：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can try it right away in a Python shell:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即在 Python shell 中尝试：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You may think that’s not very different from what we did with the standard
    `logging` module. However, notice the resulting log already includes the timestamp,
    the level, and the position of the function call in the code. That’s one of the
    main benefits of Loguru: it comes with sensible defaults working out of the box.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为这与我们使用标准的 `logging` 模块没什么不同。然而，注意到生成的日志已经包含了时间戳、级别以及函数调用的位置。这就是 Loguru
    的主要优势之一：它自带合理的默认设置，开箱即用。
- en: 'Let’s see it in action in a more complete script. We’ll define a simple function
    to check whether an integer, `n`, is odd or not. We’ll add a debug line to let
    us know the function starts its logic. Then, before computing the result, we’ll
    first check whether `n` truly is an integer and log an error if not. The implementation
    of this function looks like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个更完整的脚本中看看它的实际效果。我们将定义一个简单的函数，检查一个整数 `n` 是否为奇数。我们将添加一行调试日志，让我们知道函数开始执行逻辑。然后，在计算结果之前，我们将首先检查
    `n` 是否确实是一个整数，如果不是，就记录一个错误。这个函数的实现如下：
- en: chapter15_logs_01.py
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: chapter15_logs_01.py
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_01.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_01.py)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_01.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_01.py)'
- en: 'As you can see, it’s really simple to use: we just have to import `logger`
    and call it wherever we need to log something. Notice also how we can add variables
    to format our string: we just need to add a placeholder around curly braces inside
    the string and then map each placeholder to its value with keyword arguments.
    This syntax is actually similar to the standard `str.format` method. You can read
    more about it in the official Python documentation: [https://docs.python.org/fr/3/library/stdtypes.html#str.format](https://docs.python.org/fr/3/library/stdtypes.html#str.format).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它的使用非常简单：我们只需要导入 `logger` 并在需要记录日志的地方调用它。还注意到我们如何可以添加变量来格式化字符串：只需要在字符串中添加大括号内的占位符，然后通过关键字参数将每个占位符映射到其值。这个语法实际上类似于标准的
    `str.format` 方法。你可以在官方的 Python 文档中了解更多内容：[https://docs.python.org/fr/3/library/stdtypes.html#str.format](https://docs.python.org/fr/3/library/stdtypes.html#str.format)。
- en: 'If we run this simple script, we’ll see our log lines in the console output:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行这个简单的脚本，我们将在控制台输出中看到我们的日志行：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Our log lines are correctly added to the output before the actual exception
    is raised. Notice how Loguru is able to precisely tell us where the log call comes
    from in the code: we have the function’s name and line.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的日志行在实际抛出异常之前已经正确添加到输出中。注意，Loguru 能够准确告诉我们日志调用来自代码的哪个位置：我们有函数名和行号。
- en: Understanding and configuring sinks
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和配置 sinks
- en: 'We’ve seen that, by default, logs are added to the console output. By default,
    Loguru defines a **sink** targeted at a standard error. A sink is a concept introduced
    by Loguru to define how log lines should be handled by the logger. We’re not limited
    to console output: we can also save them to a file, or a database, or even send
    them to a web service!'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，默认情况下，日志会添加到控制台输出。默认情况下，Loguru 定义了一个指向标准错误的**sink**。Sink 是 Loguru 引入的一个概念，用于定义日志行应该如何由日志记录器处理。我们不限于控制台输出：我们还可以将它们保存到文件、数据库，甚至发送到
    Web 服务！
- en: The good thing is that you’re not limited to only one sink; you can have as
    many as you need! Then, each log call will be processed through each sink accordingly.
    You can see a schematic representation of this approach in *Figure 15**.1*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 好的一点是，你并不只限于使用一个 sink；你可以根据需要使用多个！然后，每个日志调用都会通过每个 sink 进行处理。你可以在*图 15.1*中看到这种方法的示意图。
- en: '![Figure 15.1 – Schema of Loguru sinks](img/Figure_15.01_B19528.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.1 – Loguru sinks 架构](img/Figure_15.01_B19528.jpg)'
- en: Figure 15.1 – Schema of Loguru sinks
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.1 – Loguru sinks 架构
- en: 'Each *sink is associated* *with* *a log level*. This means that we could have
    different log levels depending on the sink. For example, we could choose to output
    all logs to a file and keep only the most important warning and error logs in
    the console. Let’s again take our previous example and configure Loguru with this
    approach:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 每个*sink 都与* *一个日志级别* *相关联*。这意味着我们可以根据 sink 使用不同的日志级别。例如，我们可以选择将所有日志输出到文件中，并且只在控制台保留最重要的警告和错误日志。我们再次以之前的示例为例，使用这种方式配置
    Loguru：
- en: chapter15_logs_02.py
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: chapter15_logs_02.py
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_02.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_02.py)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_02.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_02.py)'
- en: The `remove` method of `logger` is helpful for removing a previously defined
    sink. When calling it like this with no parameter, all the defined sinks are removed.
    By doing this, we start fresh without the default sink.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`logger`的`remove`方法有助于删除先前定义的接收器。当这样调用时，没有参数传递，所有定义的接收器都会被移除。通过这样做，我们可以从没有默认接收器的全新状态开始。'
- en: Then, we call `add` to define new sinks. The first parameter, like `sys.stdout`
    or `file.log` here, defines how the log calls should be handled. This parameter
    can be many things, such as a callable function, but Loguru allows us, for convenience,
    to directly pass file-like objects, such as `sys.stdout`, or strings, which will
    be interpreted as filenames. Several arguments are accepted to customize all the
    aspects of the sink and, in particular, the level.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们调用`add`来定义新的接收器。第一个参数，像`sys.stdout`或这里的`file.log`，定义了日志调用应该如何处理。这个参数可以是很多东西，比如一个可调用的函数，但为了方便，Loguru允许我们直接传递类似文件的对象，如`sys.stdout`，或被解释为文件名的字符串。接收器的所有方面都可以通过多个参数进行定制，尤其是日志级别。
- en: As we said, the standard output sink will only log messages with at least a
    `WARNING` level, while the file sink will log all messages.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说，标准输出接收器只会记录至少为`WARNING`级别的消息，而文件接收器会记录所有消息。
- en: 'Notice also that we added a `rotation` parameter for the file sink. Since logs
    will continuously be appended to a file, it can quickly grow in size during the
    lifetime of your application. That’s why we have access to a couple of options:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们为文件接收器添加了`rotation`参数。由于日志会不断附加到文件中，文件大小会在应用程序生命周期内迅速增长。因此，我们提供了一些选项供您选择：
- en: '**“Rotate” the file**: This means that the current file will be renamed, and
    new logs will be added to a new file. This operation can be configured so it happens
    after a certain amount of time (for example, every day, as in our example) or
    when it reaches a certain size.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“轮换”文件**：这意味着当前文件将被重命名，并且新的日志会添加到一个新文件中。此操作可以配置为在一段时间后发生（例如每天，如我们的示例）或当文件达到一定大小时。'
- en: '**Remove older files**: After a certain amount of time, it’s probably not very
    useful to keep older logs that take up unnecessary space on your disk.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**删除旧文件**：经过一段时间后，保留占用磁盘空间的旧日志可能就不太有用了。'
- en: 'You can read all the details about these features in the official documentation
    for Loguru: [https://loguru.readthedocs.io/en/stable/api/logger.html#file](https://loguru.readthedocs.io/en/stable/api/logger.html#file).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Loguru的官方文档中阅读有关这些功能的所有详细信息：[https://loguru.readthedocs.io/en/stable/api/logger.html#file](https://loguru.readthedocs.io/en/stable/api/logger.html#file)。
- en: 'Now, if we run this example, we’ll see this in the console output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们运行这个示例，我们将在控制台输出中看到以下内容：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `DEBUG` logs don’t appear anymore. However, if we read the `file.log` file,
    we’ll have both:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`DEBUG`级别的日志不再出现了。然而，如果我们读取`file.log`文件，我们将看到两者：'
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: That’s it! Sinks are really useful for routing our logs to different places
    depending on their nature or importance.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！接收器非常有用，可以根据日志的性质或重要性，将日志路由到不同的位置。
- en: Structuring logs and adding context
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志结构化和添加上下文
- en: In their simplest form, logs consist of free-form text. While convenient, we’ve
    seen that we usually need to log variable values to better understand what’s going
    on. With only strings, this usually ends up in a messy string consisting of multiple
    concatenated values.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的形式下，日志由自由格式的文本组成。虽然这样很方便，但我们已经看到，通常需要记录变量值，以便更好地理解发生了什么。仅用字符串时，通常会导致多个连接值拼接成的混乱字符串。
- en: 'A better approach to handle this is to adopt **structured logging**. The goal
    is to have a clear and proper structure for each log line, so we can embed all
    the information we need without sacrificing readability. Loguru supports this
    approach natively, thanks to contexts. The next example shows you how to use it:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的处理方式是采用**结构化日志记录**。目标是为每个日志行提供清晰且适当的结构，这样我们就可以在不牺牲可读性的前提下嵌入所有需要的信息。Loguru本身通过上下文支持这种方法。下一个示例展示了如何使用它：
- en: chapter15_logs_03.py
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: chapter15_logs_03.py
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_03.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_03.py)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_03.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_03.py)'
- en: We once again took the same example as before. As you can see, we use the `bind`
    method of logger to retain extra information. Here, we set the `n` variable. This
    method returns a new instance of our logger with those attributes attached. Then,
    we can use this instance normally to log things. We don’t need to add `n` in the
    formatted string anymore.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用之前的相同示例。如你所见，我们使用了 logger 的 `bind` 方法来保留额外信息。在这里，我们设置了 `n` 变量。这个方法返回一个新的
    logger 实例，并附加了这些属性。然后，我们可以正常使用这个实例来记录日志。我们不需要在格式化字符串中再添加 `n` 了。
- en: 'However, if you try this example directly, you won’t see the value of `n` in
    the logs. That’s normal: by default, Loguru doesn’t add context information to
    the formatted log line. We need to customize it! Let’s see how:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你直接运行这个示例，你将不会在日志中看到 `n` 的值。这是正常的：默认情况下，Loguru 不会将上下文信息添加到格式化的日志行中。我们需要自定义它！让我们看看如何操作：
- en: chapter15_logs_04.py
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: chapter15_logs_04.py
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_04.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_04.py)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_04.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_04.py)'
- en: To format log output, we have to use the `format` parameter when configuring
    a sink. It expects a template string. Here, we copied and pasted the default Loguru
    format and added a part with the `extra` variable. `extra` is a dictionary where
    Loguru stores all the values you added in context. Here, we just output it directly
    so we can see all variables.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要格式化日志输出，我们必须在配置 sink 时使用 `format` 参数。它期望一个模板字符串。在这里，我们复制并粘贴了默认的 Loguru 格式，并添加了一个包含
    `extra` 变量的部分。`extra` 是一个字典，Loguru 在其中存储所有你在上下文中添加的值。在这里，我们只是直接输出它，这样我们就能看到所有变量。
- en: Format syntax and available variables
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 格式语法和可用变量
- en: 'You can find all the available variables you can output in the format string,
    such as `extra` or `level`, in the Loguru documentation: [https://loguru.readthedocs.io/en/stable/api/logger.html#record](https://loguru.readthedocs.io/en/stable/api/logger.html#record).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 Loguru 文档中找到所有可用的变量，这些变量可以在格式字符串中输出，如 `extra` 或 `level`，网址为：[https://loguru.readthedocs.io/en/stable/api/logger.html#record](https://loguru.readthedocs.io/en/stable/api/logger.html#record)。
- en: 'The format string supports standard formatting directives, which are useful
    for retrieving values, format numbers, pad strings, and so on. You can read more
    about it in the Python documentation: [https://docs.python.org/3/library/string.html#format-string-syntax](https://docs.python.org/3/library/string.html#format-string-syntax).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 格式字符串支持标准的格式化指令，这些指令对于提取值、格式化数字、填充字符串等非常有用。你可以在 Python 文档中阅读更多相关内容：[https://docs.python.org/3/library/string.html#format-string-syntax](https://docs.python.org/3/library/string.html#format-string-syntax)。
- en: 'Also, Loguru adds special markup so you can color the output. You can read
    more about it here: [https://loguru.readthedocs.io/en/stable/api/logger.html#color](https://loguru.readthedocs.io/en/stable/api/logger.html#color).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Loguru 还添加了特殊的标记语法，你可以用它来为输出着色。你可以在这里了解更多内容：[https://loguru.readthedocs.io/en/stable/api/logger.html#color](https://loguru.readthedocs.io/en/stable/api/logger.html#color)。
- en: 'This time, if you run this example, you’ll see the extra context added to the
    log lines:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，如果你运行这个示例，你会看到额外的上下文信息已经被添加到日志行中：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This approach is very convenient and powerful: if you want to keep track of
    a value you care about across logs, you just have to add it once.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法非常方便且强大：如果你想在日志中追踪一个你关心的值，只需添加一次。
- en: Logs as JSON objects
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以 JSON 对象形式记录日志
- en: 'Another approach to structured logging is to serialize all the data of a log
    into a JSON object. This can be enabled easily with Loguru by setting `serialize=True`
    when configuring the sink. This approach can be interesting if you plan to use
    a log ingestion service such as Logstash or Datadog: they will be able to parse
    the JSON data and make it available for querying.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种结构化日志的方法是将日志的所有数据序列化为一个 JSON 对象。通过在配置 sink 时设置 `serialize=True`，可以轻松启用此功能。如果你计划使用日志摄取服务，如
    Logstash 或 Datadog，这种方法可能会很有用：它们能够解析 JSON 数据并使其可供查询。
- en: You now have the basics of adding and configuring logs with Loguru. Let’s now
    see how we can leverage them in a FastAPI application.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经掌握了使用 Loguru 添加和配置日志的基本知识。接下来，让我们看看如何在 FastAPI 应用中利用它们。
- en: Configuring Loguru as the central logger
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 Loguru 作为中央日志记录器
- en: Adding logs to your FastAPI application can be really useful to know what’s
    happening in your different routes and dependencies.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 向你的 FastAPI 应用添加日志非常有用，它能帮助你了解不同路由和依赖项中发生了什么。
- en: 'Let’s take an example from [*Chapter 5*](B19528_05.xhtml#_idTextAnchor285),
    where we added a global dependency to check for a secret value that should be
    set in the header. In this new version, we’ll add a debug log to trace when the
    `secret_header` dependency is called and a warning log to inform us when this
    secret is missing or invalid:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个来自 [*第 5 章*](B19528_05.xhtml#_idTextAnchor285) 的例子，我们在其中添加了一个全局依赖项，用于检查应该在头部设置的密钥值。在这个新版本中，我们将添加一个调试日志，以跟踪
    `secret_header` 依赖项何时被调用，并添加一个警告日志，告知我们此密钥缺失或无效：
- en: chapter15_logs_05.py
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: chapter15_logs_05.py
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_05.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_05.py)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_05.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_05.py)'
- en: 'That’s nothing really surprising if you have followed us so far! Now, let’s
    run this application with Uvicorn and make a request with an invalid header:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直跟随我们的教程，到这里应该没有什么令人惊讶的！现在，让我们用 Uvicorn 运行这个应用，并发出一个带有无效头部的请求：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Our own logs are here, but there is a problem: Uvicorn also adds its own logs,
    but it doesn’t follow our format! Actually, that’s expected: other libraries,
    such as Uvicorn, may have their own logs with their own settings. As such, they
    won’t follow what we defined with Loguru. It’s a bit annoying because if we have
    a complex, well-thought-out setup, we would like every log to follow it. Fortunately,
    there are ways to configure this.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们自己的日志在这里，但有一个问题：Uvicorn 也添加了它自己的日志，但是它没有遵循我们的格式！实际上，这是可以预料的：其他库，比如 Uvicorn，可能有自己的日志和设置。因此，它们不会遵循我们用
    Loguru 定义的格式。这有点让人烦恼，因为如果我们有一个复杂且经过深思熟虑的设置，我们希望每个日志都能遵循它。幸运的是，还是有一些方法可以配置它。
- en: 'First of all, we’ll create a module named `logger.py`, where we’ll put all
    our logger configurations. It’s a good practice in your project to have this module
    so your configuration is centralized in one place. The first thing we do in this
    file is to configure Loguru:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个名为 `logger.py` 的模块，在其中放置所有的日志配置。在你的项目中创建这个模块是一个很好的做法，这样你的配置就能集中在一个地方。我们在这个文件中做的第一件事是配置
    Loguru：
- en: logger.py
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: logger.py
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py)'
- en: As we did in the previous section, we removed the default handler and defined
    our own. Notice that we set the level thanks to a constant named `LOG_LEVEL`.
    We hardcoded it here, but a better way would be to take the value from a `Settings`
    object, as we showed in [*Chapter 10*](B19528_10.xhtml#_idTextAnchor694). This
    way, we could directly set the level from environment variables!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在上一节中所做的那样，我们移除了默认的处理器并定义了我们自己的。注意，我们通过一个名为 `LOG_LEVEL` 的常量来设置级别。我们在这里硬编码了它，但更好的做法是从
    `Settings` 对象中获取这个值，就像我们在 [*第 10 章*](B19528_10.xhtml#_idTextAnchor694) 中所示的那样。这样，我们可以直接从环境变量中设置级别！
- en: After that, we have a quite complex piece of code in the class named `InterceptHandler`.
    It’s a custom handler for the standard logging module that will forward every
    standard log call to Loguru. This code is directly taken from the Loguru documentation.
    We won’t go into much detail about its functioning but just know that it’ll retrieve
    the log level and go through the call stack to retrieve the original caller and
    forward this information to Loguru.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们在名为 `InterceptHandler` 的类中有一段相当复杂的代码。它是一个自定义处理器，针对标准日志模块，会将每个标准日志调用转发到
    Loguru。这段代码直接取自 Loguru 文档。我们不会深入讲解它的工作原理，但只需要知道它会获取日志级别并遍历调用栈来获取原始调用者，然后将这些信息转发给
    Loguru。
- en: 'The most important part, however, is how we use this class. Let’s see this
    here:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最重要的部分是我们如何使用这个类。让我们在这里看看：
- en: logger.py
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: logger.py
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py)'
- en: The trick here is to call the `basicConfig` method from the standard logging
    module to set our custom interception handler. This way, every log call made with
    the root logger, even ones from external libraries, will go through it and be
    handled by Loguru.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的技巧是调用标准日志模块中的`basicConfig`方法来设置我们的自定义拦截处理程序。这样，通过根日志记录器发出的每个日志调用，即使是来自外部库的，也会通过它并由Loguru处理。
- en: 'In some cases, however, this configuration is not sufficient. Some libraries
    define their own loggers with their own handlers, so they won’t use the root configuration.
    That’s the case for Uvicorn, which defines two main loggers: `uvicorn.error` and
    `uvicorn.access`. By retrieving those loggers and changing their handler, we force
    them to go through Loguru as well.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，这种配置是不够的。一些库定义了自己的日志记录器和处理程序，因此它们不会使用根配置。这对于Uvicorn来说就是这样，它定义了两个主要的日志记录器：`uvicorn.error`和`uvicorn.access`。通过获取这些日志记录器并更改其处理程序，我们强制它们也通过Loguru。
- en: If you use other libraries that define their own loggers like Uvicorn does,
    you’ll probably need to apply the same technique. All you need to determine is
    the name of their logger, which should be quite easy to find in the library’s
    source code.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用其他像Uvicorn一样定义自己日志记录器的库，你可能需要应用相同的技巧。你需要做的就是确定它们日志记录器的名称，这应该很容易在库的源代码中找到。
- en: It works out of the box with Dramatiq
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 它与Dramatiq开箱即用
- en: If you implement a worker with Dramatiq, as we showed in [*Chapter 14*](B19528_14.xhtml#_idTextAnchor1041),
    you’ll see that, if you use the `logger` module, the default logs of Dramatiq
    will be correctly handled by Loguru.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你实现了一个Dramatiq工作程序，正如我们在[*第14章*](B19528_14.xhtml#_idTextAnchor1041)中展示的那样，你会看到，如果你使用`logger`模块，Dramatiq的默认日志将会被Loguru正确处理。
- en: 'Finally, we take care of setting the `__all__` variable at the end of the module:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在模块的末尾处理设置`__all__`变量：
- en: logger.py
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: logger.py
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py)'
- en: '`__all__` is a special variable telling Python which variables should be made
    publicly available when importing this module. Here, we’ll expose `logger` from
    Loguru, so we can easily import it everywhere we need in our project.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`__all__`是一个特殊变量，告诉Python在导入此模块时应该公开哪些变量。在这里，我们将暴露Loguru中的`logger`，以便在项目中任何需要的地方都能轻松导入它。'
- en: 'Bear in mind that it’s not strictly necessary to use `__all__`: we could very
    well import `logger` without it, but it’s a clean way to hide other things we
    want to keep private, such as `InterceptHandler`, for example.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，使用`__all__`并不是严格必要的：我们完全可以在没有它的情况下导入`logger`，但它是一种干净的方式来隐藏我们希望保持私有的其他内容，例如`InterceptHandler`。
- en: 'Finally, we can use it as we saw previously in our code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以像之前在代码中看到的那样使用它：
- en: logger.py
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: logger.py
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py)'
- en: 'If we run it with Uvicorn, you’ll now see that all our logs are formatted the
    same way:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用Uvicorn运行它，你会发现我们所有的日志现在都以相同的格式显示：
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Great! Now, whenever you need to add logs in your app, all you need to do is
    to import `logger` from your `logger` module.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！现在，每当你需要在应用程序中添加日志时，所要做的就是从`logger`模块中导入`logger`。
- en: You now have the basics to add logs to your application, with plenty of options
    to fine-tune how and where you output them. Logs are very useful for monitoring
    what your application is doing at a micro-level, operation per operation. Another
    important aspect of monitoring is to have information at a more general level
    in order to have big figures and quickly detect if something goes wrong. That’s
    what we’ll see now with metrics.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经掌握了将日志添加到应用程序的基本知识，并有许多选项可以微调如何以及在哪里输出日志。日志对于监控你的应用程序在微观层面上的行为非常有用，逐操作地了解它在做什么。监控的另一个重要方面是获取更一般层面的信息，以便获取大的数据图并快速发现问题。这就是我们现在要通过指标来实现的目标。
- en: Adding Prometheus metrics
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加 Prometheus 指标
- en: 'In the previous section, we saw how logs can help us understand what our program
    is doing by finely tracing the operations it does over time. However, most of
    the time, you can’t afford to keep an eye on the logs all day: they are useful
    for understanding and debugging a particular situation but way less useful for
    getting global insights to alert you when something goes wrong.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到日志如何帮助我们通过精细地追踪程序随时间执行的操作，来理解程序的行为。然而，大多数时候，你不能整天盯着日志看：它们对于理解和调试特定情况非常有用，但对于获取全球性洞察力、在出现问题时发出警报却要差得多。
- en: 'To solve this, we’ll see in this section how to add **metrics** to our application.
    Their role is to measure things that matter in the execution of our program: the
    number of requests made, the time taken to give a response, the number of pending
    tasks in the worker queue, the accuracy of our ML predictions… Anything that we
    could easily monitor over time – usually, with charts and graphs – so we can easily
    monitor the health of our system. We say that we **instrument** our application.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将在本节中学习如何将**指标**添加到我们的应用程序中。它们的作用是衡量程序执行中重要的事项：发出的请求数量、响应时间、工作队列中待处理任务的数量、机器学习预测的准确性……任何我们可以轻松地随时间监控的事情——通常通过图表和图形——这样我们就能轻松监控系统的健康状况。我们称之为**为应用程序添加监控**。
- en: 'To achieve this task, we’ll use two widely used technologies in the industry:
    Prometheus and Grafana.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个任务，我们将使用两种在行业中广泛使用的技术：Prometheus 和 Grafana。
- en: Understanding Prometheus and the different metrics
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Prometheus 和不同的指标
- en: 'Prometheus is a technology to help you instrument your application. It consists
    of three things:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 是一种帮助你为应用程序添加监控的技术。它由三部分组成：
- en: Libraries for a wide range of programming languages, including Python, to add
    metrics to an application
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种编程语言的库，包括 Python，用于向应用程序添加指标
- en: A server to aggregate and store those metrics over time
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个服务器，用于聚合并存储这些指标随时间变化的值
- en: A query language, PromQL, so we can pull data from those metrics into visualization
    tools
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种查询语言 PromQL，用于将这些指标中的数据提取到可视化工具中
- en: Prometheus has very precise guidelines and conventions about how to define metrics.
    Actually, it defines four different types of metrics.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 对如何定义指标有非常精确的指南和约定。实际上，它定义了四种不同类型的指标。
- en: The counter metric
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计数器指标
- en: The counter metric is a way to measure a *value that goes up over time*. For
    example, this could be the number of requests answered or the number of predictions
    done. This will not be used for values that can go down. For that, there is the
    gauge metric.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 计数器指标是一种衡量*随着时间推移上升的值*的方法。例如，这可以是已答复的请求数量或完成的预测数量。它不能用于可以下降的值。对于这种情况，有仪表指标。
- en: '![Figure 15.2 – Possible representation of a counter](img/Figure_15.02_B19528.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.2 – 计数器的可能表示](img/Figure_15.02_B19528.jpg)'
- en: Figure 15.2 – Possible representation of a counter
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.2 – 计数器的可能表示
- en: The gauge metric
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 仪表指标
- en: The gauge metric is a way to measure a *value that can go up or down over time*.
    For example, this could be the current memory usage or the number of pending tasks
    in a worker queue.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**仪表**指标是一种衡量*随着时间的推移可以上升或下降的值*的方法。例如，这可以是当前的内存使用量或工作队列中待处理任务的数量。'
- en: '![Figure 15.3 – Possible representation of a gauge](img/Figure_15.03_B19528.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.3 – 仪表的可能表示](img/Figure_15.03_B19528.jpg)'
- en: Figure 15.3 – Possible representation of a gauge
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.3 – 仪表的可能表示
- en: The histogram metric
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直方图指标
- en: Contrary to counters and gauges, a histogram will *measure values and count
    them in buckets*. Typically, if we want to measure the response time of our API,
    we can count the number of requests that have been processed in less than 10 milliseconds,
    less than 100 milliseconds, and less than 1 second. Doing this is much more insightful
    than getting a simple average or median, for example.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与计数器和仪表不同，直方图将*测量值并将其计入桶中*。通常，如果我们想测量 API 的响应时间，我们可以统计处理时间少于 10 毫秒、少于 100 毫秒和少于
    1 秒的请求数量。例如，做这个比仅获取一个简单的平均值或中位数要有洞察力得多。
- en: When using a histogram, it’s our responsibility to define the buckets we want
    with their value threshold.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用直方图时，我们有责任定义所需的桶以及它们的值阈值。
- en: '![Figure 15.4 – Possible representation of a histogram](img/Figure_15.04_B19528.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.4 – 直方图的可能表示](img/Figure_15.04_B19528.jpg)'
- en: Figure 15.4 – Possible representation of a histogram
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.4 – 直方图的可能表示
- en: Prometheus defines a fourth type of metric, a summary. It’s quite similar to
    the histogram metric, but it works with sliding quantiles instead of defined buckets.
    We won’t go through it since it has quite limited support in Python. Besides,
    we’ll see in the Grafana section of this chapter that we’ll be able to compute
    quantiles with the histogram metric.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 定义了第四种类型的指标——摘要。它与直方图指标非常相似，但它使用滑动分位数而不是定义的桶。由于在 Python 中支持有限，我们不会详细介绍。此外，在本章的
    Grafana 部分，我们将看到能够使用直方图指标计算分位数。
- en: 'You can read more details about those metrics in the official Prometheus documentation:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在官方 Prometheus 文档中阅读有关这些指标的更多详细信息：
- en: '[https://prometheus.io/docs/concepts/metric_types/](https://prometheus.io/docs/concepts/metric_types/)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://prometheus.io/docs/concepts/metric_types/](https://prometheus.io/docs/concepts/metric_types/)'
- en: Measuring and exposing metrics
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量和暴露指标
- en: Once the metrics have been defined, we can start to measure things during the
    lifetime of our program. Similar to what we do with logs, metrics expose methods
    so we can store values during the execution of the application. Prometheus will
    then retain those values in memory to build the metrics.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了指标，我们就可以开始在程序生命周期中进行测量。与我们记录日志的方式类似，指标暴露了方法，使我们能够在应用程序执行期间存储值。然后，Prometheus
    会将这些值保存在内存中，以便构建指标。
- en: But then, how can we access those metrics so we can actually analyze and monitor
    them? Quite simply, apps using Prometheus usually expose an HTTP endpoint called
    `/metrics`, which will return the current values of all metrics in a specific
    format. You can see what it looks like in *Figure 15**.5*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何访问这些指标以便实际分析和监控呢？很简单，使用 Prometheus 的应用程序通常会暴露一个名为 `/metrics` 的 HTTP 端点，返回所有指标的当前值，格式是特定的。您可以在*图
    15.5*中查看它的样子。
- en: '![Figure 15.5 – Output of a Prometheus metrics endpoint](img/Figure_15.05_B19528.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.5 – Prometheus 指标端点的输出](img/Figure_15.05_B19528.jpg)'
- en: Figure 15.5 – Output of a Prometheus metrics endpoint
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.5 – Prometheus 指标端点的输出
- en: This endpoint can then be polled at regular intervals by a Prometheus server,
    which will store those metrics over time and make them available through PromQL.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 该端点可以由 Prometheus 服务器定期轮询，Prometheus 会随着时间推移存储这些指标，并通过 PromQL 提供访问。
- en: Metrics are reset when your application restarts
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的应用程序重启时，指标会被重置。
- en: It’s worth noting that every time you restart your application, like your FastAPI
    server, metric values are lost, and you start from zero. It may be a bit surprising,
    but it’s key to understand that metric values are only stored in memory in your
    app. The responsibility for properly storing them permanently belongs to the Prometheus
    server.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，每次重启应用程序时（如 FastAPI 服务器），指标值都会丢失，并且从零开始。这可能有些令人惊讶，但理解指标值仅保存在应用程序的内存中是非常重要的。永久保存它们的责任属于
    Prometheus 服务器。
- en: Now that we have a good idea of how they work, let’s see how to add metrics
    to FastAPI and Dramatiq applications.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对它们的工作原理有了一个大致了解，接下来让我们看看如何将指标添加到 FastAPI 和 Dramatiq 应用程序中。
- en: Adding Prometheus metrics to FastAPI
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Prometheus 指标添加到 FastAPI
- en: As we said, Prometheus maintains official libraries for various languages, including
    Python.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说，Prometheus 为各种语言（包括 Python）维护了官方库。
- en: We could very well use it on its own and manually define various metrics to
    monitor our FastAPI app. We would also need to come up with some logic to hook
    into a FastAPI request handler so we could measure things such as the requests
    count, response time, payload size, and so on.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完全可以单独使用它，并手动定义各种指标来监控我们的 FastAPI 应用。我们还需要编写一些逻辑，将其挂钩到 FastAPI 请求处理程序中，以便我们可以衡量诸如请求计数、响应时间、负载大小等指标。
- en: 'While definitely doable, we’ll take a shortcut and rely once again on the open
    source community, which proposes a ready-to-use library for integrating Prometheus
    into a FastAPI project: `/``metrics` endpoint.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然完全可以实现，但我们将采取捷径，再次依赖开源社区，它提供了一个现成的库，用于将 Prometheus 集成到 FastAPI 项目中：`/metrics`
    端点。
- en: 'The first thing is, of course, to install it with `pip`. Run the following
    command:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当然需要通过 `pip` 安装它。运行以下命令：
- en: '[PRE20]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the following example, we’ve implemented a very simple FastAPI app and enabled
    the instrumentator:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们实现了一个非常简单的 FastAPI 应用，并启用了仪表监控器：
- en: chapter15_metrics_01.py
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: chapter15_metrics_01.py
- en: '[PRE21]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_01.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_01.py)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_01.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_01.py)'
- en: 'Enabling the instrumentator consists of three lines:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 启用仪表监控器只需要三行代码：
- en: Instantiate the `Instrumentator` class.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化 `Instrumentator` 类。
- en: Enable the default metrics proposed by the library.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用库中提供的默认指标。
- en: Wire it to our FastAPI `app` and expose the `/``metrics` endpoint.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其与我们的 FastAPI `app` 连接并暴露 `/metrics` 端点。
- en: That’s it! FastAPI is instrumented with Prometheus!
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！FastAPI 已经集成了 Prometheus！
- en: 'Let’s run this app with Uvicorn and access the `hello` endpoint. Internally,
    Prometheus will measure things about this request. Let’s now access `/metrics`
    to see the result. If you scroll down this big list of metrics, you should come
    across these lines:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 Uvicorn 运行这个应用并访问 `hello` 端点。内部，Prometheus 将会测量有关这个请求的一些信息。现在让我们访问 `/metrics`
    来查看结果。如果你滚动查看这个长长的指标列表，你应该能看到以下几行：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This is the metrics counting the number of requests. We see that we have one
    request in total, which corresponds to our call to `hello`. Notice that the instrumentator
    is smart enough to label the metrics by path, method, and even status code. This
    is very convenient, as it’ll enable us to pull interesting figures depending on
    the characteristics of the request.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这是计数请求数量的指标。我们看到总共有一个请求，这对应于我们对`hello`的调用。请注意，仪表监控工具足够智能，可以根据路径、方法，甚至状态码为指标打上标签。这非常方便，因为它使我们能够根据请求的特征提取有趣的数据。
- en: Adding custom metrics
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加自定义指标
- en: The built-in metrics are a good start, but we’ll likely need to come up with
    our own to measure things specific to our application.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 内置的指标是一个不错的开始，但我们可能需要根据我们应用的特定需求来定义自己的指标。
- en: 'Let’s say we want to implement a function that rolls a dice with six faces
    and exposes it via a REST API. We want to define a metric allowing us to count
    the number of times each face has appeared. For this task, a counter is a good
    match. Let’s see how to declare it in the code:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要实现一个掷骰子的函数，骰子有六个面，并通过 REST API 暴露它。我们希望定义一个指标，允许我们计算每个面出现的次数。对于这个任务，计数器是一个很好的选择。让我们看看如何在代码中声明它：
- en: chapter15_metrics_02.py
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: chapter15_metrics_02.py
- en: '[PRE23]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_02.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_02.py)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_02.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_02.py)'
- en: We have to instantiate a `Counter` object. The two first arguments are, respectively,
    the name and description of the metric. The name will be used by Prometheus to
    uniquely identify this metric. Since we want to count the rolls per face, we also
    add a single label named `face`. Every time we count a roll of the dice, we’ll
    have to set this label to the corresponding result face.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须实例化一个 `Counter` 对象。前两个参数分别是指标的名称和描述。名称将由 Prometheus 用来唯一标识这个指标。因为我们想要统计每个面出现的次数，所以我们还添加了一个名为
    `face` 的标签。每次我们统计骰子的投掷次数时，都需要将此标签设置为相应的面值。
- en: Conventions for metric names
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 度量命名规范
- en: 'Prometheus defines very precise conventions for naming your metrics. In particular,
    it should start with the domain the metrics belong to, such as `http_` or `app_`,
    and should end with the unit, such as `_seconds`, `_bytes`, or `_total` if this
    is just a value count. We strongly recommend you read the Prometheus guidelines:
    [https://prometheus.io/docs/practices/naming/](https://prometheus.io/docs/practices/naming/).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 为度量命名定义了非常精确的规范。特别是，它应该以度量所属的领域开始，例如 `http_` 或 `app_`，并且如果仅是一个值计数，则应该以单位结尾，例如
    `_seconds`、`_bytes` 或 `_total`。我们强烈建议您阅读 Prometheus 的命名规范：[https://prometheus.io/docs/practices/naming/](https://prometheus.io/docs/practices/naming/)。
- en: 'We can now use this metric in our code. In the following snippet, you’ll see
    the implementation of the `roll_dice` function:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在代码中使用这个度量了。在下面的代码片段中，您将看到 `roll_dice` 函数的实现：
- en: chapter15_metrics_02.py
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: chapter15_metrics_02.py
- en: '[PRE24]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_02.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_02.py)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_02.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_02.py)'
- en: You can see that we directly use the metrics instance, `DICE_COUNTER`, and first
    call the `labels` method to set the face, and then `inc` to actually increment
    the counter.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，我们直接使用度量实例 `DICE_COUNTER`，首先调用 `labels` 方法来设置骰子面数，然后调用 `inc` 来实际增加计数器。
- en: 'That’s all we need to do: our metric is automatically registered in the Prometheus
    client and will start to be exposed by the `/metrics` endpoint. In *Figure 15**.6*,
    you can see a possible visualization of this metric in Grafana.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要做的：我们的度量已经自动注册到 Prometheus 客户端，并将通过 `/metrics` 端点开始暴露。在 *图 15.6* 中，您可以看到此度量在
    Grafana 中的可能可视化。
- en: '![Figure 15.6 – Representation of the dice roll metric in Grafana](img/Figure_15.06_B19528.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.6 – 在 Grafana 中表示骰子投掷度量](img/Figure_15.06_B19528.jpg)'
- en: Figure 15.6 – Representation of the dice roll metric in Grafana
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.6 – 在 Grafana 中表示骰子投掷度量
- en: 'As you can see, declaring and using a new metric is quite straightforward:
    we can just call it directly in the code we want to monitor.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，声明和使用新度量是非常简单的：我们只需在想要监控的代码中直接调用它。
- en: Handling multiple processes
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理多个进程
- en: In [*Chapter 10*](B19528_10.xhtml#_idTextAnchor694), we mentioned in the *Adding
    Gunicorn as a server process for deployment* section that, in a production deployment,
    FastAPI apps are usually run with several workers. Basically, it spawns several
    processes of the same application and balances the incoming requests between them.
    This allows us to serve more requests concurrently and avoid blocks if one of
    the operations is blocking the process.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第10章*](B19528_10.xhtml#_idTextAnchor694)中，我们在 *为部署添加 Gunicorn 作为服务器进程* 部分提到过，在生产部署中，FastAPI
    应用程序通常会使用多个工作进程运行。基本上，它会启动多个相同应用程序的进程，并在它们之间平衡传入的请求。这使得我们可以并发处理更多请求，避免因某个操作阻塞进程而导致的阻塞。
- en: Do not confuse Gunicorn workers and Dramatiq workers
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 不要混淆 Gunicorn 工作进程和 Dramatiq 工作进程
- en: When we talk about workers in the context of a Gunicorn deployment for FastAPI,
    we are referring to the fact that we are spawning multiple processes that’ll be
    able to serve our API requests concurrently. We are not talking about workers
    in the context of Dramatiq that are processing tasks in the background.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论 Gunicorn 部署中的工作进程时，我们指的是通过启动多个进程来并发处理 API 请求的方式。我们不是指 Dramatiq 中的工作进程，这些进程是在后台处理任务。
- en: Having multiple processes for the same application is a bit problematic for
    Prometheus metrics. Indeed, as we mentioned before, those metrics are only stored
    in memory and exposed through a `/``metrics` endpoint.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于同一应用程序，拥有多个进程在 Prometheus 度量方面是有点问题的。事实上，正如我们之前提到的，这些度量仅存储在内存中，并通过 `/``metrics`
    端点暴露。
- en: If we have several processes answering requests, each one will have its own
    set of metrics values. Then, when the Prometheus server asks for `/metrics`, we’ll
    get the values of the process that answered our request but not the ones of the
    others. And it may change in the next poll! Obviously, this will totally defeat
    our initial goal.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有多个进程来处理请求，每个进程都会有自己的一组度量值。然后，当 Prometheus 服务器请求 `/metrics` 时，我们将获得响应我们请求的进程的度量值，而不是其他进程的度量值。这些值在下次轮询时可能会发生变化！显然，这将完全破坏我们最初的目标。
- en: To circumvent this, the Prometheus client has a special multiprocess mode. Basically,
    instead of storing the values in memory, it’ll store them in files in a dedicated
    folder. When calling `/metrics`, it’ll take care of loading all the files and
    reconciling the values of all processes together.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绕过这个问题，Prometheus 客户端有一个特殊的多进程模式。基本上，它不会将值存储在内存中，而是将它们存储在专用文件夹中的文件里。当调用 `/metrics`
    时，它会负责加载所有文件并将所有进程的值进行合并。
- en: 'Enabling this mode requires us to set the environment variable called `PROMETHEUS_MULTIPROC_DIR`.
    It should point to a valid folder in your filesystem where the metrics files will
    be stored. Here is a command example of how to set this variable and start Gunicorn
    with four workers:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 启用此模式需要我们设置一个名为 `PROMETHEUS_MULTIPROC_DIR` 的环境变量。它应该指向文件系统中一个有效的文件夹，存储指标文件。以下是如何设置这个变量并启动带有四个工作进程的
    Gunicorn 的命令示例：
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Of course, in a production deployment, you would set the environment variable
    globally on your platform, as we explained in [*Chapter 10*](B19528_10.xhtml#_idTextAnchor694).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在生产环境部署时，你应该在平台上全局设置环境变量，正如我们在[*第 10 章*](B19528_10.xhtml#_idTextAnchor694)中所解释的那样。
- en: If you try this command, you’ll see that Prometheus will start to store some
    `.db` files inside the folder, each one corresponding to a metric and a process.
    The side effect is that *metrics won’t be cleared when restarting the process*.
    It can lead to unexpected behaviors if you change your metrics definition or if
    you run a completely different application. Make sure to choose a dedicated folder
    for each of your apps and clean it up when you run a new version.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试这个命令，你会看到 Prometheus 会开始在文件夹内存储一些 `.db` 文件，每个文件对应一个指标和一个进程。副作用是，*在重启进程时，指标不会被清除*。如果你更改了指标定义，或者运行了完全不同的应用程序，可能会导致意外的行为。确保为每个应用选择一个专用的文件夹，并在运行新版本时清理该文件夹。
- en: We are now able to precisely instrument a FastAPI app. However, we saw in the
    previous chapter that data science applications can be constituted of a separate
    worker process, where a lot of logic and intelligence is run. Thus, it’s also
    crucial to instrument this part of the application.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在能够精确地对 FastAPI 应用进行监控。然而，正如我们在前一章中所看到的，数据科学应用可能包含一个独立的工作进程，其中运行着大量的逻辑和智能。因此，对应用的这一部分进行监控也至关重要。
- en: Adding Prometheus metrics to Dramatiq
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向 Dramatiq 添加 Prometheus 指标
- en: In [*Chapter 14*](B19528_14.xhtml#_idTextAnchor1041), we implemented a complex
    application with a distinct worker process that was in charge of loading and executing
    the Stable Diffusion model to generate images. Hence, this part of the architecture
    is critical and needs to be monitored to be sure everything is going well.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 14 章*](B19528_14.xhtml#_idTextAnchor1041)中，我们实现了一个复杂的应用，包含一个独立的工作进程，该进程负责加载并执行
    Stable Diffusion 模型来生成图像。因此，架构中的这一部分非常关键，需要进行监控，以确保一切顺利。
- en: In this section, we’ll see how to add Prometheus metrics to a Dramatiq worker.
    The good news is that Dramatiq already comes with built-in metrics and exposes
    the `/metrics` endpoint by default. Really, there is nothing much to do!
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将学习如何向 Dramatiq 工作进程添加 Prometheus 指标。好消息是，Dramatiq 已经内置了指标，并且默认暴露了 `/metrics`
    端点。实际上，几乎不需要做什么！
- en: 'Let’s take a very basic example of a Dramatiq worker with a dummy task:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个非常基础的 Dramatiq 工作进程的例子，里面包含一个虚拟任务：
- en: chapter15_metrics_03.py
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: chapter15_metrics_03.py
- en: '[PRE26]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_03.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_03.py)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_03.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_03.py)'
- en: 'As you probably understand by now, Dramatiq is by nature a multiprocessing
    program: it spawns several workers to handle tasks concurrently. As such, we need
    to make sure Prometheus is in multiprocessing mode, as we mentioned in the *Handling
    multiple processes* section. Thus, we’ll need to set the `PROMETHEUS_MULTIPROC_DIR`
    environment variable, as we explained earlier, but also `dramatiq_prom_db`. Indeed,
    Dramatiq implements its own mechanism to enable Prometheus’s multiprocessing mode,
    which should work out of the box, but it turns out, in our experience, that it’s
    better to be explicit about it.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你现在可能已经理解的，Dramatiq本质上是一个多进程程序：它会启动多个工作进程来并发处理任务。因此，我们需要确保Prometheus处于多进程模式，正如我们在*处理多个进程*部分中提到的那样。因此，我们需要设置`PROMETHEUS_MULTIPROC_DIR`环境变量，正如我们之前解释的那样，还需要设置`dramatiq_prom_db`。事实上，Dramatiq实现了自己的机制来启用Prometheus的多进程模式，这应该是开箱即用的，但根据我们的经验，明确指出这一点会更好。
- en: 'The following command shows you how to start our worker with `PROMETHEUS_MULTIPROC_DIR`
    and `dramatiq_prom_db` set:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令展示了如何启动带有`PROMETHEUS_MULTIPROC_DIR`和`dramatiq_prom_db`设置的工作进程：
- en: '[PRE27]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To allow you to schedule a task easily in this worker, we’ve added a small
    `__name__ == "__main__"` instruction. In another terminal, run the following command:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你能轻松在这个工作进程中调度任务，我们添加了一个小的`__name__ == "__main__"`指令。在另一个终端中，运行以下命令：
- en: '[PRE28]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: It’ll schedule a task in the worker. You’ll probably see it being executed in
    the worker logs.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 它将在工作进程中调度一个任务。你可能会在工作进程日志中看到它的执行情况。
- en: 'Now, try to open the following URL in your browser: `http://localhost:9191/metrics`.
    You’ll see a result similar to what we show in *Figure 15**.7*.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，尝试在浏览器中打开以下URL：`http://localhost:9191/metrics`。你将看到类似于我们在*图15.7*中展示的结果。
- en: '![Figure 15.7 – Output of a Dramatiq Prometheus metrics endpoint](img/Figure_15.07_B19528.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图15.7 – Dramatiq Prometheus度量端点的输出](img/Figure_15.07_B19528.jpg)'
- en: Figure 15.7 – Output of a Dramatiq Prometheus metrics endpoint
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.7 – Dramatiq Prometheus度量端点的输出
- en: 'We already see several metrics, including a counter for the total number of
    messages processed by Dramatiq, a histogram to measure the execution time of our
    tasks, and a gauge to measure the number of tasks currently in progress. You can
    review the complete list of metrics included by Dramatiq in its official documentation:
    [https://dramatiq.io/advanced.html#prometheus-metrics](https://dramatiq.io/advanced.html#prometheus-metrics).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到几个度量指标，包括一个用于统计Dramatiq处理的消息总数的计数器，一个用于测量任务执行时间的直方图，以及一个用于衡量当前正在进行的任务数量的仪表。你可以在Dramatiq的官方文档中查看完整的度量指标列表：[https://dramatiq.io/advanced.html#prometheus-metrics](https://dramatiq.io/advanced.html#prometheus-metrics)。
- en: Adding custom metrics
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加自定义指标
- en: 'Of course, as for FastAPI, we would probably like to add our own metrics to
    the Dramatiq worker. Actually, this is very similar to what we saw in the previous
    section. Let’s again take the dice roll example:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对于FastAPI，我们可能也希望向Dramatiq工作进程添加我们自己的指标。事实上，这与我们在上一节中看到的非常相似。让我们再次以掷骰子为例：
- en: chapter15_metrics_04.py
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: chapter15_metrics_04.py
- en: '[PRE29]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_04.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_04.py)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_04.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_04.py)'
- en: All we needed to do was to create our `Counter` object, as we did before, and
    use it in our task. If you try to run the worker and request the `/metrics` endpoint,
    you’ll see this new metric appear.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所需要做的只是创建我们的`Counter`对象，正如我们之前所做的那样，并在任务中使用它。如果你尝试运行工作进程并请求`/metrics`端点，你会看到这个新指标出现。
- en: We are now able to instrument our FastAPI and Dramatiq apps. As we have already
    mentioned several times, we now need to aggregate those metrics in a Prometheus
    server and visualize them in Grafana. That’s what we’ll look at in the next section.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以对我们的FastAPI和Dramatiq应用进行指标收集。正如我们之前多次提到的那样，我们现在需要将这些指标汇总到Prometheus服务器中，并在Grafana中进行可视化。这就是我们将在下一节中讨论的内容。
- en: Monitoring metrics in Grafana
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Grafana中监控指标
- en: Having metrics is nice, but being able to visualize them is better! In this
    section, we’ll see how we can collect Prometheus metrics, send them to Grafana,
    and create dashboards to monitor them.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有度量指标固然不错，但能够可视化它们更好！在本节中，我们将看到如何收集Prometheus度量指标，将它们发送到Grafana，并创建仪表板来监控它们。
- en: Grafana is an open source web application for data visualization and analytics.
    It’s able to connect to various data sources, such as timeseries databases and,
    of course, Prometheus. Its powerful query and graph builder allows us to create
    detailed dashboards where we can monitor our data in real time.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 是一个开源的 Web 应用程序，用于数据可视化和分析。它能够连接到各种数据源，比如时间序列数据库，当然也包括 Prometheus。其强大的查询和图形构建器使我们能够创建详细的仪表板，在其中实时监控我们的数据。
- en: Configuring Grafana to collect metrics
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 Grafana 收集指标
- en: 'Since it’s open source, you can run it from your own machine or server. Detailed
    instructions are available in the official documentation: [https://grafana.com/docs/grafana/latest/setup-grafana/installation/](https://grafana.com/docs/grafana/latest/setup-grafana/installation/).
    However, to speed things up and get you started quickly, we’ll rely here on Grafana
    Cloud, an official hosting platform. It offers a free plan, which should be enough
    for you to get started. You can create your account here: [https://grafana.com/auth/sign-up/create-user](https://grafana.com/auth/sign-up/create-user).
    Once done, you’ll be asked to create your own instance, a “Grafana Stack,” by
    choosing a subdomain and a data center region, as you can see in *Figure 15**.8*.
    Choose a region close to your geographic location.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它是开源的，你可以在自己的机器或服务器上运行它。详细的安装说明可以在官方文档中找到：[https://grafana.com/docs/grafana/latest/setup-grafana/installation/](https://grafana.com/docs/grafana/latest/setup-grafana/installation/)。不过，为了加快进程并快速开始，我们这里依赖的是
    Grafana Cloud，这是一个官方托管平台。它提供了一个免费的计划，足以让你开始使用。你可以在这里创建账户：[https://grafana.com/auth/sign-up/create-user](https://grafana.com/auth/sign-up/create-user)。完成后，你将被要求创建自己的实例，即一个“Grafana
    Stack”，通过选择子域名和数据中心区域，如*图 15**.8* 所示。请选择一个靠近你地理位置的区域。
- en: '![Figure 15.8 – Instance creation on Grafana Cloud](img/Figure_15.08_B19528.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.8 – 在 Grafana Cloud 上创建实例](img/Figure_15.08_B19528.jpg)'
- en: Figure 15.8 – Instance creation on Grafana Cloud
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.8 – 在 Grafana Cloud 上创建实例
- en: You’ll then be presented with a set of common actions to get started with Grafana.
    The first thing we’ll do is add Prometheus metrics. Click on **Scale and centralize
    existing data**, then **Hosted Prometheus metrics**. You’ll be taken to a page
    to configure a Prometheus metrics collection. Click on the tab named **Configuration
    Details** at the top. The page will look like the one shown in *Figure 15**.9*.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将看到一组常见的操作，帮助你开始使用 Grafana。我们要做的第一件事是添加 Prometheus 指标。点击**扩展和集中现有数据**，然后选择**托管
    Prometheus 指标**。你将进入一个配置 Prometheus 指标收集的页面。在顶部点击名为**配置详情**的选项卡。页面将呈现如*图 15**.9*所示。
- en: '![Figure 15.9 – Hosted Prometheus metrics configuration on Grafana](img/Figure_15.09_B19528.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.9 – 在 Grafana 上配置托管 Prometheus 指标](img/Figure_15.09_B19528.jpg)'
- en: Figure 15.9 – Hosted Prometheus metrics configuration on Grafana
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.9 – 在 Grafana 上配置托管 Prometheus 指标
- en: 'You see that we have two ways to forward metrics: via Grafana Agent or via
    a Prometheus server.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们有两种方式来转发指标：通过 Grafana Agent 或通过 Prometheus 服务器。
- en: 'As we mentioned earlier, a Prometheus server is responsible for collecting
    metrics for all our apps and storing the data in a database. It’s the standard
    way to do it. You can find instructions on how to install it in the official documentation:
    [https://prometheus.io/docs/prometheus/latest/installation/](https://prometheus.io/docs/prometheus/latest/installation/).
    Bear in mind, though, that it’s a dedicated application server that’ll need proper
    backups, as it’ll store all your metrics data.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Prometheus 服务器负责收集我们所有应用程序的指标，并将数据存储在数据库中。这是标准的做法。你可以在官方文档中找到如何安装它的说明：[https://prometheus.io/docs/prometheus/latest/installation/](https://prometheus.io/docs/prometheus/latest/installation/)。不过，请注意，它是一个专用的应用服务器，需要适当的备份，因为它会存储所有的指标数据。
- en: The most straightforward way is to use Grafana Agent. It consists of a small
    command-line program with a single configuration file. When it runs, it’ll poll
    the metrics of each of your apps and send the data to Grafana Cloud. All the data
    is stored on Grafana Cloud, so nothing is lost, even if you stop or delete the
    agent. This is what we’ll use here.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的方式是使用 Grafana Agent。它由一个小型命令行程序和一个配置文件组成。当它运行时，它会轮询每个应用程序的指标，并将数据发送到 Grafana
    Cloud。所有数据都会存储在 Grafana Cloud 上，因此即使停止或删除代理，数据也不会丢失。这就是我们在这里使用的方法。
- en: Grafana shows you commands on the page to download, unzip, and execute the Grafana
    Agent program. Execute those commands so you have it at the root of your project.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 会在页面上显示下载、解压并执行 Grafana Agent 程序的命令。执行这些命令，以便将其放在项目的根目录中。
- en: Then, in the last step, you’ll have to create an API token so Grafana Agent
    can send data to your instance. Give it a name and click on **Create API Token**.
    A new text area will appear with a new command to create the agent’s configuration
    file, as you can see in *Figure 15**.10*.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在最后一步，你需要创建一个 API 令牌，以便 Grafana Agent 可以将数据发送到你的实例。给它起个名字，然后点击**创建 API 令牌**。一个新的文本区域将出现，显示一个新的命令，用于创建代理的配置文件，正如你在*图
    15.10*中看到的那样。
- en: '![Figure 15.10 – Command to create Grafana Agent configuration](img/Figure_15.10_B19528.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.10 – 创建 Grafana Agent 配置的命令](img/Figure_15.10_B19528.jpg)'
- en: Figure 15.10 – Command to create Grafana Agent configuration
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.10 – 创建 Grafana Agent 配置的命令
- en: 'Execute the `./grafana-agent-linux-amd64 –config.file=agent-config.yaml` command.
    A file named `agent-config.yaml` will be created in your project. We now have
    to edit it so we can configure our actual FastAPI and Dramatiq applications. You
    can see the result in the following snippet:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 `./grafana-agent-linux-amd64 –config.file=agent-config.yaml` 命令。一个名为 `agent-config.yaml`
    的文件将被创建在你的项目中。我们现在需要编辑它，以便配置我们的实际 FastAPI 和 Dramatiq 应用程序。你可以在以下代码片段中看到结果：
- en: agent-config.yaml
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: agent-config.yaml
- en: '[PRE30]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/agent-config.yaml](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/agent-config.yaml)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/agent-config.yaml](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/agent-config.yaml)'
- en: 'It’s a YAML configuration file where we can set the various options for Grafana
    Agent. The most important part is the `scrape_configs` key. As you can see, we
    can define the list of all the apps we want to gather the metrics for and specify
    their hostname, the “target”: `localhost:8000` for the FastAPI app and `localhost:9191`
    for the Dramatiq worker. Of course, this configuration is valid for local development,
    but you’ll have to adapt it with the proper hostnames of your apps in a production
    deployment.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 YAML 配置文件，我们可以在其中设置 Grafana Agent 的各种选项。最重要的部分是 `scrape_configs` 键。如你所见，我们可以定义所有要收集指标的应用程序列表，并指定它们的主机名，对于
    FastAPI 应用程序是“目标”：`localhost:8000`，而 Dramatiq worker 是 `localhost:9191`。当然，这个配置适用于本地开发，但在生产环境中，你需要根据实际应用程序的主机名进行调整。
- en: 'We are now ready to start Grafana Agent and collect the metrics! Make sure
    your FastAPI and Dramatiq apps are running, and then run Grafana Agent. Depending
    on your system, the name of the executable will vary, but it’ll look similar to
    this:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备启动 Grafana Agent 并收集指标数据了！确保你的 FastAPI 和 Dramatiq 应用程序正在运行，然后启动 Grafana
    Agent。根据你的系统，执行文件的名称会有所不同，但大致如下所示：
- en: '[PRE31]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Grafana Agent will start and will collect the metrics at regular intervals before
    sending them to Grafana. We’re now ready to plot some data!
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana Agent 将启动并定期收集指标数据，然后将其发送到 Grafana。我们现在可以开始绘制一些数据了！
- en: Visualizing metrics in Grafana
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Grafana 中可视化指标
- en: Our metrics data is now sent to Grafana. We’re ready to query it and build some
    graphs. The first step is to create a new **dashboard**, a place where you’ll
    be able to create and organize multiple graphs. Click on the plus button at the
    top right and then **New dashboard**.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的指标数据现在已发送到 Grafana。我们准备好查询它并构建一些图表了。第一步是创建一个新的**仪表板**，这是一个可以创建和组织多个图表的地方。点击右上角的加号按钮，然后选择**新建仪表板**。
- en: A new blank dashboard will appear, as you can see in *Figure 15**.11*.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新的空白仪表板将出现，正如你在*图 15.11*中看到的那样。
- en: '![Figure 15.11 – Create a new dashboard in Grafana](img/Figure_15.11_B19528.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.11 – 在 Grafana 中创建新仪表板](img/Figure_15.11_B19528.jpg)'
- en: Figure 15.11 – Create a new dashboard in Grafana
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.11 – 在 Grafana 中创建新仪表板
- en: 'Click on **Add a new panel**. The interface to build a new graph will appear.
    There are three main parts:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**添加新面板**。将会出现一个用于构建新图表的界面。主要有三个部分：
- en: The graph preview at the top left. When starting, it’s empty.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左上角的图表预览。在开始时，它是空的。
- en: The query builder at the bottom left. This is where we’ll query the metrics
    data.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左下角的查询构建器。这是我们查询指标数据的地方。
- en: The graph settings on the right. This is where we’ll choose the type of graph
    and finely configure its look and feel, similar to what we have in spreadsheet
    software.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右侧的图表设置。这是我们选择图表类型并精细配置其外观和感觉的地方，类似于电子表格软件中的操作。
- en: Let’s try to create a graph for the duration of HTTP requests in our FastAPI
    app. In the select menu called **Metric**, you’ll have access to all the Prometheus
    metrics that have been reported by our apps. Select **http_request_duration_seconds_bucket**.
    This is the histogram metric defined by default by Prometheus FastAPI Instrumentator
    to measure the response time of our endpoints.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试为我们 FastAPI 应用中的 HTTP 请求时长创建一个图表。在名为**指标**的选择菜单中，你将能访问到我们应用所报告的所有 Prometheus
    指标。选择**http_request_duration_seconds_bucket**。这是 Prometheus FastAPI Instrumentator
    默认定义的直方图指标，用于衡量我们端点的响应时间。
- en: Then, click on **Run queries**. Under the hood, Grafana will build and execute
    PromQL queries to retrieve the data.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，点击**运行查询**。在后台，Grafana 会构建并执行 PromQL 查询来检索数据。
- en: At the top right of the graph, let’s select a shorter time span, such as **Last
    15 minutes**. Since we do not have much data yet, we’ll have a clearer view if
    we look at only a few minutes of data instead of hours. You should see a graph
    similar to the one in *Figure 15**.12*.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表的右上角，我们选择一个较短的时间跨度，比如**过去 15 分钟**。由于我们还没有太多数据，如果只看几分钟的数据，而不是几小时的数据，图表会更加清晰。你应该会看到一个类似*图
    15.12*的图表。
- en: '![Figure 15.12 – Basic plot of a histogram metric in Grafana](img/Figure_15.12_B19528.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.12 – Grafana 中直方图指标的基本图](img/Figure_15.12_B19528.jpg)'
- en: Figure 15.12 – Basic plot of a histogram metric in Grafana
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.12 – Grafana 中直方图指标的基本图
- en: 'Grafana has plotted several series: for each `handler` (which corresponds to
    the endpoint pattern), we have several buckets, `le`. Each line roughly represents
    *the number of times we answered “handler” in less than “**le” seconds*.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 已绘制出多个系列：对于每个`handler`（对应于端点模式），我们有多个桶，`le`。每条线大致代表了*我们在少于“**le**”秒内处理“handler”请求的次数*。
- en: This is the raw representation of the metric. However, you probably see that
    it’s not very convenient to read and analyze. It would be better if we could look
    at this data another way, in terms of response time, arranged by quantiles.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这是指标的原始表示。然而，你可能会发现它不太方便阅读和分析。如果我们能以另一种方式查看这些数据，按分位数排列的响应时间，可能会更好。
- en: 'Fortunately, PromQL includes some math operations so we can arrange the raw
    data. The part below the **Metric** menu allows us to add those operations. We
    can even see that Grafana suggests we use **add histogram_quantile**. If you click
    on this blue button, Grafana will automatically add three operations: a *Rate*,
    a *Sum by le*, and finally, a *Histogram quantile*, set by default to *0.95*.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，PromQL 包含一些数学运算，这样我们就可以对原始数据进行处理。在**指标**菜单下方的部分允许我们添加这些运算。我们甚至可以看到 Grafana
    建议我们使用**添加 histogram_quantile**。如果点击这个蓝色按钮，Grafana 会自动添加三种操作：*速率*、*按 le 求和*，最后是*直方图分位数*，默认设置为*0.95*。
- en: 'By doing this, we’ll now have a view of the evolution of our response time:
    95% of the time, we answer in less than *x* seconds.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们现在可以看到响应时间的变化情况：95%的时间，我们的响应时间少于*x*秒。
- en: The default *y* axis unit is not very convenient. Since we know we work with
    seconds, let’s select this unit in the graph options. On the right, look for the
    **Standard options** part and, in the **Unit** menu, look for **seconds (s)**
    under the **Time** group. Your graph will now look like *Figure 15**.13*.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的*y*轴单位不太方便。由于我们知道我们使用的是秒，接下来在图表选项中选择这个单位。在右侧，找到**标准选项**部分，然后在**单位**菜单中，在**时间**组下选择**秒（s）**。现在你的图表应该像*图
    15.13*一样。
- en: '![Figure 15.13 – Quantile representation of a histogram metric in Grafana](img/Figure_15.13_B19528.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.13 – Grafana 中直方图指标的分位数表示](img/Figure_15.13_B19528.jpg)'
- en: Figure 15.13 – Quantile representation of a histogram metric in Grafana
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.13 – Grafana 中直方图指标的分位数表示
- en: 'Now it’s much more insightful: we can see that we answer nearly all our requests
    (95%) in under 100 milliseconds. If our server starts to slow down, we’ll immediately
    see an increase in our graph, which could alert us that something has gone wrong.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在情况更具洞察力了：我们可以看到，我们几乎处理了所有的请求（95%）都在 100 毫秒以内。如果我们的服务器开始变慢，我们会立即在图表中看到上升，这能提醒我们系统出现了问题。
- en: If we want to have other quantiles on the same graph, we can duplicate this
    query by clicking on the **Duplicate** button right above **Run queries**. Then,
    all we have to do is to select another quantile. We show the result with quantiles
    *0.95*, *0.90*, and *0.50* in *Figure 15**.14*.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望在同一个图表上显示其他分位数，可以通过点击**复制**按钮（位于**运行查询**上方）来复制这个查询。然后，我们只需要选择另一个分位数。我们展示了*0.95*、*0.90*和*0.50*分位数的结果，见*图
    15.14*。
- en: '![Figure 15.14 – Several quantiles on the same graph in Grafana](img/Figure_15.14_B19528.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.14 – Grafana 中同一图表上的多个分位数](img/Figure_15.14_B19528.jpg)'
- en: Figure 15.14 – Several quantiles on the same graph in Grafana
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.14 – Grafana 中同一图表上的多个分位数
- en: The legend can be customized
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图例可以自定义
- en: Notice that the name of the series in the legend can be customized. Under the
    **Options** part of each query, you can customize it at will. You can even include
    dynamic values coming from the query, such as metrics labels.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，图例中的系列名称是可以自定义的。在每个查询的**选项**部分，你可以根据需要进行自定义。你甚至可以包含来自查询的动态值，例如指标标签。
- en: Finally, we can give a name to our graph by setting **Panel title**, in the
    right column. Now that we’re happy with our graph, we can click on **Apply** at
    the top right to add it to our dashboard, as we see in *Figure 15**.15*.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过在右侧列中设置**面板标题**来给我们的图表命名。现在我们对图表感到满意，可以点击右上角的**应用**按钮，将其添加到我们的仪表板中，如*图
    15.15*所示。
- en: '![Figure 15.15 – Grafana dashboard](img/Figure_15.15_B19528.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.15 – Grafana 仪表板](img/Figure_15.15_B19528.jpg)'
- en: Figure 15.15 – Grafana dashboard
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.15 – Grafana 仪表板
- en: That’s it! We can start to monitor our application. You can resize and position
    each panel at will. You can set the query time span you want to look at and even
    enable auto-refresh so the data gets updated in real time! Don’t forget to click
    on the **Save** button to save your dashboard.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们可以开始监控我们的应用程序了。你可以随意调整每个面板的大小和位置。你还可以设置想要查看的查询时间范围，甚至启用自动刷新功能，这样数据就能实时更新！别忘了点击**保存**按钮来保存你的仪表板。
- en: We can build a similar graph with the exact same configuration to monitor the
    time needed to execute tasks in Dramatiq, thanks to the metric named `dramatiq_message_duration_milliseconds_bucket`.
    Notice that this one is expressed in milliseconds instead of seconds, so you should
    be careful when selecting the unit of your graph. We see here one of the benefits
    of the Prometheus naming convention for metrics!
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用完全相同的配置，构建一个类似的图表，用于监控执行 Dramatiq 任务所需的时间，这要感谢名为`dramatiq_message_duration_milliseconds_bucket`的指标。注意，这个指标是以毫秒为单位表示的，而不是秒，所以在选择图表单位时需要特别小心。我们在这里看到了
    Prometheus 指标命名约定的一个优点！
- en: Adding a bar chart graph
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加柱状图
- en: 'There are a lot of different types of graphs available in Grafana. For example,
    we could plot our dice roll metric in the form of a bar chart, where each bar
    represents the number of times a face has been seen. Let’s try it: add a new panel
    and select the `app_dice_rolls_total` metric. You’ll see something similar to
    what is shown in *Figure 15**.6*.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 提供了许多不同类型的图表。例如，我们可以将骰子投掷指标绘制成柱状图，其中每根柱子表示某一面出现的次数。让我们来试试：添加一个新面板并选择`app_dice_rolls_total`指标。你会看到类似*图
    15.6*所示的内容。
- en: '![Figure 15.16 – Default representation of a counter metric with a bar chart
    in Grafana](img/Figure_15.16_B19528.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.16 – Grafana 中计数器指标的默认表示方式（柱状图）](img/Figure_15.16_B19528.jpg)'
- en: Figure 15.16 – Default representation of a counter metric with a bar chart in
    Grafana
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.16 – Grafana 中计数器指标的默认表示方式（柱状图）
- en: 'We do have a bar for each face, but there is something strange: there are bars
    for each point in time. That’s a key thing to understand with Prometheus metrics
    and PromQL: all metrics are stored as *time series*. This allows us to go back
    in time and see the evolution of the metrics over time.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，我们为每个面都有一个柱子，但有一个奇怪的地方：每个时间点都有一根柱子。这是理解 Prometheus 指标和 PromQL 的关键：所有指标都作为*时间序列*存储。这使我们能够回溯时间，查看指标随时间的演变。
- en: However, for some representations, like the one shown here, it’s not really
    insightful. For this case, it would be better to show us the latest values for
    the time span we selected. We can do this by setting **Type** to **Instant** under
    the **Options** part of the metric panel. We’ll see that we now have a single
    graph with a single point in time, as you can see in *Figure 15**.17*.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于某些表示方式，像这里显示的这种，实际上并不具备很高的洞察力。对于这种情况，最好是展示我们选择的时间范围内的最新值。我们可以通过将指标面板中的**类型**设置为**即时**来实现这一点。我们会看到现在我们有一个单一的图表，显示一个时间点的数据，如*图
    15.17*所示。
- en: '![Figure 15.17 – Counter metric configured as Instant in Grafana](img/Figure_15.17_B19528.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.17 – 在 Grafana 中将计数器指标配置为即时类型](img/Figure_15.17_B19528.jpg)'
- en: Figure 15.17 – Counter metric configured as Instant in Grafana
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.17 – 在 Grafana 中将计数器指标配置为即时类型
- en: It’s better, but we can go further. Typically, we would like the *x* axis to
    show the face labels instead of the point in time. First, let’s customize the
    legend with a `{{face}}`. The legend will now only show the `face` label.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这样已经更好了，但我们可以更进一步。通常，我们希望 *x* 轴显示面孔标签，而不是时间点。首先，让我们用 `{{face}}` 自定义图例。现在图例将只显示
    `face` 标签。
- en: Now, we’ll transform the data so the *x* axis is the `face` label. Click on
    the **Transform** tab. You’ll see a list of functions that can be applied by Grafana
    to your data before visualizing it. For our case here, we’ll choose **Reduce**.
    The effect of this function is to take each series, take a specific value from
    it, and plot it on the *x* axis. By default, Grafana will take the maximum value,
    **Max**, but there are other choices, such as **Last**, **Mean**, or **StdDev**.
    In this context, they won’t make a difference since we already queried the instant
    value.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将数据转换，使得 *x* 轴为 `face` 标签。点击 **Transform** 标签。你会看到一系列可以在可视化之前应用到数据的函数。在我们这里，我们将选择
    **Reduce**。这个函数的作用是取每个序列，从中提取一个特定的值并将其绘制在 *x* 轴上。默认情况下，Grafana 会取最大值 **Max**，但也有其他选项，如
    **Last**、**Mean** 或 **StdDev**。在这种情况下，它们没有区别，因为我们已经查询了即时值。
- en: That’s it! Our graph now shows the number of times we’ve seen a face. This is
    the one we showed in *Figure 15**.6* earlier in the chapter.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们的图表现在显示了我们看到面孔的次数。这就是我们在*第15.6图*中展示的内容。
- en: Summary
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Congratulations! You are now able to report metrics and build your own dashboards
    in Grafana to monitor your data science applications. Over time, don’t hesitate
    to add new metrics or complete your dashboards if you notice some blind spots:
    the goal is to be able to watch over every important part at a glance so you can
    quickly take corrective actions. Those metrics can also be used to drive the evolution
    of your work: by monitoring the performance and accuracy of your ML models, you
    can track the effects of your changes and see whether you are going in the right
    direction.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！现在你可以在 Grafana 中报告指标，并构建自己的仪表盘来监控你的数据科学应用程序。随着时间的推移，如果你发现一些盲点，不要犹豫添加新的指标或完善你的仪表盘：目标是能够一目了然地监控每个重要部分，从而快速采取纠正措施。这些指标也可以用来推动你工作的演进：通过监控你的机器学习模型的性能和准确性，你可以跟踪你所做的改动的效果，看看自己是否走在正确的道路上。
- en: This is the end of this book and our FastAPI journey. We sincerely hope that
    you liked it and that you learned a lot along the way. We’ve covered many subjects,
    sometimes just by scratching the surface, but you should now be ready to build
    your own projects with FastAPI and serve smart data science algorithms. Be sure
    to check all the external resources we proposed along the way, as they will give
    you all the insights you need to master them.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的内容和我们的 FastAPI 之旅到此结束。我们真诚希望你喜欢本书，并且在这段旅程中学到了很多。我们覆盖了许多主题，有时只是稍微触及表面，但现在你应该已经准备好使用
    FastAPI 构建自己的项目，并提供智能数据科学算法。一定要查看我们在旅程中提到的所有外部资源，它们将为你提供掌握这些技能所需的所有见解。
- en: In recent years, Python has gained a lot of popularity, especially in data science
    communities, and the FastAPI framework, even though still very young, is already
    a game-changer and has seen an unprecedented adoption rate. It’ll likely be at
    the heart of many data science systems in the coming years... And as you read
    this book, you’ll probably be one of the developers behind them. Cheers!
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，Python 在数据科学社区中获得了极大的关注，尽管 FastAPI 框架仍然非常年轻，但它已经是一个改变游戏规则的工具，并且已经看到了前所未有的采用率。它很可能会成为未来几年许多数据科学系统的核心……而当你读完这本书时，你可能就是这些系统背后的开发者之一。干杯！
