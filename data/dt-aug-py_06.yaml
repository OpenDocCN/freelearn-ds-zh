- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Text Augmentation with Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用机器学习的文本增强
- en: Text augmentation with **machine learning** (**ML**) is an advanced technique
    compared to the standard text augmenting methods we covered in the previous chapter.
    Ironically, text augmentation aims to improve ML model accuracy, but we used a
    pre-trained ML model to create additional training NLP data. It’s a circular process.
    ML coding is not in this book’s scope, but understanding the difference between
    using libraries and ML for text augmentation is beneficial.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**机器学习**（**ML**）的文本增强是一种比我们在上一章中介绍的标准文本增强方法更为先进的技术。具有讽刺意味的是，文本增强旨在提高机器学习模型的准确性，但我们使用了一个预训练的机器学习模型来创建额外的训练NLP数据。这是一个循环过程。本书不涉及机器学习编程，但理解使用库与使用机器学习进行文本增强之间的区别是非常有益的。
- en: 'Augmentation libraries, whether for image, text, or audio, follow the traditional
    programming methodologies with structure data, loops, and conditional statements
    in the algorithm. For example, as shown in [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101),
    the pseudocode for implementing the `_print_aug_reserved()` method could be as
    follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 增强库，无论是用于图像、文本还是音频，都会遵循传统的编程方法论，包含结构化数据、循环和条件语句。例如，如在[*第5章*](B17990_05.xhtml#_idTextAnchor101)中所示，实施`_print_aug_reserved()`方法的伪代码可能如下所示：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The happy path code does not cover error checking, but the salient point is
    that the library’s function follows the standard sequential coding method.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 正常路径的代码未覆盖错误检查，但重点是该库的功能遵循标准的顺序编码方法。
- en: On the other hand, ML is based on one of the 13 known ML algorithms, including
    `_print_aug_reserved()` pseudocode algorithm.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，机器学习基于13种已知的机器学习算法之一，包括`_print_aug_reserved()`伪代码算法。
- en: 'The following is a representation of a DL architecture for image classification.
    It illustrates the difference between a procedural approach and the Neural Network
    algorithm. This figure was created from **Latex** and the **Overleaf** cloud system.
    The output is as follows:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于图像分类的深度学习（DL）架构表示。它展示了过程化方法与神经网络算法之间的区别。此图是通过**Latex**和**Overleaf**云系统创建的，输出结果如下：
- en: '![Figure 6.1 – Representation of a DL model](img/B17990_06_01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 深度学习模型的表示](img/B17990_06_01.jpg)'
- en: Figure 6.1 – Representation of a DL model
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 深度学习模型的表示
- en: The Overleaf project and its code are from Mr. Duc Haba’s public repository,
    and the URL is [https://www.overleaf.com/project/6369a1eaba583e7cd423171b](https://www.overleaf.com/project/6369a1eaba583e7cd423171b).
    You can clone and hack the code to display other AI models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Overleaf项目及其代码来自Duc Haba先生的公开代码库，网址为[https://www.overleaf.com/project/6369a1eaba583e7cd423171b](https://www.overleaf.com/project/6369a1eaba583e7cd423171b)。你可以克隆并修改代码以展示其他AI模型。
- en: 'This chapter will cover text augmentation with ML, and in particular, the following
    topics:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍使用机器学习的文本增强，特别是以下主题：
- en: Machine learning models
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型
- en: Word augmenting
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇增强
- en: Sentence augmenting
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子增强
- en: Real-world NLP datasets
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实世界的NLP数据集
- en: Reinforcing your learning through the Python Notebook
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Python笔记本强化你的学习
- en: Let’s briefly describe the ML models used in the Python wrapper function code.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要描述在Python包装函数代码中使用的机器学习模型。
- en: Machine learning models
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型
- en: 'In this chapter, the text augmentation wrapper functions use ML to generate
    new text for training the ML model. Understanding how these models are built is
    not in scope, but a brief description of these ML models and their algorithms
    is necessary. The Python wrapper functions will use the following ML models under
    the hood:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，文本增强包装函数使用机器学习生成新的文本，用于训练机器学习模型。理解这些模型是如何构建的并不在本书范围内，但对这些机器学习模型及其算法的简要描述是必要的。Python包装函数将在后台使用以下机器学习模型：
- en: Tomáš Mikolov published the NLP algorithm using a neural network named **Word2Vec**
    in 2013\. The model can propose synonym words from the input text.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tomáš Mikolov于2013年发布了一个名为**Word2Vec**的神经网络NLP算法。该模型可以根据输入文本提议同义词。
- en: The **Global Vectors for Word Representation** (**GloVe**) algorithm was created
    by Jeffrey Pennington, Richard Socher, and Christopher D. Manning in 2014\. It
    is an unsupervised learning NLP algorithm for representing words in vector format.
    The results are a linear algorithm that groups the closest neighboring words.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局词向量表示**（**GloVe**）算法由Jeffrey Pennington、Richard Socher和Christopher D. Manning于2014年创建。它是一种无监督学习的自然语言处理（NLP）算法，用于将词语表示为向量格式。其结果是一个线性算法，能够将最接近的相邻词汇聚集在一起。'
- en: '**Wiki-news-300d-1M** is a pre-trained ML model that uses the **fastText**
    open source library. It was trained on 1 million words from Wikipedia 2017 articles,
    the UMBC WebBase corpus, which consists of over 3 billion words, and the Statmt.org
    news dataset, which consists of over 16 billion tokens. T. Mikolov, E. Grave,
    P. Bojanowski, C. Puhrsch, and A. Joulin introduced **Wiki-news-300d-1M** in their
    *Advances in Pre-Training Distributed Word Representations* paper. The license
    is the Creative Commons Attribution-Share-Alike License 3.0.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Wiki-news-300d-1M** 是一个预训练的机器学习模型，使用了**fastText**开源库。它是基于维基百科2017年文章中的100万个单词、UMBC
    WebBase语料库（包含超过30亿个单词）和Statmt.org新闻数据集（包含超过160亿个词元）进行训练的。T. Mikolov、E. Grave、P.
    Bojanowski、C. Puhrsch 和 A. Joulin在他们的论文《*预训练分布式词表示的进展*》中介绍了**Wiki-news-300d-1M**。该模型的许可协议为Creative
    Commons Attribution-Share-Alike License 3.0。'
- en: '**GoogleNews-vectors-negative300** is a pre-trained **Word2Vec** model that
    uses the Google News dataset, which contains about 100 billion words and 300 dimensions.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GoogleNews-vectors-negative300** 是一个预训练的**Word2Vec**模型，使用了包含约1000亿个单词和300个维度的Google新闻数据集。'
- en: Google introduced the **transformer** neural network algorithm in 2017\. Recent
    cutting-edge breakthroughs in NLP and computer vision are from the transformer
    model.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google于2017年推出了**transformer**神经网络算法。近期自然语言处理和计算机视觉领域的前沿突破均来源于该变换器模型。
- en: The **BERT** model was introduced by Jacob Devlin, Ming-Wei Chang, Kenton Lee,
    and Kristina Toutanova in 2018\. It is specialized in language inference and prediction.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BERT** 模型由Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova于2018年提出，专门用于语言推理和预测。'
- en: '**RoBERTa** is an optimized algorithm for the self-supervised NLP model. It
    is a model built on top of BERT. It excels in performance on many NLP inferences.
    Meta AI published RoBERTa in 2019.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RoBERTa** 是一种优化过的自监督自然语言处理模型算法，建立在BERT的基础上。它在许多自然语言处理推理任务中表现优异。Meta AI于2019年发布了RoBERTa。'
- en: Facebook’s **wmt19-en-de** and **wmt19-de-en** are pre-trained NLP models from
    *HuggingFace* for translating from English to German (Deutsch) and back. It was
    made publicly available in 2021.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Facebook的**wmt19-en-de**和**wmt19-de-en**是来自*HuggingFace*的预训练自然语言处理模型，用于从英语翻译到德语（Deutsch）并相互转换。该模型于2021年公开发布。
- en: Facebook’s **wmt19-en-ru** and **wmt19-ru-en** are pre-trained NLP models from
    *HuggingFace* for translating from English to Russian (Русский) and back. It was
    made publicly available in 2021.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Facebook的**wmt19-en-ru**和**wmt19-ru-en**是来自*HuggingFace*的预训练自然语言处理模型，用于从英语翻译到俄语（Русский）并相互转换。该模型于2021年公开发布。
- en: '**XLNet** is a transformer-XL pre-trained model that was made publicly available
    by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R. Salakhutdinov,
    and Quoc V. Le on *HuggingFace* in 2021\. It was published in the scholarly paper
    *XLNet: Generalized Autoregressive Pretraining for* *Language Understanding*.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**XLNet** 是一个基于transformer-XL的预训练模型，由Zhilin Yang、Zihang Dai、Yiming Yang、Jaime
    Carbonell、Russ R. Salakhutdinov 和 Quoc V. Le于2021年在*HuggingFace*上公开发布。该模型在学术论文《*XLNet:
    用于语言理解的广义自回归预训练*》中进行了介绍。'
- en: The **Generative Pre-trained Transformer 2** (**GPT-2**) algorithm is an open
    source AI that was published by OpenAI in 2019\. The model excels in writing feedback
    questions and answers and generating text summarization of an article. It is at
    the level of actual human writing.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成式预训练变换器2**（**GPT-2**）算法是一个开源AI模型，由OpenAI于2019年发布。该模型在生成反馈问题与答案以及文章文本摘要方面表现出色，且与人类写作水平相当。'
- en: The **T5** and **T5X** models use the text-to-text transformer algorithm. They
    were trained on a massive corpus. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
    Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu introduced
    T5 in their paper *Exploring the Limits of Transfer Learning with a Unified Text-to-Text
    Transformer* in 2020.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**T5** 和 **T5X** 模型使用文本到文本的变换器算法。它们是基于庞大的语料库进行训练的。Colin Raffel、Noam Shazeer、Adam
    Roberts、Katherine Lee、Sharan Narang、Michael Matena、Yanqi Zhou、Wei Li 和 Peter J.
    Liu在2020年的论文《*探索统一文本到文本变换器的迁移学习极限*》中介绍了T5。'
- en: Fun fact
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: Generative AI, when using a transformer model, such as OpenAI’s GPT-3, GPT-4,
    or Google Bard, can write as well or better than a human writer.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能，当使用变换器模型时，例如OpenAI的GPT-3、GPT-4或Google Bard，能够写出与人类作家一样好的内容，甚至更好。
- en: Now that we know about some of the ML models, let’s see which augmenting function
    uses which ML models.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了一些机器学习模型，接下来我们来看看哪些增强功能使用了哪些机器学习模型。
- en: Word augmenting
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词汇增强
- en: In this chapter, the word augmenting techniques are similar to the methods from
    [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101), which used the **Nlpaug** library.
    The difference is that rather than Python libraries, the wrapper functions use
    powerful ML models to achieve remarkable results. Sometimes, the output or rewritten
    text is akin to human writers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，词语增强技术类似于[*第5章*](B17990_05.xhtml#_idTextAnchor101)中的方法，该方法使用了**Nlpaug**库。不同之处在于，包装函数使用强大的机器学习模型来实现显著的效果，而非Python库。有时，输出或重写的文本与人类写作非常相似。
- en: 'In particular, you will learn four new techniques and two variants each. Let’s
    start with Word2Vec:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你将学习四种新技术，每种技术有两种变体。让我们从Word2Vec开始：
- en: The **Word2Vec** method uses the neural network NLP Word2Vec algorithm and the
    GoogleNews-vectors-negative300 pre-trained model. Google trained it using a large
    corpus containing about 100 billion words and 300 dimensions. Substitute and insert
    are the two mode variants.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Word2Vec**方法使用神经网络NLP Word2Vec算法和GoogleNews-vectors-negative300预训练模型。Google使用包含约1000亿个单词和300维度的大型语料库进行了训练。替换和插入是两种模式变体。'
- en: The **BERT** method uses Google’s transformer algorithm and BERT pre-trained
    model. Substitute and insert are the two mode variants.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BERT**方法使用Google的Transformer算法和BERT预训练模型。替换和插入是两种模式变体。'
- en: The **RoBERTa** method is a variation of the BERT model. Substitute and insert
    are the two mode variants.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RoBERTa**方法是BERT模型的一种变体。替换和插入是两种模式变体。'
- en: The last word augmenting technique that we’ll look at in this chapter is **back
    translation** using Facebook’s (Meta’s) pre-trained translation model. It translates
    the input English text into a different language and back to English. The two
    variants we’ll look at involve translating from English into German (Deutsch)
    and back to English using the **facebook/wmt19-en-de** and **facebook/wmt19-de-en**
    models, and from English to Russian (Русский) and back to English using the **facebook/wmt19-en-ru**
    and **facebook/wmt19-ru-en** models.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍的最后一种词语增强技术是使用Facebook（Meta）的预训练翻译模型进行**反向翻译**。它将输入的英文文本翻译成另一种语言，然后再翻译回英文。我们将研究的两种变体涉及将英文翻译成德语（Deutsch），然后再翻译回英文，使用的是**facebook/wmt19-en-de**和**facebook/wmt19-de-en**模型；将英文翻译成俄语（Русский），然后再翻译回英文，使用的是**facebook/wmt19-en-ru**和**facebook/wmt19-ru-en**模型。
- en: It will be easier to understand this by reading the output from the word wrapper
    functions, but before we do, let’s describe sentence augmenting.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看词语包装函数的输出，这将更容易理解，但在此之前，我们先来描述句子增强。
- en: Sentence augmenting
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子增强
- en: Augmenting at the sentence level is a powerful concept. It was not possible
    5 years ago. You had to be working in an ML research company or a billionaire
    before accessing these acclaimed pre-trained models. Some transformer and **large
    language models** (**LLMs**) became available in 2019 and 2020 as open source,
    but they are generally for research. Convenient access to online AI servers via
    a GPU was not widely available at that time. The LLM and pre-trained models have
    recently become publicly accessible for incorporating them into your projects,
    such as the HuggingFace website. The salient point is that for independent researchers
    or students, LLM and pre-trained models only became accessible in mid-2021.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子级别进行增强是一个强大的概念，5年前这是无法实现的。你必须在机器学习研究公司工作，或者是亿万富翁，才能访问这些著名的预训练模型。某些Transformer和**大型语言模型**（**LLMs**）在2019和2020年作为开源发布，但它们通常用于研究。当时，方便通过GPU访问在线AI服务器并不普及。LLM和预训练模型最近才开始向公众开放，可以将它们集成到你的项目中，如HuggingFace网站。重要的是，对于独立研究人员或学生来说，LLM和预训练模型直到2021年中才开始变得可访问。
- en: The sentence and word augmenting methods that use ML can’t be done dynamically
    as with methods using the **Nlpaug** library. In other words, you have to write
    and save the augmented text to your local or cloud disk space. The primary reason
    is that the augmentation step takes too long per training cycle. The upside is
    that you can increase the original text by 20 to 100 times its size.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习的句子和词语增强方法不能像使用**Nlpaug**库的方法那样动态进行。换句话说，你必须将增强后的文本写入并保存到本地或云端磁盘空间。其主要原因是每个训练周期的增强步骤所需时间过长。优点是，你可以将原始文本的大小增加20到100倍。
- en: 'In particular, we will cover the following techniques:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将介绍以下技术：
- en: Summarizing text using the **T5** NLP algorithm.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**T5** NLP算法进行文本摘要。
- en: '**Sequence** and **Sometimes** are two sentence flow methods. The flow methods
    use a combination of the **GloVe** and **BERT** NLP algorithms.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序**和**有时**是两种句子流方法。这些流方法结合了 **GloVe** 和 **BERT** NLP 算法。'
- en: For the sentence augmentation techniques, they are easier to understand by reading
    the output of the wrapper functions using real-world NLP datasets as input text.
    Thus, the following section is about writing wrapper functions with Python code
    to gain insight into sentence augmenting, but first, let’s download the real-world
    NLP datasets.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于句子增强技术，通过使用真实世界的 NLP 数据集作为输入文本并查看包装函数的输出，更容易理解这些技术。因此，接下来的部分将介绍如何用 Python
    代码编写包装函数来深入了解句子增强，但首先，让我们下载真实世界的 NLP 数据集。
- en: Real-world NLP datasets
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真实世界的 NLP 数据集
- en: 'This chapter will use the same Netflix and Twitter real-world NLP datasets
    from [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101). In addition, both datasets
    have been vetted, cleaned, and stored in the `pluto_data` directory in this book’s
    GitHub repository. The startup sequence is similar to the previous chapters. It
    is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用与[**第 5 章**](B17990_05.xhtml#_idTextAnchor101)相同的 Netflix 和 Twitter 真实世界
    NLP 数据集。此外，两个数据集已经经过审核、清洗，并存储在本书 GitHub 仓库中的 `pluto_data` 目录中。启动过程与前几章相似，步骤如下：
- en: Clone the Python Notebook and Pluto.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆 Python Notebook 和 Pluto。
- en: Verify Pluto.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证 Pluto。
- en: Locate the NLP data.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定位 NLP 数据。
- en: Load the data into pandas.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据加载到 pandas。
- en: View the data.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看数据。
- en: Let’s start with the Python Notebook and Pluto.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 Python Notebook 和 Pluto 开始。
- en: Python Notebook and Pluto
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python Notebook 和 Pluto
- en: Start by loading the `data_augmentation_with_python_chapter_6.ipynb` file into
    Google Colab or your chosen Jupyter Notebook or JupyterLab environment. From this
    point onward, we will only display code snippets. The complete Python code can
    be found in the Python Notebook.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先将 `data_augmentation_with_python_chapter_6.ipynb` 文件加载到 Google Colab 或您选择的
    Jupyter Notebook 或 JupyterLab 环境中。从这一点开始，我们将仅显示代码片段。完整的 Python 代码可以在 Python Notebook
    中找到。
- en: 'The next step is to clone the repository. We will reuse the code from [*Chapter
    5*](B17990_05.xhtml#_idTextAnchor101). The `!git` and `%run` statements are used
    to instantiate Pluto:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是克隆仓库。我们将重用[**第 5 章**](B17990_05.xhtml#_idTextAnchor101)的代码。使用 `!git` 和 `%run`
    语句来实例化 Pluto：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The following setup step is checking if Pluto loaded correctly.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的设置步骤是检查 Pluto 是否正确加载。
- en: Verify
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证
- en: 'The following command asks Pluto to display his status:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令要求 Pluto 显示他的状态：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output will be as follows or similar, depending on your system:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示，具体取决于您的系统：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Pluto showed that he is from [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101)
    (version 5.0), which is correct. In addition, the cleaned NLP Twitter and Netflix
    datasets are in the `~/``Data-Augmentation-with-Python/pluto_data` directory.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 显示他来自[**第 5 章**](B17990_05.xhtml#_idTextAnchor101)（版本 5.0），这是正确的。此外，清洗后的
    NLP Twitter 和 Netflix 数据集已存储在 `~/``Data-Augmentation-with-Python/pluto_data` 目录中。
- en: Real-world NLP data
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 真实世界的 NLP 数据
- en: 'Pluto is using the clean versions of the data without profanity from [*Chapter
    5*](B17990_05.xhtml#_idTextAnchor101). They are the Netflix and Twitter NLP datasets
    from the Kaggle website. The clean datasets were saved in this book’s GitHub repository.
    Thus, Pluto does not need to download them again. Still, you can download them
    or other real-world datasets by using the `fetch_kaggle_dataset()` function. Pluto
    locates the cleaned NLP datasets with the following commands:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 正在使用来自[**第 5 章**](B17990_05.xhtml#_idTextAnchor101)的干净版本数据，这些数据没有脏话。它们是来自
    Kaggle 网站的 Netflix 和 Twitter NLP 数据集。清洗后的数据集已经保存在本书的 GitHub 仓库中。因此，Pluto 不需要再次下载它们。不过，您仍然可以通过
    `fetch_kaggle_dataset()` 函数下载它们或其他真实世界数据集。Pluto 使用以下命令来定位已清洗的 NLP 数据集：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Fun fact
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: Pluto gets lazy, and instead of using a Python library and coding it in Python,
    he cheats by dropping down to the Linux Bash command-line code. The exclamation
    character (`!`) allows the Python Notebook to backdoor the kernel, such as via
    `!ls -la` on Linux or`!dir` on Windows. You can use any OS command-line code.
    Still, it is not portable code because the commands for Windows, iOS, Linux, Android,
    and other OSs that support web browsers such as Safari, Chrome, Edge, and Firefox
    are different.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 变懒了，他没有使用 Python 库和用 Python 编码，而是通过下达 Linux Bash 命令行代码作弊。感叹号字符 (`!`) 允许
    Python Notebook 后门进入内核，例如在 Linux 上使用 `!ls -la` 或在 Windows 上使用 `!dir`。您可以使用任何操作系统的命令行代码。但这不是可移植的代码，因为
    Windows、iOS、Linux、Android 等操作系统（支持 Safari、Chrome、Edge 和 Firefox 等网页浏览器）上的命令是不同的。
- en: The next step is to load the data into Pluto’s buddy, pandas.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将数据加载到 Pluto 的伙伴 pandas 中。
- en: Pandas
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pandas
- en: 'Pluto reuses the `fetch_df()` method from [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038)
    to load the data into pandas. The following commands import the real-world Netflix
    data into pandas:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 重用了 [*第 2 章*](B17990_02.xhtml#_idTextAnchor038) 中的 `fetch_df()` 方法，将数据加载到
    pandas 中。以下命令将真实世界的 Netflix 数据导入 pandas：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Similarly, the commands for loading the real-world Twitter data are as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，加载真实世界 Twitter 数据的命令如下：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Fun challenge
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的挑战
- en: 'Pluto challenges you to find and download two additional NLP data from the
    Kaggle website. Hint: use Pluto’s `fetch_kaggle_dataset()` function. Import it
    into pandas using the `fetch_df()` function.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 挑战你从 Kaggle 网站找到并下载两个额外的 NLP 数据。提示：使用 Pluto 的 `fetch_kaggle_dataset()`
    函数。使用 `fetch_df()` 函数将其导入 pandas。
- en: Now that Pluto has located and imported the data into pandas, the last step
    in loading the data sequence is to view and verify the data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Pluto 已经定位并将数据导入 pandas，加载数据序列的最后一步是查看和验证数据。
- en: Viewing the text
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看文本
- en: The `draw_word_count()` and `draw_null_data()` methods help us understand the
    NLP data, and Pluto recommends revisiting [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101)
    to view those Netflix and Twitter graphs. A more colorful and fun method is to
    use the `draw_word_cloud()` function.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`draw_word_count()` 和 `draw_null_data()` 方法帮助我们理解 NLP 数据，Pluto 建议重新查看 [*第 5
    章*](B17990_05.xhtml#_idTextAnchor101)，以查看那些 Netflix 和 Twitter 图表。一种更有趣、更丰富多彩的方法是使用
    `draw_word_cloud()` 函数。'
- en: 'Pluto draws the Netflix word cloud infographic graph with the following command:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 使用以下命令绘制 Netflix 词云信息图：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.2– Netflix word cloud](img/B17990_06_02.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – Netflix 词云](img/B17990_06_02.jpg)'
- en: Figure 6.2– Netflix word cloud
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – Netflix 词云
- en: 'Similarly, Pluto displays the Twitter word cloud using the following commands:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Pluto 使用以下命令显示 Twitter 词云：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.3 – Twitter word cloud](img/B17990_06_03.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – Twitter 词云](img/B17990_06_03.jpg)'
- en: Figure 6.3 – Twitter word cloud
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – Twitter 词云
- en: Along with the real-world NLP data, Pluto uses the first few lines of the *Tale
    of Two Cities*, by Charles Dickens, as the control text. In this chapter, Pluto
    will extend the control text to the first page of Mr. Dickens’ book, the *Moby
    Dick* book, by Melville, and the *Alice in Wonderland* book, by Carroll. These
    books are in public domain, as defined in Project Gutenberg.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了真实世界的 NLP 数据，Pluto 还使用了查尔斯·狄更斯的《双城记》前几行作为控制文本。在本章中，Pluto 将扩展控制文本，加入狄更斯先生的《双城记》第一页、梅尔维尔的《白鲸》和卡罗尔的《爱丽丝梦游仙境》。这些书籍是公共领域作品，定义见古腾堡计划。
- en: The varibles are `pluto.orig_text`, `pluto.orig_dickens_page`, `pluto.orig_melville_page`,
    and `pluto.orig_carroll_page`, respectively.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 变量分别为 `pluto.orig_text`、`pluto.orig_dickens_page`、`pluto.orig_melville_page`
    和 `pluto.orig_carroll_page`。
- en: Fun fact
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: ML is good at altering text in typical human writing but modifying the masterworks
    is borderline criminal. Pluto seeks only to illustrate the augmentation concepts
    and never to bastardize the classics. It is in the name of science.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习擅长改变典型的人类写作文本，但修改经典作品几乎是犯罪行为。Pluto 只希望阐明增强概念，绝不亵渎经典。这一切都是为了科学。
- en: You have loaded the Python Notebook, instantiated Pluto, accessed the cleaned
    NLP real-world data, and verified it with the word cloud infographic. Now, it
    is time to write and hack Python code to gain a deeper insight into word and sentence
    augmentation with ML.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经加载了 Python Notebook，实例化了 Pluto，访问了清理过的 NLP 真实世界数据，并通过词云信息图进行了验证。现在，是时候编写并调试
    Python 代码，以更深入地了解机器学习中的单词和句子增强。
- en: Reinforcing your learning through the Python Notebook
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 Python Notebook 加强你的学习
- en: Even though NLP ML is highly complex, the implementation for the wrapper code
    is deceptively simple. This is because of Pluto’s structured object-oriented approach.
    First, we created a base class for Pluto in [*Chapter 1*](B17990_01.xhtml#_idTextAnchor016)
    and used the decorator to add a new method as we learned new augmentation concepts.
    In [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038), Pluto learned to download
    any of the thousands of real-world datasets from the Kaggle website. *Chapters
    3* and *4* introduced the wrapper functions process using powerful open source
    libraries under the hood. Finally, [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101)
    put forward the text augmentation concepts and methods when using the **Nlpaug**
    library.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管NLP ML非常复杂，包装代码的实现却出奇地简单。这是因为Pluto采用了结构化的面向对象方法。首先，我们在[*第1章*](B17990_01.xhtml#_idTextAnchor016)中为Pluto创建了一个基础类，并使用装饰器在学习新的增强概念时添加了新方法。在[*第2章*](B17990_02.xhtml#_idTextAnchor038)中，Pluto学会了从Kaggle网站下载成千上万的真实世界数据集。*第3章*和*第4章*介绍了包装函数的处理过程，使用了强大的开源库。最后，[*第5章*](B17990_05.xhtml#_idTextAnchor101)提出了使用**Nlpaug**库时的文本增强概念和方法。
- en: Therefore, building upon our previous knowledge, the wrapper functions in this
    chapter use the powerful NLP ML pre-trained model to perform the augmentations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基于我们之前的知识，本章中的包装函数使用强大的NLP ML预训练模型来执行增强。
- en: 'In particular, this chapter will present wrapper functions and the augmenting
    results for the Netflix and Twitter real-world datasets using the following techniques:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将特别介绍包装函数和使用以下技术对Netflix和Twitter真实世界数据集进行增强的结果：
- en: '**Word2Vec** word augmenting'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Word2Vec** 词语增强'
- en: '**BERT** and **Transformer** word augmenting'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BERT** 和 **Transformer** 词语增强'
- en: '**RoBERTa** augmenting'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RoBERTa** 增强'
- en: '**Back translation**'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回译**'
- en: '**T5** augmenting'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**T5** 增强'
- en: '**Sequential** and **Sometime** augmenting'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sequential** 和 **Sometime** 增强'
- en: Let’s start with Word2Vec.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从Word2Vec开始。
- en: Word2Vec word augmenting
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2Vec词语增强
- en: 'The `print_aug_ai_word2vec()` wrapper function’s key parameters are as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`print_aug_ai_word2vec()`包装函数的关键参数如下：'
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The full functions can be found in the Python Notebook. Pluto uses the real-world
    NLP Netflix data to test the function, as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的函数可以在Python Notebook中找到。Pluto使用真实世界的NLP Netflix数据来测试该函数，具体如下：
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Fun fact
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 趣味事实
- en: When you run a wrapper function, new data is randomly selected and processed.
    Thus, it would be best if you run the wrapper function repeatedly to see different
    movie reviews from the Netflix dataset or tweets from the Twitter dataset.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行一个包装函数时，新的数据会被随机选择并处理。因此，最好反复运行包装函数，查看来自Netflix数据集的不同电影评论或来自Twitter数据集的不同推文。
- en: 'The output is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.4 – Word2Vec using insert mode on the Netflix data](img/B17990_06_04.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 使用插入模式的Word2Vec在Netflix数据上的应用](img/B17990_06_04.jpg)'
- en: Figure 6.4 – Word2Vec using insert mode on the Netflix data
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 使用插入模式的Word2Vec在Netflix数据上的应用
- en: In *Figure 6**.3*, the first row is the control input. It is a quote from the
    book *A Tale of Two Cities*. You will find that the augmented effects are easier
    to spot by comparing the control text with the text in the datasets. In addition,
    the control text is needed to compare the differences between augmentation techniques.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6.3*中，第一行是控制输入。它摘自《*双城记*》这本书。你会发现，通过将控制文本与数据集中的文本进行对比，增强效果更容易察觉。此外，控制文本对于比较不同增强技术之间的差异也是必需的。
- en: 'Pluto found the injection of names on *row #1*, such as **Punta** (a believable
    Spanish writer name) and **Poydras**, as actual names and plausible additions
    to this celebrity movie review context. It was not factual in the movie, but it
    is acceptable for text augmentation for movie sentiment prediction.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto发现，在*第1行*中注入的名称，如**Punta**（一个可信的西班牙作家名字）和**Poydras**，作为实际名称和可行的添加物，符合这个名人电影评论的背景。虽然这些名字在电影中并不真实出现，但它们在电影情感预测的文本增强中是可以接受的。
- en: 'On *row #2*, the words **blending**, **dangerous**, and **original 1960s**
    add flare to the movie description without altering the intent of the spy movie’s
    description.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2行*中，**blending**、**dangerous**和**original 1960s**这几个词为电影描述增添了色彩，同时又不改变间谍电影描述的意图。
- en: 'On *row #3*, the addition of names, such as **Kent of Cabus** (Kent from a
    village in English named Cabus), **Rangjung** (a village in Bhutan, served as
    a possible hero name), and **Elizabeth** (as the villain) in the comic Green Arrow
    movie description is 100% plausible plot for our superhero.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3行*，在漫画《绿箭侠》电影描述中，加入像**Kent of Cabus**（来自英国Cabus村的Kent）、**Rangjung**（不丹的一个村庄，作为可能的英雄名字）和**Elizabeth**（作为反派角色）的名字是100%合理的情节设定，适合我们的超级英雄。
- en: Overall, Pluto is flabbergasted by the **Word2Vec** ML model. The word and name
    injections are contexts that are appropriate as if a human writer were creating
    them. However, the control text from Dickens is funny to read, and it is not ML’s
    fault. The system does not know that the book was written in the 1800s and has
    only the first few lines of the text to go off. The movie review is a complete
    thought, while the control text is a tiny fragment of the whole.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，Pluto对**Word2Vec**机器学习模型感到惊讶。单词和名字的插入情境非常合适，仿佛是人类作家创作的。然而，狄更斯的控制文本读起来非常有趣，这并不是机器学习的错。系统并不知道这本书是19世纪写的，只根据文本的前几行做出了推断。电影评论是完整的思路，而控制文本只是片段之一。
- en: 'Pluto runs a similar command on the real-world Twitter data, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto对真实世界的Twitter数据执行类似的命令，具体如下：
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.5 – Word2Vec using insert mode on the Twitter data](img/B17990_06_05.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – Word2Vec在Twitter数据上的插入模式](img/B17990_06_05.jpg)'
- en: Figure 6.5 – Word2Vec using insert mode on the Twitter data
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – Word2Vec在Twitter数据上的插入模式
- en: Since tweets are like random thoughts written without forethoughts or editing,
    in *Figure 6**.4*, the **Word2Vec** injections are like a bored high school student
    doing homework while playing a computer game. Pluto can’t judge if the altered
    text is plausible or not. Would it increase or decrease the AI prediction accuracy
    for sentiment analysis?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于推文就像是没有经过深思熟虑或编辑的随机想法，在*图6.4*中，**Word2Vec**的插入就像一个无聊的高中生一边做作业一边玩电脑游戏。Pluto无法判断修改后的文本是否合理。这会增加还是减少AI在情感分析中的预测准确性？
- en: For Dickens’ control text, Pluto flinched. It was dreadful, but he promised
    the AI would be better in the later model when using transformers and generative
    AI.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于狄更斯的控制文本，Pluto感到震惊。它非常糟糕，但他承诺当使用变换器和生成AI时，后续模型会更好。
- en: Now that we’ve looked at **insert** mode, let’s see how the **Word2Vec** model
    performs in **substitute** mode.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看过了**insert**模式，让我们看看**Word2Vec**模型在**substitute**模式下的表现。
- en: Substitute
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Substitute
- en: '**Substitute** mode replaces words and then adds words to the sentence. Pluto
    applies the **Word2Vec** model using **substitute** mode to the Netflix data like
    so:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**Substitute**模式会替换单词，并且在句子中添加单词。Pluto将**Word2Vec**模型以**substitute**模式应用于Netflix数据，具体如下：'
- en: '[PRE14]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.6 – Word2Vec using substitute mode on the Netflix data](img/B17990_06_06.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – Word2Vec在Netflix数据上的替换模式](img/B17990_06_06.jpg)'
- en: Figure 6.6 – Word2Vec using substitute mode on the Netflix data
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – Word2Vec在Netflix数据上的替换模式
- en: 'In *Figure 6**.5*, *row #0* is the control text, and on *row #1*, **zany adventure**
    is suitable for a kid adventure movie, but **liquid viagra** is definitely off
    the mark.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.5*中，*第0行*是控制文本，而*第1行*中，**zany adventure**适合儿童冒险电影，但**liquid viagra**显然不合适。
- en: 'On *row #2*, replacing **police** with **troopers**, **job** with **plum assignment**,
    **wrest** with **must take**, **figure** with **hand**, and **offenders** with
    **criminals** are suitable in the police movie. Thus, the **Word2Vec** model did
    a proper augmentation job.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2行*，将**police**替换为**troopers**、**job**替换为**plum assignment**、**wrest**替换为**must
    take**、**figure**替换为**hand**，以及将**offenders**替换为**criminals**，这些在警匪电影中是合适的。因此，**Word2Vec**模型做了适当的增强处理。
- en: 'On *row #3*, replacing **cinematic distillation** with **Scorcese decaffeination**
    is an intriguing choice worthy of a human writer. Changing **electrifying** to
    **sparkling** is clever because electricity can spark. Substituting **shadowy**
    with **clandestine** is a good choice, but switching **seven** with **five** is
    unnecessary.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3行*，将**cinematic distillation**替换为**Scorcese decaffeination**是一个有趣的选择，值得人类作家考虑。将**electrifying**改为**sparkling**很聪明，因为电力可以产生火花。将**shadowy**替换为**clandestine**是个不错的选择，但将**seven**改为**five**则显得不必要。
- en: Once again, the **Word2Vec** model could have done better for the control text.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，**Word2Vec**模型在控制文本的处理上本可以做得更好。
- en: 'Pluto does the same to the Twitter data with the following command:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto对Twitter数据做了同样的处理，命令如下：
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.7 – Word2Vec using substitute mode on the Twitter data](img/B17990_06_07.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – Word2Vec 在 Twitter 数据上使用替换模式](img/B17990_06_07.jpg)'
- en: Figure 6.7 – Word2Vec using substitute mode on the Twitter data
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – Word2Vec 在 Twitter 数据上使用替换模式
- en: In *Figure 6**.6*, the tweets are chaotic, and many are incomplete thoughts.
    The **Word2Vec** model does its best, and Pluto doesn’t think a human can do better.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6.6*中，推文杂乱无章，许多内容是不完整的想法。**Word2Vec**模型尽力而为，而 Pluto 认为没有人类能做得更好。
- en: The next technique we’ll look at is **BERT**, which uses the transformer model
    and generative AI.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要探讨的技术是 **BERT**，它使用变压器模型和生成性 AI。
- en: BERT
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT
- en: BERT is a Google transformer model trained on a massive corpus. The result is
    a near-perfect human-quality output. BERT and many other transformer models were
    made available and easily accessible on *HuggingFace* to the public starting around
    mid-August 2022.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 是一个由 Google 开发的变压器模型，经过大量语料库的训练。结果是接近完美的人类质量输出。从 2022 年 8 月中旬开始，BERT 和许多其他变压器模型通过
    *HuggingFace* 向公众开放，易于获取。
- en: 'The key code lines for the `print_aug_ai_bert()` function are as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`print_aug_ai_bert()` 函数的关键代码行如下：'
- en: '[PRE16]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The full function can be found in the Python Notebook. Pluto feeds in the NLP
    Netflix data using `insert` mode with the following command:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的函数可以在 Python Notebook 中找到。Pluto 使用 `insert` 模式将 NLP Netflix 数据输入，命令如下：
- en: '[PRE17]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The result is as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 6.8 – BERT using insert mode on the Netflix data](img/B17990_06_08.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – BERT 在 Netflix 数据上使用插入模式](img/B17990_06_08.jpg)'
- en: Figure 6.8 – BERT using insert mode on the Netflix data
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – BERT 在 Netflix 数据上使用插入模式
- en: 'In *Figure 6**.7*, Pluto immediately recognizes the improvement over the **Word2Vec**
    model. In the control text, *row #0*, the injection of words is acceptable. It
    lacks the elegance of the prose, but if you must add words to the text, it could
    pass as a human writer.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6.7*中，Pluto 立刻识别出与 **Word2Vec** 模型的改进。在控制文本*第 0 行*中，插入的单词是可以接受的。虽然缺乏散文的优雅，但如果必须在文本中加入单词，这样的文本看起来像人类写的。
- en: 'In *row #1*, the added phrases are spot on, such as **afterward**, **financial
    dubious**, **knee surgery**, and **to play a national** **film stage**.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 1 行*，添加的短语恰到好处，如 **afterward**（之后）、**financial dubious**（财务可疑）、**knee surgery**（膝盖手术）、和
    **to play a national** **film stage**（演奏国家级电影舞台）。
- en: 'In *row #2*, the augmented phrases are at human writer quality, such as **whilst
    in hiding**, **deeply suspect**, **unknown maid**, **perhaps his only**, **outside
    Russian world**, and **maybe hiding** **quite something**.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 2 行*，增强的短语质量达到了人类作家的水平，如 **whilst in hiding**（在隐藏中）、**deeply suspect**（深度可疑）、**unknown
    maid**（未知的女仆）、**perhaps his only**（或许是他唯一的）、**outside Russian world**（俄罗斯外部世界）、以及
    **maybe hiding** **quite something**（也许隐藏着某种东西）。
- en: 'In *row #3*, Pluto is impressed with the results, such as **arriving in February**,
    **little Indian lad**, **despite sparse funding**, **funding mathematics and physics**,
    and **first** **functioning airplane**.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 3 行*，Pluto 对结果印象深刻，例如 **arriving in February**（二月到达）、**little Indian lad**（小印度男孩）、**despite
    sparse funding**（尽管资金稀缺）、**funding mathematics and physics**（资助数学和物理）、和 **first**
    **functioning airplane**（第一架有功能的飞机）。
- en: Fun fact
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: Are you as amazed as Pluto regarding the BERT model’s output? It is like BERT
    is a real person, not an ANN.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否像 Pluto 一样对 BERT 模型的输出感到惊讶？它就像是一个真人，而不是一个人工神经网络（ANN）。
- en: Please rerun the wrapper function to see BERT’s augmentation on other movie
    reviews. The more you read, the more you will appreciate the advanced breakthrough
    in using the **transformer** model. It is the foundation of generative AI.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请重新运行包装函数，查看 BERT 对其他电影评论的增强效果。你读得越多，就越能欣赏使用 **变压器** 模型的先进突破。它是生成性 AI 的基础。
- en: 'Next, Pluto feeds the Twitter data into the BERT model with insert mode with
    the following command:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，Pluto 使用插入模式将 Twitter 数据输入 BERT 模型，命令如下：
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The result is as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 6.9 – BERT using insert mode on the Twitter data](img/B17990_06_09.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – BERT 在 Twitter 数据上使用插入模式](img/B17990_06_09.jpg)'
- en: Figure 6.9 – BERT using insert mode on the Twitter data
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – BERT 在 Twitter 数据上使用插入模式
- en: Fun fact
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: In *Figure 6**.8*, the BERT model gives another version of Dicken’s control
    text. There is a new rendition every time Pluto runs the wrapper function. The
    possibilities are endless. Pluto must have run the wrapper functions over 50 times.
    Not once did he notice the same result.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6.8*中，BERT 模型给出了另一版本的狄更斯控制文本。每次 Pluto 运行包装函数时都会得到一个新的版本。可能性是无穷无尽的。Pluto
    必定运行了超过 50 次包装函数。每次结果都不同。
- en: Pluto discovered that there is better NLP data to study than tweets, but they
    represent the real world, so it is worth continuing to use them. As Pluto repeatedly
    rerun the wrapper function, he preferred the BERT augmented version over the original
    tweets because inserting text made it easier to read.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 发现有比推文更适合研究的 NLP 数据，但它们代表了真实世界，因此值得继续使用。当 Pluto 反复运行包装函数时，他更喜欢 BERT 增强版的推文而不是原始推文，因为插入文本使其更易阅读。
- en: When switching to **substitute** mode, the output from BERT is better than average
    human writers.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当切换到**替换**模式时，BERT 的输出优于普通人类写作。
- en: Substitute
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 替换
- en: 'Next, Pluto feeds the Netflix data to BERT in **substitute** mode using the
    following command:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，Pluto 使用以下命令将 Netflix 数据以**替换**模式输入到 BERT 中：
- en: '[PRE19]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The result is as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 6.10 – BERT using substitute mode on the Netflix data](img/B17990_06_10.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – BERT 在 Netflix 数据上使用替换模式](img/B17990_06_10.jpg)'
- en: Figure 6.10 – BERT using substitute mode on the Netflix data
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – BERT 在 Netflix 数据上使用替换模式
- en: 'In *Figure 6**.9*, for the control text, *row #0*, BERT replaced **it was the
    age of foolishness** with **death was the age** **of love**.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '在*图 6.9*中，对于控制文本，*第 #0 行*，BERT 将**它是愚蠢的时代**替换为**死亡是爱的时代**。'
- en: Fun fact
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: Full stop. Pluto’s mind is being blown. Even Pluto’s human companion is speechless.
    Pluto expects a transformer model such as BERT to be good, but philosophical thoughts
    or poetry are on another level. Now, are you impressed with BERT?
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 停止。Pluto 的思维被震撼了。甚至 Pluto 的人类伙伴也无言以对。Pluto 期待像 BERT 这样的变压器模型能做到很好，但哲学思考或诗歌是另一个层次的。现在，你对
    BERT 印象深刻了吗？
- en: 'The rest of the movie review augmentation, shown are *rows #1*, *#2*, and *#3*,
    is flawless. The augmented words match the movie genre and context. It is like
    BERT understands the movie’s meaning, but this isn’t true. The BERT model is no
    more sentient than a toaster. However, BERT can mimic a human writer well.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '剩余的电影评论增强内容，如*第 #1 行*、*第 #2 行*和*第 #3 行*，完美无瑕。增强后的词语与电影的类型和背景契合。就像 BERT 理解了电影的意义，但这并不真实。BERT
    模型并不像烤面包机那样具备意识。然而，BERT 确实能很好地模仿人类写作。'
- en: 'One interesting note is that in *row #1*, in the movie description about a
    couple’s relationship, BERT uses the word *gay*, which was discussed in the previous
    chapter about data biases. This is because *gay* is a perfectly nice word for
    lighthearted and carefree, but in a modern context, *gay* is associated with a
    person’s homosexual orientation, especially of a man.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '一个有趣的注释是，在*第 #1 行*中，在描述一对情侣关系的电影描述中，BERT 使用了词汇*gay*，这在上一章关于数据偏见中有讨论过。因为*gay*是一个形容轻松和无忧无虑的词汇，但在现代语境下，*gay*通常与一个人的同性恋倾向，尤其是男性的同性恋倾向相关联。'
- en: Once again, Pluto encourages you to rerun the wrapper function repeatedly on
    the Python Notebook. You will appreciate it beyond the technical achievement and
    think that BERT has a personality.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，Pluto 鼓励你在 Python Notebook 上反复运行包装函数。你将会超越技术成就的欣赏，觉得 BERT 甚至有了个性。
- en: 'Pluto does the same for the Twitter data with the following command:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 使用以下命令对 Twitter 数据进行了相同的处理：
- en: '[PRE20]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The result is as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 6.11 – BERT using substitute mode on the Twitter data](img/B17990_06_11.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – BERT 在 Twitter 数据上使用替换模式](img/B17990_06_11.jpg)'
- en: Figure 6.11 – BERT using substitute mode on the Twitter data
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – BERT 在 Twitter 数据上使用替换模式
- en: As Pluto repeatedly ran the wrapper function on the Python Notebook, in *Figure
    6**.10*, he found that the augmented tweets were more accessible to read than
    the original text.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Pluto 在 Python Notebook 上反复运行包装函数时，在*图 6.10*中，他发现增强后的推文比原始文本更易于阅读。
- en: 'For the control text, *row #0*, Pluto found that having the augmented text
    **it was the age of youth**, replace the original text of **it was the epoch of
    belief** profoundly appropriate. It fits into the context and style of Mr. Dickens’s
    book.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '对于控制文本，*第 #0 行*，Pluto 发现将增强后的文本**它是青春的时代**替换原文**它是信仰的时代**，是非常合适的。这与狄更斯先生书中的背景和风格非常契合。'
- en: Fun challenge
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的挑战
- en: 'This challenge is a thought experiment. BERT is built on an ANN algorithm.
    It does not contain grammar rules, such as nouns and verbs for constructing sentences.
    With no grammar rules, how does it write English so well? Hint: think about patterns.
    BERT is trained on a massive corpus. The number of words and sentences is so large
    that it was impossible to conceive 5 years ago. A few, if any, know how neural
    network algorithms learn. It is not complex math. It is gradient descent and matrix
    multiplication nudging billions of nodes (or neurons), but how does that collection
    of nodes write English convincingly?'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这个挑战是一个思维实验。BERT是基于人工神经网络（ANN）算法构建的。它不包含语法规则，比如名词和动词用来构造句子。没有语法规则，它是如何写出如此流利的英语的呢？提示：考虑模式。BERT在庞大的语料库上进行了训练。单词和句子的数量大到五年前根本无法想象。很少有人知道神经网络算法是如何学习的。这不是复杂的数学问题。它是梯度下降和矩阵乘法推动数十亿个节点（或神经元），但这些节点的集合又是如何写出让人信服的英语的呢？
- en: Pluto can spend days talking about BERT, but let’s move forward with **RoBERTa**
    (**Roberta**). It sounds like a female version of BERT.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 冥王星可以花几天时间讨论BERT，但让我们继续深入了解**RoBERTa**（**Roberta**）。这听起来像是BERT的女性版。
- en: RoBERTa
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RoBERTa
- en: RoBERTa is an optimized algorithm for self-supervising BERT. While Google created
    BERT, Meta AI (or Facebook) developed RoBERTa.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa是BERT的自监督优化算法。虽然BERT是由谷歌创建的，但RoBERTa是Meta AI（或Facebook）开发的。
- en: 'Pluto feeds the Netflix data to RoBERTa in insert mode with the following command:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 冥王星用以下命令将Netflix数据输入RoBERTa的插入模式：
- en: '[PRE21]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The result is as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 6.12 – RoBERTa using insert mode on the Netflix data](img/B17990_06_12.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12 – RoBERTa在Netflix数据上使用插入模式](img/B17990_06_12.jpg)'
- en: Figure 6.12 – RoBERTa using insert mode on the Netflix data
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – RoBERTa在Netflix数据上使用插入模式
- en: The output in *Figure 6**.11* is similar to the output from **BERT**, which
    is impressive. The words are not randomly inserted in the sentence. They expressed
    a possible interpretation and gave the impression that **RoBERTa** understood
    the meaning of the words. This level of technical achievement was not feasible
    1 year ago, and **RoBERTa** was only made available a few months ago.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.11*中的输出与**BERT**的输出相似，令人印象深刻。单词并不是随机插入句中的。它们表达了可能的解释，并给人一种**RoBERTa**理解单词意义的印象。这样的技术成就1年前是无法实现的，而**RoBERTa**几个月前才发布。'
- en: 'Pluto ran the wrapper function repeatedly and never tired of reading the result.
    He does the same for the Twitter data with the following command:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 冥王星反复运行包装函数，并且从未厌倦阅读结果。他用以下命令对Twitter数据做了相同的操作：
- en: '[PRE22]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The result is as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 6.13 – RoBERTa using insert mode on the Twitter data](img/B17990_06_13.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图6.13 – RoBERTa在Twitter数据上使用插入模式](img/B17990_06_13.jpg)'
- en: Figure 6.13 – RoBERTa using insert mode on the Twitter data
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – RoBERTa在Twitter数据上使用插入模式
- en: Pluto can’t turn lead into gold, and RoBERTa can’t turn tweets, as shown in
    *Figure 6**.12*, that contain misspellings and incomplete thoughts into coherent
    sentences. Nevertheless, RoBERTa is one of the best choices for augmenting real-world
    tweets.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 冥王星无法将铅变成金子，而RoBERTa也无法将如*图6.12*所示的包含拼写错误和不完整想法的推文转化为连贯的句子。尽管如此，RoBERTa仍然是增强现实世界推文的最佳选择之一。
- en: Next, Pluto will try **RoBERTa** with **substitute** mode.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，冥王星将尝试使用**替换**模式的**RoBERTa**。
- en: Substitute
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 替换
- en: In substitute mode, RoBERTa will replace words or phrases with uncanny accuracy
    matching the context and writing style.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在替换模式下，RoBERTa将根据上下文和写作风格，以惊人的准确度替换单词或短语。
- en: 'Pluto drops the Netflix data into the RoBERTa model in substitute mode using
    the following command:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 冥王星将Netflix数据以替换模式输入RoBERTa模型，使用以下命令：
- en: '[PRE23]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.14 – RoBERTa using substitute mode on the Netflix data](img/B17990_06_14.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图6.14 – RoBERTa在Netflix数据上使用替换模式](img/B17990_06_14.jpg)'
- en: Figure 6.14 – RoBERTa using substitute mode on the Netflix data
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – RoBERTa在Netflix数据上使用替换模式
- en: 'No matter how often Pluto executes the wrapper function, he continues to be
    astonished by the output RoBERTa provides in *Figure 6**.13*. For example, in
    *row #1*, she changed the phrase **Alex discovers he has little in common with
    the local** to **Alex discovers Flix had special romantic chemistry with the local**.
    RoBERTa has quite the imagination. Is that what humans do when we write?'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 无论冥王星执行多少次包装函数，他仍然对RoBERTa在*图6.13*中提供的输出感到惊讶。例如，在*第1行*，她将短语**Alex discovers
    he has little in common with the local**改为**Alex discovers Flix had special romantic
    chemistry with the local**。RoBERTa真是富有想象力。这就是人类写作时的方式吗？
- en: 'Pluto does the same with the Twitter data using the following command:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 冥王星使用以下命令对Twitter数据执行相同操作：
- en: '[PRE24]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The result is as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 6.15 – RoBERTa using substitute mode on the Twitter data](img/B17990_06_15.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图6.15 – RoBERTa在Twitter数据上的替代模式使用](img/B17990_06_15.jpg)'
- en: Figure 6.15 – RoBERTa using substitute mode on the Twitter data
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 – RoBERTa在Twitter数据上的替代模式使用
- en: 'As shown in *Figure 6**.14*, text augmentation does not have to be boring or
    clinical. Using transformer models such as BERT and RoBERTa, augmentations are
    fun and full of wonders. For example, in the control text, on *row #0*, RoBERTa
    wrote, **It preached a curse at arrogance**, replacing **It was an epoch** **of
    belief**.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如**图6.14**所示，文本增强不必是枯燥或临床的。使用BERT和RoBERTa等变换器模型，增强过程既有趣又充满奇迹。例如，在控制文本中，**第0行**，RoBERTa写道，**它对傲慢宣讲了诅咒**，取代了**这是一个信仰的时代**。
- en: Fun fact
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: Pluto’s human companion has to ponder a long time to conclude that the augmented
    text does mean the same as the original text in *Figure 6**.14*, the control text.
    It is easy to be fooled that RoBERTa has a conscience. We pair intelligence with
    consciousness, meaning if you have intelligence, you must be self-aware or vice
    versa, but we know that is not true. For example, a career politician is self-aware.
    He talks about himself all the time, but is he intelligent?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 冥王星的人类伴侣需要经过长时间的思考，才能得出结论：增强文本确实与**图6.14**中的原始文本——控制文本——意义相同。很容易被误导，认为RoBERTa有良知。我们将智能与意识相联系，意味着如果你有智能，你必须是自我意识的，反之亦然，但我们知道这并不成立。例如，一名职业政治家非常自我意识。他一直在谈论自己，但他聪明吗？
- en: Continuing to use the latest powerful ML models, Pluto will take a different
    path to text augmentation by using the **back** **translation** technique.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用最新的强大ML模型，冥王星将通过使用**反向翻译**技术走上不同的文本增强道路。
- en: Back translation
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向翻译
- en: Back translation is a new concept in text augmentation because it was not possible
    2 years ago. ML NLP existed earlier, with Google Translate leading the charge.
    Still, only a few data scientists could access the large language model using
    a transformer and the powerful servers required for language translation.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 反向翻译是文本增强中的一个新概念，因为在两年前是不可行的。机器学习的自然语言处理（ML NLP）技术早已存在，Google 翻译也一直在推进。但只有少数数据科学家能够使用变换器和进行语言翻译所需的强大服务器，访问大型语言模型。
- en: The technique for text augmentation is to translate into another language and
    back to the original language. In doing so, the result will be an augmented version
    of the original. No language translation is perfect. Hence, the extended version
    will be slightly different from the original text. For example, the original text
    is in English. Using a powerful NLP model, we translated it into German and back
    to English again. The translated English text will be different from the original
    English text.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 文本增强的技术是将文本翻译成另一种语言，然后再翻译回原语言。通过这种方式，结果将是原始文本的增强版本。没有任何语言翻译是完美的，因此扩展版本会与原始文本略有不同。例如，原文是英文的。使用强大的NLP模型，我们将其翻译成德语，然后再翻译回英语。翻译后的英语文本将与原始的英语文本有所不同。
- en: Compared to **Word2Vec**, **BERT**, and **RoBERTa**, the back translation method
    could be more robust. This is because translating back to the original text gives
    the same result the second or third time. In other words, other methods’ output
    results in thousands of variations, while back translations have two or three
    augmented versions.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 与**Word2Vec**、**BERT**和**RoBERTa**相比，反向翻译方法可能更具鲁棒性。这是因为翻译回原文时，第二次或第三次得到的结果是相同的。换句话说，其他方法的输出会产生成千上万种变化，而反向翻译则只有两三种增强版本。
- en: Pluto found two NLP pre-trained translation models from Facebook, or Meta AI,
    that were made available on the *HuggingFace* site. They are for English to German
    (Deutsch) and English to Russian (Русский). There are dozens more, but two are
    sufficient to demonstrate the technique. Let’s start with German.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 冥王星在*HuggingFace*网站上找到了两个来自Facebook（或Meta AI）的NLP预训练翻译模型，它们支持英语到德语（Deutsch）和英语到俄语（Русский）。还有更多的模型，但这两个足以展示该技术。让我们从德语开始。
- en: German (Deutsch)
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 德语 (Deutsch)
- en: 'The `print_aug_ai_back_translation()` method follows the same structure as
    any other wrapper function. It looks deceptively simple with five lines of code,
    but it has truly complex theories and coding techniques. It reminds Pluto of a
    famous quote by Sir Isaac Newton: “*If I have seen further, it is by standing
    on the shoulders* *of giants.*”'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`print_aug_ai_back_translation()`方法遵循与其他包装函数相同的结构。它看起来 deceptively 简单，只有五行代码，但实际上包含了复杂的理论和编码技巧。这让Pluto想起了艾萨克·牛顿爵士的名言：“*如果我看得更远，那是因为站在了*
    *巨人的肩膀上。*”'
- en: 'The key code lines are as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 关键代码行如下：
- en: '[PRE25]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The full function can be found in the Python Notebook. Pluto feeds in the Netflix
    data using the following command:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的函数可以在 Python Notebook 中找到。Pluto 使用以下命令将 Netflix 数据输入：
- en: '[PRE26]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The result is as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 6.16 – Back translation, German on Netflix data](img/B17990_06_16.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.16 – 回译，Netflix 数据中的德语](img/B17990_06_16.jpg)'
- en: Figure 6.16 – Back translation, German on Netflix data
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16 – 回译，Netflix 数据中的德语
- en: Fun fact
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 趣事
- en: The output in *Figure 6**.15* is anticlimactic because it reads similarly to
    the original text, but the technical achievement is mind-blowing. First, you need
    an expert to translate from English to German. It is a challenging task for a
    human to learn. Second, you must translate back to English with no errors. The
    difference in choosing similar words is expressing the phrase. Maybe 5% of the
    world’s population can do this task. For a machine to do it 24 hours a day, 7
    days a week, and maintain the same accuracy level is miraculous. No human can
    match this level.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.15*中的输出令人失望，因为它与原文相似，但这一技术成就令人震惊。首先，你需要一位专家将英语翻译成德语。这对人类来说是一个具有挑战性的任务。其次，你必须将其无误地翻译回英语。选择类似词语的差异体现在表达上。也许只有全球
    5% 的人能完成这个任务。让机器全天候、每周 7 天、维持同样的准确性水平来完成这项任务是神奇的。没有人类能够达到这个水平。'
- en: 'The output in *Figure 6**.15* gives an almost perfect English to German and
    back translation. Pluto does the same with the Twitter data using the following
    command:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.15*中的输出提供了几乎完美的英德回译。Pluto 用以下命令对 Twitter 数据做了相同的操作：'
- en: '[PRE27]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.17 – Back translation, German on Twitter data](img/B17990_06_17.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.17 – 回译，Twitter 数据中的德语](img/B17990_06_17.jpg)'
- en: Figure 6.17 – Back translation, German on Twitter data
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17 – 回译，Twitter 数据中的德语
- en: In *Figure 6**.16*, translating nonsensible tweets into German and back is harder
    for humans because our minds get tired more quickly and give up. Only a machine
    can do this work around the clock. The control text translations into German and
    back are acceptable.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6.16*中，将无意义的推文翻译成德语并回译对人类来说更困难，因为我们的思维更容易疲劳并且会放弃。只有机器才能全天候完成这项工作。控制文本的德语回译是可以接受的。
- en: Translation to Russian and back would yield similar results. Let’s take a look.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本翻译成俄语并回译会产生类似的结果。我们来看看。
- en: Russian (Русский)
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 俄语（Русский）
- en: Pluto chose to repeat the same back translation technique with English to Russian
    and back because he is curious to see if choosing a non-Romance family language
    would affect the augmentation results differently.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 选择重复相同的英俄回译技术，因为他好奇选择非罗曼语族语言是否会对增强结果产生不同的影响。
- en: 'Using the same `print_aug_ai_back_translation()` function, Pluto defines the
    Russian translation Facebook model as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的`print_aug_ai_back_translation()`函数，Pluto 将俄语翻译的 Facebook 模型定义如下：
- en: '[PRE28]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The full function code can be found in the Python Notebook. Pluto feeds the
    Netflix data to the wrapper function as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的函数代码可以在 Python Notebook 中找到。Pluto 将 Netflix 数据传递给包装函数如下：
- en: '[PRE29]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The result is as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 6.18 – Back translation, Russian on Netflix data](img/B17990_06_18.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.18 – 回译，Netflix 数据中的俄语](img/B17990_06_18.jpg)'
- en: Figure 6.18 – Back translation, Russian on Netflix data
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18 – 回译，Netflix 数据中的俄语
- en: Remarkably, in *Figure 6**.17*, the NLP **T5** model translates a Romance family
    language (English) into an East Slavic language (Russian) and back with almost
    perfect accuracy. The grammar rules, sentence structures, alphabets, histories,
    cultures, and languages are different, yet a machine can do the task without awareness.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在*图 6.17*中，NLP **T5** 模型将一种罗曼语族语言（英语）翻译成一种东斯拉夫语言（俄语）并回译，几乎达到了完美的准确性。语法规则、句子结构、字母表、历史、文化和语言都不同，但机器能够在没有意识的情况下完成这项任务。
- en: 'Tweets are not perfect for testing, but not all projects are logical. Pluto
    had worked on real-world NLP projects that were ill-conceived. The command for
    feeding Twitter data to the wrapper function is as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 推文不适合测试，但并非所有项目都是合乎逻辑的。Pluto曾参与过一些构思不周的现实世界NLP项目。向包装函数提供推特数据的命令如下：
- en: '[PRE30]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The result is as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 6.19 – Back translation, Russian on Twitter data](img/B17990_06_19.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图6.19 – 反向翻译，推特数据上的俄语](img/B17990_06_19.jpg)'
- en: Figure 6.19 – Back translation, Russian on Twitter data
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 – 反向翻译，推特数据上的俄语
- en: If Russians don’t understand tweets, then who else can? Reading the control
    text in *Figure 6**.18*, Pluto can tell the translations are correct. Since some
    tweets are short, the translations to Russian and back are perfect.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如果俄罗斯人不能理解推文，那么还有谁能呢？阅读*图6.18*中的控制文本，Pluto可以判断翻译是否正确。由于某些推文较短，翻译成俄语后再翻回英语非常完美。
- en: Fun challenge
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的挑战
- en: This challenge is a thought experiment. Can you use the same techniques to augment
    the German language? Or can you string several back translations together – for
    example, from English to German to Russian and back to English?
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这个挑战是一个思想实验。你能否使用相同的技术来增强德语？或者你能否将几个反向翻译组合在一起——例如，从英语翻译到德语，再到俄语，最后回到英语？
- en: The **back translation**, **RoBERTa**, **BERT**, and **Word2Vec** NLP ML models
    are the state of the art for text augmentation. The next level is sentence augmentation
    using summarization and the Sequential and Sometimes techniques.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向翻译**、**RoBERTa**、**BERT**和**Word2Vec** NLP机器学习模型是文本增强的最先进技术。下一个层次是使用摘要和顺序及有时技术的句子增强。'
- en: Sentence augmentation
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句子增强
- en: The sentence **flow** level uses a combination of word augmentation methods.
    But before that, Pluto will use the **T5** NLP model to generate a text summary.
    The **summarization** technique is one of the novel concepts made possible recently.
    It takes a page, an article, or even a book and generates a summary to be used
    in the NLP text augmentation model.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 句子**流**级别使用了多种词汇增强方法的结合。但在此之前，Pluto将使用**T5** NLP模型生成文本摘要。**摘要**技巧是最近才实现的一项新概念。它可以将一页、一篇文章甚至一本书生成一个摘要，用于NLP文本增强模型中。
- en: Summary technique
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要技巧
- en: For text augmentation, the **summary** technique may bring a few different versions
    for training. However, suppose Pluto combines the **flow** and **summary** techniques,
    such as by feeding the synopsis text, instead of the original text, to the **flow**
    technique. In that case, it will yield many new original texts for training.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本增强，**摘要**技巧可能会带来几个不同的版本进行训练。然而，假设Pluto将**流**和**摘要**技巧结合起来，例如，向**流**技巧提供摘要文本，而不是原始文本，在这种情况下，它将生成许多新的原始文本用于训练。
- en: Fun fact
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: Pluto pioneered the **summary-to-flow** concept for text augmentation. He had
    done a preliminary search on the web and scholarly publications, but he needs
    help finding a reference to the summary-to-flow technique. If none are found,
    then Pluto is the first to implement the summary-to-flow strategy.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto首创了**摘要到流**的概念，用于文本增强。他在网上和学术出版物中做了初步搜索，但他需要帮助找到摘要到流技巧的参考文献。如果没有找到相关资料，那么Pluto就是第一个实现该策略的人。
- en: 'Pluto will not use the Netflix movie description or Twitter tweets for the
    summary method. This is because they are too short to showcase the power of the
    T5 NLP model. Instead, Pluto will use the first page of the following books mentioned
    in the *Real-world NLP* *data* section:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto不会使用Netflix电影描述或Twitter推文作为摘要方法。这是因为它们太短，无法展示T5 NLP模型的强大功能。相反，Pluto将使用以下*现实世界NLP*
    *数据*部分提到的书籍的第一页：
- en: '*Tale of Two Cities* by Dickens'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*双城记* 由狄更斯所著'
- en: '*Moby Dick* by Melville'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*白鲸* 由梅尔维尔所著'
- en: '*Alice in Wonderland* by Carroll'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*爱丽丝梦游仙境* 由卡罗尔所著'
- en: Once again, the books are in the public domain, as defined in Project Gutenberg.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这些书籍是公有领域的，如古腾堡计划所定义。
- en: In addition, Pluto will use the first page of this chapter because you have
    read this book, but you may not have read those three classic books.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Pluto将使用本章的第一页，因为你已经读过这本书，但你可能没有读过那三本经典书籍。
- en: 'The key code line for the `print_aug_ai_t5()` wrapper function is as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`print_aug_ai_t5()`包装函数的关键代码行如下：'
- en: '[PRE31]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Pluto is playing a guessing game with you. First, he will list the four command
    lines to generate the four summaries, but he will shuffle the output. Thus, you
    have to guess which summary belongs to which book. Once again, you will be amazed
    at the quality output of the **T5** NLP model. It is comparable to human writers.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 普鲁托正在和你玩猜谜游戏。首先，他将列出四条命令行来生成四个摘要，但他会将输出结果打乱顺序。因此，你必须猜出哪个摘要属于哪本书。你将再次对**T5**
    NLP 模型的输出质量感到惊讶，它可与人类作家相媲美。
- en: The profound implication is that you or Pluto can auto-generate summaries of
    books, papers, documents, articles, and posts with a few lines of Python code.
    This task was deemed impossible a few years ago.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 深刻的意义在于，你或普鲁托可以通过几行 Python 代码自动生成书籍、论文、文档、文章和帖子摘要。几年前，这项任务被认为是不可能完成的。
- en: Fun challenge
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的挑战
- en: Here is a thought experiment. Can you be an expert in German laws without speaking
    German? It was impossible a year ago because the ML breakthrough wasn’t available,
    but today, you can use the code in the Python Notebook as the base to translate
    all German law books.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个思维实验。你能在不懂德语的情况下成为德国法律的专家吗？一年前这是不可能的，因为当时没有机器学习的突破，但今天，你可以使用 Python Notebook
    中的代码作为基础，翻译所有德国语法书籍。
- en: 'The four commands to get a summary of the first page of the four books we’ll
    be looking at are as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 获取我们将要查看的四本书第一页摘要的四条命令如下：
- en: '[PRE32]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The shuffled results are as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 打乱顺序的结果如下：
- en: '![Figure 6.20 – Summary T5 NLP engine – 1](img/B17990_06_20.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.20 – 摘要 T5 NLP 引擎 – 1](img/B17990_06_20.jpg)'
- en: Figure 6.20 – Summary T5 NLP engine – 1
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.20 – 摘要 T5 NLP 引擎 – 1
- en: 'The second output is as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个输出如下：
- en: '![Figure 6.21 – Summary T5 NLP engine – 2](img/B17990_06_21.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.21 – 摘要 T5 NLP 引擎 – 2](img/B17990_06_21.jpg)'
- en: Figure 6.21 – Summary T5 NLP engine – 2
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.21 – 摘要 T5 NLP 引擎 – 2
- en: 'The third output is as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个输出如下：
- en: '![Figure 6.22 – Summary T5 NLP engine – 3](img/B17990_06_22.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.22 – 摘要 T5 NLP 引擎 – 3](img/B17990_06_22.jpg)'
- en: Figure 6.22 – Summary T5 NLP engine – 3
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.22 – 摘要 T5 NLP 引擎 – 3
- en: 'The fourth output is as follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个输出如下：
- en: '![Figure 6.23 – Summary T5 NLP engine – 4](img/B17990_06_23.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.23 – 摘要 T5 NLP 引擎 – 4](img/B17990_06_23.jpg)'
- en: Figure 6.23 – Summary T5 NLP engine – 4
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.23 – 摘要 T5 NLP 引擎 – 4
- en: Fun challenge
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的挑战
- en: Can you match the summarized output with the book? The T5 engine is not a generative
    AI engine like OpenAI GPT3, GPT4, or Google Bard. Still, the summary is very accurate.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 你能将摘要与书籍匹配起来吗？**T5** 引擎并不像 OpenAI GPT3、GPT4 或 Google Bard 那样是一个生成型人工智能引擎，但它的摘要非常准确。
- en: The *Tale of Two Cities* book, shown in *Figure 6**.19*, is a relatively hard
    book to read, and Pluto thinks that it is funny that *David Rothkopf*, a contemporary
    political commentator, is associated with Dickens’ book. The first page does talk
    about the **congress of British subjects in America**. Thus, the Mr. Rothkopf
    association is a good guess. Maybe Pluto should feed the first 10 pages of the
    chapter into the **T5** NLP engine and see the summary.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '*双城记*书籍，如*图 6.19*所示，是一本相对较难读的书，普鲁托觉得非常有趣的是，现代政治评论员*大卫·罗斯科普夫*竟然与狄更斯的这本书有关联。书的第一页确实提到了**英国臣民在美国的议会**。因此，罗斯科普夫先生的关联是一个很好的猜测，也许普鲁托应该将章节的前10页输入到**T5**
    NLP 引擎中，并查看摘要。'
- en: The *Moby Dick* first-page summary is spot on, as shown in *Figure 6**.20*.
    It could pass as a human writer, and the first word is **Ishmael**. Pluto wishes
    that the **T5** NLP model was available during Pluto’s early days in school.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*白鲸*的第一页摘要非常到位，如*图 6.20*所示。它可以与人类作家的作品相媲美，第一页的第一个词是**以实马利**。普鲁托希望**T5** NLP
    模型能在他上学的早期就出现。'
- en: Pluto’s human companion is delighted to admit that the summary of this chapter’s
    first page is more precise and easier to read, as shown in *Figure 6**.21*. Maybe
    the **T5** NLP engine should co-write this book with Pluto so that his companion
    can enjoy chasing squirrels on a sunny afternoon.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 普鲁托的人类伙伴高兴地承认，这一章节第一页的摘要更为精准且易于阅读，如*图 6.21*所示。也许**T5** NLP 引擎应该和普鲁托一起共写这本书，这样他的伙伴就能在阳光明媚的下午享受追逐松鼠的乐趣。
- en: 'The *Alice in Wonderland* first-page summary is perfect, as shown in *Figure
    6**.22*. The **T5** NLP engine captures the assent of the opening page flawlessly.
    As a bonus, Pluto only inputted the first five sentences. The output is as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '*爱丽丝梦游仙境*的第一页摘要完美无缺，如*图 6.22*所示。**T5** NLP 引擎无误地捕捉到了开篇的内容。作为额外奖励，普鲁托只输入了前五句话。输出如下：'
- en: '![Figure 6.24 – Summary T5 NLP Engine – the first five lines of Alice in Wonderland](img/B17990_06_24.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.24 – 摘要 T5 NLP 引擎 – 《爱丽丝梦游仙境》前五行](img/B17990_06_24.jpg)'
- en: Figure 6.24 – Summary T5 NLP Engine – the first five lines of Alice in Wonderland
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24 – 总结T5 NLP引擎 – 《爱丽丝梦游仙境》的前五行
- en: In *Figure 6**.23*, how does **T5** know that the white rabbit is essential
    to the story? The rabbit only appears in the last sentence in the input text,
    and referring to Alice as the daisy-chain maker is delightful.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.23*中，**T5**如何知道白兔是故事中的关键？兔子只在输入文本的最后一句话中出现，称爱丽丝为雏菊链的制造者非常有趣。
- en: The next step in sentence augmentation is to feed these summaries to the **flow**
    methods.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 句子增强的下一步是将这些摘要输入到**流**方法中。
- en: Summary-to-flow technique
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结到流技术
- en: 'The Sequential method in the flow technique applies a list of augmentation
    in successive order. Pluto creates two text augmentation methods, as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 流技术中的顺序方法按顺序应用增强列表。Pluto创建了两个文本增强方法，如下所示：
- en: '[PRE33]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The first uses the `print_aug_ai_sequential()` wrapper function uses the augmentation
    list with the key code lines, as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个使用`print_aug_ai_sequential()`包装函数，它使用增强列表和关键代码行，如下所示：
- en: '[PRE34]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Pluto feeds the four summaries to the flow method, as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto将四个摘要输入流方法，如下所示：
- en: '[PRE35]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Let’s take a look at the results.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下结果。
- en: 'The *Alice in Wonderland* augmented summary output is as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '*《爱丽丝梦游仙境》*的增强摘要输出如下：'
- en: '![Figure 6.25 – Summary-to-flow method, Alice in Wonderland](img/B17990_06_25.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![图6.25 – 总结到流方法，爱丽丝梦游仙境](img/B17990_06_25.jpg)'
- en: Figure 6.25 – Summary-to-flow method, Alice in Wonderland
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25 – 总结到流方法，爱丽丝梦游仙境
- en: 'The *Tale of Two Cities* augmented summary output is as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '*《双城记》*的增强摘要输出如下：'
- en: '![Figure 6.26 – Summary-to-flow method, Tale of Two Cities](img/B17990_06_26.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![图6.26 – 总结到流方法，双城记](img/B17990_06_26.jpg)'
- en: Figure 6.26 – Summary-to-flow method, Tale of Two Cities
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.26 – 总结到流方法，双城记
- en: 'The *Moby Dick* augmented summary output is as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '*《白鲸》*的增强摘要输出如下：'
- en: '![Figure 6.27 – Summary-to-flow method, Moby Dick](img/B17990_06_27.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图6.27 – 总结到流方法，白鲸](img/B17990_06_27.jpg)'
- en: Figure 6.27 – Summary-to-flow method, Moby Dick
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.27 – 总结到流方法，白鲸
- en: 'This chapter’s augmented summary output is as follows:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的增强摘要输出如下：
- en: '![Figure 6.28 – Summary-to-flow method, this chapter](img/B17990_06_28.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![图6.28 – 总结到流方法，本章](img/B17990_06_28.jpg)'
- en: Figure 6.28 – Summary-to-flow method, this chapter
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.28 – 总结到流方法，本章
- en: Pluto enjoyed reading the augmented summaries. Some are clever, and some are
    exaggerated, but for augmentation text, they are sufficient.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto喜欢阅读增强后的摘要。有些聪明，有些夸张，但对于增强文本来说，它们足够了。
- en: The next `print_aug_ai_sometime()` wrapper function on the Python Notebook,
    but he does not think explaining the results in this chapter would add more insight.
    You can run the wrapper function in the Python Notebook and view the results.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是`print_aug_ai_sometime()`包装函数，位于Python Notebook中，但他认为在这一章解释结果不会带来更多的洞察。你可以在Python
    Notebook中运行这个包装函数并查看结果。
- en: Fun challenge
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的挑战
- en: Pluto challenges you to refactor the **Pluto class** to make it faster and more
    compact. You should also include all the image wrapper and helper functions from
    previous chapters. Pluto encourages you to create and upload your library to *GitHub*
    and *PyPI.org*. Furthermore, you don’t have to name the class **PacktDataAug**,
    but it would give Pluto and his human companion a great big smile if you cited
    or mentioned this book. The code goals were for ease of understanding, reusable
    patterns, and teaching on the **Python Notebook**. Thus, refactoring the code
    as a Python library would be relatively painless and fun.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto挑战你重构**Pluto类**，使其更快、更紧凑。你还应包含前几章的所有图片包装器和辅助函数。Pluto鼓励你创建并上传你的库到*GitHub*和*PyPI.org*。此外，你不必将类命名为**PacktDataAug**，但如果你引用或提到这本书，Pluto和他的人类伙伴会露出灿烂的笑容。代码目标是为了易于理解、可重用的模式，以及在**Python
    Notebook**中的教学。因此，将代码重构为Python库会相对轻松且有趣。
- en: The **summary-to-flow** technique is the last method that will be covered in
    the chapter. Now, let’s summarize this chapter.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结到流**技术是本章最后将介绍的方法。现在，让我们总结一下这一章。'
- en: Summary
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Text augmentation with machine learning (ML) is an advanced technique. We used
    a pre-trained ML model to create additional training NLP data.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习（ML）进行文本增强是一项高级技术。我们使用了一个预训练的ML模型来创建额外的训练NLP数据。
- en: After inputting the first three paragraphs, the **T5** NLP ML engine wrote the
    preceding summary for this chapter. It is perfect and illustrates the spirit of
    this chapter. Thus, Pluto has kept it as-is.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入前三个段落后，**T5** NLP ML 引擎为本章写出了前面的总结。它非常完美，体现了本章的精神。因此，Pluto 保留了这个总结不做更改。
- en: In addition, we discussed 14 NLP ML models and four word augmentation methods.
    They were **Word2Vec**, **BERT**, **RoBERTa**, and **back translation**.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们讨论了 14 种 NLP ML 模型和四种词语增强方法。它们分别是**Word2Vec**、**BERT**、**RoBERTa**和**回译**。
- en: Pluto demonstrated that BERT and RoBERTa are as good as human writers. The augmented
    text is not just merely appropriate but inspirational, such as replacing *it was
    the age of foolishness* with *death was the age of love* or *it was the epoch
    of belief* with *it was the age* *of youth*.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Pluto 展示了 BERT 和 RoBERTa 能与人类写作相媲美。增强后的文本不仅仅是合适的，还富有启发性，例如将*这是愚蠢的时代*替换为*死亡是爱的时代*，或者将*这是信仰的时代*替换为*这是*青春*的时代*。
- en: For the **back translation** method, Pluto used the Facebook or Meta AI NLP
    model to translate to German and Russian and back to English.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**回译**方法，Pluto 使用 Facebook 或 Meta 的 AI NLP 模型将文本翻译成德语和俄语，然后再翻译回英语。
- en: For sentence augmentation, Pluto dazzled with the accuracy of the **T5** NLP
    ML engine to summarize the first page of three classic books. Furthermore, he
    pioneered the **summary-to-flow** concept for text augmentation. Pluto might be
    the first to implement the **summary-to-flow** strategy.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 对于句子增强，Pluto 通过 **T5** NLP ML 引擎精准总结了三本经典书籍的第一页，给大家留下了深刻印象。此外，他还开创了**从总结到流**的文本增强概念。Pluto
    可能是第一个实现**从总结到流**策略的人。
- en: Throughout this chapter, there were *fun facts* and *fun challenges*. Pluto
    hopes you will take advantage of these and expand your experience beyond the scope
    of this chapter.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，有许多*有趣的事实*和*有趣的挑战*。Pluto 希望你能充分利用这些内容，拓展你的经验，超越本章的范围。
- en: The next chapter is about audio augmentation, which will pose different challenges,
    but Pluto is ready for them.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论音频增强，这将带来不同的挑战，但 Pluto 已经准备好应对它们。
- en: 'Part 4: Audio Data Augmentation'
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分：音频数据增强
- en: 'This part includes the following chapters:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 7*](B17990_07.xhtml#_idTextAnchor135), *Audio Data Augmentation*'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B17990_07.xhtml#_idTextAnchor135)，*音频数据增强*'
- en: '[*Chapter 8*](B17990_08.xhtml#_idTextAnchor167), *Audio Data Augmentation with
    Spectogram*'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B17990_08.xhtml#_idTextAnchor167)，*使用谱图进行音频数据增强*'
