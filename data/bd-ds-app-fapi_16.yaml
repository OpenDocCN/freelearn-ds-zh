- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Implementing a Real-Time Object Detection System Using WebSockets with FastAPI
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 WebSockets 和 FastAPI 实现实时物体检测系统
- en: In the previous chapter, you learned how to create efficient REST API endpoints
    to make predictions with trained machine learning models. This approach covers
    a lot of use cases, given that we have a single observation we want to work on.
    In some cases, however, we may need to continuously perform predictions on a stream
    of input – for instance, an object detection system that works in real time with
    video input. This is exactly what we’ll build in this chapter. How? If you remember,
    besides HTTP endpoints, FastAPI also has the ability to handle WebSockets endpoints,
    which allow us to send and receive streams of data. In this case, the browser
    will send into the WebSocket a stream of images from the webcam, and our application
    will run an object detection algorithm and send back the coordinates and label
    of each detected object in the image. For this task, we’ll rely on **Hugging Face**,
    which is both a set of tools and a library of pretrained AI models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何创建高效的 REST API 端点，用于通过训练好的机器学习模型进行预测。这种方法涵盖了很多使用场景，假设我们有一个单一的观测值需要处理。然而，在某些情况下，我们可能需要持续对一系列输入进行预测——例如，一个实时处理视频输入的物体检测系统。这正是我们将在本章中构建的内容。怎么做？如果你还记得，除了
    HTTP 端点，FastAPI 还具备处理 WebSockets 端点的能力，这让我们能够发送和接收数据流。在这种情况下，浏览器会通过 WebSocket
    发送来自摄像头的图像流，我们的应用程序会运行物体检测算法，并返回图像中每个检测到的物体的坐标和标签。为此任务，我们将依赖于**Hugging Face**，它既是一组工具，也是一个预训练
    AI 模型的库。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主要内容：
- en: Using a computer vision model with Hugging Face libraries
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Hugging Face 库的计算机视觉模型
- en: Implementing an HTTP endpoint to perform object detection on a single image
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个 HTTP 端点，执行单张图像的物体检测
- en: Sending a stream of images from the browser in a WebSocket
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 WebSocket 中从浏览器发送图像流
- en: Showing the object detection results in a browser
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在浏览器中显示物体检测结果
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, you’ll require a Python virtual environment, just as we set
    up in [*Chapter 1*](B19528_01.xhtml#_idTextAnchor024), *Python Development* *Environment
    Setup*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章你将需要一个 Python 虚拟环境，就像我们在[*第 1 章*](B19528_01.xhtml#_idTextAnchor024)中所设置的那样，*Python
    开发* *环境设置*。
- en: You’ll find all the code examples for this chapter in the dedicated GitHub repository
    at [https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个专门的 GitHub 仓库中找到本章的所有代码示例：[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13)。
- en: Using a computer vision model with Hugging Face
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Hugging Face 库的计算机视觉模型
- en: 'Computer vision is a field of study and technology that focuses on enabling
    computers to extract meaningful information from digital images or videos, simulating
    human vision capabilities. It involves developing algorithms based on statistical
    methods or machine learning that allow machines to understand, analyze, and interpret
    visual data. A typical example of computer vision’s application is object detection:
    a system able to detect and recognize objects in an image. This is the kind of
    system we’ll build in this chapter.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是一个研究和技术领域，致力于使计算机能够从数字图像或视频中提取有意义的信息，从而模拟人类视觉能力。它涉及基于统计方法或机器学习开发算法，使机器能够理解、分析和解释视觉数据。计算机视觉的一个典型应用是物体检测：一个能够在图像中检测和识别物体的系统。这正是我们将在本章中构建的系统。
- en: 'To help us in this task, we’ll use a set of tools provided by Hugging Face.
    Hugging Face is a company whose goal is to allow developers to use the most recent
    and powerful AI models quickly and easily. For this, it has built two things:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们完成这个任务，我们将使用 Hugging Face 提供的一组工具。Hugging Face 是一家旨在让开发者能够快速、轻松地使用最新、最强大的
    AI 模型的公司。为此，它构建了两个工具：
- en: A set of open source Python tools built on top of machine learning libraries
    such as PyTorch and TensorFlow. We’ll use some of them in this chapter.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一套基于机器学习库（如 PyTorch 和 TensorFlow）构建的开源 Python 工具集。我们将在本章中使用其中的一些工具。
- en: An online library to share and download pretrained models for various machine
    learning tasks, such as computer vision or image generation.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个在线库，用于分享和下载各种机器学习任务的预训练模型，例如计算机视觉或图像生成。
- en: 'You can read more about what it''s doing on its official website: [https://huggingface.co/](https://huggingface.co/).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在其官方网站上了解更多它的功能：[https://huggingface.co/](https://huggingface.co/)。
- en: 'You’ll see that it’ll greatly help us build a powerful and accurate object
    detection system in no time! To begin with, we’ll install all the libraries we
    need for this project:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到，这将极大地帮助我们在短时间内构建一个强大且精确的目标检测系统！首先，我们将安装项目所需的所有库：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `transformers` library from Hugging Face will allow us to download and run
    pretrained machine learning models. Notice that we install it with the optional
    `torch` dependency. Hugging Face tools can be used either with PyTorch or TensorFlow,
    which are both very powerful ML frameworks. Here, we chose to use PyTorch. Pillow
    is a widely used Python library for working with images. We’ll see why we need
    it soon.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 的 `transformers` 库将允许我们下载并运行预训练的机器学习模型。请注意，我们通过可选的 `torch` 依赖项来安装它。Hugging
    Face 工具可以与 PyTorch 或 TensorFlow 一起使用，这两者都是非常强大的机器学习框架。在这里，我们选择使用 PyTorch。Pillow
    是一个广泛使用的 Python 库，用于处理图像。稍后我们会看到为什么需要它。
- en: 'Before starting to work with FastAPI, let’s implement a simple script to run
    an object detection algorithm. It consists of four main steps:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用 FastAPI 之前，让我们先实现一个简单的脚本来运行一个目标检测算法。它包括四个主要步骤：
- en: Load an image from the disk using Pillow.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Pillow 从磁盘加载图像。
- en: Load a pretrained object detection model.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一个预训练的目标检测模型。
- en: Run the model on our image.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的图像上运行模型。
- en: Display the results by drawing rectangles around the detected objects.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在检测到的物体周围绘制矩形来显示结果。
- en: 'We’ll go step by step through the implementation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一步一步地进行实现：
- en: chapter13_object_detection.py
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: chapter13_object_detection.py
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py)'
- en: 'As you can see, the first step is to load our image from the disk. For this
    example, we use the image named `coffee-shop.jpg`, which is available in our examples
    repository at [https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/blob/main/assets/coffee-shop.jpg](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/blob/main/assets/coffee-shop.jpg):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，第一步是从磁盘加载我们的图像。在这个示例中，我们使用名为 `coffee-shop.jpg` 的图像，该图像可以在我们的示例仓库中找到，地址是
    [https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/blob/main/assets/coffee-shop.jpg](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/blob/main/assets/coffee-shop.jpg)：
- en: chapter13_object_detection.py
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: chapter13_object_detection.py
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py)'
- en: 'Next, we load a model from Hugging Face. For this example, we chose the YOLOS
    model. It’s a cutting-edge approach to object detection that has been trained
    on 118K annotated images. You can read more about the technical approach in the
    following Hugging Face article: [https://huggingface.co/docs/transformers/model_doc/yolos](https://huggingface.co/docs/transformers/model_doc/yolos).
    To limit the download size and preserve your computer disk space, we chose here
    to use the tiny version, which is a lighter version of the original model that
    can be run on an average machine while maintaining good accuracy. This particular
    version is described here on Hugging Face: [https://huggingface.co/hustvl/yolos-tiny](https://huggingface.co/hustvl/yolos-tiny).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从 Hugging Face 加载一个模型。在这个示例中，我们选择了 YOLOS 模型。它是一种先进的目标检测方法，已在 118K 个带注释的图像上进行训练。你可以在以下
    Hugging Face 文章中了解更多关于该技术的方法：[https://huggingface.co/docs/transformers/model_doc/yolos](https://huggingface.co/docs/transformers/model_doc/yolos)。为了限制下载大小并节省计算机磁盘空间，我们选择使用精简版，这是原始模型的一个更轻量化版本，可以在普通机器上运行，同时保持良好的精度。这个版本在
    Hugging Face 上有详细描述：[https://huggingface.co/hustvl/yolos-tiny](https://huggingface.co/hustvl/yolos-tiny)。
- en: 'Notice that we instantiate two things: an **image processor** and a **model**.
    If you remember what we said in [*Chapter 11*](B19528_11.xhtml#_idTextAnchor797),
    *Introduction to Data Science in Python*, you know that we need to have a set
    of features that will feed our ML algorithm. Hence, the role of the image processor
    is to transform a raw image into a set of characteristics that are meaningful
    to the model.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们实例化了两个东西：**图像处理器**和**模型**。如果你还记得我们在[*第11章*](B19528_11.xhtml#_idTextAnchor797)《Python中的数据科学入门》中提到的内容，你会知道我们需要一组特征来供我们的机器学习算法使用。因此，图像处理器的作用是将原始图像转换为对模型有意义的一组特征。
- en: 'And that’s exactly what we’re doing in the following lines: we create an `inputs`
    variable by calling `image_processor` on our image. Notice that the `return_tensors`
    argument is set to `pt` for PyTorch since we chose to go with PyTorch as our underlying
    ML framework. Then, we can feed this `inputs` variable to our model to get `outputs`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在接下来的代码行中所做的：我们通过调用`image_processor`处理图像，创建一个`inputs`变量。请注意，`return_tensors`参数被设置为`pt`，因为我们选择了PyTorch作为我们的底层机器学习框架。然后，我们可以将这个`inputs`变量输入到模型中以获得`outputs`：
- en: chapter13_object_detection.py
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: chapter13_object_detection.py
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py)'
- en: 'You might think that this is it for the prediction phase and that we could
    now display the results. However, that’s not the case. The result of such algorithms
    is a set of multi-dimensional matrices, the famous `post_process_object_detection`
    operation provided by `image_processor`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为这就是预测阶段的全部内容，我们现在可以展示结果了。然而，事实并非如此。此类算法的结果是一组多维矩阵，著名的`post_process_object_detection`操作由`image_processor`提供：
- en: chapter13_object_detection.py
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: chapter13_object_detection.py
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py)'
- en: 'The result of this operation is a dictionary with the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作的结果是一个字典，包含以下内容：
- en: '`labels`: The list of labels of each detected object'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`：每个检测到的物体的标签列表'
- en: '`boxes`: The coordinates of the bounding box of each detected object'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boxes`：每个检测到的物体的边界框坐标'
- en: '`scores`: The confidence score of the algorithm for each detected object'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores`：算法对于每个检测到的物体的置信度分数'
- en: 'All we need to do then is to iterate over them so we can draw the rectangle
    and the corresponding label thanks to Pillow. We just show the resulting image
    at the end. Notice that we only consider objects with a score greater than `0.7`
    to limit the number of false positives:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的就是遍历这些对象，以便利用Pillow绘制矩形和相应的标签。最后，我们只展示处理后的图像。请注意，我们只考虑那些得分大于`0.7`的物体，以减少假阳性的数量：
- en: chapter13_object_detection.py
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: chapter13_object_detection.py
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py)'
- en: 'Thanks to Pillow, we’re able to draw rectangles and add a label above the detected
    objects. Notice that we loaded a custom font, Open Sans, which is an open font
    available on the web: [https://fonts.google.com/specimen/Open+Sans](https://fonts.google.com/specimen/Open+Sans).
    Let’s try to run this script and see the result:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了Pillow，我们能够绘制矩形并在检测到的物体上方添加标签。请注意，我们加载了一个自定义字体——Open Sans，它是一个可以在网上获得的开源字体：[https://fonts.google.com/specimen/Open+Sans](https://fonts.google.com/specimen/Open+Sans)。让我们尝试运行这个脚本，看看结果：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first time it’ll run, you’ll see the model being downloaded. The prediction
    can then take a few seconds to run depending on your computer. When it’s done,
    the resulting image should automatically open, as shown in *Figure 13**.1*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首次运行时，你会看到模型被下载。根据你的计算机性能，预测过程可能需要几秒钟。完成后，生成的图像应该会自动打开，如*图 13.1*所示。
- en: '![Figure 13.1 – Object detection result on a sample image](img/Figure_13.1_B19528.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.1 – 在示例图像上的目标检测结果](img/Figure_13.1_B19528.jpg)'
- en: Figure 13.1 – Object detection result on a sample image
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 – 在示例图像上的目标检测结果
- en: You can see that the model detected several persons in the image, along with
    various objects such as the couch and a chair. And that’s it! Less than 30 lines
    of code to have a working object detection script! Hugging Face lets us harness
    all the power of the latest AI advances very efficiently.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，模型检测到图像中的几个人物，以及各种物体，如沙发和椅子。就这样！不到 30 行代码就能实现一个可运行的目标检测脚本！Hugging Face
    使我们能够高效地利用最新 AI 技术的强大功能。
- en: Of course, our goal in this chapter is to put all this intelligence on a remote
    server so that we can serve this experience to thousands of users. Once again,
    FastAPI will be our ally here.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们在这一章的目标是将所有这些智能功能部署到远程服务器上，以便能够为成千上万的用户提供这一体验。再次强调，FastAPI 将是我们在这里的得力助手。
- en: Implementing a REST endpoint to perform object detection on a single image
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个 REST 端点，用于在单张图像上执行目标检测
- en: 'Before working with WebSockets, we’ll start simple and implement, using FastAPI,
    a classic HTTP endpoint to accept image uploads and perform object detection on
    them. As you’ll see, the main difference from the previous example is in how we
    acquire the image: instead of reading it from the disk, we get it from a file
    upload that we have to convert into a Pillow image object.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 WebSockets 之前，我们先从简单开始，利用 FastAPI 实现一个经典的 HTTP 端点，接受图像上传并对其进行目标检测。正如你所看到的，与之前的示例的主要区别在于我们如何获取图像：不是从磁盘读取，而是通过文件上传获取，之后需要将其转换为
    Pillow 图像对象。
- en: Besides, we’ll also use the exact same pattern we saw in [*Chapter 12*](B19528_12.xhtml#_idTextAnchor960),
    *Creating an Efficient Prediction API Endpoint with FastAPI* – that is, having
    a dedicated class for our prediction model, which will be loaded during the lifespan
    handler.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将使用我们在[*第 12 章*](B19528_12.xhtml#_idTextAnchor960)中看到的完全相同的模式，*使用 FastAPI
    创建高效的预测 API 端点*——也就是为我们的预测模型创建一个专门的类，该类将在生命周期处理程序中加载。
- en: 'The first thing we do in this implementation is to define Pydantic models in
    order to properly structure the output of our prediction model. You can see this
    as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在此实现中，我们做的第一件事是定义 Pydantic 模型，以便正确地结构化我们预测模型的输出。你可以看到如下所示：
- en: chapter13_api.py
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: chapter13_api.py
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_api.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_api.py)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_api.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_api.py)'
- en: We have a model for a single detected object, which consists of `box`, a tuple
    of four numbers describing the coordinates of the bounding box, and `label`, which
    corresponds to the type of detected object. The `Objects` model is a simple structure
    bearing a list of objects.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个表示单个检测到的目标的模型，它包括`box`，一个包含四个数字的元组，描述边界框的坐标，以及`label`，表示检测到的物体类型。`Objects`模型是一个简单的结构，包含物体列表。
- en: 'We won’t go through the model prediction class, as it’s very similar to what
    we saw in the previous chapter and section. Instead, let’s directly focus on the
    FastAPI endpoint implementation:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细介绍模型预测类，因为它与我们在上一章和上一节中看到的非常相似。相反，我们直接关注 FastAPI 端点的实现：
- en: chapter13_api.py
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: chapter13_api.py
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_api.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_api.py)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_api.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_api.py)'
- en: Nothing very surprising here! The main point of attention is to correctly use
    the `UploadFile` and `File` dependencies so we get the uploaded file. If you need
    a refresher on this, you can check the *Form data and file uploads* section from
    [*Chapter 3*](B19528_03.xhtml#_idTextAnchor058), *Developing a RESTful API with
    FastAPI*. All we need to do then is to instantiate it as a proper Pillow image
    object and call our prediction model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有什么特别令人惊讶的！需要关注的重点是正确使用`UploadFile`和`File`依赖项，以便获取上传的文件。如果你需要复习这一部分内容，可以查看[*第三章*](B19528_03.xhtml#_idTextAnchor058)中关于*表单数据和文件上传*的章节，*快速开发
    RESTful API 使用 FastAPI*。然后，我们只需将其实例化为合适的 Pillow 图像对象，并调用我们的预测模型。
- en: As we said, we don’t forget to load the model inside the lifespan handler.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所说，别忘了在生命周期处理程序中加载模型。
- en: 'You can run this example using the usual Uvicorn command:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用常规的 Uvicorn 命令来运行这个示例：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We’ll use the same coffee shop picture we already saw in the previous section.
    Let’s upload it on our endpoint with HTTPie:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上一节中看到的相同的咖啡店图片。让我们用 HTTPie 上传它到我们的端点：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We correctly get the list of detected objects, each one with its bounding box
    and label. Great! Our object detection system is now available as a web server.
    However, our goal is still to make a real-time system: thanks to WebSockets, we’ll
    be able to handle a stream of images.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正确地得到了检测到的对象列表，每个对象都有它的边界框和标签。太棒了！我们的目标检测系统现在已经作为一个 Web 服务器可用。然而，我们的目标仍然是创建一个实时系统：借助
    WebSockets，我们将能够处理图像流。
- en: Implementing a WebSocket to perform object detection on a stream of images
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 WebSocket 以对图像流进行目标检测
- en: One of the main benefits of WebSockets, as we saw in [*Chapter 8*](B19528_08.xhtml#_idTextAnchor551),
    *Defining WebSockets for Two-Way Interactive Communication in FastAPI*, is that
    it opens a full-duplex communication channel between the client and the server.
    Once the connection is established, messages can be passed quickly without having
    to go through all the steps of the HTTP protocol. Therefore, it’s much more suited
    to sending a lot of data in real time.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: WebSockets 的主要优势之一，正如我们在[*第八章*](B19528_08.xhtml#_idTextAnchor551)中看到的，*在 FastAPI
    中定义用于双向交互通信的 WebSockets*，是它在客户端和服务器之间打开了一个全双工通信通道。一旦连接建立，消息可以快速传递，而不需要经过 HTTP
    协议的所有步骤。因此，它更适合实时传输大量数据。
- en: The point here will be to implement a WebSocket endpoint that is able to both
    accept image data and run object detection on it. The main challenge here will
    be to handle a phenomenon known as **backpressure**. Put simply, we’ll receive
    more images from the browser than the server is able to handle because of the
    time needed to run the detection algorithm. Thus, we’ll have to work with a queue
    (or buffer) of limited size and drop some images along the way to handle the stream
    in near real time.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键是实现一个 WebSocket 端点，能够接收图像数据并进行目标检测。这里的主要挑战是处理一个被称为**背压**的现象。简单来说，我们将从浏览器接收到的图像比服务器能够处理的要多，因为运行检测算法需要一定的时间。因此，我们必须使用一个有限大小的队列（或缓冲区），并在处理流时丢弃一些图像，以便接近实时地处理。
- en: 'We’ll go step by step through the implementation:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步讲解实现过程：
- en: app.py
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: app.py
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/app.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/app.py)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/app.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/app.py)'
- en: 'We defined two tasks: `receive` and `detect`. The first one is waiting for
    raw bytes from the WebSocket, while the second one is performing the detection
    and sending the result, exactly as we saw in the last section.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了两个任务：`receive` 和 `detect`。第一个任务是等待从 WebSocket 接收原始字节，而第二个任务则执行检测并发送结果，正如我们在上一节中看到的那样。
- en: 'The key here is to use the `asyncio.Queue` object. This is a convenient structure
    allowing us to queue some data in memory and retrieve it in a **first in, first
    out** (**FIFO**) strategy. We are able to set a limit on the number of elements
    we store in the queue: this is how we’ll be able to limit the number of images
    we handle.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键是使用`asyncio.Queue`对象。这是一个便捷的结构，允许我们在内存中排队一些数据，并以**先进先出**（**FIFO**）策略来检索它。我们可以设置存储在队列中的元素数量限制：这就是我们限制处理图像数量的方式。
- en: 'The `receive` function receives data and puts it at the end of the queue. When
    working with `asyncio.Queue`, we have two methods to put a new element in the
    queue: `put` and `put_nowait`. If the queue is full, the first one will wait until
    there is room in the queue. This is not what we want here: we want to drop images
    that we won’t be able to handle in time. With `put_nowait`, the `QueueFull` exception
    is raised if the queue is full. In this case, we just pass and drop the data.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`receive` 函数接收数据并将其放入队列末尾。在使用 `asyncio.Queue` 时，我们有两个方法可以将新元素放入队列：`put` 和 `put_nowait`。如果队列已满，第一个方法会等待直到队列有空间。这不是我们在这里想要的：我们希望丢弃那些无法及时处理的图像。使用
    `put_nowait` 时，如果队列已满，会抛出 `QueueFull` 异常。在这种情况下，我们只需跳过并丢弃数据。'
- en: On the other hand, the `detect` function pulls the first message from the queue
    and runs its detection before sending the result. Notice that since we get raw
    image bytes directly, we have to wrap them with `io.BytesIO` to make it acceptable
    for Pillow.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`detect` 函数从队列中提取第一个消息，并在发送结果之前运行检测。请注意，由于我们直接获取的是原始图像字节，我们需要用 `io.BytesIO`
    将它们包装起来，才能让 Pillow 处理。
- en: 'The WebSocket implementation in itself is similar to what we saw in [*Chapter
    8*](B19528_08.xhtml#_idTextAnchor551), *Defining WebSockets for Two-Way Interactive
    Communication in FastAPI*. We are scheduling both tasks and waiting until one
    of them has stopped. Since they both run an infinite loop, this will happen when
    the WebSocket is disconnected:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: WebSocket 的实现本身类似于我们在[*第 8 章*](B19528_08.xhtml#_idTextAnchor551)中看到的内容，*在 FastAPI
    中定义 WebSocket 进行双向交互通信*。我们正在调度这两个任务并等待其中一个任务停止。由于它们都运行一个无限循环，因此当 WebSocket 断开连接时，这个情况会发生：
- en: app.py
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: app.py
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/app.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/app.py)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/app.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/app.py)'
- en: Serving static files
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 提供静态文件
- en: 'If you look at the full implementation of the preceding example, you’ll notice
    that we defined two more things in our server: an `index` endpoint, which just
    returns the `index.html` file, and a `StaticFiles` app, which is mounted under
    the `/assets` path. Both of them are here to allow our FastAPI application to
    directly serve our HTML and JavaScript code. This way, browsers will be able to
    query those files on the same server.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看前面示例的完整实现，你会注意到我们在服务器中定义了另外两个东西：一个 `index` 端点，它仅返回 `index.html` 文件，以及一个
    `StaticFiles` 应用，它被挂载在 `/assets` 路径下。这两个功能的存在是为了让我们的 FastAPI 应用直接提供 HTML 和 JavaScript
    代码。这样，浏览器就能够在同一个服务器上查询这些文件。
- en: The key takeaway of this is that even though FastAPI was designed to build REST
    APIs, it’s also perfectly able to serve HTML and static files.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分的关键点是，尽管 FastAPI 是为构建 REST API 设计的，但它同样可以完美地提供 HTML 和静态文件。
- en: Our backend is now ready! Let’s now see how to use its power from a browser.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的后端现在已经准备好！接下来，让我们看看如何在浏览器中使用它的功能。
- en: "Sending a stream of images from the browser in \La WebSocket"
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 WebSocket 从浏览器发送图像流
- en: In this section, we’ll see how you can capture images from the webcam in the
    browser and send them through a WebSocket. Since it mainly involves JavaScript
    code, it’s admittedly a bit beyond the scope of this book, but it’s necessary
    to make the application work fully.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何在浏览器中捕捉来自摄像头的图像并通过 WebSocket 发送。由于这主要涉及 JavaScript 代码，坦率来说，它有点超出了本书的范围，但它对于让应用程序正常工作是必要的。
- en: 'The first step is to enable a camera input in the browser, open the WebSocket
    connection, pick a camera image, and send it through the WebSocket. Basically,
    it’ll work like this: thanks to the `MediaDevices` browser API, we’ll be able
    to list all the camera inputs available on the device. With this, we’ll build
    a selection form so the user can select the camera they want to use. You can see
    the concrete JavaScript implementation in the following code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是在浏览器中启用摄像头输入，打开 WebSocket 连接，捕捉摄像头图像并通过 WebSocket 发送。基本上，它会像这样工作：通过 `MediaDevices`
    浏览器 API，我们将能够列出设备上所有可用的摄像头输入。借此，我们将构建一个选择表单，供用户选择他们想要使用的摄像头。你可以在以下代码中看到具体的 JavaScript
    实现：
- en: script.js
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: script.js
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js)'
- en: 'Once the user submits the form, we call a `startObjectDetection` function with
    the selected camera. Most of the actual detection logic is implemented in this
    function:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦用户提交表单，我们会调用一个`startObjectDetection`函数，并传入选定的摄像头。大部分实际的检测逻辑是在这个函数中实现的：
- en: script.js
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: script.js
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js)'
- en: 'Let’s have a look at the `startObjectDetection` function in the following code
    block. First, we establish a connection with the WebSocket. Once it’s opened,
    we can start to get an image stream from the selected camera. For this, we use
    the `MediaDevices` API to start capturing video and display the output in an HTML
    `<video>` element. You can read all the details about the `MediaDevices` API in
    the MDN documentation: [https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下面代码块中的`startObjectDetection`函数。首先，我们与WebSocket建立连接。连接打开后，我们可以开始从选定的摄像头获取图像流。为此，我们使用`MediaDevices`
    API来启动视频捕获，并将输出显示在一个HTML的`<video>`元素中。你可以在MDN文档中阅读有关`MediaDevices` API的所有细节：[https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices)：
- en: script.js
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: script.js
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js)'
- en: 'Then, as shown in the next code block, we launch a repetitive task that captures
    an image from the video input and sends it to the server. To do this, we have
    to use a `<canvas>` element, an HTML tag dedicated to graphics drawing. It comes
    with a complete JavaScript API so that we can programmatically draw images in
    it. There, we’ll be able to draw the current video image and convert it into valid
    JPEG bytes. If you want to know more about this, MDN gives a very detailed tutorial
    on `<``canvas>`: [https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial](https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，正如下一个代码块所示，我们启动一个重复的任务，捕获来自视频输入的图像并将其发送到服务器。为了实现这一点，我们必须使用一个`<canvas>`元素，这是一个专门用于图形绘制的HTML标签。它提供了完整的JavaScript
    API，允许我们以编程方式在其中绘制图像。在这里，我们可以绘制当前的视频图像，并将其转换为有效的JPEG字节。如果你想了解更多关于这个内容，MDN提供了一个非常详细的`<canvas>`教程：[https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial](https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial)：
- en: script.js
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: script.js
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js)'
- en: Notice that we limit the size of the video input to 640 by 480 pixels, so that
    we don’t blow up the server with images that are too big. Besides, we set the
    interval to run every 42 milliseconds (the value is set in the `IMAGE_INTERVAL_MS`
    constant), which is roughly equivalent to 24 images per second.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将视频输入的大小限制为640x480像素，以防止上传过大的图像使服务器崩溃。此外，我们将间隔设置为每42毫秒执行一次（该值在`IMAGE_INTERVAL_MS`常量中设置），大约相当于每秒24帧图像。
- en: 'Finally, we wire the event listener to handle the messages received from the
    WebSocket. It calls the `drawObjects` function, which we’ll detail in the next
    section:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将事件监听器连接起来，以处理从 WebSocket 接收到的消息。它调用了`drawObjects`函数，我们将在下一节中详细介绍：
- en: script.js
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: script.js
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js)'
- en: Showing the object detection results in the browser
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在浏览器中展示物体检测结果
- en: Now that we are able to send input images to the server, we have to show the
    result of the detection in the browser. In a similar way to what we showed in
    the *Using a computer vision model with Hugging Face* section, we’ll draw a green
    rectangle around the detected objects, along with their label. Thus, we have to
    find a way to take the rectangle coordinates sent by the server and draw them
    in the browser.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够将输入图像发送到服务器，我们需要在浏览器中展示检测结果。与我们在*使用 Hugging Face 计算机视觉模型*部分中展示的类似，我们将围绕检测到的对象绘制一个绿色矩形，并标注它们的标签。因此，我们需要找到一种方式，将服务器发送的矩形坐标在浏览器中绘制出来。
- en: 'To do this, we’ll once again use a `<canvas>` element. This time, it’ll be
    visible to the user and we’ll draw the rectangles using it. The trick here is
    to use CSS so that this element overlays the video: this way, the rectangles will
    be shown right on top of the video and the corresponding objects. You can see
    the HTML code here:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将再次使用`<canvas>`元素。这次，它将对用户可见，我们将使用它来绘制矩形。关键是使用 CSS 使这个元素覆盖视频：这样，矩形就会直接显示在视频和对应对象的上方。你可以在这里看到
    HTML 代码：
- en: index.html
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: index.html
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/index.html](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/index.html)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/index.html](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/index.html)'
- en: We are using CSS classes from Bootstrap, a very common CSS library with a lot
    of helpers like this. Basically, we set the canvas with absolute positioning and
    put it at the top left so that it covers the video element.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了来自 Bootstrap 的 CSS 类，Bootstrap 是一个非常常见的 CSS 库，提供了很多类似这样的辅助工具。基本上，我们通过绝对定位设置了
    canvas，并将其放置在左上角，这样它就能覆盖视频元素。
- en: 'The key now is to use the Canvas API to draw the rectangles according to the
    received coordinates. This is the purpose of the `drawObjects` function, which
    is shown in the next sample code block:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于使用 Canvas API 根据接收到的坐标绘制矩形。这正是`drawObjects`函数的目的，下面的示例代码块展示了这一点：
- en: script.js
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: script.js
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js)'
- en: 'With the `<canvas>` element, we can use a 2D context to draw things in the
    object. Notice that we first clean everything to remove the rectangles from the
    previous detection. Then, we loop through all the detected objects and draw a
    rectangle with the given coordinates: `x1`, `y1`, `x2`, and `y2`. Finally, we
    take care of drawing the label slightly above the rectangle.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`<canvas>`元素，我们可以使用2D上下文在对象中绘制内容。请注意，我们首先清除所有内容，以移除上次检测的矩形。然后，我们遍历所有检测到的对象，并使用给定的坐标`x1`、`y1`、`x2`和`y2`绘制一个矩形。最后，我们会在矩形上方稍微绘制标签。
- en: Our system is now complete! *Figure 13**.2* gives you an overview of the file
    structure we’ve implemented.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统现在完成了！*图13.2* 给出了我们实现的文件结构概览。
- en: '![Figure 13.2 – Object detection application structure](img/Figure_13.2_B19528.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图13.2 – 对象检测应用结构](img/Figure_13.2_B19528.jpg)'
- en: Figure 13.2 – Object detection application structure
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 – 对象检测应用结构
- en: 'It’s time to give it a try! We can start it using the usual Uvicorn command:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候尝试一下了！我们可以使用常见的Uvicorn命令启动它：
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You can access the application in your browser with the address `http://localhost:8000`.
    As we said in the previous section, the `index` endpoint will be called and will
    return our `index.html` file.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过地址`http://localhost:8000`在浏览器中访问应用程序。正如我们在前一部分所说，`index`端点将被调用并返回我们的`index.html`文件。
- en: 'You’ll see an interface inviting you to choose the camera you want to use,
    as shown in *Figure 13**.3*:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到一个界面，邀请您选择要使用的摄像头，如*图13.3*所示：
- en: '![Figure 13.3 – Webcam selection for the object detection web application](img/Figure_13.3_B19528.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图13.3 – 用于对象检测网页应用的摄像头选择](img/Figure_13.3_B19528.jpg)'
- en: Figure 13.3 – Webcam selection for the object detection web application
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 – 用于对象检测网页应用的摄像头选择
- en: 'Select the webcam you wish to use and click on **Start**. The video output
    will show up, object detection will start via the WebSocket, and green rectangles
    will be drawn around the detected objects. We show this in *Figure 13**.4*:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 选择您想要使用的摄像头并点击**开始**。视频输出将显示出来，通过WebSocket开始对象检测，并且绿色矩形将会围绕检测到的对象绘制。我们在*图13.4*中展示了这一过程：
- en: '![Figure 13.4 – Running the object detection web application](img/Figure_13.4_B19528.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图13.4 – 运行对象检测网页应用](img/Figure_13.4_B19528.jpg)'
- en: Figure 13.4 – Running the object detection web application
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4 – 运行对象检测网页应用
- en: It works! We brought the intelligence of our Python system right to the user’s
    web browser. This is just an example of what you could achieve using WebSockets
    and ML algorithms, but this definitely enables you to create near real-time experiences
    for your users.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 它成功了！我们将我们Python系统的智能带到了用户的网页浏览器中。这只是使用WebSockets和机器学习算法可以实现的一种示例，但它绝对可以让您为用户创建接近实时的体验。
- en: Summary
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we showed how WebSockets can help us bring a more interactive
    experience to users. Thanks to the pretrained models provided by the Hugging Face
    community, we were able to quickly implement an object detection system. Then,
    we integrated it into a WebSocket endpoint with the help of FastAPI. Finally,
    by using a modern JavaScript API, we sent video input and displayed algorithm
    results directly in the browser. All in all, a project like this might sound complex
    to make at first, but we saw that powerful tools such as FastAPI enable us to
    get results in a very short time and with very comprehensible source code.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了WebSockets如何帮助我们为用户带来更具互动性的体验。得益于Hugging Face社区提供的预训练模型，我们能够迅速实现一个对象检测系统。接着，在FastAPI的帮助下，我们将其集成到一个WebSocket端点。最后，通过使用现代JavaScript
    API，我们直接在浏览器中发送视频输入并显示算法结果。总的来说，像这样的项目乍一看可能显得很复杂，但我们看到强大的工具，如FastAPI，能够让我们在非常短的时间内并且通过易于理解的源代码实现结果。
- en: 'Until now, in our different examples and projects, we assumed the ML model
    we used was fast enough to be run directly in an API endpoint or a WebSocket task.
    However, that’s not always the case. In some cases, the algorithm is so complex
    it takes a couple of minutes to run. If we run this kind of algorithm directly
    inside an API endpoint, the user would have to wait a long time before getting
    a response. Not only would this be strange for them but this would also quickly
    block the whole server, preventing other users from using the API. To solve this,
    we’ll need a companion for our API server: a worker.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在我们的不同示例和项目中，我们假设我们使用的机器学习模型足够快，可以直接在 API 端点或 WebSocket 任务中运行。然而，情况并非总是如此。在某些情况下，算法如此复杂，以至于运行需要几分钟。如果我们直接在
    API 端点内部运行这种算法，用户将不得不等待很长时间才能得到响应。这不仅会让用户感到困惑，还会迅速堵塞整个服务器，阻止其他用户使用 API。为了解决这个问题，我们需要为
    API 服务器配备一个助手：一个工作者。
- en: 'In the next chapter, we’ll study a concrete example of this challenge: we’ll
    build our very own AI system to generate images from a text prompt!'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究这个挑战的一个具体例子：我们将构建我们自己的 AI 系统，从文本提示生成图像！
