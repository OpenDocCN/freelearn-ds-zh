- en: Special RDD Operations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特殊的RDD操作
- en: '"It''s supposed to be automatic, but actually you have to push this button."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “它应该是自动的，但实际上你必须按下这个按钮。”
- en: '- John Brunner'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 约翰·布鲁纳'
- en: In this chapter, you learn how RDDs can be tailored to different needs, and
    how these RDDs provide new functionalities (and dangers!) Moreover, we investigate
    other useful objects that Spark provides, such as broadcast variables and accumulators.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解如何根据不同的需求定制RDD，以及这些RDD如何提供新的功能（和危险！）此外，我们还将研究Spark提供的其他有用对象，如广播变量和累加器。
- en: 'In a nutshell, the following topics will be covered throughout this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章将涵盖以下主题：
- en: Types of RDDs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD的类型
- en: Aggregations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合
- en: Partitioning and shuffling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区和洗牌
- en: Broadcast variables
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广播变量
- en: Accumulators
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累加器
- en: Types of RDDs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD的类型
- en: '**Resilient Distributed Datasets** (**RDDs**) are the fundamental object used
    in Apache Spark. RDDs are immutable collections representing datasets and have
    the inbuilt capability of reliability and failure recovery. By nature, RDDs create
    new RDDs upon any operation such as transformation or action. They also store
    the lineage, which is used to recover from failures. We have also seen in the
    previous chapter some details about how RDDs can be created and what kind of operations
    can be applied to RDDs.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集**（**RDD**）是Apache Spark中使用的基本对象。RDD是不可变的集合，代表数据集，并具有内置的可靠性和故障恢复能力。根据性质，RDD在任何操作（如转换或动作）时创建新的RDD。它们还存储血统，用于从故障中恢复。在上一章中，我们还看到了有关如何创建RDD以及可以应用于RDD的操作的一些详细信息。'
- en: 'The following is a simply example of the RDD lineage:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是RDD血统的简单示例：
- en: '![](img/00056.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00056.jpeg)'
- en: 'Let''s start looking at the simplest RDD again by creating a RDD from a sequence
    of numbers:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次从一系列数字创建最简单的RDD开始查看：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding example shows RDD of integers and any operation done on the RDD
    results in another RDD. For example, if we multiply each element by `3`, the result
    is shown in the following snippet:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例显示了整数RDD，对RDD进行的任何操作都会产生另一个RDD。例如，如果我们将每个元素乘以`3`，结果将显示在以下片段中：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s do one more operation, adding `2` to each element and also print all
    three RDDs:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再做一个操作，将每个元素加`2`，并打印所有三个RDD：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'An interesting thing to look at is the lineage of each RDD using the `toDebugString`
    function:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的事情是使用`toDebugString`函数查看每个RDD的血统：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following is the lineage shown in the Spark web UI:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在Spark web UI中显示的血统：
- en: '![](img/00064.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00064.jpeg)'
- en: RDD does not need to be the same datatype as the first RDD (integer). The following
    is a RDD which writes a different datatype of a tuple of (string, integer).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RDD不需要与第一个RDD（整数）相同的数据类型。以下是一个RDD，它写入了一个不同数据类型的元组（字符串，整数）。
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The following is a RDD of the `StatePopulation` file where each record is converted
    to `upperCase`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`StatePopulation`文件的RDD，其中每个记录都转换为`upperCase`。
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following is a diagram of the preceding transformation:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述转换的图表：
- en: '![](img/00156.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00156.jpeg)'
- en: Pair RDD
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pair RDD
- en: Pair RDDs are RDDs consisting of key-value tuples which suits many use cases
    such as aggregation, sorting, and joining data. The keys and values can be simple
    types such as integers and strings or more complex types such as case classes,
    arrays, lists, and other types of collections. The key-value based extensible
    data model offers many advantages and is the fundamental concept behind the MapReduce
    paradigm.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Pair RDD是由键值元组组成的RDD，适用于许多用例，如聚合、排序和连接数据。键和值可以是简单类型，如整数和字符串，也可以是更复杂的类型，如案例类、数组、列表和其他类型的集合。基于键值的可扩展数据模型提供了许多优势，并且是MapReduce范式背后的基本概念。
- en: Creating a `PairRDD` can be done easily by applying transformation to any RDD
    to convert the RDD to an RDD of key-value pairs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对任何RDD应用转换来轻松创建`PairRDD`，将RDD转换为键值对的RDD。
- en: Let's read the `statesPopulation.csv` into an RDD using the `SparkContext`,
    which is available as `sc`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`SparkContext`将`statesPopulation.csv`读入RDD，该`SparkContext`可用作`sc`。
- en: 'The following is an example of a basic RDD of the state population and how
    `PairRDD` looks like for the same RDD splitting the records into tuples (pairs)
    of state and population:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个基本RDD的示例，显示了州人口以及相同RDD的`PairRDD`是什么样子，将记录拆分为州和人口的元组（对）：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following is a diagram of the preceding example showing how the RDD elements
    are converted to `(key - value)` pairs:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面示例的图表，显示了RDD元素如何转换为`(键 - 值)`对：
- en: '![](img/00341.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00341.jpeg)'
- en: DoubleRDD
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DoubleRDD
- en: DoubleRDD is an RDD consisting of a collection of double values. Due to this
    property, many statistical functions are available to use with the DoubleRDD.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: DoubleRDD是由一系列双精度值组成的RDD。由于这个属性，许多统计函数可以与DoubleRDD一起使用。
- en: 'The following are examples of DoubleRDD where we create an RDD from a sequence
    of double numbers:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们从一系列双精度数字创建RDD的DoubleRDD示例：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following is a diagram of the DoubleRDD and how you can run a `sum()` function
    on the DoubleRDD:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是DoubleRDD的图表，以及如何在DoubleRDD上运行`sum()`函数：
- en: '![](img/00371.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00371.jpeg)'
- en: SequenceFileRDD
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SequenceFileRDD
- en: '`SequenceFileRDD` is created from a `SequenceFile` which is a format of files
    in the Hadoop File System. The `SequenceFile` can be compressed or uncompressed.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`SequenceFileRDD`是从Hadoop文件系统中的`SequenceFile`创建的格式。`SequenceFile`可以是压缩或未压缩的。'
- en: Map Reduce processes can use SequenceFiles, which are pairs of Keys and Values.
    Key and Value are of Hadoop writable datatypes, such as Text, IntWritable, and
    so on.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Map Reduce进程可以使用SequenceFiles，这是键和值的对。键和值是Hadoop可写数据类型，如Text、IntWritable等。
- en: 'The following is an example of a `SequenceFileRDD`, which shows how we can
    write and read `SequenceFile`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个`SequenceFileRDD`的示例，显示了如何写入和读取`SequenceFile`：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is a diagram of **SequenceFileRDD** as seen in the preceding
    example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在前面示例中看到的**SequenceFileRDD**的图表：
- en: '![](img/00013.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00013.jpeg)'
- en: CoGroupedRDD
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CoGroupedRDD
- en: '`CoGroupedRDD` is an RDD that cogroups its parents. Both parent RDDs have to
    be pairRDDs for this to work, as a cogroup essentially generates a pairRDD consisting
    of the common key and list of values from both parent RDDs. Take a look at the
    following code snippet:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`CoGroupedRDD`是一个cogroup其父级的RDD。这个工作的两个父RDD都必须是pairRDDs，因为cogroup实质上生成一个由来自两个父RDD的公共键和值列表组成的pairRDD。看一下下面的代码片段：'
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following is an example of a CoGroupedRDD where we create a cogroup of
    two pairRDDs, one having pairs of State, Population and the other having pairs
    of State, Year:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个CoGroupedRDD的示例，我们在其中创建了两个pairRDDs的cogroup，一个具有州、人口对，另一个具有州、年份对：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following is a diagram of the cogroup of **pairRDD** and **pairRDD2** by
    creating pairs of values for each key:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是通过为每个键创建值对的**pairRDD**和**pairRDD2**的cogroup的图表：
- en: '![](img/00179.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00179.jpeg)'
- en: ShuffledRDD
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ShuffledRDD
- en: '`ShuffledRDD` shuffles the RDD elements by key so as to accumulate values for
    the same key on the same executor to allow an aggregation or combining logic.
    A very good example is to look at what happens when `reduceByKey()` is called
    on a PairRDD:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`ShuffledRDD`通过键对RDD元素进行洗牌，以便在同一个执行器上累积相同键的值，以允许聚合或组合逻辑。一个很好的例子是看看在PairRDD上调用`reduceByKey()`时会发生什么：'
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following is a `reduceByKey` operation on the `pairRDD` to aggregate the
    records by the State:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对`pairRDD`进行`reduceByKey`操作，以按州聚合记录的示例：
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following diagram, is an illustration of the shuffling by Key to send the
    records of the same Key(State) to the same partitions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表是按键进行洗牌以将相同键（州）的记录发送到相同分区的示例：
- en: '![](img/00024.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00024.jpeg)'
- en: UnionRDD
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UnionRDD
- en: '`UnionRDD` is the result of a union operation of two RDDs. Union simply creates
    an RDD with elements from both RDDs as shown in the following code snippet:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`UnionRDD`是两个RDD的并集操作的结果。Union简单地创建一个包含来自两个RDD的元素的RDD，如下面的代码片段所示：'
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following diagram is an illustration of a union of two RDDs where the elements
    from both **RDD 1** and **RDD 2** are combined into a new RDD **UnionRDD**:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表是两个RDD的并集的示例，其中来自**RDD 1**和**RDD 2**的元素被合并到一个新的RDD **UnionRDD**中：
- en: '![](img/00305.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00305.jpeg)'
- en: HadoopRDD
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HadoopRDD
- en: '`HadoopRDD` provides core functionality for reading data stored in HDFS using
    the MapReduce API from the Hadoop 1.x libraries. `HadoopRDD` is the default used
    and can be seen when loading data from any file system into an RDD:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`HadoopRDD`提供了使用Hadoop 1.x库的MapReduce API从HDFS中读取数据的核心功能。`HadoopRDD`是默认使用的，可以在从任何文件系统加载数据到RDD时看到：'
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When loading the state population records from the CSV, the underlying base
    RDD is actually `HadoopRDD` as in the following code snippet:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当从CSV加载州人口记录时，底层基本RDD实际上是`HadoopRDD`，如下面的代码片段所示：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following diagram is an illustration of a **HadoopRDD** created by loading
    a textfile from the file system into an RDD:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表是通过将文本文件从文件系统加载到RDD中创建的**HadoopRDD**的示例：
- en: '![](img/00032.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00032.jpeg)'
- en: NewHadoopRDD
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NewHadoopRDD
- en: '`NewHadoopRDD` provides core functionality for reading data stored in HDFS,
    HBase tables, Amazon S3 using the new MapReduce API from Hadoop 2.x `libraries.NewHadoopRDD`
    can read from many different formats thus is used to interact with several external
    systems.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`NewHadoopRDD`提供了使用Hadoop 2.x库的新MapReduce API从HDFS、HBase表、Amazon S3中读取数据的核心功能。`NewHadoopRDD`可以从许多不同的格式中读取数据，因此用于与多个外部系统交互。'
- en: Prior to `NewHadoopRDD`, `HadoopRDD` was the only available option which used
    the old MapReduce API from Hadoop 1.x
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在`NewHadoopRDD`之前，`HadoopRDD`是唯一可用的选项，它使用了Hadoop 1.x的旧MapReduce API
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The simplest example is to use SparkContext''s `wholeTextFiles` function to
    create `WholeTextFileRDD`. Now, `WholeTextFileRDD` actually extends `NewHadoopRDD`
    as shown in the following code snippet:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的例子是使用SparkContext的`wholeTextFiles`函数创建`WholeTextFileRDD`。现在，`WholeTextFileRDD`实际上扩展了`NewHadoopRDD`，如下面的代码片段所示：
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s look at another example where we will use the function `newAPIHadoopFile`
    using the `SparkContext`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看另一个例子，我们将使用`SparkContext`的`newAPIHadoopFile`函数：
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Aggregations
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合
- en: Aggregation techniques allow you to combine the elements in the RDD in arbitrary
    ways to perform some computation. In fact, aggregation is the most important part
    of big data analytics. Without aggregation, we would not have any way to generate
    reports and analysis like *Top States by Population*, which seems to be a logical
    question asked when given a dataset of all State populations for the past 200
    years. Another simpler example is that of a need to just count the number of elements
    in the RDD, which asks the executors to count the number of elements in each partition
    and send to the Driver, which then adds the subsets to compute the total number
    of elements in the RDD.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合技术允许您以任意方式组合RDD中的元素以执行一些计算。事实上，聚合是大数据分析中最重要的部分。没有聚合，我们将无法生成报告和分析，比如*按人口排名的州*，这似乎是在给定过去200年所有州人口的数据集时提出的一个合乎逻辑的问题。另一个更简单的例子是只需计算RDD中元素的数量，这要求执行器计算每个分区中的元素数量并发送给Driver，然后将子集相加以计算RDD中元素的总数。
- en: In this section, our primary focus is on the aggregation functions used to collect
    and combine data by key. As seen earlier in this chapter, a PairRDD is an RDD
    of (key - value) pairs where key and value are arbitrary and can be customized
    as per the use case.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们的主要重点是聚合函数，用于按键收集和组合数据。正如本章前面所看到的，PairRDD是一个(key - value)对的RDD，其中key和value是任意的，并且可以根据用例进行自定义。
- en: In our example of state populations, a PairRDD could be the pairs of `<State,
    <Population, Year>>` which means `State` is taken as the key and the tuple `<Population,
    Year>` is considered the value. This way of breaking down the key and value can
    generate aggregations such as *Top Years by Population per State*. On the contrary,
    in case our aggregations are done around Year say *Top States by Population per
    Year*, we can use a `pairRDD` of pairs of `<Year, <State, Population>>`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的州人口示例中，PairRDD可以是`<State，<Population，Year>>`的对，这意味着`State`被视为键，元组`<Population，Year>`被视为值。通过这种方式分解键和值可以生成诸如*每个州人口最多的年份*之类的聚合。相反，如果我们的聚合是围绕年份进行的，比如*每年人口最多的州*，我们可以使用`<Year，<State，Population>>`的`pairRDD`。
- en: 'The following is the sample code to generate a `pairRDD` from the `StatePopulation`
    dataset both with `State` as the key as well as the `Year` as the key:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从`StatePopulation`数据集生成`pairRDD`的示例代码，其中`State`作为键，`Year`也作为键：
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we can generate a `pairRDD` using `State` as the key and a tuple of `<Year,
    Population>` as the value as shown in the following code snippet:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以生成一个`pairRDD`，使用`State`作为键，`<Year，Population>`元组作为值，如下面的代码片段所示：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As mentioned earlier, we can also generate a `PairRDD` using `Year` as the
    key and a tuple of `<State, Population>` as the value as shown in the following
    code snippet:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们还可以生成一个`PairRDD`，使用`Year`作为键，`<State，Population>`元组作为值，如下面的代码片段所示：
- en: '[PRE22]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will now look into how we can use the common aggregation functions on the
    `pairRDD` of `<State, <Year, Population>>`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看看如何在`<State，<Year，Population>>`的`pairRDD`上使用常见的聚合函数：
- en: '`groupByKey`'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groupByKey`'
- en: '`reduceByKey`'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduceByKey`'
- en: '`aggregateByKey`'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregateByKey`'
- en: '`combineByKey`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`combineByKey`'
- en: groupByKey
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: groupByKey
- en: '`groupByKey` groups the values for each key in the RDD into a single sequence.
    `groupByKey` also allows controlling the partitioning of the resulting key-value
    pair RDD by passing a partitioner. By default, a `HashPartitioner` is used but
    a custom partitioner can be given as an argument. The ordering of elements within
    each group is not guaranteed, and may even differ each time the resulting RDD
    is evaluated.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey`将RDD中每个键的值分组为单个序列。`groupByKey`还允许通过传递分区器来控制生成的键值对RDD的分区。默认情况下，使用`HashPartitioner`，但可以作为参数给出自定义分区器。每个组内元素的顺序不能保证，并且每次评估结果RDD时甚至可能不同。'
- en: '`groupByKey` is an expensive operation due to all the data shuffling needed.
    `reduceByKey` or `aggregateByKey` provide much better performance. We will look
    at this later in this section.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey`是一个昂贵的操作，因为需要所有的数据洗牌。`reduceByKey`或`aggregateByKey`提供了更好的性能。我们将在本节的后面进行讨论。'
- en: '`groupByKey` can be invoked either using a custom partitioner or just using
    the default `HashPartitioner` as shown in the following code snippet:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey`可以使用自定义分区器调用，也可以只使用默认的`HashPartitioner`，如下面的代码片段所示：'
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As currently implemented, `groupByKey` must be able to hold all the key-value
    pairs for any key in memory. If a key has too many values, it can result in an
    `OutOfMemoryError`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 目前实现的`groupByKey`必须能够在内存中保存任何键的所有键值对。如果一个键有太多的值，可能会导致`OutOfMemoryError`。
- en: '`groupByKey` works by sending all elements of the partitions to the partition
    based on the partitioner so that all pairs of (key - value) for the same key are
    collected in the same partition. Once this is done, the aggregation operation
    can be done easily.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey`通过将分区的所有元素发送到基于分区器的分区，以便将相同键的所有键值对收集到同一分区中。完成此操作后，可以轻松进行聚合操作。'
- en: 'Shown here is an illustration of what happens when `groupByKey` is called:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示了调用`groupByKey`时发生的情况的示例：
- en: '![](img/00036.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00036.jpeg)'
- en: reduceByKey
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: reduceByKey
- en: '`groupByKey` involves a lot of shuffling and `reduceByKey` tends to improve
    the performance by not sending all elements of the `PairRDD` using shuffles, rather
    using a local combiner to first do some basic aggregations locally and then send
    the resultant elements as in `groupByKey`. This greatly reduces the data transferred,
    as we don''t need to send everything over. `reduceBykey` works by merging the
    values for each key using an associative and commutative reduce function. Of course,
    first, this will'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey`涉及大量的数据洗牌，而`reduceByKey`倾向于通过不使用洗牌发送`PairRDD`的所有元素来提高性能，而是使用本地组合器首先在本地进行一些基本的聚合，然后像`groupByKey`一样发送结果元素。这大大减少了数据传输，因为我们不需要发送所有内容。`reduceBykey`通过使用关联和可交换的减少函数合并每个键的值。当然，首先这将'
- en: also perform the merging locally on each mapper before sending results to a
    reducer.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以在每个mapper上本地执行合并，然后将结果发送到reducer。
- en: If you are familiar with Hadoop MapReduce, this is very similar to a combiner
    in MapReduce programming.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉Hadoop MapReduce，这与MapReduce编程中的组合器非常相似。
- en: '`reduceByKey` can be invoked either using a custom partitioner or just using
    the default HashPartitioner as shown in the following code snippet:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey`可以使用自定义分区器调用，也可以只使用默认的`HashPartitioner`，如下面的代码片段所示：'
- en: '[PRE24]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`reduceByKey` works by sending all elements of the partitions to the partition
    based on the `partitioner` so that all pairs of (key - value) for the same Key
    are collected in the same partition. But before the shuffle, local aggregation
    is also done reducing the data to be shuffled. Once this is done, the aggregation
    operation can be done easily in the final partition.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey`通过将分区的所有元素发送到基于`partitioner`的分区，以便将相同键的所有键值对收集到同一分区中。但在洗牌之前，还进行本地聚合，减少要洗牌的数据。完成此操作后，可以在最终分区中轻松进行聚合操作。'
- en: 'The following diagram is an illustration of what happens when `reduceBykey`
    is called:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是调用`reduceBykey`时发生的情况的示例：
- en: '![](img/00039.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00039.jpeg)'
- en: aggregateByKey
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: aggregateByKey
- en: '`aggregateByKey` is quite similar to `reduceByKey`, except that `aggregateByKey`
    allows more flexibility and customization of how to aggregate within partitions
    and between partitions to allow much more sophisticated use cases such as generating
    a list of all `<Year, Population>` pairs as well as total population for each
    State in one function call.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregateByKey`与`reduceByKey`非常相似，只是`aggregateByKey`允许更灵活和定制如何在分区内和分区之间进行聚合，以允许更复杂的用例，例如在一个函数调用中生成所有`<Year,
    Population>`对的列表以及每个州的总人口。'
- en: '`aggregateByKey` works by aggregating the values of each key, using given combine
    functions and a neutral initial/zero value.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregateByKey`通过使用给定的组合函数和中性初始/零值对每个键的值进行聚合。'
- en: 'This function can return a different result type, `U`, than the type of the
    values in this RDD `V`, which is the biggest difference. Thus, we need one operation
    for merging a `V` into a `U` and one operation for merging two `U`''s. The former
    operation is used for merging values within a partition, and the latter is used
    for merging values between partitions. To avoid memory allocation, both of these
    functions are allowed to modify and return their first argument instead of creating
    a new `U`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可以返回一个不同的结果类型`U`，而不是这个RDD`V`中的值的类型，这是最大的区别。因此，我们需要一个操作将`V`合并为`U`，以及一个操作将两个`U`合并。前一个操作用于在分区内合并值，后一个用于在分区之间合并值。为了避免内存分配，这两个函数都允许修改并返回它们的第一个参数，而不是创建一个新的`U`：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`aggregateByKey` works by performing an aggregation within the partition operating
    on all elements of each partition and then applies another aggregation logic when
    combining the partitions themselves. Ultimately, all pairs of (key - value) for
    the same Key are collected in the same partition; however, the aggregation as
    to how it is done and the output generated is not fixed as in `groupByKey` and
    `reduceByKey`, but is more flexible and customizable when using `aggregateByKey`.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregateByKey`通过在分区内对每个分区的所有元素进行聚合操作，然后在合并分区本身时应用另一个聚合逻辑来工作。最终，相同Key的所有（键-值）对都被收集在同一个分区中；然而，与`groupByKey`和`reduceByKey`中的固定输出不同，使用`aggregateByKey`时更灵活和可定制。'
- en: 'The following diagram is an illustration of what happens when `aggregateByKey`
    is called. Instead of adding up the counts as in `groupByKey` and `reduceByKey`,
    here we are generating lists of values for each Key:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是调用`aggregateByKey`时发生的情况的示例。与`groupByKey`和`reduceByKey`中添加计数不同，这里我们为每个Key生成值列表：
- en: '![](img/00043.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00043.jpeg)'
- en: combineByKey
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: combineByKey
- en: '`combineByKey` is very similar to `aggregateByKey`; in fact, `combineByKey`
    internally invokes `combineByKeyWithClassTag`, which is also invoked by `aggregateByKey`.
    As in `aggregateByKey`, the `combineByKey` also works by applying an operation
    within each partition and then between combiners.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey`与`aggregateByKey`非常相似；实际上，`combineByKey`在内部调用`combineByKeyWithClassTag`，这也被`aggregateByKey`调用。与`aggregateByKey`一样，`combineByKey`也通过在每个分区内应用操作，然后在组合器之间工作。'
- en: '`combineByKey` turns an `RDD[K,V]` into an `RDD[K,C]`, where `C` is a list
    of Vs collected or combined under the name key `K`.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey`将`RDD[K,V]`转换为`RDD[K,C]`，其中`C`是在名称键`K`下收集或组合的V的列表。'
- en: There are three functions expected when you call combineByKey.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 调用combineByKey时期望有三个函数。
- en: '`createCombiner`, which turns a `V` into `C`, which is a one element list'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`createCombiner`，将`V`转换为`C`，这是一个元素列表'
- en: '`mergeValue` to merge a `V` into a `C` by appending the `V` to the end of the
    list'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mergeValue`将`V`合并到`C`中，将`V`附加到列表的末尾'
- en: '`mergeCombiners` to combine two Cs into one'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mergeCombiners`将两个C合并为一个'
- en: In `aggregateByKey`, the first argument is simply a zero value but in `combineByKey`,
    we provide the initial function which takes the current value as a parameter.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在`aggregateByKey`中，第一个参数只是一个零值，但在`combineByKey`中，我们提供了以当前值作为参数的初始函数。
- en: '`combineByKey` can be invoked either using a custom partitioner or just using
    the default HashPartitioner as shown in the following code snippet:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey`可以使用自定义分区器调用，也可以只使用默认的HashPartitioner，如下面的代码片段所示：'
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`combineByKey` works by performing an aggregation within the partition operating
    on all elements of each partition and then applies another aggregation logic when
    combining the partitions themselves. Ultimately, all pairs of (key - value) for
    the same Key are collected in the same partition however the aggregation as to
    how it is done and the output generated is not fixed as in `groupByKey` and `reduceByKey`,
    but is more flexible and customizable when using `combineByKey`.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey`通过在分区内对每个分区的所有元素进行聚合操作，然后在合并分区本身时应用另一个聚合逻辑来工作。最终，相同Key的所有（键-值）对都被收集在同一个分区中，但是与`groupByKey`和`reduceByKey`中的固定输出不同，使用`combineByKey`时更灵活和可定制。'
- en: 'The following diagram is an illustration of what happens when `combineBykey`
    is called:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是调用`combineBykey`时发生的情况的示例：
- en: '![](img/00045.jpeg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00045.jpeg)'
- en: Comparison of groupByKey, reduceByKey, combineByKey, and aggregateByKey
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: groupByKey、reduceByKey、combineByKey和aggregateByKey的比较
- en: Let's consider the example of StatePopulation RDD generating a `pairRDD` of
    `<State, <Year, Population>>`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑StatePopulation RDD生成一个`pairRDD`的例子，其中包含`<State, <Year, Population>>`。
- en: '`groupByKey` as seen in the preceding section will do `HashPartitioning` of
    the `PairRDD` by generating a hashcode of the keys and then shuffling the data
    to collect the values for each key in the same partition. This obviously results
    in too much shuffling.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupByKey`如前面的部分所示，将通过生成键的哈希码对`PairRDD`进行`HashPartitioning`，然后洗牌数据以在同一分区中收集每个键的值。这显然会导致过多的洗牌。'
- en: '`reduceByKey` improves upon `groupByKey` using a local combiner logic to minimize
    the data sent in a shuffle phase. The result will be the same as `groupByKey`,
    but will be much more performant.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey`通过使用本地组合逻辑改进了`groupByKey`，以最小化在洗牌阶段发送的数据。结果与`groupByKey`相同，但性能更高。'
- en: '`aggregateByKey` is very similar to `reduceByKey` in how it works but with
    one big difference, which makes it the most powerful one among the three. `aggregateBykey`
    does not need to operate on the same datatype and can do different aggregation
    within the partition and do a different aggregation between partitions.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregateByKey`在工作方式上与`reduceByKey`非常相似，但有一个重大区别，这使它成为这三种方法中最强大的一个。`aggregateBykey`不需要在相同的数据类型上操作，并且可以在分区内进行不同的聚合，在分区之间进行不同的聚合。'
- en: '`combineByKey` is very similar in performance to `aggregateByKey` except for
    the initial function to create the combiner.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`combineByKey`在性能上与`aggregateByKey`非常相似，除了用于创建组合器的初始函数。'
- en: The function to use depends on your use case but when in doubt just refer to
    this section on *Aggregation* to choose the right function for your use case.
    Also, pay close attention to the next section as *Partitioning and shuffling*
    are covered in that section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的函数取决于您的用例，但如果有疑问，只需参考本节关于*聚合*的部分，选择适合您用例的正确函数。此外，要特别关注下一节，因为*分区和洗牌*将在该部分中介绍。
- en: The following is the code showing all four ways of calculating total population
    by state.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是显示通过州计算总人口的四种方法的代码。
- en: '**Step 1\. Initialize the RDD:**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步。初始化RDD：
- en: '[PRE27]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**Step 2\. Convert to pair RDD:**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步。转换为成对的RDD：
- en: '[PRE28]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Step 3\. groupByKey - Grouping the values and then adding up populations:**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步。groupByKey - 分组值，然后添加人口：
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Step 4\. reduceByKey - Reduce the values by key simply adding the populations:**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步。reduceByKey - 通过简单地添加人口来减少键的值：
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**Step 5\. aggregateBykey - aggregate the populations under each key and adds
    them up:**'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步。按键聚合 - 聚合每个键下的人口并将它们相加：
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Step 6\. combineByKey - combine within partitions and then merging combiners:**'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 第6步。combineByKey - 在分区内进行组合，然后合并组合器：
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you see, all four aggregations result in the same output. It's just how they
    work that is different.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，所有四种聚合都产生相同的输出。只是它们的工作方式不同。
- en: Partitioning and shuffling
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分区和洗牌
- en: We have seen how Apache Spark can handle distributed computing much better than
    Hadoop. We also saw the inner workings, mainly the fundamental data structure
    known as **Resilient Distributed Dataset** (**RDD**). RDDs are immutable collections
    representing datasets and have the inbuilt capability of reliability and failure
    recovery. RDDs operate on data not as a single blob of data, rather RDDs manage
    and operate data in partitions spread across the cluster. Hence, the concept of
    data partitioning is critical to the proper functioning of Apache Spark Jobs and
    can have a big effect on the performance as well as how the resources are utilized.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到Apache Spark如何比Hadoop更好地处理分布式计算。我们还看到了内部工作，主要是基本数据结构，称为**弹性分布式数据集**（**RDD**）。RDD是不可变的集合，代表数据集，并具有内置的可靠性和故障恢复能力。RDD在数据上的操作不是作为单个数据块，而是在整个集群中分布的分区中管理和操作数据。因此，数据分区的概念对于Apache
    Spark作业的正常运行至关重要，并且可能对性能以及资源的利用方式产生重大影响。
- en: RDD consists of partitions of data and all operations are performed on the partitions
    of data in the RDD. Several operations like transformations are functions executed
    by an executor on the specific partition of data being operated on. However, not
    all operations can be done by just performing isolated operations on the partitions
    of data by the respective executors. Operations like aggregations (seen in the
    preceding section) require data to be moved across the cluster in a phase known
    as **shuffling**. In this section, we will look deeper into the concepts of partitioning
    and shuffling.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: RDD由数据分区组成，所有操作都是在RDD的数据分区上执行的。诸如转换之类的几个操作是由执行器在正在操作的特定数据分区上执行的函数。然而，并非所有操作都可以通过在各自的执行器上对数据分区执行孤立的操作来完成。像聚合（在前面的部分中看到）这样的操作需要在整个集群中移动数据，这个阶段被称为**洗牌**。在本节中，我们将更深入地了解分区和洗牌的概念。
- en: Let's start looking at a simple RDD of integers by executing the following code.
    Spark Context's `parallelize` function creates an RDD from the Sequence of integers.
    Then, using the `getNumPartitions()` function, we can get the number of partitions
    of this RDD.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过执行以下代码来查看整数的简单RDD。Spark上下文的`parallelize`函数从整数序列创建RDD。然后，使用`getNumPartitions()`函数，我们可以获取此RDD的分区数。
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The RDD can be visualized as shown in the following diagram, which shows the
    8 partitions in the RDD:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: RDD可以如下图所示进行可视化，显示了RDD中的8个分区：
- en: '![](img/00136.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00136.jpeg)'
- en: The number of partitions is important because this number directly influences
    the number of tasks that will be running RDD transformations. If the number of
    partitions is too small, then we will use only a few CPUs/cores on a lot of data
    thus having a slower performance and leaving the cluster underutilized. On the
    other hand, if the number of partitions is too large then you will use more resources
    than you actually need and in a multi-tenant environment could be causing starvation
    of resources for other Jobs being run by you or others in your team.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 分区数很重要，因为这个数字直接影响将运行RDD转换的任务数量。如果分区数太小，那么我们将在大量数据上只使用少量CPU/核心，从而导致性能较慢，并且使集群利用不足。另一方面，如果分区数太大，那么您将使用比实际需要更多的资源，在多租户环境中可能会导致为您或您团队中的其他作业运行的资源饥饿。
- en: Partitioners
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分区器
- en: Partitioning of RDDs is done by partitioners. Partitioners assign a partition
    index to the elements in the RDD. All elements in the same partition will have
    the same partition index.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: RDD的分区是由分区器完成的。分区器为RDD中的元素分配分区索引。同一分区中的所有元素将具有相同的分区索引。
- en: Spark comes with two partitioners the `HashPartitioner` and the `RangePartitioner`.
    In addition to these, you can also implement a custom partitioner.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了两种分区器`HashPartitioner`和`RangePartitioner`。除此之外，您还可以实现自定义分区器。
- en: HashPartitioner
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HashPartitioner
- en: '`HashPartitioner` is the default partitioner in Spark and works by calculating
    a hash value for each key of the RDD elements. All the elements with the same
    hashcode end up in the same partition as shown in the following code snippet:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`HashPartitioner`是Spark中的默认分区器，它通过为RDD元素的每个键计算哈希值来工作。所有具有相同哈希码的元素最终都会进入同一个分区，如下面的代码片段所示：'
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following is an example of the String `hashCode()` function and how we
    can generate `partitionIndex`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是String `hashCode()`函数的示例，以及我们如何生成`partitionIndex`：
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The default number of partitions is either from the Spark configuration parameter
    `spark.default.parallelism` or the number of cores in the cluster
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 默认分区数要么来自Spark配置参数`spark.default.parallelism`，要么来自集群中的核心数
- en: 'The following diagram is an illustration of how hash partitioning works. We
    have an RDD with 3 elements **a**, **b**, and **e**. Using String hashcode we
    get the `partitionIndex` for each element based on the number of partitions set
    at 6:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了哈希分区的工作原理。我们有一个包含3个元素**a**、**b**和**e**的RDD。使用String哈希码，我们可以根据设置的6个分区得到每个元素的`partitionIndex`：
- en: '![](img/00140.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00140.jpeg)'
- en: RangePartitioner
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RangePartitioner
- en: '`RangePartitioner` works by partitioning the RDD into roughly equal ranges.
    Since the range has to know the starting and ending keys for any partition, the
    RDD needs to be sorted first before a `RangePartitioner` can be used.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`RangePartitioner`通过将RDD分区为大致相等的范围来工作。由于范围必须知道任何分区的起始和结束键，因此在使用`RangePartitioner`之前，RDD需要首先进行排序。'
- en: '`RangePartitioning` first needs reasonable boundaries for the partitions based
    on the RDD and then create a function from key K to the `partitionIndex` where
    the element belongs. Finally, we need to repartition the RDD, based on the `RangePartitioner`
    to distribute the RDD elements correctly as per the ranges we determined.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`RangePartitioning` 首先需要根据RDD确定合理的分区边界，然后创建一个从键K到`partitionIndex`的函数，该函数确定元素所属的分区。最后，我们需要根据`RangePartitioner`重新分区RDD，以便根据我们确定的范围正确分发RDD元素。'
- en: 'The following is an example of how we can use `RangePartitioning` of a `PairRDD`.
    We also can see how the partitions changed after we repartition the RDD using
    a `RangePartitioner`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们如何使用`RangePartitioning`对`PairRDD`进行分区的示例。我们还可以看到在使用`RangePartitioner`重新分区RDD后分区发生了变化：
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following diagram is an illustration of the `RangePartitioner` as seen
    in the preceding example:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了`RangePartitioner`，就像在前面的示例中看到的那样：
- en: '![](img/00143.jpeg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00143.jpeg)'
- en: Shuffling
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 洗牌
- en: Whatever the partitioner used, many operations will cause a repartitioning of
    data across the partitions of an RDD. New partitions can be created or several
    partitions can be collapsed/coalesced. All the data movement necessary for the
    repartitioning is called **shuffling,** and this is an important concept to understand
    when writing a Spark Job. The shuffling can cause a lot of performance lag as
    the computations are no longer in memory on the same executor but rather the executors
    are exchanging data over the wire.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 无论使用何种分区器，许多操作都会导致RDD数据在分区之间进行重新分区。可以创建新分区，也可以合并/压缩多个分区。为了进行重新分区所需的所有数据移动都称为**shuffling**，这是编写Spark作业时需要理解的重要概念。洗牌可能会导致性能严重下降，因为计算不再在同一个执行器的内存中进行，而是执行器在网络上传输数据。
- en: A good example is the example of `groupByKey()`, we saw earlier in the *Aggregations*
    section. Obviously, lot of data was flowing between executors to make sure all
    values for a key are collected onto the same executor to perform the `groupBy`
    operation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的例子是我们在*聚合*部分早些时候看到的`groupByKey()`的例子。显然，大量数据在执行器之间流动，以确保所有键的值都被收集到同一个执行器上执行`groupBy`操作。
- en: Shuffling also determines the Spark Job execution process and influences how
    the Job is split into Stages. As we have seen in this chapter and the previous
    chapter, Spark holds a DAG of RDDs, which represent the lineage of the RDDs such
    that not only does Spark use the lineage to plan the execution of the job but
    also any loss of executors can be recovered from. When an RDD is undergoing a
    transformation, an attempt is made to make sure the operations are performed on
    the same node as the data. However, often we use join operations, reduce, group,
    or aggregate operations among others, which cause repartitioning intentionally
    or unintentionally. This shuffling in turn determines where a particular stage
    in the processing has ended and a new stage has begun.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Shuffling还确定了Spark作业的执行过程，并影响作业如何分成阶段。正如我们在本章和上一章中所看到的，Spark保存了RDD的DAG，它代表了RDD的血统，因此Spark不仅使用血统来规划作业的执行，而且可以从中恢复任何执行器的丢失。当RDD正在进行转换时，会尝试确保操作在与数据相同的节点上执行。然而，通常我们使用连接操作、reduce、group或聚合等操作，这些操作会有意或无意地导致重新分区。这种洗牌反过来又决定了处理中的特定阶段在哪里结束，新阶段从哪里开始。
- en: 'The following diagram is an illustration of how a Spark Job is split into stages.
    This example shows a `pairRDD` being filtered, transformed using map before invoking
    `groupByKey` followed by one last transformation using `map()`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了Spark作业如何分成阶段。此示例显示了对`pairRDD`进行过滤，使用map进行转换，然后调用`groupByKey`，最后使用`map()`进行最后一次转换：
- en: '![](img/00147.jpeg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00147.jpeg)'
- en: The more shuffling we have, the more stages occur in the job execution affecting
    the performance. There are two key aspects which are used by Spark Driver to determine
    the stages. This is done by defining two types of dependencies of the RDDs, the
    narrow dependencies and the wide dependencies.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行的洗牌越多，作业执行中就会出现越多的阶段，从而影响性能。Spark Driver用于确定阶段的两个关键方面是定义RDD的两种依赖关系，即窄依赖和宽依赖。
- en: Narrow Dependencies
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窄依赖
- en: When an RDD can be derived from another RDD using a simple one-to-one transformation
    such as a `filter()` function, `map()` function, `flatMap()` function, and so
    on, then the child RDD is said to depend on the parent RDD on a one-to-one basis.
    This dependency is known as narrow dependency as the data can be transformed on
    the same node as the one containing the original RDD/parent RDD partition without
    requiring any data transfer over the wire between other executors.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个RDD可以通过简单的一对一转换（如`filter()`函数、`map()`函数、`flatMap()`函数等）从另一个RDD派生出来时，子RDD被认为是依赖于父RDD的一对一基础。这种依赖关系被称为窄依赖，因为数据可以在包含原始RDD/父RDD分区的同一节点上进行转换，而无需在其他执行器之间进行任何数据传输。
- en: Narrow dependencies are in the same stage of the job execution.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 窄依赖在作业执行的同一阶段中。
- en: 'The following diagram is an illustration of how a narrow dependency transforms
    one RDD to another RDD, applying one-to-one transformation on the RDD elements:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是一个窄依赖如何将一个RDD转换为另一个RDD的示例，对RDD元素进行一对一的转换：
- en: '![](img/00152.jpeg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00152.jpeg)'
- en: Wide Dependencies
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广泛依赖
- en: When an RDD can be derived from one or more RDDs by transferring data over the
    wire or exchanging data to repartition or redistribute the data using functions,
    such as `aggregateByKey`, `reduceByKey` and so on, then the child RDD is said
    to depend on the parent RDDs participating in a shuffle operation. This dependency
    is known as a Wide dependency as the data cannot be transformed on the same node
    as the one containing the original RDD/parent RDD partition thus requiring data
    transfer over the wire between other executors.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个RDD可以通过在线传输数据或使用函数进行数据重分区或重新分发数据（如`aggregateByKey`、`reduceByKey`等）从一个或多个RDD派生出来时，子RDD被认为依赖于参与洗牌操作的父RDD。这种依赖关系被称为广泛依赖，因为数据不能在包含原始RDD/父RDD分区的同一节点上进行转换，因此需要在其他执行器之间通过网络传输数据。
- en: Wide dependencies introduce new stages in the job execution.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛的依赖关系引入了作业执行中的新阶段。
- en: 'The following diagram is an illustration of how wide dependency transforms
    one RDD to another RDD shuffling data between executors:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是一个广泛依赖如何在执行器之间洗牌数据将一个RDD转换为另一个RDD的示例：
- en: '![](img/00155.jpeg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00155.jpeg)'
- en: Broadcast variables
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广播变量
- en: Broadcast variables are shared variables across all executors. Broadcast variables
    are created once in the Driver and then are read only on executors. While it is
    simple to understand simple datatypes broadcasted, such as an `Integer`, broadcast
    is much bigger than simple variables conceptually. Entire datasets can be broadcasted
    in a Spark cluster so that executors have access to the broadcasted data. All
    the tasks running within an executor all have access to the broadcast variables.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量是所有执行器共享的变量。广播变量在驱动程序中创建一次，然后在执行器上只读。虽然理解简单数据类型的广播，比如`Integer`，是很简单的，但广播在概念上比简单的变量要大得多。整个数据集可以在Spark集群中广播，以便执行器可以访问广播的数据。在执行器中运行的所有任务都可以访问广播变量。
- en: Broadcast uses various optimized methods to make the broadcasted data accessible
    to all executors. This is an important challenge to solve as if the size of the
    datasets broadcasted is significant, you cannot expect 100s or 1000s of executors
    to connect to the Driver and pull the dataset. Rather, the executors pull the
    data via HTTP connection and the more recent addition which is similar to BitTorrent
    where the dataset itself is distributed like a torrent amongst the cluster. This
    enables a much more scalable method to distribute the broadcasted variables to
    all executors rather than having each executor pull the data from the Driver one
    by one which can cause failures on the Driver when you have a lot of executors.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 广播使用各种优化方法使广播的数据对所有执行器都可访问。这是一个重要的挑战，因为如果广播的数据集的大小很大，你不能指望100个或1000个执行器连接到驱动程序并拉取数据集。相反，执行器通过HTTP连接拉取数据，还有一个类似于BitTorrent的最近添加的方法，其中数据集本身就像种子一样分布在集群中。这使得将广播变量分发给所有执行器的方法比每个执行器逐个从驱动程序拉取数据更具可伸缩性，这可能会导致驱动程序在有大量执行器时出现故障。
- en: The driver can only broadcast the data it has and you cannot broadcast RDDs
    by using references. This is because only Driver knows how to interpret RDDs and
    executors only know the particular partitions of data they are handling.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序只能广播它拥有的数据，你不能使用引用来广播RDD。这是因为只有驱动程序知道如何解释RDD，执行器只知道它们正在处理的数据的特定分区。
- en: If you look deeper into how broadcast works, you will see that the mechanism
    works by first having the Driver divide the serialized object into small chunks
    and then stores those chunks in the BlockManager of the driver. When the code
    is serialized to be run on the executors, then each executor first attempts to
    fetch the object from its own internal BlockManager. If the broadcast variable
    was fetched before, it will find it and use it. However, if it does not exist,
    the executor then uses remote fetches to fetch the small chunks from the driver
    and/or other executors if available. Once it gets the chunks, it puts the chunks
    in its own BlockManager, ready for any other executors to fetch from. This prevents
    the driver from being the bottleneck in sending out multiple copies of the broadcast
    data (one per executor).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你深入研究广播的工作原理，你会发现这种机制首先由驱动程序将序列化对象分成小块，然后将这些块存储在驱动程序的BlockManager中。当代码被序列化以在执行器上运行时，每个执行器首先尝试从自己的内部BlockManager中获取对象。如果广播变量之前已经被获取过，它会找到并使用它。然而，如果它不存在，执行器将使用远程获取从驱动程序和/或其他可用的执行器中获取小块。一旦获取了这些块，它就会将这些块放入自己的BlockManager中，准备让其他执行器从中获取。这可以防止驱动程序成为发送广播数据的瓶颈（每个执行器一个副本）。
- en: 'The following diagram is an illustration of how broadcast works in a Spark
    cluster:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是一个Spark集群中广播工作的示例：
- en: '![](img/00006.jpeg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00006.jpeg)'
- en: Broadcast variables can be both created and destroyed too. We will look into
    the creation and destruction of broadcast variables. There is also a way to remove
    broadcasted variables from memory which we will also look at.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量既可以创建也可以销毁。我们将研究广播变量的创建和销毁。还有一种方法可以从内存中删除广播变量，我们也将研究。
- en: Creating broadcast variables
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建广播变量
- en: Creating a broadcast variable can be done using the Spark Context's `broadcast()`
    function on any data of any data type provided that the data/variable is serializable.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Spark上下文的`broadcast()`函数在任何数据类型的任何数据上创建广播变量，前提是数据/变量是可序列化的。
- en: 'Let''s look at how we can broadcast an Integer variable and then use the broadcast
    variable inside a transformation operation executed on the executors:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何广播一个整数变量，然后在执行程序上执行转换操作时使用广播变量：
- en: '[PRE37]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Broadcast variables can also be created on more than just primitive data types
    as shown in the next example where we will broadcast a `HashMap` from the Driver.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量也可以创建在不仅仅是原始数据类型上，如下一个示例所示，我们将从Driver广播一个`HashMap`。
- en: 'The following is a simple transformation of an integer RDD by multiplying each
    element with another integer by looking up the HashMap. The RDD of 1,2,3 is transformed
    to 1 X 2 , 2 X 3, 3 X 4 = 2,6,12 :'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是通过查找HashMap将整数RDD进行简单转换的示例，将RDD的1,2,3转换为1 X 2，2 X 3，3 X 4 = 2,6,12：
- en: '[PRE38]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Cleaning broadcast variables
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理广播变量
- en: Broadcast variables do occupy memory on all executors and depending on the size
    of the data contained in the broadcasted variable, this could cause resource issues
    at some point. There is a way to remove broadcasted variables from the memory
    of all executors.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量在所有执行程序上占用内存，并且根据广播变量中包含的数据的大小，这可能会在某个时刻引起资源问题。有一种方法可以从所有执行程序的内存中删除广播变量。
- en: Calling `unpersist()` on a broadcast variable removed the data of the broadcast
    variable from the memory cache of all executors to free up resources. If the variable
    is used again, then the data is retransmitted to the executors in order for it
    to be used again. The Driver, however, holds onto the memory as if the Driver
    does not have the data, then broadcast variable is no longer valid.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在广播变量上调用`unpersist()`会从所有执行程序的内存缓存中删除广播变量的数据，以释放资源。如果再次使用变量，则数据将重新传输到执行程序，以便再次使用。但是，Driver会保留内存，如果Driver没有数据，则广播变量将不再有效。
- en: We look at destroying broadcast variables next.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将看看如何销毁广播变量。
- en: The following is an example of how `unpersist()` can be invoked on a broadcast
    variable. After calling `unpersist` if we access the broadcast variable again,
    it works as usual but behind the scenes, the executors are pulling the data for
    the variable again.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何在广播变量上调用`unpersist()`。调用`unpersist`后，如果我们再次访问广播变量，则它会像往常一样工作，但在幕后，执行程序再次获取变量的数据。
- en: '[PRE39]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Destroying broadcast variables
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 销毁广播变量
- en: You can also destroy broadcast variables, completely removing them from all
    executors and the Driver too making them inaccessible. This can be quite helpful
    in managing the resources optimally across the cluster.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以销毁广播变量，将其从所有执行程序和Driver中完全删除，使其无法访问。这在跨集群有效地管理资源方面非常有帮助。
- en: Calling `destroy()` on a broadcast variable destroys all data and metadata related
    to the specified broadcast variable. Once a broadcast variable has been destroyed,
    it cannot be used again and will have to be recreated all over again.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在广播变量上调用`destroy()`会销毁与指定广播变量相关的所有数据和元数据。一旦广播变量被销毁，就无法再次使用，必须重新创建。
- en: 'The following is an example of destroying broadcast variables:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是销毁广播变量的示例：
- en: '[PRE40]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: If an attempt is made to use a destroyed broadcast variable, an exception is
    thrown
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尝试使用已销毁的广播变量，则会抛出异常
- en: 'The following is an example of an attempt to reuse a destroyed broadcast variable:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是尝试重用已销毁的广播变量的示例：
- en: '[PRE41]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Thus, broadcast functionality can be use to greatly improve the flexibility
    and performance of Spark jobs.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，广播功能可以用于大大提高Spark作业的灵活性和性能。
- en: Accumulators
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 累加器
- en: Accumulators are shared variables across executors typically used to add counters
    to your Spark program. If you have a Spark program and would like to know errors
    or total records processed or both, you can do it in two ways. One way is to add
    extra logic to just count errors or total records, which becomes complicated when
    handling all possible computations. The other way is to leave the logic and code
    flow fairly intact and add Accumulators.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 累加器是跨执行程序共享的变量，通常用于向Spark程序添加计数器。如果您有一个Spark程序，并且想要知道错误或总记录数或两者，可以通过两种方式实现。一种方法是添加额外的逻辑来仅计算错误或总记录数，当处理所有可能的计算时变得复杂。另一种方法是保持逻辑和代码流相当完整，并添加累加器。
- en: Accumulators can only be updated by adding to the value.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 累加器只能通过将值添加到值来更新。
- en: The following is an example of creating and using a long Accumulator using Spark
    Context and the `longAccumulator` function to initialize a newly created accumulator
    variable to zero. As the accumulator is used inside the map transformation, the
    Accumulator is incremented. At the end of the operation, the Accumulator holds
    a value of 351.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用Spark上下文和`longAccumulator`函数创建和使用长累加器的示例，以将新创建的累加器变量初始化为零。由于累加器在map转换内部使用，因此累加器会递增。操作结束时，累加器保持值为351。
- en: '[PRE42]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'There are inbuilt accumulators which can be used for many use cases:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 有内置的累加器可用于许多用例：
- en: '`LongAccumulator`: for computing sum, count, and average of 64-bit integers'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LongAccumulator`：用于计算64位整数的总和、计数和平均值'
- en: '`DoubleAccumulator`: for computing sum, count, and averages for double precision
    floating numbers.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DoubleAccumulator`：用于计算双精度浮点数的总和、计数和平均值。'
- en: '`CollectionAccumulator[T]` : for collecting a list of elements'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CollectionAccumulator[T]`：用于收集元素列表'
- en: All the preceding Accumulators are built on top of the `AccumulatorV2` class.
    By following the same logic, we can potentially build very complex and customized
    Accumulators to use in our project.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 所有前面的累加器都是建立在`AccumulatorV2`类之上的。通过遵循相同的逻辑，我们可以潜在地构建非常复杂和定制的累加器来在我们的项目中使用。
- en: 'We can build a custom accumulator by extending the `AccumulatorV2` class. The
    following is an example showing the necessary functions to implement. `AccumulatorV2[Int,
    Int]` shown in the following code means that the Input and Output are both of
    Integer type:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过扩展`AccumulatorV2`类来构建自定义累加器。以下是一个示例，显示了实现所需函数的必要性。在下面的代码中，`AccumulatorV2[Int,
    Int]`表示输入和输出都是整数类型：
- en: '[PRE43]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Next, we will look at a practical example of a custom accumulator. Again, we
    shall use the `statesPopulation` CSV file for this. Our goal is to accumulate
    the sum of year and sum of population in a custom accumulator.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一个自定义累加器的实际例子。同样，我们将使用`statesPopulation` CSV文件。我们的目标是在自定义累加器中累积年份的总和和人口的总和。
- en: '**Step 1\. Import the package containing the AccumulatorV2 class:**'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1. 导入包含AccumulatorV2类的包：**'
- en: '[PRE44]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**Step 2\. Case class to contain the Year and Population:**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2. 包含年份和人口的Case类：**'
- en: '[PRE45]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Step 3\. StateAccumulator class extends AccumulatorV2:**'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3. StateAccumulator类扩展AccumulatorV2：**'
- en: '[PRE46]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '**Step 4\. Create a new StateAccumulator and register the same with SparkContext:**'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4. 创建一个新的StateAccumulator并在SparkContext中注册：**'
- en: '[PRE47]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '**Step 5\. Read the statesPopulation.csv as an RDD:**'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5. 将statesPopulation.csv作为RDD读取：**'
- en: '[PRE48]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '**Step 6\. Use the StateAccumulator:**'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6. 使用StateAccumulator：**'
- en: '[PRE49]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '**Step 7\. Now, we can examine the value of the StateAccumulator:**'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7. 现在，我们可以检查StateAccumulator的值：**'
- en: '[PRE50]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In this section, we examined accumulators and how to build a custom accumulator.
    Thus, using the preceding illustrated example, you can create complex accumulators
    to meet your needs.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们研究了累加器以及如何构建自定义累加器。因此，使用前面举例的例子，您可以创建复杂的累加器来满足您的需求。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed the many types of RDDs, such as `shuffledRDD`,
    `pairRDD`, `sequenceFileRDD`, `HadoopRDD`, and so on. We also looked at the three
    main types of aggregations, `groupByKey`, `reduceByKey`, and `aggregateByKey`.
    We looked into how partitioning works and why it is important to have a proper
    plan around partitioning to increase the performance. We also looked at shuffling
    and the concepts of narrow and wide dependencies which are basic tenets of how
    Spark jobs are broken into stages. Finally, we looked at the important concepts
    of broadcast variables and accumulators.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们讨论了许多类型的RDD，比如`shuffledRDD`，`pairRDD`，`sequenceFileRDD`，`HadoopRDD`等等。我们还看了三种主要的聚合类型，`groupByKey`，`reduceByKey`和`aggregateByKey`。我们研究了分区是如何工作的，以及为什么围绕分区需要一个合适的计划来提高性能。我们还研究了洗牌和窄依赖和宽依赖的概念，这些是Spark作业被分成阶段的基本原则。最后，我们看了广播变量和累加器的重要概念。
- en: The true power of the flexibility of RDDs makes it easy to adapt to most use
    cases and perform the necessary operations to accomplish the goal.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: RDD的灵活性使其易于适应大多数用例，并执行必要的操作以实现目标。
- en: In the next chapter, we will switch gears to the higher layer of abstraction
    added to the RDDs as part of the Tungsten initiative known as DataFrames and Spark
    SQL and how it all comes together in the [Chapter 8](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c),
    *Introduce a Little Structure – Spark SQL*.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转向RDD的更高抽象层，作为Tungsten计划的一部分添加到RDD中的DataFrame和Spark SQL，以及它们如何在[第8章](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c)
    *引入一点结构 - Spark SQL*中结合在一起。
