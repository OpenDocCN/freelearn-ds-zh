- en: An Introduction to Machine Learning Concepts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习概念简介
- en: Machine learning has become a commonplace topic in our day-to-day lives. The
    advancement in the field has been so dramatic that today, even cell phones incorporate
    advanced machine learning and artificial intelligence-related facilities, capable
    of responding and taking actions based on human instructions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习已经成为我们日常生活中司空见惯的话题。该领域的发展如此戏剧性，以至于今天，甚至手机都集成了先进的机器学习和人工智能相关设施，能够根据人类指令做出响应和采取行动。
- en: A subject that was once limited to university classrooms has transformed into
    a full-fledged industry, pervading our daily lives in ways we could not have envisioned
    even just a few years ago.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经只限于大学课堂的一个学科，如今已经发展成为一个完全成熟的行业，以一种我们几年前无法想象的方式渗透到我们的日常生活中。
- en: The aim of this chapter is to introduce the reader to the underpinnings of machine
    learning and explain the concepts in simple, lucid terms that will help readers
    become familiar with the core ideas in the subject. We'll start off with a high-level
    overview of machine learning, and explain the different categories and how to
    distinguish them. We'll explain some of the salient concepts in machine learning,
    such as data pre-processing, feature engineering, and variable importance. The
    next chapter will go into more detail regarding individual algorithms and theoretical
    machine learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是向读者介绍机器学习的基础知识，并以简单明了的术语解释概念，帮助读者熟悉该学科的核心思想。我们将从机器学习的高层概述开始，解释不同的类别以及如何加以区分。我们将解释机器学习中一些显著的概念，如数据预处理、特征工程和变量重要性。下一章将更详细地介绍单个算法和理论机器学习。
- en: We'll conclude with exercises that leverage real-world datasets to perform machine
    learning operations using R.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用R来执行机器学习操作的真实数据集来结束本章。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中涵盖以下主题：
- en: What is machine learning?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: The popular emergence
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的出现
- en: Machine learning, statistics, and artificial intelligence (AI)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习、统计学和人工智能（AI）
- en: Categories of machine learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的类别
- en: Core concepts in machine learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的核心概念
- en: Machine learning tutorial
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习教程
- en: What is machine learning?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: '**Machine learning** is not a new subject; it has existed in academia for well
    over 70 years as a formal discipline, but known by different names: statistics,
    and more generally mathematics, then **artificial intelligence** (**AI**), and
    today as machine learning. While the other related subject areas of statistics
    and AI are just as prevalent, machine learning has carved out a separate niche
    and become an independent discipline in and of itself.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**并不是一个新的学科；它作为一个正式学科已经存在了70多年，但是以不同的名称存在：统计学，更普遍的是数学，然后是**人工智能**（**AI**），今天是机器学习。虽然统计学和人工智能等其他相关学科同样普遍，但是机器学习已经开辟了一个独立的领域，成为一个独立的学科。'
- en: In simple terms, machine learning involves predicting future events based on
    historical data. We see it manifested in our day-to-day lives and indeed we employ,
    knowingly or otherwise, principles of machine learning on a daily basis.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，机器学习涉及基于历史数据预测未来事件。我们在日常生活中看到它的体现，无论我们是否知情，我们都会每天应用机器学习的原则。
- en: When we casually comment on whether a movie will succeed at the box office using
    our understanding of the popularity of the individuals in the lead roles, we are
    applying machine learning, albeit subconsciously. Our understanding of the characters
    in the lead roles has been shaped over years of watching movies where they appeared.
    And, when we make a determination of the success of a future movie featuring the
    same person, we are using historical information to make an assessment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们随意评论一部电影是否会在票房上成功，使用我们对主演的人气的理解时，我们正在应用机器学习，尽管是下意识地。我们对主演角色的理解是在多年观看他们出演的电影中形成的。当我们对未来出演同一人的电影的成功做出评估时，我们是在利用历史信息进行评估。
- en: As another example, if we had data on temperature, humidity, and precipitation
    (rain) over a period of say, 12 months, can we use that information to predict
    whether it will rain today, given information on temperature and humidity?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，如果我们有关于温度、湿度和降水（雨）的数据，比如12个月的数据，我们能否利用这些信息来预测今天是否会下雨，给定温度和湿度的信息？
- en: This is akin to common regression problems found in statistics. But, machine
    learning involves applying a much higher level of rigor to the exercise to reach
    a conclusive decision based not only on theoretical calculations, but verification
    of the calculations hundreds or thousands of times using iterative methods before
    reaching a conclusion.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于统计学中常见的回归问题。但是，机器学习涉及对练习应用更高级别的严谨性，以便基于不仅仅是理论计算，而且是使用迭代方法进行数百次甚至数千次验证计算后得出结论。
- en: It should be noted and clarified here that the term *machine learning* relates
    to algorithms or programs that are executed typically on a computing device whose
    objective it is to predict outcomes. The algorithms build mathematical models
    that can then be used to make predictions. It is a common misconception that machine
    learning quite literally refers to a *machine* that is *learning*. The actual
    implication, as just explained, is much less dramatic.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 需要在这里指出和澄清的是，术语“机器学习”指的是通常在计算设备上执行的旨在预测结果的算法或程序。这些算法构建数学模型，然后可以用来进行预测。人们普遍错误地认为机器学习实际上是指一个“学习”的“机器”。正如刚才解释的那样，实际含义要逊色得多。
- en: The evolution of machine learning
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的演变
- en: The timeline of machine learning, as available on Wikipedia ([https://en.wikipedia.org/wiki/Timeline_of_machine_learning](https://en.wikipedia.org/wiki/Timeline_of_machine_learning)),
    provides a succinct and insightful overview of the evolution of the field. The
    roots can be traced back to as early as the mid-1700s, when Thomas Bayes presented
    his paper on *inverse probability* at the Royal Society of London. Inverse probability,
    more commonly known today as probability distribution, deals with the problem
    of determining the state of a system given a prior set of events. For example,
    if a box contained milk chocolate and white chocolate, you took out a few at random,
    and received two milk and three white chocolates, can we infer how many of each
    chocolate there are in the box?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的时间线，如维基百科上所述（[https://en.wikipedia.org/wiki/Timeline_of_machine_learning](https://en.wikipedia.org/wiki/Timeline_of_machine_learning)），提供了该领域演变的简明而富有洞察力的概述。其根源可以追溯到18世纪中期，当时托马斯·贝叶斯在伦敦皇家学会上发表了他关于*逆概率*的论文。逆概率，今天更常被称为概率分布，涉及确定给定一组先前事件的系统状态的问题。例如，如果一个盒子里有牛奶巧克力和白巧克力，你随机拿出几个，得到两块牛奶巧克力和三块白巧克力，我们能推断盒子里有多少块巧克力吗？
- en: In other words, what can we infer about the unknown given a few points of data
    with which we can postulate a formal theory? Bayes' work was developed further
    into Bayes' Theorem by Pierre-Simon Laplace in his text, *Théorie Analytique des
    Probabilités*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们能根据我们可以假设一个正式理论的一些数据点推断出未知的情况吗？贝叶斯的工作被皮埃尔-西蒙·拉普拉斯进一步发展成为贝叶斯定理，收录在他的著作《概率分析理论》中。
- en: In the early 1900s, Andrey Markov's analysis of Pushkin's Poem, Eugeny Onegin,
    to determine the alliteration of consonants and vowels in Russian literature,
    led to the development of a technique known as Markov Chains, used today to model
    complex situations involving random events. Google's PageRank algorithm implements
    a form of Markov Chains.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪初，安德烈·马尔可夫对普希金的诗《叶甫盖尼·奥涅金》的分析，以确定俄罗斯文学中辅音和元音的押韵，导致了一种称为马尔可夫链的技术的发展，该技术今天用于对涉及随机事件的复杂情况进行建模。谷歌的PageRank算法实现了马尔可夫链的一种形式。
- en: 'The first formal application of machine learning, or more generally, AI, and
    its eventual emergence as a discipline, should be attributed to Alan Turing. He
    developed the Turing Test - a way to determine whether a machine is intelligent
    enough to mimic human behavior. Turing presented this in his paper, *Computing
    Machinery and Intelligence*, which starts out with the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的第一个正式应用，或者更普遍地说，人工智能的最终出现作为一门学科，应归功于艾伦·图灵。他开发了图灵测试——一种确定机器是否足够智能以模仿人类行为的方法。图灵在他的论文《计算机器械与智能》中提出了这一点，论文开头是这样的：
- en: I propose to consider the question, "Can machines think?" This should begin
    with definitions of the meaning of the terms "machine" and "think." The definitions
    might be framed so as to reflect so far as possible the normal use of the words,
    but this attitude is dangerous, If the meaning of the words "machine" and "think"
    are to be found by examining how they are commonly used it is difficult to escape
    the conclusion that the meaning and the answer to the question, "Can machines
    think?" is to be sought in a statistical survey such as a Gallup poll. But this
    is absurd. Instead of attempting such a definition I shall replace the question
    by another, which is closely related to it and is expressed in relatively unambiguous
    words.[...]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议考虑这个问题，“机器能思考吗？”这应该从定义“机器”和“思考”的含义开始。这些定义可能被构造得尽可能反映这些词的正常用法，但这种态度是危险的。如果通过检查它们通常的用法来找到“机器”和“思考”的含义，很难逃脱这样的结论，即问题“机器能思考吗？”的含义和答案应该在统计调查中寻找，比如盖洛普民意调查。但这是荒谬的。我不打算尝试这样的定义，而是用另一个问题来代替它，这个问题与它密切相关，并用相对明确的词语表达。
- en: 'Later in the paper, Turing writes:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图灵在论文的后面写道：
- en: '*The original question, "Can machines think?" I believe to be too meaningless
    to deserve discussion. Nevertheless I believe that at the end of the century the
    use of words and general educated opinion will have altered so much that one will
    be able to speak of machines thinking without expecting to be contradicted. I
    believe further that no useful purpose is served by concealing these beliefs.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*“原始问题‘机器能思考吗？’我认为太毫无意义，不值得讨论。尽管如此，我相信在本世纪末，词语的使用和一般受过教育的观点将发生如此大的变化，以至于人们将能够谈论机器思考而不期望遭到反驳。我进一步相信，隐藏这些信念是没有任何有益的目的。”*'
- en: Turing's work on AI was followed by a series of seminal events in machine learning
    and AI. The first neural network was developed by Marvin Misky in 1951, Arthur
    Samuel began his work on the first machine learning programs that played checkers
    in 1952, and Rosenblatt invented the perceptron, a fundamental unit of neural
    networks, in 1957\. Pioneers such as Leo Breiman, Jerome Friedman, Vladimir Vapnik
    and Alexey Chervonenkis, Geoff Hinton, and YannLeCun made significant contributions
    through the late 1990s to bring machine learning into the limelight. We are greatly
    indebted to their work and contributions, which have made machine learning stand
    out as a distinct area of research today.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图灵在人工智能领域的工作之后，机器学习和人工智能出现了一系列重要事件。1951年，马文·米斯基开发了第一个神经网络，阿瑟·塞缪尔在1952年开始了第一个下棋的机器学习程序的工作，罗森布拉特在1957年发明了感知器，这是神经网络的基本单元。杰出人物如利奥·布雷曼、杰罗姆·弗里德曼、弗拉迪米尔·瓦普尼克和阿列克谢·切尔沃年基斯、杰夫·辛顿和杨立昆通过20世纪90年代末做出了重大贡献，使机器学习成为当今独立的研究领域。我们对他们的工作和贡献深表感激，这使得机器学习在当今的研究领域中脱颖而出。
- en: In 1997, IBM's Deep Blue beat Kasparov and it immediately became a worldwide
    sensation. The ability of a machine to beat the world's top chess champion was
    no ordinary achievement. The event gave some much-needed credibility to machine
    learning as a formidable contender for the intelligent machines that Turing envisaged.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 1997年，IBM的深蓝击败了卡斯帕罗夫，这立刻成为了全球轰动的事件。一台机器能够击败世界顶级国际象棋冠军并非寻常的成就。这一事件为机器学习赢得了一些急需的可信度，使其成为图灵所设想的智能机器的有力竞争者。
- en: Factors that led to the success of machine learning
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习成功的因素
- en: 'Given machine learning, as a subject, has existed for many decades, it begs
    the question: why hadn''t it become as popular as it is today much sooner? Indeed,
    the theories of complex machine learning algorithms such as neural networks were
    well known by the late 1990s, and the foundation had been established well before
    that in the theoretical realm.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于机器学习作为一个学科已经存在了几十年，人们不禁要问：为什么它没有比今天更早地变得如此受欢迎？事实上，诸如神经网络之类的复杂机器学习算法的理论在20世纪90年代晚期就已经广为人知，而在理论领域，基础也早已奠定。
- en: 'There are a few factors that can be attributed to the success of machine learning:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习成功的几个因素：
- en: '**The Internet**: The web played a critical role in democratizing information
    and connecting people in an unprecedented way. It made the exchange of information
    simple in a way that could not have been achieved through the pre-existing methods
    of print media communication. Not only did the web transform and revolutionize
    the dissemination of information, it also opened up new opportunities. Google''s
    PageRank, as mentioned earlier, was one of the first large-scale and highly visible
    successes in the application of statistical models to develop a highly successful
    web enterprise.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**互联网**：网络在民主化信息和以前所未有的方式连接人们方面发挥了关键作用。它使信息交换变得简单，这是通过印刷媒体传播信息的现有方法所无法实现的。网络不仅转变和革新了信息传播，还开辟了新的机会。正如前面提到的，谷歌的PageRank是将统计模型应用于开发高度成功的网络企业的最早大规模和高度可见的成功之一。'
- en: '**Social media**: While the web provided a platform for communication, it lacked
    a level of flexibility akin to how people interacted with one another in the real
    world. There was a noticeable, but understated, and arguably unexplored gap. Tools
    such as IRC and Usenet were the precursors to social network websites such as
    Myspace, which was one of the first web-based platforms intended to create personal
    networks. By early-mid 2000, Facebook had emerged as the leader in social networking.
    These platforms provided a unique opportunity to leverage the Internet to collect
    data at an individual level. Each user left a trail of messages, ripe for collection
    and analysis using Natural Language Processing and other techniques.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社交媒体**：虽然网络提供了一个交流平台，但缺乏与现实世界中人们互动的灵活性。有一个明显的但低调的、可以说是未被开发的差距。诸如IRC和Usenet之类的工具是社交网络网站的前身，比如Myspace，这是最早用于创建个人网络的基于网络的平台之一。到了21世纪初至中期，Facebook成为社交网络的领导者。这些平台提供了一个独特的机会，利用互联网以个人层面收集数据。每个用户留下了一串信息，可以使用自然语言处理和其他技术进行收集和分析。'
- en: '**Computing hardware**: Hardware used for computers developed at an exponential
    rate. Machine learning algorithms are inherently compute and resource intensive,
    that is, they require powerful CPUs, fast disks, and high memory depending on
    the size of data. The invention of new ways to store data on **solid state drives**
    (**SSDs**) was a leap from the erstwhile process of storing on spinning hard drives.
    Faster access meant that data could be delivered to the CPU at a much faster rate
    and reduce the I/O bottleneck that has traditionally been a weak area in computing.
    Faster CPUs meant it was possible to perform hundreds and thousands of iterations
    demanded by machine learning algorithms in a timely manner. Finally, the demand
    led to the reduction in prices for computing resources, allowing more people to
    be able to afford buying computing hardware that was prohibitively expensive.
    Algorithms existed, but the resources were finally able to execute them in a reasonable
    time and cost.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算硬件**：用于计算机的硬件以指数速度发展。机器学习算法本质上是计算和资源密集型的，也就是说，它们需要强大的CPU、快速的磁盘和根据数据大小的高内存。在**固态硬盘**（**SSD**）上存储数据的新方法是从以前的旋转硬盘存储方式中跨越的一大步。更快的访问意味着数据可以以更快的速度传递给CPU，并减少了传统计算中一直存在的I/O瓶颈。更快的CPU意味着可以及时执行机器学习算法所需的数百甚至数千次迭代。最后，需求导致了计算资源价格的降低，使更多人能够负担得起原本价格昂贵的计算硬件。算法是存在的，但资源最终能够以合理的时间和成本来执行它们。'
- en: '**Programming languages and packages**: Communities such as R and Python developers
    seized the opportunity, and individuals started releasing packages that exposed
    their work to a broader community of programmers. In particular, packages that
    provided machine learning algorithms became immediately popular and inspired other
    practitioners to release their individual code repositories, making platforms
    such as R a truly global collaborative effort. Today there are over 10,000 packages
    in R, up from 2000 in 2010.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编程语言和软件包**：R和Python开发者等社区抓住了机会，个人开始发布暴露他们的工作给更广泛的程序员社区的软件包。特别是提供机器学习算法的软件包立即受到欢迎，并激发了其他从业者发布他们的个人代码库，使得R等平台成为一个真正的全球协作努力。如今，R中有超过10,000个软件包，而2010年只有2000个。'
- en: Machine learning, statistics, and AI
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习、统计学和人工智能
- en: Machine learning is a term that has various synonyms - names that are the result
    of either marketing activities by corporates or just terms that have been used
    interchangeably. Although some may argue that they have different implications,
    they all ultimately refer to machine learning as a subject that facilitates the
    prediction of future events using historical information.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个有各种同义词的术语——这些名称是企业的营销活动的结果，或者只是可以互换使用的术语。尽管有人可能会争辩它们有不同的含义，但它们最终都指的是机器学习作为一门学科，它利用历史信息来预测未来事件。
- en: The commonly heard terms for machine learning include predictive analysis, predictive
    analytics, predictive modeling, and many others. As such, unless the entity that
    publishes material explaining their interpretation of the term and more specifically,
    how it is different, it is generally safe to assume that they are referring to
    machine learning. This is often a source of confusion among those new to the subject,
    largely due to the misuse and overuse of technical verbiage.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习常听到的术语包括预测分析、预测分析、预测建模等等。因此，除非发布材料的实体解释了他们对术语的解释，更具体地说明了它的不同之处，否则可以安全地假设他们是在指机器学习。这往往会让新手感到困惑，主要是由于技术术语的误用和滥用。
- en: Statistics, on the other hand, is a distinct subject area that has been well
    known for over 200 years. The word is derived from the new Latin, *statisticum
    collegium* (council of state, in English) and the Italian word *statista*, meaning
    statesman or politician. You can visit [https://en.wikipedia.org/wiki/History_of_statistics#Etymology](https://en.wikipedia.org/wiki/History_of_statistics#Etymology)
    for more details on this topic. Machine learning implements various statistical
    models, which due to the rigor of computation involved, is distinct from the branch
    of classical statistics.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，统计学是一个已经有200多年历史的独立学科领域。这个词源自新拉丁语*statisticum collegium*（英语中的国务院）和意大利语*statista*，意思是政治家或政治家。您可以访问[https://en.wikipedia.org/wiki/History_of_statistics#Etymology](https://en.wikipedia.org/wiki/History_of_statistics#Etymology)了解更多关于这个主题的详情。机器学习实现了各种统计模型，由于涉及到的计算严谨，它与经典统计学的分支有所不同。
- en: AI is also closely related to machine learning, but is a much broader subject.
    It can be loosely defined as systems (software/hardware) that, in the presence
    of uncertainties, can arrive at a concrete decision in (usually) a responsible
    and socially aware manner to attain a target end objective. In other words, AI
    aims to produce actions by systematically processing a situation that involves
    both known and unknown (latent) factors.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能也与机器学习密切相关，但它是一个更广泛的主题。它可以被宽泛地定义为在存在不确定性的情况下，能够以（通常）负责任和社会意识的方式做出具体决策，以达到目标终极目标的系统（软件/硬件）。换句话说，人工智能旨在通过系统地处理既包括已知又包括未知（潜在）因素的情况来产生行动。
- en: AI conjures up images of smart and sometimes rebellious robots in sci-fi movies,
    just as much as it reminds us of intelligent systems, such as IBM Watson, that
    can parse complex questions and process ambiguous statements to find concrete
    answers.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能唤起了智能机器人在科幻电影中的形象，就像它提醒我们智能系统，比如IBM Watson，可以解析复杂问题并处理模糊陈述以找到具体答案一样。
- en: Machine learning shares some of the same traits - the step-wise development
    of a model using training data, and measuring accuracy using test data. However,
    AI has existed for many decades and has been a familiar household term. Institutions
    in the US, such as Carnegie Mellon University, have led the way in establishing
    key principles and guidelines of AI.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与一些相同特征-使用训练数据逐步开发模型，并使用测试数据测量准确性。然而，人工智能已经存在了很多年，并且是一个家喻户晓的术语。美国的卡内基梅隆大学等机构一直在制定人工智能的关键原则和指导方针。
- en: The online resources/articles on AI versus machine learning do not seem to provide
    any conclusive answers on how they differ. However, the syllabus of AI courses
    at universities makes the differences very obvious. You can learn more about AI
    at [https://cs.brown.edu/courses/csci1410/lectures.html](https://cs.brown.edu/courses/csci1410/lectures.html).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 关于人工智能与机器学习的在线资源/文章似乎没有提供任何关于它们之间区别的定论。然而，大学的人工智能课程大纲使这些区别变得非常明显。您可以在[https://cs.brown.edu/courses/csci1410/lectures.html](https://cs.brown.edu/courses/csci1410/lectures.html)了解更多关于人工智能的信息。
- en: 'AI refers to a vast array of study areas that involve:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能涉及涉及的广泛研究领域：
- en: '**Constrained optimization**: Reach best possible results given a set of constraints
    or limitations in a given situation'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**受限优化**：在给定情况下，达到最佳结果，考虑一组约束或限制'
- en: '**Game theory**: For instance, zero-sum games, equilibrium, and others - taking
    a measured decision based on how the decision can affect future decisions and
    impact desired end goals'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**博弈论**：例如，零和游戏，均衡等-根据决策如何影响未来决策和影响期望的最终目标来做出权衡决策'
- en: '**Uncertainty/Bayes'' rule**: Given prior information, what is the likelihood
    of this happening given something else has already happened'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不确定性/贝叶斯定理**：在先验信息的情况下，发生这种情况的可能性是多少，考虑到已经发生了其他事情'
- en: '**Planning**: Formulating a plan of action = a set of paths (graph) to tackle
    a situation/reach an end goal'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划**：制定行动计划=一组路径（图），以应对情况/达到最终目标'
- en: '**Machine learning**: The implementation (realization) of the preceding goals
    by using algorithms that are designed to handle uncertainties and imitate human
    reasoning. The machine learning algorithms generally used for AI include:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习**：通过使用旨在处理不确定性并模仿人类推理的算法来实现（实现）前述目标。通常用于人工智能的机器学习算法包括：'
- en: Neural networks/deep learning (find hidden factors)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络/深度学习（发现隐藏因素）
- en: Natural language processing (NLP) (understand context using tenor, linguistics,
    and such)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）（使用语气，语言学等理解上下文）
- en: Visual object recognition
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉对象识别
- en: Probabilistic models (for example, Bayes' classifiers)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率模型（例如，贝叶斯分类器）
- en: Markov decision processes (decisions for random events, for example, gambling)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（例如，随机事件的决策，例如赌博）
- en: Various other machine learning Algorithms (clustering, SVM)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种其他机器学习算法（聚类、支持向量机）
- en: '**Sociology**: A study of how machine learning decisions affect society and
    take remedial steps to correct issues'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社会学**：研究机器学习决策如何影响社会，并采取补救措施纠正问题'
- en: Categories of machine learning
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的分类
- en: Arthur Samuel coined the term **machine learning** in 1959 while at IBM. A popular
    definition of machine learning is due to Arthur, who, it is believed, called machine
    learning *a field of computer science that gives computers the ability to learn
    without being explicitly programmed*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 1959年，Arthur Samuel在IBM工作时创造了**机器学习**这个术语。机器学习的一个流行定义归功于Arthur，据信他称机器学习为*一门计算机科学领域，使计算机能够在没有明确编程的情况下学习*。
- en: Tom Mitchell, in 1998, added a more specific definition to machine learning
    and called it a, study of algorithms that improve their performance P at some
    task T with experience E.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 1998年，Tom Mitchell对机器学习增加了更具体的定义，并称其为一种研究算法的学科，这些算法通过经验E在某个任务T上提高其性能P。
- en: A simple explanation would help to illustrate this concept. By now, most of
    us are familiar with the concept of spam in emails. Most email accounts also contain
    a separate folder known as **Junk**, **Spam**, or a related term. A cursory check
    of the folders will usually indicate the presence of several emails, many of which
    were presumably unsolicited and contain meaningless information.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的解释可以帮助说明这个概念。现在，我们大多数人都熟悉电子邮件中的垃圾邮件概念。大多数电子邮件账户也包含一个名为**垃圾邮件**、**垃圾**或类似术语的单独文件夹。对文件夹的粗略检查通常会显示出许多邮件，其中许多可能是未经请求的并包含无意义的信息。
- en: The mere task of categorizing emails as spam and moving them to a folder involves
    the application of machine learning. Andrew Ng highlighted this elegantly in his
    popular MOOC course on machine learning.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将电子邮件分类为垃圾邮件并将其移动到文件夹中的简单任务也涉及机器学习的应用。Andrew Ng在他的流行机器学习MOOC课程中优雅地强调了这一点。
- en: 'In Mitchell''s terms, the spam classification process involves:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在Mitchell的术语中，垃圾邮件分类过程涉及：
- en: '**Task T**: Classifying emails as spam/not spam'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务T**：将电子邮件分类为垃圾邮件/非垃圾邮件'
- en: '**Performance P**: Number of emails accurately identified as spam'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能P：准确识别为垃圾邮件的数量
- en: '**Experience E**: The model is provided emails that have been marked as spam/not
    spam and uses that information to determine whether a new email is spam or not'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经验E：模型提供了被标记为垃圾邮件/非垃圾邮件的电子邮件，并利用这些信息来确定新邮件是否为垃圾邮件
- en: 'Broadly speaking, there are two distinct types of machine learning:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上讲，机器学习有两种明显的类型：
- en: Supervised machine learning
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督式机器学习
- en: Unsupervised machine learning
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督式机器学习
- en: We shall discuss them in turn here.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里依次讨论它们。
- en: Supervised and unsupervised machine learning
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督式和无监督式机器学习
- en: Let us start with supervised machine learning first.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从监督式机器学习开始。
- en: Supervised machine learning
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督式机器学习
- en: '**Supervised machine learning** refers to machine learning exercises that involve
    predicting outcomes with labelled data. Labelled data simply refers to the fact
    that the dataset we are using to make the predictions (as well as the outcome
    we will predict) has a definite value (irrespective of what it is). For instance,
    classifying emails as spam or not spam, predicting temperature, and identifying
    faces from images are all examples of supervised machine learning.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督式机器学习**指的是涉及使用标记数据预测结果的机器学习练习。标记数据简单地指的是我们用来进行预测的数据集（以及我们将要预测的结果）具有明确的值（不管是什么）。例如，将电子邮件分类为垃圾邮件或非垃圾邮件、预测温度和从图像中识别人脸都是监督式机器学习的例子。'
- en: Vehicle Mileage, Number Recognition and other examples
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 车辆里程、数字识别和其他例子
- en: Given a dataset containing information on miles per gallon, number of cylinders,
    and such of various cars, can we predict what the value for miles per gallon would
    be if we only had the other values available?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含有关每加仑英里数、汽缸数等各种汽车信息的数据集，如果我们只有其他值可用，我们能预测每加仑英里数的值吗？
- en: 'In this case, our outcome is `mpg` and we are using the other variables of
    `cyl` (Cylinders), `hp` (Horsepower), `gear` (number of gears), and others to
    build a model that can then be applied against a dataset where the values for
    mpg are marked as `MISSING`. The model reads the information in these columns
    in the first five rows of the data and, based on that information, predicts what
    the value for `mpg` would be in the other rows, as shown in the following image:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的结果是`mpg`，我们使用`cyl`（汽缸数）、`hp`（马力）、`gear`（齿轮数）等其他变量来构建一个模型，然后应用于一个数据集，其中`mpg`的值标记为`MISSING`。模型读取数据的前五行中这些列的信息，并根据这些信息预测其他行中`mpg`的值，如下图所示：
- en: '![](img/5c1b160c-2b40-4223-9d7c-b9fb43c86b0d.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c1b160c-2b40-4223-9d7c-b9fb43c86b0d.png)'
- en: 'The reason this is considered supervised is that in the course of building
    our machine learning model, we provided the model with information on what the
    outcome was. Other examples include:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以被认为是监督式的是因为在构建我们的机器学习模型的过程中，我们向模型提供了关于结果的信息。其他例子包括：
- en: '**Recognizing letters and numbers**: In such cases, the input to the model
    are the images, say of letters and numbers, and the outcome is the alpha-numeric
    value shown on the image. Once the model is built, it can then be used against
    pictures to recognize and predict what numbers are shown in the picture. A simple
    example, but very powerful. Imagine if you were given 100,000 images of houses
    with house numbers. The manual way of identifying the house numbers would be to
    go through each image individually and write down the numbers. A machine learning
    model allows us to completely automate the entire operation. Instead of having
    to manually go through individual images, you could simply run the model against
    the images and get the results in a very short amount of time.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别字母和数字**：在这种情况下，模型的输入是图像，比如字母和数字的图像，结果是图像上显示的字母数字值。构建模型后，可以用于识别和预测图像中显示的数字。这是一个简单的例子，但非常强大。想象一下，如果你拿到了10万张带有门牌号码的房屋图片。手动识别门牌号码的方式是逐个查看每张图片并写下号码。机器学习模型使我们能够完全自动化整个操作。你可以简单地运行模型来识别图像，以极短的时间内获得结果，而不必手动查看每个图像。'
- en: '**Self-driving autonomous cars**: The input to the algorithms are images where
    the objects in the image have been identified, for example, person, street sign,
    car, trees, shops, and other elements. The algorithm *learns* to recognize and
    differentiate among different elements once a sufficient number of images have
    been shown and thereafter given an unlabeled image, that is, an image where the
    objects have not been identified is able to recognize them individually. To be
    fair, this is a highly simplified explanation of a very complex topic, but the
    overall principle is the same.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动驾驶汽车**：算法的输入是图像，图像中的对象已被识别，例如人、街道标志、汽车、树木、商店和其他元素。一旦展示了足够数量的图像，并且给出了一个未标记的图像，也就是对象尚未被识别的图像，算法就能够识别它们。公平地说，这是对一个非常复杂的主题的高度简化的解释，但总体原则是相同的。'
- en: 'MNIST Dataset used for number recognition:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 用于数字识别的MNIST数据集：
- en: '![](img/18749693-441f-4359-8c23-45af6249b0c1.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18749693-441f-4359-8c23-45af6249b0c1.png)'
- en: Unsupervised machine learning
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督机器学习
- en: '**Unsupervised machine learning** involves datasets that do not have labeled
    outcomes. Taking the example of predicting mpg values for cars, in an unsupervised
    exercise, our dataset would have looked as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督机器学习**涉及没有标记结果的数据集。以预测汽车的每加仑英里数为例，在无监督练习中，我们的数据集将如下所示：'
- en: '![](img/22338df9-3af6-4217-a4b9-be7b4556b7db.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22338df9-3af6-4217-a4b9-be7b4556b7db.png)'
- en: If all the outcomes are *missing*, it would be impossible to know what the values
    might have been. Recall that the primary premise of machine learning is to use
    historical information to make predictions on datasets whose outcome is not known.
    But, if the historical information itself does not have any identified outcomes,
    then it would not be possible to build a model. Without knowing any other information,
    the values of mpg in the table could be all 0 or all 100; it is not possible to
    tell, as we do not have any data point that will help lead us to the value.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有的结果都*缺失*，那么就不可能知道这些值可能是什么。请记住，机器学习的主要前提是利用历史信息对结果未知的数据集进行预测。但是，如果历史信息本身没有任何确定的结果，那么就不可能建立模型。在不知道任何其他信息的情况下，表中的mpg值可能全部为0或全部为100；我们无法判断，因为没有任何数据点可以帮助我们得出这个值。
- en: This is where *unsupervised* machine learning gets applied. In this type of
    machine learning, we are not trying to predict outcomes. Rather, we are trying
    to determine which items are most similar to one another.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是*无监督*机器学习的应用。在这种类型的机器学习中，我们并不试图预测结果。相反，我们试图确定哪些项目彼此最相似。
- en: A common name for such an exercise is *clustering*, that is, we are attempting
    to find *clusters* or groups of records that are most similar to one another.
    Where can we use this information and what are some examples of unsupervised learning?
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这种练习的一个常见名称是*聚类*，也就是说，我们试图找到彼此最相似的记录的*簇*或组。我们可以在哪些地方使用这些信息，无监督学习的一些例子是什么？
- en: 'There are various news aggregators on the web - sites that do not themselves
    publish information, but collect information from other news sources. One such
    aggregator is Google News. If, say, we had to search for information on the last
    images taken by the satellite Cassini of Saturn, we could do a simple search for
    the phrase on Google News [https://news.google.com/news/?gl=US&amp;ned=us&amp;hl=en](https://news.google.com/news/?gl=US&ned=us&hl=en).
    An example is shown here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 网络上有各种新闻聚合器 - 这些网站本身不发布信息，而是从其他新闻来源收集信息。谷歌新闻就是这样的一个聚合器。比如，如果我们要搜索卡西尼号对土星拍摄的最新图像的信息，我们可以在谷歌新闻上简单搜索这个短语[https://news.google.com/news/?gl=US&amp;ned=us&amp;hl=en](https://news.google.com/news/?gl=US&ned=us&hl=en)。这里有一个示例：
- en: '![](img/4ad124ec-c378-451b-a50d-c4da5aab8847.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ad124ec-c378-451b-a50d-c4da5aab8847.png)'
- en: Notice that there is a link for View all at the bottom of the news articles.
    Clicking the link will take you to a page with all the other related news articles.
    Surely, Google didn't manually classify the articles as belonging to the specific
    search term. In fact, Google doesn't know in advance what the user will search
    for. The search term could have well been *images of Saturn rings from space*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在新闻文章底部有一个“查看全部”的链接。点击该链接将带您到包含所有其他相关新闻文章的页面。当然，谷歌并没有手动将文章分类为特定的搜索词。事实上，谷歌事先并不知道用户会搜索什么。搜索词本来也可能是“太空中土星环的图片”。
- en: So, how does Google know which articles belong to a specific search term? The
    answer lies in the application of clustering or principles of unsupervised learning.
    Unsupervised learning examines the attributes of a specific dataset in order to
    determine which articles are most similar to one another. To do this, the algorithm
    doesn't even need to know the contextual background.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，谷歌是如何知道哪些文章属于特定的搜索词的呢？答案在于聚类或无监督学习原则的应用。无监督学习检查特定数据集的属性，以确定哪些文章彼此最相似。为了做到这一点，算法甚至不需要知道上下文背景。
- en: Suppose you were given two sets of books with no covers, a set of books on gardening
    and a set of books on computer programming. Although you may not know the title
    of the book, it would be fairly easy to distinguish books on computers from books
    on gardening. One set of books would have an overwhelming number of terms related
    to computing, while the other would have an overwhelming number of terms related
    to plants. To make the distinction that there were two distinct categories of
    books would not be difficult just by virtue of the images in the books, even for
    a reader who, let's assume, is not aware of either computers or gardening.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你拿到了两套没有封面的书，一套是关于园艺的书，另一套是关于计算机编程的书。尽管你可能不知道书的标题，但很容易区分计算机书和园艺书。一套书会有大量与计算机相关的术语，而另一套会有大量与植物相关的术语。仅凭书中的图片就能区分出两种不同的书类别，即使是一个不了解计算机或园艺的读者也不难。
- en: Other examples of unsupervised machine learning include detection of malignant
    and non-malignant tumors, and gene sequencing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习的其他示例包括检测恶性和非恶性肿瘤以及基因测序。
- en: Subdividing supervised machine learning
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 细分监督机器学习
- en: 'Supervised machine learning can be further subdivided into exercises that involve
    either of the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习可以进一步细分为以下练习之一：
- en: '**Classification**'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: '**Regression**'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**'
- en: The concepts are quite straightforward.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念非常简单。
- en: Classification involves a machine learning task that has a discrete outcome
    - a **categorical** outcome. All **nouns** are categorical variables, such as
    fruits, trees, color, and true/false.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 分类涉及具有离散结果的机器学习任务 - **分类**结果。所有**名词**都是分类变量，例如水果、树木、颜色和真/假。
- en: The outcome variables in classification exercises are also known as **discrete
    or categorical variables**.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 分类练习中的结果变量也被称为**离散或分类变量**。
- en: 'Some examples include:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一些例子包括：
- en: Identifying the fruit given size, weight, and shape
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据大小、重量和形状确定水果
- en: Identifying numbers given a set of images of numbers (as shown in the earlier
    chapter)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一组数字图像的数字（如前一章所示）
- en: Identifying objects on the streets
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别街道上的物体
- en: Identifying playing cards as diamonds, spades, hearts and clubs
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别红心、黑桃、红桃和梅花的扑克牌
- en: Identifying the class rank of a student based on the student's grade
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据学生的成绩确定学生的班级排名
- en: The last one might not seem obvious, but a rank, that is, 1^(st), 2^(nd), 3^(rd)
    denotes a fixed category. A student could rank, say, 1^(st) or 2^(nd), but not
    have a rank of 1.5!
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个可能看起来不明显，但是排名，即1^(st)、2^(nd)、3^(rd)表示一个固定的类别。一个学生可以排名，比如1^(st)或2^(nd)，但不能有1.5的排名！
- en: 'Images of some atypical classification examples are shown below:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示了一些非典型的分类示例的图像：
- en: '| ![](img/06771d1b-3810-4a9b-a9d2-5f5f6c48be27.png)Classification of different
    types of fruits | ![](img/02f51c3d-2e4f-46dc-b332-3cbac84c3bfe.png)Classification
    of playing cards: diamonds, spades, hearts, and clubs |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/06771d1b-3810-4a9b-a9d2-5f5f6c48be27.png)不同类型水果的分类 | ![](img/02f51c3d-2e4f-46dc-b332-3cbac84c3bfe.png)扑克牌的分类：红心、黑桃、红桃和梅花
    |'
- en: '**Regression**, on the other hand, involves calculating numeric outcomes. Any
    outcome on which you can perform numeric operations, such as addition, subtraction,
    multiplication, and division, would constitute a regression problem.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 回归，另一方面，涉及计算数值结果。您可以执行数值运算的任何结果，例如加法、减法、乘法和除法，都将构成回归问题。
- en: 'Examples of regression include:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 回归的例子包括：
- en: Predicting daily temperature
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测每日温度
- en: Calculating stock prices
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算股价
- en: Predicting the sales price of residential properties and others
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测住宅物业和其他物业的销售价格
- en: Images of some atypical regression examples are shown below. In both the cases,
    we are dealing with quantitative numeric data that is continuous. Hence, the outcome
    variables of regression are also known as **quantitative or continuous variables**.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示了一些非典型的回归示例的图像。在这两种情况下，我们处理的是连续的定量数值数据。因此，回归的结果变量也被称为**定量或连续变量**。
- en: '| ![](img/10902bef-e6e1-49ef-879d-212ac46fbea9.png)Calculating house prices
    | ![](img/40718ef7-1629-42bc-868a-c3e136387bf9.png)Calculating stock prices using
    other market data |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/10902bef-e6e1-49ef-879d-212ac46fbea9.png)计算房价 | ![](img/40718ef7-1629-42bc-868a-c3e136387bf9.png)使用其他市场数据计算股价
    |'
- en: Note that the concepts of classification or regression do not as such apply
    to unsupervised learning. Since there are no labels in unsupervised learning,
    there is no discrete classification or regression in the strict sense. That said,
    since unsupervised learning categories data into clusters, objects in a cluster
    are often said to belong to the same class (as other objects in the same cluster).
    This is akin to classification, except that it is created after-the-fact and no
    classes existed prior to the objects being classified into individual clusters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，分类或回归的概念并不适用于无监督学习。由于无监督学习中没有标签，因此在严格意义上不存在离散的分类或回归。也就是说，由于无监督学习将数据分类为簇，簇中的对象通常被认为属于同一类（与同一簇中的其他对象相同）。这类似于分类，只是在事后创建，而在对象被分类到各个簇之前并不存在类。
- en: Common terminologies in machine learning
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的常见术语
- en: In machine learning, you'll often hear the terms features, predictors, and dependent
    variables. They are all one and the same. They all refer to the variables that
    are used to predict an outcome. In our previous example of cars, the variables
    **cyl** (Cylinder), **hp** (Horsepower), **wt** (Weight), and **gear** (Gear)
    are the predictors and **mpg** (Miles Per Gallon) is the outcome.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，您经常会听到特征、预测变量和因变量这些术语。它们都是一样的。它们都指的是用于预测结果的变量。在我们之前关于汽车的例子中，变量**cyl**（汽缸）、**hp**（马力）、**wt**（重量）和**gear**（齿轮）是预测变量，而**mpg**（每加仑英里数）是结果。
- en: 'In simpler terms, taking the example of a spreadsheet, the names of the columns
    are, in essence, known as features, predictors, and dependent variables. As an
    example, if we were given a dataset of toll booth charges and were tasked with
    predicting the amount charged based on the time of day and other factors, a hypothetical
    example could be as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，以电子表格为例，列的名称本质上被称为特征、预测变量和因变量。例如，如果我们获得了一个收费站收费的数据集，并被要求根据一天的时间和其他因素来预测收费金额，一个假设的例子可能如下：
- en: '![](img/3fbda53b-25e6-41f7-996f-4c992c3bbee5.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fbda53b-25e6-41f7-996f-4c992c3bbee5.png)'
- en: In this spreadsheet, the columns **date**, **time**, **agency**, **type**, **prepaid**,
    and **rate** are the features or predictors, whereas, the column **amount** is
    our outcome or dependent variable (what we are predicting).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个电子表格中，列**date**、**time**、**agency**、**type**、**prepaid**和**rate**是特征或预测变量，而列**amount**是我们的结果或因变量（我们正在预测的内容）。
- en: The value of amount *depends* on the value of the other variables (which are
    thus known as *independent variables*).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 金额的值*取决于*其他变量的值（因此被称为*自变量*）。
- en: Simple equations also reflect the obvious distinction, for example, in an equation,
    *y = a + b + c*, the **left hand side** (**LHS**) is the dependent/outcome variable
    and *a*, *b* and *c* are the features/predictors.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的方程也反映了明显的区别，例如，在一个方程中，*y = a + b + c*，**左手边**（**LHS**）是因变量/结果变量，*a*、*b*和*c*是特征/预测变量。
- en: 'In summary:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 总之：
- en: '![](img/09eb5136-5bcd-42a3-960d-e0cefa6602b0.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09eb5136-5bcd-42a3-960d-e0cefa6602b0.png)'
- en: The core concepts in machine learning
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的核心概念
- en: There are many important concepts in machine learning; we'll go over some of
    the more common topics. Machine learning involves a multi-step process that starts
    with data acquisition, data mining, and eventually leads to building the predictive
    models.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中有许多重要的概念；我们将介绍一些更常见的主题。机器学习涉及一个多步骤的过程，从数据获取、数据挖掘，最终到构建预测模型。
- en: 'The key aspects of the model-building process involve:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建过程的关键方面包括：
- en: '**Data pre-processing**: Pre-processing and feature selection (for example,
    centering and scaling, class imbalances, and variable importance)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据预处理**：预处理和特征选择（例如，居中和缩放，类别不平衡和变量重要性）'
- en: '**Train, test splits and cross-validation**:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练，测试分割和交叉验证**：'
- en: Creating the training set (say, 80 percent of the data)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建训练集（比如说，数据的80%）
- en: Creating the test set (~ 20 percent of the data)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建测试集（数据的大约20%）
- en: Performing cross-validation
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行交叉验证
- en: '**Create model, get predictions**:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建模型，获取预测**：'
- en: Which algorithms should you try?
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该尝试哪些算法？
- en: What accuracy measures are you trying to optimize?
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你试图优化哪些准确性指标？
- en: What tuning parameters should you use?
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该使用哪些调整参数？
- en: Data management steps in machine learning
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的数据管理步骤
- en: Pre-processing, or more generally processing the data, is an integral part of
    most machine learning exercises. A dataset that you start out with is seldom going
    to be in the exact format against which you'll be building your machine learning
    models; it will invariably require a fair amount of cleansing in the majority
    of cases. In fact, data cleansing is often the most time-consuming part of the
    entire process. In this section, we will briefly highlight a few of the top data
    processing steps that you may encounter in practice.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理，或者更一般地处理数据，是大多数机器学习练习的一个重要部分。你开始使用的数据集很少会与你构建机器学习模型的确切格式一致；在大多数情况下，它都需要进行相当多的清理。事实上，数据清理通常是整个过程中最耗时的部分。在本节中，我们将简要介绍一些你在实践中可能遇到的顶级数据处理步骤。
- en: Pre-processing and feature selection techniques
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理和特征选择技术
- en: '**Data pre-processing**, as the name implies, involves curating the data to
    make it suitable for machine learning exercises. There are various methods for
    pre-processing and a few of the more common ones have been illustrated here.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据预处理**，顾名思义，涉及筛选数据，使其适用于机器学习练习。有各种各样的预处理方法，这里列举了一些比较常见的方法。'
- en: Note that data pre-processing should be performed as part of the cross-validation
    step, that is, pre-processing should not be done *before the fact*, but rather
    during the model-building process. This will be explained in more detail afterward.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据预处理应该作为交叉验证步骤的一部分进行，也就是说，预处理不应该在事先进行，而应该在模型构建过程中进行。稍后将对此进行更详细的解释。
- en: Centering and scaling
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 居中和缩放
- en: Applying center and scale function on numeric columns is often done in order
    to standardize data and remove the effect of large variations in the magnitude
    or differences of numbers. You may have encountered this in college or university
    courses where students would be graded on a standardized basis, or a curve.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对数值列应用中心和缩放函数通常是为了标准化数据并消除数字的数量或差异的影响。你可能在大学课程中遇到过这种情况，学生会按照标准化的方式或曲线进行评分。
- en: For instance, say an exam paper was unusually difficult and half of all the
    students in a class of 10 students received scores below 60 - the passing rate
    set for the course. The professor can either a) make a determination that 50%
    of the students should re-take the course, or b) standardize the scores to find
    how students performed relative to one another.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一张考试试卷异常困难，10名学生中有一半的学生得分低于60分 - 这是课程的及格率。教授可以选择a)决定让50%的学生重新上课，或者b)标准化分数以找出学生相对于彼此的表现。
- en: 'Say the class scores were:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 假设班级分数是：
- en: 45,66,66,55,55,52,61,64,65,49
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 45,66,66,55,55,52,61,64,65,49
- en: With the passing score set at 60, this implies that the students who scored
    45, 55, 55, 52 and 49 will not successfully complete the course.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以60分为及格分数，这意味着得分为45、55、55、52和49的学生将无法成功完成课程。
- en: 'However, this might not be a truly accurate representation of their relative
    merits. The professor may alternatively choose to instead use a center-and-scale
    method, commonly known as standardization, which involves:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这可能并不是他们相对优点的真正准确的表示。教授可以选择使用一种称为标准化的中心和缩放方法，它包括：
- en: Finding the mean of all the scores
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有分数的平均值
- en: Subtracting the mean from the scores
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从分数中减去平均值
- en: Dividing the result by the standard deviation of all the scores
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将结果除以所有分数的标准差
- en: The operation is illustrated below for reference.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是操作的示例。
- en: 'The mean of the scores is 57.8\. Hence, subtracting 57.8 from each of the numbers
    produce the numbers shown in the second row. But, we are not done yet. We need
    to divide the numbers by the *standard deviation* of the scores to get the final
    standardized values:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 分数的平均值是57.8。因此，从每个数字中减去57.8会产生第二行中显示的数字。但是，我们还没有完成。我们需要将这些数字除以分数的*标准差*，以获得最终的标准化值：
- en: '![](img/4b4b2a16-5582-41f7-84df-f219eec1c0e6.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b4b2a16-5582-41f7-84df-f219eec1c0e6.png)'
- en: Dividing by the **SD** (**standard deviation**) shows that there were only two
    students whose scores were below one standard deviation across the range of all
    the test scores. Hence, instead of five students who do not complete the course
    successfully based on the raw numbers, we can narrow it down to only two students.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 除以**SD**（**标准差**）表明，在所有测试成绩范围内，只有两名学生的成绩低于一个标准差。因此，根据原始数字，不是五名学生未能成功完成课程，而是只有两名学生。
- en: Although this is a truly simple operation, it is not hard to see that it is
    very effective in smoothing out large variations in data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个真正简单的操作，但不难看出，它在平滑数据的大波动方面非常有效。
- en: 'Centering and scaling can be performed very easily in R using the scale command
    as shown here:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中，可以使用scale命令非常容易地进行居中和缩放，如下所示：
- en: '[PRE0]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The near-zero variance function
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接近零方差函数
- en: 'The near-zero variance, available in the `nearZeroVar` function in the `R package,
    caret`, is used to identify variables that have little or no variance. Consider
    a set of 10,000 numbers with only three distinct values. Such a variable may add
    very little value to an algorithm. In order to use the `nearZeroVar` function,
    first install the R package, caret, in RStudio (which we had set up [Chapter 3](5ca02405-8ab4-4274-8611-af003aab7c9f.xhtml), *The
    Analytics Toolkit*. The exact code to replicate the effect of using `nearZeroVar`
    is shown here:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`nearZeroVar`函数在`R package, caret`中可用于识别具有很少或没有方差的变量。考虑一个具有仅三个不同值的10,000个数字集。这样的变量可能对算法几乎没有价值。为了使用`nearZeroVar`函数，首先在RStudio中安装R软件包caret（我们在[第3章](5ca02405-8ab4-4274-8611-af003aab7c9f.xhtml)中设置了*The
    Analytics Toolkit*）。使用`nearZeroVar`的效果的确切代码如下所示：'
- en: '[PRE1]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As the example shows, the function was able to correctly detect the variable
    that met the criteria.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 正如示例所示，该函数能够正确检测到符合标准的变量。
- en: Removing correlated variables
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去除相关变量
- en: Correlated variables can produce results that over-emphasize the contribution
    of the variables. In regression exercises, this has the effect of increasing the
    value of R^2, and does not accurately represent the actual performance of the
    model. Although many classes of machine learning algorithms are resistant to the
    effects of correlated variables, it deserves some mention as it is a common topic
    in the discipline.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 相关变量可能会产生过分强调变量贡献的结果。在回归练习中，这会增加R^2的值，并且不准确地代表模型的实际性能。尽管许多类别的机器学习算法对相关变量的影响具有抵抗力，但它值得一提，因为这是该学科中的一个常见主题。
- en: The premise of removing such variables is related to the fact that redundant
    variables do not add incremental value to a model. For instance, if a dataset
    contained height in inches and height in meters, these variables would have a
    near exact correlation of 1, and using one of them is just as good as using the
    other. Practical exercises that involve variables that we cannot judge intuitively,
    using methods of removing correlated variables, can greatly help in simplifying
    the model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 删除这样的变量的前提是冗余变量不会为模型增加增量值。例如，如果数据集包含英寸和米的身高，这些变量的相关性几乎完全为1，使用其中一个与使用另一个一样好。使用去除相关变量的方法进行实际练习，可以极大地帮助简化模型，特别是涉及我们无法直观判断的变量。
- en: The following example illustrates the process of removing correlated variables.
    The dataset, **Pima Indians Diabetes**, contains vital statistics about the diet
    of Pima Indians and an outcome variable called `diabetes`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例说明了去除相关变量的过程。数据集**Pima Indians Diabetes**包含有关Pima印第安人饮食的重要统计数据，以及名为`diabetes`的结果变量。
- en: 'During the course of the examples in successive chapters, we will refer to
    this dataset often. A high level overview of the meaning of the different columns
    in the dataset is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节示例中，我们将经常提到这个数据集。数据集中不同列的含义的高级概述如下：
- en: '[PRE2]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We are interested in finding out if any of the variables, apart from diabetes
    (which is our outcome variable) are correlated. If so, it may be useful to remove
    the redundant variables.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有兴趣查找除糖尿病（我们的结果变量）以外的任何相关变量。如果有的话，删除冗余变量可能会有用。
- en: 'Install the packages `mlbench` and `corrplot` in RStudio and execute the commands
    as shown here:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在RStudio中安装`mlbench`和`corrplot`软件包，并执行以下命令：
- en: '[PRE3]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The command will produce a plot as shown here using the `corrplot` package
    from [http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram](http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将使用`corrplot`软件包从[http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram](http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram)生成一个图表，如下所示：
- en: '| ![](img/edccd408-6cf1-49de-b18d-a77282d496a8.png) | >![](img/81bbdc2b-2af2-4adb-93c0-b35330a18bb5.png)
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/edccd408-6cf1-49de-b18d-a77282d496a8.png) | >![](img/81bbdc2b-2af2-4adb-93c0-b35330a18bb5.png)
    |'
- en: The darker the shade, the higher the correlation. In this case, it shows that
    age and pregnancy have a relatively high correlation. We can find the exact values
    by using `method="number"` as shown. You can also view the plot at [http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram](http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 阴影越深，相关性越高。在这种情况下，它显示年龄和怀孕之间有相对较高的相关性。我们可以使用`method="number"`找到确切的值。您也可以在[http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram](http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram)上查看图表。
- en: 'We can also use functions such as the following to directly find the correlated
    variables without plotting the correlograms:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用以下函数直接查找相关变量，而无需绘制相关图：
- en: '[PRE4]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Other common data transformations
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他常见的数据转换
- en: Several other data transformations are available and applicable to different
    situations. A summary of these transformations can be found at the documentation
    site for the `caret` package under **Pre-Processing** at [https://topepo.github.io/caret/pre-processing.html](https://topepo.github.io/caret/pre-processing.html).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他几种数据转换方法适用于不同的情况。这些转换的摘要可以在`caret`软件包的文档网站的**Pre-Processing**下找到[https://topepo.github.io/caret/pre-processing.html](https://topepo.github.io/caret/pre-processing.html)。
- en: 'The options available in the pre-process function of caret can be found from
    its help section, by running the command `?preProcess` in RStudio. The code for
    it is given here:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在caret的预处理函数中提供的选项可以在其帮助部分中找到，通过在RStudio中运行命令`?preProcess`。其代码如下：
- en: '[PRE5]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Data sampling
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据抽样
- en: You may encounter datasets that have a high level of imbalanced outcome classes.
    For instance, if you were working with a dataset on a rare disease, with your
    outcome variable being true or false, due to the rarity of the occurrence, you
    may find that the number of observations marked as false (that is, the person
    did not have the rare disease) is much higher than the number of observations
    marked as true (that is, the person did have the rare disease).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会遇到具有高度不平衡结果类别的数据集。例如，如果您正在处理一个罕见疾病的数据集，您的结果变量是真或假，由于发生的罕见性，您可能会发现标记为假的观察数量（即，该人没有罕见疾病）远远高于标记为真的观察数量（即，该人患有罕见疾病）。
- en: Machine learning algorithms attempt to maximize performance, which in many cases
    could be the accuracy of the predictions. Say, in a sample of 1000 records, only
    10 are marked as true and the rest of the `990` observations are false.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法试图最大化性能，在许多情况下可能是预测的准确性。比如，在1000条记录的样本中，只有10条被标记为真，其余的`990`条观察结果都是假的。
- en: 'If someone were to randomly assign *all* observations as false, the accuracy
    rate would be:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人随机将*所有*观察结果都标记为假，准确率将是：
- en: '[PRE6]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: But, the objective of the exercise was to find the *individuals who had the
    rare disease*. We are already well aware that due to the nature of the disease,
    most individuals will not belong to the category.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这项练习的目标是找到患有罕见疾病的*个体*。我们已经很清楚，由于疾病的性质，大多数个体不会属于这一类别。
- en: Data sampling, in essence, is the process of *maximizing machine learning metrics
    such as specificity, sensitivity, precision, recall, and kappa*. These will be
    discussed at a later stage, but for the purposes of this section, we'll show some
    ways by which you can *sample* the data so as to produce a more evenly balanced
    dataset.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 数据抽样本质上是*最大化机器学习指标，如特异性、敏感性、精确度、召回率和kappa*的过程。这些将在后面讨论，但在本节的目的上，我们将展示一些方法，通过这些方法，您可以对数据进行*抽样*，以产生一个更均衡的数据集。
- en: The R package, `caret`, includes several helpful functions to create a balanced
    distribution of the classes from an imbalanced dataset.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: R软件包`caret`包括几个有用的函数，用于从不平衡的数据集中创建一个平衡的类别分布。
- en: In these cases, we need to re-sample the data to get a better distribution of
    the classes in order to build a more effective model.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，我们需要重新对数据进行重新抽样，以获得更好的类别分布，以建立一个更有效的模型。
- en: 'Some of the general methods include:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的方法包括：
- en: '**Up-sample**: Increase instances of the class with lesser examples'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上采样**：增加具有较少实例的类别'
- en: '**Down-sample**: Reduce the instances of the class with higher examples'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下采样**：减少具有更多实例的类别'
- en: '**Create synthetic examples** (for example, **SMOTE** (**Synthetic Minority
    Oversampling TechniquE**))'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建合成示例**（例如，**SMOTE**（**合成少数过采样技术**））'
- en: Random oversampling (for example, (**ROSE**) **Randomly OverSampling Examples**)
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机过采样（例如，(**ROSE**) **随机过采样示例**）
- en: 'We will create a simulated dataset using the same data from the prior example
    where 95% of the rows will be marked as negative:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与先前示例相同的数据创建一个模拟数据集，其中95%的行将被标记为负：
- en: '[PRE7]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The **SMOTE** (**Synthetic Minority Over-sampling TechniquE**) is a third method
    that, instead of plain vanilla up-/down-sampling, creates synthetic records from
    the nearest neighbors of the minority class. In our simulated dataset, it is obvious
    that `neg` is the minority class, that is, the class with the lowest number of
    occurrences.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SMOTE**（**合成少数类过采样技术**）是第三种方法，它不是简单的上/下采样，而是从少数类的最近邻中创建合成记录。在我们的模拟数据集中，很明显`neg`是少数类，也就是发生次数最少的类别。'
- en: 'The help file on the SMOTE function explains the concept succinctly:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE函数的帮助文件简洁地解释了这个概念：
- en: Unbalanced classification problems cause problems to many learning algorithms.
    These problems are characterized by the uneven proportion of cases that are available
    for each class of the problem.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡的分类问题给许多学习算法带来了问题。这些问题的特点是每个问题类别的案例比例不均衡。
- en: 'SMOTE (Chawla et al., 2002) is a well-known algorithm to fight this problem.
    The general idea of this method is to artificially generate new examples of the
    minority class using the nearest neighbors of these cases. Furthermore, the majority
    class examples are also under-sampled, leading to a more balanced dataset:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE（Chawla等人，2002）是一个用于解决这个问题的著名算法。该方法的一般思想是使用少数类的最近邻人工生成新的示例。此外，多数类的示例也被下采样，从而导致更平衡的数据集：
- en: '[PRE8]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**ROSE (Randomly OverSampling Examples)**, the final method in this section,
    is available via the ROSE package in R. Like SMOTE, it is a method for generating
    synthetic samples. The help file for ROSE states the high-level use of the function
    as follows:'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROSE（随机过采样示例）**，本节的最后一种方法，通过R中的ROSE软件包提供。与SMOTE类似，它是一种生成合成样本的方法。ROSE的帮助文件说明了该函数的高级用法如下：'
- en: Generation of synthetic data by Randomly Over Sampling Examples creates a sample
    of synthetic data by enlarging the features space of minority and majority class
    examples. Operationally, the new examples are drawn from a conditional kernel
    density estimate of the two classes, as described in Menardi and Torelli (2013).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 通过随机过采样示例生成合成数据，通过扩大少数和多数类示例的特征空间来创建合成数据样本。在操作上，新的示例是从两个类的条件核密度估计中抽取的，如Menardi和Torelli（2013）中所述。
- en: '[PRE9]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Data imputation
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据插补
- en: Sometimes, your data may have missing values. This could be due to errors in
    the data collection process, genuinely missing data, or any other reason, with
    the net result being that the information is not available. Real world examples
    of missing data can be found in surveys where the respondent did not answer a
    specific question on the survey.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您的数据可能存在缺失值。这可能是由于数据收集过程中的错误、真正缺失的数据或其他原因，导致信息不可用。缺失数据的现实世界例子可以在调查中找到，调查对象没有回答调查中的特定问题。
- en: You may have a dataset of, say, 1,000 records and 20 columns of which a certain
    column has 100 missing values. You may choose to discard this column altogether,
    but that also means discarding 90 percent of the information. You still have 19
    other columns that have complete data. Another option is to simply exclude the
    column, but that means you cannot leverage the benefit afforded by the data that
    is available in the respective column.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能有一个包含1000条记录和20列的数据集，其中某一列有100个缺失值。您可以选择完全丢弃这一列，但这也意味着丢弃了90%的信息。您仍然有其他19列具有完整数据。另一个选择是简单地排除该列，但这意味着您无法利用该列中可用的数据所带来的好处。
- en: Several methods exist for data imputation, that is, the process of filling in
    missing data. We do not know what the exact values are, but by looking at the
    other entries in the table, we may be able to make an educated and systematic
    assessment of what the values might be.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种数据插补方法，即填充缺失数据的过程。我们不知道确切的值是什么，但通过查看表中的其他条目，我们可能能够对值进行系统的评估。
- en: 'Some of the common methods in data imputation involve:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的数据插补方法包括：
- en: '**Mean, median, mode imputation**: Substituting the missing values using the
    mean, median, or mode value for the column. This, however, has the disadvantage
    of increasing the correlation among the variables that are imputed, which might
    not be desirable for multivariate analysis.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均值、中位数、众数插补**：使用列的均值、中位数或众数值替换缺失值。然而，这样做会增加被插补的变量之间的相关性，这对多变量分析可能不是理想的。'
- en: '**K-nearest neighbors imputation**: kNN imputation is a process of using a
    machine learning approach (nearest-neighbors) in order to impute missing values.
    It works by finding k records that are most similar to the one that has missing
    values and calculates the weighted average using Euclidean distance relative to
    k records.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K最近邻插补**：kNN插补是使用机器学习方法（最近邻）来填补缺失值的过程。它通过找到与具有缺失值的记录最相似的k条记录，并使用欧几里德距离相对于k条记录计算加权平均值来工作。'
- en: '**Imputation using regression models**: Regression methods use standard regression
    methods in R to predict the value of the missing variables. However, as noted
    in the respective section on Regression-based imputation on Wikipedia [https://en.wikipedia.org/wiki/Imputation_(statistics)#Regression](https://en.wikipedia.org/wiki/Imputation_(statistics)#Regression),
    the problem (with regression imputation) is that the imputed data do not have
    an error term included in their estimation. Thus, the estimates fit perfectly
    along the regression line without any residual variance. This causes relationships
    to be over identified and suggests greater precision in the imputed values than
    is warranted.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用回归模型进行插补**：回归方法使用R中的标准回归方法来预测缺失变量的值。然而，正如维基百科上关于基于回归的插补的相应部分所指出的那样[https://en.wikipedia.org/wiki/Imputation_(statistics)#Regression](https://en.wikipedia.org/wiki/Imputation_(statistics)#Regression)，问题在于（回归插补）估计的插补数据没有包括误差项。因此，估计值完全符合回归线，没有任何残差方差。这导致关系被过度识别，并表明插补值的精度比实际情况更高。'
- en: '**Hot-deck imputation**: Another technique for filling missing values with
    observations from the dataset itself. This method, although very prevalent, does
    have a limitation in that, by assigning say, a single value, to a large range
    of missing values, it could add a significant bias in the observations and can
    produce misleading results.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**热卡插补**：使用数据集本身的观察值填充缺失值的另一种技术。这种方法虽然非常普遍，但有一个局限性，即通过为大范围的缺失值分配一个单一值，可能会在观察中增加显著的偏差，并产生误导性的结果。'
- en: A short example has been provided here to demonstrate how imputation can be
    done using kNN Imputation. We simulate missing data by changing a large number
    of values to NA in the `PimaIndiansDiabetes` dataset.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供了一个简短的示例，演示了如何使用kNN插补进行插补。我们通过在`PimaIndiansDiabetes`数据集中将大量值更改为NA来模拟缺失数据。
- en: 'We make use of the following factors for the process:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用以下因素进行处理：
- en: We use mean to fill in the NA values.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用均值来填充NA值。
- en: 'We use kNN imputation to fill in the missing values. We then compare how the
    two methods performed:'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用kNN插补来填补缺失值。然后比较这两种方法的表现：
- en: '[PRE10]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We get the output as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/0dec61a7-716d-4f85-9804-2290ca5419ea.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0dec61a7-716d-4f85-9804-2290ca5419ea.png)'
- en: '[PRE11]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '![](img/6ad62a66-066c-4ce9-b69b-258304df178c.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ad62a66-066c-4ce9-b69b-258304df178c.png)'
- en: '[PRE12]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: While it may not represent a dramatic change, it's still better than using a
    naïve approach such as using simply a mean or constant value.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能不代表一个显著的变化，但仍然比使用简单的方法（如使用均值或常数值）要好。
- en: 'There are several packages in R for data imputation. A few prominent ones are
    as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: R中有几个数据插补的包。其中一些著名的包如下：
- en: '**Amelia II**: Missing information in time-series data'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amelia II**：时间序列数据中的缺失信息'
- en: '[https://gking.harvard.edu/amelia](https://gking.harvard.edu/amelia)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://gking.harvard.edu/amelia](https://gking.harvard.edu/amelia)'
- en: '**Hot-deck imputation with R package**: HotDeckImputation and hot.deck'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用R包进行热卡补充**: HotDeckImputation 和 hot.deck'
- en: '[https://cran.r-project.org/web/packages/HotDeckImputation/](https://cran.r-project.org/web/packages/HotDeckImputation/)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cran.r-project.org/web/packages/HotDeckImputation/](https://cran.r-project.org/web/packages/HotDeckImputation/)'
- en: '[https://cran.r-project.org/web/packages/hot.deck/](https://cran.r-project.org/web/packages/hot.deck/)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cran.r-project.org/web/packages/hot.deck/](https://cran.r-project.org/web/packages/hot.deck/)'
- en: '**Multivariate imputation (by Chained Equations)**'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多变量填充（通过链式方程）**'
- en: '[https://cran.r-project.org/web/packages/mice/index.html](https://cran.r-project.org/web/packages/mice/index.html)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cran.r-project.org/web/packages/mice/index.html](https://cran.r-project.org/web/packages/mice/index.html)'
- en: '**Imputing values in a Bayesian framework with R package**: mi'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在R包中使用贝叶斯框架进行值的填充**: mi'
- en: '[https://cran.r-project.org/web/packages/mi/index.html](https://cran.r-project.org/web/packages/mi/index.html)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cran.r-project.org/web/packages/mi/index.html](https://cran.r-project.org/web/packages/mi/index.html)'
- en: The importance of variables
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变量的重要性
- en: During model-building exercises, datasets may have tens of variables. Not all
    of them may add value to the predictive model. It is not uncommon to reduce the
    dataset to include a subset of the variables and allow the machine learning programmer
    to devote more time toward fine-tuning the chosen variables and the model-building
    process. There is also a technical justification for reducing the number of variables
    in the dataset. Performing machine learning modeling on very large, that is, high
    dimensional datasets can be very compute-intensive, that is, it may require a
    significant amount of time, CPU, and RAM to perform the numerical operations.
    This not only makes the application of certain algorithms impractical, it also
    has the effect of causing unwarranted delays. Hence, the methodical selection
    of variables helps both in terms of analysis time as well as computational requirements
    for algorithmic analysis.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建过程中，数据集可能有数十个变量。并非所有变量都可能对预测模型有价值。将数据集减少到包括变量子集并允许机器学习程序员花更多时间来调整选择的变量和模型构建过程是很常见的。减少数据集中变量数量也有技术上的理由。在非常大的、即高维数据集上执行机器学习建模可能非常计算密集，即可能需要大量的时间、CPU和RAM来执行数值运算。这不仅使得应用某些算法变得不切实际，还会导致不必要的延迟。因此，变量的系统选择有助于分析时间和算法分析的计算要求。
- en: '**Variable selection** is also known as feature selection/attribute selection.
    Algorithms such as random forests and lasso regression implement variable selection
    as part of their algorithmic operations. But, variable selection can be done as
    a separate exercise.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**变量选择**也被称为特征选择/属性选择。随机森林和套索回归等算法实现了变量选择作为其算法操作的一部分。但是，变量选择也可以作为一个单独的练习来完成。'
- en: The R package, `caret`, provides a very simple-to-use and intuitive interface
    for variable selection. As we haven't yet discussed the modeling process, we will
    learn how to find the important variables, and in the next chapter delve deeper
    into the subject.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: R包`caret`为变量选择提供了一个非常简单易用和直观的接口。由于我们还没有讨论建模过程，我们将学习如何找到重要的变量，并在下一章深入探讨这个主题。
- en: We'll use a common, well-known algorithm called `RandomForest` that is used
    for building decision trees. The algorithm will be described in more detail in
    the next chapter, but the purpose of using it here is merely to show how variable
    selection can be performed. The example is illustrative of what the general process
    is.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个常见的、众所周知的算法，称为`RandomForest`，用于构建决策树。该算法将在下一章中更详细地描述，但在这里使用它的目的仅仅是为了展示如何进行变量选择。这个例子说明了一般过程是什么样的。
- en: 'We''ll re-use the dataset we have been working with, that is, the `PimaIndiansDiabetes`
    data from the `mlbench` package. We haven''t discussed the model training process
    yet, but it has been used here in order to derive the values for variable importance.
    The outcome variable in this case is diabetes and the other variables are used
    as the independent variables. In other words, can we predict if the person has
    diabetes using the data available:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重复使用我们一直在处理的数据集，即来自`mlbench`包的`PimaIndiansDiabetes`数据。我们还没有讨论模型训练过程，但在这里使用它是为了得出变量重要性的值。在这种情况下，结果变量是糖尿病，其他变量被用作自变量。换句话说，我们能否使用可用的数据来预测一个人是否患有糖尿病：
- en: '[PRE13]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output of the preceding code is as shown below. It indicates that glucose,
    mass and age were the variables that contributed the most towards creating the
    model (to predict diabetes)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下所示。它表明葡萄糖、体重指数和年龄是对创建模型（预测糖尿病）做出最大贡献的变量。
- en: '![](img/d6f247da-1244-4e39-b6e8-fd56c9607d9f.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6f247da-1244-4e39-b6e8-fd56c9607d9f.png)'
- en: The train, test splits, and cross-validation concepts
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练、测试拆分和交叉验证概念
- en: The train, test splits, and cross-validation sets are a fundamental concept
    in machine learning. This is one of the areas where a pure statistical approach
    differs materially from the machine learning approach. Whereas in a statistical
    modeling task, one may perform regressions, parametric/non-parametric tests, and
    apply other methods, in machine learning, the algorithmic approach is supplemented
    with an element of iterative assessment of the results being produced and subsequent
    improvisation of the model with each iteration.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，训练、测试拆分和交叉验证集是一个基本概念。这是一个纯统计方法与机器学习方法有实质区别的领域之一。在统计建模任务中，一个人可能进行回归、参数/非参数测试，并应用其他方法，而在机器学习中，算法方法被补充了对产生的结果的迭代评估和随后的模型改进的元素。
- en: Splitting the data into train and test sets
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集
- en: Every machine learning modeling exercise begins with the process of data cleansing,
    as discussed earlier. The next step is to split the data into a train and test
    set. This is usually done by randomly selecting rows from the data that will be
    used to create the model. The rows that weren't selected would then be used to
    test the final model.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习建模练习都始于数据清洗的过程，正如前面讨论的那样。下一步是将数据分割成训练集和测试集。通常是通过随机选择数据中的行来完成的，这些行将被用来创建模型。然后未被选择的行将被用来测试最终模型。
- en: The usual split varies between 70-80 percent (training data versus test data).
    In an 80-20 split, 80% of the data would be used in order to create the model.
    The remaining 20% would be used to test the final model produced.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的分割在70-80%之间（训练数据与测试数据）。在80-20的分割中，80%的数据将被用来创建模型。剩下的20%将被用来测试最终模型。
- en: 'We applied this in the earlier section, but we can revisit the code once again.
    The `createDataPartition` function was used with the parameter `p = 0.80` in order
    to split the data. The `training_index` variable holds the training indices (of
    the `dataset`, `diab`) that we will use:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的部分中应用了这个方法，但我们可以再次查看代码。`createDataPartition`函数被用来分割数据，参数为`p = 0.80`。`training_index`变量保存了我们将使用的训练索引（`dataset`，`diab`）：
- en: '[PRE14]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We do not have to necessarily use the `createDataPartition` function and instead,
    a random sample created using simple R commands as shown here will suffice:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不一定要使用`createDataPartition`函数，而是可以使用简单的R命令创建一个随机样本，如下所示：
- en: '[PRE15]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The cross-validation parameter
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证参数
- en: Cross-validation takes the train-test split concept to the next stage. The aim
    of the machine learning exercise is, in essence, to find what set of model parameters
    will provide the best performance. A model parameter indicates the arguments that
    the function (the model) takes. For example, for a decision tree model, parameters
    may include the number of levels deep the model should be built, number of splits,
    and so on. If, say, there are *n* different parameters, each having *k* different
    values, the total number of parameters would be *k*^*n*. We generally select a
    fixed set of combinations for each of the parameters and could easily end with
    100-1000+ combinations. We will test the performance of the model (for example,
    accuracy in predicting the outcome correctly) for each of the parameters.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证将训练-测试分割的概念推向了下一个阶段。机器学习练习的目标本质上是找到哪组模型参数能提供最佳的性能。模型参数指的是函数（模型）所需的参数。例如，对于决策树模型，参数可能包括模型应该构建的深度级别、分割数量等。如果有*n*个不同的参数，每个参数有*k*个不同的值，那么总参数数量将是*k*^*n*。通常我们会为每个参数选择一组固定的组合，可能最终会有100-1000+个组合。我们将测试模型的性能（例如，正确预测结果的准确度）。
- en: With a simple train-test split, say, if there were 500 combinations of parameters
    we had selected, we just need to run them against the training dataset and determine
    which one shows the optimal performance.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个简单的训练-测试分割，比如说，如果我们选择了500个参数组合，我们只需要对训练数据集运行它们，并确定哪一个显示出最佳性能。
- en: With cross-validation, we further split the training set into smaller subsets,
    for example, three- or five-fold is commonly used. If there are three folds, that
    is, we split the training set into three subsets, we keep aside one fold, say,
    Fold 2, and create a model using a set of parameters using Folds 1 and 3\. We
    then test its accuracy against Fold 2\. This step is repeated several times, with
    each iteration representing a unique set of folds on which the training-test process
    is being executed and accuracy measures are collected. Eventually, we would arrive
    at an optimal combination by selecting the parameters that showed the best overall
    performance.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 通过交叉验证，我们进一步将训练集分成更小的子集，比如说通常使用三折或五折。如果有三折，也就是说，我们将训练集分成三个子集，我们将一折放在一边，比如说第2折，然后使用第1折和第3折构建一个模型。然后测试它对第2折的准确性。这个步骤会重复多次，每次迭代都代表了一组独特的折，训练-测试过程和准确度测量。最终，我们会选择表现最佳的参数组合。
- en: 'The standard approach can be summarized as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的方法可以总结如下：
- en: Create an 80-20 train-test split
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个80-20的训练-测试分割
- en: Execute your model(s) using different combinations of model parameters
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用不同的模型参数组合执行你的模型
- en: Select the model parameters that show the best overall performance and create
    the final model
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择表现最佳的模型参数并创建最终模型
- en: Apply the final model on the test set to see the results
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上应用最终模型以查看结果
- en: 'The cross-validation approach mandates that we should further split the training
    dataset into smaller subsets. These subsets are generally known as **folds** and
    collectively they are known as the **k-folds**, where *k* represents the number
    of splits:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证方法要求我们进一步将训练数据集分成更小的子集。这些子集通常被称为**折**，总称为**k折**，其中*k*代表分割的数量：
- en: Create an 80-20 train-test split
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个80-20的训练-测试分割
- en: Split the training set into k-folds, say, three folds
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练集分成k折，比如说三折
- en: Set aside Fold 1 and build a model using Fold 2 and Fold 3
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第1折放在一边，使用第2折和第3折构建模型
- en: Test your model performance on Fold 1 (for example, the percentage of accurate
    results)
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第1折上测试你的模型表现（例如，准确结果的百分比）
- en: Set aside Fold 2 and build a model using Fold 1 and Fold 3
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第2折放在一边，使用第1折和第3折构建模型
- en: Test your model performance on Fold 2
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第2折上测试你的模型表现
- en: Set aside Fold 3 and build a model using Fold 1 and Fold 2
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第3折放在一边，使用第1折和第2折构建模型
- en: Test your model performance on Fold 3
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第3折上测试你的模型表现
- en: Take the average performance of the model across all three folds
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取所有三折模型的平均性能
- en: Repeat Step 1 for *each set of model parameters*
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对*每组模型参数*重复步骤1
- en: Select the model parameters that show the best overall performance and create
    the final model
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择表现最佳的模型参数并创建最终模型
- en: Apply the final model on the test set to see the results
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上应用最终模型以查看结果
- en: '![](img/c300d7bd-53ec-426a-9b9d-71455234a676.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c300d7bd-53ec-426a-9b9d-71455234a676.png)'
- en: This image illustrates the difference between using an approach without cross-validation
    and one with cross-validation. The cross-validation method is arguably more robust
    and involves a rigorous evaluation of the model. That said, it is often useful
    to attempt creating a model initially without cross-validation to get a sense
    of the kind of performance that may be expected. For example, if a model built
    with say 2-3 training-test splits shows a performance of say, 30% accuracy, it
    is unlikely that any other approach, including cross-validation would somehow
    make that 90%. In other words the standard approach helps to get a sense of the
    kind of performance that may be expected. As cross-validations can be quite compute-intensive
    and time consuming getting an initial feedback on performance helps in a preliminary
    analysis of the overall process.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片说明了使用不带交叉验证的方法和带交叉验证的方法之间的差异。交叉验证方法可以说更加健壮，并且涉及对模型的严格评估。也就是说，尝试最初创建一个不带交叉验证的模型通常是有用的，以了解可能期望的性能。例如，如果使用2-3个训练-测试分割构建的模型显示出30%的准确性，那么很可能任何其他方法，包括交叉验证，都不会使其达到90%。换句话说，标准方法有助于了解可能期望的性能。由于交叉验证可能非常耗费计算资源和时间，因此在性能的初步分析中获得初始反馈是有帮助的。
- en: The caret package in R provides a very user-friendly approach to building models
    using cross-validation. Recall that data pre-processing must be passed or made
    an integral part of the cross-validation process. So, say, we had to center and
    scale the dataset and perform a five-fold cross-validation, all we would have
    to do is define the type of sampling we'd like to use in caret's `trainControl`
    function.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: R中的caret包提供了一个非常用户友好的方法来使用交叉验证构建模型。请记住，数据预处理必须通过或作为交叉验证过程的一个组成部分。因此，假设我们需要对数据集进行中心化和缩放，并进行五折交叉验证，我们只需要在caret的`trainControl`函数中定义我们想要使用的抽样类型。
- en: Caret's webpage on `trainControl` provides a detailed overview of the functions
    along with worked-out examples at [https://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning](https://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Caret关于`trainControl`的网页提供了函数的详细概述，并附有示例，网址为[https://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning](https://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning)。
- en: 'We have used this approach in our earlier exercise where we built a model using
    `RandomForest` on the `PimaIndiansDiabetes` dataset. It is shown again here to
    indicate where the technique was used:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的练习中使用了这种方法，在`PimaIndiansDiabetes`数据集上使用`RandomForest`构建了一个模型。这里再次展示出来，以表明这种技术的使用情况：
- en: '[PRE16]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can get a more detailed explanation of `summaryFunction` from [https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf](https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf](https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf)获取有关`summaryFunction`的更详细解释。
- en: 'The `summaryFunction` argument is used to pass in a function that takes the
    observed and predicted values and estimates some measure of performance. Two such
    functions are already included in the package: `defaultSummary` and `twoClassSummary`.
    The latter will compute measures specific to two-class problems, such as the area
    under the ROC curve, the sensitivity and specificity. Since the ROC curve is based
    on the predicted class probabilities (which are not computed automatically), another
    option is required. The `classProbs = TRUE` option is used to include these calculations.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`summaryFunction`参数用于传递一个函数，该函数接受观察值和预测值，并估计某种性能指标。该包中已经包含了两个这样的函数：`defaultSummary`和`twoClassSummary`。后者将计算特定于两类问题的度量，例如ROC曲线下面积、灵敏度和特异性。由于ROC曲线是基于预测类别概率的（这些概率不会自动计算），因此需要另一个选项。`classProbs
    = TRUE`选项用于包括这些计算。'
- en: Here is an explanation of `tuneLength` from the help file for the train function
    of `caret`.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自`caret`的`train`函数的帮助文件中关于`tuneLength`的解释。
- en: '`tuneLength` is an integer denoting the amount of granularity in the tuning
    parameter grid. By default, this argument is the number of levels for each tuning
    parameter that should be generated by train. If `trainControl` has the option
    `search = random`, this is the maximum number of tuning parameter combinations
    that will be generated by the random search.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`tuneLength`是一个整数，表示调整参数网格中的粒度。默认情况下，该参数是由`train`生成的每个调整参数的级别数。如果`trainControl`选项中有`search
    = random`，则这是由随机搜索生成的调整参数组合的最大数量。'
- en: Note that if this argument is given it must be named.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果给出了这个参数，必须要有名称。
- en: Creating the model
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建模型
- en: The final step after creating the model is to use the model against the test
    dataset to get the predictions. This is generally done using the `predict` function
    in R, with the first argument being the model that was created and the second
    argument being the dataset against which you'd like to get the predictions for.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 创建模型后的最后一步是使用模型对测试数据集进行预测。通常使用R中的`predict`函数来完成，第一个参数是创建的模型，第二个参数是您想要获取预测结果的数据集。
- en: 'Taking our example of the `PimaIndiansDiabetes` dataset, after the model has
    been built, we can get the predictions on the test dataset as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 以`PimaIndiansDiabetes`数据集为例，在模型构建完成后，我们可以按以下方式在测试数据集上进行预测：
- en: '[PRE17]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s check what the confusion matrix tells us:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看混淆矩阵告诉我们什么：
- en: '[PRE18]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The plot is as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 绘图如下：
- en: '![](img/7d0a354a-6134-40f5-adc9-2a7a7932b0f5.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d0a354a-6134-40f5-adc9-2a7a7932b0f5.png)'
- en: '[PRE19]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We get the plot as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的绘图如下：
- en: '![](img/08ce8ffe-fa37-435e-a852-bd95dd78a78d.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08ce8ffe-fa37-435e-a852-bd95dd78a78d.png)'
- en: 'Per the documentation of *`fourfoldplot`* [Source: [https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/fourfoldplot.html](https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/fourfoldplot.html)],
    an association (odds ratio different from 1) between the binary row and column
    variables is indicated by the tendency of diagonally opposite cells in one direction
    to differ in size from those in the other direction; color is used to show this
    direction. Confidence rings for the odds ratio allow a visual test of the null
    of no association; the rings for adjacent quadrants overlap if and only if the
    observed counts are consistent with the null hypothesis.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 根据*`fourfoldplot`*的文档[来源：[https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/fourfoldplot.html](https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/fourfoldplot.html)]，二元行和列变量之间的关联（与1不同的几率比）由对角线相对方向的单元格大小差异的倾向来指示；颜色用于显示这个方向。几率比的置信环允许对无关联的零假设进行视觉检验；如果相邻象限的环重叠，那么观察计数与零假设一致。
- en: Leveraging multicore processing in the model
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用模型中的多核处理
- en: The exercise in the previous section is repeated here using the PimaIndianDiabetes2
    dataset instead. This dataset contains several missing values. As a result, we
    will first impute the missing values and then run the machine learning example.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里使用PimaIndianDiabetes2数据集重复上一节的练习。该数据集包含一些缺失值。因此，我们将首先填补缺失值，然后运行机器学习示例。
- en: The exercise has been repeated with some additional nuances, such as using multicore/parallel
    processing in order to make the cross-validations run faster.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 该练习已经以一些额外的细微差别重复进行，比如使用多核/并行处理以使交叉验证运行更快。
- en: 'To leverage multicore processing, install the package `doMC` using the following
    code:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 要利用多核处理，使用以下代码安装`doMC`包：
- en: '[PRE20]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we will run the program as shown in the code here:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将按照这里的代码运行程序：
- en: '[PRE21]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Even with 650+ missing values, our model was able to achieve an accuracy of
    80%+.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有650多个缺失值，我们的模型也能够达到80%以上的准确率。
- en: It can certainly be improved, but as a baseline, it shows the kind of performance
    that can be expected of machine learning models.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 它肯定可以得到改进，但作为基准，它展示了机器学习模型可以期望的性能类型。
- en: 'In a case of a dichotomous outcome variable, a random guess would have had
    a 50% chance of being accurate. An accuracy of 80% is significantly higher than
    the accuracy we could have achieved using just guess-work:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元结果变量的情况下，随机猜测的准确率为50%。80%的准确率显然比我们只使用猜测所能达到的准确率要高得多：
- en: '[PRE22]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The resulting plot is as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的图如下：
- en: '![](img/e3f98e5b-e43e-4a7e-bad9-3242aed7141b.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3f98e5b-e43e-4a7e-bad9-3242aed7141b.png)'
- en: '[PRE23]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The result is depicted in the following plot:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下图所示：
- en: '![](img/b2a4c384-7f1d-4f36-ac67-f15fd3df0f0d.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2a4c384-7f1d-4f36-ac67-f15fd3df0f0d.png)'
- en: Summary
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learnt about the basic fundamentals of Machine Learning,
    the different types such as Supervised and Unsupervised and major concepts such
    as data pre-processing, data imputation, managing imbalanced classes and other
    topics.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了机器学习的基本原理，不同类型，如监督和无监督，以及数据预处理、数据填补、管理不平衡类别和其他主题。
- en: We also learnt about the key distinctions between terms that are being used
    interchangeably today, in particular the terms AI and Machine Learning. We learned
    that artificial intelligence deals with a vast array of topics, such as game theory,
    sociology, constrained optimizations, and machine learning; AI is much broader
    in scope relative to machine learning.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还了解了今天可以互换使用的一些关键术语之间的区别，特别是AI和机器学习这两个术语。我们了解到人工智能涉及到各种各样的主题，如博弈论、社会学、受限优化和机器学习；相对于机器学习，AI的范围要广得多。
- en: Machine learning facilitates AI; namely, machine learning algorithms are used
    to create systems that are *artificially intelligent*, but they differ in scope.
    A regression problem (finding the line of best fit given a set of points) can
    be considered a machine learning *algorithm*, but it is much less likely to be
    seen as an AI algorithm (conceptually, although it technically could be).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习促进了人工智能；也就是说，机器学习算法被用来创建*人工智能*系统，但它们的范围不同。回归问题（在给定一组点的情况下找到最佳拟合线）可以被视为机器学习*算法*，但在概念上，它不太可能被视为AI算法（尽管从技术上讲它可能是）。
- en: In the next chapter, we will look at some of the other concepts in Machine Learning
    such as Bias, Variance and Regularization. We will also read about a few important
    algorithms and learn how to apply them using machine learning packages in R.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究机器学习中的其他概念，如偏差、方差和正则化。我们还将了解一些重要的算法，并学习如何使用R中的机器学习包应用它们。
