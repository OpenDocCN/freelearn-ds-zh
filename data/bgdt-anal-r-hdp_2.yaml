- en: Chapter 2. Writing Hadoop MapReduce Programs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：编写 Hadoop MapReduce 程序
- en: In the previous chapter, we learned how to set up the R and Hadoop development
    environment. Since we are interested in performing Big Data analytics, we need
    to learn Hadoop to perform operations with Hadoop MapReduce. In this chapter,
    we will discuss what MapReduce is, why it is necessary, how MapReduce programs
    can be developed through Apache Hadoop, and more.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何设置 R 和 Hadoop 开发环境。由于我们有兴趣执行大数据分析，因此我们需要学习 Hadoop 以便使用 Hadoop MapReduce
    执行操作。在本章中，我们将讨论什么是 MapReduce，为什么它是必要的，如何通过 Apache Hadoop 开发 MapReduce 程序，以及更多内容。
- en: 'In this chapter, we will cover:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Understanding the basics of MapReduce
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 MapReduce 的基本概念
- en: Introducing Hadoop MapReduce
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Hadoop MapReduce
- en: Understanding the Hadoop MapReduce fundamentals
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Hadoop MapReduce 基础
- en: Writing a Hadoop MapReduce example
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写一个 Hadoop MapReduce 示例
- en: Understanding several possible MapReduce definitions to solve business problems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解多种可能的 MapReduce 定义，以解决业务问题
- en: Learning different ways to write Hadoop MapReduce in R
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习在 R 中编写 Hadoop MapReduce 的不同方式
- en: Understanding the basics of MapReduce
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 MapReduce 基础
- en: Understanding the basics of MapReduce could well be a long-term solution if
    one doesn't have a cluster or uses **Message Passing Interface** (**MPI**). However,
    a more realistic use case is when the data doesn't fit on one disk but fits on
    a **Distributed File System** (**DFS**), or already lives on Hadoop-related software.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有集群或使用**消息传递接口**（**MPI**），理解 MapReduce 基础可能是一个长期的解决方案。然而，更现实的使用场景是当数据无法存储在一块磁盘上，但可以存储在**分布式文件系统**（**DFS**）中，或者已经存储在
    Hadoop 相关软件上。
- en: Moreover, MapReduce is a programming model that works in a distributed fashion,
    but it is not the only one that does. It might be illuminating to describe other
    programming models, for example, MPI and **Bulk Synchronous Parallel** (**BSP**).
    To process Big Data with tools such as R and several machine learning techniques
    requires a high-configuration machine, but that's not the permanent solution.
    So, distributed processing is the key to handling this data. This distributed
    computation can be implemented with the MapReduce programming model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，MapReduce 是一种分布式编程模型，但它并不是唯一一种支持分布式的编程模型。描述其他编程模型可能会有所启发，例如 MPI 和**批量同步并行**（**BSP**）。使用
    R 等工具和若干机器学习技术处理大数据需要高配置的机器，但这并不是永久解决方案。因此，分布式处理是处理这些数据的关键。这种分布式计算可以通过 MapReduce
    编程模型来实现。
- en: MapReduce is the one that answers the Big Data question. Logically, to process
    data we need parallel processing, which means processing over large computation;
    it can either be obtained by clustering the computers or increasing the configuration
    of the machine. Using the computer cluster is an ideal way to process data with
    a large size.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 是解决大数据问题的答案。从逻辑上讲，要处理数据，我们需要并行处理，这意味着需要大规模计算；这种处理方式可以通过将计算机集群进行聚集或提高单机配置来实现。使用计算机集群是处理大规模数据的理想方式。
- en: Before we talk more about MapReduce by parallel processing, we will discuss
    Google MapReduce research and a white paper written by *Jeffrey Dean* and *Sanjay
    Ghemawat* in 2004\. They introduced MapReduce as simplified data processing software
    on large clusters. MapReduce implementation runs on large clusters with commodity
    hardware. This data processing platform is easier for programmers to perform various
    operations. The system takes care of input data, distributes data across the computer
    network, processes it in parallel, and finally combines its output into a single
    file to be aggregated later. This is very helpful in terms of cost and is also
    a time-saving system for processing large datasets over the cluster. Also, it
    will efficiently use computer resources to perform analytics over huge data. Google
    has been granted a patent on MapReduce.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论并行处理中的 MapReduce 之前，我们将讨论 Google MapReduce 研究以及*Jeffrey Dean*和*Sanjay
    Ghemawat*在 2004 年撰写的白皮书。它们将 MapReduce 介绍为一种简化的大规模集群数据处理软件。MapReduce 实现运行在由普通硬件组成的大型集群上。这种数据处理平台使得程序员可以更容易地执行各种操作。系统负责处理输入数据、将数据分配到计算机网络中、并行处理这些数据，最后将输出结果合并成一个文件，供之后聚合。这对于成本控制非常有帮助，而且也是一种节省时间的系统，适用于在集群上处理大型数据集。此外，它能高效地利用计算机资源来处理大量数据进行分析。Google
    已获得 MapReduce 的专利。
- en: 'For MapReduce, programmers need to just design/migrate applications into two
    phases: Map and Reduce. They simply have to design Map functions for processing
    a key-value pair to generate a set of intermediate key-value pairs, and Reduce
    functions to merge all the intermediate keys. Both the Map and Reduce functions
    maintain MapReduce workflow. The Reduce function will start executing the code
    after completion or once the Map output is available to it.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MapReduce，程序员只需要将应用程序设计/迁移为两个阶段：Map和Reduce。他们只需要设计Map函数，用于处理键值对并生成一组中间的键值对，然后设计Reduce函数，用于合并所有的中间键。Map和Reduce函数共同维护MapReduce的工作流。Reduce函数将在Map输出可用后开始执行代码。
- en: 'Their execution sequence can be seen as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的执行顺序如下所示：
- en: '![Understanding the basics of MapReduce](img/3282OS_02_00.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![理解MapReduce的基础](img/3282OS_02_00.jpg)'
- en: MapReduce assumes that the Maps are independent and will execute them in parallel.
    The key aspect of the MapReduce algorithm is that if every Map and Reduce is independent
    of all other ongoing Maps and Reduces in the network, the operation will run in
    parallel on different keys and lists of data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce假设Map是独立的，并且会并行执行它们。MapReduce算法的关键是，如果每个Map和Reduce都与网络中其他正在进行的Map和Reduce独立，那么操作将在不同的键和数据列表上并行运行。
- en: A distributed filesystem spreads multiple copies of data across different machines.
    This offers reliability as well as fault tolerance. If a machine with one copy
    of the file crashes, the same data will be provided from another replicated data
    source.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式文件系统将数据的多个副本分布到不同的机器上。这提供了可靠性和容错能力。如果一台机器上的文件副本崩溃，另一台复制的数据源将提供相同的数据。
- en: The master node of the MapReduce daemon will take care of all the responsibilities
    of the MapReduce jobs, such as the execution of jobs, the scheduling of Mappers,
    Reducers, Combiners, and Partitioners, the monitoring of successes as well as
    failures of individual job tasks, and finally, the completion of the batch job.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce守护进程的主节点将负责MapReduce作业的所有职责，如作业的执行、Mappers、Reducers、Combiners和Partitioners的调度、监控个别作业任务的成功与失败，最后完成批处理作业。
- en: Apache Hadoop processes the distributed data in a parallel manner by running
    Hadoop MapReduce jobs on servers near the data stored on Hadoop's distributed
    filesystem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop通过在存储在Hadoop分布式文件系统上的数据附近运行Hadoop MapReduce作业，以并行方式处理分布式数据。
- en: 'Companies using MapReduce include:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MapReduce的公司包括：
- en: '**Amazon**: This is an online e-commerce and cloud web service provider for
    Big Data analytics'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon**：这是一个在线电子商务和云服务提供商，专注于大数据分析'
- en: '**eBay**: This is an e-commerce portal for finding articles by its description'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**eBay**：这是一个电子商务门户网站，用于根据描述查找商品'
- en: '**Google**: This is a web search engine for finding relevant pages relating
    to a particular topic'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google**：这是一个网页搜索引擎，用于查找与特定主题相关的页面'
- en: '**LinkedIn**: This is a professional networking site for Big Data storage and
    generating personalized recommendations'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LinkedIn**：这是一个专业的社交网站，用于大数据存储和生成个性化推荐'
- en: '**Trovit**: This is a vertical search engine for finding jobs that match a
    given description'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Trovit**：这是一个垂直搜索引擎，用于查找与给定描述匹配的工作'
- en: '**Twitter**: This is a social networking site for finding messages'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Twitter**：这是一个社交网站，用于查找消息'
- en: Apart from these, there are many other brands that are using Hadoop for Big
    Data analytics.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，还有许多其他品牌正在使用Hadoop进行大数据分析。
- en: Introducing Hadoop MapReduce
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入Hadoop MapReduce
- en: Basically, the MapReduce model can be implemented in several languages, but
    apart from that, Hadoop MapReduce is a popular Java framework for easily written
    applications. It processes vast amounts of data (multiterabyte datasets) in parallel
    on large clusters (thousands of nodes) of commodity hardware in a reliable and
    fault-tolerant manner. This MapReduce paradigm is divided into two phases, Map
    and Reduce, that mainly deal with key-value pairs of data. The Map and Reduce
    tasks run sequentially in a cluster, and the output of the Map phase becomes the
    input of the Reduce phase.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，MapReduce模型可以用多种语言实现，但除了这些，Hadoop MapReduce是一个流行的Java框架，便于编写应用程序。它以可靠和容错的方式在大型集群（成千上万的节点）上的商品硬件上并行处理大量数据（多TB数据集）。这个MapReduce范式分为两个阶段：Map和Reduce，主要处理数据的键值对。Map和Reduce任务在集群中顺序运行，Map阶段的输出成为Reduce阶段的输入。
- en: All data input elements in MapReduce cannot be updated. If the input `(key,
    value)` pairs for mapping tasks are changed, it will not be reflected in the input
    files. The Mapper output will be piped to the appropriate Reducer grouped with
    the key attribute as input. This sequential data process will be carried away
    in a parallel manner with the help of Hadoop MapReduce algorithms as well as Hadoop
    clusters.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 中的所有数据输入元素都无法更新。如果映射任务的输入 `(key, value)` 对发生更改，将不会反映在输入文件中。Mapper
    输出将通过管道传输到适当的 Reducer，并按键属性分组作为输入。这个顺序的数据处理将通过 Hadoop MapReduce 算法和 Hadoop 集群以并行方式执行。
- en: MapReduce programs transform the input dataset present in the list format into
    output data that will also be in the list format. This logical list translation
    process is mostly repeated twice in the Map and Reduce phases. We can also handle
    these repetitions by fixing the number of Mappers and Reducers. In the next section,
    MapReduce concepts are described based on the old MapReduce API.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 程序将以列表格式呈现的输入数据集转换为同样以列表格式呈现的输出数据。这一逻辑列表转换过程通常在 Map 阶段和 Reduce 阶段重复两次。我们还可以通过固定
    Mapper 和 Reducer 的数量来处理这些重复。在接下来的部分中，将根据旧的 MapReduce API 描述 MapReduce 概念。
- en: Listing Hadoop MapReduce entities
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列出 Hadoop MapReduce 实体
- en: 'The following are the components of Hadoop that are responsible for performing
    analytics over Big Data:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是负责在大数据上执行分析的 Hadoop 组件：
- en: '**Client**: This initializes the job'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Client**：这是用来初始化任务的。'
- en: '**JobTracker**: This monitors the job'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JobTracker**：这是用来监控任务的。'
- en: '**TaskTracker**: This executes the job'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TaskTracker**：这是执行任务的。'
- en: '**HDFS**: This stores the input and output data'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HDFS**：这是用来存储输入和输出数据的。'
- en: Understanding the Hadoop MapReduce scenario
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Hadoop MapReduce 场景
- en: 'The four main stages of Hadoop MapReduce data processing are as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop MapReduce 数据处理的四个主要阶段如下：
- en: The loading of data into HDFS
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据加载到 HDFS 中
- en: The execution of the Map phase
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行 Map 阶段
- en: Shuffling and sorting
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 洗牌和排序
- en: The execution of the Reduce phase
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行 Reduce 阶段
- en: Loading data into HDFS
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据加载到 HDFS 中
- en: The input dataset needs to be uploaded to the Hadoop directory so it can be
    used by MapReduce nodes. Then, **Hadoop Distributed File System** (**HDFS**) will
    divide the input dataset into data splits and store them to DataNodes in a cluster
    by taking care of the replication factor for fault tolerance. All the data splits
    will be processed by TaskTracker for the Map and Reduce tasks in a parallel manner.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据集需要上传到 Hadoop 目录，以便 MapReduce 节点可以使用它。然后，**Hadoop 分布式文件系统**（**HDFS**）将把输入数据集分割成数据块，并将它们存储到集群中的
    DataNodes，同时确保为容错设置复制因子。所有的数据块将由 TaskTracker 以并行方式处理 Map 和 Reduce 任务。
- en: 'Also, there are some alternative ways to get the dataset in HDFS with Hadoop
    components:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些替代方法可以通过 Hadoop 组件将数据集获取到 HDFS 中：
- en: '**Sqoop**: This is an open source tool designed for efficiently transferring
    bulk data between Apache Hadoop and structured, relational databases. Suppose
    your application has already been configured with the MySQL database and you want
    to use the same data for performing data analytics, Sqoop is recommended for importing
    datasets to HDFS. Also, after the completion of the data analytics process, the
    output can be exported to the MySQL database.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sqoop**：这是一个开源工具，旨在高效地在 Apache Hadoop 和结构化关系型数据库之间传输大量数据。假设你的应用已经配置了 MySQL
    数据库，并且你想用相同的数据进行数据分析，建议使用 Sqoop 将数据集导入到 HDFS。此外，数据分析过程完成后，输出可以导出到 MySQL 数据库中。'
- en: '**Flume**: This is a distributed, reliable, and available service for efficiently
    collecting, aggregating, and moving large amounts of log data to HDFS. Flume is
    able to read data from most sources, such as logfiles, sys logs, and the standard
    output of the Unix process.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Flume**：这是一个分布式、可靠且可用的服务，用于高效地收集、汇总和传输大量日志数据到 HDFS。Flume 能够从大多数源读取数据，例如日志文件、系统日志和
    Unix 进程的标准输出。'
- en: Using the preceding data collection and moving the framework can make this data
    transfer process very easy for the MapReduce application for data analytics.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的数据收集和移动框架，可以让 MapReduce 应用程序的数据传输过程变得非常简单，便于数据分析。
- en: Executing the Map phase
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行 Map 阶段
- en: Executing the client application starts the Hadoop MapReduce processes. The
    Map phase then copies the job resources (unjarred class files) and stores it to
    HDFS, and requests JobTracker to execute the job. The JobTracker initializes the
    job, retrieves the input, splits the information, and creates a Map task for each
    job.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 执行客户端应用程序启动 Hadoop MapReduce 进程。然后 Map 阶段复制作业资源（未解压的类文件）并将其存储到 HDFS，并请求 JobTracker
    执行作业。JobTracker 初始化作业，检索输入，拆分信息，并为每个作业创建一个 Map 任务。
- en: The JobTracker will call TaskTracker to run the Map task over the assigned input
    data subset. The Map task reads this input split data as input `(key, value)`
    pairs provided to the Mapper method, which then produces intermediate `(key, value)`
    pairs. There will be at least one output for each input `(key, value)` pair.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: JobTracker 将调用 TaskTracker 运行分配的输入数据子集的 Map 任务。Map 任务将此输入拆分数据作为输入 `(key, value)`
    对提供给 Mapper 方法，然后生成中间 `(key, value)` 对。对于每个输入 `(key, value)` 对至少会有一个输出。
- en: '![Executing the Map phase](img/3282OS_02_01.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![执行 Map 阶段](img/3282OS_02_01.jpg)'
- en: Mapping individual elements of an input list
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 映射输入列表的各个元素
- en: The list of (key, value) pairs is generated such that the key attribute will
    be repeated many times. So, its key attribute will be re-used in the Reducer for
    aggregating values in MapReduce. As far as format is concerned, Mapper output
    format values and Reducer input values must be the same.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的 (key, value) 对列表是这样生成的，即键属性将被多次重复使用。因此，对于在 MapReduce 中聚合值的 Reducer，它的键属性将被重新使用。就格式而言，Mapper
    输出格式的值和 Reducer 输入的值必须相同。
- en: After the completion of this Map operation, the TaskTracker will keep the result
    in its buffer storage and local disk space (if the output data size is more than
    the threshold).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成此 Map 操作后，TaskTracker 将在其缓冲存储和本地磁盘空间中保留结果（如果输出数据大小超过阈值）。
- en: For example, suppose we have a `Map` function that converts the input text into
    lowercase. This will convert the list of input strings into a list of lowercase
    strings.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个将输入文本转换为小写的 `Map` 函数。这将把输入字符串列表转换为小写字符串列表。
- en: Tip
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Keys and values**: In MapReduce, every value has its identifier that is considered
    as key. The key-value pairs received by the Mapper are dependent on the input
    datatype as specified in the job configuration file.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**键和值**：在 MapReduce 中，每个值都有其被视为键的标识符。由 Mapper 接收的键值对依赖于作业配置文件中指定的输入数据类型。'
- en: Shuffling and sorting
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分组和排序
- en: To optimize the MapReduce program, this intermediate phase is very important.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要优化 MapReduce 程序，这个中间阶段非常重要。
- en: As soon as the Mapper output from the Map phase is available, this intermediate
    phase will be called automatically. After the completion of the Map phase, all
    the emitted intermediate (key, value) pairs will be partitioned by a Partitioner
    at the Mapper side, only if the Partitioner is present. The output of the Partitioner
    will be sorted out based on the key attribute at the Mapper side. Output from
    sorting the operation is stored on buffer memory available at the Mapper node,
    TaskTracker.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Mapper 阶段的 Map 输出可用，此中间阶段将自动调用。在 Map 阶段完成后，所有发出的中间 (key, value) 对将由 Mapper
    侧的分区器进行分区，只要分区器存在。分区器的输出将根据 Mapper 侧的键属性进行排序。排序操作的输出存储在 Mapper 节点 TaskTracker
    上可用的缓冲内存中。
- en: The Combiner is often the Reducer itself. So by compression, it's not **Gzip**
    or some similar compression but the Reducer on the node that the map is outputting
    the data on. The data returned by the Combiner is then shuffled and sent to the
    reduced nodes. To speed up data transmission of the Mapper output to the Reducer
    slot at TaskTracker, you need to compress that output with the `Combiner` function.
    By default, the Mapper output will be stored to buffer memory, and if the output
    size is larger than threshold, it will be stored to a local disk. This output
    data will be available through **Hypertext Transfer Protocol** (**HTTP**).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Combiner 通常就是 Reducer 本身。因此，通过压缩，它不是 **Gzip** 或类似的压缩，而是 Map 输出数据的 Reducer 节点。由
    Combiner 返回的数据然后被分组并发送到减少节点。为了加速 Mapper 输出到 TaskTracker 的 Reducer 插槽的数据传输，需要使用
    `Combiner` 函数压缩该输出。默认情况下，Mapper 输出将存储到缓冲内存中，如果输出大小大于阈值，则会存储到本地磁盘。这些输出数据可通过 **超文本传输协议**
    (**HTTP**) 访问。
- en: Reducing phase execution
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 减少阶段的执行
- en: As soon as the Mapper output is available, TaskTracker in the Reducer node will
    retrieve the available partitioned Map's output data, and they will be grouped
    together and merged into one large file, which will then be assigned to a process
    with a `Reducer` method. Finally, this will be sorted out before data is provided
    to the `Reducer` method.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Mapper 输出可用，Reducer 节点上的 TaskTracker 将检索可用的分区 Map 输出数据，这些数据将被分组并合并成一个大文件，然后分配给一个包含
    `Reducer` 方法的进程。最后，在数据提供给 `Reducer` 方法之前，会对其进行排序。
- en: The `Reducer` method receives a list of input values from an input `(key, list
    (value))` and aggregates them based on custom logic, and produces the output `(key,
    value)` pairs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`Reducer` 方法接收来自输入 `(key, list (value))` 的输入值列表，并根据自定义逻辑对它们进行聚合，生成输出 `(key,
    value)` 对。'
- en: '![Reducing phase execution](img/3282OS_02_02.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![Reduce 阶段执行](img/3282OS_02_02.jpg)'
- en: Reducing input values to an aggregate value as output
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入值减少为一个聚合值作为输出
- en: The output of the `Reducer` method of the Reduce phase will directly be written
    into HDFS as per the format specified by the MapReduce job configuration class.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Reduce 阶段的 `Reducer` 方法的输出将根据 MapReduce 作业配置类指定的格式直接写入 HDFS。
- en: Understanding the limitations of MapReduce
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 MapReduce 的局限性
- en: 'Let''s see some of Hadoop MapReduce''s limitations:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Hadoop MapReduce 的一些局限性：
- en: The MapReduce framework is notoriously difficult to leverage for transformational
    logic that is not as simple, for example, real-time streaming, graph processing,
    and message passing.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapReduce 框架对于非简单的转换逻辑（例如实时流处理、图处理和消息传递）来说，众所周知很难使用。
- en: Data querying is inefficient over distributed, unindexed data than in a database
    created with indexed data. However, if the index over the data is generated, it
    needs to be maintained when the data is removed or added.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分布式、未索引的数据上进行数据查询比在使用已索引数据创建的数据库中要低效。然而，如果为数据生成了索引，在数据删除或添加时需要维护该索引。
- en: We can't parallelize the Reduce task to the Map task to reduce the overall processing
    time because Reduce tasks do not start until the output of the Map tasks is available
    to it. (The Reducer's input is fully dependent on the Mapper's output.) Also,
    we can't control the sequence of the execution of the Map and Reduce task. But
    sometimes, based on application logic, we can definitely configure a slow start
    for the Reduce tasks at the instance when the data collection starts as soon as
    the Map tasks complete.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们无法将 Reduce 任务并行化到 Map 任务上以减少整体处理时间，因为 Reduce 任务只有在 Map 任务的输出可用时才能开始。（Reducer
    的输入完全依赖于 Mapper 的输出。）此外，我们无法控制 Map 和 Reduce 任务执行的顺序。但有时，根据应用逻辑，我们可以在 Map 任务完成后，数据收集开始时，配置一个
    Reduce 任务的慢启动。
- en: Long-running Reduce tasks can't be completed because of their poor resource
    utilization either if the Reduce task is taking too much time to complete and
    fails or if there are no other Reduce slots available for rescheduling it (this
    can be solved with YARN).
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长时间运行的 Reduce 任务无法完成，原因是资源利用率低，可能是因为 Reduce 任务花费太多时间导致失败，或者没有其他 Reduce 插槽可供重新调度（这可以通过
    YARN 来解决）。
- en: Understanding Hadoop's ability to solve problems
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Hadoop 解决问题的能力
- en: Since this book is geared towards analysts, it might be relevant to provide
    analytical examples; for instance, if the reader has a problem similar to the
    one described previously, Hadoop might be of use. Hadoop is not a universal solution
    to all Big Data issues; it's just a good technique to use when large data needs
    to be divided into small chunks and distributed across servers that need to be
    processed in a parallel fashion. This saves time and the cost of performing analytics
    over a huge dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书面向分析师，因此提供一些分析示例可能是相关的；例如，如果读者遇到与之前描述的类似问题，Hadoop 可能会有所帮助。Hadoop 不是解决所有大数据问题的万能方案；它只是当需要将大量数据分割成小块并分布到服务器上进行并行处理时，使用的一种不错的技术。这可以节省时间和在大数据集上执行分析的成本。
- en: 'If we are able to design the Map and Reduce phase for the problem, it will
    be possible to solve it with MapReduce. Generally, Hadoop provides computation
    power to process data that does not fit into machine memory. (R users mostly found
    an error message while processing large data and see the following message: cannot
    allocate vector of size 2.5 GB.)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够为问题设计 Map 和 Reduce 阶段，那么就可以使用 MapReduce 来解决它。通常，Hadoop 提供计算能力来处理不适合机器内存的数据。（R
    用户在处理大数据时，通常会遇到以下错误消息：无法分配大小为 2.5 GB 的向量。）
- en: Understanding the different Java concepts used in Hadoop programming
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Hadoop 编程中使用的不同 Java 概念
- en: 'There are some classic Java concepts that make Hadoop more interactive. They
    are as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些经典的 Java 概念，使得 Hadoop 更具互动性。它们如下：
- en: '**Remote procedure calls**: This is an interprocess communication that allows
    a computer program to cause a subroutine or procedure to execute in another address
    space (commonly on another computer on shared network) without the programmer
    explicitly coding the details for this remote interaction. That is, the programmer
    writes essentially the same code whether the subroutine is local to the executing
    program or remote.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**远程过程调用**：这是一种进程间通信方式，允许计算机程序在另一个地址空间（通常是在共享网络上的另一台计算机）执行子程序或过程，而不需要程序员明确编写远程交互的详细代码。也就是说，程序员编写的代码基本相同，无论子程序是本地执行的还是远程执行的。'
- en: '**Serialization/Deserialization**: With serialization, a **Java Virtual Machine**
    (**JVM**) can write out the state of the object to some stream so that we can
    basically read all the members and write out their state to a stream, disk, and
    so on. The default mechanism is in a binary format so it''s more compact than
    the textual format. Through this, machines can send data across the network. Deserialization
    is vice versa and is used for receiving data objects over the network.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列化/反序列化**：通过序列化，**Java 虚拟机**（**JVM**）可以将对象的状态写入某个流中，这样我们就可以基本上读取所有成员并将其状态写入流、磁盘等。默认机制采用二进制格式，因此它比文本格式更加紧凑。通过这种方式，计算机可以通过网络发送数据。反序列化则相反，用于接收网络中的数据对象。'
- en: '**Java generics**: This allows a type or method to operate on objects of various
    types while providing compile-time type safety, making Java a fully static typed
    language.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Java 泛型**：这使得类型或方法能够在不同类型的对象上操作，同时提供编译时类型安全，使得 Java 成为一种完全静态类型的语言。'
- en: '**Java collection**: This framework is a set of classes and interfaces for
    handling various types of data collection with single Java objects.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Java 集合**：该框架是一组用于处理各种类型数据集合的类和接口，能够使用单一的 Java 对象进行操作。'
- en: '**Java concurrency**: This has been designed to support concurrent programming,
    and all execution takes place in the context of threads. It is mainly used for
    implementing computational processes as a set of threads within a single operating
    system process.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Java 并发**：该设计用于支持并发编程，所有执行都发生在线程的上下文中。它主要用于将计算过程作为一组线程实现，在单个操作系统进程中执行。'
- en: '**Plain Old Java Objects** (**POJO**): These are actually ordinary JavaBeans.
    POJO is temporarily used for setting up as well as retrieving the value of data
    objects.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**普通旧 Java 对象**（**POJO**）：这些实际上是普通的 JavaBeans。POJO 被临时用来设置和获取数据对象的值。'
- en: Understanding the Hadoop MapReduce fundamentals
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Hadoop MapReduce 基本原理
- en: 'To understand Hadoop MapReduce fundamentals properly, we will:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确理解 Hadoop MapReduce 的基本原理，我们将：
- en: Understand MapReduce objects
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 MapReduce 对象
- en: Learn how to decide the number of Maps in MapReduce
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何决定 MapReduce 中的 Map 数量
- en: Learn how to decide the number of Reduces in MapReduce
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何决定 MapReduce 中的 Reduce 数量
- en: Understand MapReduce dataflow
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 MapReduce 数据流
- en: Take a closer look at Hadoop MapReduce terminologies
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更深入地了解 Hadoop MapReduce 的术语
- en: Understanding MapReduce objects
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 MapReduce 对象
- en: 'As we know, MapReduce operations in Hadoop are carried out mainly by three
    objects: Mapper, Reducer, and Driver.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，Hadoop 中的 MapReduce 操作主要由三个对象执行：Mapper、Reducer 和 Driver。
- en: '**Mapper**: This is designed for the Map phase of MapReduce, which starts MapReduce
    operations by carrying input files and splitting them into several pieces. For
    each piece, it will emit a key-value data pair as the output value.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mapper**：这是为 MapReduce 的 Map 阶段设计的，它通过携带输入文件并将其拆分成若干块来启动 MapReduce 操作。对于每一块，它会生成一个键值对作为输出值。'
- en: '**Reducer**: This is designed for the Reduce phase of a MapReduce job; it accepts
    key-based grouped data from the Mapper output, reduces it by aggregation logic,
    and emits the `(key, value)` pair for the group of values.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reducer**：这是为 MapReduce 作业的 Reduce 阶段设计的；它接受来自 Mapper 输出的按键分组的数据，通过聚合逻辑进行处理，并输出该组数据的
    `(key, value)` 键值对。'
- en: '**Driver**: This is the main file that drives the MapReduce process. It starts
    the execution of MapReduce tasks after getting a request from the client application
    with parameters. The Driver file is responsible for building the configuration
    of a job and submitting it to the Hadoop cluster. The Driver code will contain
    the `main()` method that accepts arguments from the command line. The input and
    output directory of the Hadoop MapReduce job will be accepted by this program.
    Driver is the main file for defining job configuration details, such as the job
    name, job input format, job output format, and the Mapper, Combiner, Partitioner,
    and Reducer classes. MapReduce is initialized by calling this `main()` function
    of the Driver class.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**驱动程序**：这是驱动MapReduce过程的主文件。它在从客户端应用程序获取请求和参数后启动MapReduce任务的执行。驱动程序负责构建作业的配置并将其提交到Hadoop集群。驱动程序代码包含接受命令行参数的`main()`方法。此程序将接受Hadoop
    MapReduce作业的输入和输出目录。驱动程序是定义作业配置细节的主要文件，如作业名称、作业输入格式、作业输出格式以及Mapper、Combiner、Partitioner和Reducer类。通过调用驱动程序类的`main()`函数来初始化MapReduce。'
- en: Not every problem can be solved with a single Map and single Reduce program,
    but fewer can't be solved with a single Map and single Reduce task. Sometimes,
    it is also necessary to design the MapReduce job with multiple Map and Reduce
    tasks. We can design this type of job when we need to perform data operations,
    such as data extraction, data cleaning, and data merging, together in a single
    job. Many problems can be solved by writing multiple Mapper and Reducer tasks
    for a single job. The MapReduce steps that will be called sequentially in the
    case of multiple Map and Reduce tasks are Map1 followed by Reduce1, Map2 followed
    by Reduce2, and so on.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个问题都可以用单个Map和单个Reduce程序解决，但更少的问题无法用单个Map和单个Reduce任务解决。有时，还需要设计具有多个Map和Reduce任务的MapReduce作业。当需要在单个作业中执行数据操作（如数据提取、数据清洗和数据合并）时，可以设计这种类型的作业。通过为单个作业编写多个Mapper和Reducer任务，可以解决许多问题。在多个Map和Reduce任务的情况下，将依次调用Map1后跟Reduce1，Map2后跟Reduce2等MapReduce步骤。
- en: When we need to write a MapReduce job with multiple Map and Reduce tasks, we
    have to write multiple MapReduce application drivers to run them sequentially.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要编写具有多个Map和Reduce任务的MapReduce作业时，我们必须编写多个MapReduce应用程序驱动程序来依次运行它们。
- en: At the time of the MapReduce job submission, we can provide a number of Map
    tasks, and a number of Reducers will be created based on the output from the Mapper
    input and Hadoop cluster capacity. Also, note that setting the number of Mappers
    and Reducers is not mandatory.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在提交MapReduce作业时，我们可以提供一定数量的Map任务，根据Mapper输入和Hadoop集群容量创建一定数量的Reducers。同时，注意设置Mappers和Reducers的数量并非强制要求。
- en: Deciding the number of Maps in MapReduce
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决定MapReduce中的Maps数量
- en: The number of Maps is usually defined by the size of the input data and size
    of the data split block that is calculated by the size of the HDFS file / data
    split. Therefore, if we have an HDFS datafile of 5 TB and a block size of 128
    MB, there will be 40,960 maps present in the file. But sometimes, the number of
    Mappers created will be more than this count because of speculative execution.
    This is true when the input is a file, though it entirely depends on the `InputFormat`
    class.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Map数量通常由输入数据的大小和由HDFS文件/数据划分计算的数据块大小定义。因此，如果我们有一个5 TB的HDFS数据文件和128 MB的块大小，文件中将有40,960个Map。但有时，由于推测执行，创建的Mapper数量将超过此计数。当输入是文件时，这是真实的，尽管它完全取决于`InputFormat`类。
- en: In Hadoop MapReduce processing, there will be a delay in the result of the job
    when the assigned Mapper or Reducer is taking a long time to finish. If you want
    to avoid this, speculative execution in Hadoop can run multiple copies of the
    same Map or Reduce task on different nodes, and the result from the first completed
    nodes can be used. From the Hadoop API with the `setNumMapTasks(int)` method,
    we can get an idea of the number of Mappers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop MapReduce处理中，当分配的Mapper或Reducer需要较长时间完成时，作业结果会有延迟。如果要避免这种情况，在Hadoop中的推测执行可以在不同节点上运行同一Map或Reduce任务的多个副本，并可以使用首次完成节点的结果。通过Hadoop
    API的`setNumMapTasks(int)`方法，我们可以了解Mapper的数量。
- en: Deciding the number of Reducers in MapReduce
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决定MapReduce中Reducers的数量
- en: A numbers of Reducers are created based on the Mapper's input. However, if you
    hardcode the number of Reducers in MapReduce, it won't matter how many nodes are
    present in a cluster. It will be executed as specified in the configuration.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Reducer 的数量是根据 Mapper 的输入创建的。然而，如果你在 MapReduce 中硬编码 Reducer 的数量，集群中有多少节点也无关紧要。它将按照配置中指定的方式执行。
- en: Additionally, we can set the number of Reducers at runtime along with the MapReduce
    command at the command prompt `-D mapred.reduce.tasks`, with the number you want.
    Programmatically, it can be set via `conf.setNumReduceTasks(int)`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以在运行时通过命令行 `-D mapred.reduce.tasks` 设置 Reducer 的数量，并指定需要的数量。程序上也可以通过 `conf.setNumReduceTasks(int)`
    设置。
- en: Understanding MapReduce dataflow
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 MapReduce 数据流
- en: 'Now that we have seen the components that make a basic MapReduce job possible,
    we will distinguish how everything works together at a higher level. From the
    following diagram, we will understand MapReduce dataflow with multiple nodes in
    a Hadoop cluster:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了构成基本 MapReduce 作业所需的组件，我们将从更高层次区分每个部分如何协同工作。从下面的图示中，我们将理解 Hadoop 集群中多个节点的
    MapReduce 数据流：
- en: '![Understanding MapReduce dataflow](img/3282OS_02_03.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![理解 MapReduce 数据流](img/3282OS_02_03.jpg)'
- en: MapReduce dataflow
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 数据流
- en: 'The two APIs available for Hadoop MapReduce are: New (Hadoop 1.x and 2.x) and
    Old Hadoop (0.20). YARN is the next generation of Hadoop MapReduce and the new
    Apache Hadoop subproject that has been released for Hadoop resource management.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop MapReduce 可用的两种 API 是：新版本（Hadoop 1.x 和 2.x）和旧版本 Hadoop（0.20）。YARN 是下一代
    Hadoop MapReduce，是为 Hadoop 资源管理发布的新 Apache Hadoop 子项目。
- en: 'Hadoop data processing includes several tasks that help achieve the final output
    from an input dataset. These tasks are as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 数据处理包括多个任务，帮助从输入数据集中获得最终输出。这些任务如下：
- en: Preloading data in HDFS.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 HDFS 中预加载数据。
- en: Running MapReduce by calling Driver.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 Driver 来运行 MapReduce。
- en: Reading of input data by the Mappers, which results in the splitting of the
    data execution of the Mapper custom logic and the generation of intermediate key-value
    pairs
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由 Mappers 读取输入数据，这会导致数据的拆分，执行 Mapper 自定义逻辑，并生成中间的键值对。
- en: Executing Combiner and the shuffle phase to optimize the overall Hadoop MapReduce
    process.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行 Combiner 和 shuffle 阶段，以优化整体 Hadoop MapReduce 过程。
- en: Sorting and providing of intermediate key-value pairs to the Reduce phase. The
    Reduce phase is then executed. Reducers take these partitioned key-value pairs
    and aggregate them based on Reducer logic.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对中间键值对进行排序，并将其提供给 Reduce 阶段。然后执行 Reduce 阶段。Reducers 根据 Reducer 逻辑聚合这些分区的键值对。
- en: The final output data is stored at HDFS.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终的输出数据存储在 HDFS 中。
- en: 'Here, Map and Reduce tasks can be defined for several data operations as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，可以为以下几种数据操作定义 Map 和 Reduce 任务：
- en: Data extraction
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据提取
- en: Data loading
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据加载
- en: Data segmentation
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分割
- en: Data cleaning
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据清洗
- en: Data transformation
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转化
- en: Data integration
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集成
- en: We will explore MapReduce tasks in more detail in the next part of this chapter.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的下一部分更详细地探讨 MapReduce 任务。
- en: Taking a closer look at Hadoop MapReduce terminologies
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更详细地了解 Hadoop MapReduce 术语
- en: In this section, we will see further details on Hadoop MapReduce dataflow with
    several MapReduce terminologies and their Java class details. In the MapReduce
    dataflow figure in the previous section, multiple nodes are connected across the
    network for performing distributed processing with a Hadoop setup. The ensuing
    attributes of the Map and Reduce phases play an important role for getting the
    final output.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进一步了解 Hadoop MapReduce 数据流，并介绍多个 MapReduce 术语及其 Java 类细节。在前面一节的 MapReduce
    数据流图中，多个节点通过网络连接进行分布式处理，基于 Hadoop 设置。Map 和 Reduce 阶段的随之属性在获取最终输出时起着重要作用。
- en: 'The attributes of the Map phase are as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Map 阶段的属性如下：
- en: The `InputFiles` term refers to input, raw datasets that have been created/extracted
    to be analyzed for business analytics, which have been stored in HDFS. These input
    files are very large, and they are available in several types.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`InputFiles` 术语指的是已经创建/提取并存储在 HDFS 中，供业务分析使用的输入原始数据集。这些输入文件非常大，并且有多种类型。'
- en: The `InputFormat` is a Java class to process the input files by obtaining the
    text of each line of offset and the contents. It defines how to split and read
    input data files. We can set the several input types, such as `TextInputFormat`,
    `KeyValueInputFormat`, and `SequenceFileInputFormat`, of the input format that
    are relevant to the Map and Reduce phase.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`InputFormat` 是一个 Java 类，用于通过获取每行的偏移量和内容来处理输入文件。它定义了如何拆分和读取输入数据文件。我们可以设置多个输入类型，例如
    `TextInputFormat`、`KeyValueInputFormat` 和 `SequenceFileInputFormat`，这些输入格式与 Map
    和 Reduce 阶段相关。'
- en: The `InputSplits` class is used for setting the size of the data split.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`InputSplits` 类用于设置数据分割的大小。'
- en: The `RecordReader` is a Java class that comes with several methods to retrieve
    key and values by iterating them among the data splits. Also, it includes other
    methods to get the status on the current progress.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RecordReader` 是一个 Java 类，提供多个方法，通过迭代数据分割来检索键和值。它还包括其他方法，用于获取当前进度的状态。'
- en: 'The `Mapper` instance is created for the Map phase. The `Mapper` class takes
    input `(key, value)` pairs (generated by RecordReader) and produces an intermediate
    `(key, value)` pair by performing user-defined code in a `Map()` method. The `Map()`
    method mainly takes two input parameters: key and value; the remaining ones are
    `OutputCollector` and `Reporter. OutputCollector`. They will provide intermediate
    the key-value pair to reduce the phase of the job. Reporter provides the status
    of the current job to JobTracker periodically. The JobTracker will aggregate them
    for later retrieval when the job ends.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 Map 阶段创建 `Mapper` 实例。`Mapper` 类接受输入的 `(key, value)` 对（由 RecordReader 生成），并通过在
    `Map()` 方法中执行用户定义的代码生成一个中间的 `(key, value)` 对。`Map()` 方法主要接受两个输入参数：key 和 value；其余的参数是
    `OutputCollector` 和 `Reporter`。它们将提供中间的键值对，以供作业的 Reduce 阶段使用。`Reporter` 会定期向 JobTracker
    提供当前作业的状态。JobTracker 会在作业结束时汇总这些状态，供后续检索。
- en: 'The attributes of the Reduce phase are as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Reduce 阶段的属性如下：
- en: After completing the Map phase, the generated intermediate `(key, value)` pairs
    are partitioned based on a key attribute similarity consideration in the `hash`
    function. So, each Map task may emit `(key, value)` pairs to partition; all values
    for the same key are always reduced together without it caring about which Mapper
    is its origin. This partitioning and shuffling will be done automatically by the
    MapReduce job after the completion of the Map phase. There is no need to call
    them separately. Also, we can explicitly override their logic code as per the
    requirements of the MapReduce job.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在完成 Map 阶段后，生成的中间 `(key, value)` 对会根据 `hash` 函数中的键属性相似性进行分区。因此，每个 Map 任务可能会将
    `(key, value)` 对发射到不同的分区；对于相同的键，所有值总是一起被 Reducer 处理，而不管它们来自哪个 Mapper。这个分区和洗牌操作将在
    Map 阶段完成后由 MapReduce 作业自动执行，无需单独调用。此外，我们可以根据 MapReduce 作业的要求显式覆盖它们的逻辑代码。
- en: After completing partitioning and shuffling and before initializing the Reduce
    task, the intermediate `(key, value)` pairs are sorted based on a key attribute
    value by the Hadoop MapReduce job.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在完成分区和洗牌操作并在初始化 Reduce 任务之前，Hadoop MapReduce 作业会根据键属性值对中间 `(key, value)` 对进行排序。
- en: The `Reduce` instance is created for the Reduce phase. It is a section of user-provided
    code that performs the Reduce task. A `Reduce()` method of the `Reducer` class
    mainly takes two parameters along with `OutputCollector` and `Reporter`, which
    is the same as the `Map()` function. They are the `OutputCollector` and `Reporter`
    objects. `OutputCollector` in both Map and Reduce has the same functionality,
    but in the Reduce phase, `OutputCollector` provides output to either the next
    Map phase (in case of multiple map and Reduce job combinations) or reports it
    as the final output of the jobs based on the requirement. Apart from that, `Reporter`
    periodically reports to JobTracker about the current status of the running task.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 Reduce 阶段创建 `Reduce` 实例。它是一个用户提供的代码段，用于执行 Reduce 任务。`Reducer` 类的 `Reduce()`
    方法主要接受两个参数，以及 `OutputCollector` 和 `Reporter`，与 `Map()` 函数相同。它们是 `OutputCollector`
    和 `Reporter` 对象。Map 和 Reduce 中的 `OutputCollector` 功能相同，但在 Reduce 阶段，`OutputCollector`
    将输出提供给下一个 Map 阶段（在多个 Map 和 Reduce 作业组合的情况下），或者根据需求将其报告为作业的最终输出。此外，`Reporter` 会定期向
    JobTracker 报告当前任务的状态。
- en: Finally, in `OutputFormat` the generated output (key, value) pairs are provided
    to the `OutputCollector` parameter and then written to `OutputFiles`, which is
    governed by `OutputFormat`. It controls the setting of the `OutputFiles` format
    as defined in the MapReduce Driver. The format will be chosen from either `TextOutputFormat`,
    `SequenceFileOutputFileFormat`, or `NullOutputFormat`.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，在`OutputFormat`中，生成的输出（键，值）对被提供给`OutputCollector`参数，然后写入`OutputFiles`，该过程由`OutputFormat`控制。它控制`OutputFiles`格式的设置，格式的选择由MapReduce
    Driver定义。格式将从`TextOutputFormat`、`SequenceFileOutputFileFormat`或`NullOutputFormat`中选择。
- en: The factory `RecordWriter` used by `OutputFormat` to write the output data in
    the appropriate format.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RecordWriter`是`OutputFormat`使用的工厂，用于以适当的格式写入输出数据。'
- en: The output files are the output data written to HDFS by `RecordWriter` after
    the completion of the MapReduce job.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出文件是`RecordWriter`在MapReduce作业完成后写入HDFS的输出数据。
- en: 'To run this MapReduce job efficiently, we need to have some knowledge of Hadoop
    shell commands to perform administrative tasks. Refer to the following table:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效地运行此MapReduce作业，我们需要了解一些Hadoop Shell命令以执行管理任务。请参考下表：
- en: '| Shell commands | Usage and code sample |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Shell命令 | 用法和代码示例 |'
- en: '| --- | --- |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE0]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '| To copy source paths to `stdout`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '| 将源路径复制到`stdout`：'
- en: '[PRE1]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE2]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '| To change the permissions of files:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '| 更改文件的权限：'
- en: '[PRE3]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE4]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '| To copy a file from local storage to HDFS:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '| 将文件从本地存储复制到HDFS：'
- en: '[PRE5]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE6]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '| To copy a file from HDFS to local storage:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '| 将文件从HDFS复制到本地存储：'
- en: '[PRE7]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE8]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '| To copy a file from the source to the destination in HDFS:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '| 将文件从源路径复制到目标路径（HDFS）：'
- en: '[PRE9]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE10]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '| To display the aggregate length of a file:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '| 显示文件的总长度：'
- en: '[PRE11]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '|'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE12]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '| To display the summary of file length:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '| 显示文件长度的汇总信息：'
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE14]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '| To copy files to a local filesystem:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '| 将文件复制到本地文件系统：'
- en: '[PRE15]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '|'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '| To list all files in the current directory in HDFS:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '| 在HDFS中列出当前目录的所有文件：'
- en: '[PRE17]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE18]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '| To create a directory in HDFS:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '| 在HDFS中创建目录：'
- en: '[PRE19]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE20]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '| To move files from the source to the destination:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '| 将文件从源路径移动到目标路径：'
- en: '[PRE21]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '|'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE22]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '| To remove files from the current directory:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '| 从当前目录中删除文件：'
- en: '[PRE23]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '|'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE24]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '| To change the replication factor of a file:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '| 更改文件的副本因子：'
- en: '[PRE25]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '|'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE26]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '| To display the last kilobyte of a file to `stdout`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '| 将文件的最后一个千字节显示到`stdout`：'
- en: '[PRE27]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '|'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Writing a Hadoop MapReduce example
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写一个Hadoop MapReduce示例
- en: Now we will move forward with MapReduce by learning a very common and easy example
    of word count. The goal of this example is to calculate how many times each word
    occurs in the provided documents. These documents can be considered as input to
    MapReduce's file.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过学习一个非常常见且简单的单词计数示例来继续深入了解MapReduce。此示例的目标是计算每个单词在提供的文档中出现的次数。这些文档可以视为MapReduce的输入文件。
- en: In this example, we already have a set of text files—we want to identify the
    frequency of all the unique words existing in the files. We will get this by designing
    the Hadoop MapReduce phase.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们已经有了一组文本文件——我们想要识别文件中所有唯一单词的出现频率。我们将通过设计Hadoop MapReduce阶段来实现这一点。
- en: In this section, we will see more on Hadoop MapReduce programming using Hadoop
    MapReduce's old API. Here we assume that the reader has already set up the Hadoop
    environment as described in [Chapter 1](ch01.html "Chapter 1. Getting Ready to
    Use R and Hadoop"), *Getting Ready to Use R and Hadoop*. Also, keep in mind that
    we are not going to use R to count words; only Hadoop will be used here.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将更多地了解使用Hadoop MapReduce的旧API进行编程。在这里，我们假设读者已经按照[第1章](ch01.html "第1章：准备使用R和Hadoop")中的说明设置好了Hadoop环境，*准备使用R和Hadoop*。另外，请记住，我们在此不会使用R来计数单词；这里只使用Hadoop。 '
- en: 'Basically, Hadoop MapReduce has three main objects: Mapper, Reducer, and Driver.
    They can be developed with three Java classes; they are the `Map` class, `Reduce`
    class, and `Driver` class, where the `Map` class denotes the Map phase, the `Reducer`
    class denotes the Reduce phase, and the `Driver` class denotes the class with
    the `main()` method to initialize the Hadoop MapReduce program.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，Hadoop MapReduce有三个主要对象：Mapper、Reducer和Driver。它们可以通过三个Java类进行开发；分别是`Map`类、`Reduce`类和`Driver`类，其中`Map`类表示Map阶段，`Reducer`类表示Reduce阶段，`Driver`类表示包含`main()`方法以初始化Hadoop
    MapReduce程序的类。
- en: In the previous section of Hadoop MapReduce fundamentals, we already discussed
    what Mapper, Reducer, and Driver are. Now, we will learn how to define them and
    program for them in Java. In upcoming chapters, we will be learning to do more
    with a combination of R and Hadoop.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节的Hadoop MapReduce基础中，我们已经讨论了什么是Mapper、Reducer和Driver。现在，我们将学习如何在Java中定义它们并进行编程。在接下来的章节中，我们将学习如何将R和Hadoop结合使用做更多的事情。
- en: Tip
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: There are many languages and frameworks that are used for building MapReduce,
    but each of them has different strengths. There are multiple factors that by modification
    can provide high latency over MapReduce. Refer to the article *10 MapReduce Tips*
    by Cloudera at [http://blog.cloudera.com/blog/2009/05/10-mapreduce-tips/](http://blog.cloudera.com/blog/2009/05/10-mapreduce-tips/).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多语言和框架用于构建MapReduce，但每种语言或框架都有其独特的优势。有多个因素可以通过修改来提高MapReduce的延迟。请参考Cloudera的文章
    *10 MapReduce Tips* [http://blog.cloudera.com/blog/2009/05/10-mapreduce-tips/](http://blog.cloudera.com/blog/2009/05/10-mapreduce-tips/)。
- en: To make MapReduce development easier, use **Eclipse** configured with **Maven**,
    which supports the old MapReduce API.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化MapReduce开发，使用**Eclipse**并配置**Maven**，该工具支持旧版MapReduce API。
- en: Understanding the steps to run a MapReduce job
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解运行MapReduce作业的步骤
- en: 'Let''s see the steps to run a MapReduce job with Hadoop:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下运行MapReduce作业的步骤：
- en: In the initial steps of preparing Java classes, we need you to develop a Hadoop
    MapReduce program as per the definition of our business problem. In this example,
    we have considered a word count problem. So, we have developed three Java classes
    for the MapReduce program; they are `Map.java`, `Reduce.java`, and `WordCount.java`,
    used for calculating the frequency of the word in the provided text files.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在准备Java类的初始步骤中，我们需要您根据我们的业务问题定义开发一个Hadoop MapReduce程序。在这个示例中，我们考虑了一个词频统计问题。因此，我们开发了三个用于MapReduce程序的Java类，它们分别是
    `Map.java`、`Reduce.java` 和 `WordCount.java`，用于计算提供的文本文件中单词的频率。
- en: '`Map.java`: This is the Map class for the word count Mapper.'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Map.java`：这是用于词频统计Mapper的Map类。'
- en: '[PRE28]'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`Reduce.java`: This is the Reduce class for the word count Reducer.'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Reduce.java`：这是用于词频统计Reducer的Reduce类。'
- en: '[PRE29]'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`WordCount.java`: This is the task of Driver in the Hadoop MapReduce Driver
    main file.'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WordCount.java`：这是Hadoop MapReduce Driver主文件中的Driver任务。'
- en: '[PRE30]'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Compile the Java classes.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译Java类。
- en: '[PRE31]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Create a `.jar` file from the compiled classes.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从编译后的类创建一个`.jar`文件。
- en: '[PRE32]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Start the Hadoop daemons.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Hadoop守护进程。
- en: '[PRE33]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Check all the running daemons.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查所有正在运行的守护进程。
- en: '[PRE34]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Create the HDFS directory `/wordcount/input/`.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建HDFS目录 `/wordcount/input/`。
- en: '[PRE35]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Extract the input dataset to be used in the word count example. As we need to
    have text files to be processed by the word count example, we will use the text
    files provided with the Hadoop distribution (`CHANGES.txt`, `LICENSE.txt`, `NOTICE.txt`,
    and `README.txt`) by copying them to the Hadoop directory. We can have other text
    datasets from the Internet input in this MapReduce algorithm instead of using
    readymade text files. We can also extract data from the Internet to process them,
    but here we are using readymade input files.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取用于词频统计示例的输入数据集。由于我们需要有文本文件来供词频统计示例处理，因此我们将通过将Hadoop发行版中提供的文本文件（`CHANGES.txt`、`LICENSE.txt`、`NOTICE.txt`
    和 `README.txt`）复制到Hadoop目录来使用它们。我们也可以在这个MapReduce算法中使用来自互联网的其他文本数据集，而不是使用现成的文本文件。我们也可以从互联网上提取数据进行处理，但在这里我们使用的是现成的输入文件。
- en: Copy all the text files to HDFS.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有文本文件复制到HDFS。
- en: '[PRE36]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Run the Hadoop MapReduce job with the following command:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行Hadoop MapReduce作业：
- en: '[PRE37]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This is how the final output will look.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就是最终输出的样子。
- en: '[PRE38]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Tip
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'During the MapReduce phase, you need to monitor the job as well as the nodes.
    Use the following to monitor MapReduce jobs in web browsers:'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在MapReduce阶段，您需要监控作业和节点。使用以下方法在网页浏览器中监控MapReduce作业：
- en: 'localhost:50070: NameNode Web interface (for HDFS)'
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: localhost:50070：NameNode Web界面（用于HDFS）
- en: '`localhost:50030`: JobTracker Web interface (for MapReduce layer)'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`localhost:50030`：JobTracker Web界面（用于MapReduce层）'
- en: '`localhost:50060`: TaskTracker Web interface (for MapReduce layer)'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`localhost:50060`：TaskTracker Web界面（用于MapReduce层）'
- en: Learning to monitor and debug a Hadoop MapReduce job
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习如何监控和调试Hadoop MapReduce作业
- en: In this section, we will learn how to monitor as well as debug a Hadoop MapReduce
    job without any commands.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何在没有命令的情况下监控和调试Hadoop MapReduce作业。
- en: 'This is one of the easiest ways to use the Hadoop MapReduce administration
    UI. We can access this via a browser by entering the URL `http://localhost:50030`
    (web UI for the JobTracker daemon). This will show the logged information of the
    Hadoop MapReduce jobs, which looks like following screenshot:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用 Hadoop MapReduce 管理 UI 的最简单方式之一。我们可以通过浏览器访问 `http://localhost:50030`（JobTracker
    守护进程的 web UI）。这将显示 Hadoop MapReduce 作业的日志信息，界面如下所示：
- en: '![Learning to monitor and debug a Hadoop MapReduce job](img/3282OS_02_04.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![学习监控和调试 Hadoop MapReduce 作业](img/3282OS_02_04.jpg)'
- en: Map/Reduce administration
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Map/Reduce 管理
- en: Here we can check the information and status of running jobs, the status of
    the Map and Reduce tasks of a job, and the past completed jobs as well as failed
    jobs with failed Map and Reduce tasks. Additionally, we can debug a MapReduce
    job by clicking on the hyperlink of the failed Map or Reduce task of the failed
    job. This will produce an error message printed on standard output while the job
    is running.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以查看正在运行的作业的信息和状态，作业的 Map 和 Reduce 任务状态，以及过去已完成的作业和失败的作业（包含失败的 Map 和 Reduce
    任务）。此外，我们还可以通过点击失败作业中失败的 Map 或 Reduce 任务的超链接来调试 MapReduce 作业。这样会在作业运行时，在标准输出上显示错误信息。
- en: Exploring HDFS data
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 浏览 HDFS 数据
- en: In this section, we will see how to explore HDFS directories without running
    any **Bash** command. The web UI of the NameNode daemon provides such a facility.
    We just need to locate it at `http://localhost:50070`.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将看到如何在不运行任何 **Bash** 命令的情况下浏览 HDFS 目录。NameNode 守护进程的 web UI 提供了这样的功能。我们只需要在浏览器中访问
    `http://localhost:50070` 即可。
- en: '![Exploring HDFS data](img/3282OS_02_05.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![浏览 HDFS 数据](img/3282OS_02_05.jpg)'
- en: NameNode administration
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: NameNode 管理
- en: This UI enables us to get a cluster summary (memory status), NameNode logs,
    as well as information on live and dead nodes in the cluster. Also, this allows
    us to explore the Hadoop directory that we have created for storing input and
    output data for Hadoop MapReduce jobs.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 UI 使我们能够获取集群摘要（内存状态）、NameNode 日志以及集群中活跃和死节点的信息。此外，它还允许我们浏览为 Hadoop MapReduce
    作业存储输入和输出数据的 Hadoop 目录。
- en: Understanding several possible MapReduce definitions to solve business problems
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解几种可能的 MapReduce 定义，以解决商业问题
- en: Until now we have learned what MapReduce is and how to code it. Now, we will
    see some common MapReduce problem definitions that are used for business analytics.
    Any reader who knows MapReduce with Hadoop will easily be able to code and solve
    these problem definitions by modifying the MapReduce example for word count. The
    major changes will be in data parsing and in the logic behind operating the data.
    The major effort will be required in data collection, data cleaning, and data
    storage.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了什么是 MapReduce 以及如何编写 MapReduce 代码。接下来，我们将查看一些常见的 MapReduce 问题定义，这些问题定义通常用于商业分析。任何了解
    MapReduce 和 Hadoop 的读者，都会通过修改用于单词计数的 MapReduce 示例，轻松编写代码并解决这些问题定义。主要的变化将在数据解析和数据操作逻辑上。主要的工作将集中在数据收集、数据清理和数据存储上。
- en: '**Server web log processing**: Through this MapReduce definition, we can perform
    web log analysis. Logs of the web server provide information about web requests,
    such as requested page''s URL, date, time, and protocol. From this, we can identify
    the peak load hours of our website from the web server log and scale our web server
    configuration based on the traffic on the site. So, the identification of no traffic
    at night will help us save money by scaling down the server. Also, there are a
    number of business cases that can be solved by this web log server analysis.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器 web 日志处理**：通过这个 MapReduce 定义，我们可以进行 web 日志分析。web 服务器的日志提供了关于 web 请求的信息，比如请求的页面
    URL、日期、时间和协议。从这些信息中，我们可以从 web 服务器日志中识别出我们网站的高峰负载时段，并根据网站的流量调整 web 服务器的配置。因此，识别出夜间没有流量的时间段将帮助我们通过缩减服务器规模来节省费用。此外，还有许多商业案例可以通过这种
    web 日志服务器分析来解决。'
- en: '**Web analytics with website statistics**: Website statistics can provide more
    detailed information about the visitor''s metadata, such as the source, campaign,
    visitor type, visitor location, search keyword, requested page URL, browser, and
    total time spent on pages. Google analytics is one of the popular, free service
    providers for websites. By analyzing all this information, we can understand visitors''
    behavior on a website. By descriptive analytics, we can identify the importance
    of web pages or other web attributes based on visitors'' addiction towards them.
    For an e-commerce website, we can identify popular products based on the total
    number of visits, page views, and time spent by a visitor on a page. Also, predictive
    analytics can be implemented on web data to predict the business.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网站分析与网站统计**：网站统计能够提供关于访客元数据的更详细信息，例如来源、广告活动、访客类型、访客位置、搜索关键词、请求的页面 URL、浏览器以及在页面上花费的总时间。Google
    Analytics 是其中一个流行且免费的服务提供商，通过分析所有这些信息，我们可以了解访客在网站上的行为。通过描述性分析，我们可以基于访客对页面的依赖性，识别网站页面或其他网页属性的重要性。对于电子商务网站，我们可以根据访客的访问总数、页面浏览量以及在页面上花费的时间来识别热门产品。此外，还可以在网站数据上实施预测分析，以预测业务趋势。'
- en: '**Search engine**: Suppose we have a large set of documents and want to search
    the document for a specific keyword, inverted indices with Hadoop MapReduce will
    help us find keywords so we can build a search engine for Big Data.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索引擎**：假设我们有大量的文档，并希望在这些文档中搜索特定的关键词，使用 Hadoop MapReduce 的倒排索引将帮助我们找到关键词，从而为大数据构建一个搜索引擎。'
- en: '**Stock market analysis**: Let''s say that we have collected stock market data
    (Big Data) for a long period of time and now want to identify the pattern and
    predict it for the next time period. This requires training of all historical
    datasets. Then we can compute the frequency of the stock market changes for the
    said time period using several machine-learning libraries with Hadoop MapReduce.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**股市分析**：假设我们收集了长期的股市数据（大数据），现在希望识别其中的模式并预测下一时间段的走势。这需要对所有历史数据集进行训练。然后，我们可以使用多个机器学习库与
    Hadoop MapReduce 计算股市在特定时间段内的变化频率。'
- en: Also, there are too many possible MapReduce applications that can be applied
    to improve business cost.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有许多可以应用 MapReduce 的场景，帮助降低企业成本。
- en: Learning the different ways to write Hadoop MapReduce in R
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习在 R 中编写 Hadoop MapReduce 的不同方法
- en: We know that Hadoop Big Data processing with MapReduce is a big deal for statisticians,
    web analysts, and product managers who used to use the R tool for analyses because
    supplementary programming knowledge of MapReduce is required to migrate the analyses
    into MapReduce with Hadoop. Also, we know R is a tool that is consistently increasing
    in popularity; there are many packages/libraries that are being developed for
    integrating with R. So to develop a MapReduce algorithm or program that runs with
    the log of R and computation power of Hadoop, we require the middleware for R
    and Hadoop. RHadoop, RHIPE, and Hadoop streaming are the middleware that help
    develop and execute Hadoop MapReduce within R. In this last section, we will talk
    about RHadoop, RHIPE, and introducing Hadoop streaming, and from the later chapters
    we will purely develop MapReduce with these packages.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，Hadoop 大数据处理与 MapReduce 对于统计学家、网站分析师和产品经理来说非常重要，尤其是对于那些曾经使用 R 工具进行分析的人，因为将分析迁移到
    MapReduce 和 Hadoop 上需要额外的编程知识。同时，我们也知道 R 是一个持续增长受欢迎的工具；有许多开发中的包/库可以与 R 集成。因此，要开发一个结合
    R 的日志和 Hadoop 的计算能力的 MapReduce 算法或程序，我们需要 R 和 Hadoop 之间的中间件。RHadoop、RHIPE 和 Hadoop
    streaming 是帮助在 R 中开发和执行 Hadoop MapReduce 的中间件。在最后一部分中，我们将介绍 RHadoop、RHIPE，并介绍
    Hadoop streaming，从后面的章节开始，我们将仅使用这些包来开发 MapReduce。
- en: Learning RHadoop
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习 RHadoop
- en: 'RHadoop is a great open source software framework of R for performing data
    analytics with the Hadoop platform via R functions. RHadoop has been developed
    by **Revolution Analytics**, which is the leading commercial provider of software
    and services based on the open source R project for statistical computing. The
    RHadoop project has three different R packages: `rhdfs`, `rmr`, and `rhbase`.
    All these packages are implemented and tested on the Cloudera Hadoop distributions
    CDH3, CDH4, and R 2.15.0\. Also, these are tested with the R version 4.3, 5.0,
    and 6.0 distributions of Revolution Analytics.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: RHadoop是一个出色的开源软件框架，用于通过R函数在Hadoop平台上执行数据分析。RHadoop由**Revolution Analytics**开发，是基于开源R项目进行统计计算的领先商业软件和服务提供商。RHadoop项目有三个不同的R包：`rhdfs`、`rmr`和`rhbase`。所有这些包都在Cloudera的Hadoop发行版CDH3、CDH4和R
    2.15.0上实现和测试。此外，这些包还与Revolution Analytics的R版本4.3、5.0和6.0发行版进行了测试。
- en: 'These three different R packages have been designed on Hadoop''s two main features
    HDFS and MapReduce:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个不同的R包是基于Hadoop的两个主要特性HDFS和MapReduce设计的：
- en: '`rhdfs`: This is an R package for providing all Hadoop HDFS access to R. All
    distributed files can be managed with R functions.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhdfs`：这是一个为R提供所有Hadoop HDFS访问的R包。所有分布式文件都可以通过R函数进行管理。'
- en: '`rmr`: This is an R package for providing Hadoop MapReduce interfaces to R.
    With the help of this package, the Mapper and Reducer can easily be developed.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rmr`：这是一个为R提供Hadoop MapReduce接口的R包。借助这个包，Mapper和Reducer可以轻松开发。'
- en: '`rhbase`: This is an R package for handling data at HBase distributed database
    through R.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhbase`：这是一个用于通过R处理HBase分布式数据库数据的R包。'
- en: Learning RHIPE
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习RHIPE
- en: '**R and Hadoop Integrated Programming Environment** (**RHIPE**) is a free and
    open source project. RHIPE is widely used for performing Big Data analysis with
    **D&R** analysis. D&R analysis is used to divide huge data, process it in parallel
    on a distributed network to produce intermediate output, and finally recombine
    all this intermediate output into a set. RHIPE is designed to carry out D&R analysis
    on complex Big Data in R on the Hadoop platform. RHIPE was developed by *Saptarshi
    Joy Guha* (Data Analyst at Mozilla Corporation) and her team as part of her PhD
    thesis in the Purdue Statistics Department.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**R和Hadoop集成编程环境**（**RHIPE**）是一个自由和开源的项目。RHIPE广泛用于通过**D&R**分析执行大数据分析。D&R分析用于将庞大的数据划分、在分布式网络上并行处理以生成中间输出，最后将所有这些中间输出重新组合成一个集合。RHIPE旨在通过R在Hadoop平台上进行复杂大数据的D&R分析。RHIPE由*Saptarshi
    Joy Guha*（Mozilla公司数据分析师）及其团队开发，作为她在普渡大学统计系的博士论文的一部分。'
- en: Learning Hadoop streaming
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习Hadoop streaming
- en: Hadoop streaming is a utility that comes with the Hadoop distribution. This
    utility allows you to create and run MapReduce jobs with any executable or script
    as the Mapper and/or Reducer. This is supported by R, Python, Ruby, Bash, Perl,
    and so on. We will use the R language with a bash script.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop streaming是Hadoop发行版中提供的一个工具。该工具允许你使用任何可执行文件或脚本作为Mapper和/或Reducer创建并运行MapReduce作业。它支持R、Python、Ruby、Bash、Perl等语言。我们将使用R语言和bash脚本。
- en: Also, there is one R package named `HadoopStreaming` that has been developed
    for performing data analysis on Hadoop clusters with the help of R scripts, which
    is an interface to Hadoop streaming with R. Additionally, it also allows the running
    of MapReduce tasks without Hadoop. This package was developed by *David Rosenberg*,
    Chief Scientist at SenseNetworks. He has expertise in machine learning and statistical
    modeling.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，还有一个名为`HadoopStreaming`的R包，它是为在Hadoop集群上通过R脚本执行数据分析而开发的，这是一个R与Hadoop streaming的接口。此外，它还允许在没有Hadoop的情况下运行MapReduce任务。该包由*David
    Rosenberg*（SenseNetworks的首席科学家）开发，他在机器学习和统计建模方面具有专业知识。
- en: Summary
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have seen what Hadoop MapReduce is, and how to develop it
    as well as run it. In the next chapter, we will learn how to install RHIPE and
    RHadoop, and develop MapReduce and its available functional libraries with examples.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经了解了Hadoop MapReduce是什么，以及如何开发和运行它。在下一章中，我们将学习如何安装RHIPE和RHadoop，并通过示例开发MapReduce及其可用的功能库。
