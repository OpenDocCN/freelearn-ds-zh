- en: Chapter 1. Distributed Word Count
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。分布式单词计数
- en: In this chapter, we will introduce you to the core concepts involved in creating
    distributed stream processing applications with Storm. We do this by building
    a simple application that calculates a running word count from a continuous stream
    of sentences. The word count example involves many of the structures, techniques,
    and patterns required for more complex computation, yet it is simple and easy
    to follow.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍使用Storm创建分布式流处理应用程序涉及的核心概念。我们通过构建一个简单的应用程序来计算连续句子流的运行单词计数来实现这一点。单词计数示例涉及许多用于更复杂计算所需的结构、技术和模式，但它简单且易于理解。
- en: We will begin with an overview of Storm's data structures and move on to implementing
    the components that comprise a fully fledged Storm application. By the end of
    the chapter, you will have gained a basic understanding of the structure of Storm
    computations, setting up a development environment, and techniques for developing
    and debugging Storm applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Storm的数据结构概述开始，然后实现组成完整Storm应用程序的组件。在本章结束时，您将对Storm计算的结构、设置开发环境以及开发和调试Storm应用程序的技术有了基本的了解。
- en: 'This chapter covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Storm's basic constructs – topologies, streams, spouts, and bolts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm的基本构造 - 拓扑、流、喷口和螺栓
- en: Setting up a Storm development environment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置Storm开发环境
- en: Implementing a basic word count application
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现基本的单词计数应用程序
- en: Parallelization and fault tolerance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行化和容错
- en: Scaling by parallelizing computation tasks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过并行化计算任务进行扩展
- en: Introducing elements of a Storm topology – streams, spouts, and bolts
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Storm拓扑的元素 - 流、喷口和螺栓
- en: In Storm, the structure of a distributed computation is referred to as a **topology**
    and is made up of streams of data, spouts (stream producers), and bolts (operations).
    Storm topologies are roughly analogous to jobs in batch processing systems such
    as Hadoop. However, while batch jobs have clearly defined beginning and end points,
    Storm topologies run forever, until explicitly killed or undeployed.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在Storm中，分布式计算的结构被称为**拓扑**，由数据流、喷口（流生产者）和螺栓（操作）组成。Storm拓扑大致类似于Hadoop等批处理系统中的作业。然而，批处理作业具有明确定义的起点和终点，而Storm拓扑会永远运行，直到明确终止或取消部署。
- en: '![Introducing elements of a Storm topology – streams, spouts, and bolts](img/8294OS_01_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![介绍Storm拓扑的元素 - 流、喷口和螺栓](img/8294OS_01_01.jpg)'
- en: A Storm topology
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Storm拓扑
- en: Streams
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流
- en: The core data structure in Storm is the *tuple*. A tuple is simply a list of
    named values (key-value pairs), and a Stream is an unbounded sequence of tuples.
    If you are familiar with **complex event processing** (**CEP**), you can think
    of Storm tuples as *events*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Storm中的核心数据结构是*元组*。元组只是具有命名值（键值对）的列表，而流是元组的无界序列。如果您熟悉**复杂事件处理**（**CEP**），您可以将Storm元组视为*事件*。
- en: Spouts
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 喷口
- en: Spouts represent the main entry point of data into a Storm topology. Spouts
    act as adapters that connect to a source of data, transform the data into tuples,
    and emit the tuples as a stream.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 喷口代表数据进入Storm拓扑的主要入口点。喷口充当连接到数据源的适配器，将数据转换为元组，并将元组作为流发出。
- en: 'As you will see, Storm provides a simple API for implementing spouts. Developing
    a spout is largely a matter of writing the code necessary to consume data from
    a raw source or API. Potential data sources include:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将看到的，Storm提供了一个简单的API来实现喷口。开发喷口主要是编写代码以从原始来源或API中获取数据。潜在的数据来源包括：
- en: Click streams from a web-based or mobile application
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自基于Web或移动应用程序的点击流
- en: Twitter or other social network feeds
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter或其他社交网络的信息源
- en: Sensor output
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传感器输出
- en: Application log events
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序日志事件
- en: Since spouts typically don't implement any specific business logic, they can
    often be reused across multiple topologies.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于喷口通常不实现任何特定的业务逻辑，它们通常可以在多个拓扑中重复使用。
- en: Bolts
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 螺栓
- en: Bolts can be thought of as the *operators* or *functions* of your computation.
    They take as input any number of streams, process the data, and optionally emit
    one or more streams. Bolts may subscribe to streams emitted by spouts or other
    bolts, making it possible to create a complex network of stream transformations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 螺栓可以被视为您计算的*运算符*或*函数*。它们接受任意数量的流作为输入，处理数据，并可选择发出一个或多个流。螺栓可以订阅喷口或其他螺栓发出的流，从而可以创建一个复杂的流转换网络。
- en: 'Bolts can perform any sort of processing imaginable and like the Spout API,
    the bolt interface is simple and straightforward. Typical functions performed
    by bolts include:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 螺栓可以执行任何想象得到的处理，就像喷口API一样，螺栓接口简单而直接。螺栓通常执行的典型功能包括：
- en: Filtering tuples
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤元组
- en: Joins and aggregations
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接和聚合
- en: Calculations
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算
- en: Database reads/writes
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库读取/写入
- en: Introducing the word count topology data flow
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍单词计数拓扑的数据流
- en: Our word count topology (depicted in the following diagram) will consist of
    a single spout connected to three downstream bolts.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的单词计数拓扑（如下图所示）将由一个连接到三个下游螺栓的喷口组成。
- en: '![Introducing the word count topology data flow](img/8294OS_01_02.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![介绍单词计数拓扑的数据流](img/8294OS_01_02.jpg)'
- en: Word count topology
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 单词计数拓扑
- en: Sentence spout
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句子喷口
- en: 'The `SentenceSpout` class will simply emit a stream of single-value tuples
    with the key name `"sentence"` and a string value (a sentence), as shown in the
    following code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`SentenceSpout`类将简单地发出一个单值元组流，键名为`"sentence"`，值为字符串（句子），如下面的代码所示：'
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To keep things simple, the source of our data will be a static list of sentences
    that we loop over, emitting a tuple for every sentence. In a real-world application,
    a spout would typically connect to a dynamic source, such as tweets retrieved
    from the Twitter API.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持简单，我们的数据源将是一个静态的句子列表，我们将循环遍历，为每个句子发出一个元组。在现实世界的应用程序中，喷口通常会连接到动态来源，例如从Twitter
    API检索的推文。
- en: Introducing the split sentence bolt
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍拆分句子螺栓
- en: 'The split sentence bolt will subscribe to the sentence spout''s tuple stream.
    For each tuple received, it will look up the `"sentence"` object''s value, split
    the value into words, and emit a tuple for each word:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 拆分句子螺栓将订阅句子spout的元组流。对于接收到的每个元组，它将查找“sentence”对象的值，将该值拆分为单词，并为每个单词发出一个元组：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Introducing the word count bolt
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍单词计数螺栓
- en: 'The word count bolt subscribes to the output of the `SplitSentenceBolt` class,
    keeping a running count of how many times it has seen a particular word. Whenever
    it receives a tuple, it will increment the counter associated with a word and
    emit a tuple containing the word and the current count:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 单词计数螺栓订阅`SplitSentenceBolt`类的输出，持续计算它见过特定单词的次数。每当它接收到一个元组时，它将增加与单词关联的计数器并发出一个包含单词和当前计数的元组：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Introducing the report bolt
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍报告螺栓
- en: The report bolt subscribes to the output of the `WordCountBolt` class and maintains
    a table of all words and their corresponding counts, just like `WordCountBolt`.
    When it receives a tuple, it updates the table and prints the contents to the
    console.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 报告螺栓订阅`WordCountBolt`类的输出，并维护所有单词及其对应计数的表，就像`WordCountBolt`一样。当它接收到一个元组时，它会更新表并将内容打印到控制台。
- en: Implementing the word count topology
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现单词计数拓扑
- en: Now that we've introduced the basic Storm concepts, we're ready to start developing
    a simple application. For now, we'll be developing and running a Storm topology
    in local mode. Storm's local mode simulates a Storm cluster within a single JVM
    instance, making it easy to develop and debug Storm topologies in a local development
    environment or IDE. In later chapters, we'll show you how to take Storm topologies
    developed in local mode and deploy them to a fully clustered environment.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了基本的Storm概念，我们准备开始开发一个简单的应用程序。目前，我们将在本地模式下开发和运行Storm拓扑。Storm的本地模式在单个JVM实例中模拟了一个Storm集群，使得在本地开发环境或IDE中开发和调试Storm拓扑变得容易。在后面的章节中，我们将向您展示如何将在本地模式下开发的Storm拓扑部署到完全集群化的环境中。
- en: Setting up a development environment
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置开发环境
- en: Creating a new Storm project is just a matter of adding the Storm library and
    its dependencies to the Java classpath. However, as you'll learn in [Chapter 2](ch02.html
    "Chapter 2. Configuring Storm Clusters"), *Configuring Storm Clusters*, deploying
    a Storm topology to a clustered environment requires special packaging of your
    compiled classes and dependencies. For this reason, it is highly recommended that
    you use a build management tool such as Apache Maven, Gradle, or Leinengen. For
    the distributed word count example, we will use Maven.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的Storm项目只是将Storm库及其依赖项添加到Java类路径的问题。然而，正如您将在[第2章](ch02.html "第2章。配置Storm集群")中了解到的那样，*配置Storm集群*，将Storm拓扑部署到集群环境中需要对编译类和依赖项进行特殊打包。因此，强烈建议您使用构建管理工具，如Apache
    Maven、Gradle或Leinengen。对于分布式单词计数示例，我们将使用Maven。
- en: 'Let''s begin by creating a new Maven project:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始创建一个新的Maven项目：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, edit the `pom.xml` file and add the Storm dependency:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，编辑`pom.xml`文件并添加Storm依赖项：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, test the Maven configuration by building the project with the following
    command:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用以下命令构建项目来测试Maven配置：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Downloading the example code**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载示例代码**'
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book elsewhere, you can visit [http://www.packtpub.com/ support](http://www.packtpub.com/
    support) and register to have the files e-mailed directly to you.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[http://www.packtpub.com](http://www.packtpub.com)的帐户中下载您购买的所有Packt图书的示例代码文件。如果您在其他地方购买了本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便直接通过电子邮件接收文件。
- en: Maven will download the Storm library and all its dependencies. With the project
    set up, we're now ready to begin writing our Storm application.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Maven将下载Storm库及其所有依赖项。有了项目设置好了，我们现在准备开始编写我们的Storm应用程序。
- en: Implementing the sentence spout
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现句子spout
- en: To keep things simple, our `SentenceSpout` implementation will simulate a data
    source by creating a static list of sentences that gets iterated. Each sentence
    is emitted as a single field tuple. The complete spout implementation is listed
    in *Example 1.1*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们的`SentenceSpout`实现将通过创建一个静态的句子列表来模拟数据源。每个句子都作为一个单字段元组发出。完整的spout实现在*示例1.1*中列出。
- en: '**Example 1.1: SentenceSpout.java**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例1.1：SentenceSpout.java**'
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `BaseRichSpout` class is a convenient implementation of the `ISpout` and
    `IComponent` interfaces and provides default implementations for methods we don't
    need in this example. Using this class allows us to focus only on the methods
    we need.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`BaseRichSpout`类是`ISpout`和`IComponent`接口的方便实现，并为我们在这个例子中不需要的方法提供了默认实现。使用这个类可以让我们只关注我们需要的方法。'
- en: The `declareOutputFields()` method is defined in the `IComponent` interface
    that all Storm components (spouts and bolts) must implement and is used to tell
    Storm what streams a component will emit and the fields each stream's tuples will
    contain. In this case, we're declaring that our spout will emit a single (default)
    stream of tuples containing a single field (`"sentence"`).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`declareOutputFields()`方法在所有Storm组件（spouts和bolts）必须实现的`IComponent`接口中定义，并用于告诉Storm组件将发出哪些流以及每个流的元组将包含哪些字段。在这种情况下，我们声明我们的spout将发出一个包含单个字段（“sentence”）的元组的单个（默认）流。'
- en: 'The `open()` method is defined in the `ISpout` interface and is called whenever
    a spout component is initialized. The `open()` method takes three parameters:
    a map containing the Storm configuration, a `TopologyContext` object that provides
    information about a components placed in a topology, and a `SpoutOutputCollector`
    object that provides methods for emitting tuples. In this example, we don''t need
    to perform much in terms of initialization, so the `open()` implementation simply
    stores a reference to the `SpoutOutputCollector` object in an instance variable.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`open（）`方法在`ISpout`接口中定义，并在初始化spout组件时调用。`open（）`方法接受三个参数：包含Storm配置的映射，提供有关拓扑中放置的组件的信息的`TopologyContext`对象，以及提供发出元组方法的`SpoutOutputCollector`对象。在这个例子中，我们在初始化方面不需要做太多，所以`open（）`实现只是将对`SpoutOutputCollector`对象的引用存储在一个实例变量中。'
- en: The `nextTuple()` method represents the core of any spout implementation. Storm
    calls this method to request that the spout emit tuples to the output collector.
    Here, we just emit the sentence at the current index, and increment the index.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`nextTuple（）`方法代表任何spout实现的核心。Storm调用此方法请求spout向输出收集器发出元组。在这里，我们只发出当前索引处的句子，并增加索引。'
- en: Implementing the split sentence bolt
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现拆分句子螺栓
- en: The `SplitSentenceBolt` implementation is listed in *Example 1.2*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`SplitSentenceBolt`的实现在*示例1.2*中列出。'
- en: '**Example 1.2 – SplitSentenceBolt.java**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例1.2 - SplitSentenceBolt.java**'
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `BaseRichBolt` class is another convenience class that implements both the
    `IComponent` and `IBolt` interfaces. Extending this class frees us from having
    to implement methods we're not concerned with and lets us focus on the functionality
    we need.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`BaseRichBolt`类是另一个方便的类，它实现了`IComponent`和`IBolt`接口。扩展此类使我们不必实现我们不关心的方法，并让我们专注于我们需要的功能。'
- en: The `prepare()` method defined by the `IBolt` interface is analogous to the
    `open()` method of `ISpout`. This is where you would prepare resources such as
    database connections during bolt initialization. Like the `SentenceSpout` class,
    the `SplitSentenceBolt` class does not require much in terms of initialization,
    so the `prepare()` method simply saves a reference to the `OutputCollector` object.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`IBolt`接口定义的`prepare（）`方法类似于`ISpout`的`open（）`方法。这是您在螺栓初始化期间准备资源（例如数据库连接）的地方。与`SentenceSpout`类一样，`SplitSentenceBolt`类在初始化方面不需要太多，因此`prepare（）`方法只是保存对`OutputCollector`对象的引用。'
- en: In the `declareOutputFields()` method, the `SplitSentenceBolt` class declares
    a single stream of tuples, each containing one field (`"word"`).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在`declareOutputFields（）`方法中，`SplitSentenceBolt`类声明了一个包含一个字段（“word”）的元组流。
- en: The core functionality of the `SplitSentenceBolt` class is contained in the
    `execute()` method defined by `IBolt`. This method is called every time the bolt
    receives a tuple from a stream to which it subscribes. In this case, it looks
    up the value of the `"sentence"` field of the incoming tuple as a string, splits
    the value into individual words, and emits a new tuple for each word.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`SplitSentenceBolt`类的核心功能包含在`IBolt`定义的`execute（）`方法中。每次螺栓从其订阅的流接收元组时，都会调用此方法。在这种情况下，它查找传入元组的“句子”字段的值作为字符串，将该值拆分为单词，并为每个单词发出一个新元组。'
- en: Implementing the word count bolt
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现单词计数螺栓
- en: The `WordCountBolt` class (Example 1.3) is the topology component that actually
    maintains the word count. In the bolt's `prepare()` method, we instantiate an
    instance of `HashMap<String, Long>` that will store all the words and their corresponding
    counts. It is common practice to instantiate most instance variables in the `prepare()`
    method. The reason behind this pattern lies in the fact that when a topology is
    deployed, its component spouts and bolts are serialized and sent across the network.
    If a spout or bolt has any non-serializable instance variables instantiated before
    serialization (created in the constructor, for example) a `NotSerializableException`
    will be thrown and the topology will fail to deploy. In this case, since `HashMap<String,
    Long>` is serializable, we could have safely instantiated it in the constructor.
    However, in general, it is best to limit constructor arguments to primitives and
    serializable objects and instantiate non-serializable objects in the `prepare()`
    method.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`WordCountBolt`类（示例1.3）实际上是维护单词计数的拓扑组件。在螺栓的`prepare（）`方法中，我们实例化了一个`HashMap<String，Long>`的实例，该实例将存储所有单词及其相应的计数。在`prepare（）`方法中实例化大多数实例变量是常见做法。这种模式背后的原因在于拓扑部署时，其组件spouts和bolts会被序列化并通过网络发送。如果一个spout或bolt在序列化之前实例化了任何不可序列化的实例变量（例如在构造函数中创建），将抛出`NotSerializableException`，拓扑将无法部署。在这种情况下，由于`HashMap<String，Long>`是可序列化的，我们可以安全地在构造函数中实例化它。然而，一般来说，最好将构造函数参数限制为基本类型和可序列化对象，并在`prepare（）`方法中实例化不可序列化的对象。'
- en: In the `declareOutputFields()` method, the `WordCountBolt` class declares a
    stream of tuples that will contain both the word received and the corresponding
    count. In the `execute()` method, we look up the count for the word received (initializing
    it to `0` if necessary), increment and store the count, and then emit a new tuple
    consisting of the word and current count. Emitting the count as a stream allows
    other bolts in the topology to subscribe to the stream and perform additional
    processing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在`declareOutputFields（）`方法中，`WordCountBolt`类声明了一个元组流，其中包含接收到的单词和相应的计数。在`execute（）`方法中，我们查找接收到的单词的计数（必要时将其初始化为`0`），增加并存储计数，然后发出由单词和当前计数组成的新元组。将计数作为流发出允许拓扑中的其他螺栓订阅该流并执行其他处理。
- en: '**Example 1.3 – WordCountBolt.java**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例1.3 - WordCountBolt.java**'
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Implementing the report bolt
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现报告螺栓
- en: The purpose of the `ReportBolt` class is to produce a report of the counts for
    each word. Like the `WordCountBolt` class, it uses a `HashMap<String, Long>` object
    to record the counts, but in this case, it just stores the count received from
    the counter bolt.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReportBolt`类的目的是生成每个单词的计数报告。与`WordCountBolt`类一样，它使用`HashMap<String，Long>`对象记录计数，但在这种情况下，它只存储从计数螺栓接收到的计数。'
- en: One difference between the report bolt and the other bolts we've written so
    far is that it is a terminal bolt—it only receives tuples. Because it does not
    emit any streams, the `declareOutputFields()` method is left empty.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们编写的报告bolt与其他bolt之间的一个区别是它是一个终端bolt - 它只接收元组。因为它不发出任何流，所以`declareOutputFields()`方法为空。
- en: The report bolt also introduces the `cleanup()` method defined in the `IBolt`
    interface. Storm calls this method when a bolt is about to be shutdown. We exploit
    the `cleanup()` method here as a convenient way to output our final counts when
    the topology shuts down, but typically, the `cleanup()` method is used to release
    resources used by a bolt, such as open files or database connections.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 报告bolt还引入了`IBolt`接口中定义的`cleanup()`方法。当bolt即将关闭时，Storm会调用此方法。我们在这里利用`cleanup()`方法作为在拓扑关闭时输出最终计数的便捷方式，但通常，`cleanup()`方法用于释放bolt使用的资源，如打开的文件或数据库连接。
- en: One important thing to keep in mind about the `IBolt.cleanup()` method when
    writing bolts is that there is no guarantee that Storm will call it when a topology
    is running on a cluster. We'll discuss the reasons behind this when we talk about
    Storm's fault tolerance mechanisms in the next chapter. But for this example,
    we'll be running Storm in a development mode where the `cleanup()` method is guaranteed
    to be called.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写bolt时，要牢记`IBolt.cleanup()`方法的一点是，当拓扑在集群上运行时，Storm不保证会调用它。我们将在下一章讨论Storm的容错机制时讨论这背后的原因。但是在这个示例中，我们将在开发模式下运行Storm，其中保证会调用`cleanup()`方法。
- en: The full source for the `ReportBolt` class is listed in Example 1.4.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReportBolt`类的完整源代码在示例1.4中列出。'
- en: '**Example 1.4 – ReportBolt.java**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例1.4 - ReportBolt.java**'
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Implementing the word count topology
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现单词计数拓扑
- en: Now that we've defined the spout and bolts that will make up our computation,
    we're ready to wire them together into a runnable topology (refer to *Example
    1.5*).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了组成我们计算的spout和bolts，我们准备将它们连接到一个可运行的拓扑中（参考*示例1.5*）。
- en: '**Example 1.5 – WordCountTopology.java**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例1.5 - WordCountTopology.java**'
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Storm topologies are typically defined and run (or submitted if the topology
    is being deployed to a cluster) in a Java `main()` method. In this example, we
    begin by defining string constants that will serve as unique identifiers for our
    Storm components. We begin the `main()` method by instantiating our spout and
    bolts and creating an instance of `TopologyBuilder`. The `TopologyBuilder` class
    provides a fluent-style API for defining the data flow between components in a
    topology. We start by registering the sentence spout and assigning it a unique
    ID:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Storm拓扑通常在Java的`main()`方法中定义和运行（或者如果拓扑正在部署到集群，则提交）。在这个示例中，我们首先定义了字符串常量，它们将作为我们Storm组件的唯一标识符。我们通过实例化我们的spout和bolts并创建`TopologyBuilder`的实例来开始`main()`方法。`TopologyBuilder`类提供了一种流畅的API，用于定义拓扑中组件之间的数据流。我们首先注册了句子spout并为其分配了一个唯一的ID：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The next step is to register `SplitSentenceBolt` and establish a subscription
    to the stream emitted by the `SentenceSpout` class:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是注册`SplitSentenceBolt`并订阅`SentenceSpout`类发出的流：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `setBolt()` method registers a bolt with the `TopologyBuilder` class and
    returns an instance of `BoltDeclarer` that exposes methods for defining the input
    source(s) for a bolt. Here we pass in the unique ID we defined for the `SentenceSpout`
    object to the `shuffleGrouping()` method establishing the relationship. The `shuffleGrouping()`
    method tells Storm to shuffle tuples emitted by the `SentenceSpout` class and
    distribute them evenly among instances of the `SplitSentenceBolt` object. We will
    explain stream groupings in detail shortly in our discussion of parallelism in
    Storm.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`setBolt()`方法使用`TopologyBuilder`类注册一个bolt，并返回一个`BoltDeclarer`的实例，该实例公开了定义bolt的输入源的方法。在这里，我们将为`SentenceSpout`对象定义的唯一ID传递给`shuffleGrouping()`方法来建立关系。`shuffleGrouping()`方法告诉Storm对`SentenceSpout`类发出的元组进行洗牌，并将它们均匀分布在`SplitSentenceBolt`对象的实例之间。我们将在Storm的并行性讨论中很快详细解释流分组。'
- en: 'The next line establishes the connection between the `SplitSentenceBolt` class
    and the `WordCountBolt` class:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行建立了`SplitSentenceBolt`类和`WordCountBolt`类之间的连接：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you'll learn, there are times when it's imperative that tuples containing
    certain data get routed to a specific instance of a bolt. Here, we use the `fieldsGrouping()`
    method of the `BoltDeclarer` class to ensure that all tuples containing the same
    `"word"` value get routed to the same `WordCountBolt` instance.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将了解的那样，有时候有必要将包含特定数据的元组路由到特定的bolt实例。在这里，我们使用`BoltDeclarer`类的`fieldsGrouping()`方法，以确保所有包含相同“word”值的元组都被路由到同一个`WordCountBolt`实例。
- en: 'The last step in defining our data flow is to route the stream of tuples emitted
    by the `WordCountBolt` instance to the `ReportBolt` class. In this case, we want
    all tuples emitted by `WordCountBolt` routed to a single `ReportBolt` task. This
    behavior is provided by the `globalGrouping()` method, as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们数据流的最后一步是将`WordCountBolt`实例发出的元组流路由到`ReportBolt`类。在这种情况下，我们希望`WordCountBolt`发出的所有元组都路由到单个`ReportBolt`任务。这种行为由`globalGrouping()`方法提供，如下所示：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'With our data flow defined, the final step in running our word count computation
    is to build the topology and submit it to a cluster:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们定义的数据流，运行单词计数计算的最后一步是构建拓扑并将其提交到集群：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we're running Storm in local mode using Storm's `LocalCluster` class to
    simulate a full-blown Storm cluster within our local development environment.
    Local mode is a convenient way to develop and test Storm applications without
    the overhead of deploying to a distributed cluster. Local mode also allows you
    to run Storm topologies within an IDE, setting breakpoints, halting execution,
    inspecting variables and profiling the application in ways that are much more
    time consuming or near impossible when deploying to a Storm cluster.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用Storm的`LocalCluster`类在本地模式下运行Storm，以模拟在本地开发环境中完整的Storm集群。本地模式是一种方便的方式来开发和测试Storm应用程序，而不需要部署到分布式集群中的开销。本地模式还允许您在IDE中运行Storm拓扑，设置断点，停止执行，检查变量并以更加耗时或几乎不可能的方式对应用程序进行分析，而不需要部署到Storm集群。
- en: In this example, we create a `LocalCluster` instance and call the `submitTopology()`
    method with the topology name, an instance of `backtype.storm.Config`, and the
    `Topology` object returned by the `TopologyBuilder` class' `createTopology()`
    method. As you'll see in the next chapter, the `submitTopology()` method used
    to deploy a topology in local mode has the same signature as the method to deploy
    a topology in remote (distributed) mode.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们创建了一个`LocalCluster`实例，并使用拓扑名称、`backtype.storm.Config`的实例以及`TopologyBuilder`类的`createTopology()`方法返回的`Topology`对象调用了`submitTopology()`方法。正如你将在下一章中看到的，用于在本地模式部署拓扑的`submitTopology()`方法与用于在远程（分布式）模式部署拓扑的方法具有相同的签名。
- en: Storm's `Config` class is simply an extension of `HashMap<String, Object>`,
    which defines a number of Storm-specific constants and convenience methods for
    configuring a topology's runtime behavior. When a topology is submitted, Storm
    will merge its predefined default configuration values with the contents of the
    `Config` instance passed to the `submitTopology()` method, and the result will
    be passed to the `open()` and `prepare()` methods of the topology spouts and bolts
    respectively. In this sense, the `Config` object represents a set of configuration
    parameters that are global to all components in a topology.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Storm的`Config`类只是`HashMap<String, Object>`的扩展，它定义了一些Storm特定的常量和方便的方法，用于配置拓扑的运行时行为。当一个拓扑被提交时，Storm将其预定义的默认配置值与传递给`submitTopology()`方法的`Config`实例的内容合并，结果将传递给拓扑spouts和bolts的`open()`和`prepare()`方法。在这个意义上，`Config`对象代表了一组对拓扑中所有组件都是全局的配置参数。
- en: 'We''re now ready to run the `WordCountTopology` class. The `main()` method
    will submit the topology, wait for ten seconds while it runs, kill (undeploy)
    the topology, and finally shut down the local cluster. When the program run is
    complete, you should see console output similar to the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备运行`WordCountTopology`类。`main()`方法将提交拓扑，在其运行时等待十秒，终止（取消部署）拓扑，最后关闭本地集群。当程序运行完成时，您应该看到类似以下的控制台输出：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Introducing parallelism in Storm
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Storm中引入并行性
- en: Recall from the introduction that Storm allows a computation to scale horizontally
    across multiple machines by dividing the computation into multiple, independent
    *tasks* that execute in parallel across a cluster. In Storm, a task is simply
    an instance of a spout or bolt running somewhere on the cluster.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下介绍中提到的，Storm允许计算通过将计算分成多个独立的*任务*并行执行在集群中的多台机器上进行水平扩展。在Storm中，任务简单地是在集群中某处运行的spout或bolt的实例。
- en: 'To understand how parallelism works, we must first explain the four main components
    involved in executing a topology in a Storm cluster:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解并行性是如何工作的，我们必须首先解释在Storm集群中执行拓扑涉及的四个主要组件：
- en: '**Nodes (machines)**: These are simply machines configured to participate in
    a Storm cluster and execute portions of a topology. A Storm cluster contains one
    or more nodes that perform work.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点（机器）**：这些只是配置为参与Storm集群并执行拓扑部分的机器。Storm集群包含执行工作的一个或多个节点。'
- en: '**Workers (JVMs)**: These are independent JVM processes running on a node.
    Each node is configured to run one or more workers. A topology may request one
    or more workers be assigned to it.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作者（JVMs）**：这些是在节点上运行的独立JVM进程。每个节点配置为运行一个或多个工作者。一个拓扑可以请求分配给它一个或多个工作者。'
- en: '**Executors (threads)**: These are Java threads running within a worker JVM
    process. Multiple tasks can be assigned to a single executor. Unless explicitly
    overridden, Storm will assign one task for each executor.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行器（线程）**：这些是在工作者JVM进程中运行的Java线程。可以将多个任务分配给单个执行器。除非明确覆盖，否则Storm将为每个执行器分配一个任务。'
- en: '**Tasks (bolt/spout instances)**: Tasks are instances of spouts and bolts whose
    `nextTuple()` and `execute()` methods are called by executor threads.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务（bolt/spout实例）**：任务是spout和bolt的实例，其`nextTuple()`和`execute()`方法由执行器线程调用。'
- en: WordCountTopology parallelism
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WordCountTopology并行性
- en: So far in our word count example, we have not explicitly used any of Storm's
    parallelism APIs; instead, we allowed Storm to use its default settings. In most
    cases, unless overridden, Storm will default most parallelism settings to a factor
    of one.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在我们的单词计数示例中，我们并没有显式地使用Storm的并行性API；相反，我们允许Storm使用其默认设置。在大多数情况下，除非被覆盖，否则Storm将默认大多数并行性设置为一个因子。
- en: 'Before changing the parallelism settings for our topology, let''s consider
    how our topology will execute with the default settings. Assuming we have one
    machine (node), have assigned one worker to the topology, and allowed Storm to
    one task per executor, our topology execution would look like the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在更改我们拓扑的并行性设置之前，让我们考虑一下我们的拓扑将如何在默认设置下执行。假设我们有一台机器（节点），已经为拓扑分配了一个worker，并允许Storm为每个执行器分配一个任务，我们的拓扑执行将如下所示：
- en: '![WordCountTopology parallelism](img/8294OS_01_03.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![WordCountTopology并行性](img/8294OS_01_03.jpg)'
- en: Topology execution
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑执行
- en: As you can see, the only parallelism we have is at the thread level. Each task
    runs on a separate thread within a single JVM. How can we increase the parallelism
    to more effectively utilize the hardware we have at our disposal? Let's start
    by increasing the number of workers and executors assigned to run our topology.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们唯一的并行性是在线程级别。每个任务在单个JVM内的不同线程上运行。我们如何增加并行性以更有效地利用我们手头的硬件呢？让我们从增加分配给运行我们拓扑的工作进程和执行器的数量开始。
- en: Adding workers to a topology
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向拓扑添加工作进程
- en: Assigning additional workers is an easy way to add computational power to a
    topology, and Storm provides the means to do so through its API as well as pure
    configuration. Whichever method we choose, our component spouts and bolts do not
    have to change, and can be reused as is.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 分配额外的工作进程是增加拓扑的计算能力的一种简单方法，Storm提供了通过API和纯配置来实现这一点的方法。无论我们选择哪种方法，我们的组件spouts和bolts都不需要改变，可以原样重用。
- en: 'In the previous version of the word count topology, we introduced the `Config`
    object that gets passed to the `submitTopology()` method at deployment time but
    left it largely unused. To increase the number of workers assigned to a topology,
    we simply call the `setNumWorkers()` method of the `Config` object:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的单词计数拓扑的版本中，我们介绍了`Config`对象，在部署时传递给`submitTopology()`方法，但基本上没有使用。要增加分配给拓扑的工作进程数量，我们只需调用`Config`对象的`setNumWorkers()`方法：
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This assigns two workers to our topology instead of the default of one. While
    this will add computation resources to our topology, in order to effectively utilize
    those resources, we will also want to adjust the number of executors in our topology
    as well as the number of tasks per executor.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们的拓扑分配两个工作进程，而不是默认的一个。虽然这将为我们的拓扑增加计算资源，但为了有效利用这些资源，我们还需要调整拓扑中执行器的数量以及每个执行器的任务数量。
- en: Configuring executors and tasks
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置执行器和任务
- en: As we've seen, Storm creates a single task for each component defined in a topology,
    by default, and assigns a single executor for each task. Storm's parallelism API
    offers control over this behavior by allowing you to set the number of executors
    per task as well as the number of tasks per executor.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，Storm默认为拓扑中定义的每个组件创建一个任务，并为每个任务分配一个执行器。Storm的并行性API通过允许你设置每个任务的执行器数量以及每个执行器的任务数量来控制这种行为。
- en: 'The number of executors assigned to a given component is configured by setting
    a parallelism hint when defining a stream grouping. To illustrate this feature,
    let''s modify our topology definition to parallelize `SentenceSpout` such that
    it is assigned two tasks and each task is assigned its own executor thread:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义流分组时，通过设置并行性提示来配置给定组件分配的执行器数量。为了说明这个特性，让我们修改我们的拓扑定义，使`SentenceSpout`并行化，分配两个任务，并且每个任务分配自己的执行器线程：
- en: '[PRE18]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If we''re using one worker, the execution of our topology now looks like the
    following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用一个工作进程，我们拓扑的执行现在看起来像下面这样：
- en: '![Configuring executors and tasks](img/8294OS_01_04.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![配置执行器和任务](img/8294OS_01_04.jpg)'
- en: Two spout tasks
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 两个spout任务
- en: 'Next, we will set up the split sentence bolt to execute as four tasks with
    two executors. Each executor thread will be assigned two tasks to execute (4 /
    2 = 2). We''ll also configure the word count bolt to run as four tasks, each with
    its own executor thread:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将设置拆分句子的bolt以四个任务执行，每个任务有两个执行器。每个执行器线程将被分配两个任务来执行（4/2=2）。我们还将配置单词计数bolt以四个任务运行，每个任务都有自己的执行器线程：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With two workers, the execution of the topology will now look like the following
    diagram:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有了两个工作进程，拓扑的执行现在看起来像下面的图表：
- en: '![Configuring executors and tasks](img/8294OS_01_05.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![配置执行器和任务](img/8294OS_01_05.jpg)'
- en: Parallelism with multiple workers
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个工作进程的并行性
- en: 'With the topology parallelism increased, running the updated `WordCountTopology`
    class should yield higher total counts for each word:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 随着拓扑并行性的增加，运行更新的`WordCountTopology`类应该会产生每个单词的更高总计数：
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Since spout emits data indefinitely and only stops when the topology is killed,
    the actual counts will vary depending on the speed of your computer and what other
    processes are running on it, but you should see an overall increase in the number
    of words emitted and processed.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于spout会无限发出数据，并且只有在拓扑被终止时才会停止，实际的计数会根据您的计算机速度和其他正在运行的进程而变化，但您应该会看到发出和处理的单词数量总体上增加。
- en: It's important to point out that increasing the number of workers has no effect
    when running a topology in local mode. A topology running in local mode always
    runs in a single JVM process, so only task and executor parallelism settings have
    any effect. Storm's local mode offers a decent approximation of cluster behavior
    and is very useful for development, but you should always test your application
    in a true clustered environment before moving to production.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要指出，增加工作进程的数量在本地模式下运行拓扑时没有任何效果。在本地模式下运行的拓扑始终在单个JVM进程中运行，因此只有任务和执行器并行性设置才会产生任何效果。Storm的本地模式提供了对集群行为的一个不错的近似，并且对开发非常有用，但在移动到生产环境之前，您应该始终在真正的集群环境中测试您的应用程序。
- en: Understanding stream groupings
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解流分组
- en: Based on the previous example, you may wonder why we did not bother increasing
    the parallelism of `ReportBolt`. The answer is that it does not make any sense
    to do so. To understand why, you need to understand the concept of stream groupings
    in Storm.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的例子，你可能会想知道为什么我们没有费心增加`ReportBolt`的并行性。答案是这样做没有任何意义。要理解原因，你需要理解Storm中流分组的概念。
- en: A stream grouping defines how a stream's tuples are distributed among bolt tasks
    in a topology. For example, in the parallelized version of the word count topology,
    the `SplitSentenceBolt` class was assigned four tasks in the topology. The stream
    grouping determines which one of those tasks will receive a given tuple.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 流分组定义了流的元组在拓扑中的bolt任务之间如何分布。例如，在单词计数拓扑的并行化版本中，`SplitSentenceBolt`类在拓扑中被分配了四个任务。流分组确定了哪个任务会接收给定的元组。
- en: 'Storm defines seven built-in stream groupings:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 风暴定义了七种内置的流分组：
- en: '**Shuffle grouping**: This randomly distributes tuples across the target bolt''s
    tasks such that each bolt receives an equal number of tuples.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机分组**：这会随机分发元组到目标bolt任务的任务，以便每个bolt都会收到相同数量的元组。'
- en: '**Fields grouping**: This routes tuples to bolt tasks based on the values of
    the fields specified in the grouping. For example, if a stream is grouped on the
    `"word"` field, tuples with the same value for the `"word"` field will always
    be routed to the same bolt task.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字段分组**：根据分组中指定字段的值将元组路由到bolt任务。例如，如果流根据`"word"`字段分组，具有相同`"word"`字段值的元组将始终路由到同一个bolt任务。'
- en: '**All grouping**: This replicates the tuple stream across all bolt tasks such
    that each task will receive a copy of the tuple.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全部分组**：这会将元组流复制到所有bolt任务中，以便每个任务都会收到元组的副本。'
- en: '**Global grouping**: This routes all tuples in a stream to a single task, choosing
    the task with the lowest task ID value. Note that setting a parallelism hint or
    number of tasks on a bolt when using the global grouping is meaningless since
    all tuples will be routed to the same bolt task. The global grouping should be
    used with caution since it will route all tuples to a single JVM instance, potentially
    creating a bottleneck or overwhelming a specific JVM/machine in a cluster.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局分组**：这会将流中的所有元组路由到单个任务，选择具有最低任务ID值的任务。请注意，当使用全局分组时，在bolt上设置并行性提示或任务数量是没有意义的，因为所有元组都将路由到同一个bolt任务。全局分组应谨慎使用，因为它会将所有元组路由到单个JVM实例，可能会在集群中创建瓶颈或压倒特定的JVM/机器。'
- en: '**None grouping**: The none grouping is functionally equivalent to the shuffle
    grouping. It has been reserved for future use.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无分组**：无分组在功能上等同于随机分组。它已被保留以供将来使用。'
- en: '**Direct grouping**: With a direct grouping, the source stream decides which
    component will receive a given tuple by calling the `emitDirect()` method. It
    and can only be used on streams that have been declared direct streams.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接分组**：使用直接分组，源流通过调用`emitDirect()`方法决定哪个组件将接收给定的元组。它只能用于已声明为直接流的流。'
- en: '**Local or shuffle grouping**: The local or shuffle grouping is similar to
    the shuffle grouping but will shuffle tuples among bolt tasks running in the same
    worker process, if any. Otherwise, it will fall back to the shuffle grouping behavior.
    Depending on the parallelism of a topology, the local or shuffle grouping can
    increase topology performance by limiting network transfer.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地或随机分组**：本地或随机分组类似于随机分组，但会在同一工作进程中运行的bolt任务之间随机传输元组，如果有的话。否则，它将回退到随机分组的行为。根据拓扑的并行性，本地或随机分组可以通过限制网络传输来提高拓扑性能。'
- en: 'In addition to the predefined groupings, you can define your own stream grouping
    by implementing the `CustomStreamGrouping` interface:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 除了预定义的分组，您还可以通过实现`CustomStreamGrouping`接口来定义自己的流分组：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `prepare()` method is called at runtime to initiate the grouping with information
    the grouping implementation can use to make decisions on how to group tuples to
    receiving tasks. The `WorkerTopologyContext` object provides contextual information
    about the topology, and the `GlobalStreamId` object provides metadata about the
    stream being grouped on. The most useful parameter is `targetTasks`, which is
    a list of all the task identifiers the grouping needs to take into account. You
    will usually want to store the `targetTasks` parameter as an instance variable
    for reference in the implementation of the `chooseTasks()` method.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`prepare()`方法在运行时调用，以使用分组实现可以用来决定如何将元组分组到接收任务的信息。`WorkerTopologyContext`对象提供有关拓扑的上下文信息，`GlobalStreamId`对象提供有关正在分组的流的元数据。最有用的参数是`targetTasks`，它是需要考虑的所有任务标识符的列表。通常，您会希望将`targetTasks`参数存储为一个实例变量，以便在`chooseTasks()`方法的实现中进行参考。'
- en: The `chooseTasks()` method returns a list of task identifiers to which a tuple
    should be sent. Its parameters are the task identifier of the component emitting
    the tuple and the values of the tuple.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`chooseTasks()`方法返回应将元组发送到的任务标识符列表。它的参数是发出元组的组件的任务标识符和元组的值。'
- en: 'To illustrate the importance of stream groupings, let''s introduce a bug into
    our topology. Begin by modifying the `nextTuple()` method of `SentenceSpout` so
    it only emits each sentence once:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明流分组的重要性，让我们在拓扑中引入一个bug。首先修改`SentenceSpout`的`nextTuple()`方法，使其只发出每个句子一次：
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now run the topology to get the following output:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行拓扑以获得以下输出：
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now change the field grouping on the `CountBolt` parameter to a shuffle grouping
    and rerun the topology:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将`CountBolt`参数上的字段分组更改为随机分组，并重新运行拓扑：
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output should look like the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Our counts are off because the `CountBolt` parameter is stateful: it maintains
    a count for each word it''s seen. In this case, the accuracy of our computation
    depends on the ability to group based on a tuple''s content when components have
    been parallelized. The bug we introduced will only be manifested if the parallelism
    of the `CountBolt` parameter is greater than one. This underscores the importance
    of testing topologies with various parallelism configurations.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计数不准确，因为`CountBolt`参数是有状态的：它会维护每个单词的计数。在这种情况下，我们的计算准确性取决于在组件被并行化时基于元组内容进行分组的能力。我们引入的bug只有在`CountBolt`参数的并行性大于一时才会显现。这凸显了使用不同并行性配置测试拓扑的重要性。
- en: Tip
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In general, you should avoid storing state information in a bolt since any time
    a worker fails and/or has its tasks reassigned, that information will be lost.
    One solution is to periodically take a snapshot of state information to a persistent
    store, such as a database, so it can be restored if a task is reassigned.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，您应该避免在bolt中存储状态信息，因为每当一个worker失败和/或其任务被重新分配时，该信息将丢失。一种解决方案是定期将状态信息快照到持久存储中，例如数据库，以便在任务重新分配时可以恢复。
- en: Guaranteed processing
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保证处理
- en: Storm provides an API that allows you to guarantee that a tuple emitted by a
    spout is fully processed. So far in our example, we've not worried about failures.
    We've seen that a spout stream can be split and can generate any number of streams
    in a topology, depending on the behavior of downstream bolts. What happens in
    the event of a failure? As an example, consider a bolt that persists information
    to tuple data based on a database. How do we handle situations where the database
    update fails?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Storm提供了一个API，允许您保证喷嘴发出的元组被完全处理。到目前为止，在我们的示例中，我们并不担心失败。我们已经看到，喷嘴流可以被分割，并且可以根据下游螺栓的行为在拓扑中生成任意数量的流。在发生故障时会发生什么？例如，考虑一个将信息持久化到基于数据库的元组数据的螺栓。我们如何处理数据库更新失败的情况？
- en: Reliability in spouts
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 喷嘴的可靠性
- en: 'In Storm, guaranteed message processing begins with the spout. A spout that
    supports guaranteed processing needs a way to keep track of tuples it has emitted
    and be prepared to re-emit a tuple if downstream processing of that tuple, or
    any child tuples, fails. A child tuple can be thought of as any tuple emitted
    as a result of a tuple originating from a spout. Another way to look at it is
    to consider the spout''s stream(s) as the trunk of a tuple tree (shown in the
    following diagram):'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在Storm中，可靠的消息处理始于喷嘴。支持可靠处理的喷嘴需要一种方式来跟踪它发出的元组，并准备好在下游处理该元组或任何子元组失败时重新发出元组。子元组可以被认为是源自喷嘴的元组的任何派生元组。另一种看待它的方式是将喷嘴的流视为元组树的主干（如下图所示）：
- en: '![Reliability in spouts](img/8294OS_01_06.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![喷嘴的可靠性](img/8294OS_01_06.jpg)'
- en: Tuple tree
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 元组树
- en: In the preceding diagram, the solid lines represent the original trunk tuples
    emitted by a spout, and the dotted lines represent tuples derived from the original
    tuple. The resulting graph represents the tuple **tree**. With guaranteed processing,
    each bolt in the tree can either acknowledge (`ack`) or fail a tuple. If all bolts
    in the tree acknowledge tuples derived from the trunk tuple, the spout's `ack`
    method will be called to indicate that message processing is complete. If any
    of the bolts in the tree explicitly fail a tuple, or if processing of the tuple
    tree exceeds the time-out period, the spout's `fail` method will be called.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，实线代表喷嘴发出的原始主干元组，虚线代表从原始元组派生的元组。结果图表示元组**树**。通过可靠处理，树中的每个螺栓都可以确认（`ack`）或失败一个元组。如果树中的所有螺栓都确认从主干元组派生的元组，喷嘴的`ack`方法将被调用以指示消息处理已完成。如果树中的任何螺栓明确失败一个元组，或者如果元组树的处理超过了超时期限，喷嘴的`fail`方法将被调用。
- en: 'Storm''s `ISpout` interface defines three methods involved in the reliability
    API: `nextTuple`, `ack`, and `fail`.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Storm的`ISpout`接口定义了可靠性API中涉及的三种方法：`nextTuple`，`ack`和`fail`。
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As we''ve seen before, when Storm requests that a spout emit a tuple, it calls
    the `nextTuple()` method. The first step in implementing guaranteed processing
    is to assign the outbound tuple a unique ID and pass that value to the `emit()`
    method of `SpoutOutputCollector`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所看到的，当Storm请求喷嘴发出一个元组时，它会调用`nextTuple()`方法。实现可靠处理的第一步是为出站元组分配一个唯一的ID，并将该值传递给`SpoutOutputCollector`的`emit()`方法：
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Assigning the tuple a message ID tells Storm that a spout would like to receive
    notifications either when the tuple tree is completed or if it fails at any point.
    If processing succeeds, the spout's `ack()` method will be called with the message
    ID assigned to the tuple. If processing fails or times out, the spout's `fail`
    method will be called.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 分配元组的消息ID告诉Storm，喷嘴希望在元组树完成或在任何时候失败时接收通知。如果处理成功，喷嘴的`ack()`方法将使用分配给元组的消息ID进行调用。如果处理失败或超时，喷嘴的`fail`方法将被调用。
- en: Reliability in bolts
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 螺栓的可靠性
- en: 'Implementing a bolt that participates in guaranteed processing involves two
    steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 参与可靠处理的螺栓的实现涉及两个步骤：
- en: Anchoring to an incoming tuple when emitting a derived tuple.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在发出派生元组时锚定到传入的元组。
- en: Acknowledging or failing tuples that have been processed successfully or unsuccessfully,
    respectively.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认或失败已成功或不成功处理的元组。
- en: Anchoring to a tuple means that we are creating a link between an incoming tuple
    and derived tuples such that any downstream bolts are expected to participate
    in the tuple tree by acknowledging the tuple, failing the tuple, or allowing it
    to time out.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 锚定到元组意味着我们正在创建一个链接，使传入的元组和派生的元组之间建立联系，以便任何下游的螺栓都应该参与元组树，确认元组，失败元组，或允许其超时。
- en: 'You can anchor to a tuple (or a list of tuples) by calling one of the overloaded
    `emit` methods of `OutputCollector`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过调用`OutputCollector`的重载的`emit`方法将锚定到元组（或元组列表）：
- en: '[PRE28]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here, we''re anchoring to the incoming tuple and emitting a new tuple that
    downstream bolts should acknowledge or fail. An alternative form of the `emit`
    method will emit unanchored tuples:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将锚定到传入的元组，并发出一个新的元组，下游的螺栓应该承认或失败。`emit`方法的另一种形式将发出未锚定的元组：
- en: '[PRE29]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Unanchored tuples do not participate in the reliability of a stream. If an unanchored
    tuple fails downstream, it will not cause a replay of the original root tuple.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 未锚定的元组不参与流的可靠性。如果未锚定的元组在下游失败，它不会导致原始根元组的重播。
- en: 'After successfully processing a tuple and optionally emitting new or derived
    tuples, a bolt processing a reliable stream should acknowledge the inbound tuple:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 成功处理一个元组并可选地发出新的或派生的元组后，处理可靠流的螺栓应该确认传入的元组：
- en: '[PRE30]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If tuple processing fails in such a way that the spout must replay (re-emit)
    the tuple, the bolt should explicitly fail the tuple:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果元组处理失败，以至于喷嘴必须重播（重新发出）元组，螺栓应该显式失败元组：
- en: '[PRE31]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: If tuple processing fails as a result of a time out or through an explicit call,
    the `OutputCollector.fail()` method, the spout that emitted the original tuple,
    will be notified, allowing it to re-emit the tuple, as you'll see shortly.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果元组处理因超时或显式调用`OutputCollector.fail()`方法而失败，将通知发出原始元组的喷嘴，从而允许它重新发出元组，您很快就会看到。
- en: Reliable word count
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠的字数统计
- en: 'To further illustrate reliability, let''s begin by enhancing the `SentenceSpout`
    class to make it support guaranteed delivery. It will need to keep track of all
    tuples emitted and assign each one a unique ID. We''ll use a `HashMap<UUID, Values>`
    object to store the tuples that are pending. For each tuple we emit, we''ll assign
    a unique identifier and store it in our map of pending tuples. When we receive
    an acknowledgement, we''ll remove the tuple from our pending list. On failure,
    we''ll replay the tuple:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明可靠性，让我们从增强`SentenceSpout`类开始，使其支持保证交付。它将需要跟踪所有发出的元组，并为每个元组分配一个唯一的ID。我们将使用`HashMap<UUID,
    Values>`对象来存储待处理的元组。对于我们发出的每个元组，我们将分配一个唯一的标识符，并将其存储在我们的待处理元组映射中。当我们收到确认时，我们将从待处理列表中删除元组。在失败时，我们将重放元组：
- en: '[PRE32]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Modifying the bolts to provide guaranteed processing simply involves anchoring
    outbound tuples to the incoming tuple and then acknowledging the inbound tuple:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 修改螺栓以提供保证的处理只是简单地将出站元组锚定到传入的元组，然后确认传入的元组：
- en: '[PRE33]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Summary
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've built a simple distributed computation application using
    Storm's core API and covered a large part of Storm's feature set, all without
    even installing Storm or setting up a cluster. Storm's local mode is powerful
    in terms of productivity and ease of development, but to see Storm's true power
    and horizontal scalability, you'll want to deploy applications to a real cluster.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用Storm的核心API构建了一个简单的分布式计算应用程序，并涵盖了Storm的大部分功能集，甚至没有安装Storm或设置集群。Storm的本地模式在生产力和开发便利性方面非常强大，但要看到Storm的真正力量和水平可伸缩性，您需要将应用程序部署到一个真正的集群中。
- en: In the next chapter, we'll walk through the process of installing and setting
    up a clustered Storm environment and deploying topologies in a distributed environment.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将步入安装和设置集群Storm环境以及在分布式环境中部署拓扑的过程。
