- en: Machine Learning with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python进行机器学习
- en: In this chapter, we get into machine learning and how to actually implement
    machine learning models in Python.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍机器学习以及如何在Python中实际实现机器学习模型。
- en: We'll examine what supervised and unsupervised learning means, and how they're
    different from each other. We'll see techniques to prevent overfitting, and then
    look at an interesting example where we implement a spam classifier. We'll analyze
    what K-Means clustering is a long the way, with a working example that clusters
    people based on their income and age using scikit-learn!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究监督学习和无监督学习的含义，以及它们之间的区别。我们将看到防止过拟合的技术，然后看一个有趣的示例，我们在其中实现了一个垃圾邮件分类器。我们将分析K均值聚类是什么，并使用scikit-learn对基于收入和年龄的人群进行聚类的工作示例！
- en: We'll also cover a really interesting application of machine learning called
    **decision trees** and we'll build a working example in Python that predict shiring
    decisions in a company. Finally, we'll walk through the fascinating concepts of
    ensemble learning and SVMs, which are some of my favourite machine learning areas!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将介绍一种非常有趣的机器学习应用，称为**决策树**，并且我们将在Python中构建一个工作示例，用于预测公司的招聘决策。最后，我们将深入探讨集成学习和SVM的迷人概念，这些是我最喜欢的机器学习领域之一！
- en: 'More specifically, we''ll cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们将涵盖以下主题：
- en: Supervised and unsupervised learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督和无监督学习
- en: Avoiding overfitting by using train/test
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用训练/测试来避免过拟合
- en: Bayesian methods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯方法
- en: Implementation of an e-mail spam classifier with NaÃ¯ve Bayes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯实现电子邮件垃圾邮件分类器
- en: Concept of K-means clustering
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值聚类的概念
- en: Example of clustering in Python
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python中聚类的示例
- en: Entropy and how to measure it
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵及其测量方法
- en: Concept of decision trees and its example in Python
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的概念及其在Python中的示例
- en: What is ensemble learning
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是集成学习
- en: '**Support Vector Machine** (**SVM**) and its example using scikit-learn'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）及其在scikit-learn中的示例'
- en: Machine learning and train/test
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习和训练/测试
- en: So what is machine learning? Well, if you look it up on Wikipedia or whatever,
    it'll say that it is algorithms that can learn from observational data and can
    make predictions based on it. It sounds really fancy, right? Like artificial intelligence
    stuff, like you have a throbbing brain inside of your computer. But in reality,
    these techniques are usually very simple.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么什么是机器学习？如果您在维基百科或其他地方查找，它会说这是一种可以从观测数据中学习并基于此进行预测的算法。听起来很花哨，对吧？就像人工智能一样，就像您的计算机内部有一个跳动的大脑。但实际上，这些技术通常非常简单。
- en: We've already looked at regressions, where we took a set of observational data,
    we fitted a line to it, and we used that line to make predictions. So by our new
    definition, that was machine learning! And your brain works that way too.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过回归，我们从中获取了一组观测数据，我们对其进行了拟合，并使用该线进行预测。所以按照我们的新定义，那就是机器学习！您的大脑也是这样工作的。
- en: Another fundamental concept in machine learning is something called **train**/**test**,
    which lets us very cleverly evaluate how good a machine learning model we've made.
    As we look now at unsupervised and supervised learning, you'll see why train/test
    is so important to machine learning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的另一个基本概念是称为**训练**/**测试**的东西，它让我们非常聪明地评估我们制作的机器学习模型有多好。当我们现在看无监督和监督学习时，您将看到为什么训练/测试对机器学习如此重要。
- en: Unsupervised learning
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'Let''s talk in detail now about two different types of machine learning: supervised
    and unsupervised learning. Sometimes there can be kind of a blurry line between
    the two, but the basic definition of unsupervised learning is that you''re not
    giving your model any answers to learn from. You''re just presenting it with a
    group of data and your machine learning algorithm tries to make sense out of it
    given no additional information:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细讨论两种不同类型的机器学习：监督学习和无监督学习。有时两者之间可能存在一种模糊的界限，但无监督学习的基本定义是，您不会给模型任何答案来学习。您只是向其提供一组数据，您的机器学习算法会尝试在没有额外信息的情况下理解它：
- en: '![](img/54c8b166-bc40-4c03-8a14-a47bd74e93dc.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54c8b166-bc40-4c03-8a14-a47bd74e93dc.jpg)'
- en: Let's say I give it a bunch of different objects, like these balls and cubes
    and sets of dice and what not. Let's then say have some algorithm that will cluster
    these objects into things that are similar to each other based on some similarity
    metric.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我给它一堆不同的对象，比如这些球和立方体以及一些骰子之类的东西。然后让我们假设有一些算法，它将根据某种相似性度量将这些对象聚类成相互相似的东西。
- en: Now I haven't told the machine learning algorithm, ahead of time, what categories
    certain objects belong to. I don't have a cheat sheet that it can learn from where
    I have a set of existing objects and my correct categorization of it. The machine
    learning algorithm must infer those categories on its own. This is an example
    of unsupervised learning, where I don't have a set of answers that I'm getting
    it learn from. I'm just trying to let the algorithm gather its own answers based
    on the data presented to it alone.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我没有提前告诉机器学习算法，某些对象属于哪些类别。我没有一个可以从中学习的作弊表，其中有一组现有对象和我对其正确分类的信息。机器学习算法必须自行推断这些类别。这是无监督学习的一个例子，我没有一组答案可以让它学习。我只是试图让算法根据所呈现给它的数据自行收集答案。
- en: The problem with this is that we don't necessarily know what the algorithm will
    come up with! If I gave it that bunch of objects shown in the preceding image,
    is it going to group things into things that are round, things that are large
    versus small, things that are red versus blue, I don't know. It's going to depend
    on the metric that I give it for similarity between items primarily. But sometimes
    you'll find clusters that are surprising, and emerged that you didn't expect to
    see.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于我们不一定知道算法会得出什么结果！如果我给它前面图像中显示的一堆物体，它会将物品分成圆形的，大的与小的，红色的与蓝色的吗，我不知道。这将取决于我为物品之间的相似性给出的度量标准。但有时您会发现令人惊讶的聚类，并且会出现您没有预料到的结果。
- en: 'So that''s really the point of unsupervised learning: if you don''t know what
    you''re looking for, it can be a powerful tool for discovering classifications
    that you didn''t even know were there. We call this a **latent variable**. Some
    property of your data that you didn''t even know was there originally, can be
    teased out by unsupervised learning.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这确实是无监督学习的重点：如果你不知道你在寻找什么，它可以成为一个强大的工具，用来发现你甚至不知道存在的分类。我们称之为**潜在变量**。你最初甚至不知道的数据属性，可以通过无监督学习来挖掘出来。
- en: Let's take another example around unsupervised learning. Say I was clustering
    people instead of balls and dice. I'm writing a dating site and I want to see
    what sorts of people tend to cluster together. There are some attributes that
    people tend to cluster around, which decide whether they tend to like each other
    and date each other for example. Now you might find that the clusters that emerge
    don't conform to your predisposed stereotypes. Maybe it's not about college students
    versus middle-aged people, or people who are divorced and whatnot, or their religious
    beliefs. Maybe if you look at the clusters that actually emerged from that analysis,
    you'll learn something new about your users and actually figure out that there's
    something more important than any of those existing features of your people that
    really count toward, to decide whether they like each other. So that's an example
    of supervised learning providing useful results.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个无监督学习的例子。假设我是在对人群进行聚类而不是对球和骰子进行聚类。我正在写一个约会网站，我想看看哪些类型的人倾向于聚集在一起。人们倾向于围绕一些属性进行聚类，这些属性决定了他们是否倾向于彼此喜欢和约会。现在你可能会发现出现的聚类并不符合你的先入为主的刻板印象。也许这与大学生与中年人、离婚者等无关，或者他们的宗教信仰。也许如果你看看实际从分析中出现的聚类，你会对你的用户学到一些新东西，并且真正发现有些东西比你的人群的任何现有特征更重要，以决定他们是否喜欢彼此。这就是监督学习提供有用结果的一个例子。
- en: Another example could be clustering movies based on their properties. If you
    were to run clustering on a set of movies from like IMDb or something, maybe the
    results would surprise you. Perhaps it's not just about the genre of the movie.
    Maybe there are other properties, like the age of the movie or the running length
    or what country it was released in, that are more important. You just never know.
    Or we could analyze the text of product descriptions and try to find the terms
    that carry the most meaning for a certain category. Again, we might not necessarily
    know ahead of time what terms, or what words, are most indicative of a product
    being in a certain category; but through unsupervised learning, we can tease out
    that latent information.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子可能是根据电影的属性对电影进行聚类。如果你对像IMDb这样的一组电影运行聚类，也许结果会让你惊讶。也许这不仅仅是关于电影的类型。也许还有其他属性，比如电影的年龄或播放时长或上映国家，更重要。你永远不知道。或者我们可以分析产品描述的文本，尝试找出对某个类别具有最重要意义的术语。同样，我们可能并不一定知道哪些术语或词语最能表明产品属于某个类别；但通过无监督学习，我们可以挖掘出这些潜在信息。
- en: Supervised learning
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: Now in contrast, supervised learning is a case where we have a set of answers
    that the model can learn from. We give it a set of training data, that the model
    learns from. It can then infer relationships between the features and the categories
    that we want, and apply that to unseen new values - and predict information about
    them.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，监督学习是一种模型可以从一组答案中学习的情况。我们给它一组训练数据，模型从中学习。然后它可以推断我们想要的特征和类别之间的关系，并将其应用于未见过的新值，并预测有关它们的信息。
- en: Going back to our earlier example, where we were trying to predict car prices
    based on the attributes of those cars. That's an example where we are training
    our model using actual answers. So I have a set of known cars and their actual
    prices that they sold for. I train the model on that set of complete answers,
    and then I can create a model that I'm able to use to predict the prices of new
    cars that I haven't seen before. That's an example of supervised learning, where
    you're giving it a set of answers to learn from. You've already assigned categories
    or some organizing criteria to a set of data, and your algorithm then uses that
    criteria to build a model from which it can predict new values.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们之前的例子，我们试图根据这些汽车的属性来预测汽车价格。这是一个我们使用实际答案来训练模型的例子。所以我有一组已知的汽车及其实际售价。我在这组完整答案上训练模型，然后我可以创建一个能够预测我以前没有见过的新车价格的模型。这是一个监督学习的例子，你给它一组答案来学习。你已经给一组数据分配了类别或一些组织标准，然后你的算法使用这些标准来建立一个模型，从中可以预测新的数值。
- en: Evaluating supervised learning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估监督学习
- en: So how do you evaluate supervised learning? Well, the beautiful thing about
    supervised learning is that we can use a trick called train/test. The idea here
    is to split our observational data that I want my model to learn from into two
    groups, a training set and a testing set. So when I train/build my model based
    on the data that I have, I only do that with part of my data that I'm calling
    my training set, and I reserve another part of my data that I'm going to use for
    testing purposes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如何评估监督学习呢？监督学习的美妙之处在于我们可以使用一个称为训练/测试的技巧。这里的想法是将我希望我的模型学习的观察数据分成两组，一个训练集和一个测试集。所以当我基于我拥有的数据来训练/构建我的模型时，我只使用我称之为训练集的部分数据，然后我保留另一部分数据，我将用于测试目的。
- en: I can build my model using a subset of my data for training data, and then I'm
    in a position to evaluate the model that comes out of that, and see if it can
    successfully predict the correct answers for my testing data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以使用我的数据子集来构建我的模型作为训练数据，然后我可以评估从中得出的模型，并看看它是否能成功地预测我的测试数据的正确答案。
- en: So you see what I did there? I have a set of data where I already have the answers
    that I can train my model from, but I'm going to withhold a portion of that data
    and actually use that to test my model that was generated using the training set!
    That it gives me a very concrete way to test how good my model is on unseen data
    because I actually have a bit of data that I set aside that I can test it with.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你看到我在这里做了什么？我有一组数据，我已经有了可以训练模型的答案，但我将保留一部分数据，实际上用它来测试使用训练集生成的模型！这让我有一个非常具体的方法来测试我的模型在未知数据上的表现，因为我实际上有一些数据被我留出来可以用来测试。
- en: You can then measure quantitatively how well it did using r-squared or some
    other metric, like root-mean-square error, for example. You can use that to test
    one model versus another and see what the best model is for a given problem. You
    can tune the parameters of that model and use train/test to maximize the accuracy
    of that model on your testing data. So this is a great way to prevent overfitting.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以定量地测量它的表现如何，使用R平方或其他指标，比如均方根误差。你可以用它来测试一个模型与另一个模型，看看对于给定问题哪个是最好的模型。你可以调整该模型的参数，并使用训练/测试来最大化该模型在测试数据上的准确性。这是防止过拟合的一个很好的方法。
- en: There are some caveats to supervised learning. need to make sure that both your
    training and test datasets are large enough to actually be representative of your
    data. You also need to make sure that you're catching all the different categories
    and outliers that you care about, in both training and testing, to get a good
    measure of its success, and to build a good model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些监督学习的注意事项。需要确保你的训练和测试数据集足够大，能够真正代表你的数据。你还需要确保你在训练和测试中捕捉到所有你关心的不同类别和异常值，以便对其成功进行良好的衡量和建立一个好的模型。
- en: You have to make sure that you've selected from those datasets randomly, and
    that you're not just carving your dataset in two and saying everything left of
    here is training and right here is testing. You want to sample that randomly,
    because there could be some pattern sequentially in your data that you don't know
    about.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须确保你从这些数据集中随机选择，并且不是只将你的数据集分成两部分，说这里左边的是训练，右边的是测试。你想要随机抽样，因为你的数据中可能有一些你不知道的顺序模式。
- en: Now, if your model is overfitting, and just going out of its way to accept outliers
    in your training data, then that's going to be revealed when you put it against
    unset scene of testing data. This is because all that gyrations for outliers won't
    help with the outliers that it hasn't seen before.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你的模型过拟合，并且只是竭尽全力接受训练数据中的异常值，那么当你将其放在未见过的测试数据中时，这将会显露出来。这是因为所有为了异常值而进行的旋转对于它以前没有见过的异常值是没有帮助的。
- en: 'Let''s be clear here that train/test is not perfect, and it is possible to
    get misleading results from it. Maybe your sample sizes are too small, like we
    already talked about, or maybe just due to random chance your training data and
    your test data look remarkably similar, they actually do have a similar set of
    outliers - and you can still be overfitting. As you can see in the following example,
    it really can happen:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们清楚地说一下，训练/测试并不完美，有可能会得到误导性的结果。也许你的样本量太小，就像我们已经讨论过的那样，或者可能仅仅由于随机机会，你的训练数据和测试数据看起来非常相似，它们实际上确实有一组相似的异常值-你仍然可能会过拟合。正如你在下面的例子中所看到的，这确实可能发生：
- en: '![](img/dfd6dacd-41fc-415b-ab5b-dda9abf05c09.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfd6dacd-41fc-415b-ab5b-dda9abf05c09.jpg)'
- en: K-fold cross validation
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-fold交叉验证
- en: Now there is a way around this problem, called k-fold cross-validation, and
    we'll look at an example of this later in the book, but the basic concept is you
    train/test many times. So you actually split your data not into just one training
    set and one test set, but into multiple randomly assigned segments, k segments.
    That's where the k comes from. And you reserve one of those segments as your test
    data, and then you start training your model on the remaining segments and measure
    their performance against your test dataset. Then you take the average performance
    from each of those training sets' models' results and take their r-squared average
    score.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有一种解决这个问题的方法，叫做k-fold交叉验证，我们将在本书的后面看一个例子，但基本概念是你要多次进行训练/测试。所以你实际上不是将数据分成一个训练集和一个测试集，而是分成多个随机分配的段，k个段。这就是k的含义。然后你保留其中一个段作为测试数据，然后开始在剩余的段上训练模型，并测量它们对测试数据集的表现。然后你取每个训练集模型结果的平均表现，并取它们的R平方平均分数。
- en: So this way, you're actually training on different slices of your data, measuring
    them against the same test set, and if you have a model that's overfitting to
    a particular segment of your training data, then it will get averaged out by the
    other ones that are contributing to k-fold cross-validation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你实际上是在不同的数据片段上进行训练，将它们与相同的测试集进行比较，如果你的模型对训练数据的特定部分过拟合，那么它将被k-fold交叉验证中其他部分的平均值抵消。
- en: 'Here are the K-fold cross validation steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是K-fold交叉验证的步骤：
- en: Split your data into K randomly-assigned segments
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分成K个随机分配的段
- en: Reserve one segment as your test data
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一个段保留为测试数据
- en: Train on each of the remaining K-1 segments and measure their performance against
    the test set
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在剩余的K-1个段上进行训练，并测量它们对测试集的表现
- en: Take the average of the K-1 r-squared scores
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算K-1个R平方分数的平均值
- en: This will make more sense later in the book, right now I would just like for
    you to know that this tool exists for actually making train/test even more robust
    than it already is. So let's go and actually play with some data and actually
    evaluate it using train/test next.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这在本书的后面会更有意义，现在我只是想让你知道这个工具实际上可以使训练/测试比它已经是更加健壮。所以让我们去实际玩一些数据，并使用训练/测试来评估它。
- en: Using train/test to prevent overfitting of a polynomial regression
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用训练/测试来防止多项式回归的过拟合
- en: Let's put train/test into action. So you might remember that a regression can
    be thought of as a form of supervised machine learning. Let's just take a polynomial
    regression, which we covered earlier, and use train/test to try to find the right
    degree polynomial to fit a given set of data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把训练/测试付诸实践。你可能记得回归可以被看作是一种监督式机器学习。让我们尝试一下多项式回归，我们之前介绍过，使用训练/测试来尝试找到适合给定数据集的正确次数的多项式。
- en: Just like in our previous example, we're going to set up a little fake dataset
    of randomly generated page speeds and purchase amounts, and I'm going to create
    a quirky little relationship between them that's exponential in nature.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前的例子一样，我们将建立一个小的虚拟数据集，其中包括随机生成的页面速度和购买金额，我将在它们之间创建一个怪异的指数关系。
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s go ahead and generate that data. We''ll use a normal distribution of
    random data for both page speeds and purchase amount using the relationship as
    shown in the following screenshot:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续生成这些数据。我们将使用页面速度和购买金额的正态分布的随机数据，使用如下截图中所示的关系：
- en: '![](img/a215de99-d26a-4838-9482-274b254b3d20.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a215de99-d26a-4838-9482-274b254b3d20.jpg)'
- en: Next, we'll split that data. We'll take 80% of our data, and we're going to
    reserve that for our training data. So only 80% of these points are going to be
    used for training the model, and then we're going to reserve the other 20% for
    testing that model against unseen data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将拆分数据。我们将取80%的数据，保留给训练数据。所以这些点中只有80%会用于训练模型，然后我们将保留另外的20%用于测试模型对未知数据的预测。
- en: 'We''ll use Python''s syntax here for splitting the list. The first 80 points
    are going to go to the training set, and the last 20, everything after 80, is
    going to go to test set. You may remember this from our Python basics chapter
    earlier on, where we covered the syntax to do this, and we''ll do the same thing
    for purchase amounts here:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Python的语法来拆分列表。前80个点将用于训练集，最后的20个点将用于测试集。你可能还记得我们在之前的Python基础章节中介绍过这个语法，我们将在购买金额中也做同样的事情：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now in our earlier sections, I've said that you shouldn't just slice your dataset
    in two like this, but that you should randomly sample it for training and testing.
    In this case though, it works out because my original data was randomly generated
    anyway, so there's really no rhyme or reason to where things fell. But in real-world
    data you'll want to shuffle that data before you split it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的章节中，我说过你不应该像这样简单地将数据集分成两部分，而是应该随机抽样进行训练和测试。但在这种情况下，它是有效的，因为我的原始数据本来就是随机生成的，所以没有任何规律可循。但在现实世界的数据中，你会希望在拆分之前对数据进行洗牌。
- en: We'll look now at a handy method that you can use for that purpose of shuffling
    your data. Also, if you're using the pandas package, there's some handy functions
    in there for making training and test datasets automatically for you. But we're
    going to do it using a Python list here. So let's visualize our training dataset
    that we ended up with. We'll do a scatter plot of our training page speeds and
    purchase amounts.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看一下一个方便的方法，你可以用它来洗牌你的数据。另外，如果你使用pandas包，那里有一些方便的函数可以自动为你创建训练和测试数据集。但我们将在这里使用Python列表来做。所以让我们来可视化我们最终得到的训练数据集。我们将绘制我们的训练页面速度和购买金额的散点图。
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is what your output should now look like:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你的输出现在应该看起来的样子：
- en: '![](img/d0fc1eec-b655-424f-956f-798327e5a2f8.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0fc1eec-b655-424f-956f-798327e5a2f8.jpg)'
- en: Basically, 80 points that were selected at random from the original complete
    dataset have been plotted. It has basically the same shape, so that's a good thing.
    It's representative of our data. That's important!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，从原始完整数据集中随机选择的80个点已经被绘制出来。它基本上具有相同的形状，这是一个好事。它代表了我们的数据。这很重要！
- en: Now let's plot the remaining 20 points that we reserved as test data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制剩下的20个点，作为测试数据。
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/534acada-d2fb-4cc7-b4b7-82c40fb4fd64.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/534acada-d2fb-4cc7-b4b7-82c40fb4fd64.jpg)'
- en: Here, we see our remaining 20 for testing also has the same general shape as
    our original data. So I think that's a representative test set too. It's a little
    bit smaller than you would like to see in the real world, for sure. You probably
    get a little bit of a better result if you had 1,000 points instead of 100, for
    example, to choose from and reserved 200 instead of 20.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到我们剩下的20个测试点也与我们原始数据的形状相同。所以我认为这也是一个代表性的测试集。当然，它比你在现实世界中想看到的要小一点。例如，如果你有1000个点而不是100个点可以选择，并且保留200个点而不是20个点，你可能会得到更好的结果。
- en: Now we're going to try to fit an 8th degree polynomial to this data, and we'll
    just pick the number `8` at random because I know it's a really high order and
    is probably overfitting.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将尝试对这些数据拟合一个8次多项式，我们只是随机选择了数字`8`，因为我知道这是一个非常高的阶数，可能会导致过拟合。
- en: 'Let''s go ahead and fit our 8th degree polynomial using `np.poly1d(np.polyfit(x,
    y, 8))`, where *x* is an array of the training data only, and *y* is an array
    of the training data only. We are finding our model using only those 80 points
    that we reserved for training. Now we have this `p4` function that results that
    we can use to predict new values:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用`np.poly1d(np.polyfit(x, y, 8)`来拟合我们的8次多项式，其中*x*是仅包含训练数据的数组，*y*也是仅包含训练数据的数组。我们只使用了那80个保留用于训练的点来找到我们的模型。现在我们有了这个`p4`函数，可以用它来预测新的值：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we''ll plot the polynomial this came up with against the training data.
    We can scatter our original data for the training data set, and then we can plot
    our predicted values against them:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将绘制这个多项式与训练数据的关系。我们可以散点绘制我们的原始训练数据，然后我们可以绘制我们的预测值：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can see in the following graph that it looks like a pretty good fit, but
    you know that clearly it''s doing some overfitting:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下图中看到，它看起来非常匹配，但你知道显然它有一些过拟合：
- en: '![](img/16b521a0-d571-4cd7-a5a8-e5557dcb7c1f.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16b521a0-d571-4cd7-a5a8-e5557dcb7c1f.jpg)'
- en: 'What''s this craziness out at the right? I''m pretty sure our real data, if
    we had it out there, wouldn''t be crazy high, as this function would implicate.
    So this is a great example of overfitting your data. It fits the data you gave
    it very well, but it would do a terrible job of predicting new values beyond the
    point where the graph is going crazy high on the right. So let''s try to tease
    that out. Let''s give it our test dataset:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 右边的这种疯狂是什么？我非常确定，如果我们真的有真实数据，它不会像这个函数所暗示的那样疯狂高。所以这是一个很好的过拟合数据的例子。它非常适合你提供的数据，但是在图表右侧疯狂高的点之后，它会对预测新值做出糟糕的预测。所以让我们试着揭示这一点。让我们给它我们的测试数据集：
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Indeed, if we plot our test data against that same function, well, it doesn't
    actually look that bad.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果我们将我们的测试数据绘制到同样的函数上，嗯，它看起来并不那么糟糕。
- en: '![](img/8b6b4a61-0d3b-45a7-b7fd-4bfb2d6ad3fa.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b6b4a61-0d3b-45a7-b7fd-4bfb2d6ad3fa.jpg)'
- en: 'We got lucky and none of our test is actually out here to begin with, but you
    can see that it''s a reasonable fit, but far from perfect. And in fact, if you
    actually measure the r-squared score, it''s worse than you might think. We can
    measure that using the `r2_score()` function from `sklearn.metrics`. We just give
    it our original data and our predicted values and it just goes through and measures
    all the variances from the predictions and squares them all up for you:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很幸运，我们的测试数据实际上并不在这里开始，但你可以看到这是一个合理的拟合，但远非完美。事实上，如果你实际测量R平方分数，它比你想象的要糟糕。我们可以使用`sklearn.metrics`中的`r2_score()`函数来测量。我们只需给它我们的原始数据和我们预测的值，它就会测量所有预测值的方差并为你平方：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We end up with an r-squared score of just `0.3`. So that''s not that hot! You
    can see that it fits the training data a lot better:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到的R平方分数只有`0.3`。所以并不是很高！你可以看到它更适合训练数据：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The r-squared value turns out to be `0.6`, which isn't too surprising, because
    we trained it on the training data. The test data is sort of its unknown, its
    test, and it did fail the test, quite frankly. 30%, that's an F!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: R平方值结果为`0.6`，这并不令人意外，因为它是在训练数据上训练的。测试数据是它的未知，它的测试，而它确实没有通过测试。30%，这是F！
- en: So this has been an example where we've used train/test to evaluate a supervised
    learning algorithm, and like I said before, pandas has some means of making this
    even easier. We'll look at that a little bit later, and we'll also look at more
    examples of train/test, including k-fold cross validation, later in the book as
    well.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子，我们使用训练/测试来评估监督学习算法，就像我之前说的那样，pandas有一些方法可以使这变得更容易。我们稍后会看一下，我们还将在本书的后面看到更多关于训练/测试的例子，包括k折交叉验证。
- en: Activity
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动
- en: You can probably guess what your homework is. So we know that an 8th order polynomial
    isn't very useful. Can you do better? So I want you to go back through our example,
    and use different values for the degree polynomial that you're going to use to
    fit. Change that 8 to different values and see if you can figure out what degree
    polynomial actually scores best using train/test as a metric. Where do you get
    your best r-squared score for your test data? What degree fits here? Go play with
    that. It should be a pretty easy exercise and a very enlightening one for you
    as well.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能能猜到你的作业是什么。所以我们知道8次多项式并不是很有用。你能做得更好吗？所以我希望你回到我们的例子中，使用不同的多项式阶数来拟合。将8更改为不同的值，看看你能否找出使用训练/测试作为度量标准的最佳多项式阶数。你的测试数据在哪里得到最好的R平方分数？哪个阶数更适合？去尝试一下。这应该是一个相当简单的练习，对你来说也是一个非常有启发性的练习。
- en: So that's train/test in action, a very important technique to have under your
    belt, and you're going to use it over and over again to make sure that your results
    are a good fit for the model that you have, and that your results are a good predictor
    of unseen values. It's a great way to prevent overfitting when you're doing your
    modeling.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是训练/测试的实际应用，这是一个非常重要的技术，你将一遍又一遍地使用它，以确保你的结果与你拥有的模型非常匹配，并且你的结果对未知值有很好的预测能力。这是在进行建模时防止过拟合的好方法。
- en: Bayesian methods - Concepts
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯方法-概念
- en: Did you ever wonder how the spam classifier in your e-mail works? How does it
    know that an e-mail might be spam or not? Well, one popular technique is something
    called Naive Bayes, and that's an example of a Bayesian method. Let's learn more
    about how that works. Let's discuss Bayesian methods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经想过你的电子邮件中的垃圾邮件分类器是如何工作的？它是如何知道一封电子邮件可能是垃圾邮件还是不是？嗯，一个流行的技术是一种称为朴素贝叶斯的技术，这是贝叶斯方法的一个例子。让我们更多地了解它是如何工作的。让我们讨论贝叶斯方法。
- en: We did talk about Bayes' theorem earlier in this book in the context of talking
    about how things like drug tests could be very misleading in their results. But
    you can actually apply the same Bayes' theorem to larger problems, like spam classifiers.
    So let's dive into how that might work, it's called a Bayesian method.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书的早些时候讨论了贝叶斯定理，讨论了像药物测试这样的事情在结果上可能非常误导。但你实际上可以将相同的贝叶斯定理应用到更大的问题，比如垃圾邮件分类器。所以让我们深入了解一下它可能是如何工作的，这就是所谓的贝叶斯方法。
- en: 'So just a refresher on Bayes'' theorem -remember, the probability of A given
    B is equal to the overall probability of A times the probability of B given A
    over the overall probability of B:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以对贝叶斯定理的一个复习-记住，给定B的A的概率等于A的整体概率乘以给定A的B的概率除以B的整体概率：
- en: '![](img/95ca9a4e-01b9-4dbe-a066-5ff2db89e08d.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95ca9a4e-01b9-4dbe-a066-5ff2db89e08d.jpg)'
- en: 'How can we use that in machine learning? I can actually build a spam classifier
    for that: an algorithm that can analyze a set of known spam e-mails and a known
    set of non-spam e-mails, and train a model to actually predict whether new e-mails
    are spam or not. This is a real technique used in actual spam classifiers in the
    real world.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何在机器学习中使用它？我实际上可以为此构建一个垃圾邮件分类器：一个可以分析一组已知的垃圾邮件和一组已知的非垃圾邮件，并训练一个模型来预测新邮件是否为垃圾邮件的算法。这是实际世界中实际使用的垃圾邮件分类器的真正技术。
- en: 'As an example, let''s just figure out the probability of an e-mail being spam
    given that it contains the word "free". If people are promising you free stuff,
    it''s probably spam! So let''s work that out. The probability of an email being
    spam given that you have the word "free" in that e-mail works out to the overall
    probability of it being a spam message times the probability of containing the
    word "free" given that it''s spam over the probability overall of being free:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，让我们来计算一下包含单词“free”的电子邮件被认为是垃圾邮件的概率。如果有人向你承诺免费的东西，那很可能是垃圾邮件！所以让我们来计算一下。在电子邮件中包含单词“free”时，电子邮件被认为是垃圾邮件的概率等于它是垃圾邮件的总体概率乘以包含单词“free”的概率，假设它是垃圾邮件，除以总体概率是免费的概率：
- en: '![](img/6992cb5a-12dd-4397-b787-571b1d13777c.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6992cb5a-12dd-4397-b787-571b1d13777c.jpg)'
- en: 'The numerator can just be thought of as the probability of a message being
    `Spam` and containing the word `Free`. But that''s a little bit different than
    what we''re looking for, because that''s the odds out of the complete dataset
    and not just the odds within things that contain the word `Free`. The denominator
    is just the overall probability of containing the word `Free`. Sometimes that
    won''t be immediately accessible to you from the data that you have. If it''s
    not, you can expand that out to the following expression if you need to derive
    it:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 分子可以被认为是消息是“垃圾邮件”并包含单词“免费”的概率。但这与我们要寻找的有点不同，因为这是完整数据集中的几率，而不仅仅是包含单词“免费”的几率。分母只是包含单词“免费”的总体概率。有时，这可能不会立即从您拥有的数据中获得。如果没有，如果需要推导出来，可以将其扩展为以下表达式：
- en: '![](img/ca351b51-13d1-432c-814b-43bcf6a55db0.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca351b51-13d1-432c-814b-43bcf6a55db0.jpg)'
- en: This gives you the percentage of e-mails that contain the word "free" that are
    spam, which would be a useful thing to know when you're trying to figure out if
    it's spam or not.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了包含单词“free”的电子邮件中是垃圾邮件的百分比，这在您试图确定它是否是垃圾邮件时是一个有用的信息。
- en: What about all the other words in the English language, though? So our spam
    classifier should know about more than just the word "free". It should automatically
    pick up every word in the message, ideally, and figure out how much does that
    contribute to the likelihood of a particular e-mail being spam. So what we can
    do is train our model on every word that we encounter during training, throwing
    out things like "a" and "the" and "and" and meaningless words like that. Then
    when we go through all the words in a new e-mail, we can multiply the probability
    of being spam for each word together, and we get the overall probability of that
    e-mail being spam.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但是英语中的所有其他单词呢？所以我们的垃圾邮件分类器应该知道的不仅仅是单词“free”。理想情况下，它应该自动选择消息中的每个单词，并弄清楚每个单词对特定电子邮件被认为是垃圾邮件的可能性有多大的贡献。所以我们可以在训练时对我们遇到的每个单词进行训练，丢弃像“a”、“the”和“and”这样的东西以及无意义的单词。然后当我们浏览新电子邮件中的所有单词时，我们可以将每个单词的垃圾邮件概率相乘在一起，然后得到该电子邮件是垃圾邮件的总体概率。
- en: Now it's called Naive Bayes for a reason. It's naive is because we're assuming
    that there's no relationships between the words themselves. We're just looking
    at each word in isolation, individually within a message, and basically combining
    all the probabilities of each word's contribution to it being spam or not. We're
    not looking at the relationships between the words. So a better spam classifier
    would do that, but obviously that's a lot harder.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它被称为朴素贝叶斯是有原因的。它是朴素的，因为我们假设单词之间没有关系。我们只是独立地查看消息中的每个单词，并基本上结合每个单词对其是否是垃圾邮件的概率。我们不考虑单词之间的关系。因此，更好的垃圾邮件分类器会这样做，但显然这要困难得多。
- en: So this sounds like a lot of work. But the overall idea is not that hard, and
    scikit-learn in Python makes it actually pretty easy to do. It offers a feature
    called **CountVectorizer** that makes it very simple to actually split up an e-mail
    to all of its component words and process those words individually. Then it has
    a `MultinomialNB` function, where NB stands for Naive Bayes, which will do all
    the heavy lifting for Naive Bayes for us.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来好像是很多工作。但总体想法并不难，而且Python中的scikit-learn使得实际上很容易做到。它提供了一个名为CountVectorizer的功能，可以非常简单地将电子邮件拆分为其所有组成单词，并逐个处理这些单词。然后它有一个MultinomialNB函数，其中NB代表朴素贝叶斯，它将为我们完成所有朴素贝叶斯的繁重工作。
- en: Implementing a spam classifier with Naïve Bayes
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯实现垃圾邮件分类器
- en: 'Let''s write a spam classifier using Naive Bayes. You''re going to be surprised
    how easy this is. In fact, most of the work ends up just being reading all the
    input data that we''re going to train on and actually parsing that data in. The
    actual spam classification bit, the machine learning bit, is itself just a few
    lines of code. So that''s usually how it works out: reading in and massaging and
    cleaning up your data is usually most of the work when you''re doing data science,
    so get used to the idea!'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用朴素贝叶斯编写一个垃圾邮件分类器。你会惊讶地发现这是多么容易。实际上，大部分工作最终都是读取我们将要训练的所有输入数据，并实际解析这些数据。实际的垃圾邮件分类部分，机器学习部分，本身只是几行代码。所以通常情况下是这样的：当你在做数据科学时，读取和整理数据通常是大部分工作，所以要习惯这个想法！
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So the first thing we need to do is read all those e-mails in somehow, and we're
    going to again use pandas to make this a little bit easier. Again, pandas is a
    useful tool for handling tabular data. We import all the different packages that
    we're going to use within our example here, that includes the os library, the
    io library, numpy, pandas, and CountVectorizer and MultinomialNB from scikit-learn.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要做的第一件事是以某种方式读取所有这些电子邮件，我们将再次使用pandas使这变得更容易一些。再次，pandas是处理表格数据的有用工具。我们在这里的示例中导入了我们将在其中使用的所有不同包，包括os库、io库、numpy、pandas，以及scikit-learn中的CountVectorizer和MultinomialNB。
- en: Let's go through this code in detail now. We can skip past the function definitions
    of `readFiles()` and `dataFrameFromDirectory()`for now and go down to the first
    thing that our code actually does which is to create a pandas DataFrame object.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细地看一下这段代码。我们现在可以跳过`readFiles()`和`dataFrameFromDirectory()`函数的定义，然后继续到我们的代码实际上要做的第一件事，那就是创建一个pandas
    DataFrame对象。
- en: 'We''re going to construct this from a dictionary that initially contains a
    little empty list for messages in an empty list of class. So this syntax is saying,
    "I want a DataFrame that has two columns: one that contains the message, the actual
    text of each e-mail; and one that contains the class of each e-mail, that is,
    whether it''s spam or ham". So it''s saying I want to create a little database
    of e-mails, and this database has two columns: the actual text of the e-mail and
    whether it''s spam or not.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个最初包含消息的空列表和一个空类列表的字典中构建这个DataFrame。这个语法是在说：“我想要一个DataFrame，它有两列：一个包含消息，即每封电子邮件的实际文本；另一个包含每封电子邮件的类别，也就是它是垃圾邮件还是正常邮件”。所以它是在说我想要创建一个电子邮件的小数据库，这个数据库有两列：电子邮件的实际文本和它是否是垃圾邮件。
- en: Now we needed to put something in that database, that is, into that DataFrame,
    in Python syntax. So we call the two methods `append()` and `dataFrameFromDirectory()`
    to actually throw into the DataFrame all the spam e-mails from my `spam` folder,
    and all the ham e-mails from the `ham` folder.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要向数据库中添加一些内容，也就是说，向那个DataFrame中添加内容，使用Python语法。所以我们调用了`append()`和`dataFrameFromDirectory()`两个方法，实际上将来自我的`spam`文件夹的所有垃圾邮件和来自`ham`文件夹的所有正常邮件都放入了DataFrame中。
- en: If you are playing along here, make sure you modify the path passed to the `dataFrameFromDirectory()`
    function to match wherever you installed the book materials in your system! And
    again, if you're on Mac or Linux, please pay attention to backslashes and forward
    slashes and all that stuff. In this case, it doesn't matter, but you won't have
    a drive letter, if you're not on Windows. So just make sure those paths are actually
    pointing to where your `spam` and `ham` folders are for this example.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这里跟着做，请确保修改传递给`dataFrameFromDirectory()`函数的路径，以匹配你在系统中安装书籍材料的位置！再次强调，如果你使用的是Mac或Linux，请注意反斜杠和正斜杠等等。在这种情况下，这并不重要，但如果你不是在Windows上，你就不会有一个驱动器号。所以请确保这些路径实际上指向你的`spam`和`ham`文件夹，以便进行本示例。
- en: Next, `dataFrameFromDirectory()` is a function I wrote, which basically says
    I have a path to a directory, and I know it's given classification, spam or ham,
    then it uses the `readFiles()` function, that I also wrote, which will iterate
    through every single file in a directory. So `readFiles()` is using the `os.walk()`
    function to find all the files in a directory. Then it builds up the full pathname
    for each individual file in that directory, and then it reads it in. And while
    it's reading it in, it actually skips the header for each e-mail and just goes
    straight to the text, and it does that by looking for the first blank line.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`dataFrameFromDirectory()`是我写的一个函数，基本上它说我有一个目录的路径，并且我知道它给定了分类，垃圾邮件或正常邮件，然后它使用了我也写的`readFiles()`函数，它会遍历目录中的每一个文件。所以`readFiles()`使用`os.walk()`函数来查找目录中的所有文件。然后它会为该目录中的每个单独的文件构建完整的路径名，然后读取它。在读取时，它实际上会跳过每封电子邮件的标题，直接进入文本，它是通过查找第一个空行来实现的。
- en: It knows that everything after the first empty line is actually the message
    body, and everything in front of that first empty line is just a bunch of header
    information that I don't actually want to train my spam classifier on. So it gives
    me back both, the full path to each file and the body of the message. So that's
    how we read in all of the data, and that's the majority of the code!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 它知道第一个空行之后的所有内容实际上是消息正文，而在第一个空行之前的所有内容只是一堆我实际上不想让我的垃圾邮件分类器进行训练的头部信息。所以它会把每个文件的完整路径和消息正文都返回给我。这就是我们读取所有数据的方式，也是代码的大部分内容！
- en: 'So what I have at the end of the day is a DataFrame object, basically a database
    with two columns, that contains message bodies, and whether it''s spam or not.
    We can go ahead and run that, and we can use the `head` command from the DataFrame
    to actually preview what this looks like:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，最终我得到的是一个DataFrame对象，基本上是一个有两列的数据库，包含了消息正文，以及是否是垃圾邮件。我们可以继续运行，并且可以使用DataFrame的`head`命令来预览一下它的样子：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The first few entries in our DataFrame look like this: for each path to a given
    file full of e-mails we have a classification and we have the message body:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们DataFrame中的前几个条目看起来是这样的：对于给定文件中的每个电子邮件的路径，我们有一个分类和消息正文：
- en: '![](img/5eb289f1-dcfb-460a-984b-6b07d9c8dc56.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5eb289f1-dcfb-460a-984b-6b07d9c8dc56.jpg)'
- en: Alright, now for the fun part, we're going to use the `MultinomialNB()` function
    from scikit-learn to actually perform Naive Bayes on the data that we have.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在到了有趣的部分，我们将使用scikit-learn中的`MultinomialNB()`函数来对我们的数据执行朴素贝叶斯。
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This is what your output should now look like:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的输出应该是这样的：
- en: '![](img/389232ea-b526-4051-9297-89bc6f7f7d09.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/389232ea-b526-4051-9297-89bc6f7f7d09.jpg)'
- en: Once we build a `MultinomialNB` classifier, it needs two inputs. It needs the
    actual data that we're training on (`counts`), and the targets for each thing
    (`targets`). So `counts` is basically a list of all the words in each e-mail and
    the number of times that word occurs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们构建了`MultinomialNB`分类器，它需要两个输入。它需要我们正在训练的实际数据（`counts`），以及每个数据的目标（`targets`）。所以`counts`基本上是每封电子邮件中所有单词的列表，以及该单词出现的次数。
- en: 'So this is what `CountVectorizer()` does: it takes the `message` column from
    the DataFrame and takes all the values from it. I''m going to call `vectorizer.fit_transform`
    which basically tokenizes or converts all the individual words seen in my data
    into numbers, into values. It then counts up how many times each word occurs.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 所以`CountVectorizer()`的作用是：它从DataFrame中取出`message`列并获取其中的所有值。我将调用`vectorizer.fit_transform`，它基本上是将我数据中出现的所有单词进行标记或转换为数字，为其赋予数值。然后它会计算每个单词出现的次数。
- en: This is a more compact way of representing how many times each word occurs in
    an e-mail. Instead of actually preserving the words themselves, I'm representing
    those words as different values in a sparse matrix, which is basically saying
    that I'm treating each word as a number, as a numerical index, into an array.
    What that does is, just in plain English, it split each message up into a list
    of words that are in it, and counts how many times each word occurs. So we're
    calling that `counts`. It's basically that information of how many times each
    word occurs in each individual message. Mean while `targets` is the actual classification
    data for each e-mail that I've encountered. So I can call `classifier.fit()` using
    my `MultinomialNB()` function to actually create a model using Naive Bayes, which
    will predict whether new e-mails are spam or not based on the information we've
    given it.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种更紧凑的方式来表示每个单词在电子邮件中出现的次数。我不是保留单词本身，而是将这些单词表示为稀疏矩阵中的不同值，这基本上是说我将每个单词视为一个数字，作为一个数值索引，进入一个数组。它所做的是，用简单的英语说，将每个消息拆分成其中包含的单词列表，并计算每个单词出现的次数。所以我们称之为“counts”。它基本上是每个单词在每个单独消息中出现的次数的信息。同时，“targets”是我遇到的每封电子邮件的实际分类数据。所以我可以使用我的MultinomialNB()函数调用classifier.fit()来实际使用朴素贝叶斯创建一个模型，该模型将根据我们提供的信息预测新的电子邮件是否是垃圾邮件。
- en: Let's go ahead and run that. It runs pretty quickly! I'm going to use a couple
    of examples here. Let's try a message body that just says `Free Money now!!!`
    which is pretty clearly spam, and a more innocent message that just says `"Hi
    Bob, how about a game of golf tomorrow?"` So we're going to pass these in.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续运行。它运行得相当快！我将在这里使用几个例子。让我们尝试一个只说“现在免费赚钱！”的消息正文，这显然是垃圾邮件，还有一个更无辜的消息，只是说“嗨鲍勃，明天打一场高尔夫怎么样？”所以我们要传递这些消息。
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The first thing we do is convert the messages into the same format that I trained
    my model on. So I use that same vectorizer that I created when creating the model
    to convert each message into a list of words and their frequencies, where the
    words are represented by positions in an array. Then once I''ve done that transformation,
    I can actually use the `predict()` function on my classifier, on that array of
    examples that have transformed into lists of words, and see what we come up with:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是将消息转换为我训练模型的相同格式。所以我使用了创建模型时创建的相同的向量化器，将每条消息转换为一个单词和它们的频率的列表，其中单词由数组中的位置表示。一旦我完成了这个转换，我实际上可以在我的分类器上使用predict()函数，对已经转换成单词列表的示例数组进行预测，看看我们得到了什么：
- en: '![](img/9cd78d24-18d0-4700-b154-aa2ac869322b.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9cd78d24-18d0-4700-b154-aa2ac869322b.jpg)'
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: And sure enough, it works! So, given this array of two input messages, `Free
    Money now!!!` and `Hi Bob`, it's telling me that the first result came back as
    spam and the second result came back as ham, which is what I would expect. That's
    pretty cool. So there you have it.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，它有效！所以，给定这两个输入消息的数组，“现在免费赚钱！”和“嗨鲍勃”，它告诉我第一个结果是垃圾邮件，第二个结果是正常邮件，这正是我所期望的。这很酷。就是这样。
- en: Activity
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动
- en: We had a pretty small dataset here, so you could try running some different
    e-mails through it if you want and see if you get different results. If you really
    want to challenge yourself, try applying train/test to this example. So the real
    measure of whether or not my spam classifier is good or not is not just intuitively
    whether it can figure out that `Free Money now!!!` is spam. You want to measure
    that quantitatively.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里有一个相当小的数据集，所以如果你愿意，你可以尝试通过一些不同的电子邮件，并查看是否会得到不同的结果。如果你真的想挑战自己，尝试将训练/测试应用到这个例子中。所以是否我的垃圾邮件分类器好不好的真正衡量标准不仅仅是它是否能直观地判断“现在免费赚钱！”是垃圾邮件。你想要定量地衡量它。
- en: So if you want a little bit of a challenge, go ahead and try to split this data
    up into a training set and a test dataset. You can actually look up online how
    pandas can split data up into train sets and testing sets pretty easily for you,
    or you can do it by hand. Whatever works for you. See if you can actually apply
    your `MultinomialNB` classifier to a test dataset and measure its performance.
    So, if you want a little bit of an exercise, a little bit of a challenge, go ahead
    and give that a try.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你想挑战一下，尝试将这些数据分成一个训练集和一个测试数据集。你实际上可以在网上查找如何使用pandas很容易地将数据分成训练集和测试集，或者你可以手动操作。无论哪种方式都可以。看看你是否能够将你的MultinomialNB分类器应用到一个测试数据集上，并衡量其性能。所以，如果你想要一点挑战，一点挑战，那就试试看吧。
- en: How cool is that? We just wrote our own spam classifier just using a few lines
    of code in Python. It's pretty easy using scikit-learn and Python. That's Naive
    Bayes in action, and you can actually go and classify some spam or ham messages
    now that you have that under your belt. Pretty cool stuff. Let's talk about clustering
    next.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这有多酷？我们只是用几行Python代码编写了自己的垃圾邮件分类器。使用scikit-learn和Python非常容易。这就是朴素贝叶斯的实际应用，现在你可以去分类一些垃圾邮件或正常邮件了。非常酷。接下来让我们谈谈聚类。
- en: K-Means clustering
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-Means聚类
- en: Next, we're going to talk about k-means clustering, and this is an unsupervised
    learning technique where you have a collection of stuff that you want to group
    together into various clusters. Maybe it's movie genres or demographics of people,
    who knows? But it's actually a pretty simple idea, so let's see how it works.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论K均值聚类，这是一种无监督学习技术，你有一堆东西想要分成各种不同的簇。也许是电影类型或人口统计学，谁知道呢？但这实际上是一个相当简单的想法，所以让我们看看它是如何工作的。
- en: K-means clustering is a very common technique in machine learning where you
    just try to take a bunch of data and find interesting clusters of things just
    based on the attributes of the data itself. Sounds fancy, but it's actually pretty
    simple. All we do in k-means clustering is try to split our data into K groups
    - that's where the K comes from, it's how many different groups you're trying
    to split your data into - and it does this by finding K centroids.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类是机器学习中非常常见的技术，您只需尝试获取一堆数据，并根据数据本身的属性找到有趣的集群。听起来很花哨，但实际上非常简单。在k-means聚类中，我们所做的就是尝试将我们的数据分成K组-这就是K的含义，它是您尝试将数据分成多少不同组的数量-它通过找到K个质心来实现这一点。
- en: 'So, basically, what group a given data point belongs to is defined by which
    of these centroid points it''s closest to in your scatter plot. You can visualize
    this in the following image:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，基本上，给定数据点属于哪个组是由散点图中它最接近的质心点来定义的。您可以在以下图像中可视化这一点：
- en: '![](img/df576023-6bcc-4691-9d5d-baa565c15445.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df576023-6bcc-4691-9d5d-baa565c15445.jpg)'
- en: This is showing an example of k-means clustering with K of three, and the squares
    represent data points in a scatter plot. The circles represent the centroids that
    the k-means clustering algorithm came up with, and each point is assigned a cluster
    based on which centroid it's closest to. So that's all there is to it, really.
    It's an example of unsupervised learning. It isn't a case where we have a bunch
    of data and we already know the correct cluster for a given set of training data;
    rather, you're just given the data itself and it tries to converge on these clusters
    naturally just based on the attributes of the data alone. It's also an example
    where you are trying to find clusters or categorizations that you didn't even
    know were there. As with most unsupervised learning techniques, the point is to
    find latent values, things you didn't really realize were there until the algorithm
    showed them to you.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个K为三的k-means聚类的示例，方块代表散点图中的数据点。圆圈代表k-means聚类算法得出的质心，并且每个点根据它最接近的质心被分配到一个集群中。所以，这就是全部内容，真的。这是无监督学习的一个例子。这不是一个情况，我们有一堆数据，我们已经知道给定一组训练数据的正确集群；相反，你只是给出了数据本身，它试图仅基于数据的属性自然地收敛到这些集群。这也是一个例子，您正在尝试找到甚至您自己都不知道存在的集群或分类。与大多数无监督学习技术一样，重点是找到潜在价值，直到算法向您展示它们之前，您并没有真正意识到它们的存在。
- en: For example, where do millionaires live? I don't know, maybe there is some interesting
    geographical cluster where rich people tend to live, and k-means clustering could
    help you figure that out. Maybe I don't really know if today's genres of music
    are meaningful. What does it mean to be alternative these days? Not much, right?
    But by using k-means clustering on attributes of songs, maybe I could find interesting
    clusters of songs that are related to each other and come up with new names for
    what those clusters represent. Or maybe I can look at demographic data, and maybe
    existing stereotypes are no longer useful. Maybe Hispanic has lost its meaning
    and there's actually other attributes that define groups of people, for example,
    that I could uncover with clustering. Sounds fancy, doesn't it? Really complicated
    stuff. Unsupervised machine learning with K clusters, it sounds fancy, but as
    with most techniques in data science, it's actually a very simple idea.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，百万富翁住在哪里？我不知道，也许有一些有趣的地理集群，富人倾向于居住在那里，k-means聚类可以帮助您找出答案。也许我真的不知道今天的音乐流派是否有意义。现在成为另类是什么意思？不多，对吧？但是通过对歌曲属性进行k-means聚类，也许我可以找到相关的歌曲集群，并为这些集群代表的内容想出新的名称。或者我可以查看人口统计数据，也许现有的刻板印象已经不再有用。也许西班牙裔已经失去了意义，实际上有其他属性可以定义人群，例如，我可以通过聚类发现。听起来很花哨，不是吗？真的很复杂。具有K个集群的无监督机器学习，听起来很花哨，但与数据科学中的大多数技术一样，实际上是一个非常简单的想法。
- en: 'Here''s the algorithm for us in plain English:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们用简单英语的算法：
- en: '**Randomly pick K centroids (k-means):** We start off with a randomly chosen
    set of centroids. So if we have a K of three we''re going to look for three clusters
    in our group, and we will assign three randomly positioned centroids in our scatter
    plot.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**随机选择K个质心（k-means）：**我们从一组随机选择的质心开始。所以如果我们有三个K，我们将在我们的组中寻找三个集群，并且我们将在我们的散点图中分配三个随机位置的质心。'
- en: '**Assign each data point to the centroid it is closest to:** We then assign
    each data point to the randomly assigned centroid that it is closest to.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将每个数据点分配给最接近的质心点：**然后我们将每个数据点分配给它最接近的随机分配的质心点。'
- en: '**Recompute the centroids based on the average position of each centroid''s
    points**: Then recompute the centroid for each cluster that we come up with. That
    is, for a given cluster that we end up with, we will move that centroid to be
    the actual center of all those points.'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**根据每个质心点的平均位置重新计算质心：**然后重新计算我们得出的每个集群的质心。也就是说，对于我们最终得到的给定集群，我们将移动该质心以成为所有这些点的实际中心。'
- en: '**Iterate until points stop changing assignment to centroids:** We will do
    it all again until those centroids stop moving, we hit some threshold value that
    says OK, we have converged on something here.'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代直到点停止改变分配到质心：**我们将一直重复这个过程，直到这些质心停止移动，我们达到了一些阈值值，表示我们已经收敛到了某些东西。'
- en: '**Predict the cluster for new points:** To predict the clusters for new points
    that I haven''t seen before, we can just go through our centroid locations and
    figure out which centroid it''s closest to to predict its cluster.'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预测新点的集群：**要预测我以前没有见过的新点的集群，我们只需通过我们的质心位置并找出它最接近的质心来预测其集群。'
- en: Let's look at a graphical example to make a little bit more sense. We'll call
    the first figure in the following image as A, second as B, third as C and the
    fourth as D.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个图形示例，以便更容易理解。我们将以下图像中的第一个图形称为A，第二个称为B，第三个称为C，第四个称为D。
- en: '![](img/a1e5ec68-fe61-4e99-b6e6-395b3354fef1.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1e5ec68-fe61-4e99-b6e6-395b3354fef1.jpg)'
- en: The gray squares in image A represent data points in our scatter plot. The axes
    represent some different features of something. Maybe it's age and income; it's
    an example I keep using, but it could be anything. And the gray squares might
    represent individual people or individual songs or individual something that I
    want to find relationships between.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图A中的灰色方块代表我们散点图中的数据点。坐标轴代表某些不同特征。也许是年龄和收入；这是我一直在使用的一个例子，但它可以是任何东西。灰色方块可能代表个体人员、个体歌曲或我想要找到它们之间关系的任何东西。
- en: So I start off by just picking three points at random on my scatterplot. Could
    be anywhere. Got to start somewhere, right? The three points (centroids) I selected
    have been shown as circles in image A. So the next thing I'm going to do is for
    each centroid I'll compute which one of the gray points it's closest to. By doing
    that, the points shaded in blue are associated with this blue centroid. The green
    points are closest to the green centroid, and this single red point is closest
    to that red random point that I picked out.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我首先随机在我的散点图上选择了三个点。可以是任何地方。总得从某个地方开始，对吧？我选择的三个点（质心）在图A中被表示为圆圈。接下来，我要做的是对于每个质心，计算它最接近的灰色点是哪一个。通过这样做，图中蓝色阴影的点与蓝色质心相关联。绿色点最接近绿色质心，而这个单独的红点最接近我选择的那个红色随机点。
- en: Of course, you can see that's not really reflective of where the actual clusters
    appear to be. So what I'm going to do is take the points that ended up in each
    cluster and compute the actual center of those points. For example, in the green
    cluster, the actual center of all data turns out to be a little bit lower. We're
    going to move the centroid down a little bit. The red cluster only had one point,
    so its center moves down to where that single point is. And the blue point was
    actually pretty close to the center, so that just moves a little bit. On this
    next iteration we end up with something that looks like image D. Now you can see
    that our cluster for red things has grown a little bit and things have moved a
    little bit, that is, those got taken from the green cluster.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以看到这并不真正反映实际聚类的情况。因此，我要做的是取出每个聚类中的点，并计算这些点的实际中心。例如，在绿色聚类中，所有数据的实际中心实际上要低一点。我们会将质心向下移动一点。红色聚类只有一个点，所以它的中心移动到那个单个点的位置。而蓝色点实际上离中心相当近，所以只是移动了一点。在下一次迭代中，我们得到了类似图D的结果。现在你可以看到我们红色聚类的范围有所增加，事物也有所移动，也就是说，它们被从绿色聚类中取走了。
- en: If we do that again, you can probably predict what's going to happen next. The
    green centroid will move a little bit, the blue centroid will still be about where
    it is. But at the end of the day you're going to end up with the clusters you'd
    probably expect to see. That's how k-means works. So it just keeps iterating,
    trying to find the right centroids until things start moving around and we converge
    on a solution.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次这样做，你可能可以预测接下来会发生什么。绿色的质心会移动一点，蓝色的质心仍然会保持在原来的位置。但最终你会得到你可能期望看到的聚类。这就是k-means的工作原理。因此，它会不断迭代，试图找到正确的质心，直到事物开始移动并收敛于一个解决方案。
- en: Limitations to k-means clustering
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means聚类的局限性
- en: 'So there are some limitations to k-means clustering. Here they are:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，k-means聚类存在一些局限性。以下是其中一些：
- en: '**Choosing K:** First of all, we need to choose the right value of K, and that''s
    not a straightforward thing to do at all. The principal way of choosing K is to
    just start low and keep increasing the value of K depending on how many groups
    you want, until you stop getting large reductions in squared error. If you look
    at the distances from each point to their centroids, you can think of that as
    an error metric. At the point where you stop reducing that error metric, you know
    you probably have too many clusters. So you''re not really gaining any more information
    by adding additional clusters at that point.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择K：**首先，我们需要选择正确的K值，这并不是一件简单的事情。选择K的主要方法是从较低的值开始，根据你想要的群组数量不断增加K的值，直到停止获得平方误差的大幅减少。如果你观察每个点到它们的质心的距离，你可以将其视为一个误差度量。当你停止减少这个误差度量时，你就知道你可能有太多的聚类。因此，在那一点上，通过添加额外的聚类，你实际上并没有获得更多的信息。'
- en: '**Avoiding local minima:** Also, there is a problem of local minima. You could
    just get very unlucky with those initial choices of centroids and they might end
    up just converging on local phenomena instead of more global clusters, so usually,
    you want to run this a few times and maybe average the results together. We call
    that ensemble learning. We''ll talk about that more a little bit later on, but
    it''s always a good idea to run k-means more than once using a different set of
    random initial values and just see if you do in fact end up with the same overall
    results or not.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**避免局部最小值：**此外，还存在局部最小值的问题。你可能会因为初始质心的选择而非常不幸，它们最终可能只会收敛于局部现象，而不是更全局的聚类，因此通常情况下，你需要多次运行这个过程，然后将结果进行平均。我们称之为集成学习。我们稍后会更详细地讨论这个问题，但多次运行k-means并使用不同的随机初始值是一个很好的主意，看看你最终是否得到了相同的结果。'
- en: '**Labeling the clusters:** Finally, the main problem with k-means clustering
    is that there''s no labels for the clusters that you get. It will just tell you
    that this group of data points are somehow related, but you can''t put a name
    on it. It can''t tell you the actual meaning of that cluster. Let''s say I have
    a bunch of movies that I''m looking at, and k-means clustering tells me that bunch
    of science fiction movies are over here, but it''s not going to call them "science
    fiction" movies for me. It''s up to me to actually dig into the data and figure
    out, well, what do these things really have in common? How might I describe that
    in English? That''s the hard part, and k-means won''t help you with that. So again,
    scikit-learn makes it very easy to do this.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标记聚类：**最后，k-means聚类的主要问题是得到的聚类没有标签。它只会告诉你这组数据点在某种程度上是相关的，但你无法给它们贴上名字。它无法告诉你该聚类的实际含义。假设我有一堆电影，我正在观看，k-means聚类告诉我一堆科幻电影在这里，但它不会为我称它们为“科幻”电影。我需要深入数据并弄清楚，这些东西真正有什么共同点？我如何用英语描述它们？这是困难的部分，k-means不会帮助你。所以再次，scikit-learn使这变得非常容易。'
- en: Let's now work up an example and put k-means clustering into action.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们举个例子，让k-means聚类付诸实践。
- en: Clustering people based on income and age
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于收入和年龄对人进行聚类
- en: Let's see just how easy it is to do k-means clustering using scikit-learn and
    Python.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看使用scikit-learn和Python进行k-means聚类有多容易。
- en: The first thing we're going to do is create some random data that we want to
    try to cluster. Just to make it easier, we'll actually build some clusters into
    our fake test data. So let's pretend there's some real fundamental relationship
    between these data, and there are some real natural clusters that exist in it.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是创建一些我们想要尝试进行聚类的随机数据。为了简化，我们实际上会在我们的假测试数据中构建一些聚类。所以假设这些数据之间存在一些真实的基本关系，并且其中存在一些真实的自然聚类。
- en: 'So to do that, we can work with this little `createClusteredData()` function
    in Python:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们可以使用Python中的`createClusteredData()`函数：
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The function starts off with a consistent random seed so you'll get the same
    result every time. We want to create clusters of N people in k clusters. So we
    pass `N` and `k` to `createClusteredData().`
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数从一致的随机种子开始，因此每次都会得到相同的结果。我们想要在k个聚类中创建N个人的聚类。所以我们将`N`和`k`传递给`createClusteredData()`。
- en: Our code figures out how many points per cluster that works out to first and
    stores it in `pointsPerCluster`. Then, it builds up list `X` that starts off empty.
    For each cluster, we're going to create some random centroid of income (`incomeCentroid`)
    between 20,000 and 200,000 dollars and some random centroid of age (`ageCentroid`)
    between the age of 20 and 70.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码首先计算出每个聚类的点数，并将其存储在`pointsPerCluster`中。然后，它构建了一个起始为空的列表`X`。对于每个聚类，我们将创建一些收入的随机中心（`incomeCentroid`），介于20,000到200,000美元之间，以及一些年龄的随机中心（`ageCentroid`），介于20到70岁之间。
- en: What we're doing here is creating a fake scatter plot that will show income
    versus age for `N` people and `k` clusters. So for each random centroid that we
    created, I'm then going to create a normally distributed set of random data with
    a standard deviation of 10,000 in income and a standard deviation of 2 in age.
    That will give us back a bunch of age income data that is clustered into some
    pre-existing clusters that we can chose at random. OK, let's go ahead and run
    that.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的是创建一个假的散点图，显示了`N`个人和`k`个聚类的收入与年龄。所以对于我们创建的每个随机中心，我将创建一组正态分布的随机数据，收入的标准差为10,000，年龄的标准差为2。这将给我们一堆年龄收入数据，它们被聚类到一些我们可以随机选择的预先存在的聚类中。好的，让我们继续运行。
- en: Now, to actually do k-means, you'll see how easy it is.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要实际进行k-means，你会看到它有多容易。
- en: '[PRE15]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: All you need to do is import `KMeans` from scikit-learn's `cluster` package.
    We're also going to import `matplotlib` so we can visualize things, and also import
    `scale` so we can take a look at how that works.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你所需要做的就是从scikit-learn的cluster包中导入`KMeans`。我们还要导入`matplotlib`，这样我们就可以可视化数据，还要导入`scale`，这样我们就可以看看它是如何工作的。
- en: So we use our `createClusteredData()` function to say 100 random people around
    5 clusters. So there are 5 natural clusters for the data that I'm creating. We
    then create a model, a KMeans model with k of 5, so we're picking 5 clusters because
    we know that's the right answer. But again, in unsupervised learning you don't
    necessarily know what the real value of `k` is. You need to iterate and converge
    on it yourself. And then we just call `model.fit` using my KMeans `model` using
    the data that we had.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们使用我们的`createClusteredData()`函数来说有100个随机人分布在5个聚类中。所以对于我创建的数据，有5个自然的聚类。然后我们创建一个模型，一个k为5的KMeans模型，所以我们选择5个聚类，因为我们知道这是正确的答案。但是在无监督学习中，你不一定知道`k`的真实值。你需要自己迭代和收敛到它。然后我们只需使用我们的KMeans模型使用我们的数据来调用`model.fit`。
- en: Now the scale I alluded to earlier, that's normalizing the data. One important
    thing with k-means is that it works best if your data is all normalized. That
    means everything is at the same scale. So a problem that I have here is that my
    ages range from 20 to 70, but my incomes range all the way up to 200,000\. So
    these values are not really comparable. The incomes are much larger than the age
    values. `Scale` will take all that data and scale it together to a consistent
    scale so I can actually compare these things as apples to apples, and that will
    help a lot with your k-means results.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我之前提到的规模，那是对数据进行归一化。k-means的一个重要问题是，如果你的数据都归一化，它的效果会更好。这意味着一切都在相同的尺度上。所以我在这里遇到的问题是，我的年龄范围是20到70岁，但我的收入范围高达20万美元。所以这些值并不真正可比。收入远远大于年龄值。`Scale`将把所有数据一起缩放到一个一致的尺度，这样我就可以将这些数据进行比较，这将有助于你的k-means结果。
- en: 'So, once we''ve actually called `fit` on our model, we can actually look at
    the resulting labels that we got. Then we can actually visualize it using a little
    bit of `matplotlib` magic. You can see in the code we have a little trick where
    we assigned the color to the labels that we ended up with converted to some floating
    point number. That''s just a little trick you can use to assign arbitrary colors
    to a given value. So let''s see what we end up with:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，一旦我们在我们的模型上调用了`fit`，我们实际上可以查看我们得到的结果标签。然后我们可以使用一点`matplotlib`的魔法来可视化它。你可以在代码中看到我们有一个小技巧，我们将颜色分配给了我们最终得到的标签，转换为一些浮点数。这只是一个小技巧，你可以用来为给定的值分配任意颜色。所以让我们看看我们最终得到了什么：
- en: '![](img/5799932c-1451-4c6a-a10a-14372dbc79b6.jpg)![](img/e988ba57-00c7-4e2e-a3c5-45cf7165bd4c.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5799932c-1451-4c6a-a10a-14372dbc79b6.jpg)![](img/e988ba57-00c7-4e2e-a3c5-45cf7165bd4c.jpg)'
- en: It didn't take that long. You see the results are basically what clusters I
    assigned everything into. We know that our fake data is already pre-clustered,
    so it seems that it identified the first and second clusters pretty easily. It
    got a little bit confused beyond that point, though, because our clusters in the
    middle are actually a little bit mushed together. They're not really that distinct,
    so that was a challenge for k-means. But regardless, it did come up with some
    reasonable guesses at the clusters. This is probably an example of where four
    clusters would more naturally fit the data.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这并没有花太长时间。你可以看到结果基本上是我分配给每个东西的聚类。我们知道我们的假数据已经被预先聚类，所以它似乎很容易地识别了第一和第二个聚类。然而，在那之后它有点困惑了，因为我们中间的聚类实际上有点混在一起。它们并不是真的那么明显，所以这对k均值来说是一个挑战。但无论如何，它确实对聚类提出了一些合理的猜测。这可能是四个聚类更自然地适应数据的一个例子。
- en: Activity
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动
- en: So what I want you to do for an activity is to try a different value of k and
    see what you end up with. Just eyeballing the preceding graph, it looks like four
    would work well. Does it really? What happens if I increase k too large? What
    happens to my results? What does it try to split things into, and does it even
    make sense? So, play around with it, try different values of `k`. So in the `n_clusters()`
    function, change the 5 to something else. Run all through it again and see you
    end up with.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想让你做的一个活动是尝试不同的k值，看看你最终得到了什么。仅仅凭眼前的图表，看起来四个可能效果很好。真的吗？如果我把k增加得太大会发生什么？我的结果会怎样？它会尝试将事物分成什么，这甚至有意义吗？所以，玩一下，尝试不同的`k`值。所以在`n_clusters()`函数中，将5改为其他值。再次运行一遍，看看你最终得到了什么。
- en: 'That''s all there is to k-means clustering. It''s just that simple. You can
    just use scikit-learn''s `KMeans` thing from `cluster`. The only real gotcha:
    make sure you scale the data, normalize it. You want to make sure the things that
    you''re using k-means on are comparable to each other, and the `scale()` function
    will do that for you. So those are the main things for k-means clustering. Pretty
    simple concept, even simpler to do it using scikit-learn.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是k均值聚类的全部内容。就是这么简单。你可以使用scikit-learn的`KMeans`从`cluster`中的东西。唯一真正需要注意的是：确保你对数据进行缩放，归一化。你希望确保你用k均值进行处理的东西是可比较的，`scale()`函数会为你做到这一点。所以这些是k均值聚类的主要内容。非常简单的概念，使用scikit-learn更简单。
- en: That's all there is to it. That's k-means clustering. So if you have a bunch
    of data that is unclassified and you don't really have the right answers ahead
    of time, it's a good way to try to naturally find interesting groupings of your
    data, and maybe that can give you some insight into what that data is. It's a
    good tool to have. I've used it before in the real world and it's really not that
    hard to use, so keep that in your tool chest.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。这就是k均值聚类。所以如果你有一堆未分类的数据，而且你事先并没有正确的答案，这是一个很好的方法来自然地找到数据的有趣分组，也许这可以让你对数据有一些见解。这是一个很好的工具。我以前在现实世界中使用过它，而且真的并不难使用，所以记住它。
- en: Measuring entropy
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量熵
- en: Quite soon we're going to get to one of the cooler parts of machine learning,
    at least I think so, called decision trees. But before we can talk about that,
    it's a necessary to understand the concept of entropy in data science.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 很快我们就要进入机器学习中更酷的部分之一，至少我认为是，叫做决策树。但在我们谈论那之前，理解数据科学中熵的概念是必要的。
- en: So entropy, just like it is in physics and thermodynamics, is a measure of a
    dataset's disorder, of how same or different the dataset is. So imagine we have
    a dataset of different classifications, for example, animals. Let's say I have
    a bunch of animals that I have classified by species. Now, if all of the animals
    in my dataset are an iguana, I have very low entropy because they're all the same.
    But if every animal in my dataset is a different animal, I have iguanas and pigs
    and sloths and who knows what else, then I would have a higher entropy because
    there's more disorder in my dataset. Things are more different than they are the
    same.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 所以熵，就像在物理学和热力学中一样，是数据集的混乱程度的度量，数据集的相同或不同程度。所以想象一下，我们有一个不同分类的数据集，例如动物。比如说我有一堆我已经按物种分类的动物。现在，如果我的数据集中的所有动物都是鬣蜥，我就有很低的熵，因为它们都是一样的。但如果我的数据集中的每个动物都是不同的动物，我有鬣蜥和猪和树懒和谁知道还有什么，那么我就会有更高的熵，因为我的数据集中有更多的混乱。事物之间的不同性大于相同性。
- en: Entropy is just a way of quantifying that sameness or difference throughout
    my data. So, an entropy of 0 implies all the classes in the data are the same,
    whereas if everything is different, I would have a high entropy, and something
    in between would be a number in between. Entropy just describes how same or different
    the things in a dataset are.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 熵只是一种量化我的数据中相同或不同的方式。所以，熵为0意味着数据中的所有类别都是相同的，而如果一切都不同，我就会有很高的熵，而介于两者之间的情况将是介于两者之间的数字。熵只是描述数据集中的事物是相同还是不同的方式。
- en: 'Now mathematically, it''s a little bit more involved than that, so when I actually
    compute a number for entropy, it''s computed using the following expression:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从数学上讲，它比那复杂一点，所以当我实际计算熵的数值时，它是使用以下表达式计算的：
- en: '![](img/7dd736b4-b705-487c-94e6-df5a8640a08a.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7dd736b4-b705-487c-94e6-df5a8640a08a.jpg)'
- en: 'So for every different class that I have in my data, I''m going to have one
    of these p terms, p[1], p[2], and so on and so forth through p[n], for n different
    classes that I might have. The p just represents the proportion of the data that
    is that class. And if you actually plot what this looks like for each term- `pi*
    ln * pi`, it''ll look a little bit something like the following graph:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 所以对于我数据中的每个不同类，我将有一个这样的p项，p[1]，p[2]，等等，直到p[n]，对于我可能有的n个不同类。p只是表示数据中是那个类的比例。如果你实际上绘制出每个项的样子-
    `pi* ln * pi`，它看起来会有点像下面的图表：
- en: '![](img/b3ae9e6d-662c-4568-b28f-e9fed56739e7.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3ae9e6d-662c-4568-b28f-e9fed56739e7.jpg)'
- en: You add these up for each individual class. For example, if the proportion of
    the data, that is, for a given class is 0, then the contribution to the overall
    entropy is 0\. And if everything is that class, then again the contribution to
    the overall entropy is 0 because in either case, if nothing is this class or everything
    is this class, that's not really contributing anything to the overall entropy.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 你为每个单独的类加起来。例如，如果数据的比例，也就是说，对于给定的类是0，那么对总熵的贡献就是0。如果一切都是这个类，那么对总熵的贡献也是0，因为在任何一种情况下，如果没有任何东西是这个类或者一切都是这个类，那实际上并没有对总熵做出任何贡献。
- en: It's the things in the middle that contribute entropy of the class, where there's
    some mixture of this classification and other stuff. When you add all these terms
    together, you end up with an overall entropy for the entire dataset. So mathematically,
    that's how it works out, but again, the concept is very simple. It's just a measure
    of how disordered your dataset, how same or different the things in your data
    are.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 中间的事物会为类的熵做出贡献，那里有一些这种分类和其他东西的混合。当你把所有这些项加在一起时，你就得到了整个数据集的总熵。所以从数学上讲，就是这样运作的，但是，这个概念非常简单。它只是衡量你的数据集有多无序，你的数据中的事物有多相同或不同。
- en: Decision trees - Concepts
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树-概念
- en: Believe it or not, given a set of training data, you can actually get Python
    to generate a flowchart for you to make a decision. So if you have something you're
    trying to predict on some classification, you can use a decision tree to actually
    look at multiple attributes that you can decide upon at each level in the flowchart.
    You can print out an actual flowchart for you to use to make a decision from,
    based on actual machine learning. How cool is that? Let's see how it works.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，给定一组训练数据，你实际上可以让Python为你生成一个流程图来做出决定。所以如果你有一些你想要在某些分类上进行预测的东西，你可以使用决策树来实际查看流程图中每个级别上可以决定的多个属性。你可以打印出一个实际的流程图供你使用，以便基于实际的机器学习做出决定。这有多酷？让我们看看它是如何运作的。
- en: I personally find decision trees are one of the most interesting applications
    of machine learning. A decision tree basically gives you a flowchart of how to
    make some decision.You have some dependent variable, like whether or not I should
    go play outside today or not based on the weather. When you have a decision like
    that that depends on multiple attributes or multiple variables, a decision tree
    could be a good choice.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我个人认为决策树是机器学习中最有趣的应用之一。决策树基本上给出了如何做出某些决定的流程图。你有一些依赖变量，比如今天是否应该根据天气出去玩。当你有一个依赖于多个属性或多个变量的决定时，决策树可能是一个不错的选择。
- en: There are many different aspects of the weather that might influence my decision
    of whether I should go outside and play. It might have to do with the humidity,
    the temperature, whether it's sunny or not, for example. A decision tree can look
    at all these different attributes of the weather, or anything else, and decide
    what are the thresholds? What are the decisions I need to make on each one of
    those attributes before I arrive at a decision of whether or not I should go play
    outside? That's all a decision tree is. So it's a form of supervised learning.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 天气的许多不同方面可能会影响我是否应该出去玩的决定。这可能与湿度、温度、是否晴天等有关。决策树可以查看天气的所有这些不同属性，或者其他任何东西，并决定什么是阈值？在每个属性上我需要做出什么决定，然后才能决定我是否应该出去玩？这就是决策树的全部内容。所以它是一种监督学习。
- en: 'The way it would work in this example would be as follows. I would have some
    sort of dataset of historical weather, and data about whether or not people went
    outside to play on a particular day. I would feed the model this data of whether
    it was sunny or not on each day, what the humidity was, and if it was windy or
    not; and whether or not it was a good day to go play outside. Given that training
    data, a decision tree algorithm can then arrive at a tree that gives us a flowchart
    that we can print out. It looks just like the following flow chart. You can just
    walk through and figure out whether or not it''s a good day to play outside based
    on the current attributes. You can use that to predict the decision for a new
    set of values:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，它的工作方式如下。我会有一些关于历史天气的数据集，以及关于人们在特定一天是否出去玩的数据。我会向模型提供这些数据，比如每天是否晴天，湿度是多少，是否刮风，以及那天是否适合出去玩。在给定这些训练数据的情况下，决策树算法可以得出一棵树，给我们一个流程图，我们可以打印出来。它看起来就像下面的流程图。你可以浏览并根据当前属性来判断是否适合出去玩。你可以用它来预测新一组值的决定：
- en: '![](img/f0d013a2-8886-4d48-8a56-3b4e166a551a.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0d013a2-8886-4d48-8a56-3b4e166a551a.jpg)'
- en: How cool is that? We have an algorithm that will make a flowchart for you automatically
    just based on observational data. What's even cooler is how simple it all works
    once you learn how it works.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这有多酷？我们有一个算法，可以根据观测数据自动生成流程图。更酷的是，一旦你学会了它的工作原理，它的一切都是如此简单。
- en: Decision tree example
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树示例
- en: Let's say I want to build a system that will automatically filter out resumes
    based on the information in them. A big problem that technology companies have
    is that we get tons and tons of resumes for our positions. We have to decide who
    we actually bring in for an interview, because it can be expensive to fly somebody
    out and actually take the time out of the day to conduct an interview. So what
    if there were a way to actually take historical data on who actually got hired
    and map that to things that are found on their resume?
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我想建立一个系统，根据其中的信息自动筛选简历。技术公司面临的一个大问题是，我们为我们的职位收到了大量的简历。我们必须决定我们实际上要邀请谁来面试，因为飞某人出来并且实际上花时间进行面试可能是很昂贵的。那么，如果有一种方法可以将实际上被雇佣的人的历史数据与他们简历中的信息相匹配，那会怎么样呢？
- en: We could construct a decision tree that will let us go through an individual
    resume and say, "OK, this person actually has a high likelihood of getting hired,
    or not". We can train a decision tree on that historical data and walk through
    that for future candidates. Wouldn't that be a wonderful thing to have?
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建一个决策树，让我们可以浏览个人简历，并说，“好的，这个人实际上有很高的被雇佣可能性，或者没有”。我们可以根据历史数据训练一个决策树，并为未来的候选人走过这个流程。那不是一件很美好的事情吗？
- en: 'So let''s make some totally fabricated hiring data that we''re going to use
    in this example:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们制作一些完全捏造的雇佣数据，我们将在这个例子中使用。
- en: '![](img/c4a0232c-b18c-41a8-a032-92b7466ca4a0.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4a0232c-b18c-41a8-a032-92b7466ca4a0.jpg)'
- en: In the preceding table, we have candidates that are just identified by numerical
    identifiers. I'm going to pick some attributes that I think might be interesting
    or helpful to predict whether or not they're a good hire or not. How many years
    of experience do they have? Are they currently employed? How many employers have
    they had previous to this one? What's their level of education? What degree do
    they have? Did they go to what we classify as a top-tier school? Did they do an
    internship while they were in college? We can take a look at this historical data,
    and the dependent variable here is `Hired`. Did this person actually get a job
    offer or not based on that information?
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表中，我们有一些只用数字标识的候选人。我将挑选一些我认为可能有趣或有助于预测他们是否是一个好的雇佣者的属性。他们有多少年的工作经验？他们目前有工作吗？他们之前有多少雇主？他们的教育水平是多少？他们有什么学位？他们是否上过我们分类为顶尖学校？他们在大学期间是否做过实习？我们可以看一下这些历史数据，这里的因变量是“被雇佣”。这个人实际上是否根据这些信息得到了工作机会？
- en: 'Now, obviously there''s a lot of information that isn''t in this model that
    might be very important, but the decision tree that we train from this data might
    actually be useful in doing an initial pass at weeding out some candidates. What
    we end up with might be a tree that looks like the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，显然这个模型中没有的很多信息可能非常重要，但是我们从这些数据中训练出来的决策树实际上可能有助于在初步筛选一些候选人时使用。我们最终得到的可能是一个看起来像下面这样的树：
- en: '![](img/47031034-ef5a-4c31-b66f-63953d852abf.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47031034-ef5a-4c31-b66f-63953d852abf.jpg)'
- en: So it just turns out that in my totally fabricated data, anyone that did an
    internship in college actually ended up getting a job offer. So my first decision
    point is "did this person do an internship or not?" If yes, go ahead and bring
    them in. In my experience, internships are actually a pretty good predictor of
    how good a person is. If they have the initiative to actually go out and do an
    internship, and actually learn something at that internship, that's a good sign.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所以事实证明，在我完全捏造的数据中，任何在大学实习的人最终都得到了工作机会。所以我的第一个决策点是“这个人是否做过实习？”如果是，那就让他们进来吧。根据我的经验，实习实际上是一个相当好的人才预测指标。如果他们有主动性去实习，并且在实习中真正学到了东西，那是一个好迹象。
- en: Do they currently have a job? Well, if they are currently employed, in my very
    small fake dataset it turned out that they are worth hiring, just because somebody
    else thought they were worth hiring too. Obviously it would be a little bit more
    of a nuanced decision in the real world.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们目前有工作吗？嗯，如果他们目前有工作，那么在我的非常小的虚拟数据集中，结果表明他们值得雇佣，只是因为其他人也认为他们值得雇佣。显然，在现实世界中，这将是一个更微妙的决定。
- en: If they're not currently employed, do they have less than one prior employer?
    If yes, this person has never held a job and they never did an internship either.
    Probably not a good hire decision. Don't hire that person.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果他们目前没有工作，他们之前的雇主少于一个吗？如果是，这个人以前从未工作过，他们也没有做过实习。可能不是一个好的雇佣决定。不要雇佣这个人。
- en: But if they did have a previous employer, did they at least go to a top-tier
    school? If not, it's kind of iffy. If so, then yes, we should hire this person
    based on the data that we trained on.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但是如果他们之前有过雇主，他们是否至少上过一所顶尖学校？如果没有，那就有点靠不住。如果是，那么是的，我们应该根据我们训练的数据来雇佣这个人。
- en: Walking through a decision tree
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 浏览决策树
- en: So that's how you walk through the results of a decision tree. It's just like
    going through a flowchart, and it's kind of awesome that an algorithm can produce
    this for you. The algorithm itself is actually very simple. Let me explain how
    the algorithm works.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你如何浏览决策树的结果。就像浏览流程图一样，算法可以为你产生这样的结果，这真是太棒了。算法本身实际上非常简单。让我解释一下算法是如何工作的。
- en: 'At each step of the decision tree flowchart, we find the attribute that we
    can partition our data on that minimizes the entropy of the data at the next step.
    So we have a resulting set of classifications: in this case hire or don''t hire,
    and we want to choose the attribute decision at that step that will minimize the
    entropy at the next step.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树流程图的每一步，我们找到可以将我们的数据分区的属性，以最小化下一步数据的熵。所以我们得到了一组分类：在这种情况下是雇佣或不雇佣，我们希望选择在下一步最小化熵的属性决策。
- en: At each step we want to make all of the remaining choices result in either as
    many no hires or as many hire decisions as possible. We want to make that data
    more and more uniform so as we work our way down the flowchart, and we ultimately
    end up with a set of candidates that are either all hires or all no hires so we
    can classify into yes/no decisions on a decision tree. So we just walk down the
    tree, minimize entropy at each step by choosing the right attribute to decide
    on, and we keep on going until we run out.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步，我们希望所有剩下的选择都导致尽可能多的不录用或尽可能多的录用决定。我们希望使数据变得越来越统一，因此当我们沿着流程图向下工作时，最终我们最终得到一组候选人，要么全部录用，要么全部不录用，这样我们就可以在决策树上对是/否做出分类。因此，我们只需沿着树走，通过选择正确的属性来最小化每一步的熵，直到我们用完为止。
- en: There's a fancy name for this algorithm. It's called **ID3** (**Iterative Dichotomiser
    3**). It is what's known as a greedy algorithm. So as it goes down the tree, it
    just picks the attribute that will minimize entropy at that point. Now that might
    not actually result in an optimal tree that minimizes the number of choices that
    you have to make, but it will result in a tree that works, given the data that
    you gave it.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这种算法有一个花哨的名字。它被称为**ID3**（**迭代二分器3**）。这是一个贪婪算法。因此，当它沿着树走时，它只选择在那一点上最小化熵的属性。现在，这可能实际上不会导致最小化你必须做出的选择的最佳树，但它将会得到一个树，鉴于你给它的数据。
- en: Random forests technique
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林技术
- en: Now one problem with decision trees is that they are very prone to overfitting,
    so you can end up with a decision tree that works beautifully for the data that
    you trained it on, but it might not be that great for actually predicting the
    correct classification for new people that it hasn't seen before. Decision trees
    are all about arriving at the right decision for the training data that you gave
    it, but maybe you didn't really take into account the right attributes, maybe
    you didn't give it enough of a representative sample of people to learn from.
    This can result in real problems.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在决策树的一个问题是它们很容易过拟合，所以你可能会得到一个对训练数据非常有效的决策树，但对于那些它以前没有见过的新人的正确分类预测可能并不那么好。决策树的核心是为你提供的训练数据做出正确的决策，但也许你并没有真正考虑到正确的属性，也许你没有给它足够代表性的人员样本来学习。这可能会导致真正的问题。
- en: So to combat this issue, we use a technique called random forests, where the
    idea is that we sample the data that we train on, in different ways, for multiple
    different decision trees. Each decision tree takes a different random sample from
    our set of training data and constructs a tree from it. Then each resulting tree
    can vote on the right result.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们使用一种称为随机森林的技术，其思想是我们以不同的方式对我们进行训练的数据进行采样，用于多个不同的决策树。每棵决策树从我们的训练数据集中随机选择不同的样本，并从中构建一棵树。然后每棵树都可以对正确的结果进行投票。
- en: Now that technique of randomly resampling our data with the same model is a
    term called bootstrap aggregating, or bagging. This is a form of what we call
    ensemble learning, which we'll cover in more detail shortly. But the basic idea
    is that we have multiple trees, a forest of trees if you will, each that uses
    a random subsample of the data that we have to train on. Then each of these trees
    can vote on the final result, and that will help us combat overfitting for a given
    set of training data.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用相同模型对我们的数据进行随机重采样的技术被称为自举聚合，或者称为装袋。这是一种我们称之为集成学习的形式，我们很快会更详细地介绍。但基本思想是我们有多个树，如果你愿意的话，可以称之为树的森林，每个树都使用我们要训练的数据的随机子样本。然后每棵树都可以对最终结果进行投票，这将帮助我们对给定的训练数据进行过拟合。
- en: The other thing random forests can do is actually restrict the number of attributes
    that it can choose, between at each stage, while it is trying to minimize the
    entropy as it goes. And we can randomly pick which attributes it can choose from
    at each level. So that also gives us more variation from tree to tree, and therefore
    we get more of a variety of algorithms that can compete with each other. They
    can all vote on the final result using slightly different approaches to arriving
    at the same answer.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林可以做的另一件事是在它尝试最小化熵的同时，实际上限制它可以在每个阶段选择的属性数量。我们可以在每个级别随机选择它可以选择的属性。因此，这也使我们的树与树之间更加多样化，因此我们得到了更多可以相互竞争的算法的变化。它们可以使用略有不同的方法对最终结果进行投票，以达到相同的答案。
- en: So that's how random forests work. Basically, it is a forest of decision trees
    where they are drawing from different samples and also different sets of attributes
    at each stage that it can choose between.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是随机森林的工作原理。基本上，它是一组决策树的森林，它们从不同的样本和不同的属性集中进行选择。
- en: So, with all that, let's go make some decision trees. We'll use random forests
    as well when we're done, because scikit-learn makes it really really easy to do,
    as you'll see soon.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有了这一切，让我们去做一些决策树。当我们完成后，我们也将使用随机森林，因为scikit-learn使得这变得非常容易，很快你就会看到。
- en: Decision trees - Predicting hiring decisions using Python
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树 - 使用Python预测招聘决策
- en: Turns out that it's easy to make decision trees; in fact it's crazy just how
    easy it is, with just a few lines of Python code. So let's give it a try.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，制作决策树很容易；事实上，只需几行Python代码就可以做到这一点。所以让我们试一试。
- en: I've included a `PastHires.csv` file with your book materials, and that just
    includes some fabricated data, that I made up, about people that either got a
    job offer or not based on the attributes of those candidates.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经在你的书材料中包含了一个`PastHires.csv`文件，里面只包含了一些虚构的数据，是我根据候选人的属性编造的，这些候选人要么得到了工作机会，要么没有。
- en: '[PRE16]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You'll want to please immediately change that path I used here for my own system
    (`c:/spark/DataScience/PastHires.csv`) to wherever you have installed the materials
    for this book. I'm not sure where you put it, but it's almost certainly not there.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请立即更改我在这里使用的路径，用于我的系统（`c:/spark/DataScience/PastHires.csv`），改为你安装本书材料的位置。我不确定你把它放在哪里，但几乎肯定不是那里。
- en: We will use `pandas` to read our CSV in, and create a DataFrame object out of
    it. Let's go ahead and run our code, and we can use the `head()` function on the
    DataFrame to print out the first few lines and make sure that it looks like it
    makes sense.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`pandas`读取我们的CSV文件，并将其创建为一个DataFrame对象。让我们继续运行我们的代码，并可以使用DataFrame的`head()`函数打印出前几行，确保它看起来是有意义的。
- en: '[PRE17]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Sure enough we have some valid data in the output:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，我们在输出中有一些有效的数据：
- en: '![](img/39aa9b7f-af63-4996-ae11-fc8d96174087.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39aa9b7f-af63-4996-ae11-fc8d96174087.jpg)'
- en: So, for each candidate ID, we have their years of past experience, whether or
    not they were employed, their number of previous employers, their highest level
    of education, whether they went to a top-tier school, and whether they did an
    internship; and finally here, in the Hired column, the answer - where we knew
    that we either extended a job offer to this person or not.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个候选人ID，我们有他们的过去工作经验年限，是否就业，以前的雇主数量，他们的最高教育水平，是否就读顶级学校，是否做过实习；最后，在Hired列中，答案-我们知道我们是否向这个人提供了工作机会。
- en: 'As usual, most of the work is just in massaging your data, preparing your data,
    before you actually run the algorithms on it, and that''s what we need to do here.
    Now scikit-learn requires everything to be numerical, so we can''t have Ys and
    Ns and BSs and MSs and PhDs. We have to convert all those things to numbers for
    the decision tree model to work. The way to do this is to use some short-hand
    in pandas, which makes these things easy. For example:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，大部分工作只是在处理数据，准备数据，然后才实际运行算法，这就是我们需要在这里做的。现在scikit-learn要求一切都是数字，所以我们不能有Y和N和BS和MS和PhD。我们必须将所有这些东西转换为数字，以便决策树模型能够工作。在pandas中使用一些简写可以使这些事情变得容易。例如：
- en: '[PRE18]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Basically, we're making a dictionary in Python that maps the letter Y to the
    number 1, and the letter N to the value 0\. So, we want to convert all our Ys
    to 1s and Ns to 0s. So 1 will mean yes and 0 will mean no. What we do is just
    take the Hired column from the DataFrame, and call `map()` on it, using a dictionary.
    This will go through the entire Hired column, in the entire DataFrame and use
    that dictionary lookup to transform all the entries in that column. It returns
    a new DataFrame column that I'm putting back into the Hired column. This replaces
    the Hired column with one that's been mapped to 1s and 0s.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们在Python中创建一个字典，将字母Y映射为数字1，将字母N映射为值0。因此，我们想将所有的Y转换为1，将所有的N转换为0。因此1表示是，0表示否。我们只需从DataFrame中取出Hired列，并在其上调用`map()`，使用一个字典。这将遍历整个DataFrame中的Hired列，并使用该字典查找来转换该列中的所有条目。它返回一个新的DataFrame列，我将其放回到Hired列中。这将用1和0映射的列替换Hired列。
- en: 'We do the same thing for Employed, Top-tier school and Interned, so all those
    get mapped using the yes/no dictionary. So, the Ys and Ns become 1s and 0s instead.
    For the Level of Education, we do the same trick, we just create a dictionary
    that assigns BS to 0, MS to 1, and PhD to 2 and uses that to remap those degree
    names to actual numerical values. So if I go ahead and run that and do a `head()`
    again, you can see that it worked:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对就业，顶级学校和实习做同样的处理，所以所有这些都使用是/否字典进行映射。因此，Y和N变成了1和0。对于教育水平，我们也使用同样的技巧，创建一个将BS分配为0，MS分配为1，PhD分配为2的字典，并使用它来重新映射这些学位名称为实际的数值。所以如果我继续运行并再次使用`head()`，你会看到它起作用了：
- en: '![](img/8f975ef7-dde7-4e2c-988f-a115252ad95d.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f975ef7-dde7-4e2c-988f-a115252ad95d.jpg)'
- en: All my yeses are 1's, my nos are 0's, and my Level of Education is now represented
    by a numerical value that has real meaning.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我的所有是1，我的所有否是0，我的教育水平现在由具有实际含义的数值表示。
- en: Next we need to prepare everything to actually go into our decision tree classifier,
    which isn't that hard. To do that, we need to separate our feature information,
    which are the attributes that we're trying to predict from, and our target column,
    which contains the thing that we're trying to predict.To extract the list of feature
    name columns, we are just going to create a list of columns up to number 6\. We
    go ahead and print that out.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要准备一切以实际进入我们的决策树分类器，这并不难。为此，我们需要分离我们的特征信息，即我们试图预测的属性，以及我们的目标列，其中包含我们试图预测的东西。为了提取特征名称列的列表，我们只需创建一个列的列表，直到第6列。我们继续打印出来。
- en: '[PRE19]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We get the following output:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/b121561b-c7b2-4418-aad6-bc33f5c608d2.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b121561b-c7b2-4418-aad6-bc33f5c608d2.jpg)'
- en: 'Above are the column names that contain our feature information: Years Experience,
    Employed?, Previous employers, Level of Education, Top-tier school, and Interned.
    These are the attributes of candidates that we want to predict hiring on.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 上面是包含我们特征信息的列名：工作经验年限，是否就业，以前的雇主，教育水平，顶级学校和实习。这些是我们想要预测雇佣的候选人的属性。
- en: 'Next, we construct our *y* vector which is assigned what we''re trying to predict,
    that is our Hired column:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们构建我们的*y*向量，它被分配为我们要预测的内容，也就是我们的Hired列：
- en: '[PRE20]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This code extracts the entire Hired column and calls it `y`. Then it takes all
    of our columns for the feature data and puts them in something called `X`. This
    is a collection of all of the data and all of the feature columns, and `X` and
    `y` are the two things that our decision tree classifier needs.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码提取整个Hired列并将其命名为`y`。然后它将所有特征数据的列放入一个名为`X`的对象中。这是所有数据和所有特征列的集合，`X`和`y`是我们的决策树分类器需要的两个东西。
- en: 'To actually create the classifier itself, two lines of code: we call `tree.DecisionTreeClassifier()`
    to create our classifier, and then we fit it to our feature data (`X`) and the
    answers (`y`)- whether or not people were hired. So, let''s go ahead and run that.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 要实际创建分类器本身，只需两行代码：我们调用`tree.DecisionTreeClassifier()`来创建我们的分类器，然后将其拟合到我们的特征数据(`X`)和答案(`y`)
    - 是否雇佣了人。所以，让我们继续运行。
- en: 'Displaying graphical data is a little bit tricky, and I don''t want to distract
    us too much with the details here, so please just consider the following boilerplate
    code. You don''t need to get into how Graph viz works here - and dot files and
    all that stuff: it''s not important to our journey right now. The code you need
    to actually display the end results of a decision tree is simply:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 显示图形数据有点棘手，我不想在这里分散我们太多的注意力，所以请只考虑以下样板代码。你不需要深入了解Graph viz在这里的工作方式 - 以及dot文件和所有这些东西：这对我们的旅程现在不重要。你需要实际显示决策树最终结果的代码只是：
- en: '[PRE21]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: So let's go ahead and run this.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们继续运行这个。
- en: 'This is what your output should now look like:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你的输出现在应该是这样的：
- en: '![](img/19fbf5b0-4476-46de-897e-ce3bb7a5237f.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19fbf5b0-4476-46de-897e-ce3bb7a5237f.jpg)'
- en: There we have it! How cool is that?! We have an actual flow chart here.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有了！这有多酷！我们这里有一个实际的流程图。
- en: 'Now, let me show you how to read it. At each stage, we have a decision. Remember
    most of our data which is yes or no, is going to be **0** or **1**. So, the first
    decision point becomes: is Employed? less than **0.5**? Meaning that if we have
    an employment value of 0, that is no, we''re going to go left.If employment is
    1, that is yes, we''re going to go right.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我告诉你如何阅读它。在每个阶段，我们都有一个决定。记住，我们的大多数数据是是或否，将是0或1。所以，第一个决定点是：就业？小于0.5吗？这意味着如果我们有一个就业价值为0，那就是不，我们将向左走。如果就业是1，也就是是，我们将向右走。
- en: So, were they previously employed? If not go left, if yes go right. It turns
    out that in my sample data, everyone who is currently employed actually got a
    job offer, so I can very quickly say if you are currently employed, yes, you're
    worth bringing in, we're going to follow down to the second level here.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，他们以前受雇吗？如果没有，向左走，如果是，向右走。结果是，在我的样本数据中，每个目前受雇的人实际上都得到了一个工作机会，所以我可以非常快速地说，如果你目前受雇，是的，你值得被带进来，我们将继续到第二个层级。
- en: So, how do you interpret this? The gini score is basically a measure of entropy
    that it's using at each step. Remember as we're going down the algorithm is trying
    to minimize the amount of entropy. And the samples are the remaining number of
    samples that haven't beensectioned off by a previous decision.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你如何解释这个呢？基尼分数基本上是在每一步使用的熵的度量。记住，当我们进行算法时，它试图最小化熵的量。样本是之前的决定没有分割的剩余样本数量。
- en: So say this person was employed. The way to read the right leaf node is the
    value column that tells you at this point we have 0 candidates that were no hires
    and 5 that were hires. So again, the way to interpret the first decision point
    is if Employed? was 1, I'm going to go to the right, meaning that they are currently
    employed, and this brings me to a world where everybody got a job offer. So, that
    means I should hire this person.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 所以说这个人以前是受雇的。阅读右叶节点的方法是值列，告诉你在这一点上，我们有0个候选人是不被雇佣的，有5个是被雇佣的。所以，解释第一个决定点的方法是，如果就业？是1，我会向右走，这意味着他们目前是受雇的，这让我进入了一个每个人都得到了工作机会的世界。所以，这意味着我应该雇佣这个人。
- en: Now let's say that this person doesn't currently have a job. The next thing
    I'm going to look at is, do they have an internship. If yes, then we're at a point
    where in our training data everybody got a job offer. So, at that point, we can
    say our entropy is now 0 (`gini=0.0000`), because everyone's the same, and they
    all got an offer at that point. However, you know if we keep going down(where
    the person has not done an internship),we'll be at a point where the entropy is
    0.32\. It's getting lower and lower, that's a good thing.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设这个人目前没有工作。我接下来要看的是，他们有实习吗。如果是，那么在我们的训练数据中，每个人都得到了一个工作机会。所以，在那一点上，我们可以说我们的熵现在是0（`gini=0.0000`），因为每个人都一样，在那一点上他们都得到了一个工作机会。然而，你知道，如果我们继续下去（在这个人没有做实习的情况下），我们将到达一个熵为0.32的点。它越来越低，这是一件好事。
- en: Next we're going to look at how much experience they have, do they have less
    than one year of experience? And, if the case is that they do have some experience
    and they've gotten this far they're a pretty good no hire decision. We end up
    at the point where we have zero entropy but, all three remaining samples in our
    training set were no hires. We have 3 no hires and 0 hires. But, if they do have
    less experience, then they're probably fresh out of college, they still might
    be worth looking at.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要看的是他们有多少经验，他们有不到一年的经验吗？如果情况是他们确实有一些经验，并且他们已经走到了这一步，他们是一个相当好的不被雇佣的决定。我们最终到达了熵为零的点，但是，在我们的训练集中剩下的三个样本都是不被雇佣的。我们有3个不被雇佣和0个被雇佣。但是，如果他们经验较少，那么他们可能刚刚从大学毕业，他们仍然值得一看。
- en: The final thing we're going to look at is whether or not they went to a Top-tier
    school, and if so, they end up being a good prediction for being a hire. If not,
    they end up being a no hire. We end up with one candidate that fell into that
    category that was a no hire and 0 that were a hire. Whereas, in the case candidates
    did go to a top tier school, we have 0 no hires and 1 hire.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要看的最后一件事是他们是否上了一所顶尖学校，如果是的话，他们最终会成为一个好的预测被雇佣。如果不是，他们最终会成为一个不被雇佣。我们最终有一个候选人属于这个类别，是一个不被雇佣，还有0个被雇佣。而在候选人确实上了一所顶尖学校的情况下，我们有0个不被雇佣和1个被雇佣。
- en: So, you can see we just keep going until we reach an entropy of 0, if at all
    possible, for every case.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你可以看到，我们一直走下去，直到我们达到熵为0，如果可能的话，对于每种情况。
- en: Ensemble learning – Using a random forest
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习 - 使用随机森林
- en: Now, let's say we want to use a random forest, you know, we're worried that
    we might be over fitting our training data. It's actually very easy to create
    a random forest classifier of multiple decision trees.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们想使用随机森林，你知道，我们担心我们可能过度拟合我们的训练数据。实际上，很容易创建一个多个决策树的随机森林分类器。
- en: 'So, to do that, we can use the same data that we created before. You just need
    your `*X*` and `*y*` vectors, that is the set of features and the column that
    you''re trying to predict on:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，为了做到这一点，我们可以使用之前创建的相同数据。你只需要你的`*X*`和`*y*`向量，也就是特征集和你试图预测的列：
- en: '[PRE22]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We make a random forest classifier, also available from scikit-learn, and pass
    it the number of trees we want in our forest. So, we made ten trees in our random
    forest in the code above. We then fit that to the model.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们制作了一个随机森林分类器，也可以从scikit-learn中获得，并传递给它我们想要在我们的森林中的树的数量。所以，在上面的代码中，我们的随机森林中有十棵树。然后我们将其适配到模型上。
- en: You don't have to walk through the trees by hand, and when you're dealing with
    a random forest you can't really do that anyway. So, instead we use the `predict()`
    function on the model, that is on the classifier that we made. We pass in a list
    of all the different features for a given candidate that we want to predict employment
    for.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 你不必手动遍历树，而且当你处理随机森林时，你也不能真正这样做。所以，我们在模型上使用`predict()`函数，也就是我们制作的分类器上。我们传入一个给定候选人的所有不同特征的列表，我们想要预测他们的就业情况。
- en: 'If you remember this maps to these columns: Years Experience, Employed?, Previous
    employers, Level of Education, Top-tier school, and Interned; interpreted as numerical
    values. We predict the employment of an employed 10-year veteran. We also predict
    the employment of an unemployed 10-year veteran. And, sure enough, we get a result:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得，这些映射到这些列：工作经验，就业情况，以前的雇主，教育水平，顶级学校和实习；被解释为数值。我们预测一个有工作的10年经验的老手的就业情况。我们还预测一个失业的10年经验的老手的就业情况。果然，我们得到了一个结果：
- en: '![](img/76760aeb-0024-4cec-8ca3-5cd9c64c2ebe.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76760aeb-0024-4cec-8ca3-5cd9c64c2ebe.jpg)'
- en: So, in this particular case, we ended up with a hire decision on both. But,
    what's interesting is there is a random component to that. You don't actually
    get the same result every time! More often than not, the unemployed person does
    not get a job offer, and if you keep running this you'll see that's usually the
    case. But, the random nature of bagging, of bootstrap aggregating each one of
    those trees, means you're not going to get the same result every time. So, maybe
    10 isn't quite enough trees. So, anyway, that's a good lesson to learn here!
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们最终都做出了雇佣决定。但有趣的是，这其中有一个随机因素。你实际上并不会每次都得到相同的结果！往往情况下，失业者并不会得到工作机会，如果你继续运行这个过程，你会发现通常情况下都是这样。但是，bagging的随机性，每棵树的自助聚合的随机性，意味着你不会每次都得到相同的结果。所以，也许10棵树还不够。总之，这是一个很好的教训！
- en: Activity
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动
- en: For an activity, if you want to go back and play with this, mess around with
    my input data. Go ahead and edit the code we've been exploring, and create an
    alternate universe where it's a topsy turvy world; for example, everyone that
    I gave a job offer to now doesn't get one and vice versa. See what that does to
    your decision tree. Just mess around with it and see what you can do and try to
    interpret the results.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个活动，如果你想回去玩一下，可以玩弄我的输入数据。随意编辑我们一直在探索的代码，并创建一个颠倒世界的替代宇宙；例如，我给工作机会的每个人现在都不再得到工作机会，反之亦然。看看这对你的决策树有什么影响。只是随意玩弄一下，看看你能做什么，并尝试解释结果。
- en: So, that's decision trees and random forests, one of the more interesting bits
    of machine learning, in my opinion. I always think it's pretty cool to just generate
    a flowchart out of thin air like that. So, hopefully you'll find that useful.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，那就是决策树和随机森林，我认为这是机器学习中更有趣的部分之一。我总是觉得能够从空中生成一个流程图非常酷。所以，希望你会觉得这很有用。
- en: Ensemble learning
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习
- en: When we talked about random forests, that was an example of ensemble learning,
    where we're actually combining multiple models together to come up with a better
    result than any single model could come up with. So, let's learn about that in
    a little bit more depth. Let's talk about ensemble learning a little bit more.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论随机森林时，那是集成学习的一个例子，我们实际上将多个模型组合在一起，以得到比任何单个模型更好的结果。所以，让我们更深入地了解一下。让我们更多地谈谈集成学习。
- en: 'So, remember random forests? We had a bunch of decision trees that were using
    different subsamples of the input data, and different sets of attributes that
    it would branch on, and they all voted on the final result when you were trying
    to classify something at the end. That''s an example of ensemble learning. Another
    example: when we were talking about k-means clustering, we had the idea of maybe
    using different k-means models with different initial random centroids, and letting
    them all vote on the final result as well. That is also an example of ensemble
    learning.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，还记得随机森林吗？我们有一堆使用输入数据的不同子样本和不同属性集的决策树，当你试图在最后对某些东西进行分类时，它们都对最终结果进行投票。这就是集成学习的一个例子。另一个例子：当我们谈论k均值聚类时，我们有一个想法，也许使用不同的k均值模型和不同的初始随机质心，让它们都对最终结果进行投票。这也是集成学习的一个例子。
- en: Basically, the idea is that you have more than one model, and they might be
    the same kind of model or it might be different kinds of models, but you run them
    all, on your set of training data, and they all vote on the final result for whatever
    it is you're trying to predict. And oftentimes, you'll find that this ensemble
    of different models produces better results than any single model could on its
    own.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这个想法是你有不止一个模型，它们可能是相同类型的模型，也可能是不同类型的模型，但你在你的训练数据集上运行它们所有，并且它们都对你试图预测的最终结果进行投票。往往情况下，你会发现这个不同模型的集合产生比任何单个模型本身更好的结果。
- en: 'A good example, from a few years ago, was the Netflix prize. Netflix ran a
    contest where they offered, I think it was a million dollars, to any researcher
    who could outperform their existing movie recommendation algorithm. The ones that
    won were ensemble approaches, where they actually ran multiple recommender algorithms
    at once and let them all vote on the final result. So, ensemble learning can be
    a very powerful, yet simple tool, for increasing the quality of your final results
    in machine learning. Let us now try to explore various types of ensemble learning:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前的一个很好的例子是Netflix奖。Netflix举办了一项比赛，他们提供了我认为是一百万美元的奖金，给任何研究人员，如果他们能够超越现有的电影推荐算法。获胜的方法是集成方法，他们实际上同时运行多个推荐算法，并让它们都对最终结果进行投票。因此，集成学习可以是一种非常强大而简单的工具，用于提高机器学习中最终结果的质量。现在让我们尝试探索各种类型的集成学习：
- en: '**Bootstrap aggregating or bagging:** Now, random forests use a technique called
    bagging, short for bootstrap aggregating. This means that we take random subsamples
    of our training data and feed them into different versions of the same model and
    let them all vote on the final result. If you remember, random forests took many
    different decision trees that use a different random sample of the training data
    to train on, and then they all came together in the end to vote on a final result.
    That''s bagging.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自举聚合或装袋：**现在，随机森林使用一种称为装袋的技术，简称自举聚合。这意味着我们从训练数据中随机抽取子样本，并将它们馈送到同一模型的不同版本中，让它们都对最终结果进行投票。如果你还记得，随机森林采用许多不同的决策树，这些决策树使用训练数据的不同随机样本进行训练，然后它们最终汇聚在一起对最终结果进行投票。这就是装袋。'
- en: '**Boosting:** Boosting is an alternate model, and the idea here is that you
    start with a model, but each subsequent model boosts the attributes that address
    the areas that were misclassified by the previous model. So, you run train/tests
    on a model, you figure out what are the attributes that it''s basically getting
    wrong, and then you boost those attributes in subsequent models - in hopes that
    those subsequent models will pay more attention to them, and get them right. So,
    that''s the general idea behind boosting. You run a model, figure out its weak
    points, amplify the focus on those weak points as you go, and keep building more
    and more models that keep refining that model, based on the weaknesses of the
    previous one.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升：**提升是一种替代模型，其思想是你从一个模型开始，但每个后续模型都增强了前一个模型误分类的属性。因此，你在一个模型上进行训练/测试，找出它基本上搞错了什么属性，然后在后续模型中增强这些属性
    - 希望后续模型会更加关注它们，并且把它们搞对。这就是提升的一般思想。你运行一个模型，找出它的弱点，随着你的进行，增强对这些弱点的关注，并且不断构建更多的模型，这些模型根据前一个模型的弱点进行改进。'
- en: '**Bucket of models:** Another technique, and this is what that Netflix prize-winner
    did, is called a bucket of models, where you might have entirely different models
    that try to predict something. Maybe I''m using k-means, a decision tree, and
    regression. I can run all three of those models together on a set of training
    data and let them all vote on the final classification result when I''m trying
    to predict something. And maybe that would be better than using any one of those
    models in isolation.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型桶：**另一种技术，这就是Netflix奖的获奖者所做的，称为模型桶，你可能有完全不同的模型来尝试预测某些东西。也许我正在使用k-means、决策树和回归。我可以同时在一组训练数据上运行这三个模型，并让它们都对在预测时的最终分类结果进行投票。也许这比单独使用其中任何一个模型要好。'
- en: '**Stacking:** Stacking has the same idea. So, you run multiple models on the
    data, combine the results together somehow. The subtle difference here between
    bucket of models and stacking, is that you pick the model that wins. So, you''d
    run train/test, you find the model that works best for your data, and you use
    that model. By contrast, stacking will combine the results of all those models
    together, to arrive at a final result.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**堆叠：**堆叠有相同的思想。因此，你在数据上运行多个模型，以某种方式将结果组合在一起。桶模型和堆叠之间的微妙差异在于你选择获胜的模型。因此，你运行训练/测试，找出最适合你的数据的模型，并使用该模型。相比之下，堆叠将所有这些模型的结果组合在一起，得出最终结果。'
- en: Now, there is a whole field of research on ensemble learning that tries to find
    the optimal ways of doing ensemble learning, and if you want to sound smart, usually
    that involves using the word Bayes a lot. So, there are some very advanced methods
    of doing ensemble learning but all of them have weak points, and I think this
    is yet another lesson in that we should always use the simplest technique that
    works well for us.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有一个关于集成学习的研究领域，试图找到最佳的集成学习方法，如果你想显得聪明，通常这涉及大量使用贝叶斯这个词。因此，有一些非常先进的集成学习方法，但它们都有弱点，我认为这又是一个教训，即我们应该始终使用对我们有效的最简单的技术。
- en: 'Now these are all very complicated techniques that I can''t really get into
    in the scope of this book, but at the end of the day, it''s hard to outperform
    just the simple techniques that we''ve already talked about. A few of the complex
    techniques are listed here:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些都是非常复杂的技术，在本书的范围内我无法深入讨论，但归根结底，很难超越我们已经讨论过的简单技术。以下列出了一些复杂的技术：
- en: '**Bayes optical classifier:** In theory, there''s something called the Bayes
    Optimal Classifier that will always be the best, but it''s impractical, because
    it''s computationally prohibitive to do it.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝叶斯最优分类器：**理论上，有一种称为贝叶斯最优分类器，它将始终是最好的，但这是不切实际的，因为计算上是禁止的。'
- en: '**Bayesian parameter averaging:** Many people have tried to do variations of
    the Bayes Optimal Classifier to make it more practical, like the Bayesian Parameter
    Averaging variation. But it''s still susceptible to overfitting and it''s often
    outperformed by bagging, which is the same idea behind random forests; you just
    resample the data multiple times, run different models, and let them all vote
    on the final result. Turns out that works just as well, and it''s a heck of a
    lot simpler!'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯参数平均化：许多人尝试对贝叶斯最优分类器进行变化，使其更实用，比如贝叶斯参数平均化变化。但它仍然容易过拟合，通常被随机森林背包法超越；你只需多次重新采样数据，运行不同模型，让它们投票决定最终结果。结果表明这样做同样有效，而且简单得多！
- en: '**Bayesian model combination:** Finally, there''s something called Bayesian
    Model Combination that tries to solve all the shortcomings of Bayes Optimal Classifier
    and Bayesian Parameter Averaging. But, at the end of the day, it doesn''t do much
    better than just cross validating against the combination of models.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯模型组合：最后，有一种称为贝叶斯模型组合的东西，试图解决贝叶斯最优分类器和贝叶斯参数平均化的所有缺点。但是，归根结底，它并没有比只是交叉验证组合模型做得更好。
- en: Again, these are very complex techniques that are very difficult to use. In
    practice, we're better off with the simpler ones that we've talked about in more
    detail. But, if you want to sound smart and use the word Bayes a lot it's good
    to be familiar with these techniques at least, and know what they are.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这些都是非常复杂的技术，非常难以使用。在实践中，我们最好使用我们更详细讨论过的更简单的技术。但是，如果你想显得聪明并经常使用贝叶斯这个词，熟悉这些技术并知道它们是什么是很好的。
- en: So, that's ensemble learning. Again, the takeaway is that the simple techniques,
    like bootstrap aggregating, or bagging, or boosting, or stacking, or bucket of
    models, are usually the right choices. There are some much fancier techniques
    out there but they're largely theoretical. But, at least you know about them now.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是集成学习。再次强调的是，像自助聚合、背包法、提升法、堆叠法或模型桶之类的简单技术通常是正确的选择。还有一些更花哨的技术，但它们在很大程度上是理论性的。但是，至少现在你知道它们了。
- en: It's always a good idea to try ensemble learning out. It's been proven time
    and time again that it will produce better results than any single model, so definitely
    consider it!
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试集成学习总是一个好主意。一次又一次地证明，它将产生比任何单一模型更好的结果，因此一定要考虑它！
- en: Support vector machine overview
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机概述
- en: Finally, we're going to talk about **support vector machines** (**SVM**), which
    is a very advanced way of clustering or classifying higher dimensional data.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将讨论支持向量机（SVM），这是一种非常先进的方法，用于聚类或分类高维数据。
- en: So, what if you have multiple features that you want to predict from? SVM can
    be a very powerful tool for doing that, and the results can be scarily good! It's
    very complicated under the hood, but the important things are understanding when
    to use it, and how it works at a higher level. So, let's cover SVM now.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果你有多个要预测的特征呢？支持向量机可能是一个非常强大的工具，结果可能非常好！它在内部非常复杂，但重要的是要理解何时使用它，以及它在更高层次上是如何工作的。所以，现在让我们来讨论支持向量机。
- en: Support vector machines is a fancy name for what actually is a fancy concept.
    But fortunately, it's pretty easy to use. The important thing is knowing what
    it does, and what it's good for. So, support vector machines works well for classifying
    higher-dimensional data, and by that I mean lots of different features. So, it's
    easy to use something like k-means clustering, to cluster data that has two dimensions,
    you know, maybe age on one axis and income on another. But, what if I have many,
    many different features that I'm trying to predict from. Well, support vector
    machines might be a good way of doing that.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机是一个花哨的名字，实际上是一个花哨的概念。但幸运的是，它非常容易使用。重要的是要知道它的作用和用途。因此，支持向量机对于分类高维数据效果很好，我指的是许多不同的特征。因此，使用k均值聚类之类的东西很容易对具有两个维度的数据进行聚类，也许一个维度是年龄，另一个维度是收入。但是，如果我有许多不同的特征要预测，那么支持向量机可能是一个不错的选择。
- en: Support vector machines finds higher-dimensional support vectors across which
    to divide the data (mathematically, these support vectors define hyperplanes).
    That is, mathematically, what support vector machines can do is find higher dimensional
    support vectors (that's where it gets its name from) that define the higher-dimensional
    planes that split the data into different clusters.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机找到高维支持向量，用于划分数据（数学上，这些支持向量定义超平面）。也就是说，数学上，支持向量机可以找到高维支持向量（这就是它得名的地方），这些支持向量定义了将数据分成不同簇的高维平面。
- en: Obviously the math gets pretty weird pretty quickly with all this. Fortunately,
    the `scikit-learn` package will do it all for you, without you having to actually
    get into it. Under the hood, you need to understand though that it uses something
    called the kernel trick to actually find those support vectors or hyperplanes
    that might not be apparent in lower dimensions. There are different kernels you
    can use, to do this in different ways. The main point is that SVM's are a good
    choice if you have higher- dimensional data with lots of different features, and
    there are different kernels you can use that have varying computational costs
    and might be better fits for the problem at hand.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，所有这些都很快变得非常奇怪。幸运的是，scikit-learn软件包将为您完成所有这些，而无需您实际参与其中。在内部，您需要理解它使用一种称为核技巧的东西来实际找到那些在较低维度中可能不明显的支持向量或超平面。您可以使用不同的核来以不同的方式执行此操作。主要问题是，如果您有具有许多不同特征的高维数据，支持向量机是一个不错的选择，您可以使用不同的核，其计算成本不同，并且可能更适合手头的问题。
- en: The important point is that SVMs employ some advanced mathematical trickery
    to cluster data, and it can handle data sets with lots of features. It's also
    fairly expensive - the "kernel trick" is the only thing that makes it possible.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的一点是SVM使用一些高级数学技巧来对数据进行聚类，并且可以处理具有许多特征的数据集。它也相当昂贵 - "核技巧"是唯一使其可能的东西。
- en: I want to point out that SVM is a supervised learning technique. So, we're actually
    going to train it on a set of training data, and we can use that to make predictions
    for future unseen data or test data. It's a little bit different than k-means
    clustering and that k-means was completely unsupervised; with a support vector
    machine, by contrast, it is training based on actual training data where you have
    the answer of the correct classification for some set of data that it can learn
    from. So, SVM's are useful for classification and clustering, if you will - but
    it's a supervised technique!
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我想指出SVM是一种监督学习技术。因此，我们实际上要在一组训练数据上对其进行训练，并且我们可以使用它来对未来未见数据或测试数据进行预测。这与k均值聚类有点不同，k均值是完全无监督的；相比之下，支持向量机是基于实际训练数据进行训练的，其中你有一些数据集的正确分类答案可以学习。因此，如果你愿意的话，SVM对于分类和聚类是有用的
    - 但这是一种监督技术！
- en: One example that you often see with SVMs is using something called support vector
    classification. The typical example uses the Iris dataset which is one of the
    sample datasets that comes with scikit-learn. This set is a classification of
    different flowers, different observations of different Iris flowers and their
    species. The idea is to classify these using information about the length and
    width of the petal on each flower, and the length and width of the sepal of each
    flower. (The sepal, apparently, is a little support structure underneath the petal.
    I didn't know that until now either.) You have four dimensions of attributes there;
    you have the length and width of the petal, and the length and the width of the
    sepal. You can use that to predict the species of an Iris given that information.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: SVM经常使用的一个例子是使用称为支持向量分类的东西。典型的例子使用了Iris数据集，这是scikit-learn附带的样本数据集之一。这个数据集是对不同的鸢尾花进行分类，不同的鸢尾花的不同观察和它们的种类。这个想法是使用关于每朵花瓣的长度和宽度以及每朵花萼的长度和宽度的信息来对这些进行分类。
    （萼片显然是花瓣下面的一个小支撑结构。我直到现在也不知道。）你有四个属性的维度；你有花瓣的长度和宽度，以及萼片的长度和宽度。你可以使用这些信息来预测给定信息的鸢尾花的种类。
- en: 'Here''s an example of doing that with SVC: basically, we have sepal width and
    sepal length projected down to two dimensions so we can actually visualize it:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用SVC进行的一个例子：基本上，我们将萼片宽度和萼片长度投影到二维，这样我们就可以实际可视化它：
- en: '![](img/11af3028-7b21-40ac-94ae-b9db94d97655.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11af3028-7b21-40ac-94ae-b9db94d97655.jpg)'
- en: With different kernels you might get different results. SVC with a linear kernel
    will produce something very much as you see in the preceding image. You can use
    polynomial kernels or fancier kernels that might project down to curves in two
    dimensions as shown in the image. You can do some pretty fancy classification
    this way.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的核心可能会得到不同的结果。具有线性核心的SVC将产生与前面图像中看到的非常相似的东西。你可以使用多项式核心或更复杂的核心，可能会在图像中显示为二维曲线。你可以通过这种方式进行一些相当花哨的分类。
- en: These have increasing computational costs, and they can produce more complex
    relationships. But again, it's a case where too much complexity can yield misleading
    results, so you need to be careful and actually use train/test when appropriate.
    Since we are doing supervised learning, you can actually do train/test and find
    the right model that works, or maybe use an ensemble approach.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都会增加计算成本，并且可以产生更复杂的关系。但同样，这是一种情况，过于复杂可能会产生误导性的结果，因此你需要小心，并在适当的时候使用训练/测试。由于我们正在进行监督学习，你实际上可以进行训练/测试，并找到适合的模型，或者使用集成方法。
- en: You need to arrive at the right kernel for the task at hand. For things like
    polynomial SVC, what's the right degree polynomial to use? Even things like linear
    SVC will have different parameters associated with them that you might need to
    optimize for. This will make more sense with a real example, so let's dive into
    some actual Python code and see how it works!
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要找到适合当前任务的正确核心。对于多项式SVC之类的东西，使用什么程度的多项式才是正确的？即使是线性SVC也会有与之相关的不同参数，你可能需要进行优化。这将在一个真实的例子中更有意义，所以让我们深入一些实际的Python代码，看看它是如何工作的！
- en: Using SVM to cluster people by using scikit-learn
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SVM通过scikit-learn对人进行聚类
- en: Let's try out some support vector machines here. Fortunately, it's a lot easier
    to use than it is to understand. We're going to go back to the same example I
    used for k-means clustering, where I'm going to create some fabricated cluster
    data about ages and incomes of a hundred random people.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这里尝试一些支持向量机。幸运的是，使用起来比理解起来要容易得多。我们将回到我用于k均值聚类的相同示例，我将创建一些关于一百个随机人的年龄和收入的虚构集群数据。
- en: 'If you want to go back to the k-means clustering section, you can learn more
    about kind of the idea behind this code that generates the fake data. And if you''re
    ready, please consider the following code:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想回到k均值聚类部分，你可以了解更多关于生成虚假数据的代码背后的想法。如果你准备好了，请考虑以下代码：
- en: '[PRE23]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Please note that because we're using supervised learning here, we not only need
    the feature data again, but we also need the actual answers for our training dataset.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，因为我们在这里使用的是监督学习，我们不仅需要再次使用特征数据，还需要我们训练数据集的实际答案。
- en: What the `createClusteredData()` function does here, is to create a bunch of
    random data for people that are clustered around `k` points, based on age and
    income, and it returns two arrays. The first array is the feature array, that
    we're calling `X`, and then we have the array of the thing we're trying to predict
    for, which we're calling `y`. A lot of times in scikit-learn when you're creating
    a model that you can predict from, those are the two inputs that it will take,
    a list of feature vectors, and the thing that you're trying to predict, that it
    can learn from. So, we'll go ahead and run that.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`createClusteredData（）`函数的作用是根据年龄和收入创建一堆围绕`k`点聚集的随机数据，并返回两个数组。第一个数组是我们称之为`X`的特征数组，然后我们有我们试图预测的东西的数组，我们称之为`y`。在scikit-learn中，当你创建一个可以进行预测的模型时，这些是它将接受的两个输入，特征向量的列表和你试图预测的东西，它可以从中学习。所以，我们将继续运行。
- en: 'So now we''re going to use the `createClusteredData()` function to create 100
    random people with 5 different clusters. We will just create a scatter plot to
    illustrate those, and see where they land up:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们将使用`createClusteredData（）`函数创建100个随机人，分为5个不同的集群。我们将创建一个散点图来说明这些人的情况，并看看他们最终落在哪里：
- en: '[PRE24]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The following graph shows our data that we're playing with. Every time you run
    this you're going to get a different set of clusters. So, you know, I didn't actually
    use a random seed... to make life interesting.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了我们正在处理的数据。每次运行这个程序，你都会得到一组不同的集群。所以，你知道，我实际上没有使用随机种子...让生活变得有趣。
- en: 'A couple of new things here--I''m using the `figsize` parameter on `plt.figure()`
    to actually make a larger plot. So, if you ever need to adjust the size in `matplotlib`,
    that''s how you do it. I''m using that same trick of using the color as the classification
    number that I end up with. So the number of the cluster that I started with is
    being plotted as the color of these data points. You can see, it''s a pretty challenging
    problem, there''s definitely some intermingling of clusters here:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个新东西 - 我在`plt.figure（）`上使用了`figsize`参数来实际上制作一个更大的图。所以，如果你需要在`matplotlib`中调整大小，就是这样做的。我使用了相同的技巧，将颜色作为我最终得到的分类号。所以我开始的集群号被绘制为这些数据点的颜色。你可以看到，这是一个相当具有挑战性的问题，这里肯定有一些集群的交错：
- en: '![](img/18ecc82e-4a00-47b9-b93e-7b7de259f4ab.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18ecc82e-4a00-47b9-b93e-7b7de259f4ab.jpg)'
- en: Now we can use linear SVC (SVC is a form of SVM), to actually partition that
    into clusters. We're going to use SVM with a linear kernel, and with a C value
    of `1.0`. C is just an error penalty term that you can adjust; it's `1` by default.
    Normally, you won't want to mess with that, but if you're doing some sort of convergence
    on the right model using ensemble learning or train/test, that's one of the things
    you can play with. Then, we will fit that model to our feature data, and the actual
    classifications that we have for our training dataset.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用线性SVC（SVC是SVM的一种形式）来将其分成集群。我们将使用具有线性核的SVM，并且C值为`1.0`。C只是一个可以调整的错误惩罚项；默认为`1`。通常情况下，你不会想去改变它，但如果你正在使用集成学习或训练/测试对正确模型进行一些收敛，那就是你可以玩耍的东西之一。然后，我们将将该模型拟合到我们的特征数据和我们的训练数据集的实际分类。
- en: '[PRE25]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: So, let's go ahead and run that. I don't want to get too much into how we're
    actually going to visualize the results here, just take it on faith that `plotPredictions()`
    is a function that can plot the classification ranges and SVC.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们继续运行。我不想过多地讨论我们实际上将如何可视化结果，只是相信`plotPredictions（）`是一个可以绘制分类范围和SVC的函数。
- en: 'It helps us visualize where different classifications come out. Basically,
    it''s creating a mesh across the entire grid, and it will plot different classifications
    from the SVC models as different colors on that grid, and then we''re going to
    plot our original data on top of that:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 它帮助我们可视化不同分类的位置。基本上，它在整个网格上创建一个网格，并且会将来自SVC模型的不同分类作为网格上的不同颜色进行绘制，然后我们将在其上绘制我们的原始数据：
- en: '[PRE26]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'So, let''s see how that works out. SVC is computationally expensive, so it
    takes a long time to run:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们看看它是如何工作的。SVC的计算成本很高，所以运行时间很长：
- en: '![](img/a1342ff6-9d4a-4546-a76a-ed1092efdbb0.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1342ff6-9d4a-4546-a76a-ed1092efdbb0.jpg)'
- en: You can see here that it did its best. Given that it had to draw straight lines,
    and polygonal shapes, it did a decent job of fitting to the data that we had.
    So, you know, it did miss a few - but by and large, the results are pretty good.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到它尽了最大努力。考虑到它必须绘制直线和多边形形状，它做了不错的工作来适应我们的数据。所以，你知道，它错过了一些 - 但总体上，结果还是相当不错的。
- en: 'SVC is actually a very powerful technique; it''s real strength is in higher
    dimensional feature data. Go ahead and play with it. By the way if you want to
    not just visualize the results, you can use the `predict()` function on the SVC
    model, just like on pretty much any model in scikit-learn, to pass in a feature
    array that you''re interested in. If I want to predict the classification for
    someone making $200,000 a year who was 40 years old, I would use the following
    code:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: SVC实际上是一种非常强大的技术；它的真正优势在于更高维度的特征数据。继续玩耍吧。顺便说一句，如果你不仅想可视化结果，你可以像在scikit-learn中的几乎任何模型一样使用`predict（）`函数在SVC模型上，传入你感兴趣的特征数组。如果我想预测一个年收入为20万美元，年龄为40岁的人的分类，我将使用以下代码：
- en: '[PRE27]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This would put that person in, in our case, cluster number 1:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把这个人放在我们的情况下，集群编号1中：
- en: '![](img/8a7954d2-6d7d-482b-a3dc-05b75deeab61.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a7954d2-6d7d-482b-a3dc-05b75deeab61.jpg)'
- en: 'If I had a someone making $50,000 here who was 65, I would use the following
    code:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我在这里有一个年收入为50,000美元，年龄为65岁的人，我将使用以下代码：
- en: '[PRE28]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This is what your output should now look like:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你的输出现在应该看起来的样子：
- en: '![](img/30e1d81a-b05c-48a1-9f17-ac864d6e9cd6.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30e1d81a-b05c-48a1-9f17-ac864d6e9cd6.jpg)'
- en: That person would end up in cluster number 2, whatever that represents in this
    example. So, go ahead and play with it.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这个人最终会进入集群编号2，无论在这个例子中代表什么。所以，继续玩耍吧。
- en: Activity
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动
- en: Now, linear is just one of many kernels that you can use, like I said there
    are many different kernels you can use. One of them is a polynomial model, so
    you might want to play with that. Please do go ahead and look up the documentation.
    It's good practice for you to looking at the docs. If you're going to be using
    scikit-learn in any sort of depth, there's a lot of different capabilities and
    options that you have available to you. So, go look up scikit-learn online, find
    out what the other kernels are for the SVC method, and try them out, see if you
    actually get better results or not.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，线性只是你可以使用的许多内核之一，就像我说的，你可以使用许多不同的内核。其中之一是多项式模型，所以你可能想试试。请继续查阅文档。查看文档对你来说是一个很好的练习。如果你要深入使用scikit-learn，你有很多不同的功能和选项可供选择。所以，去在线查找scikit-learn，找出SVC方法的其他内核是什么，然后尝试它们，看看你是否真的得到了更好的结果。
- en: This is a little exercise, not just in playing with SVM and different kinds
    of SVC, but also in familiarizing yourself with how to learn more on your own
    about SVC. And, honestly, a very important trait of any data scientist or engineer
    is going to be the ability to go and look up information yourself when you don't
    know the answers.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅是一个关于玩SVM和不同类型的SVC的练习，还是一个让你熟悉如何自己学习更多关于SVC的内容的练习。而且，老实说，任何数据科学家或工程师的一个非常重要的特质将是在你不知道答案时自己去查找信息的能力。
- en: So, you know, I'm not being lazy by not telling you what those other kernels
    are, I want you to get used to the idea of having to look this stuff up on your
    own, because if you have to ask someone else about these things all the time you're
    going to get really annoying, really fast in a workplace. So, go look that up,
    play around it, see what you come up with.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你知道，我没有懒惰，没有告诉你那些其他内核是什么，我希望你习惯于自己查找这些东西的想法，因为如果你总是不得不问别人这些事情，你在工作中会变得非常烦人，非常快。所以，去查一下，玩一下，看看你能得到什么。
- en: So, that's SVM/SVC, a very high power technique that you can use for classifying
    data, in supervised learning. Now you know how it works and how to use it, so
    keep that in your bag of tricks!
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是SVM/SVC，一种非常强大的技术，你可以用它来对数据进行分类，在监督学习中。现在你知道它是如何工作的，以及如何使用它，所以记住它吧！
- en: Summary
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw some interesting machine learning techniques. We covered
    one of the fundamental concepts behind machine learning called train/test. We
    saw how to use train/test to try to find the right degree polynomial to fit a
    given set of data. We then analyzed the difference between supervised and unsupervised
    machine learning.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了一些有趣的机器学习技术。我们涵盖了机器学习背后的一个基本概念，称为训练/测试。我们看到如何使用训练/测试来尝试找到合适的多项式度数来适应给定的数据集。然后我们分析了监督学习和无监督学习之间的区别。
- en: We saw how to implement a spam classifier and enable it to determine whether
    an email is spam or not using the Naive Bayes technique. We talked about k-means
    clustering, an unsupervised learning technique, which helps group data into clusters.
    We also looked at an example using scikit-learn which clustered people based on
    their income and age.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了如何实现一个垃圾邮件分类器，并使其能够使用朴素贝叶斯技术确定一封电子邮件是否是垃圾邮件。我们讨论了k均值聚类，一种无监督学习技术，它有助于将数据分组成簇。我们还看了一个使用scikit-learn的例子，根据他们的收入和年龄对人进行了聚类。
- en: We then went on to look at the concept of entropy and how to measure it. We
    walked through the concept of decision trees and how, given a set of training
    data, you can actually get Python to generate a flowchart for you to actually
    make a decision. We also built a system that automatically filters out resumes
    based on the information in them and predicts the hiring decision of a person.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续讨论了熵的概念以及如何衡量它。我们深入讨论了决策树的概念，以及如何在给定一组训练数据的情况下，实际上可以让Python为您生成一个流程图来做出决策。我们还建立了一个系统，根据简历中的信息自动过滤简历，并预测一个人的招聘决定。
- en: We learned along the way the concept of ensemble learning, and we concluded
    by talking about support vector machines, which is a very advanced way of clustering
    or classifying higher dimensional data. We then moved on to use SVM to cluster
    people using scikit-learn. In the next chapter, we'll talk about recommender systems.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们沿途学到了集成学习的概念，并最后谈到了支持向量机，这是一种非常先进的聚类或分类高维数据的方法。然后我们继续使用SVM来使用scikit-learn对人进行聚类。在下一章中，我们将讨论推荐系统。
