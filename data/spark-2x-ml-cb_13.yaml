- en: Spark Streaming and Machine Learning Library
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming和机器学习库
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Structured streaming for near real-time machine learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流式处理用于近实时机器学习
- en: Streaming DataFrames for real-time machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时机器学习的流式数据框架
- en: Streaming Datasets for real-time machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时机器学习的流式数据集
- en: Streaming data and debugging with queueStream
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用queueStream进行流式数据和调试
- en: Downloading and understanding the famous Iris data for unsupervised classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载和理解著名的鸢尾花数据，用于无监督分类
- en: Streaming KMeans for a real-time online classifier
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式KMeans用于实时在线分类器
- en: Downloading wine quality data for streaming regression
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载葡萄酒质量数据进行流式回归
- en: Streaming linear regression for a real-time regression
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式线性回归用于实时回归
- en: Downloading Pima Diabetes data for supervised classification
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载皮马糖尿病数据进行监督分类
- en: Streaming logistic regression for an on-line classifier
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式逻辑回归用于在线分类器
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Spark streaming is an evolving journey toward unification and structuring of
    the APIs in order to address the concerns of batch versus stream. Spark streaming
    has been available since Spark 1.3 with **Discretized Stream** (**DStream**).
    The new direction is to abstract the underlying framework using an unbounded table
    model in which the users can query the table using SQL or functional programming
    and write the output to another output table in multiple modes (complete, delta,
    and append output). The Spark SQL Catalyst optimizer and Tungsten (off-heap memory
    manager) are now an intrinsic part of the Spark streaming, which leads to a much
    efficient execution.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Spark流式处理是朝着统一和结构化API的发展之路，以解决批处理与流处理的问题。自Spark 1.3以来，Spark流式处理一直可用，使用离散流（DStream）。新的方向是使用无界表模型来抽象底层框架，用户可以使用SQL或函数式编程查询表，并以多种模式（完整、增量和追加输出）将输出写入另一个输出表。Spark
    SQL Catalyst优化器和Tungsten（堆外内存管理器）现在是Spark流式处理的固有部分，这导致了更高效的执行。
- en: In this chapter, we not only cover the streaming facilities available in Spark's
    machine library out of the box, but also provide four introductory recipes that
    we found useful as we journeyed toward our better understanding of Spark 2.0.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不仅涵盖了Spark机器库中提供的流式设施，还提供了四个介绍性的配方，这些配方在我们对Spark 2.0的更好理解之旅中非常有用。
- en: 'The following figure depicts what is covered in this chapter:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了本章涵盖的内容：
- en: '![](img/00284.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00284.jpeg)'
- en: Spark 2.0+ builds on the success of the previous generation by abstracting away
    some of the framework's inner workings and presenting it to the developer without
    worrying about *end-to-end write only once* semantics. It is a journey from DStream
    based on RDD to a structured streaming paradigm in which your world of streaming
    can be viewed as infinite tables with multiple modes for output.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0+通过抽象掉一些框架的内部工作原理，并将其呈现给开发人员，而不必担心端到端的一次写入语义，来构建上一代的成功。这是从基于RDD的DStream到结构化流处理范式的一次旅程，在这个范式中，您的流处理世界可以被视为具有多种输出模式的无限表。
- en: The state management has evolved from `updateStateByKey` (Spark 1.3 to Spark
    1.5) to `mapWithState` (Spark 1.6+) to the third generation state management with
    structured streaming (Spark 2.0+).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 状态管理已经从`updateStateByKey`（Spark 1.3到Spark 1.5）发展到`mapWithState`（Spark 1.6+），再到结构化流处理（Spark
    2.0+）的第三代状态管理。
- en: A modern ML streaming system is a complex continuous application that needs
    to not only combine various ML steps into a pipeline, but also interact with other
    subsystems to provide a real-life useful, end-to-end information system.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现代ML流式系统是一个复杂的连续应用程序，不仅需要将各种ML步骤组合成管道，还需要与其他子系统交互，以提供实用的端到端信息系统。
- en: 'As we were wrapping up the book, Databricks, the company that empowers the
    Spark community, made the following announcement at Spark Summit West 2017 regarding
    the future direction of Spark streaming (not prod release yet):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们完成这本书时，Databricks，这家支持Spark社区的公司，在Spark Summit West 2017上宣布了关于Spark流处理未来方向的声明（尚未发布）：
- en: '*"Today, we are excited to propose a new extension,* *continuous processing**,
    that also eliminates micro-batches from execution. As we demonstrated at Spark
    Summit this morning, this new execution mode lets users achieve sub-millisecond
    end-to-end latency for many important workloads - with no change to their Spark
    application."*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: “今天，我们很高兴提出一个新的扩展，连续处理，它还消除了执行中的微批处理。正如我们今天早上在Spark Summit上展示的那样，这种新的执行模式让用户在许多重要的工作负载中实现亚毫秒的端到端延迟
    - 而不需要更改他们的Spark应用程序。”
- en: 'Source: [https://databricks.com/blog/2017/06/06/simple-super-fast-streaming-engine-apache-spark.html](https://databricks.com/blog/2017/06/06/simple-super-fast-streaming-engine-apache-spark.html)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://databricks.com/blog/2017/06/06/simple-super-fast-streaming-engine-apache-spark.html](https://databricks.com/blog/2017/06/06/simple-super-fast-streaming-engine-apache-spark.html)
- en: 'The following figure depicts a minimum viable streaming system that is the
    foundation of most streaming systems (over simplified for presentation):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了大多数流式系统的最小可行流式系统（为了演示而过于简化）：
- en: '![](img/00285.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00285.jpeg)'
- en: As seen in the preceding figure, any real-life system must interact with batch
    (for example, offline learning of the model parameters) while the faster subsystem
    concentrates on real-time response to external events (that is, online learning).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，任何现实生活中的系统都必须与批处理（例如，模型参数的离线学习）进行交互，而更快的子系统则集中于对外部事件的实时响应（即在线学习）。
- en: Spark's structured streaming full integration with ML library is on the horizon,
    but meanwhile we can create and use streaming DataFrames and streaming Datasets
    to compensate, as will be seen in some of the following recipes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的结构化流处理与ML库的完全集成即将到来，但与此同时，我们可以创建和使用流式数据框架和流式数据集来进行补偿，这将在接下来的一些配方中看到。
- en: 'The new structured streaming has several advantages, such as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 新的结构化流式处理具有多个优势，例如：
- en: Unification of Batch and Stream APIs (no need to translate)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理和流处理API的统一（无需翻译）
- en: Functional programming with more concise expressive language
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更简洁的表达式语言的函数式编程
- en: Fault-tolerant state management (third generation)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容错状态管理（第三代）
- en: 'Significantly simplified programming model:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大大简化的编程模型：
- en: Trigger
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 触发
- en: Input
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入
- en: Query
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询
- en: Result
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果
- en: Output
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出
- en: Data stream as a unbounded table
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据流作为无界表
- en: 'The following figure depicts the basic concepts beyond a data stream being
    modeled as an infinite unbounded table:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了将数据流建模为无限无界表的基本概念：
- en: '![](img/00286.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00286.jpeg)'
- en: The pre-Spark 2.0 paradigm advanced the DStream construct, which modeled the
    stream as a set of discrete data structures (RDDs) that was very difficult to
    deal with when we had late arrivals. The inherent late arrival problem made it
    difficult to build systems that had a real-time chargeback model (very prominent
    in the cloud) due to the uncertainty around the actual charges.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0之前的范式中，DStream构造推进了流作为一组离散数据结构（RDDs）的模型，当我们有延迟到达时，这是非常难处理的。固有的延迟到达问题使得难以构建具有实时回溯模型的系统（在云中非常突出），因为实际费用的不确定性。
- en: 'The following figure depicts the DStream model in a visual way so it can be
    compared accordingly:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表以可视化方式描述了DStream模型，以便进行相应比较：
- en: '![](img/00287.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00287.jpeg)'
- en: In comparison, by using the new model, there are fewer concepts that a developer
    needs to worry about and there is no need to translate the code from a batch model
    (often ETL like code) to a real-time stream model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，使用新模型，开发人员需要担心的概念更少，也不需要将代码从批处理模型（通常是ETL类似的代码）转换为实时流模型。
- en: Currently, due to the timeline and legacy, one must know both models (DStream
    and structured streaming) for a while before all pre-Spark 2.0 code is replaced.
    We found the new structured streaming model particularly simple compared to DStream
    and have tried to demonstrate and highlight the differences in the four introductory
    recipes covered in this chapter.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，由于时间线和遗留问题，必须在所有Spark 2.0之前的代码被替换之前一段时间内了解两种模型（DStream和结构化流）。我们发现新的结构化流模型特别简单，与DStream相比，并尝试在本章涵盖的四个入门配方中展示和突出显示差异。
- en: Structured streaming for near real-time machine learning
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流用于近实时机器学习
- en: In this recipe, we explore the new structured streaming paradigm introduced
    in Spark 2.0\. We explore real-time streaming using sockets and structured streaming
    API to vote and tabulate the votes accordingly.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们探索了Spark 2.0引入的新的结构化流范式。我们使用套接字和结构化流API进行实时流处理，以进行投票和统计投票。
- en: We also explore the newly introduced subsystem by simulating a stream of randomly
    generated votes to pick the most unpopular comic book villain.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过模拟随机生成的投票流来探索新引入的子系统，以选择最不受欢迎的漫画恶棍。
- en: There are two distinct programs (`VoteCountStream.scala` and `CountStreamproducer.scala`)
    that make up this recipe.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方由两个不同的程序（`VoteCountStream.scala`和`CountStreamproducer.scala`）组成。
- en: How to do it...
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the necessary packages for the Spark context to get access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以便Spark上下文可以访问集群和`log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define a Scala class to generate voting data onto a client socket:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala类来生成投票数据到客户端套接字：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define an array containing literal string values of people to vote for:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个包含人们投票的文字字符串值的数组：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we will override the `Threads` class `run` method to randomly simulate
    a vote for a particular villain:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将覆盖`Threads`类的`run`方法，随机模拟对特定恶棍的投票：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we define a Scala singleton object to accept connections on a defined
    port `9999` and generate voting data:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个Scala单例对象，以接受在定义的端口`9999`上的连接并生成投票数据：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Don't forget to start up the data generation server, so our streaming example
    can process the streaming vote data.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要忘记启动数据生成服务器，这样我们的流式示例就可以处理流式投票数据。
- en: 'Set output level to `ERROR` to reduce Spark''s output:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的输出：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create a `SparkSession` yielding access to the Spark cluster and underlying
    session object attributes such as the `SparkContext` and `SparkSQLContext`:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`SparkSession`，以访问Spark集群和底层会话对象属性，如`SparkContext`和`SparkSQLContext`：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Import spark implicits, therefore adding in behavior with only an import:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入spark implicits，因此只需导入行为：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create a streaming DataFrame by connecting to localhost on port `9999`, which
    utilizes a Spark socket source as the source of streaming data:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过连接到本地端口`9999`创建一个流DataFrame，该端口利用Spark套接字源作为流数据的来源：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this step, we group streaming data by villain name and count to simulate
    user votes streaming in real time:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们通过恶棍名称和计数对流数据进行分组，以模拟用户实时投票：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we define a streaming query to trigger every 10 seconds, dump the whole
    result set into the console, and invoke it by calling the `start()` method:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们定义一个流查询，每10秒触发一次，将整个结果集转储到控制台，并通过调用`start()`方法来调用它：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The first output batch is displayed here as batch `0`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个输出批次显示在这里作为批次`0`：
- en: '![](img/00288.gif)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00288.gif)'
- en: 'An additional batch result is displayed here:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的批处理结果显示在这里：
- en: '![](img/00289.gif)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00289.gif)'
- en: 'Finally, wait for termination of the streaming query or stop the process using
    the `SparkSession` API:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，等待流查询的终止或使用`SparkSession` API停止进程：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How it works...
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In this recipe, we created a simple data generation server to simulate a stream
    of voting data and then counted the vote. The following figure provides a high-level
    depiction of this concept:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们创建了一个简单的数据生成服务器来模拟投票数据的流，然后计算了投票。下图提供了这个概念的高级描述：
- en: '![](img/00290.jpeg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00290.jpeg)'
- en: First, we began by executing the data generation server. Second, we defined
    a socket data source, which allows us to connect to the data generation server.
    Third, we constructed a simple Spark expression to group by villain (that is,
    bad superheroes) and count all currently received votes. Finally, we configured
    a threshold trigger of 10 seconds to execute our streaming query, which dumps
    the accumulated results onto the console.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过执行数据生成服务器来开始。其次，我们定义了一个套接字数据源，允许我们连接到数据生成服务器。第三，我们构建了一个简单的Spark表达式，按反派（即坏超级英雄）分组，并计算当前收到的所有选票。最后，我们配置了一个10秒的阈值触发器来执行我们的流查询，将累积的结果转储到控制台上。
- en: 'There are two short programs involved in this recipe:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方涉及两个简短的程序：
- en: '`CountStreamproducer.scala`:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CountStreamproducer.scala`:'
- en: The producer - data generation server
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产者-数据生成服务器
- en: Simulates the voting for itself and broadcasts it
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟为自己投票并广播
- en: '`VoteCountStream.scala`:'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VoteCountStream.scala`:'
- en: The consumer - consumes and aggregates/tabulates the data
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者-消费和聚合/制表数据
- en: Receives and count votes for our villain superhero
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接收并计算我们的反派超级英雄的选票
- en: There's more...
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The topic of how to program using Spark streaming and structured streaming in
    Spark is out of scope for this book, but we felt it is necessary to share some
    programs to introduce the concepts before drilling down into ML streaming offering
    for Spark.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用Spark流处理和结构化流处理编程的主题超出了本书的范围，但我们认为有必要在深入研究Spark的ML流处理提供之前分享一些程序来介绍这些概念。
- en: 'For a solid introduction to streaming, please consult the following documentation
    on Spark:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解流处理的基本知识，请参阅以下关于Spark的文档：
- en: Information of Spark 2.0+ structured streaming is available at [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0+结构化流的信息可在[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes)找到
- en: Information of Spark 1.6 streaming is available at [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 1.6流处理的信息可在[https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)找到
- en: See also
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: Documentation for structured streaming is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.package](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.package)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流处理的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.package](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.package)找到
- en: Documentation for DStream (pre-Spark 2.0) is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream)
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DStream（Spark 2.0之前）的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream)找到
- en: Documentation for `DataStreamReader` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataStreamReader`的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)找到'
- en: Documentation for `DataStreamWriter` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataStreamWriter`的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)找到'
- en: Documentation for `StreamingQuery` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamingQuery`的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery)找到'
- en: Streaming DataFrames for real-time machine learning
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于实时机器学习的流DataFrame
- en: In this recipe, we explore the concept of a streaming DataFrame. We create a
    DataFrame consisting of the name and age of individuals, which we will be streaming
    across a wire. A streaming DataFrame is a popular technique to use with Spark
    ML since we do not have a full integration between Spark structured ML at the
    time of writing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们探讨了流DataFrame的概念。我们创建了一个由个人的姓名和年龄组成的DataFrame，我们将通过电线进行流式传输。流DataFrame是与Spark
    ML一起使用的一种流行技术，因为在撰写本文时，我们尚未完全集成Spark结构化ML。
- en: We limit this recipe to only the extent of demonstrating a streaming DataFrame
    and leave it up to the reader to adapt this to their own custom ML pipelines.
    While streaming DataFrame is not available out of the box in Spark 2.1.0, it will
    be a natural evolution to see it in later versions of Spark.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将此配方限制为仅演示流DataFrame的范围，并留给读者将其适应其自定义ML管道。虽然在Spark 2.1.0中，流DataFrame并不是开箱即用的，但在后续版本的Spark中，它将是一个自然的演进。
- en: How to do it...
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Import the necessary packages:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create a `SparkSession` as an entry point to the Spark cluster:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`SparkSession`作为连接到Spark集群的入口点：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日志消息的交错会导致难以阅读的输出，因此将日志级别设置为警告：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, load the person data file to infer a data schema without hand coding
    the structure types:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，加载人员数据文件以推断数据模式，而无需手动编写结构类型：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'From the console, you will see the following output:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台，您将看到以下输出：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now configure a streaming DataFrame for ingestion of the data:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在配置一个用于摄取数据的流DataFrame：
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let us execute a simple data transform, by filtering on age greater than `60`:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们执行一个简单的数据转换，通过筛选年龄大于`60`：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We now output the transformed streaming data to the console, which will trigger
    every second:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将转换后的流数据输出到控制台，每秒触发一次：
- en: '[PRE21]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We start our defined streaming query and wait for data to appear in the stream:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们启动我们定义的流查询，并等待数据出现在流中：
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, the result of our streaming query will appear in the console:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们的流查询结果将出现在控制台中：
- en: '![](img/00291.gif)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00291.gif)'
- en: How it works...
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we first discover the underlying schema for a person object
    using a quick method (using a JSON object) as described in step 6\. The resulting
    DataFrame will know the schema that we subsequently impose on the streaming input
    (simulated via streaming a file) and treated as a streaming DataFrame as seen
    in step 7.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们首先使用一个快速方法（使用JSON对象）发现一个人对象的基础模式，如第6步所述。结果DataFrame将知道我们随后对流输入施加的模式（通过模拟流式传输文件）并将其视为流DataFrame，如第7步所示。
- en: The ability to treat the stream as a DataFrame and act on it using a functional
    or SQL paradigm is a powerful concept that can be seen in step 8\. We then proceed
    to output the result using `writestream()` with `append` mode and a 1-second batch
    interval trigger.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 将流视为DataFrame并使用函数式或SQL范式对其进行操作的能力是一个强大的概念，可以在第8步中看到。然后，我们使用`writestream()`以`append`模式和1秒批处理间隔触发器输出结果。
- en: There's more...
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The combination of DataFrames and structured programming is a powerful concept
    that helps us to separate the data layer from the stream, which makes the programming
    significantly easier. One of the biggest drawbacks with DStream (pre-Spark 2.0)
    was its inability to isolate the user from details of the underlying details of
    stream/RDD implementation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame和结构化编程的结合是一个强大的概念，它帮助我们将数据层与流分离，使编程变得更加容易。DStream（Spark 2.0之前）最大的缺点之一是无法将用户与流/RDD实现的细节隔离开来。
- en: 'Documentation for DataFrames:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames的文档：
- en: '`DataFrameReader`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataFrameReader`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader)'
- en: '`DataFrameWriter`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataFrameWriter`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter)'
- en: See also
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Documentation for Spark data stream reader and writer:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Spark数据流读取器和写入器的文档：
- en: DataStreamReader: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataStreamReader: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)
- en: DataStreamWriter: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataStreamWriter: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)
- en: Streaming Datasets for real-time machine learning
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于实时机器学习的流数据集
- en: In this recipe, we create a streaming Dataset to demonstrate the use of Datasets
    with a Spark 2.0 structured programming paradigm. We stream stock prices from
    a file using a Dataset and apply a filter to select the day's stock that closed
    above $100.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们创建一个流数据集来演示在Spark 2.0结构化编程范式中使用数据集的方法。我们从文件中流式传输股票价格，并使用数据集应用过滤器来选择当天收盘价高于100美元的股票。
- en: The recipe demonstrates how streams can be used to filter and to act on the
    incoming data using a simple structured streaming programming model. While it
    is similar to a DataFrame, there are some differences in the syntax. The recipe
    is written in a generalized manner so the user can customize it for their own
    Spark ML programming projects.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例演示了如何使用流来过滤和处理传入数据，使用简单的结构化流编程模型。虽然它类似于DataFrame，但语法上有一些不同。该示例以一种通用的方式编写，因此用户可以根据自己的Spark
    ML编程项目进行自定义。
- en: How to do it...
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Import the necessary packages:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Define a Scala `case class` to model streaming data:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala `case class`来建模流数据：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create `SparkSession` to use as an entry point to the Spark cluster:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`SparkSession`以用作进入Spark集群的入口点：
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日志消息的交错导致输出难以阅读，因此将日志级别设置为警告：
- en: '[PRE27]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, load the general electric CSV file inferring the schema:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，加载通用电气CSV文件并推断模式：
- en: '[PRE28]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You will see the following in console output:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在控制台输出中看到以下内容：
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we load the general electric CSV file into a dataset of type `StockPrice`:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将通用电气CSV文件加载到类型为`StockPrice`的数据集中：
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We will filter the stream for any close price greater than $100 USD:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将过滤流，以获取任何收盘价大于100美元的股票：
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We now output the transformed streaming data to the console that will trigger
    every second:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将转换后的流数据输出到控制台，每秒触发一次：
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We start our defined streaming query and wait for data to appear in the stream:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们启动我们定义的流式查询，并等待数据出现在流中：
- en: '[PRE33]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, the result of our streaming query will appear in the console:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们的流式查询结果将出现在控制台中：
- en: '![](img/00292.jpeg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/00292.jpeg)
- en: How it works...
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we will be utilizing the market data of closing prices for **General
    Electric** (**GE**) dating back to 1972\. To simplify the data, we have preprocessed
    for the purposes of this recipe. We use the same method from the previous recipe, *Streaming
    DataFrames for real-time machine learning*, by peeking into the JSON object to
    discover the schema (step 7), which we impose on the stream in step 8.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将利用追溯到1972年的**通用电气**（**GE**）的收盘价格市场数据。为了简化数据，我们已经对此示例进行了预处理。我们使用了上一个示例中的相同方法，*用于实时机器学习的流式数据框架*，通过窥探JSON对象来发现模式（步骤7），然后在步骤8中将其强加到流中。
- en: 'The following code shows how to use the schema to make the stream look like
    a simple table that you can read from on the fly. This is a powerful concept that
    makes stream programming accessible to more programmers. The `schema(s.schema)` and
    `as[StockPrice]`from the following code snippet are required to create the streaming
    Dataset, which has a schema associated with it:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了如何使用模式使流看起来像一个可以即时读取的简单表格。这是一个强大的概念，使流编程对更多程序员可访问。以下代码片段中的`schema(s.schema)`和`as[StockPrice]`是创建具有相关模式的流式数据集所需的：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: There's more...
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Documentation for all the APIs available under Dataset at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
    website[.](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 有关数据集下所有可用API的文档，请访问[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)网站[.](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
- en: See also
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'The following documentation is helpful while exploring the streaming Dataset
    concept:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索流式数据集概念时，以下文档很有帮助：
- en: '`StreamReader`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamReader`：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)'
- en: '`StreamWriter`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamWriter`：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)'
- en: '`StreamQuery`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamQuery`：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery)'
- en: Streaming data and debugging with queueStream
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用queueStream流式数据和调试
- en: In this recipe, we explore the concept of `queueStream()`*,* which is a valuable
    tool while trying to get a streaming program to work during the development cycle.
    We found the `queueStream()` API very useful and felt that other developers can
    benefit from a recipe that fully demonstrates its usage.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探讨了`queueStream()`的概念，这是一个有价值的工具，可以在开发周期中尝试使流式程序工作。我们发现`queueStream()`API非常有用，并且认为其他开发人员可以从完全演示其用法的示例中受益。
- en: 'We start by simulating a user browsing various URLs associated with different
    web pages using the program `ClickGenerator.scala` and then proceed to consume
    and tabulate the data (user behavior/visits) using the `ClickStream.scala` program:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过使用程序`ClickGenerator.scala`模拟用户浏览与不同网页相关的各种URL，然后使用`ClickStream.scala`程序消耗和制表数据（用户行为/访问）：
- en: '![](img/00293.jpeg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/00293.jpeg)
- en: We use Spark's streaming API with `Dstream()`, which will require the use of
    a streaming context. We are calling this out explicitly to highlight one of the
    differences between Spark streaming and the Spark structured streaming programming
    model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Spark的流式API与`Dstream()`，这将需要使用流式上下文。我们明确指出这一点，以突出Spark流和Spark结构化流编程模型之间的差异之一。
- en: There are two distinct programs (`ClickGenerator.scala` and `ClickStream.scala`)
    that make up this recipe.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例由两个不同的程序（`ClickGenerator.scala`和`ClickStream.scala`）组成。
- en: How to do it...
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到…
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE35]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Import the necessary packages:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE36]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Define a Scala `case class` to model click events by users that contains user
    identifier, IP address, time of event, URL, and HTTP status code:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala`case class`，用于模拟用户的点击事件，包含用户标识符、IP地址、事件时间、URL和HTTP状态码：
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Define status codes for generation:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为生成定义状态码：
- en: '[PRE38]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Define URLs for generation:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为生成定义URL：
- en: '[PRE39]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define IP address range for generation:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为生成定义IP地址范围：
- en: '[PRE40]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Define timestamp range for generation:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为生成定义时间戳范围：
- en: '[PRE41]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Define user identifier range for generation:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为生成定义用户标识符范围：
- en: '[PRE42]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Define a function to generate one or more pseudo random events:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来生成一个或多个伪随机事件：
- en: '[PRE43]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Define a function to parse a pseudo random `ClickEvent` from a string:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，从字符串中解析伪随机的`ClickEvent`：
- en: '[PRE44]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Create Spark''s configuration and Spark streaming context with 1-second duration:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和具有1秒持续时间的Spark流上下文：
- en: '[PRE45]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日志消息的交错导致难以阅读的输出，因此将日志级别设置为警告：
- en: '[PRE46]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Create a mutable queue to append our generated data onto:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个可变队列，将我们生成的数据附加到上面：
- en: '[PRE47]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Create a Spark queue stream from the streaming context passing in a reference
    of our data queue:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从流上下文中创建一个Spark队列流，传入我们数据队列的引用：
- en: '[PRE48]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Process any data received by the queue stream and count the total number of
    each particular link users have clicked upon:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理队列流接收的任何数据，并计算用户点击每个特定链接的总数：
- en: '[PRE49]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Print out the `12` URLs and their totals:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出`12`个URL及其总数：
- en: '[PRE50]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Start our streaming context to receive micro-batches:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动我们的流上下文以接收微批处理：
- en: '[PRE51]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Loop 10 times generating 100 pseudo random events on each iteration and append
    them our mutable queue so they materialize in the streaming queue abstraction:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环10次，在每次迭代中生成100个伪随机事件，并将它们附加到我们的可变队列中，以便它们在流队列抽象中实现：
- en: '[PRE52]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We close the program by stopping the Spark streaming context:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止Spark流上下文来关闭程序：
- en: '[PRE53]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: How it works...
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: With this recipe, we introduced Spark Streaming using a technique many overlook,
    which allows us to craft a streaming application utilizing Spark's `QueueInputDStream`
    class. The `QueueInputDStream` class is not only a beneficial tool for understanding
    Spark streaming, but also for debugging during the development cycle. In the beginning
    steps, we set up a few data structures, in order to generate pseudo random `clickstream`
    event data for stream processing at a later stage.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个配方，我们介绍了使用许多人忽视的技术来引入Spark Streaming，这使我们能够利用Spark的`QueueInputDStream`类来创建流应用程序。`QueueInputDStream`类不仅是理解Spark流的有益工具，也是在开发周期中进行调试的有用工具。在最初的步骤中，我们设置了一些数据结构，以便在稍后的阶段为流处理生成伪随机的`clickstream`事件数据。
- en: It should be noted that in step 12, we are creating a streaming context instead
    of a SparkContext. The streaming context is what we use for Spark streaming applications.
    Next, the creation of a queue and queue stream is done to receive streaming data.
    Now steps 15 and 16 resemble a general Spark application manipulating RDDs. The
    next step starts the streaming context processing. After the streaming context
    is started, we append data to the queue and the processing begins with micro-batches.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意，在第12步中，我们创建的是一个流上下文而不是SparkContext。流上下文是我们用于Spark流应用程序的。接下来，创建队列和队列流以接收流数据。现在的第15步和第16步类似于操作RDD的一般Spark应用程序。下一步是启动流上下文处理。流上下文启动后，我们将数据附加到队列，处理开始以微批处理方式进行。
- en: 'Documentation for some of the related topics is mentioned here:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提到了一些相关主题的文档：
- en: '`StreamingContext` and `queueStream()`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamingContext`和`queueStream()`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext)'
- en: '`DStream`:[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DStream`:[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream)'
- en: '`InputDStream`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.InputDStream](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.InputDStream)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`InputDStream`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.InputDStream](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.InputDStream)'
- en: See also
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'At its core, `queueStream()` is just a queue of RDDs that we have after the
    Spark streaming (pre-2.0) turns into RDD:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，`queueStream()`只是一个队列，我们在Spark流（2.0之前）转换为RDD后拥有的RDD队列：
- en: Documentation for structured streaming (Spark 2.0+): [https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html](https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html)
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流的文档（Spark 2.0+）： [https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html](https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html)
- en: Documentation for streaming (pre-Spark 2.0): [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流处理的文档（Spark 2.0之前）： [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
- en: Downloading and understanding the famous Iris data for unsupervised classification
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载并理解著名的鸢尾花数据，用于无监督分类
- en: In this recipe, we download and inspect the well-known Iris dataset in preparation
    for the upcoming streaming KMeans recipe, which lets you see classification/clustering
    in real-time.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们下载并检查了著名的鸢尾花数据，为即将到来的流式KMeans配方做准备，这让您可以实时查看分类/聚类。
- en: The data is housed on the UCI machine learning repository, which is a great
    source of data to prototype algorithms on. You will notice that R bloggers tend
    to love this dataset.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储在UCI机器学习库中，这是一个很好的原型算法数据来源。您会注意到R博客作者倾向于喜欢这个数据集。
- en: How to do it...
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'You can start by downloading the dataset using either two of the following
    commands:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过以下两个命令之一下载数据集：
- en: '[PRE54]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'You can also use the following command:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用以下命令：
- en: '[PRE55]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'You can also use the following command:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用以下命令：
- en: '[PRE56]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now we begin our first step of data exploration by examining how the data in
    `iris.data` is formatted:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们通过检查`iris.data`中的数据格式来开始数据探索的第一步：
- en: '[PRE57]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now we take a look at the iris data to know how it is formatted:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来看一下鸢尾花数据的格式：
- en: '[PRE58]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: How it works...
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The data is made of 150 observations. Each observation is made of four numerical
    features (measured in centimeters) and a label that signifies which class each
    Iris belongs to:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 数据由150个观测组成。每个观测由四个数值特征（以厘米为单位测量）和一个标签组成，该标签表示每个鸢尾花属于哪个类别：
- en: '**Features/attributes**:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征/属性**：'
- en: Sepal length in cm
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花萼长度（厘米）
- en: Sepal width in cm
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花萼宽度（厘米）
- en: Petal length in cm
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣长度（厘米）
- en: Petal width in cm
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣宽度（厘米）
- en: '**Label/class**:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签/类别**：'
- en: Iris Setosa
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iris Setosa
- en: Iris Versicolour
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iris Versicolour
- en: Iris Virginic
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iris Virginic
- en: There's more...
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The following image depicts an Iris flower with Petal and Sepal marked for
    clarity:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片描述了一朵鸢尾花，标有花瓣和萼片以便清晰显示：
- en: '![](img/00294.jpeg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00294.jpeg)'
- en: See also
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'The following link explores the Iris dataset in more detail:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接更详细地探讨了鸢尾花数据集：
- en: '[https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)'
- en: Streaming KMeans for a real-time on-line classifier
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时在线分类器的流式KMeans
- en: In this recipe, we explore the streaming version of KMeans in Spark used in
    unsupervised learning schemes. The purpose of streaming KMeans algorithm is to
    classify or group a set of data points into a number of clusters based on their
    similarity factor.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们探讨了Spark中用于无监督学习方案的KMeans的流式版本。流式KMeans算法的目的是根据它们的相似性因子将一组数据点分类或分组成多个簇。
- en: There are two implementations of the KMeans classification method, one for static/offline
    data and another version for continuously arriving, real-time updating data.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: KMeans分类方法有两种实现，一种用于静态/离线数据，另一种用于不断到达的实时更新数据。
- en: We will be streaming iris dataset clustering as new data streams into our streaming
    context.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把鸢尾花数据集作为新数据流流入我们的流式上下文进行聚类。
- en: How to do it...
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE59]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Import the necessary packages:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE60]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We begin by defining a function to load iris data into memory, filtering out
    blank lines, attaching an identifier to each element, and finally returning tuple
    of type string and long:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义一个函数，将鸢尾花数据加载到内存中，过滤掉空白行，为每个元素附加一个标识符，最后返回类型为字符串和长整型的元组：
- en: '[PRE61]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Create a parser to take the string portion of our tuple and create a label
    point:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个解析器来获取我们元组的字符串部分并创建一个标签点：
- en: '[PRE62]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Create a lookup map to convert the identifier back to the text label feature:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个查找映射，将标识符转换回文本标签特征：
- en: '[PRE63]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Create Spark''s configuration and Spark streaming context with 1-second duration:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和Spark流式上下文，持续1秒：
- en: '[PRE64]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日志消息的交错导致输出难以阅读，因此将日志级别设置为警告：
- en: '[PRE65]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We read in the Iris data and build a lookup map to display the final output:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取鸢尾花数据并构建一个查找映射来显示最终输出：
- en: '[PRE66]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Create mutable queues to append streaming data onto:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建可变队列以追加流式数据：
- en: '[PRE67]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Create Spark streaming queues to receive data:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark流式队列以接收数据：
- en: '[PRE68]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Create streaming KMeans object to cluster data into three groups:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建流式KMeans对象将数据聚类成三组：
- en: '[PRE69]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Set up KMeans model to accept streaming training data to build a model:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置KMeans模型以接受流式训练数据来构建模型：
- en: '[PRE70]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Set up KMeans model to predict clustering group values:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置KMeans模型以预测聚类组值：
- en: '[PRE71]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Start streaming context so it will process data when received:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动流式上下文，以便在接收到数据时处理数据：
- en: '[PRE72]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Convert Iris data into label points:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将鸢尾花数据转换为标签点：
- en: '[PRE73]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Now split label point data into training dataset and test dataset:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将标签点数据分成训练数据集和测试数据集：
- en: '[PRE74]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Append training data to streaming queue for processing:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据追加到流式队列进行处理：
- en: '[PRE75]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Now we split test data into four groups and append to streaming queues for
    processing:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将测试数据分成四组，并追加到流式队列进行处理：
- en: '[PRE76]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The configured streaming queues print out the following results of clustered
    prediction groups:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置的流式队列打印出聚类预测组的以下结果：
- en: '[PRE77]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'We close the program by stopping the SparkContext:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止SparkContext来关闭程序：
- en: '[PRE78]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: How it works...
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we begin by loading the iris dataset and using the `zip()` API
    to pair data with a unique identifier to the data for generating *labeled points* data
    structure for use with the KMeans algorithm.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们首先加载鸢尾花数据集，并使用`zip()` API将数据与唯一标识符配对，以生成用于KMeans算法的*标记点*数据结构。
- en: Next, the mutable queues and `QueueInputDStream` are created for appending data
    to simulate streaming. Once the `QueueInputDStream` starts receiving data then
    the streaming k-mean clustering begins to dynamically cluster data and printing
    out results. The interesting thing you will notice here is we are streaming the
    training dataset on one queue stream and the test data on another queue stream.
    As we append data to our queues, the KMeans clustering algorithm is processing
    our incoming data and dynamically generating clusters.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建可变队列和`QueueInputDStream`，以便追加数据以模拟流式。一旦`QueueInputDStream`开始接收数据，流式k均值聚类就开始动态聚类数据并打印结果。你会注意到的有趣的事情是，我们在一个队列流上流式训练数据，而在另一个队列流上流式测试数据。当我们向我们的队列追加数据时，KMeans聚类算法正在处理我们的传入数据并动态生成簇。
- en: There's more...
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Documentation for *StreamingKMeans()*:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '*StreamingKMeans()*的文档：'
- en: '`StreamingKMeans`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamingKMeans`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)'
- en: '`StreamingKMeansModel`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamingKMeansModel`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)'
- en: See also
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'The hyper parameters defined via a builder pattern or `streamingKMeans` are:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 通过构建模式或`streamingKMeans`定义的超参数为：
- en: '[PRE79]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Please refer to the *Building a KMeans classifying system in Spark* recipe in
    [Chapter 8](part0401.html#BUDHI0-4d291c9fed174a6992fd24938c2f9c77), *Unsupervised
    Clustering with Apache Spark 2.0 *for more details.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参阅[第8章](part0401.html#BUDHI0-4d291c9fed174a6992fd24938c2f9c77)中的*在Spark中构建KMeans分类系统*食谱，*使用Apache
    Spark 2.0进行无监督聚类*。
- en: Downloading wine quality data for streaming regression
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载用于流回归的葡萄酒质量数据
- en: In this recipe, we download and inspect the wine quality dataset from the UCI
    machine learning repository to prepare data for Spark's streaming linear regression
    algorithm from MLlib.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们下载并检查了UCI机器学习存储库中的葡萄酒质量数据集，以准备数据用于Spark的流线性回归算法。
- en: How to do it...
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'You will need one of the following command-line tools `curl` or `wget` to retrieve
    specified data:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 您将需要以下命令行工具之一`curl`或`wget`来检索指定的数据：
- en: 'You can start by downloading the dataset using either of the following three
    commands. The first one is as follows:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过以下三个命令之一开始下载数据集。第一个如下：
- en: '[PRE80]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'You can also use the following command:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用以下命令：
- en: '[PRE81]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'This command is the third way to do the same:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令是做同样事情的第三种方式：
- en: '[PRE82]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Now we begin our first steps of data exploration by seeing how the data in
    `winequality-white.csv` is formatted:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们开始通过查看`winequality-white.csv`中的数据格式来进行数据探索的第一步：
- en: '[PRE83]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now we take a look at the wine quality data to know how it is formatted:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来看一下葡萄酒质量数据，了解其格式：
- en: '[PRE84]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: How it works...
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The data is comprised of 1,599 red wines and 4,898 white wines with 11 features
    and an output label that can be used during training.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 数据由1,599种红葡萄酒和4,898种白葡萄酒组成，具有11个特征和一个输出标签，可在训练过程中使用。
- en: 'The following is a list of features/attributes:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是特征/属性列表：
- en: Fixed acidity
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 固定酸度
- en: Volatile acidity
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挥发性酸度
- en: Citric acid
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 柠檬酸
- en: Residual sugar
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残留糖
- en: Chlorides
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 氯化物
- en: Free sulfur dioxide
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游离二氧化硫
- en: Total sulfur dioxide
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总二氧化硫
- en: Density
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密度
- en: pH
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pH
- en: Sulphates
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硫酸盐
- en: Alcohol
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 酒精
- en: 'The following is the output label:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出标签：
- en: quality (a numeric value between 0 to 10)
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质量（0到10之间的数值）
- en: There's more...
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The following link lists datasets for popular machine learning algorithms. A
    new dataset can be chosen to experiment with as needed.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接列出了流行机器学习算法的数据集。可以根据需要选择新的数据集进行实验。
- en: Alternative datasets are available at [https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 可在[https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research)找到替代数据集。
- en: We selected the Iris dataset so we can use continuous numerical features for
    a linear regression model.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了鸢尾花数据集，因此可以使用连续的数值特征进行线性回归模型。
- en: Streaming linear regression for a real-time regression
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时回归的流线性回归
- en: In this recipe, we will use the wine quality dataset from UCI and Spark's streaming
    linear regression algorithm from MLlib to predict the quality of a wine based
    on a group of wine features.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将使用UCI的葡萄酒质量数据集和MLlib中的Spark流线性回归算法来预测葡萄酒的质量。
- en: The difference between this recipe and the traditional regression recipes we
    saw before is the use of Spark ML streaming to score the quality of the wine in
    real time using a linear regression model.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱与我们之前看到的传统回归食谱的区别在于使用Spark ML流来实时评估葡萄酒的质量，使用线性回归模型。
- en: How to do it...
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE85]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Import the necessary packages:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE86]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Create Spark''s configuration and streaming context:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和流上下文：
- en: '[PRE87]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日志消息的交错会导致难以阅读的输出，因此将日志级别设置为警告：
- en: '[PRE88]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Load the wine quality CSV using the Databricks CSV API into a DataFrame:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Databricks CSV API将葡萄酒质量CSV加载到DataFrame中：
- en: '[PRE89]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Convert the DataFrame into an `rdd` and `zip` a unique identifier onto it:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将DataFrame转换为`rdd`并将唯一标识符`zip`到其中：
- en: '[PRE90]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Build a lookup map to compare predicted quality against actual quality value
    later:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建查找映射，以便稍后比较预测的质量与实际质量值：
- en: '[PRE91]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Convert wine quality into label points for use with the machine learning library:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将葡萄酒质量转换为标签点，以便与机器学习库一起使用：
- en: '[PRE92]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Create a mutable queue for appending data to:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个可变队列以追加数据：
- en: '[PRE93]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Create Spark streaming queues to receive streaming data:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark流队列以接收流数据：
- en: '[PRE94]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Configure streaming linear regression model:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置流线性回归模型：
- en: '[PRE95]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Train regression model and predict final values:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练回归模型并预测最终值：
- en: '[PRE96]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Start Spark streaming context:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark流上下文：
- en: '[PRE97]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Split label point data into training set and test set:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标签点数据拆分为训练集和测试集：
- en: '[PRE98]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Append data to training data queue for processing:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据追加到训练数据队列以进行处理：
- en: '[PRE99]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Now split test data in half and append to queue for processing:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将测试数据分成两半，并追加到队列以进行处理：
- en: '[PRE100]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Once data is received by the queue stream, you will see the following output:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦队列流接收到数据，您将看到以下输出：
- en: '![](img/00295.gif)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00295.gif)'
- en: 'Close the program by stopping the Spark streaming context:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止Spark流上下文来关闭程序：
- en: '[PRE101]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: How it works...
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We started by loading the wine quality dataset into a DataFrame via Databrick's
    `spark-csv` library. The next step was to attach a unique identifier to each row
    in our dataset to later match the predicted quality to the actual quality. The
    raw data was converted to labeled points so it can be used as input for the streaming
    linear regression algorithm. In steps 9 and 10, we created instances of mutable
    queues and Spark's `QueueInputDStream` class to be used as a conduit into the
    regression algorithm.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过Databrick的`spark-csv`库将葡萄酒质量数据集加载到DataFrame中。接下来的步骤是为数据集中的每一行附加一个唯一标识符，以便稍后将预测的质量与实际质量进行匹配。原始数据被转换为带标签的点，以便用作流线性回归算法的输入。在第9步和第10步，我们创建了可变队列的实例和Spark的`QueueInputDStream`类的实例，以用作进入回归算法的导管。
- en: We then created the streaming linear regression model, which will predict wine
    quality for our final results. We customarily created training and test datasets
    from the original data and appended them to the appropriate queue to start our
    model processing streaming data. The final results for each micro-batch displays
    the unique generated identifier, predicted quality value, and quality value contained
    in the original dataset.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建了流线性回归模型，它将预测我们最终结果的葡萄酒质量。我们通常从原始数据中创建训练和测试数据集，并将它们附加到适当的队列中，以开始我们的模型处理流数据。每个微批处理的最终结果显示了唯一生成的标识符、预测的质量值和原始数据集中包含的质量值。
- en: There's more...
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Documentation for `StreamingLinearRegressionWithSGD()`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD).
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamingLinearRegressionWithSGD()`的文档：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD)。'
- en: See also
  id: totrans-408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Hyper parameters for `StreamingLinearRegressionWithSGD()`*:*
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamingLinearRegressionWithSGD()`的超参数*：*'
- en: '`setInitialWeights(Vectors.*zeros*())`'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setInitialWeights(Vectors.*zeros*())`'
- en: '`setNumIterations()`'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setNumIterations()`'
- en: '`setStepSize()`'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setStepSize()`'
- en: '`setMiniBatchFraction()`'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setMiniBatchFraction()`'
- en: 'There is also a `StreamingLinearRegression()` API that does not use the **stochastic
    gradient descent** (**SGD**) version:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个不使用**随机梯度下降**（**SGD**）版本的`StreamingLinearRegression()` API：
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearAlgorithm](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearAlgorithm)'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearAlgorithm](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearAlgorithm)'
- en: 'The following link provides a quick reference for linear regression:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接提供了线性回归的快速参考：
- en: '[https://en.wikipedia.org/wiki/Linear_regression](https://en.wikipedia.org/wiki/Linear_regression)'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Linear_regression](https://en.wikipedia.org/wiki/Linear_regression)'
- en: Downloading Pima Diabetes data for supervised classification
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载皮马糖尿病数据进行监督分类
- en: In this recipe, we download and inspect the Pima Diabetes dataset from the UCI
    machine learning repository. We will use the dataset later with Spark's streaming
    logistic regression algorithm.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们从UCI机器学习库下载并检查了皮马糖尿病数据集。我们将稍后使用该数据集与Spark的流式逻辑回归算法。
- en: How to do it...
  id: totrans-420
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'You will need one of the following command-line tools `curl` or `wget` to retrieve
    the specified data:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 您将需要以下命令行工具`curl`或`wget`来检索指定的数据：
- en: 'You can start by downloading the dataset using either two of the following
    commands. The first command is as follows:'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过以下两个命令之一开始下载数据集。第一个命令如下：
- en: '[PRE102]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'This is an alternative that you can use:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您可以使用的另一种选择：
- en: '[PRE103]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Now we begin our first steps of data exploration by seeing how the data in
    `pima-indians-diabetes.data` is formatted (from Mac or Linux Terminal):'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们开始通过查看`pima-indians-diabetes.data`中的数据格式（从Mac或Linux终端）来探索数据的第一步：
- en: '[PRE104]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Now we take a look at the Pima Diabetes data to understand how it is formatted:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来看一下皮马糖尿病数据，以了解其格式：
- en: '[PRE105]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: How it works...
  id: totrans-430
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We have 768 observations for the dataset. Each line/record is comprised of 10
    features and a label value that can used for a supervised learning model (that
    is, logistic regression). The label/class is either a `1`, meaning tested positive
    for diabetes, and `0` if the test came back negative.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有768个观测值的数据集。每行/记录由10个特征和一个标签值组成，可以用于监督学习模型（即逻辑回归）。标签/类别要么是`1`，表示糖尿病检测呈阳性，要么是`0`，表示检测呈阴性。
- en: '**Features/Attributes:**'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征/属性：**'
- en: Number of times pregnant
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 怀孕次数
- en: Plasma glucose concentration a 2 hours in an oral glucose tolerance test
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 口服葡萄糖耐量试验2小时后的血浆葡萄糖浓度
- en: Diastolic blood pressure (mm Hg)
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 舒张压（毫米汞柱）
- en: Triceps skin fold thickness (mm)
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三头肌皮褶厚度（毫米）
- en: 2-hour serum insulin (mu U/ml)
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 口服葡萄糖耐量试验2小时后的血清胰岛素（mu U/ml）
- en: Body mass index (weight in kg/(height in m)^2)
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 身体质量指数（体重（公斤）/（身高（米）^2））
- en: Diabetes pedigree function
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 糖尿病谱系功能
- en: Age (years)
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄（岁）
- en: Class variable (0 or 1)
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类变量（0或1）
- en: '[PRE106]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: There's more...
  id: totrans-443
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We found the following alternative datasets from Princeton University very
    helpful:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现普林斯顿大学提供的以下替代数据集非常有帮助：
- en: '[http://data.princeton.edu/wws509/datasets](http://data.princeton.edu/wws509/datasets)'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://data.princeton.edu/wws509/datasets](http://data.princeton.edu/wws509/datasets)'
- en: See also
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The dataset that you can use to explore this recipe has to be structured in
    a way that the label (prediction class) has to be binary (tested positive/negative
    for diabetes).
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以用来探索此配方的数据集必须以标签（预测类）为二进制（糖尿病检测呈阳性/阴性）的方式进行结构化。
- en: Streaming logistic regression for an on-line classifier
  id: totrans-448
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线分类器的流式逻辑回归
- en: In this recipe, we will be using the Pima Diabetes dataset we downloaded in
    the previous recipe and Spark's streaming logistic regression algorithm with SGD
    to predict whether a Pima with various features will test positive as a diabetic.
    It is an on-line classifier that learns and predicts based on the streamed data.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用在上一个示例中下载的Pima糖尿病数据集和Spark的流式逻辑回归算法进行预测，以预测具有各种特征的Pima是否会测试为糖尿病阳性。这是一种在线分类器，它根据流式数据进行学习和预测。
- en: How to do it...
  id: totrans-450
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE107]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Import the necessary packages:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE108]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Create a `SparkSession` object as an entry point to the cluster and a `StreamingContext`:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`SparkSession`对象作为集群的入口点和一个`StreamingContext`：
- en: '[PRE109]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日志消息的交错导致输出难以阅读，因此将日志级别设置为警告：
- en: '[PRE110]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Load the Pima data file into a Dataset of type string:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Pima数据文件加载到类型为字符串的数据集中：
- en: '[PRE111]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Build a RDD from our raw Dataset by generating a tuple consisting of the last
    item into a record as the label and everything else as a sequence:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的原始数据集中构建一个RDD，方法是生成一个元组，其中最后一项作为标签，其他所有内容作为序列：
- en: '[PRE112]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Convert the preprocessed data into label points for use with the machine learning
    library:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预处理数据转换为标签点，以便与机器学习库一起使用：
- en: '[PRE113]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Create mutable queues for appending data to:'
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建用于附加数据的可变队列：
- en: '[PRE114]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Create Spark streaming queues to receive streaming data:'
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark流队列以接收流数据：
- en: '[PRE115]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Configure the streaming logistic regression model:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置流式逻辑回归模型：
- en: '[PRE116]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Train the regression model and predict final values:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练回归模型并预测最终值：
- en: '[PRE117]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Start Spark streaming context:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark流上下文：
- en: '[PRE118]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Split label point data into training set and test set:'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标签点数据拆分为训练集和测试集：
- en: '[PRE119]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Append data to training data queue for processing:'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据附加到训练数据队列以进行处理：
- en: '[PRE120]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'Now split test data in half and append to the queue for processing:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将测试数据分成两半，并附加到队列以进行处理：
- en: '[PRE121]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'Once data is received by the queue stream, you will see the following output:'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据被队列流接收，您将看到以下输出：
- en: '[PRE122]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'Close the program by stopping the Spark streaming context:'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止Spark流上下文来关闭程序：
- en: '[PRE123]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: How it works...
  id: totrans-486
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we loaded the Pima Diabetes Dataset into a Dataset and parsed it into
    a tuple by taking every element as a feature except the last one, which we used
    as a label. Second, we morphed the RDD of tuples into labeled points so it can
    be used as input to the streaming logistic regression algorithm. Third, we created
    instances of mutable queues and Spark's `QueueInputDStream` class to be used as
    a pathway into the logistic algorithm.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将Pima糖尿病数据集加载到一个数据集中，并通过将每个元素作为特征，除了最后一个元素作为标签，将其解析为元组。其次，我们将元组的RDD变形为带有标签的点，以便用作流式逻辑回归算法的输入。第三，我们创建了可变队列的实例和Spark的`QueueInputDStream`类，以用作逻辑算法的路径。
- en: Fourth, we created the streaming logistic regression model, which will predict
    wine quality for our final results. Finally, we customarily created training and
    test datasets from original data and appended it to the appropriate queue to trigger
    the model's processing of streaming data. The final results for each micro-batch
    displays the original label and predicted label of 1.0 for testing true positive
    as a diabetic or 0.0 as true negative.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，我们创建了流式逻辑回归模型，它将预测我们最终结果的葡萄酒质量。最后，我们通常从原始数据创建训练和测试数据集，并将其附加到适当的队列中，以触发模型对流数据的处理。每个微批处理的最终结果显示了测试真正阳性的原始标签和预测标签为1.0，或者真正阴性的标签为0.0。
- en: There's more...
  id: totrans-489
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Documentation for `StreamingLogisticRegressionWithSGD()` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD)
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamingLogisticRegressionWithSGD()`的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD)上找到'
- en: "[\uFEFF](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD)"
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: "[\uFEFF](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD)"
- en: See also
  id: totrans-492
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'The hyper parameters for the model:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的超参数：
- en: '`setInitialWeights()`'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setInitialWeights()`'
- en: '`setNumIterations()`'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setNumIterations()`'
- en: '`setStepSize()`'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setStepSize()`'
- en: '`setMiniBatchFraction()`'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setMiniBatchFraction()`'
