- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: The pandas Ecosystem
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pandas 生态系统
- en: While the pandas library offers an impressive array of features, its popularity
    owes much to the vast amount of third-party libraries that work with it in a complementary
    fashion. We cannot hope to cover all of those libraries in this chapter, nor can
    we even dive too deep into how any individual library works. However, just knowing
    these tools exist and understanding what they offer can serve as a great inspiration
    for future learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 pandas 库提供了大量令人印象深刻的功能，但它的流行很大程度上得益于大量与之互补工作的第三方库。我们不能在本章中覆盖所有这些库，也无法深入探讨每个单独库的工作原理。然而，仅仅知道这些工具的存在并理解它们提供的功能，就能为未来的学习提供很大的启发。
- en: While pandas is an amazing tool, it has its flaws, which we have tried to highlight
    throughout this book; pandas cannot hope to solve every analytical problem there
    is. I strongly encourage you to get familiar with the tools outlined in this chapter
    and to also refer to the pandas ecosystem documentation ([https://pandas.pydata.org/about/](https://pandas.pydata.org/about/))
    when looking for new and specialized tools.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 pandas 是一个令人惊叹的工具，但它也有自己的缺陷，我们在本书中已经尽力突出了这些缺陷；pandas 不可能解决所有分析问题。我强烈建议你熟悉本章中提到的工具，并在寻找新工具和专用工具时，参考
    pandas 生态系统的文档（[https://pandas.pydata.org/about/](https://pandas.pydata.org/about/)）。
- en: As a technical note on this chapter, it is possible that these code blocks may
    break or change behavior as new releases of the libraries are released. While
    we went to great lengths throughout this book to try and write pandas code that
    is “future-proof”, it becomes more difficult to guarantee that as we write about
    third party dependencies (and their dependencies). If you encounter any issues
    running the code in this chapter, be sure to reference the `requirements.txt`
    file provided alongside the code samples with this book. That file will contain
    a list of dependencies and versions that are known to work with this chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术说明是，随着库的新版本发布，这些代码块可能会中断或行为发生变化。虽然我们在本书中尽力编写“未来可用”的 pandas 代码，但随着我们讨论第三方依赖库（及其依赖项），要保证这一点变得更加困难。如果你在运行本章代码时遇到问题，请确保参考与本书代码示例一起提供的
    `requirements.txt` 文件。该文件将包含已知与本章兼容的依赖项和版本的列表。
- en: 'We will cover the following recipes in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Foundational libraries
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础库
- en: Exploratory data analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: Data validation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据验证
- en: Visualization
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化
- en: Data science
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学
- en: Databases
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库
- en: Other DataFrame libraries
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他 DataFrame 库
- en: Foundational libraries
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础库
- en: Like many open source libraries, pandas builds functionality on top of other
    foundational libraries, letting them manage lower-level details while pandas offers
    more user-friendly functionality. If you find yourself wanting to dive deeper
    into technical details beyond what you learn with pandas, these are the libraries
    you’ll want to focus on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 和许多开源库一样，pandas 在其他基础库的基础上构建了功能，使得它们可以处理低层次的细节，而 pandas 则提供了更友好的功能。如果你希望深入研究比在
    pandas 中学到的更技术性的细节，那么这些库就是你需要关注的重点。
- en: NumPy
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NumPy
- en: NumPy labels itself as the *fundamental package for scientific computing with
    Python*, and it is the library on top of which pandas was originally built. NumPy
    is actually an *n*-dimensional library, so you are not limited to two-dimensional
    data like we get with a `pd.DataFrame` (pandas actually used to offer 3-d and
    4-d panel structures, but they are now long gone).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 自称为*Python 科学计算的基础包*，它是 pandas 最初构建之上的库。实际上，NumPy 是一个*n*维度的库，因此你不仅仅局限于像
    `pd.DataFrame`（pandas 实际上曾经提供过 3 维和 4 维的面板结构，但现在这些已不再使用）那样的二维数据。
- en: 'Throughout this book, we have shown you how to construct pandas objects from
    NumPy objects, as you can see in the following `pd.DataFrame` constructor:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们向你展示了如何从 NumPy 对象构建 pandas 对象，如以下 `pd.DataFrame` 构造函数所示：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'However, you can also create NumPy arrays from `pd.DataFrame` objects by using
    the `pd.DataFrame.to_numpy` method:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你也可以通过使用 `pd.DataFrame.to_numpy` 方法，从 `pd.DataFrame` 对象创建 NumPy 数组：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Many NumPy functions accept a `pd.DataFrame` as an argument and will still
    even return a `pd.DataFrame`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 NumPy 函数接受 `pd.DataFrame` 作为参数，甚至仍然会返回一个 `pd.DataFrame`：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The main thing to keep in mind with NumPy is that its interoperability with
    pandas will degrade the moment you need missing values in non-floating point types,
    or more generally when you try to use data types that are neither integral nor
    floating point.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的主要一点是，NumPy 与 pandas 的互操作性会在你需要处理非浮动类型的缺失值时下降，或者更一般地，当你尝试使用既非整数也非浮动点类型的数据时。
- en: The exact rules for this are too complicated to list in this book, but generally,
    I would advise against ever calling `pd.Series.to_numpy` or `pd.DataFrame.to_numpy`
    for anything other than floating point and integral data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点的具体规则过于复杂，无法在本书中列出，但通常，我建议除非是浮动点或整数数据，否则不要调用 `pd.Series.to_numpy` 或 `pd.DataFrame.to_numpy`。
- en: PyArrow
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyArrow
- en: The other main library that pandas is built on top of is Apache Arrow, which
    labels itself as a *cross-language development platform for in-memory analytics*.
    Started by Wes McKinney (the creator of pandas) and announced in his influential
    Apache Arrow and the *10 Things I Hate About pandas* post ([https://wesmckinney.com/blog/apache-arrow-pandas-internals/](https://wesmckinney.com/blog/apache-arrow-pandas-internals/)),
    the Apache Arrow project defines the memory layout for one-dimensional data structures
    in a way that allows different languages, programs, and libraries to work with
    the same data. In addition to defining these structures, the Apache Arrow project
    offers a vast suite of tooling for libraries to implement the Apache Arrow specifications.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 的另一个主要依赖库是 Apache Arrow，Apache Arrow 自称是一个 *跨语言的内存分析开发平台*。这个项目由 pandas
    的创建者 Wes McKinney 启动，并在他具有影响力的博客文章《Apache Arrow 与 *我讨厌 pandas 的 10 件事*》中宣布 ([https://wesmckinney.com/blog/apache-arrow-pandas-internals/](https://wesmckinney.com/blog/apache-arrow-pandas-internals/))。Apache
    Arrow 项目为一维数据结构定义了内存布局，允许不同的语言、程序和库共享相同的数据。除了定义这些结构外，Apache Arrow 项目还提供了一整套工具，供库实现
    Apache Arrow 的规范。
- en: 'An implementation of Apache Arrow in Python, PyArrow, has been used in particular
    instances throughout this book. While pandas does not expose a method to convert
    a `pd.DataFrame` into PyArrow, the PyArrow library offers a `pa.Table.from_pandas`
    method for that exact purpose:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中在特定场景下使用了 Apache Arrow 的 Python 实现 PyArrow。虽然 pandas 没有提供将 `pd.DataFrame`
    转换为 PyArrow 的方法，但 PyArrow 库提供了一个 `pa.Table.from_pandas` 方法，专门用于此目的：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'PyArrow similarly offers a `pa.Table.to_pandas` method to get you from a `pa.Table`
    into a `pd.DataFrame`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PyArrow 同样提供了一个 `pa.Table.to_pandas` 方法，可以将 `pa.Table` 转换为 `pd.DataFrame`：
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Generally, PyArrow is considered a lower-level library than pandas. It mostly
    aims to serve other library authors more than it does general users looking for
    a DataFrame library, so, unless you are authoring a library, you may not often
    need to convert to PyArrow from a `pd.DataFrame`. However, as the Apache Arrow
    ecosystem grows, the fact that pandas and PyArrow can interoperate opens up a
    world of integration opportunities for pandas with many other analytical libraries
    and databases.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，PyArrow 被认为是比 pandas 更低级的库。它主要旨在为其他库的开发者提供服务，而非为寻求 DataFrame 库的一般用户提供服务，因此，除非你正在编写库，否则你可能不需要经常将
    `pd.DataFrame` 转换为 PyArrow。然而，随着 Apache Arrow 生态系统的发展，pandas 和 PyArrow 可以互操作这一事实为
    pandas 与许多其他分析库和数据库的集成提供了无限的可能性。
- en: Exploratory data analysis
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: Oftentimes, you will find yourself provided with a dataset that you know very
    little about. Throughout this book, we’ve shown ways to manually sift through
    data, but there are also tools out there that can help automate potentially tedious
    tasks and help you grasp the data in a shorter amount of time.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 很多时候，你会遇到一个对数据几乎不了解的数据集。在本书中，我们展示了手动筛选数据的方法，但也有一些工具可以帮助自动化这些潜在的繁琐任务，帮助你更短时间内掌握数据。
- en: YData Profiling
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YData Profiling
- en: YData Profiling bills itself as the “*leading package for data profiling, that
    automates and standardizes the generation of detailed reports, complete with statistics
    and visualizations*.” While we discovered how to manually explore data back in
    the chapter on visualization, this package can be used as a quick-start to automatically
    generate many useful reports and features.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: YData Profiling 自称是“*领先的数据概况分析包，能够自动化并标准化生成详细报告，包含统计信息和可视化*”。虽然我们在可视化章节中已经学会了如何手动探索数据，但这个包可以作为一个快速启动工具，自动生成许多有用的报告和特性。
- en: 'To compare this to some of the work we did in those chapters, let’s take another
    look at the vehicles dataset. For now, we are just going to pick a small subset
    of columns to keep our YData Profiling minimal; for large datasets, the performance
    can often degrade:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将其与我们在那些章节中做的工作进行对比，我们再来看一看车辆数据集。现在，我们只挑选一个小子集的列，以保持我们的YData Profiling尽量简洁；对于大型数据集，性能往往会下降：
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: YData Profiling allows you to easily create a profile report, which contains
    many common visualizations and helps describe the columns you are working with
    in your `pd.DataFrame`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: YData Profiling使你能够轻松创建一个概况报告，该报告包含许多常见的可视化内容，并有助于描述你在`pd.DataFrame`中工作的各列。
- en: 'This book was written using `ydata_profiling` version 4.9.0\. To create the
    profile report, simply run:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是使用`ydata_profiling`版本4.9.0编写的。要创建概况报告，只需运行：
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If running code within a Jupyter notebook, you can see the output of this directly
    within the notebook with a call to:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在Jupyter notebook中运行代码，你可以直接在notebook中看到输出，方法是调用：
- en: '[PRE13]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If you are not using Jupyter, you can alternatively export that profile to
    a local HTML file and open it from there:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有使用Jupyter，你还可以将该概况导出为本地HTML文件，然后从那里打开：
- en: '[PRE14]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When looking at the profile, the first thing you will see is a high-level **Overview**
    section that lists the number of cells with missing data, number of duplicate
    rows, etc.:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看概况时，你首先会看到一个高层次的**概述**部分，其中列出了缺失数据的单元格数、重复行数等：
- en: '![A screenshot of a computer  Description automatically generated](img/B31091_11_01.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![一张计算机屏幕截图 说明自动生成](img/B31091_11_01.png)'
- en: 'Figure 11.1: Overview provided by YData Profiling'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：YData Profiling提供的概述
- en: 'Each column from your `pd.DataFrame` will be detailed. In the case of a column
    with continuous values, YData Profiling will create a histogram for you:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每一列来自`pd.DataFrame`的数据都会被详细列出。如果某列包含连续值，YData Profiling会为你创建一个直方图：
- en: '![A screenshot of a graph  Description automatically generated](img/B31091_11_02.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![一张图表截图 说明自动生成](img/B31091_11_02.png)'
- en: 'Figure 11.2: Histogram generated by YData Profiling'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：YData Profiling生成的直方图
- en: 'For categorical variables, the tool will generate a word cloud visualization:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类变量，该工具将生成一个词云可视化图：
- en: '![A screenshot of a computer  Description automatically generated](img/B31091_11_03.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![一张计算机屏幕截图 说明自动生成](img/B31091_11_03.png)'
- en: 'Figure 11.3: Word cloud generated by YData Profiling'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：YData Profiling生成的词云
- en: 'To understand how your continuous variables may or may not be correlated, the
    profile contains a very concise heat map that colors each pair accordingly:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解你的连续变量是否存在相关性，概况报告中包含了一个简洁的热图，根据每一对变量之间的关系着色：
- en: '![A screenshot of a color chart  Description automatically generated](img/B31091_11_04.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![一张颜色图表截图 说明自动生成](img/B31091_11_04.png)'
- en: 'Figure 11.4: Heat map generated by YData Profiling'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：YData Profiling生成的热图
- en: While you still will likely need to dive further into your datasets than what
    this library provides, it can be a great starting point and can help automate
    the generation of otherwise tedious plots.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你可能仍然需要深入分析数据集，超出该库所提供的功能，但它是一个很好的起点，并可以帮助自动生成那些本来可能是繁琐的图表。
- en: Data validation
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据验证
- en: The “garbage in, garbage out” principle in computing says that no matter how
    great your code may be, if you start with poor-quality data, your analysis will
    yield poor-quality results. All too often, data practitioners struggle with issues
    like unexpected missing data, duplicate values, and broken relationships between
    modeling entities.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 计算中的“垃圾进，垃圾出”原则指出，无论你的代码多么出色，如果从质量差的数据开始，分析结果也会是质量差的。数据从业者经常面临诸如意外缺失数据、重复值和建模实体之间断裂关系等问题。
- en: Fortunately, there are tools to help you automate both the data that is input
    to and output from your models, which ensures trust in the work that you are performing.
    In this recipe, we are going to look at Great Expectations.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一些工具可以帮助你自动化输入到模型和从模型输出的数据，这样可以确保你所执行工作的可信度。在这篇文章中，我们将介绍Great Expectations。
- en: Great Expectations
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Great Expectations
- en: 'This book was written using Great Expectations version 1.0.2\. To get started,
    let’s once again look at our vehicles dataset:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是使用Great Expectations版本1.0.2编写的。为了开始，让我们再次看看我们的车辆数据集：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: There are a few different ways to use Great Expectations, not all of which can
    be documented in this cookbook. For the sake of having a self-contained example,
    we are going to set up and process all of our expectations in memory.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Great Expectations的方式有几种，不是所有的方式都能在本手册中记录。为了展示一个自包含的例子，我们将在内存中设置并处理所有的期望。
- en: 'To do this, we are going to import the `great_expectations` library and create
    a `context` for our tests:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将导入`great_expectations`库，并为我们的测试创建一个`context`：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Within the context, you can create a data source and a data asset. For non-DataFrame
    sources like SQL, the data source would typically contain connection credentials,
    but with the `pd.DataFrame` residing in memory there is less work to do. The data
    asset is a grouping mechanism for results. Here we are just creating one data
    asset, but in real-life use cases you may decide that you want multiple assets
    to store and organize the validation results that Great Expectations outputs:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在此上下文中，你可以创建数据源和数据资产。对于像SQL这样的非DataFrame源，数据源通常会包含连接凭证，而对于存在内存中的`pd.DataFrame`，则无需做太多工作。数据资产是用于存储结果的分组机制。这里我们只创建了一个数据资产，但在实际应用中，你可能会决定创建多个资产来存储和组织Great
    Expectations输出的验证结果：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'From there, you can create a batch definition within Great Expectations. For
    non-DataFrame sources, the batch definition would tell the library how to retrieve
    data from the source. In the case of pandas, the batch definition will simply
    retrieve all of the data from the associated `pd.DataFrame`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，你可以在Great Expectations中创建一个批次定义。对于非DataFrame源，批次定义会告诉库如何从源中获取数据。对于pandas，批次定义将简单地从关联的`pd.DataFrame`中检索所有数据：
- en: '[PRE19]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'At this point, you can start to make assertions about the data. For instance,
    you can use Great Expectations to ensure that a column does not contain any null
    values:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 到此时，你可以开始对数据进行断言。例如，你可以使用Great Expectations来确保某一列不包含任何空值：
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'That same expectation applied to the `cylinders` column will not be successful:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 同样应用于`cylinders`列的期望将不会成功：
- en: '[PRE22]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: For brevity, we have only shown you how to set expectations around nullability,
    but there is an entire Expectations Gallery at [https://greatexpectations.io/expectations/](https://greatexpectations.io/expectations/)
    you can use for other assertions. Great Expectations also works with other tools
    like Spark, PostgreSQL, etc., so you can apply your expectations at many different
    points in your data transformation pipeline.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们只展示了如何设置关于空值的期望，但你可以在[https://greatexpectations.io/expectations/](https://greatexpectations.io/expectations/)中找到完整的期望库，供你用于其他断言。Great
    Expectations还与其他工具（如Spark、PostgreSQL等）兼容，因此你可以在数据转换管道中的多个点应用你的期望。
- en: Visualization
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化
- en: Back in *Chapter 6*, *Visualization*, we discussed at length visualization using
    matplotlib, and we even discussed using Seaborn for advanced plots. These tools
    are great for generating static charts, but when you want to add some level of
    interactivity, you will need to opt for other libraries.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在第*6章*，*可视化*中，我们详细讨论了使用matplotlib进行可视化，并且还讨论了使用Seaborn进行高级图表的绘制。这些工具非常适合生成静态图表，但当你想要加入一些交互性时，你需要选择其他的库。
- en: 'For this recipe, we are going to load the same data from the vehicles dataset
    we used back in our *Scatter plots* recipe from *Chapter 6*:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将加载来自车辆数据集的数据，这是我们在第*6章*的*散点图*例子中使用过的：
- en: '[PRE24]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Plotly
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Plotly
- en: 'Let’s start by looking at Plotly, which can be used to create visualizations
    with a high degree of interactivity, making it a popular choice within Jupyter
    notebooks. To use it, simply pass `plotly` as the `backend=` argument to `pd.DataFrame.plot`.
    We are also going to add a `hover_data=` argument, which Plotly can use to add
    labels to each data point:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来看看Plotly，它可以用来创建具有高交互性的可视化图表，因此在Jupyter笔记本中非常受欢迎。使用它时，只需将`plotly`作为`backend=`参数传递给`pd.DataFrame.plot`。我们还将添加一个`hover_data=`参数，Plotly可以用它为每个数据点添加标签：
- en: '[PRE26]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If you inspect this in a Jupyter notebook or HTML page, you will see that you
    can hover over any data point to reveal more details:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在Jupyter笔记本或HTML页面中查看此内容，你将看到你可以将鼠标悬停在任何数据点上，查看更多细节：
- en: '![](img/B31091_11_05.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_11_05.png)'
- en: 'Figure 11.5: Hovering over a data point with Plotly'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11.5: 使用Plotly悬停在数据点上'
- en: 'You can even select an area of the chart to zoom into the data points:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以选择图表的某个区域，以便放大数据点：
- en: '![](img/B31091_11_06.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_11_06.png)'
- en: 'Figure 11.6: Zooming in with Plotly'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11.6: 使用Plotly进行缩放'
- en: As you can see, Plotly is very easy to use with the same pandas API you have
    seen throughout this book. If you desire interactivity with your plots, it is
    a great tool to make use of.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Plotly与您在本书中看到的相同的pandas API配合使用，非常简单。如果您希望图表具有互动性，这是一个非常适合使用的工具。
- en: PyGWalker
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyGWalker
- en: All of the plotting code you have seen so far is declarative in nature; i.e.,
    you tell pandas that you want a bar, line, scatter plot, etc., and pandas generates
    that for you. However, many users may prefer having a more “free-form” tool for
    exploration, where they can just drag and drop elements to make charts on the
    fly.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您看到的所有绘图代码都是声明式的；即，您告诉pandas您想要一个条形图、折线图、散点图等，然后pandas为您生成相应的图表。然而，许多用户可能更喜欢一种“自由形式”的工具进行探索，在这种工具中，他们可以直接拖放元素，即时制作图表。
- en: 'If that is what you are after, then you will want to take a look at the PyGWalker
    library. With a very succinct API, you can generate an interactive tool within
    a Jupyter notebook, with which you can drag and drop different elements to generate
    various charts:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这正是您所追求的，那么您将希望查看PyGWalker库。通过一个非常简洁的API，您可以在Jupyter笔记本中生成一个互动工具，您可以拖放不同的元素来生成各种图表：
- en: '[PRE27]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](img/B31091_11_07.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_11_07.png)'
- en: 'Figure 11.7: PyGWalker within a Jupyter notebook'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：Jupyter笔记本中的PyGWalker
- en: Data science
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学
- en: While pandas offers some built-in statistical algorithms, it cannot hope to
    cover all of the statistical and machine learning algorithms that are used in
    the domain of data science. Fortunately, however, many of the libraries that do
    specialize further in data science offer very tight integrations with pandas,
    letting you move data from one library to the next rather seamlessly.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然pandas提供了一些内建的统计算法，但它无法涵盖所有数据科学领域使用的统计和机器学习算法。幸运的是，许多专注于数据科学的库与pandas有很好的集成，让您能够在不同库之间无缝地移动数据。
- en: scikit-learn
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn
- en: scikit-learn is a popular machine learning library that can help with both supervised
    and unsupervised learning. The scikit-learn library offers an impressive array
    of algorithms for classification, prediction, and clustering tasks, while also
    providing tools to pre-process and cleanse your data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn是一个流行的机器学习库，能够帮助进行监督学习和无监督学习。scikit-learn库提供了一个令人印象深刻的算法库，用于分类、预测和聚类任务，同时还提供了数据预处理和清洗工具。
- en: 'We cannot hope to cover all of these features, but for the sake of showcasing
    something, let’s once again load the vehicles dataset:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法涵盖所有这些功能，但为了展示一些内容，我们再次加载车辆数据集：
- en: '[PRE28]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now let’s assume that we want to create an algorithm to predict the combined
    mileage a vehicle will achieve, inferring it from other attributes in the data.
    Since mileage is a continuous variable, we can opt for a linear regression model
    to make our predictions.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们想创建一个算法来预测车辆的综合行驶里程，从数据中的其他属性推断出来。由于里程是一个连续变量，我们可以选择线性回归模型来进行预测。
- en: 'The linear regression model we are going to work with will want to use features
    that are also numeric. While there are ways we could artificially convert some
    of our non-numeric data into numeric (e.g., using the technique from the *One-hot
    encoding with pd.get_dummies* recipe back in *Chapter 5*, *Algorithms and How
    to Apply Them*), we are just going to ignore any non-numeric columns for now.
    The linear regression model is also unable to handle missing data. We know from
    the *Exploring continuous* *data* recipe from *Chapter 6* that this dataset has
    two continuous variables with missing data. While we could try to interpolate
    those values, we are again going to take the simple route in this example and
    just drop them:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的线性回归模型希望使用的特征也应该是数值型的。虽然我们可以通过某些方法将部分非数值数据人工转换为数值（例如，使用*《第五章，算法及其应用》*中提到的*One-hot编码与pd.get_dummies*技术），但我们现在暂时忽略任何非数值列。线性回归模型也无法处理缺失数据。我们从*《第六章，探索连续数据》*的*探索连续数据*食谱中知道，该数据集有两个缺失数据的连续变量。虽然我们可以尝试插值来填充这些值，但在这个例子中，我们还是采用简单的方法，直接删除这些数据：
- en: '[PRE30]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The scikit-learn model will need to know the *features* we want to use for
    prediction (commonly notated as `X`) and the target variable we are trying to
    predict (commonly notated as `y`). It is also a good practice to split the data
    into training and testing datasets, which we can do with the `train_test_split`
    function:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 模型需要知道我们想用来进行预测的 *特征*（通常标记为 `X`）和我们试图预测的目标变量（通常标记为 `y`）。将数据拆分为训练集和测试集也是一个好习惯，我们可以使用
    `train_test_split` 函数来完成：
- en: '[PRE31]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'With our data in this form, we can go ahead and train the linear regression
    model and then apply it to our test data to generate predictions:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们这样的数据格式，我们可以继续训练线性回归模型，然后将其应用于测试数据，以生成预测结果：
- en: '[PRE32]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now that we have predictions from our test dataset, we can compare them back
    to the actual values we withheld as part of testing. This is a good way to measure
    how accurate the model is that we fit.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从测试数据集得到了预测结果，我们可以将其与实际的测试数据对比。这是衡量我们训练模型准确性的好方法。
- en: 'There are many different ways to manage model accuracy, but for now, we can
    opt for the commonly used and relatively simple `mean_squared_error`, which scikit-learn
    also provides as a convenience function:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 管理模型准确性的方式有很多种，但现在我们可以选择一个常用且相对简单的 `mean_squared_error`，这是 scikit-learn 也提供的一个便捷函数：
- en: '[PRE33]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If you are interested in knowing more, I highly recommend you read through
    the documentation and examples on the scikit-learn website, or check out books
    like *Machine Learning with PyTorch and Scikit-Learn: Develop machine learning
    and deep learning models with Python* ([https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312](https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312)).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你有兴趣了解更多，我强烈推荐你阅读 scikit-learn 网站上的文档和示例，或者阅读像《*Machine Learning with PyTorch
    and Scikit-Learn: Develop machine learning and deep learning models with Python*》这样的书籍
    ([https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312](https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312))。'
- en: XGBoost
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost
- en: For another great machine learning library, let’s now turn our attention to
    XGBoost, which implements algorithms using Gradient boosting. XGBoost is extremely
    performant, scales well, scores well in machine learning competitions, and pairs
    well with data that is stored in a `pd.DataFrame`. If you are already familiar
    with scikit-learn, the API it uses will feel familiar.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向另一个出色的机器学习库 XGBoost，它使用梯度提升算法实现了多种算法。XGBoost 性能极其出色，扩展性强，在机器学习竞赛中表现优异，且与存储在
    `pd.DataFrame` 中的数据兼容。如果你已经熟悉 scikit-learn，那么它使用的 API 会让你感到非常熟悉。
- en: XGBoost can be used for both classification and regression. Since we just performed
    a regression analysis with scikit-learn, let’s now work through a classification
    example where we try to predict the make of a vehicle from the numeric features
    in the dataset.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 可用于分类和回归。由于我们刚刚使用 scikit-learn 进行了回归分析，接下来让我们通过一个分类示例来尝试预测车辆的品牌，这个预测是基于数据集中的数值特征。
- en: 'The vehicles dataset we are working with has 144 different makes. For our analysis,
    we are going to just pick a small subset of consumer brands:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用的车辆数据集包含144种不同的品牌。为了进行分析，我们将选择一小部分消费者品牌：
- en: '[PRE35]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'From there, we are going to split our data into features (`X`) and a target
    variable (`y`). For the purposes of the machine learning algorithm, we also need
    to convert our target variable into categorical data type, so that the algorithm
    can predict values like `0`, `1`, `2`, etc instead of `"Dodge," "Toyota," "Volvo,"`
    etc.:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把数据分为特征（`X`）和目标变量（`y`）。为了适应机器学习算法，我们还需要将目标变量转换为类别数据类型，以便算法能够预测像 `0`、`1`、`2`
    等值，而不是像 `"Dodge"`、`"Toyota"`、`"Volvo"` 这样的字符串：
- en: '[PRE36]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'With that out of the way, we can once again use the `train_test_split` function
    from scikit-learn to create training and testing data. Note that we are using
    `pd.Series.cat.codes` to use the numeric value assigned to our categorical data
    type, rather than the string:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，我们可以再次使用 scikit-learn 中的 `train_test_split` 函数来创建训练数据和测试数据。请注意，我们使用了 `pd.Series.cat.codes`
    来获取分配给类别数据类型的数字值，而不是字符串：
- en: '[PRE37]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Finally, we can import the `XGBClassifier` from XGBoost, train it on our data,
    and apply it to our test features to generate predictions:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以从 XGBoost 中导入 `XGBClassifier`，将其在我们的数据上进行训练，并应用于测试特征以生成预测结果：
- en: '[PRE38]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now that we have the predictions, we can validate how many of them matched
    the target variables included as part of our testing data:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了预测结果，可以验证它们中有多少与测试数据中包含的目标变量相匹配：
- en: '[PRE39]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Once again, we are only scratching the surface of what you can do with a library
    like XGBoost. There are many different ways to tweak your model to improve accuracy,
    prevent over-/underfitting, optimize for a different outcome, etc. For users wanting
    to learn more about this great library, I advise checking out the XGBoost documentation
    or books like *Hands-On* *Gradient Boosting with XGBoost and scikit-learn*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们只是简单地触及了像 XGBoost 这样的库所能做的皮毛。你可以通过许多不同的方法来调整模型，以提高准确性、避免过拟合/欠拟合、优化不同的结果等等。对于想要深入了解这个优秀库的用户，我建议查看
    XGBoost 的文档或像 *Hands-On* *Gradient Boosting with XGBoost and scikit-learn* 这样的书籍。
- en: Databases
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据库
- en: Database knowledge is an important tool in the toolkit of any data practitioner.
    While pandas is a great tool for single-machine, in-memory computations, databases
    offer a very complementary set of analytical tools that can help with the storage
    and distribution of analytical processes.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库知识是任何数据从业人员工具包中的重要工具。虽然 pandas 是一个适用于单机内存计算的优秀工具，但数据库提供了一套非常互补的分析工具，可以帮助存储和分发分析过程。
- en: Back in *Chapter 4*, *The pandas I/O System*, we walked through how to transfer
    data between pandas and theoretically any database. However, a relatively more
    recent database called DuckDB is worth some extra consideration, as it allows
    you to even more seamlessly bridge the worlds of dataframes and databases together.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 4 章*，*pandas I/O 系统* 中，我们介绍了如何在 pandas 和理论上任何数据库之间转移数据。然而，一个相对较新的数据库叫做
    DuckDB，值得额外关注，因为它可以让你更加无缝地将 DataFrame 和数据库的世界连接在一起。
- en: DuckDB
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DuckDB
- en: DuckDB is a lightweight database system that offers a zero-copy integration
    with Apache Arrow, a technology that also underpins efficient data sharing and
    usage with pandas. It is extremely lightweight and, unlike most database systems,
    can be easily embedded into other tools or processes. Most importantly, DuckDB
    is optimized for analytical workloads.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: DuckDB 是一个轻量级数据库系统，提供与 Apache Arrow 的零复制集成，这项技术也支持与 pandas 高效的数据共享和使用。它非常轻量，并且与大多数数据库系统不同，可以轻松嵌入到其他工具或流程中。最重要的是，DuckDB
    针对分析型工作负载进行了优化。
- en: 'DuckDB makes it easy to query data in your `pd.DataFrame` using SQL. Let’s
    see this in action by loading the vehicles dataset:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: DuckDB 使你可以轻松地使用 SQL 查询 `pd.DataFrame` 中的数据。让我们通过加载车辆数据集来实际操作一下：
- en: '[PRE41]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'By passing a `CREATE TABLE` statement to `duckdb.sql`, you can load the data
    from the `pd.DataFrame` into a table:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 `CREATE TABLE` 语句传递给 `duckdb.sql`，你可以将数据从 `pd.DataFrame` 加载到表格中：
- en: '[PRE43]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Once the table is created, you can query from it with SQL:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦表格创建完成，你就可以通过 SQL 查询它：
- en: '[PRE44]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'If you want to convert your results back to a `pd.DataFrame`, you use the `.df`
    method:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想将结果转换回 `pd.DataFrame`，你可以使用 `.df` 方法：
- en: '[PRE46]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: For a deeper dive into DuckDB, I strongly advise checking out the DuckDB documentation
    and, for a greater understanding of where it fits in the grand scheme of databases,
    the *Why DuckDB* article ([https://duckdb.org/why_duckdb](https://duckdb.org/why_duckdb)).
    Generally, DuckDB’s focus is on single-user analytics, but if you are interested
    in a shared, cloud-based data warehouse, you may also want to look at MotherDuck
    ([https://motherduck.com/](https://motherduck.com/)).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解 DuckDB，我强烈建议查看 DuckDB 的文档，并且为了更好地理解它在数据库领域的定位，阅读 *Why DuckDB* 文章（[https://duckdb.org/why_duckdb](https://duckdb.org/why_duckdb)）。通常，DuckDB
    的重点是单用户分析，但如果你对共享的云端数据仓库感兴趣，也可以看看 MotherDuck（[https://motherduck.com/](https://motherduck.com/)）。
- en: Other DataFrame libraries
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他 DataFrame 库
- en: Soon after pandas was developed, it became the de facto DataFrame library in
    the Python space. Since then, many new DataFrame libraries have been developed
    in the space, which all aim to address some of the shortcomings of pandas while
    introducing their own novel design decisions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 开发不久后，它成为了 Python 领域事实上的 DataFrame 库。从那时起，许多新的 DataFrame 库在该领域得以开发，它们都旨在解决
    pandas 的一些不足之处，同时引入自己独特的设计决策。
- en: Ibis
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ibis
- en: Ibis is yet another amazing analytics tool created by Wes McKinney, the creator
    of pandas. At a high level, Ibis is a DataFrame “frontend” that gives you one
    generic API through which you can query multiple “backends.”
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Ibis 是另一个由 pandas 创始人 Wes McKinney 创建的出色分析工具。从高层次来看，Ibis 是一个 DataFrame 的“前端”，通过一个通用的
    API，你可以查询多个“后端”。
- en: 'To help understand what that means, it is worth contrasting that with the design
    approach of pandas. In pandas, the API or “frontend” for a group by and a sum
    looks like this:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助理解这一点，值得与 pandas 的设计方法进行对比。在 pandas 中，进行分组和求和的 API 或“前端”看起来是这样的：
- en: '[PRE48]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'In Ibis, a similar expression would look like this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Ibis 中，类似的表达式看起来是这样的：
- en: '[PRE49]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: While the API exposed to the user may not be all that different, the similarities
    between Ibis and pandas stop there. Ibis does not dictate how you store the data
    you are querying; it can be stored in BigQuery, DuckDB, MySQL, PostgreSQL, etc.,
    and it can be even stored in another DataFrame library like pandas. Beyond the
    storage, Ibis does not dictate how summation should be performed; instead, it
    leaves it to an execution engine. Many SQL databases have their own execution
    engine, but others may defer to third-party libraries like Apache DataFusion ([https://datafusion.apache.org/](https://datafusion.apache.org/)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管暴露给用户的 API 可能没有太大不同，但 Ibis 和 pandas 之间的相似性就此为止。Ibis 不规定你查询的数据应该如何存储；它可以存储在
    BigQuery、DuckDB、MySQL、PostgreSQL 等数据库中，甚至可以存储在像 pandas 这样的其他 DataFrame 库中。除了存储之外，Ibis
    也不规定如何执行求和操作；它将这一过程交给执行引擎。许多 SQL 数据库有自己的执行引擎，但也有一些可能会依赖于像 Apache DataFusion ([https://datafusion.apache.org/](https://datafusion.apache.org/))
    这样的第三方库。
- en: 'To use a `pd.DataFrame` through Ibis, you will need to wrap it with the `ibis.memtable`
    function:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过 Ibis 使用 `pd.DataFrame`，你需要使用 `ibis.memtable` 函数将其包装起来：
- en: '[PRE50]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'With that out of the way, you can then start to query the data just as you
    would with pandas but using the Ibis API:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，你可以像使用 pandas 一样，使用 Ibis API 查询数据：
- en: '[PRE51]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: It is worth noting that the preceding code does not actually return a result.
    Unlike pandas, which executes all of the operations you give it *eagerly*, Ibis
    collects all of the expressions you want and waits to perform execution until
    explicitly required. This practice is commonly called *deferred* or *lazy* execution.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，前面的代码实际上并没有返回结果。与 pandas 不同，pandas 会“急切地”执行你给它的所有操作，而 Ibis 会收集你想要的所有表达式，并且直到明确要求时才执行。这种做法通常被称为
    *推迟执行* 或 *惰性执行*。
- en: The advantage of deferring is that Ibis can find ways to optimize the query
    that you are telling it to perform. Our query is asking Ibis to find all rows
    where the make is Honda and then select a few columns, but it might be faster
    for the underlying database to select the columns first and then perform the filter.
    How that works is abstracted from the end user; users are just required to tell
    Ibis what they want and Ibis takes care of how to retrieve that data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 推迟执行的优势在于 Ibis 可以找到优化你请求执行的查询的方法。我们的查询要求 Ibis 查找所有“make”为 Honda 的行，并选择几个列，但对于底层数据库来说，先选择列再执行过滤可能会更快。这一过程对最终用户是透明的；用户只需要告诉
    Ibis 他们需要什么，Ibis 会处理如何获取这些数据。
- en: 'To materialize this back into a `pd.DataFrame`, you can chain in a call to
    `.to_pandas`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要将其转化为 `pd.DataFrame`，可以链式调用 `.to_pandas`：
- en: '[PRE53]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'However, you are not required to return a `pd.DataFrame`. If you wanted a PyArrow
    table instead, you could opt for `.to_pyarrow`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你并不一定要返回 `pd.DataFrame`。如果你想要一个 PyArrow 表格，完全可以选择 `.to_pyarrow`：
- en: '[PRE55]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: For more information on Ibis, be sure to check out the Ibis documentation. There
    is even an Ibis tutorial aimed specifically at users coming from pandas.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Ibis 的更多信息，请务必查看 Ibis 文档。甚至有一个专门面向来自 pandas 用户的 Ibis 教程。
- en: Dask
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask
- en: Another popular library that has a history closely tied to pandas is Dask. Dask
    is a framework that provides a similar API to the `pd.DataFrame` but scales its
    usage to parallel computations and datasets that exceed the amount of memory available
    on your system.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与 pandas 紧密相关的流行库是 Dask。Dask 是一个框架，它提供了与 `pd.DataFrame` 类似的 API，但将其使用扩展到并行计算和超出系统可用内存的数据集。
- en: 'If we wanted to convert our vehicles dataset to a Dask DataFrame, we can use
    the `dask.dataframe.from_pandas` function with a `npartitions=` argument that
    controls how to divide up the dataset:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将车辆数据集转换为 Dask DataFrame，可以使用 `dask.dataframe.from_pandas` 函数，并设置 `npartitions=`
    参数来控制如何划分数据集：
- en: '[PRE57]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: By splitting your DataFrame into different partitions, Dask allows you to perform
    computations against each partition in parallel, which can help immensely with
    performance and scalability.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 DataFrame 划分为不同的分区，Dask 允许你对每个分区并行执行计算，这对性能和可扩展性有极大的帮助。
- en: 'Much like Ibis, Dask performs calculations lazily. If you want to force a calculation,
    you will want to call the `.compute` method:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 Ibis 一样，Dask 也懒惰地执行计算。如果你想强制执行计算，你需要调用 `.compute` 方法：
- en: '[PRE59]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'To go from a Dask DataFrame back to pandas, simply call `ddf.compute`:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 Dask DataFrame 转回 pandas，只需调用 `ddf.compute`：
- en: '[PRE61]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Polars
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Polars
- en: Polars is a newcomer to the DataFrame space and has developed impressive features
    and a dedicated following in a very short amount of time. The Polars library is
    Apache Arrow native, so it has a much cleaner type system and consistent missing
    value handling than what pandas offers today (for the history of the pandas type
    system and all of its flaws, be sure to give *Chapter 3*, *Data Types*, a good
    read).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Polars 是 DataFrame 领域的新秀，并在非常短的时间内开发出了令人印象深刻的功能，并拥有了一群忠实的追随者。Polars 库是 Apache
    Arrow 原生的，因此它拥有比 pandas 当前提供的更清晰的类型系统和一致的缺失值处理（关于 pandas 类型系统及其所有缺陷的历史，请务必阅读 *第
    3 章*，*数据类型*）。
- en: In addition to a simpler and cleaner type system, Polars can scale to datasets
    that are larger than memory, and it even offers a lazy execution engine coupled
    with a query optimizer that can make it easier to write performant, scalable code.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 除了更简洁、更清晰的类型系统外，Polars 还能够扩展到大于内存的数据集，它甚至提供了一个懒执行引擎，并配备了查询优化器，使得编写高效、可扩展的代码变得更加容易。
- en: 'For a naive conversion from pandas to Polars, you can use `polars.from_pandas`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从 pandas 到 Polars 的简单转换，你可以使用 `polars.from_pandas`：
- en: '[PRE63]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'For lazy execution, you will want to try out the `pl.LazyFrame`, which can
    take the `pd.DataFrame` directly as an argument:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于懒执行，你可能会想试试 `pl.LazyFrame`，它可以直接将 `pd.DataFrame` 作为参数：
- en: '[PRE65]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Much like we saw with Ibis, the lazy execution engine of Polars can take care
    of optimizing the best path for doing a filter and select. To execute the plan,
    you will need to chain in a call to `pl.LazyFrame.collect`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在 Ibis 中看到的那样，Polars 的懒执行引擎可以优化执行筛选和选择操作的最佳路径。要执行计划，你需要将 `pl.LazyFrame.collect`
    链接起来：
- en: '[PRE66]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'If you would like to convert back to pandas from Polars, both the `pl.DataFrame`
    and `pl.LazyFrame` offer a `.to_pandas` method:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想从 Polars 转回 pandas，`pl.DataFrame` 和 `pl.LazyFrame` 都提供了 `.to_pandas` 方法：
- en: '[PRE68]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: For a more detailed look at Polars and all of the great things it has to offer,
    I suggest checking out the *Polars Cookbook* ([https://www.packtpub.com/en-us/product/polars-cookbook-9781805121152](https://www.packtpub.com/en-us/product/polars-cookbook-9781805121152)).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 若想更详细地了解 Polars 及其所有优秀的功能，建议查看 *Polars Cookbook* ([https://www.packtpub.com/en-us/product/polars-cookbook-9781805121152](https://www.packtpub.com/en-us/product/polars-cookbook-9781805121152))。
- en: cuDF
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: cuDF
- en: 'If you have a Nvidia device and the CUDA toolkit available to you, you may
    also be interested in cuDF. In theory, cuDF is a “drop-in” replacement for pandas;
    as long as you have the right hardware and tooling, it will take your pandas expressions
    and run them on your GPU, simply by importing cuDF before pandas:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你拥有 Nvidia 设备并且已经安装了 CUDA 工具包，你可能会对 cuDF 感兴趣。理论上，cuDF 是 pandas 的“即插即用”替代品；只要你有合适的硬件和工具，就可以将
    pandas 表达式运行在你的 GPU 上，只需在 pandas 之前导入 cuDF：
- en: '[PRE70]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Given the power of modern GPUs compared to CPUs, this library can offer users
    a significant performance boost without having to change the way code is written.
    For the right users with the right hardware, that type of out-of-the-box performance
    boost can be invaluable.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于现代 GPU 相较于 CPU 的强大，这个库能够为用户提供显著的性能提升，而无需改变代码的编写方式。对于拥有合适硬件的用户来说，这种即插即用的性能提升可能是无价的。
- en: Join our community on Discord
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的社区 Discord
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/pandas](https://packt.link/pandas)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/pandas](https://packt.link/pandas)'
- en: '![](img/QR_Code5040900042138312.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5040900042138312.png)'
- en: Leave a Review!
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留下评价！
- en: Thank you for purchasing this book from Packt Publishing—we hope you enjoyed
    it! Your feedback is invaluable and helps us improve and grow. Please take a moment
    to leave an [Amazon review](Chapter_11.xhtml); it will only take a minute, but
    it makes a big difference for readers like you.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你购买这本由 Packt Publishing 出版的书籍——我们希望你喜欢它！你的反馈非常宝贵，能够帮助我们改进和成长。请花一点时间在 [Amazon
    上留下评论](Chapter_11.xhtml)，这只需要一分钟，但对像你这样的读者来说意义重大。
- en: Scan the QR code below to receive a free ebook of your choice.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描下方的二维码，领取你选择的免费电子书。
- en: '![](img/QR_Code1474021820358918656.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1474021820358918656.png)'
- en: '[https://packt.link/NzOWQ](https://packt.link/NzOWQ)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/NzOWQ](https://packt.link/NzOWQ)'
- en: '![](img/New_Packt_Logo1.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/New_Packt_Logo1.png)'
- en: '[packt.com](https://www.packt.com)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[packt.com](https://www.packt.com)'
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 订阅我们的在线数字图书馆，全面访问超过7,000本书籍和视频，及行业领先的工具，帮助你规划个人发展并推进职业生涯。欲了解更多信息，请访问我们的网站。
- en: Why subscribe?
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么订阅？
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过来自4000多位行业专家的实用电子书和视频，减少学习时间，增加编程时间
- en: Improve your learning with Skill Plans built especially for you
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过专为你打造的技能计划，提升你的学习效果
- en: Get a free eBook or video every month
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每月免费获得一本电子书或视频
- en: Fully searchable for easy access to vital information
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全可搜索，便于快速访问关键信息
- en: Copy and paste, print, and bookmark content
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制、粘贴、打印和收藏内容
- en: At [www.packt.com](https://www.packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在[www.packt.com](https://www.packt.com)上，你还可以阅读一系列免费的技术文章，注册各种免费的电子邮件通讯，并获得Packt图书和电子书的独家折扣和优惠。
- en: Other Books You May Enjoy
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你可能会喜欢的其他书籍
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这本书，你可能会对Packt出版的这些其他书籍感兴趣：
- en: '[![](img/978-1-80181-931-2.png)](https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/978-1-80181-931-2.png)](https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312)'
- en: '**Machine Learning with PyTorch and Scikit-Learn**'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习与PyTorch和Scikit-Learn**'
- en: Sebastian Raschka
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 塞巴斯蒂安·拉什卡（Sebastian Raschka）
- en: Yuxi (Hayden) Liu
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 刘宇熙（Hayden Liu）
- en: Vahid Mirjalili
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 瓦希德·米尔贾利利（Vahid Mirjalili）
- en: 'ISBN: 978-1-80181-931-2'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ISBN：978-1-80181-931-2
- en: Explore frameworks, models, and techniques for machines to learn from data
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索机器学习的数据框架、模型和技术
- en: Use scikit-learn for machine learning and PyTorch for deep learning
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行机器学习，使用PyTorch进行深度学习
- en: Train machine learning classifiers on images, text, and more
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图像、文本等数据上训练机器学习分类器
- en: Build and train neural networks, transformers, and boosting algorithms
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建并训练神经网络、变换器和提升算法
- en: Discover best practices for evaluating and tuning models
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现评估和调优模型的最佳实践
- en: Predict continuous target outcomes using regression analysis
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用回归分析预测连续目标结果
- en: Dig deeper into textual and social media data using sentiment analysis
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过情感分析深入挖掘文本和社交媒体数据
- en: '[![](img/978-1-80323-291-1.png)](https://www.packtpub.com/en-us/product/deep-learning-with-tensorflow-and-keras-3rd-edition-9781803232911)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/978-1-80323-291-1.png)](https://www.packtpub.com/en-us/product/deep-learning-with-tensorflow-and-keras-3rd-edition-9781803232911)'
- en: '**Deep Learning with TensorFlow and Keras, Third Edition**'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习与TensorFlow和Keras（第三版）**'
- en: Amita Kapoor
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 阿米塔·卡普尔（Amita Kapoor）
- en: Antonio Gulli
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 安东尼奥·古利（Antonio Gulli）
- en: Sujit Pal
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 苏吉特·帕尔（Sujit Pal）
- en: 'ISBN: 978-1-80323-291-1'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ISBN：978-1-80323-291-1
- en: Learn how to use the popular GNNs with TensorFlow to carry out graph mining
    tasks
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用流行的GNNs与TensorFlow进行图形挖掘任务
- en: Discover the world of transformers, from pretraining to fine-tuning to evaluating
    them
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索变换器的世界，从预训练到微调再到评估
- en: Apply self-supervised learning to natural language processing, computer vision,
    and audio signal processing
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将自监督学习应用于自然语言处理、计算机视觉和音频信号处理
- en: Combine probabilistic and deep learning models using TensorFlow Probability
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow概率结合概率模型和深度学习模型
- en: Train your models on the cloud and put TF to work in real environments
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在云端训练你的模型，并在真实环境中应用TF
- en: Build machine learning and deep learning systems with TensorFlow 2.x and the
    Keras API
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow 2.x和Keras API构建机器学习和深度学习系统
- en: '[![](img/978-1-78934-641-1.png)](https://www.packtpub.com/en-us/product/machine-learning-for-algorithmic-trading-9781839217715)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/978-1-78934-641-1.png)](https://www.packtpub.com/en-us/product/machine-learning-for-algorithmic-trading-9781839217715)'
- en: '**Machine Learning for Algorithmic Trading, Second Edition**'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法交易的机器学习（第二版）**'
- en: Stefan Jansen
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 斯特凡·詹森（Stefan Jansen）
- en: 'ISBN: 978-1-83921-771-5'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ISBN：978-1-83921-771-5
- en: Leverage market, fundamental, and alternative text and image data
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用市场、基本面和替代性文本与图像数据
- en: Research and evaluate alpha factors using statistics, Alphalens, and SHAP values
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用统计学、Alphalens和SHAP值研究和评估alpha因子
- en: Implement machine learning techniques to solve investment and trading problems
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现机器学习技术解决投资和交易问题
- en: Backtest and evaluate trading strategies based on machine learning using Zipline
    and Backtrader
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Zipline和Backtrader回测并评估机器学习的交易策略
- en: Optimize portfolio risk and performance analysis using pandas, NumPy, and pyfolio
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用pandas、NumPy和pyfolio优化投资组合的风险和表现分析
- en: Create a pairs trading strategy based on cointegration for US equities and ETFs
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于协整为美国股票和ETF创建配对交易策略
- en: Train a gradient boosting model to predict intraday returns using AlgoSeek s
    high-quality trades and quotes data
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AlgoSeek的高质量交易和报价数据训练梯度提升模型，以预测日内收益
- en: Packt is searching for authors like you
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Packt正在寻找像你这样的作者
- en: If you’re interested in becoming an author for Packt, please visit [authors.packtpub.com](https://authors.packtpub.com)
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣成为Packt的作者，请访问[authors.packtpub.com](https://authors.packtpub.com)并立即申请。我们与成千上万的开发者和技术专业人士合作，帮助他们与全球技术社区分享他们的见解。你可以提交一个通用申请，申请我们正在招募的具体热门话题，或者提交你自己的想法。
- en: Join our community on Discord
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们在Discord上的社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/pandas](Other_Books_You_May_Enjoy.xhtml)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/pandas](Other_Books_You_May_Enjoy.xhtml)'
- en: '![](img/QR_Code5040900042138312.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5040900042138312.png)'
