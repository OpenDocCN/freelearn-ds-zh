- en: '3'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '3'
- en: Web Scraping and Interactive Visualizations
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网页抓取和交互式可视化
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Describe how HTTP requests work
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述 HTTP 请求的工作原理
- en: Scrape tabular data from a web page
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网页抓取表格数据
- en: Build and transform Pandas DataFrames
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和转换 Pandas DataFrame
- en: Create interactive visualizations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建交互式可视化
- en: In this chapter, you will learn the fundamentals of HTTP requests, scrape web
    page data, and then create interactive visualizations using the Jupyter Notebook.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习 HTTP 请求的基本原理，抓取网页数据，然后使用 Jupyter Notebook 创建交互式可视化。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: So far in this book, we have focused on using Jupyter to build reproducible
    data analysis pipelines and predictive models. We'll continue to explore these
    topics in this chapter, but the main focus here is data acquisition. In particular,
    we will show you how data can be acquired from the web using HTTP requests. This
    will involve scraping web pages by requesting and parsing HTML. We will then wrap
    up this chapter by using interactive visualization techniques to explore the data
    we've collected.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们主要集中在使用 Jupyter 构建可重复的数据分析管道和预测模型。本章我们将继续探讨这些话题，但这里的主要重点是数据获取。特别是，我们将展示如何使用
    HTTP 请求从互联网上获取数据。这将涉及通过请求和解析 HTML 来抓取网页。接着，我们将通过使用交互式可视化技术来探索我们收集的数据，作为本章的总结。
- en: The amount of data available online is huge and relatively easy to acquire.
    It's also continuously growing and becoming increasingly important. Part of this
    continual growth is the result of an ongoing global shift from newspapers, magazines,
    and TV to online content. With customized newsfeeds available all the time on
    cell phones, and live-news sources such as Facebook, Reddit, Twitter, and YouTube,
    it's difficult to imagine the historical alternatives being relevant much longer.
    Amazingly, this accounts for only some of the increasingly massive amounts of
    data available online.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 网上可用的数据量巨大，且相对容易获取。而且，这些数据还在不断增长，变得越来越重要。持续增长的一部分是全球从报纸、杂志和电视转向在线内容的结果。随着定制化的新闻源随时可通过手机获取，还有
    Facebook、Reddit、Twitter 和 YouTube 等实时新闻来源，很难想象历史上的替代方案还能再持续多久。令人惊讶的是，这仅仅是互联网上越来越庞大数据量的一部分。
- en: With this global shift toward consuming content using HTTP services (blogs,
    news sites, Netflix, and so on), there are plenty of opportunities to use data-driven
    analytics. For example, Netflix looks at the movies a user watches and predicts
    what they will like. This prediction is used to determine the suggested movies
    that appear. In this chapter, however, we won't be looking at "business-facing"
    data as such, but instead we will see how the client can leverage the internet
    as a database. Never before has this amount and variety of data been so easily
    accessible. We'll use web-scraping techniques to collect data, and then we'll
    explore it with interactive visualizations in Jupyter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着全球向使用 HTTP 服务（博客、新闻网站、Netflix 等）消费内容的转变，使用数据驱动分析的机会越来越多。例如，Netflix 会根据用户观看的电影预测他们喜欢什么。这些预测会用来决定推荐的电影。在本章中，我们不会讨论“面向商业”的数据，而是将看到客户端如何将互联网作为数据库来利用。以前从未有过如此多样和丰富的数据能如此轻松地获取。我们将使用网页抓取技术收集数据，并在
    Jupyter 中通过交互式可视化进行探索。
- en: Interactive visualization is a visual form of data representation, which helps
    users understand the data using graphs or charts. Interactive visualization helps
    a developer or analyst present data in a simple form, which can be understood
    by non-technical personnel too.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式可视化是一种数据表现形式，帮助用户通过图表或图形理解数据。交互式可视化帮助开发者或分析师将数据以简单的形式呈现，使非技术人员也能理解。
- en: Scraping Web Page Data
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网页数据抓取
- en: In the spirit of leveraging the internet as a database, we can think about acquiring
    data from web pages either by scraping content or by interfacing with web APIs.
    Generally, scraping content means getting the computer to read data that was intended
    to be displayed in a human-readable format. This is in contradistinction to web
    APIs, where data is delivered in machine-readable formats—the most common being
    JSON.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本着将互联网作为数据库的精神，我们可以通过抓取网页内容或与 Web API 接口来获取数据。通常，抓取内容意味着让计算机读取原本是为了人类可读格式显示的数据。这与
    Web API 相对，后者是以机器可读的格式传递数据——最常见的是 JSON。
- en: In this topic, we will focus on web scraping. The exact process for doing this
    will depend on the page and desired content. However, as we will see, it's quite
    easy to scrape anything we need from an HTML page so long as we have an understanding
    of the underlying concepts and tools. In this topic, we'll use Wikipedia as an
    example and scrape tabular content from an article. Then, we'll apply the same
    techniques to scrape
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本主题中，我们将重点讨论网页抓取。具体过程会根据页面和所需内容有所不同。然而，正如我们将看到的，只要我们理解底层的概念和工具，从 HTML 页面抓取任何所需内容其实是相当简单的。在本主题中，我们将使用
    Wikipedia 作为示例，从文章中抓取表格内容。然后，我们会将相同的技术应用于抓取其他内容。
- en: data from a page on an entirely separate domain. But first, we'll take some
    time to introduce HTTP requests.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个完全不同域名页面获取数据。但在此之前，我们将花时间介绍 HTTP 请求。
- en: Introduction to HTTP Requests
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HTTP 请求简介
- en: The Hypertext Transfer Protocol, or HTTP for short, is the foundation of data
    communication for the internet. It defines how a page should be requested and
    how the response should look. For example, a client can request an Amazon page
    of laptops for sale, a Google search of local restaurants, or their Facebook feed.
    Along with the URL, the request will contain the user agent and available browsing
    cookies among the contents of the request header. The user agent tells the server
    what browser and device the client is using, which is usually used to provide
    the most user-friendly version of the web page's response. Perhaps they have recently
    logged in to the web page; such information would be stored in a cookie that might
    be used to automatically log the user in.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 超文本传输协议（HTTP）简称为 HTTP，是互联网数据通信的基础。它定义了页面如何被请求以及响应应如何呈现。例如，客户端可以请求一个亚马逊的笔记本电脑销售页面、一个谷歌本地餐厅搜索，或者他们的
    Facebook 动态。除了 URL 外，请求还会包含用户代理和可用的浏览器 cookies，位于请求头部内容中。用户代理告诉服务器客户端使用的是哪种浏览器和设备，通常用于提供最符合用户需求的网页响应版本。也许他们最近登录了网页，这类信息会保存在
    cookie 中，用于自动登录用户。
- en: These details of HTTP requests and responses are taken care of under the hood
    thanks to web browsers. Luckily for us, today the same is true when making requests
    with high-level languages such as Python. For many purposes, the contents of request
    headers can be largely ignored. Unless otherwise specified, these are automatically
    generated in Python when requesting a URL. Still, for the purposes of troubleshooting
    and understanding the responses yielded by our requests, it's useful to have a
    foundational understanding of HTTP.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 HTTP 请求和响应的细节是由 Web 浏览器在后台处理的。幸运的是，今天在使用 Python 等高级语言发起请求时，情况也是如此。对于许多用途，请求头的内容可以被大致忽略。除非另有指定，否则在
    Python 中请求 URL 时这些内容会自动生成。不过，为了故障排除和理解我们请求的响应结果，了解 HTTP 的基本概念是很有用的。
- en: There are many types of HTTP methods, such as GET, HEAD, POST, and PUT. The
    first two are used for requesting that data be sent from the server to the client,
    whereas the last two are used for sending data to the server.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP 方法有很多种类型，如 GET、HEAD、POST 和 PUT。前两个用于请求从服务器向客户端发送数据，而后两个则用于向服务器发送数据。
- en: Note
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Take a look at this GET request example for the `User-Agent` is Mozilla/5.0,
    which corresponds to a standard desktop browser. Among other lines in the header,
    we note the `Accept` and `Accept-Language` fields, which specify the acceptable
    content types and language of the response.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下这个 GET 请求示例，其中 `User-Agent` 为 Mozilla/5.0，代表标准的桌面浏览器。在请求头的其他字段中，我们注意到 `Accept`
    和 `Accept-Language` 字段，这些字段指定了响应的可接受内容类型和语言。
- en: 'These HTTP methods are summarized below:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 HTTP 方法的总结：
- en: '**GET**: Retrieves the information from the specified URL'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GET**：从指定 URL 获取信息'
- en: '**HEAD**: Retrieves the meta information from the HTTP header of the specified
    URL'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HEAD**：从指定 URL 的 HTTP 头部检索元信息'
- en: '**POST**: Sends the attached information for appending to the resource(s) at
    the specified URL'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**POST**：发送附加的信息以追加到指定 URL 上的资源'
- en: '**PUT**: Sends the attached information for replacing the resource(s) at the
    specified URL'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PUT**：发送附加的信息以替换指定 URL 上的资源'
- en: A**GET** request is sent each time we type a web page address into our browser
    and press Enter. For web scraping, this is usually the only HTTP method we are
    interested in, and it's the only method we'll be using in this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们在浏览器中输入网页地址并按回车时，都会发送一个**GET**请求。对于网页抓取来说，这通常是我们唯一关心的 HTTP 方法，也是我们在本章中将使用的唯一方法。
- en: 'Once the request has been sent, a variety of response types can be returned
    from the server. These are labeled with 100-level to 500-level codes, where the
    first digit in the code represents the response class. These can be described
    as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦请求被发送，服务器可能会返回多种响应类型。这些响应会用 100 到 500 级的代码进行标记，其中代码的第一个数字表示响应类别。可以按以下方式描述这些类别：
- en: '**1xx**: Informational response, for example, server is processing a request.
    It''s uncommon to see this.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1xx**: 信息性响应，例如，服务器正在处理请求。通常很少看到这种情况。'
- en: '**2xx**: Success, for example, page has loaded properly.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2xx**: 成功，例如，页面已正确加载。'
- en: '**3xx**: Redirection, for example, the requested resource has been moved and
    we were redirected to a new URL.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3xx**: 重定向，例如，请求的资源已被移动，我们被重定向到了一个新的 URL。'
- en: '**4xx**: Client error, for example, the requested resource does not exist.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4xx**: 客户端错误，例如，请求的资源不存在。'
- en: '**5xx**: Server error, for example, the website server is receiving too much
    traffic and could not fulfill the request.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5xx**: 服务器错误，例如，网站服务器接收到了过多的流量，无法处理请求。'
- en: For the purposes of web scraping, we usually only care about the response class,
    that is, the first digit of the response code. However, there exist subcategories
    of responses within each class that offer more granularity on what's going on.
    For example, a 401 code indicates an unauthorized response, whereas a 404 code
    indicates a page not found response. This distinction is noteworthy because a
    404 would indicate we've requested a page that does not exist, whereas 401 tells
    us we need to log in to view the particular resource.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 出于网页抓取的目的，我们通常只关心响应类别，即响应代码的第一个数字。然而，每个类别内还存在响应的子类别，这些子类别提供了更精细的分类，帮助我们了解发生了什么。例如，401
    代码表示未授权响应，而 404 代码表示页面未找到响应。这一点很重要，因为 404 表示我们请求的页面不存在，而 401 则告诉我们需要登录才能查看该资源。
- en: Let's see how HTTP requests can be done in Python and explore some of these
    topics using the Jupyter Notebook.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在 Python 中发起 HTTP 请求，并在 Jupyter Notebook 中探索其中的一些主题。
- en: Making HTTP Requests in the Jupyter Notebook
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebook 中发起 HTTP 请求
- en: Now that we've talked about how HTTP requests work and what type of responses
    we should expect, let's see how this can be done in Python. We'll use a library
    called `urllib`, for making HTTP requests, but `urllib` in the official Python
    documentation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了 HTTP 请求的工作原理以及我们应该预期的响应类型，接下来让我们看看如何在 Python 中实现这一点。我们将使用一个名为 `urllib`
    的库来发起 HTTP 请求，但在官方的 Python 文档中，`urllib` 是一个不同的库。
- en: '**Requests** is a great choice for making simple and advanced web requests.
    It allows for all sorts of customization with respect to headers, cookies, and
    authorization. It tracks redirects and provides methods for returning specific
    page content such as JSON. Furthermore, there''s an extensive suite of advanced
    features. However, it does not allow JavaScript to be rendered.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**Requests** 是一个非常好的库，可以用于发起简单和复杂的网页请求。它允许对请求的头部、cookie 和授权进行各种定制。它还会跟踪重定向，并提供返回特定页面内容（如
    JSON）的方式。此外，它还具有大量的高级功能。然而，它不支持渲染 JavaScript。'
- en: Oftentimes, servers return HTML with JavaScript code snippets included, which
    are automatically run in the browser on load time. When requesting content with
    Python using Requests, this JavaScript code is visible, but it does not run. Therefore,
    any elements that would be altered or created by doing so are missing. Often,
    this does not affect the ability to get the desired information, but in some cases
    we may need to render the JavaScript in order to scrape the page properly. For
    doing this, we could use a library like Selenium.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，服务器返回包含 JavaScript 代码片段的 HTML，这些代码会在浏览器加载时自动运行。当使用 Python 的 Requests 库请求内容时，这些
    JavaScript 代码是可见的，但并不会执行。因此，任何通过执行 JavaScript 代码可以改变或创建的元素都会缺失。通常，这不会影响我们获取所需信息的能力，但在某些情况下，我们可能需要渲染
    JavaScript 才能正确抓取页面。为此，我们可以使用像 Selenium 这样的库。
- en: This has a similar API to the Requests library, but provides support for rendering
    JavaScript using web drivers. It can even run JavaScript commands on live pages,
    for example, to change the text color or scroll to the bottom of the page.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库的 API 与 Requests 库相似，但它提供了通过 web 驱动程序渲染 JavaScript 的支持。它甚至可以在实时页面上执行 JavaScript
    命令，例如，改变文本颜色或滚动到页面底部。
- en: Note
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information, refer to: [http://docs.python-requests.org/en/master/user/advanced/](http://docs.python-requests.org/en/master/user/advanced/)
    and [http://selenium-python.readthedocs.io/.](http://selenium-python.readthedocs.io/)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参考：[http://docs.python-requests.org/en/master/user/advanced/](http://docs.python-requests.org/en/master/user/advanced/)
    和 [http://selenium-python.readthedocs.io/.](http://selenium-python.readthedocs.io/)
- en: Let's dive into an exercise using the Requests library with Python in a Jupyter
    Notebook.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 Jupyter Notebook 中使用 Requests 库进行一个练习。
- en: 'Exercise 14: Handling HTTP Requests With Python in a Jupyter Notebook'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 14：在 Jupyter Notebook 中使用 Python 处理 HTTP 请求
- en: Start the `NotebookApp` from the project directory by executing jupyter notebook.
    Navigate to the `lesson-3` directory and open up the l`esson- 3-workbook.ipynb`
    file. Find the cell near the top where the packages are loaded and run it.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从项目目录启动`NotebookApp`，通过执行 jupyter notebook 命令。导航到`lesson-3`目录并打开`lesson-3-workbook.ipynb`文件。找到位于顶部的加载包的单元并运行它。
- en: We are going to request a web page and then examine the response object. There
    are many different libraries for making requests and many choices for exactly
    how to do so with each. We'll only use the Requests library, as it provides excellent
    documentation, advanced features, and a simple API.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将请求一个网页，然后检查响应对象。有许多不同的库可以用于发起请求，每个库有许多选择来实现具体的请求方法。我们只使用 Requests 库，因为它提供了优秀的文档、先进的功能和简单的
    API。
- en: 'Scroll down to `Subtopic A: Introduction to HTTP requests` and run the first
    cell in that section to import the Requests library. Then, prepare a request by
    running the cell containing the following code:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到`子主题 A：HTTP 请求简介`，并运行该部分中的第一个单元以导入 Requests 库。然后，通过运行包含以下代码的单元来准备请求：
- en: '[PRE0]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We use the Request class to prepare a GET request to the jupyter.org homepage.
    By specifying the user agent as Mozilla/5.0, we are asking for a response that
    would be suitable for a standard desktop browser. Finally, we prepare the request.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用 Request 类准备一个 GET 请求，访问 jupyter.org 的首页。通过将用户代理指定为 Mozilla/5.0，我们请求返回适合标准桌面浏览器的响应。最后，我们准备请求。
- en: 'Print the docstring for the "prepared request" req, by running the cell containing `req?`:![
    Figure 3.1: Printing the docstring for req'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含`req?`的单元，打印“已准备的请求”req 的文档字符串：![图 3.1：打印 req 的文档字符串]
- en: '](img/C13018_03_01.jpg)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_01.jpg)'
- en: 'Figure 3.1: Printing the docstring for req'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.1：打印 req 的文档字符串
- en: Looking at its usage, we see how the request can be sent using a session. This
    is similar to opening a web browser (starting a session) and then requesting a
    URL.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过查看它的使用方式，我们可以看到如何使用会话发送请求。这类似于打开一个网页浏览器（启动会话），然后请求一个 URL。
- en: 'Make the request and store the response in a variable named page, by running
    the following code:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码发起请求，并将响应存储在名为 page 的变量中：
- en: '[PRE1]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This code returns the HTTP response, as referenced by the page variable. By
    using the `with` statement, we initialize a session whose scope is limited to
    the indented code block. This means we do not have to worry about explicitly closing
    the session, as it is done automatically.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码返回 HTTP 响应，如页面变量所引用的那样。通过使用`with`语句，我们初始化一个会话，其作用域仅限于缩进的代码块。这意味着我们无需显式关闭会话，因为它会自动完成。
- en: Run the next two cells in the notebook to investigate the response. The string
    representation of page should indicate a 200 status code response. This should
    agree with the `status_code` attribute.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 notebook 中的接下来的两个单元，调查响应。页面的字符串表示应该显示 200 状态码响应。这应该与`status_code`属性一致。
- en: 'Save the response text to the `page_html` variable and take a look at the head
    of the string with `page_html[:1000]`:![Figure 3.2: The HTML response text](img/C13018_03_02.jpg)'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将响应文本保存到`page_html`变量，并使用`page_html[:1000]`查看字符串的头部：![图 3.2：HTML 响应文本](img/C13018_03_02.jpg)
- en: 'Figure 3.2: The HTML response text'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.2：HTML 响应文本
- en: As expected, the response is HTML. We can format this output better with the
    help of `BeautifulSoup`, a library which will be used extensively for HTML parsing
    later in this section.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如预期的那样，响应是 HTML 格式。我们可以借助`BeautifulSoup`，一个将在本节后续部分广泛用于 HTML 解析的库，来更好地格式化此输出。
- en: 'Print the head of the formatted HTML by running the following:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令打印格式化后的 HTML 头部：
- en: '[PRE2]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We import `BeautifulSoup` and then print the output, where newlines are indented
    depending on their hierarchy in the HTML structure.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们导入`BeautifulSoup`，然后打印输出，其中新行根据它们在 HTML 结构中的层级进行缩进。
- en: 'We can take this a step further and actually display the HTML in Jupyter by
    using the IPython display module. Do this by running the following code:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以进一步通过使用 IPython 显示模块，实际上在 Jupyter 中显示 HTML。通过运行以下代码来实现：
- en: '[PRE3]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Figure 3.3: The output obtained when no images are loaded'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.3：未加载图像时获得的输出'
- en: '](img/C13018_03_03.jpg)'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_03.jpg)'
- en: 'Figure 3.3: The output obtained when no images are loaded'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.3：未加载图像时获得的输出
- en: 'Let''s compare this to the live website, which can be opened in Jupyter using
    an IFrame. Do this by running the following code:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将其与可以在 Jupyter 中通过 IFrame 打开的实时网站进行比较。通过运行以下代码来实现：
- en: '[PRE4]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure 3.4: Rendering of the entire Jupyter website'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.4：整个 Jupyter 网站的渲染'
- en: '](img/C13018_03_04.jpg)'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_04.jpg)'
- en: 'Figure 3.4: Rendering of the entire Jupyter website'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.4：整个 Jupyter 网站的渲染
- en: Here, we see the full site rendered, including JavaScript and external resources.
    In fact, we can even click on the hyperlinks and load those pages in the IFrame,
    just like a regular browsing session.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到完整的站点渲染，包括 JavaScript 和外部资源。事实上，我们甚至可以点击超链接，并像常规浏览会话一样在 IFrame 中加载这些页面。
- en: It's good practice to close the IFrame after using it. This prevents it from
    eating up memory and processing power. It can be closed by selecting the cell
    and clicking **Current Outputs** | **Clear** from the Cell menu in the Jupyter
    Notebook.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用完 IFrame 后，最好将其关闭。这可以防止它占用内存和处理能力。可以通过选择单元格并从 Jupyter Notebook 中的单元格菜单点击 **当前输出**
    | **清除** 来关闭它。
- en: Recall how we used a prepared request and session to request this content as
    a string in Python. This is often done using a shorthand method instead. The drawback
    is that we do not have as much customization of the request header, but that's
    usually fine.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回想一下我们是如何使用预设请求和会话将此内容作为字符串请求到 Python 中的。通常也可以使用简写方法来完成此操作。缺点是我们不能对请求头进行太多自定义，但通常这没问题。
- en: 'Make a request to [http://www.python.org/](http://www.python.org/) by running
    the following code:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码向[http://www.python.org/](http://www.python.org/)发送请求：
- en: '[PRE5]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The string representation of the page (as displayed beneath the cell) should
    indicate a 200 status code, indicating a successful response.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 页面（如单元格下方显示的字符串表示）应该指示一个 200 状态码，表示成功响应。
- en: Run the next two cells. Here, we print the `url` and history attributes of our
    page.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行接下来的两个单元格。在这里，我们打印页面的 `url` 和历史记录属性。
- en: The URL returned is not what we input; notice the difference? We were redirected
    from the input URL, [http://www.python.org/,](http://www.python.org/) to the secured
    version of that page, [https://www.python.org/](http://www.python.org/). The difference
    is indicated by an additional s at the start of the URL, in the protocol. Any
    redirects are stored in the history attribute; in this case, we find one page
    in here with status code 301 (permanent redirect), corresponding to the original
    URL requested.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 返回的 URL 不是我们输入的 URL；注意其中的差异吗？我们从输入的 URL，[http://www.python.org/，](http://www.python.org/)
    被重定向到该页面的安全版本，[https://www.python.org/](http://www.python.org/)。差异体现在协议中的 URL
    开头多了一个 s。任何重定向都会保存在历史记录属性中；在这种情况下，我们可以在历史记录中找到一个状态码为 301（永久重定向）的页面，对应于原始请求的 URL。
- en: Now that we're comfortable making requests, we'll turn our attention to parsing
    the HTML. This can be something of an art, as there are usually multiple ways
    to approach it, and the best method often depends on the details of the specific
    HTML in question.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了请求的过程，接下来我们将关注 HTML 解析。这可能有些艺术性，因为通常有多种方法可以处理它，而最好的方法通常取决于具体 HTML 的细节。
- en: Parsing HTML in the Jupyter Notebook
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebook 中解析 HTML
- en: When scraping data from a web page, after making the request, we must extract
    the data from the response content. If the content is HTML, then the easiest way
    to do this is with a high-level parsing library such as Beautiful Soup. This is
    not to say it's the only way; in principle, it would be possible to pick out the
    data using regular expressions or Python string methods such as split, but pursuing
    either of these options would be an inefficient use of time and could easily lead
    to errors. Therefore, it's generally frowned upon and instead, the use of a trustworthy
    parsing tool is recommended.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当从网页抓取数据时，在发出请求后，我们必须从响应内容中提取数据。如果内容是 HTML，那么最简单的方法是使用高级解析库，如 Beautiful Soup。并不是说这是唯一的方法；原则上，可以使用正则表达式或
    Python 字符串方法（如 split）来提取数据，但采用这些方法会低效且容易出错。因此，通常不推荐这样做，建议使用可靠的解析工具。
- en: In order to understand how content can be extracted from HTML, it's important
    to know the fundamentals of HTML. For starters, HTML stands for Hyper Text Markup
    Language. Like Markdown or XML (eXtensible Markup Language), it's simply a language
    for marking up text. In HTML, the display text is contained within the content
    section of HTML elements, where element attributes specify how that element should
    appear on the page.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解如何从 HTML 中提取内容，了解 HTML 的基本原理是很重要的。首先，HTML 代表超文本标记语言（Hyper Text Markup Language）。像
    Markdown 或 XML（可扩展标记语言）一样，它仅仅是用于标记文本的语言。在 HTML 中，显示的文本位于 HTML 元素的内容部分，而元素的属性指定该元素在页面上的显示方式。
- en: '![Figure 3.5: Fundamental blocks of HTML'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.5：HTML 的基本块'
- en: '](img/C13018_03_05.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13018_03_05.jpg)'
- en: 'Figure 3.5: Fundamental blocks of HTML'
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.5：HTML 的基本块
- en: Looking at the anatomy of an HTML element, as seen in the preceding picture,
    we see the content enclosed between start and end tags. In this example, the tags
    are `<p>` for paragraph; other common tag types are `<div>` (text block), `<table>`
    (data table),
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 看看 HTML 元素的结构，如上图所示，我们可以看到内容被包含在开始标签和结束标签之间。在这个例子中，标签是 `<p>`（段落）；其他常见的标签类型有
    `<div>`（文本块）、`<table>`（数据表格）。
- en: '`<h1>` (heading), `<img>` (image), and `<a>` (hyperlinks). Tags have attributes,
    which can hold important metadata. Most commonly, this metadata is used to specify
    how the element text should appear on the page. This is where CSS files come into
    play. The attributes can store other useful information, such as the hyperlink
    `href` in an `<a>` tag, which specifies a URL link, or the alternate alt label
    in an `<img>` tag, which specifies the text to display if the image resource cannot
    be loaded.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`<h1>`（标题）、`<img>`（图片）和 `<a>`（超链接）。标签有属性，这些属性可以存储重要的元数据。最常见的用途是指定元素文本在页面上的显示方式。这就是
    CSS 文件发挥作用的地方。属性还可以存储其他有用的信息，例如 `<a>` 标签中的超链接 `href`，它指定了一个 URL 链接，或者 `<img>`
    标签中的备用 alt 标签，它指定了当图像无法加载时显示的文本。'
- en: Now, let's turn our attention back to the Jupyter Notebook and parse some HTML!
    Although not necessary when following along with this exercise, it's very helpful
    in real-world situations to use the developer tools in Chrome or Firefox to help
    identify the HTML elements of interest. We'll include instructions for doing this
    with Chrome in the following exercise.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到 Jupyter Notebook，并开始解析一些 HTML！虽然在本练习中跟随操作时不必使用，但在实际应用中，使用 Chrome 或
    Firefox 的开发者工具来帮助识别感兴趣的 HTML 元素是非常有用的。在接下来的练习中，我们将为您提供如何使用 Chrome 的相关说明。
- en: 'Exercise 15: Parsing HTML With Python in a Jupyter Notebook'
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 15：在 Jupyter Notebook 中使用 Python 解析 HTML
- en: 'In `lesson-3-workbook.ipynb` file, scroll to the top of `Subtopic B: Parsing
    HTML` with Python.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在 `lesson-3-workbook.ipynb` 文件中，滚动至 `子主题 B: 使用 Python 解析 HTML` 的顶部。'
- en: In this exercise, we'll scrape the central bank interest rates for each country,
    as reported by Wikipedia. Before diving into the code, let's first open up the
    web page containing this data.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将从维基百科抓取各国的中央银行利率。在开始编码之前，我们先打开包含这些数据的网页。
- en: Go to [https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates](https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates)
    in a web browser. Use Chrome, if possible, as later in this exercise we'll show
    you how to view and search the HTML using Chrome's developer tools.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在网页浏览器中访问 [https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates](https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates)。如果可能的话，请使用
    Chrome，因为在接下来的练习中，我们将展示如何使用 Chrome 的开发者工具查看和搜索 HTML。
- en: Looking at the page, we see very little content other than a big list of countries
    and their interest rates. This is the table we'll be scraping.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从页面上看，我们几乎只能看到一个列出各国及其利率的大表格。这就是我们将要抓取的数据。
- en: 'Return to the Jupyter Notebook and load the HTML as a Beautiful Soup object
    so that it can be parsed. Do this by running the following code:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 Jupyter Notebook，将 HTML 加载为 Beautiful Soup 对象，以便解析。通过运行以下代码来完成此操作：
- en: '[PRE6]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We use Python's default html.parser as the parser, but third-party parsers such
    as `lxml` may be used instead, if desired. Usually, when working with a new object
    like this Beautiful Soup one, it's a good idea to pull up the docstring by doing
    `soup?`. However, in this case, the docstring is not particularly informative.
    Another tool for exploring Python objects is `pdir`, which lists all of an object's
    attributes and methods (this can be installed with pip install `pdir2`). It's
    basically a formatted version of Python's built-in `dir` function.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用 Python 默认的 html.parser 作为解析器，但如果需要，也可以使用 `lxml` 等第三方解析器。通常，在处理像 Beautiful
    Soup 这样的新对象时，最好通过 `soup?` 来查看文档字符串。然而，在这种情况下，文档字符串并没有提供太多有用的信息。另一个探索 Python 对象的工具是
    `pdir`，它列出了一个对象的所有属性和方法（可以通过 pip install `pdir2` 安装）。它基本上是 Python 内置 `dir` 函数的格式化版本。
- en: 'Display the attributes and methods for the BeautifulSoup object by running
    the following code. This will run, regardless of whether or not the `pdir` external
    library is installed:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码显示 BeautifulSoup 对象的属性和方法。无论是否安装了 `pdir` 外部库，都会运行：
- en: '[PRE7]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, we see a list of methods and attributes that can be called on soup. The
    most commonly used function is probably `find_all`, which returns a list of elements
    that match the given criteria.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们看到了一个可以在 soup 上调用的方法和属性的列表。最常用的函数可能是 `find_all`，它返回符合给定条件的元素列表。
- en: 'Get the h1 heading for the page with the following code:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下代码获取页面的 h1 标题：
- en: '[PRE8]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Usually, pages only have one H1 (top-level heading) element, so it's no surprise
    that we only find one here.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通常，页面上只有一个 H1（顶级标题）元素，所以我们只找到一个也就不奇怪了。
- en: 'Run the next couple of cells. We redefine H1 to the first (and only) list element
    with `h1 = h1[0]`, and then print out the HTML element attributes with `h1.attrs`:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行接下来的几个单元格。我们将 H1 重新定义为第一个（也是唯一的）列表元素，`h1 = h1[0]`，然后通过 `h1.attrs` 打印出 HTML
    元素的属性：
- en: We see the class and ID of this element, which can both be referenced by CSS
    code to define the style of this element.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们看到这个元素的类和 ID，可以通过 CSS 代码引用它们来定义该元素的样式。
- en: Get the HTML element content (that is, the visible text) by printing `h1.text`.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过打印 `h1.text` 获取 HTML 元素内容（即可见文本）。
- en: 'Get all the images on the page by running the following code:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码获取页面上的所有图片：
- en: '[PRE9]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There are lots of images on the page. Most of these are for the country flags.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 页面上有很多图片，其中大多数是国旗的图片。
- en: 'Print the source of each image by running the following code:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码打印每张图片的源代码：
- en: '[PRE10]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We use a list comprehension to iterate through the elements, selecting the `src`
    attribute of each (so long as that attribute is actually available).
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用列表推导来遍历元素，选择每个元素的 `src` 属性（只要该属性实际存在）。
- en: Now, let's scrape the table. We'll use Chrome's developer tools to hunt down
    the element this is contained within.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们来抓取表格。我们将使用 Chrome 的开发者工具来找出包含该元素的部分。
- en: '![Figure 3.6: Scraping the table on the target web page'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.6：抓取目标网页上的表格'
- en: '](img/C13018_03_06.jpg)'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_06.jpg)'
- en: 'Figure 3.6: Scraping the table on the target web page'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.6：抓取目标网页上的表格
- en: If not already done, open the Wikipedia page we're looking at in Chrome. Then,
    in the browser, select **Developer Tools** from the **View** menu. A sidebar will
    open. The HTML is available to look at from the **Elements** tab in Developer
    Tools.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果还没有完成，请在 Chrome 中打开我们正在查看的 Wikipedia 页面。然后，在浏览器中从 **查看** 菜单中选择 **开发者工具**。侧边栏将会打开。HTML
    可以从开发者工具的 **元素** 标签中查看。
- en: 'Select the little arrow in the top left of the tools sidebar. This allows us
    to hover over the page and see where the HTML element is located, in the **Elements**
    section of the sidebar:![Figure 3.7: Arrow Icon for locating the HTML element'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择工具侧边栏左上角的小箭头图标。这样，我们可以在页面上悬停，并看到 HTML 元素在侧边栏 **元素** 部分中的位置：![图 3.7：定位 HTML
    元素的箭头图标
- en: '](img/C13018_03_07.jpg)'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_07.jpg)'
- en: 'Figure 3.7: Arrow Icon for locating the HTML element'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.7：定位 HTML 元素的箭头图标
- en: 'Hover over the body to see how the table is contained within the div that has
    `id="bodyContent"`:![Figure 3.8: HTML code for table on the target web page'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 悬停在正文部分，查看表格如何被包含在 `id="bodyContent"` 的 div 中：![图 3.8：目标网页中表格的 HTML 代码
- en: '](img/C13018_03_08.jpg)'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_08.jpg)'
- en: 'Figure 3.8: HTML code for table on the target web page'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.8：目标网页中表格的 HTML 代码
- en: 'Select that `div` by running the following code:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码选择该 `div`：
- en: '[PRE11]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can now seek out the table within this subset of the full HTML. Usually,
    tables are organized into headers `<th>`, rows `<tr>`, and data entries `<td>`.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们现在可以在 HTML 的这一子集内查找表格。通常，表格按头部 `<th>`、行 `<tr>` 和数据条目 `<td>` 组织。
- en: 'Get the table headers by running the following code:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码获取表格头部：
- en: '[PRE12]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we see three headers. In the content of each is a break element `<br/>`,
    which will make the text a bit more difficult to cleanly parse.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们看到三个头部。每个头部的内容中都有一个换行元素 `<br/>`，这将使得文本清理起来有点困难。
- en: 'Get the text by running the following code:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码获取文本：
- en: '[PRE13]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we get the content with the `get_text` method, and then run the replace
    string method to remove the newline resulting from the `<br/>` element.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `get_text` 方法获取内容，然后运行替换字符串方法去除由于 `<br/>` 元素导致的换行符。
- en: To get the data, we'll first perform some tests and then scrape all the data
    in a single cell.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了获取数据，我们首先执行一些测试，然后将所有数据抓取到一个单元格中。
- en: 'Get the data for each cell in the second `<tr>` (row) element by running the
    following code:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码获取第二个 `<tr>`（行）元素中每个单元格的数据：
- en: '[PRE14]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We find all the row elements, pick out the third one, and then find the three
    data elements inside that.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们找到所有的行元素，挑选出第三行，然后在其中找到三个数据元素。
- en: Let's look at the resulting data and see how to parse the text from each row.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看结果数据，并了解如何从每一行中解析文本。
- en: 'Run the next couple of cells to print `d1` and its text attribute:![Figure
    3.9: Printing d1 and its text attribute'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行接下来的几个单元格，打印 `d1` 及其文本属性：[图 3.9：打印 d1 及其文本属性
- en: '](img/C13018_03_09.jpg)'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_09.jpg)'
- en: 'Figure 3.9: Printing d1 and its text attribute'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.9：打印 d1 及其文本属性
- en: We're getting some undesirable characters at the front. This can be solved by
    searching for only the text of the `<a>` tag.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在前面得到了一些不需要的字符。可以通过仅搜索 `<a>` 标签的文本来解决这个问题。
- en: Run `d1.find('a').text` to return the properly *cleaned* data for that cell.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`d1.find('a').text`来返回该单元格的正确*清洗*数据。
- en: Run the next couple of cells to print `d2` and its text. This data appears to
    be clean enough to convert directly into a float.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行接下来的几个单元格，打印 `d2` 及其文本。这个数据似乎已经足够干净，可以直接转换为浮动值。
- en: 'Run the next couple of cells to print `d3` and its text:![Figure 3.10: Printing
    d3 and its text attribute'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行接下来的几个单元格，打印 `d3` 及其文本：[图 3.10：打印 d3 及其文本属性
- en: '](img/C13018_03_10.jpg)'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_10.jpg)'
- en: 'Figure 3.10: Printing `d3` and its text attribute'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.10：打印 `d3` 及其文本属性
- en: Similar to `d1`, we see that it would be better to get only the span element's
    text.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与`d1`类似，我们发现最好只获取 `span` 元素的文本。
- en: 'Properly parse the date for this table entry by running the following code:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码正确解析该表格条目的日期：
- en: '[PRE15]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we''re ready to perform the full scrape by iterating over the row elements
    `<th>`. Run the following code:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备通过迭代行元素 `<th>` 来执行完整的抓取操作。运行以下代码：
- en: '[PRE16]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the complete code, refer to the following: [https://bit.ly/2EKMNbV](https://bit.ly/2EKMNbV).'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完整代码请参阅以下链接：[https://bit.ly/2EKMNbV](https://bit.ly/2EKMNbV)。
- en: We iterate over the rows, ignoring any that contain more than three data elements.
    These rows will not correspond to data in the table we are interested in. Rows
    that do have three data elements are assumed to be in the table, and we parse
    the text from these as identified during the testing.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们迭代这些行，忽略任何包含超过三个数据元素的行。这些行将不对应我们感兴趣的表格数据。那些包含三个数据元素的行则被认为是表格中的数据，我们按照测试时确定的方法解析这些文本。
- en: The text parsing is done inside a `try/except` statement, which will catch any
    errors and allow this row to be skipped without stopping the iteration. Any rows
    that raise errors due to this statement should be looked at. The data for these
    could be recorded manually or accounted for by altering the scraping loop and
    re-running it. In this case, we'll ignore any errors for the sake of time.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文本解析是在 `try/except` 语句内部完成的，这样可以捕捉到任何错误并跳过该行而不停止迭代。任何由于该语句而引发错误的行应当被查看。这些数据可以手动记录，或通过修改抓取循环并重新运行来解决。在这种情况下，为了节省时间，我们会忽略任何错误。
- en: 'Print the head of the scraped data list by running `print(data[:10])`:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行`print(data[:10])`来打印抓取数据列表的前十项：
- en: '[PRE17]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We''ll visualize this data later in the chapter. For now, save the data to
    a CSV file by running the following code:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们稍后会在本章中可视化这些数据。现在，通过运行以下代码将数据保存到 CSV 文件中：
- en: '[PRE18]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that we are using semicolons to separate the fields.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们使用分号来分隔字段。
- en: 'Activity 3: Web Scraping With Jupyter Notebooks'
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 3：使用 Jupyter Notebooks 进行网页抓取
- en: You should have completed the previous exercise in this chapter.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经完成了本章的前一个练习。
- en: In this activity, we are going to get the population of each country. Then,
    in the next topic, this will be visualized along with the interest rate data scraped
    in the previous exercise.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将获取每个国家的人口数据。接下来，在下一个话题中，这些数据将与前一个练习中抓取的利率数据一起进行可视化。
- en: 'The page we look at in this activity is available here: [http://www.worldometers.info/world-population/population-by-country/](http://www.worldometers.info/world-population/population-by-country/).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本活动中查看的页面可以通过以下链接访问：[http://www.worldometers.info/world-population/population-by-country/](http://www.worldometers.info/world-population/population-by-country/)。
- en: Our aim is to apply the basic of web scrapping to a new web page and scrape
    some more data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将网页抓取的基础应用到一个新网页，并抓取更多的数据。
- en: Note
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'This page may have changed since this document was created. If this URL no
    longer leads to a table of country populations, please use this Wikipedia page
    instead: [https://en.wikipedia.org/wiki/List_of_countries_by_population(United_Nations)](https://en.wikipedia.org/wiki/List_of_countries_by_population(United_Nations)).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本文档创建时该页面内容可能已经发生变化。如果该 URL 不再指向国家人口表格，请改用此 Wikipedia 页面：[https://en.wikipedia.org/wiki/List_of_countries_by_population(United_Nations)](https://en.wikipedia.org/wiki/List_of_countries_by_population(United_Nations))。
- en: 'In order to do this, the following steps have to be executed:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，需要执行以下步骤：
- en: Scrape the data from the web page.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从网页抓取数据。
- en: 'In the `lesson-3-workbook.ipynb` Jupyter Notebook, scroll to `Activity A: Web
    scraping with Python`.'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在`lesson-3-workbook.ipynb` Jupyter Notebook 中，滚动至`Activity A: 使用 Python 进行网页抓取`。'
- en: Set the `url` variable and load an IFrame of our page in the notebook.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`url`变量并在笔记本中加载我们的页面的 IFrame。
- en: Close the IFrame by selecting the cell and clicking **Current Outputs** | Clear
    from the **Cell** menu in the Jupyter Notebook.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选择单元格并点击 Jupyter Notebook 中 **当前输出** | 清除，从**单元格**菜单中清除 IFrame。
- en: Request the page and load it as a `BeautifulSoup` object.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求页面并将其加载为`BeautifulSoup`对象。
- en: Print the H1 for the page.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印页面的 H1 标签。
- en: Get and print the table headings.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取并打印表格的标题。
- en: Select first three columns and parse the text.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择前三列并解析文本。
- en: Get the data for a sample row.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取样本行的数据。
- en: How many columns of data do we have? Print the length of `row_data`.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有多少列数据？打印`row_data`的长度。
- en: Print the first elements of the row.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印行的前几个元素。
- en: Select the data elements d1, d2, and d3.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择数据元素 d1、d2 和 d3。
- en: Looking at the `row_data` output, we can find out how to correctly parse the
    data. Select the content of the `<a>` element in the first data element, and then
    simply get the text from the others.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看`row_data`输出，我们可以发现如何正确地解析数据。选择第一个数据元素中`<a>`元素的内容，然后直接从其他元素中获取文本。
- en: Scrape and parse the table data.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 抓取并解析表格数据。
- en: Print the head of the scraped data.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印抓取数据的头部。
- en: Finally, save the data to a CSV file for later use.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将数据保存为 CSV 文件以便后续使用。
- en: Note
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The detailed steps along with the solutions are presented in the *Appendix
    A*     (pg. no. 160).'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 详细的步骤及解决方案已在*附录 A*（第 160 页）中给出。
- en: To summarize, we've seen how Jupyter Notebooks can be used for web scraping.
    We started this chapter by learning about HTTP methods and status codes. Then,
    we used the Requests library to actually perform HTTP requests with Python and
    saw how the Beautiful Soup library can be used to parse the HTML responses.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们已经看到了如何使用 Jupyter Notebooks 进行网页抓取。我们从本章开始学习了 HTTP 方法和状态码。接着，我们使用 Requests
    库实际执行了 Python 中的 HTTP 请求，并看到了 Beautiful Soup 库如何用于解析 HTML 响应。
- en: Our Jupyter Notebook turned out to be a great tool for this type of work. We
    were able to explore the results of our web requests and experiment with various
    HTML parsing techniques. We were also able to render the HTML and even load a
    live version of the web page inside the notebook!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Jupyter Notebook 证明是进行这类工作的一个很好的工具。我们能够探索网页请求的结果，并实验各种 HTML 解析技术。我们还能够渲染
    HTML，甚至在笔记本内加载网页的实时版本！
- en: 'In the next topic of this chapter, we shift to a completely new topic: interactive
    visualizations. We''ll see how to create and display interactive charts right
    inside the notebook, and use these charts as a way to explore the data we have
    just collected.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的下一个主题中，我们将转向一个全新的话题：互动可视化。我们将学习如何在笔记本中创建并展示互动图表，并将这些图表用作探索我们刚刚收集的数据的方式。
- en: Interactive Visualizations
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 互动可视化
- en: Visualizations are quite useful as a means of extracting information from a
    dataset. For example, with a bar graph it's very easy to distinguish the value
    distribution, compared to looking at the values in a table. Of course, as we have
    seen earlier in this book, they can be used to study patterns in the dataset that
    would otherwise be quite difficult to identify. Furthermore, they can be used
    to help explain a dataset to an unfamiliar party. If included in a blog post,
    for example, they can boost reader interest levels and be used to break up blocks
    of text.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化是从数据集提取信息的有力工具。例如，通过柱状图，与查看表格中的值相比，很容易区分出值的分布。当然，正如我们在本书中之前看到的，它们还可以用来研究数据集中的模式，这些模式在其他情况下可能非常难以识别。此外，它们还可以帮助向不熟悉的人解释数据集。例如，如果包含在博客文章中，它们可以提高读者的兴趣并用来打破文本块。
- en: When thinking about interactive visualizations, the benefits are similar to
    static visualizations, but enhanced because they allow for active exploration
    on the viewer's part. Not only do they allow the viewer to answer questions they
    may have about the data, they also think of new questions while exploring. This
    can benefit a separate party such as a blog reader or co-worker, but also a creator,
    as it allows for easy ad hoc exploration of the data in detail, without having
    to change any code.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑交互式可视化时，其优点类似于静态可视化，但因为它们允许观众进行主动探索，所以更加增强了体验。它们不仅让观众能够回答自己对数据的疑问，同时在探索过程中还会激发出新的问题。这不仅能为独立方如博客读者或同事带来好处，也能帮助创作者，因为它允许在不需要修改任何代码的情况下轻松进行数据的临时详细探索。
- en: In this topic, we'll discuss and show how to use Bokeh to build interactive
    visualizations in Jupyter. Prior to this, however, we'll briefly revisit pandas
    DataFrames, which play an important role in doing data visualization with Python.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本主题中，我们将讨论并展示如何在 Jupyter 中使用 Bokeh 构建交互式可视化。然而，在此之前，我们将简要回顾 pandas DataFrame，它在使用
    Python 进行数据可视化时起着重要作用。
- en: Building a DataFrame to Store and Organize Data
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建 DataFrame 来存储和组织数据
- en: As we've seen time and time again in this book, pandas is an integral part of
    doing data science with Python and Jupyter Notebooks. DataFrames offer a way to
    organize and store labeled data, but more importantly, pandas provides time saving
    methods for transforming data within a DataFrame. Examples we have seen in this
    book include dropping duplicates, mapping dictionaries to columns, applying functions
    over columns, and filling in missing values.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中反复看到的，pandas 是使用 Python 和 Jupyter Notebook 进行数据科学不可或缺的一部分。DataFrame
    提供了一种组织和存储标记数据的方法，但更重要的是，pandas 提供了节省时间的处理方法，用于在 DataFrame 中转换数据。本书中我们已经看到的例子包括删除重复项、将字典映射到列、在列上应用函数和填充缺失值。
- en: With respect to visualizations, DataFrames offer methods for creating all sorts
    of matplotlib graphs, including `df.plot.barh()`, `df.plot.hist()`, and more.
    The interactive visualization library Bokeh previously relied on pandas DataFrames
    for their *high-level charts*. These worked similar to Seaborn, as we saw earlier
    in the previous chapter, where a DataFrame is passed to the plotting function
    along with the specific columns to plot. The most recent version of Bokeh, however,
    has dropped support for this behavior. Instead, plots are now created in much
    the same way as matplotlib, where the data can be stored in simple lists or NumPy
    arrays. The point of this discussion is that DataFrames are not entirely necessary,
    but still very helpful for organizing and manipulating the data prior to visualization.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 关于可视化，DataFrame 提供了创建各种 matplotlib 图形的方法，包括 `df.plot.barh()`、`df.plot.hist()`
    等。之前，交互式可视化库 Bokeh 依赖 pandas DataFrame 来生成其 *高级图表*。这些图表的工作方式类似于我们在上一章中看到的 Seaborn，即将
    DataFrame 传递给绘图函数，并指定要绘制的列。然而，Bokeh 的最新版本已不再支持这种行为。现在，图表的创建方式与 matplotlib 类似，数据可以存储在简单的列表或
    NumPy 数组中。讨论的重点是，DataFrame 并不是绝对必要的，但它仍然对在可视化前整理和处理数据非常有帮助。
- en: 'Exercise 16: Building and Merging Pandas DataFrames'
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 16：构建和合并 Pandas DataFrame
- en: Let's dive right into an exercise, where we'll continue working on the country
    data we scraped earlier. Recall that we extracted the central bank interest rates
    and populations of each country, and saved the results in CSV files. We'll load
    the data from these files and merge them into a DataFrame, which will then be
    used as the data source for the interactive visualizations to follow.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接进入一个练习，继续处理我们之前抓取的国家数据。回想一下，我们提取了各国的中央银行利率和人口数据，并将结果保存到 CSV 文件中。我们将从这些文件中加载数据，并将它们合并成一个
    DataFrame，随后用于交互式可视化。
- en: 'In the `lesson-3-workbook.ipynb` of the Jupyter Notebook, scroll to the `Subtopic A:
    Building a DataFrame to store and organize data` subsection in the `Topic B` section.'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在 Jupyter Notebook 的 `lesson-3-workbook.ipynb` 中，滚动到 `Topic B` 部分的 `Subtopic
    A: 构建 DataFrame 来存储和组织数据` 子主题。'
- en: We are first going to load the data from the CSV files, so that it's back to
    the state it was in after scraping. This will allow us to practice building DataFrames
    from Python objects, as opposed to using the `pd.read_csv` function.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们首先将从 CSV 文件加载数据，使其恢复到抓取后的状态。这将允许我们练习从 Python 对象构建 DataFrame，而不是使用 `pd.read_csv`
    函数。
- en: Note
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: When using `pd.read_csv`, the datatype for each column will be inferred from
    the string input. On the other hand, when using `pd.DataFrame` as we do here,
    the datatype is instead taken as the type of the input variables. In our case, as will
    be seen, we read the file and do not bother converting the variables to numeric
    or date-time until after instantiating the DataFrame.
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 `pd.read_csv` 时，数据类型会根据字符串输入进行推断。另一方面，使用 `pd.DataFrame`（如我们这里所做的）时，数据类型会根据输入变量的类型来确定。在我们的例子中，正如后续所看到的，我们读取文件后，并没有急于将变量转换为数值或日期时间类型，而是在实例化
    DataFrame 后再进行转换。
- en: 'Load the CSV files into lists by running the following code:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码将 CSV 文件加载到列表中：
- en: '[PRE19]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Check what the resulting lists look like by running the next two cells. We
    should see an output similar to the following:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行接下来的两个单元格来检查结果列表的内容。我们应该能看到类似以下的输出：
- en: '[PRE20]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now, the data is in a standard Python list structure, just as it was after scraping
    from the web pages in the previous sections. We're now going to create two DataFrames
    and merge them, so that all of the data is organized within one object.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，数据已经变成了标准的 Python 列表结构，就像我们在之前的章节中从网页抓取数据后看到的那样。接下来，我们将创建两个 DataFrame 并合并它们，使所有数据都组织在一个对象中。
- en: 'Use the standard DataFrame constructor to create the two DataFrames by running
    the following code:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准的 DataFrame 构造函数，通过运行以下代码来创建两个 DataFrame：
- en: '[PRE21]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This isn't the first time we've used this function in this book. Here, we pass
    the lists of data (as seen previously) and the corresponding column names. The
    input data can also be of dictionary type, which can be useful when each column
    is contained in a separate list.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这并不是我们在本书中第一次使用这个函数。在这里，我们传递了之前看到的数据列表和相应的列名。输入数据也可以是字典类型，这在每一列的数据存储在单独的列表中时非常有用。
- en: Next, we're going to clean up each DataFrame. Starting with the interest rates
    one, let's print the head and tail, and list the data types.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将清理每个 DataFrame。首先是利率 DataFrame，让我们打印头部和尾部，并列出数据类型。
- en: 'When displaying the entire DataFrame, the default maximum number of rows is
    60 (for version 0.18.1). Let''s reduce this to 10 by running the following code:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当显示整个 DataFrame 时，默认的最大行数为 60（对于版本 0.18.1）。让我们通过运行以下代码将其减少到 10 行：
- en: '[PRE22]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Display the head and tail of the interest rates DataFrame by running the following
    code:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码显示利率 DataFrame 的头部和尾部：
- en: '[PRE23]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Figure 3.11: Table for interest rates by country'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.11：各国利率表'
- en: '](img/C13018_03_11.jpg)'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_11.jpg)'
- en: 'Figure 3.11: Table for interest rates by country'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.11：各国利率表
- en: 'Print the data types by running:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码打印数据类型：
- en: '[PRE24]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Pandas has assigned each column as a string datatype, which makes sense because
    the input variables were all strings. We'll want to change these to string, float,
    and datetime, respectively.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pandas 已将每一列指定为字符串数据类型，这是合理的，因为输入变量都是字符串。我们需要将这些列分别转换为字符串、浮动数值和日期时间类型。
- en: 'Convert to the proper datatypes by running the following code:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码将数据转换为正确的数据类型：
- en: '[PRE25]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We use `astype` to cast the Interest Rate values as floats, setting `copy=False`
    to save memory. Since the date values are given in such an easy-to-read format,
    these can be converted simply by using `pd.to_datetime`.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用 `astype` 将利率值转换为浮动数值类型，设置 `copy=False` 以节省内存。由于日期值已经是易于阅读的格式，因此可以通过使用 `pd.to_datetime`
    轻松转换。
- en: 'Check the new datatypes of each column by running the following code:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码检查每一列的新数据类型：
- en: '[PRE26]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As can be seen, everything is now in the proper format.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如图所示，一切现在都处于正确的格式中。
- en: 'Let''s apply the same procedure to the other DataFrame. Run the next few cells
    to repeat the preceding steps for `df_populations`:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们对另一个数据框应用相同的过程。运行接下来的几个单元，重复之前针对`df_populations`的步骤：
- en: '[PRE27]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Figure 3.12: Table for population by country'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.12：按国家划分的人口表格'
- en: '](img/C13018_03_12.jpg)'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_12.jpg)'
- en: 'Figure 3.12: Table for population by country'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.12：按国家划分的人口表格
- en: 'Then, run this code:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，运行此代码：
- en: '[PRE28]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: To cast the numeric columns as a float, we had to first apply some modifications
    to the strings in this case. We stripped away any commas from the populations
    and removed the percent sign from the Yearly Change column, using string methods.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了将数字列转换为浮动类型，我们必须首先对这些字符串进行一些修改。在这种情况下，我们去除了人口中的逗号，并去掉了年变化列中的百分号，使用了字符串方法。
- en: Now, we're going to merge the DataFrames on the country name for each row. Keep
    in mind that these are still the raw country names as scraped from the web, so
    there might be some work involved with matching the strings.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们将根据每行的国家名称合并数据框。请记住，这些仍然是从网络抓取的原始国家名称，所以可能需要进行一些字符串匹配的工作。
- en: 'Merge the DataFrames by running the following code:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码合并数据框：
- en: '[PRE29]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We pass the population data in the left DataFrame and the interest rates in
    the right one, performing an outer match on the country columns. This will result
    in `NaN` values where the two do not overlap.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将左侧数据框中的人口数据与右侧数据框中的利率数据进行合并，在国家列上执行外部匹配。如果两个数据框没有交集，将会出现`NaN`值。
- en: 'For the sake of time, let''s just look at the most populated countries to see
    whether we missed matching any. Ideally, we would want to check everything. Look
    at the most populous countries by running the following code:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了节省时间，让我们先看看人口最多的国家，看看我们是否错过了匹配。理想情况下，我们应该检查所有内容。通过运行以下代码查看人口最多的国家：
- en: '[PRE30]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![Figure 3.13: The table for most populous countries'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.13：人口最多国家的表格'
- en: '](img/C13018_03_13.jpg)'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_13.jpg)'
- en: 'Figure 3.13: The table for most populous countries'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.13：人口最多国家的表格
- en: It looks like U.S. didn't match up. This is because it's listed as *United States*
    in the interest rates data. Let's remedy this.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 看起来美国没有匹配上。这是因为在利率数据中，它被列为*United States*。让我们来解决这个问题。
- en: 'Fix the label for U.S. in the populations table by running the following code:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码修正人口表中美国的标签：
- en: '[PRE31]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We rename the country for the populations DataFrame with the use of the `loc`
    method to locate that row.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用`loc`方法重命名人口数据框中的国家列，通过定位该行来进行操作。
- en: Now, let's merge the DataFrames properly.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们正确地合并数据框。
- en: 'Re-merge the DataFrames on the country names, but this time use an inner merge
    to remove the `NaN` values:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新根据国家名称合并数据框，但这次使用内连接合并，以去除`NaN`值：
- en: '[PRE32]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We are left with two identical columns in the merged DataFrame. Drop one of
    them by running the following code:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在合并后的数据框中得到了两列相同的列。通过运行以下代码删除其中一列：
- en: '[PRE33]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Rename the columns by running the following code:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码重命名列：
- en: '[PRE34]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We are left with the following merged and cleaned DataFrame:'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们得到以下合并并清理后的数据框：
- en: '![Figure 3.14: Ouput after cleaning and merging tables'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.14：清理和合并表格后的输出'
- en: '](img/C13018_03_14.jpg)'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_14.jpg)'
- en: 'Figure 3.14: Ouput after cleaning and merging tables'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.14：清理和合并表格后的输出
- en: 'Now that we have all the data in a nicely organized table, we can move on to
    the fun part: visualizing it. Let''s save this table to a CSV file for later use,
    and then move on to discuss how visualizations can be created with Bokeh.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经将所有数据整理成一个漂亮的表格，可以进入有趣的部分了：可视化它。让我们将这个表格保存到CSV文件中以供以后使用，然后继续讨论如何使用Bokeh创建可视化。
- en: 'Write the merged data to a CSV file for later use with the following code:'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用以下代码将合并后的数据写入CSV文件以供以后使用：
- en: '[PRE35]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Introduction to Bokeh
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bokeh简介
- en: Bokeh is an interactive visualization library for Python. Its goal is to provide
    similar functionality to D3, the popular interactive visualization library for
    JavaScript. Bokeh functions very differently than D3, which is not surprising
    given the differences between Python and JavaScript. Overall, it's much simpler
    and it doesn't allow nearly as much customization as D3 does. This works to its
    advantage though, as it's much easier to use, and it still boasts an excellent
    suite of features that we'll explore in this section.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Bokeh是一个Python的交互式可视化库。它的目标是提供类似于D3的功能，D3是JavaScript中流行的交互式可视化库。Bokeh的工作方式与D3有很大的不同，这并不奇怪，因为Python和JavaScript之间存在差异。总体来说，Bokeh要简单得多，而且不像D3那样允许进行大量定制。然而，这也正是它的优势所在，因为它更容易使用，并且仍然拥有一套出色的功能，我们将在本节中进行探索。
- en: Let's dive right into a quick exercise with the Jupyter Notebook and introduce
    Bokeh by example.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的练习来快速了解Jupyter Notebook，并通过示例介绍Bokeh。
- en: Note
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: There is good documentation online for Bokeh, but much of it is outdated. Searching
    something like Bokeh bar plot in Google still tends to turn up documentation for
    legacy modules that no longer exist, for example, the high-level plotting tools
    that used to be available through `bokeh.charts` (prior to version 0.12.0). These
    are the ones that take pandas DataFrames as input in much the same way that Seaborn
    plotting functions do. Removing the high-level plotting tools module has simplified
    Bokeh, and will allow for more focused development going forward. Now, the plotting
    tools are largely grouped into the `bokeh.plotting` module, as will be seen in
    the next exercise and following activity.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 网上有关于Bokeh的良好文档，但许多文档已经过时。例如，使用Google搜索"Bokeh条形图"时，通常会找到关于不再存在的遗留模块的文档，例如，曾通过`bokeh.charts`提供的高级绘图工具（在0.12.0版本之前）。这些工具与Seaborn的绘图函数类似，接受pandas
    DataFrame作为输入。去除高级绘图工具模块使Bokeh变得更加简洁，并将为今后的集中开发提供更多空间。现在，绘图工具主要集中在`bokeh.plotting`模块中，下一次练习和随后的活动中将会看到这一点。
- en: 'Exercise 17: Introduction to Interactive Visualization With Bokeh'
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习17：Bokeh交互式可视化简介
- en: We'll load the required Bokeh modules and show some simple interactive plots
    that can be made with Bokeh. Please note that the examples in this book have been
    designed using version 0.12.10 of Bokeh.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载所需的Bokeh模块，并展示一些可以用Bokeh创建的简单交互式图形。请注意，本书中的示例是基于Bokeh版本0.12.10设计的。
- en: 'In the `lesson-3-workbook.ipynb` Jupyter notebook, scroll to `Subtopic B: Introduction
    to Bokeh`.'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`lesson-3-workbook.ipynb` Jupyter notebook中，滚动至`子主题B：Bokeh简介`。
- en: 'Like scikit-learn, Bokeh modules are usually loaded in pieces (unlike pandas,
    for example, where the whole library is loaded at once). Import some basic plotting
    modules by running the following code:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和scikit-learn一样，Bokeh模块通常是按需加载的（与pandas不同，后者会一次性加载整个库）。通过运行以下代码导入一些基本的绘图模块：
- en: '[PRE36]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We need to run `output_notebook()` in order to render the interactive visuals
    within the Jupyter notebook.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们需要运行`output_notebook()`才能在Jupyter notebook中渲染交互式可视化。
- en: 'Generate random data to plot by running the following code:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码生成随机数据以进行绘图：
- en: '[PRE37]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The random data is generated using the cumulative sum of a random set of numbers
    that are distributed about zero. The effect is a trend that looks similar to a
    stock price time series, for example.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机数据是通过对一组随机数字进行累积求和生成的，这些数字分布在零附近。其效果类似于股价时间序列的趋势。
- en: 'Plot the data with a line plot in Bokeh by running the following code:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码，在Bokeh中使用折线图绘制数据：
- en: '[PRE38]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![Figure 3.15: An example data plot'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.15：示例数据图'
- en: '](img/C13018_03_15.jpg)'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_15.jpg)'
- en: 'Figure 3.15: An example data plot'
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.15：示例数据图
- en: We instantiate the figure, as referenced by the variable `p`, and then plot
    a line. Running this in Jupyter yields an interactive figure with various options
    along the right-hand side.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们实例化图形，使用变量`p`引用它，然后绘制一条线。在Jupyter中运行后，会显示一个交互式图形，并在右侧提供各种选项。
- en: The top three options (as of version 0.12.10) are **Pan**, **Box Zoom**, and
    **Wheel Zoom**. Play around with these and experiment with how they work. Use
    the reset option to re-load the default plot limits.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前三个选项（截至版本0.12.10）是**平移**、**框选缩放**和**滚轮缩放**。尝试这些功能，并实验它们的工作方式。使用重置选项重新加载默认的绘图范围。
- en: 'Other plots can be created with the alternative methods of `figure`. Draw a
    scatter plot by running the following code, where we replace `line` in the preceding
    code with `circle`:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用`figure`的其他方法创建其他图表。通过运行以下代码绘制散点图，将前面代码中的`line`替换为`circle`：
- en: '[PRE39]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![Figure 3.16: An example scatter plot'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.16：一个示例散点图](img/C13018_03_16.jpg)'
- en: '](img/C13018_03_16.jpg)'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_16.jpg)'
- en: 'Figure 3.16: An example scatter plot'
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.16：一个示例散点图
- en: Here, we've specified the size of each circle using a random set of numbers.
    A very enticing feature of interactive visualizations is the tooltip. This is
    a hover tool that allows the user to get information about a point by hovering
    over it.
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用一组随机数字来指定每个圆圈的大小。交互式可视化的一个非常吸引人的特点是工具提示。它是一种悬停工具，允许用户通过悬停在某个点上查看该点的信息。
- en: 'In order to add this tool, we''re going to use a slightly different method
    for creating the plot. This will require us to import a couple of new libraries.
    Run the following code:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了添加这个工具，我们将使用一种稍微不同的方法来创建图表。这将需要我们导入几个新的库。运行以下代码：
- en: '[PRE40]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This time, we'll create a data source to pass to the plotting method. This can
    contain metadata, which can be included in the visualization via the hover tool.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这次，我们将创建一个数据源并传递给绘图方法。这个数据源可以包含元数据，通过悬停工具在可视化中显示。
- en: 'Create random labels and plot the interactive visualization with a hover tool
    by running the following code:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码，创建随机标签并绘制带有悬停工具的交互式可视化：
- en: '[PRE41]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the complete code, refer to the following: [https://bit.ly/2RhpU1r](https://bit.ly/2RhpU1r).'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完整代码请参考：[https://bit.ly/2RhpU1r](https://bit.ly/2RhpU1r)。
- en: '![Figure 3.17: A random scatter plot with labels'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.17：带标签的随机散点图](img/C13018_03_17.jpg)'
- en: '](img/C13018_03_17.jpg)'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13018_03_17.jpg)'
- en: 'Figure 3.17: A random scatter plot with labels'
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.17：带标签的随机散点图
- en: We define a data source for the plot by passing a dictionary of key/value pairs
    to the `ColumnDataSource` constructor. This source includes the *x* location,
    *y* location, and size of each point, along with the random letter `A`, `B`, or
    `C` for each point. These random letters are assigned as labels for the hover
    tool, which will also display the size of each point. The `bokeh.plotting.figure`.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过将键值对字典传递给`ColumnDataSource`构造函数来定义图表的数据源。这个数据源包括每个点的*X*位置、*Y*位置以及大小，同时还包括随机字母`A`、`B`或`C`，这些字母作为悬停工具的标签，悬停时会显示每个点的大小。`bokeh.plotting.figure`。
- en: 'Add pan, zoom, and reset tools to the plot by running the following code:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码，将平移、缩放和重置工具添加到图表中：
- en: '[PRE42]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This code is identical to what was previously shown except for the `tools` variable,
    which now references several new tools we've imported from the Bokeh library.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码与之前显示的完全相同，唯一不同的是`tools`变量，它现在引用了我们从Bokeh库导入的几个新工具。
- en: We'll stop the introductory exercise here, but we'll continue creating and exploring
    plots in the following activity.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在此处停止介绍性练习，但将在接下来的活动中继续创建和探索图表。
- en: 'Activity 4: Exploring Data with Interactive Visualizations'
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 4：使用交互式可视化探索数据
- en: You should have completed the previous exercise in order to complete this activity.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经完成了之前的练习，才能继续完成此活动。
- en: We'll pick up using Bokeh right where we left off with the previous exercise,
    except instead of using the randomly generated data seen there, we'll instead
    use the data we scraped from the web in the first part of this chapter. Our aim
    is to use Bokeh to create interactive visualizations of our scraped data.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用Bokeh，从上一个练习结束的地方开始，只不过这次我们不再使用之前看到的随机生成数据，而是使用我们在本章第一部分从网页抓取的数据。我们的目标是使用Bokeh来创建抓取数据的交互式可视化。
- en: 'In order to do so, we need to execute the following steps:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要执行以下步骤：
- en: 'In the `lesson-3-workbook.ipynb` file, scroll to the `Activity B: Interactive
    visualizations with Bokeh` section.'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在`lesson-3-workbook.ipynb`文件中，滚动到`Activity B: 使用Bokeh进行交互式可视化`部分。'
- en: Load the previously scraped, merged, and cleaned web page data
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载之前抓取、合并并清洗过的网页数据
- en: Recall what the data looks like by displaying the DataFrame.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过显示DataFrame来回顾数据的样子。
- en: Draw a scatter plot of the population as a function of the interest rate.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制一个人口与利率之间关系的散点图。
- en: In the data, we see some clear outliers with high populations. Hover over these
    to see what they are. Select the Box Zoom tool and alter the viewing window to
    better see the majority of the data.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据中，我们看到一些明显的异常值，具有较高的值。将鼠标悬停在这些点上查看它们是什么。选择框选缩放工具，并调整视图窗口，以便更好地看到大部分数据。
- en: Some of the lower population countries appear to have negative interest rates.
    Select the **Wheel Zoom** tool and use it to zoom in on this region. Use the **Pan**
    tool to re-center the plot, if needed, so that the negative interest rate samples
    are in view. Hover over some of these and see what countries they correspond to.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些人口较少的国家似乎有负利率。选择**滚轮缩放**工具，使用它放大该区域。若需要，使用**平移**工具重新定位图表，确保负利率样本在视野内。悬停在这些样本上，查看它们对应的国家。
- en: Add a **Year of last change** column to the DataFrame and add a color based
    on the date of last interest rate change
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向 DataFrame 中添加一个**最后变更年份**列，并根据最后利率变化的日期为其添加颜色。
- en: Create a map to group the last change date into color categories.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个地图，将最后变更日期分成不同的颜色类别。
- en: Create the colored visualization.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建彩色可视化。
- en: Looking for patterns, zoom in on the lower population countries.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 寻找模式时，放大查看低人口国家。
- en: Plot the interest rate as a function of the year-over-year population change
    by running the following code.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码，将利率绘制为与年同比人口变化的函数。
- en: Determine the line of best fit for the previously plotted relationship.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定先前绘制关系的最佳拟合线。
- en: Re-plot the output obtained in the preceding step and add a line of best fit.
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新绘制前一步得到的输出，并添加最佳拟合线。
- en: Explore the plot by using the zoom tools and hovering over interesting samples.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用缩放工具探索图表，并悬停在有趣的样本上。
- en: Note
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The detailed steps along with the solutions are presented in the *Appendix
    A*     (pg. no. 163).'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 详细步骤及解决方案展示在*附录 A*（第163页）。
- en: Summary
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we scraped web page tables and then used interactive visualizations
    to study the data.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们抓取了网页表格，然后使用互动式可视化来研究数据。
- en: We started by looking at how HTTP requests work, focusing on GET requests and
    their response status codes. Then, we went into the Jupyter Notebook and made
    HTTP requests with Python using the Requests library. We saw how Jupyter can be
    used to render HTML in the notebook, along with actual web pages that can be interacted
    with. After making requests, we saw how Beautiful Soup can be used to parse text
    from the HTML, and used this library to scrape tabular data.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先了解了 HTTP 请求的工作原理，重点讨论了 GET 请求及其响应状态码。然后，我们进入 Jupyter Notebook，使用 Python
    的 Requests 库发出了 HTTP 请求。我们看到了 Jupyter 如何在笔记本中渲染 HTML，以及实际可以交互的网页。发出请求后，我们看到 Beautiful
    Soup 如何解析 HTML 中的文本，并使用该库抓取表格数据。
- en: After scraping two tables of data, we stored them in pandas DataFrames. The
    first table contained the central bank interest rates for each country and the
    second table contained the populations. We combined these into a single table
    that was then used to create interactive visualizations.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在抓取了两张数据表后，我们将其存储在 pandas DataFrame 中。第一张表包含每个国家的中央银行利率，第二张表包含人口数据。我们将这两张表合并成一个表，然后用它来创建互动式可视化图表。
- en: Finally, we used Bokeh to render interactive visualizations in Jupyter. We saw
    how to use the Bokeh API to create various customized plots and made scatter plots
    with specific interactive abilities such as zoom, pan, and hover. In terms of
    customization, we explicitly showed how to set the point radius and color for
    each data sample.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 Bokeh 在 Jupyter 中渲染了互动式可视化。我们学习了如何使用 Bokeh API 创建各种自定义图表，并制作了具有特定交互功能（如缩放、平移和悬停）的散点图。在自定义方面，我们明确展示了如何为每个数据样本设置点的半径和颜色。
- en: Furthermore, when using Bokeh to explore the scraped population data, the tooltip
    was utilized to show country names and associated data when hovering over the
    points.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在使用 Bokeh 探索抓取的人口数据时，我们利用了工具提示（tooltip）来显示国家名称和相关数据，当鼠标悬停在数据点上时。
- en: Congratulations for completing this introductory course on data science using
    Jupyter Notebooks! Regardless of your experience with Jupyter and Python coming
    into the book, you've learned some useful and applicable skills for practical
    data science!
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了使用 Jupyter Notebooks 进行数据科学入门课程！无论你之前有多少 Jupyter 和 Python 的经验，你已经掌握了一些实用且可以应用于实际数据科学的技能！
- en: Before finishing up, let's quickly recap the topics we've covered in this book.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束之前，让我们快速回顾一下本书中涵盖的主题。
- en: The first chapter was an introduction to the Jupyter Notebook platform, where
    we covered all of the fundamentals. We learned about the interface and how to
    use and install magic functions. Then, we introduced the Python libraries we'll
    be using and walked through an exploratory analysis of the *Boston housing* dataset.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 第一章介绍了Jupyter Notebook平台，我们涵盖了所有基础知识。我们了解了界面以及如何使用和安装魔法函数。接着，我们介绍了将要使用的Python库，并通过*波士顿住房*数据集进行了一次探索性分析。
- en: In the second chapter, we focused on doing machine learning with Jupyter. We
    first discussed the steps for developing a predictive analytics plan, and then
    looked at a few different types of models including SVM, a KNN classifier, and
    Random Forests.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们专注于使用Jupyter进行机器学习。我们首先讨论了制定预测分析计划的步骤，然后介绍了几种不同类型的模型，包括SVM、KNN分类器和随机森林。
- en: Working with an *employee retention* dataset, we applied data cleaning methods
    and then trained models to predict whether an employee has left or not. We also
    explored more advanced topics such as overfitting, k-fold cross-validation, and
    validation curves.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理*员工离职*数据集时，我们应用了数据清洗方法，并训练了模型来预测员工是否离职。我们还探讨了更高级的话题，如过拟合、k折交叉验证和验证曲线。
- en: Finally, in the third chapter, we shifted briefly from data analysis to data
    collection using web scraping and saw how to make HTTP requests and parse the
    HTML responses in Jupyter. Then, we finished up the book by using interactive
    visualizations to explore our collected data.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第三章中，我们暂时从数据分析转向了数据收集，使用了网页抓取技术，并学习了如何在Jupyter中进行HTTP请求以及解析HTML响应。随后，我们通过使用交互式可视化工具来探索我们收集的数据，完成了本书内容。
