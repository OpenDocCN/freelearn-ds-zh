- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Going at Scale
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展计算能力
- en: After building and testing models in the previous chapter, we will now address
    the requirements and considerations for scaling time-series analysis in large
    and distributed computing environments. We will cover the different ways that
    Apache Spark can be used to scale the previous examples in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133),
    starting with feature engineering and moving on to hyperparameter tuning and single-
    and multi-model training. This information is crucial as we face the requirement
    to analyze large volumes of time-series data in a timely manner.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章构建并测试模型后，我们将讨论在大型分布式计算环境中扩展时间序列分析的需求和注意事项。我们将讨论Apache Spark如何扩展[**第7章**](B18568_07.xhtml#_idTextAnchor133)中的示例，内容从特征工程开始，接着是超参数调优，以及单模型和多模型训练。这些信息对于我们在时间紧迫的情况下分析大量时间序列数据至关重要。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要话题：
- en: Why do we need to scale time-series analysis?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我们需要扩展时间序列分析？
- en: Scaling out feature engineering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展特征工程
- en: Scaling out model training
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展模型训练
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Before getting into the main topics, we will cover here the technical requirements
    for this chapter, which are as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入主要话题之前，我们将先介绍本章的技术要求，具体如下：
- en: '`ch8` folder of the book’s GitHub repository at this URL:'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 书籍GitHub仓库中的`ch8`文件夹，网址为：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch8](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch8)'
  id: totrans-10
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch8](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch8)'
- en: '**Synthetic data**: We will use the Synthetic Data Vault tool, a Python library
    for creating synthetic tabular data. You can find more information on Synthetic
    Data Vault here: [https://docs.sdv.dev/sdv](https://docs.sdv.dev/sdv).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合成数据**：我们将使用合成数据库工具（Synthetic Data Vault），这是一个用于生成合成表格数据的Python库。你可以在这里找到更多关于合成数据库的信息：[https://docs.sdv.dev/sdv](https://docs.sdv.dev/sdv)。'
- en: '**Databricks platform**: The Databricks Community Edition, while free to use,
    is limited in resources. Similarly, the resources are likely to be limited when
    using a personal computer or laptop. With the requirement to demonstrate the scaling
    of computing power in this chapter, we will be using the non-Community version
    of the Databricks platform. As discussed in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016),
    you can sign up for a 14-day free trial of Databricks, which will require you
    to first have an account with a cloud provider. Some cloud providers offer free
    credits at the start. This will provide you with more resources than on the Community
    Edition, for a limited time. Note that at the end of the trial period, the billing
    will switch to the credit card you provided at registration.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Databricks平台**：虽然Databricks社区版是免费的，但其资源有限。类似地，在个人计算机或笔记本电脑上使用时，资源也可能受到限制。鉴于本章需要演示计算能力的扩展，我们将使用Databricks的非社区版。如[**第1章**](B18568_01.xhtml#_idTextAnchor016)中所讨论的，你可以注册Databricks的14天免费试用版，但前提是你需要先拥有一个云服务提供商的账户。一些云服务提供商在开始时提供免费积分，这将为你提供比社区版更多的资源，且仅限时使用。请注意，试用期结束后，费用将转到你注册时提供的信用卡。'
- en: 'The Databricks compute configuration used is as per *Figure 8**.1*. The worker
    and driver types shown here are based on AWS, which is different from what is
    available on Azure and GCP. Note that the UI can be subject to change, in which
    case refer to the latest Databricks documentation here:'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用的Databricks计算配置如*图8.1*所示。这里展示的工作节点和驱动节点类型基于AWS，与Azure和GCP上的配置不同。请注意，UI界面可能会发生变化，在这种情况下，请参考最新的Databricks文档：
- en: '[https://docs.databricks.com/en/compute/configure.html](https://docs.databricks.com/en/compute/configure.html)'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/en/compute/configure.html](https://docs.databricks.com/en/compute/configure.html)'
- en: '![](img/B18568_08_1.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_08_1.jpg)'
- en: 'Figure 8.1: Databricks compute configuration'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：Databricks计算配置
- en: Why do we need to scale time-series analysis?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们需要扩展时间序列分析？
- en: The need to scale time-series analysis usually results from a requirement to
    perform the analysis faster or on a bigger dataset. In this chapter, we will look
    at decreasing the processing time achieved in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133)
    while increasing the dataset size fivefold. This will be possible thanks to the
    scale of processing offered by Apache Spark.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展时间序列分析的需求通常源于需要更快地进行分析或处理更大的数据集。在本章中，我们将探讨如何在将数据集大小扩大五倍的同时，减少在[*第7章*](B18568_07.xhtml#_idTextAnchor133)中实现的处理时间。这将得益于Apache
    Spark所提供的处理能力。
- en: Scaled-up dataset
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展后的数据集
- en: To exercise Spark’s scalability, we will need a more extensive dataset than
    we have used. While you may already have such a dataset, for the sake of this
    chapter, we will scale the household energy consumption dataset we used in [*Chapter
    7*](B18568_07.xhtml#_idTextAnchor133) and earlier chapters. The scaled dataset
    will be generated using the Synthetic Data Vault tool, mentioned in the *Technical*
    *requirements* section.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试Spark的可扩展性，我们需要一个比以往更广泛的数据集。虽然你可能已经拥有这样的数据集，但为了本章的目的，我们将扩展在[*第7章*](B18568_07.xhtml#_idTextAnchor133)和之前章节中使用的家庭能源消耗数据集。扩展后的数据集将使用Synthetic
    Data Vault工具生成，如*技术要求*部分所述。
- en: 'The code for this section is in `ts-spark_ch8_1.dbc`. We import the code into
    Databricks, similar to the approach explained for the Community Edition in the
    *Step-by-step: Loading and visualizing time series* section of [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的代码位于`ts-spark_ch8_1.dbc`中。我们将代码导入到Databricks中，方法与[*第1章*](B18568_01.xhtml#_idTextAnchor016)中*逐步操作：加载和可视化时间序列*部分对于社区版的讲解类似。
- en: In this code, we want to generate energy consumption data for four other families
    using one household’s data to scale it fivefold.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们希望使用一个家庭的数据生成四个其他家庭的能源消耗数据，以将数据规模扩大五倍。
- en: We begin by capturing the metadata of `pdf_main`, which is the smaller reference
    dataset. The metadata is used as input to create a `GaussianCopulaSynthesizer`
    object named `synthesizer`, which represents a statistical model of the data.
    The synthesizer is, in turn, trained on the reference dataset (`pdf_main`) with
    the `fit` method. This model is finally used to generate synthetic data with the
    `sample` method.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先捕获`pdf_main`的元数据，这是较小的参考数据集。元数据作为输入，用于创建一个名为`synthesizer`的`GaussianCopulaSynthesizer`对象，它表示数据的统计模型。然后，该合成器通过`fit`方法在参考数据集（`pdf_main`）上进行训练。最终，这个模型将用于通过`sample`方法生成合成数据。
- en: 'The smaller reference dataset (`pdf_main`) is associated with customer identifier
    (`cust_id`) `1`, and the synthetic datasets are associated with identifiers `2`,
    `3`, `4`, and `5`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的参考数据集（`pdf_main`）与客户标识符（`cust_id`）`1`相关联，合成数据集则与标识符`2`、`3`、`4`和`5`相关联：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Running the code in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133) on this
    new, larger dataset is not going to be performant. We can scale up or out time-series
    analysis on larger datasets. We will explain both types of scaling next.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新的、更大的数据集上运行[*第7章*](B18568_07.xhtml#_idTextAnchor133)中的代码将无法达到高效的性能。我们可以对更大的数据集进行纵向扩展或横向扩展时间序列分析。接下来，我们将解释这两种扩展方式。
- en: Scaling up
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展
- en: Scaling up is the simpler way to scale and does not require us to change the
    code we wrote in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133). We improve performance
    in this way by adding more memory (RAM) and using a more powerful CPU or even
    GPU. This works to a certain point before we reach scaling limits, prohibitive
    costs, or diminishing returns. In fact, due to system bottlenecks and overheads,
    scaling up does not result in linear performance improvement.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展是一种更简单的扩展方式，不需要我们修改在[*第7章*](B18568_07.xhtml#_idTextAnchor133)中编写的代码。通过增加更多的内存（RAM）和使用更强大的CPU或甚至GPU，我们可以提高性能。这样做在一定程度上是有效的，但会在达到扩展极限、成本过高或回报递减时遇到瓶颈。实际上，由于系统瓶颈和开销，扩展并不会导致性能线性提升。
- en: To scale further, we need to scale out.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要进一步扩展，我们需要进行横向扩展。
- en: Scaling out
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 横向扩展
- en: Instead of making our one machine more powerful, scaling out involves adding
    more machines and parallelizing the processing. This requires a mechanism for
    the code to be distributed and executed in parallel, which is what Apache Spark
    provides.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展不仅仅是让我们的单一机器变得更强大，它还涉及增加更多的机器并行处理。这需要一种机制，让代码能够被分发并并行执行，而Apache Spark正是提供了这种机制。
- en: 'In the upcoming sections, we will cover the following different ways that Apache
    Spark can be used to scale out time-series analysis:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍Apache Spark可用于扩展时间序列分析的几种不同方法：
- en: Feature engineering
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: Model training
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练
- en: Feature engineering
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: Apache Spark can be used to scale out feature engineering with its distributed
    computing framework. This enables parallel processing of feature engineering tasks,
    which we will demonstrate in this section.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark可以利用其分布式计算框架扩展特征工程的处理。这使得特征工程任务能够进行并行处理，我们将在本节中展示如何操作。
- en: We will continue our discussion on data preparation in [*Chapter 5*](B18568_05.xhtml#_idTextAnchor103)
    and improve the feature engineering done in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133).
    We will be using the pandas-based code examples from the *Development and testing*
    section of *Chapter 7* as a base for our discussion in this section. We will see
    in the following examples how non-Spark code is rewritten to be Spark compatible
    to avail the benefits of its scalability feature.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第5章*](B18568_05.xhtml#_idTextAnchor103)继续讨论数据准备，并改进[*第7章*](B18568_07.xhtml#_idTextAnchor133)中的特征工程。我们将在本节中以[*第7章*](B18568_07.xhtml#_idTextAnchor133)中*开发与测试*部分的基于pandas的代码示例为基础进行讨论。接下来的示例将展示如何将非Spark代码重写为Spark兼容代码，从而利用其可扩展性的优势。
- en: 'While there are many ways in which Spark can be used for feature engineering,
    we will focus on the following three related to improving [*Chapter* *7*](B18568_07.xhtml#_idTextAnchor133)’s
    code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Spark可以用于多种特征工程方式，但我们将重点讨论以下三种与改进[*第7章*](B18568_07.xhtml#_idTextAnchor133)代码相关的方式：
- en: Column transformations
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列转换
- en: Resampling
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重采样
- en: Lag values calculation
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滞后值计算
- en: Let’s begin with column transformations in the next section.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将开始进行列转换的讨论。
- en: Column transformations
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列转换
- en: 'In the first code example, we will rewrite the column transformations code
    present in `ts-spark_ch7_1e_lgbm_comm.dbc`, which is used in the *Development
    and testing* section of [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133). We will
    change the code to a Spark-enabled version by using the `pyspark.sql.functions`
    library. For this, we need to do the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个代码示例中，我们将重写`ts-spark_ch7_1e_lgbm_comm.dbc`中已有的列转换代码，该代码用于[*第7章*](B18568_07.xhtml#_idTextAnchor133)中的*开发与测试*部分。我们将通过使用`pyspark.sql.functions`库将代码修改为支持Spark的版本。为此，我们需要执行以下操作：
- en: Replace the `Date` column with the concatenation (the `concat_ws` function)
    of the existing `Date` and `Time` columns.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`concat_ws`函数，将现有的`Date`和`Time`列合并，替换`Date`列。
- en: Convert (the `to_timestamp` function) the `Date` column into timestamp format.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Date`列转换为时间戳格式（`to_timestamp`函数）。
- en: Replace selectively (with the `when` and `otherwise` condition) the incorrect
    values, `?`, in `Global_active_power` to `None`.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有选择性地（使用`when`和`otherwise`条件）将`Global_active_power`中的错误值`?`替换为`None`。
- en: Replace (the `regexp_replace` function) `,` with `.` to be in the proper format
    for a `float` value.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`regexp_replace`函数将`Global_active_power`中的`,`替换为`.`，以确保符合`float`值的正确格式。
- en: 'The following code extract demonstrates the preceding steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例演示了前述步骤：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After leveraging Spark to parallelize the column transformations, the next code
    improvement we will be covering is for resampling the time-series data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Spark并行化列转换后，我们接下来要讲解的代码优化是对时间序列数据进行重采样。
- en: Resampling
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重采样
- en: 'In the second code conversion example, we will rewrite the hourly resampling
    code present in `ts-spark_ch7_1e_lgbm_comm.dbc`, which is used in the *Development
    and testing* section of [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133). We want
    to calculate the hourly mean of `Global_active_power` for each customer. For this,
    we need to do the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个代码转换示例中，我们将重写`ts-spark_ch7_1e_lgbm_comm.dbc`中每小时重采样的代码，该代码用于[*第7章*](B18568_07.xhtml#_idTextAnchor133)中的*开发与测试*部分。我们希望计算每个客户的`Global_active_power`的每小时均值。为此，我们需要执行以下操作：
- en: Convert the `Date` column to its date and hour components using the `date_format`
    function.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`date_format`函数将`Date`列转换为日期和小时组件。
- en: Resample to the hourly mean of `Global_active_power` (the `agg` and `mean` functions)
    for each customer (the `groupBy` function).
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个客户（`groupBy`函数），将`Global_active_power`的重采样平均值转换为每小时的均值（使用`agg`和`mean`函数）。
- en: 'The following code demonstrates the preceding steps:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了前述步骤：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that we have used Spark to parallelize the resampling, the next code improvement
    we will be covering is for calculating the lag values of the time-series data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用Spark对重采样过程进行了并行化，接下来我们要介绍的代码优化是计算时间序列数据的滞后值。
- en: Calculating lag values
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算滞后值
- en: 'In the third example of scaling feature engineering with Apache Spark, we will
    rewrite the lag calculation code present in `ts-spark_ch7_1e_lgbm_comm.dbc`, which
    is used in the *Development and testing* section of [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133).
    We want to calculate different lag values for each customer. For this, we need
    to do the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个例子中，使用 Apache Spark 进行特征工程的扩展时，我们将重写存在于 `ts-spark_ch7_1e_lgbm_comm.dbc`
    中的滞后计算代码，该代码用于[*第七章*](B18568_07.xhtml#_idTextAnchor133)中的*开发和测试*部分。我们希望为每个客户计算不同的滞后值。为此，我们需要执行以下操作：
- en: Define a sliding date window over which to calculate the lags for each of the
    customers (the `partitionBy` function). We have the dates ordered (the `orderBy`
    function) for each customer.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个滑动日期窗口来计算每个客户的滞后值（`partitionBy` 函数）。我们已经为每个客户按日期排序（`orderBy` 函数）。
- en: Calculate the different lags over the sliding window (the `lag` and `over` functions).
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算滑动窗口上的不同滞后值（`lag` 和 `over` 函数）。
- en: Note that as the lag calculation is based on previous values, some of the lag
    values at the beginning of the dataset will not have enough prior values for calculation
    and will be empty. We remove the rows with these empty lag values using the `dropna`
    function.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，由于滞后计算基于先前的值，数据集开头的某些滞后值可能没有足够的先前值进行计算，因此会为空。我们使用 `dropna` 函数删除这些空滞后值的行。
- en: 'The following code demonstrates the preceding steps:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了上述步骤：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: By using Spark functions instead of pandas, we will enable Spark to parallelize
    the lag calculations for large datasets.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 Spark 函数而不是 pandas，我们将使 Spark 能够并行化处理大数据集的滞后计算。
- en: Now that we have covered the different ways of leveraging Apache Spark to improve
    the feature engineering part of [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133)’s
    code, we will dive deep into the scaling out of model training.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了不同的方法来利用 Apache Spark 提升[*第七章*](B18568_07.xhtml#_idTextAnchor133)中特征工程部分的代码，接下来我们将深入探讨模型训练的扩展。
- en: Model training
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'In this section, we will cover the following different ways that Apache Spark
    can be used for model training at scale:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖以下几种不同的方式，即 Apache Spark 如何用于规模化模型训练：
- en: Hyperparameter tuning
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调优
- en: Single model training in parallel
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单模型并行训练
- en: Multiple models training in parallel
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模型并行训练
- en: These approaches enable efficient model training when we have large datasets
    or many models to train.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法使得在拥有大数据集或需要训练多个模型时能够高效地进行模型训练。
- en: Hyperparameter tuning can be an expensive computation when the same model is
    trained repeatedly with many different hyperparameters. We want to be able to
    leverage Spark to find the best hyperparameters efficiently.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用不同的超参数重复训练同一模型时，超参数调优可能是昂贵的计算。我们希望能够利用 Spark 高效地找到最佳超参数。
- en: Similarly, training a single model on a large dataset can take a long time.
    In other cases, we may have many models to train for distinct time-series datasets.
    We want to speed these up by parallelizing the training on Spark clusters.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，对于大数据集，训练单一模型可能需要很长时间。在其他情况下，我们可能需要训练许多模型以适应不同的时间序列数据集。我们希望通过在 Spark 集群上并行化训练来加速这些过程。
- en: We will go into the details of these approaches next, starting with hyperparameter
    tuning in the next section.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节详细介绍这些方法，从超参数调优开始。
- en: Hyperparameter tuning
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调优
- en: As discussed in [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087), hyperparameter
    tuning in machine learning is the process of finding the best set of configurations
    for a machine learning algorithm. This search for optimal hyperparameters can
    be parallelized using libraries such as GridSearchCV, Hyperopt, and Optuna, which
    provide the framework, in conjunction with Apache Spark, for the backend processing
    parallelism.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第四章*](B18568_04.xhtml#_idTextAnchor087)中所述，机器学习中的超参数调优是为机器学习算法找到最佳配置集的过程。这种寻找最佳超参数的过程可以使用
    GridSearchCV、Hyperopt 和 Optuna 等库来并行化，这些库与 Apache Spark 结合提供后端处理并行性的框架。
- en: We discussed Spark’s processing parallelism in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063).
    Here we will focus more specifically on the use of Optuna in conjunction with
    Apache Spark for hyperparameter tuning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第三章*](B18568_03.xhtml#_idTextAnchor063)中讨论了 Spark 的处理并行性。这里我们将更专注于使用 Optuna
    与 Apache Spark 结合进行超参数调整。
- en: If you recall, in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133), we used GridSearchCV
    on a single node to tune the hyperparameters of the LightGBM model. We will improve
    on this in the code example in this section by parallelizing the process. We will
    use Optuna with Spark to find the best hyperparameters for the LightGBM model
    we explored in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，在[*第7章*](B18568_07.xhtml#_idTextAnchor133)中，我们在单个节点上使用 GridSearchCV 调整了
    LightGBM 模型的超参数。在这一节的代码示例中，我们将通过并行化过程来改进这一点。我们将使用Optuna与Spark一起，找到我们在[*第7章*](B18568_07.xhtml#_idTextAnchor133)中探索的LightGBM模型的最佳超参数。
- en: 'Optuna is an open source hyperparameter optimization framework that is used
    to automate hyperparameter searches. You can find more information on Optuna here:
    [https://optuna.org/](https://optuna.org/).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna 是一个开源的超参数优化框架，用于自动化超参数搜索。您可以在这里找到有关Optuna的更多信息：[https://optuna.org/](https://optuna.org/)。
- en: 'We will begin the tuning process by defining an `objective` function (which
    we will later optimize using Optuna). This `objective` function does the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过定义一个 `objective` 函数开始调优过程（稍后我们将使用Optuna优化该函数）。此 `objective` 函数执行以下操作：
- en: Define the search space in `params` with the range of hyperparameter values.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`params`中定义超参数值的搜索空间。
- en: Initialize the LightGBM `LGBMRegressor` model with the parameters specific to
    the trial.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化LightGBM `LGBMRegressor`模型，使用特定于试验的参数。
- en: Train (`fit`) the model on the training dataset.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据集上训练（`fit`）模型。
- en: Use the model to predict the validation dataset.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型对验证数据集进行预测。
- en: Calculate the model evaluation metric (`mean_absolute_percentage_error`).
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型的评估指标（`mean_absolute_percentage_error`）。
- en: Return the evaluation metric.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回评估指标。
- en: 'The following code demonstrates the preceding steps:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了前面的步骤：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once the objective function is defined, the next steps are as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了目标函数，接下来的步骤如下：
- en: Register Spark (the `register_spark` function) as the backend.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册 Spark（`register_spark` 函数）作为后端。
- en: Create a study (the `create_study` function), which is a collection of trials,
    to minimize the evaluation metric.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个研究（`create_study`函数），它是一个包含试验的集合，用于最小化评估指标。
- en: Run the study on the Spark `parallel_backend` to optimize the `objective` function
    over `n_trials`.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Spark `parallel_backend` 上运行研究，以优化 `objective` 函数在 `n_trials` 上的表现。
- en: 'The following code demonstrates the preceding steps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了前面的步骤：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Retrieve the best trial from the optimization study
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从优化研究中获取最佳试验
- en: trial = study2.best_trial
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: trial = study2.best_trial
- en: Print the best trial's objective function value,
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打印最佳试验的目标函数值，
- en: '#  typically accuracy or loss'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#    通常是准确性或损失'
- en: 'print(f"Best trial accuracy: {trial.value}")'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: print(f"最佳试验准确率：{trial.value}")
- en: 'print("Best trial params: ")'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: print("最佳试验参数：")
- en: Iterate through the best trial's hyperparameters and print them
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遍历最佳试验的超参数并打印它们
- en: 'for key, value in trial.params.items():'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'for key, value in trial.params.items():'
- en: 'print(f"    {key}: {value}")'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 'print(f"    {key}: {value}")'
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: from pyspark.ml.feature import VectorAssembler
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: from pyspark.ml.feature import VectorAssembler
- en: Define a list to hold the names of the lag feature columns
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义一个列表来保存滞后特征列的名称
- en: inputCols = []
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: inputCols = []
- en: Loop through the list of lag intervals to create feature column
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遍历滞后间隔列表以创建特征列
- en: names
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 名称
- en: 'for l in lags:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 'for l in lags:'
- en: inputCols.append('Global_active_power_lag' + str(l))
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: inputCols.append('Global_active_power_lag' + str(l))
- en: Initialize VectorAssembler with the
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化VectorAssembler并使用
- en: created feature column names and specify the output column name
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建特征列名称并指定输出列名称
- en: assembler = VectorAssembler(
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: assembler = VectorAssembler(
- en: inputCols=inputCols, outputCol="features")
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: inputCols=inputCols, outputCol="features")
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: from xgboost.spark import SparkXGBRegressor
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: from xgboost.spark import SparkXGBRegressor
- en: Initialize the SparkXGBRegressor for the regression task.
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化SparkXGBRegressor用于回归任务。
- en: '`num_workers` is set to the default parallelism level of -'
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`num_workers` 设置为默认的并行度级别 -'
- en: '#   the Spark context to utilize all available cores.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#    Spark上下文，用于利用所有可用核心。'
- en: '`label_col` specifies the target variable column name for'
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`label_col` 指定目标变量列名'
- en: prediction.
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测。
- en: '`missing` is set to 0.0 to handle missing values in the dataset.'
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`missing` 设置为0.0，以处理数据集中的缺失值。'
- en: xgb_model = SparkXGBRegressor(
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: xgb_model = SparkXGBRegressor(
- en: num_workers=sc.defaultParallelism,
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: num_workers=sc.defaultParallelism,
- en: label_col="Global_active_power", missing=0.0
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: label_col="Global_active_power", missing=0.0
- en: )
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '[PRE8]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
- en: from pyspark.ml.evaluation import RegressionEvaluator
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: from pyspark.ml.evaluation import RegressionEvaluator
- en: Initialize the parameter grid for hyperparameter tuning
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化超参数调整的参数网格
- en: '- max_depth: specifies the maximum depth of the trees in the model'
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '- max_depth: 指定模型中树的最大深度'
- en: '- n_estimators: defines the number of trees in the ensemble'
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '- n_estimators: 定义模型中树的数量'
- en: paramGrid = ParamGridBuilder()\
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: paramGrid = ParamGridBuilder()\
- en: .addGrid(xgb_model.max_depth, [5, 10])\
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: .addGrid(xgb_model.max_depth, [5, 10])\
- en: .addGrid(xgb_model.n_estimators, [30, 100])\
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: .addGrid(xgb_model.n_estimators, [30, 100])\
- en: .build()
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: .build()
- en: Initialize the regression evaluator for model evaluation
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化回归评估器用于模型评估
- en: '- metricName: specifies the metric to use for evaluation,'
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '- metricName: 指定用于评估的指标'
- en: '#    here RMSE (Root Mean Squared Error)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#    这里是 RMSE（均方根误差）'
- en: '- labelCol: the name of the label column'
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '- labelCol: 标签列的名称'
- en: '- predictionCol: the name of the prediction column'
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '- predictionCol: 预测列的名称'
- en: evaluator = RegressionEvaluator(
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: evaluator = RegressionEvaluator(
- en: metricName="rmse",
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: metricName="rmse",
- en: LabelCol = xgb_model.getLabelCol(),
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: LabelCol = xgb_model.getLabelCol(),
- en: PredictionCol = xgb_model.getPredictionCol()
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: PredictionCol = xgb_model.getPredictionCol()
- en: )
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: Initialize the CrossValidator for hyperparameter tuning
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化 CrossValidator 进行超参数调整
- en: '- estimator: the model to be tuned'
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '- estimator: 需要调优的模型'
- en: '- evaluator: the evaluator to be used for model evaluation'
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '- evaluator: 用于模型评估的评估器'
- en: '- estimatorParamMaps: the grid of parameters to be used for tuning'
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '- estimatorParamMaps: 用于调优的参数网格'
- en: cv = CrossValidator(
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: cv = CrossValidator(
- en: estimator = xgb_model, evaluator = evaluator,
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: estimator = xgb_model, evaluator = evaluator,
- en: estimatorParamMaps = paramGrid)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: estimatorParamMaps = paramGrid)
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: from pyspark.ml import Pipeline
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: from pyspark.ml import Pipeline
- en: 'Initialize a Pipeline object with two stages:'
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化一个包含两个阶段的管道对象：
- en: a feature assembler and a cross-validator for model tuning
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个特征组装器和一个交叉验证器用于模型调优
- en: 'pipeline = filter function) the training data to cust_id 1. We then take all
    the records (the head function) for training except the last 48 hours as these
    will be used for testing. This results in the train_hr DataFrame with the hourly
    training data:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: pipeline = filter function) 训练数据筛选为 cust_id 1。接着，我们使用所有记录（head function）进行训练，除去最后48小时的数据，因为这些数据将用于测试。最终得到的
    train_hr DataFrame 包含了每小时的训练数据：
- en: '[PRE10]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Similarly, for testing, we will filter in `cust_id` `1` and, in this case,
    use the last 48 hours. We can then apply (`transform`) the model (`pipelineModel`)
    to the test data (`test_hr`) to get the prediction of energy consumption for these
    48 hours:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于测试，我们将筛选出 `cust_id` `1`，并在这种情况下使用最后的48小时数据。然后，我们可以将模型（`pipelineModel`）应用于测试数据（`test_hr`），以获取这48小时的能耗预测：
- en: '[PRE11]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Evaluate the model's performance using
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用评估器评估模型的性能
- en: Root Mean Squared Error (RMSE) metric
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方根误差（RMSE）指标
- en: rmse = evaluator.evaluate(predictions)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: rmse = evaluator.evaluate(predictions)
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'def train_model(df_pandas: pd.DataFrame) -> pd.DataFrame:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 'def train_model(df_pandas: pd.DataFrame) -> pd.DataFrame:'
- en: '# Extract the customer ID for which the model is being trained'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '# 提取用于训练模型的客户 ID'
- en: cust_id = df_pandas["cust_id"].iloc[0]
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: cust_id = df_pandas["cust_id"].iloc[0]
- en: '# Select features and target variables from the DataFrame'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '# 从 DataFrame 中选择特征和目标变量'
- en: X = df_pandas[[
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: X = df_pandas[[
- en: '''Global_active_power_lag1'', ''Global_active_power_lag2'','
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '''Global_active_power_lag1'', ''Global_active_power_lag2'','
- en: '''Global_active_power_lag3'', ''Global_active_power_lag4'','
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '''Global_active_power_lag3'', ''Global_active_power_lag4'','
- en: '''Global_active_power_lag5'', ''Global_active_power_lag12'','
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '''Global_active_power_lag5'', ''Global_active_power_lag12'','
- en: '''Global_active_power_lag24'', ''Global_active_power_lag168'''
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '''Global_active_power_lag24'', ''Global_active_power_lag168'''
- en: ']]'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ']]'
- en: y = df_pandas['Global_active_power']
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: y = df_pandas['Global_active_power']
- en: '# Split the dataset into training and testing sets, preserving'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '# 将数据集分为训练集和测试集，并保持'
- en: '# time order'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '# 时间顺序'
- en: X_train, X_test, y_train, y_test = train_test_split(
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: X_train, X_test, y_train, y_test = train_test_split(
- en: X, y, test_size=0.2, shuffle=False, random_state=12
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: X, y, test_size=0.2, shuffle=False, random_state=12
- en: )
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '# Define the hyperparameter space for LightGBM model tuning'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '# 定义 LightGBM 模型调优的超参数空间'
- en: param_grid = {
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: param_grid = {
- en: '''num_leaves'': [30, 50, 100],'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '''num_leaves'': [30, 50, 100],'
- en: '''learning_rate'': [0.1, 0.01, 0.001],'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '''learning_rate'': [0.1, 0.01, 0.001],'
- en: '''n_estimators'': [50, 100, 200]'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '''n_estimators'': [50, 100, 200]'
- en: '}'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '# Initialize the LightGBM regressor model'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '# 初始化 LightGBM 回归模型'
- en: lgbm = lgb.LGBMRegressor()
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: lgbm = lgb.LGBMRegressor()
- en: '# Initialize TimeSeriesSplit for cross-validation to'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '# 初始化 TimeSeriesSplit 进行交叉验证'
- en: '#  respect time series data structure'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '# 尊重时间序列数据结构'
- en: tscv = TimeSeriesSplit(n_splits=10)
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: tscv = TimeSeriesSplit(n_splits=10)
- en: '# Perform grid search with cross-validation'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '# 使用交叉验证进行网格搜索'
- en: gsearch = GridSearchCV(
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: gsearch = GridSearchCV(
- en: estimator=lgbm, param_grid=param_grid, cv=tscv)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: estimator=lgbm, param_grid=param_grid, cv=tscv)
- en: gsearch.fit(X_train, y_train)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: gsearch.fit(X_train, y_train)
- en: '# Extract the best hyperparameters'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '# 提取最佳超参数'
- en: best_params = gsearch.best_params_
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: best_params = gsearch.best_params_
- en: '# Train the final model using the best parameters'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '# 使用最佳参数训练最终模型'
- en: final_model = lgb.LGBMRegressor(**best_params)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: final_model = lgb.LGBMRegressor(**best_params)
- en: final_model.fit(X_train, y_train)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: final_model.fit(X_train, y_train)
- en: '# Make predictions on the test set'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '# 对测试集进行预测'
- en: y_pred = final_model.predict(X_test)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: y_pred = final_model.predict(X_test)
- en: '# Calculate RMSE and MAPE metrics'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '# 计算 RMSE 和 MAPE 指标'
- en: rmse = np.sqrt(mean_squared_error(y_test, y_pred))
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: rmse = np.sqrt(mean_squared_error(y_test, y_pred))
- en: mape = mean_absolute_percentage_error(y_test, y_pred)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: mape = mean_absolute_percentage_error(y_test, y_pred)
- en: '# Prepare the results DataFrame to return'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '# 准备返回的结果 DataFrame'
- en: return_df = pd.DataFrame(
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: return_df = pd.DataFrame(
- en: '[[cust_id, str(best_params), rmse, mape]],'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[[cust_id, str(best_params), rmse, mape]],'
- en: columns=["cust_id", "best_params", "rmse", "mape"]
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: columns=["cust_id", "best_params", "rmse", "mape"]
- en: )
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: return return_df
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: return return_df
- en: '[PRE13]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: from pyspark.sql.functions import lit
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: from pyspark.sql.functions import lit
- en: Group the data by customer ID and apply the
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按客户 ID 对数据进行分组，并应用
- en: '#  train_model function to each group using Pandas UDF'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '# 将 train_model 函数应用于每个组，使用 Pandas UDF'
- en: The schema for the resulting DataFrame is defined by
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果 DataFrame 的模式由以下定义
- en: '#  train_model_result_schema'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#  train_model_result_schema'
- en: Cache the resulting DataFrame to optimize performance for
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存结果 DataFrame 以优化性能
- en: '#  subsequent actions'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '# 后续操作'
- en: train_model_result_df = (
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: train_model_result_df = (
- en: data_hr
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: data_hr
- en: .groupby("cust_id")
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: .groupby("cust_id")
- en: .applyInPandas(train_model, schema=train_model_result_schema)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: .applyInPandas(train_model, schema=train_model_result_schema)
- en: .cache()
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: .cache()
- en: )
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '[PRE14]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
