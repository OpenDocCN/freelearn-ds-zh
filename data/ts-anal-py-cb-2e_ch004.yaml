- en: 3 Reading Time Series Data from Databases
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 从数据库中读取时间序列数据
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的书籍社区，加入 Discord
- en: '![](img/file0.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/file0.png)'
- en: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
- en: '**Databases** extend what you can store to include text, images, and media
    files and are designed for efficient read-and-write operations at a massive scale.
    Databases can store terabytes and petabytes of data with efficient and optimized
    data retrieval capabilities, such as when performing analytical operations on
    **data warehouses** and **data lakes**. A data warehouse is a database designed
    to store large amounts of structured data, mostly integrated from multiple source
    systems, built specifically to support business intelligence reporting, dashboards,
    and advanced analytics. A data lake, on the other hand, stores a large amount
    of structured, semi-structured, or unstructured data in its raw format. In this
    chapter, we will continue to use the **pandas** library to read data from databases.
    We will create time series DataFrames by reading data from **relational** (SQL)
    databases and **non-relational** (NoSQL) databases.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据库**扩展了你可以存储的内容，包括文本、图像和媒体文件，并且被设计用于在大规模下进行高效的读写操作。数据库能够存储数TB甚至PB的数据，并提供高效优化的数据检索功能，例如在执行分析操作时用于**数据仓库**和**数据湖**。数据仓库是一个专门设计用于存储大量结构化数据的数据库，数据大多来自多个源系统的集成，专为支持商业智能报告、仪表板和高级分析而构建。另一方面，数据湖以原始格式存储大量结构化、半结构化或非结构化数据。本章中，我们将继续使用**pandas**库从数据库中读取数据。我们将通过读取**关系型**（SQL）数据库和**非关系型**（NoSQL）数据库的数据来创建时间序列
    DataFrame。'
- en: Additionally, you will explore working with third-party data providers to pull
    financial data from their database systems.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你将探索如何与第三方数据提供商合作，从他们的数据库系统中拉取金融数据。
- en: 'In this chapter, you will create time series DataFrames with a `DatetimeIndex`
    data type by covering the following recipes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将创建带有 `DatetimeIndex` 数据类型的时间序列 DataFrame，并涉及以下食谱：
- en: Reading data from a relational database
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从关系型数据库中读取数据
- en: Reading data from Snowflake
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Snowflake 中读取数据
- en: Reading data from a document database
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文档数据库中读取数据
- en: Reading data from a time series databases
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从时间序列数据库中读取数据
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will be using pandas 2.2.2 (released April 10, 2024) extensively.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将广泛使用 pandas 2.2.2（于 2024 年 4 月 10 日发布）。
- en: You will be working with different types of databases, such as PostgreSQL, Amazon
    Redshift, MongoDB, InfluxDB, and Snowflake. You will need to install additional
    Python libraries to connect to these databases.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你将与不同类型的数据库一起工作，例如 PostgreSQL、Amazon Redshift、MongoDB、InfluxDB 和 Snowflake。你需要安装额外的
    Python 库以连接到这些数据库。
- en: You can also download the Jupyter notebooks from this book's GitHub repository
    ([https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook))
    to follow along.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以从本书的 GitHub 仓库下载 Jupyter 笔记本（[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook)）来进行跟随练习。
- en: 'As a good practice, you will store your database credentials in a config `database.cfg`
    file outside your Python script. You can use `configparser` to read and store
    the values in Python variables. You do not want your credentials exposed or hard
    coded in your code:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个良好的实践，你会将你的数据库凭据存储在 Python 脚本之外的 `database.cfg` 配置文件中。你可以使用 `configparser`
    来读取并将值存储在 Python 变量中。你不希望凭据暴露或硬编码在代码中：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can load the `database.cfg` file using `config.read()`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `config.read()` 加载 `database.cfg` 文件：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Reading data from a relational database
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从关系型数据库中读取数据
- en: In this recipe, you will read data from PostgreSQL, a popular open-source relational
    database.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，你将从 PostgreSQL 读取数据，这是一个流行的开源关系型数据库。
- en: You will explore two methods for connecting to and interacting with PostgreSQL.
    First, you will use `psycopg`, a PostgreSQL Python connector, to connect and query
    the database, then parse the results into a pandas DataFrame. In the second approach,
    you will query the same database again, but this time, you will use **SQLAlchemy**,
    an **object-relational mapper** (**ORM**) that is well-integrated with pandas.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你将探索两种连接并与 PostgreSQL 交互的方法。首先，你将使用 `psycopg`，这是一个 PostgreSQL Python 连接器，来连接并查询数据库，然后将结果解析到
    pandas DataFrame 中。第二种方法是再次查询相同的数据库，但这次你将使用**SQLAlchemy**，一个与 pandas 集成良好的**对象关系映射器**（**ORM**）。
- en: Getting ready
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, it is assumed that you have the latest PostgreSQL installed.
    At the time of writing, version 16 is the latest stable version.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，假设你已经安装了最新版本的 PostgreSQL。在写这篇文章时，版本 16 是最新的稳定版本。
- en: To connect to and query the database in Python, you will need to install `psycopg`,
    a popular PostgreSQL database adapter for Python. You will also need to install
    `SQLAlchemy`, which provides flexibility regarding how you want to manage the
    database, whether for writing or reading data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Python 中连接和查询数据库，你需要安装 `psycopg`，它是一个流行的 PostgreSQL 数据库适配器。你还需要安装 `SQLAlchemy`，它提供了灵活性，可以根据你希望管理数据库的方式（无论是写入还是读取数据）来进行选择。
- en: 'To install the libraries using `conda`, run the following command:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `conda` 安装库，运行以下命令：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To install the libraries using `pip`, run the following command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `pip` 安装库，运行以下命令：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you cannot access a PostgreSQL database, the fastest way to get up and running
    is via Docker ([https://hub.docker.com/_/postgres](https://hub.docker.com/_/postgres)).
    The following is an example command:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法访问 PostgreSQL 数据库，最快的方式是通过 Docker ([https://hub.docker.com/_/postgres](https://hub.docker.com/_/postgres))
    来启动。以下是一个示例命令：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This will create a container named `postgres-ch3`. The `username` is `postgres,`
    and the password is `password`. The default `database` created is called `postgres`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为 `postgres-ch3` 的容器。`username` 为 `postgres`，密码是 `password`。创建的默认 `database`
    名为 `postgres`。
- en: 'Once the **postgres-ch3** container is up and running you can connect to it
    using **DBeaver** as shown:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦**postgres-ch3**容器启动并运行，你可以使用**DBeaver**进行连接，如下所示：
- en: '![Figure 3.1 – DBeaver PostgreSQL connection settings](img/file22.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – DBeaver PostgreSQL 连接设置](img/file22.png)'
- en: Figure 3.1 – DBeaver PostgreSQL connection settings
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – DBeaver PostgreSQL 连接设置
- en: You will be working with MSFT stock dataset provided in the `datasets/Ch3/MSFT.csv`
    folder. I have uploaded the dataset into the database using **DBeaver Community
    Edition,** which you can download here [https://dbeaver.io/download/](https://dbeaver.io/download/)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用存放在 `datasets/Ch3/MSFT.csv` 文件夹中的 MSFT 股票数据集。我已经通过**DBeaver Community Edition**上传了数据集到数据库，你可以在这里下载
    [https://dbeaver.io/download/](https://dbeaver.io/download/)
- en: 'You can import the CSV file by right-clicking on `tables` under the `public`
    schema, as shown in the figure below:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过右键点击 `public` 模式下的 `tables` 来导入 CSV 文件，如下图所示：
- en: '![Figure 3.2 – Importing data to PostgreSQL using DBeaver](img/file23.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 使用 DBeaver 导入数据到 PostgreSQL](img/file23.png)'
- en: Figure 3.2 – Importing data to PostgreSQL using DBeaver
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 使用 DBeaver 导入数据到 PostgreSQL
- en: You can confirm all records have been written to the `msft` table in the `postgres`
    database as shown
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以确认所有记录已被写入 `postgres` 数据库中的 `msft` 表，如下所示。
- en: '![Figure 3.3 – Using SQL Editor in DBeaver to run a SQL query against the msft
    table](img/file24.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – 在 DBeaver 中使用 SQL 编辑器运行 SQL 查询，以查询 msft 表](img/file24.png)'
- en: Figure 3.3 – Using SQL Editor in DBeaver to run a SQL query against the msft
    table
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 在 DBeaver 中使用 SQL 编辑器运行 SQL 查询，以查询 msft 表
- en: How to do it…
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作…
- en: We will start by connecting to the PostgreSQL instance, querying the database,
    loading the result set into memory, and parsing the data into a time series DataFrame.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先连接到 PostgreSQL 实例，查询数据库，将结果集加载到内存中，并将数据解析为时间序列 DataFrame。
- en: In this recipe, I will connect to a PostgreSQL instance that runs locally, so
    my connection would be to `localhost (127.0.0.1)`. You will need to adjust this
    for your own PostgreSQL database setting.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我将连接到一个本地运行的 PostgreSQL 实例，因此我的连接将是 `localhost (127.0.0.1)`。你需要根据你自己的
    PostgreSQL 数据库设置来调整此连接。
- en: Using the psycopg
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 psycopg
- en: '**Psycopg** is a Python library (and a database driver) that provides additional
    functionality and features when working with a PostgreSQL database. Follow these
    steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**Psycopg** 是一个 Python 库（也是数据库驱动程序），它在使用 PostgreSQL 数据库时提供了额外的功能和特性。请按照以下步骤操作：'
- en: 'Start by importing the necessary libraries. You will import the required connection
    parameters from the `database.cfg` highlighted in the *Technical Requirements*
    section. You will create a Python dictionary to store all the parameter values
    required to establish a connection to the database, such as `host`, `database`
    name, `user` name, and `password`:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始时导入必要的库。你将从 `database.cfg` 文件中导入所需的连接参数，正如*技术要求*部分中所突出显示的那样。你将创建一个 Python
    字典来存储所有连接到数据库所需的参数值，比如 `host`、`database` 名称、`user` 名称和 `password`：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can establish a connection by passing the parameters to the `connect()`
    method. Once connected, you can create a **cursor** object that can be used to
    execute SQL queries:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过将参数传递给 `connect()` 方法来建立连接。一旦连接成功，你可以创建一个**游标**对象，用来执行 SQL 查询：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The cursor object provides several attributes and methods, including `execute`,
    `executemany`, `fetchall`, `fetchmany` and `fetchone`. The following code uses
    the cursor object to pass a SQL query and then checks the number of records that
    have been produced by that query using the `rowcount` attribute:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 游标对象提供了多个属性和方法，包括 `execute`、`executemany`、`fetchall`、`fetchmany` 和 `fetchone`。以下代码使用游标对象传递
    SQL 查询，然后使用 `rowcount` 属性检查该查询所产生的记录数：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The returned result set after executing the query will not include a header
    (no columns names). Alternatively, you can grab the column names from the cursor
    object using the `description` attribute, as shown in the following code:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行查询后返回的结果集将不包括标题（没有列名）。或者，你可以通过使用 `description` 属性从游标对象中获取列名，代码如下所示：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can use a list comprehension to extract the column names from `cursor.description`
    to pass as column headers when creating the DataFrame:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用列表推导式从 `cursor.description` 中提取列名，并在创建 DataFrame 时将其作为列标题传入：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To fetch the results that were produced by the executed query you will use
    the `fetchall` method. You will create a DataFrame based on the fetched result
    set. Make sure that you pass the column names that you just captured:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取执行查询所产生的结果，你将使用 `fetchall` 方法。你将根据获取的结果集创建一个 DataFrame。确保传入你刚才捕获的列名：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Notice the `date` column is returned as an `object` type, not a `datetime` type.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`date` 列作为 `object` 类型返回，而不是 `datetime` 类型。
- en: 'Parse the `date` column using `pd.to_datetime()` and set it as the index for
    the DataFrame:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pd.to_datetime()` 解析 `date` 列，并将其设置为 DataFrame 的索引：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the preceding code, the cursor returned a list of tuples **without a header**.
    You can confirm this by running the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，游标返回了一个没有标题的元组列表**（没有列名）**。你可以通过运行以下代码来确认这一点：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can instruct the cursor to return a `dict_row` type, which will include
    the column name information (the header). This is more convenient when converting
    into a DataFrame. This can be done by passing the `dict_row` class to the `row_factory`
    parameter:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以指示游标返回一个 `dict_row` 类型，它将包括列名信息（即标题）。这在转换为 DataFrame 时更为方便。可以通过将 `dict_row`
    类传递给 `row_factory` 参数来实现：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Notice the column names being available. You can now create a DataFrame as
    in the following code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意列名已经可用。你现在可以像以下代码所示那样创建一个 DataFrame：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Close the cursor and the connection to the database:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭游标和数据库连接：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Note that `psycopg` connections and cursors can be used in Python''s `with`
    statement for exception handling when committing a transaction. The cursor object
    provides three different fetching functions; that is, `fetchall`, `fetchmany`,
    and `fetchone`. The `fetchone` method returns a single tuple. The following example
    shows this concept:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`psycopg` 连接和游标可以在 Python 的 `with` 语句中用于处理异常，以便在提交事务时进行异常处理。游标对象提供了三种不同的获取函数；即
    `fetchall`、`fetchmany` 和 `fetchone`。`fetchone` 方法返回一个单独的元组。以下示例展示了这一概念：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Using pandas and SQLAlchemy
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 pandas 和 SQLAlchemy
- en: SQLAlchemy is a very popular open-source library for working with relational
    databases in Python. SQLAlchemy can be referred to as an **Object-Relational Mapper
    (ORM)**, which provides an abstraction layer (think of it as an interface) so
    that you can use object-oriented programming to interact with a relational database.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: SQLAlchemy 是一个非常流行的开源库，用于在 Python 中操作关系型数据库。SQLAlchemy 可以被称为**对象关系映射器（ORM）**，它提供了一个抽象层（可以把它当作一个接口），让你可以使用面向对象编程与关系型数据库进行交互。
- en: You will be using SQLAlchemy because it integrates very well with pandas, and
    several of the pandas SQL reader and writer functions depend on SQLAlchemy as
    the abstraction layer. SQLAlchemy does the behind-the-scenes translation for any
    pandas SQL read or write requests. This translation ensures that the SQL statement
    from pandas is represented in the correct syntax/format for the underlying database
    type (MySQL, Oracle, SQL Server, or PostgreSQL, to name a few).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用 SQLAlchemy，因为它与 pandas 的集成非常好，多个 pandas SQL 读写函数依赖于 SQLAlchemy 作为抽象层。SQLAlchemy
    会为任何 pandas 的 SQL 读写请求做幕后翻译。这种翻译确保了 pandas 中的 SQL 语句会以适用于底层数据库类型（如 MySQL、Oracle、SQL
    Server 或 PostgreSQL 等）的正确语法/格式表示。
- en: 'Some of the pandas SQL reader functions that rely on SQLAlchemy include `pandas.read_sql`,
    `pandas.read_sql_query`, and `pandas.read_sql_table`. Let''s perform the following
    steps:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一些依赖于 SQLAlchemy 的 pandas SQL 读取函数包括 `pandas.read_sql`、`pandas.read_sql_query`
    和 `pandas.read_sql_table`。让我们执行以下步骤：
- en: 'Start by importing the necessary libraries. Note that, behind the scenes, SQLAlchemy
    will use **psycopg** (or any other database driver that is installed and supported
    by SQLAlchemy):'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始时导入必要的库。注意，在幕后，SQLAlchemy 将使用 **psycopg**（或任何其他已安装且 SQLAlchemy 支持的数据库驱动程序）：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In the preceding example, for `parse_dates`, you passed a dictionary in the
    format of `{key: value}`, where `key` is the column name and the `value` is a
    string representation of the date format. Unlike the previous **psycopg** approach,
    `pandas.read_sql` did a better job in getting the data types correct. Notice that
    our index is of the `DatetimeIndex` type:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '在前面的示例中，对于 `parse_dates`，你传递了一个字典格式的参数 `{key: value}`，其中 `key` 是列名，`value`
    是日期格式的字符串表示。与之前的 **psycopg** 方法不同，`pandas.read_sql` 更好地处理了数据类型的正确性。注意，我们的索引是 `DatetimeIndex`
    类型：'
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You could also accomplish the same results using the `pandas.read_sql_query`:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也可以使用 `pandas.read_sql_query` 来完成相同的操作：
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `pandas.read_sql_table` is another SQL reader function provided by pandas
    that takes in a table name instead of a SQL query. Think of this as a `SELECT
    * FROM tablename` query:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pandas.read_sql_table` 是 pandas 提供的另一个 SQL 读取函数，它接受表名而不是 SQL 查询。可以把它看作是一个
    `SELECT * FROM tablename` 查询：'
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `read_sql` reader function is more versatile as it is a wrapper to `read_sql_query`
    and `read_sql_table`. The `read_sql` function can take either a SQL query or a
    table name.
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`read_sql` 读取函数更为通用，因为它是 `read_sql_query` 和 `read_sql_table` 的封装器。`read_sql`
    函数既可以接受 SQL 查询，也可以接受表名。'
- en: How it works…
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'You explored two methods to connect to a PostgreSQL database in this recipe:
    using the psycopg driver directly or utilizing pandas and SQLAlchemy.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你探索了两种连接 PostgreSQL 数据库的方法：直接使用 psycopg 驱动程序或利用 pandas 和 SQLAlchemy。
- en: 'When using **psycopg** to connect to a PostgreSQL, you first need to create
    a connection object followed by a cursor object. This concept of a connection
    objects and cursors is consistent across different database drivers in Python.
    Once you create a cursor object, you have access to several methods, including:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**psycopg**连接 PostgreSQL 时，首先需要创建一个连接对象，然后创建一个游标对象。连接对象和游标的概念在 Python 中的不同数据库驱动程序之间是一致的。创建游标对象后，可以访问多个方法，包括：
- en: '`Execute()` – executes a SQL query (CRUD) or command to the database'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Execute()` – 执行 SQL 查询（CRUD）或命令到数据库'
- en: '`Executemany()` – executes the same database operation with a sequence of input
    data, for example, this can be useful with INSERT INTO for bulk insert.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Executemany()` – 使用一系列输入数据执行相同的数据库操作，例如，这对于批量插入的 `INSERT INTO` 操作非常有用。'
- en: '`Fetchall()` – returns all remaining records from the current query result
    set'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Fetchall()` – 返回当前查询结果集中的所有剩余记录'
- en: '`Fetchone()` - returns the next record (one record) from the current query
    result set'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Fetchone()` - 从当前查询结果集中返回下一条记录（单条记录）'
- en: '`fetchmany(n)` – returns `n` number of records from the current query result
    set'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fetchmany(n)` – 从当前查询结果集中返回 `n` 条记录'
- en: '`close()` - close the current cursor and free associated resources'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`close()` - 关闭当前游标并释放关联的资源'
- en: On the other hand, creating an **engine** object is the first step when working
    with SQLAlchemy, as it provides instructions on the database that is being considered.
    This is known as a **dialect**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，创建 **engine** 对象是使用 SQLAlchemy 时的第一步，因为它提供了关于正在使用的数据库的说明，这被称为 **dialect**（方言）。
- en: 'When you created the engine with `create_engine,` you passed a URL as the connection
    string. Let''s examine the engine connection string for SQLAlchemy:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用 `create_engine` 创建引擎时，你传递了一个作为连接字符串的 URL。让我们检查一下 SQLAlchemy 的引擎连接字符串：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`dialect` –the name of the SQLAlchemy dialect (database type) such as postgresql,
    mysql, sqlite, oracle, or mssql.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dialect` – SQLAlchemy 方言（数据库类型）的名称，例如 postgresql、mysql、sqlite、oracle 或 mssql。'
- en: '`driver` –the name of the installed driver (DBAPI) to connect to the specified
    dialect, such as `psycopg` or `pg8000` for PostgreSQL database.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`driver` – 连接指定方言的已安装驱动程序（DBAPI）的名称，例如，PostgreSQL 数据库的 `psycopg` 或 `pg8000`。'
- en: '`username` -the login username for database authentication'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`username` - 数据库身份验证的登录用户名'
- en: '`password` -the password for the username specified'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`password` - 为指定的用户名设置的密码'
- en: '`host` -the server where the database is hosted'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`host` - 数据库所在的服务器'
- en: '`port` -the specific port for the database'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`port` - 数据库的特定端口'
- en: '`database` -the name of the specific database you want to connect to'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`database` - 你要连接的具体数据库的名称'
- en: 'Earlier, you used `psycopg` as the database driver for PostgreSQL. The `psycopg`
    driver is referred to as a **database API (DBAPI)** and SQLAlchemy supports many
    DBAPI wrappers based on Python''s DBAPI specifications to connect to and interface
    with various types of relational databases. SQLAlchemy already comes with built-in
    dialects to work with different flavors of RDBMS, such as the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，你使用 `psycopg` 作为 PostgreSQL 的数据库驱动程序。`psycopg` 驱动程序被称为 **数据库 API (DBAPI)**，SQLAlchemy
    支持多种基于 Python DBAPI 规范的 DBAPI 包装器，用于连接和与各种类型的关系型数据库进行交互。SQLAlchemy 已经内置了多种方言，以便与不同类型的关系数据库管理系统（RDBMS）协作，具体包括：
- en: SQL Server
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL Server
- en: SQLite
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQLite
- en: PostgreSQL
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PostgreSQL
- en: MySQL
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MySQL
- en: Oracle
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oracle
- en: Snowflake
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snowflake
- en: 'When connecting to a database using SQLAlchemy, you need to specify the **dialect**
    and the **driver** (DBAPI) to be used. This is what the URL string looks like
    for PostgreSQL:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 SQLAlchemy 连接到数据库时，你需要指定要使用的 **方言** 和 **驱动程序**（DBAPI）。这是 PostgreSQL 的 URL
    字符串格式：
- en: '[PRE22]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This is what it would look like if you are using **MySQL** database with `PyMySQL`
    driver:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 **MySQL** 数据库并配合 `PyMySQL` 驱动，连接 URL 将如下所示：
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In the previous code examples in the *How to do it…* section, you did not need
    to specify the `psycopg` driver since it is the default DBAPI that SQLAlchemy
    uses. This example would work just fine, assuming that `psycopg` is installed:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面 *如何做……* 部分的代码示例中，你无需指定 `psycopg` 驱动程序，因为它是 SQLAlchemy 默认使用的 DBAPI。假设已经安装了
    `psycopg`，此示例将可以正常工作：
- en: '[PRE24]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'There are other PostgreSQL drivers (DBAPI) that are supported by SQLAlchemy,
    including the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: SQLAlchemy 支持其他 PostgreSQL 驱动程序（DBAPI），包括以下内容：
- en: '`psycopg`'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`psycopg`'
- en: '`pg8000`'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pg8000`'
- en: '`asyncpg`'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`asyncpg`'
- en: For a more comprehensive list of supported dialects and drivers, you can visit
    the official documentation page at [https://docs.sqlalchemy.org/en/20/dialects/index.html](https://docs.sqlalchemy.org/en/20/dialects/index.html).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如需查看更全面的支持的方言和驱动程序列表，你可以访问官方文档页面 [https://docs.sqlalchemy.org/en/20/dialects/index.html](https://docs.sqlalchemy.org/en/20/dialects/index.html)。
- en: The advantage of using SQLAlchemy is that it is well-integrated with pandas.
    If you read the official pandas documentation for `read_sql`, `read_sql_query`,
    `read_sql_table`, and `to_sql` you will notice that the `con` argument is expecting
    a SQLAlchemy connection object (engine).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SQLAlchemy 的优势在于，它与 pandas 集成良好。如果你查阅官方 pandas 文档中的 `read_sql`、`read_sql_query`、`read_sql_table`
    和 `to_sql`，你会注意到 `con` 参数期望一个 SQLAlchemy 连接对象（引擎）。
- en: Another advantage is that you can easily change the backend engine (database),
    for example from PostgreSQL to MySQL, without needing to change the rest of the
    code.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个优势是，你可以轻松更换后端引擎（数据库），例如从 PostgreSQL 切换到 MySQL，而无需修改其余代码。
- en: There's more…
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多……
- en: In this section we will explore some additional concepts to help you better
    grasp the versatility of SQLAclhemy and bring some of the previous concepts discussed
    in the recipe *Working with large data files* in Chapter 2, *Reading Time Series
    Data from Files*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一些附加概念，帮助你更好地理解 SQLAlchemy 的多功能性，并将第二章 *处理大型数据文件* 中介绍的一些概念与本教程结合起来，这些概念讨论了如何从文件中读取时间序列数据。
- en: 'Specifically, we will discuss:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将讨论以下内容：
- en: Generating the connecitoin URL in SQLAlchemy
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 SQLAlchemy 中生成连接 URL
- en: Extending to Amazon Redshift database
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展到 Amazon Redshift 数据库
- en: Chunking in pandas
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas 中的分块处理
- en: Generating the connection URL in SQLAlchemy
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 SQLAlchemy 中生成连接 URL
- en: In this recipe you were introduced to the `create_engine` function from SQLAlchemy
    library which takes a URL string to establish a connection to a database. So far,
    you have been creating the URL string manually but there is a more convenient
    way to generate the URL for you. This can be accomplished with the `create` method
    form the `URL` class in SQLAlchemy.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，你已经学习了 SQLAlchemy 库中的 `create_engine` 函数，它通过 URL 字符串来建立数据库连接。到目前为止，你一直是手动创建
    URL 字符串，但有一种更方便的方式可以为你生成 URL。这可以通过 SQLAlchemy 中 `URL` 类的 `create` 方法来实现。
- en: 'The following code demonstrates this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了这一点：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Notice the `drivername` consists of the *dialect* and the *driver* in format
    `dialct+driver`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`drivername` 包含 *方言* 和 *驱动程序*，格式为 `dialct+driver`。
- en: You can now pass the `url` to `create_engine` as you have done before.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以像以前一样将 `url` 传递给 `create_engine`。
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Extending to Amazon Redshift database
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩展到 Amazon Redshift 数据库
- en: We discussed the versatility of SQLAlchmey, which allows you to change the engine
    (database backend) and keep the remaining code as is. For example, using PostgreSQL
    or MySQL, or any other supported dialect by SQLAlchemy. We will also explore connecting
    to a cloud data warehouse like Amazon Redshift.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了SQLAlchemy的多功能性，它允许你更改数据库引擎（后端数据库），而保持其余代码不变。例如，使用PostgreSQL或MySQL，或者SQLAlchemy支持的任何其他方言。我们还将探讨如何连接到像Amazon
    Redshift这样的云数据仓库。
- en: It is worth mentioning that **Amazon Redshift**, a cloud data warehouse, is
    based on PostgreSQL at its core. You will install the Amazon Redshift driver for
    SQLAlchemy (it uses the psycopg2 DBAPI).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，**Amazon Redshift**是一个基于PostgreSQL的云数据仓库。你将安装适用于SQLAlchemy的Amazon Redshift驱动程序（它使用psycopg2
    DBAPI）。
- en: 'You can install using **conda**:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用**conda**进行安装：
- en: '[PRE27]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You can also install using **pip**:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用**pip**进行安装：
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Because we do not want to expose your AWS credentials in your code, you will
    update the `database.cfg` file discussed in the *Technical Requirements* section
    to include your AWS Redshift information:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们不希望在代码中暴露你的AWS凭证，你将更新在*技术要求*部分讨论的`database.cfg`文件，以包含你的AWS Redshift信息：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You will use `configparser` to load your values:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用`configparser`加载你的值：
- en: '[PRE30]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You will use the URL.create method to generate your URL:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用URL.create方法来生成你的URL：
- en: '[PRE31]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now, you can switch the engine from our previous code, which originally pointing
    to our local instance of PostgreSQL, to run the same query on Amazon Redshift.
    This assumes you have a `msft` table in Amazon Redshift.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将之前代码中的引擎切换，从指向本地PostgreSQL实例的引擎，改为在Amazon Redshift上运行相同的查询。这假设你在Amazon
    Redshift中有一个`msft`表。
- en: '[PRE32]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To learn more about `sqlalchemy-redshift`, you can refer to the project''s
    repository here: [https://github.com/sqlalchemy-redshift/sqlalchemy-redshift](https://github.com/sqlalchemy-redshift/sqlalchemy-redshift).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`sqlalchemy-redshift`的信息，可以访问该项目的仓库：[https://github.com/sqlalchemy-redshift/sqlalchemy-redshift](https://github.com/sqlalchemy-redshift/sqlalchemy-redshift)。
- en: The Amazon Redshift example can be extended to other databases such as Google
    BigQuery, Teradata, or Microsoft SQL Server as long there is a supported SQLAlchemy
    dialect for that database. For a complete list visit the official page here [https://docs.sqlalchemy.org/en/20/dialects/index.html](https://docs.sqlalchemy.org/en/20/dialects/index.html)
  id: totrans-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Amazon Redshift的例子可以扩展到其他数据库，例如Google BigQuery、Teradata或Microsoft SQL Server，只要这些数据库有支持的SQLAlchemy方言即可。要查看完整的列表，请访问官方页面：[https://docs.sqlalchemy.org/en/20/dialects/index.html](https://docs.sqlalchemy.org/en/20/dialects/index.html)
- en: Chunking with pandas
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用pandas进行chunking
- en: When you executed the query against `msft` table, it returned 1259 records.
    Imagine working with a much larger database that returns millions of records,
    if not more. This is where the `chunking` parameter helps.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行针对`msft`表的查询时，它返回了1259条记录。试想一下，如果处理一个更大的数据库，可能会返回数百万条记录，甚至更多。这就是`chunking`参数派上用场的地方。
- en: 'The `chunksize` parameter allows you to break down a large dataset into smaller
    and more manageable chunks of data that can fit into your local memory. When executing
    the `read_sql` function, just pass the number of rows to be retrieved (per chunk)
    to the `chunksize` parameter, which then returns a `generator` object. You can
    then loop through the generator object, or use `next()` to capture one chunk at
    a time, and perform whatever calculations or processing needed. Let''s look at
    an example of how chunking works. You will request `500` records (rows) at a time:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`chunksize`参数允许你将一个大的数据集拆分成较小的、更易于管理的数据块，这些数据块能够适应本地内存。当执行`read_sql`函数时，只需将要检索的行数（每个数据块）传递给`chunksize`参数，之后它会返回一个`generator`对象。你可以循环遍历这个生成器对象，或者使用`next()`一次获取一个数据块，并进行所需的计算或处理。让我们看一个如何实现chunking的例子。你将每次请求`500`条记录（行）：'
- en: '[PRE33]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code will generate three (3) chunks. You can iterate through
    the `df_gen` generator object as shown:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将生成三个（3）数据块。你可以像下面这样遍历`df_gen`生成器对象：
- en: '[PRE34]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The preceding code demonstrated how chunking works. Using the `chunksize` parameter
    should reduce memory usage since the code loads a smaller number of rows per chunk
    at a time.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码展示了chunking如何工作。使用`chunksize`参数应该减少内存使用，因为每次加载的行数较少。
- en: See also
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见：
- en: 'For additional information regarding these topics, take a look at the following
    links:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 若要获取关于这些主题的更多信息，请查看以下链接：
- en: For **SQLAlchemy**, you can visit [https://www.sqlalchemy.org/](https://www.sqlalchemy.org/)
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于**SQLAlchemy**，你可以访问[https://www.sqlalchemy.org/](https://www.sqlalchemy.org/)
- en: For the `pandas.read_sql` function visit [https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html)
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于`pandas.read_sql`函数，请访问[https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html)
- en: For the `pandas.read_sql_query` function visit [https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于`pandas.read_sql_query`函数，请访问[https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html)
- en: For the `pandas.read_sql_table` function visit [https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html)
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于`pandas.read_sql_table`函数，请访问[https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html)
- en: Reading data from Snowflake
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Snowflake读取数据
- en: A very common place to extract data for analytics is usually a company's *data
    warehouse*. Data warehouses host a massive amount of data that, in most cases,
    contains integrated data to support various reporting and analytics needs, in
    addition to historical data from various source systems.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常常见的数据分析提取来源通常是公司的*数据仓库*。数据仓库托管了大量的数据，这些数据大多是集成的，用来支持各种报告和分析需求，此外还包含来自不同源系统的历史数据。
- en: The evolution of the cloud brought us cloud data warehouses such as **Amazon
    Redshift**, **Google BigQuery**, **Azure SQL Data Warehouse**, and **Snowflake**.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算的发展为我们带来了云数据仓库，如**Amazon Redshift**、**Google BigQuery**、**Azure SQL Data
    Warehouse**和**Snowflake**。
- en: In this recipe, you will work with *Snowflake*, a powerful **Software as a Service**
    (**SaaS**) cloud-based data warehousing platform that can be hosted on different
    cloud platforms, such as **Amazon Web Services** (**AWS**), **Google Cloud Platform**
    (**GCP**), and **Microsoft Azure**. You will learn how to connect to Snowflake
    using Python to extract time series data and load it into a pandas DataFrame.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，你将使用*Snowflake*，一个强大的**软件即服务**（**SaaS**）基于云的数据仓库平台，可以托管在不同的云平台上，例如**Amazon
    Web Services**（**AWS**）、**Google Cloud Platform**（**GCP**）和**Microsoft Azure**。你将学习如何使用Python连接到Snowflake，提取时间序列数据，并将其加载到pandas
    DataFrame中。
- en: Getting ready
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe assumes you have access to Snowflake. You will explore three (3)
    different methods to connect to Snowflake so you will need to install three (3)
    different libraries .
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程假设你有访问Snowflake的权限。你将探索三种（3）不同的方法来连接Snowflake，因此你需要安装三种（3）不同的库。
- en: 'The recommended approach for the `snowflake-connector-python` library is to
    install it using **pip** allowing you to install *extras* such as `pandas` as
    shown:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的`雪花连接器-python`库安装方法是使用**pip**，这样可以让你安装像`pandas`这样的*附加组件*，如下所示：
- en: '[PRE35]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: You can also install with **conda**, but if you want to use `snowflake-connector-python`
    with pandas you will need to use the pip install.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用**conda**进行安装，但如果你想要将`snowflake-connector-python`与pandas一起使用，你需要使用pip安装。
- en: '[PRE36]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Make sure you the configuration file `database.cfg` that you created in the
    *Technical Requirements* section contains your **Snowflake** connection information:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你在*技术要求*部分创建的配置文件`database.cfg`包含了你的**Snowflake**连接信息：
- en: '[PRE37]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In this recipe you will be working with the `SNOWFLAKE_SAMPLE_DATA` database
    and the `TPCH_SF1 schema provided by Snowflake`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，你将使用`SNOWFLAKE_SAMPLE_DATA`数据库和Snowflake提供的`TPCH_SF1`模式。
- en: Capturing the proper `account` value can cause confusion for many. To ensure
    you have the right format use the *Copy account URL* option from Snowflake, which
    may look like this `https://abc1234.us-east-1.snowflakecomputing.com` the `abc1234.us-east-1`
    part is what you will use as the `account` value.
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 获取正确的`account`值可能会让许多人感到困惑。为了确保你获得正确的格式，请使用Snowflake中的*复制帐户URL*选项，它可能像这样`https://abc1234.us-east-1.snowflakecomputing.com`，其中`abc1234.us-east-1`部分是你将用作`account`值的部分。
- en: How to do it...
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We will explore three (3) methods and libraries to connect to the Snowflake
    database. In the first method, you will use the Snowflake Python connector to
    establish a connection and create a cursor to query and fetch the data. In the
    second method, you will use the Snowflake **SQLAlchemy**. In the third method,
    you will explore the **Snowpark** Python API. Let''s get started:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索三种（3）方法和库来连接到Snowflake数据库。在第一种方法中，你将使用Snowflake Python连接器建立连接，并创建一个游标来查询和提取数据。在第二种方法中，你将使用Snowflake
    **SQLAlchemy**。在第三种方法中，你将探索**Snowpark** Python API。让我们开始吧：
- en: Using snowflake-connector-python
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用snowflake-connector-python
- en: 'We will start by importing the necessary libraries:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从导入必要的库开始：
- en: '[PRE38]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Using `ConfigParser`, you will extract the content under the `[SNOWFLAKE]`
    section to avoid exposing or hardcoding your credentials. You can read the entire
    content of the `[SNOWFLAKE]` section and convert it into a dictionary object,
    as shown here:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `ConfigParser`，你将提取 `[SNOWFLAKE]` 部分下的内容，以避免暴露或硬编码你的凭据。你可以读取 `[SNOWFLAKE]`
    部分的所有内容并将其转换为字典对象，如下所示：
- en: '[PRE39]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You will need to pass the parameters to the `connector.connect()` to establish
    a connection with Snowflake. We can easily *unpack* the dictionary''s content
    since the dictionary keys match the parameter names. Once the connection has been
    established, we can create our *cursor*:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要将参数传递给 `connector.connect()` 来与 Snowflake 建立连接。我们可以轻松地 *解包* 字典内容，因为字典的键与参数名匹配。一旦连接建立，我们可以创建我们的
    *游标*：
- en: '[PRE40]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The cursor object has many methods, such as `execute`, `fetchall`, `fetchmany`,
    `fetchone`, fetch_pandas_all, and `fetch_pandas_batches`.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 游标对象有许多方法，如 `execute`、`fetchall`、`fetchmany`、`fetchone`、`fetch_pandas_all` 和
    `fetch_pandas_batches`。
- en: 'You will start with the `execute` method to pass a SQL query to the database,
    then use any of the available fetch methods to retrieve the data. In the following
    example, you will query the `ORDERS` table and then leverage the `fetch_pandas_all`
    method to retrieve the entire result set as a pandas DataFrame:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你将从 `execute` 方法开始，向数据库传递 SQL 查询，然后使用任何可用的获取方法来检索数据。在以下示例中，你将查询 `ORDERS` 表，然后利用
    `fetch_pandas_all` 方法将整个结果集作为 pandas DataFrame 检索：
- en: '[PRE41]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The previsou code could have been written as:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码可以写成如下：
- en: '[PRE42]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Inspect the DataFrame using `df.info()`:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `df.info()` 检查 DataFrame：
- en: '[PRE43]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'From the preceding output, you can see that the DataFrame''s Index is just
    a sequence of numbers and that the `O_ORDERDATE` column is not a `Date` field.
    You can parse the `O_ORDERDATE` column to a datetime type using `pandas.to_datetime()`
    function and then setting the column as the DataFrame’s index with the `DataFrame.set_index()`
    method:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从前面的输出中可以看到，DataFrame 的索引仅是一个数字序列，且 `O_ORDERDATE` 列不是一个 `Date` 类型的字段。你可以使用 `pandas.to_datetime()`
    函数将 `O_ORDERDATE` 列解析为日期时间类型，然后使用 `DataFrame.set_index()` 方法将该列设置为 DataFrame 的索引：
- en: '[PRE44]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let’s display the first four (4) columns and first five (5) rows of the `df_ts`
    DataFrame:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示 `df_ts` DataFrame 的前四（4）列和前五（5）行：
- en: '[PRE45]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Inspect the index of the DataFrame. Print the first two indexes:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 DataFrame 的索引。打印前两个索引：
- en: '[PRE46]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Finally, you can close the cursor and current connection
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以关闭游标和当前连接。
- en: '[PRE47]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: You now have a time series DataFrame with a `DatetimeIndex`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你拥有一个具有`DatetimeIndex`的时间序列 DataFrame。
- en: Using SQLAlchmey
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 SQLAlchemy
- en: In the previous recipe, `Reading data from a relational database`, you explored
    the pandas read_sql, read_sql_query, and read_sql_table functions. This was accomplished
    by utilizing SQLAlchemy and installing one of the supported dialicts. Here, we
    will use the Snowflake dialect after installing the snowflake-sqlalchemy driver.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的例子中，`从关系数据库读取数据`，你探索了 pandas 的 `read_sql`、`read_sql_query` 和 `read_sql_table`
    函数。这是通过使用 SQLAlchemy 和安装一个支持的方言来完成的。在这里，我们将在安装 `snowflake-sqlalchemy` 驱动程序后使用
    Snowflake 方言。
- en: SQLAclhemy is better integrated with pandas as you will experience in this section.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: SQLAlchemy 与 pandas 更好地集成，正如你将在本节中体验的那样。
- en: Start by importing the necessary libraries and reading the Snowflake connection
    parameters from the `[SNOWFLAKE]` section in the `database.cfg` file.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始时，导入必要的库，并从 `database.cfg` 文件中的 `[SNOWFLAKE]` 部分读取 Snowflake 连接参数。
- en: '[PRE48]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You will use the URL class to generate the URL connection string. We will create
    our engine object and then open a connection with the engine.connect():'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将使用 URL 类来生成 URL 连接字符串。我们将创建我们的引擎对象，然后使用 `engine.connect()` 打开连接：
- en: '[PRE49]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, you can use either the read_sql or read_sql_query to execute our query
    against the ORDERS table in the SNOWFLAKE_SAMPLE_DATA database:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以使用 `read_sql` 或 `read_sql_query` 来对 SNOWFLAKE_SAMPLE_DATA 数据库中的 `ORDERS`
    表执行查询：
- en: '[PRE50]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Notice how we were able to parse the o_orderdate column and set it as index
    all in one step when compared to the Snowflake Python connector method performed
    earlier.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意与之前使用 Snowflake Python 连接器的方法相比，我们是如何在一个步骤中解析 `o_orderdate` 列并将其设置为索引的。
- en: Finally, close the connection to the database
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，关闭数据库连接。
- en: '[PRE51]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The code could be further simplified by using a **context manager** to automatically
    allocate and release resources. The following example uses the `with engine.connect()`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 **上下文管理器**，可以进一步简化代码，以自动分配和释放资源。以下示例使用了 `with engine.connect()`：
- en: '[PRE52]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This should produce the same results without needing to close connection and
    dispose the engine.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这样应该能够得到相同的结果，而无需关闭连接或处理引擎。
- en: Using snowflake-snowpark-python
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 snowflake-snowpark-python
- en: The Snowpark API supports Java, Python, and Scala. You have already installed
    the `snowflake-snowpark-python` as described in the *Getting Ready* section of
    this recipe.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark API 支持 Java、Python 和 Scala。你已经按照本配方中 *准备工作* 部分的描述安装了 `snowflake-snowpark-python`。
- en: Start by importing the necessary libraries and reading the Snowflake connection
    parameters from the `[SNOWFLAKE]` section in the `database.cfg` file
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从导入必要的库并从 `database.cfg` 文件的 `[SNOWFLAKE]` 部分读取 Snowflake 连接参数开始
- en: '[PRE53]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Create a session by establishing a connection with the Snowflake database
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过与 Snowflake 数据库建立连接来创建会话
- en: '[PRE54]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The session has several `DataFrameReader` methods such as `read`, `table`, and
    `sql.` Any of these methods would return a Snowpark DataFrame object. The returned
    Snowpark DataFrame object has access to the `to_pandas` method to convert into
    a more familiar pandas DataFrame. You will explore the `read`, `table`, and `sql`
    methods to return the same result set.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 会话有多个 `DataFrameReader` 方法，如 `read`、`table` 和 `sql`。这些方法中的任何一个都会返回一个 Snowpark
    DataFrame 对象。返回的 Snowpark DataFrame 对象可以使用 `to_pandas` 方法转换为更常见的 pandas DataFrame。你将探索
    `read`、`table` 和 `sql` 方法，以返回相同的结果集。
- en: Start with the `read` method. More specifically, you will be using the `read.table`
    and pass it a table name. This will return the content of the table and covert
    into a pandas DataFrame with the `to_pandas` method. Think of this as equivalent
    to `SELECT * FROM TABLE`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `read` 方法开始。更具体地说，你将使用 `read.table` 并传入一个表名。这将返回该表的内容并通过 `to_pandas` 方法转换为
    pandas DataFrame。可以将其视为等同于 `SELECT * FROM TABLE`。
- en: '[PRE55]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Similarly, the `table` method take a table name, and the returned object (Snowpark
    DataFrame) as access to the `to_pandas` method:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，`table` 方法接受一个表名，返回的对象（Snowpark DataFrame）可以使用 `to_pandas` 方法：
- en: '[PRE56]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Lastly, the `sql` method which takes a SQL query:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`sql` 方法接受一个 SQL 查询：
- en: '[PRE57]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: All three approaches should produce the same pandas DataFrame.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种方法应该产生相同的 pandas DataFrame。
- en: How it works...
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The Snowflake Python connector, Snowflake SQLAlchmey driver, and Snowpark Python
    require the same input variables to establish a connection to the Snowflake database.
    These include the following:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake Python 连接器、Snowflake SQLAlchemy 驱动程序和 Snowpark Python 需要相同的输入变量来建立与
    Snowflake 数据库的连接。这些包括以下内容：
- en: '![Table 3.1 – Input variables for the Snowflake Python connector](img/file25.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![表 3.1 – Snowflake Python 连接器的输入变量](img/file25.jpg)'
- en: Table 3.1 – Input variables for the Snowflake Python connector
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.1 – Snowflake Python 连接器的输入变量
- en: Recall that in the previous activity, you used the same configuration for all
    three methods in the `database.cfg` file under the `[SNOWFLAKE]` section.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在前一个活动中，你在 `database.cfg` 文件的 `[SNOWFLAKE]` 部分为所有三种方法使用了相同的配置。
- en: Snowflake Python Connector
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Snowflake Python 连接器
- en: When using the Python connector, you first establish a connection to the database
    with `con = connector.connect(**params).` Once the connection is accepted, you
    create a **cursor** object with `cursor = con.cursor()`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Python 连接器时，你首先通过 `con = connector.connect(**params)` 来建立与数据库的连接。一旦连接被接受，你使用
    `cursor = con.cursor()` 创建一个**游标**对象。
- en: The cursor provides methods for performing execute and fetch operations such
    as `describe()`, `execute()`, `execute_async()`, `executemany()`, `fetchone()`,
    `fetchall()`, `fetchmany()`, `fetch_pandas_all()`, and `fetch_pandas_batches()`,
    and each cursor has several attributes including `description`, `rowcount`, `rownumber`,
    to name few. Note there are familiar methods and attributes discussed in the previous
    recipe, *Reading data from a relational database*, when using the Python connector
    **psycopg.**
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 游标提供了执行和获取操作的方法，如 `describe()`、`execute()`、`execute_async()`、`executemany()`、`fetchone()`、`fetchall()`、`fetchmany()`、`fetch_pandas_all()`
    和 `fetch_pandas_batches()`，每个游标还具有若干属性，包括 `description`、`rowcount`、`rownumber`
    等。注意，当使用 Python 连接器 **psycopg** 时，之前的配方 *从关系数据库读取数据* 中讨论了熟悉的方法和属性。
- en: '`Execute()` – executes a SQL query (CRUD) or command to the database'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Execute()` – 执行 SQL 查询（CRUD）或命令到数据库'
- en: '`executemany()` – executes the same database operation with a sequence of input
    data, for example, this can be useful with INSERT INTO for bulk insert.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`executemany()` – 使用一系列输入数据执行相同的数据库操作，例如，这在使用 INSERT INTO 进行批量插入时非常有用。'
- en: '`Fetchall()` – returns all remaining records from the current query result
    set'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Fetchall()` – 返回当前查询结果集中的所有剩余记录'
- en: '`fetchone()` - returns the next record (one record) from the current query
    result set'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fetchone()` - 从当前查询结果集中返回下一条记录（一条记录）'
- en: '`fetchmany(n)` – returns `n` number of records from the current query result
    set'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fetchmany(n)` – 从当前查询结果集中返回`n`条记录'
- en: '`fetch_pandas_all() -` returns all remaining records from the current query
    result set and loads them into a pandas DataFrame'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fetch_pandas_all()` - 返回当前查询结果集中的所有剩余记录，并将它们加载到pandas DataFrame中'
- en: '`fetch_pandas_batches()` - returns a subset of the remaining records form the
    current query result set and loads them into a pandas DataFrame'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fetch_pandas_batches()` - 返回当前查询结果集中的一部分剩余记录，并将它们加载到pandas DataFrame中'
- en: '`close()` - close the current cursor and free associated resources'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`close()` - 关闭当前游标并释放相关资源'
- en: '`describe()` – returns metadata about the result set without executing the
    query. Alternatively, you can use `execute()` followed by `description` attribute
    to obtain the same metadata information.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`describe()` – 返回结果集的元数据，但不会执行查询。或者，你可以使用`execute()`，然后使用`description`属性来获取相同的元数据信息。'
- en: For a complete list of attributes and methods, you can refer to the official
    documentation at [https://docs.snowflake.com/en/user-guide/python-connector-api.html#object-cursor](https://docs.snowflake.com/en/user-guide/python-connector-api.html#object-cursor).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看完整的属性和方法列表，请参考官方文档：[https://docs.snowflake.com/en/user-guide/python-connector-api.html#object-cursor](https://docs.snowflake.com/en/user-guide/python-connector-api.html#object-cursor)。
- en: SQLAlchmey API
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SQLAlchemy API
- en: When SQLAlchemy, you are able to leverage the `pandas.read_sql, pandas.read_sql_query,`
    and `pandas.read_sql_query` reader functions and leverage many of the available
    parameters to transform and process the data at read time such as `index_col`
    and `parse_dates`. On the other hand, when using the Snowflake Python connector,
    the `fetch_pandas_all()` function does not take in any parameters, and you will
    need to parse and adjust the DataFrame afterward.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用SQLAlchemy时，你可以利用`pandas.read_sql`、`pandas.read_sql_query`和`pandas.read_sql_query`读取函数，并利用许多可用参数在读取时转换和处理数据，如`index_col`和`parse_dates`。另一方面，当使用Snowflake
    Python连接器时，`fetch_pandas_all()`函数不接受任何参数，你需要在之后解析和调整DataFrame。
- en: 'The Snowflake SQLAlchemy library provides a convenience method, `URL`, to help
    construct the connection string to connect to the Snowflake database. Typically,
    SQLAlchemy expects a URL to be provided in the following format:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake SQLAlchemy库提供了一个方便的方法`URL`，帮助构建连接字符串以连接到Snowflake数据库。通常，SQLAlchemy期望提供以下格式的URL：
- en: '[PRE58]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Using the `URL` method, we passed our parameters, and the method took care
    of constructing the connection string that is expected:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`URL`方法，我们传递了参数，方法会自动构造所需的连接字符串：
- en: '[PRE59]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Alternatively, we stored our Snowflake parameters in the configuration file
    database.cfg and stored as Python dictionary. This way, you will not be exposing
    your credentials within the code.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们将Snowflake参数存储在配置文件database.cfg中，并以Python字典的形式存储。这样，你就不会在代码中暴露你的凭证。
- en: '[PRE60]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: If you compare the process in this recipe, using **SQLAlchemy**, for Snowflake,
    and that of the previous recipe, *Reading data from a relational database*, you
    will observe similarities in the process and code. This is one of the advantages
    of using SQLAlchemy, it creates a standard process across a variety of databases
    as long as SQLAlchmey supports them. *SQLAlchemy is well integrated with pandas
    and makes it easy to switch dialects (backend databases) without much change to
    your code*.
  id: totrans-261
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你比较本食谱中的过程，使用**SQLAlchemy**连接Snowflake，与之前食谱中的*从关系数据库读取数据*过程，你会发现两者在过程和代码上有许多相似之处。这就是使用SQLAlchemy的优势之一，它为各种数据库创建了一个标准流程，只要SQLAlchemy支持这些数据库。*SQLAlchemy与pandas集成得很好，可以轻松地切换方言（后端数据库），而无需对代码做太多修改*。
- en: Snowpark API
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Snowpark API
- en: In the previous methods, you simply used the libraries `snowflake-connector-python`
    and `snowflake-connector-python` as connectors to your Snowflake database, and
    then fetch the data to process the data locally.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的方法中，你只是使用了`snowflake-connector-python`和`snowflake-connector-python`库作为连接器连接到你的Snowflake数据库，然后提取数据以在本地处理。
- en: Snowpark offers more than just a mechanism to connect to your database. It allows
    you to process the data directly within the Snowflake environment on the cloud
    without the need to move the data outside or process locally. In addition, Snowpark
    is well suited for more complex tasks such as building complex data pipelines
    or Snowpark ML for working with machine learning models all on the Snowflake cloud.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark 不仅仅提供了一种连接数据库的机制。它允许您直接在 Snowflake 云环境中处理数据，而无需将数据移出或在本地处理。此外，Snowpark
    还非常适合执行更复杂的任务，如构建复杂的数据管道或使用 Snowpark ML 处理机器学习模型，所有这些都在 Snowflake 云中完成。
- en: In our recipe, and similar to the other methods, we need to establish our connection
    with Snowflake. This was accomplished using the `Session` class.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的方案中，类似于其他方法，我们需要与 Snowflake 建立连接。这是通过使用`Session`类来实现的。
- en: '[PRE61]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'There are similarity in the API and conecpts between Snowpark and PySpark (Spark).
    More specifically, Snowpark DataFrame are considered *lazily-evaluated relational
    dataset*. The `to_pandas` method does two things: it executes the query and loads
    the results into a pandas DataFrame (data is being fetched outside of Snowflake).
    To convert the pandas DataFrame back to a Snowpark DataFrame (inside Snowflake)
    you can use the `create_dataframe` method as shown:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark 和 PySpark（Spark）在 API 和概念上有很多相似之处。更具体地说，Snowpark DataFrame 被视为*延迟求值的关系数据集*。`to_pandas`方法执行了两件事：它执行查询并将结果加载到
    pandas DataFrame 中（数据会被从 Snowflake 外部提取）。要将 pandas DataFrame 转换回 Snowpark DataFrame（在
    Snowflake 内部），您可以使用如下的`create_dataframe`方法：
- en: '[PRE62]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'In order successfully execute the preceding code, you need have *write permission*
    since Snowflake will create a **temporary** table to store the pandas DataFrame
    (inside Snowflake) and then returns a Snowpark DataFrame that points to that temporary
    table. Alternatively, if you want to persist the pandas DataFrame into a table
    you can use the `write_pandas` method as shown:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功执行前面的代码，您需要具有*写权限*，因为 Snowflake 会创建一个**临时**表来存储 pandas DataFrame（在 Snowflake
    中），然后返回一个指向该临时表的 Snowpark DataFrame。或者，如果您希望将 pandas DataFrame 持久化到一个表中，您可以使用如下的`write_pandas`方法：
- en: '[PRE63]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: In the preceding code, you passed the pandas DataFrame and table name.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，您传递了 pandas DataFrame 和表名。
- en: There's more...
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多...
- en: You may have noticed that the columns in the returned DataFrame, when using
    the Snowflake Python connector and Snowpark, all came back in uppercase, while
    they were lowercased when using Snowflake SQLAlchemy.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，当使用 Snowflake Python 连接器和 Snowpark 时，返回的 DataFrame 中的列名全部以大写字母显示，而在使用
    Snowflake SQLAlchemy 时，它们则是小写的。
- en: The reason for this is because Snowflake, by default, stores unquoted object
    names in uppercase when these objects are created. In the previous code, for example,
    our `Order Date` column was returned as `O_ORDERDATE`.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以如此，是因为 Snowflake 默认在创建对象时将未加引号的对象名称存储为大写。例如，在之前的代码中，我们的`Order Date`列被返回为`O_ORDERDATE`。
- en: To explicitly indicate the name is case-sensitive, you will need to use quotes
    when creating the object in Snowflake (for example, `'o_orderdate'` or `'OrderDate'`).
    In contrast, using Snowflake SQLAlchemy converts the names into lowercase by default.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确指出名称是区分大小写的，您在创建 Snowflake 对象时需要使用引号（例如，`'o_orderdate'` 或 `'OrderDate'`）。相对而言，使用
    Snowflake SQLAlchemy 时，默认会将名称转换为小写。
- en: See also
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: For more information on the **Snowflake Connector for Python**, you can visit
    the official documentation at [https://docs.snowflake.com/en/user-guide/python-connector.html](https://docs.snowflake.com/en/user-guide/python-connector.html)
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关**Snowflake Python 连接器**的更多信息，您可以访问官方文档：[https://docs.snowflake.com/en/user-guide/python-connector.html](https://docs.snowflake.com/en/user-guide/python-connector.html)
- en: For more information regarding **Snowflake SQLAlchemy**, you can visit the official
    documentation at [https://docs.snowflake.com/en/user-guide/sqlalchemy.html](https://docs.snowflake.com/en/user-guide/sqlalchemy.html)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关**Snowflake SQLAlchemy**的更多信息，您可以访问官方文档：[https://docs.snowflake.com/en/user-guide/sqlalchemy.html](https://docs.snowflake.com/en/user-guide/sqlalchemy.html)
- en: For more information regarding **Snowpark API**, you can visit the official
    documentation at [https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/index](https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/index)
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关**Snowpark API**的更多信息，您可以访问官方文档：[https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/index](https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/index)
- en: Reading data from a document database
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从文档数据库读取数据
- en: '**MongoDB**, a **NoSQL** database, stores data in documents and uses BSON (a
    JSON-like structure) to store schema-less data. Unlike relational databases, where
    data is stored in tables that consist of rows and columns, document-oriented databases
    store data in collections and documents.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**MongoDB** 是一个**NoSQL** 数据库，使用文档存储数据，并使用 BSON（一种类似 JSON 的结构）来存储无模式的数据。与关系型数据库不同，关系型数据库中的数据存储在由行和列组成的表中，而面向文档的数据库则将数据存储在集合和文档中。'
- en: A document represents the lowest granular level of data being stored, as rows
    do in relational databases. A collection, like a table in relational databases,
    stores documents. Unlike relational databases, a collection can store documents
    of different schemas and structures.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 文档代表存储数据的最低粒度，就像关系型数据库中的行一样。集合像关系型数据库中的表一样存储文档。与关系型数据库不同，集合可以存储不同模式和结构的文档。
- en: Getting ready
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, it is assumed that you have a running instance of MongoDB. To
    get ready for this recipe, you will need to install the `PyMongo` Python library
    to connect to MongoDB.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，假设你已经有一个正在运行的 MongoDB 实例。为准备此教程，你需要安装 `PyMongo` Python 库来连接 MongoDB。
- en: 'To install MongoDB using `conda`, run the following command:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `conda` 安装 MongoDB，请运行以下命令：
- en: '[PRE64]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'To install MongoDB using `pip`, run the following command:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `pip` 安装 MongoDB，请运行以下命令：
- en: '[PRE65]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'If you do not have access to a PostgreSQL database, then the fastest way to
    get up and running is via Docker ([https://hub.docker.com/_/mongo](https://hub.docker.com/_/mongo)).
    The following is an example command:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法访问 PostgreSQL 数据库，最快的启动方式是通过 Docker ([https://hub.docker.com/_/mongo](https://hub.docker.com/_/mongo))。以下是一个示例命令：
- en: '[PRE66]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Alternatively, you can try **MongoDB Atlas** for free here [https://www.mongodb.com/products/platform/atlas-database](https://www.mongodb.com/products/platform/atlas-database).
    **MongoDB Atlas** is a fully managed cloud database that can be deployed on your
    favorite cloud providers such as AWS, Azure, and GCP.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你可以尝试免费使用**MongoDB Atlas**，访问 [https://www.mongodb.com/products/platform/atlas-database](https://www.mongodb.com/products/platform/atlas-database)。**MongoDB
    Atlas** 是一个完全托管的云数据库，可以部署在你喜欢的云提供商上，如 AWS、Azure 和 GCP。
- en: NOTE ABOUT USING MONGODB ATLAS
  id: totrans-292
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于使用 MongoDB Atlas 的注意事项
- en: ''
  id: totrans-293
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you are connecting to MongoDB Atlas (Cloud) Free Tier or their M2/M5 shared
    tier cluster, then you will be using the `mongodb+srv` protocol. In this case,
    you can either specify this during the pip install with `python -m pip install
    "pymongo[srv]"`
  id: totrans-294
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你连接的是 MongoDB Atlas（云）免费版或其 M2/M5 共享版集群，则需要使用 `mongodb+srv` 协议。在这种情况下，你可以在
    pip 安装时指定 `python -m pip install "pymongo[srv]"`
- en: Optionally, for a GUI interface to your MongoDB you can install **MongoDB Compass**
    form here [https://www.mongodb.com/products/tools/compass](https://www.mongodb.com/products/tools/compass)
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，如果你希望通过图形界面访问 MongoDB，可以从这里安装**MongoDB Compass** [https://www.mongodb.com/products/tools/compass](https://www.mongodb.com/products/tools/compass)
- en: I am using MongoDB Compass to create the database, collection, and load the
    data. In Chapter 5, *Persisting Time Series Data to Databases*, you learn how
    to create your database, collection, and load data using Python.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在使用 MongoDB Compass 来创建数据库、集合并加载数据。在第五章，*将时间序列数据持久化到数据库*，你将学习如何使用 Python 创建数据库、集合并加载数据。
- en: Using **Compass** select the option **Create Database**. For the Database Name
    you can enter `stock_data` and `microsoft` for the **Collection** Name. Click
    the **Time-Series** checkbox and specify the `date` as the **timeField**.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**Compass**选择**创建数据库**选项。对于数据库名称，可以输入 `stock_data`，对于**集合**名称，可以输入 `microsoft`。勾选**时间序列**复选框，并将
    `date` 设置为**时间字段**。
- en: '![Figure 3.4 – MongoDB Compass Create Database screen](img/file26.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – MongoDB Compass 创建数据库界面](img/file26.png)'
- en: Figure 3.4 – MongoDB Compass Create Database screen
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – MongoDB Compass 创建数据库界面
- en: Once the **Database** and **Collection** are created, click **Import Data**
    and select the MSFT stock dataset provided in the `datasets/Ch3/MSFT.csv` folder.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦**数据库**和**集合**创建完成，点击**导入数据**并选择 `datasets/Ch3/MSFT.csv` 文件夹中的 MSFT 股票数据集。
- en: '![Figure 3.5 – MongoDB Compass Import Data screen](img/file27.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – MongoDB Compass 导入数据界面](img/file27.png)'
- en: Figure 3.5 – MongoDB Compass Import Data screen
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – MongoDB Compass 导入数据界面
- en: '![Figure 3.6- MonogDB Compass review data types screen before import](img/file28.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – MongoDB Compass 在导入前审核数据类型界面](img/file28.png)'
- en: Figure 3.6- MonogDB Compass review data types screen before import
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – MongoDB Compass 在导入前审核数据类型界面
- en: The final page confirms the data types. Finally, click **Import**.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 最终页面确认数据类型。最后，点击**导入**。
- en: How to do it…
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作……
- en: In this recipe, you will connect to the MongoDB instance you have set up. If
    you are using an on-premises install (local install or Docker container), then
    your connection string will be something like `mongodb://<username>:<password>@<host>:<port>/<DatabaseName>.`
    If you are using Atlas, then your connection may look more like `mongodb+srv://<username>:<password>@<clusterName>.mongodb.net/<DatabaseName>?retryWrites=true&w=majority`.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将连接到你已设置的MongoDB实例。如果你使用的是本地安装（本地安装或Docker容器），则连接字符串可能类似于`mongodb://<username>:<password>@<host>:<port>/<DatabaseName>`。如果你使用的是Atlas，连接字符串可能更像是`mongodb+srv://<username>:<password>@<clusterName>.mongodb.net/<DatabaseName>?retryWrites=true&w=majority`。
- en: 'Perform the following steps:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'First, let''s import the necessary libraries:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE67]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Establish a connection to MongoDB. For a self-hosted instance, such as a local
    install, this would look something like this:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 建立与MongoDB的连接。对于自托管实例，例如本地安装，连接字符串可能是这样的：
- en: '[PRE68]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'This is equivalent to the following:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这等同于以下内容：
- en: '[PRE69]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: If your self-hosted MongoDB instance has a username and password, you must supply
    those.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的自托管MongoDB实例具有用户名和密码，你必须提供这些信息。
- en: 'The `uri_parser` is a useful utility function that allows you to *validate*
    a MongoDB URL as shown:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`uri_parser`是一个有用的实用函数，允许你*验证*MongoDB的URL，如下所示：'
- en: '[PRE70]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'If you are connecting to **MongoDB** **Atlas** then your connection string
    will look more like this:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你连接的是**MongoDB** **Atlas**，那么你的连接字符串看起来应该像这样：
- en: '[PRE71]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: In the previous recipes of this chapter, we leveraged a configuration file,
    for example a `database.cfg` file to store our connection information and hide
    our credentials. You should follow that recommendation as well.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章之前的教程中，我们使用了一个配置文件，例如`database.cfg`文件，用来存储我们的连接信息并隐藏凭证。你也应该遵循这个建议。
- en: 'If your username or password contain special characters including the space
    chatacter (`: / ? # [ ] @ ! $ & '' ( ) * , ; = %`),then you will need to encode
    them. You can perform percent-encoding (percent-escape) using the `quote_plus()`
    function from the `urlib` Python library.'
  id: totrans-321
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你的用户名或密码包含特殊字符，包括空格字符（`:/?#[]@!$&'()* ,;=%`），你需要对它们进行编码。你可以使用`urllib` Python库中的`quote_plus()`函数进行百分号编码（百分号转义）。
- en: ''
  id: totrans-322
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Here is an example:'
  id: totrans-323
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是一个示例：
- en: ''
  id: totrans-324
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`username = urllib.parse.quote_plus(''user!*@'')`'
  id: totrans-325
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`username = urllib.parse.quote_plus(''user!*@'')`'
- en: ''
  id: totrans-326
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`password = urllib.parse.quote_plus(''pass/w@rd'')`'
  id: totrans-327
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`password = urllib.parse.quote_plus(''pass/w@rd'')`'
- en: ''
  id: totrans-328
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For more information, you can read the documentation here [https://www.mongodb.com/docs/atlas/troubleshoot-connection/#std-label-special-pass-characters](https://www.mongodb.com/docs/atlas/troubleshoot-connection/#std-label-special-pass-characters)
  id: totrans-329
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 更多信息，请阅读[这里的文档](https://www.mongodb.com/docs/atlas/troubleshoot-connection/#std-label-special-pass-characters)
- en: 'Once connected, you can list all the databases available. In this example,
    I named the database `stock_data` and the collection `microsoft`:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦连接成功，你可以列出所有可用的数据库。在此示例中，我将数据库命名为`stock_data`，集合命名为`microsoft`：
- en: '[PRE72]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'You can list the collections that are available under the `stock_data` database
    using `list_collection_names`:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用`list_collection_names`列出`stock_data`数据库下可用的集合：
- en: '[PRE73]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Now, you can specify which collection to query. In this case, we are interested
    in the one called `microsoft`:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以指定查询哪个集合。在这个例子中，我们感兴趣的是名为`microsoft`的集合：
- en: '[PRE74]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Now, query the database into a pandas DataFrame using the `find` method:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用`find`方法将数据库查询到一个pandas DataFrame中：
- en: '[PRE75]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: How it works…
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The first step is to connect to the database, which we did by creating a client
    object with `MongoClient` for MongoDB instance. This will give you access to a
    set of methods, such as `list_databases_names(), list_databases()`, and additional
    attributes, such as `address` and `HOST`.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是连接到数据库，我们通过使用`MongoClient`创建MongoDB实例的客户端对象来实现。这将使你能够访问一组方法，如`list_databases_names()`、`list_databases()`，以及其他属性，如`address`和`HOST`。
- en: '`MongoClient()` accepts a connection string that should follow MongoDB''s URI
    format, as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '`MongoClient()`接受一个连接字符串，该字符串应遵循MongoDB的URI格式，如下所示：'
- en: '[PRE76]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Alternatively, the same can be accomplished by explicitly providing *host*
    (string) and *port* (numeric) positional arguments, as follows:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，也可以通过显式提供*host*（字符串）和*port*（数字）位置参数来完成相同的操作，如下所示：
- en: '[PRE77]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The host string can either be the hostname or the IP address, as shown here:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 主机字符串可以是主机名或IP地址，如下所示：
- en: '[PRE78]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Note that to connect to your **localhost** that uses the default port (`27017`),
    you can establish a connection without providing any arguments, as shown in the
    following code:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，要连接到使用默认端口（`27017`）的 **localhost**，你可以在不提供任何参数的情况下建立连接，如以下代码所示：
- en: '[PRE79]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Further, you can explicitly supply the named parameters as shown:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以显式地提供命名参数，如下所示：
- en: '[PRE80]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Let’s explore these parameters:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来探讨这些参数：
- en: '`host` – this can be a hostname or IP address or a MongoDB URI. It can also
    take a Python list of hostnames.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`host` – 这可以是主机名、IP 地址或 MongoDB URI。它也可以是一个包含主机名的 Python 列表。'
- en: '`password` – your assigned password. Read the note regarding special characters
    in the *Getting Ready* section.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`password` – 你分配的密码。请参阅 *Getting Ready* 部分中关于特殊字符的说明。'
- en: '`username` - your assigned username. Read the note regarding special characters
    in the *Getting Ready* section.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`username` – 你分配的用户名。请参阅 *Getting Ready* 部分中关于特殊字符的说明。'
- en: '`document_class` – specify the class to use for the documents returned from
    your query. The default value is `dict`.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`document_class` – 指定用于查询结果中文档的类。默认值是 `dict`。'
- en: '`tz_aware` – specify if datetime instances are time zone aware. The default
    is `False`, meaning they are naive (not time zone aware).'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tz_aware` – 指定 datetime 实例是否是时区感知的。默认值为 `False`，意味着它们是“天真”的（不具备时区感知）。'
- en: '`connect` – Whether to immediately connect to the MongoDB instance. The default
    value is `True`.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`connect` – 是否立即连接到 MongoDB 实例。默认值是 `True`。'
- en: For additional parameters, you can reference the official documentation page
    here [https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html](https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 若需要更多参数，你可以参考官方文档页面 [https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html](https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html)。
- en: Once the connection to your MongoDB instance has been established, you can specify
    which database to use, list its collections, and query any available collections.
    The overall flow in terms of navigation before you can query and retrieve the
    documents is to specify the **database**, select the **collection** you are interested
    in, and then submit the **query**.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦与 MongoDB 实例建立连接，你可以指定使用的数据库，列出其集合，并查询任何可用的集合。在可以查询和检索文档之前的整体流程是：指定 **数据库**，选择你感兴趣的
    **集合**，然后提交 **查询**。
- en: In the preceding example, our database was called `stock_data`, which contained
    a collection called `microsoft`. You can have multiple collections in a database
    and multiple documents in a collection. To think of this in terms of relational
    databases, recall that a collection is like a table and that documents represent
    rows in that table.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们的数据库名为 `stock_data`，其中包含一个名为 `microsoft` 的集合。一个数据库可以包含多个集合，而一个集合可以包含多个文档。如果从关系型数据库的角度考虑，集合就像是表格，而文档代表表格中的行。
- en: 'In PyMongo, you can specify a database using different syntax, as shown in
    the following code. Keep in mind that all these statements will produce a `pymongo.database.Database`
    object:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyMongo 中，你可以使用不同的语法来指定数据库，如以下代码所示。请记住，所有这些语句都会生成一个 `pymongo.database.Database`
    对象：
- en: '[PRE81]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: In the preceding code, `get_database()` can take in additional arguments for
    the `codec_options`, `read_preference, write_concern`, and `read_concern` parameters,
    where the latter two are focused more on operations across nodes and how to determine
    if the operation was successful or not.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`get_database()` 可以接受额外的参数，如 `codec_options`、`read_preference`、`write_concern`
    和 `read_concern`，其中后两个参数更关注节点间的操作以及如何确定操作是否成功。
- en: 'Similarly, once you have the `PyMongo` database object, you can specify a collection
    using different syntax, as shown in the following example:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，一旦你拥有了 `PyMongo` 数据库对象，你可以使用不同的语法来指定集合，如以下示例所示：
- en: '[PRE82]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The `get_collection()` method provides additional parameters, similar to `get_database()`.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_collection()` 方法提供了额外的参数，类似于 `get_database()`。'
- en: The three syntax variations in the preceding example return a `pymongo.database.Collection`
    object, which comes with additional built-in methods and attributes such as `find`,
    `find_one`, `find_one_and_delete`, `find_one_and_reaplce`, `find_one_and_update`,
    `update`, `update_one`, `update_many`, `delete_one`, and `delete_many`, to name
    a few.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的三个语法变体返回一个 `pymongo.database.Collection` 对象，它带有额外的内建方法和属性，如 `find`、`find_one`、`find_one_and_delete`、`find_one_and_replace`、`find_one_and_update`、`update`、`update_one`、`update_many`、`delete_one`
    和 `delete_many` 等。
- en: 'Let’s explore the different *retrieval* collection methods:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索不同的*检索*集合方法：
- en: '`find()` – retrieves multiple documents from a collection based on the submitted
    query'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`find()` – 基于提交的查询从集合中检索多个文档。'
- en: '`find_one()` – retrieves a single document form a collection based on the submitted
    query. If multiple documents match, then the first instance is returned.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`find_one()` – 基于提交的查询，从集合中检索单个文档。如果多个文档匹配，则返回第一个匹配的文档。'
- en: '`find_one_and_delete()` – finds the single document like `find_one`, but it
    deletes it from the collection, and returns it (the deleted document).'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`find_one_and_delete()` – 查找单个文档，类似于`find_one`，但它会从集合中删除该文档，并返回删除的文档。'
- en: '`find_one_and_replace()` - finds a single document and replaces it with a new
    document, returning either the original or the replaced document.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`find_one_and_replace()` - 查找单个文档并用新文档替换它，返回原始文档或替换后的文档。'
- en: '`find_one_and_update()` - finds a single document and updates it, returning
    either the original or the updated document. This is different than the `find_one_and_replace`,
    since it updates the existing document instead of replacing the entire document.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`find_one_and_update()` - 查找单个文档并更新它，返回原始文档或更新后的文档。与`find_one_and_replace`不同，它是更新现有文档，而不是替换整个文档。'
- en: Once you are at the collection level, you can start querying the data. In the
    recipe, you used `find()`, which you can think of as doing something similar to
    a `SELECT` statement in SQL.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你到达集合级别，你可以开始查询数据。在本示例中，你使用了`find()`，它类似于SQL中的`SELECT`语句。
- en: 'In the `How to do it…` section, in `step 5`, you queried the entire collection
    to retrieve all the documents using this line of code:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在`如何实现…`部分的`步骤5`中，你查询了整个集合，通过以下代码检索所有文档：
- en: '[PRE83]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: The empty dictionary, `{}`, in `find()` represents our filtering criteria. When
    you pass an empty filter criterion with `{}`, you are retrieving everything. This
    resembles `SELECT *` in a SQL database. Alternatively, one could also use `collection.find()`
    to retrieve all documents.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 空字典`{}`在`find()`中表示我们的过滤条件。当你传递一个空的过滤条件`{}`时，实际上是检索所有数据。这类似于SQL数据库中的`SELECT
    *`。另外，你也可以使用`collection.find()`来检索所有文档。
- en: To query documents in MongoDB you will need to be familiar with the MongoDB
    Query Language (MQL). You would normally write your query and pass it to the `find`
    method which acts like a filter.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 要在MongoDB中查询文档，你需要熟悉MongoDB查询语言（MQL）。通常，你会编写查询并将其传递给`find`方法，`find`方法类似于一个过滤器。
- en: 'A query or a filter takes a **key-value** pair to return a select number of
    documents where the keys match the values specified. The following is an example
    of a query to find stocks with closing price great than 130:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 查询或过滤器采用**键值对**来返回匹配指定值的文档。以下是一个示例查询，查找收盘价大于130的股票：
- en: '[PRE84]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: The results objects is actually a **cursor** and does not contain the result
    set yet. You can loop through the cursor or convert into a DataFrame. Generally,
    when `collection.find()` is executed, it returns a **cursor** (more specifically,
    a `pymongo.cursor.Cursor` object). This cursor object is just a pointer to the
    result set of the query, which allows you to iterate over the results. You can
    then use a `for` loop or `next()` method (think of a Python iterator). However,
    in this recipe, instead of looping through our cursor object, we conveniently
    converted the entire result set into a pandas DataFrame.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 结果对象实际上是一个**游标**，它还没有包含结果集。你可以遍历游标或将其转换为DataFrame。通常，当执行`collection.find()`时，它返回一个**游标**（更具体地说，是一个`pymongo.cursor.Cursor`对象）。这个游标对象只是查询结果集的指针，允许你遍历结果。你可以使用`for`循环或`next()`方法（可以类比为Python中的迭代器）。然而，在这个示例中，我们没有直接循环遍历游标对象，而是将整个结果集便捷地转换成了pandas
    DataFrame。
- en: Here is an example of retrieving the result set into a pandas DataFrame
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个将结果集检索到pandas DataFrame的示例。
- en: '[PRE85]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Notice from the output the `_id` column is added which was not part of the original
    `MSFT.csv` file. This was automatically added by MongoDB as a **unique identifier**
    for each document in a collection.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出中添加了`_id`列，这是原始`MSFT.csv`文件中没有的。MongoDB自动为集合中的每个文档添加了这个**唯一标识符**。
- en: 'In the preceding code, the query, which acts as a filter to only retrieve data
    where `close` values are greater than `130.`PyMongo allows you to pass a dictionary
    (key-value pair) that specifies which fields to be retrieved. Here is an example:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，查询作为过滤器，仅检索`close`值大于`130`的数据。PyMongo允许你传递一个字典（键值对），指定要检索的字段。下面是一个示例：
- en: '[PRE86]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: In the preceding code we specified that `_id` should not be returned, and only
    `date`, `close`, and `volume` fields to be returned.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们指定了不返回`_id`，只返回`date`、`close`和`volume`字段。
- en: 'Lastly, in our previous example, notice the `$gt` used in the query. This represents
    greater than, and more specifically it translates to *“greater than 130”*. In
    MQL, operators start with the dollar sign `$`. Here is a sample list of commonly
    used operators in MQL:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我们前面的例子中，注意查询中使用的`$gt`。它表示“大于”，更具体地说，它翻译为*“大于130”*。在MQL中，操作符以美元符号`$`开头。以下是MQL中常用操作符的示例列表：
- en: '`$eq` - matches values that are equal to a specified value'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$eq` - 匹配等于指定值的值'
- en: 'Example:The query `{"close": {"$eq": 130}}` finds documents where the `close`
    field is exactly 130.'
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '示例：查询`{"close": {"$eq": 130}}`查找`close`字段值恰好为130的文档。'
- en: '`$gt` - matches values that are greater than a specified value'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$gt` - 匹配大于指定值的值'
- en: 'Example: The query `{"close": {"$gt": 130}}` finds documents where the close
    field is greater than 130.'
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '示例：查询`{"close": {"$gt": 130}}`查找收盘价大于130的文档。'
- en: '`$gte` - matches values that are greater than or equal to a specified value'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$gte` - 匹配大于或等于指定值的值'
- en: 'Example: The query `{"close": {"$gte": 130}}` finds documents where the close
    field is greater than or equal to 130.'
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '示例：查询`{"close": {"$gte": 130}}`查找收盘价大于或等于130的文档。'
- en: '`$lt` - matches values that are less than a specified value'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$lt` - 匹配小于指定值的值'
- en: 'Example: The query `{"close": {"$lt": 130}}` finds documents where the close
    field is less than 130.'
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '示例：查询`{"close": {"$lt": 130}}`查找收盘价小于130的文档。'
- en: '`$lte` - matches values that are less than or equal to a specified value'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$lte` - 匹配小于或等于指定值的值'
- en: 'Example: The query `{"close": {"$lt3": 130}}` finds documents where the close
    field is less than or equal to 130.'
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '示例：查询`{"close": {"$lt3": 130}}`查找收盘价小于或等于130的文档。'
- en: '`$and` - joins query clauses with a logical **AND** operator. All conditions
    must be true.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$and` - 使用逻辑**AND**操作符连接查询子句，所有条件必须为真。'
- en: 'Example: The query `{"$and": [{"close": {"$gt": 130}}, {"volume": {"$lt": 20000000}}]}`
    finds documents where the close field is greater than 130 **AND** the volume is
    less than 20,000,000.'
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '示例：查询`{"$and": [{"close": {"$gt": 130}}, {"volume": {"$lt": 20000000}}]}`查找收盘价大于130**且**交易量小于20,000,000的文档。'
- en: '`$or`- joins query clauses with a logical **OR** operator. At least one condition
    must be true.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$or` - 使用逻辑**OR**操作符连接查询子句，至少有一个条件必须为真。'
- en: '`Example: The query {"$or": [{"close": {"$gt": 135}}, {"volume": {"$gt": 30000000}}]}`
    finds documents where the close field is greater than 135 **OR** the volume is
    greater than 30,000,000.'
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '示例：查询`{"$or": [{"close": {"$gt": 135}}, {"volume": {"$gt": 30000000}}]}`查找收盘价大于135**或**交易量大于30,000,000的文档。'
- en: '`$in`- matches values specified in an array (list)'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$in` - 匹配数组（列表）中指定的值'
- en: 'Example:The query `{"date": {"$in": [datetime.datetime(2019, 9, 4), datetime.datetime(2019,
    9, 5), datetime.datetime(2019, 9, 6)]}}` finds documents where the date field
    matches any of the specified dates: September 4, 2019; September 5, 2019; September
    6, 2019.'
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '示例：查询`{"date": {"$in": [datetime.datetime(2019, 9, 4), datetime.datetime(2019,
    9, 5), datetime.datetime(2019, 9, 6)]}}`查找日期字段与以下指定日期之一匹配的文档：2019年9月4日；2019年9月5日；2019年9月6日。'
- en: For a more comprehensive list of operators in MQL you can visit the official
    documentation here [https://www.mongodb.com/docs/manual/reference/operator/query/](https://www.mongodb.com/docs/manual/reference/operator/query/)
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看MQL中操作符的完整列表，您可以访问官方文档：[https://www.mongodb.com/docs/manual/reference/operator/query/](https://www.mongodb.com/docs/manual/reference/operator/query/)
- en: There's more…
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'There are different ways to retrieve data from MongoDB using `PyMongo`. In
    the previous section, we used `db.collection.find()`, which always returns a cursor.
    As we discussed earlier, `find()` returns all the matching documents that are
    available in the specified collection. If you want to return the first occurrence
    of matching documents, then `db.collection.find_one()` would be the best choice
    and would return a **dictionary** object, not a cursor. Keep in mind that this
    only returns one document, as shown in the following example:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以使用`PyMongo`从MongoDB中检索数据。在前面的部分，我们使用了`db.collection.find()`，它总是返回一个游标。正如我们之前讨论的，`find()`返回的是指定集合中所有匹配的文档。如果您只想返回匹配文档的第一个实例，那么`db.collection.find_one()`将是最佳选择，它会返回一个**字典**对象，而不是游标。请记住，这只会返回一个文档，示例如下：
- en: '[PRE87]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'When it comes to working with cursors, there are several ways you can traverse
    through the data:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理游标时，您有多种方法可以遍历数据：
- en: 'Converting into a pandas DataFrame using `pd.DataFrame(cursor)`, as shown in
    the following code:'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `pd.DataFrame(cursor)` 将数据转换为 pandas DataFrame，如以下代码所示：
- en: '[PRE88]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Converting into a Python **list** or **tuple**:'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换为 Python **list** 或 **tuple**：
- en: '[PRE89]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'You can also convert the **Cursor** object into a Python list and then convert
    that into a pandas DataFrame, like this:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将 **Cursor** 对象转换为 Python 列表，然后将其转换为 pandas DataFrame，像这样：
- en: '[PRE90]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Using `next()` to get move the pointer to the next item in the result set:'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `next()` 将指针移动到结果集中的下一个项目：
- en: '[PRE91]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '**Looping** through the object, for example, with a `for` loop:'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对象进行**循环**，例如，使用 `for` 循环：
- en: '[PRE92]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'The previous code will loop through the entire result set. If for example you
    want to loop thorugh the first 5 records you can use the following:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将遍历整个结果集。如果您想遍历前 5 条记录，可以使用以下代码：
- en: '[PRE93]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Specifying an **index**. Here, we are printing the first value:'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定 **index**。在这里，我们打印第一个值：
- en: '[PRE94]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Note that if you provided a slice, such as `cursor[0:1]`, which is a range,
    then it will return a cursor object (not a document).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果提供了一个切片，例如 `cursor[0:1]`，这是一个范围，那么它将返回一个游标对象（而不是文档）。
- en: See also
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: 'For more information on the **PyMongo** API, please refer to the official documentation,
    which you can find here: [https://pymongo.readthedocs.io/en/stable/index.html](ch004.xhtml).'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多有关 **PyMongo** API 的信息，请参考官方文档，您可以在此找到：[https://pymongo.readthedocs.io/en/stable/index.html](ch004.xhtml)。
- en: Reading data from a time series databases
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从时间序列数据库读取数据
- en: 'A time series database, a type of **NoSQL** database, is optimized for time-stamped
    or time series data and provides improved performance, especially when working
    with large datasets containing IoT data or sensor data. In the past, common use
    cases for time series databases were mostly associated with financial stock data,
    but their use cases have expanded into other disciplines and domains. In this
    recipe you will explore three popular time series databases: **InfluxDB**, **TimescaleDB,**
    and **TDEngine**.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据库，一种 **NoSQL** 数据库，专为时间戳或时间序列数据进行了优化，并在处理包含 IoT 数据或传感器数据的大型数据集时提供了更好的性能。过去，时间序列数据库的常见使用场景主要与金融股票数据相关，但它们的应用领域已经扩展到其他学科和领域。在本教程中，您将探索三种流行的时间序列数据库：**InfluxDB**、**TimescaleDB**
    和 **TDEngine**。
- en: '**InfluxDB** is a popular open source time series database with a large community
    base. In this recipe, we will be using InfluxDB''s latest release as of this writing;
    that is, version 2.7.10\. The most recent InfluxDB releases introduced the Flux
    data scripting language, which you will use with the Python API to query our time
    series data.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '**InfluxDB** 是一个流行的开源时间序列数据库，拥有一个庞大的社区基础。在本教程中，我们将使用本文写作时的 InfluxDB 最新版本，即
    2.7.10 版本。最近的 InfluxDB 版本引入了 Flux 数据脚本语言，您将通过 Python API 使用该语言查询我们的时间序列数据。'
- en: '**TimescaleDB**, on the other hand, is an extension of PostgreSQL specifically
    optimized for time series data. It leverages the power and flexibility of PostgreSQL
    while providing additional features tailored for handling time-stamped information
    efficiently. One advantage with TimescaleDB is that you can leverage SQL for querying
    the data. TimescaleDB is an open-source time series database and in this recipe,
    we will be using TimescaleDB''s latest release as of this writing, version 2.16.1'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '**TimescaleDB** 是 PostgreSQL 的扩展，专门为时间序列数据进行了优化。它利用 PostgreSQL 的强大功能和灵活性，同时提供了额外的功能，专门为高效处理带时间戳的信息而设计。使用
    TimescaleDB 的一个优势是，您可以利用 SQL 查询数据。TimescaleDB 是一个开源的时间序列数据库，在本教程中，我们将使用 TimescaleDB
    最新的版本 2.16.1。'
- en: '**TDEngine** is an open-source time series database designed for Internet of
    Things (IoT), big data, and real-time analytics. Like TimescaleDB, TDEngine utilizes
    SQL for querying the data. In this recipe, we will be working with TDEngine version
    3.3.2.0, the latest as of this writing.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '**TDEngine** 是一个开源的时间序列数据库，专为物联网（IoT）、大数据和实时分析设计。与 TimescaleDB 类似，TDEngine
    使用 SQL 查询数据。在本教程中，我们将使用 TDEngine 的最新版本 3.3.2.0。'
- en: Getting ready
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe assumes that you have access to a running instance of InfluxDB,
    TimeseriesDB, or TDEngine. You will install the appropriate libraries to connect
    and interact with these databases using Python. For **InfluxDB** V2, you will
    need to install `influxdb-client`; for **TimescaleDB,** you will need to install
    the PostgreSQL Python library `psycopg2` (recall in the *Reading from a relational
    database* recipe of this chapter, we used `psycopg3`); and finally, for **TDEngine,**
    you will need to install `taospy`.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱假设您可以访问正在运行的 InfluxDB、TimeseriesDB 或 TDEngine 实例。您将安装适当的库来使用 Python 连接和与这些数据库交互。对于**InfluxDB**
    V2，您需要安装 `influxdb-client`；对于**TimescaleDB**，您需要安装 PostgreSQL Python 库 `psycopg2`（回想在本章的*从关系数据库读取数据*食谱中，我们使用了
    `psycopg3`）；最后，对于**TDEngine**，您需要安装 `taospy`。
- en: 'You can install these libraries using `pip`, as follows:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下方式使用 `pip` 安装这些库：
- en: '[PRE95]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'To install using **conda** use the following:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 **conda** 安装，请使用以下命令：
- en: '[PRE96]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'If you do not have access to these databases, then the fastest way to get up
    and running is via Docker. The following are example commands for InlfuxDB, TimescaleDB,
    and TDEngine:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有访问这些数据库的权限，那么最快速的方式是通过 Docker。以下是 InlfuxDB、TimescaleDB 和 TDEngine 的示例命令：
- en: InfluxDB Docker Container
  id: totrans-438
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: InfluxDB Docker 容器
- en: 'To create a InfluxDB container you will need to run the following command:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建 InfluxDB 容器，您需要运行以下命令：
- en: '[PRE97]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: For more information you can access the official docker hub page [https://hub.docker.com/_/influxdb](https://hub.docker.com/_/influxdb)
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，您可以访问官方 Docker Hub 页面 [https://hub.docker.com/_/influxdb](https://hub.docker.com/_/influxdb)
- en: Once the **influxdb-ch3** container is up and running you can navigate to `http://localhost:8086`
    using your favorite browser and continue the setup, such as username, password,
    initial organization name, and initial bucket name.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦**influxdb-ch3**容器启动并运行，您可以使用您喜欢的浏览器导航到 `http://localhost:8086`，并继续设置，例如用户名、密码、初始组织名称和初始存储桶名称。
- en: For this recipe, we will use the **National Oceanic and Atmospheric Administration**
    (**NOAA**) water level sample data from August 17, 2019, to September 17, 2019,
    for Santa Monica and Coyote Creek.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本食谱，我们将使用**国家海洋和大气管理局**（**NOAA**）的水位样本数据，时间范围为2019年8月17日至2019年9月17日，数据来源为圣塔莫尼卡和科约特溪。
- en: 'In the Data Explorer UI, you can run the following **Flux** query to load the
    sample dataset:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据浏览器 UI 中，您可以运行以下**Flux**查询来加载样本数据集：
- en: '[PRE98]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: In the previous snippet, the NOAA dataset was loaded into the `tscookbook` bucket
    created during the initial setup.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，NOAA 数据集已加载到初始设置时创建的 `tscookbook` 存储桶中。
- en: For instructions on how to load the sample data, or other provided *sample datasets*,
    please refer to the InfluxDB official documentation at [https://docs.influxdata.com/influxdb/v2/reference/sample-data/](https://docs.influxdata.com/influxdb/v2/reference/sample-data/)
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何加载样本数据或其他提供的*样本数据集*的说明，请参阅 InfluxDB 官方文档 [https://docs.influxdata.com/influxdb/v2/reference/sample-data/](https://docs.influxdata.com/influxdb/v2/reference/sample-data/)
- en: TimescaleDB Docker Container
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TimescaleDB Docker 容器
- en: 'To create a TimescaleDB container you will need to run the following command:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建 TimescaleDB 容器，您需要运行以下命令：
- en: '[PRE99]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: For more information you can access the official docker hub page [https://hub.docker.com/r/timescale/timescaledb](https://hub.docker.com/r/timescale/timescaledb)
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，您可以访问官方 Docker Hub 页面 [https://hub.docker.com/r/timescale/timescaledb](https://hub.docker.com/r/timescale/timescaledb)
- en: Once the **timescaledb-ch3** container is up and running, you can load the `MSFT.csv`
    file using the same instructions as in the `Getting Ready` section of the *Reading
    data from a relational database* recipe.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦**timescaledb-ch3**容器启动并运行，您可以使用与*从关系数据库读取数据*食谱中的`准备工作`部分相同的说明加载 `MSFT.csv`
    文件。
- en: Note, the default username is `postgres` and the password is whatever password
    you setup in the docker command.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，默认的用户名是 `postgres`，密码是您在 Docker 命令中设置的密码。
- en: '![Figure 3\. – DBeaver TimescaleDB/Postgres connection settings (should look
    similar to Figure 3.1)](img/file29.png)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![图 3\. – DBeaver TimescaleDB/Postgres 连接设置（应该与图 3.1 类似）](img/file29.png)'
- en: Figure 3\. – DBeaver TimescaleDB/Postgres connection settings (should look similar
    to Figure 3.1)
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. – DBeaver TimescaleDB/Postgres 连接设置（应该与图 3.1 类似）
- en: Since TimescaleDB is based on PostgreSQL, it also defaults to port 5432\. So,
    if you are already running a local instance of PostgreSQL database which defaults
    to port 5432 you may run into an issue with TimescaleDB. In such case, you may
    opt to change the docker run configuration and change the port.
  id: totrans-456
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 由于 TimescaleDB 基于 PostgreSQL，它也默认使用 5432 端口。因此，如果你已经在本地运行一个默认使用 5432 端口的 PostgreSQL
    数据库，你可能会遇到 TimescaleDB 的端口冲突问题。在这种情况下，你可以选择修改 Docker 运行配置并更改端口。
- en: TDEngine Docker Container [TO BE DELETED SECTION]
  id: totrans-457
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TDEngine Docker 容器 [待删除部分]
- en: 'To create a TDEngine container you will need to run the following command:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个 TDEngine 容器，你需要运行以下命令：
- en: '[PRE100]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: For more information you can access the official docker hub page [https://hub.docker.com/r/tdengine/tdengine](https://hub.docker.com/r/tdengine/tdengine)
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息可以访问官方的 Docker Hub 页面 [https://hub.docker.com/r/tdengine/tdengine](https://hub.docker.com/r/tdengine/tdengine)
- en: 'Once the **tdengine-ch3** container is up and running, you can create a demo
    dataset by running the `taosBenchmark` command from inside the container shell.
    Here are the steps to access the shell from inside the running container to run
    the needed command to install and setup the demo data set:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 **tdengine-ch3** 容器启动并运行，你可以通过在容器 shell 中运行 `taosBenchmark` 命令来创建一个演示数据集。以下是从正在运行的容器内部访问
    shell 并运行所需命令来安装和设置演示数据集的步骤：
- en: '[PRE101]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Once the demo set is created you can exit out of the terminal. You can now leverage
    DBeaver to verify the data is created. You can use the same instructions as in
    the `Getting Ready` section of the *Reading data from a relational database* recipe.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦演示数据集创建完成，你可以退出终端。现在你可以使用 DBeaver 验证数据是否已创建。你可以使用与 *从关系数据库读取数据* 食谱中 `准备工作`
    部分相同的说明。
- en: Note, the default username is `root` and the default password is `taosdata`
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，默认用户名是 `root`，默认密码是 `taosdata`
- en: '![Figure 3\. – DBeaver TDEngine connection settings](img/file30.png)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![图 3\. – DBeaver TDEngine 连接设置](img/file30.png)'
- en: Figure 3\. – DBeaver TDEngine connection settings
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. – DBeaver TDEngine 连接设置
- en: You should now see a `test` **database** created, and a `meters` **supertable**
    with 10,000 **subtables** named `d0` to `d9999` and each table contains around
    10,000 rows and four columns (`ts`, `current`, `voltage`, and `phase`). You may
    not be able to see the `meters` supertable in the DBeaver navigator pane, but
    if you run the following SQL query “`SELECT COUNT(*) FROM test.meters;”` which
    should output 100,000,000 rows in the meters supertable (10,000 subtables multiplied
    by 10,000 rows in each).
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该能看到一个名为 `test` 的 **数据库** 被创建，并且一个名为 `meters` 的 **超级表**，它包含 10,000 个 **子表**，命名为
    `d0` 到 `d9999`，每个表包含大约 10,000 行和四列（`ts`、`current`、`voltage` 和 `phase`）。你可能无法在
    DBeaver 导航窗格中看到 `meters` 超级表，但如果你运行以下 SQL 查询 "`SELECT COUNT(*) FROM test.meters;`"，它应该会输出
    100,000,000 行（10,000 个子表乘以每个子表的 10,000 行）。
- en: How to do it…
  id: totrans-468
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: This recipe will demonstrate how you can connect and interact with three popular
    time series database systems.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将演示如何连接并与三种流行的时序数据库系统进行交互。
- en: InfluxDB
  id: totrans-470
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: InfluxDB
- en: 'We will be leveraging the `Influxdb_client` Python SDK for InfluxDB 2.x, which
    provides support for pandas DataFrames in terms of both read and write functionality.
    Let''s get started:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用 `Influxdb_client` Python SDK 来访问 InfluxDB 2.x，它支持 pandas DataFrame 进行读取和写入功能。让我们开始吧：
- en: 'First, let''s import the necessary libraries:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE102]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'To establish your connection using `InfluxDBClient(url="http://localhost:8086",
    token=token)`, you will need to define the `token`, `org`, and `bucket` variables:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用 `InfluxDBClient(url="http://localhost:8086", token=token)` 建立连接，你需要定义 `token`、`org`
    和 `bucket` 变量：
- en: '[PRE103]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Think of a bucket as database in relational databases.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将桶看作是关系数据库中的数据库。
- en: 'Now, you are ready to establish your connection by passing the `url`, `token`,
    and `org` parameters to `InlfuxDBClient()`:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以通过将 `url`、`token` 和 `org` 参数传递给 `InlfuxDBClient()` 来建立连接：
- en: '[PRE104]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Next, you will instantiate `query_api`:'
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你将实例化 `query_api`：
- en: '[PRE105]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Pass your Flux query and request the results to be in pandas DataFrame format
    using the `query_data_frame` method:'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 传递你的 Flux 查询，并使用 `query_data_frame` 方法请求以 pandas DataFrame 格式返回结果：
- en: '[PRE106]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'In the preceding Flux script, selected the measurement `h2o_temparature` and
    where the location is `coyote_creek`. Let''s inspect the DataFrame. Pay attention
    to the data types in the following output:'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的 Flux 脚本中，选择了度量 `h2o_temparature`，并且位置是 `coyote_creek`。现在让我们检查一下 DataFrame。请注意以下输出中的数据类型：
- en: '[PRE107]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'If you want to retrieve only the time and degrees columns, you can update the
    Flux query as shown:'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你只想检索时间和温度列，你可以更新 Flux 查询，如下所示：
- en: '[PRE108]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: The original dataset contains 15,258 observations collected every six (6) minutes
    between the two stations (locations). The moving average is calculated over 120
    data points. It is important to understand the graduality of your dataset. The
    final DataFrame contains 3885 records.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集包含 15,258 条观察数据，数据每 6 分钟收集一次，来源于两个站点（位置）。移动平均是基于 120 个数据点计算的。理解数据集的渐进性非常重要。最终的
    DataFrame 包含 3885 条记录。
- en: TimescaleDB
  id: totrans-488
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TimescaleDB
- en: Since TimescaleDB is based on PostgreSQL and we have already installed **psycopg2**
    , retrieving and querying your data should be similar to the approach used in
    the recipe *Reading data from a relational database*.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 TimescaleDB 基于 PostgreSQL，并且我们已经安装了 **psycopg2**，因此检索和查询数据的方式应该与示例 *从关系数据库中读取数据*
    中使用的方法类似。
- en: 'Here is a brief on how this can be done using pandas from_sql:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 这里简要说明如何使用 pandas 的 from_sql 来实现：
- en: Import SQLAlchemy and pandas
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 SQLAlchemy 和 pandas
- en: '[PRE109]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Create the engine object with the proper connection string to the PostgreSQL
    backend
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正确的连接字符串创建 PostgreSQL 后端的引擎对象。
- en: '[PRE110]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Finally, use the `read_sql` method to retrieve the result set of your query
    into a pandas DataFrame:'
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用 `read_sql` 方法将查询结果集检索到 pandas DataFrame 中：
- en: '[PRE111]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: TimescaleDB offers many advantages over PostgreSQL and you will explore some
    of these in *Chapter 5*, *Persisting Time Series Data to Databases*. Still, querying
    TimescaleDB brings a similar experience to those familiar with SQL and PostgreSQL.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: TimescaleDB 提供了比 PostgreSQL 更多的优势，你将在 *第 5 章* *持久化时间序列数据到数据库* 中探索其中的一些优势。然而，查询
    TimescaleDB 的体验与熟悉 SQL 和 PostgreSQL 的用户类似。
- en: TDEngine
  id: totrans-498
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TDEngine
- en: 'For this recipe let’s update our configuration file `database.cfg` from the
    *Technical Requirements* to include a [TDENGINE] section as shown:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，让我们更新配置文件 `database.cfg`，根据 *技术要求* 包括一个 [TDENGINE] 部分，如下所示：
- en: '[PRE112]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: You will start by establishing a connection to the TDEngine server, and then
    run a query against the demo dataset from the **taosBenchmark** described in the
    *Getting Read* section.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 你将首先建立与 TDEngine 服务器的连接，然后对 **taosBenchmark** 演示数据集进行查询，该数据集在 *Getting Read*
    部分中有所描述。
- en: Start by importing the required libraries.
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从导入所需的库开始。
- en: '[PRE113]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: You will create a Python dictionary to store all the parameter values required
    to establish a connection to the database, such as `url`, `user` and `password`
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将创建一个 Python 字典，存储所有连接数据库所需的参数值，如 `url`、`user` 和 `password`。
- en: '[PRE114]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: Establish a connection to the server
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立与服务器的连接。
- en: '[PRE115]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: Run the following query and execute the query using the `query` method from
    the connection object conn
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下查询，并使用连接对象 conn 的 `query` 方法执行该查询。
- en: '[PRE116]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: You can verify the number of rows and column names in the result set
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以验证结果集中的行数和列名。
- en: '[PRE117]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: The `results.data` contains the values from the result set but without column
    headers. Before we write our result set into a pandas DataFrame we need to capture
    the column names in a list from `results.fields:`
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`results.data` 包含结果集中的值，但没有列标题。在将结果集写入 pandas DataFrame 之前，我们需要从 `results.fields`
    捕获列名列表：'
- en: '[PRE118]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: How it works…
  id: totrans-514
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Both TimescaleDB and TDEngine use SQL to query the data, while InfluxDB utilizes
    their proprietary query language, Flux.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: TimescaleDB 和 TDEngine 都使用 SQL 来查询数据，而 InfluxDB 使用他们的专有查询语言 Flux。
- en: InfluxDB 1.8x introduced the **Flux** query language as an alternative query
    language to **InfluxQL**, with the latter having a closer resemblance to SQL.
    InfluxDB 2.0 introduced the concept of **buckets**, which is where data is stored,
    whereas InfluxDB 1.x stored data in databases.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: InfluxDB 1.8x 引入了 **Flux** 查询语言，作为 **InfluxQL** 的替代查询语言，后者与 SQL 更加相似。InfluxDB
    2.0 引入了 **bucket** 的概念，数据存储在这里，而 InfluxDB 1.x 将数据存储在数据库中。
- en: 'In this recipe, we started by creating an instance of `InfluxDbClient`, which
    later gave us access to the `query_api` method, which gives additional methods,
    including:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们从创建一个 `InfluxDbClient` 实例开始，这样我们就可以访问 `query_api` 方法，进而获得包括以下方法：
- en: '`query()` returns the result as a **FluxTable**.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query()` 返回结果作为 **FluxTable**。'
- en: '`query_csv()` returns the result as a CSV iterator (CSV reader).'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query_csv()` 返回结果作为 CSV 迭代器（CSV 读取器）。'
- en: '`query_data_frame()` returns the result as a pandas DataFrame.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query_data_frame()` 返回结果作为 pandas DataFrame。'
- en: '`query_data_frame_stream()` returns a stream of pandas DataFrames as a generator.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query_data_frame_stream()` 返回一个 pandas DataFrame 流作为生成器。'
- en: '`query_raw()` returns the result as raw unprocessed data in `s` string format.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query_raw()` 返回原始未处理的数据，格式为 `s` 字符串。'
- en: '`query_stream()` is similar to `query_data_frame_stream` but returns a stream
    of `FluxRecord` as a generator.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query_stream()`类似于`query_data_frame_stream`，但它返回的是一个生成器流，其中包含`FluxRecord`。'
- en: 'In the recipe, you used `client.query_api()` to fetch the data, as shown here:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，你使用了`client.query_api()`来获取数据，如下所示：
- en: '[PRE119]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: You used `query_data_frame`, which executes a synchronous Flux query and returns
    a pandas DataFrame with which you are familiar.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用了`query_data_frame`，它执行一个同步的 Flux 查询并返回一个你熟悉的 pandas DataFrame。
- en: Notice we had to use the `pivot` function in the Flux query to transform the
    results into a tabular format suitable for pandas DataFrames.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在 Flux 查询中必须使用`pivot`函数来将结果转换为适合 pandas DataFrame 的表格格式。
- en: '[PRE120]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'Let’s break the preceding line of code:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析前面的代码：
- en: The `pivot()` is used to reshape the data and transform it from a long to a
    wide format.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '`pivot()`用于重塑数据，并将其从长格式转换为宽格式。'
- en: The `rowKey` parameter specifies which column to use as the unique identifier
    for each row. In our example, we specified `["_time"]` so each row will have a
    unique timestamp
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '`rowKey`参数指定了哪一列用作每行的唯一标识符。在我们的示例中，我们指定了`["_time"]`，因此每行将有一个唯一的时间戳。'
- en: The `columnKey` parameter specifies which column’s values will be used to create
    new columns in the output. In our example, we specified `["_field"]` to create
    columns from field names
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '`columnKey`参数指定了哪一列的值将用于在输出中创建新列。在我们的示例中，我们指定了`["_field"]`来从字段名称创建列。'
- en: The `valueColumn` parameter specifies which column contains the values, we specified
    `"_value"` to fill the new columns with corresponding values.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '`valueColumn`参数指定了哪个列包含数值，我们指定了`"_value"`来填充新列中的相应值。'
- en: There's more…
  id: totrans-534
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'When working with **InfluxDB** and the `influxdb-client,` there is an additional
    argument that you can use to create the DataFrame index. In `query_data_frame(),`
    you can pass a list as an argument to the `data_frame_index` parameter, as shown
    in the following example:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用**InfluxDB**和`influxdb-client`时，有一个额外的参数可以用来创建 DataFrame 索引。在`query_data_frame()`中，你可以将一个列表作为`data_frame_index`参数的参数传入，如下面的示例所示：
- en: '[PRE121]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: This returns a time series DataFrame with a `DatetimeIndex` (`_time`).
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个带有`DatetimeIndex`（`_time`）的时间序列 DataFrame。
- en: See also
  id: totrans-538
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: If you are new to InfluxDB Flux query language, check out the *Get Started with
    Flux* official documentation at [https://docs.influxdata.com/influxdb/v2.0/query-data/get-started/](https://docs.influxdata.com/influxdb/v2.0/query-data/get-started/).
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你是 Flux 查询语言的新手，可以查看官方文档中的*Flux 入门指南*：[https://docs.influxdata.com/influxdb/v2.0/query-data/get-started/](https://docs.influxdata.com/influxdb/v2.0/query-data/get-started/).
- en: Please refer to the official **InfluxDB-Client** Python library documentation
    on GitHub at [https://github.com/influxdata/influxdb-client-python](https://github.com/influxdata/influxdb-client-python).
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参考官方的**InfluxDB-Client** Python 库文档，地址为 GitHub：[https://github.com/influxdata/influxdb-client-python](https://github.com/influxdata/influxdb-client-python).
- en: To learn more about the **TDEngine** python library, you refer to the official
    documentation at [https://docs.tdengine.com/cloud/programming/client-libraries/python/](https://docs.tdengine.com/cloud/programming/client-libraries/python/)
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于**TDEngine** Python 库的信息，请参阅官方文档：[https://docs.tdengine.com/cloud/programming/client-libraries/python/](https://docs.tdengine.com/cloud/programming/client-libraries/python/)
