- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Time Series Forecasting as Regression
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将时间序列预测作为回归问题
- en: In the previous part of the book, we developed a fundamental understanding of
    time series and equipped ourselves with tools and techniques to analyze and visualize
    time series and even generate our first baseline forecasts. We have mainly covered
    classical and statistical techniques in this book so far. Let’s now dip our toes
    into modern **machine learning** and learn how we can leverage this comparatively
    new field for **time series forecasting**. Machine learning is a field that has
    grown in leaps and bounds in recent times, and being able to leverage these new
    techniques for time series forecasting is a skill that will be invaluable in today’s
    world.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前一部分中，我们对时间序列有了基本的理解，并装备了分析和可视化时间序列的工具和技术，甚至生成了我们第一个基线预测。到目前为止，我们主要介绍了经典和统计技术。现在，让我们深入了解现代**机器学习**，并学习如何利用这个相对较新的领域来进行**时间序列预测**。机器学习是近年来迅速发展的一个领域，能够利用这些新技术进行时间序列预测，将在今天的世界中成为一项无价的技能。
- en: 'In this chapter, we will be covering these main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主要主题：
- en: Understanding the basics of machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解机器学习的基础
- en: Time series forecasting as regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将时间序列预测作为回归问题
- en: Local versus global models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部模型与全局模型
- en: Understanding the basics of machine learning
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解机器学习的基础
- en: We want to use machine learning for time series forecasting. But before we get
    started with it, let’s spend some time establishing what machine learning is and
    setting up a framework to demonstrate what it does (if you are already very comfortable
    with machine learning, feel free to skip ahead to the next section, *Time series
    forecasting as regression*, or just stay with us and refresh the concepts). In
    1959, Arthur Samuel defined machine learning as a “*field of study that gives
    computers the ability to learn without being explicitly programmed*.” Traditionally,
    programming has been a paradigm under which we know a set of rules/logic to perform
    an action, and that action is performed on the given data to get the output that
    we want. But machine learning flipped this on its head.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用机器学习进行时间序列预测。但在开始之前，让我们花一些时间来了解什么是机器学习，并建立一个框架来展示它的功能（如果你已经对机器学习非常熟悉，可以跳到下一节“*将时间序列预测作为回归问题*”，或者继续跟我们一起复习这些概念）。1959年，Arthur
    Samuel 将机器学习定义为“*一种使计算机能够在没有明确编程的情况下学习的研究领域*。”传统上，编程是一种我们知道一套规则/逻辑来执行某个动作，并且在给定的数据上执行该动作以获得我们想要的输出的范式。而机器学习则颠覆了这一点。
- en: 'In machine learning, we start with data and the output, and we ask the computer
    to tell us about the rules with which the desired output can be achieved from
    the data:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们从数据和输出开始，要求计算机告诉我们通过哪些规则可以从数据中获得期望的输出：
- en: '![Figure 5.1 – Traditional programming versus machine learning ](img/B22389_05_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 传统编程与机器学习](img/B22389_05_01.png)'
- en: 'Figure 5.1: Traditional programming versus machine learning'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：传统编程与机器学习
- en: There are many kinds of problem settings in machine learning, such as supervised
    learning, unsupervised learning, self-supervised learning, and so on, but we will
    stick to supervised learning, which is the most common one and the most applicable
    to the content of this book. Supervised learning refers to what we already touched
    upon in the paradigm shift example with the program, data, and output. We use
    a dataset with paired examples of input and expected output and ask the model
    to learn the relationship.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中有许多种问题设置，如监督式学习、无监督式学习、自监督学习等，但我们将专注于监督式学习，这是最常见的，也是本书内容最适用的。监督式学习指的是我们在程序、数据和输出的范式转换示例中已经提到的内容。我们使用一个包含输入和预期输出的配对样本的数据集，并要求模型学习它们之间的关系。
- en: 'Let’s start our discussion small and slowly build up to the whole schematic,
    which encapsulates most of the key components of a supervised machine learning
    problem:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个小的讨论开始，逐步构建整个示意图，它包含了监督式机器学习问题的主要关键组件：
- en: '![Figure 5.2 – Supervised machine learning schematic, part 1 – the ideal function
    ](img/B22389_05_02.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 监督式机器学习示意图，部分 1 – 理想函数](img/B22389_05_02.png)'
- en: 'Figure 5.2: Supervised machine learning schematic, part 1—the ideal function'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：监督式机器学习示意图，部分 1—理想函数
- en: 'As we have already discussed, what we want from machine learning is to *learn*
    from the data and come up with a set of rules/logic. The closest analogy in mathematics
    for logic/rules is a function, which takes in an input (here, data) and provides
    an output. Mathematically, it can be written as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经讨论过的，我们希望机器学习从数据中*学习*并得出一套规则/逻辑。在数学中，与逻辑/规则最接近的类比是函数，它接受一个输入（这里是数据）并提供一个输出。从数学上看，可以写作如下：
- en: '*y* = *g*(*X*)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *g*(*X*)'
- en: where *X* is the set of features and *g* is the **ideal target function** (denoted
    by **1** in *Figure 5.2*) that maps the *X* input (denoted by **2** in the schematic)
    to the target (ideal) output, *y* (denoted by **3** in the schematic). The ideal
    target function is largely an unknown function, similar to the **data-generating
    process** (**DGP**) we saw in *Chapter 1*, *Introducing Time Series*, which is
    not in our control.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*X* 是特征集合，*g* 是**理想目标函数**（在*图 5.2*中用**1**表示），它将 *X* 输入（在示意图中用**2**表示）映射到目标（理想）输出，*y*（在示意图中用**3**表示）。理想目标函数在很大程度上是一个未知函数，类似于我们在*第1章*《引入时间序列》中看到的**数据生成过程**（**DGP**），它不在我们的控制之下。
- en: '![Figure 5.3 – Supervised machine learning schematic, part 2 – the learned
    approximation ](img/B22389_05_03.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 监督式机器学习示意图，第二部分 – 学到的近似值](img/B22389_05_03.png)'
- en: 'Figure 5.3: Supervised machine learning schematic, part 2—the learned approximation'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：监督式机器学习示意图，第二部分—学到的近似值
- en: 'But we want the computer to *learn* this ideal target function. This approximation
    of the ideal target function is denoted by another function, *h* (**4** in the
    schematic), which takes in the same set of features, *X*, and outputs a predicted
    target, ![](img/B22389_05_001.png) (**5** in the schematic). ![](img/B22389_05_002.png)
    is the parameters of the *h* function (or model parameters):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们希望计算机能够*学习*这个理想目标函数。这个理想目标函数的近似值用另一个函数 *h* 表示（在示意图中用**4**表示），它接受相同的特征集 *X*，并输出预测的目标，![](img/B22389_05_001.png)（在示意图中用**5**表示）。![](img/B22389_05_002.png)
    是 *h* 函数的参数（或模型参数）：
- en: '![Figure 5.4 – Supervised machine learning schematic, part 3 – putting it all
    together ](img/B22389_05_04.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 监督式机器学习示意图，第三部分 – 将所有内容整合](img/B22389_05_04.png)'
- en: 'Figure 5.4: Supervised machine learning schematic, part 3—putting it all together'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：监督式机器学习示意图，第三部分—将所有内容整合
- en: Now, how do we find this approximation *h* function and its parameters, ![](img/B22389_05_002.png)?
    With the dataset of examples (**6** in the schematic). The supervised machine
    learning problem works on the premise that we are able to collect a set of examples
    that shows the features, *X*, and the corresponding target, *y*, which is also
    referred to as *labels* in the literature. It is from this set of examples (the
    dataset) that the computer *learns* the approximation function, *h*, and the optimal
    model parameters, ![](img/B22389_05_002.png). In the preceding diagram, the only
    real unknown entity is the ideal target function, *g*. So, we can use the training
    dataset, *D*, to get predicted targets for every sample in the dataset. We already
    know the ideal target for all the examples. We need a way to compare the ideal
    targets and predicted targets, and this is where the loss function (**7** in the
    schematic) comes in. This loss function tells us how far away from the real truth
    we are with the approximated function, *h*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何找到这个近似 *h* 函数及其参数，![](img/B22389_05_002.png)？通过示例数据集（在示意图中用**6**表示）。监督式机器学习问题的前提是我们能够收集一组包含特征
    *X* 和相应目标 *y* 的示例，这些目标在文献中也称为*标签*。计算机就是从这组示例（数据集）中*学习*近似函数 *h* 以及最优模型参数，![](img/B22389_05_002.png)。在之前的示意图中，唯一真正未知的实体是理想目标函数
    *g*。因此，我们可以使用训练数据集 *D* 来为数据集中的每个样本预测目标。我们已经知道所有示例的理想目标。我们需要一种方法来比较理想目标和预测目标，这就是损失函数（在示意图中用**7**表示）发挥作用的地方。损失函数告诉我们，使用近似函数
    *h* 时，我们离真实结果有多远。
- en: Although *h* can be any function, it is typically chosen from a set of a well-known
    class of functions, *H*. *H* is the finite set of functions that can be fit to
    the data. This class of functions is what we colloquially call **models**. For
    instance, *h* can be chosen from all the linear functions or all the tree-based
    functions, and so on. Choosing an *h* from *H* is done by a combination of hyperparameters
    (which the modeler specifies) and the model parameter, which is learned from data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 *h* 可以是任何函数，但它通常是从一个著名函数类别 *H* 中选择的。*H* 是一个有限的函数集，可以拟合数据。这个函数类别就是我们口语中所说的**模型**。例如，*h*
    可以从所有线性函数或所有基于树的函数中选择，等等。从 *H* 中选择一个 *h* 是通过超参数（由模型设计者指定）和模型参数（从数据中学习）来组合完成的。
- en: Now, all that is left is to run through the different functions so that we find
    the best approximation function, *h*, which gives us the lowest loss. This is
    an optimization process that we call **training**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，剩下的就是运行不同的函数，找到最好的近似函数 *h*，它给我们带来最低的损失。这是一个优化过程，我们称之为**训练**。
- en: Let’s also take a look at a few key concepts, which will be important in all
    our discussions ahead.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还看看一些关键概念，这些概念将在接下来的讨论中非常重要。
- en: Supervised machine learning tasks
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有监督的机器学习任务
- en: Machine learning can be used to solve a wide variety of tasks, such as **regression**,
    **classification**, and **recommendation**. But since classification and regression
    are the most common classes of problems, we will spend just a little bit of time
    reviewing what they are.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以用来解决各种任务，如**回归**、**分类**和**推荐**。但由于分类和回归是最常见的两类问题，我们将花一点时间回顾它们是什么。
- en: The difference between classification and regression tasks is very simple. In
    the machine learning schematic (*Figure 5.2*), we talked about *y*, the target.
    This target can be either a real-valued number or a class of items. For instance,
    we could be predicting the stock price for next week or we could just predict
    whether the stock was going to go up or down. In the first case, we are predicting
    a real-valued number, which is called **regression**. In the other case, we are
    predicting one out of two classes (*up* or *down*), and this is called **classification**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 分类任务和回归任务的区别非常简单。在机器学习框架图（*图 5.2*）中，我们讨论了 *y*，即目标。这个目标可以是一个实数值，或者是一个项的类别。例如，我们可能在预测下周的股价，或者我们只预测股价是上涨还是下跌。在第一种情况下，我们是在预测一个实数值，这叫做**回归**。在另一种情况下，我们预测的是两个类别中的一个（*上涨*
    或 *下跌*），这叫做**分类**。
- en: Overfitting and underfitting
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合
- en: The biggest challenge in machine learning systems is that the model we trained
    must perform well on a new and unseen dataset. The ability of a machine learning
    model to do that is called the **generalization capability** of the model. The
    training process in a machine learning setup is akin to mathematical optimization,
    with one subtle difference. The aim of mathematical optimization is to arrive
    at the global maxima in the provided dataset. But in machine learning, the aim
    is to achieve a low test error by using the training error as a proxy. How well
    a machine learning model is doing on the training error and testing error is closely
    related to the concepts of overfitting and underfitting. Let’s use an example
    to understand these terms.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统面临的最大挑战是，我们训练出的模型必须在新的、未见过的数据集上表现良好。一个机器学习模型在这方面的能力被称为**泛化能力**。机器学习中的训练过程类似于数学优化，但有一个微妙的区别。数学优化的目标是到达提供的数据集中的全局最大值。而在机器学习中，目标是通过使用训练误差作为代理，达到较低的测试误差。一个机器学习模型在训练误差和测试误差上的表现与过拟合和欠拟合的概念密切相关。让我们通过一个例子来理解这些术语。
- en: The learning process of a machine learning model has many parallels to how humans
    learn. Suppose three students, *A*, *B*, and *C*, are studying for an examination.
    *A* is a slacker and went clubbing the night before. *B* decided to double down
    and memorize the textbook from end to end. *C* paid attention in class and understood
    the topics for the examination.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的学习过程与人类学习有很多相似之处。假设三名学生，*A*、*B* 和 *C*，正在为考试做准备。*A* 是个懒汉，前一天晚上去夜店了。*B*
    决定死记硬背，把教科书从头到尾都背一遍。*C* 上课时认真听讲，理解了考试的内容。
- en: As expected, *A* flunked the examination, *C* got the highest score, and *B*
    did OK.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，*A* 没有通过考试，*C* 得了最高分，*B* 还行。
- en: '*A* flunked the examination because they didn’t learn enough. This happens
    to machine learning models as well when they don’t learn enough patterns, and
    this is called **underfitting**. This is characterized by high training errors
    and high test errors.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*因为没有学到足够的内容而挂科。这在机器学习模型中也会发生，当它们没有学到足够的模式时，这被称为**欠拟合**。其特点是训练误差和测试误差都很高。'
- en: '*B* didn’t score as highly as expected; after all, they did memorize the whole
    text, word for word. But many questions in the examination weren’t directly from
    the textbook and *B* wasn’t able to answer them correctly. In other words, the
    questions in the examination were *new and unseen*. And because *B* memorized
    everything but didn’t make an effort to understand the underlying concepts, *B*
    wasn’t able to *generalize* the knowledge they had to new questions. This situation,
    in machine learning, is called **overfitting**. This is typically characterized
    by a big delta in training and test errors. Typically, we will see very low training
    errors and high test errors.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*B*的成绩没有预期的高；毕竟，他们确实把整篇文章逐字逐句地记住了。但考试中的许多问题并不是直接来自教科书，*B*没有能够正确回答这些问题。换句话说，考试中的问题是*新的和未见过*的。而且，由于*B*记住了所有内容，但没有努力理解基础概念，*B*没有能够将所学知识*推广*到新的问题上。在机器学习中，这种情况被称为**过拟合**。其特点通常是训练误差和测试误差之间的差距很大。通常我们会看到非常低的训练误差和很高的测试误差。'
- en: The third student, *C*, learned the right way and understood the underlying
    concepts, and because of that was able to *generalize* to *new and unseen* questions.
    This is the ideal state for a machine learning model, as well. This is characterized
    by reasonably low test errors and a small delta between training and test errors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个学生，*C*，学得正确并理解了基础概念，因此能够*推广*到*新的和未见过*的问题。这也是机器学习模型的理想状态。这种状态的特点是合理较低的测试误差，以及训练误差和测试误差之间的小差距。
- en: We just saw the two greatest challenges in machine learning. Now, let’s also
    look at a few ways we have that can be used to tackle these challenges.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了机器学习中的两个最大挑战。现在，让我们也来看一些可以用来应对这些挑战的方法。
- en: There is a close relationship between the **capacity** of a model and underfitting
    or overfitting. A model’s capacity is its ability to be flexible enough to fit
    a wide variety of functions. Models with low capacity may struggle to fit the
    training data, leading to underfitting. Models with high capacity may overfit
    by memorizing the training data too much. Just to develop an understanding of
    this concept of capacity, let’s look at an example. When we move from linear regression
    to polynomial regression, we are adding more capacity to the model. Instead of
    fitting just straight lines, we are letting the model fit curved lines as well.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的**容量**与欠拟合或过拟合之间有着密切的关系。一个模型的容量是指它足够灵活，能够拟合各种不同函数的能力。容量较低的模型可能很难拟合训练数据，导致欠拟合。容量较高的模型可能通过过度记忆训练数据而发生过拟合。为了更好地理解这个容量的概念，我们来看一个例子。当我们从线性回归转向多项式回归时，我们正在增加模型的容量。我们不仅仅拟合直线，而是让模型也能拟合曲线。
- en: Machine learning models generally do well when their capacity is appropriate
    for the learning problem at hand.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习模型的容量与当前学习问题相匹配时，通常表现良好。
- en: '![Figure 5.5 – Underfitting versus overfitting ](img/B22389_05_05.png)Figure
    5.5: Underfitting versus overfitting'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.5 – 欠拟合与过拟合](img/B22389_05_05.png)图5.5：欠拟合与过拟合'
- en: '*Figure 5.5* shows a very popular case to illustrate overfitting and underfitting.
    We create a few random points using a known function and try to learn that by
    using those data samples. We can see that the linear regression, which is one
    of the simplest models, has underfitted the data by drawing a straight line through
    those points. Polynomial regression is linear regression, but with some higher-order
    features. For now, you can consider the move from linear regression to polynomial
    regression with higher degrees as increasing the capacity of the model. So, when
    we use a degree of 4, we see that the learned function fits the data well and
    matches our ideal function. But if we keep increasing the capacity of the model
    and reach `degree = 15`, we see that the learned function is still passing through
    the training samples, but has learned a very different function, overfitting to
    the training data. Finding the optimal capacity to learn a generalizable function
    is one of the core challenges of machine learning.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.5* 显示了一个非常常见的例子，用于说明过拟合和欠拟合。我们通过已知函数创建一些随机点，并尝试使用这些数据样本进行学习。我们可以看到，作为最简单模型之一的线性回归，通过在这些点之间画一条直线，未能充分拟合数据。多项式回归是线性回归，但加入了一些更高阶的特征。现在，你可以将从线性回归到多项式回归（增加高阶）的转变，视为增加模型的容量。因此，当我们使用4次时，可以看到所学函数很好地拟合了数据，并与我们的理想函数匹配。但是，如果我们继续增加模型的容量，达到
    `degree = 15` 时，我们会看到所学的函数仍然通过训练样本，但它已经学到了一个完全不同的函数，导致了对训练数据的过拟合。找到能够学习出具有良好泛化能力的函数的最佳容量，是机器学习中的核心挑战之一。'
- en: While capacity is one aspect of the model, another aspect is **regularization**.
    Even with the same capacity, there are multiple functions a model can choose from
    the hypothesis space of all functions. With regularization, we try to give preference
    to a set of functions in the hypothesis space over the others.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 容量是模型的一个方面，另一个方面是**正则化**。即使在相同容量下，模型也可以从所有函数的假设空间中选择多个函数。通过正则化，我们试图在假设空间中对某些函数给予偏好，而非其他函数。
- en: While all these functions are valid functions that can be chosen, we nudge the
    optimization process in such a way that we end up with a kind of function toward
    which we have a preference. Although regularization is a general term used to
    refer to any kind of constraint we place on the learning process to reduce the
    complexity of the learned function, more commonly, it is used in the form of a
    weight decay. Let’s take an example of linear regression, which is when we fit
    a straight line to the input features by learning a weight associated with each
    feature.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有这些函数都是有效的，可以选择的函数，我们会通过某种方式推动优化过程，使其最终趋向我们偏好的某种函数。尽管正则化是一个广泛的术语，用于指代我们在学习过程中施加的任何约束，以减少所学函数的复杂性，但更常见的是将其以权重衰减的形式使用。我们以线性回归为例，线性回归是通过学习与每个特征相关联的权重，将一条直线拟合到输入特征上。
- en: 'A linear regression model can be written mathematically as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型可以用数学公式表示如下：
- en: '![](img/B22389_05_005.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_05_005.png)'
- en: Here, *N* is the number of features, *c* is the intercept, *x*[i] is the *i*^(th)
    feature, and *w*[i] is the weight associated with the *i*^(th) feature. We estimate
    the right weight (*L*) by considering this as an optimization problem that minimizes
    the error between ![](img/B22389_05_001.png) and *y* (real output).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*N* 是特征的数量，*c* 是截距，*x*[i] 是第 *i* 个特征，*w*[i] 是与第 *i* 个特征相关的权重。我们通过将这个问题视为优化问题，最小化
    ![](img/B22389_05_001.png) 与 *y*（真实输出）之间的误差，从而估计出正确的权重 (*L*)。
- en: 'Now, with regularization, we add an additional term to *L*, which forces the
    weights to become smaller. Commonly, this is done using an *L1* or *L2* regularizer.
    An *L1* regularizer is when you add the sum of squared weights to *L*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过正则化，我们向 *L* 添加了一个额外的项，强制权重变得更小。通常，这是通过使用 *L1* 或 *L2* 正则化器来完成的。*L1* 正则化器是将权重的平方和添加到
    *L* 上：
- en: '![](img/B22389_05_007.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_05_007.png)'
- en: 'where ![](img/B22389_05_008.png) is the regularization coefficient that determines
    how strongly we penalize the weights. An *L2* regularizer is when you add the
    sum of absolute weights to *L*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/B22389_05_008.png) 是正则化系数，决定了我们对权重的惩罚强度。*L2* 正则化器是将权重的平方和加到 *L* 上：
- en: '![](img/B22389_05_009.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_05_009.png)'
- en: In both cases, we are enforcing a preference for smaller weights over larger
    weights because it keeps the function from relying too much on any one feature
    from the ones used in the machine learning model. Regularization is an entire
    topic unto itself; if you want to learn more, head over to the *Further reading*
    section for a few resources on regularization.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们都在强制要求更小的权重优于更大的权重，因为这样可以避免函数过度依赖于机器学习模型中任何一个特征。正则化是一个独立的话题；如果你想了解更多，请前往*进一步阅读*部分查看一些关于正则化的资源。
- en: Another really effective way to reduce overfitting is to simply train the model
    with more data. With a larger dataset, the chances of the model overfitting become
    less because of the sheer variety that can be captured in a large dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有效减少过拟合的方法是简单地使用更多的数据来训练模型。通过使用更大的数据集，模型过拟合的可能性会减少，因为大数据集能够捕获更多的多样性。
- en: Now, how do we tune the knobs to strike a balance between underfitting and overfitting?
    Let’s look at it in the following section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何调整参数，以在欠拟合和过拟合之间取得平衡呢？让我们在下一节中详细探讨。
- en: Hyperparameters and validation sets
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数和验证集
- en: Almost all machine learning models have a few hyperparameters associated with
    them. **Hyperparameters** are parameters of the model that are not learned from
    data but rather are set before the start of training. For instance, the weight
    of the regularization is a hyperparameter. Most hyperparameters either help us
    control the capacity of the model or apply regularization to the model. By controlling
    either capacity or regularization or both, we can travel the frontier between
    underfitting and overfitting models and arrive at a model that is just right.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的机器学习模型都有一些超参数。**超参数**是模型的参数，它们不是从数据中学习到的，而是在训练开始之前就已经设置好的。例如，正则化的权重就是一个超参数。大多数超参数要么帮助我们控制模型的容量，要么对模型应用正则化。通过控制容量、正则化或两者，我们可以找到在欠拟合和过拟合模型之间的边界，得到一个恰到好处的模型。
- en: But since these hyperparameters have to be set outside of the algorithm, how
    do we estimate the best hyperparameters? Although it is not part of the core *learning
    process*, we also learn the hyperparameters from the data. But if we just use
    the training data to learn the hyperparameters, it will just choose the maximum
    possible model capacity, which results in overfitting. This is where we need a
    **validation set**, a part of the data that the training process does not have
    access to. Following the same analogy we saw earlier, the validation set is the
    mock exam students take to check if they have learned well enough. But when the
    dataset is small (not hundreds of thousands of samples), the performance on a
    single validation set doesn’t guarantee a fair evaluation. In such cases, we rely
    on **cross-validation**. The general trick is to repeat the training and evaluation
    procedure on different subsets of the original dataset. A common way of doing
    this is called **k-fold cross-validation**, where the original dataset is divided
    into *k* equal, non-overlapping, and random subsets, and each subset is evaluated
    after training on all the other subsets. We have provided a link in the *Further
    reading* section if you want to read up on cross-validation techniques. Later
    in the book, we will also be covering this topic, but from the time series perspective,
    which has a few differences from the standard way of doing cross-validation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，由于这些超参数必须在算法外部设置，我们如何估计最佳的超参数呢？虽然它不是核心的*学习过程*的一部分，但我们也可以从数据中学习超参数。不过，如果我们仅仅使用训练数据来学习超参数，那么它会选择最大的可能模型容量，这会导致过拟合。这就是我们需要**验证集**的原因，验证集是训练过程中无法访问的部分数据。借用之前的类比，验证集就像学生参加的模拟考试，用来检查他们是否已经学得足够好。但当数据集较小（不是成千上万的样本）时，单一验证集上的表现并不能保证公平的评估。在这种情况下，我们依赖于**交叉验证**。常见的做法是对原始数据集的不同子集重复进行训练和评估程序。常见的一种方法叫做**k折交叉验证**，它将原始数据集分成*k*个相等的、互不重叠且随机的子集，每个子集在训练其他子集后都会被评估。如果你想了解更多关于交叉验证的技术，我们在*进一步阅读*部分提供了相关链接。稍后在本书中，我们也会从时间序列的角度讲解这个话题，它与标准的交叉验证方式略有不同。
- en: '**Suggested reading**:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**建议阅读**：'
- en: Although we have scratched the surface of machine learning in this book, there
    is a lot more, and to truly appreciate the rest of the book better, we suggest
    gaining more understanding of machine learning. We suggest starting with *Machine
    Learning by Stanford (Andrew Ng)*—[https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning).
    If you are in a hurry, *Machine Learning Crash Course* by *Google* is also a good
    starting point—[https://developers.google.com/machine-learning/crash-course/ml-intro](https://developers.google.com/machine-learning/crash-course/ml-intro).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书已经触及了机器学习的皮毛，但还有很多内容，若要更好地理解本书的其余部分，建议更深入地学习机器学习。我们建议从*斯坦福大学的机器学习课程（Andrew
    Ng）*开始—[https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning)。如果你时间紧迫，*谷歌的机器学习速成课程*也是一个不错的起点—[https://developers.google.com/machine-learning/crash-course/ml-intro](https://developers.google.com/machine-learning/crash-course/ml-intro)。
- en: Machine learning has progressed a lot in recent times, and along with it, powerful
    models that are able to learn complex patterns from data have come out of it.
    When we compare these models with the classical models of time series forecasting,
    we can see that there is a lot of potential in these newer classes of models.
    But there are some fundamental differences between machine learning and time series
    forecasting. In the next section, let’s understand how we can get over those differences
    and use machine learning for time series forecasting.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 近些年，机器学习取得了很大进展，伴随着这些进展，能够从数据中学习复杂模式的强大模型也随之出现。当我们将这些模型与经典的时间序列预测模型进行比较时，我们可以看到这些新型模型有着巨大的潜力。但机器学习与时间序列预测之间仍然存在一些根本的差异。在下一节中，我们将了解如何克服这些差异，并使用机器学习进行时间序列预测。
- en: Time series forecasting as regression
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将时间序列预测视为回归
- en: A time series, as we saw in *Chapter 1*, *Introducing Time Series*, is a set
    of observations taken sequentially in time. And typically, time series forecasting
    is about trying to predict what these observations will be in the future. Given
    a sequence of observations of arbitrary length of history, we predict the future
    to an arbitrary horizon.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第一章*《*介绍时间序列*》中所看到的，时间序列是按时间顺序采集的一组观察值。通常，时间序列预测是关于尝试预测这些观察值在未来将会是什么样。给定一段任意长度的历史观察序列，我们可以预测未来某个时间点的值。
- en: We saw that regression, or machine learning to predict a continuous variable,
    works on a dataset of examples, and each example is a set of input features and
    targets. We can see that regression, which is tasked with predicting a single
    output provided with a set of inputs, is fundamentally incompatible with forecasting,
    where we are given a set of historical values and asked to predict the future
    values. This fundamental incompatibility between the time series and machine learning
    regression paradigms is why we cannot use regression for time series forecasting
    directly.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到回归，或者说机器学习用于预测连续变量，是在一组示例数据集上进行的，每个示例由输入特征和目标组成。我们可以看出，回归任务是基于一组输入来预测单一输出，这与预测任务本质上是不同的，预测任务是基于一组历史值来预测未来值。这种时间序列与机器学习回归模型之间的根本不兼容性，就是我们不能直接使用回归来进行时间序列预测的原因。
- en: Moreover, time series forecasting, by definition, is an extrapolation problem,
    whereas regression, most of the time, is an interpolation one. Extrapolation is
    typically harder to solve using data-driven methods. Another key assumption in
    regression problems is that the samples used for training are **independent and
    identically distributed** (**iid**). But time series break that assumption as
    well because subsequent observations in a time series display considerable dependence.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，时间序列预测从定义上来说是一个外推问题，而回归大多数情况下是一个插值问题。外推通常比插值更难通过数据驱动方法来解决。回归问题中的另一个关键假设是训练所使用的样本是**独立同分布**（**iid**）的。但时间序列破坏了这个假设，因为时间序列中的后续观察值显示出显著的依赖性。
- en: However, to use the wide variety of techniques from machine learning, we need
    to cast time series forecasting as a regression. Thankfully, there are ways to
    convert a time series into a regression and get over the IID assumption by introducing
    some memory to the machine learning model through some features. Let’s see how
    it can be done.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了利用机器学习的各种技术，我们需要将时间序列预测转化为回归问题。幸运的是，有方法可以将时间序列转换为回归，并通过引入一些特征来为机器学习模型添加记忆，从而克服IID假设。让我们看看如何做到这一点。
- en: Time delay embedding
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间延迟嵌入
- en: 'We talked about the ARIMA model in *Chapter 4*, *Setting a Strong Baseline
    Forecast*, and saw how it is an autoregressive model. We can use the same concept
    to convert a time series problem into a regression one. Let’s use the following
    diagram to make the concept clear:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第4章*《设定强基准预测》中讨论了ARIMA模型，并看到了它是一个自回归模型。我们可以使用相同的概念，将一个时间序列问题转换为回归问题。让我们通过以下图示来明确这一概念：
- en: '![Figure 5.6 – Time series to regression conversion using a sliding window
    ](img/B22389_05_06.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – 使用滑动窗口将时间序列转换为回归](img/B22389_05_06.png)'
- en: 'Figure 5.6: Time series to regression conversion using a sliding window'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：使用滑动窗口将时间序列转换为回归
- en: Let’s assume we have a time series with *L* time steps, just like in the diagram.
    We have *T* as the latest observation, *T - 1*, *T - 2*, and so on as we move
    backward in time, all the way to*T - L*. In an ideal world, each observation should
    be conditioned on all the previous observations when we forecast. But this is
    not practical because *L* can be arbitrarily long. We often restrict the forecasting
    function to use only the most recent *M* observations of the series, where *M
    < L*. These are called finite memory models, or **Markov models**, and *M* is
    called the order of autoregression, memory size, or the receptive field.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个时间序列，包含*L*个时间步长，就像图示中所示。我们有*T*作为最新的观测值，*T - 1*、*T - 2*，依此类推，随着时间倒退，一直到*T
    - L*。在理想的世界中，每个观测值在进行预测时应该以所有先前的观测值为条件。但这是不切实际的，因为*L*可以非常长。我们通常会限制预测函数，只使用序列中最新的*M*个观测值，其中*M
    < L*。这些被称为有限记忆模型，或**马尔科夫模型**，而*M*被称为自回归的阶数、记忆大小或感受野。
- en: Therefore, in time delay embedding, we assume a window of arbitrary length *M
    < L* and extract fixed-length subsequences from the time series by sliding the
    window over the length of the time series.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在时间延迟嵌入中，我们假设一个任意长度的窗口*M < L*，并通过将窗口在时间序列的长度上滑动，提取固定长度的子序列。
- en: In the diagram, we have taken a sliding window with a memory size of **3**.
    So, the first subsequence we can extract (if we are starting from the most recent
    and working backward) is *T – 3*, *T – 2*, *T - 1*. And *T* is the observation
    that comes right after the subsequence. This becomes our first example in the
    dataset (row **1** in the table in the diagram).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在图示中，我们采用了一个记忆大小为**3**的滑动窗口。所以，首先提取的子序列（如果从最新时间点开始，按时间倒序提取）是*T – 3*、*T – 2*、*T
    - 1*。而*T*是紧接在该子序列之后的观测值。这将成为数据集中的第一个例子（图示中表格的第**1**行）。
- en: Now, we slide the window one time step to the left (backward in time) and extract
    the new subsequence, *T – 4*, *T – 3*, *T - 2*. The corresponding target would
    become *T - 1*. We repeat this process as we move back to the beginning of the
    time series, and at each step of the sliding window, we add one more example to
    the dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将窗口向左滑动一个时间步长（即倒退到过去），并提取新的子序列，*T – 4*、*T – 3*、*T - 2*。对应的目标将变为*T - 1*。我们在回到时间序列的开始时重复这一过程，在每一步滑动窗口时，我们都会向数据集中添加一个新的例子。
- en: At the end of it, we have an aligned dataset with a fixed vector size of features
    (which will be equal to the window size) and a single target, which is what a
    typical machine learning dataset looks like.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们得到了一个对齐的数据集，特征的固定向量大小（即等于窗口大小）和一个单一目标，这就是典型的机器学习数据集的样子。
- en: Now that we have a table with three features, let’s also assign semantic meaning
    to the three features. If we look at the right-most column in the table in the
    diagram, we can see that the time step present in the column is always one time
    step behind the target. We call it **Lag 1**. The second column from the right
    is always two time steps behind the target, and this is called **Lag 2**. Generalizing
    this, the feature that has observations that are *n* time steps behind the target,
    we call **Lag n**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个包含三个特征的表格，我们也给这三个特征赋予了语义意义。如果我们查看图示中表格的最右列，我们可以看到该列中的时间步长总是比目标落后一个时间步长。我们称之为**滞后
    1**。从右数第二列总是比目标滞后两个时间步长，我们称之为**滞后 2**。一般化地说，特征中观测值比目标滞后*n*个时间步长时，我们称之为**滞后 n**。
- en: This transformation from time series to regression using **time-delay embedding**
    encodes the autoregressive structure of a time series in a way that can be utilized
    by standard regression frameworks. Another way we can think about using regression
    for time series forecasting is to perform **regression on time**.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**时间延迟嵌入**将时间序列转换为回归模型，能够以一种标准回归框架能够利用的方式，编码时间序列的自回归结构。我们还可以考虑另一种使用回归进行时间序列预测的方法，那就是对**时间**进行回归。
- en: Temporal embedding
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间嵌入
- en: If we rely on previous observations in autoregressive models, we rely on the
    concept of time for temporal embedding models. The core idea is that we forget
    the autoregressive nature of the time series and assume that any value in the
    time series is only dependent on time. We derive features that capture time, the
    passage of time, the periodicity of time, and so on, from the timestamps associated
    with the time series, and then we use these features to predict the target using
    a regression model. There are many ways to do this, from simply aligning a monotonically
    and uniformly increasing numerical column that captures the passage of time to
    sophisticated **Fourier** terms to capture the periodic components in time. We
    will talk about those techniques in detail in *Chapter 6*, *Feature Engineering
    for Time Series Forecasting*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们依赖自回归模型中的先前观察值，那么我们在时间嵌入模型中则依赖于时间的概念。核心思想是，我们忽略时间序列的自回归特性，并假设时间序列中的任何值仅仅依赖于时间。我们从与时间序列相关的时间戳中提取能够捕捉时间、时间的流逝、时间的周期性等特征，然后使用这些特征通过回归模型来预测目标值。实现这一点的方法有很多，从简单地对齐一个单调且均匀递增的数值列来捕捉时间的流逝，到使用复杂的**傅里叶**项来捕捉时间中的周期性成分。我们将在*第6章*，*时间序列预测的特征工程*中详细讨论这些技术。
- en: Before we wind up the chapter, let’s also talk about a key concept that is gaining
    ground steadily in the time series forecasting space. A large part of this book
    embraces this new paradigm of forecasting.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束本章之前，让我们讨论一个在时间序列预测领域逐渐受到关注的关键概念。本书的大部分内容都采纳了这种新的预测范式。
- en: Global forecasting models—a paradigm shift
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全球预测模型——范式转变
- en: Traditionally, each time series was treated in isolation. Because of that, traditional
    forecasting has always looked at the history of a single time series alone in
    fitting a forecasting function. But recently, because of the ease of collecting
    data in today’s digital-first world, many companies have started collecting large
    amounts of time series from similar sources, or related time series.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，每个时间序列都是孤立地处理的。正因为如此，传统的预测方法总是仅仅基于单一时间序列的历史来拟合预测函数。但近年来，由于在当今以数字为主的世界中，收集数据变得更加容易，许多公司开始收集来自相似来源或相关时间序列的大量数据。
- en: For example, retailers such as Walmart collect data on the sales of millions
    of products across thousands of stores. Companies such as Uber and Lyft collect
    the demand for rides from all the zones in a city. In the energy sector, energy
    consumption data is collected across all consumers. All these sets of time series
    have shared behavior and are hence called **related time series**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，零售商如沃尔玛会收集跨千家商店的数百万种产品的销售数据。像Uber和Lyft这样的公司会收集城市中所有区域的乘车需求。在能源领域，能源消费数据会跨所有消费者进行收集。所有这些时间序列数据集都有共同的行为，因此被称为**相关时间序列**。
- en: We can consider that all the time series in a related time series come from
    separate DGPs, and thereby model them all separately. We call these the **local**
    models of forecasting. An alternative to this approach is to assume that all the
    time series are coming from a single DGP. Instead of fitting a separate forecast
    function for each time series individually, we fit a single forecast function
    to all the related time series. This approach has been called **global** or **cross-learning**
    in the literature. Most modern deep learning models as well as machine learning
    approaches adopt the global model paradigm. We will see those in detail in the
    coming chapters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以认为，所有相关时间序列中的时间序列都来自不同的DGP（数据生成过程），因此可以将它们分别建模。我们称这些为**局部**预测模型。该方法的另一种替代方式是假设所有时间序列都来自同一个DGP。我们不为每个时间序列单独拟合预测函数，而是为所有相关时间序列拟合一个单一的预测函数。这种方法在文献中被称为**全局**或**跨学习**。大多数现代深度学习模型以及机器学习方法都采用了全局模型的范式。我们将在接下来的章节中详细看到这些内容。
- en: '**Reference check**:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The term *global* was introduced by *David Salinas et al*. in the *DeepAR* paper
    (reference *1*) and *cross-learning* by *Slawek Smyl* (reference *2*).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*全球*一词由*David Salinas等人*在*DeepAR*论文（参考文献*1*）中提出，*跨学习*则由*Slawek Smyl*（参考文献*2*）提出。'
- en: We saw earlier that having more data will lead to lower chances of overfitting
    and, therefore, lowers the generalization error (the difference between training
    and testing errors). This is one of the shortcomings of the local approach. Traditionally,
    time series are not very long, and in many cases, it is difficult and time-consuming
    to collect more data as well. Fitting a machine learning model (with all its expressiveness)
    on small data is prone to overfitting. This is why time series models that enforce
    strong priors were used to forecast such time series, traditionally. But these
    strong priors, which restrict the fitting of traditional time series models, can
    also lead to a form of underfitting and limit accuracy.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，拥有更多数据将减少过拟合的可能性，因此可以降低泛化误差（训练误差与测试误差之间的差异）。这是局部方法的一个缺点。传统上，时间序列数据通常不长，且在很多情况下，收集更多数据既困难又耗时。在小数据上拟合机器学习模型（具有强大的表达能力）容易导致过拟合。这也是为什么传统上用于预测此类时间序列的时间序列模型会强制施加强先验的原因。但这些限制传统时间序列模型拟合的强先验，也可能导致一种形式的欠拟合，限制了准确性。
- en: Strong and expressive data-driven models, as in machine learning, require a
    larger amount of data to have a model that generalizes to new and unseen data.
    A time series, by definition, is tied to time, and sometimes, collecting more
    data means waiting for months or years and that is not desirable. So, if we cannot
    increase the *length* of the time series dataset, we can increase the *width*
    of the time series dataset. If we add multiple time series to the dataset, we
    increase the width of the dataset, and thereby increase the amount of data the
    model is getting trained with.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 强大且具有表达能力的数据驱动模型，如机器学习模型，需要大量数据来生成能够对新数据进行泛化的模型。时间序列本质上是与时间相关的，有时收集更多数据意味着需要等待数月甚至数年，这是不理想的。因此，如果我们无法增加时间序列数据集的*长度*，我们可以增加时间序列数据集的*宽度*。如果我们将多个时间序列加入数据集，我们就增加了数据集的宽度，从而增加了模型训练时可用的数据量。
- en: '*Figure 5.7* shows the concept of increasing the width of a time series dataset
    visually:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.7* 展示了通过视觉化增加时间序列数据集宽度的概念：'
- en: '![Figure 5.7 – The length and width of a time series dataset ](img/B22389_05_07.png)Figure
    5.7: The length and width of a time series dataset'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7 – 时间序列数据集的长度和宽度](img/B22389_05_07.png)图 5.7：时间序列数据集的长度和宽度'
- en: This works in favor of machine learning models because with higher flexibility
    in fitting a forecast function and the addition of more data to work with, the
    machine learning model can learn a more complex forecast function than traditional
    time series models, which are typically shared between the related time series,
    in a completely data-driven way.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这对机器学习模型有利，因为在拟合预测函数时，机器学习模型具有更高的灵活性，且能够利用更多数据，从而能够学习出比传统时间序列模型更复杂的预测函数，而传统时间序列模型通常是通过与相关时间序列共享的方式进行的，完全基于数据驱动。
- en: Another shortcoming of the local approach revolves around scalability. In the
    case of Walmart we mentioned earlier, there are millions of time series that need
    to be forecasted and it is not possible to have human oversight on all these models.
    If we think about this from an engineering perspective, training and maintaining
    millions of models in a production system would give any engineer a nightmare.
    But under the global approach, we only train a single model for all these time
    series, which drastically reduces the number of models we need to maintain and
    yet can generate all the required forecasts.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 局部方法的另一个缺点是可扩展性问题。以我们之前提到的沃尔玛为例，需要预测数百万个时间序列，并且无法对所有这些模型进行人工监督。如果从工程角度考虑，在生产系统中训练和维护数百万个模型对任何工程师来说都是一场噩梦。但在全球方法下，我们只需为所有这些时间序列训练一个模型，这大大减少了我们需要维护的模型数量，同时还能够生成所有所需的预测。
- en: This new paradigm of forecasting has gained traction and has consistently been
    shown to improve the local approaches in multiple time series competitions, mostly
    in datasets of related time series. In Kaggle competitions, such as *Rossman Store
    Sales* (2015), *Wikipedia WebTraffic Time Series Forecasting* (2017), *Corporación
    Favorita Grocery Sales Forecasting* (2018), and *M5 Competition* (2020), the winning
    entries were all global models—either machine learning or deep learning or a combination
    of both. The *Intermarché Forecasting Competition* (2021) also had global models
    as the winning submissions. Links to these competitions are provided in the *Further
    reading* section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这种新的预测范式已经获得了广泛关注，并且在多个时间序列竞赛中持续证明能够改进局部方法，尤其是在相关时间序列数据集上。在Kaggle竞赛中，例如*Rossman商店销售*（2015年）、*维基百科网页流量时间序列预测*（2017年）、*Favorita公司杂货销售预测*（2018年）以及*M5竞赛*（2020年），获胜的参赛作品都是全球模型——无论是机器学习、深度学习还是二者的结合。*Intermarché预测竞赛*（2021年）的获胜作品也是全球模型。有关这些竞赛的链接可以在*进一步阅读*部分找到。
- en: Although we have many empirical findings where the global models have outperformed
    local models for related time series, global models are still a relatively new
    area of research. *Montero-Manson and Hyndman* (2020) showed a few very interesting
    results and showed that any local method can be approximated by a global model
    with required complexity, and the most interesting finding they put forward is
    that the global model will perform better, even with unrelated time series. We
    will talk more about global models and strategies for global models in *Chapter
    10*, *Global Forecasting Models*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们有许多经验性发现表明，全球模型在相关时间序列预测中优于局部模型，但全球模型仍然是一个相对较新的研究领域。*Montero-Manson 和 Hyndman*
    (2020) 展示了一些非常有趣的结果，并表明任何局部方法都可以通过具备必要复杂度的全球模型进行逼近，他们提出的最有趣的发现是，即使面对不相关的时间序列，全球模型也能表现得更好。我们将在*第10章*中进一步讨论全球模型及其策略，即*全球预测模型*。
- en: '**Reference check**:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The *Montero-Manson and Hyndman* (2020) research paper is cited in *References*
    under reference *3*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*Montero-Manson 和 Hyndman* (2020) 的研究论文在*参考文献*中被引用，参考文献编号为*3*。'
- en: Summary
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We have started our journey beyond baseline forecasting methods and dipped our
    toes into the world of machine learning. After a brief refresher on machine learning,
    where we looked at key concepts such as overfitting, underfitting, regularization,
    and so on, we saw how we can convert a time series forecasting problem into a
    regression problem from the machine learning world. We also developed a conceptual
    understanding of different embeddings, such as time delay embedding and temporal
    embedding, which can be used to convert a time series problem into a regression
    problem. To wrap things up, we also learned about a new paradigm in time series
    forecasting—global models—and contrasted them with local models on a conceptual
    level. In the next few chapters, we will start putting these concepts into practice,
    and see techniques for feature engineering and strategies for global models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经开始了超越基准预测方法的探索，并初步涉足机器学习领域。在简要回顾机器学习的基础知识后，我们了解了诸如过拟合、欠拟合、正则化等关键概念，之后我们看到如何将时间序列预测问题转化为机器学习中的回归问题。我们还对不同的嵌入方法，如时间延迟嵌入和时间嵌入，进行了概念性的理解，这些方法可以用于将时间序列问题转化为回归问题。最后，我们还了解了一种新的时间序列预测范式——全球模型，并在概念层面上将其与局部模型进行了对比。在接下来的几章中，我们将开始实践这些概念，学习特征工程技术和全球模型的策略。
- en: References
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'The following are the references that we used in this chapter:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在本章中使用的参考文献：
- en: 'David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). *DeepAR:
    Probabilistic forecasting with autoregressive recurrent networks*. International
    Journal of Forecasting. 36-3\. 1181–1191: [https://doi.org/10.1016/j.ijforecast.2019.07.001](https://doi.org/10.1016/j.ijforecast.2019.07.001)'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). *DeepAR:
    基于自回归递归网络的概率预测*. 国际预测学杂志. 36-3. 1181–1191: [https://doi.org/10.1016/j.ijforecast.2019.07.001](https://doi.org/10.1016/j.ijforecast.2019.07.001)'
- en: 'Slawek Smyl (2020). *A hybrid method of exponential smoothing and recurrent
    neural networks for time series forecasting*. International Journal of Forecasting.
    36-1: 75–85 [https://doi.org/10.1016/j.ijforecast.2019.03.017](https://doi.org/10.1016/j.ijforecast.2019.03.017)'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Slawek Smyl (2020). *指数平滑与递归神经网络的混合方法用于时间序列预测*. 国际预测学杂志. 36-1: 75–85 [https://doi.org/10.1016/j.ijforecast.2019.03.017](https://doi.org/10.1016/j.ijforecast.2019.03.017)'
- en: 'Pablo Montero-Manso, Rob J Hyndman (2020), *Principles and Algorithms for Forecasting
    Groups of Time Series: Locality and Globality*. arXiv:2008.00444[cs.LG]: [https://arxiv.org/abs/2008.00444](https://arxiv.org/abs/2008.00444
    )'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Pablo Montero-Manso, Rob J Hyndman (2020), *Principles and Algorithms for Forecasting
    Groups of Time Series: Locality and Globality*. arXiv:2008.00444[cs.LG]： [https://arxiv.org/abs/2008.00444](https://arxiv.org/abs/2008.00444)'
- en: Further reading
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can check out the following resources for further reading:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看以下资源以便进一步阅读：
- en: '*Regularization for Sparsity from Google Machine Learning Crash Course*: [https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization](https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Regularization for Sparsity from Google Machine Learning Crash Course*： [https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization](https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization)'
- en: '*L1 & L2 Regularization, Inside Bloomberg*: [https://www.youtube.com/watch?v=d6XDOS4btck](https://www.youtube.com/watch?v=d6XDOS4btck)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L1 & L2 Regularization, Inside Bloomberg*： [https://www.youtube.com/watch?v=d6XDOS4btck](https://www.youtube.com/watch?v=d6XDOS4btck)'
- en: '*Cross-validation: evaluating estimator performance from scikit-learn*: [https://scikit-learn.org/stable/modules/cross_validation.html](https://scikit-learn.org/stable/modules/cross_validation.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Cross-validation: evaluating estimator performance from scikit-learn*： [https://scikit-learn.org/stable/modules/cross_validation.html](https://scikit-learn.org/stable/modules/cross_validation.html)'
- en: '*Rossmann Store Sales*: [https://www.kaggle.com/c/rossmann-store-sales](https://www.kaggle.com/c/rossmann-store-sales)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Rossmann Store Sales*： [https://www.kaggle.com/c/rossmann-store-sales](https://www.kaggle.com/c/rossmann-store-sales)'
- en: '*Web Traffic Time Series Forecasting*: [https://www.kaggle.com/c/web-traffic-time-series-forecasting](https://www.kaggle.com/c/web-traffic-time-series-forecasting)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Web Traffic Time Series Forecasting*： [https://www.kaggle.com/c/web-traffic-time-series-forecasting](https://www.kaggle.com/c/web-traffic-time-series-forecasting)'
- en: '*Corporación Favorita Grocery Sales Forecasting*: [https://www.kaggle.com/c/favorita-grocery-sales-forecasting](https://www.kaggle.com/c/favorita-grocery-sales-forecasting)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Corporación Favorita Grocery Sales Forecasting*： [https://www.kaggle.com/c/favorita-grocery-sales-forecasting](https://www.kaggle.com/c/favorita-grocery-sales-forecasting)'
- en: '*M5 Forecasting—Accuracy*: [https://www.kaggle.com/c/m5-forecasting-accuracy](https://www.kaggle.com/c/m5-forecasting-accuracy)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*M5 Forecasting—Accuracy*： [https://www.kaggle.com/c/m5-forecasting-accuracy](https://www.kaggle.com/c/m5-forecasting-accuracy)'
- en: Join our community on Discord
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with authors and other readers:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mts](https://packt.link/mts)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mts](https://packt.link/mts)'
- en: '![](img/QR_Code15080603222089750.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code15080603222089750.png)'
