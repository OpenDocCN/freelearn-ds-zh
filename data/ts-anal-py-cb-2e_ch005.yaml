- en: 4 Persisting Time Series Data to Files
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 将时间序列数据持久化到文件
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的书籍社区，访问 Discord
- en: '![](img/file0.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/file0.png)'
- en: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
- en: In this chapter, you will use the **pandas** library to persist your **time
    series DataFrames** to different file formats, such as **CSV**, **Excel**, **Parquet**,
    and **pickle** files. When performing analysis or data transformations on DataFrames,
    you essentially leverage pandas' in-memory analytics capabilities, offering great
    performance. However, being in memory means the data can easily be lost since
    it has not yet persisted in disk storage.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将使用**pandas**库将你的**时间序列DataFrame**持久化到不同的文件格式中，如**CSV**、**Excel**、**Parquet**和**pickle**文件。在对DataFrame进行分析或数据转换时，实际上是利用了pandas的内存分析能力，提供了极好的性能。然而，内存中的数据意味着它很容易丢失，因为它尚未被持久化到磁盘存储中。
- en: When working with DataFrames, you will need to persist your data for future
    retrieval, creating backups, or sharing your data with others. The **pandas**
    library is bundled with a rich set of writer functions to persist your in-memory
    DataFrame (or series) to disk in various file formats. These writer functions
    allow you to store data on a local drive or a remote server location, such as
    a cloud storage filesystem, including **Google Drive**, **AWS S3**, **Azure Blob
    Storage**, and **Dropbox**.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理DataFrame时，你需要持久化数据以便将来取回、创建备份或与他人共享数据。**pandas**库附带了一套丰富的写入函数，可以将内存中的DataFrame（或系列）持久化到磁盘上的不同文件格式中。这些写入函数使你能够将数据存储到本地驱动器或远程服务器位置，例如云存储文件系统，包括**Google
    Drive**、**AWS S3**、**Azure Blob Storage**和**Dropbox**。
- en: In this chapter, you will explore writing to different file formats locally
    (on-premises) and cloud storage locations like on Amazon Web Services (AWS), Google
    Cloud, and Azure.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将探索将数据写入不同的文件格式（本地存储）和云存储位置，如Amazon Web Services（AWS）、Google Cloud和Azure。
- en: 'Here are the recipes that will be covered in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章将涵盖的食谱：
- en: Time series data serialization with `pickle`
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pickle`进行时间序列数据序列化
- en: Writing to CSV and other delimited files
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写入CSV和其他分隔符文件
- en: Writing data to an Excel file
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写入Excel文件
- en: Storing Data to a Cloud Storage (AWS, GCP, and Azure)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据存储到云存储（AWS、GCP和Azure）
- en: Writing Large Datasets
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写入大规模数据集
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter and beyond, we will extensively use pandas 2.2.2 (released April
    10, 2023).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章及之后的内容中，我们将广泛使用pandas 2.2.2版本（2023年4月10日发布）。
- en: Throughout our journey, you will be installing several Python libraries to work
    in conjunction with pandas. These are highlighted in the *Getting ready* section
    for each recipe. You can also download Jupyter notebooks from the GitHub repository
    ([https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook))
    to follow along. You can download the datasets used in this chapter here [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch4](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch4)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中，你将安装多个Python库，以与pandas协同工作。这些库在每个食谱的*准备工作*部分中都有突出说明。你还可以从GitHub仓库下载Jupyter笔记本（[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook)）来跟随学习。你可以在这里下载本章使用的数据集：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch4](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch4)
- en: Serializing time series data with pickle
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用pickle序列化时间序列数据
- en: When working with data in Python, you may want to persist Python data structures
    or objects, such as a pandas DataFrame, to disk instead of keeping it in memory.
    One technique is to serialize your data into a byte stream to store it in a file.
    In Python, the **pickle** module is a popular approach to object serialization
    and de-serialization (the reverse of serialization), also known as *pickling*
    and *unpickling*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中处理数据时，你可能希望将Python数据结构或对象（如pandas DataFrame）持久化到磁盘，而不是将其保留在内存中。一个方法是将数据序列化为字节流，以便将其存储在文件中。在Python中，**pickle**模块是一种常见的对象序列化与反序列化方法（序列化的反过程），也被称为*pickling*（序列化）和*unpickling*（反序列化）。
- en: Getting ready
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 准备工作
- en: The `pickle` module comes with Python, so no additional installation is needed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`pickle`模块是Python自带的，因此无需额外安装。'
- en: In this recipe, we will explore two different methods for serializing the data,
    commonly referred to as **pickling**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将探索两种常见的序列化数据的方法，这些方法通常被称为**pickling**。
- en: 'You will be using the COVID-19 dataset provided by the *COVID-19 Data Repository
    by the Center for Systems Science and Engineering (CSSE)* at *Johns Hopkins University*,
    which you can download from the official GitHub repository here: [https://github.com/CSSEGISandData/COVID-19](https://github.com/CSSEGISandData/COVID-19).
    Note that John Hopkins University is no longer updating the dataset as of March
    10, 2023.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用由*约翰·霍普金斯大学*的*系统科学与工程中心（CSSE）*提供的COVID-19数据集，你可以从官方GitHub仓库下载该数据集，链接为：[https://github.com/CSSEGISandData/COVID-19](https://github.com/CSSEGISandData/COVID-19)。请注意，约翰·霍普金斯大学自2023年3月10日起不再更新该数据集。
- en: How to do it…
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何实现…
- en: You will write to a **pickle** file using pandas' `DataFrame.to_pickle()` function
    and then explore an alternative option using the `pickle` library directly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用pandas的`DataFrame.to_pickle()`函数将数据写入**pickle**文件，然后使用`pickle`库直接探索另一种选择。
- en: Writing to a pickle file using pandas
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用pandas写入pickle文件
- en: 'You will start by reading the COVID-19 time series data into a DataFrame, making
    some transformations, and then persisting the results to a `pickle` file for future
    analysis. This should resemble a typical scenario for persisting data that is
    still a work in progress (in terms of analysis):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你将从读取COVID-19时间序列数据到DataFrame开始，进行一些转换，然后将结果持久化到`pickle`文件中，以便未来分析。这应该类似于持久化仍在进行中的数据（就分析而言）的一种典型场景：
- en: 'To start, let''s load the CSV data into a pandas DataFrame:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们将CSV数据加载到pandas DataFrame中：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding code will display the first five rows of the DataFrame:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将显示DataFrame的前五行：
- en: '![Figure 4.1: The first five rows of the COVID-19 confirmed global cases](img/file32.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1：COVID-19全球确诊病例的前五行](img/file32.png)'
- en: 'Figure 4.1: The first five rows of the COVID-19 confirmed global cases'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：COVID-19全球确诊病例的前五行
- en: You can observe from the output that this is a wide DataFrame with 1147 columns,
    where each column represents the data's collection date, starting from **1/22/20**
    to **3/9/23**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从输出中观察到，这是一个宽格式DataFrame，共有1147列，每列代表一个数据收集日期，从**1/22/20**到**3/9/23**。
- en: 'Let''s assume that part of the analysis is to focus on United States and only
    data collected in the summer of 2021 (June, July, August, and September). You
    will transform the DataFrame by applying the necessary filters, and then unpivot
    the data so that the dates are in rows as opposed to columns (converting from
    a wide to a long format):'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设分析的一部分是聚焦于美国，并且只使用2021年夏季（6月、7月、8月和9月）收集的数据。你将通过应用必要的过滤器转换DataFrame，然后将数据反向旋转，使日期显示在行中而不是列中（从宽格式转换为长格式）：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Inspect the `df_usa_summer_unpivoted` DataFrame and print the first five records:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查`df_usa_summer_unpivoted` DataFrame并打印前五条记录：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You filtered the dataset and transformed it from a wide DataFrame to a long
    time series DataFrame.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你已对数据集进行了筛选，并将其从宽格式的DataFrame转换为长格式的时间序列DataFrame。
- en: 'Let’s say you are now satisfied with the dataset and ready to pickle (serialize)
    the dataset. You will write the DataFrame to a `covid_usa_summer_2020.pkl` file
    using the `DataFrame.to_pickle()` function:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你现在对数据集已经满意，准备将数据集进行pickling（序列化）。你将使用`DataFrame.to_pickle()`函数将DataFrame写入`covid_usa_summer_2020.pkl`文件：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Pickling preserves the schema of the DataFrame. When you ingest the pickled
    data again (de-serialization), you will return the DataFrame in its original construct,
    for example, with a `DatetimeIndex` type.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Pickling保留了DataFrame的结构。当你再次加载pickle数据时（反序列化），你将恢复DataFrame的原始结构，例如，带有`DatetimeIndex`类型。
- en: 'Read the pickle file using the `pandas.read_pickle()` reader function and inspect
    the DataFrame:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas.read_pickle()`读取pickle文件并检查DataFrame：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: From the preceding example, you were able to de-serialize the data using `pandas.read_pickle()`
    into a DataFrame, with all the previously committed transformations and datatypes
    preserved.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的示例中，你能够使用`pandas.read_pickle()`反序列化数据到DataFrame中，并保留之前所做的所有转换和数据类型。
- en: Writing a pickle file using the pickle library
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用pickle库写入pickle文件
- en: Python comes shipped with the **pickle** library, which you can import and use
    to serialize (pickle) objects using `dump` (to write) and `load` (to read). In
    the following steps, you will use `pickle.dump()` and `pickle.load()` to serialize
    and then de-serialize the `df_usa_summer_unpivoted` DataFrame.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Python自带**pickle**库，你可以导入并使用它来序列化（pickle）对象，使用`dump`（写入）和`load`（读取）。在接下来的步骤中，你将使用`pickle.dump()`和`pickle.load()`来序列化并反序列化`df_usa_summer_unpivoted`
    DataFrame。
- en: 'Import the `pickle` library:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`库：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You then persist the `df_usa_summer_unpivoted` DataFrame using the `dump()`
    method:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以使用 `dump()` 方法将 `df_usa_summer_unpivoted` DataFrame 持久化：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice the mode used is `“wb”` because we are writing in binary mode (written
    as raw bytes).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用的模式是 `“wb”`，因为我们是以二进制模式写入（以原始字节写入）。
- en: 'You can read the file and inspect the DataFrame using the `load()` method.
    Notice in the following code that the ingested object is a pandas DataFrame, even
    though you used `pickle.load()` instead of `Pandas.read_pickle()`. This is because
    pickling preserved the schema and data structures:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用 `load()` 方法读取文件并检查 DataFrame。请注意，在以下代码中，导入的对象是一个 pandas DataFrame，尽管您使用的是
    `pickle.load()` 而不是 `Pandas.read_pickle()`。这是因为 pickling 保留了模式和数据结构：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Notice the mode used is `“rb”` because we are reading in binary mode (read as
    raw bytes).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用的模式是 `“rb”`，因为我们是以二进制模式读取（作为原始字节读取）。
- en: How it works…
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In Python, pickling is the process of serializing any Python object. More concretely,
    it uses a binary serialization protocol to convert objects into binary information,
    which is not a human-readable format. The protocol allows us to reconstruct (de-serialize)
    the pickled file, binary format, into its original content without losing valuable
    information. As in the preceding examples, we confirmed that a time series DataFrame,
    when reconstructed (de-serialization), returned to its exact form (schema).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，Pickling 是将任何 Python 对象序列化的过程。更具体地说，它使用一种二进制序列化协议将对象转换为二进制信息，这是一种不可人类读取的格式。该协议允许我们重新构建（反序列化）被
    pickle 的文件（二进制格式），使其恢复到原始内容而不丢失宝贵的信息。如前面的示例所示，我们确认时间序列 DataFrame 在重新构建（反序列化）时，能够恢复到其精确的形式（模式）。
- en: The pandas `DataFrame.to_pickle()` function has two additional parameters that
    are important to know. The first is the `compression` parameter, which is also
    available in other writer functions such as `to_csv()`, `to_json()`, and `to_paraquet()`,
    to name a few.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 的 `DataFrame.to_pickle()` 函数有两个额外的参数，重要的是需要了解第一个参数是 `compression`，该参数在其他写入函数中也可用，如
    `to_csv()`、`to_json()` 和 `to_parquet()` 等。
- en: 'In the case of the `DataFrame.to_pickle()` function, the default compression
    value is set to `infer`, which lets pandas determine which compression mode to
    use based on the file extension provided. In the previous example, we used `DataFrame.to_pickle(output)`,
    where `output` was defined with a `.pkl` file extension, as in `covid_usa_summer_2020.pkl`.
    If you change it to `covid_usa_summer_2020.zip`, the output will be a compressed
    binary serialized file stored in ZIP format. You can try the following example:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `DataFrame.to_pickle()` 函数的情况下，默认的压缩值设置为 `infer`，这让 pandas 根据提供的文件扩展名来确定使用哪种压缩模式。在前面的示例中，我们使用了
    `DataFrame.to_pickle(output)`，其中 `output` 被定义为 `.pkl` 文件扩展名，如 `covid_usa_summer_2020.pkl`。如果将其更改为
    `covid_usa_summer_2020.zip`，则输出将是存储在 ZIP 格式中的压缩二进制序列化文件。您可以尝试以下示例：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Other supported compression modes include **gzip**, **bz2**, **tar**, and **xz**.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其他支持的压缩模式包括 **gzip**、**bz2**、**tar** 和 **xz**。
- en: The second parameter is **protocol**. By default, the `DataFrame.to_pickle()`
    writer function uses the highest protocol, which, as of this writing, is set to
    5\. According to the Pickle documentation, there are six (6) different protocols
    to choose from when pickling, starting from protocol version 0 to the latest protocol
    version, 5.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数是 **protocol**。默认情况下，`DataFrame.to_pickle()` 写入函数使用最高协议，截至本文编写时，该协议设置为
    5。根据 Pickle 文档，当进行 pickle 时，有六个（6）不同的协议可供选择，从协议版本 0 到最新的协议版本 5。
- en: 'Outside of pandas, you can check what is the highest protocol configuration
    by using the following command:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pandas 之外，您可以使用以下命令检查最高协议配置是什么：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Similarly, by default, `pickle.dump()` uses the `HIGHEST_PROTOCOL` value if
    no other value was provided. The construct looks like the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，默认情况下，`pickle.dump()` 使用 `HIGHEST_PROTOCOL` 值，如果没有提供其他值的话。该构造如下所示：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding two code snippets are equivalent.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 前面两个代码片段是等效的。
- en: There's more…
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 还有更多……
- en: One of the advantages of pickling a binary serialization method is that we can
    pretty much pickle most Python objects, whether a Python dictionary, a machine
    learning model, a Python function, or a more complex data structure, such as a
    pandas DataFrame. However, there are some limitations on certain objects, such
    as lambda and nested functions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Pickling（二进制序列化方法）的一个优点是我们几乎可以 pickle 大多数 Python 对象，无论是 Python 字典、机器学习模型、Python
    函数，还是更复杂的数据结构，如 pandas DataFrame。然而，某些对象（如 lambda 和嵌套函数）存在一些限制。
- en: 'Let''s examine how you can pickle a function and its output. You will create
    a `covid_by_country` function that takes three arguments: *the CSV file to read*,
    the *number of days back*, and the *country*. The function will return a time
    series DataFrame. You will then pickle both the function, the function''s output,
    and it’s plot as well:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何将一个函数及其输出进行序列化。你将创建一个`covid_by_country`函数，该函数需要三个参数：*要读取的CSV文件*、*回溯的天数*和*国家*。该函数将返回一个时间序列DataFrame。接着你将对函数、函数的输出以及其图形进行序列化：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The function would output the following plot:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将输出以下图形：
- en: '![Figure 4.:– Output of the covid_by_country function](img/file33.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.：– covid_by_country函数的输出](img/file33.png)'
- en: Figure 4.:– Output of the covid_by_country function
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.：– covid_by_country函数的输出
- en: 'Before pickling your objects, you can further enhance the content by adding
    additional information to remind you what the content is all about. In the following
    code, you will serialize the function and the returned DataFrame with additional
    information (known as **metadata**) encapsulated using a Python dictionary:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在将对象序列化之前，你可以通过添加额外的信息来进一步增强内容，以提醒你内容的含义。在以下代码中，你将序列化函数以及返回的DataFrame，并使用Python字典封装附加信息（称为**元数据**）：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To gain a better intuition on how this works, you can load the content and
    de-serialize using `pickle.load()`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解它是如何工作的，你可以加载内容并使用`pickle.load()`进行反序列化：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can retrieve and use the function, as shown in the following code:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照以下代码检索并使用该函数：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can also retrieve the previous DataFrame stored for the US:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以检索之前为美国存储的DataFrame：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can also load the view of the figure you just stored. The following code
    will display the plot like that in *Figure 4.2*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以加载你刚刚存储的图形视图。以下代码将展示类似于*图 4.2*中的图形：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The preceding examples demonstrate how pickling can be useful to store objects
    and additional **metadata** information. This can be helpful when storing a work-in-progress
    or performing multiple experiments and wanting to keep track of them and their
    outcomes. A similar approach can be taken when working with machine learning experiments,
    as you can store your model and any related information around the experiment
    and its outputs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例展示了序列化如何有助于存储对象及附加的**元数据**信息。这在存储一个正在进行的工作或执行多个实验并希望跟踪它们及其结果时非常有用。在机器学习实验中也可以采用类似的方法，因为你可以存储模型及与实验和其输出相关的任何信息。
- en: See also
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 另见
- en: 'For more information about `Pandas.DataFrame.to_pickle`, please visit this
    page: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_pickle.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_pickle.html).'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关`Pandas.DataFrame.to_pickle`的更多信息，请访问此页面：[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_pickle.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_pickle.html)。
- en: 'For more information about the Python Pickle module, please visit this page:
    [https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html).'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关Python Pickle模块的更多信息，请访问此页面：[https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html)。
- en: Writing to CSV and other delimited files
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写入CSV及其他分隔符文件
- en: In this recipe, you will export a DataFrame as a CSV file and leverage the different
    parameters in the `DataFrame.to_csv()` writer function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，你将导出一个DataFrame为CSV文件，并利用`DataFrame.to_csv()`写入函数中的不同参数。
- en: Getting ready
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The file is provided in the GitHub repository for this book, which you can
    find here: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook).
    The file you will be working with is named `movieboxoffice.csv` which you read
    first to create your DataFrame.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件已提供在本书的GitHub代码库中，你可以在这里找到：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook)。你将使用的文件名为`movieboxoffice.csv`，首先读取该文件以创建你的DataFrame。
- en: 'To prepare for this recipe, you will read the file into a DataFrame with the
    following code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备本例，你将使用以下代码将文件读取为一个DataFrame：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You now have a time series DataFrame with an index of type `DatetimeIndex`
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在有了一个以`DatetimeIndex`为索引的时间序列DataFrame。
- en: How to do it…
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Writing a DataFrame to a CSV file is pretty straightforward with pandas. The
    DataFrame object has access to many writer methods, such as `.to_csv`, which is
    what you will be using in the following steps:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 将 DataFrame 写入 CSV 文件非常简单。DataFrame 对象可以访问许多写入方法，如 `.to_csv`，你将在接下来的步骤中使用这个方法：
- en: 'You will use the pandas DataFrame writer method to persist the DataFrame as
    a CSV file. The method has several parameters, but at a minimum, all you need
    is to pass a file path and filename:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将使用 pandas DataFrame 写入方法来将 DataFrame 持久化为 CSV 文件。该方法有多个参数，但至少你需要传递文件路径和文件名：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The CSV file created is **comma-delimited** by default.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，创建的 CSV 文件是 **逗号分隔的**。
- en: 'To change the delimiter, use the `sep` parameter and pass in a different argument.
    In the following code, you are creating a pipe `(|)` delimited file:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要更改分隔符，可以使用 `sep` 参数并传入不同的参数。在以下代码中，你将创建一个管道符 `(|)` 分隔的文件：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Read the pipe-delimited file and inspect the resulting DataFrame object:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取管道分隔的文件并检查生成的 DataFrame 对象：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Notice from the preceding output that some information was lost when reading
    the CSV file. For example, the `Date` column in the original DataFrame “`movies`”
    was not a column but an index of type `DatetimeIndex`. The current DataFrame “`moveis_df`”
    does not have an index of type `DatetimeIndex` (the index is now a `RangeIndex`
    type, which is just a range for the row numbers). This means you will need to
    configure the `read_csv()` function and pass the necessary arguments to parse
    the file appropriately (this is in contrast to when reading a **pickled** file,
    as demonstrated from the preceding recipe, *Serializing time series data with
    pickle*).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的输出可以看出，读取 CSV 文件时丢失了一些信息。例如，原始 DataFrame “`movies`” 中的 `Date` 列实际上不是列，而是
    `DatetimeIndex` 类型的索引。当前的 DataFrame “`movies_df`” 并没有 `DatetimeIndex` 类型的索引（现在的索引是
    `RangeIndex` 类型，仅为行号的范围）。这意味着你需要配置 `read_csv()` 函数并传递必要的参数，以便正确解析文件（这与读取 **pickle**
    文件的情况不同，正如前面的示例中所演示的，*使用 pickle 序列化时间序列数据*）。
- en: Generally, CSV file formats do not preserve index type or column datatype information.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，CSV 文件格式不会保留索引类型或列数据类型信息。
- en: How it works…
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The default behavior for `DataFrame.to_csv()` is to write a **comma-delimited**
    CSV file based on the default `sep` parameter, which is set to `","`. You can
    overwrite this by passing a different delimiter, such as tab `("\t")`, `pipe ("|")`,
    or semicolon `(";")`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame.to_csv()` 的默认行为是根据默认的 `sep` 参数（默认为 `","`）写入一个 **逗号分隔的** CSV 文件。你可以通过传递不同的分隔符来覆盖这个默认行为，例如制表符
    `("\t")`、管道符 `("|")` 或分号 `(";")`。'
- en: 'The following code shows examples of different **delimiters** and their representations:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了不同 **分隔符** 及其表示方式：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: There's more…
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: Notice in the preceding example that the comma-separated string values are not
    encapsulated within double quotes (`""`). What will happen if our string object
    contains commas (`,`) and we write it to a comma-separated CSV file? Let's see
    how pandas handles this scenario.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在上面的示例中，逗号分隔的字符串值没有被双引号（`""`）包围。如果我们的字符串对象包含逗号（`,`）并且我们将其写入逗号分隔的 CSV 文件，会发生什么呢？让我们看看
    pandas 如何处理这个场景。
- en: 'In the following code, we will create a `person` DataFrame:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将创建一个 `person` DataFrame：
- en: '[PRE22]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, export the DataFrame to a CSV file. You will specify `index=False` to
    ignore the index (row numbers) when exporting:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将 DataFrame 导出为 CSV 文件。你将指定 `index=False` 来忽略导出的索引（行号）：
- en: '[PRE23]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If you inspect a `person_a.csv` file, you will see the following representation
    (notice the double quotes added by pandas):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查 `person_a.csv` 文件，你会看到以下表示方式（注意 pandas 添加的双引号）：
- en: '[PRE24]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `to_csv()` function has a `quoting` parameter with a default value set to
    `csv.QUOTE_MINIMAL`. This comes from the Python `csv` module, which is part of
    the Python installation. The `QUOTE_MINIMAL` argument only quotes fields that
    contain special characters, such as a comma (`","`).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_csv()` 函数有一个 `quoting` 参数，默认值为 `csv.QUOTE_MINIMAL`。这个默认值来自 Python 的 `csv`
    模块，它是 Python 安装的一部分。`QUOTE_MINIMAL` 参数只会为包含特殊字符的字段加上引号，例如逗号（`","`）。'
- en: 'The `csv` module provides four constants we can pass as arguments to the `quoting`
    parameter within the `to_csv()` function. These include the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`csv` 模块提供了四个常量，我们可以将它们作为参数传递给 `to_csv()` 函数中的 `quoting` 参数。这些常量包括以下内容：'
- en: '`csv.QUOTE_ALL`: Quotes all the fields, whether numeric or non-numeric'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`csv.QUOTE_ALL`：为所有字段加上引号，无论是数字型还是非数字型'
- en: '`csv.QUOTE_MINIMAL`: The default option in the `to_csv()` function, which quotes
    values that contain special characters'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`csv.QUOTE_MINIMAL`：`to_csv()`函数中的默认选项，用于引用包含特殊字符的值。'
- en: '`csv.QUOTE_NONNUMERIC`: Quotes all non-numeric fields'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`csv.QUOTE_NONNUMERIC`：引用所有非数字字段'
- en: '`csv.QUOTE_NONE`: To not quote any field'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`csv.QUOTE_NONE`：不引用任何字段'
- en: 'To better understand how these values can impact the output CSV, you will test
    passing different quoting arguments in the following example. This is done using
    the `person` DataFrame:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这些值如何影响输出的CSV，你将在以下示例中测试传递不同的引用参数。这是通过使用`person` DataFrame来完成的：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, if you open and inspect these files, you should see the following representations:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你打开并检查这些文件，你应该能看到以下表示：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that in the preceding example, when using `csv.QUOTE_NONE`, you must provide
    an additional argument for the `escapechar` parameter; otherwise, it will throw
    an error.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的示例中，使用`csv.QUOTE_NONE`时，你必须为`escapechar`参数提供额外的参数，否则会抛出错误。
- en: See also
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: 'For more information on the `Pandas.DataFrame.to_csv()` function, please refer
    to this page: [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欲了解更多关于`Pandas.DataFrame.to_csv()`函数的信息，请参考此页面：[https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)。
- en: 'For more information on the CSV module, please refer to this page: [https://docs.python.org/3/library/csv.html](https://docs.python.org/3/library/csv.html).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欲了解更多关于CSV模块的信息，请参考此页面：[https://docs.python.org/3/library/csv.html](https://docs.python.org/3/library/csv.html)。
- en: Writing data to an Excel file
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据写入Excel文件
- en: In this recipe, you will export a DataFrame as an Excel file format and leverage
    the different parameters available in the `DataFrame.to_excel()` writer function.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，你将把DataFrame导出为Excel文件格式，并利用`DataFrame.to_excel()`写入函数中可用的不同参数。
- en: Getting ready
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: In the *Reading data from an Excel file* recipe in *Chapter 2*, *Reading Time
    Series Data from Files*, you had to install `openpyxl` as the engine for reading
    Excel files with `pandas.read_excel()`. For this recipe, you will use the same
    `openpyxl` as the engine for writing Excel files with `DataFrame.to_excel()`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2章*的*从Excel文件读取数据*配方中，你需要安装`openpyxl`作为使用`pandas.read_excel()`读取Excel文件的引擎。在本配方中，你将使用相同的`openpyxl`作为使用`DataFrame.to_excel()`写入Excel文件的引擎。
- en: 'To install `openpyxl` using `conda`, run the following:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`conda`安装`openpyxl`，请运行以下命令：
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You can also use `pip`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用`pip`：
- en: '[PRE28]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The file is provided in the GitHub repository for this book, which you can
    find here: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook).
    The file is named `movieboxoffice.csv`.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件已提供在本书的GitHub仓库中，你可以在这里找到：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook)。文件名为`movieboxoffice.csv`。
- en: 'To prepare for this recipe, you will read the file into a DataFrame with the
    following code:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备这个配方，你将使用以下代码将文件读取到一个DataFrame中：
- en: '[PRE29]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How to do it…
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作…
- en: To write the DataFrame to an Excel file, you need to provide the writer function
    with `filename` and `sheet_name` parameters. The file name contains the file path
    and name. Make sure the file extension is `.xlsx` since you are using openpyxl.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要将DataFrame写入Excel文件，你需要提供包含`filename`和`sheet_name`参数的写入函数。文件名包含文件路径和名称。确保文件扩展名为`.xlsx`，因为你使用的是openpyxl。
- en: 'The `DataFrame.to_excel()` method will determine which engine to use based
    on the file extension, for example, `.xlsx` or `.xls`. You can also explicitly
    specify which engine to use with the `engine` parameter, as shown in the following
    code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame.to_excel()`方法将根据文件扩展名来决定使用哪个引擎，例如`.xlsx`或`.xls`。你也可以通过`engine`参数明确指定使用的引擎，示例如下：'
- en: 'Determine the location of file output and create the file path, desired sheet
    name, and engine to the `DataFrame.to_excel()` writer function:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定文件输出的位置，并将文件路径、所需的工作表名称以及引擎传递给`DataFrame.to_excel()`写入函数：
- en: '[PRE30]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The preceding code will create a new Excel file in the specified location.
    You can open and inspect the file, as shown in the following figure:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将在指定位置创建一个新的Excel文件。你可以打开并检查该文件，如下图所示：
- en: '![Figure 4.3: Example output from the daily_boxoffice.xlsx file](img/file34.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3：来自daily_boxoffice.xlsx文件的示例输出](img/file34.png)'
- en: 'Figure 4.3: Example output from the daily_boxoffice.xlsx file'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：来自daily_boxoffice.xlsx文件的示例输出
- en: Note that the sheet name is `movies_data`. In the Excel file, you will notice
    that the `Date` is not in the format you would expect. Let's say the expectation
    was for the `Date` column to be in a specific format, such as `MM-DD-YYYY`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，工作表名称是`movies_data`。在Excel文件中，你会注意到`Date`列的格式与预期的不符。假设预期`Date`列是一个特定格式，比如`MM-DD-YYYY`。
- en: Reading the same file using `read_excel` will read the `Date` column properly,
    as expected.
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用`read_excel`读取相同文件将正确读取`Date`列，符合预期。
- en: 'To achieve this, you will use another pandas-provided class, the `pandas.ExcelWriter`
    class gives us access to two properties for date formatting: `datetime_format`
    and `date_format`. These two parameters work nicely when using the `xlsxwriter`
    engine, but as of this writing, there is an open bug with the openpyxl integration.
    The openpyxl has several advantages over xlsxwriter, specifically for appending
    existing Excel files. We will utilize openpyxl `number_format` property for the
    Date cell to fix this issue. The following code shows how this can be achieved:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了实现这一点，你将使用另一个由pandas提供的类，`pandas.ExcelWriter`类为我们提供了两个用于日期格式化的属性：`datetime_format`和`date_format`。这两个参数在使用`xlsxwriter`引擎时效果很好，但截至目前，openpyxl集成存在一个已知的bug。openpyxl相较于xlsxwriter有几个优势，尤其是在追加现有Excel文件时。我们将利用openpyxl的`number_format`属性来修复这个问题。以下代码展示了如何实现这一点：
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following is a representation of what the new output would look like. This
    was accomplished by passing `MM-DD-YYYY` to the `datetime_format` property of
    the `writer` object:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是新输出的表现形式。这是通过将`MM-DD-YYYY`传递给`writer`对象的`datetime_format`属性实现的：
- en: '![Figure 4.4: Using pd.ExcelWriter and number_format to update the Date column
    format to MM-DD-YYYY](img/file35.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4：使用pd.ExcelWriter和number_format将Date列格式更新为MM-DD-YYYY](img/file35.png)'
- en: 'Figure 4.4: Using pd.ExcelWriter and number_format to update the Date column
    format to MM-DD-YYYY'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：使用pd.ExcelWriter和number_format将Date列格式更新为MM-DD-YYYY
- en: How it works…
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The `DataFrame.to_excel()` method by default creates a new Excel file if it
    doesn't exist or overwrites the file if it exists. To append to an existing Excel
    file or write to multiple sheets, you will need to use the `Pandas.ExcelWriter`
    class. The `ExcelWriter()` class has a `mode` parameter that can accept either
    `"w"` for write or `"a"` for append. As of this writing, xlsxwriter does not support
    the append mode, while the openpyxl supports both modes.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame.to_excel()`方法默认会创建一个新的Excel文件（如果文件不存在）或覆盖文件（如果文件已存在）。要向现有的Excel文件追加内容或写入多个工作表，你需要使用`Pandas.ExcelWriter`类。`ExcelWriter()`类有一个`mode`参数，可以接受`"w"`（写入）或`"a"`（追加）。截至目前，xlsxwriter不支持追加模式，而openpyxl支持两种模式。'
- en: Keep in mind that in `ExcelWriter` the default mode is set to `"w"` (write mode)
    and, thus, if `"a"` (append mode) is not specified, it will result in overwriting
    the Excel file (any existing content will be erased).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在`ExcelWriter`中，默认模式设置为`"w"`（写入模式），因此，如果未指定`"a"`（追加模式），将导致覆盖Excel文件（任何现有内容将被删除）。
- en: 'Additionally, when using append mode (`mode="a"`) you will need to specify
    how to handle existing sheets through the `if_sheet_exists` parameter, which accepts
    one of three values:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在使用追加模式（`mode="a"`）时，你需要通过`if_sheet_exists`参数指定如何处理现有的工作表，该参数接受以下三种值之一：
- en: '`error`, which raises a `ValueError` exception.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`error`，会引发`ValueError`异常。'
- en: '`replace`, which overwrites the existing worksheet.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replace`，它会覆盖现有的工作表。'
- en: '`new`, which creates a new worksheet with a new name. If you re-execute the
    preceding code and update `if_sheet_exists=''new''`, then a new sheet will be
    created and named `movies_fixed_dates1`.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new`，创建一个具有新名称的新工作表。如果重新执行前面的代码并更新`if_sheet_exists=''new''`，那么将创建一个新的工作表并命名为`movies_fixed_dates1`。'
- en: There's more…
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多……
- en: If you need to create multiple worksheets in the same Excel file, then `ExcelWriter`
    can be used to achieve this. For example, assume the goal is to split each month's
    data into its own sheet and name the sheet accordingly. In the following code,
    you will add a `Month` column and use that to split that DataFrame by month, using
    `groupby` to write each group into a new sheet.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要在同一个Excel文件中创建多个工作表，那么`ExcelWriter`可以帮助实现这一目标。例如，假设目标是将每个月的数据分开到自己的工作表中，并按月命名工作表。在下面的代码中，你将添加一个`Month`列并使用它按月拆分DataFrame，使用`groupby`将每个组写入一个新工作表。
- en: 'First, let’s create our helper function `sheet_date_format` to format our `Date`
    column in each sheet to MM-DD-YYYY format:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建辅助函数`sheet_date_format`，将每个工作表中的`Date`列格式化为MM-DD-YYYY格式：
- en: '[PRE32]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The next piece of code will add a Month column to the movies DataFrame, and
    then write each month to individual sheets and name each sheet with the corresponding
    month name:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的代码将向movies DataFrame添加一个“Month”列，然后将每个月的数据写入独立的工作表，并为每个工作表命名为相应的月份名称：
- en: '[PRE33]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code will create a new Excel file named `boxoffice_by_month.xlsx`
    with five sheets for each month, as shown in the following figure:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将创建一个名为`boxoffice_by_month.xlsx`的新Excel文件，并为每个月创建五个工作表，如下图所示：
- en: '![Figure 4.5: Each month in the movies DataFrame was written to its own sheet
    in Excel](img/file36.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5：movies DataFrame中的每个月都被写入到各自的Excel工作表中](img/file36.png)'
- en: 'Figure 4.5: Each month in the movies DataFrame was written to its own sheet
    in Excel'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5：movies DataFrame中的每个月都被写入到各自的Excel工作表中
- en: See also
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: The pandas `to_excel()` method and `ExcelWriter` class make writing DataFrames
    to an Excel file very convenient. If you want a more granular control outside
    of pandas DataFrames, you should consider exploring the `openpyxl` library you
    installed as the reader/writer engine. For example, the `openpyxl` library has
    a dataframe module (`openpyxl.utils.dataframe`) for working with pandas DataFrames.
    An example is the `dataframe_to_rows()` function.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: pandas的`to_excel()`方法和`ExcelWriter`类使将DataFrame写入Excel文件变得非常方便。如果您需要对pandas
    DataFrame以外的部分进行更精细的控制，您应该考虑使用已安装的`openpyxl`库作为读取/写入引擎。例如，`openpyxl`库有一个用于处理pandas
    DataFrame的模块（`openpyxl.utils.dataframe`）。一个例子是`dataframe_to_rows()`函数。
- en: To learn more about `Pandas.DataFrame.to_excel()`, please refer to [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于`Pandas.DataFrame.to_excel()`的信息，请参考[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html)。
- en: To learn more about `Pandas.ExcelWriter()`, please refer to [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.ExcelWriter.html#pandas.ExcelWriter.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.ExcelWriter.html#pandas.ExcelWriter.)
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于`Pandas.ExcelWriter()`的信息，请参考[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.ExcelWriter.html#pandas.ExcelWriter.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.ExcelWriter.html#pandas.ExcelWriter.)
- en: To learn more about `openpyxl`, please refer to [https://openpyxl.readthedocs.io/en/stable/index.html](ch005.xhtml).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于`openpyxl`的信息，请参考[https://openpyxl.readthedocs.io/en/stable/index.html](ch005.xhtml)。
- en: To learn more about `openpyxl.utils.dataframe`, please refer to [https://openpyxl.readthedocs.io/en/stable/pandas.html#working-with-pandas-dataframes](https://openpyxl.readthedocs.io/en/stable/pandas.html#working-with-pandas-dataframes)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于`openpyxl.utils.dataframe`的信息，请参考[https://openpyxl.readthedocs.io/en/stable/pandas.html#working-with-pandas-dataframes](https://openpyxl.readthedocs.io/en/stable/pandas.html#working-with-pandas-dataframes)
- en: Storing Data to a Cloud Storage (AWS, GCP, and Azure)
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据存储到云存储（AWS、GCP和Azure）
- en: In this recipe, you will use pandas to write to cloud storage such as Amazon
    S3, Google Cloud Storage, and Azure Blob Storage. Several of the pandas writer
    functions support writing to cloud storage through the `storage_options` parameter.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将使用pandas将数据写入云存储，如Amazon S3、Google Cloud Storage和Azure Blob Storage。多个pandas写入函数支持通过`storage_options`参数将数据写入云存储。
- en: Getting ready
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: In the Reading data from a URL recipe in *Chapter 2*, *Reading Time Series Data
    from Files*, you were instructed to install `boto3` and `s3fs` to read from AWS
    S3 buckets. In this recipe, you will be leveraging the same libraries in addition
    to the needed libraries for Google Cloud Storage (`gcsfs`) and Azure Blob Storage
    (`adlfs`).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第二章*的“从URL读取数据”中，您被要求安装`boto3`和`s3fs`来从AWS S3桶读取数据。在本教程中，除了需要的Google Cloud
    Storage（`gcsfs`）和Azure Blob Storage（`adlfs`）库外，您还将使用相同的库。
- en: 'To install using `pip`, you can use this:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pip`安装，您可以使用以下命令：
- en: '[PRE34]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To install using `conda`, you can use this:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`conda`安装，您可以使用以下命令：
- en: '[PRE35]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You will work with the `boxoffice_by_month.xlsx` file we created in the previous
    recipe, *Writing data to an Excel file*. The file is provided in the GitHub repository
    for this book, which you can find here: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用我们在前一个教程中创建的`boxoffice_by_month.xlsx`文件，*将数据写入Excel文件*。该文件可在本书的GitHub仓库中找到，链接如下：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook)。
- en: 'To prepare for this recipe, you will read the file into a DataFrame with the
    following code:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备这个操作，你将使用以下代码将文件读取到一个DataFrame中：
- en: '[PRE36]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Notice the movie DataFrame has two columns (Daily and Month) and a DatetimeIndex
    (Date).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，movie DataFrame有两列（Daily和Month），并且有一个DatetimeIndex（Date）。
- en: 'Next, you will store your AWS, Google Cloud, and Azure credentials in a config
    `cloud.cfg` file outside your Python script. Then, use `configparser` to read
    and store the values in Python variables. You do not want your credentials exposed
    or hard coded in your code:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将把AWS、Google Cloud和Azure的凭证存储在Python脚本外部的`cloud.cfg`配置文件中。然后，使用`configparser`读取并将值存储在Python变量中。你不希望将凭证暴露或硬编码在代码中：
- en: '[PRE37]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can then load an `aws.cfg` file using `config.read()`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`config.read()`加载`aws.cfg`文件：
- en: '[PRE38]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: How to do it…
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何实现…
- en: Several of the pandas writer functions support writing directly to a remote
    or cloud storage filesystem using, for example, AWS's `s3://`, Google's `gs://`,
    and Azure’s `abfs://` and `az://` protocols. These writer functions provide the
    `storage_options` parameter to support working with remote file storage systems.
    This is in part because pandas utilized `fsspec` to handle system non HTTP(s)
    URLs such as those specific for each cloud storage. For each cloud, you will need
    to use the specific filesystem implementation, for example, `s3fs` for AWS S3,
    `gcsfs` for Google Cloud, and `adlfs` for Azure.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 多个pandas写入函数支持直接写入远程或云存储文件系统，例如AWS的`s3://`、Google的`gs://`以及Azure的`abfs://`和`az://`协议。这些写入函数提供了`storage_options`参数，支持与远程文件存储系统的协作。这部分得益于pandas使用`fsspec`来处理非HTTP(s)的系统URL，例如每个云存储专用的URL。对于每个云存储，你需要使用特定的文件系统实现，例如，AWS
    S3使用`s3fs`，Google Cloud使用`gcsfs`，Azure使用`adlfs`。
- en: The `storage_options` parameter takes a Python dictionary to provide additional
    information such as credentials, tokens, or any information the cloud provider
    requires as a key-value pair.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`storage_options`参数接受一个Python字典，用于提供附加信息，如凭证、令牌或云提供商要求的任何信息，以键值对的形式提供。'
- en: Writing to Amazon S3 using pandas
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用pandas写入Amazon S3
- en: 'In this section, you will write the `movies` DataFrame to the `tscookbook-private`
    S3 bucket as CSV and Excel files using pandas:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将使用pandas将`movies` DataFrame写入`tscookbook-private` S3桶，保存为CSV和Excel文件：
- en: 'Several pandas writer functions such as `to_csv`, `to_parquet`, and `to_excel`
    allow you to pass AWS S3-specific credentials (`key` and `sercret`) as specified
    in s3fs through the `storage_accounts` parameter. The following code shows how
    you can utilize `to_csv` and `to_excel` to write your movies DataFrame to `tscookbook`
    S3 bucket as `movies_s3.csv` and `movies_s3.xlsx`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 若干pandas写入函数，如`to_csv`、`to_parquet`和`to_excel`，允许你通过`storage_accounts`参数传递AWS
    S3特定的凭证（`key`和`sercret`），这些凭证在`s3fs`中有说明。以下代码展示了如何利用`to_csv`和`to_excel`将你的movies
    DataFrame写入`tscookbook` S3桶，分别保存为`movies_s3.csv`和`movies_s3.xlsx`：
- en: '[PRE39]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The following figure shows the content of the `tscookbook-private` bucket:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了`tscookbook-private`桶的内容：
- en: '![Figure 4.6: movies_s3.csv and movie_s3.xlsx successfully written to AWS S3
    using pandas](img/file37.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6: 使用pandas成功写入AWS S3的movies_s3.csv和movie_s3.xlsx](img/file37.png)'
- en: 'Figure 4.6: movies_s3.csv and movie_s3.xlsx successfully written to AWS S3
    using pandas'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4.6: 使用pandas成功写入AWS S3的movies_s3.csv和movie_s3.xlsx'
- en: Writing to Google Cloud Storage using pandas
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用pandas写入Google Cloud Storage
- en: 'In this section, you will write the `movies` DataFrame to the `tscookbook`
    bucket on Google Cloud Storage as CSV and Excel files using pandas:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将使用pandas将`movies` DataFrame写入Google Cloud Storage的`tscookbook`桶，保存为CSV和Excel文件：
- en: 'When working with Google Cloud you will use a **service account private key**
    stored as a JSON file. This is generated and downloaded from Google Cloud. In
    `storage_options,` you will pass the file path. The following code shows how you
    can utilize `to_csv` and `to_excel` to write your movies DataFrame to `tscookbook`
    bucket as `movies_gs.csv` and `movies_gs.xlsx`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Google Cloud时，你将使用存储为JSON文件的**服务帐户私钥**。这个文件可以从Google Cloud生成并下载。在`storage_options`中，你将传递文件路径。以下代码展示了如何使用`to_csv`和`to_excel`将你的movies
    DataFrame写入`tscookbook`桶，分别保存为`movies_gs.csv`和`movies_gs.xlsx`：
- en: '[PRE40]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following figure shows the content of the `tscookbook` bucket:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了`tscookbook`桶的内容：
- en: '![Figure 4.7: movies_gs.csv and movie_gs.xlsx successfully written to Google
    Cloud Storage using pandas](img/file38.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7: 使用pandas成功写入Google Cloud Storage的movies_gs.csv和movie_gs.xlsx](img/file38.png)'
- en: 'Figure 4.7: movies_gs.csv and movie_gs.xlsx successfully written to Google
    Cloud Storage using pandas'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4.7: 使用pandas成功写入Google Cloud Storage的movies_gs.csv和movie_gs.xlsx'
- en: Writing to Azure Blob Storage using pandas
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 pandas 向 Azure Blob 存储写入数据
- en: 'In this section, you will write the `movies` DataFrame to Azure Blob Storage
    in a container named `objects` as a CSV file using pandas:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你将使用 pandas 将`movies` DataFrame 以 CSV 文件的形式写入 Azure Blob 存储中的名为`objects`的容器：
- en: 'When working with Azure Blob Storage, you use either the `abfs://` or `az://`
    protocols. In `storage_options` you will pass the account_key, this is your API
    Key for the Storage Account on Azure. The following code shows how you can utilize
    `to_csv` to write your movies DataFrame to `objects` container. The three code
    snippets below are equivalent and illustrate the different URI and `storage_options`
    you will need to pass:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Azure Blob 存储时，你可以使用`abfs://`或`az://`协议。在`storage_options`中，你将传递`account_key`，这是你在
    Azure 存储账户中的 API 密钥。以下代码展示了如何利用`to_csv`将你的 movies DataFrame 写入`objects`容器。下面的三段代码是等效的，并展示了你需要传递的不同
    URI 和`storage_options`：
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The following figure shows the content of the `objects` container:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了`objects`容器的内容：
- en: '![Figure 4.8: movies_abfs.csv, movies_az.csv, and movie_az2.csv successfully
    written to Azure Blob Storage using pandas](img/file39.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8：movies_abfs.csv、movies_az.csv 和 movie_az2.csv 成功写入 Azure Blob 存储，使用
    pandas](img/file39.png)'
- en: 'Figure 4.8: movies_abfs.csv, movies_az.csv, and movie_az2.csv successfully
    written to Azure Blob Storage using pandas'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：movies_abfs.csv、movies_az.csv 和 movie_az2.csv 成功写入 Azure Blob 存储，使用 pandas
- en: How it works…
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In the preceding code section, we used the `DataFrame.to_csv()` and `DataFrame.to_excel()`
    methods to write to Amazon S3, Azure Blob Storage, and Google Cloud Storage. The
    `storage_options` parameter allows passing a key-value pair containing the information
    required for the storage connection; for example, AWS S3 requires passing a `key`
    and a `secret`, GCP requires `token`, and Azure requires an `account_key`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码部分，我们使用了`DataFrame.to_csv()`和`DataFrame.to_excel()`方法，将数据写入 Amazon S3、Azure
    Blob 存储和 Google Cloud 存储。`storage_options`参数允许传递一个包含存储连接所需信息的键值对；例如，AWS S3 需要传递`key`和`secret`，GCP
    需要`token`，而 Azure 需要`account_key`。
- en: 'Examples of pandas DataFrame writer functions that support `storage_options`
    include:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 支持`storage_options`的 pandas DataFrame 写入函数示例包括：
- en: '`Pandas.DataFrame.to_excel()`'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pandas.DataFrame.to_excel()`'
- en: '`Pandas.DataFrame.to_json()`'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pandas.DataFrame.to_json()`'
- en: '`Pandas.DataFrame.to_parquet()`'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pandas.DataFrame.to_parquet()`'
- en: '`Pandas.DataFrame.to_pickle()`'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pandas.DataFrame.to_pickle()`'
- en: '`Pandas.DataFrame.to_markdown()`'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pandas.DataFrame.to_markdown()`'
- en: '`Pandas.DataFrame.to_pickle()`'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pandas.DataFrame.to_pickle()`'
- en: '`Pandas.DataFrame.to_stata()`'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pandas.DataFrame.to_stata()`'
- en: '`Pandas.DataFrame.to_xml()`'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pandas.DataFrame.to_xml()`'
- en: There's more…
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多……
- en: For more granular control, you can use the specific Python SDK for AWS (`boto3`),
    Google Cloud (`google-cloud-storage`), or Azure (`azure-storage-blob`) to write
    your data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更细粒度的控制，你可以使用 AWS（`boto3`）、Google Cloud（`google-cloud-storage`）或 Azure（`azure-storage-blob`）的特定
    Python SDK 来写入数据。
- en: First, let’s store our movie DataFrame in CSV format for uploading the data
    to the various cloud storage services.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将把我们的电影 DataFrame 存储为 CSV 格式，以便将数据上传到不同的云存储服务。
- en: '[PRE43]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that the `index=True` because our Date column is an index, and we need
    to ensure it is included as a column when written as a CSV file.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`index=True`是因为我们的日期列是索引，我们需要确保它在写入 CSV 文件时作为列被包含。
- en: Writing to Amazon S3 using boto3 library
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 boto3 库向 Amazon S3 写入数据
- en: You will explore both the **Resource API** and **Client API.** The Resource
    API is a higher-level abstraction that simplifies the code and interactions with
    AWS services. At the same time, the Client API provides a low-level abstraction,
    allowing for more granular control over AWS services.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你将探索**资源 API**和**客户端 API**。资源 API 是一个更高级的抽象，它简化了代码并与 AWS 服务的交互。与此同时，客户端 API
    提供了一个低级别的抽象，允许对 AWS 服务进行更细粒度的控制。
- en: When using the resource API with `boto3.resource("s3"),` you will first need
    create an Object resource using the Object method by supplying the S3 bucket name
    and an Object key (file name). Once defined, you will have access to several methods,
    including `copy`, `delete`, `put`, `download_file`, `load`, `get`, and `upload,`
    to name a few. The `put` method will add an object to the S3 bucket defined.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用资源 API 与`boto3.resource("s3")`时，你首先需要通过提供 S3 桶名称和对象键（文件名）来创建一个对象资源。一旦定义，你将可以访问多个方法，包括`copy`、`delete`、`put`、`download_file`、`load`、`get`和`upload`等。`put`方法将把一个对象添加到定义的
    S3 桶中。
- en: When using the client API with `boto3.client("s3"),` you have access to many
    methods at the Bucket and Object levels, including `create_bucket`, `delete_bucket`,
    `download_file`, `put_object`, `delete_object`, `get_bucket_llifecycle`, `get_bucket_location`,
    `list_buckets` and much more. The `put_object` method will add an object to the
    S3 bucket defined.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`boto3.client("s3")`客户端API时，你可以访问许多Bucket和Object级别的方法，包括`create_bucket`、`delete_bucket`、`download_file`、`put_object`、`delete_object`、`get_bucket_lifecycle`、`get_bucket_location`、`list_buckets`等。`put_object`方法将把一个对象添加到定义的S3存储桶中。
- en: '[PRE44]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Writing to Google Cloud Storage using google-cloud-storage library
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用google-cloud-storage库写入Google Cloud Storage
- en: You will first need to create a client object, an instance of the Client class
    from the storage module. You will authenticate using the service account JSON
    key file. This is specified using the `from_service_account_json` method. You
    will use the `bucket` and `blob` methods to create a reference to the blob object
    you want to place in the `tscookbook` bucket in Google Storage. Finally, you can
    upload the data using the `upload_from_string` method into the blob object specified.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先需要创建一个客户端对象，这是来自存储模块的Client类的一个实例。你将使用服务账户的JSON密钥文件进行身份验证。通过`from_service_account_json`方法指定该文件。你将使用`bucket`和`blob`方法创建一个引用，指向你希望放入Google
    Storage中`tscookbook`存储桶的blob对象。最后，你可以使用`upload_from_string`方法将数据上传到指定的blob对象中。
- en: '[PRE45]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Writing to Azure Blob Storage using azure-storage-blob library
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用azure-storage-blob库写入Azure Blob Storage
- en: You will start by creating a `BlobServiceClient` object and authenticate using
    your Azure Storage Account API key. You then create the blob object for the specified
    container using `get_blob_client` and upload that data into the specified object
    using the `upload_blob` method.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 你将首先创建一个`BlobServiceClient`对象，并使用Azure Storage Account API密钥进行身份验证。然后，你将使用`get_blob_client`为指定的容器创建blob对象，并使用`upload_blob`方法将数据上传到指定的对象中。
- en: '[PRE46]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: See also
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: To learn more about managing cloud storage with Python, explore the official
    documentation for these popular libraries
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多关于如何使用Python管理云存储的信息，请查看这些流行库的官方文档
- en: '**Amazon S3 (Boto3)** [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon S3 (Boto3)** [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html)'
- en: '**Azure Blob Storage** [https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Blob Storage** [https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python)'
- en: '**Google Cloud Storage** [https://cloud.google.com/python/docs/reference/storage/latest](https://cloud.google.com/python/docs/reference/storage/latest)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Cloud Storage** [https://cloud.google.com/python/docs/reference/storage/latest](https://cloud.google.com/python/docs/reference/storage/latest)'
- en: Writing Large Datasets
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写入大数据集
- en: In this recipe, you will explore how the choice of the different file formats
    can impact the overall write and read performance. You will explore Parquet, Optimized
    Row Columnar (ORC), and Feather and compare their performance to other popular
    file formats such as JSON and CSV.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，你将探索不同文件格式的选择如何影响整体的写入和读取性能。你将探索Parquet、优化行列式（ORC）和Feather，并将它们的性能与其他流行的文件格式，如JSON和CSV，进行比较。
- en: The three file formats, ORC, Feather, and Parquet, are columnar file formats,
    making them efficient for analytical needs, and showing improved querying performance
    overall. The three file formats are also supported in Apache Arrow (PyArrow),
    which offers an in-memory columnar format for optimized data analysis performance.
    To persist this in-memory columnar and store it, you can use pandas `to_orc`,
    `to_feather`, and `to_parquet` writer functions to persist your data to disk.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种文件格式ORC、Feather和Parquet是列式文件格式，适用于分析需求，并且总体上显示出更好的查询性能。这三种文件格式也得到了Apache
    Arrow（PyArrow）的支持，后者提供了内存中的列式格式，优化了数据分析性能。为了将这种内存中的列式数据持久化并存储，你可以使用pandas的`to_orc`、`to_feather`和`to_parquet`写入函数将数据持久化到磁盘。
- en: Arrow provides the in-memory representation of the data as a columnar format
    while Feather, ORC, and Parquet allows us to store this representation to disk.
  id: totrans-254
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Arrow提供数据的内存表示，采用列式格式，而Feather、ORC和Parquet则允许我们将这种表示存储到磁盘中。
- en: Getting Ready
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, you will be working with the New York Taxi data set from ([https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page))
    and we will be working with Yellow Taxi Trip Records for 2023.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，您将使用来自([https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page))的纽约出租车数据集，我们将处理2023年的黄出租车行程记录。
- en: In the following examples, we will be using one of these files, `yellow_tripdata_2023-01.parquet`,
    but you can select any other file to follow along. In the recipe *Reading data
    from Parquet files* from *Chapter 2*, you installed **PyArrow**. Below are the
    instructions for installing PyArrow using either Conda or Pip.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将使用这些文件之一，`yellow_tripdata_2023-01.parquet`，但您可以选择其他任何文件来跟随学习。在*第二章*的*从Parquet文件读取数据*示例中，您安装了**PyArrow**。以下是使用Conda或Pip安装PyArrow的说明。
- en: 'To install PyArrow using `conda,` run the following command:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`conda`安装PyArrow，运行以下命令：
- en: '[PRE47]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To install PyArrow using `pip` run the following command:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`pip`安装PyArrow，运行以下命令：
- en: '[PRE48]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'To prepare for this recipe, you will read the file into a DataFrame with the
    following code:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备本示例，您将使用以下代码将文件读取到DataFrame中：
- en: '[PRE49]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: How to do it
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何实现
- en: You will write the DataFrame into different file formats and then compare the
    output in terms of compression efficiency (file size), write, and read speed.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 您将把DataFrame写入不同的文件格式，并随后比较压缩效率（文件大小）、写入和读取速度。
- en: 'To accomplish this, you will need to create a function that returns the file
    size:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，您需要创建一个返回文件大小的函数：
- en: '[PRE50]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The function will take the file you created and return the size in megabytes.
    The `os.path.getsize()` will return the size in bytes and the line `size_bytes
    / (1024**2)` will convert it into megabytes.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将获取您创建的文件并返回文件大小（单位：MB）。`os.path.getsize()`将返回文件大小（单位：字节），而`size_bytes /
    (1024**2)`这一行将其转换为兆字节（MB）。
- en: We will be writing these files into a `formats` folder so later we can read
    from that folder to evaluate read performance.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这些文件写入`formats`文件夹，以便稍后可以从该文件夹读取以评估读取性能。
- en: Writing as JSON and CSV
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 写入为JSON和CSV
- en: 'You will use the `DataFrame.to_json()` method to write a `yellow_tripdata.json`
    file:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用`DataFrame.to_json()`方法写入一个`yellow_tripdata.json`文件：
- en: '[PRE51]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Note the file size is around 1.16 GB and took around 5.24 seconds.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，文件大小约为1.16 GB，耗时约为5.24秒。
- en: 'You will use the `DataFrame.to_csv()` method to write a `yellow_tripdata.csv`
    file:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用`DataFrame.to_csv()`方法写入一个`yellow_tripdata.csv`文件：
- en: '[PRE52]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Note the file size is around 307 MB and took around 17.1 seconds,
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，文件大小约为307 MB，耗时约为17.1秒。
- en: Writing as Parquet
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 写入为Parquet
- en: 'The `to_parquet` writer function supports several compression algorithms, including
    `snappy`, `GZIP`, `brotli`, `LZ4`, `ZSTD`. You will use the `DataFrame.to_parquet()`
    method to write three files to compare `snappy`, `LZ4`, and `ZSTD` compression
    algorithms:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_parquet`写入函数支持多种压缩算法，包括`snappy`、`GZIP`、`brotli`、`LZ4`、`ZSTD`。您将使用`DataFrame.to_parquet()`方法写入三个文件，以比较`snappy`、`LZ4`和`ZSTD`压缩算法：'
- en: '[PRE53]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Notice that the three compression algorithms produce similar compression results
    (file size) and speed.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，三种压缩算法产生相似的压缩结果（文件大小）和速度。
- en: Writing as Feather
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 写入为Feather
- en: 'You will use the `DataFrame.to_feather()` method to write three feather files
    using the two supported compression algorithms `LZ4` and `ZSTD.` The last file
    format will be the uncompressed format for comparison:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用`DataFrame.to_feather()`方法，使用两个支持的压缩算法`LZ4`和`ZSTD`写入三个feather文件。最后一个文件格式将是未压缩的格式，以便进行比较：
- en: '[PRE54]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Notice the difference in file size between uncompressed, using LZ4 and ZSTD
    compression algorithms. You can further explore the `compression_level` to find
    the optimal output. Overall, LZ4 offers great performance on write and read (*compression*
    and *decompression* speed). The ZSTD algorithm may offer a higher compression
    ratio, resulting in much smaller files, but it may not be faster than LZ4.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意未压缩文件、使用LZ4和ZSTD压缩算法之间的文件大小差异。您可以进一步探索`compression_level`来找到最佳输出。总体而言，LZ4在写入和读取（*压缩*和*解压缩*速度）上提供了出色的性能。ZSTD算法可能提供更高的压缩比，生成更小的文件，但其速度可能不如LZ4。
- en: Writing as ORC
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 写入为ORC
- en: 'Similar to the Feather and Parquet file formats, ORC supports different compression
    algorithms, including uncompressed, `snappy`, `ZLIB`, `LZ4`, and `ZSTD`. You will
    use the `DataFrame.to_orc()` method to write three ORC files to explore ZSTD and
    LZ4 compression algorithms and an uncompressed file for comparison:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Feather 和 Parquet 文件格式，ORC 支持不同的压缩算法，包括无压缩、`snappy`、`ZLIB`、`LZ4` 和 `ZSTD`。你将使用
    `DataFrame.to_orc()` 方法写入三个 ORC 文件，以探索 ZSTD 和 LZ4 压缩算法，并将无压缩文件作为对比：
- en: '[PRE55]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Notice that the LZ4 algorithm did not offer better compression when compared
    with the uncompressed version. The ZSTD algorithm did offer better compression
    but took a bit longer to execute.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，LZ4 算法在与无压缩版本进行比较时并未提供更好的压缩效果。ZSTD 算法确实提供了更好的压缩效果，但执行时间稍长。
- en: How it works…
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它的工作原理…
- en: Often, when working with large datasets that need to persist into disk after
    completing your transformations, deciding which file format to opt for can significantly
    impact your overall data storage strategy.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在处理需要完成转换后持久化到磁盘的大型数据集时，决定选择哪种文件格式会显著影响整体的数据存储策略。
- en: For example, JSON and CSV formats are human-readable choices, and pretty much
    any commercial or open-source data visualization or analysis tools can handle
    such formats. Both CSV and JSON formats do not offer compression for large file
    sizes and can lead to poor performance on both write and read operations. On the
    other hand, Parquet, Feather, and ORC are binary file formats (not human-readable)
    but support several compression algorithms and are columnar-based, which are optimized
    for analytical applications with fast read performance.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，JSON 和 CSV 格式是人类可读的格式，几乎任何商业或开源的数据可视化或分析工具都可以处理这些格式。CSV 和 JSON 格式不支持大文件的压缩，会导致写入和读取操作的性能较差。另一方面，Parquet、Feather
    和 ORC 是二进制文件格式（不可读），但支持多种压缩算法，并且是基于列的，这使得它们非常适合用于分析应用程序，具有快速的读取性能。
- en: The pandas library supports Parquet, Feather, and ORC thanks to PyArrow, a Python
    wrapper to Apache Arrow.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 库通过 PyArrow 支持 Parquet、Feather 和 ORC，PyArrow 是 Apache Arrow 的 Python
    封装。
- en: There’s more…
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: You have evaluated different file formats' write performance (and size). Next,
    you will compare read time performance and the efficiency of the various file
    formats and compression algorithms.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经评估了不同文件格式的写入性能（及大小）。接下来，你将比较读取时间性能以及各种文件格式和压缩算法的效率。
- en: To do this, you will create a function (`measure_read_performance`) that reads
    all files in a specified folder (for example, the `formats` folder). The function
    will evaluate each file extension (for instance, *.feather*, *.orc*, *.json*,
    .*csv*, *.parquet*) to determine which pandas read function to use. The function
    will then capture the performance time for each file format, append the results,
    and return a DataFrame containing all results sorted by read time.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，你将创建一个函数（`measure_read_performance`），该函数会读取指定文件夹中的所有文件（例如，`formats` 文件夹）。该函数将评估每个文件扩展名（例如，*.feather*、*.orc*、*.json*、*.csv*、*.parquet*），以确定应使用哪种
    pandas 读取函数。然后，该函数会捕获每个文件格式的性能时间，附加结果，并返回一个按读取时间排序的包含所有结果的 DataFrame。
- en: '[PRE56]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'You can execute the function by specifying the folder, in this case, the `formats`
    folder, to display the final results:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过指定文件夹（例如，`formats` 文件夹）来执行该函数，以显示最终结果：
- en: '[PRE57]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Overall, the results for read performance indicate that Parquet file formats
    perform the best, followed by Feather, then ORC. The time `read_time` is measured
    in seconds.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，读取性能的结果表明，Parquet 文件格式表现最佳，其次是 Feather，然后是 ORC。时间 `read_time` 以秒为单位。
- en: See also
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: To learn more about file formats for efficient data storage with pandas
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多有关高效数据存储的文件格式，可以参考 pandas。
- en: '**Parquet**'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Parquet**'
- en: 'pandas documentation: [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html)'
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas 文档：[https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html)
- en: '**Feather**'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Feather**'
- en: 'pandas documentation: [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_feather.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_feather.html)'
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas 文档：[https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_feather.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_feather.html)
- en: 'Additional arguments in Arrow documentation: [https://arrow.apache.org/docs/python/generated/pyarrow.feather.write_feather.html](https://arrow.apache.org/docs/python/generated/pyarrow.feather.write_feather.html)'
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arrow 文档中的附加参数: [https://arrow.apache.org/docs/python/generated/pyarrow.feather.write_feather.html](https://arrow.apache.org/docs/python/generated/pyarrow.feather.write_feather.html)'
- en: '**ORC**'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ORC**'
- en: 'pandas documentation: [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_orc.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_orc.html)'
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'pandas 文档: [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_orc.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_orc.html)'
- en: 'Additional arguments in Arrow documentation: [https://arrow.apache.org/docs/python/generated/pyarrow.orc.write_table.html](https://arrow.apache.org/docs/python/generated/pyarrow.orc.write_table.html)'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arrow 文档中的附加参数: [https://arrow.apache.org/docs/python/generated/pyarrow.orc.write_table.html](https://arrow.apache.org/docs/python/generated/pyarrow.orc.write_table.html)'
- en: '**Apache Arrow**: [https://arrow.apache.org/overview/](https://arrow.apache.org/overview/)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Arrow**: [https://arrow.apache.org/overview/](https://arrow.apache.org/overview/)'
