- en: Chapter 2. Spark Programming Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 Spark编程模型
- en: '**Extract**, **Transform**, and **Load** (**ETL**) tools proliferated along
    with the growth of the data in organizations. The need to move data from one source
    to one or more destinations, processing it on the fly before it reaches its destination,
    were all the requirements of the time. Most of the time, these ETL tools were
    supporting only a few types of data, only a few types of data sources and destinations,
    and were closed to extension to allow them to support new data types and new sources
    and destinations. Because of these stringent limitations on the tools, sometimes
    even a one-step transformation process had to be done in multiple steps. These
    convoluted approaches mandated the need to have unnecessary wastage in terms of
    manpower, as well as other computing resources. The main argument from the commercial
    ETL vendors all the time remained the same, one size doesn''t fit all. So use
    *our* suite of tools instead of the point products available on the market. Many
    organizations got into vendor lock-in because of the profuse need to process data.
    Almost all the tools introduced before the year 2005 did not make use of the real
    power of the multi-core architecture of the computers if they supported running
    their tools on the commodity hardware. So, simple but voluminous data processing
    jobs took hours and sometimes even days to complete with these tools.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**提取**、**转换**和**加载**（**ETL**）工具随着组织中数据的增长，大量涌现。将数据从一个源移动到一个或多个目的地，并在到达目的地之前对其进行实时处理，这些都是当时的需求。大多数情况下，这些ETL工具仅支持少数类型的数据，少数类型的数据源和目的地，并且对扩展以支持新数据类型和新源和目的地持封闭态度。由于这些工具的严格限制，有时甚至一个步骤的转换过程也必须分多个步骤完成。这些复杂的方法要求在人力以及其他计算资源方面产生不必要的浪费。商业ETL供应商的主要论点始终如一，那就是“一刀切”并不适用。因此，请使用*我们*的工具套件，而不是市场上可用的单一产品。许多组织因处理数据的迫切需求而陷入供应商锁定。几乎所有在2005年之前推出的工具，如果它们支持在商品硬件上运行，都没有充分利用计算机多核架构的真正力量。因此，简单但大量的数据处理任务使用这些工具需要数小时甚至数天才能完成。'
- en: Spark became an instant hit in the market because of its ability to process
    a huge amount of data types and a growing number of data sources and data destinations.
    The most important and basic data abstraction Spark provides is the **resilient
    distributed dataset** (**RDD**). As discussed in the previous chapter, Spark supports
    distributed processing on a cluster of nodes. The moment there is a cluster of
    nodes, there is a good chance that when the data processing is going on, some
    of the nodes can die. When such failures happen, the framework should be capable
    of coming out of such failures. Spark is designed to do that and that is what
    the *resilient* part in the RDD signifies. If there is a huge amount of data to
    be processed and there are nodes available in the cluster, the framework should
    have the capability to split the big dataset into smaller chunks and distribute
    them to be processed on more than one node in a cluster, in parallel. Spark is
    capable of doing that and that is what the *distributed* part in the RDD signifies.
    In other words, Spark is designed from the ground up to have its basic dataset
    abstraction capable of getting split into smaller pieces deterministically and
    distributed to more than one node in the cluster for parallel processing, while
    elegantly handling the failures in the nodes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Spark因其处理大量数据类型以及不断增长的数据源和数据目的地的能力而在市场上迅速走红。Spark提供的最重要的基本数据抽象是**弹性分布式数据集**（**RDD**）。如前一章所述，Spark支持在节点集群上进行分布式处理。一旦有了节点集群，在数据处理过程中，某些节点可能会死亡。当此类故障发生时，框架应能够从中恢复。Spark的设计就是为了做到这一点，这就是RDD中*弹性*部分的含义。如果有大量数据要处理，并且集群中有可用节点，框架应具备将大数据集分割成小块并在集群中多个节点上并行处理的能力。Spark能够做到这一点，这就是RDD中*分布式*部分的含义。换句话说，Spark从一开始就设计其基本数据集抽象能够确定性地分割成小块，并在集群中多个节点上并行处理，同时优雅地处理节点故障。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Functional programming with Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行函数式编程
- en: Spark RDD
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark RDD
- en: Data transformations and actions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转换与操作
- en: Spark monitoring
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark监控
- en: Spark programming basics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark编程基础
- en: Creating RDDs from files
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文件创建RDD
- en: Spark libraries
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark库
- en: Functional programming with Spark
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行函数式编程
- en: The mutation of objects at run time, and the inability to get consistent results
    from a program or function because of the side effect that the program logic creates
    makes many applications very complex. If the functions in programming languages
    start behaving exactly like mathematical functions in such a way that the output
    of the function depends only on the inputs, that gives lots of predictability
    to applications. The computer programming paradigm giving lots of emphasis to
    the process of building such functions and other elements based on that, and using
    those functions just in the way that any other data types are being used, is popularly
    known as the functional programming paradigm. Out of the JVM-based programming
    languages, Scala is one of the most important ones that has very strong functional
    programming capabilities without losing any object orientation. Spark is written
    predominantly in Scala. Because of that itself, Spark has taken lots of very good
    concepts from Scala.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时对象的变异，以及由于程序逻辑产生的副作用而无法从程序或函数中获得一致的结果，使得许多应用程序变得非常复杂。如果编程语言中的函数开始表现得完全像数学函数一样，即函数的输出仅依赖于输入，那么这为应用程序提供了大量的可预测性。计算机编程范式强调构建这种函数和其他元素的过程，并使用这些函数就像使用任何其他数据类型一样，这种范式被称为函数式编程范式。在基于JVM的编程语言中，Scala是最重要的语言之一，它具有非常强大的函数式编程能力，同时不失面向对象的特性。Spark主要使用Scala编写。正因为如此，Spark从Scala中借鉴了许多优秀的概念。
- en: Understanding Spark RDD
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Spark RDD
- en: The most important feature that Spark took from Scala is the ability to use
    functions as parameters to the Spark transformations and Spark actions. Quite
    often, the RDD in Spark behaves just like a collection object in Scala. Because
    of that, some of the data transformation method names of Scala collections are
    used in Spark RDD to do the same thing. This is a very neat approach and those
    who have expertise in Scala will find it very easy to program with RDDs. We will
    see a few important features in the following sections.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Spark从Scala中借鉴的最重要特性是能够将函数作为参数传递给Spark转换和Spark操作。通常情况下，Spark中的RDD表现得就像Scala中的集合对象一样。因此，Scala集合中的一些数据转换方法名称在Spark
    RDD中被用来执行相同的操作。这是一种非常简洁的方法，熟悉Scala的开发者会发现使用RDD编程非常容易。我们将在后续章节中看到一些重要的特性。
- en: Spark RDD is immutable
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark RDD是不可变的
- en: There are some strong rules based on which an RDD is created. Once an RDD is
    created, intentionally or unintentionally, it cannot be changed. This gives another
    insight into the construction of an RDD. Because of that, when the nodes processing
    some part of an RDD die, the driver program can recreate those parts and assign
    the task of processing it to another node, ultimately, completing the data processing
    job successfully.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 创建RDD有一些严格的规则。一旦RDD被创建，无论是故意还是无意，它都不能被更改。这为我们理解RDD的构造提供了另一个视角。正因为如此，当处理RDD某部分的节点崩溃时，驱动程序可以重新创建这些部分，并将处理任务分配给另一个节点，最终成功完成数据处理工作。
- en: Since the RDD is immutable, splitting a big one to smaller ones, distributing
    them to various worker nodes for processing, and finally compiling the results
    to produce the final result can be done safely without worrying about the underlying
    data getting changed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RDD是不可变的，因此可以将大型RDD分割成小块，分发到各个工作节点进行处理，并最终编译结果以生成最终结果，而无需担心底层数据被更改。
- en: Spark RDD is distributable
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark RDD是可分布的
- en: If Spark is run in a cluster mode where there are multiple worker nodes available
    to take the tasks, all these nodes will have different execution contexts. The
    individual tasks are distributed and run on different JVMs. All these activities
    of a big RDD getting divided into smaller chunks, getting distributed for processing
    to the worker nodes, and finally, assembling the results back, are completely
    hidden from the users.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Spark在集群模式下运行，其中有多台工作节点可以接收任务，所有这些节点将具有不同的执行上下文。各个任务被分发并在不同的JVM上运行。所有这些活动，如大型RDD被分割成小块，被分发到工作节点进行处理，最后将结果重新组装，对用户是完全隐藏的。
- en: Spark has its own mechanism for recovering from the system faults and other
    forms of errors which occur during the data processing and hence this data abstraction
    is highly resilient.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark拥有自己的机制来从系统故障和其他数据处理过程中发生的错误中恢复，因此这种数据抽象具有极高的弹性。
- en: Spark RDD lives in memory
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark RDD存储在内存中
- en: Spark does keep all the RDDs in the memory as much as it can. Only in rare situations,
    where Spark is running out of memory or if the data size is growing beyond the
    capacity, is it written to disk. Most of the processing on RDD happens in the
    memory, and that is the reason why Spark is able to process the data at a lightning
    fast speed.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Spark尽可能将所有RDD保留在内存中。仅在极少数情况下，如Spark内存不足或数据量增长超出容量时，数据才会被写入磁盘。RDD的大部分处理都在内存中进行，这也是Spark能够以极快速度处理数据的原因。
- en: Spark RDD is strongly typed
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark RDD是强类型的
- en: Spark RDD can be created using any supported data types. These data types can
    be Scala/Java supported intrinsic data types or custom created data types such
    as your own classes. The biggest advantage coming out of this design decision
    is the freedom from runtime errors. If it is going to break because of a data
    type issue, it will break during compile time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Spark RDD可以使用任何受支持的数据类型创建。这些数据类型可以是Scala/Java支持的固有数据类型，也可以是自定义创建的数据类型，如您自己的类。这一设计决策带来的最大优势是避免了运行时错误。如果因数据类型问题导致程序崩溃，它将在编译时崩溃。
- en: 'The following table captures the structure of an RDD containing tuples of a
    retail bank account data. It is of the type RDD[(string, string, string, double)]:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下表描述了包含零售银行账户数据元组的RDD结构。其类型为RDD[(string, string, string, double)]：
- en: '| **AccountNo** | **FirstName** | **LastName** | **AccountBalance** |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **账户号** | **名字** | **姓氏** | **账户余额** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SB001 | John | Mathew | 250.00 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| SB001 | John | Mathew | 250.00 |'
- en: '| SB002 | Tracy | Mason | 450.00 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| SB002 | Tracy | Mason | 450.00 |'
- en: '| SB003 | Paul | Thomson | 560.00 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| SB003 | Paul | Thomson | 560.00 |'
- en: '| SB004 | Samantha | Grisham | 650.00 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| SB004 | Samantha | Grisham | 650.00 |'
- en: '| SB005 | John | Grove | 1000.00 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| SB005 | John | Grove | 1000.00 |'
- en: 'Suppose this RDD is going through a process to calculate the total amount of
    all these accounts in a cluster of three nodes, N1, N2, and N3; it can be split
    and distributed for something such as parallelizing the data processing. The following
    table contains the elements of the RDD[(string, string, string, double)] distributed
    to node N1 for processing:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设此RDD正在一个包含三个节点N1、N2和N3的集群中进行处理，以计算所有这些账户的总金额；它可以被分割并分配，例如用于并行数据处理。下表包含了分配给节点N1进行处理的RDD[(string,
    string, string, double)]的元素：
- en: '| **AccountNo** | **FirstName** | **LastName** | **AccountBalance** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **账户号** | **名字** | **姓氏** | **账户余额** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SB001 | John | Mathew | 250.00 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| SB001 | John | Mathew | 250.00 |'
- en: '| SB002 | Tracy | Mason | 450.00 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| SB002 | Tracy | Mason | 450.00 |'
- en: 'The following table contains the elements of the RDD[(string, string, string,
    double)] distributed to node N2 for processing:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下表包含了分配给节点N2进行处理的RDD[(string, string, string, double)]的元素：
- en: '| **AccountNo** | **FirstName** | **LastName** | **AccountBalance** |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **账户号** | **名字** | **姓氏** | **账户余额** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SB003 | Paul | Thomson | 560.00 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| SB003 | Paul | Thomson | 560.00 |'
- en: '| SB004 | Samantha | Grisham | 650.00 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| SB004 | Samantha | Grisham | 650.00 |'
- en: '| SB005 | John | Grove | 1000.00 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| SB005 | John | Grove | 1000.00 |'
- en: On node N1, the summation process happens and the result is returned to the
    Spark driver program. Similarly, on node N2, the summation process happens, the
    result is returned to the Spark driver program, and the final result is computed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在节点N1上，求和过程发生并将结果返回给Spark驱动程序。同样，在节点N2上，求和过程发生，结果返回给Spark驱动程序，并计算最终结果。
- en: Spark has very deterministic rules on splitting a big RDD into smaller chunks
    for distribution to various nodes and because of that, even if something happens
    to, say, node N1, Spark knows how to recreate exactly the chunk that was lost
    in the node N1 and continue with the data processing operation by sending the
    same payload to node N3.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在将大型RDD分割成小块并分配给各个节点方面有非常确定的规则，因此，即使某个节点如N1出现问题，Spark也知道如何精确地重新创建丢失的块，并通过将相同的负载发送到节点N3来继续数据处理操作。
- en: 'Figure 1 captures the essence of the process:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图1捕捉了该过程的本质：
- en: '![Spark RDD is strongly typed](img/image_02_002.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Spark RDD是强类型的](img/image_02_002.jpg)'
- en: Figure 1
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: Tip
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Spark does a lot of processing in its driver memory and in the executor memory
    on the cluster nodes. Spark has various parameters that can be configured and
    fine-tuned so that the required resources are made available before the processing
    starts.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在其驱动内存和集群节点的执行器内存中进行大量处理。Spark有多种可配置和微调的参数，以确保在处理开始前所需的资源已就绪。
- en: Data transformations and actions with RDDs
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RDD进行数据转换和操作
- en: 'Spark does the data processing using the RDDs. From the relevant data source
    such as text files and NoSQL data stores, data is read to form the RDDs. On such
    an RDD, various data transformations are performed and finally, the result is
    collected. To be precise, Spark comes with Spark transformations and Spark actions
    that act upon RDDs. Let us take the following RDD capturing a list of retail banking
    transactions, which is of the type RDD[(string, string, double)]:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Spark使用RDD进行数据处理。从相关的数据源（如文本文件和NoSQL数据存储）读取数据以形成RDD。对这样的RDD执行各种数据转换，并最终收集结果。确切地说，Spark提供了作用于RDD的Spark转换和Spark动作。让我们以捕获零售银行业务交易列表的以下RDD为例，其类型为RDD[(string,
    string, double)]：
- en: '| **AccountNo** | **TranNo** | **TranAmount** |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **账户号** | **交易号** | **交易金额** |'
- en: '| --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| SB001 | TR001 | 250.00 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| SB001 | TR001 | 250.00 |'
- en: '| SB002 | TR004 | 450.00 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| SB002 | TR004 | 450.00 |'
- en: '| SB003 | TR010 | 120.00 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| SB003 | TR010 | 120.00 |'
- en: '| SB001 | TR012 | -120.00 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| SB001 | TR012 | -120.00 |'
- en: '| SB001 | TR015 | -10.00 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| SB001 | TR015 | -10.00 |'
- en: '| SB003 | TR020 | 100.00 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| SB003 | TR020 | 100.00 |'
- en: 'To calculate the account level summary of the transactions from the RDD of
    the form `(AccountNo,TranNo,TranAmount)`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式为`(AccountNo,TranNo,TranAmount)`的RDD计算账户级交易摘要：
- en: First it has to be transformed to the form of key-value pairs `(AccountNo,TranAmount)`,
    where `AccountNo` is the key but there will be multiple elements with the same
    key.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先需要将其转换为键值对形式`(AccountNo,TranAmount)`，其中`AccountNo`是键，但会有多个具有相同键的元素。
- en: On this key, do a summation operation on `TranAmount`, resulting in another
    RDD of the form (AccountNo,TotalAmount),where every AccountNo will have only one
    element and TotalAmount is the sum of all the TranAmount for the given AccountNo.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此键上对`TranAmount`执行求和操作，生成另一个RDD，形式为(AccountNo,TotalAmount)，其中每个AccountNo只有一个元素，TotalAmount是给定AccountNo的所有TranAmount的总和。
- en: Now sort the key-value pairs on the `AccountNo` and store the output.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在按`AccountNo`对键值对进行排序并存储输出。
- en: In the whole process described, all are Spark transformations except the storing
    of the output. Storing of the output is a **Spark action**. Spark does all these
    operations on a need-to-do basis. Spark does not act when a Spark transformation
    is applied. The real act happens when the first Spark action in the chain is called.
    Then it diligently applies all the preceding Spark transformations in order, and
    then does the first encountered Spark action. This is based on the concept called
    **Lazy Evaluation**.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个描述的过程中，除了存储输出结果外，其余均为Spark转换操作。存储输出结果是一项**Spark动作**。Spark根据需要执行这些操作。当应用Spark转换时，Spark不会立即执行。真正的执行发生在链中的第一个Spark动作被调用时。然后它会按顺序勤奋地应用所有先前的Spark转换，并执行遇到的第一个Spark动作。这是基于**惰性求值**的概念。
- en: Note
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In the programming language context of declaring and using variables, *Lazy
    Evaluation* means that a variable is evaluated only when it is first used in the
    program.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在编程语言中声明和使用变量的上下文中，*惰性求值*意味着变量只在程序中首次使用时才进行求值。
- en: 'Apart from this action of storing the output to disk, there are many other
    possible Spark actions including, but not limited to, some of the ones given in
    the following list:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将输出存储到磁盘的动作外，还有许多其他可能的Spark动作，包括但不限于以下列表中的一些：
- en: Collecting all the contents in the resultant RDD to an array residing in the
    driver program
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将结果RDD中的所有内容收集到驱动程序中的数组
- en: Counting the number of elements in the RDD
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算RDD中元素的数量
- en: Counting the number of elements for each key in the RDD element
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算RDD元素中每个键的元素数量
- en: Taking the first element in the RDD
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取RDD中的第一个元素
- en: Taking a given number of elements from the RDD commonly used for top N reports
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从常用的RDD中取出指定数量的元素用于生成Top N报告
- en: Taking a sample of elements from the RDD
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从RDD中抽取元素样本
- en: Iterating through all the elements in the RDD
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遍历RDD中的所有元素
- en: In this example, many transformations are done on various RDDs that get created
    on the fly till the process is completed. In other words, whenever a transformation
    is done on an RDD, a new RDD gets created. This is because RDDs are inherently
    immutable. These RDDs that are getting created at the end of each transformation
    can be saved for future reference, or they will go out of scope eventually.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，对各种RDD进行了多次转换，这些RDD是在流程完成过程中动态创建的。换句话说，每当对RDD进行转换时，都会创建一个新的RDD。这是因为RDD本质上是不可变的。在每个转换结束时创建的这些RDD可以保存以供将来参考，或者它们最终会超出作用域。
- en: To summarize, the process of creating one or more RDDs and applying transformations
    and actions on them is a very common usage pattern seen ubiquitously in Spark
    applications.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，创建一个或多个RDD并对它们应用转换和操作的过程是Spark应用程序中非常普遍的使用模式。
- en: Note
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The table referred in the preceding data transformation example contains the
    values in an RDD of type the RDD[(string, string, double)]. In this RDD, there
    are multiple elements, and each one is a tuple of the type (string, string, double).
    It is very common among programmers and the user community, for easy reference
    and conveying ideas, that the term `record` is being used to refer one to element
    in the RDD. In Spark RDD there is no concept of records, rows and columns. In
    other words the term `record` is mistakenly used synonymously to an element in
    the RDD, which may be a complex data type such as a tuple or a non-scalar data
    type. In this book, this practice is highly refrained to use the correct terms.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 前面数据转换示例中提到的表包含一个类型为RDD[(string, string, double)]的RDD中的值。在这个RDD中，有多个元素，每个元素都是一个类型为(string,
    string, double)的元组。为了便于参考和传达思想，程序员和用户社区通常使用术语`记录`来指代RDD中的一个元素。在Spark RDD中，没有记录、行和列的概念。换句话说，术语`记录`被错误地用作RDD中元素的同义词，这可能是一个复杂的数据类型，如元组或非标量数据类型。在本书中，我们尽量避免使用这种做法，而是使用正确的术语。
- en: In Spark there are a good amount of Spark transformations available. These are
    really powerful because most of these take functions as input parameters to do
    the transformation. In other words, these transformations act on the RDD based
    on the functions that are defined and supplied by the user. This becomes even
    more powerful with Spark's uniform programming model. Whether the programming
    language of choice is Scala, Java, Python, or R, the way Spark transformations
    and Spark actions are used is similar. This lets the organizations choose their
    programming language of choice.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了大量的Spark转换。这些转换非常强大，因为大多数转换都以函数作为输入参数来进行转换。换句话说，这些转换根据用户定义和提供的函数作用于RDD。Spark的统一编程模型使得这一点更加强大。无论选择的编程语言是Scala、Java、Python还是R，使用Spark转换和Spark操作的方式都是相似的。这使得组织可以选择他们偏好的编程语言。
- en: In Spark, even though the number of Spark actions are limited in number, they
    are really powerful, and users can write their own Spark actions if there is a
    need. There are many Spark connector programs that are available in the market,
    mainly to read and write data from various data stores. These connector programs
    are designed and developed either by the user community or by the data store vendors
    themselves to have connectivity to Spark. In addition to the available Spark actions,
    they may define their own actions to supplement existing sets of Spark actions.
    For example, the Spark Cassandra Connector is used to connect to Cassandra from
    Spark. This has an action `saveToCassandra`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中虽然Spark操作的数量有限，但它们非常强大，如果需要，用户可以编写自己的Spark操作。市场上有许多Spark连接器程序，主要用于从各种数据存储中读取和写写数据。这些连接器程序由用户社区或数据存储供应商设计和开发，以实现与Spark的连接。除了现有的Spark操作外，它们可能还会定义自己的操作来补充现有的Spark操作集合。例如，Spark
    Cassandra连接器用于从Spark连接到Cassandra，它有一个操作`saveToCassandra`。
- en: Monitoring with Spark
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark监控
- en: The previous chapter covered the details of the installation and development
    tool setup that is required for developing and running data processing applications
    using Spark. In most of the real-world applications, the Spark applications can
    become very complex with a really huge **directed acyclic graph**  (**DAG**) of
    Spark transformations and Spark actions. Spark comes with really powerful monitoring
    tools for monitoring the jobs that are running in a given Spark ecosystem. The
    monitoring doesn't start automatically.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 前一章节详细介绍了使用Spark开发和运行数据处理应用程序所需的安装和开发工具设置。在大多数现实世界的应用中，Spark应用程序可能会变得非常复杂，涉及一个庞大的**有向无环图**(**DAG**)，其中包含Spark转换和Spark操作。Spark自带了非常强大的监控工具，用于监控特定Spark生态系统中运行的作业。但监控不会自动启动。
- en: Tip
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Note that this is a completely optional step for running Spark applications.
    If enabled, it will give a very good insight into the way the Spark applications
    are run. Discretion has to be used to enable this in production environments,
    as it can affect the response time of the applications.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是运行Spark应用程序的一个完全可选步骤。如果启用，它将提供关于Spark应用程序运行方式的深刻见解。在生产环境中启用此功能需谨慎，因为它可能会影响应用程序的响应时间。
- en: 'First of all, there are some configuration changes to be made. The event logging
    mechanism should be turned on. For this, take the following steps:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，需要进行一些配置更改。事件日志机制应开启。为此，请执行以下步骤：
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the preceding steps are completed, edit the newly created `spark-defaults.conf`
    file to have the following properties:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前述步骤后，编辑新创建的`spark-defaults.conf`文件，使其包含以下属性：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Once the preceding steps are completed, make sure that the previously used log
    directory exists in the filesystem.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前述步骤后，确保之前使用的日志目录存在于文件系统中。
- en: 'Apart from the preceding configuration file changes, there are many properties
    in that configuration file that can be changed to fine tune the Spark runtime.
    The most important among them that is used frequently is the Spark driver memory.
    If the applications are dealing with a huge amount of data, it is a good idea
    to customize this property `spark.driver.memory`to have a higher value. Then run
    the following commands to start the Spark master:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述配置文件的更改外，该配置文件中还有许多属性可以更改以微调Spark运行时。其中最常用且最重要的是Spark驱动程序内存。如果应用程序处理大量数据，将此属性`spark.driver.memory`设置为较高值是个好主意。然后运行以下命令以启动Spark主节点：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once the preceding steps are completed, make sure that the Spark web **user
    interface** (**UI**) is starting up by going to `http://localhost:8080/`. The
    assumption here is that there is no other application running in the `8080` port.
    If for some reason, there is a need to run this application on a different port,
    the command line option `--webui-port <PORT>` can be used in the script while
    starting the web user interface.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前述步骤后，确保通过访问`http://localhost:8080/`启动Spark Web **用户界面** (**UI**)。这里假设`8080`端口上没有其他应用程序运行。如果出于某种原因，需要在不同的端口上运行此应用程序，可以在启动Web用户界面的脚本中使用命令行选项`--webui-port
    <PORT>`。
- en: 'The web UI should look something similar to that shown in Figure 2:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Web UI应该类似于图2所示：
- en: '![Monitoring with Spark](img/image_02_003.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark进行监控](img/image_02_003.jpg)'
- en: Figure 2
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图2
- en: The most important information to be noted in the preceding figure is the fully-qualified
    Spark master URL (not the REST URL). It is going to be used again and again for
    many of the hands-on exercises that are going to be discussed in this book. The
    URL can change from system to system and the DNS settings. Also note that throughout
    this book, for all the hands-on exercises, Spark standalone deployment is used,
    which is the easiest among the deployments to get started with a single computer.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图中最重要的信息是完整的Spark主URL（不是REST URL）。它将在本书中讨论的许多实践练习中反复使用。该URL可能因系统而异，并受DNS设置影响。还要注意，本书中所有实践练习均使用Spark独立部署，这是在单台计算机上开始部署最简单的方式。
- en: Tip
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: These Spark application monitoring steps are given now to make the readers familiar
    with the toolset that Spark provides. Those who are familiar with these tools
    or those who are very confident of the application behavior need not need the
    help of these tools. But to understand the concepts, debugging, and some visualizations
    of the processes, these tools definitely provide immense help.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在给出这些Spark应用程序监控步骤，是为了让读者熟悉Spark提供的工具集。熟悉这些工具或对应用程序行为非常自信的人可能不需要这些工具的帮助。但为了理解概念、调试以及一些过程的可视化，这些工具无疑提供了巨大的帮助。
- en: 'From the Spark web UI that is given in Figure 2, it can be noted that there
    are no worker nodes available to do any task, and there are no running applications.
    The following steps capture the instructions to start the worker nodes. Note how
    the Spark master URL is being used while starting the worker node:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从图2所示的Spark Web UI中可以看出，没有可用于执行任何任务的工作节点，也没有正在运行的应用程序。以下步骤记录了启动工作节点的指令。注意在启动工作节点时如何使用Spark主URL：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once the worker node is started, in the Spark web UI, the newly started worker
    node is displayed. The  `$SPARK_HOME/conf/slaves.template`template captures the
    default worker nodes that will be started with the invocation of the preceding
    command.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦worker节点启动，在Spark Web UI中，新启动的worker节点将被显示。`$SPARK_HOME/conf/slaves.template`模板捕获了默认的worker节点，这些节点将在执行上述命令时启动。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If additional worker nodes are required, copy the `slaves.template` file to
    name it to slaves and have the entries captured in there. When a spark-shell,
    pyspark, or sparkR is started, instructions can be given to it to use a given
    Spark master. This is useful when there is a need to run Spark applications or
    statements on a remote Spark cluster or against a given Spark master. If nothing
    is given, the Spark applications will run in the local mode.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要额外的worker节点，将`slaves.template`文件复制并重命名为slaves，并在其中捕获条目。当启动spark-shell、pyspark或sparkR时，可以给出指令让其使用特定的Spark
    master。这在需要远程Spark集群或针对特定Spark master运行Spark应用程序或语句时非常有用。如果没有给出任何内容，Spark应用程序将在本地模式下运行。
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The Spark web UI will look similar to that shown in Figure 3 once a worker
    node is started successfully. After this, if an application is run with the preceding
    Spark master URL, even that application''s details will be displayed in the Spark
    web UI. A detailed coverage of the applications is to follow in this chapter.
    Use the following scripts to stop the workers and master processes:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Web UI在成功启动worker节点后将类似于图3所示。之后，如果使用上述Spark master URL运行应用程序，该应用程序的详细信息也会显示在Spark
    Web UI中。本章后续将详细介绍应用程序。使用以下脚本停止worker和master进程：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Monitoring with Spark](img/image_02_004.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark进行监控](img/image_02_004.jpg)'
- en: Figure 3
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图3
- en: The basics of programming with Spark
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark编程基础
- en: Spark programming revolves around RDDs. In any Spark application, the input
    data to be processed is taken to create an appropriate RDD. To begin with, start
    with the most basic way of creating an RDD, which is from a list. The input data
    used for this `hello world` kind of application is a small collection of retail
    banking transactions. To explain the core concepts, only some very elementary
    data items have been picked up. The transaction records contain account numbers
    and transaction amounts.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Spark编程围绕RDD展开。在任何Spark应用程序中，待处理的数据被用来创建适当的RDD。首先，从创建RDD的最基本方式开始，即从一个列表开始。用于这种`hello
    world`类型应用程序的输入数据是一小部分零售银行交易。为了解释核心概念，只选取了一些非常基础的数据项。交易记录包含账户号和交易金额。
- en: Tip
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In these use cases and all the upcoming use cases in the book, if the term record
    is used, that will be in the business or use case context.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的所有用例中，如果使用术语“记录”，那将是在业务或用例的上下文中。
- en: 'The use cases selected for elucidating the Spark transformations and Spark
    actions here are given as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于阐释Spark转换和Spark动作的用例：
- en: The transaction records are coming as comma-separated values.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交易记录以逗号分隔的值形式传入。
- en: Filter out only the good transaction records from the list. The account number
    should start with `SB` and the transaction amount should be greater than zero.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从列表中筛选出仅包含良好交易记录的部分。账户号应以`SB`开头，且交易金额应大于零。
- en: Find all the high value transaction records with a transaction amount greater
    than 1000.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找所有交易金额大于1000的高价值交易记录。
- en: Find all the transaction records where the account number is bad.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找所有账户号有问题的交易记录。
- en: Find all the transaction records where the transaction amount is less than or
    equal to zero.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找所有交易金额小于或等于零的交易记录。
- en: Find a combined list of all the bad transaction records.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找所有不良交易记录的合并列表。
- en: Find the total of all the transaction amounts.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有交易金额的总和。
- en: Find the maximum of all the transaction amounts.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出所有交易金额的最大值。
- en: Find the minimum of all the transaction amounts.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出所有交易金额的最小值。
- en: Find all the good account numbers.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找所有良好账户号。
- en: 'The approach that is going to be followed throughout the book for any application
    that is going to be developed begins with the Spark REPL for the appropriate language.
    Start the Scala REPL for Spark and make sure that it starts without any errors
    and the prompt is seen. For this application, we will enable monitoring to learn
    how to do that and use it along with the development process. Other than explicitly
    starting the Spark master and the slaves separately, Spark comes with a script
    that will start both of these together using a single script. Then, fire up the
    Scala REPL with the Spark master URL:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中将遵循的方法是，对于任何将要开发的应用程序，都从适用于相应语言的Spark REPL开始。启动Scala REPL以使用Spark，并确保它无错误启动且可以看到提示符。对于此应用程序，我们将启用监控以学习如何操作，并在开发过程中使用它。除了显式启动Spark主节点和从节点外，Spark还提供了一个脚本，该脚本将使用单个脚本同时启动这两个节点。然后，使用Spark主URL启动Scala
    REPL：
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'At the Scala REPL prompt, try the following statements. The output of the statements
    is given in bold. Note that `scala>` is the Scala REPL prompt:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala REPL提示符下，尝试以下语句。语句的输出以粗体显示。请注意，`scala>`是Scala REPL提示符：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'All the preceding statements fall into one category except the first RDD creation
    and two function value definitions. They are all Spark transformations. Here is
    the step-by-step detail capturing what has been done so far:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 除第一个RDD创建和两个函数值定义外，所有先前的语句都属于一类，即Spark转换。以下是迄今为止所做操作的逐步详细说明：
- en: The value `acTransList` is the array containing the comma separated transaction
    records.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值`acTransList`是包含逗号分隔交易记录的数组。
- en: The value `acTransRDD` is the RDD created out of the array where `sc` is the
    Spark context or the Spark driver and the RDD is created in a parallelized way
    so that the RDD elements can form a distributed dataset. In other words, an instruction
    is given to the Spark driver to form a parallel collection or RDD from the given
    collection of values.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值`acTransRDD`是从数组创建的RDD，其中`sc`是Spark上下文或Spark驱动程序，RDD以并行化方式创建，使得RDD元素能够形成分布式数据集。换句话说，向Spark驱动程序发出指令，以从给定值集合形成并行集合或RDD。
- en: The value `goodTransRecords` is the RDD created from `acTransRDD`after filtering
    the conditions transaction amount is > 0 and the account number starts with `SB`.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值`goodTransRecords`是从`acTransRDD`创建的RDD，经过过滤条件筛选，交易金额大于0且账户号码以`SB`开头。
- en: The value `highValueTransRecords` is the RDD created from `goodTransRecords`after
    filtering the conditions transaction amount is > 1000.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值`highValueTransRecords`是从`goodTransRecords`创建的RDD，经过过滤条件筛选，交易金额大于1000。
- en: The next two statements are storing the function definitions in a Scala value
    for easy reference later.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来的两条语句将函数定义存储在Scala值中，以便稍后轻松引用。
- en: The values `badAmountRecords` and `badAccountRecords` are RDDs created from
    `acTransRDD` to filter the bad records containing the wrong transaction amount
    and invalid account number, respectively.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值`badAmountRecords`和`badAccountRecords`是从`acTransRDD`创建的RDD，分别用于过滤包含错误交易金额和无效账户号码的不良记录。
- en: The value `badTransRecords` contains the union of the elements of both of the
    `badAmountRecords` and `badAccountRecords` RDDs.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值`badTransRecords`包含`badAmountRecords`和`badAccountRecords`两个RDD元素的并集。
- en: The Spark web UI for this application so far will not show anything at this
    point because only Spark transformations have been executed. The real activity
    will start only after the first Spark action is executed.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，此应用程序的Spark Web UI将不会显示任何内容，因为仅执行了Spark转换。真正的活动将在执行第一个Spark动作后开始。
- en: 'The following statements are the continuation of the already executed statements:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下语句是已执行语句的延续：
- en: '[PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'All the preceding statements did one thing, which is execute a Spark action
    on the RDDs *defined* earlier. All the evaluations of the RDDs happened only when
    a Spark action was called on those RDDs. The following statements are doing some
    of the calculations on the RDDs:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 所有先前的语句执行了一项操作，即对之前*定义*的RDD执行Spark动作。只有在RDD上触发Spark动作时，才会对RDD进行评估。以下语句正在对RDD进行一些计算：
- en: '[PRE9]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding numbers calculated the sum, maximum and minimum, of all transaction
    amounts from the good records. In all the preceding transformations, the transaction
    records are processed one at a time. From those records, the account number and
    transaction amount are extracted and processed. It was done like that because
    the use case requirement was like that. Now the comma-separated values in each
    transaction record are split without looking at whether it is an account number
    or a transaction amount. The resulting RDD will contain a collection with all
    these mixed up. Out of that, if the elements starting with `SB` are picked up,
    that will result in good account numbers. The following statements are going to
    do that:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 前述数字计算了来自良好记录的所有交易金额的总和、最大值和最小值。在前述所有转换中，交易记录一次处理一条。从这些记录中，提取账户号和交易金额并进行处理。这样做是因为用例需求如此。现在，每个交易记录中的逗号分隔值将被分割，而不考虑它是账户号还是交易金额。结果RDD将包含一个集合，其中所有这些混合在一起。从中提取以`SB`开头的元素，将得到良好的账户号码。以下语句将执行此操作：
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now at this point, if the Spark web UI is opened, unlike what is seen in Figure
    3, one difference can be noticed. Since some Spark actions have been done, an
    application entry will show up. Since the Scala REPL of Spark is still running,
    it is shown in the list of applications that are still running. The following
    Figure 4 captures that:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，如果打开Spark Web UI，与图3不同，可以注意到一个差异。由于已经执行了一些Spark操作，将显示一个应用程序条目。由于Spark的Scala
    REPL仍在运行，它显示在仍在运行的应用程序列表中。图4捕捉了这一点：
- en: '![The basics of programming with Spark](img/image_02_005.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![Spark编程基础](img/image_02_005.jpg)'
- en: Figure 4
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图4
- en: Navigate by clicking on the application ID to see all the metrics related to
    the running applications including the DAG visualizations and many more.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 点击应用程序ID进行导航，以查看与运行中的应用程序相关的所有指标，包括DAG可视化图表等更多内容。
- en: 'These statements covered all the use cases discussed, and it is worth going
    through the Spark transformations covered so far. These are some of the basic
    but very important transformations that will be used in most of the applications
    again and again:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这些语句涵盖了讨论过的所有用例，值得回顾迄今为止介绍的Spark转换。以下是一些基本但非常重要的转换，它们将在大多数应用程序中反复使用：
- en: '| **Spark transformation** | **What it does** |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **Spark转换** | **功能描述** |'
- en: '| --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `filter(fn)` | **Iterates through all the elements of the RDD, applies the
    function that is passed, and picks up the elements that return true as evaluated
    by the function on the element.** |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| `filter(fn)` | **遍历RDD中的所有元素，应用传入的函数，并选取函数评估为真的元素。** |'
- en: '| `map(fn)` | Iterates through all the elements of the RDD, applies the function
    that is passed, and picks up the output returned by the function. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `map(fn)` | 遍历RDD中的所有元素，应用传入的函数，并选取函数返回的输出。 |'
- en: '| `flatMap(fn)` | Iterates through all the elements of the RDD, applies the
    function that is passed, and picks up the output returned by the function. The
    big difference here as compared to the Spark transformation `map(fn)` is that
    the function acts on a single element and returns a flat collection of elements.
    For example, it takes one banking transaction record and splits it into multiple
    fields, resulting in a collection from a single element. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `flatMap(fn)` | 遍历RDD中的所有元素，应用传入的函数，并选取函数返回的输出。与Spark转换`map(fn)`的主要区别在于，该函数作用于单个元素并返回一个扁平的元素集合。例如，它将一条银行交易记录拆分为多个字段，从单个元素生成一个集合。
    |'
- en: '| `union(other)` | Takes the union of all the elements of this RDD and the
    other RDD. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `union(other)` | 获取此RDD和另一个RDD的所有元素的并集。 |'
- en: It is also worth going through the Spark actions covered so far. These are some
    of the basic ones, but more actions will be covered in due course.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 同样值得回顾迄今为止介绍的Spark动作。这些是一些基本动作，但后续将介绍更多动作。
- en: '| **Spark action** | **What it does** |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| **Spark动作** | **功能描述** |'
- en: '| --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `collect()` | **Collects all the elements in the RDD to an array in the Spark
    driver.** |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `collect()` | **将RDD中的所有元素收集到Spark驱动程序中的数组中。** |'
- en: '| `reduce(fn)` | Applies the function fn on all the elements of the RDD and
    the final result is calculated as defined by the function. It should be a function
    that takes two parameters and returns one, which is also commutative and associative.
    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `reduce(fn)` | 对RDD的所有元素应用函数fn，并根据函数定义计算最终结果。该函数应接受两个参数并返回一个结果，且具有交换性和结合性。
    |'
- en: '| `foreach(fn)` | Applies the function fn on all the elements of the RDD. This
    is mainly used for side effects. The Spark transformation `map(fn)` applies the
    function to all the elements of the RDD and returns another RDD. But the `foreach(fn)`
    Spark transformation does not return an RDD. For example, `foreach(println)` will
    take each element from the RDD and print it onto the console. Even though it is
    not used in the use cases covered here, it is worth mentioning. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `foreach(fn)` | 对RDD的所有元素应用函数fn。这主要用于产生副作用。Spark转换`map(fn)`将函数应用于RDD的所有元素并返回另一个RDD。但`foreach(fn)`
    Spark转换不返回RDD。例如，`foreach(println)`将从RDD中取出每个元素并将其打印到控制台。尽管这里未涉及的用例中未使用它，但值得一提。'
- en: The next step in the Spark learning process is to try the statements in the
    Python REPL, covering exactly the same use case. The variable definitions have
    been maintained as similar as possible in both the languages to have easy assimilation
    of ideas. There may be minor variations in the way they are used here as compared
    to the Scala way; conceptually, it is independent of the language of choice.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 学习Spark的下一步是尝试在Python REPL中执行语句，覆盖完全相同的用例。变量定义在两种语言中尽可能保持相似，以便轻松吸收概念。与Scala方式相比，这里使用的方式可能会有细微差别；从概念上讲，它与所选语言无关。
- en: 'Start the Python REPL for Spark and make sure that it starts without any errors
    and the prompt is seen. While playing around with Scala code, the monitoring was
    already enabled. Now fire up the Python REPL with the Spark master URL:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 启动Spark的Python REPL，并确保它无错误启动且能看到提示符。在尝试Scala代码时，监控已启用。现在使用Spark主URL启动Python
    REPL：
- en: '[PRE11]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'At the Python REPL prompt, try the following statements. The output of the
    statements is given in bold. Note that `>>>` is the Python REPL prompt:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python REPL提示符下，尝试以下语句。语句的输出以粗体显示。请注意，`>>>`是Python REPL提示符：
- en: '[PRE12]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The real power of the uniform programming model of Spark is very clearly visible
    if both the Scala and Python code sets are compared. The Spark transformations
    and Spark actions are the same in both the language implementations. The way functions
    are passed into these are different because of the programming language syntax
    differences.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果比较Scala和Python代码集，Spark统一编程模型的真正力量就非常明显。Spark转换和Spark操作在两种语言实现中都是相同的。由于编程语言语法的差异，函数传递给这些操作的方式不同。
- en: Before running the Python REPL for Spark, the Scala REPL was closed and this
    was done on purpose. Then the Spark web UI should look something similar to that
    shown in Figure 5\. Since the Scala REPL was closed, that is getting listed under
    the completed applications list. Since the Python REPL is still open, that is
    getting listed under the running applications list. Note the application names
    of both the Scala REPL and Python REPL of Spark in the Spark web UI. These are
    standard names. When custom applications are run from files, there are ways to
    assign custom names while defining the Spark context object for the applications
    to facilitate monitoring of the applications and for logging purposes. These details
    will be covered later in this chapter.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行Spark的Python REPL之前，有意关闭了Scala REPL。然后，Spark Web UI应类似于图5所示。由于Scala REPL已关闭，它被列在已完成应用程序列表中。由于Python
    REPL仍在运行，它被列在运行中的应用程序列表中。请注意Spark Web UI中Scala REPL和Python REPL的应用程序名称。这些都是标准名称。当从文件运行自定义应用程序时，可以在定义Spark上下文对象时指定自定义名称，以便于监控应用程序和日志记录。这些细节将在本章后面介绍。
- en: It is a good idea to spend time with the Spark web UI, getting familiar with
    all the metrics that are being captured, and how the DAG visualization is given
    in the UI. It will help a lot while debugging complex Spark applications.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 花时间熟悉Spark Web UI是个好主意，了解所有捕获的指标以及如何在UI中呈现DAG可视化。这将大大有助于调试复杂的Spark应用程序。
- en: '![The basics of programming with Spark](img/image_02_006.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![Spark编程基础](img/image_02_006.jpg)'
- en: Figure 5
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图5
- en: MapReduce
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MapReduce
- en: Since day one, Spark has been placed as the replacement for Hadoop MapReduce
    programs. In general, data processing jobs are done in MapReduce style if that
    job can be divided into multiple tasks and they can be executed in parallel, and
    the final results can be computed after collecting the results from all these
    distributed pieces. Unlike Hadoop MapReduce, Spark can do this even if the DAG
    of activities is more than the two stages, such as Map and Reduce. Spark is designed
    to do that and that is one of the biggest value propositions that Spark highlights.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 自Spark诞生之日起，它就被定位为Hadoop MapReduce程序的替代品。通常，如果一个数据处理任务可以分解为多个子任务，并且这些子任务能够并行执行，且最终结果可以在收集所有这些分布式片段的结果后计算得出，那么该任务就会采用MapReduce风格。与Hadoop
    MapReduce不同，即使活动的有向无环图（DAG）超过两个阶段（如Map和Reduce），Spark也能完成这一过程。Spark正是为此而设计，这也是Spark强调的最大价值主张之一。
- en: This section is going to continue with the same retail banking application and
    pick up some of the use cases that are ideal candidates for the MapReduce kind
    of data processing.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将继续探讨同一零售银行应用程序，并选取一些适合MapReduce类型数据处理的理想用例。
- en: 'The use cases selected for elucidating the MapReduce kind of data processing
    here are given as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此处为阐明MapReduce类型数据处理所选用的用例如下：
- en: The retail banking transaction records come with account numbers and the transaction
    amounts in comma-separated strings.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 零售银行交易记录带有以逗号分隔的账户号码和交易金额字符串。
- en: Pair the transactions to have key/value pairs such as (`AccNo`, `TranAmount`).
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将交易配对成键/值对，例如(`AccNo`, `TranAmount`)。
- en: Find an account level summary of all the transactions to get the account balance.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找所有交易的账户级别汇总，以获取账户余额。
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala REPL提示符下，尝试以下语句：
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is the step-by-step detail capturing what has been done so far:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是迄今为止所做工作的详细步骤记录：
- en: The value `acTransList` is the array containing the comma-separated transaction
    records.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 值`acTransList`是包含逗号分隔交易记录的数组。
- en: The value `acTransRDD` is the RDD created out of the array, where sc is the
    Spark context or the Spark driver and the RDD is created in a parallelized way
    so that the RDD elements can form a distributed dataset.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 值`acTransRDD`是由数组创建的RDD，其中sc是Spark上下文或Spark驱动程序，RDD以并行化方式创建，以便RDD元素可以形成分布式数据集。
- en: Transform the `acTransRDD` to `acKeyVal` to have key-value pairs of the form
    (K,V), where the account number is chosen as the key. In this set of elements
    in the RDD, there will be multiple elements with the same key.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`acTransRDD`转换为`acKeyVal`，以拥有形式为(K,V)的键值对，其中选择账户号码作为键。在此RDD的元素集合中，将存在多个具有相同键的元素。
- en: In the next step, the key-value pairs are grouped by the key and a reduction
    function has been passed, which will add the transaction amount to form key-value
    pairs containing one element for a specific key in the RDD and the total of all
    the amounts for the same key. Then sort the elements on the key before producing
    the final result.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，将键值对按键分组，并传递一个缩减函数，该函数会将交易金额累加，形成包含特定键的一个元素以及同一键下所有金额总和的键值对。然后在生成最终结果前，根据键对元素进行排序。
- en: Collect the elements to an array at the driver level.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在驱动程序级别收集元素到数组中。
- en: 'Assuming that the RDD `acKeyVal` is partitioned into two parts and distributed
    to a cluster for processing, Figure 6 captures the essence of the processing:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 假设RDD `acKeyVal`被分为两部分并分布到集群进行处理，图6捕捉了处理的核心：
- en: '![MapReduce](img/image_02_008.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![MapReduce](img/image_02_008.jpg)'
- en: Figure 6
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图6
- en: 'The following table captures the Spark actions that are introduced in this
    use case:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 下表概述了本用例中引入的Spark操作：
- en: '| **Spark action** | **What it does?** |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| **Spark操作** | **其作用是什么？** |'
- en: '| --- | --- |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `reduceByKey(fn,[noOfTasks])` | **Applies the function fn on the RDD of the
    form (K,V) and is reduced to remove duplicate keys and apply the function passed
    as the parameter to be acted on the values at the key level.** |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKey(fn,[noOfTasks])` | **对形式为(K,V)的RDD应用函数fn，并通过减少重复键并应用作为参数传递的函数来在键级别对值进行操作，从而实现缩减。**
    |'
- en: '| `sortByKey([ascending], [numTasks])` | Sorts the RDD elements if the RDD
    is of the form (K,V) by its key K |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| `sortByKey([ascending], [numTasks])` | 如果RDD为形式(K,V)，则根据其键K对RDD元素进行排序 |'
- en: 'The `reduceByKey`action deserves a special mention. In Figure 6, the grouping
    of the elements by the key is a well-known operation. But in the next step, for
    the same key, the function passed to as a parameter takes two parameters and returns
    one. It is not very intuitive to get this right and you may be wondering from
    where the two inputs are coming while iterating through the values of the (K,V)
    pair for each key. This behavior takes the concept from the Scala collection method
    `reduceLeft`. The following Figure 7, with the values of the key **SB10001** doing
    the `reduceByKey(_ + _)`operation, is an attempt to explain the concept. This
    is just for the elucidation purposes of this example and the actual Spark implementation
    to do the same may be different:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey`操作值得特别提及。在图6中，按键对元素进行分组是一个众所周知的操作。但在下一步中，对于相同的键，作为参数传递的函数接受两个参数并返回一个。要正确理解这一点并不直观，你可能会疑惑在遍历每个键的(K,V)对值时，这两个输入从何而来。这种行为借鉴了Scala集合方法`reduceLeft`的概念。下图7展示了键**SB10001**执行`reduceByKey(_
    + _)`操作的情况，旨在解释这一概念。这只是为了阐明此示例的目的，实际的Spark实现可能有所不同：'
- en: '![MapReduce](img/image_02_010.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![MapReduce](img/image_02_010.jpg)'
- en: Figure 7
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图7
- en: On the right-hand side of Figure 7, the `reduceLeft` operation of the Scala
    collection method is illustrated. That is an attempt to give some insight into
    from where the two parameters are coming for the `reduceLeft`function. As a matter
    of fact, many of the transformations that are being used on Spark RDD are adapted
    from Scala collection methods.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7的右侧，展示了Scala集合方法中的`reduceLeft`操作。这是为了提供一些关于`reduceLeft`函数两个参数来源的见解。事实上，Spark
    RDD上使用的许多转换都是从Scala集合方法改编而来的。
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python REPL提示符下，尝试以下语句：
- en: '[PRE14]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `reduceByKey` took an input parameter, which is a function. Similar to this,
    there is another transformation that does the key-based operation in a slightly
    different way. It is `groupByKey()`. This gathers all the values of a given key
    and forms the list of values from all the individual elements.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey`接受一个输入参数，即一个函数。与此类似，还有一种转换以略有不同的方式执行基于键的操作，即`groupByKey()`。它将给定键的所有值聚集起来，形成来自所有单独元素的值列表。'
- en: If there is a need to do multiple levels of processing with the same value elements
    as a collection for each key, this is the suitable transformation. In other words,
    if there are many (K,V) pairs, this transformation returns (K, Iterable<V>) for
    each key.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要对每个键的相同值元素集合进行多级处理，这种转换是合适的。换句话说，如果有许多(K,V)对，此转换将为每个键返回(K, Iterable<V>)。
- en: Tip
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The only thing the developer needs to be cognizant about is to make sure that
    the number of such (K,V) pairs is not really huge so that the operation doesn't
    create performance problems. There is no hard and fast rule to find this out and
    it rather depends on the use case.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者唯一需要注意的是，确保此类(K,V)对的数量不会过于庞大，以免操作引发性能问题。并没有严格的规则来确定这一点，它更多取决于具体用例。
- en: In all the preceding code snippets, for extracting account numbers or any other
    field from the comma-separated transaction record, split(`,`) is used multiple
    times within the `map()` transformation. This is to demonstrate the use of array
    elements within `map()`, or any other transformation or method. A better way of
    extracting the fields of the transaction record is to transform them as a tuple
    containing the required fields and then use the fields from the tuple to employ
    them in some of the following code snippets. In this way, there is no need to
    call split (`,`) repeatedly for each field extraction.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述所有代码片段中，为了从逗号分隔的交易记录中提取账号或其他字段，`map()`转换过程中多次使用了`split(`,`)`。这是为了展示在`map()`或其他转换或方法中使用数组元素的用法。更佳的做法是将交易记录字段转换为包含所需字段的元组，然后从元组中提取字段，用于后续代码片段。这样，就无需为每个字段提取重复调用`split(`,`)`。
- en: Joins
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接
- en: In the **Relational Database Management Systems** (**RDBMS**) world, joining
    multiple tables rows based on a key is a very common practice. When it comes to
    the NoSQL data stores, joining multiple tables became a real problem because many
    of the NoSQL data stores don't have support for the table joins. In the NoSQL
    world, redundancy is allowed. Whether a technology supports table joins or not,
    business use cases mandate joins of datasets based on keys all the time. Because
    of this, it is imperative to have the joins done in a batch mode in many of the
    use cases.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在**关系型数据库管理系统**（**RDBMS**）领域，基于键连接多个表的行是一种非常常见的做法。而在NoSQL数据存储中，多表连接成为一个真正的问题，因为许多NoSQL数据存储不支持表连接。在NoSQL世界中，允许冗余。无论技术是否支持表连接，业务用例始终要求基于键连接数据集。因此，在许多用例中，批量执行连接是至关重要的。
- en: Spark provides transformations to join multiple RDDs based on a key. This supports
    many use cases. These days there are many NoSQL data stores having connectors
    to talk to Spark. When working with such data stores, it is very simple to construct
    RDDs of data from multiple tables, do the join from Spark, and store the results
    back into the data stores in batch mode or even in near-to-real-time mode. Spark
    transformations are available for left outer join and right outer join, as well
    as full outer join.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了基于键连接多个RDD的转换。这支持了许多用例。如今，许多NoSQL数据存储都有与Spark通信的连接器。当与这些数据存储一起工作时，从多个表构建RDD、通过Spark执行连接并将结果以批量模式甚至近实时模式存储回数据存储变得非常简单。Spark转换支持左外连接、右外连接以及全外连接。
- en: The use cases selected for elucidating the join of multiple datasets using a
    key are given as follows.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于阐明使用键连接多个数据集的用例。
- en: The first dataset contains a retail banking master records summary with an account
    number, first name, and last name. The second dataset contains the retail banking
    account balance with an account number, and balance amount. The key on both of
    the datasets is the account number. Join the two datasets and create one dataset
    containing the account number, full name, and balance amount.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数据集包含零售银行主记录摘要，包括账户号、名字和姓氏。第二个数据集包含零售银行账户余额，包括账户号和余额金额。两个数据集的关键字都是账户号。将这两个数据集连接起来，创建一个包含账户号、全名和余额金额的数据集。
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala REPL提示符下，尝试以下语句：
- en: '[PRE15]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'All the statements given previously must be familiar by now, except the Spark
    transformation join. Similar to this transformation, the `leftOuterJoin`, `rightOuterJoin`,
    and `fullOuterJoin`are also available with the same usage pattern:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Spark转换连接之外，之前给出的所有语句现在应该都很熟悉了。类似地，`leftOuterJoin`、`rightOuterJoin`和`fullOuterJoin`也以相同的用法模式提供：
- en: '| **Spark transformation** | **What it does** |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| **Spark转换** | **功能** |'
- en: '| --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `join(other, [numTasks])` | **Joins this RDD with the other RDD, and the
    elements are joined together based on the key. Suppose the original RDD is of
    the form (K,V1) and the second RDD is of the form (K,V2), then the join operation
    will produce tuples of the form (K, (V1,V2)) with all the pairs of each key.**
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| `join(other, [numTasks])` | **将此RDD与另一个RDD连接，元素基于键进行连接。假设原始RDD的形式为(K,V1)，第二个RDD的形式为(K,V2)，则连接操作将生成形式为(K,
    (V1,V2))的元组，包含每个键的所有配对。** |'
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python REPL提示符下，尝试以下语句：
- en: '[PRE16]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: More actions
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多动作
- en: 'So far, the focus was mainly on Spark transformations. Spark actions are also
    important. To get insight into some more important Spark actions, take the following
    use cases, continuing from where it was stopped in the preceding section''s use
    cases:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，重点主要放在Spark转换上。Spark动作同样重要。为了深入了解一些更重要的Spark动作，请继续从上一节用例停止的地方开始，考虑以下用例：
- en: From the list containing account numbers, names, and account balances, get the
    one that has the highest account balance
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从包含账户号、姓名和账户余额的列表中，获取余额最高的账户
- en: From the list containing account numbers, names, and account balances, get the
    top three having the highest account balance
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从包含账户号、姓名和账户余额的列表中，获取余额最高的前三个账户
- en: Count the number of balance transaction records at an account level
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计账户级别上的余额交易记录数量
- en: Count the total number of balance transaction records
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计余额交易记录的总数
- en: Print the name and account balance of all the accounts
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打印所有账户的姓名和账户余额
- en: Calculate the total of the account balance
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算账户余额总额
- en: Tip
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: It is a very common requirement to iterate through the elements in a collection,
    do some mathematical calculation on each of the elements, and at the end of it,
    use the result. The RDD is partitioned and distributed across worker nodes. If
    any normal variable is used for storing the cumulative result while iterating
    through the RDD elements, it may not yield the correct result. In such situations,
    instead of using regular variables, use Spark provided accumulators.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历集合中的元素，对每个元素进行一些数学计算，并在最后使用结果，这是一个非常常见的需求。RDD被分区并分布在worker节点上。如果在遍历RDD元素时使用普通变量存储累积结果，可能无法得到正确的结果。在这种情况下，不要使用常规变量，而是使用Spark提供的累加器。
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala REPL提示符下，尝试以下语句：
- en: '[PRE17]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following table captures the Spark actions that are introduced in this
    use case:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 下表概述了本用例中引入的Spark行动：
- en: '| **Spark action** | **What it does** |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| **触发行动** | **其作用** |'
- en: '| --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `first()` | **Returns the first element in the RDD.** |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| `first()` | **返回RDD的第一个元素。** |'
- en: '| `take(n)` | Returns an array of the first `n` elements from the RDD. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| `take(n)` | 返回RDD的前`n`个元素的数组。 |'
- en: '| `countByKey()` | Returns the count of elements by the key. If the RDD contains
    (K,V) pairs, this will return a dictionary of `(K, numOfValues)`. |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `countByKey()` | 按键返回元素计数。如果RDD包含(K,V)对，这将返回一个字典`(K, numOfValues)`。 |'
- en: '| `count()` | Returns the number of elements in the RDD. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `count()` | 返回RDD中的元素数量。 |'
- en: '| `foreach(fn)` | Applies the function fn to each element in the RDD. In the
    preceding use case, Spark Accumulator is being used with `foreach(fn)`. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `foreach(fn)` | 将函数fn应用于RDD中的每个元素。在前述用例中，使用`foreach(fn)`与Spark Accumulator。
    |'
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python REPL提示符下，尝试以下语句：
- en: '[PRE18]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Creating RDDs from files
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文件创建RDD
- en: So far, the focus of the discussion was on the RDD functionality and programming
    with RDDs. In all the preceding use cases, the RDD creation was done from the
    collection objects. But in the real-world use cases, the data will come from files
    stored in the local filesystems, and HDFS. Quite often, the data will come from
    NoSQL data stores such as Cassandra. It is possible to create RDDs by reading
    the contents from these data sources. Once RDD is created, then all the operations
    are uniform, as given in the preceding use cases. The data files coming out of
    the filesystems may be fixed width, comma-separated, or any other format. But
    the common pattern used for reading such data files is to read the data line by
    line and split the line to have the necessary separation of data items. In the
    case of data coming from other sources, the appropriate Spark connector program
    is to be used and the appropriate API for reading data is to be used.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，讨论的重点是RDD功能和使用RDD编程。在前述所有用例中，RDD的创建都是从集合对象开始的。但在现实世界的用例中，数据将来自存储在本地文件系统、HDFS中的文件。数据通常来自Cassandra等NoSQL数据存储。可以通过从这些数据源读取内容来创建RDD。一旦创建了RDD，所有操作都是统一的，如前述用例所示。来自文件系统的数据文件可能是固定宽度、逗号分隔或其他格式。但读取此类数据文件的常用模式是逐行读取数据，并将行分割以获得必要的数据项分离。对于来自其他来源的数据，应使用适当的Spark连接器程序和读取数据的适当API。
- en: Many third-party libraries are available to read the contents from various types
    of text files. For example, the Spark CSV library available from GitHub is a very
    useful one for creating RDDs from CSV files.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多第三方库可用于从各种类型的文本文件读取内容。例如，GitHub上提供的Spark CSV库对于从CSV文件创建RDD非常有用。
- en: 'The following table captures the way text files are read from various sources,
    such as local filesystems, HDFS, and so on. As discussed earlier, the processing
    of the text file is up to the use case requirements:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 下表概述了从各种来源（如本地文件系统、HDFS等）读取文本文件的方式。如前所述，文本文件的处理取决于用例需求：
- en: '| **File location** | **RDD creation** | **What it does** |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| **文件位置** | **RDD创建** | **其作用** |'
- en: '| --- | --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Local filesystem | `val textFile = sc.textFile("README.md")` | **Creates
    an RDD by reading the contents of the file named** `README.md` **from the directory
    from where the Spark shell is invoked. Here, the RDD is of the type RDD[string]
    and the elements will be lines from the file.** |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 本地文件系统 | `val textFile = sc.textFile("README.md")` | **通过读取目录中名为`README.md`的文件内容创建RDD，该目录是Spark
    shell被调用的位置。这里，RDD的类型为RDD[string]，元素将是文件中的行。** |'
- en: '| HDFS | `val textFile = sc.textFile ("hdfs://<location in HDFS>")` | Creates
    an RDD by reading the contents of the file specified in the HDFS URL |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| HDFS | `val textFile = sc.textFile("hdfs://<location in HDFS>")` | 通过读取HDFS
    URL中指定的文件内容创建RDD |'
- en: The most important aspect while reading the files from the local filesystem
    is that the file should be available in all the nodes of the Spark worker nodes.
    Apart from these two file locations given in the preceding table, any supported
    filesystem URI may be used.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 从本地文件系统读取文件时，最重要的是该文件应位于所有Spark工作节点上。除了上表中给出的这两个文件位置外，还可以使用任何支持的文件系统URI。
- en: Just like reading the contents from files in various filesystems, it is also
    possible to write the RDD onto files using the `saveAsTextFile`(path) Spark action.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 就像从各种文件系统中读取文件内容一样，也可以使用`saveAsTextFile`(path) Spark操作将RDD写入文件。
- en: Tip
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: All the Spark application use cases discussed here are run on the appropriate
    language's REPL of Spark. When writing applications, they will be written in proper
    source code files. In the case of Scala and Java, the application code files have
    to be compiled, packaged, and run with proper library dependencies, and are typically
    built using maven or sbt. This will be covered in detail when designing data processing
    applications using Spark, in the last chapter of this book.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论的所有Spark应用案例均在Spark相应语言的REPL上运行。编写应用程序时，它们将被编写到适当的源代码文件中。对于Scala和Java，应用程序代码文件需要编译、打包，并在适当的库依赖项下运行，通常使用maven或sbt构建。本书最后一章设计数据处理应用程序时，将详细介绍这一点。
- en: Understanding the Spark library stack
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Spark库栈
- en: Spark comes with a core data processing engine and a stack of libraries working
    on top of the core engine. It is very important to understand the concept of stacking
    libraries on top of the core framework.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Spark自带一个核心数据处理引擎以及一系列在核心引擎之上的库。理解在核心框架之上堆叠库的概念非常重要。
- en: All these libraries that are making use of the services provided by the core
    framework support the data abstractions offered by the core framework and much
    more. Before Spark came onto market, there were lots of independent open source
    products doing what the library stack in discussion here is now doing. The biggest
    disadvantage with these point products was their interoperability. They don't
    stack together well. They were implemented in different programming languages.
    The programming language of choice supported by these products, and the lack of
    uniformity in the APIs exposed by these products, were really challenging to get
    one application done with two or more such products. That is the relevance of
    the stack of libraries that work on top of Spark. They all work together with
    the same programming model. This helps organizations to standardize on the data
    processing toolset without vendorlock-in.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些利用核心框架提供的服务的库都支持核心框架提供的数据抽象，以及更多。在Spark进入市场之前，有很多独立的开放源代码产品在做这里讨论的库栈现在所做的事情。这些点产品最大的缺点是它们的互操作性。它们不能很好地堆叠在一起。它们是用不同的编程语言实现的。这些产品支持的编程语言选择，以及这些产品暴露的API缺乏统一性，对于使用两个或更多此类产品完成一个应用程序来说确实具有挑战性。这就是在Spark之上工作的库栈的相关性。它们都使用相同的编程模型协同工作。这有助于组织在没有供应商锁定的情况下标准化数据处理工具集。
- en: 'Spark comes with the following stack of domain-specific libraries, and Figure
    8 gives a comprehensive picture of the whole ecosystem as seen by a developer:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Spark附带了以下一系列特定领域的库，图8为开发者提供了一个全面的生态系统概览：
- en: '**Spark SQL**'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**'
- en: '**Spark Streaming**'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Streaming**'
- en: '**Spark MLlib**'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark MLlib**'
- en: '**Spark GraphX**'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark GraphX**'
- en: '![Understanding the Spark library stack](img/image_02_012.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![理解Spark库栈](img/image_02_012.jpg)'
- en: Figure 8
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图8
- en: In any organization, structured data is still very widely used. The most ubiquitous
    data access mechanism with structured data is SQL. Spark SQL provides the capability
    to write SQL-like queries on top of the structured data abstraction called the
    DataFrame API. DataFrame and SQL go very well and support data coming from various
    sources, such as Hive, Avro, Parquet, JSON, and many more. Once the data is loaded
    into the Spark context, they can be operated as if they are all coming from the
    same source. In other words, if required, SQL-like queries can be used to join
    data coming from different sources, such as Hive and JSON. Another big advantage
    that Spark SQL and the DataFrame API bring onto the developers table is the ease
    of use and no need-to-know functional programming methods, which is a requirement
    to do programming with RDDs.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何组织中，结构化数据仍然被广泛使用。最普遍的结构化数据访问机制是SQL。Spark SQL提供了在称为DataFrame API的结构化数据抽象之上编写类似SQL查询的能力。DataFrame和SQL非常契合，支持来自各种来源的数据，如Hive、Avro、Parquet、JSON等。一旦数据加载到Spark上下文中，它们就可以被操作，就像它们都来自同一来源一样。换句话说，如果需要，可以使用类似SQL的查询来连接来自不同来源的数据，例如Hive和JSON。Spark
    SQL和DataFrame API带给开发者的另一个巨大优势是易于使用，无需了解函数式编程方法，而这是使用RDD编程的要求。
- en: Tip
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Using Spark SQL and the DataFrame API, data can be read from various data sources
    and processed as if it is all coming from a unified source. Spark transformations
    and Spark actions support uniform programming interfaces. So the data source unification,
    API unification, and ability to use multiple programming languages to write data
    processing applications help the organizations to standardize on one data processing
    framework.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark SQL和DataFrame API，可以从各种数据源读取数据并处理，就像它们都来自统一来源一样。Spark转换和Spark操作支持统一的编程接口。因此，数据源的统一、API的统一以及能够使用多种编程语言编写数据处理应用程序，帮助组织标准化一个数据处理框架。
- en: The data ingestion into the organizational data sinks is increasing every day.
    At the same time, the velocity at which data is getting ingested is also increasing.
    Spark Streaming provides the library to process the data that is ingested from
    various sources at a very high velocity.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 组织数据池中的数据摄取量每天都在增加。同时，数据被摄取的速度也在加快。Spark Streaming提供了处理来自各种来源的高速摄取数据的库。
- en: In the past, data scientists had the challenge of building their own implementations
    of the machine learning algorithms and utilities in their programming language
    of choice. Quite often, such programming languages don't interoperate with the
    data processing toolset of the organization. Spark MLlib provides the unification
    process, where it comes with a lot of machine learning algorithms and utilities
    working on top of the Spark data processing engine.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，数据科学家面临的挑战是在他们选择的编程语言中构建自己的机器学习算法和实用程序实现。通常，这些编程语言与组织的数据处理工具集不兼容。Spark MLlib提供了统一过程，它自带了许多在Spark数据处理引擎之上工作的机器学习算法和实用程序。
- en: The IoT applications, especially the social media applications, mandated the
    need to have data processing capabilities where the data fits into a graph-like
    structure. For example, the connections in LinkedIn, relationship between friends
    in Facebook, workflow applications, and many such use cases, make use of the graph
    abstraction extensively. Using the graph to do various computations requires very
    high data processing capabilities and employment of sophisticated algorithms.
    Spark GraphX library comes with an API for graphs and makes use of Spark's parallel
    computing paradigm.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网应用，特别是社交媒体应用，要求具备将数据处理成类似图结构的能力。例如，LinkedIn中的连接、Facebook中朋友之间的关系、工作流应用以及许多此类用例，都广泛使用了图抽象。使用图进行各种计算需要非常高的数据处理能力和复杂的算法。Spark
    GraphX库提供了一个图API，并利用了Spark的并行计算范式。
- en: Tip
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: There are many Spark libraries available that are developed by the community
    for various purposes. Many such third-party library packages are featured in the
    site [http://spark-packages.org/](http://spark-packages.org/). The number of packages
    is growing day by day as the Spark user community is growing. When developing
    data processing applications in Spark, if there is a need to have a domain-specific
    library, it would be a good idea to check this site first and see whether anybody
    has already developed it.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多由社区为各种目的开发的Spark库。许多这样的第三方库包都在网站[http://spark-packages.org/](http://spark-packages.org/)上有所介绍。随着Spark用户社区的增长，这些包的数量也在日益增长。在开发Spark数据处理应用程序时，如果需要一个特定领域的库，首先检查这个网站看看是否已经有人开发了它，这将是一个好主意。
- en: Reference
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考
- en: For more information plese visit:[https://github.com/databricks/spark-csv](https://github.com/databricks/spark-csv)
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息请访问：[https://github.com/databricks/spark-csv](https://github.com/databricks/spark-csv)
- en: Summary
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter discussed the basic programming model of Spark with its primary
    dataset abstraction RDDs. The creation of RDDs from various data sources, and
    processing of the data in RDDs using Spark transformations and Spark actions,
    were covered using Scala and Python APIs. All the important features of the Spark
    programming model were covered with the help of real-world use cases. This chapter
    also discussed the library stack that comes with Spark and what each one is doing.
    In summary, Spark comes with a very user-friendly programming model and in turn
    provides a very powerful data processing toolset.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了Spark的基本编程模型及其主要数据集抽象RDDs。从各种数据源创建RDDs，以及使用Spark转换和Spark操作处理RDDs中的数据，这些内容都通过Scala和Python
    API进行了介绍。所有Spark编程模型的重要特性都通过真实世界的用例进行了讲解。本章还讨论了随Spark一起提供的库栈以及每个库的功能。总之，Spark提供了一个非常用户友好的编程模型，并因此提供了一个非常强大的数据处理工具集。
- en: The next chapter will discuss the Dataset API and the DataFrame API. The Dataset
    API is going to be the new way of programming with Spark, while the DataFrame
    API deals with more structured data. Spark SQL is also introduced to manipulate
    structured data and show how that can be intermixed with any Spark data processing
    application.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论数据集API和数据帧API。数据集API将成为使用Spark编程的新方式，而数据帧API则处理更结构化的数据。Spark SQL也被引入，用于操作结构化数据，并展示如何将其与任何Spark数据处理应用程序混合使用。
