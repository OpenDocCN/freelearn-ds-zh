- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Machine Learning for Bioinformatics
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物信息学中的机器学习
- en: Machine learning is used in a wide variety of contexts and computational biology
    is not an exception. Machine learning has countless applications in the field,
    probably the oldest and most known being the use of **Principal Component Analysis**
    (**PCA**) to study population structure using genomics. There are many other potential
    applications as this is a burgeoning field. In this chapter, we are going to introduce
    machine learning concepts from a bioinformatics perspective.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习在许多不同的领域中都有应用，计算生物学也不例外。机器学习在该领域有着无数的应用，最古老且最为人熟知的应用之一就是使用**主成分分析**（**PCA**）通过基因组学研究种群结构。随着该领域的蓬勃发展，还有许多其他潜在的应用。在本章中，我们将从生物信息学的角度介绍机器学习的概念。
- en: Given that machine learning is a very complex topic that could easily fill a
    book, here we intend to take an intuitive approach that will allow you to broadly
    understand how some machine learning techniques can be useful to tackle biological
    problems. If you find these techniques useful, you will understand the fundamental
    concepts and can proceed to more detailed literature.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于机器学习是一个非常复杂的主题，足以填满一本书，在这里我们打算采用直观的方法，让你大致了解一些机器学习技术如何帮助解决生物学问题。如果你发现这些技术有用，你将理解基本概念，并可以进一步阅读更详细的文献。
- en: If you are using Docker, and because all the libraries in this chapter are fundamental
    for data analysis, they all can be found on the Docker image `tiagoantao/bioinformatics_ml`.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Docker，由于本章中的所有库对于数据分析都是基础库，它们都可以在Docker镜像`tiagoantao/bioinformatics_ml`中找到。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下配方：
- en: Introducing scikit-learn with a PCA example
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用PCA示例介绍scikit-learn
- en: Using clustering over PCA to classify samples
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PCA聚类来分类样本
- en: Exploring breast cancer traits using Decision Trees
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树探索乳腺癌特征
- en: Predicting breast cancer outcomes using Random Forests
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林预测乳腺癌结果
- en: Introducing scikit-learn with a PCA example
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用PCA示例介绍scikit-learn
- en: PCA is a statistical procedure that’s used to perform a reduction of the dimension
    of a number of variables to a smaller subset that is linearly uncorrelated. In
    [*Chapter 6*](B17942_06.xhtml#_idTextAnchor154), we saw a PCA implementation based
    on using an external application. In this recipe, we will implement the same PCA
    for population genetics but will use the `scikit-learn` library. Scikit-learn
    is one of the fundamental Python libraries for machine learning and this recipe
    is an introduction to the library. PCA is a form of unsupervised machine learning
    – we don’t provide information about the class of the sample. We will discuss
    supervised techniques in the other recipes of this chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种统计方法，用于将多个变量的维度降低到一个较小的、线性无关的子集。在[*第6章*](B17942_06.xhtml#_idTextAnchor154)中，我们已经看过基于使用外部应用程序的PCA实现。在这个配方中，我们将为种群遗传学实现相同的PCA，但将使用`scikit-learn`库。Scikit-learn是Python中用于机器学习的基础库之一，本配方是对该库的介绍。PCA是一种无监督的机器学习方法——我们不提供样本的类别信息。我们将在本章的其他配方中讨论有监督的技术。
- en: As a reminder, we will compute PCA for 11 human populations from the HapMap
    project.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，我们将计算来自HapMap项目的11个人类种群的PCA。
- en: Getting ready
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You will need to run the first recipe from [*Chapter 6*](B17942_06.xhtml#_idTextAnchor154)
    in order to generate the `hapmap10_auto_noofs_ld_12` PLINK file (with alleles
    recorded as 1 and 2). From a population genetics perspective, we require LD-pruned
    markers to produce a reliable PCA. We will not risk using the offspring here because
    it would probably bias the result. Our recipe will require the `pygenomics` library,
    which can be installed using the following command:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要运行[*第6章*](B17942_06.xhtml#_idTextAnchor154)中的第一个配方，以生成`hapmap10_auto_noofs_ld_12`
    PLINK文件（等位基因记录为1和2）。从种群遗传学的角度来看，我们需要LD修剪标记来生成可靠的PCA。我们这里不使用后代样本，因为它可能会导致结果偏差。我们的配方将需要`pygenomics`库，可以使用以下命令进行安装：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The code is in the `Chapter10/PCA.py` notebook.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 代码位于`Chapter10/PCA.py`笔记本中。
- en: How to do it...
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Take a look at the following steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下步骤：
- en: 'We start by loading the metadata for our samples. In our case, we will be loading
    the human population that each sample belongs to:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载样本的元数据。在我们的案例中，我们将加载每个样本所属的人类种群：
- en: '[PRE1]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We now get the order of individuals along with the number of SNPs that we will
    be processing:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在获得了个体的顺序以及我们将要处理的SNP数量：
- en: '[PRE2]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We create the array that will be fed to the PCA:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建将要输入PCA的数组：
- en: '[PRE3]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, we compute the PCA with up to eight components. We then get the 8-D
    coordinates for all samples using a `transform` method.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们计算包含最多八个成分的PCA。然后，使用`transform`方法获取所有样本的8维坐标。
- en: '[PRE4]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we plot the PCA:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们绘制PCA图：
- en: '[PRE5]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Figure 10.1 - PC1 to PC8 for our dataset as produced by scikit-learn ](img/B17942_10_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 - 由scikit-learn生成的我们数据集的PC1到PC8](img/B17942_10_01.jpg)'
- en: Figure 10.1 - PC1 to PC8 for our dataset as produced by scikit-learn
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 - 由scikit-learn生成的我们数据集的PC1到PC8
- en: There’s more...
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: For publication in scientific journals, I would recommend using the recipe in
    [*Chapter 6*](B17942_06.xhtml#_idTextAnchor154), simply because it’s based on
    a published and highly regarded method. That being said, the results from this
    code are qualitatively similar and cluster data in a very similar fashion (the
    inversion of direction on the vertical axis, if you compare it with the figure
    in [*Chapter 6*](B17942_06.xhtml#_idTextAnchor154), is irrelevant when interpreting
    a PCA chart).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于科学期刊中的发布，我建议使用[*第6章*](B17942_06.xhtml#_idTextAnchor154)中的食谱，因为它基于一个已发布的、广受好评的方法。也就是说，这段代码的结果在定性上是相似的，并且以非常类似的方式对数据进行了聚类（如果你与[*第6章*](B17942_06.xhtml#_idTextAnchor154)中的图进行比较，垂直轴方向的反转对于解读PCA图表而言是无关紧要的）。
- en: Using clustering over PCA to classify samples
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PCA进行聚类来对样本进行分类
- en: PCA in genomics allows us to see how samples cluster. In many cases, individuals
    from the same population will be in the same area of the chart. But we would like
    to go further and predict where new individuals fall in terms of populations.
    To do that, we will start with PCA data, as it does dimensionality reduction –
    making working with the data easier – and then apply a K-Means clustering algorithm
    to predict where new samples fall. We will use the same dataset as in the recipe
    above. We will use all our samples save one to train the algorithm, and then we
    will predict where the remaining sample falls.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基因组学中的PCA可以让我们看到样本如何聚类。在许多情况下，同一群体的个体会聚集在图表的同一区域。但我们希望进一步预测新个体在群体中的位置。为此，我们将从PCA数据开始，因为它进行了降维处理—使得处理数据更为简便—然后应用K-Means聚类算法来预测新样本的位置。我们将使用与上述食谱相同的数据集。我们将使用除了一个样本外的所有样本来训练算法，然后预测剩下的样本位置。
- en: K-Means clustering can be an example of a supervised algorithm. In these types
    of algorithms, we need a training dataset so that the algorithm is able to learn.
    After training the algorithm, it will be able to predict a certain outcome for
    new samples. In our case, we are hoping that we can predict the population.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means聚类可以是监督算法的一个例子。在这类算法中，我们需要一个训练数据集，以便算法能够学习。训练算法之后，它将能够对新样本预测某个结果。在我们的案例中，我们希望能够预测群体。
- en: WARNING
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: The current recipe intends to serve as a gentle introduction to supervised algorithms
    and the concepts behind them. The way we are training the algorithm is far from
    optimal. The issue of properly training a supervised algorithm will be alluded
    to in the last recipe of this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的食谱旨在作为对监督算法及其背后概念的温和介绍。我们训练算法的方式远非最佳。关于如何正确训练一个监督算法的问题将在本章最后一个食谱中提到。
- en: Getting ready
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will be using the same data as in the previous recipe. The code for this
    recipe can be found in `Chapter10/Clustering.py`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与之前食谱中相同的数据。此食谱的代码可以在`Chapter10/Clustering.py`中找到。
- en: How to do it...
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let’s have a look:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下：
- en: 'We start by loading the population information – this is similar to what we
    did in the previous recipe:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载群体信息——这与我们在之前的食谱中所做的相似：
- en: '[PRE6]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We now load all sample data – SNPs – into a NumPy array:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将所有样本数据（SNPs）加载到一个NumPy数组中：
- en: '[PRE7]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We separate the array into two datasets, namely, a training case with all individuals
    except one, and a case to test with a single individual:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数组分成两个数据集，即包含所有个体（除一个外）作为训练集，和单个个体用于测试的案例：
- en: '[PRE8]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Our test case is individual Y076/NA19124, who we know belongs to the Yoruban
    population.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试案例是个体Y076/NA19124，我们知道他属于约鲁巴族群体。
- en: 'We now compute the PCA for the training set that we will use for K-Means clustering:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们计算用于K-Means聚类的训练集的PCA：
- en: '[PRE9]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is the output, which will be useful to check clustering results:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果，将有助于检查聚类结果：
- en: '![Figure 10.2 - PC1 and PC2 with populations color-coded ](img/B17942_10_02.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 - 具有群体色彩编码的PC1和PC2](img/B17942_10_02.jpg)'
- en: Figure 10.2 - PC1 and PC2 with populations color-coded
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 - PC1和PC2与按种群编码的颜色
- en: 'Before we start computing K-means clustering, let’s write a function to plot
    the clustering surface from running the algorithm:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们开始计算K均值聚类之前，先写一个函数来绘制运行算法后的聚类面：
- en: '[PRE10]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s now fit the algorithm with our samples. Because we have 11 populations,
    we will train for 11 clusters:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们用我们的样本来拟合算法。因为我们有11个人群，我们将训练11个簇：
- en: '[PRE11]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here is the output:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '![Figure 10.3 - The cluster surface for 11 clusters ](img/B17942_10_03.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 - 11个簇的簇面](img/B17942_10_03.jpg)'
- en: Figure 10.3 - The cluster surface for 11 clusters
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 - 11个簇的簇面
- en: 'If you compare with the figure here, you can intuitively see that the clustering
    makes little sense: it doesn’t map to the known populations very well. One could
    argue that this clustering algorithm with 11 clusters is not very useful.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你与这里的图进行比较，你会直观地看到这个聚类没有多大意义：它并没有很好地映射到已知的人群上。有人可能会认为，使用11个簇的聚类算法并不是很有用。
- en: TIP
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: There are many other clustering algorithms implemented in scikit-learn, and
    in several scenarios, they might perform better than K-means. You can find them
    at [https://scikit-learn.org/stable/modules/clustering.xhtml](https://scikit-learn.org/stable/modules/clustering.xhtml).
    It is doubtful that, in this specific case, any alternative would perform much
    better for 11 clusters.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中实现了许多其他聚类算法，在多种情况下，它们的表现可能优于K均值聚类。你可以在[https://scikit-learn.org/stable/modules/clustering.xhtml](https://scikit-learn.org/stable/modules/clustering.xhtml)找到它们。值得怀疑的是，在这个特定的案例中，任何替代方法都不太可能在11个簇的情况下表现得更好。
- en: While it seems K-means clustering cannot resolve the 11 populations, maybe it
    can still provide some predictions if we use a different number of clusters. Simply
    by looking at the chart, we see four separate blocks. What would be the result
    if we used four clusters?
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管看起来K均值聚类无法解决11个人群的划分，但如果我们使用不同数量的簇，或许它仍然可以提供一些预测。仅通过查看图表，我们可以看到四个独立的块。如果我们使用四个簇，会得到什么结果呢？
- en: '[PRE12]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here is the output:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '![Figure 10.4 - The cluster surface for four clusters ](img/B17942_10_04.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4 - 四个簇的簇面](img/B17942_10_04.jpg)'
- en: Figure 10.4 - The cluster surface for four clusters
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 - 四个簇的簇面
- en: The four groups are now mostly clear. But do they make intuitive sense? If they
    do, we can make use of this clustering approach. And in fact, they do. The cluster
    on the left is composed of African populations, the top cluster European ones,
    and the bottom one East Asians. The middle one is the most cryptic as it contains
    both Gujarati and Mexican descendants, but that mix comes originally from PCA
    and it is not caused by clustering itself.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 四个群体现在大致清晰了。但它们直观上有意义吗？如果有，那么我们可以利用这种聚类方法。事实上，它们确实有意义。左侧的簇由非洲人口组成，顶部的簇由欧洲人组成，底部的簇由东亚人组成。中间的簇最为难以理解，因为它包含了古吉拉特人和墨西哥后裔，但这种混合最初来自于主成分分析（PCA），而非聚类本身所致。
- en: 'Let’s see how prediction does for the single case we left out:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看我们漏掉的那个单独个案的预测结果：
- en: '[PRE13]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Our sample is predicted to be in cluster 1\. We need to dig a little deeper
    now.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的样本被预测为属于簇1。现在我们需要进一步挖掘一下。
- en: 'Let’s find out what cluster 1 means. We take the last individual from the training
    set, who is also a Yoruba, and see to which cluster he is allocated:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看簇1代表什么。我们取训练集中的最后一个个体，他也是一名约鲁巴人，看看他被分配到了哪个簇：
- en: '[PRE14]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It is indeed cluster 1, so the prediction is correct.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 确实是簇1，因此预测是正确的。
- en: There’s more...
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: It is worth reiterating that we are trying to achieve an intuitive understanding
    of machine learning. At this stage, you should have a grasp of what you can gain
    from supervised learning, and also example usage of a clustering algorithm. There
    is much more to be said about the procedure to train a machine learning algorithm,
    something that we will partially unveil in the last recipe.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 值得重申的是，我们正在努力实现对机器学习的直观理解。在这个阶段，你应该对监督学习能够带来什么有所了解，并且已经掌握了一个聚类算法的使用示例。关于训练机器学习算法的过程，还有很多内容值得探讨，我们将在最后的食谱中部分揭示。
- en: Exploring breast cancer traits using Decision Trees
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树探索乳腺癌特征
- en: One of the first problems that we have when we receive a dataset is deciding
    what to start analyzing. At the very beginning, there is quite often a feeling
    of loss about what to do first. Here, we will present an exploratory approach
    based on Decision Trees. The big advantage of Decision Trees is that they will
    give us the rules that constructed the decision tree, allowing us a first tentative
    understanding of what is going on with our data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们接收到一个数据集时，第一个问题之一就是决定从哪里开始分析。一开始，往往会有一种迷茫的感觉，不知道该先做什么。这里，我们将展示基于决策树的探索性方法。决策树的最大优点是，它们会给我们提供构建决策树的规则，让我们初步了解数据的情况。
- en: In this example, we will be using a dataset with trait observations from patients
    with breast cancer. The dataset with 699 data entries includes information such
    as clump thickness, uniformity of cell size, or type of chromatin. The outcome
    is either a benign or malignant tumor. The features are encoded with values from
    0 to 10\. More information about the project can be found at [http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用一个包含乳腺癌患者特征观察的数据集。该数据集包含699条数据，包含如肿块厚度、细胞大小的均匀性或染色质类型等信息。结果是良性或恶性肿瘤。特征值被编码为0到10之间的值。关于该项目的更多信息可以在[http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29)中找到。
- en: Getting ready
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We are going to download the data along with the documentation:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将下载数据及其文档：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The data file is formatted as a CSV file. Information about the content can
    be found in the second downloaded file.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数据文件的格式为CSV文件。关于内容的信息可以在第二个下载的文件中找到。
- en: The code for this recipe can be found in `Chapter10/Decision_Tree.py`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码可以在`Chapter10/Decision_Tree.py`中找到。
- en: How to do it...
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Follow these steps:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: 'The first thing we do is to remove a small fraction of individuals that have
    incomplete data:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们做的第一件事是移除一小部分数据不完整的个体：
- en: '[PRE16]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: TIP
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Removing individuals with incomplete data is adequate in this case because they
    are a small fraction of the dataset, and we are only doing exploratory analysis.
    For cases with lots of missingness or when we are trying to do something more
    rigorous, you will have to use methods to deal with missing data, which we will
    not explore here.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 移除数据不完整的个体在这种情况下是合适的，因为它们只是数据集的一小部分，而且我们只是进行探索性分析。如果数据缺失很多，或者我们需要做更严格的分析，你需要使用处理缺失数据的方法，但我们这里不会进行探讨。
- en: 'We are now going to read the data, giving names to all columns:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将读取数据，为所有列命名：
- en: '[PRE17]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will now separate the features from the outcome and recode the outcome using
    0 and 1:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将特征与结果分离，并使用0和1重新编码结果：
- en: '[PRE18]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s now create a Decision Tree based on this data with a max depth of 3:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们基于这些数据创建一个最大深度为3的决策树：
- en: '[PRE19]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let’s start by seeing which features are the most important:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先看看哪些特征最重要：
- en: '[PRE20]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following are the features ranked by importance:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是按重要性排名的特征：
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Remember that this is just exploratory analysis. In the next recipe, we will
    try to produce more reliable rankings. The reason why the bottom features are
    zero is we asked for a max depth of 3 and in that case, it is possible that not
    all features are used.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这只是探索性分析。在下一个配方中，我们将尝试生成更可靠的排名。底部特征为零的原因是我们要求最大深度为3，在这种情况下，可能并非所有特征都会被使用。
- en: 'We can do some native analysis of the accuracy of our implementation:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以对实现的准确性进行一些原生分析：
- en: '[PRE22]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We get a performance of 96%. We shouldn’t be testing the algorithm with its
    own training set as this is quite circular. We will revisit that in the next recipe.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的性能为96%。我们不应该用训练集测试算法，因为这会导致结果较为循环。我们将在下一个配方中重新审视这一点。
- en: 'Finally, let’s plot the decision tree:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们绘制决策树：
- en: '[PRE23]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This results in the following output:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![Figure 10.5 - The decision tree for the breast cancer dataset ](img/B17942_10_05.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.5 - 乳腺癌数据集的决策树](img/B17942_10_05.jpg)'
- en: Figure 10.5 - The decision tree for the breast cancer dataset
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 - 乳腺癌数据集的决策树
- en: 'Let’s look at the root node to start: it has a criterion of `uniformity_cell_size
    < 2.5` and a classification of benign. The main feature of splitting the tree
    is the uniformity of the cell size. The classification of benign at the top node
    comes simply from the fact that most samples on the dataset are benign. Now look
    at the right node from the root: it has 265 samples, most of which are malignant
    and with criteria of `uniformity_cell_shape < 2.5`.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从根节点开始看：它的标准是`uniformity_cell_size < 2.5`，分类为良性。分裂树的主要特征是细胞大小的一致性。根节点的良性分类仅仅是因为数据集中的大多数样本都是良性的。现在看一下根节点右侧的节点：它有265个样本，其中大部分是恶性的，并且标准为`uniformity_cell_shape
    < 2.5`。
- en: These rules allow you to have an initial understanding of what might be driving
    the dataset. Decision Trees are not very precise, so take these as your initial
    step.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则帮助你初步理解可能驱动数据集的因素。决策树的精确度不是很高，所以将这些视为你的初步步骤。
- en: Predicting breast cancer outcomes using Random Forests
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林预测乳腺癌结果
- en: 'We are now going to predict the outcomes for some patients using Random Forests.
    A random forest is an ensemble method (it will use several instances of other
    machine learning algorithms) that uses many decision trees to arrive at robust
    conclusions about the data. We are going to use the same example as in the previous
    recipe: breast cancer traits and outcomes.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用随机森林预测一些患者的结果。随机森林是一种集成方法（它将使用多个机器学习算法的实例），通过多个决策树得出关于数据的稳健结论。我们将使用与前一个例子相同的例子：乳腺癌特征和结果。
- en: 'This recipe has two main goals: to introduce you to random forests and issues
    regarding the training of machine learning algorithms.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子有两个主要目标：介绍随机森林及机器学习算法训练中的问题。
- en: Getting ready
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: The code for this recipe can be found in `Chapter10/Random_Forest.py`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子的代码可以在`Chapter10/Random_Forest.py`找到。
- en: How to do it…
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Take a look at the code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下代码：
- en: 'We start, as in the previous recipe, by getting rid of samples with missing
    information:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们像前一个例子一样，首先去除缺失信息的样本：
- en: '[PRE24]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We now load the cleaned data:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们加载清理过的数据：
- en: '[PRE25]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We separate the data read in features and outcomes:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据分为特征和结果：
- en: '[PRE26]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We create a classifier and fit the data to it:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个分类器并将数据拟合到它：
- en: '[PRE27]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The most important parameter here is `n_estimators`: we are requesting the
    Forest be constructed with 200 trees.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最重要的参数是`n_estimators`：我们要求构建一个由200棵树组成的森林。
- en: 'We now rank the features in order of importance:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们按重要性对特征进行排序：
- en: '[PRE28]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is the output:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE29]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The result is non-deterministic, meaning that you might have different results.
    Also, note that the Random Forest has quite different numbers than the Decision
    Tree of the previous recipe. This is to be expected as the Decision Tree is a
    single estimator where the Forest weighs 200 trees and is more reliable.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是非确定性的，意味着你可能会得到不同的结果。另外，请注意，随机森林与前一个例子中的决策树有很大的不同。这是预期之中的，因为决策树是一个单一的估计器，而随机森林权衡了200棵树，因此更加可靠。
- en: 'We can score this case:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以对这个案例进行评分：
- en: '[PRE30]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: I get a result of 97.95%. You might get a slightly different value as the algorithm
    is stochastic. As we said in the previous recipe, getting a score from the training
    set is quite circular and far from best practice.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我得到的结果是97.95%。你可能会得到稍微不同的值，因为算法是随机的。正如我们在前一个例子中所说，从训练集获取得分是一个循环过程，远非最佳实践。
- en: 'In order to have a more realistic view of the accuracy of the algorithm, we
    need to separate our data into two parts – a training set and a test set:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更真实地了解算法的准确性，我们需要将数据分为两部分——训练集和测试集：
- en: '[PRE31]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is the following (remember that you will get different values):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下（记住你会得到不同的值）：
- en: '[PRE32]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: If you only train with 1% of the data, you only get 71% accuracy, whereas if
    you train with more, the accuracy goes above 90%. Note that accuracy does not
    monotonically increase with the size of the training set. Deciding on the size
    of the training set is a complex affair with various issues causing unexpected
    side effects.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仅用1%的数据进行训练，你的准确率只有71%，而如果你使用更多的数据，准确率会超过90%。注意，准确率并不随着训练集的大小单调增加。决定训练集的大小是一个复杂的问题，涉及许多因素，可能会导致意想不到的副作用。
- en: There’s more...
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We only scratched the surface of training and testing machine learning algorithms.
    For example, supervised datasets are normally split into 3, not 2 (training, test,
    and cross-validation). There are many more issues that you need to consider in
    order to train your algorithm and many more types of algorithms. In this chapter,
    we tried to develop basic intuition to understand machine learning, but this is
    nothing more than your starting point if you intend to follow this route.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅仅触及了训练和测试机器学习算法的表面。例如，监督数据集通常会被分成3部分，而不是2部分（训练、测试和交叉验证）。在训练算法时，你需要考虑许多其他问题，还有更多种类的算法。在本章中，我们试图培养理解机器学习的基本直觉，但如果你打算继续这条路线，这只是你的起点。
