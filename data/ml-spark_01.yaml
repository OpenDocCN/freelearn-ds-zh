- en: Getting Up and Running with Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始并运行Spark
- en: '**Apache Spark** is a framework for distributed computing; this framework aims
    to make it simpler to write programs that run in parallel across many nodes in
    a cluster of computers or virtual machines. It tries to abstract the tasks of
    resource scheduling, job submission, execution, tracking, and communication between
    nodes as well as the low-level operations that are inherent in parallel data processing.
    It also provides a higher level API to work with distributed data. In this way,
    it is similar to other distributed processing frameworks such as Apache Hadoop;
    however, the underlying architecture is somewhat different.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Spark** 是一个分布式计算框架；该框架旨在简化编写并行程序的过程，这些程序可以在计算机集群或虚拟机中的许多节点上并行运行。它试图将资源调度、作业提交、执行、跟踪和节点之间的通信以及并行数据处理中固有的低级操作抽象出来。它还提供了一个更高级的API来处理分布式数据。因此，它类似于其他分布式处理框架，如Apache
    Hadoop；但是，底层架构有所不同。'
- en: Spark began as a research project at the AMP lab in University of California,
    Berkeley ([https://amplab.cs.berkeley.edu/projects/spark-lightning-fast-cluster-computing/](https://amplab.cs.berkeley.edu/projects/spark-lightning-fast-cluster-computing/)).
    The university was focused on the use case of distributed machine learning algorithms.
    Hence, it is designed from the ground up for high performance in applications
    of an iterative nature, where the same data is accessed multiple times. This performance
    is achieved primarily through caching datasets in memory combined with low latency
    and overhead to launch parallel computation tasks. Together with other features
    such as fault tolerance, flexible distributed-memory data structures, and a powerful
    functional API, Spark has proved to be broadly useful for a wide range of large-scale
    data processing tasks, over and above machine learning and iterative analytics.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Spark最初是加州大学伯克利分校（[https://amplab.cs.berkeley.edu/projects/spark-lightning-fast-cluster-computing/](https://amplab.cs.berkeley.edu/projects/spark-lightning-fast-cluster-computing/)）的AMP实验室的一个研究项目。该大学专注于分布式机器学习算法的用例。因此，它从头开始设计，以实现迭代性质的高性能，其中相同的数据被多次访问。这种性能主要通过将数据集缓存在内存中并结合低延迟和启动并行计算任务的开销来实现。除了容错性、灵活的分布式内存数据结构和强大的函数API等其他特性，Spark已被证明在大规模数据处理任务中广泛适用，超越了机器学习和迭代分析。
- en: 'For more information, you can visit:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请访问：
- en: '[http://spark.apache.org/community.html](http://spark.apache.org/community.html)'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/community.html](http://spark.apache.org/community.html)'
- en: '[http://spark.apache.org/community.html#history](http://spark.apache.org/community.html#history)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/community.html#history](http://spark.apache.org/community.html#history)'
- en: 'Performance wise, Spark is much faster than Hadoop for related workloads. Refer
    to the following graph:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能方面，Spark在相关工作负载方面比Hadoop快得多。请参考以下图表：
- en: '![](img/image_01_001.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_001.png)'
- en: 'Source: https://amplab.cs.berkeley.edu/wp-content/uploads/2011/11/spark-lr.png'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：https://amplab.cs.berkeley.edu/wp-content/uploads/2011/11/spark-lr.png
- en: 'Spark runs in four modes:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有四种运行模式：
- en: The standalone local mode, where all Spark processes are run within the same
    **Java Virtual Machine** (**JVM**) process
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立的本地模式，其中所有Spark进程都在同一个**Java虚拟机**（**JVM**）进程中运行
- en: The standalone cluster mode, using Spark's own built-in, job-scheduling framework
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark自带的作业调度框架，设置独立集群模式
- en: Using **Mesos**, a popular open source cluster-computing framework
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Mesos**，一个流行的开源集群计算框架
- en: Using YARN (commonly referred to as NextGen MapReduce), Hadoop
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用YARN（通常称为NextGen MapReduce），Hadoop
- en: 'In this chapter, we will do the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将进行以下操作：
- en: Download the Spark binaries and set up a development environment that runs in
    Spark's standalone local mode. This environment will be used throughout the book
    to run the example code.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载Spark二进制文件并设置在Spark的独立本地模式下运行的开发环境。本环境将在整本书中用于运行示例代码。
- en: Explore Spark's programming model and API using Spark's interactive console.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark的交互式控制台来探索Spark的编程模型和API。
- en: Write our first Spark program in Scala, Java, R, and Python.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Scala、Java、R和Python中编写我们的第一个Spark程序。
- en: Set up a Spark cluster using Amazon's **Elastic Cloud Compute** (**EC2**) platform,
    which can be used for large-sized data and heavier computational requirements,
    rather than running in the local mode.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用亚马逊的**弹性云计算**（**EC2**）平台设置Spark集群，该平台可用于大型数据和更重的计算需求，而不是在本地模式下运行。
- en: Set up a Spark Cluster using Amazon Elastic Map Reduce
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用亚马逊弹性Map Reduce设置Spark集群
- en: If you have previous experience in setting up Spark and are familiar with the
    basics of writing a Spark program, feel free to skip this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您之前有设置Spark的经验并且熟悉编写Spark程序的基础知识，可以跳过本章。
- en: Installing and setting up Spark locally
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地安装和设置Spark
- en: Spark can be run using the built-in standalone cluster scheduler in the local
    mode. This means that all the Spark processes are run within the same JVM-effectively,
    a single, multithreaded instance of Spark. The local mode is very used for prototyping,
    development, debugging, and testing. However, this mode can also be useful in
    real-world scenarios to perform parallel computation across multiple cores on
    a single computer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以在本地模式下使用内置的独立集群调度程序运行。这意味着所有Spark进程都在同一个JVM内运行，实际上是Spark的单个多线程实例。本地模式非常适用于原型设计、开发、调试和测试。然而，这种模式在实际场景中也可以用于在单台计算机的多个核心上执行并行计算。
- en: As Spark's local mode is fully compatible with the cluster mode; programs written
    and tested locally can be run on a cluster with just a few additional steps.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark的本地模式与集群模式完全兼容；在本地编写和测试的程序只需进行一些额外的步骤即可在集群上运行。
- en: The first step in setting up Spark locally is to download the latest version
    [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html),
    which contains links to download various versions of Spark as well as to obtain
    the latest source code via GitHub.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地设置Spark的第一步是下载最新版本[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)，其中包含下载各种版本的Spark的链接，以及通过GitHub获取最新源代码的链接。
- en: The documents/docs available at [http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)
    are a comprehensive resource to learn more about Spark. We highly recommend that
    you explore it!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)提供的文档/文档是了解Spark的全面资源。我们强烈建议您探索一下！'
- en: Spark needs to be built against a specific version of Hadoop in order to access
    **Hadoop Distributed File System** (**HDFS**) as well as standard and custom Hadoop
    input sources Cloudera's Hadoop Distribution, MapR's Hadoop distribution, and
    Hadoop 2 (YARN). Unless you wish to build Spark against a specific Hadoop version,
    we recommend that you download the prebuilt Hadoop 2.7 package from an Apache
    mirror from [http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.7.tgz](http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.7.tgz).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了访问**Hadoop分布式文件系统**（**HDFS**）以及标准和自定义Hadoop输入源Cloudera的Hadoop分发、MapR的Hadoop分发和Hadoop
    2（YARN），Spark需要根据特定版本的Hadoop构建。除非您希望根据特定的Hadoop版本构建Spark，我们建议您从[http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.7.tgz](http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.7.tgz)的Apache镜像下载预构建的Hadoop
    2.7包。
- en: Spark requires the Scala programming language (version 2.10.x or 2.11.x at the
    time of writing this book) in order to run. Fortunately, the prebuilt binary package
    comes with the Scala runtime packages included, so you don't need to install Scala
    separately in order to get started. However, you will need to have a **Java Runtime
    Environment** (**JRE**) or **Java Development Kit** (**JDK**).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，Spark需要Scala编程语言（版本为2.10.x或2.11.x）才能运行。幸运的是，预构建的二进制包包含了Scala运行时包，因此您无需单独安装Scala即可开始。但是，您需要安装**Java运行环境**（**JRE**）或**Java开发工具包**（**JDK**）。
- en: Refer to the software and hardware list in this book's code bundle for installation
    instructions. R 3.1+ is needed.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有关安装说明，请参考本书代码包中的软件和硬件列表。需要R 3.1+。
- en: 'Once you have downloaded the Spark binary package, unpack the contents of the
    package and change it to the newly created directory by running the following
    commands:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下载Spark二进制包后，通过运行以下命令解压包的内容并切换到新创建的目录：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Spark places user scripts to run Spark in the `bin` directory. You can test
    whether everything is working correctly by running one of the example programs
    included in Spark. Run the following command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Spark将用户脚本放置在`bin`目录中以运行Spark。您可以通过运行Spark中包含的示例程序之一来测试一切是否正常工作。运行以下命令：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will run the example in Spark''s local standalone mode. In this mode,
    all the Spark processes are run within the same JVM, and Spark uses multiple threads
    for parallel processing. By default, the preceding example uses a number of threads
    equal to the number of cores available on your system. Once the program is executed,
    you should see something similar to the following lines toward the end of the
    output:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在Spark的本地独立模式下运行示例。在此模式下，所有Spark进程都在同一个JVM中运行，并且Spark使用多个线程进行并行处理。默认情况下，上面的示例使用的线程数等于系统上可用的核心数。程序执行完毕后，您应该看到输出的末尾类似于以下行：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding command calls class `org.apache.spark.examples.SparkPi` class.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令调用`org.apache.spark.examples.SparkPi`类。
- en: This class takes parameter in the `local[N]` form, where `N` is the number of
    threads to use. For example, to use only two threads, run the following command
    `instead:N` is the number of threads to use. Giving `local[*]` will use all of
    the cores on the local machine--that is a common usage.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此类以`local[N]`形式接受参数，其中`N`是要使用的线程数。例如，要仅使用两个线程，请运行以下命令`instead:N`是要使用的线程数。给定`local[*]`将使用本地机器上的所有核心--这是常见用法。
- en: 'To use only two threads, run the following command instead:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要仅使用两个线程，请运行以下命令：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Spark clusters
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark集群
- en: 'A Spark cluster is made up of two types of processes: a driver program and
    multiple executors. In the local mode, all these processes are run within the
    same JVM. In a cluster, these processes are usually run on separate nodes.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Spark集群由两种类型的进程组成：驱动程序和多个执行器。在本地模式下，所有这些进程都在同一个JVM中运行。在集群中，这些进程通常在单独的节点上运行。
- en: 'For example, a typical cluster that runs in Spark''s standalone mode (that
    is, using Spark''s built-in cluster management modules) will have the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，运行在Spark独立模式下的典型集群（即使用Spark内置的集群管理模块）将具有以下内容：
- en: A master node that runs the Spark standalone master process as well as the driver
    program
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Spark独立主进程和驱动程序的主节点
- en: A number of worker nodes, each running an executor process
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个工作节点，每个节点运行一个执行器进程
- en: 'While we will be using Spark''s local standalone mode throughout this book
    to illustrate concepts and examples, the same Spark code that we write can be
    run on a Spark cluster. In the preceding example, if we run the code on a Spark
    standalone cluster, we could simply pass in the URL for the master node, as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在本书中，我们将使用Spark的本地独立模式来说明概念和示例，但我们编写的相同Spark代码可以在Spark集群上运行。在上面的示例中，如果我们在Spark独立集群上运行代码，我们可以简单地传递主节点的URL，如下所示：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, `IP` is the IP address and `PORT` is the port of the Spark master. This
    tells Spark to run the program on the cluster where the Spark master process is
    running.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`IP`是Spark主节点的IP地址，`PORT`是端口。这告诉Spark在运行Spark主进程的集群上运行程序。
- en: A full treatment of Spark's cluster management and deployment is beyond the
    scope of this book. However, we will briefly teach you how to set up and use an
    Amazon EC2 cluster later in this chapter.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的集群管理和部署的全面处理超出了本书的范围。但是，我们将简要教您如何在本章后面设置和使用Amazon EC2集群。
- en: 'For an overview of the Spark cluster-application deployment, take a look at
    the following links:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Spark集群应用部署的概述，请查看以下链接：
- en: '[http://spark.apache.org/docs/latest/cluster-overview.html](http://spark.apache.org/docs/latest/cluster-overview.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/cluster-overview.html](http://spark.apache.org/docs/latest/cluster-overview.html)'
- en: '[http://spark.apache.org/docs/latest/submitting-applications.html](http://spark.apache.org/docs/latest/submitting-applications.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/submitting-applications.html](http://spark.apache.org/docs/latest/submitting-applications.html)'
- en: The Spark programming model
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark编程模型
- en: Before we delve into a high-level overview of Spark's design, we will introduce
    the `SparkContext` object as well as the Spark shell, which we will use to interactively
    explore the basics of the Spark programming model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解Spark设计的高级概述之前，我们将介绍`SparkContext`对象以及Spark shell，我们将使用它们来交互式地探索Spark编程模型的基础知识。
- en: 'While this section provides a brief overview and examples of using Spark, we
    recommend that you read the following documentation to get a detailed understanding:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本节提供了对Spark的简要概述和示例，但我们建议您阅读以下文档以获得详细的理解：
- en: 'Refer to the following URLs:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅以下URL：
- en: For the Spark Quick Start refer to, [http://spark.apache.org/docs/latest/quick-start](http://spark.apache.org/docs/latest/quick-start)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关Spark快速入门，请参阅[http://spark.apache.org/docs/latest/quick-start](http://spark.apache.org/docs/latest/quick-start)
- en: For the Spark Programming guide, which covers Scala, Java, Python and R--, refer
    to, [http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关Spark编程指南，涵盖Scala、Java、Python和R--，请参阅[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html)
- en: SparkContext and SparkConf
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkContext和SparkConf
- en: The starting point of writing any Spark program is `SparkContext` (or `JavaSparkContext`
    in Java). `SparkContext` is initialized with an instance of a `SparkConf` object,
    which contains various Spark cluster-configuration settings (for example, the
    URL of the master node).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 编写任何Spark程序的起点是`SparkContext`（在Java中为`JavaSparkContext`）。`SparkContext`使用包含各种Spark集群配置设置的`SparkConf`对象的实例进行初始化（例如，主节点的URL）。
- en: It is a main entry point for Spark functionality. A `SparkContext` is a connection
    to a Spark cluster. It can be used to create RDDs, accumulators, and broadcast
    variables on the cluster.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Spark功能的主要入口点。`SparkContext`是与Spark集群的连接。它可以用于在集群上创建RDD、累加器和广播变量。
- en: Only one `SparkContext` is active per JVM. You must call `stop()`, which is
    the active `SparkContext`, before creating a new one.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 每个JVM只能有一个活动的`SparkContext`。在创建新的`SparkContext`之前，必须调用`stop()`来停止活动的`SparkContext`。
- en: 'Once initialized, we will use the various methods found in the `SparkContext`
    object to create and manipulate distributed datasets and shared variables. The
    Spark shell (in both Scala and Python, which is unfortunately not supported in
    Java) takes care of this context initialization for us, but the following lines
    of code show an example of creating a context running in the local mode in Scala:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化后，我们将使用`SparkContext`对象中的各种方法来创建和操作分布式数据集和共享变量。Spark shell（在Scala和Python中，遗憾的是Java不支持）会为我们处理这个上下文的初始化，但以下代码示例展示了在Scala中创建本地模式下运行的上下文的示例：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This creates a context running in the local mode with four threads, with the
    name of the application set to `Test Spark App`. If we wish to use the default
    configuration values, we could also call the following simple constructor for
    our `SparkContext` object, which works in the exact same way:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个在本地模式下运行的上下文，使用四个线程，应用程序的名称设置为`Test Spark App`。如果我们希望使用默认配置值，我们也可以调用`SparkContext`对象的以下简单构造函数，它的工作方式完全相同：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Downloading the example code
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下载示例代码
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book from any other source, you can visit[http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从您在[http://www.packtpub.com](http://www.packtpub.com)的帐户中下载您购买的所有Packt图书的示例代码文件。如果您从其他来源购买了本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便直接通过电子邮件接收文件。
- en: SparkSession
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkSession
- en: '`SparkSession` allows programming with the `DataFrame` and Dataset APIs. It
    is a single point of entry for these APIs.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession`允许使用`DataFrame`和Dataset API进行编程。它是这些API的唯一入口点。'
- en: 'First, we need to create an instance of the `SparkConf` class and use it to
    create the `SparkSession` instance. Consider the following example:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建`SparkConf`类的实例，并使用它创建`SparkSession`实例。考虑以下示例：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next we can use spark object to create a `DataFrame`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用spark对象来创建一个`DataFrame`：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The Spark shell
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark shell
- en: Spark supports writing programs interactively using the Scala, Python, or R
    **REPL** (that is, the **Read-Eval-Print-Loop**, or interactive shell). The shell
    provides instant feedback as we enter code, as this code is immediately evaluated.
    In the Scala shell, the return result and type is also displayed after a piece
    of code is run.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持使用Scala、Python或R的**REPL**（即**Read-Eval-Print-Loop**，或交互式shell）进行交互式编程。当我们输入代码时，shell会立即提供反馈，因为该代码会立即被评估。在Scala
    shell中，运行一段代码后还会显示返回结果和类型。
- en: To use the Spark shell with Scala, simply run `./bin/spark-shell` from the Spark
    base directory. This will launch the Scala shell and initialize `SparkContext`,
    which is available to us as the Scala value, `sc`. With Spark 2.0, a `SparkSession`
    instance in the form of Spark variable is available in the console as well.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Scala中使用Spark shell，只需从Spark基本目录运行`./bin/spark-shell`。这将启动Scala shell并初始化`SparkContext`，作为Scala值`sc`对我们可用。在Spark
    2.0中，`SparkSession`实例以Spark变量的形式也在控制台中可用。
- en: 'Your console output should look similar to the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您的控制台输出应该类似于以下内容：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To use the Python shell with Spark, simply run the `./bin/pyspark` command.
    Like the Scala shell, the Python `SparkContext` object should be available as
    the Python variable, `sc`. Your output should be similar to this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Python shell与Spark一起使用，只需运行`./bin/pyspark`命令。与Scala shell一样，Python的`SparkContext`对象应该作为Python变量`sc`可用。您的输出应该类似于这样：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**R** is a language and has a runtime environment for statistical computing
    and graphics. It is a GNU project. R is a different implementation of **S** (a
    language developed by Bell Labs).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**R**是一种语言，具有用于统计计算和图形的运行时环境。它是GNU项目。R是**S**的另一种实现（由贝尔实验室开发的语言）。'
- en: R provides statistical (linear and nonlinear modeling, classical statistical
    tests, time-series analysis, classification, and clustering) and graphical techniques.
    It is considered to be highly extensible.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: R提供了统计（线性和非线性建模、经典统计测试、时间序列分析、分类和聚类）和图形技术。它被认为是高度可扩展的。
- en: 'To use Spark using R, run the following command to open Spark-R shell:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用R来使用Spark，运行以下命令打开Spark-R shell：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Resilient Distributed Datasets
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Resilient Distributed Datasets
- en: The core of Spark is a concept called the **Resilient Distributed Dataset**
    (**RDD**). An RDD is a collection of *records* (strictly speaking, objects of
    some type) that are distributed or partitioned across many nodes in a cluster
    (for the purposes of the Spark local mode, the single multithreaded process can
    be thought of in the same way). An RDD in Spark is fault-tolerant; this means
    that if a given node or task fails (for some reason other than erroneous user
    code, such as hardware failure, loss of communication, and so on), the RDD can
    be reconstructed automatically on the remaining nodes and the job will still be
    completed.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的核心是一个叫做**Resilient Distributed Dataset**（**RDD**）的概念。RDD是一个*记录*（严格来说，是某种类型的对象）的集合，分布或分区在集群中的许多节点上（对于Spark本地模式，单个多线程进程可以以相同的方式来看待）。Spark中的RDD是容错的；这意味着如果给定的节点或任务失败（除了错误的用户代码之外的某些原因，如硬件故障、通信丢失等），RDD可以在剩余的节点上自动重建，作业仍将完成。
- en: Creating RDDs
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建RDDs
- en: 'RDDs can be Scala Spark shells that you launched earlier:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: RDDs可以是您之前启动的Scala Spark shells：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: RDDs can also be created from Hadoop-based input sources, including the local
    filesystem, HDFS, and Amazon S3\. A Hadoop-based RDD can utilize any input format
    that implements the Hadoop `InputFormat` interface, including text files, other
    standard Hadoop formats, HBase, Cassandra, tachyon, and many more.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: RDDs也可以从基于Hadoop的输入源创建，包括本地文件系统、HDFS和Amazon S3。基于Hadoop的RDD可以利用实现Hadoop `InputFormat`接口的任何输入格式，包括文本文件、其他标准Hadoop格式、HBase、Cassandra、tachyon等等。
- en: 'The following code is an example of creating an RDD from a text file located
    on the local filesystem:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是一个示例，演示如何从本地文件系统上的文本文件创建RDD：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding `textFile` method returns an RDD where each record is a `String`
    object that represents one line of the text file. The output of the preceding
    command is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`textFile`方法返回一个RDD，其中每个记录都是一个代表文本文件一行的`String`对象。前面命令的输出如下：'
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following code is an example of how to create an RDD from a text file located
    on the HDFS using `hdfs://` protocol:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是一个示例，演示如何使用`hdfs://`协议从HDFS上的文本文件创建RDD：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following code is an example of how to create an RDD from a text file located
    on the Amazon S3 using `s3n://` protocol:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是一个示例，演示如何使用`s3n://`协议从Amazon S3上的文本文件创建RDD：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Spark operations
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark操作
- en: Once we have created an RDD, we have a distributed collection of records that
    we can manipulate. In Spark's programming model, operations are split into transformations
    and actions. Generally speaking, a transformation operation applies some function
    to all the records in the dataset, changing the records in some way. An action
    typically runs some computation or aggregation operation and returns the result
    to the driver program where `SparkContext` is running.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了一个RDD，我们就有了一个可以操作的分布式记录集合。在Spark的编程模型中，操作分为转换和动作。一般来说，转换操作将某个函数应用于数据集中的所有记录，以某种方式改变记录。动作通常运行一些计算或聚合操作，并将结果返回给运行`SparkContext`的驱动程序程序。
- en: Spark operations are functional in style. For programmers familiar with functional
    programming in Scala, Python, or Lambda expressions in Java 8, these operations
    should seem natural. For those without experience in functional programming, don't
    worry; the Spark API is relatively easy to learn.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Spark操作是函数式的风格。对于熟悉Scala、Python或Java 8中的Lambda表达式的函数式编程的程序员来说，这些操作应该看起来很自然。对于没有函数式编程经验的人，不用担心；Spark
    API相对容易学习。
- en: 'One of the most common transformations that you will use in Spark programs
    is the map operator. This applies a function to each record of an RDD, thus *mapping*
    the input to some new output. For example, the following code fragment takes the
    RDD we created from a local text file and applies the `size` function to each
    record in the RDD. Remember that we created an RDD of Strings. Using `map`, we
    can transform each string to an integer, thus returning an RDD of `Ints`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark程序中，您将使用的最常见的转换之一是map操作符。这将对RDD的每个记录应用一个函数，从而将输入*映射*到一些新的输出。例如，以下代码片段将我们从本地文本文件创建的RDD，并将`size`函数应用于RDD中的每个记录。请记住，我们创建了一个String的RDD。使用`map`，我们可以将每个字符串转换为整数，从而返回一个`Ints`的RDD：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You should see output similar to the following line in your shell; this indicates
    the type of the RDD:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该在shell中看到类似以下行的输出；这表示RDD的类型：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, we saw the use of the `=>` syntax. This is the Scala
    syntax for an anonymous function, which is a function that is not a named method
    (that is, one defined using the `def` keyword in Scala or Python, for example).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们看到了`=>`语法的使用。这是Scala中匿名函数的语法，它是一个不是命名方法的函数（也就是说，使用`def`关键字在Scala或Python中定义的方法）。
- en: While a detailed treatment of anonymous functions is beyond the scope of this
    book, they are used extensively in Spark code in Scala and Python, as well as
    in Java 8 (both in examples and real-world applications), so it is useful to cover
    a few practicalities.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然匿名函数的详细处理超出了本书的范围，但它们在Scala和Python的Spark代码中被广泛使用，以及在Java 8中（在示例和实际应用中），因此涵盖一些实用性是有用的。
- en: The line `=> line.size` syntax means that we are applying a function where `=>`
    is the operator, and the output is the result of the code to the right of the
    `=>` operator. In this case, the input is line, and the output is the result of
    calling `line.size`. In Scala, this function that maps a string to an integer
    is expressed as `String => Int`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 行`=> line.size`的语法意味着我们正在应用一个函数，其中`=>`是操作符，输出是`=>`操作符右侧代码的结果。在这种情况下，输入是行，输出是调用`line.size`的结果。在Scala中，将字符串映射为整数的函数表示为`String
    => Int`。
- en: This syntax saves us from having to separately define functions every time we
    use methods such as map; this is useful when the function is simple and will only
    be used once, as in this example.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这种语法使我们不必每次使用map等方法时单独定义函数；当函数简单且只使用一次时，这是很有用的，就像这个例子。
- en: 'Now, we can apply a common action operation, count, to return the number of
    records in our RDD:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以对我们的RDD应用一个常见的动作操作，count，以返回记录的数量：
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The result should look something like the following console output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该看起来像以下的控制台输出：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Perhaps we want to find the average length of each line in this text file.
    We can first use the `sum` function to add up all the lengths of all the records
    and then divide the sum by the number of records:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们想要找到这个文本文件中每行的平均长度。我们可以首先使用`sum`函数将所有记录的长度相加，然后将总和除以记录的数量：
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The result will be as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE22]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Spark operations, in most cases, return a new RDD, with the exception of most
    actions, which return the result of a computation (such as `Long` for count and
    `Double` for sum in the preceding example). This means that we can naturally chain
    together operations to make our program flow more concise and expressive. For
    example, the same result as the one in the preceding line of code can be achieved
    using the following code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Spark操作在大多数情况下都会返回一个新的RDD，除了大多数动作，它们返回计算的结果（例如在前面的示例中的`Long`表示计数，`Double`表示求和）。这意味着我们可以自然地链接操作以使我们的程序流更简洁和表达力更强。例如，可以使用以下代码实现与前一行代码相同的结果：
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: An important point to note is that Spark transformations are lazy. That is,
    invoking a transformation on an RDD does not immediately trigger a computation.
    Instead, transformations are chained together and are effectively only computed
    when an action is called. This allows Spark to be more efficient by only returning
    results to the driver when necessary so that the majority of operations are performed
    in parallel on the cluster.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的要点是，Spark的转换是惰性的。也就是说，在RDD上调用转换不会立即触发计算。相反，转换被链接在一起，只有在调用动作时才有效地计算。这使得Spark能够更有效地只在必要时将结果返回给驱动程序，以便大多数操作在集群上并行执行。
- en: 'This means that if your Spark program never uses an action operation, it will
    never trigger an actual computation, and you will not get any results. For example,
    the following code will simply return a new RDD that represents the chain of transformations:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着如果您的Spark程序从不使用动作操作，它将永远不会触发实际的计算，也不会得到任何结果。例如，以下代码将简单地返回一个表示转换链的新RDD：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This returns the following result in the console:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在控制台中返回以下结果：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Notice that no actual computation happens and no result is returned. If we
    now call an action, such as sum, on the resulting RDD, the computation will be
    triggered:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，没有实际的计算发生，也没有结果返回。如果我们现在对生成的RDD调用一个动作，比如sum，计算将被触发：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You will now see that a Spark job is run, and it results in the following console
    output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您将看到运行了一个Spark作业，并且结果显示在控制台上：
- en: '[PRE27]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The complete list of transformations and actions possible on RDDs, as well as
    a set of more detailed examples, are available in the Spark programming guide
    (located at [http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html)),
    and the API documentation (the Scala API documentation) is located at ([http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有关RDD可能的所有转换和操作的完整列表，以及一组更详细的示例，都可以在Spark编程指南（位于[http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html)）和API文档（Scala
    API文档）中找到（位于[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)）。
- en: Caching RDDs
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存RDDs
- en: 'One of the most powerful features of Spark is the ability to cache data in
    memory across a cluster. This is achieved through the use of the cache method
    on an RDD:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Spark最强大的功能之一是能够在整个集群中将数据缓存在内存中。这是通过在RDD上使用cache方法来实现的：
- en: '[PRE28]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Calling `cache` on an RDD tells Spark that the RDD should be kept in memory.
    The first time an action is called on the RDD that initiates a computation, the
    data is read from its source and put into memory. Hence, the first time such an
    operation is called, the time it takes to run the task is partly dependent on
    the time it takes to read the data from the input source. However, when the data
    is accessed the next time (for example, in subsequent queries in analytics or
    iterations in a machine learning model), the data can be read directly from memory,
    thus avoiding expensive I/O operations and speeding up the computation, in many
    cases, by a significant factor.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在RDD上调用`cache`告诉Spark应该将RDD保存在内存中。第一次调用RDD上的操作以启动计算时，数据将从其源读取并放入内存。因此，第一次调用此类操作时，运行任务所需的时间部分取决于从输入源读取数据所需的时间。但是，当下一次访问数据时（例如，在分析中的后续查询或机器学习模型的迭代中），数据可以直接从内存中读取，从而避免昂贵的I/O操作，并在许多情况下显着加快计算速度。
- en: 'If we now call the `count` or `sum` function on our cached RDD, the RDD is
    loaded into memory:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在在缓存的RDD上调用`count`或`sum`函数，RDD将加载到内存中：
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Spark also allows more fine-grained control over caching behavior. You can
    use the persist method to specify what approach Spark uses to cache data. More
    information on RDD caching can be found here:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Spark还允许更精细地控制缓存行为。您可以使用`persist`方法指定Spark用于缓存数据的方法。有关RDD缓存的更多信息，请参见此处：
- en: '[http://spark.apache.org/docs/latest/programmingguide.html#rdd-persistence](http://spark.apache.org/docs/latest/programmingguide.html#rdd-persistence)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/programmingguide.html#rdd-persistence](http://spark.apache.org/docs/latest/programmingguide.html#rdd-persistence)'
- en: Broadcast variables and accumulators
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广播变量和累加器
- en: Another core feature of Spark is the ability to create two special types of
    variables--broadcast variables and accumulators.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的另一个核心特性是能够创建两种特殊类型的变量--广播变量和累加器。
- en: 'A **broadcast variable** is a *read-only* variable that is created from the
    driver program object and made available to the nodes that will execute the computation.
    This is very useful in applications that need to make the same data available
    to the worker nodes in an efficient manner, such as distributed systems. Spark
    makes creating broadcast variables as simple as calling a method on `SparkContext`,
    as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**广播变量**是从驱动程序对象创建的*只读*变量，并提供给将执行计算的节点。这在需要以高效的方式将相同数据提供给工作节点的应用程序中非常有用，例如分布式系统。Spark使创建广播变量变得非常简单，只需在`SparkContext`上调用一个方法即可，如下所示：'
- en: '[PRE30]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'A broadcast variable can be accessed from nodes other than the driver program
    that created it (that is, the worker nodes) by calling `value` on the variable:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量可以通过在变量上调用`value`来从创建它的驱动程序之外的节点（即工作节点）访问：
- en: '[PRE31]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This code creates a new RDD with three records from a collection (in this case,
    a Scala `List`) of `("1", "2", "3")`. In the map function, it returns a new collection
    with the relevant rom our new RDD appended to the `broadcastAList` that is our
    broadcast variable:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用来自集合（在本例中为Scala `List`）的三条记录创建一个新的RDD。在映射函数中，它返回一个新的集合，其中包含从我们的新RDD附加到`broadcastAList`的相关记录，这是我们的广播变量：
- en: '[PRE32]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Notice the `collect` method in the preceding code. This is a Spark *action*
    that returns the entire RDD to the driver as a Scala (or Python or Java) collection.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意前面代码中的`collect`方法。这是一个Spark *操作*，它将整个RDD作为Scala（或Python或Java）集合返回给驱动程序。
- en: We will often use when we wish to apply further processing to our results locally
    within the driver program.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常在希望在驱动程序中本地应用进一步处理结果时使用。
- en: Note that `collect` should generally only be used in cases where we really want
    to return the full result set to the driver and perform further processing. If
    we try to call `collect` on a very large dataset, we might run out of memory on
    the driver and crash our program.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`collect`通常只应在我们真正希望将完整结果集返回给驱动程序并执行进一步处理的情况下使用。如果我们尝试在非常大的数据集上调用`collect`，可能会在驱动程序上耗尽内存并使程序崩溃。
- en: It is preferable to perform as much heavy-duty processing on our Spark cluster
    as possible, preventing the driver from becoming a bottleneck. In many cases,
    however, such as during iterations in many machine learning models, collecting
    results to the driver is necessary.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最好尽可能在我们的Spark集群上执行尽可能多的重型处理，以防止驱动程序成为瓶颈。然而，在许多情况下，例如在许多机器学习模型的迭代中，将结果收集到驱动程序是必要的。
- en: 'On inspecting the result, we will see that for each of the three records in
    our new RDD, we now have a record that is our original broadcasted `List`, with
    the new element appended to it (that is, there is now `"1"`, `"2"`, or `"3"` at
    the end):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 检查结果后，我们将看到我们的新RDD中的每个三条记录，现在都有一个记录，即我们原始广播的`List`，并将新元素附加到其中（即现在末尾有`"1"`、`"2"`或`"3"`）：
- en: An **accumulator** is also a variable that is broadcasted to the worker nodes.
    The key difference between a broadcast variable and an accumulator is that while
    the `broadcast` variable is read-only, the accumulator can be added to. There
    are limitations to this, that is, in particular, the addition must be an associative
    operation so that the global accumulated value can be correctly computed in parallel
    and returned to the driver program. Each worker node can only access and add to
    its own local accumulator value, and only the driver program can access the global
    value. Accumulators are also accessed within the Spark code using the value method.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**累加器**也是广播到工作节点的变量。广播变量和累加器之间的关键区别在于，虽然`广播`变量是只读的，但累加器可以添加。这方面存在一些限制，即特别是加法必须是可关联的操作，以便可以正确地并行计算全局累积值并将其返回到驱动程序。每个工作节点只能访问并添加到其本地累加器值，只有驱动程序才能访问全局值。累加器也可以使用`value`方法在Spark代码中访问。'
- en: For more details on broadcast variables and accumulators, refer to the *Shared
    Variables* section of the *Spark Programming Guide* at [http://spark.apache.org/docs/latest/programming-guide.html#shared-variables](http://spark.apache.org/docs/latest/programming-guide.html#shared-variables).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 有关广播变量和累加器的更多详细信息，请参考*Spark编程指南*中的*共享变量*部分，网址为[http://spark.apache.org/docs/latest/programming-guide.html#shared-variables](http://spark.apache.org/docs/latest/programming-guide.html#shared-variables)。
- en: SchemaRDD
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SchemaRDD
- en: '**SchemaRDD** is a combination of RDD and schema information. It also offers
    many rich and easy-to-use APIs (that is, the `DataSet` API). SchemaRDD is not
    used with 2.0 and is internally used by `DataFrame` and `Dataset` APIs.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**SchemaRDD**是RDD和模式信息的组合。它还提供了许多丰富且易于使用的API（即`DataSet` API）。SchemaRDD在2.0中不再使用，而是由`DataFrame`和`Dataset`
    API在内部使用。'
- en: A schema is used to describe how structured data is logically organized. After
    obtaining the schema information, the SQL engine is able to provide the structured
    query capability for the corresponding data. The `DataSet` API is a replacement
    for Spark SQL parser's functions. It is an API to achieve the original program
    logic tree. Subsequent processing steps reuse Spark SQL's core logic. We can safely
    consider `DataSet` API's processing functions as completely equivalent to that
    of SQL queries.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 模式用于描述结构化数据的逻辑组织方式。在获取模式信息后，SQL引擎能够为相应的数据提供结构化查询功能。`DataSet` API是Spark SQL解析器函数的替代品。它是一个用于实现原始程序逻辑树的API。后续处理步骤重用了Spark
    SQL的核心逻辑。我们可以安全地将`DataSet` API的处理函数视为与SQL查询完全等效。
- en: SchemaRDD is an RDD subclass. When a program calls the `DataSet` API, a new
    SchemaRDD object is created, and a logic plan attribute of the `new` object is
    created by adding a new logic operation node on the original logic plan tree.
    Operations of the `DataSet` API (like RDD) are of two types--**Transformation**
    and **Action**.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: SchemaRDD是一个RDD子类。当程序调用`DataSet` API时，会创建一个新的SchemaRDD对象，并通过在原始逻辑计划树上添加一个新的逻辑操作节点来创建`new`对象的逻辑计划属性。`DataSet`
    API的操作（与RDD一样）有两种类型--**Transformation**和**Action**。
- en: APIs related to the relational operations are attributed to the Transformation
    type.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 与关系操作相关的API归属于Transformation类型。
- en: Operations associated with data output sources are of Action type. Like RDD,
    a Spark job is triggered and delivered for cluster execution, only when an Action
    type operation is called.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据输出源相关的操作属于Action类型。与RDD一样，只有在调用Action类型操作时，Spark作业才会被触发并交付给集群执行。
- en: Spark data frame
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark数据框
- en: In Apache Spark, a `Dataset` is a distributed collection of data. The `Dataset`
    is a new interface added since Spark 1.6\. It provides the benefits of RDDs with
    the benefits of Spark SQL's execution engine. A `Dataset` can be constructed from
    JVM objects and then manipulated using functional transformations (`map`, `flatMap`,
    `filter`, and so on). The `Dataset` API is available only for in Scala and Java.
    It is not available for Python or R.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在Apache Spark中，`Dataset`是分布式数据集合。`Dataset`是自Spark 1.6以来新增的接口。它结合了RDD的优点和Spark
    SQL的执行引擎的优点。`Dataset`可以从JVM对象构建，然后使用功能转换（`map`、`flatMap`、`filter`等）进行操作。`Dataset`
    API仅适用于Scala和Java，不适用于Python或R。
- en: A `DataFrame` is a dataset with named columns. It is equivalent to a table in
    a relational database or a data frame in R/Python, with richer optimizations.
    `DataFrame` is constructed from structured data files, tables in Hive, external
    databases, or existing RDDs. The `DataFrame` API is available in Scala, Python,
    Java, and R.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame`是一个带有命名列的数据集。它相当于关系数据库中的表或R/Python中的数据框，但具有更丰富的优化。`DataFrame`可以从结构化数据文件、Hive中的表、外部数据库或现有的RDD构建。`DataFrame`
    API在Scala、Python、Java和R中都可用。'
- en: 'A Spark `DataFrame` needs the Spark session instantiated first:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Spark `DataFrame`首先需要实例化Spark会话：
- en: '[PRE33]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we create a `DataFrame` from a Json file using the `spark.read.json`
    function:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`spark.read.json`函数从Json文件创建一个`DataFrame`：
- en: '[PRE34]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Note that Spark `Implicits` are being used to implicitly convert RDD to Data
    Frame types:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Spark `Implicits`正在被用来隐式地将RDD转换为数据框类型：
- en: '[PRE35]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Implicit methods available in Scala for converting common Scala objects into
    `DataFrames`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Scala中可用的隐式方法，用于将常见的Scala对象转换为`DataFrames`。
- en: 'Output will be similar to the following listing:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下清单：
- en: '[PRE36]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now we want to see how this is actually loaded in the `DataFrame`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想看看这实际上是如何加载到`DataFrame`中的：
- en: '[PRE37]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The first step to a Spark program in Scala
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark程序在Scala中的第一步
- en: We will now use the ideas we introduced in the previous section to write a basic
    Spark program to manipulate a dataset. We will start with Scala and then write
    the same program in Java and Python. Our program will be based on exploring some
    data from an online store, about which users have purchased which products. The
    data is contained in a **Comma-Separated-Value** (**CSV**) file called `UserPurchaseHistory.csv`.
    This file is expected to be in the `data` directory.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用我们在上一节介绍的思想来编写一个基本的Spark程序来操作数据集。我们将从Scala开始，然后在Java和Python中编写相同的程序。我们的程序将基于探索来自在线商店的一些数据，关于哪些用户购买了哪些产品。数据包含在名为`UserPurchaseHistory.csv`的**逗号分隔值**（**CSV**）文件中。该文件应该位于`data`目录中。
- en: 'The contents are shown in the following snippet. The first column of the CSV
    is the username, the second column is the product name, and the final column is
    the price:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 内容如下所示。CSV的第一列是用户名，第二列是产品名称，最后一列是价格：
- en: '[PRE38]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: For our Scala program, we need to create two files-our Scala code and our project
    build configuration file-using the build tool **Scala Build Tool** (**SBT**).
    For ease of use, we recommend that you use -spark-app for this chapter. This code
    also contains the CSV file under the data directory. You will need SBT installed
    on your system in order to run this example program (we use version 0.13.8 at
    the time of writing this book).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的Scala程序，我们需要创建两个文件-我们的Scala代码和我们的项目构建配置文件-使用构建工具**Scala Build Tool**（**SBT**）。为了方便使用，我们建议您在本章中使用-spark-app。此代码还包含了data目录下的CSV文件。您需要在系统上安装SBT才能运行此示例程序（我们在撰写本书时使用的是版本0.13.8）。
- en: Setting up SBT is beyond the scope of this book; however, you can find more
    information at [http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html](http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 设置SBT超出了本书的范围；但是，您可以在[http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html](http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html)找到更多信息。
- en: 'Our SBT configuration file, `build.sbt`, looks like this (note that the empty
    lines between each line of code are required):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的SBT配置文件`build.sbt`看起来像这样（请注意，代码每一行之间的空行是必需的）：
- en: '[PRE39]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The last line adds the dependency on Spark to our project.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行将Spark的依赖项添加到我们的项目中。
- en: 'Our Scala program is contained in the `ScalaApp.scala` file. We will walk through
    the program piece by piece. First, we need to import the required Spark classes:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Scala程序包含在`ScalaApp.scala`文件中。我们将逐步讲解程序。首先，我们需要导入所需的Spark类：
- en: '[PRE40]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In our main method, we need to initialize our `SparkContext` object and use
    this to access our CSV data file with the `textFile` method. We will then map
    the raw text by splitting the string on the delimiter character (a comma in this
    case) and extracting the relevant records for username, product, and price:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的主方法中，我们需要初始化我们的`SparkContext`对象，并使用它来访问我们的CSV数据文件的`textFile`方法。然后，我们将通过在分隔符字符上拆分字符串并提取有关用户名、产品和价格的相关记录来映射原始文本：
- en: '[PRE41]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now that we have an RDD, where each record is made up of (`user`, `product`,
    `price`), we can compute various interesting metrics for our store, such as the
    following ones:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个RDD，其中每条记录由（`用户`，`产品`，`价格`）组成，我们可以为我们的商店计算各种有趣的指标，例如以下指标：
- en: The total number of purchases
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 购买总数
- en: The number of unique users who purchased
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 购买的独特用户数量
- en: Our total revenue
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的总收入
- en: Our most popular product
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们最受欢迎的产品
- en: 'Let''s compute the preceding metrics:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算前述指标：
- en: '[PRE42]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This last piece of code to compute the most popular product is an example of
    the *Map/Reduce* pattern made popular by Hadoop. First, we mapped our records
    of (`user`, `product`, `price`) to the records of `(product, 1)`. Then, we performed
    a `reduceByKey` operation, where we summed up the 1s for each unique product.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 计算最受欢迎产品的最后一段代码是Hadoop流行的*Map/Reduce*模式的一个例子。首先，我们将我们的记录（`用户`，`产品`，`价格`）映射到（`产品`，`1`）的记录。然后，我们执行了`reduceByKey`操作，对每个唯一产品的1进行求和。
- en: Once we have this transformed RDD, which contains the number of purchases for
    each product, we will call `collect`, which returns the results of the computation
    to the driver program as a local Scala collection. We will then sort these counts
    locally (note that in practice, if the amount of data is large, we will perform
    the sorting in parallel, usually with a Spark operation such as `sortByKey`).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这个转换后的RDD，其中包含了每种产品的购买数量，我们将调用`collect`，将计算结果返回给驱动程序作为本地Scala集合。然后我们会在本地对这些计数进行排序（请注意，在实践中，如果数据量很大，我们通常会使用Spark操作（例如`sortByKey`）并行进行排序）。
- en: 'Finally, we will print out the results of our computations to the console:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在控制台上打印出我们的计算结果：
- en: '[PRE43]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can run this program by running `sbt run` in the project''s base directory
    or by running the program in your Scala IDE if you are using one. The output should
    look similar to the following:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在项目的基本目录中运行`sbt run`或者在使用Scala IDE时运行程序来运行此程序。输出应该类似于以下内容：
- en: '[PRE44]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We can see that we have `5` purchases from four different users with total revenue
    of `39.91`. Our most popular product is an `iPhone cover with 2 purchases`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们有来自四个不同用户的`5`次购买，总收入为`39.91`。我们最受欢迎的产品是一个带有`2`次购买的`iPhone保护套`。
- en: The first step to a Spark program in Java
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java中Spark程序的第一步
- en: The Java API is very similar in principle to the Scala API. However, while Scala
    can call the Java code quite easily, in some cases, it is not possible to call
    the Scala code from Java. This is particularly the case when Scala code makes
    use of Scala features such as implicit conversions, default parameters, and the
    Scala reflection API.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Java API在原则上与Scala API非常相似。但是，虽然Scala可以很容易地调用Java代码，但在某些情况下，从Java调用Scala代码是不可能的。特别是当Scala代码使用Scala特性，如隐式转换、默认参数和Scala反射API时。
- en: Spark makes heavy use of these features in general, so it is necessary to have
    a separate API specifically for Java that includes Java versions of the common
    classes. Hence, `SparkContext` becomes `JavaSparkContext` and RDD becomes JavaRDD.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Spark通常大量使用这些功能，因此有必要专门为Java提供一个单独的API，其中包括常见类的Java版本。因此，`SparkContext`变成了`JavaSparkContext`，而RDD变成了JavaRDD。
- en: Java versions prior to version 8 do not support anonymous functions and do not
    have succinct syntax for functional-style programming, so functions in the Spark
    Java API must implement a `WrappedFunction` interface with the `call` method signature.
    While it is significantly more verbose, we will often create one-off anonymous
    classes to pass to our Spark operations, which implement this interface and the
    `call` method to achieve much the same effect as anonymous functions in Scala.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Java 8之前的版本不支持匿名函数，也没有简洁的函数式编程语法，因此Spark Java API中的函数必须实现`WrappedFunction`接口，并具有`call`方法签名。虽然这种方式更加冗长，但我们经常会创建一次性的匿名类，将其传递给我们的Spark操作，这些匿名类实现了这个接口和`call`方法，从而实现了与Scala中匿名函数几乎相同的效果。
- en: Spark provides support for Java 8's anonymous function (or *lambda*) syntax.
    Using this syntax makes a Spark program written in Java 8 look very close to the
    equivalent Scala program.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持Java 8的匿名函数（或*lambda*）语法。使用这种语法使得用Java 8编写的Spark程序看起来非常接近等效的Scala程序。
- en: In Scala, an RDD of key/value pairs provides special operators (such as `reduceByKey`
    and `saveAsSequenceFile`, for example) that are accessed automatically via implicit
    conversions. In Java, special types of `JavaRDD` classes are required in order
    to access similar functions. These include `JavaPairRDD` to work with key/value
    pairs and `JavaDoubleRDD` to work with numerical records.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中，键/值对的RDD提供了特殊的运算符（例如`reduceByKey`和`saveAsSequenceFile`），这些运算符可以通过隐式转换自动访问。在Java中，需要特殊类型的`JavaRDD`类才能访问类似的函数。这些包括`JavaPairRDD`用于处理键/值对和`JavaDoubleRDD`用于处理数值记录。
- en: In this section, we covered the standard Java API syntax. For more details and
    examples related to working RDDs in Java, as well as the Java 8 lambda syntax,
    refer to the Java sections of the *Spark Programming Guide* found at [http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了标准的Java API语法。有关在Java中使用RDD以及Java 8 lambda语法的更多细节和示例，请参阅*Spark编程指南*中的Java部分，网址为[http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations)。
- en: We will see examples of most of these differences in the following Java program,
    which is included in the example code of this chapter in the directory named `java-spark-app`.
    The `code` directory also contains the CSV data file under the `data` subdirectory.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的Java程序中看到大部分这些差异的示例，该程序包含在本章示例代码的`java-spark-app`目录中。`code`目录还包含`data`子目录下的CSV数据文件。
- en: We will build and run this project with the **Maven** build tool, which we assume
    you have installed on your system.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**Maven**构建工具构建和运行这个项目，我们假设您已经在系统上安装了它。
- en: Installing and setting up Maven is beyond the scope of this book. Usually, Maven
    can easily be installed using the package manager on your Linux system or HomeBrew
    or MacPorts on Mac OS X.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 安装和设置Maven超出了本书的范围。通常，可以使用Linux系统上的软件包管理器，或者在Mac OS X上使用HomeBrew或MacPorts轻松安装Maven。
- en: Detailed installation instructions can be found at [http://maven.apache.org/download.cgi](http://maven.apache.org/download.cgi).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 详细的安装说明可以在[http://maven.apache.org/download.cgi](http://maven.apache.org/download.cgi)找到。
- en: 'The project contains a Java file called `JavaApp.java`, which contains our
    program code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目包含一个名为`JavaApp.java`的Java文件，其中包含我们的程序代码：
- en: '[PRE45]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'As in our Scala example, we first need to initialize our context. Note that
    we will use the `JavaSparkContext` class here instead of the `SparkContext` class
    that we used earlier. We will use the `JavaSparkContext` class in the same way
    to access our data using `textFile` and then split each row into the required
    fields. Note how we used an anonymous class to define a split function that performs
    the string processing in the highlighted code:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的Scala示例一样，我们首先需要初始化我们的上下文。请注意，我们将在这里使用`JavaSparkContext`类，而不是之前使用的`SparkContext`类。我们将以相同的方式使用`JavaSparkContext`类来使用`textFile`访问我们的数据，然后将每一行拆分为所需的字段。请注意我们如何使用匿名类来定义一个拆分函数，该函数在突出显示的代码中执行字符串处理：
- en: '[PRE46]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, we can compute the same metrics as we did in our Scala example. Note how
    some methods are the same (for example, `distinct` and `count`) for the Java and
    Scala APIs. Also note the use of anonymous classes that we pass to the map function.
    This code is highlighted here:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算与我们在Scala示例中所做的相同的指标。请注意，一些方法对于Java和Scala API是相同的（例如`distinct`和`count`）。还请注意我们传递给map函数的匿名类的使用。这段代码在这里突出显示：
- en: '[PRE47]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'In the following lines of code, we can see that the approach to compute the
    most popular product is the same as that in the Scala example. The extra code
    might seem complex, but it is mostly related to the Java code required to create
    the anonymous functions (which we have highlighted here). The actual functionality
    is the same:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码行中，我们可以看到计算最受欢迎产品的方法与Scala示例中的方法相同。额外的代码可能看起来复杂，但它主要与创建匿名函数所需的Java代码相关（我们在这里进行了突出显示）。实际功能是相同的：
- en: '[PRE48]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: As can be seen, the general structure is similar to the Scala version, apart
    from the extra boilerplate code used to declare variables and functions via anonymous
    inner classes. It is a good exercise to work through both examples and compare
    lines of Scala code to those in Java to understand how the same result is achieved
    in each language.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，一般结构与Scala版本相似，除了额外的样板代码用于通过匿名内部类声明变量和函数。通过逐行比较Scala代码和Java代码，理解如何在每种语言中实现相同的结果是一个很好的练习。
- en: 'This program can be run with the following command executed from the project''s
    base directory:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过从项目的基本目录执行以下命令来运行此程序：
- en: '[PRE49]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You will see output that looks very similar to the Scala version with identical
    results of the computation:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到与Scala版本非常相似的输出，计算结果相同：
- en: '[PRE50]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The first step to a Spark program in Python
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中Spark程序的第一步
- en: Spark's Python API exposes virtually all the functionalities of Spark's Scala
    API in the Python language. There are some features that are not yet supported
    (for example, graph processing with GraphX and a few API methods here and there).
    Refer to the Python section of *Spark Programming Guide* ([http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html))
    for more details.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的Python API在Python语言中几乎暴露了Spark的Scala API的所有功能。有一些功能目前尚不支持（例如，使用GraphX进行图处理以及一些API方法）。有关更多详细信息，请参阅*Spark编程指南*的Python部分（[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html)）。
- en: '**PySpark** is built using Spark''s Java API. Data is processed in native Python,
    cached, and shuffled in JVM. Python driver program''s `SparkContext` uses Py4J
    to launch a JVM and create a `JavaSparkContext`. The driver uses Py4J for local
    communication between the Python and Java `SparkContext` objects. RDD transformations
    in Python map to transformations on `PythonRDD` objects in Java. `PythonRDD` object
    launches Python sub-processes on remote worker machines, communicate with them
    using pipes. These sub-processes are used to send the user''s code and to process
    data.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**PySpark**是使用Spark的Java API构建的。数据在本地Python中处理，缓存，并在JVM中进行洗牌。Python驱动程序的`SparkContext`使用Py4J来启动JVM并创建`JavaSparkContext`。驱动程序使用Py4J在Python和Java的`SparkContext`对象之间进行本地通信。Python中的RDD转换映射到Java中的`PythonRDD`对象上的转换。`PythonRDD`对象在远程工作机器上启动Python子进程，并使用管道与它们通信。这些子进程用于发送用户的代码和处理数据。'
- en: Following on from the preceding examples, we will now write a Python version.
    We assume that you have Python version 2.6 and higher installed on your system
    (for example, most Linux and Mac OS X systems come with Python preinstalled).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子之后，我们现在将写一个Python版本。我们假设你的系统上已安装了Python 2.6及更高版本（例如，大多数Linux和Mac OS X系统都预装了Python）。
- en: The example program is included in the sample code for this chapter, in the
    directory named `python-spark-app`, which also contains the CSV data file under
    the `data` subdirectory. The project contains a script, `pythonapp.py`, provided
    here.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 示例程序包含在本章的示例代码中，位于名为`python-spark-app`的目录中，该目录还包含`data`子目录下的CSV数据文件。该项目包含一个名为`pythonapp.py`的脚本，如下所示。
- en: 'A simple Spark app in Python:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的Python中的Spark应用：
- en: '[PRE51]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'If you compare the Scala and Python versions of our program, you will see that
    generally, the syntax looks very similar. One key difference is how we express
    anonymous functions (also called `lambda` functions; hence, the use of this keyword
    for the Python syntax). In Scala, we''ve seen that an anonymous function mapping
    an input `x` to an output `y` is expressed as `x => y`, while in Python, it is
    `lambda x: y`. In the highlighted line in the preceding code, we are applying
    an anonymous function that maps two inputs, `a` and `b`, generally of the same
    type, to an output. In this case, the function that we apply is the plus function;
    hence, `lambda a, b: a + b`.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你比较我们程序的Scala和Python版本，你会发现语法看起来非常相似。一个关键的区别是我们如何表达匿名函数（也称为`lambda`函数；因此，Python语法中使用了这个关键字）。在Scala中，我们已经看到，将输入`x`映射到输出`y`的匿名函数表示为`x
    => y`，而在Python中，它是`lambda x: y`。在前面代码的突出行中，我们应用了一个将两个输入`a`和`b`（通常是相同类型的）映射到输出的匿名函数。在这种情况下，我们应用的函数是加法函数；因此，`lambda
    a, b: a + b`。'
- en: 'The best way to run the script is to run the following command from the base
    directory of the sample project:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本的最佳方法是从示例项目的基本目录运行以下命令：
- en: '[PRE52]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Here, the `SPARK_HOME` variable should be replaced with the path of the directory
    in which you originally unpacked the Spark prebuilt binary package at the start
    of this chapter.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`SPARK_HOME`变量应该被替换为你在本章开始时解压Spark预构建二进制包的目录路径。
- en: 'Upon running the script, you should see output similar to that of the Scala
    and Java examples, with the results of our computation being the same:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本后，你应该看到与Scala和Java示例相似的输出，我们的计算结果也是相同的：
- en: '[PRE53]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The first step to a Spark program in R
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在R中编写Spark程序的第一步
- en: '**SparkR** is an R package which provides a frontend to use Apache Spark from
    R. In Spark 1.6.0; SparkR provides a distributed data frame on large datasets.
    SparkR also supports distributed machine learning using MLlib. This is something
    you should try out while reading machine learning chapters.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**SparkR**是一个R包，提供了一个前端来使用Apache Spark。在Spark 1.6.0中，SparkR提供了一个分布式数据框架用于大型数据集。SparkR还支持使用MLlib进行分布式机器学习。在阅读机器学习章节时，你应该尝试一下这个。'
- en: SparkR DataFrames
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkR数据框
- en: '`DataFrame` is a collection of data organized into names columns that are distributed.
    This concept is very similar to a relational database or a data frame of R but
    with much better optimizations. Source of these data frames could be a CSV, a
    TSV, Hive tables, local R data frames, and so on.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame`是一个由名称列组织的分布式数据集合。这个概念与关系数据库或R的数据框非常相似，但优化更好。这些数据框的来源可以是CSV、TSV、Hive表、本地R数据框等。'
- en: Spark distribution can be run using the `./bin/sparkR shell`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Spark分发可以使用`./bin/sparkR shell`来运行。
- en: Following on from the preceding examples, we will now write an R version. We
    assume that you have R (R version 3.0.2 (2013-09-25)-*Frisbee Sailing*), R Studio
    and higher installed on your system (for example, most Linux and Mac OS X systems
    come with Python preinstalled).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子之后，我们现在将写一个R版本。我们假设你的系统上已安装了R（例如，大多数Linux和Mac OS X系统都预装了Python）。
- en: The example program is included in the sample code for this chapter, in the
    directory named `r-spark-app`, which also contains the CSV data file under the
    `data` subdirectory. The project contains a script, `r-script-01.R`, which is
    provided in the following. Make sure you change `PATH` to appropriate value for
    your environment.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 示例程序包含在本章的示例代码中，位于名为`r-spark-app`的目录中，该目录还包含`data`子目录下的CSV数据文件。该项目包含一个名为`r-script-01.R`的脚本，如下所示。确保你将`PATH`更改为适合你的环境的值。
- en: '[PRE54]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Run the script with the following command on the bash terminal:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在bash终端上使用以下命令运行脚本：
- en: '[PRE55]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Your output will be similar to the following listing:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你的输出将类似于以下清单：
- en: '[PRE56]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Getting Spark running on Amazon EC2
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在亚马逊EC2上运行Spark
- en: 'The Spark project provides scripts to run a Spark cluster in the cloud on Amazon''s
    EC2 service. These scripts are located in the `ec2` directory. You can run the
    `spark-ec2` script contained in this directory with the following command:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Spark项目提供了在亚马逊的EC2服务上在云中运行Spark集群的脚本。这些脚本位于`ec2`目录中。你可以使用以下命令在这个目录中运行`spark-ec2`脚本：
- en: '[PRE57]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Running it in this way without an argument will show the help output:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式运行它而不带参数将显示帮助输出：
- en: '[PRE58]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Before creating a Spark EC2 cluster, you will need to ensure that you have an
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建Spark EC2集群之前，您需要确保您有一个
- en: Amazon account.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊账户。
- en: If you don't have an Amazon Web Services account, you can sign up at [http://aws.amazon.com/](http://aws.amazon.com/).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有Amazon Web Services账户，可以在[http://aws.amazon.com/](http://aws.amazon.com/)注册。
- en: The AWS console is available at [http://aws.amazon.com/console/](http://aws.amazon.com/console/).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: AWS控制台可在[http://aws.amazon.com/console/](http://aws.amazon.com/console/)找到。
- en: 'You will also need to create an Amazon EC2 key pair and retrieve the relevant
    security credentials. The Spark documentation for EC2 (available at [http://spark.apache.org/docs/latest/ec2-scripts.html](http://spark.apache.org/docs/latest/ec2-scripts.html))
    explains the requirements:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要创建一个Amazon EC2密钥对并检索相关的安全凭据。EC2的Spark文档（可在[http://spark.apache.org/docs/latest/ec2-scripts.html](http://spark.apache.org/docs/latest/ec2-scripts.html)找到）解释了要求：
- en: Create an Amazon EC2 key pair for yourself. This can be done by logging into
    your Amazon Web Services account through the AWS console, clicking on Key Pairs
    on the left sidebar, and creating and downloading a key. Make sure that you set
    the permissions for the private key file to 600 (that is, only you can read and
    write it) so that ssh will work.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为自己创建一个Amazon EC2密钥对。这可以通过登录您的Amazon Web Services账户，点击左侧边栏上的密钥对，创建和下载一个密钥来完成。确保将私钥文件的权限设置为600（即只有您可以读取和写入它），以便ssh正常工作。
- en: Whenever you want to use the spark-ec2 script, set the environment variables
    `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` to your Amazon EC2 access key
    `ID` and secret access key, respectively. These can be obtained from the AWS homepage
    by clicking Account | Security Credentials | Access Credentials.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 每当您想要使用spark-ec2脚本时，将环境变量`AWS_ACCESS_KEY_ID`和`AWS_SECRET_ACCESS_KEY`设置为您的Amazon
    EC2访问密钥`ID`和秘密访问密钥。这些可以从AWS主页上通过点击账户|安全凭据|访问凭据来获取。
- en: 'When creating a key pair, choose a name that is easy to remember. We will simply
    use the name *spark* for the key pair. The key pair file itself will be called
    `spark.pem`. As mentioned earlier, ensure that the key pair file permissions are
    set appropriately and that the environment variables for the AWS credentials are
    exported using the following commands:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建密钥对时，选择一个易于记住的名称。我们将简单地使用名称*spark*作为密钥对。密钥对文件本身将被称为`spark.pem`。如前所述，确保密钥对文件权限设置正确，并且通过以下命令导出AWS凭据的环境变量：
- en: '[PRE59]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: You should also be careful to keep your downloaded key pair file safe and not
    lose it, as it can only be downloaded once when it is created!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要小心保管您下载的密钥对文件，不要丢失，因为它只能在创建时下载一次！
- en: Note that launching an Amazon EC2 cluster in the following section will *incur
    costs* to your AWS account.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在下一节中启动Amazon EC2集群将会对您的AWS账户产生费用。
- en: Launching an EC2 Spark cluster
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动EC2 Spark集群
- en: 'We''re now ready to launch a small Spark cluster by changing into the `ec2`
    directory and then running the cluster launch command:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好通过切换到`ec2`目录然后运行集群启动命令来启动一个小的Spark集群：
- en: '[PRE60]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: This will launch a new Spark cluster called test-cluster with one master and
    one slave node of instance type `m3.medium`. This cluster will be launched with
    a Spark version built for Hadoop 2\. The key pair name we used is spark, and the
    key pair file is `spark.pem` (if you gave the files different names or have an
    existing AWS key pair, use that name instead).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动一个名为test-cluster的新Spark集群，其中包含一个master和一个slave节点，实例类型为`m3.medium`。此集群将使用为Hadoop
    2构建的Spark版本。我们使用的密钥对名称是spark，密钥对文件是`spark.pem`（如果您给文件取了不同的名称或者有现有的AWS密钥对，请使用该名称）。
- en: 'It might take quite a while for the cluster to fully launch and initialize.
    You should see something like the following immediately after running the launch
    command:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 集群完全启动和初始化可能需要相当长的时间。在运行启动命令后，您应该立即看到类似以下的内容：
- en: '[PRE61]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'If the cluster has launched successfully, you should eventually see a console
    output similar to the following listing:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果集群已经成功启动，最终你应该会看到类似以下清单的控制台输出：
- en: '[PRE62]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'This will create two VMs - Spark Master and Spark Slave of type m1.large as
    shown in the following screenshot :'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建两个VM - Spark Master和Spark Slave，类型为m1.large，如下截图所示：
- en: '![](img/image_01_003.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_003.png)'
- en: 'To test whether we can connect to our new cluster, we can run the following
    command:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们是否可以连接到我们的新集群，我们可以运行以下命令：
- en: '[PRE63]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Remember to replace the public domain name of the master node (the address after
    `root@` in the preceding command) with the correct Amazon EC2 public domain name
    that will be shown in your console output after launching the cluster.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，用正确的Amazon EC2公共域名替换主节点的公共域名（在上述命令中`root@`之后的地址），该域名将在启动集群后显示在您的控制台输出中。
- en: 'You can also retrieve your cluster''s master public domain name by running
    this line of code:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过运行以下代码来检索集群的主公共域名：
- en: '[PRE64]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'After successfully running the `ssh` command, you will be connected to your
    Spark master node in EC2, and your terminal output should match the following
    screenshot:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 成功运行`ssh`命令后，您将连接到EC2中的Spark主节点，并且您的终端输出应该与以下截图相匹配：
- en: '![](img/image_01_004.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_004.png)'
- en: 'We can test whether our cluster is correctly set up with Spark by changing
    into the `Spark` directory and running an example in the local mode:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过切换到`Spark`目录并在本地模式下运行示例来测试我们的集群是否正确设置了Spark：
- en: '[PRE65]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'You should see output similar to what you would get on running the same command
    on your local computer:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似于在本地计算机上运行相同命令时得到的输出：
- en: '[PRE66]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now that we have an actual cluster with multiple nodes, we can test Spark in
    the cluster mode. We can run the same example on the cluster, using our one slave
    node by passing in the master URL instead of the local version:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个实际的多节点集群，我们可以在集群模式下测试Spark。我们可以通过传入主URL而在集群上运行相同的示例，使用我们的一个slave节点：
- en: '[PRE67]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Note that you will need to substitute the preceding master domain name with
    the correct domain name for your specific cluster.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您需要用您特定集群的正确域名替换前面的主机域名。
- en: 'Again, the output should be similar to running the example locally; however,
    the log messages will show that your driver program has connected to the Spark
    master:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，输出应类似于在本地运行示例；但是，日志消息将显示您的驱动程序已连接到Spark主机：
- en: '[PRE68]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Feel free to experiment with your cluster. Try out the interactive console
    in Scala, for example:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 随时尝试您的集群。例如，尝试在Scala中使用交互式控制台：
- en: '[PRE69]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Once you''ve finished, type `exit` to leave the console. You can also try the
    PySpark console by running the following command:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，键入`exit`以离开控制台。您还可以通过运行以下命令尝试PySpark控制台：
- en: '[PRE70]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: You can use the Spark Master web interface to see the applications registered
    with the master. To load the Master Web UI, navigate to `ec2-52-90-110-128.compute-1.amazonaws.com:8080`
    (again, remember to replace this domain name with your own master domain name).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Spark Master Web界面查看与主机注册的应用程序。要加载Master Web UI，请导航至`ec2-52-90-110-128.compute-1.amazonaws.com:8080`（再次，请记住用您自己的主机域名替换此域名）。
- en: 'Remember that *you will be charged by Amazon* for usage of the cluster. Don''t
    forget to stop or terminate this test cluster once you''re done with it. To do
    this, you can first exit the `ssh` session by typing `exit` to return to your
    own local system and then run the following command:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 记住*您将被Amazon收费*用于使用集群。完成测试后，请不要忘记停止或终止此测试集群。要执行此操作，您可以首先通过键入`exit`退出`ssh`会话，返回到您自己的本地系统，然后运行以下命令：
- en: '[PRE71]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'You should see the following output:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下输出：
- en: '[PRE72]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Hit *Y* and then *Enter* to destroy the cluster.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 按*Y*然后按*Enter*来销毁集群。
- en: Congratulations! You've just set up a Spark cluster in the cloud, run a fully
    parallel example program on this cluster, and terminated it. If you would like
    to try out any of the example code in the subsequent chapters (or your own Spark
    programs) on a cluster, feel free to experiment with the Spark EC2 scripts and
    launch a cluster of your chosen size and instance profile. (Just be mindful of
    the costs and remember to shut it down when you're done!)
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您刚刚在云中设置了一个Spark集群，在该集群上运行了一个完全并行的示例程序，并终止了它。如果您想在集群上尝试后续章节中的任何示例代码（或您自己的Spark程序），请随时尝试使用Spark
    EC2脚本并启动您选择大小和实例配置文件的集群。（只需注意成本，并在完成后记得关闭它！）
- en: Configuring and running Spark on Amazon Elastic Map Reduce
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Amazon Elastic Map Reduce上配置和运行Spark
- en: 'Launch a Hadoop cluster with Spark installed using the Amazon Elastic Map Reduce.
    Perform the following steps to create an EMR cluster with Spark installed:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Amazon Elastic Map Reduce安装了Spark的Hadoop集群。执行以下步骤创建安装了Spark的EMR集群：
- en: Launch an Amazon EMR Cluster.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Amazon EMR集群。
- en: Open the Amazon EMR UI console at [https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/).
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/)上打开Amazon
    EMR UI控制台。
- en: 'Choose Create cluster:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择创建集群：
- en: '![](img/image_01_005.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_005.png)'
- en: 'Choose appropriate Amazon AMI Version 3.9.0 or later as shown in the following
    screenshot:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择适当的Amazon AMI版本3.9.0或更高版本，如下截图所示：
- en: '![](img/image_01_006.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_006.png)'
- en: For the applications to be installed field, choose Spark 1.5.2 or later from
    the list shown on the User Interface and click on Add.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要安装的应用程序字段中，从用户界面上显示的列表中选择Spark 1.5.2或更高版本，然后单击添加。
- en: 'Select other hardware options as necessary:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需要选择其他硬件选项：
- en: The Instance Type
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例类型
- en: The keypair to be used with SSH
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于SSH的密钥对
- en: Permissions
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权限
- en: IAM roles (Default orCustom)
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IAM角色（默认或自定义）
- en: 'Refer to the following screenshot:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下截图：
- en: '![](img/image_01_007.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_007.png)'
- en: 'Click on Create cluster. The cluster will start instantiating as shown in the
    following screenshot:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击创建集群。集群将开始实例化，如下截图所示：
- en: '![](img/image_01_008.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_008.png)'
- en: 'Log in into the master. Once the EMR cluster is ready, you can SSH into the
    master:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到主机。一旦EMR集群准备就绪，您可以SSH登录到主机：
- en: '[PRE73]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The output will be similar to following listing:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下清单：
- en: '[PRE74]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Start the Spark Shell:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark Shell：
- en: '[PRE75]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Run Basic Spark sample from the EMR:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从EMR运行基本的Spark示例：
- en: '[PRE76]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Your output will be as follows:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 您的输出将如下所示：
- en: '[PRE77]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: UI in Spark
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark中的UI
- en: Spark provides a web interface which can be used to monitor jobs, see the environment,
    and run SQL commands.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了一个Web界面，可用于监视作业，查看环境并运行SQL命令。
- en: '`SparkContext` launches a web UI on port `4040` that displays useful information
    about the application. This includes the following:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkContext`在端口`4040`上启动Web UI，显示有关应用程序的有用信息。这包括以下内容：'
- en: A list of scheduler stages and tasks
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度程序阶段和任务的列表
- en: A summary of RDD sizes and memory usage
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD大小和内存使用情况摘要
- en: Environmental information
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境信息
- en: Information about the running executors
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关正在运行的执行程序的信息
- en: This interface can be accessed by going to `http://<driver-node>:4040` in a
    web browser. If multiple `SparkContexts` are running on the same host, they will
    bind to ports beginning with port `4040` (`4041`, `4042`, and so on).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在Web浏览器中转到`http://<driver-node>:4040`来访问此界面。如果在同一主机上运行多个`SparkContexts`，它们将绑定到以端口`4040`（`4041`，`4042`等）开头的端口。
- en: 'The following screenshots display some of the information provided by the Web
    UI:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示Web UI提供的一些信息：
- en: '![](img/image_01_009.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_009.png)'
- en: UI showing the Environment of the Spark Content
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 显示Spark内容环境的UI
- en: '![](img/image_01_010.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_010.png)'
- en: UI table showing Executors available
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 显示可用执行程序的UI表
- en: Supported machine learning algorithms by Spark
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark支持的机器学习算法
- en: 'The following algorithms are supported by Spark ML:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 以下算法由Spark ML支持：
- en: '**Collaborative filtering**'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同过滤**'
- en: '**Alternating Least Squares (ALS):** Collaborative filtering is often used
    for recommender systems. These techniques aim to fill the missing entries of a
    user-item association matrix. The `spark.mllib` currently supports model-based
    collaborative filtering. In this implementation, users and products are described
    by a small set of latent factors that can be used to predict missing entries.
    The `spark.mllib` uses the ALS algorithm to learn these latent factors.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交替最小二乘法（ALS）：** 协同过滤经常用于推荐系统。这些技术旨在填补用户-项目关联矩阵的缺失条目。`spark.mllib`目前支持基于模型的协同过滤。在这种实现中，用户和产品由一小组潜在因子描述，这些因子可以用来预测缺失的条目。`spark.mllib`使用ALS算法来学习这些潜在因子。'
- en: '**Clustering**: This is an unsupervised learning problem where the aim is to
    group subsets of entities with one another based on the notion of similarity.
    Clustering is used for exploratory analysis and as a component of a hierarchical
    supervised learning pipeline. When used in a learning pipeline, distinct classifiers
    or regression models are trained for each cluster. The following clustering techniques
    are implemented in Spark:'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：这是一个无监督学习问题，其目的是基于相似性的概念将实体的子集分组在一起。聚类用于探索性分析，并作为分层监督学习流水线的组成部分。在学习流水线中，为每个簇训练不同的分类器或回归模型。以下聚类技术在Spark中实现：'
- en: '**k-means**: This is one of the commonly used clustering algorithms that cluster
    the data points into a predefined number of clusters. It is up to the user to
    choose the number of clusters. The `spark.mllib` implementation includes a parallelized
    variant of the k-means++ method ([http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf)).'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**k均值**：这是一种常用的聚类算法，将数据点聚类到预定义数量的簇中。用户可以选择簇的数量。`spark.mllib`的实现包括k均值++方法的并行化变体（[http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf)）。'
- en: '**Gaussian mixture**: A **Gaussian Mixture Model** (**GMM**) represents a composite
    distribution where points are taken from one of the k Gaussian sub-distributions.
    Each of these distributions has its own probability. The `spark.mllib` implementation
    uses the expectation-maximization algorithm to induce the maximum-likelihood model
    given a set of samples.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯混合**：**高斯混合模型**（**GMM**）表示一个复合分布，其中的点来自k个高斯子分布之一。每个分布都有自己的概率。`spark.mllib`的实现使用期望最大化算法来诱导给定一组样本的最大似然模型。'
- en: '**Power Iteration Clustering (PIC)**: This is a scalable algorithm for clustering
    vertices of a graph given pairwise similarities as edge properties. It computes
    a pseudo-eigenvector of the (affinity matrix which is normalized) of the graph
    using power iteration.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幂迭代聚类（PIC）**：这是一种可扩展的算法，用于根据边属性的成对相似性对图的顶点进行聚类。它使用幂迭代计算图的（归一化的亲和矩阵的）伪特征向量。'
- en: Power iteration is an eigenvalue algorithm. Given a matrix *X*, the algorithm
    will produce a number*λ* ( eigenvalue) and a non-zero vector*v* (the eigenvector),
    such that*Xv = λv*.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 幂迭代是一种特征值算法。给定一个矩阵*X*，该算法将产生一个数字*λ*（特征值）和一个非零向量*v*（特征向量），使得*Xv = λv*。
- en: 'Pseudo eigenvectors of a matrix can be thought of as the eigenvectors of a
    nearby matrix. More specifically, pseudo eigenvectors are defined as:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的伪特征向量可以被视为附近矩阵的特征向量。更具体地说，伪特征向量被定义为：
- en: Let *A* be an *n* by *n* matrix. Let *E* be any matrix such that *||E|| = €*.
    Then the eigenvectors of *A + E* are defined to be pseudo-eigenvectors of *A*.
    This eigenvector uses it to cluster graph vertices.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 设*A*为一个*n*乘以*n*的矩阵。设*E*为任何矩阵，使得*||E|| = €*。那么*A + E*的特征向量被定义为*A*的伪特征向量。这个特征向量用于对图顶点进行聚类。
- en: The `spark.mllib` includes an implementation of PIC using *GraphX*. It takes
    an RDD of tuples and outputs a model with the clustering assignments. The similarities
    must be non-negative. PIC makes the assumption that the similarity measure is
    symmetric.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.mllib`包括使用*GraphX*实现的PIC。它接受一个元组的RDD，并输出具有聚类分配的模型。相似性必须是非负的。PIC假设相似性度量是对称的。'
- en: (In statistics, a similarity measure or similarity function is a real-valued
    function that quantifies the similarity between two objects. Such measures are
    inverse of distance metrics; an example of this is the Cosine similarity)
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: （在统计学中，相似性度量或相似性函数是一种实值函数，用于量化两个对象之间的相似性。这些度量是距离度量的倒数；其中的一个例子是余弦相似性）
- en: A pair (`srcId`, `dstId`) regardless of the ordering should appear at the most
    once in the input data.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 无论顺序如何，输入数据中的一对（`srcId`，`dstId`）应该最多出现一次。
- en: '**Latent Dirichlet Allocation** (**LDA**): This is a form of a topic model
    that infers topics from a collection of text documents. LDA is a form clustering
    algorithm. The following points explain the topics:'
  id: totrans-356
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）：这是一种从文本文档集合中推断主题的主题模型。LDA是一种聚类算法。以下几点解释了主题：'
- en: Topics are cluster centers and documents correspond to examples in a dataset
    Topics and documents both exist in a feature space, where feature vectors are
    vectors of word counts ( also known as bag of words)
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 主题是聚类中心，文档对应于数据集中的示例。主题和文档都存在于特征空间中，其中特征向量是词频向量（也称为词袋）。
- en: Instead of estimating a clustering using a traditional distance approach, LDA
    uses a function based on a model of how text documents are generated
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: LDA不使用传统的距离方法来估计聚类，而是使用基于文本文档生成模型的函数。
- en: '**Bisecting k-means**: This is a type of hierarchical clustering. **Hierarchical
    Cluster Analysis** (**HCA**) is a method of cluster analysis that builds a hierarchy
    of clusters*top down*. In this approach, all observations start in one cluster
    and splits are performed recursively as one moves down the hierarchy.'
  id: totrans-359
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二分k均值**：这是一种层次聚类的类型。**层次聚类分析**（**HCA**）是一种构建聚类层次结构的聚类分析方法。在这种方法中，所有观察开始在一个簇中，并且随着向下移动层次结构，递归地执行分裂。'
- en: Hierarchical clustering is one of the commonly used methods of cluster analysis
    that seek to build a hierarchy of clusters.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类是一种常用的聚类分析方法，旨在构建一个集群的层次结构。
- en: '**Streaming k-means**: When data arrives in a stream, we want to estimate clusters
    dynamically and update them as new data arrives. The `spark.mllib` supports streaming
    k-means clustering, with parameters to control the decay of the estimates. The
    algorithm uses a generalization of the mini-batch k-means update rule.'
  id: totrans-361
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式k均值：**当数据以流的形式到达时，我们希望动态估计集群并在新数据到达时更新它们。`spark.mllib`支持流式k均值聚类，具有控制估计衰减的参数。该算法使用小批量k均值更新规则的泛化。'
- en: '**Classification**'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**'
- en: '**Decision Trees:** Decision trees and their ensembles are one of the methods
    for classification and regression. Decision trees are popular as they are easy
    to interpret, handle categorical features, and extend to the multiclass classification
    setting. They do not require feature scaling and are also able to capture non-linearities
    and feature interactions. Tree ensemble algorithms, random forests and boosting
    are among the top performers for classification and regression scenarios.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策树：**决策树及其集成是分类和回归的方法之一。决策树很受欢迎，因为它们易于解释，处理分类特征，并扩展到多类分类设置。它们不需要特征缩放，也能捕捉非线性和特征交互。树集成算法，随机森林和提升是分类和回归场景中的顶级表现者之一。'
- en: The `spark.mllib` implements decision trees for binary and multiclass classification
    and regression. It supports both continuous and categorical features. The implementation
    partitions data by rows, which allows distributed training with millions of instances.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.mllib`实现了用于二元和多类分类和回归的决策树。它支持连续和分类特征。该实现通过行对数据进行分区，从而允许使用数百万个实例进行分布式训练。'
- en: '**Naive Bayes**: Naive Bayes classifiers are a family of simple probabilistic
    classifiers based on applying Bayes'' theorem ([https://en.wikipedia.org/wiki/Bayes%27_theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem))
    with strong (naive) independence assumptions between the features.'
  id: totrans-365
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯：**朴素贝叶斯分类器是一类简单的概率分类器，基于应用贝叶斯定理（[https://en.wikipedia.org/wiki/Bayes%27_theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)）并假设特征之间有强（朴素）独立性。'
- en: Naive Bayes is a multiclass classification algorithm with the assumption of
    independence between every pair of features. In a single pass of training data,
    the algorithm computes the conditional probability distribution of each feature
    given the label, and then it applies Bayes' theorem to compute the conditional
    probability distribution of a label given an observation, which is then used for
    prediction. The `spark.mllib` supports multinomial naive Bayes and Bernoulli Naive
    Bayes. These models are generally used for document classification.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一种多类分类算法，假设每对特征之间都是独立的。在训练数据的单次传递中，该算法计算每个特征在给定标签的条件概率分布，然后应用贝叶斯定理来计算给定观察结果的标签的条件概率分布，然后用于预测。`spark.mllib`支持多项式朴素贝叶斯和伯努利朴素贝叶斯。这些模型通常用于文档分类。
- en: '**Probabil****ity Classifier**: In machine learning, a probabilistic classifier
    is a classifier that can predict, given an input, a probability distribution over
    a set of classes, rather than outputting the most likely class that the sample
    should belong to. Probabilistic classifiers provide classification with some certainty,
    which can be useful on its own or when combining classifiers into ensembles.'
  id: totrans-367
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率分类器：**在机器学习中，概率分类器是一种可以预测给定输入的类别集上的概率分布的分类器，而不是输出样本应属于的最有可能的类别。概率分类器提供一定程度的分类确定性，这在单独使用或将分类器组合成集成时可能会有用。'
- en: '**Logistical Regression**: This is a method used to predict a binary response.
    Logistic regression measures the relationship between the categorical dependent
    variable and independent variables by estimating probabilities using a logistical
    function. This function is a cumulative logistic distribution.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑回归：**这是一种用于预测二元响应的方法。逻辑回归通过估计概率来衡量分类因变量和自变量之间的关系，使用逻辑函数。这个函数是一个累积逻辑分布。'
- en: It is a special case of **Generalized Linear Models** (**GLM**) that predicts
    the probability of the outcome. For more background and more details about the
    implementation, refer to the documentation on the logistic regression in `spark.mllib`.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这是**广义线性模型**（**GLM**）的特例，用于预测结果的概率。有关更多背景和实施细节，请参阅`spark.mllib`中关于逻辑回归的文档。
- en: GLM is considered a generalization of linear regression that allows for response
    variables that have an error distribution other than a normal distribution.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: GLM被认为是允许具有非正态分布的响应变量的线性回归的泛化。
- en: '**Random Forest**: This algorithms use ensembles of decision trees to decide
    decision boundaries. Random forests combine many decision trees. This reduces
    the risk of overfitting the result.'
  id: totrans-371
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机森林：**这些算法使用决策树的集成来决定决策边界。随机森林结合了许多决策树。这降低了过拟合的风险。'
- en: Spark ML supports random forest for binary and multi-class classification as
    well as regression. It can use used for continuous or categorical values.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML支持用于二元和多类分类以及回归的随机森林。它可以用于连续或分类值。
- en: '**Dimensionality reduction**: This is the process of reducing the number of
    variables on which machine learning will be done. It can be used to extract latent
    features from raw features or to compress data while maintaining the overall structure.
    MLlib provides support dimensionality reduction on top of the `RowMatrix` class.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维：**这是减少机器学习变量数量的过程。它可以用于从原始特征中提取潜在特征，或者在保持整体结构的同时压缩数据。MLlib在`RowMatrix`类的基础上支持降维。'
- en: '**Singular value decomposition (SVD)**: Singular value decomposition of a matrix
    *M: m x n* (real or complex) is a factorization of the form *UΣV**, where *U*
    is an*m x R* matrix. *Σ* is an *R x R* rectangular diagonal matrix with non-negative
    real numbers on the diagonal, and *V* is an *n x r* unitary matrix. *r* is equal
    to the rank of the matrix *M*.'
  id: totrans-374
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奇异值分解（SVD）**：矩阵*M：m x n*（实数或复数）的奇异值分解是一个形式为*UΣV**的分解，其中*U*是一个*m x R*矩阵。*Σ*是一个*R
    x R*的矩形对角矩阵，对角线上有非负实数，*V*是一个*n x r*的酉矩阵。*r*等于矩阵*M*的秩。'
- en: '**Principal component analysis (PCA)**: This is a statistical method used to
    find a rotation to find largest variance in the first coordinate. Each succeeding
    coordinate, in turn, has the largest variance possible. The columns of the rotation
    matrix are called principal components. PCA is used widely in dimensionality reduction.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析（PCA）**：这是一种统计方法，用于找到一个旋转，使得第一个坐标轴上的方差最大。依次，每个后续坐标轴的方差都尽可能大。旋转矩阵的列称为主成分。PCA在降维中被广泛使用。'
- en: MLlib supports PCA for tall-and-skinny matrices stored in row-oriented format
    using `RowMatrix`.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib支持使用`RowMatrix`对以行为导向格式存储的高瘦矩阵进行PCA。
- en: Spark supports features extraction and transforation using TF-IDF, ChiSquare,
    Selector, Normalizer, and Word2Vector.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持使用TF-IDF、ChiSquare、Selector、Normalizer和Word2Vector进行特征提取和转换。
- en: '**Frequent pattern mining**:'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频繁模式挖掘**'
- en: '**FP-growth**: FP stands for frequent pattern. Algorithm first counts item
    occurrences (attribute and value pairs) in the dataset and stores them in the
    header table.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FP-growth**：FP代表频繁模式。算法首先计算数据集中项（属性和值对）的出现次数，并将它们存储在头表中。'
- en: In the second pass, the algorithm builds the FP-tree structure by inserting
    instances (made of items). Items in each instance are sorted by descending order
    of their frequency in the dataset; this ensures that the tree can be processed
    quickly. Items in each instance that do not meet minimum coverage threshold are
    discarded. For a use case where many instances share most frequent items, the
    FP-tree provides high compression close to the tree root.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次遍历中，算法通过插入实例（由项组成）来构建FP树结构。每个实例中的项按其在数据集中的频率降序排序；这确保了树可以快速处理。不满足最小覆盖阈值的实例中的项将被丢弃。对于许多实例共享最频繁项的用例，FP树在树根附近提供了高压缩。
- en: '**Association rules**: Association rule learning is a mechanism for discovering
    interesting relations between variables in large databases.'
  id: totrans-381
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关联规则**：关联规则学习是一种在大型数据库中发现变量之间有趣关系的机制。'
- en: It implements a parallel rule generation algorithm for constructing rules that
    have a single item as the consequent.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 它实现了一个并行规则生成算法，用于构建具有单个项作为结论的规则。
- en: '**PrefixSpan**: This is a sequential pattern mining algorithm.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PrefixSpan**：这是一种序列模式挖掘算法。'
- en: '**Evaluation metrics**: The `spark.mllib` comes with a suite of metrics for
    evaluating the algorithms.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估指标**：`spark.mllib`配备了一套用于评估算法的指标。'
- en: '****PMML model export****: The **Predictive Model Markup Language** (**PMML**)
    is an XML-based predictive model interchange format. PMML provides a mechanism
    for analytic applications to describe and exchange predictive models produced
    by machine learning algorithms.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '****PMML模型导出****：**预测模型标记语言**（**PMML**）是一种基于XML的预测模型交换格式。PMML提供了一种机制，使分析应用程序能够描述和交换由机器学习算法产生的预测模型。'
- en: The `spark.mllib` allows the export of its machine learning models to PMML and
    their equivalent PMML models.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.mllib`允许将其机器学习模型导出为PMML及其等效的PMML模型。'
- en: '**Optimization (Developer)**'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化（开发人员）**'
- en: '**Stochastic Gradient Descent**: This is used to optimize gradient descent
    to minimize an objective function; this function is a sum of differentiable functions.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降**：这用于优化梯度下降以最小化目标函数；该函数是可微函数的和。'
- en: Gradient descent methods and the **Stochastic Subgradient Descent** (**SGD**)
    are included as a low-level primitive in MLlib, on top of which various ML algorithms
    are developed.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降方法和**随机次梯度下降**（**SGD**）作为MLlib中的低级原语，各种ML算法都是在其之上开发的。
- en: '**Limited-Memory BFGS (L-BFGS)**: This is an optimization algorithm and belongs
    to the family of quasi-Newton methods that approximates the **Broyden-Fletcher-Goldfarb-Shanno**
    (**BFGS**) algorithm. It uses a limited amount of computer memory. It is used
    for parameter estimation in machine learning.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限内存BFGS（L-BFGS）**：这是一种优化算法，属于拟牛顿方法家族，近似**Broyden-Fletcher-Goldfarb-Shanno**（**BFGS**）算法。它使用有限的计算机内存。它用于机器学习中的参数估计。'
- en: The BFGS method approximates Newton's method, which is a class of hill-climbing
    optimization techniques that seeks a stationary point of a function. For such
    problems, a necessary optimal condition is that the gradient should be zero**.**
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: BFGS方法近似牛顿法，这是一类寻找函数的稳定点的爬山优化技术。对于这样的问题，一个必要的最优条件是梯度应该为零**。**
- en: Benefits of using Spark ML as compared to existing libraries
  id: totrans-392
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与现有库相比，使用Spark ML的好处
- en: AMQ Lab at Berkley Evaluated Spark, and RDDs were evaluated through a series
    of experiments on Amazon EC2 as well as benchmarks of user applications.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 伯克利的AMQ实验室评估了Spark，并通过一系列在Amazon EC2上的实验以及用户应用程序的基准测试来评估了RDD。
- en: '**Algorithms used**: Logistical Regression and k-means'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用的算法**：逻辑回归和k-means'
- en: '**Use case**: First iteration, multiple iterations.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用例**：第一次迭代，多次迭代。'
- en: 'All the tests used `m1.xlarge` EC2 nodes with 4 cores and 15 GB of RAM. HDFS
    was for storage with 256 MB blocks. Refer to the following graph:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 所有测试都使用了具有4个核心和15 GB RAM的`m1.xlarge` EC2节点。HDFS用于存储，块大小为256 MB。参考以下图表：
- en: '![](img/image_01_011.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_011.png)'
- en: 'The preceding graph shows the comparison between the performance of Hadoop
    and Spark for the first and subsequent iteration for **Logistical Regression**:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表显示了Hadoop和Spark在**逻辑回归**的第一次和后续迭代中的性能比较：
- en: '![](img/image_01_012.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_012.png)'
- en: The preceding graph shows the comparison between the performance of Hadoop and
    Spark for the first and subsequent iteration for K Means clustering algorithm.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了K均值聚类算法的第一次和后续迭代中Hadoop和Spark的性能比较。
- en: 'The overall results show the following:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 总体结果如下：
- en: Spark outperforms Hadoop by up to 20 times in iterative machine learning and
    graph applications. The speedup comes from avoiding I/O and deserialization costs
    by storing data in memory as Java objects.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark在迭代式机器学习和图应用程序中的性能比Hadoop快20倍。这种加速来自于通过将数据存储在内存中作为Java对象来避免I/O和反序列化成本。
- en: The applications written perform and scale well. Spark can speed up an analytics
    report that was running on Hadoop by 40 times.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写的应用程序表现良好并且扩展性好。Spark可以将在Hadoop上运行的分析报告加速40倍。
- en: When nodes fail, Spark can recover quickly by rebuilding only the lost RDD partitions.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当节点失败时，Spark可以通过仅重建丢失的RDD分区来快速恢复。
- en: Spark was be used to query a 1-TB dataset interactively with latencies of 5-7
    seconds.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark被用来与1TB数据集进行交互式查询，延迟为5-7秒。
- en: For more information, go to [http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf](http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请访问[http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf](http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf)。
- en: Spark versus Hadoop for a SORT Benchmark--In 2014, the Databricks team participated
    in a SORT benchmark test ([http://sortbenchmark.org/](http://sortbenchmark.org/)).
    This was done on a 100-TB dataset. Hadoop was running in a dedicated data center
    and a Spark cluster of over 200 nodes was run on EC2\. Spark was run on HDFS distributed
    storage.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: Spark与Hadoop的SORT基准测试-2014年，Databricks团队参加了SORT基准测试（[http://sortbenchmark.org/](http://sortbenchmark.org/)）。这是在一个100TB数据集上进行的。Hadoop在一个专用数据中心运行，而Spark集群在EC2上运行了200多个节点。Spark在HDFS分布式存储上运行。
- en: 'Spark was 3 times faster than Hadoop and used 10 times fewer machines. Refer
    to the following graph:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: Spark比Hadoop快3倍，并且使用的机器数量少10倍。请参考以下图表：
- en: '![](img/image_01_013.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_013.png)'
- en: Spark Cluster on Google Compute Engine - DataProc
  id: totrans-410
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Compute Engine上的Spark集群-DataProc
- en: '**Cloud Dataproc** is a Spark and Hadoop service running on Google Compute
    Engine. It is a managed service. Cloud Dataproc automation helps create clusters
    quickly, manage them easily, and save money by turning clusters off when you don''t
    need them.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '**Cloud Dataproc**是在Google Compute Engine上运行的Spark和Hadoop服务。这是一个托管服务。Cloud
    Dataproc自动化帮助快速创建集群，轻松管理它们，并在您不需要它们时关闭集群以节省费用。'
- en: In this section, we will learn how to create a Spark cluster using DataProc
    and running a Sample app on it.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用DataProc创建一个Spark集群，并在其上运行一个示例应用程序。
- en: Make sure that you have created a Google Compute Engine account and installed
    Google Cloud SDK ([https://cloud.google.com/sdk/gcloud/](https://cloud.google.com/sdk/gcloud/)).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已经创建了Google Compute Engine帐户并安装了Google Cloud SDK ([https://cloud.google.com/sdk/gcloud/](https://cloud.google.com/sdk/gcloud/))。
- en: Hadoop and Spark Versions
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop和Spark版本
- en: 'DataProc supports the following Hadoop and Spark versions. Note that this will
    change with time as new versions come out:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: DataProc支持以下Hadoop和Spark版本。请注意，随着新版本的推出，这些将会发生变化：
- en: Spark 1.5.2
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 1.5.2
- en: Hadoop 2.7.1
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 2.7.1
- en: Pig 0.15.0
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pig 0.15.0
- en: Hive 1.2.1
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive 1.2.1
- en: GCS connector 1.4.3-hadoop2
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCS连接器1.4.3-hadoop2
- en: BigQuery connector 0.7.3-hadoop2 ([https://github.com/GoogleCloudPlatform/bigdata-interop](https://github.com/GoogleCloudPlatform/bigdata-interop))
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BigQuery连接器0.7.3-hadoop2 ([https://github.com/GoogleCloudPlatform/bigdata-interop](https://github.com/GoogleCloudPlatform/bigdata-interop))
- en: For more information, go to [http://cloud.google.com/dataproc-versions](http://cloud.google.com/dataproc-versions).
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请访问[http://cloud.google.com/dataproc-versions](http://cloud.google.com/dataproc-versions)。
- en: In the following steps, we will use Google Cloud Console (the user interface
    used to create a Spark Cluster and submit a job).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将使用Google Cloud控制台（用于创建Spark集群和提交作业的用户界面）。
- en: Creating a Cluster
  id: totrans-424
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建集群
- en: You can create a Spark cluster by going to the Cloud Platform Console. Select
    the project, and then click Continue to open the Clusters page. You would see
    the Cloud Dataproc clusters that belong to your project, if you have created any.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过转到Cloud平台控制台来创建一个Spark集群。选择项目，然后点击“继续”打开“集群”页面。如果您已经创建了任何集群，您将看到属于您项目的Cloud
    Dataproc集群。
- en: 'Click on the Create a cluster button to open the Create a Cloud Data pros cluster
    page. Refer to the following screenshot:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“创建集群”按钮，打开“创建Cloud Data pros集群”页面。请参考以下截图：
- en: '![](img/image_01_014.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_014.png)'
- en: 'Once you click on Create a cluster, a detailed form, which is as shown in the
    following screenshot, shows up:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您点击“创建集群”，一个详细的表单将显示出来，如下截图所示：
- en: '![](img/image_01_015.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_015.png)'
- en: 'The previous screenshot shows the Create a Cloud Dataproc cluster page with
    the default fields automatically filled in for a new cluster-1 cluster. Take a
    look at the following screenshot:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图显示了默认字段自动填充为新集群-1集群的“创建Cloud Dataproc集群”页面。请看以下截图：
- en: '![](img/image_01_016.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_016.png)'
- en: You can expand the workers, bucket, network, version, initialization, and access
    options panel to specify one or more worker nodes, a staging bucket, network,
    initialization, the Cloud Dataproc image version, actions, and project-level access
    for your cluster. Providing these values is optional.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以展开工作节点、存储桶、网络、版本、初始化和访问选项面板，以指定一个或多个工作节点、一个暂存桶、网络、初始化、Cloud Dataproc镜像版本、操作和项目级别的访问权限。提供这些值是可选的。
- en: 'The default cluster is created with no worker nodes, an auto-created staging
    bucket, and a default network It also has the latest released Cloud Dataproc image
    version. You can change these default settings:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 默认集群创建时没有工作节点，自动创建的暂存桶和默认网络。它还具有最新发布的Cloud Dataproc镜像版本。您可以更改这些默认设置：
- en: '![](img/image_01_017.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_017.png)'
- en: Once you have configured all fields on the page, click on the Create button
    to create the cluster. The cluster name created appears on the Clusters page.
    The status is updated to Running once the spark cluster is created.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在页面上配置所有字段后，点击“创建”按钮创建集群。创建的集群名称将显示在集群页面上。一旦创建了Spark集群，状态就会更新为“运行”。
- en: Click on the cluster name created earlier to open the cluster details page.
    It also has a Overview tab and the CPU utilization graph selected.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 点击之前创建的集群名称以打开集群详情页面。它还有一个概述选项卡和CPU利用率图表选定。
- en: You can examine jobs, instances, and so on for the cluster from the other tabs.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从其他选项卡查看集群的作业、实例等。
- en: Submitting a Job
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提交作业
- en: 'To submit a job from the Cloud Platform Console to the cluster, go to the Cloud
    Platform UI. Select the appropriate project and then click on Continue. The first
    time you submit a job, the following dialog appears:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 要从Cloud Platform控制台向集群提交作业，请转到Cloud Platform UI。选择适当的项目，然后点击“继续”。第一次提交作业时，会出现以下对话框：
- en: '![](img/image_01_018.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_018.png)'
- en: 'Click on Submit a job:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“提交作业”：
- en: '![](img/image_01_019.png)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_019.png)'
- en: 'To submit a Spark sample job, fill the fields on the Submit a job page, as
    follows:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 要提交一个Spark示例作业，请在“提交作业”页面上填写字段，如下所示：
- en: Select a cluster name from the cluster list on the screen.
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕上的集群列表中选择一个集群名称。
- en: Set Job type toSpark.
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将作业类型设置为Spark。
- en: Add `file:///usr/lib/spark/lib/spark-examples.jar` to Jar files. Here, `file:///`
    denotes a Hadoop `LocalFileSystem` scheme; Cloud Dataproc installs `/usr/lib/spark/lib/spark-examples.jar`
    on the cluster's master node when it creates the cluster. Alternatively, you can
    specify a Cloud Storage path (`gs://my-bucket/my-jarfile.jar`) or an `HDFS` path
    (`hdfs://examples/myexample.jar`) to one of the custom jars.
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`file:///usr/lib/spark/lib/spark-examples.jar`添加到Jar文件。这里，`file:///`表示Hadoop的`LocalFileSystem`方案；Cloud
    Dataproc在创建集群时会在主节点上安装`/usr/lib/spark/lib/spark-examples.jar`。或者，您可以指定一个Cloud
    Storage路径（`gs://my-bucket/my-jarfile.jar`）或一个`HDFS`路径（`hdfs://examples/myexample.jar`）到其中一个自定义的jar。
- en: Set `Main` class or jar to `org.apache.spark.examples.SparkPi`.
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Main`类或jar设置为`org.apache.spark.examples.SparkPi`。
- en: Set Arguments to the single argument `1000`.
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将参数设置为单个参数`1000`。
- en: Click on Submit to start the job.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 点击提交以开始作业。
- en: 'Once the job starts, it is added to the Jobs list. Refer to the following screenshot:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 作业开始后，它将被添加到作业列表中。请参考以下截图：
- en: '![](img/image_01_020.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_020.png)'
- en: 'Once the job is complete, its status changes:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 作业完成后，其状态会发生变化：
- en: '![](img/image_01_021.png)'
  id: totrans-453
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_01_021.png)'
- en: Take a look at the `job` output as listed here.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 查看此处列出的`job`输出。
- en: Execute the command from the terminal with the appropriate Job ID.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 从终端执行带有适当作业ID的命令。
- en: 'In our case, the Job ID was `1ed4d07f-55fc-45fe-a565-290dcd1978f7` and project-ID
    was `rd-spark-1`; hence, the command looks like this:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，作业ID是`1ed4d07f-55fc-45fe-a565-290dcd1978f7`，项目ID是`rd-spark-1`；因此，命令如下所示：
- en: '[PRE78]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The (abridged) output is shown here:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: （删节）输出如下所示：
- en: '[PRE79]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: You can also SSH into the Spark Instance and run spark-shell in the interactive
    mode.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过SSH连接到Spark实例，并以交互模式运行spark-shell。
- en: Summary
  id: totrans-461
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered how to set up Spark locally on our own computer
    as well as in the cloud as a cluster running on Amazon EC2\. You learned how to
    run Spark on top of Amazon's **Elastic Map Reduce** (**EMR**). You also learned
    how to use Google Compute Engine's Spark Service to create a cluster and run a
    simple job. We discussed the basics of Spark's programming model and API using
    the interactive Scala console, and we wrote the same basic Spark program in Scala,
    Java, R, and Python. We also compared the performance metrics of Hadoop versus
    Spark for different machine learning algorithms as well as SORT benchmark tests.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何在我们自己的计算机上以及作为在云上运行的集群上本地设置Spark。您学习了如何在Amazon EC2上运行Spark。您还学习了如何使用Google
    Compute Engine的Spark服务来创建集群并运行简单作业。我们讨论了Spark的编程模型和API的基础知识，使用交互式Scala控制台编写了相同的基本Spark程序，并在Scala、Java、R和Python中编写了相同的基本Spark程序。我们还比较了不同机器学习算法的Hadoop与Spark的性能指标，以及SORT基准测试。
- en: In the next chapter, we will consider how to go about using Spark to create
    a machine learning system.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将考虑如何使用Spark创建一个机器学习系统。
