- en: Chapter 10. Story De-duplication and Mutation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。故事去重和变异
- en: How large is the World Wide Web? Although it is impossible to know the exact
    size - not to mention the Deep and Dark Web - it was estimated to hold more than
    a trillion pages in 2008, that, in the data era, was somehow the middle age. Almost
    a decade later, it is safe to assume that the Internet's collective brain has
    more neurons than our actual gray matter that's stuffed between our *ears*. But
    out of these trillion plus URLs, how many web pages are truly identical, similar,
    or covering the same topic?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 全球网络有多大？虽然几乎不可能知道确切的大小 - 更不用说深网和暗网了 - 但据估计，2008年它的页面数量超过了一万亿，那在数据时代，有点像中世纪。将近十年后，可以肯定地假设互联网的集体大脑比我们实际的灰质在我们的*耳朵*之间更多。但在这万亿以上的URL中，有多少网页是真正相同的，相似的，或者涵盖相同的主题？
- en: In this chapter, we will de-duplicate and index the GDELT database into stories.
    Then, we will track stories over time and understand the links between them, how
    they may mutate and if they could lead to any subsequent event in the near future.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将对GDELT数据库进行去重和索引，然后，我们将随时间跟踪故事，并了解它们之间的联系，它们可能如何变异，以及它们是否可能导致不久的将来发生任何后续事件。
- en: 'We will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Understand the concept of *Simhash* to detect near duplicates
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解*Simhash*的概念以检测近似重复
- en: Build an online de-duplication API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建在线去重API
- en: Build vectors using TF-IDF and reduce dimensionality using *Random Indexing*
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TF-IDF构建向量，并使用*随机索引*减少维度
- en: Build stories connection in pseudo real-time using Streaming KMeans
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流式KMeans实时构建故事连接
- en: Detecting near duplicates
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测近似重复
- en: 'While this chapter is about grouping articles into stories, this first section
    is all about detecting near duplicates. Before delving into the de-duplication
    algorithm itself, it is worth introducing the notion of story and de-duplication
    in the context of news articles. Given two distinct articles - by distinct we
    mean two different URLs - we may observe the following scenarios:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章是关于将文章分组成故事，但这一节是关于检测近似重复。在深入研究去重算法之前，值得介绍一下在新闻文章的背景下故事和去重的概念。给定两篇不同的文章
    - 通过不同的URL我们指的是两个不同的URL - 我们可能会观察到以下情况：
- en: The URL of article 1 actually redirects to article 2 or is an extension of the
    URL provided in article 2 (some additional URL parameters, for instance, or a
    shortened URL). Both articles with the same content are considered as *true duplicates*
    although their URLs are different.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文章1的URL实际上重定向到文章2，或者是文章2中提供的URL的扩展（例如一些额外的URL参数，或者缩短的URL）。尽管它们的URL不同，但具有相同内容的两篇文章被视为*真正的重复*。
- en: Both article 1 and article 2 are covering the exact same event, but could have
    been written by two different publishers. They share lots of content in common,
    but are not truly similar. Based on certain rules explained hereafter, they might
    be considered as *near-duplicates*.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文章1和文章2都涵盖了完全相同的事件，但可能由两个不同的出版商撰写。它们有很多共同的内容，但并不真正相似。根据下文解释的某些规则，它们可能被视为*近似重复*。
- en: Both article 1 and article 2 are covering the same type of event. We observe
    major differences in style or different *flavors* of the same topic. They could
    be grouped into a common *story*.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文章1和文章2都涵盖了相同类型的事件。我们观察到风格上的主要差异或相同主题的不同*风味*。它们可以被归为一个共同的*故事*。
- en: Both article 1 and article 2 are covering two different events. Both contents
    are *different* and should not be grouped within the same story, nor should they
    be considered as near duplicates.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文章1和文章2涵盖了两个不同的事件。两篇内容是*不同*的，不应该被归为同一个故事，也不应该被视为近似重复。
- en: Facebook users must have noticed the *related articles* feature. When you like
    a news article-click on an article's link or play an article's video, Facebook
    thinks this link is interesting and updates its timeline (or whatever it is called)
    to display more content that looks similar. In *Figure 1*, I was really amused
    to see the Samsung Galaxy Note 7 smartphone emitting smoke or catching fire, therefore
    banned from most of US flights. Facebook automatically suggested me similar articles
    around this Samsung fiasco. What probably happened is that by opening this link
    I may have queried the Facebook internal APIs and asked for similar content. Here
    comes the notion of finding near duplicates in real-time, and here is what we
    will try to build in the first section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook用户一定注意到了*相关文章*功能。当你喜欢一篇新闻文章 - 点击一篇文章的链接或播放一篇文章的视频时，Facebook认为这个链接很有趣，并更新其时间线（或者称之为）以显示更多看起来相似的内容。在*图1*中，我真的很惊讶地看到三星Galaxy
    Note 7智能手机冒烟或着火，因此被大部分美国航班禁止。Facebook自动为我推荐了这个三星惨案周围的类似文章。可能发生的事情是，通过打开这个链接，我可能已经查询了Facebook内部API，并要求相似的内容。这就是实时查找近似重复的概念，这也是我们将在第一节中尝试构建的内容。
- en: '![Detecting near duplicates](img/image_10_001.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![检测近似重复](img/image_10_001.jpg)'
- en: 'Figure 1: Facebook suggesting related articles'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Facebook推荐相关文章
- en: First steps with hashing
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哈希处理的第一步
- en: 'Finding true duplicates is easy. Two articles will be considered as identical
    if their content is the same. But instead of comparing strings (that may be large
    and therefore not efficient), we can compare their hash values just like one would
    compare hand signatures; two articles having the same signature should be considered
    as identical. A simple `groupBy` function would detect the true duplicatess out
    of a string array as shown as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 查找真正的重复很容易。如果两篇文章的内容相同，它们将被视为相同。但是，我们可以比较它们的哈希值，而不是比较字符串（可能很大，因此不高效）；就像比较手写签名一样；具有相同签名的两篇文章应被视为相同。如下所示，一个简单的`groupBy`函数将从字符串数组中检测出真正的重复：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'But even the most complex hash function leads to some collisions. Java''s built-in
    `hashCode` function is encoding a string into a 32-bit integer, which means that,
    in theory, we *only* have 2^(32) possibilities of getting different words sharing
    the same hash value. In practice, collisions should always be handled carefully,
    as, according to the *birthday paradox*, they will appear much more often than
    once every 2^(32) values. To prove our point, the following example considers
    the four different strings to be identical:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使是最复杂的哈希函数也会导致一些碰撞。Java内置的`hashCode`函数将字符串编码为32位整数，这意味着理论上，我们只有2^(32)种可能性可以得到相同哈希值的不同单词。实际上，碰撞应该始终小心处理，因为根据*生日悖论*，它们会比2^(32)的值更频繁地出现。为了证明我们的观点，以下示例认为四个不同的字符串是相同的：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Also, some articles may sometimes only differ by a very small portion of text,
    for example, a piece of advertisement, an additional footer, or an extra bit in
    the HTML code that make a hash signature different from almost identical content.
    In fact, even a minor typo on one single word would result in a total different
    hash value, making two near-duplicate articles to be considered as totally different.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有些文章有时可能只是在很小的文本部分上有所不同，例如广告片段、额外的页脚或HTML代码中的额外位，这使得哈希签名与几乎相同的内容不同。事实上，即使一个单词有一个小的拼写错误，也会导致完全不同的哈希值，使得两篇近似重复的文章被认为是完全不同的。
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Although the strings `Hello Spark` and `Hello, Spark` are really close (they
    only differ by 1 character), their hash values differ by 16-bits (out of 32).
    Luckily, the elders of the Internet may have found a solution to detect near duplicates
    using hash values.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管字符串`Hello Spark`和`Hello, Spark`非常接近（它们只相差一个字符），它们的哈希值相差16位（32位中的16位）。幸运的是，互联网的长者们可能已经找到了使用哈希值来检测近似重复内容的解决方案。
- en: Standing on the shoulders of the Internet giants
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 站在互联网巨头的肩膀上
- en: Needless to say, Google is fairly good at indexing webpages. With more than
    a trillion distinct URLs, detecting duplicates is the key when it comes to indexing
    web content. Surely the Internet giants must have developed techniques over the
    years to solve this problem of scale, hence limiting the amount of computing resources
    needed to index the whole Internet. One of these techniques described here is
    called *Simhash* and is so simple and neat, albeit so efficient, that it is worth
    knowing if you truly want to *Master Spark for data science*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不用说，谷歌在索引网页方面做得相当不错。拥有超过一万亿个不同的URL，检测重复内容是索引网页内容时的关键。毫无疑问，互联网巨头们多年来一定已经开发出了解决这个规模问题的技术，从而限制了索引整个互联网所需的计算资源。这里描述的其中一种技术称为*Simhash*，它非常简单、整洁，但效率很高，如果你真的想要*精通数据科学的Spark*，那么了解它是值得的。
- en: Note
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: More information about *Simhash* could be found at [http://www.wwwconference.org/www2007/papers/paper215.pdf](http://www.wwwconference.org/www2007/papers/paper215.pdf).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 关于*Simhash*的更多信息可以在[http://www.wwwconference.org/www2007/papers/paper215.pdf](http://www.wwwconference.org/www2007/papers/paper215.pdf)找到。
- en: Simhashing
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Simhashing
- en: 'The main idea behind **Simhash** is not to compute one single hash value at
    once, but rather to look at the article''s content and compute multiple individual
    hashes. For each word, each pair of word, or even each two-character shingle,
    we can easily compute hash values using the simple Java built-in `hashCode` function
    described earlier. In the following *Figure 2*, we report all the 32-bit hash
    values (the first 20 zero values omitted) of the two characters set included in
    the string **hello simhash**:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**Simhash**的主要思想不是一次计算一个单一的哈希值，而是查看文章的内容并计算多个单独的哈希值。对于每个单词，每对单词，甚至每个两个字符的shingle，我们都可以使用前面描述的简单的Java内置`hashCode`函数轻松计算哈希值。在下面的*图2*中，我们报告了字符串**hello
    simhash**中包含的两个字符集的所有32位哈希值（省略了前20个零值）：'
- en: '![Simhashing](img/B05261_10_02-1.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![Simhashing](img/B05261_10_02-1.jpg)'
- en: 'Figure 2: Building hello simhash shingles'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：构建hello simhash shingles
- en: 'A simple Scala implementation is reported next:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来报告了一个简单的Scala实现：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With all these hash values computed, we initialize a `Simhash` object as a zero
    integer. For each bit in that 32-bit integer, we count the number of hash values
    in our list with that particular bit set to 1 and subtract the number of values
    within that same list with that particular bit that is not set. This gives us
    the array reported in *Figure 3*. Finally, any value greater than 0 will be set
    to 1, any value lower or equal to 0 will be left as 0\. The only tricky part here
    is to work on bit shifting operations, but the algorithm itself is fairly trivial.
    Note that we use recursion here to avoid the use of mutable variables (using `var`)
    or lists.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 计算了所有这些哈希值后，我们将一个`Simhash`对象初始化为零整数。对于32位整数中的每个位，我们计算具有该特定位设置为1的哈希值的数量，并减去具有该列表中具有该特定位未设置的值的数量。这给我们提供了*图3*中报告的数组。最后，任何大于0的值都将设置为1，任何小于或等于0的值都将保留为0。这里唯一棘手的部分是进行位移操作，但算法本身相当简单。请注意，我们在这里使用递归来避免使用可变变量（使用`var`）或列表。
- en: '![Simhashing](img/B05261_10_03.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![Simhashing](img/B05261_10_03.jpg)'
- en: 'Figure 3: Building hello simhash'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：构建hello simhash
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The hamming weight
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 汉明重量
- en: It is easy to understand that the more words two articles have in common, the
    more both of them will share a same bit *b* set to 1 in their Simhash. But the
    beauty of Simhash comes with this aggregation step. Many other words in our corpus
    (hence other hashes) may not have this particular bit *b* set, hence making this
    value to also decrease when some different hashes are observed. Sharing a common
    set of words is not enough, similar articles must also share the same word frequency.
    The following example shows three Simhash values computed for the strings **hello
    simhash**, **hello minhash**, and **hello world**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易理解，两篇文章共有的单词越多，它们的Simhash中都会有一个相同的位*b*设置为1。但Simhash的美妙之处在于聚合步骤。我们语料库中的许多其他单词（因此其他哈希）可能没有设置这个特定的位*b*，因此当观察到一些不同的哈希时，这个值也会减少。共享一组共同的单词是不够的，相似的文章还必须共享相同的词频。以下示例显示了为字符串**hello
    simhash**、**hello minhash**和**hello world**计算的三个Simhash值。
- en: '![The hamming weight](img/B05261_10_04.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![汉明重量](img/B05261_10_04.jpg)'
- en: 'Figure 4: Comparing hello simhash'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：比较hello simhash
- en: When both **hello simhash** and **hello world** differ by 3-bits, **hello simhash** and
    **hello minhash** only differ by **1**. In fact, we can express the distance between
    them as the hamming weight of their EXCLUSIVE OR (**XOR**) product. **Hamming
    weight** is the number of bits we need to change in order to turn a given number
    into the zero element. The hamming weight of the **XOR** operation of two numbers
    is therefore the number of bits that differ between these two elements, **1**
    in that case.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当**hello simhash**和**hello world**之间的差异为3位时，**hello simhash**和**hello minhash**之间的差异只有**1**。实际上，我们可以将它们之间的距离表示为它们的异或（**XOR**）积的汉明重量。**汉明重量**是我们需要改变的位数，以将给定数字转换为零元素。因此，两个数字的**XOR**操作的汉明重量是这两个元素之间不同的位数，这种情况下是**1**。
- en: '![The hamming weight](img/B05261_10_05.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![汉明重量](img/B05261_10_05.jpg)'
- en: 'Figure 5: Hamming weight of hello simhash'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：hello simhash的汉明重量
- en: We simply use Java's `bitCount` function that returns the number of one-bits
    in the two's complement binary representation of the specified integer value.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单地使用Java的`bitCount`函数，该函数返回指定整数值的二进制补码表示中的一位数。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We have been able to successfully build Simhash and perform some simple pairwise
    comparison. The next step is to scale this up and start detecting actual duplicates
    out of the GDELT database.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功构建了Simhash并进行了一些简单的成对比较。下一步是扩展规模并开始从GDELT数据库中检测实际的重复项。
- en: Detecting near duplicates in GDELT
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在GDELT中检测近似重复项
- en: We covered the data acquisition process in depth in [Chapter 2](ch02.xhtml "Chapter 2. Data
    Acquisition"),* Data Acquisition*. For this use case, we will use a NiFi flow
    in *Figure 6* that listens to the GDELT master URL, fetches and unzips the latest
    GKG archive, and stores this file on HDFS in a compressed format.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第2章](ch02.xhtml "第2章。数据采集")中深入讨论了数据获取过程，*数据采集*。对于这个用例，我们将使用图6中的NiFi流，该流监听GDELT主URL，获取并解压最新的GKG存档，并以压缩格式将此文件存储在HDFS中。
- en: '![Detecting near duplicates in GDELT](img/image_10_006.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![在GDELT中检测近似重复项](img/image_10_006.jpg)'
- en: 'Figure 6: Downloading GKG data'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：下载GKG数据
- en: We first parse our GKG records using the set of parsers we created earlier (available
    in our GitHub repo), extract all the distinct URLs and fetch the HTML content
    using the Goose extractor introduced in [Chapter 6](ch06.xhtml "Chapter 6. Scraping
    Link-Based External Data") *,Scraping Link-Based External Data*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用我们之前创建的一组解析器（在我们的GitHub存储库中可用）解析我们的GKG记录，提取所有不同的URL并使用[第6章](ch06.xhtml
    "第6章。抓取基于链接的外部数据")中介绍的Goose提取器获取HTML内容，*抓取基于链接的外部数据*。
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Because the `hashcode` function is case sensitive *(Spark* and *spark* result
    in total different hash values), it is strongly recommended to clean our text
    prior to a `simhash` function. Similar to what was described in [Chapter 9](ch09.xhtml
    "Chapter 9.  News Dictionary and Real-Time Tagging System") *, News Dictionary
    and Real-Time Tagging System*, we first use the following Lucene analyzer to stem
    words:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`hashcode`函数是区分大小写的（*Spark*和*spark*会产生完全不同的哈希值），强烈建议在`simhash`函数之前清理文本。与[第9章](ch09.xhtml
    "第9章。新闻词典和实时标记系统")中描述的类似，我们首先使用以下Lucene分析器来词干化单词：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you may have noticed earlier, we wrote our Simhash algorithm inside of an
    implicit class; we can apply our `simhash` function directly on a string using
    the following import statement. A bit of extra effort taken at an early stage
    of development always pays off.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能早些时候注意到的，我们在一个隐式类中编写了我们的Simhash算法；我们可以使用以下导入语句直接在字符串上应用我们的`simhash`函数。在开发的早期阶段付出额外的努力总是值得的。
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We now have an RDD of content (`Content` being a case class wrapping the article
    URL, title and body) together with its Simhash value and a unique identifier we
    may be using later. Let's first try to validate our algorithm and find our first
    duplicates. From now on, we only consider as duplicates the articles that have
    no more than 2-bits difference in their 32-bit Simhash values.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个内容的RDD（`Content`是一个包装文章URL、标题和正文的案例类），以及它的Simhash值和一个稍后可能使用的唯一标识符。让我们首先尝试验证我们的算法并找到我们的第一个重复项。从现在开始，我们只考虑在它们的32位Simhash值中最多有2位差异的文章作为重复项。
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'But here comes a scalability challenge: we certainly do not want to execute
    a Cartesian product to compare pairwise articles from our Simhash RDD. Instead,
    we want to leverage the MapReduce paradigm (using a `groupByKey` function) and
    only group articles that are duplicates. Our approach is following an *expand-and-conquer* pattern
    where we first expand our initial dataset, leverage the Spark shuffle and then
    solve our problem locally at the executor level. As we only need to deal with
    1-bit difference (we will then apply the same logic for 2-bits), our strategy
    is to expand our RDD so that for each Simhash `s`, we output all the 31 other
    1-bit combinations of **s** using the same 1-bit mask.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 但这里出现了一个可伸缩性挑战：我们肯定不想执行笛卡尔积来比较Simhash RDD中的成对文章。相反，我们希望利用MapReduce范式（使用`groupByKey`函数）并且只对重复的文章进行分组。我们的方法遵循*扩展和征服*模式，首先扩展我们的初始数据集，利用Spark
    shuffle，然后在执行器级别解决我们的问题。因为我们只需要处理1位差异（然后我们将对2位应用相同的逻辑），所以我们的策略是扩展我们的RDD，以便对于每个Simhash`s`，我们使用相同的1位掩码输出所有其他31个1位组合。
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Taking a Simhash value `s`, we output the possible 1-bit combinations of **s**
    using a XOR between each preceding mask and the Simhash value `s`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Simhash值`s`，我们使用每个前置掩码和Simhash值`s`之间的XOR输出可能的1位组合。
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Dealing with 2-bits is not that different, although a bit more aggressive in
    terms of scalability (we now have 496 possible combinations to output, meaning
    any combination of 2-bits out of 32).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 处理2位并没有太大的不同，尽管在可伸缩性方面更加激进（现在有496种可能的组合要输出，意味着32位中的任意2位组合）。
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we build our set of masks to apply (note that we also want to output
    the original Simhash by applying a 0-bit difference mask) in order to detect duplicates
    as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们构建我们的掩码集以应用（请注意，我们还希望通过应用0位差异掩码输出原始Simhash）以检测重复，如下所示：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This also helps us expand our initial RDD accordingly. This surely is an expensive
    operation as it increases the size of our RDD by a constant factor (496 + 32 +
    1 possible combinations), but stays linear in terms of time complexity while Cartesian
    join is a quadratic operation - *O(n²).*
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这也帮助我们相应地扩展我们最初的RDD。这肯定是一个昂贵的操作，因为它通过一个常数因子增加了我们的RDD的大小（496 + 32 + 1种可能的组合），但在时间复杂度方面保持线性，而笛卡尔积连接是一个二次操作
    - *O(n²).*
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We find that article A is a duplicate of article B, which is a duplicate of
    article C. This is a simple graph problem that can easily be solved through *GraphX*
    using a connected components algorithm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现文章A是文章B的副本，文章B是文章C的副本。这是一个简单的图问题，可以通过使用连接组件算法轻松解决*GraphX*。
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Out of the 15,000 articles used for that test, we extracted around 3,000 different
    stories. We report an example in *Figure 7*, which includes two near-duplicate
    articles we were able to detect, both of them being highly similar but not truly
    identical.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在用于该测试的15,000篇文章中，我们提取了大约3,000个不同的故事。我们在*图7*中报告了一个例子，其中包括我们能够检测到的两篇近似重复的文章，它们都非常相似，但并非完全相同。
- en: '![Detecting near duplicates in GDELT](img/B05261_10_07_2.jpg)![Detecting near
    duplicates in GDELT](img/B05261_10_07_1-1.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![在GDELT中检测近似重复](img/B05261_10_07_2.jpg)![在GDELT中检测近似重复](img/B05261_10_07_1-1.jpg)'
- en: 'Figure 7: Galaxy Note 7 fiasco from the GDELT database'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：GDELT数据库中的Galaxy Note 7惨败
- en: Indexing the GDELT database
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对GDELT数据库进行索引
- en: The next step is to start building our online API so that any user can detect
    near-duplicate events in real time just like Facebook does on a user's timeline.
    We use the *Play Framework* here, but we will keep the description short as this
    has already been covered in [Chapter 8](ch08.xhtml "Chapter 8. Building a Recommendation
    System"), *Building a Recommendation System*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是开始构建我们的在线API，以便任何用户都可以像Facebook在用户时间线上那样实时检测近似重复的事件。我们在这里使用*Play Framework*，但我们会简要描述，因为这已经在[第8章](ch08.xhtml
    "第8章。构建推荐系统")*构建推荐系统*中涵盖过。
- en: Persisting our RDDs
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 持久化我们的RDD
- en: Firstly, we need to extract data out of our RDD and persist it somewhere that
    is reliable, scalable, and highly efficient for search by key. As the main purpose
    of that database is to retrieve an article given a specific key (key being the
    Simhash), **Cassandra** (maven dependency as follows) sounds like a good fit for
    that job.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从我们的RDD中提取数据并将其持久化到可靠、可扩展且高效的位置以供按键搜索。由于该数据库的主要目的是在给定特定键（键为Simhash）的情况下检索文章，**Cassandra**（如下所示的maven依赖）似乎是这项工作的不错选择。
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Our data model is fairly simple and consists of one simple table:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据模型相当简单，由一个简单的表组成：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The easiest way to store our RDD into Cassandra is to wrap our result in a
    case class object that matches our earlier table definition and calls the `saveToCassandra`
    function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的RDD存储到Cassandra的最简单方法是将我们的结果包装在一个与我们之前表定义匹配的案例类对象中，并调用`saveToCassandra`函数：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Building a REST API
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建REST API
- en: 'The next step is to work on the API itself. We create a new maven module (packaged
    as `play2`) and import the following dependencies:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是着手处理API本身。我们创建一个新的maven模块（打包为`play2`）并导入以下依赖项：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We first create a new **data access layer**, that, given an input Simhash,
    builds the list of all the possible 1-bit and 2-bit masks discussed earlier and
    pulls all the matching records from Cassandra:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个新的**数据访问层**，它可以根据输入的Simhash构建我们之前讨论过的所有可能的1位和2位掩码的列表，并从Cassandra中提取所有匹配的记录：
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In our **controller**, given an input URL, we extract the HTML content, tokenize
    the text, build a Simhash value, and call our service layer to finally return
    our matching records in a JSON format.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的**控制器**中，给定一个输入URL，我们提取HTML内容，对文本进行标记化，构建Simhash值，并调用我们的服务层，最终以JSON格式返回我们的匹配记录。
- en: '[PRE21]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following `play2` route will redirect any GET request to the `detect` method
    we saw earlier:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`play2`路由将重定向任何GET请求到我们之前看到的`detect`方法：
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, our API can be started and exposed to the end users as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的API可以如下启动并向最终用户公开：
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Congratulations! You have now built an online API that can be used to detect
    near-duplicates such as the ones around Galaxy Note 7 fiasco; but how accurate
    our API is compared to the one from Facebook? This surely is accurate enough to
    start de-noising GDELT data by grouping highly similar events into stories.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您现在已经构建了一个在线API，可以用于检测近似重复，比如Galaxy Note 7惨败周围的事件；但我们的API与Facebook的API相比有多准确？这肯定足够准确，可以开始通过将高度相似的事件分组成故事来去噪GDELT数据。
- en: Area of improvement
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 改进领域
- en: Although we are already satisfied with the overall quality of the results returned
    by our API, here we discuss a major improvement in the context of news articles.
    In fact, articles are not only made of different bag of words, but follow a clear
    structure where the order truly matters. In fact, the title is always a punch
    line, and the main content is well covered within the first few lines only. The
    rest of the article does matter, but may not be as important as the introduction.
    Given that assumption, we can slightly modify our Simhash algorithm to take the
    order into account by attributing a different weight to each word.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经对API返回的结果总体质量感到满意，但在这里我们讨论了新闻文章的一个重大改进。事实上，文章不仅由不同的词袋组成，而且遵循一个清晰的结构，其中顺序确实很重要。事实上，标题总是一个噱头，主要内容仅在前几行内完全涵盖。文章的其余部分也很重要，但可能不像介绍那样重要。鉴于这一假设，我们可以稍微修改我们的Simhash算法，通过为每个单词分配不同的权重来考虑顺序。
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Instead of adding 1 or -1 any time the same bit value is set or not, we add
    the corresponding weight of that word according to its position in the article.
    Similar articles will have to share same words, same word frequency, but also
    a similar structure. In other words, we are much less permissive in any difference
    occurring in the first few lines of text than we are at the really bottom line
    of each article.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与其在设置相同的位值时每次添加1或-1，不如根据单词在文章中的位置添加相应的权重。相似的文章将共享相同的单词、相同的词频，但也具有相似的结构。换句话说，在文本的前几行发生的任何差异，我们要比在每篇文章的最后一行发生的差异更不容忍。
- en: Building stories
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建故事
- en: '*Simhash* should be used to detect near-duplicate articles only. Extending
    our search to a 3-bit or 4-bit difference becomes terribly inefficient (3-bit
    difference requires 5,488 distinct queries to Cassandra while 41,448 queries will
    be needed to detect up to 4-bit differences) and seems to bring much more noise
    than related articles. Should the user want to build larger stories, a typical
    clustering technique must be applied then.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*Simhash*应该只用于检测近似重复的文章。将我们的搜索扩展到3位或4位的差异将变得非常低效（3位差异需要5,488个不同的查询到Cassandra，而需要41,448个查询来检测高达4位的差异），并且似乎会带来比相关文章更多的噪音。如果用户想要构建更大的故事，那么必须应用典型的聚类技术。'
- en: Building term frequency vectors
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建词频向量
- en: We will start grouping events into stories using a KMeans algorithm, taking
    the articles' word frequencies as input vectors. TF-IDF is simple, efficient,
    and a proven technique to build vectors out of text content. The basic idea is
    to compute a word frequency that we normalize using the inverse document frequency
    across the dataset, hence decreasing the weight on common words (such as stop
    words) while increasing the weight of words specific to the definition of a document.
    Its implementation is part of the basics of MapReduce processing, the *Wordcount*
    algorithm. We first compute our RDD of term frequency for each word in each document.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始使用KMeans算法将事件分组成故事，以文章的词频作为输入向量。TF-IDF简单、高效，是一种构建文本内容向量的成熟技术。基本思想是计算一个词频，然后使用数据集中的逆文档频率进行归一化，从而减少常见词（如停用词）的权重，同时增加特定于文档定义的词的权重。它的实现是MapReduce处理的基础之一，*Wordcount*算法。我们首先计算每个文档中每个单词的词频的RDD。
- en: '[PRE25]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The IDF is the logarithmic value of the total number of documents divided by
    the number of documents containing the letter *w*:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: IDF是文档总数除以包含字母*w*的文档数的对数值：
- en: '![Building term frequency vectors](img/image_10_008.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![构建词频向量](img/image_10_008.jpg)'
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As our output vectors are made of words, we need to assign a sequence ID to
    each word in our corpus. We may have two solutions here. Either we build our dictionary
    and assign an ID for each word, or group different words in same buckets using
    a hash function. The former is ideal but results in vectors about a million features
    long (as many features as we do have unique words) while the latter is much smaller
    (as many features as the user specifies) but may lead to undesired effects due
    to hash collisions (the least features the more collisions).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的输出向量由单词组成，我们需要为语料库中的每个单词分配一个序列ID。我们可能有两种解决方案。要么我们建立字典并为每个单词分配一个ID，要么使用哈希函数将不同的单词分组到相同的桶中。前者是理想的，但会导致向量长度约为一百万个特征（与我们拥有的唯一单词数量一样多的特征），而后者要小得多（与用户指定的特征数量一样多），但可能会由于哈希碰撞而导致不良影响（特征越少，碰撞越多）。
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Although we describe the TF-IDF technique in detail, this hashing TF can be
    done within only a couple of lines thanks to the MLlib utilities, which we'll
    see next. We built our RDD of 256 large vectors that can (technically) be fed
    in a KMeans clustering, but, due to the hashing properties we just explained earlier,
    we would be subject to dramatic hash collisions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们详细描述了TF-IDF技术，但这种散列TF只需要几行代码就可以完成，这要归功于MLlib实用程序，接下来我们将看到。我们构建了一个包含256个大向量的RDD，（从技术上讲）可以用于KMeans聚类，但由于我们刚刚解释的哈希属性，我们将受到严重的哈希碰撞的影响。
- en: '[PRE28]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The curse of dimensionality, the data science plague
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度诅咒，数据科学的灾难
- en: Increasing our feature size from 256 to say 2^(20) will strongly limit the number
    of collisions, but will come at a price, our data points are now embedded on a
    highly dimensional space.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的特征大小从256增加到2^(20)将大大限制碰撞的数量，但代价是我们的数据点现在嵌入在一个高度维度的空间中。
- en: Here we describe a clever approach to overcome the *curse of dimensionality*
    ([http://www.stat.ucla.edu/~sabatti/statarray/textr/node5.html](http://www.stat.ucla.edu/~sabatti/statarray/textr/node5.html))
    without having to deep dive into fuzzy mathematical theories around matrix calculation
    (such as singular value decomposition) and without the need of compute-intensive
    operation. This approach is called *Random Indexing* and is similar to the *Simhash*
    concept described earlier.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们描述了一种聪明的方法来克服维度诅咒（[http://www.stat.ucla.edu/~sabatti/statarray/textr/node5.html](http://www.stat.ucla.edu/~sabatti/statarray/textr/node5.html)），而不必深入研究围绕矩阵计算的模糊数学理论（如奇异值分解），也不需要进行计算密集型的操作。这种方法被称为*随机索引*，类似于之前描述的*Simhash*概念。
- en: Note
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: More information about Random Indexing can be found at [http://eprints.sics.se/221/1/RI_intro.pdf](http://eprints.sics.se/221/1/RI_intro.pdf).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 有关随机索引的更多信息可以在[http://eprints.sics.se/221/1/RI_intro.pdf](http://eprints.sics.se/221/1/RI_intro.pdf)找到。
- en: 'The idea is to generate a sparse, randomly generated and unique representation
    of each distinct feature (a word here), composed of +1s, -1s, and mainly 0s. Then,
    each time we come across a word in a context (a document), we add this word''s
    signature to a context vector. A document vector is then the sum of each of its
    words'' vectors as per the following *Figure 8* (or the sum of each of its TF-IDF
    vectors, in our case):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是生成每个不同特征（这里是一个单词）的稀疏、随机生成和唯一表示，由+1、-1和主要是0组成。然后，每当我们在一个上下文（一个文档）中遇到一个单词时，我们将这个单词的签名添加到上下文向量中。然后，文档向量是其每个单词向量的总和，如下的*图8*（或我们的情况下每个TF-IDF向量的总和）所示：
- en: '![The curse of dimensionality, the data science plague](img/B05261_10_09.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![维度诅咒，数据科学的灾难](img/B05261_10_09.jpg)'
- en: 'Figure 8: Building a Random Indexing vector'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：构建随机索引向量
- en: We invite our purist math geek readers to dive into the *Johnson-Lindenstrauss* ([http://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/lectures/jl.pdf](http://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/lectures/jl.pdf))
    lemma that basically states that *"if we project points in a vector space into
    a randomly selected subspace of sufficiently high dimensionality, the distances
    between the points are approximately preserved"*. Although the *Random Indexing*
    technique itself can be implemented (with its fair amount of effort), the *Johnson-Lindenstrauss*
    lemma is quite useful but by far more difficult to grasp. Luckily, an implementation
    is part of the excellent spark-package *generalized-kmeans-clustering* ([https://github.com/derrickburns/generalized-kmeans-clustering](https://github.com/derrickburns/generalized-kmeans-clustering))
    from *Derrick Burns*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们邀请我们纯粹的数学极客读者深入研究*Johnson-Lindenstrauss*引理（[http://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/lectures/jl.pdf](http://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/lectures/jl.pdf)），该引理基本上陈述了*"如果我们将向量空间中的点投影到足够高维度的随机选择的子空间中，点之间的距离将被近似保留"*。尽管*Random
    Indexing*技术本身可以实现（需要相当大的努力），*Johnson-Lindenstrauss*引理非常有用，但要理解起来要困难得多。幸运的是，*Derrick
    Burns*的优秀spark-package *generalized-kmeans-clustering*（[https://github.com/derrickburns/generalized-kmeans-clustering](https://github.com/derrickburns/generalized-kmeans-clustering)）中包含了该实现。
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We were finally able to project our 2^(20) large vectors in *only* 256 dimensions.
    This technique offers huge benefits to say the least.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终能够将我们的2^(20)大向量投影到*仅*256维。这项技术至少提供了巨大的好处。
- en: We have a fixed number of features. Our vectors will never grow in size should
    we encounter a new word in the future that was not part of our initial dictionary.
    This will be particularly useful in a streaming context.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有固定数量的特征。如果将来遇到不在我们初始字典中的新单词，我们的向量大小将永远不会增长。这在流式上下文中将特别有用。
- en: Our input feature set is extremely large (2^(20)). Although the collisions will
    still occur, the risk is mitigated.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的输入特征集非常大（2^(20)）。尽管仍会发生碰撞，但风险已经减轻。
- en: The distances are preserved thanks to the *Johnson-Lindenstrauss* lemma.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于*Johnson-Lindenstrauss*引理，距离得以保留。
- en: Our output vectors are relatively small (256). We overcame the curse of dimensionality.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的输出向量相对较小（256）。我们克服了维度诅咒。
- en: As we cached our vector RDD into memory, we can now look at the KMeans clustering
    itself.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将向量RDD缓存在内存中，现在我们可以看看KMeans聚类本身。
- en: Optimizing KMeans
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KMeans的优化
- en: We assume our readers are familiar with KMeans clustering already as this algorithm
    is probably the most famous and widely used algorithm for unsupervised clustering.
    Any attempt at doing yet another explanation here will not be as good as the many
    resources you will be able to find out there after more than half a century of
    active research.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设我们的读者已经熟悉了KMeans聚类，因为这个算法可能是最著名和被广泛使用的无监督聚类算法。在这里再尝试解释将不如你能在超过半个世纪的积极研究后找到的许多资源那么好。
- en: 'We previously created our vectors based on the articles'' contents (TF-IDF).
    The next step is to start grouping articles into stories based on their similarity.
    In Spark implementation of KMeans, only the *Euclidean distance* measure is supported.
    One would argue the *Cosine distance* would be more suitable for text analysis,
    but we assume that the former is accurate enough as we do not want to repackage
    the MLlib distribution for that exercise.For more explanation regarding the use
    of cosine distance for text analysis, please refer to [http://www.cse.msu.edu/~pramanik/research/papers/2003Papers/sac04.pdf](http://www.cse.msu.edu/~pramanik/research/papers/2003Papers/sac04.pdf).
    We report in the following code both the Euclidean and Cosine functions that can
    be applied on any array of double (the logical data structure behind dense vectors):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先前根据文章内容（TF-IDF）创建了我们的向量。下一步是根据它们的相似性将文章分组成故事。在Spark实现的KMeans中，只支持*欧氏距离*度量。有人会认为*余弦距离*更适合文本分析，但我们假设前者足够准确，因为我们不想重新打包MLlib分发以进行该练习。有关在文本分析中使用余弦距离的更多解释，请参阅[http://www.cse.msu.edu/~pramanik/research/papers/2003Papers/sac04.pdf](http://www.cse.msu.edu/~pramanik/research/papers/2003Papers/sac04.pdf)。我们在以下代码中报告了可以应用于任何双精度数组（密集向量背后的逻辑数据结构）的欧氏和余弦函数：
- en: '[PRE30]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Training a new KMeans clustering is fairly simple using the MLlib package. We
    specify a threshold of 0.01 after which we consider our cluster centers to converge
    and set the maximum iterations to 1,000.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MLlib包训练新的KMeans聚类非常简单。我们指定一个阈值为0.01，之后我们认为我们的聚类中心已经收敛，并将最大迭代次数设置为1,000。
- en: '[PRE31]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: But what is the right number of clusters in our particular use case? With between
    500 to 1,000 different articles per 15mn batch, how many stories can we build?
    The right question is, *How many true events do we think happened over a 15mn
    batch window?* In fact, optimizing KMeans for news articles is not different from
    any other use case; this is done by optimizing its associated cost, cost being
    the **sum of the squared distances** (**SSE**) from the points to their respective
    centroids.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们特定的用例中，正确的聚类数是多少？在每15分钟批处理中有500到1,000篇不同的文章，我们可以构建多少个故事？正确的问题是，*我们认为在15分钟批处理窗口内发生了多少个真实事件？*实际上，为新闻文章优化KMeans与任何其他用例并无不同；这是通过优化其相关成本来实现的，成本是**点到它们各自质心的平方距离的总和**（**SSE**）。
- en: '[PRE32]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: With *k* equal to the number of articles, the associated cost will be 0 (each
    article is the center of its own cluster). Similarly, with *k* equal to 1, the
    cost will be maximum. The best value of *k* is therefore the minimum possible
    value after which adding a new cluster would not bring any gain in the associated
    cost, usually represented as an elbow in the SSE curve shown in the next figure.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当*k*等于文章的数量时，相关成本为0（每篇文章都是其自己聚类的中心）。同样，当*k*等于1时，成本将达到最大值。因此，*k*的最佳值是在添加新的聚类不会带来任何成本增益之后的最小可能值，通常在下图中显示的SSE曲线的拐点处表示。
- en: Using all the 15,000 articles we collected so far, the optimal number of clusters
    is not obvious here but would probably be around 300.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用迄今为止收集的所有1.5万篇文章，这里最佳的聚类数量并不明显，但可能大约在300左右。
- en: '![Optimizing KMeans](img/image_10_010.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![优化KMeans](img/image_10_010.jpg)'
- en: 'Figure 9: Elbow method using the cost function'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：使用成本函数的拐点法
- en: A rule of thumb is to use *k* as a function of *n* (number of articles). With
    over 15,000 articles, following this rule would return *k* ![Optimizing KMeans](img/image_10_011.jpg)
    100.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经验法则是将*k*作为*n*（文章数量）的函数。有超过1.5万篇文章，遵循这个规则将返回*k* ![优化KMeans](img/image_10_011.jpg)
    100。
- en: '![Optimizing KMeans](img/image_10_012.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![优化KMeans](img/image_10_012.jpg)'
- en: We use a value of 100 and start predicting our clusters for each of our data
    points.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用值为100，并开始预测每个数据点的聚类。
- en: '[PRE33]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Although this could be greatly improved, we confirm many articles that look
    similar are grouped within same stories. We report some Samsung-related articles
    belonging to the same cluster here:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这可能得到很大的改进，我们确认许多相似的文章被分在了同一个故事中。我们报告了一些属于同一聚类的与三星相关的文章：
- en: '*What Samsung can learn from Tylenol, Mattel, and JetBlue...*'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*三星可以从泰诺、玩具和捷蓝学到什么...*'
- en: '*Huawei Mate 9 Appears To Be A Samsung Galaxy Note 7 Clone...*'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*华为Mate 9似乎是三星Galaxy Note 7的复制品...*'
- en: '*In light of the Note 7 debacle, Samsung may be poised to...*'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴于Note 7的惨败，三星可能会...*
- en: '*Samsung''s spiralling stock draws investors betting...*'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*三星股价的螺旋式下跌吸引了投资者的赌注...*'
- en: '*Note 7 fiasco leaves Samsung''s smartphone brand...*'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Note 7惨败让三星智能手机品牌...*'
- en: '*Samsung''s smartphone brand takes beating from Note 7 fiasco...*'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Note 7惨败让三星智能手机品牌受到打击...*'
- en: '*Note 7 fiasco leaves Samsung''s smartphone brand in question...*'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Note 7惨败让三星智能手机品牌蒙上疑问的阴影...*'
- en: '*Note 7 fiasco leaves Samsung''s smartphone brand in question...*'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Note 7惨败让三星智能手机品牌蒙上疑问的阴影...*'
- en: '*Samsung''s smartphone brand takes beating from Note 7 fiasco...*'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Note 7惨败让三星智能手机品牌受到打击...*'
- en: '*Fiasco leaves Samsung''s smartphone brand in question...*'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*惨败让三星智能手机品牌蒙上疑问的阴影...*'
- en: Surely these similar articles were not eligible for a Simhash lookup as they
    differ by more than 1-bits or 2-bits. A clustering technique can be used to group
    similar (but not duplicate) articles into broader stories. It is worth mentioning
    that optimizing KMeans is a tedious task that requires many iterations and thorough
    analysis. This, however, is not part of the scope here as we will be focusing
    on much larger clusters and much smaller datasets in real time.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 可以肯定的是，这些相似的文章不符合Simhash查找的条件，因为它们的差异超过了1位或2位。聚类技术可以用来将相似（但不重复）的文章分成更广泛的故事。值得一提的是，优化KMeans是一项繁琐的任务，需要多次迭代和彻底分析。然而，在这里，这并不是范围的一部分，因为我们将专注于实时的更大的聚类和更小的数据集。
- en: Story mutation
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故事变异
- en: We now have enough material to enter the heart of the subject. We were able
    to detect near-duplicate events and group similar articles within a story. In
    this section, we will be working in real time (on a Spark Streaming context),
    listening for news articles, grouping them into stories, but also looking at how
    these stories may change over time. We appreciate that the number of stories is
    undefined as we do not know in advance what events may arise in the coming days.
    As optimizing KMeans for each batch interval (15 mn in GDELT) would not be ideal,
    neither would it be efficient, we decided to take this constraint not as a limiting
    factor but really as an advantage in the detection of breaking news articles.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有足够的材料来进入主题的核心。我们能够检测到近似重复的事件，并将相似的文章分组到一个故事中。在本节中，我们将实时工作（在Spark Streaming环境中），监听新闻文章，将它们分组成故事，同时也关注这些故事如何随时间变化。我们意识到故事的数量是不确定的，因为我们事先不知道未来几天可能出现什么事件。对于每个批次间隔（GDELT中的15分钟），优化KMeans并不理想，也不高效，因此我们决定将这一约束条件不是作为限制因素，而是作为在检测突发新闻文章方面的优势。
- en: The Equilibrium state
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平衡状态
- en: 'If we were to divide the world''s news articles into say 10 or 15 clusters,
    and fix that number to never change over time, then training a KMeans clustering
    should probably group similar (but not necessarily duplicate) articles into generic
    stories. For convenience, we give the following definitions:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将世界新闻文章分成10到15个类别，并固定该数量不会随时间改变，那么训练KMeans聚类应该能够将相似（但不一定是重复的）文章分成通用的故事。为方便起见，我们给出以下定义：
- en: An **article** is the news article covering a particular event at a time T
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文章是在时间T涵盖特定事件的新闻文章。
- en: A **story** is a group of similar articles covering an event at a time T
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故事是一组相似的文章，涵盖了一段时间T内的事件
- en: A **topic** is a group of similar stories covering different events over a period
    P
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题是一组相似的故事，涵盖了一段时间内的不同事件P
- en: An **epic** is a group of similar stories covering the same event over a period
    P
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 史诗是一组相似的故事，涵盖了一段时间内相同的事件P
- en: We assume that after some time without any major news events, any story will
    be grouped into distinct *topics* (each covering one or several themes). As an
    example, any article about politics - regardless the nature of the political event
    - may be grouped into the politics bucket. This is what we call the *Equilibrium
    state*, where the world is equally divided into 15 distinct and clear categories
    (war, politics, finance, technology, education, and so on).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设在一段时间内没有任何重大新闻事件之后，任何故事都将被分组到不同的*主题*中（每个主题涵盖一个或多个主题）。例如，任何关于政治的文章 - 无论政治事件的性质如何
    - 都可以被分组到政治桶中。这就是我们所说的*平衡状态*，在这种状态下，世界被平均分成了15个不同而清晰的类别（战争、政治、金融、技术、教育等）。
- en: But what happens if a major event just breaks through? An event may become so
    important that, over time, (and because of the fixed number of clusters) it could
    shadow the least important *topic* and become part of its own *topic*. Similar
    to the BBC broadcast that is limited to a 30 mn window, some minor events like
    the *Oyster festival in Whitstable* may be skipped in favor of a major international
    event (to the very dismay of oysters' fans). This topic is not generic anymore
    but is now associated to a particular event. We called this topic an *epic*. As
    an example, the generic *topic* [terrorism, war, and violence] became an epic
    [**Paris Attacks**] in November last year when a major terrorist attack broke
    through; what was deemed to be a broad discussion about violence and terrorism
    in general became a branch dedicated to the articles covering the events held
    in Paris.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果一个重大事件突然发生会发生什么呢？一个事件可能变得如此重要，以至于随着时间的推移（并且由于固定数量的集群），它可能会掩盖最不重要的*主题*并成为其自己的*主题*的一部分。类似于BBC广播限制在30分钟的时间窗口内，一些次要事件，比如*惠特斯特布尔的牡蛎节*，可能会被跳过，以支持一个重大的国际事件（令牡蛎的粉丝非常沮丧）。这个主题不再是通用的，而是现在与一个特定的事件相关联。我们称这个主题为一个*史诗*。例如，通用的*主题*[恐怖主义、战争和暴力]在去年11月成为了一个史诗[**巴黎袭击**]，当一个重大的恐怖袭击事件发生时，原本被认为是关于暴力和恐怖主义的广泛讨论变成了一个专门讨论巴黎事件的分支。
- en: 'Now imagine an *epic* keeps growing in size; while the first articles about
    **Paris Attacks** were covering facts, few hours later, the entire world was paying
    tribute and condemning terrorism. At the same time, investigations were led by
    both French and Belgium police to track and dismantle the terrorism network. Both
    of these stories were massively covered, hence became two different versions of
    the same *epic*. This concept of branching is reported in the following *Figure
    10*:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一个*史诗*不断增长；虽然关于**巴黎袭击**的第一篇文章是关于事实的，但几个小时后，整个世界都在向恐怖主义表示敬意和谴责。与此同时，法国和比利时警方进行了调查，追踪和解散恐怖主义网络。这两个故事都得到了大量报道，因此成为了同一个*史诗*的两个不同版本。这种分支的概念在下面的*图10*中有所体现：
- en: '![The Equilibrium state](img/image_10_013.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![平衡状态](img/image_10_013.jpg)'
- en: 'Figure 10: Concept of a story mutation branching'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：故事变异分支的概念
- en: Surely some epics will last longer than others, but when they vanish - if they
    do - their branches may be recycled to cover new breaking articles (remember the
    fixed number of clusters) or be re-used to group generic stories back to their
    generic topics. At some point in time, we eventually reach a new Equilibrium state
    where the world nicely fits again within 15 different topics. We assume, though,
    that a new Equilibrium may not be a perfect clone of the previous one as this
    disturbance may have carved and re-shaped the world somehow. As a concrete example,
    we still mention 9/11-related articles nowadays; world trade center attacks that
    happened in NYC in 2001 are still contributing to the definition of [violence,
    war, and terrorism] *topic*.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，有些史诗会比其他的持续时间更长，但当它们消失时 - 如果它们消失的话 - 它们的分支可能会被回收，以覆盖新的突发新闻（记住固定数量的集群），或者被重新用于将通用故事分组回到它们的通用主题。在某个时间点，我们最终达到了一个新的平衡状态，在这个状态下，世界再次完美地适应了15个不同的主题。我们假设，尽管如此，新的平衡状态可能不会是前一个的完美克隆，因为这种干扰可能已经在某种程度上雕刻和重新塑造了世界。作为一个具体的例子，我们现在仍然提到与9/11有关的文章；2001年发生在纽约市的世界贸易中心袭击仍然对[暴力、战争和恐怖主义]
    *主题*的定义产生影响。
- en: Tracking stories over time
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随着时间的推移跟踪故事
- en: Although the preceding description is more conceptual than anything, and would
    probably deserve a subject for a PhD in data science applied to geo-politics,
    we would like to dig that idea further and see how Streaming KMeans could be a
    fantastic tool for that use case.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的描述更多是概念性的，可能值得一篇关于应用于地缘政治的数据科学博士论文，但我们想进一步探讨这个想法，并看看流式KMeans如何成为这种用例的一个奇妙工具。
- en: Building a streaming application
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建流应用
- en: The first thing is to acquire our data in real time, hence modifying our existing
    NiFi flow to fork our downloaded archive to a Spark Streaming context. One could
    simply **netcat** the content of a file to an open socket, but we want this process
    to be resilient and fault tolerant. NiFi comes, by default, with the concept of
    output ports that provide a mechanism to transfer data to remote instances using
    *Site-To-Site*. In that case, the port works like a queue, and no data should
    be lost in transit, hopefully. We enable this functionality in the `nifi.properties`
    file by allocating a port number.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 第一件事是实时获取我们的数据，因此修改我们现有的NiFi流以将我们下载的存档分叉到一个Spark Streaming上下文。一个简单的方法是**netcat**将文件的内容发送到一个打开的套接字，但我们希望这个过程是有弹性和容错的。NiFi默认带有输出端口的概念，它提供了一个机制来使用*Site-To-Site*将数据传输到远程实例。在这种情况下，端口就像一个队列，希望在传输过程中不会丢失任何数据。我们通过在`nifi.properties`文件中分配一个端口号来启用这个功能。
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We create a port called [`Send_To_Spark`] on our canvas, and every record (hence
    the `SplitText` processor) will be sent to it just like we would be doing on a
    Kafka topic.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在画布上创建了一个名为[`Send_To_Spark`]的端口，每条记录（因此`SplitText`处理器）都将被发送到它，就像我们在Kafka主题上所做的那样。
- en: '![Building a streaming application](img/image_10_014.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![构建流应用](img/image_10_014.jpg)'
- en: 'Figure 11: Sending GKG records to Spark Streaming'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：将GKG记录发送到Spark Streaming
- en: Tip
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Although we are designing a streaming application, it is recommended to always
    keep an immutable copy of your data in a resilient data store (HDFS here). In
    our NiFi flow earlier, we did not modify our existing process, but forked it to
    also send records to our Spark Streaming. This will be particularly useful when/if
    we need to replay part of our dataset.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们正在设计一个流应用程序，但建议始终在弹性数据存储（这里是HDFS）中保留数据的不可变副本。在我们之前的NiFi流中，我们没有修改现有的流程，而是将其分叉，以便将记录发送到我们的Spark
    Streaming。当/如果我们需要重放数据集的一部分时，这将特别有用。
- en: 'On the Spark side, we need to build a Nifi receiver. This can be achieved using
    the following maven dependency:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark端，我们需要构建一个Nifi接收器。这可以通过以下maven依赖项实现：
- en: '[PRE35]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We define the NiFi endpoint together with the port name [`Send_To_Spark`] we
    assigned earlier. Our stream of data will be received as packet stream that can
    easily be converted into a String using the `getContent` method.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义NiFi端点以及我们之前分配的端口名称[`Send_To_Spark`]。我们的数据流将被接收为数据包流，可以使用`getContent`方法轻松转换为字符串。
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We start our streaming context and listen to new GDELT data coming every 15
    mn.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们启动我们的流上下文，并监听每15分钟到来的新GDELT数据。
- en: '[PRE37]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The next step is to download the HTML content for each article. The tricky
    part here is to download articles for distinct URLs only. As there is no built-in
    `distinct` operation on `DStream`, we need to access the underlying RDDs using
    a `transform` operation on top of which we pass an `extractUrlsFromRDD` function:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是为每篇文章下载HTML内容。这里的棘手部分是仅为不同的URL下载文章。由于`DStream`上没有内置的`distinct`操作，我们需要通过在其上使用`transform`操作并传递一个`extractUrlsFromRDD`函数来访问底层RDD：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Similarly, building vectors requires access to the underlying RDDs as we need
    to count the document frequency (used for TF-IDF) across the entire batch. This
    is also done within the `transform` function.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，构建向量需要访问底层RDD，因为我们需要计算整个批次的文档频率（用于TF-IDF）。这也是在`transform`函数中完成的。
- en: '[PRE39]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Streaming KMeans
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流式K均值
- en: Our use case perfectly fits in a **Streaming KMeans** algorithm. The concept
    of Streaming KMeans does not differ from the classic KMeans except that it applies
    on dynamic data and therefore needs to be constantly re-trained and updated.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的用例完全适用于**流式K均值**算法。流式K均值的概念与经典的K均值没有区别，只是应用于动态数据，因此需要不断重新训练和更新。
- en: At each batch, we find the closest center for each new data point, average the
    new cluster centers and update our model. As we track the true clusters and adapt
    to the changes in pseudo real-time, it will be particularly easy to track the
    same topics across different batches.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个批处理中，我们找到每个新数据点的最近中心，对新的聚类中心进行平均，并更新我们的模型。随着我们跟踪真实的聚类并适应伪实时的变化，跟踪不同批次中相同的主题将特别容易。
- en: The second important feature of a Streaming KMeans is the forgetfulness. This
    ensures new data points received at time t will be contributing more to the definition
    of our clusters than any other point in the past history, hence allowing our cluster
    centers to smoothly drift over time (stories will mutate). This is controlled
    by the decay factor and its half-life parameter (expressed in the number of batches
    or number of points) that specifies the time after which a given point will only
    contribute half of its original weight.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 流式K均值的第二个重要特征是遗忘性。这确保了在时间t接收到的新数据点将对我们的聚类定义产生更大的贡献，而不是过去历史中的任何其他点，因此允许我们的聚类中心随着时间平稳漂移（故事将变异）。这由衰减因子及其半衰期参数（以批次数或点数表示）控制，指定了给定点仅贡献其原始权重一半之后的时间。
- en: With an infinite decay factor, all the history will be taken into account, our
    cluster centers will be drifting slowly and will not be reactive if a major news
    event just breaks through
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用无限衰减因子，所有历史记录都将被考虑在内，我们的聚类中心将缓慢漂移，并且如果有重大新闻事件突然发生，将不会做出反应
- en: With a small decay factor, our clusters will be too reactive towards any point
    and may drastically change any time a new event is observed
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用较小的衰减因子，我们的聚类将对任何点过于敏感，并且可能在观察到新事件时发生 drastical 变化
- en: The third and most important feature of a Streaming KMeans is the ability to
    detect and recycle dying clusters. When we observe a drastic change in our input
    data, one cluster may become far from any known data point. Streaming KMeans will
    eliminate this dying cluster and split the largest one in two. This is totally
    in-line with our concept of story branching, where multiple stories may share
    a common ancestor.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 流式K均值的第三个最重要的特征是能够检测和回收垂死的聚类。当我们观察到输入数据发生 drastical 变化时，一个聚类可能会远离任何已知数据点。流式K均值将消除这个垂死的聚类，并将最大的聚类分成两个。这与我们的故事分支概念完全一致，其中多个故事可能共享一个共同的祖先。
- en: 'We use a half-life parameter of two batches here. As we get new data every
    15 mn, any new data point will stay *active* for 1 hour only. The process for
    training Streaming KMeans is reported in *Figure 12*:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用两个批次的半衰期参数。由于我们每15分钟获取新数据，任何新数据点只会保持*活跃*1小时。训练流式K均值的过程如*图12*所示：
- en: '![Streaming KMeans](img/image_10_015.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![流式K均值](img/image_10_015.jpg)'
- en: 'Figure 12: Training a Streaming KMeans'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：训练流式K均值
- en: 'We create a new Streaming KMeans as follows. Because we did not observe any
    data point yet, we initialize it with 15 random centers of 256 large vectors (size
    of our TF-IDF vectors) and train it in real time using the `trainOn` method:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个新的流式K均值如下。因为我们还没有观察到任何数据点，所以我们用256个大向量（我们的TF-IDF向量的大小）的15个随机中心进行初始化，并使用`trainOn`方法实时训练它：
- en: '[PRE40]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Finally, we predict our clusters for any new data point:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对任何新数据点进行聚类预测：
- en: '[PRE41]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We then save our results to our Elasticsearch cluster using the following attributes
    (accessed through a series of join operations). We do not report here how to persist
    RDD to Elasticsearch as we believe this has been covered in depth in the previous
    chapters already. Note that we also save the vector itself as we may re-use it
    later in the process.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用以下属性将我们的结果保存到我们的Elasticsearch集群中（通过一系列连接操作访问）。我们不在这里报告如何将RDD持久化到Elasticsearch，因为我们认为这在之前的章节中已经深入讨论过了。请注意，我们还保存向量本身，因为我们可能以后会重新使用它。
- en: '[PRE42]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Visualization
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化
- en: 'As we stored our articles with their respective stories and *topics* on Elasticsearch,
    we can browse any events using a keyword search (as the articles are fully analyzed
    and indexed) or for a particular person, theme, organization, and so on. We build
    visualizations on top of our stories and try to detect their potential drifts
    on a Kibana dashboard. The different cluster IDs (our different *topics*) over
    time are reported in the following *Figure 13* for the 13^(th) of November (35,000
    articles indexed):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将文章与它们各自的故事和*主题*存储在Elasticsearch中，我们可以使用关键词搜索（因为文章已经完全分析和索引）或特定的人物、主题、组织等来浏览任何事件。我们在我们的故事之上构建可视化，并尝试在Kibana仪表板上检测它们的潜在漂移。不同的集群ID（我们的不同*主题*）随时间的变化在11月13日（索引了35,000篇文章）的*图13*中报告：
- en: '![Visualization](img/image_10_016.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![可视化](img/image_10_016.jpg)'
- en: 'Figure 13: Kibana visualization of the Paris attacks'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：Kibana巴黎袭击的可视化
- en: The results are quite promising. We were able to detect the **Paris Attacks**
    at around 9:30 p.m. on November 13th, only a few minutes after the first attacks
    started. We also confirm a relative good consistency of our clustering algorithm
    as a particular cluster was made of events related to the **Paris Attacks** only
    (5,000 articles) from 9:30 p.m. to 3:00 a.m.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 结果相当令人鼓舞。我们能够在11月13日晚上9:30左右检测到**巴黎袭击**，距离第一次袭击开始只有几分钟。我们还确认了我们的聚类算法相对良好的一致性，因为一个特定的集群仅由与**巴黎袭击**相关的事件组成（5,000篇文章），从晚上9:30到凌晨3:00。
- en: But we may wonder what this particular cluster was about before the first attack
    took place. Since we indexed all the articles together with their cluster ID and
    their GKG attributes, we can easily track a story backwards in time and detect
    its mutation. It turns out this particular *topic* was mainly covering events
    related to [MAN_MADE_DISASTER] theme (among others) until 9 p.m. to 10 p.m. when
    it turned into the **Paris Attacks** *epic* with themes around [TERROR], [STATE_OF_EMERGENCY],
    [TAX_ETHNICITY_FRENCH], [KILL], and [EVACUATION].
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可能会想知道在第一次袭击发生之前，这个特定的集群是关于什么的。由于我们将所有文章与它们的集群ID和它们的GKG属性一起索引，我们可以很容易地追踪一个故事在时间上的倒退，并检测它的变异。事实证明，这个特定的*主题*主要涵盖了与[MAN_MADE_DISASTER]主题相关的事件（等等），直到晚上9点到10点，当它转变为**巴黎袭击**的*史诗*，主题围绕着[TERROR]、[STATE_OF_EMERGENCY]、[TAX_ETHNICITY_FRENCH]、[KILL]和[EVACUATION]。
- en: '![Visualization](img/image_10_017.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![可视化](img/image_10_017.jpg)'
- en: 'Figure 14: Kibana streamgraph of the Paris attacks cluster'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：Kibana巴黎袭击集群的流图
- en: 'Needless to say the 15 mn average tone we get from GDELT dropped drastically
    after 9 p.m. for that particular *topic*:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 不用说，我们从GDELT得到的15分钟平均语调在晚上9点后急剧下降，针对那个特定的*主题*：
- en: '![Visualization](img/image_10_018.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![可视化](img/image_10_018.jpg)'
- en: 'Figure 15: Kibana average tone-Paris attacks cluster'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：Kibana平均语调-巴黎袭击集群
- en: Using these three simple visualizations, we prove that we can track a story
    over time and study its potential mutation in genre, keywords, persons, or organizations
    (basically any entity we can extract from GDELT). But we could also look at the
    geolocation from the GKG records; with enough articles, we could possibly track
    the terrorist hunt held between Paris and Brussels on a map, in pseudo real time!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这三个简单的可视化，我们证明了我们可以随着时间追踪一个故事，并研究它在类型、关键词、人物或组织（基本上我们可以从GDELT中提取的任何实体）方面的潜在变异。但我们也可以查看GKG记录中的地理位置；有了足够的文章，我们可能可以在伪实时中追踪巴黎和布鲁塞尔之间的恐怖分子追捕活动！
- en: 'Although we found one main cluster that was specific to the Paris Attacks,
    and that this particular cluster was the first one to cover this series of events,
    this may not be the only one. According to the Streaming KMeans definition earlier,
    this *topic* became so big that it surely had triggered one or several subsequent
    *epics*. We report in the following *Figure 16* the same results as per *Figure
    13*, but this time filtered out for any article matching the keyword *Paris*:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们发现了一个特定于巴黎袭击的主要集群，并且这个特定的集群是第一个涵盖这一系列事件的集群，但这可能不是唯一的。根据之前的Streaming KMeans定义，这个*主题*变得如此庞大，以至于肯定触发了一个或多个随后的*史诗*。我们在下面的*图16*中报告了与*图13*相同的结果，但这次是过滤出与关键词*巴黎*匹配的任何文章：
- en: '![Visualization](img/image_10_019.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![可视化](img/image_10_019.jpg)'
- en: 'Figure 16: Kibana Paris Attacks multiple epics'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：Kibana巴黎袭击的多个史诗
- en: It seems that around midnight, this *epic* gave rise to multiple versions of
    the same event (at least three major ones). After an hour following the attacks
    (1 hour is our decay factor), Streaming KMeans started to recycle dying clusters,
    hence creating new branches out of the most important event (our *Paris attack* cluster).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎在午夜左右，这个*史诗*产生了同一事件的多个版本（至少三个主要版本）。在袭击后一个小时（1小时是我们的衰减因子）后，Streaming KMeans开始回收垂死的集群，从而在最重要的事件（我们的*巴黎袭击*集群）中创建新的分支。
- en: While the main *epic* was still covering the event itself (the facts), the second
    most important one was more about social network related articles. A simple word
    frequency analysis tells us this *epic* was about the **#portesOuvertes** (open
    doors) and **#prayForParis** hashtags where Parisians responded to terror with
    solidarity. We also detected another cluster focusing more on all the politicians
    paying tributes to France and condemning terrorism. All these new stories share
    the *Paris attack* *epic* as a common ancestor, but cover a different flavor of
    it.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然主要的*史诗*仍然涵盖着事件本身（事实），但第二重要的是更多关于社交网络相关文章的。简单的词频分析告诉我们，这个*史诗*是关于**#portesOuvertes**（开放的大门）和**#prayForParis**标签，巴黎人以团结回应恐怖袭击。我们还发现另一个集群更关注所有向法国致敬并谴责恐怖主义的政治家。所有这些新故事都共享*巴黎袭击*
    *史诗*作为共同的祖先，但涵盖了不同的风味。
- en: Building story connections
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建故事连接
- en: How can we link these branches together? How can we track an *epic* over time
    and see when, if, how, or why it may split? Surely visualization helps, but we
    are looking at a graph problem to solve here.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将这些分支联系在一起？我们如何随着时间跟踪一个*史诗*，并查看它何时、是否、如何或为什么会分裂？当然，可视化有所帮助，但我们正在解决一个图问题。
- en: 'Because our KMeans model keeps getting updated at each batch, our approach
    is to retrieve the articles that we predicted using an outdated version of our
    model, pull them from Elasticsearch, and predict them against our updated KMeans.
    Our assumption is as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的KMeans模型在每个批次中都在不断更新，我们的方法是检索我们使用过时版本模型预测的文章，从Elasticsearch中提取它们，并根据我们更新的KMeans模型进行预测。我们的假设如下：
- en: '*If we observe many articles at a time* **t** *that belonged to a story* **s**,
    *and now belong to a story* **s''** *at a time* ![Building story connections](img/B05261_10_22.jpg),
    *then* **s** *most likely migrated to* **s''** *in* ![Building story connections](img/B05261_10_23.jpg)*time.*'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果我们观察到在时间*t*时属于故事*s*的许多文章，现在在时间*![构建故事连接](img/B05261_10_22.jpg)*属于故事*s''*，*那么*
    **s** *很可能在* ![构建故事连接](img/B05261_10_23.jpg) *时间内迁移到* **s''**。*'
- en: 'As a concrete example, the first **#prayForParis** article was surely belonging
    to the *Paris Attacks* *epic*. Few batches later, that same article belonged to
    the *Paris Attacks/Social Network* cluster. Therefore, the *Paris Attack* *epic*
    may have spawn the *Paris Attacks/Social Network* *epic*. This process is reported
    in the following *Figure 17*:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个具体的例子，第一个**#prayForParis**文章肯定属于*巴黎袭击* *史诗*。几个批次后，同一篇文章属于*巴黎袭击/社交网络*集群。因此，*巴黎袭击*
    *史诗*可能产生了*巴黎袭击/社交网络* *史诗*。这个过程在下面的*图17*中有所报道：
- en: '![Building story connections](img/image_10_020.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![构建故事连接](img/image_10_020.jpg)'
- en: 'Figure 17: Detecting story connections'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：检测故事连接
- en: 'We read a JSON RDD from Elasticsearch and applied a range query using the batch
    ID. In the following example, we want to access all the vectors built over the
    past hour (four last batches) together with their original cluster ID and re-predict
    them against our updated model (accessed through the `latestModel` function):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Elasticsearch中读取了一个JSON RDD，并使用批处理ID应用了范围查询。在下面的例子中，我们想要访问过去一小时内构建的所有向量（最后四个批次），以及它们的原始集群ID，并根据我们更新的模型重新预测它们（通过`latestModel`函数访问）：
- en: '[PRE43]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Finally, a simple `reduceByKey` function will count the number of different
    edges over the past hour. In most of the cases, articles in story *s* will stay
    in story *s*, but in case of the Paris attacks, we may observe some stories to
    drift over time towards different *epics*. Most importantly, the more connections
    two branches have in common, the more similar they are (as many of their articles
    are interconnected) and therefore the closest they seem to be in a force directed
    layout. Similarly, branches that are not sharing many connections will seem to
    be far from another in the same graph visualization. A force atlas representation
    of our story connections is done using Gephi software and reported in the following *Figure
    18*. Each node is a story at a batch *b*, and each edge is the number of connections
    we found between two stories. The 15 lines are our 15 *topics* that all share
    a common ancestor (the initial cluster spawn when first starting the streaming
    context).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个简单的`reduceByKey`函数将计算过去一小时内不同边的数量。在大多数情况下，故事*s*中的文章将保持在故事*s*中，但在巴黎袭击的情况下，我们可能会观察到一些故事随着时间的推移向不同的*史诗*漂移。最重要的是，两个分支之间共享的连接越多，它们就越相似（因为它们的文章相互连接），因此它们在力导向布局中看起来越接近。同样，不共享许多连接的分支在相同的图形可视化中看起来会相距甚远。我们使用Gephi软件对我们的故事连接进行了力导向图表示，并在下面的*图18*中报告。每个节点都是批次*b*上的一个故事，每条边都是我们在两个故事之间找到的连接数量。这15行是我们的15个*主题*，它们都共享一个共同的祖先（在首次启动流上下文时生成的初始集群）。
- en: '![Building story connections](img/B05261_10_20.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![构建故事连接](img/B05261_10_20.jpg)'
- en: 'Figure 18: Force-directed layout of story mutation'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：故事变异的力导向布局
- en: The first observation we can make is this line shape. This observation surprisingly
    confirms our theory of an Equilibrium state where the world nicely fitted in 15
    distinct *topics* until the Paris attacks happened. Before the event, most of
    the *topics* were isolated and intraconnected (hence this line shape). After the
    event, we see our main *Paris Attack* *epic* to be dense, interconnected, and
    drifting over time. It also seems to drag few branches down with it due to the
    growing number on interconnections. These two similar branches are the two other
    clusters mentioned earlier (social network and tributes). This *epic* is being
    more and more specific over time, it naturally becomes more different from others,
    hence pushing all these different stories upwards and resulting in this scatter
    shape.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做出的第一个观察是这条线形状。这一观察令人惊讶地证实了我们对平衡状态的理论，即在巴黎袭击发生之前，大部分*主题*都是孤立的并且内部连接的（因此呈现这种线形状）。事件发生之前，大部分*主题*都是孤立的并且内部连接的（因此呈现这种线形状）。事件发生后，我们看到我们的主要*巴黎袭击*
    *史诗*变得密集、相互连接，并随着时间的推移而漂移。由于相互连接的数量不断增加，它似乎还拖着一些分支下降。这两个相似的分支是前面提到的另外两个集群（社交网络和致敬）。随着时间的推移，这个*史诗*变得越来越具体，自然地与其他故事有所不同，因此将所有这些不同的故事推向上方，形成这种散点形状。
- en: We also want to know what these different branches were about, and whether or
    not we could explain why a story may have split into two. For that purpose, we
    find the main article of each story as being the closest point to its centroid.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还想知道这些不同分支是关于什么的，以及我们是否能解释为什么一个故事可能分裂成两个。为此，我们将每个故事的主要文章视为离其质心最近的点。
- en: '[PRE44]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In *Figure 19*, we report the same graph enriched with the story titles. Although
    it is difficult to find a clear pattern, we found an interesting case. A *topic*
    was covering (among others) an event related to *Prince Harry* joking around about
    his hair style, that slightly migrated to *Obama* offering a statement on the
    attack in Paris, and finally turned into the Paris attack and the tributes paid
    by politicians. This branch did not come out of nowhere but seemed to follow a
    logical flow:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图19*中，我们报告了相同的图表，并附上了故事标题。虽然很难找到一个清晰的模式，但我们找到了一个有趣的案例。一个*主题*涵盖了（其他事情之间的）与*哈里王子*开玩笑有关他的发型，稍微转移到*奥巴马*就巴黎袭击发表声明，最终变成了巴黎袭击和政客们支付的致敬。这个分支并非凭空出现，而似乎遵循了一个逻辑流程：
- en: '[ROYAL, PRINCE HARRY, JOKES]'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[皇室，哈里王子，笑话]'
- en: '[ROYAL, PRINCE HARRY]'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[皇室，哈里王子]'
- en: '[PRINCE HARRY, OBAMA]'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[哈里王子，奥巴马]'
- en: '[PRINCE HARRY, OBAMA, POLITICS]'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[哈里王子，奥巴马，政治]'
- en: '[OBAMA, POLITICS]'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[奥巴马，政治]'
- en: '[OBAMA, POLITICS, PARIS]'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[奥巴马，政治，巴黎]'
- en: '[POLITICS, PARIS]'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[政治，巴黎]'
- en: '![Building story connections](img/B05261_10_21.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![构建故事连接](img/B05261_10_21.jpg)'
- en: 'Figure 19: Force-directed layout of story mutation - title'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：故事突变的力导向布局 - 标题
- en: To summarize, it seems that a breaking news event acts as a sudden perturbation
    of an Equilibrium state. Now we may wonder how long would that disturbance last,
    whether or not a new Equilibrium will be reached in the future and what would
    be the shape of the world resulting from that *wound*. Most importantly, what
    effect a different decay factor would have on that world shape.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，似乎一条突发新闻事件作为平衡状态的突然扰动。现在我们可能会想知道这种扰动会持续多久，未来是否会达到新的平衡状态，以及由此产生的世界形状会是什么样子。最重要的是，不同的衰减因子对世界形状会产生什么影响。
- en: With enough time and motivation, we would potentially be interested in applying
    some concepts of physics around *perturbation theory* ([http://www.tcm.phy.cam.ac.uk/~bds10/aqp/handout_dep.pdf](http://www.tcm.phy.cam.ac.uk/~bds10/aqp/handout_dep.pdf)).
    I personally would be interested in finding harmonics around this Equilibrium.
    The reason of the Paris attacks events being so memorable is because of its violent
    nature for sure, but also because it happened only a few months after the *Charlie
    Hebdo* attacks in Paris.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有足够的时间和动力，我们可能会对应用物理学中的*摄动理论*（[http://www.tcm.phy.cam.ac.uk/~bds10/aqp/handout_dep.pdf](http://www.tcm.phy.cam.ac.uk/~bds10/aqp/handout_dep.pdf)）的一些概念感兴趣。我个人对在这个平衡点周围找到谐波很感兴趣。巴黎袭击事件之所以如此令人难忘，当然是因为其暴力性质，但也因为它发生在巴黎*查理周刊*袭击事件仅几个月后。
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter was really complex and the story mutation problem could not be
    easily solved in the time frame allowed for delivering this chapter. However,
    what we discovered is truly amazing as it opens up a lot of questions. We did
    not want to draw any conclusion though, so we stopped our process right after
    the observation of the Paris attack disturbance and left that discussion open
    for our readers. Feel free to download our code base and study any breaking news
    and their potential impacts in what we define as an Equilibrium state. We are
    very much looking forward to hearing back from you and learning about your findings
    and different interpretations.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章非常复杂，故事突变问题在允许交付本章的时间范围内无法轻易解决。然而，我们发现的东西真是令人惊奇，因为它引发了很多问题。我们并不想得出任何结论，所以我们在观察到巴黎袭击干扰后立即停止了我们的过程，并为我们的读者留下了这个讨论。请随意下载我们的代码库，并研究任何突发新闻及其在我们定义的平衡状态中的潜在影响。我们非常期待听到您的回音，并了解您的发现和不同的解释。
- en: Surprisingly, we did not know anything about the *Galaxy Note 7 fiasco* before
    writing this chapter, and without the API created in the first section, the related
    articles would surely have been indistinguishable from the mass. De-duplicating
    content using  **Simhash** really helped us get a better overview of the world
    news events.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，在撰写本章之前，我们对*盖乐世Note 7惨败*一无所知，如果没有第一节中创建的API，相关文章肯定会与大众无异。使用**Simhash**进行内容去重确实帮助我们更好地了解世界新闻事件。
- en: In the next chapter, we will try to detect abnormal tweets related to the US
    elections and the new president elect (*Donald Trump*). We will cover both *Word2Vec*
    algorithm and Stanford NLP for sentiment analysis.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将尝试检测与美国选举和新当选总统（*唐纳德·特朗普*）有关的异常推文。我们将涵盖*Word2Vec*算法和斯坦福NLP进行情感分析。
