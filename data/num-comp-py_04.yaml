- en: Unsupervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: The goal of unsupervised learning is to discover the hidden patterns or structures
    of the data in which no target variable exists to perform either classification
    or regression methods. Unsupervised learning methods are often more challenging,
    as the outcomes are subjective and there is no simple goal for the analysis, such
    as predicting the class or continuous variable. These methods are performed as
    part of exploratory data analysis. On top of that, it can be hard to assess the
    results obtained from unsupervised learning methods, since there is no universally
    accepted mechanism for performing the validation of results.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的目标是发现数据中的隐藏模式或结构，其中不存在用于执行分类或回归方法的目标变量。无监督学习方法通常更具挑战性，因为结果是主观的，并且没有像预测类别或连续变量这样简单的分析目标。这些方法通常作为探索性数据分析的一部分进行。此外，由于没有公认的结果验证机制，因此很难评估无监督学习方法获得的结果。
- en: 'Nonetheless, unsupervised learning methods have growing importance in various
    fields as a trending topic nowadays, and many researchers are actively working
    on them at the moment to explore this new horizon. A few good applications are:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，无监督学习方法在各个领域的应用越来越重要，成为当前的热门话题，许多研究人员正在积极研究这些方法，探索这一新领域。以下是一些良好的应用示例：
- en: '**Genomics**: Unsupervised learning applied to understanding genomic-wide biological
    insights from DNA to better understand diseases and peoples. These types of tasks
    are more exploratory in nature.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基因组学**：将无监督学习应用于理解基因组范围的生物学见解，从DNA入手更好地理解疾病和人类。这些任务本质上更具探索性。'
- en: '**Search engine**: Search engines might choose which search results to display
    to a particular individual based on the click histories of other similar users.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索引擎**：搜索引擎可能会根据其他相似用户的点击历史来选择显示哪些搜索结果给特定个体。'
- en: '**Knowledge extraction**: To extract the taxonomies of concepts from raw text
    to generate the knowledge graph to create the semantic structures in the field
    of NLP.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识提取**：从原始文本中提取概念的分类法，生成知识图谱，以创建自然语言处理（NLP）领域的语义结构。'
- en: '**Segmentation of customers**: In the banking industry, unsupervised learning
    like clustering is applied to group similar customers, and based on those segments,
    marketing departments design their contact strategies. For example, older, low-risk
    customers will be targeted with fixed deposit products and high-risk, younger
    customers will be targeted with credit cards or mutual funds, and so on.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户细分**：在银行业中，像聚类这样的无监督学习被应用于将相似的客户分组，并根据这些细分，营销部门设计他们的接触策略。例如，年龄较大、低风险的客户将通过定期存款产品进行目标营销，而高风险、年轻的客户则会通过信用卡或共同基金等进行营销，等等。'
- en: '**Social network analysis**: To identify the cohesive groups of people in networks
    who are more connected with each other and have similar characteristics in common.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社交网络分析**：识别网络中彼此更紧密连接、具有共同特征的凝聚群体。'
- en: 'In this chapter, we will be covering the following techniques to perform unsupervised
    learning with data which is openly available:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下技术，使用公开可获得的数据进行无监督学习：
- en: K-means clustering
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means 聚类
- en: Principal component analysis
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Singular value decomposition
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: Deep auto encoders
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度自编码器
- en: K-means clustering
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means 聚类
- en: Clustering is the task of grouping observations in such a way that members of
    the same cluster are more similar to each other and members of different clusters
    are very different from each other.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是将观察值分组的任务，确保同一聚类的成员彼此之间更加相似，而不同聚类的成员之间差异较大。
- en: 'Clustering is commonly used to explore a dataset to either identify the underlying
    patterns in it or to create a group of characteristics. In the case of social
    networks, they can be clustered to identify communities and to suggest missing
    connections between people. Here are a few examples:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类通常用于探索数据集，以识别其中的潜在模式或创建一组特征。在社交网络中，可以通过聚类来识别社区并建议人们之间缺失的连接。以下是一些示例：
- en: In anti-money laundering measures, suspicious activities and individuals can
    be identified using anomaly detection
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在反洗钱措施中，可以通过异常检测识别可疑的活动和个人。
- en: In biology, clustering is used to find groups of genes with similar expression
    patterns
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生物学中，聚类用于寻找具有相似表达模式的基因群体。
- en: In marketing analytics, clustering is used to find segments of similar customers
    so that different marketing strategies can be applied to different customer segments
    accordingly
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在市场营销分析中，聚类用于寻找相似客户的细分群体，从而为不同的客户群体制定不同的营销策略。
- en: The k-means clustering algorithm is an iterative process of moving the centers
    of clusters or centroids to the mean position of their constituent points, and
    reassigning instances to their closest clusters iteratively until there is no
    significant change in the number of cluster centers possible or number of iterations
    reached.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类算法是一个迭代过程，它将聚类中心或质心移动到其组成点的均值位置，并反复将实例分配到最接近的聚类，直到聚类中心数量没有显著变化或达到最大迭代次数。
- en: 'The cost function of k-means is determined by the Euclidean distance (square-norm)
    between the observations belonging to that cluster with its respective centroid
    value. An intuitive way to understand the equation is, if there is only one cluster
    (*k=1*), then the distances between all the observations are compared with its
    single mean. Whereas, if, number of clusters increases to *2* (*k= 2*), then two-means
    are calculated and a few of the observations are assigned to cluster *1* and other
    observations are assigned to cluster two*-*based on proximity. Subsequently, distances
    are calculated in cost functions by applying the same distance measure, but separately
    to their cluster centers:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: k-means的代价函数由属于某个聚类的观测值与其相应质心之间的欧几里得距离（平方范数）决定。理解该公式的一种直观方法是，如果只有一个聚类（*k=1*），那么所有观测值与该单一均值的距离将被比较。而如果聚类数增加到*2*（*k=2*），则计算两个均值，一些观测值被分配到聚类*1*，另一些观测值根据距离分配到聚类*2*。随后，通过应用相同的距离度量，但分别对其聚类中心计算距离，从而得到代价函数：
- en: '![](img/5b6d065b-e4eb-4099-98ea-1bb4ce828e7a.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b6d065b-e4eb-4099-98ea-1bb4ce828e7a.jpg)'
- en: K-means working methodology from first principles
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从第一性原理出发的K-means工作方法论
- en: The k-means working methodology is illustrated in the following example in which
    12 instances are considered with their *X* and *Y* values. The task is to determine
    the optimal clusters out of the data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: k-means的工作方法论通过以下示例进行了说明，在该示例中考虑了12个实例及其*X*和*Y*值。任务是从数据中确定最优的聚类。
- en: '| **Instance** | **X** | **Y** |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **实例** | **X** | **Y** |'
- en: '| 1 | 7 | 8 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 7 | 8 |'
- en: '| 2 | 2 | 4 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | 4 |'
- en: '| 3 | 6 | 4 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 6 | 4 |'
- en: '| 4 | 3 | 2 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3 | 2 |'
- en: '| 5 | 6 | 5 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 6 | 5 |'
- en: '| 6 | 5 | 7 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 5 | 7 |'
- en: '| 7 | 3 | 3 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 3 | 3 |'
- en: '| 8 | 1 | 4 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 1 | 4 |'
- en: '| 9 | 5 | 4 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 5 | 4 |'
- en: '| 10 | 7 | 7 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 7 | 7 |'
- en: '| 11 | 7 | 6 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 7 | 6 |'
- en: '| 12 | 2 | 1 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 2 | 1 |'
- en: After plotting the data points on a 2D chart, we can see that roughly two clusters
    are possible, where below-left is the first cluster and the top-right is another
    cluster, but in many practical cases, there would be so many variables (or dimensions)
    that, we cannot simply visualize them. Hence, we need a mathematical and algorithmic
    way to solve these types of problems.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据点绘制到二维图表上后，我们可以看到大致上有两个聚类可能性，其中左下角是第一个聚类，右上角是另一个聚类。但在许多实际案例中，变量（或维度）会非常多，以至于我们无法简单地将其可视化。因此，我们需要一种数学和算法的方法来解决这些问题。
- en: '![](img/f0c97d44-75e6-4853-92fe-a3372e260516.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0c97d44-75e6-4853-92fe-a3372e260516.png)'
- en: 'Iteration 1: Let us assume two centers from two instances out of all the *12*
    instances. Here, we have chosen instance *1* (*X = 7, Y = 8*) and instance *8*
    (*X = 1, Y = 4*), as they seem to be at both extremes. For each instance, we will
    calculate its Euclidean distances with respect to both centroids and assign it
    to the nearest cluster center.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代1：假设从所有*12*个实例中选择两个中心。这里，我们选择了实例*1*（*X = 7, Y = 8*）和实例*8*（*X = 1, Y = 4*），因为它们似乎位于两个极端。对于每个实例，我们将计算其相对于两个质心的欧几里得距离，并将其分配到最近的聚类中心。
- en: '| **Instance** | **X** | **Y** | **Centroid 1 distance** | **Centroid 2 distance**
    | **Assigned cluster** |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| **实例** | **X** | **Y** | **质心1距离** | **质心2距离** | **分配的聚类** |'
- en: '| 1 | 7 | 8 | 7.21 | 0.00 | C2 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 7 | 8 | 7.21 | 0.00 | C2 |'
- en: '| 2 | 2 | 4 | 1.00 | 6.40 | C1 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | 4 | 1.00 | 6.40 | C1 |'
- en: '| 3 | 6 | 4 | 5.00 | 4.12 | C2 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 6 | 4 | 5.00 | 4.12 | C2 |'
- en: '| 4 | 3 | 2 | 2.83 | 7.21 | C1 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3 | 2 | 2.83 | 7.21 | C1 |'
- en: '| 5 | 6 | 5 | 5.10 | 3.16 | C2 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 6 | 5 | 5.10 | 3.16 | C2 |'
- en: '| 6 | 5 | 7 | 5.00 | 2.24 | C2 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 5 | 7 | 5.00 | 2.24 | C2 |'
- en: '| 7 | 3 | 3 | 2.24 | 6.40 | C1 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 3 | 3 | 2.24 | 6.40 | C1 |'
- en: '| 8 | 1 | 4 | 0.00 | 7.21 | C1 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 1 | 4 | 0.00 | 7.21 | C1 |'
- en: '| 9 | 5 | 4 | 4.00 | 4.47 | C1 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 5 | 4 | 4.00 | 4.47 | C1 |'
- en: '| 10 | 7 | 7 | 6.71 | 1.00 | C2 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 7 | 7 | 6.71 | 1.00 | C2 |'
- en: '| 11 | 7 | 6 | 6.32 | 2.00 | C2 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 7 | 6 | 6.32 | 2.00 | C2 |'
- en: '| 12 | 2 | 1 | 3.16 | 8.60 | C1 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 2 | 1 | 3.16 | 8.60 | C1 |'
- en: '| Centroid 1 | 1 | 4 | ![](img/438e74a5-745c-43f0-ad9c-2362b2b59b9c.png) |
    ![](img/cbff530d-df02-4220-9c3e-08fed5173b00.png) | ![](img/8af3b2a7-62cc-4f43-92ae-40a7b23fd7bf.png)
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 聚类中心1 | 1 | 4 | ![](img/438e74a5-745c-43f0-ad9c-2362b2b59b9c.png) | ![](img/cbff530d-df02-4220-9c3e-08fed5173b00.png)
    | ![](img/8af3b2a7-62cc-4f43-92ae-40a7b23fd7bf.png) |'
- en: '| Centroid 2 | 7 | 8 | ![](img/358bb8ba-84bc-41af-adbc-ba5af7a1c240.png) |
    ![](img/4e3fe26d-4992-4faa-85bb-0f9e97e3c876.png) | ![](img/a739b078-ad01-43e3-a39d-ef9175add4bd.png)
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 聚类中心2 | 7 | 8 | ![](img/358bb8ba-84bc-41af-adbc-ba5af7a1c240.png) | ![](img/4e3fe26d-4992-4faa-85bb-0f9e97e3c876.png)
    | ![](img/a739b078-ad01-43e3-a39d-ef9175add4bd.png) |'
- en: 'The Euclidean distance between two points *A (X1, Y1)* and *B (X2, Y2)* is
    shown as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 两点之间的欧几里得距离 *A (X1, Y1)* 和 *B (X2, Y2)* 如下所示：
- en: '![](img/55d4981b-74a0-49ff-847c-6979ec3f53eb.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55d4981b-74a0-49ff-847c-6979ec3f53eb.jpg)'
- en: Centroid distance calculations are performed by taking Euclidean distances.
    A sample calculation has been shown as follows. For instance, six with respect
    to both centroids (centroid 1 and centroid 2).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类中心的距离计算是通过计算欧几里得距离来进行的。以下展示了一个样本计算。例如，六号实例与两个聚类中心（聚类中心1和聚类中心2）之间的距离。
- en: '![](img/110a5c4f-55e5-4f06-ba05-d66bdffb1d7f.jpg)![](img/9f064a47-95f4-4124-aa3f-8a92fadc15b2.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/110a5c4f-55e5-4f06-ba05-d66bdffb1d7f.jpg)![](img/9f064a47-95f4-4124-aa3f-8a92fadc15b2.jpg)'
- en: 'The following chart describes the assignment of instances to both centroids,
    which was shown in the preceding table format:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了实例到两个聚类中心的分配情况，格式与前面的表格相同：
- en: '![](img/77001e36-ac36-4787-a869-233ccf9d4785.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77001e36-ac36-4787-a869-233ccf9d4785.png)'
- en: If we carefully observe the preceding chart, we realize that all the instances
    seem to be assigned appropriately apart from instance *9 (X =5, Y = 4)*. However,
    in later stages, it should be assigned appropriately. Let us see in the below
    steps how the assignments evolve.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仔细观察前面的图表，我们会发现，除了实例*9 (X =5, Y = 4)*外，所有实例似乎都已被正确分配。然而，在后续阶段，它应该被正确分配。让我们看看下面的步骤中分配是如何演变的。
- en: 'Iteration 2: In this iteration, new centroids are calculated from the assigned
    instances for that cluster or centroid. New centroids are calculated based on
    the simple average of the assigned points.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代2：在此迭代中，新的聚类中心由分配给该聚类或聚类中心的实例计算得出。新的聚类中心是基于所分配点的简单平均值来计算的。
- en: '| **Instance** | **X** | **Y** | **Assigned cluster** |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| **实例** | **X** | **Y** | **分配的聚类** |'
- en: '| 1 | 7 | 8 | C2 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 7 | 8 | C2 |'
- en: '| 2 | 2 | 4 | C1 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | 4 | C1 |'
- en: '| 3 | 6 | 4 | C2 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 6 | 4 | C2 |'
- en: '| 4 | 3 | 2 | C1 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3 | 2 | C1 |'
- en: '| 5 | 6 | 5 | C2 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 6 | 5 | C2 |'
- en: '| 6 | 5 | 7 | C2 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 5 | 7 | C2 |'
- en: '| 7 | 3 | 3 | C1 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 3 | 3 | C1 |'
- en: '| 8 | 1 | 4 | C1 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 1 | 4 | C1 |'
- en: '| 9 | 5 | 4 | C1 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 5 | 4 | C1 |'
- en: '| 10 | 7 | 7 | C2 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 7 | 7 | C2 |'
- en: '| 11 | 7 | 6 | C2 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 7 | 6 | C2 |'
- en: '| 12 | 2 | 1 | C1 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 2 | 1 | C1 |'
- en: '| Centroid 1 | 2.67 | 3 | ![](img/742e1f1f-b3d1-4f1c-9784-abc6d1a3094c.png)
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 聚类中心1 | 2.67 | 3 | ![](img/742e1f1f-b3d1-4f1c-9784-abc6d1a3094c.png) |'
- en: '| Centroid 2 | 6.33 | 6.17 | ![](img/43b9c467-4b37-4ffa-8eaa-839f8fd21095.png)
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 聚类中心2 | 6.33 | 6.17 | ![](img/43b9c467-4b37-4ffa-8eaa-839f8fd21095.png) |'
- en: 'Sample calculations for centroids 1 and 2 are shown as follows. A similar methodology
    will be applied on all subsequent iterations as well:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类中心1和2的样本计算如下所示。类似的方法将应用于所有后续的迭代：
- en: '![](img/9c84a254-ad28-4346-96d2-fd7375d5af80.jpg)![](img/bc01e19a-688e-441c-af12-a666e4697e47.jpg)![](img/e17ff033-4f21-4475-844f-93fa65c45e6b.jpg)![](img/f978f046-94fd-4db3-9e3b-f5e12cbe22b9.jpg)![](img/285ee94c-54a6-4438-827d-d21c75b808e6.jpg)![](img/cbd81fbc-7003-4f1e-a7f3-5577aada051c.jpg)![](img/29582c7e-f166-4f38-8d69-6369cd60f01e.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c84a254-ad28-4346-96d2-fd7375d5af80.jpg)![](img/bc01e19a-688e-441c-af12-a666e4697e47.jpg)![](img/e17ff033-4f21-4475-844f-93fa65c45e6b.jpg)![](img/f978f046-94fd-4db3-9e3b-f5e12cbe22b9.jpg)![](img/285ee94c-54a6-4438-827d-d21c75b808e6.jpg)![](img/cbd81fbc-7003-4f1e-a7f3-5577aada051c.jpg)![](img/29582c7e-f166-4f38-8d69-6369cd60f01e.png)'
- en: After updating the centroids, we need to reassign the instances to the nearest
    centroids, which we will be performing in iteration 3.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 更新聚类中心后，我们需要将实例重新分配给最近的聚类中心，这将在迭代3中执行。
- en: 'Iteration 3: In this iteration, new assignments are calculated based on the
    Euclidean distance between instances and new centroids. In the event of any changes,
    new centroids will be calculated iteratively until no changes in assignments are
    possible or the number of iterations is reached. The following table describes
    the distance measures between new centroids and all the instances:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代3：在此迭代中，新的分配是基于实例与新聚类中心之间的欧几里得距离计算的。如果发生任何变化，将反复计算新的聚类中心，直到分配没有变化或达到迭代次数为止。下表描述了新聚类中心与所有实例之间的距离度量：
- en: '| **Instance** | **X** | **Y** | **Centroid 1 distance** | **Centroid 2 distance**
    | **Previously assigned cluster** | **Newly assigned cluster** | **Changed?**
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **实例** | **X** | **Y** | **质心1距离** | **质心2距离** | **先前分配的聚类** | **新分配的聚类**
    | **是否变化？** |'
- en: '| 1 | 7 | 8 | 6.61 | 1.95 | C2 | C2 | No |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 7 | 8 | 6.61 | 1.95 | C2 | C2 | No |'
- en: '| 2 | 2 | 4 | 1.20 | 4.84 | C1 | C1 | No |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | 4 | 1.20 | 4.84 | C1 | C1 | No |'
- en: '| 3 | 6 | 4 | 3.48 | 2.19 | C2 | C2 | No |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 6 | 4 | 3.48 | 2.19 | C2 | C2 | No |'
- en: '| 4 | 3 | 2 | 1.05 | 5.34 | C1 | C1 | No |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3 | 2 | 1.05 | 5.34 | C1 | C1 | No |'
- en: '| 5 | 6 | 5 | 3.88 | 1.22 | C2 | C2 | No |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 6 | 5 | 3.88 | 1.22 | C2 | C2 | No |'
- en: '| 6 | 5 | 7 | 4.63 | 1.57 | C2 | C2 | No |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 5 | 7 | 4.63 | 1.57 | C2 | C2 | No |'
- en: '| 7 | 3 | 3 | 0.33 | 4.60 | C1 | C1 | No |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 3 | 3 | 0.33 | 4.60 | C1 | C1 | No |'
- en: '| 8 | 1 | 4 | 1.95 | 5.75 | C1 | C1 | No |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 1 | 4 | 1.95 | 5.75 | C1 | C1 | No |'
- en: '| 9 | 5 | 4 | 2.54 | 2.55 | C1 | C1 | No |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 5 | 4 | 2.54 | 2.55 | C1 | C1 | No |'
- en: '| 10 | 7 | 7 | 5.89 | 1.07 | C2 | C2 | No |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 7 | 7 | 5.89 | 1.07 | C2 | C2 | No |'
- en: '| 11 | 7 | 6 | 5.27 | 0.69 | C2 | C2 | No |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 7 | 6 | 5.27 | 0.69 | C2 | C2 | No |'
- en: '| 12 | 2 | 1 | 2.11 | 6.74 | C1 | C1 | No |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 2 | 1 | 2.11 | 6.74 | C1 | C1 | No |'
- en: '| Centroid 1 | 2.67 | 3 | ![](img/646a430e-92f0-4fa3-aa08-984cd2bdbd34.png)
    | ![](img/09a53854-a61d-49ce-a6cb-1a001a8e975e.png) | ![](img/dd3ea496-cba7-4453-8c04-a1eea53f23db.png)
    | ![](img/c3da0bc7-e7ee-40ee-a5cb-03893bd6aedd.png) | ![](img/4ed2a736-8357-494a-ab4d-17be40cf3fc5.png)
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 质心1 | 2.67 | 3 | ![](img/646a430e-92f0-4fa3-aa08-984cd2bdbd34.png) | ![](img/09a53854-a61d-49ce-a6cb-1a001a8e975e.png)
    | ![](img/dd3ea496-cba7-4453-8c04-a1eea53f23db.png) | ![](img/c3da0bc7-e7ee-40ee-a5cb-03893bd6aedd.png)
    | ![](img/4ed2a736-8357-494a-ab4d-17be40cf3fc5.png) |'
- en: '| Centroid 2 | 6.33 | 6.17 | ![](img/a3eb34ab-9293-4099-9786-90591849fe29.png)
    | ![](img/252f60f5-038e-411c-92cb-66d4a9d35e5c.png) | ![](img/dbf84cbb-e970-4bc6-80d0-d9c30687e625.png)
    | ![](img/e9b5dd81-94dc-4f29-8a10-dd27556a7680.png) | ![](img/06298a03-a5fa-4365-8734-12379b51a337.png)
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 质心2 | 6.33 | 6.17 | ![](img/a3eb34ab-9293-4099-9786-90591849fe29.png) | ![](img/252f60f5-038e-411c-92cb-66d4a9d35e5c.png)
    | ![](img/dbf84cbb-e970-4bc6-80d0-d9c30687e625.png) | ![](img/e9b5dd81-94dc-4f29-8a10-dd27556a7680.png)
    | ![](img/06298a03-a5fa-4365-8734-12379b51a337.png) |'
- en: It seems that there are no changes registered. Hence, we can say that the solution
    is converged. One important thing to note here is that all the instances are very
    clearly classified well, apart from instance *9 (X = 5, Y = 4).* Based on instinct,
    it seems like it should be assigned to centroid 2, but after careful calculation,
    that instance is more proximate to cluster 1 than cluster 2\. However, the difference
    in distance is low (2.54 with centroid 1 and 2.55 with centroid 2).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎没有注册到任何变化。因此，我们可以说，解决方案已经收敛。需要注意的一点是，除了实例*9 (X = 5, Y = 4)*外，所有实例都已非常清晰地被分类。根据直觉，它应该分配给质心2，但经过仔细计算后，该实例实际上更接近聚类1而非聚类2。然而，距离的差异非常小（质心1的距离为2.54，质心2的距离为2.55）。
- en: Optimal number of clusters and cluster evaluation
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最优聚类数量与聚类评估
- en: Though selecting number of clusters is more of an art than science, optimal
    number of clusters are chosen where there will not be a much marginal increase
    in explanation ability by increasing number of clusters are possible. In practical
    applications, usually business should be able to provide what would be approximate
    number of clusters they are looking for.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管选择聚类的数量更像是一门艺术而非科学，但在选择最优聚类数时，若增加聚类数后，解释能力的提升非常小，则可以选择该数量。在实际应用中，通常业务方应该能够提供他们大致需要的聚类数量。
- en: The elbow method
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 肘部法则
- en: The elbow method is used to determine the optimal number of clusters in k-means
    clustering. The elbow method plots the value of the cost function produced by
    different values of *k*. As you know, if *k* increases, average distortion will
    decrease, each cluster will have fewer constituent instances, and the instances
    will be closer to their respective centroids. However, the improvements in average
    distortion will decline as *k* increases. The value of *k* at which improvement
    in distortion declines the most is called the elbow, at which we should stop dividing
    the data into further clusters.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 肘部法则用于确定k-means聚类中的最优聚类数量。肘部法则绘制不同*k*值下由代价函数产生的值。如你所知，当*k*增加时，平均失真度会降低，每个聚类的构成实例会减少，并且实例会更接近各自的质心。然而，随着*k*的增加，平均失真度的改善会逐渐减缓。失真度改善下降最明显的*k*值称为肘部，应该在此停止继续划分数据为更多的聚类。
- en: '![](img/bc03a902-fad6-4f77-8799-00a69a78965f.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc03a902-fad6-4f77-8799-00a69a78965f.png)'
- en: 'Evaluation of clusters with silhouette coefficient: the silhouette coefficient
    is a measure of the compactness and separation of the clusters. Higher values
    represent a better quality of cluster. The silhouette coefficient is higher for
    compact clusters that are well separated and lower for overlapping clusters. Silhouette
    coefficient values do change from -1 to +1, and the higher the value is, the better.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用轮廓系数评估聚类：轮廓系数是衡量聚类紧凑性和分离度的指标。较高的值代表更好的聚类质量。轮廓系数对紧凑且分离良好的聚类较高，而对重叠聚类则较低。轮廓系数值变化范围从
    -1 到 +1，值越高，聚类质量越好。
- en: The silhouette coefficient is calculated per instance. For a set of instances,
    it is calculated as the mean of the individual sample's scores.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数是按实例计算的。对于一组实例，它是各个样本得分的平均值。
- en: '![](img/c2801e6f-85c6-4465-ab6c-1a32129a3059.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2801e6f-85c6-4465-ab6c-1a32129a3059.jpg)'
- en: '*a* is the mean distance between the instances in the cluster, *b* is the mean
    distance between the instance and the instances in the next closest cluster.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*a* 是簇内实例之间的平均距离，*b* 是该实例与下一个最近簇中实例之间的平均距离。'
- en: K-means clustering with the iris data example
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用鸢尾花数据集进行 K-means 聚类示例
- en: 'The famous iris data has been used from the UCI machine learning repository
    for illustration purposes using k-means clustering. The link for downloading the
    data is here: [http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris).
    The iris data has three types of flowers: setosa, versicolor, and virginica and
    their respective measurements of sepal length, sepal width, petal length, and
    petal width. Our task is to group the flowers based on their measurements. The
    code is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的鸢尾花数据集来自 UCI 机器学习库，用于演示 K-means 聚类。数据下载链接在此：[http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)。鸢尾花数据集包含三种花卉：山鸢尾、变色鸢尾和维吉尼亚鸢尾，以及它们的萼片长度、萼片宽度、花瓣长度和花瓣宽度的相应测量值。我们的任务是根据这些测量值将花卉分组。代码如下：
- en: '[PRE0]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/ef89819f-082a-43d6-8e09-9faba82ae4b7.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef89819f-082a-43d6-8e09-9faba82ae4b7.png)'
- en: 'Following code is used to separate `class` variable as dependent variable for
    creating colors in plot and unsupervised learning algorithm applied on given `x`
    variables without any target variable does present:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于将 `class` 变量作为依赖变量来为图中的颜色创建并应用无监督学习算法，且在给定的 `x` 变量上进行操作，没有目标变量：
- en: '[PRE2]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As sample metrics, three clusters have been used, but in real life, we do not
    know how many clusters data will fall under in advance, hence we need to test
    the results by trial and error. The maximum number of iterations chosen here is
    300 in the following, however, this value can also be changed and the results
    checked accordingly:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，使用了三个簇，但在现实生活中，我们无法事先知道数据将属于多少个簇，因此需要通过反复试验来测试结果。这里选择的最大迭代次数为 300，当然，这个值也可以调整并相应地检查结果：
- en: '[PRE3]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/eb623d02-bec4-4a18-b0f7-50ea36f59690.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb623d02-bec4-4a18-b0f7-50ea36f59690.png)'
- en: From the previous confusion matrix, we can see that all the setosa flowers are
    clustered correctly, whereas 2 out of 50 versicolor, and 14 out of 50 virginica
    flowers are incorrectly classified.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的混淆矩阵中，我们可以看到所有的山鸢尾花都被正确地聚类，而 50 个变色鸢尾花中的 2 个和 50 个维吉尼亚鸢尾花中的 14 个被错误地分类。
- en: Again, to reiterate, in real-life examples we do not have the category names
    in advance, so we cannot measure accuracy, and so on.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，在现实生活中的例子中，我们事先并不知道类别名称，因此无法衡量准确性等指标。
- en: 'Following code is used to perform sensitivity analysis to check how many number
    of clusters does actually provide better explanation of segments:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于执行敏感性分析，检查实际提供更好细分解释的簇的数量：
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](img/87ed59be-8c75-4a70-9081-061fbf072085.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87ed59be-8c75-4a70-9081-061fbf072085.png)'
- en: The silhouette coefficient values in the preceding results shows that `K value
    2` and `K value 3` have better scores than all the other values. As a thumb rule,
    we need to take the next `K value` of the highest silhouette coefficient. Here,
    we can say that `K value 3` is better. In addition, we also need to see the average
    within cluster variation value and elbow plot before concluding the optimal `K
    value`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果中的轮廓系数值显示，`K 值 2` 和 `K 值 3` 的得分优于其他所有值。作为经验法则，我们需要选择轮廓系数最高的下一个 `K 值`。在这里，我们可以说
    `K 值 3` 更好。此外，在得出最佳 `K 值` 之前，我们还需要查看每个簇内的平均变化值和肘部图。
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/8daf39b3-428f-408c-85f1-2b5f00e27337.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8daf39b3-428f-408c-85f1-2b5f00e27337.png)'
- en: From the elbow plot, it seems that at the value of three, the slope changes
    drastically. Here, we can select the optimal k-value as three.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从肘部图中看，值为三时，斜率发生了剧烈变化。在这里，我们可以选择最优的k值为三。
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/d33fe9d2-c24d-462e-954b-eaec1784dc56.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d33fe9d2-c24d-462e-954b-eaec1784dc56.png)'
- en: Last but not least, the total percentage of variance explained value should
    be greater than 80 percent to decide the optimal number of clusters. Even here,
    a k-value of three seems to give a decent value of total variance explained. Hence,
    we can conclude from all the preceding metrics (silhouette, average within cluster
    variance, and total variance explained), that three clusters are ideal.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，解释的总方差百分比值应该大于80%，以决定最优的聚类数量。即使在这里，k值为三似乎也能提供一个合理的总方差解释值。因此，我们可以从前述的所有指标（轮廓系数、聚类内平均方差和总方差解释）得出结论，三类聚类是理想的。
- en: 'The R code for k-means clustering using iris data is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用鸢尾花数据的k均值聚类的R代码如下：
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Principal Component Analysis - PCA
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析 - PCA
- en: '**Principal Component Analysis** (**PCA**) is the dimensionality reduction
    technique which has so many utilities. PCA reduces the dimensions of a dataset
    by projecting the data onto a lower-dimensional subspace. For example, a 2D dataset
    could be reduced by projecting the points onto a line. Each instance in the dataset
    would then be represented by a single value, rather than a pair of values. In
    a similar way, a 3D dataset could be reduced to two dimensions by projecting variables
    onto a plane. PCA has the following utilities:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）是一种具有多种用途的降维技术。PCA通过将数据投影到低维子空间来减少数据集的维度。例如，可以通过将点投影到一条线上来减少二维数据集。数据集中的每个实例将由单个值表示，而不是一对值。以类似的方式，可以通过将变量投影到平面上将三维数据集减少到二维。PCA具有以下用途：'
- en: Mitigate the course of dimensionality
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓解维度灾难
- en: Compress the data while minimizing the information lost at the same time
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在压缩数据的同时，尽量减少信息丢失。
- en: Principal components will be further utilized in the next stage of supervised
    learning, in random forest, boosting, and so on
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分将在监督学习的下一阶段中进一步应用，如随机森林、提升方法等。
- en: Understanding the structure of data with hundreds of dimensions can be difficult,
    hence, by reducing the dimensions to 2D or 3D, observations can be visualized
    easily
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解具有数百个维度的数据结构可能很困难，因此，通过将维度减少到二维或三维，可以更容易地可视化观察结果。
- en: PCA can easily be explained with the following diagram of a mechanical bracket
    which has been drawn in the machine drawing module of a mechanical engineering
    course. The left-hand side of the diagram depicts the top view, front view, and
    side view of the component. However, on the right-hand side, an isometric view
    has been drawn, in which one single image has been used to visualize how the component
    looks. So, one can imagine that the left-hand images are the actual variables
    and the right-hand side is the first principal component, in which most variance
    has been captured.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）可以通过以下机械支架的示意图来轻松解释，该图已在机械工程课程的机械制图模块中绘制。图的左侧展示了组件的顶部视图、正面视图和侧面视图。而右侧则绘制了等轴测视图，其中使用了一张图像来可视化组件的外观。所以，可以想象，左侧的图像是实际的变量，右侧则是第一个主成分，其中捕获了大部分方差。
- en: Finally, three images have been replaced by a single image by rotating the axis
    of direction. In fact, we replicate the same technique in PCA analysis.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，通过旋转轴向，三张图像被替换为一张图像。实际上，我们在PCA分析中应用了相同的技术。
- en: '![](img/2b01cbe2-ce2d-4f9c-8f32-2726291ccd9c.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b01cbe2-ce2d-4f9c-8f32-2726291ccd9c.png)'
- en: Principal component working methodology is explained in the following sample
    example, in which actual data has been shown in a 2D space, in which *X* and *Y*
    axis are used to plot the data. Principal components are the ones in which maximum
    variation of the data is captured.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分工作方法在以下示例中进行了说明，实际数据在二维空间中展示，其中使用*X*和*Y*轴来绘制数据。主成分是捕捉数据最大变异性的部分。
- en: '![](img/bb756076-0484-4d81-91e0-7a3408877bbc.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb756076-0484-4d81-91e0-7a3408877bbc.png)'
- en: The following diagram illustrates how it looks after fitting the principal components.
    The first principal component covers the maximum variance in the data and the
    second principal component is orthogonal to the first principal component, as
    we know all principal components are orthogonal to each other. We can represent
    whole data with the first principal component itself. In fact, that is how it
    is advantageous to represent the data with fewer dimensions, to save space and
    also to grab maximum variance in the data, which can be utilized for supervised
    learning in the next stage. This is the core advantage of computing principal
    components.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了拟合主成分后的效果。第一个主成分涵盖了数据中的最大方差，第二个主成分与第一个主成分正交，正如我们所知，所有主成分彼此正交。我们可以仅用第一个主成分来表示整个数据。事实上，这就是用更少的维度表示数据的优势，不仅可以节省空间，还能抓取数据中的最大方差，这在下一阶段的监督学习中可以得到利用。这就是计算主成分的核心优势。
- en: '![](img/c14a3e04-d0bd-4e75-a88c-3bc333313de5.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c14a3e04-d0bd-4e75-a88c-3bc333313de5.png)'
- en: Eigenvectors and eigenvalues have significant importance in the field of linear
    algebra, physics, mechanics, and so on. Refreshing, basics on eigenvectors and
    eigenvalues is necessary when studying PCAs. Eigenvectors are the axes (directions)
    along which a linear transformation acts simply by *stretching/compressing* and/or
    *flipping*; whereas, eigenvalues give you the factors by which the compression
    occurs. In another way, an eigenvector of a linear transformation is a nonzero
    vector whose direction does not change when that linear transformation is applied
    to it.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量和特征值在线性代数、物理学、力学等领域具有重要意义。在学习主成分分析（PCA）时，刷新对特征向量和特征值的基础知识是必要的。特征向量是线性变换作用下仅通过
    *拉伸/压缩* 和/或 *翻转* 的轴（方向）；而特征值则告诉你压缩发生的倍数。换句话说，线性变换的特征向量是一个非零向量，在应用该线性变换时，其方向保持不变。
- en: 'More formally, *A* is a linear transformation from a vector space and ![](img/3db9b070-a499-4f85-adbb-f94db5952d8e.jpg) is
    a nonzero vector, then eigen vector of *A* if ![](img/3cb9f44d-af9b-458c-9ab7-30d0c07d0097.jpg) is
    a scalar multiple of ![](img/2bbd2b5e-e3f5-4a73-9fcf-e22a9fab8389.jpg). The condition
    can be written as the following equation:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，*A* 是从向量空间到 ![](img/3db9b070-a499-4f85-adbb-f94db5952d8e.jpg) 的线性变换，如果
    ![](img/3cb9f44d-af9b-458c-9ab7-30d0c07d0097.jpg) 是 ![](img/2bbd2b5e-e3f5-4a73-9fcf-e22a9fab8389.jpg)
    的标量倍数，则 ![](img/3cb9f44d-af9b-458c-9ab7-30d0c07d0097.jpg) 是 *A* 的特征向量。该条件可以写为以下方程：
- en: '![](img/1f928a6c-47af-4385-a6d7-fab5714d5e3e.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f928a6c-47af-4385-a6d7-fab5714d5e3e.jpg)'
- en: 'In the preceding equation, ![](img/3be9b079-d8cb-429a-a0bb-846f2f1e8376.jpg) is
    an eigenvector, *A* is a square matrix, and λ is a scalar called an eigenvalue.
    The direction of an eigenvector remains the same after it has been transformed
    by *A*; only its magnitude has changed, as indicated by the eigenvalue, That is,
    multiplying a matrix by one of its eigenvectors is equal to scaling the eigenvector,
    which is a compact representation of the original matrix. The following graph
    describes eigenvectors and eigenvalues in a graphical representation in a 2D space:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，![](img/3be9b079-d8cb-429a-a0bb-846f2f1e8376.jpg) 是一个特征向量，*A* 是一个方阵，λ
    是一个标量，称为特征值。特征向量的方向在被 *A* 变换后保持不变，只有其大小发生了变化，这一变化由特征值表示。换句话说，将一个矩阵乘以其特征向量等同于对特征向量进行缩放，这是原始矩阵的紧凑表示。下图展示了特征向量和特征值在二维空间中的图形表示：
- en: '![](img/17245038-7b03-404a-a156-64d171bafc2b.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17245038-7b03-404a-a156-64d171bafc2b.png)'
- en: The following example describes how to calculate eigenvectors and eigenvalues
    from the square matrix and its understanding. Note that eigenvectors and eigenvalues
    can be calculated only for square matrices (those with the same dimensions of
    rows and columns).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例描述了如何从方阵及其理解中计算特征向量和特征值。请注意，特征向量和特征值只能针对方阵（行列数相同的矩阵）进行计算。
- en: '![](img/cea11ff3-de17-44c2-a6f3-3ea239962f68.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cea11ff3-de17-44c2-a6f3-3ea239962f68.jpg)'
- en: 'Recall the equation that the product of *A* and any eigenvector of *A* must
    be equal to the eigenvector multiplied by the magnitude of eigenvalue:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下方程，即 *A* 与任何 *A* 的特征向量的乘积必须等于特征向量与特征值的大小相乘：
- en: '![](img/cba22798-df2e-4158-888c-8df53ef0ade2.jpg)![](img/f58e2ccc-bbc1-416b-8a2b-0e4454dcc257.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cba22798-df2e-4158-888c-8df53ef0ade2.jpg)![](img/f58e2ccc-bbc1-416b-8a2b-0e4454dcc257.jpg)'
- en: A characteristic equation states that the determinant of the matrix, that is
    the difference between the data matrix and the product of the identity matrix
    and an eigenvalue is *0*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 特征方程表明矩阵的行列式，即数据矩阵与单位矩阵和特征值的乘积之差为*0*。
- en: '![](img/40df230b-283f-4970-9b17-c0d969b6a754.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40df230b-283f-4970-9b17-c0d969b6a754.jpg)'
- en: 'Both eigenvalues for the preceding matrix are equal to *-2*. We can use eigenvalues
    to substitute for eigenvectors in an equation:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 前述矩阵的两个特征值均为*-2*。我们可以使用特征值来替代方程中的特征向量：
- en: '![](img/c8910138-9f31-4248-9895-e8cb45c7cfdd.jpg)![](img/7c827736-7dc5-4c02-8f0c-0c90a94300eb.jpg)![](img/7dc3ba7d-5947-436f-a1ae-35ddcb67a9ab.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8910138-9f31-4248-9895-e8cb45c7cfdd.jpg)![](img/7c827736-7dc5-4c02-8f0c-0c90a94300eb.jpg)![](img/7dc3ba7d-5947-436f-a1ae-35ddcb67a9ab.jpg)'
- en: 'Substituting the value of eigenvalue in the preceding equation, we will obtain
    the following formula:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 将特征值代入前述方程，我们将得到以下公式：
- en: '![](img/49410af9-b4e6-45af-8312-458cb9ad23e8.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49410af9-b4e6-45af-8312-458cb9ad23e8.jpg)'
- en: 'The preceding equation can be rewritten as a system of equations, as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方程可以重写为以下方程组：
- en: '![](img/e1073470-0ede-40de-beef-e5f1922bb29a.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1073470-0ede-40de-beef-e5f1922bb29a.jpg)'
- en: This equation indicates it can have multiple solutions of eigenvectors we can
    substitute with any values which hold the preceding equation for verification
    of equation. Here, we have used the vector *[1 1]* for verification, which seems
    to be proved.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程表明它可以有多个特征向量的解，我们可以用任何满足前述方程的值进行替代以验证方程。在这里，我们使用了向量*[1 1]*进行验证，似乎已被证明。
- en: '![](img/99fb5711-0835-4d42-a455-1bfb86610443.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99fb5711-0835-4d42-a455-1bfb86610443.jpg)'
- en: 'PCA needs unit eigenvectors to be used in calculations, hence we need to divide
    the same with the norm or we need to normalize the eigenvector. The 2-norm equation
    is shown as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 需要单位特征向量进行计算，因此我们需要用范数除以特征向量，或者我们需要对特征向量进行归一化处理。二范数方程如下所示：
- en: '![](img/59838abb-7a26-498f-914a-2c37a0ed985b.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59838abb-7a26-498f-914a-2c37a0ed985b.jpg)'
- en: 'The norm of the output vector is calculated as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 输出向量的范数计算如下：
- en: '![](img/f2c8bb1d-4a74-4d73-aef8-5f08fb11791f.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2c8bb1d-4a74-4d73-aef8-5f08fb11791f.jpg)'
- en: 'The unit eigenvector is shown as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 单位特征向量如下所示：
- en: '![](img/0339799d-78a7-4016-accb-4d2b1b091791.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0339799d-78a7-4016-accb-4d2b1b091791.jpg)'
- en: PCA working methodology from first principles
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA 从基本原理出发的工作方法
- en: 'PCA working methodology is described in the following sample data, which has
    two dimensions for each instance or data point. The objective here is to reduce
    the 2D data into one dimension (also known as the **principal component**):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 工作方法在以下示例数据中描述，每个实例或数据点有两个维度。这里的目标是将二维数据降维为一维（也称为**主成分**）：
- en: '| **Instance** | **X** | **Y** |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| **实例** | **X** | **Y** |'
- en: '| 1 | 0.72 | 0.13 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.72 | 0.13 |'
- en: '| 2 | 0.18 | 0.23 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.18 | 0.23 |'
- en: '| 3 | 2.50 | 2.30 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2.50 | 2.30 |'
- en: '| 4 | 0.45 | 0.16 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.45 | 0.16 |'
- en: '| 5 | 0.04 | 0.44 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.04 | 0.44 |'
- en: '| 6 | 0.13 | 0.24 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0.13 | 0.24 |'
- en: '| 7 | 0.30 | 0.03 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.30 | 0.03 |'
- en: '| 8 | 2.65 | 2.10 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 2.65 | 2.10 |'
- en: '| 9 | 0.91 | 0.91 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.91 | 0.91 |'
- en: '| 10 | 0.46 | 0.32 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.46 | 0.32 |'
- en: '| Column mean | 0.83 | 0.69 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 列均值 | 0.83 | 0.69 |'
- en: The first step, prior to proceeding with any analysis, is to subtract the mean
    from all the observations, which removes the scale factor of variables and makes
    them more uniform across dimensions.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步，在进行任何分析之前，是从所有观察值中减去均值，这样可以去除变量的尺度因素，并使它们在各维度之间更加统一。
- en: '| **X** | **Y** |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| **X** | **Y** |'
- en: '| *0.72 - 0.83 = -0.12* | *0.13 - 0.69 = - 0.55* |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| *0.72 - 0.83 = -0.12* | *0.13 - 0.69 = - 0.55* |'
- en: '| *0.18 - 0.83 = -0.65* | *0.23 - 0.69 = - 0.46* |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| *0.18 - 0.83 = -0.65* | *0.23 - 0.69 = - 0.46* |'
- en: '| *2.50 - 0.83 = 1.67* | *2.30 - 0.69 = 1.61* |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| *2.50 - 0.83 = 1.67* | *2.30 - 0.69 = 1.61* |'
- en: '| *0.45 - 0.83 = -0.38* | *0.16 - 0.69 = - 0.52* |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| *0.45 - 0.83 = -0.38* | *0.16 - 0.69 = - 0.52* |'
- en: '| *0.04 - 0.83 = -0.80* | *0.44 - 0.69 = - 0.25* |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| *0.04 - 0.83 = -0.80* | *0.44 - 0.69 = - 0.25* |'
- en: '| *0.13 - 0.83 = -0.71* | *0.24 - 0.69 = - 0.45* |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| *0.13 - 0.83 = -0.71* | *0.24 - 0.69 = - 0.45* |'
- en: '| *0.30 - 0.83 = -0.53* | *0.03 - 0.69 = - 0.66* |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| *0.30 - 0.83 = -0.53* | *0.03 - 0.69 = - 0.66* |'
- en: '| *2.65 - 0.83 = 1.82* | *2.10 - 0.69 = 1.41* |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| *2.65 - 0.83 = 1.82* | *2.10 - 0.69 = 1.41* |'
- en: '| *0.91 - 0.83 = 0.07* | *0.91 - 0.69 = 0.23* |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| *0.91 - 0.83 = 0.07* | *0.91 - 0.69 = 0.23* |'
- en: '| *0.46 - 0.83 = -0.37* | *0.32 - 0.69 = -0.36* |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| *0.46 - 0.83 = -0.37* | *0.32 - 0.69 = -0.36* |'
- en: 'Principal components are calculated using two different techniques:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分可以通过两种不同的技术进行计算：
- en: Covariance matrix of the data
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的协方差矩阵
- en: Singular value decomposition
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: We will be covering the singular value decomposition technique in the next section.
    In this section, we will solve eigenvectors and eigenvalues using covariance matrix
    methodology.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中介绍奇异值分解技术。在本节中，我们将使用协方差矩阵方法求解特征向量和特征值。
- en: 'Covariance is a measure of how much two variables change together and it is
    a measure of the strength of the correlation between two sets of variables. If
    the covariance of two variables is zero, we can conclude that there will not be
    any correlation between two sets of the variables. The formula for covariance
    is as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差是衡量两个变量共同变化程度的指标，也是衡量两个变量集合之间相关性强度的度量。如果两个变量的协方差为零，我们可以得出结论，说明这两个变量集合之间没有任何相关性。协方差的公式如下：
- en: '![](img/ff9c2871-d222-4ea6-ad0a-7709e9255386.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff9c2871-d222-4ea6-ad0a-7709e9255386.jpg)'
- en: A sample covariance calculation is shown for *X* and *Y* variables in the following
    formulas. However, it is a 2 x 2 matrix of an entire covariance matrix (also,
    it is a square matrix).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的公式展示了 *X* 和 *Y* 变量的样本协方差计算。然而，它是一个 2 x 2 的矩阵，表示整个协方差矩阵（此外，它是一个方阵）。
- en: '![](img/df0457af-fc6c-407e-9500-1a329a999902.jpg)![](img/56608b63-bc70-4109-bec9-369e7131b43c.jpg)![](img/a2526e8a-4126-408b-8c35-516225fa0e7d.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df0457af-fc6c-407e-9500-1a329a999902.jpg)![](img/56608b63-bc70-4109-bec9-369e7131b43c.jpg)![](img/a2526e8a-4126-408b-8c35-516225fa0e7d.jpg)'
- en: Since the covariance matrix is square, we can calculate eigenvectors and eigenvalues
    from it. You can refer to the methodology explained in an earlier section.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于协方差矩阵是方阵，我们可以从中计算特征向量和特征值。你可以参考前面章节中解释的方法。
- en: '![](img/bbf6df6e-5d7a-4a74-8d50-4cd569311ae0.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbf6df6e-5d7a-4a74-8d50-4cd569311ae0.jpg)'
- en: 'By solving the preceding equation, we can obtain eigenvectors and eigenvalues,
    as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 通过求解上述方程，我们可以获得特征向量和特征值，如下所示：
- en: '![](img/e6786705-3ce4-498c-8a3f-1f8c54286647.jpg)![](img/aeb1ea0e-ce34-4a15-b69e-8b5d4fa6cbf5.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6786705-3ce4-498c-8a3f-1f8c54286647.jpg)![](img/aeb1ea0e-ce34-4a15-b69e-8b5d4fa6cbf5.jpg)'
- en: 'The preceding mentioned results can be obtained with the following Python syntax:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 前述结果可以通过以下 Python 语法获得：
- en: '[PRE8]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/4ecbc0fb-7f02-4f7b-8c3f-450f594cf379.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ecbc0fb-7f02-4f7b-8c3f-450f594cf379.png)'
- en: Once we obtain the eigenvectors and eigenvalues, we can project data into principal
    components. The first eigenvector has the greatest eigenvalue and is the first
    principal component, as we would like to reduce the original 2D data into 1D data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦获得特征向量和特征值，我们可以将数据投影到主成分上。第一个特征向量具有最大的特征值，是第一个主成分，因为我们希望将原始的 2D 数据压缩成 1D 数据。
- en: '![](img/3a1b86ba-2783-41a7-9b6b-c9ba58786a34.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a1b86ba-2783-41a7-9b6b-c9ba58786a34.jpg)'
- en: From the preceding result, we can see the 1D projection of the first principal
    component from the original 2D data. Also, the eigenvalue of 1.5725 explains the
    fact that the principal component explains variance of 57 percent more than the
    original variables. In the case of multi-dimensional data, the thumb rule is to
    select the eigenvalues or principal components with a value greater than what
    should be considered for projection.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述结果中，我们可以看到原始 2D 数据的第一个主成分的 1D 投影。此外，特征值 1.5725 表明主成分解释了原始变量 57% 的方差。在多维数据的情况下，一般的经验法则是选择特征值或主成分的值大于某个阈值作为投影的依据。
- en: PCA applied on handwritten digits using scikit-learn
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 对手写数字应用 PCA
- en: 'The PCA example has been illustrated with the handwritten digits example from
    scikit-learn datasets, in which handwritten digits are created from 0-9 and its
    respective 64 features (8 x 8 matrix) of pixel intensities. Here, the idea is
    to represent the original features of 64 dimensions into as few as possible:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 示例已通过来自 scikit-learn 数据集的手写数字示例进行说明，其中手写数字从 0 到 9，并且其相应的 64 个特征（8 x 8 矩阵）表示像素强度。这里的核心思想是将原始的
    64 维特征尽可能压缩到更少的维度：
- en: '[PRE9]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/2153e5c6-a7cd-4825-b7f0-18243c674eee.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2153e5c6-a7cd-4825-b7f0-18243c674eee.png)'
- en: 'Plot the graph using the `plt.show` function:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `plt.show` 函数绘制图形：
- en: '[PRE10]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/d5d0f2fd-2576-4511-9ec2-985333812d25.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5d0f2fd-2576-4511-9ec2-985333812d25.png)'
- en: 'Before performing PCA, it is advisable to perform scaling of input data to
    eliminate any issues due to different dimensions of the data. For example, while
    applying PCA on customer data, their salary has larger dimensions than the customer''s
    age. Hence, if we do not put all the variables in a similar dimension, one variable
    will explain the entire variation rather than its actual impact. In the following
    code, we have applied scaling on all the columns separately:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行PCA之前，建议对输入数据进行缩放，以消除因数据维度不同而可能产生的问题。例如，在对客户数据应用PCA时，客户的薪水维度要大于客户的年龄维度。因此，如果我们没有将所有变量缩放到相同的维度，一个变量会解释整个变化，而不是其实际的影响。在下面的代码中，我们已对所有列分别进行了缩放：
- en: '[PRE11]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the following, we have used two principal components, so that we can represent
    the performance on a 2D graph. In later sections, we have applied 3D as well.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们使用了两个主成分，以便将性能表示在2D图上。在后续部分，我们还应用了3D。
- en: '[PRE12]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the following section of code, we are appending the relevant principal components
    to each digit separately so that we can create a scatter plot of all 10 digits:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码部分中，我们将相关的主成分分别附加到每个数字上，以便我们可以创建所有10个数字的散点图：
- en: '[PRE13]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/e1250b44-8893-4b58-bfef-9856a5144b97.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1250b44-8893-4b58-bfef-9856a5144b97.png)'
- en: Though the preceding plot seems a bit cluttered, it does provide some idea of
    how the digits are close to and distant from each other. We get the idea that
    digits *6* and *8* are very similar and digits *4* and *7* are very distant from
    the center group, and so on. However, we should also try with a higher number
    of PCAs, as, sometimes, we might not be able to represent every variation in two
    dimensions itself.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的图看起来有些杂乱，但它确实提供了一些关于数字彼此之间的远近的想法。我们可以得出结论，数字*6*和*8*非常相似，而数字*4*和*7*则远离中心组，等等。然而，我们也应该尝试使用更多的主成分，因为有时我们可能无法在二维中完全表示每个变化。
- en: In the following code, we have applied three PCAs so that we can get a better
    view of the data in a 3D space. The procedure is very much similar as with two
    PCAs, except for creating one extra dimension for each digit (*X*, *Y*, and *Z*).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们应用了三个主成分，以便可以在3D空间中更好地查看数据。这个过程与两个主成分非常相似，除了为每个数字创建一个额外的维度（*X*、*Y*
    和 *Z*）。
- en: '[PRE14]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/558b7610-9ae7-417c-9a54-af1327c3dcd2.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/558b7610-9ae7-417c-9a54-af1327c3dcd2.png)'
- en: matplotlib plots have one great advantage over other software plots such as
    R plot, and so on. They are interactive, which means that we can rotate them and
    see how they look from various directions. We encourage the reader to observe
    the plot by rotating and exploring. In a 3D plot, we can see a similar story with
    more explanation. Digit *2* is at the extreme left and digit *0* is at the lower
    part of the plot. Whereas digit *4* is at the top-right end, digit *6* seems to
    be more towards the *PC 1* axis. In this way, we can visualize and see how digits
    are distributed. In the case of 4 PCAs, we need to go for subplots and visualize
    them separately.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 与R图等其他软件绘图相比，matplotlib图形有一个巨大的优势。它们是交互式的，这意味着我们可以旋转它们并从不同的角度观察它们的外观。我们鼓励读者通过旋转和探索来观察图形。在3D图中，我们可以看到类似的情况，并得到更多的解释。数字*2*位于最左边，数字*0*位于图形的下方。而数字*4*位于右上角，数字*6*似乎更靠近*PC
    1*轴。通过这种方式，我们可以可视化并观察数字如何分布。在使用4个主成分时，我们需要使用子图并分别进行可视化。
- en: 'Choosing the number of PCAs to be extracted is an open-ended question in unsupervised
    learning, but there are some turnarounds to get an approximated view. There are
    two ways we can determine the number of clusters:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 选择提取多少主成分是无监督学习中的一个开放性问题，但有一些方法可以得到一个大致的估计。我们可以通过两种方式来确定聚类的数量：
- en: Check where the total variance explained is diminishing marginally
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查总方差解释是否在边际上开始减少
- en: Total variance explained greater than 80 percent
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总方差解释大于80%
- en: The following code does provide the total variance explained with the change
    in number of principal components. With the more number of PCs, more variance
    will be explained. But however, the challenge is to restrict it as fewer PCs possible,
    this will be achieved by restricting where the marginal increase in variance explained
    start diminishes.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码提供了随着主成分数目变化，所解释的总方差。随着主成分数目的增加，能够解释的方差也会增多。但挑战在于尽可能限制主成分的数量，这可以通过限制方差解释的边际增量开始下降来实现。
- en: '[PRE16]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/3cb54060-0a40-42a4-8245-5a788f0c293b.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3cb54060-0a40-42a4-8245-5a788f0c293b.png)'
- en: From the previous plot, we can see that total variance explained diminishes
    marginally at 10 PCAs; whereas, total variance explained greater than 80 percent
    is given at 21 PCAs. It is up to the business and user which value to choose.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的图中可以看到，总方差解释度在 10 个主成分（PCA）时略微减少；而 21 个主成分时，总方差解释度大于 80%。选择哪个值取决于业务和用户的需求。
- en: 'The R code for PCA applied on handwritten digits data is as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于手写数字数据的 PCA 的 R 代码如下：
- en: '[PRE18]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Singular value decomposition - SVD
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奇异值分解 - SVD
- en: 'Many implementations of PCA use singular value decomposition to calculate eigenvectors
    and eigenvalues. SVD is given by the following equation:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 PCA 实现使用奇异值分解来计算特征向量和特征值。SVD 通过以下方程给出：
- en: '![](img/2bad9800-489d-425d-abdf-8e81028dabbd.jpg)![](img/34c6d790-5379-452b-a386-e8fd4a2b1359.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2bad9800-489d-425d-abdf-8e81028dabbd.jpg)![](img/34c6d790-5379-452b-a386-e8fd4a2b1359.jpg)'
- en: Columns of *U* are called left singular vectors of the data matrix, the columns
    of *V* are its right singular vectors, and the diagonal entries of ![](img/d869b053-93bb-4e03-9a0d-52edbde6c63e.png)
    are its singular values. Left singular vectors are the eigenvectors of the covariance
    matrix and the diagonal element of ![](img/c808febf-3aac-4e39-bf3d-de049bfe7bb3.png)
    are the square roots of the eigenvalues of the covariance matrix.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '*U* 的列称为数据矩阵的左奇异向量，*V* 的列是其右奇异向量，![](img/d869b053-93bb-4e03-9a0d-52edbde6c63e.png)
    的对角线条目是其奇异值。左奇异向量是协方差矩阵的特征向量，![](img/c808febf-3aac-4e39-bf3d-de049bfe7bb3.png)
    的对角元素是协方差矩阵特征值的平方根。'
- en: 'Before proceeding with SVD, it would be advisable to understand a few advantages
    and important points about SVD:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行 SVD 之前，了解一些 SVD 的优点和重要点是明智的：
- en: SVD can be applied even on rectangular matrices; whereas, eigenvalues are defined
    only for square matrices. The equivalent of eigenvalues obtained through the SVD
    method are called singular values, and vectors obtained equivalent to eigenvectors
    are known as singular vectors. However, as they are rectangular in nature, we
    need to have left singular vectors and right singular vectors respectively for
    their dimensions.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVD 可以应用于矩形矩阵；而特征值仅定义于方阵。通过 SVD 方法获得的特征值等价物称为奇异值，得到的与特征向量等价的向量称为奇异向量。然而，由于它们本质上是矩形的，我们需要分别为它们的维度计算左奇异向量和右奇异向量。
- en: If a matrix *A* has a matrix of eigenvectors *P* that is not invertible, then
    *A* does not have an eigen decomposition. However, if *A* is *m* x *n* real matrix
    with *m* > *n*, then A can be written using a singular value decomposition.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果矩阵 *A* 的特征向量矩阵 *P* 不可逆，那么 *A* 没有特征分解。然而，如果 *A* 是一个 *m* x *n* 的实矩阵且 *m* > *n*，则
    A 可以通过奇异值分解表示。
- en: Both *U* and *V* are orthogonal matrices, which means *U^T U = I* (*I* with
    *m* x *m* dimension) or *V^T V = I* (here *I* with *n* x *n* dimension), where
    two identity matrices may have different dimensions.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*U* 和 *V* 都是正交矩阵，这意味着 *U^T U = I*（其中 *I* 是 *m* x *m* 的单位矩阵）或 *V^T V = I*（这里
    *I* 是 *n* x *n* 的单位矩阵），两个单位矩阵的维度可能不同。'
- en: '![](img/beca8756-97e1-46e6-b726-4bb3bcc97c1a.png)is a non-negative diagonal
    matrix with *m* x *n* dimensions.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/beca8756-97e1-46e6-b726-4bb3bcc97c1a.png) 是一个具有 *m* x *n* 维度的非负对角矩阵。'
- en: 'Then computation of singular values and singular vectors is done with the following
    set of equations:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，奇异值和奇异向量的计算通过以下一组方程式完成：
- en: '![](img/d7d80ce1-8853-409b-b9fa-b5743561ba83.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7d80ce1-8853-409b-b9fa-b5743561ba83.jpg)'
- en: 'In the first stage, singular values/eigenvalues are calculated with the equation.
    Once we obtain the singular/eigenvalues, we will substitute to determine the *V*
    or right singular/eigen vectors:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，使用方程计算奇异值/特征值。一旦获得奇异值/特征值，我们将代入计算 *V* 或右奇异/特征向量：
- en: '![](img/bbdd9880-81f0-4372-a22a-f2bd4930c904.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbdd9880-81f0-4372-a22a-f2bd4930c904.jpg)'
- en: 'Once we obtain the right singular vectors and diagonal values, we will substitute
    to obtain the left singular vectors *U* using the equation mentioned as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了右奇异向量和对角值，我们将代入公式以获得左奇异向量 *U*，公式如下：
- en: '![](img/585fb802-b3e1-4703-8e0b-ce0835889d61.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/585fb802-b3e1-4703-8e0b-ce0835889d61.jpg)'
- en: In this way, we will calculate the singular value decompositions of the original
    system of equations matrix.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们将计算原始方程矩阵的奇异值分解。
- en: SVD applied on handwritten digits using scikit-learn
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 对手写数字应用 SVD
- en: SVD can be applied on the same handwritten digits data to perform an apple-to-apple
    comparison of techniques.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 可以对相同的手写数字数据应用 SVD，以进行技术的“苹果对苹果”比较。
- en: '[PRE19]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the following code, 15 singular vectors with 300 iterations are used, but
    we encourage the reader to change the values and check the performance of SVD.
    We have used two types of SVD functions, as a function `randomized_svd` provide
    the decomposition of the original matrix and a `TruncatedSVD` can provide total
    variance explained ratio. In practice, uses may not need to view all the decompositions
    and they can just use the `TruncatedSVD` function for their practical purposes.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，使用了15个奇异向量和300次迭代，但我们鼓励读者修改这些值并检查SVD的性能。我们使用了两种类型的SVD函数，其中`randomized_svd`函数提供了原始矩阵的分解，而`TruncatedSVD`函数则可以提供总方差解释比例。在实际应用中，用户可能不需要查看所有的分解，只需使用`TruncatedSVD`函数来满足实际需求。
- en: '[PRE20]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/b74ae632-a243-4362-b0fd-a1f652afd617.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b74ae632-a243-4362-b0fd-a1f652afd617.png)'
- en: By looking at the previous screenshot, we can see that the original matrix of
    dimension (1797 x 64) has been decomposed into a left singular vector (1797 x
    15), singular value (diagonal matrix of 15), and right singular vector (15 x 64).
    We can obtain the original matrix by multiplying all three matrices in order.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的截图中，我们可以看到，原始矩阵（维度为1797 x 64）已经被分解为左奇异向量（1797 x 15）、奇异值（15维的对角矩阵）和右奇异向量（15
    x 64）。我们可以通过按顺序乘以这三种矩阵来恢复原始矩阵。
- en: '[PRE21]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/63a4fdf0-fb70-4097-b069-33fa26fdefa5.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63a4fdf0-fb70-4097-b069-33fa26fdefa5.png)'
- en: The total variance explained for 15 singular value features is 83.4 percent.
    But the reader needs to change the different values to decide the optimum value.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 对于15个奇异值特征，总方差解释为83.4%。但是读者需要更改不同的值，以决定最优值。
- en: 'The following code illustrates the change in total variance explained with
    respective to change in number of singular values:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码说明了奇异值个数变化时总方差解释的变化：
- en: '[PRE22]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/c77ef1df-2649-4377-8f21-a37a43d75474.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c77ef1df-2649-4377-8f21-a37a43d75474.png)'
- en: From the previous plot, we can choose either 8 or 15 singular vectors based
    on the requirement.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的图中，我们可以根据需求选择8个或15个奇异向量。
- en: 'The R code for SVD applied on handwritten digits data is as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于手写数字数据的SVD的R代码如下：
- en: '[PRE23]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Deep auto encoders
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度自编码器
- en: The auto encoder neural network is an unsupervised learning algorithm that applies
    backpropagation setting the target values to be equal to the inputs *y^((i)) =
    x^((i))*. Auto encoder tries to learn a function *h[w,b](x) ≈ x*, means it tries
    to learn an approximation to the identity function, so that output  ![](img/f7e07b21-0c88-4e4f-8778-a576c73e30c6.jpg) 
    that is similar to *x*.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码神经网络是一种无监督学习算法，它通过反向传播设置目标值，使其等于输入值 *y^((i)) = x^((i))*。自编码器试图学习一个函数 *h[w,b](x)
    ≈ x*，这意味着它试图学习一个近似的恒等函数，使得输出 ![](img/f7e07b21-0c88-4e4f-8778-a576c73e30c6.jpg)
    与 *x* 类似。
- en: '![](img/3a9a491a-e5af-49d7-b618-eb7f46b02642.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a9a491a-e5af-49d7-b618-eb7f46b02642.png)'
- en: Though trying to replicate the identity function seems trivial function to learn,
    by placing the constraints on the network, such as by limiting number of hidden
    units, we can discover interesting structures about the data. Let's say input
    picture of size 10 x 10 pixels has intensity values which have, altogether, 100
    input values, the number of neurons in the second layer (hidden layer) is 50 units,
    and the output layer, finally, has 100 units of neurons as we need to pass the
    image to map it to itself and while achieving this representation in the process
    we would force the network to learn a compressed representation of the input,
    which is hidden unit activations *a^((2)) ε  R^(100)*, with which we must try
    to reconstruct the 100 pixel input *x*. If the input data is completely random
    without any correlations, and so on. it would be very difficult to compress, whereas
    if the underlying data have some correlations or detectable structures, then this
    algorithm will be able to discover the correlations and represent them compactly.
    In fact, auto encoder often ends up learning a low-dimensional representation
    very similar to PCAs.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管试图复制恒等函数看似一个简单的任务，通过在网络上施加约束（例如限制隐藏单元的数量），我们可以发现数据中的有趣结构。假设输入的图像大小为10 x 10像素，具有强度值，总共有100个输入值，第二层（隐藏层）中的神经元数量为50个，最终输出层有100个神经元，因为我们需要将图像传递并映射到自身，在此过程中，我们将迫使网络学习输入的压缩表示，即隐藏单元激活
    *a^((2))  ε  R^(100)*，通过这个表示，我们必须尝试重建这100个像素的输入 *x*。如果输入数据完全随机，且没有任何相关性等，那么压缩将非常困难；而如果潜在的数据具有某些相关性或可检测的结构，那么这个算法将能够发现这些相关性并紧凑地表示它们。实际上，自编码器通常最终会学习到与PCA非常相似的低维表示。
- en: Model building technique using encoder-decoder architecture
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用编码器-解码器架构的模型构建技术
- en: Training the auto encoder model is a bit tricky, hence a detailed illustration
    has been provided for better understanding. During the training phase, the whole
    encoder-decoder section is trained against the same input as an output of decoder.
    In order to achieve the desired output, features will be compressed during the
    middle layer, as we are passing through the convergent and divergent layers. Once
    enough training has been done by reducing the error values over the number of
    iterations, we will use the trained encoder section to create the latent features
    for next stage of modeling, or for visualization, and so on.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 训练自编码器模型有点复杂，因此提供了详细的说明以便更好地理解。在训练阶段，整个编码器-解码器部分会针对相同的输入进行训练，作为解码器的输出。为了实现所需的输出，特征将在中间层进行压缩，因为我们通过了收敛和发散层。一旦通过多次迭代减少错误值完成足够的训练，就可以使用训练好的编码器部分来为下一阶段的建模创建潜在特征，或进行可视化等。
- en: In the following diagram, a sample has been shown. The input and output layers
    have five neurons, whereas the number of neurons has been gradually decreased
    in the middle sections. The compressed layer has only two neurons, which is the
    number of latent dimensions we would like to extract from the data.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，展示了一个样本。输入层和输出层有五个神经元，而中间部分的神经元数量逐渐减少。压缩层只有两个神经元，即我们希望从数据中提取的潜在维度的数量。
- en: '![](img/bfec2005-fe5a-4f24-a0f8-9899219bdb98.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bfec2005-fe5a-4f24-a0f8-9899219bdb98.png)'
- en: 'The following diagram depicts using the trained encoder section to create latent
    features from the new input data, which will be utilized for visualization or
    for utilizing in the next stage of the model:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了如何使用训练过的编码器部分，从新的输入数据中创建潜在特征，这些特征可以用于可视化或模型的下一个阶段：
- en: '![](img/410aa78f-a964-4567-82ee-c944940b6f2b.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/410aa78f-a964-4567-82ee-c944940b6f2b.png)'
- en: Deep auto encoders applied on handwritten digits using Keras
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras应用在手写数字上的深度自编码器
- en: 'Deep auto encoders are explained with same handwritten digits data to show
    the comparison of how this non-linear method differs to linear methods like PCA
    and SVD. Non-linear methods generally perform much better, but these methods are
    kind of black-box models and we cannot determine the explanation behind that.
    Keras software has been utilized to build the deep auto encoders here, as they
    work like Lego blocks, which makes it easy for users to play around with different
    architectures and parameters of the model for better understanding:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 深度自编码器使用相同的手写数字数据进行说明，目的是展示这种非线性方法与PCA和SVD等线性方法的区别。一般来说，非线性方法的表现要好得多，但这些方法类似黑盒模型，我们无法得知其背后的解释。本文中使用了Keras软件来构建深度自编码器，因为它们像乐高积木一样工作，用户可以通过不同的模型架构和参数组合进行实验，以便更好地理解：
- en: '[PRE24]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](img/78b1f846-5712-45a8-b1d1-1c4e16682e72.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78b1f846-5712-45a8-b1d1-1c4e16682e72.png)'
- en: 'Dense neuron modules from Keras used for constructing encoder-decoder architecture:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras的密集神经元模块构建编码器-解码器架构：
- en: '[PRE25]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/73cfd2fa-5076-4ff6-85cd-5c5ab85ef714.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73cfd2fa-5076-4ff6-85cd-5c5ab85ef714.png)'
- en: GPU of NVIDIA GTX 1060 has been used here, also `cuDNN` and `CNMeM` libraries
    are installed for further enhancement of speed up to 4x-5x on the top of regular
    GPU performance. These libraries utilize 20 percent of GPU memory, which left
    the 80 percent of memory for working on the data. The user needs to be careful,
    if they have low memory GPUs like 3 GB to 4 GB, they may not be able to utilize
    these libraries.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用了NVIDIA GTX 1060的GPU，并安装了`cuDNN`和`CNMeM`库，以进一步提升性能，使其比常规GPU性能提高4x-5x。这些库利用了20%的GPU内存，剩下的80%内存用于处理数据。用户需要注意，如果他们使用的是内存较小的GPU，如3GB到4GB的显卡，可能无法充分利用这些库。
- en: The reader needs to consider one important point that, syntax of Keras code,
    will remain same in both CPU and GPU mode.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 读者需要考虑一个重要的点，那就是，Keras代码的语法在CPU模式和GPU模式下是相同的。
- en: The following few lines of codes are the heart of the model. Input data have
    64 columns. We need to take those columns into the input of the layers, hence
    we have given the shape of 64\. Also, names have been assigned to each layer of
    the neural network, which we will explain the reason for in an upcoming section
    of the code. In the first hidden layer, 32 dense neurons are utilized, which means
    all the 64 inputs from the input layer are connected to 32 neurons in first hidden
    layer. The entire flow of dimensions are like *64, 32, 16, 2, 16, 32, 64*. We
    have compressed input to two neurons, in order to plot the components on a 2D
    plot, whereas, if we need to plot a 3D data (which we will be covering in the
    next section), we need to change the hidden three-layer number to three instead
    of two. After training is complete, we need to use encoder section and predict
    the output.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几行代码是模型的核心。输入数据有64列，我们需要将这些列作为层的输入，因此我们给出了64的形状。此外，每一层的神经网络都被赋予了名称，稍后我们会在代码的后续部分解释这样做的原因。在第一隐藏层中，使用了32个全连接神经元，这意味着输入层的所有64个输入都连接到第一隐藏层的32个神经元。整个维度流动是
    *64, 32, 16, 2, 16, 32, 64*。我们将输入压缩到两个神经元，以便在2D图上绘制分量；而如果需要绘制3D数据（我们将在下一部分讨论），则需要将隐藏层的层数从2改为3。训练完成后，我们需要使用编码器部分来预测输出。
- en: '[PRE26]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To train the model, we need to pass the starting and ending point of the architecture.
    In the following code, we have provided input as `input_layer` and output as `decoded`,
    which is the last layer (the name is `h6decode`):'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们需要传递架构的起点和终点。在以下代码中，我们提供了输入层为`input_layer`，输出为`decoded`，即最后一层（名称为`h6decode`）：
- en: '[PRE27]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Adam optimization has been used to optimize the mean square error, as we wanted
    to reproduce the original input at the end of the output layer of the network:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了Adam优化算法来优化均方误差，因为我们希望在网络的输出层末端重现原始输入：
- en: '[PRE28]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The network is trained with 100 epochs and a batch size of 256 observations
    per each batch. Validation split of 20 percent is used to check the accuracy on
    randomly selected validation data in order to ensure robustness, as if we just
    train only on the train data may create the overfitting problem, which is very
    common with highly non-linear models:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 网络使用了100个周期进行训练，每个批次的批量大小为256个观察值。使用了20%的验证集来检查随机选取的验证数据的准确性，以确保模型的鲁棒性，因为如果只在训练数据上训练，可能会导致过拟合问题，这在高度非线性模型中非常常见：
- en: '[PRE29]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](img/72ac3852-820e-495d-a7a8-08d87908150e.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72ac3852-820e-495d-a7a8-08d87908150e.png)'
- en: From the previous results, we can see that the model has been trained on 1,437
    train examples and validation on 360 examples. By looking into the loss value,
    both train and validation losses have decreased from 1.2314 to 0.9361 and 1.0451
    to 0.7326 respectively. Hence, we are moving in the right direction. However,
    readers are encouraged to try various architectures and number of iterations,
    batch sizes, and so on to see how much the accuracies can be further improved.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的结果可以看出，模型已经在1,437个训练样本上进行了训练，并在360个验证样本上进行了验证。从损失值来看，训练损失和验证损失分别从1.2314降至0.9361和从1.0451降至0.7326。因此，我们正在朝着正确的方向前进。然而，建议读者尝试不同的架构、迭代次数、批次大小等，看看准确度还能进一步提高多少。
- en: Once the encoder-decoder section has been trained, we need to take only the
    encoder section to compress the input features in order to obtain the compressed
    latent features, which is the core idea of dimensionality reduction altogether!
    In the following code, we have constructed another model with a trained input
    layer and a middle hidden layer (`h3latent_layer`). This is the reason behind
    assigning names for each layer of the network.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦编码器-解码器部分训练完成，我们只需要使用编码器部分来压缩输入特征，以获得压缩的潜在特征，这也是降维的核心思想！在以下代码中，我们构建了一个新的模型，包含了训练好的输入层和中间隐藏层（`h3latent_layer`）。这就是为每一层网络分配名称的原因。
- en: '[PRE30]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Just to check the dimensions of the reduced input variables and we can see
    that for all observations, we can see two dimensions or two column vector:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 仅检查降维后的输入变量的维度，可以看到，对于所有观察值，我们可以看到两个维度或两个列向量：
- en: '[PRE31]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](img/d340c571-4e8e-4dc5-a16e-c8fda6499fe3.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d340c571-4e8e-4dc5-a16e-c8fda6499fe3.png)'
- en: 'The following section of the code is very much similar to 2D PCA:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码部分非常类似于2D主成分分析（PCA）：
- en: '[PRE32]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](img/969f6b4b-0500-4d46-ae3f-4d363cd134b3.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/969f6b4b-0500-4d46-ae3f-4d363cd134b3.png)'
- en: From the previous plot we can see that data points are well separated, but the
    issue is the direction of view, as these features do not vary as per the dimensions
    perpendicular to each other, similar to principal components, which are orthogonal
    to each other. In the case of deep auto encoders, we need to change the view of
    direction from the *(0, 0)* to visualize this non-linear classification, which
    we will see in detail in the following 3D case.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的图中我们可以看到，数据点已经很好地分离，但问题在于视角方向，因为这些特征并没有像主成分那样沿彼此垂直的维度变化。在深度自编码器中，我们需要将视角方向从*(0,
    0)*改变，以便可视化这种非线性分类，接下来我们将在3D案例中详细展示这一点。
- en: The following is the code for 3D latent features. All the code remains the same
    apart from the `h3latent_layer`, in which we have to replace the value from `2`
    to `3`, as this is the end of encoder section and we will utilize it in creating
    the latent features and, eventually, it will be used for plotting purposes.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是3D潜在特征的代码。除了`h3latent_layer`中的值需要从`2`替换为`3`外，其余代码保持不变，因为这是编码器部分的结束，我们将在创建潜在特征时使用它，最终它将用于绘图。
- en: '[PRE33]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![](img/336bf646-4ee2-4721-8e45-ef4a33e53a74.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/336bf646-4ee2-4721-8e45-ef4a33e53a74.png)'
- en: 'From the previous results we can see that, with the inclusion of three dimensions
    instead of two, loss values obtained are less than in the 2D use case. Train and
    validation losses for two latent factors after 100 epochs are 0.9061 and 0.7326,
    and for three latent factors after 100 epochs, are 0.8032 and 0.6464\. This signifies
    that, with the inclusion of one more dimension, we can reduce the errors significantly.
    This way, the reader can change various parameters to determine the ideal architecture
    for dimensionality reduction:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的结果我们可以看到，加入三维度而不是二维度后，得到的损失值比2D用例中的值要低。对于两个潜在因子的训练和验证损失，在100轮迭代后的损失分别为0.9061和0.7326，而对于三个潜在因子，在100轮迭代后的损失为0.8032和0.6464。这表明，增加一个维度可以显著减少误差。这样，读者可以改变不同的参数，确定适合降维的理想架构：
- en: '[PRE34]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](img/d9c4ebd5-4049-4312-b0ad-120b367d964e.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9c4ebd5-4049-4312-b0ad-120b367d964e.png)'
- en: 3D plots from deep auto encoders do provide well separated classification compared
    with three PCAs. Here we have got better separation of the digits. One important
    point the reader should consider here is that the above plot is the rotated view
    from *(0, 0, 0)*, as data separation does not happen across orthogonal planes
    (like PCAs), hence we need to see the view from origin in order to see this non-linear
    classification.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 深度自编码器生成的3D图相比于三个PCA图提供了更好的分类分离。这里我们得到了更好的数字分离。读者需要注意的一点是，上述图是从*(0, 0, 0)*的旋转视角看到的，因为数据分离并不是在正交平面（如PCA）上发生的，因此我们需要从原点的视角来看才能看到这种非线性分类。
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have learned about various unsupervised learning methods
    to identify the structures and patterns within the data using k-mean clustering,
    PCA, SVD and deep auto encoders. Also, the k-means clustering algorithm explained
    with iris data. Methods were shown on how to choose the optimal k-value based
    on various performance metrics. Handwritten data from scikit-learn was been utilized
    to compare the differences between linear methods like PCA and SVD with non-linear
    techniques and deep auto encoders. The differences between PCA and SVD were given
    in detail so that the reader can understand SVD, which can be applied even on
    rectangular matrices where the number of users and number of products is not necessarily
    equal. In the end, through visualization, it has been proven that deep auto encoders
    are better at separating digits than linear unsupervised learning methods like
    PCA and SVD.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经学习了多种无监督学习方法，用于识别数据中的结构和模式，使用了k-means聚类、PCA、SVD和深度自编码器。同时，使用鸢尾花数据解释了k-means聚类算法。展示了如何根据各种性能指标选择最优的k值。利用来自scikit-learn的手写数据对比了线性方法（如PCA和SVD）与非线性技术和深度自编码器之间的差异。详细介绍了PCA和SVD之间的区别，以便读者理解SVD，它可以应用于矩形矩阵，即用户数量和产品数量不一定相等的情况。最后，通过可视化，证明了深度自编码器在分离数字方面优于PCA和SVD等线性无监督学习方法。
- en: In the next chapter, we will be discussing various reinforcement learning methods
    and their utilities in artificial intelligence and so on.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论各种强化学习方法及其在人工智能等领域的应用。
