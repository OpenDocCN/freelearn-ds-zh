- en: Unsupervised Clustering with Apache Spark 2.0
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark 2.0进行无监督聚类
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Building a KMeans classification system in Spark 2.0
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中构建KMeans分类系统
- en: Bisecting KMeans, the new kid on the block in Spark 2.0
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中的新成员Bisecting KMeans
- en: Using Gaussian Mixture and Expectation Maximization (EM) in Spark 2.0 to classify
    data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高斯混合和期望最大化（EM）在Spark 2.0中对数据进行分类
- en: Classifying the vertices of a graph using Power Iteration Clustering (PIC) in
    Spark 2.0
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark 2.0中的Power Iteration Clustering（PIC）对图的顶点进行分类
- en: Using Latent Dirichlet Allocation (LDA) to classify documents and text into
    topics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Latent Dirichlet Allocation（LDA）对文档和文本进行主题分类
- en: Streaming KMeans to classify data in near real time
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式KMeans用于近实时分类数据
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Unsupervised machine learning is a type of learning technique in which we try
    to draw inferences either directly or indirectly (through latent factors) from
    a set of unlabeled observations. In simple terms, we are trying to find the hidden
    knowledge or structures in a set of data without initially labeling the training
    data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习是一种学习技术，我们试图从一组未标记的观察中直接或间接（通过潜在因素）推断出推理。简而言之，我们试图在一组数据中找到隐藏的知识或结构，而不是最初标记训练数据。
- en: While most machine learning library implementation break down when applied to
    large datasets (iterative, multi-pass, a lot of intermediate writes), the Apache
    Spark Machine Library succeeds by providing machine library algorithms designed
    for parallelism and extremely large datasets using memory for intermediate writes
    out of the box.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数机器学习库实现在应用于大型数据集时会出现问题（迭代，多次传递，大量中间写入），但Apache Spark机器库通过提供专为并行处理和极大数据集设计的机器库算法而成功，使用内存进行中间写入。
- en: 'At the most abstract level, we can think of unsupervised learning as:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在最抽象的层面上，我们可以将无监督学习视为：
- en: '**Clustering systems**: Classify the inputs into categories either using hard
    (only belonging to a single cluster) or soft (probabilistic membership and overlaps)
    categorization.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类系统**：将输入分类为硬分类（仅属于单个簇）或软分类（概率成员和重叠）。'
- en: '**Dimensionality reduction systems**: Find hidden factors using a condensed
    representation of the original data.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维系统**：使用原始数据的简化表示找到隐藏因素。'
- en: 'The following figure shows the landscape of machine learning techniques. In
    the previous chapters, we focused on supervised machine learning techniques. In
    this chapter, we concentrate on unsupervised machine learning techniques ranging
    from clustering to latent factor models using Spark''s ML/MLIB library API:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了机器学习技术的景观。在之前的章节中，我们专注于监督机器学习技术。在本章中，我们专注于无监督机器学习技术，从聚类到使用Spark的ML/MLIB库API的潜在因素模型：
- en: '![](img/00156.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00156.jpeg)'
- en: The clusters are often modeled using intra-cluster similarity measurement, such
    as Euclidian or probabilistic techniques. Spark provides a complete and high-performing
    set of algorithms which lend themselves to parallel implementation at scale. They
    not only provide APIs, but also provide full source code which is very helpful
    for understanding bottlenecks and resolving them (forking to GPU) to fit your
    needs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些簇通常使用簇内相似度测量来建模，例如欧几里得或概率技术。Spark提供了一套完整且高性能的算法，适合于规模化的并行实现。它们不仅提供API，还提供完整的源代码，非常有助于理解瓶颈并解决它们（分叉到GPU）以满足您的需求。
- en: 'The applications of machine learning are vast and as limitless as you can imagine.
    Some of the most widely known examples and use cases are:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的应用是广泛的，可以想象的无限。一些最广为人知的例子和用例包括：
- en: Fraud detection (finance, law enforcement)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欺诈检测（金融，执法）
- en: Network security (intrusion detection, traffic analysis)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络安全（入侵检测，流量分析）
- en: Pattern recognition (marketing, intelligence community, banking)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式识别（营销，情报界，银行）
- en: Recommendation systems (retail, entertainment)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统（零售，娱乐）
- en: Affinity marketing (e-commerce, recommenders, deep personalization)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亲和营销（电子商务，推荐系统，深度个性化）
- en: Medical informatics (disease detection, patient care, asset management)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医学信息学（疾病检测，患者护理，资产管理）
- en: Image processing (object/sub-object detection, radiology)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像处理（对象/子对象检测，放射学）
- en: 'A word of caution on ML versus MLIB usage and future direction in Spark:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Spark中ML与MLIB的使用和未来方向的警告：
- en: While the MLIB is and will remain viable for the time being, there is a gradual
    movement towards Spark's ML library for future development rather than MLIB in
    Spark. The `org.apache.spark.ml.clustering` is a high-level machine learning package
    and the API is more focused on the DataFrame. The `org.apache.spark.mllib.clustering`
    is a lower-level machine learning package and the API is directly on RDD. While
    both packages will get the benefit of Spark's high performance and scalability,
    the main difference is the DataFrame. The `org.apache.spark.ml` will be the preferred
    method going forward.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然MLIB目前仍然可行，但在未来的发展中，人们逐渐转向Spark的ML库而不是Spark中的MLIB。`org.apache.spark.ml.clustering`是一个高级机器学习包，API更专注于DataFrame。`org.apache.spark.mllib.clustering`是一个较低级别的机器学习包，API直接在RDD上。虽然两个包都将受益于Spark的高性能和可伸缩性，但主要区别在于DataFrame。`org.apache.spark.ml`将是未来的首选方法。
- en: For example, we encourage the developer to look at why the introduction of KMeans
    classifying system exists in both ML and MLLIB: `org.apache.spark.ml.clustering`
    and `org.apache.spark.mllib.clustering`
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们鼓励开发人员查看为什么KMeans分类系统存在于ML和MLLIB中：`org.apache.spark.ml.clustering`和`org.apache.spark.mllib.clustering`
- en: Building a KMeans classifying system in Spark 2.0
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0中构建KMeans分类系统
- en: In this recipe, we will load a set of features (for example, x, y, z coordinates)
    using a LIBSVM file and then proceed to use `KMeans()` to instantiate an object.
    We will then set the number of desired clusters to three and then use `kmeans.fit()`
    to action the algorithm. Finally, we will print the centers for the three clusters
    that we found.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用LIBSVM文件加载一组特征（例如x、y、z坐标），然后使用`KMeans()`来实例化一个对象。然后我们将把所需的簇数设置为三，然后使用`kmeans.fit()`来执行算法。最后，我们将打印出我们找到的三个簇的中心。
- en: It is really important to note that Spark *does not* implement KMeans++, contrary
    to popular literature, instead it implements KMeans || (pronounced as KMeans Parallel).
    See the following recipe and the sections following the code for a complete explanation
    of the algorithm as it is implemented in Spark.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 非常重要的一点是，Spark *不*实现KMeans++，与流行的文献相反，它实现的是KMeans ||（读作KMeans Parallel）。请参阅以下示例和代码后面的部分，以了解Spark中实现的算法的完整解释。
- en: How to do it...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在位置的包位置：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark上下文所需的必要包，以便访问集群和`Log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create Spark''s Session object:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的Session对象：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We create a training dataset from a file in the `libsvm` format and display
    the file on the console:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从`libsvm`格式的文件中创建一个训练数据集，并在控制台上显示文件：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'From the console, you will see:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台，您将看到：
- en: '![](img/00157.gif)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00157.gif)'
- en: 'The following formula visualizes the data via contour maps that depict each
    feature vector (each row) versus the three unique features in both a 3D and flat
    contour map:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下公式通过等高线图可视化数据，描述了每个特征向量（每行）与三个唯一特征的3D和平面等高线图：
- en: '![](img/00158.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00158.jpeg)'
- en: We then create a KMeans object and set some key parameters to the KMeans model
    and set parameters.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们创建一个KMeans对象，并设置一些关键参数到KMeans模型和设置参数。
- en: In this case, we set the `K` value to `3` and set the *feature* column as column
    "features", which was defined in the previous step. This step is subjective and
    the optimal value would vary based on specific datasets. We recommend that you
    experiment with values from 2 to 50 and examine the cluster centers for a final
    value.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将`K`值设置为`3`，并将*feature*列设置为“features”列，该列在上一步中定义。这一步是主观的，最佳值会根据特定数据集而变化。我们建议您尝试值从2到50，并检查最终值的聚类中心。
- en: 'We also set the maximum iteration count to `10`. Most of the values have a
    default setting as the comments as shown in the following code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将最大迭代次数设置为`10`。大多数值都有默认设置，如下面的代码中所示的注释。
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We then train the dataset. The `fit()` function will then run the algorithm
    and perform the calculations. It is based on the dataset created in the previous
    steps. These steps are common among Spark''s ML and do not usually vary from algorithm
    to algorithm:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后训练数据集。`fit()`函数将运行算法并执行计算。它是基于前面步骤中创建的数据集。这些步骤在Spark的ML中是常见的，通常不会因算法而异：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We also display the model''s prediction on the console:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在控制台上显示模型的预测：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'From the console:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台：
- en: '![](img/00159.gif)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00159.gif)'
- en: We then calculate the cost, using the included `computeCost(x)` function.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们使用包括`computeCost(x)`函数来计算成本。
- en: 'The KMeans Cost is calculated **Within Set Sum of Squared Errors** (**WSSSE**).
    The value will be printed out in the program''s console:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KMeans成本是通过**WSSSE**（Within Set Sum of Squared Errors）计算的。该值将在程序的控制台中打印出来：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The console output will show the following information:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台输出将显示以下信息：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then print out the cluster''s center based on the calculation of the model:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后根据模型的计算打印出簇的中心：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The console output will show the following information:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制台输出将显示以下信息：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Based on the setting of the KMeans clustering, we set the `K` value to `3`;
    the model will calculate three centers based on the training dataset that we fit
    in.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 根据KMeans聚类的设置，我们将`K`值设置为`3`；该模型将根据我们拟合的训练数据集计算出三个中心。
- en: 'We then close the program by stopping the Spark context:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过停止Spark上下文来关闭程序：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How it works...
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We read a LIBSVM file with a set of coordinates (can be interpreted as a tuple
    of three numbers) and then created a `KMean()` object, but changed the default
    number of clusters from 2 (out of the box) to 3 for demonstration purposes. We
    used the `.fit()` to create the model and then used `model.summary.predictions.show()`
    to display which tuple belongs to which cluster. In the last step, we printed
    the cost and the center of the three clusters. Conceptually, it can be thought
    of as having a set of 3D coordinates as data and then assigning each individual
    coordinate to one of the three clusters using KMeans algorithms.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取了一个带有一组坐标的LIBSVM文件（可以被解释为三个数字的元组），然后创建了一个`KMean()`对象，但是出于演示目的，将默认的簇数从2（默认值）更改为3。我们使用`.fit()`来创建模型，然后使用`model.summary.predictions.show()`来显示哪个元组属于哪个簇。在最后一步，我们打印出了三个簇的成本和中心。从概念上讲，可以将其视为具有一组3D坐标作为数据，然后使用KMeans算法将每个单独的坐标分配给三个簇之一。
- en: KMeans is a form of unsupervised machine learning algorithm, with its root in
    signal processing (vector quantization) and compression (grouping similar vectors
    of items together to achieve a higher compression rate). Generally speaking, the
    KMeans algorithm attempts to group a series of observations {X[1,] X[2], ....
    , X[n]} into a series of clusters {C[1,] C[2 .....] C[n]} using a form of distance
    measure (local optimization) that is optimized in an iterative manner.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: KMeans是一种无监督机器学习算法，其根源在于信号处理（矢量量化）和压缩（将相似向量的项目分组在一起以实现更高的压缩率）。一般来说，KMeans算法试图使用一种距离度量（局部优化）将一系列观察{X[1,]
    X[2], .... , X[n]}分成一系列群集{C[1,] C[2 .....] C[n]}，并以迭代方式进行优化。
- en: There are three main types of KMeans algorithm that are in use. In a simple
    survey, we found 12 specialized variations of the KMeans algorithm. It is important
    to note that Spark implements a version called KMeans || (KMeans Parallel) and
    *not* KMeans++ or standard KMeans as referenced in some literature or videos.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有三种主要类型的KMeans算法正在使用。在一项简单的调查中，我们发现了12种专门的KMeans算法变体。重要的是要注意，Spark实现了一种称为KMeans
    ||（KMeans Parallel）的版本，而不是一些文献或视频中提到的KMeans++或标准KMeans。
- en: 'The following figure depicts KMeans in a nutshell:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 下图简要描述了KMeans：
- en: '![](img/00160.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00160.jpeg)'
- en: 'Source: Spark documentation'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：Spark文档
- en: KMeans (Lloyd Algorithm)
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KMeans（Lloyd算法）
- en: 'The steps for basic KMeans implementation (Lloyd algorithm) are:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 基本KMeans实现（Lloyd算法）的步骤是：
- en: Randomly select K datacenters from observations as the initial centroids.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从观察中随机选择K个数据中心作为初始质心。
- en: 'Keep iterating till the convergence criteria is met:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持迭代直到满足收敛标准：
- en: Measure the distance from a point to each centroid
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量从点到每个质心的距离
- en: Include each data point in a cluster which is the closest centroid
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个数据点包括在最接近的质心的群集中
- en: Calculate new cluster centroids based on a distance formula (proxy for dissimilarity)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据距离公式（代表不相似性的代理）计算新的群集质心
- en: Update the algorithm with new center points
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用新的中心点更新算法
- en: 'The three generations are depicted in the following figure:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 三代人的情况如下图所示：
- en: '![](img/00161.gif)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00161.gif)'
- en: KMeans++ (Arthur's algorithm)
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KMeans++（Arthur的算法）
- en: The next improvement over standard KMeans is the KMeans++ proposed by David
    Arthur and Sergei Vassilvitskii in 2007\. Arthur's algorithm improves the initial
    Lloyd's KMeans by being more selective during the seeding process (the initial
    step).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对标准KMeans的下一个改进是由David Arthur和Sergei Vassilvitskii于2007年提出的KMeans++。 Arthur的算法通过在种植过程（初始步骤）中更加选择性来改进最初的Lloyd's
    KMeans。
- en: KMeans++, rather than picking random centres (random centroids) as starting
    points, picks the first centroid randomly and then picks the data points one by
    one and calculates `D(x)`. Then it chooses one more data point at random and,
    using proportional probability distribution `D(x)2`, it then keeps repeating the
    last two steps until all *K* numbers are picked. After the initial seeding, we
    finally run the KMeans or a variation with the newly seeded centroid. The KMeans++
    algorithm is guaranteed to find a solution in an *Omega= O(log k)* complexity.
    Even though the initial seeding takes extra steps, the accuracy improvements are
    substantial.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: KMeans ++不是随机选择中心（随机质心）作为起始点，而是随机选择第一个质心，然后逐个选择数据点并计算`D(x)`。然后它随机选择另一个数据点，并使用比例概率分布`D(x)2`，然后重复最后两个步骤，直到选择所有*K*个数字。在初始种植之后，我们最终运行KMeans或使用新种植的质心的变体。
    KMeans++算法保证在*Omega= O(log k)*复杂度中找到解决方案。尽管初始种植需要额外的步骤，但准确性的提高是实质性的。
- en: KMeans|| (pronounced as KMeans Parallel)
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KMeans||（发音为KMeans Parallel）
- en: KMeans || is optimized to run in parallel and can result in one-two orders of
    magnitude improvement over Lloyd's original algorithm. The limitation of KMeans++
    is that it requires K-passes over the dataset, which can severely limit the performance
    and practicality of running KMeans with large or extreme datasets. Spark's KMeans||
    parallel implementation runs faster because it takes fewer passes (a lot less)
    over the data by sampling m points and oversampling in the process.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: KMeans ||经过优化以并行运行，并且可以比Lloyd的原始算法快一到两个数量级。 KMeans++的局限性在于它需要对数据集进行K次遍历，这可能严重限制使用大型或极端数据集运行KMeans的性能和实用性。
    Spark的KMeans||并行实现运行更快，因为它通过对m个点进行采样并在过程中进行过采样，从而对数据进行更少的遍历。
- en: 'The core of the algorithm and the math is depicted in the following figure:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的核心和数学内容如下图所示：
- en: '![](img/00162.jpeg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00162.jpeg)'
- en: In a nutshell, the highlight of the KMeans || (Parallel KMeans) is the course-grain
    sampling which repeats in *log(n)* rounds and at the end we are left with *k *
    log(n)* remaining points that are a C (constant) distance away from the optimal
    solution! This implementation is also less sensitive to outlier data points that
    can skew the clustering results in KMeans and KMeans++.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，KMeans ||（Parallel KMeans）的亮点是粗粒度采样，它在*log(n)*轮次中重复，并且最终我们剩下*k * log(n)*个距离最优解有C（常数）距离的点！这种实现对可能会扭曲KMeans和KMeans++中的聚类结果的异常数据点也不太敏感。
- en: For a deeper understanding of the algorithm, the reader can access the paper
    by Bahman Bahmani at [http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地理解该算法，读者可以访问Bahman Bahmani在[http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf)上的论文。
- en: There's more...
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: There is also a streaming version of KMeans implementation in Spark that allows
    you to classify the features on the fly. The streaming version of KMeans is covered
    in more detail in [Chapter 13](part0538.html#G12EK0-4d291c9fed174a6992fd24938c2f9c77), *Spark
    Streaming and Machine Learning Library*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Spark还有一个KMeans实现的流式版本，允许您即时对特征进行分类。 KMeans的流式版本在《第13章》*Spark Streaming和Machine
    Learning Library*中有更详细的介绍。
- en: 'There is also a class that helps you to generate RDD data for KMeans. We found
    this to be very useful during our application development process:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个类可以帮助您为KMeans生成RDD数据。我们发现这在应用程序开发过程中非常有用：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This call uses Spark context to create RDDs while allowing you to specify the
    number of points, clusters, dimensions, and partitions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个调用使用Spark上下文来创建RDD，同时允许您指定点数、簇、维度和分区。
- en: 'A useful related API is: `generateKMeansRDD()`. Documentation for `generateKMeansRDD`
    can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator%24) for
    generate an RDD containing test data for KMeans.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有用的相关API是：`generateKMeansRDD()`。`generateKMeansRDD`的文档可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator%24)找到，用于生成包含KMeans测试数据的RDD。
- en: See also
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'We need two pieces of objects to be able to write, measure, and manipulate
    the parameters of the KMeans || algorithm in Spark. The details of these two pieces
    of objects can be found at the following websites:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要两个对象来能够编写、测量和操作Spark中KMeans ||算法的参数。这两个对象的详细信息可以在以下网站找到：
- en: '`KMeans()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KMeans()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans)'
- en: '`KMeansModel()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KMeansModel()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel)'
- en: Bisecting KMeans, the new kid on the block in Spark 2.0
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bisecting KMeans，Spark 2.0中的新成员
- en: In this recipe, we will download the glass dataset and try to identify and label
    each glass using a bisecting KMeans algorithm. The Bisecting KMeans is a hierarchical
    version of the K-Mean algorithm implemented in Spark using the `BisectingKMeans()`
    API. While this algorithm is conceptually like KMeans, it can offer considerable
    speed for some use cases where the hierarchical path is present.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将下载玻璃数据集，并尝试使用Bisecting KMeans算法识别和标记每个玻璃。Bisecting KMeans是Spark中使用`BisectingKMeans()`API实现的K-Mean算法的分层版本。虽然这个算法在概念上类似于KMeans，但在某些具有分层路径的用例中，它可以提供相当快的速度。
- en: The dataset we used for this recipe is the Glass Identification Database. The
    study of the classification of types of glass was motivated by criminological
    research. Glass could be considered as evidence if it is correctly identified.
    The data can be found at NTU (Taiwan), already in LIBSVM format.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于这个配方的数据集是玻璃识别数据库。对玻璃类型的分类研究是由犯罪学研究激发的。如果正确识别，玻璃可以被视为证据。数据可以在NTU（台湾）找到，已经以LIBSVM格式存在。
- en: How to do it...
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: We downloaded the prepared data file in LIBSVM from: [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale)
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从以下网址下载了LIBSVM格式的准备好的数据文件：[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale)
- en: The dataset contains 11 features and 214 rows.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含11个特征和214行。
- en: The original dataset and data dictionary is also available at the UCI website: [http://archive.ics.uci.edu/ml/datasets/Glass+Identification](http://archive.ics.uci.edu/ml/datasets/Glass+Identification)
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始数据集和数据字典也可以在UCI网站上找到：[http://archive.ics.uci.edu/ml/datasets/Glass+Identification](http://archive.ics.uci.edu/ml/datasets/Glass+Identification)
- en: 'ID number: 1 to 214'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ID编号：1到214
- en: 'RI: Refractive index'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RI：折射率
- en: 'Na: Sodium (unit measurement: weight percent in corresponding oxide, as are
    attributes 4-10)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Na：钠（单位测量：相应氧化物中的重量百分比，属性4-10也是如此）
- en: 'Mg: Magnesium'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 镁：镁
- en: 'Al: Aluminum'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 铝：铝
- en: 'Si: Silicon'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硅：硅
- en: 'K: Potassium'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钾：钾
- en: 'Ca: Calcium'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钙：钙
- en: 'Ba: Barium'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ba：钡
- en: 'Fe: Iron'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 铁：铁
- en: 'Type of glass: Will find our class attributes or clusters using `BisectingKMeans()`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 玻璃类型：将使用`BisectingKMeans()`找到我们的类属性或簇：
- en: '`building_windows_float_processed`'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`building_windows_float_processed`'
- en: '`building_windows_non-_float_processed`'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`building_windows_non-_float_processed`'
- en: '`vehicle_windows_float_processed`'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vehicle_windows_float_processed`'
- en: '`vehicle_windows_non-_float_processed` (none in this database)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vehicle_windows_non-_float_processed`（此数据库中没有）'
- en: '`Containers`'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Containers`'
- en: '`Tableware`'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tableware`'
- en: '`Headlamps`'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Headlamps`'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Import the necessary packages:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create Spark''s Session object:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的Session对象：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We create a dataset from a file in the libsvm format and display the dataset
    on the console:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从libsvm格式的文件创建数据集，并在控制台上显示数据集：
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'From the console, you will see:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台，您将看到：
- en: '![](img/00163.jpeg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00163.jpeg)'
- en: 'We then split the dataset randomly into two parts in the ratio of 80% and 20%:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将数据集随机分成80%和20%的两部分：
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'From the console output (total count is 214):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出（总数为214）：
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We then create a `BisectingKMeans` object and set some key parameters to the
    model.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个`BisectingKMeans`对象，并为模型设置一些关键参数。
- en: In this case, we set the `K` value to `6` and set the `Feature` column as column
    "features", which was defined in the previous step. This step is subjective and
    the optimal value will vary based on specific datasets. We recommend you experiment
    with values from 2 to 50 and examine the cluster centers for a final value.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将`K`值设置为`6`，并将`Feature`列设置为"features"列，这在前面的步骤中已定义。这一步是主观的，最佳值将根据特定数据集而变化。我们建议您尝试值从2到50，并检查最终值的聚类中心。
- en: 'We also set the maximum iteration count to `65`. Most of the values have a
    default setting, as shown in the following code:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将最大迭代次数设置为`65`。大多数值都有默认设置，如下面的代码所示：
- en: '[PRE21]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We then train the dataset. The `fit()` function will then run the algorithm
    and do the calculations. It is based on the dataset created in the previous steps.
    We also print out the model parameters:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们训练数据集。`fit()`函数将运行算法并进行计算。它基于前面步骤中创建的数据集。我们还打印出模型参数：
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'From the console output:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We then calculate the cost, using the included computeCost(x) function:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们使用包括computeCost(x)函数来计算成本：
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The console output will show the following information:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台输出将显示以下信息：
- en: '[PRE25]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we print out the cluster''s center based on the calculation of the model:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们根据模型的计算打印出聚类中心：
- en: '[PRE26]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The console output will show the following information:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台输出将显示以下信息：
- en: '[PRE27]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](img/00164.gif)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00164.gif)'
- en: 'We then use the trained model to make a prediction on the testing dataset:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们使用训练好的模型对测试数据集进行预测：
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'From the console output:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '![](img/00165.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00165.jpeg)'
- en: 'We then close the program by stopping the Spark context:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过停止Spark上下文来关闭程序：
- en: '[PRE29]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How it works...
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this session, we explored the Bisecting KMeans model, which is new in Spark
    2.0\. We utilized the glass dataset in this session and tried to assign a glass
    type using `BisectingKMeans()`, but changed k to 6 so we have sufficient clusters.
    As usual, we loaded the data into a dataset with Spark's libsvm loading mechanism.
    We split the dataset randomly into 80% and 20%, with 80% used to train the model
    and 20% used for testing the model.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探索了Spark 2.0中新的Bisecting KMeans模型。在本节中，我们使用了玻璃数据集，并尝试使用`BisectingKMeans()`来分配玻璃类型，但将k更改为6，以便有足够的聚类。像往常一样，我们使用Spark的libsvm加载机制将数据加载到数据集中。我们将数据集随机分为80%和20%，其中80%用于训练模型，20%用于测试模型。
- en: We created the `BiSectingKmeans()` object and used the `fit(x)` function to
    create the model. We then used the `transform(x)` function for the testing dataset
    to explore the model prediction and printed out the result in the console output.
    We also output the cost of computing the clusters (sum of error squared) and then
    displayed the cluster centers. Finally, we printed the features with their assigned
    cluster number and stop operation.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了`BiSectingKmeans()`对象，并使用`fit(x)`函数创建模型。然后我们使用`transform(x)`函数对测试数据集进行探索模型预测，并在控制台输出结果。我们还输出了计算聚类的成本（误差平方和），然后显示了聚类中心。最后，我们打印出了特征及其分配的聚类编号，并停止操作。
- en: 'Approaches to hierarchical clustering include:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类的方法包括：
- en: '**Divisive**: Top down approach (Apache Spark implementation)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分裂式**：自上而下的方法（Apache Spark实现）'
- en: '**Agglomerative**: Bottom up approach'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合式**：自下而上的方法'
- en: There's more...
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'More about the Bisecting KMeans can be found at:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Bisecting KMeans的更多信息，请访问：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans)'
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel)'
- en: We use clustering to explore the data and get a feel for what the outcome looks
    like as clusters. The bisecting KMeans is an interesting case of hierarchical
    analysis versus KMeans clustering.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用聚类来探索数据，并了解聚类的结果。分裂式KMeans是分层分析与KMeans聚类的有趣案例。
- en: The best way to conceptualize it is to think of bisecting KMeans as a recursive
    hierarchical KMeans. The bisecting KMeans algorithm divides the data using similarity
    measurement techniques like KMeans, but uses a hierarchical scheme to increase
    accuracy. It is particularly prevalent in text mining where a hierarchical approach
    will minimize the intra-cluster dependencies of the corpus body among documents.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 理解分裂式KMeans的最佳方式是将其视为递归的分层KMeans。分裂式KMeans算法使用类似KMeans的相似性测量技术来划分数据，但使用分层方案来提高准确性。它在文本挖掘中特别普遍，其中分层方法将最小化语料库中文档之间的聚类内依赖性。
- en: The Bisecting KMeans algorithm starts by placing all observations in a single
    cluster first, but then breaks up the cluster into n partition (K=n) using the
    KMeans method. It then proceeds to select the most similar cluster (the highest
    inner cluster score) as the parent (the root cluster) while recursively splitting
    the other clusters untill the target number of clusters is derived in a hierarchical
    manner.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Bisecting KMeans算法首先将所有观察结果放入单个聚类中，然后使用KMeans方法将聚类分成n个分区（K=n）。然后，它继续选择最相似的聚类（最高内部聚类分数）作为父类（根聚类），同时递归地分割其他聚类，直到以分层方式得出目标聚类数。
- en: The Bisecting KMeans is a powerful tool used in text analytics to reduce the
    dimensionality of feature vectors for intelligent text/subject classification.
    By using this clustering technique, we end up grouping similar words/text/document/evidence
    into similar groups. Ultimately, if you start exploring text analytics, topic
    propagation, and scoring (for example, what article would go viral?), you are
    bound to encounter this technique in the early stages of your journey.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Bisecting KMeans是文本分析中用于智能文本/主题分类的强大工具，用于减少特征向量的维度。通过使用这种聚类技术，我们最终将相似的单词/文本/文档/证据分组到相似的组中。最终，如果您开始探索文本分析、主题传播和评分（例如，哪篇文章会成为病毒？），您一定会在旅程的早期阶段遇到这种技术。
- en: A white paper describing the use of Bisecting KMeans for text clustering is
    available at: [http://www.ijarcsse.com/docs/papers/Volume_5/2_February2015/V5I2-0229.pdf](http://www.ijarcsse.com/docs/papers/Volume_5/2_February2015/V5I2-0229.pdf)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一份白皮书描述了使用Bisecting KMeans进行文本聚类的方法，可以在以下链接找到：[http://www.ijarcsse.com/docs/papers/Volume_5/2_February2015/V5I2-0229.pdf](http://www.ijarcsse.com/docs/papers/Volume_5/2_February2015/V5I2-0229.pdf)
- en: See also
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'There are two approaches to implementing hierarchical clustering--Spark uses
    a recursive top-down approach in which a cluster is chosen and then splits are
    performed in the algorithm as it moves down the hierarchy:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种实现分层聚类的方法--Spark使用递归自顶向下的方法，在该方法中选择一个簇，然后在算法向下移动时执行拆分：
- en: Details about the hierarchical clustering approach can be found at [https://en.wikipedia.org/wiki/Hierarchical_clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关分层聚类方法的详细信息可以在以下链接找到：[https://en.wikipedia.org/wiki/Hierarchical_clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)
- en: Spark 2.0 documentation for Bisecting K-Mean can be found at [http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means](http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisecting K-Mean的Spark 2.0文档可以在以下链接找到：[http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means](http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means)
- en: A paper describing how to use Bisecting KMeans to classify web logs can be found
    at [http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf](http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一篇描述如何使用Bisecting KMeans对Web日志进行分类的论文可以在以下链接找到：[http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf](http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf)
- en: Using Gaussian Mixture and Expectation Maximization (EM) in Spark to classify
    data
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Gaussian Mixture和期望最大化（EM）在Spark中对数据进行分类
- en: In this recipe, we will explore Spark's implementation of **expectation maximization**
    (**EM**) `GaussianMixture()`*,* which calculates the maximum likelihood given
    a set of features as input. It assumes a Gaussian mixture in which each point
    can be sampled from K number of sub-distributions (cluster memberships).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将探讨Spark对**期望最大化**（**EM**）`GaussianMixture()`的实现，它根据一组特征计算最大似然。它假设一个高斯混合模型，其中每个点可以从K个子分布（簇成员资格）中抽样。
- en: How to do it...
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Import the necessary packages for vector and matrix manipulation:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于向量和矩阵操作的必要包：
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create Spark''s session object:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的会话对象：
- en: '[PRE32]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Let us take a look at the dataset and examine the input file. The Simulated
    SOCR Knee Pain Centroid Location Data represents the centroid location for the
    hypothetical knee-pain locations for 1,000 subjects. The data includes the X and
    Y coordinates of the centroids.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看数据集并检查输入文件。模拟的SOCR膝痛质心位置数据表示了1,000个主题的假设膝痛位置的质心位置。数据包括质心的X和Y坐标。
- en: This dataset can be used to illustrate the Gaussian Mixture and Expectation
    Maximization. The data is available at: [http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集可用于说明高斯混合和期望最大化。数据可在以下链接找到：[http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409)
- en: 'The sample data looks like the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 样本数据如下所示：
- en: '**X**: The *x* coordinate of the centroid location for one subject and one
    view.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X**：一个主题和一个视图的质心位置的*x*坐标。'
- en: '**Y**: The *y* coordinate of the centroid location for one subject and one
    view.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Y**：一个主题和一个视图的质心位置的*y*坐标。'
- en: X, Y
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: X，Y
- en: 11 73
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 11 73
- en: 20 88
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 20 88
- en: 19 73
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 19 73
- en: 15 65
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 15 65
- en: 21 57
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 21 57
- en: 26 101
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 26 101
- en: 24 117
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 24 117
- en: 35 106
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 35 106
- en: 37 96
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 37 96
- en: 35 147
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 35 147
- en: 41 151
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 41 151
- en: 42 137
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 42 137
- en: 43 127
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 43 127
- en: 41 206
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 41 206
- en: 47 213
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 47 213
- en: 49 238
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 49 238
- en: 40 229
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 40 229
- en: 'The following figure depicts a knee-pain map based on the SOCR dataset from
    `wiki.stat.ucla`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了基于`wiki.stat.ucla`的SOCR数据集的膝痛地图：
- en: '![](img/00166.jpeg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00166.jpeg)'
- en: We place the data file in a data directory (you can copy the data file to any
    location you prefer).
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据文件放在数据目录中（您可以将数据文件复制到任何您喜欢的位置）。
- en: 'The data file contains 8,666 entries:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 数据文件包含8,666个条目：
- en: '[PRE33]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We then load the data file into RDD:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将数据文件加载到RDD中：
- en: '[PRE34]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We now create a GaussianMixture model, and set the parameters for the model.
    We set the K value to 4, since the data was collected by four views: **Left Front**
    (**LF**), **Left Back** (**LB**), **Right Front** (**RF**), and **Right Back**
    (**RB**). We set the convergence to the default value of 0.01, and the maximum
    iteration count to 100:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们创建一个GaussianMixture模型，并设置模型的参数。我们将K值设置为4，因为数据是由四个视图收集的：**左前**（**LF**），**左后**（**LB**），**右前**（**RF**）和**右后**（**RB**）。我们将收敛值设置为默认值0.01，最大迭代次数设置为100：
- en: '[PRE35]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We run the model algorithm:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们运行模型算法：
- en: '[PRE36]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We print out the key values for the GaussianMixture model after the training:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在训练后打印出GaussianMixture模型的关键值：
- en: '[PRE37]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Since we set the K value to 4, we will have four sets of values printed out
    in the console logger:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们将K值设置为4，因此将有四组值在控制台记录器中打印出来：
- en: '![](img/00167.gif)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00167.gif)'
- en: 'We also print out the first 50 cluster-labels based on the GaussianMixture
    model predictions:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还根据GaussianMixture模型的预测打印出前50个集群标签：
- en: '[PRE38]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The sample output in the console will show the following:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制台中的示例输出将显示如下内容：
- en: '[PRE39]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We then close the program by stopping the Spark context:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过停止Spark上下文来关闭程序：
- en: '[PRE40]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: How it works...
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In the previous recipe, we observed that KMeans can discover and allocate membership
    to one and only one cluster based on an iterative method using similarity (Euclidian,
    and so on). One can think of KMeans as a specialized version of a Gaussian mixture
    model with EM models in which a discrete (hard) membership is enforced.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配方中，我们观察到KMeans可以使用相似性（欧几里得等）的迭代方法发现并分配成员资格给一个且仅一个集群。人们可以将KMeans视为具有EM模型的高斯混合模型的专门版本，在其中强制执行离散（硬）成员资格。
- en: 'But there are cases that have overlap, which is often the case in medicine
    or signal processing, as depicted in the following figure:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 但有些情况会有重叠，这在医学或信号处理中经常发生，如下图所示：
- en: '![](img/00168.jpeg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00168.jpeg)'
- en: In such cases, we need a probability density function that can express the membership
    in each sub-distribution. The Gaussian Mixture models with **Expectation Maximization**
    (**EM**) is the algorithm `GaussianMixture()` available in Spark that can deal
    with this use case.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要一个概率密度函数，可以表达在每个子分布中的成员资格。具有**期望最大化**（**EM**）的高斯混合模型是Spark中可处理此用例的算法`GaussianMixture()`。
- en: Here is Spark's API for implementing Gaussian Mixture with Expectation Maximization
    (the maximization of log likelihood).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Spark用于实现具有期望最大化的高斯混合的API（对数似然的最大化）。
- en: New GaussianMixture()
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新的GaussianMixture()
- en: 'This constructs a default instance. The default parameters that control the
    behavior of the model are:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这构造了一个默认实例。控制模型行为的默认参数是：
- en: '![](img/00169.gif)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00169.gif)'
- en: The Gaussian Mixture models with Expectation Maximization are a form of soft
    clustering in which a membership can be inferred using a log maximum likelihood
    function. In this scenario, a probability density function with mean and covariance
    is used to define the membership or likelihood of a membership to K number of
    clusters. It is flexible in the sense that the membership is not quantified which
    allows for overlapping membership based on probability (indexed to multiple sub-distributions).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 具有期望最大化的高斯混合模型是一种软聚类形式，可以使用对数最大似然函数推断成员资格。在这种情况下，使用均值和协方差的概率密度函数来定义对K个集群的成员资格或成员资格的可能性。它是灵活的，因为成员资格没有被量化，这允许基于概率的重叠成员资格（索引到多个子分布）。
- en: 'The following figure is a snapshot of the EM algorithm:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图是EM算法的快照：
- en: '![](img/00170.jpeg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00170.jpeg)'
- en: 'Here are the steps to the EM algorithm:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是EM算法的步骤：
- en: Assume *N* number of Gaussian distribution.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设* N *个高斯分布。
- en: 'Iterate until we have convergence:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代直到收敛：
- en: For each point Z drawn with conditional probability of being drawn from distribution
    Xi written as *P (Z | Xi)*
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个点Z，条件概率被绘制为从分布Xi中被绘制的*P（Z | Xi）*
- en: Adjust the parameter's mean and variance so that they fit the points that are
    assigned to the sub-distribution
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整参数的均值和方差，使它们适合分配给子分布的点
- en: 'For a more mathematical explanation, including detailed work on maximum likelihood,
    see the following link: [http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf](http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 更多数学解释，请参见以下链接：[http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf](http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf)
- en: There's more...
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The following figure provides a quick reference point to highlight some of
    the differences between hard versus soft clustering:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图提供了一个快速参考点，以突出硬聚类与软聚类之间的一些差异：
- en: '![](img/00171.jpeg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00171.jpeg)'
- en: See also
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for constructor GaussianMixture can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数GaussianMixture的文档可以在以下链接找到：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture)
- en: Documentation for constructor GaussianMixtureModel can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GaussianMixtureModel的构造函数文档可以在以下链接找到：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel)
- en: Classifying the vertices of a graph using Power Iteration Clustering (PIC) in
    Spark 2.0
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用幂迭代聚类（PIC）对图的顶点进行分类
- en: This is a classification method for the vertices of a graph given their similarities
    as defined by their edges. It uses the GraphX library which is ships out of the
    box with Spark to implement the algorithm. Power Iteration Clustering is similar
    to other Eigen Vector/Eigen Value decomposition algorithms, but without the overhead
    of matrix decomposition. It is suitable when you have a large sparse matrix (for
    example, graphs depicted as a sparse matrix).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种根据边缘定义的相似性对图的顶点进行分类的方法。它使用GraphX库，该库与Spark一起提供以实现该算法。幂迭代聚类类似于其他特征向量/特征值分解算法，但没有矩阵分解的开销。当您有一个大型稀疏矩阵时（例如，图表示为稀疏矩阵），它是合适的。
- en: GraphFrames will be the replacement/interface proper for the GraphX library
    going forward ([https://databricks.com/blog/2016/03/03/introducing-graphframes.html](https://databricks.com/blog/2016/03/03/introducing-graphframes.html)).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: GraphFrames将成为GraphX库的替代/接口，以后会继续使用（[https://databricks.com/blog/2016/03/03/introducing-graphframes.html](https://databricks.com/blog/2016/03/03/introducing-graphframes.html)）。
- en: How to do it...
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE41]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark上下文所需的包以访问集群和`Log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE42]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Set up the logger level to ERROR only to reduce the output:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志记录器级别设置为仅错误以减少输出：
- en: '[PRE43]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create Spark''s configuration and SQL context so we can have access to the
    cluster and be able to create and use a DataFrame as needed:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和SQL上下文，以便我们可以访问集群，并能够根据需要创建和使用DataFrame：
- en: '[PRE44]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We create a training dataset with a list of datasets and use the Spark `sparkContext.parallelize()`
    function to create Spark RDD:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个包含数据集列表的训练数据集，并使用Spark的`sparkContext.parallelize()`函数创建Spark RDD：
- en: '[PRE45]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We create a `PowerIterationClustering` object and set the parameters. We set
    the `K` value to `3` and max iteration count to `15`:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个`PowerIterationClustering`对象并设置参数。我们将`K`值设置为`3`，最大迭代次数设置为`15`：
- en: '[PRE46]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We then let the model run:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后让模型运行：
- en: '[PRE47]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We print out the cluster assignment based on the model for the training data:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据训练数据的模型打印出聚类分配：
- en: '[PRE48]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The console output will show the following information:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制台输出将显示以下信息：
- en: '![](img/00172.gif)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00172.gif)'
- en: 'We also print out the model assignment data in a collection for each cluster:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还打印出每个聚类的模型分配数据：
- en: '[PRE49]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The console output will display the following information (in total, we have
    three clusters which were set in the preceding parameters):'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制台输出将显示以下信息（总共有三个聚类，这些聚类是在前面的参数中设置的）：
- en: '[PRE50]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We then close the program by stopping the Spark context:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过停止Spark上下文来关闭程序：
- en: '[PRE51]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: How it works...
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We created a list of edges and vertices for a graph and then proceeded to create
    the object and set the parameters:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个图的边和顶点的列表，然后继续创建对象并设置参数：
- en: '[PRE52]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The next step was the model of training data:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是训练数据的模型：
- en: '[PRE53]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The clusters were then outputted for inspection. The code near the end prints
    out the model assignment data in a collection for each cluster using Spark transformation
    operators.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 然后输出了聚类以供检查。接近结尾的代码使用Spark转换操作符将模型分配数据打印到集合中。
- en: At the core **PIC** (**Power Iteration Clustering**) is an eigenvalue class
    algorithm which avoids matrix decomposition by producing an Eigen Value plus an
    Eigen Vector to satisfy *Av* = λ*v.* Because PIC avoids the decomposition of the
    matrix A, it is suitable when the input matrix A (describing a graph in the case
    of Spark's PIC) is a large sparse matrix.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心**PIC**（**Power Iteration Clustering**）是一种特征值类算法，它通过产生满足*Av* = λ*v*的特征值加上特征向量来避免矩阵分解。因为PIC避免了矩阵A的分解，所以当输入矩阵A（在Spark的PIC的情况下描述为图）是一个大稀疏矩阵时，它是合适的。
- en: 'An example of PIC in image processing (post enhanced for paper) is depicted
    in the following figure:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: PIC在图像处理中的示例（已增强以供论文使用）如下图所示：
- en: '![](img/00173.jpeg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00173.jpeg)'
- en: The Spark implementation of the PIC algorithm is an improvement over the previous
    common implementation (NCut) by computing a pseudo Eigen Vector of the similarities
    defined as edges given N number of vertices (like an affinity matrix).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: PIC算法的Spark实现是对以前常见实现（NCut）的改进，通过计算伪特征向量来定义相似性，这些相似性被定义为给定N个顶点的边（如亲和矩阵）。
- en: 'The input as depicted in the following figure is a trinary tuple of RDDs describing
    the graph. The output is a model with a cluster assignment for each node. The
    algorithm similarities (edges) are assumed to be positive and symmetrical (not
    shown):'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示的输入是描述图的RDD的三元组。输出是每个节点的聚类分配的模型。假定算法相似性（边）是正的和对称的（未显示）：
- en: '![](img/00174.jpeg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00174.jpeg)'
- en: There's more...
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: For a more detailed mathematical treatment of the subject (power iteration),
    see the following white paper from Carnegie Mellon University: [http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf](http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf)
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 有关该主题（幂迭代）的更详细的数学处理，请参阅卡内基梅隆大学的以下白皮书：[http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf](http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf)
- en: See also
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for the constructor `PowerIterationClustering()` can be found
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering)
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering)找到`PowerIterationClustering()`构造函数的文档。
- en: Documentation for the constructor `PowerIterationClusteringModel()` can be found
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel)
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel)找到`PowerIterationClusteringModel()`构造函数的文档。
- en: Latent Dirichlet Allocation (LDA) to classify documents and text into topics
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配（LDA）用于将文档和文本分类为主题
- en: In this recipe, we will explore the **Latent Dirichlet Allocation** (**LDA**)
    algorithm in Spark 2.0\. The LDA we use in this recipe is completely different
    from linear discrimination analysis. Both Latent Dirichlet Allocation and linear
    discrimination analysis are referred to as LDA, but they are extremely different
    techniques. In this recipe, when we use the LDA, we refer to Latent Dirichlet
    Allocation. The chapter on text analytics is also relevant to understanding the
    LDA.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将探讨Spark 2.0中的**潜在狄利克雷分配**（**LDA**）算法。我们在本教程中使用的LDA与线性判别分析完全不同。潜在狄利克雷分配和线性判别分析都被称为LDA，但它们是极其不同的技术。在本教程中，当我们使用LDA时，我们指的是潜在狄利克雷分配。文本分析章节也与理解LDA相关。
- en: LDA is often used in natural language processing which tries to classify a large
    body of document (for example, emails from the Enron fraud case) into a discrete
    number of topics or themes so it can be understood. LDA is also a good candidate
    for selecting articles based on one's interest (for example, as you turn a page
    and spend time on a specific topic) in a given magazine article or page.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: LDA经常用于自然语言处理，试图将大量文档（例如来自安然欺诈案的电子邮件）分类为离散数量的主题或主题，以便理解。 LDA也是一个很好的选择，可以根据兴趣选择文章（例如，当您翻页并在特定主题上花时间时）在给定杂志文章或页面中。
- en: How to do it...
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE54]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Import the necessary packages:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE55]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We set up the necessary Spark Session to gain access to the cluster:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置必要的Spark会话以访问集群：
- en: '[PRE56]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We have a sample LDA dataset, which is located at the following relative path
    (you can use an absolute path). The sample file is provided with any Spark distribution
    and can be found under the home directory of Spark inside the data directory (see
    the following). Assume the input is a set of features for input to the LDA method:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有一个LDA数据集，位于以下相对路径（您可以使用绝对路径）。 示例文件随任何Spark分发一起提供，并且可以在Spark的主目录下的data目录中找到（请参见以下）。假设输入是输入到LDA方法的一组特征：
- en: '[PRE57]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![](img/00175.jpeg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00175.jpeg)'
- en: 'In this step, we read the file and create the necessary dataset from the input
    file and show the top five rows in the console:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们读取文件并从输入文件创建必要的数据集，并在控制台中显示前五行：
- en: '[PRE58]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'From the console output:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '![](img/00176.gif)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00176.gif)'
- en: 'We create the LDA object and set the parameters for the object:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建LDA对象并设置对象的参数：
- en: '[PRE59]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We then run the model using the high-level API from the package:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们使用包的高级API运行模型：
- en: '[PRE60]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'From the console output:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE61]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We get the topics distribution from the LDA model for each set of features,
    and show the topics.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从LDA模型中获取每组特征的主题分布，并显示主题。
- en: 'We set the `maxTermsPerTopic` value as `3`:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`maxTermsPerTopic`值设置为`3`：
- en: '[PRE62]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'On the console, the output will show the following information:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在控制台上，输出将显示以下信息：
- en: '![](img/00177.jpeg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00177.jpeg)'
- en: 'We also transform the training dataset from the LDA model, and show the result:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还从LDA模型转换训练数据集，并显示结果：
- en: '[PRE63]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The output will display the following:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将显示以下内容：
- en: '![](img/00178.jpeg)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00178.jpeg)'
- en: 'If the preceding method is changed to:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的方法被更改为：
- en: '[PRE64]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The result will be displayed as truncated:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果将被显示为截断：
- en: '![](img/00179.jpeg)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00179.jpeg)'
- en: 'We close the Spark context to end the program:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们关闭Spark上下文以结束程序：
- en: '[PRE65]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: How it works...
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: LDA assumes that the document is a mixture of different topics with Dirichlet
    prior distribution. The words in the document are assumed to have an affinity
    towards a specific topic which allows LDA to classify the overall document (compose
    and assign a distribution) that best matches a topic.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: LDA假设文档是具有狄利克雷先验分布的不同主题的混合物。假定文档中的单词倾向于特定主题，这使得LDA能够对最匹配主题的整个文档进行分类（组成和分配分布）。
- en: A topic model is a generative latent model for discovering abstract themes (topics)
    that occur in the body of documents (often too large for humans to handle). The
    models are a pre-curser to summarize, search, and browse a large set of unlabeled
    documents and their contents. Generally speaking, we are trying to find a cluster
    of features (words, sub-images, and so on) that occur together.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型是一种生成潜在模型，用于发现文档集合中出现的抽象主题（主题）（通常对于人类来说太大）。这些模型是总结，搜索和浏览一大批未标记文档及其内容的先导条件。一般来说，我们试图找到一组特征（单词，子图像等），这些特征一起出现。
- en: 'The following figure depicts the overall LDA scheme:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了整体LDA方案：
- en: Please be sure to refer to the white paper cited here for completeness [http://ai.stanford.edu/~ang/papers/nips01-lda.pdf](http://ai.stanford.edu/~ang/papers/nips01-lda.pdf)
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 请务必参考此处引用的白皮书以获取完整信息 [http://ai.stanford.edu/~ang/papers/nips01-lda.pdf](http://ai.stanford.edu/~ang/papers/nips01-lda.pdf)
- en: '![](img/00180.jpeg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00180.jpeg)'
- en: 'The steps for the LDA algorithm are as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: LDA算法的步骤如下：
- en: 'Initialize the following parameters (controls concentration and smoothing):'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化以下参数（控制浓度和平滑）：
- en: Alpha parameter (high alpha makes documents more similar to each other and contain
    similar topics )
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alpha参数（高alpha使文档更相似，并包含相似的主题）
- en: Beta parameter ( high beta means each topic is most likely to contain a mix
    of most of the words)
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Beta参数（高beta意味着每个主题很可能包含大部分单词的混合）
- en: Randomly initialize the topic assignment.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化主题分配。
- en: 'Iterate:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代：
- en: For each document.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个文档。
- en: For each word in the document.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于文档中的每个单词。
- en: Resample the topic for each word.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个单词重新取样主题。
- en: With respect to all other words and their current assignment (for the current
    iteration).
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相对于所有其他单词及其当前分配（对于当前迭代）。
- en: Get the result.
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获得结果。
- en: Model evaluation
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型评估
- en: In statistics, Dirichlet distribution Dir(alpha) is a family of continuous multivariate
    probability distributions parameterized by a vector α of positive real numbers.
    For a more in-depth treatment of LDA, see the original paper in the
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，Dirichlet分布Dir（alpha）是由一组正实数α参数化的连续多元概率分布家族。有关LDA的更深入处理，请参阅原始论文
- en: Journal of Machine Learning at: [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习杂志：[http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
- en: The LDA does not assign any semantics to a topic and does not care what the
    topics are called. It is only a generative model that uses distribution of fine-grained
    items (for example, words about cats, dogs, fish, cars) to assign an overall topic
    that scores the best. It does not know, care, or understand about topics called
    dogs or cats.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: LDA不会为主题分配任何语义，也不在乎主题的名称。它只是一个生成模型，使用细粒度项目的分布（例如，关于猫、狗、鱼、汽车的单词）来分配得分最高的整体主题。它不知道、不关心、也不理解称为狗或猫的主题。
- en: We often have to tokenize and vectorize the document via TF-IDF prior to input
    to an LDA algorithm.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入LDA算法之前，我们经常需要通过TF-IDF对文档进行标记化和向量化。
- en: There's more...
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The following figure depicts the LDA in a nutshell:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图描述了LDA的要点：
- en: '![](img/00181.jpeg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00181.jpeg)'
- en: 'There are two approaches to document analysis. We can simply use matrix factorization
    to decompose a large matrix of datasets to a smaller matrix (topic assignments)
    times a vector (topics themselves):'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 文档分析有两种方法。我们可以简单地使用矩阵分解将大型数据集的矩阵分解为较小的矩阵（主题分配）乘以一个向量（主题本身）：
- en: '![](img/00182.jpeg)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00182.jpeg)'
- en: See also
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '**LDA**: documentation for a constructor can be found at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LDA**：构造函数的文档可以在此处找到：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)'
- en: '[**LDAModel**: documentation for a constructor can be found at: ](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LDAModel**：构造函数的文档可以在此处找到：](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)'
- en: 'See also, via Spark''s Scala API, documentation links for the following:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 另请参阅，通过Spark的Scala API，以下文档链接：
- en: DistributedLDAModel
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DistributedLDAModel
- en: EMLDAOptimizer
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMLDAOptimizer
- en: LDAOptimizer
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDAOptimizer
- en: LocalLDAModel
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LocalLDAModel
- en: OnlineLDAOptimizer
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OnlineLDAOptimizer
- en: Streaming KMeans to classify data in near real-time
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式KMeans用于实时分类数据
- en: Spark streaming is a powerful facility which lets you combine near real time
    and batch in the same paradigm. The streaming KMeans interface lives at the intersection
    of ML clustering and Spark streaming, and takes full advantage of the core facilities
    provided by Spark streaming itself (for example, fault tolerance, exactly once
    delivery semantics, and so on).
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: Spark流式处理是一个强大的工具，它让您可以在同一范式中结合近实时和批处理。流式KMeans接口位于ML聚类和Spark流式处理的交集处，并充分利用了Spark流式处理本身提供的核心功能（例如，容错性、精确一次性传递语义等）。
- en: How to do it...
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Import the necessary packages for streaming KMeans:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入流式KMeans所需的包：
- en: '`package spark.ml.cookbook.chapter8`.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.ml.cookbook.chapter8`包。'
- en: 'Import the necessary packages for streaming KMeans:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入流式KMeans所需的包：
- en: '[PRE66]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: We set up the following parameters for the streaming KMeans program. The training
    directory will be the directory to send the training data file. The KMeans clustering
    model utilizes the training data to run algorithms and calculations. The `testDirectory`
    will be the test data for predictions. The `batchDuration` is a number in seconds
    for a batch run. In the following case, the program will check every 10 seconds
    to see if there is any new data files for recalculations.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为流式KMeans程序设置以下参数。训练目录将是发送训练数据文件的目录。KMeans聚类模型利用训练数据来运行算法和计算。`testDirectory`将是用于预测的测试数据。`batchDuration`是批处理运行的秒数。在以下情况下，程序将每10秒检查一次是否有新的数据文件进行重新计算。
- en: 'The cluster is set to `2`, and the data dimensions will be `3`:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群设置为`2`，数据维度为`3`：
- en: '[PRE67]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'With the preceding settings, the sample training data will contain data like
    the following (in the format of [*X[1], X[2], ...X[n]*], where *n* is `numDimensions`:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上述设置，样本训练数据将包含以下数据（格式为[*X[1]，X[2]，...X[n]*]，其中*n*为`numDimensions`）：
- en: '[0.0,0.0,0.0]'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '[0.0,0.0,0.0]'
- en: '[0.1,0.1,0.1]'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '[0.1,0.1,0.1]'
- en: '[0.2,0.2,0.2]'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '[0.2,0.2,0.2]'
- en: '[9.0,9.0,9.0]'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '[9.0,9.0,9.0]'
- en: '[9.1,9.1,9.1]'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '[9.1,9.1,9.1]'
- en: '[9.2,9.2,9.2]'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '[9.2,9.2,9.2]'
- en: '[0.1,0.0,0.0]'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '[0.1,0.0,0.0]'
- en: '[0.2,0.1,0.1]'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '[0.2,0.1,0.1]'
- en: '....'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '....'
- en: 'The test data file will contain data like the following (in the format of (*y,
    [X1,X2, .. Xn]*), where *n* is `numDimensions` and `y` is an identifier):'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据文件将包含以下数据（格式为（*y，[X1，X2，.. Xn]*），其中*n*为`numDimensions`，`y`是标识符）：
- en: (7,[0.4,0.4,0.4])
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: (7,[0.4,0.4,0.4])
- en: (8,[0.1,0.1,0.1])
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: (8,[0.1,0.1,0.1])
- en: (9,[0.2,0.2,0.2])
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: (9,[0.2,0.2,0.2])
- en: (10,[1.1,1.0,1.0])
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: (10,[1.1,1.0,1.0])
- en: (11,[9.2,9.1,9.2])
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: (11,[9.2,9.1,9.2])
- en: (12,[9.3,9.2,9.3])
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: (12,[9.3,9.2,9.3])
- en: 'We set up the necessary Spark context to gain access to the cluster:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置必要的Spark上下文以访问集群：
- en: '[PRE68]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Define the streaming context and micro-batch window:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义流式上下文和微批处理窗口：
- en: '[PRE69]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The following code will create data by parsing the data file in the preceding
    two directories into `trainingData` and `testData RDDs`:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码将通过解析前两个目录中的数据文件来创建`trainingData`和`testData RDDs`：
- en: '[PRE70]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We create the `StreamingKMeans` model and set the parameters:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建`StreamingKMeans`模型并设置参数：
- en: '[PRE71]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The program will train the model using the training dataset and predict using
    the test dataset:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序将使用训练数据集训练模型，并使用测试数据集进行预测：
- en: '[PRE72]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We start the streaming context, and the program will run the batch every 10
    seconds to see if a new dataset is available for training, and if there is any
    new test dataset for prediction. The program will exit if a termination signal
    is received (exit the batch running):'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们启动流式上下文，并且程序将每10秒运行一次批处理，以查看是否有新的数据集可用于训练，以及是否有新的测试数据集可用于预测。如果接收到终止信号（退出批处理运行），程序将退出：
- en: '[PRE73]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'We copy the `testKStreaming1.txt` data file into the preceding `testDir` set
    and see the following printed out in the console logs:'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`testKStreaming1.txt`数据文件复制到前述`testDir`集中，并在控制台日志中看到以下内容：
- en: '![](img/00183.jpeg)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00183.jpeg)'
- en: For a Windows machine, we copied the `testKStreaming1.txt` file into the directory: `C:\spark-2.0.0-bin-hadoop2.7\data\sparkml2\chapter8\testDir\`.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于Windows机器，我们将`testKStreaming1.txt`文件复制到目录：`C:\spark-2.0.0-bin-hadoop2.7\data\sparkml2\chapter8\testDir\`。
- en: We can also check the SparkUI for more information: `http://localhost:4040/`.
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以检查SparkUI以获取更多信息：`http://localhost:4040/`。
- en: 'The job panel will display the streaming jobs, as shown in the following figure:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 作业面板将显示流式作业，如下图所示：
- en: '![](img/00184.jpeg)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00184.jpeg)'
- en: 'As shown in the following figure, the streaming panel will show the preceding
    Streaming KMeans matrix as the matrix displayed, the batch job running every 10
    seconds in this case:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，流式面板将显示前述流式KMeans矩阵作为显示的矩阵，在本例中每10秒运行一次批处理作业：
- en: '![](img/00185.jpeg)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00185.jpeg)'
- en: 'You can get more details on the streaming batch by clicking on any of the batches,
    as shown in following figure:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过单击任何批次来获取有关流式批处理的更多详细信息，如下图所示：
- en: '![](img/00186.jpeg)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00186.jpeg)'
- en: How it works...
  id: totrans-439
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In certain situations, we cannot use batch methods to load and capture the events
    and then react to them. We can use creative methods of capturing events in the
    memory or a landing DB and then rapidly marshal that over to another system for
    processing, but most of these systems fail to act as streaming systems and often
    are very expensive to build.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们无法使用批处理方法来加载和捕获事件，然后对其做出反应。我们可以使用创造性的方法在内存或着陆数据库中捕获事件，然后迅速将其调度到另一个系统进行处理，但大多数这些系统无法充当流式系统，并且通常非常昂贵。
- en: Spark provides a near real time (also referred to as subjective real time) that
    can receive incoming sources, such as Twitter feeds, signals, and so, on via connectors
    (for example, a Kafka connector) and then process and present them as an RDD interface.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了几乎实时（也称为主观实时）的功能，可以通过连接器（例如Kafka连接器）接收传入的来源，如Twitter feeds、信号等，然后将其处理并呈现为RDD接口。
- en: 'These are the elements needed to build and construct streaming KMeans in Spark:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在Spark中构建和构造流式KMeans所需的元素：
- en: 'Use the streaming context as opposed to the regular Spark context used so far:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用流式上下文，而不是迄今为止使用的常规Spark上下文：
- en: '[PRE74]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Select your connector to connect to a data source and receive events:'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择连接器以连接到数据源并接收事件：
- en: Twitter
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter
- en: Kafka
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka
- en: Third party
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三方
- en: ZeroMQ
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZeroMQ
- en: TCP
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TCP
- en: '........'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '........'
- en: 'Create your streaming KMeans model; set the parameters as needed:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建您的流式KMeans模型；根据需要设置参数：
- en: '[PRE75]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Train and predict as usual:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像往常一样进行训练和预测：
- en: Have in mind that K cannot be changed on the fly
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请记住，K不能在运行时更改
- en: 'Start the context and await for the termination signal to exit:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动上下文并等待终止信号以退出：
- en: '`ssc.start()`'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ssc.start()`'
- en: '`ssc.awaitTermination()`'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ssc.awaitTermination()`'
- en: There's more...
  id: totrans-459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Streaming KMeans are special cases of KMeans implementation in which the data
    can arrive at near real time and be classified into a cluster (hard classification)
    as needed. The applications are vast and can vary from near real-time anomaly
    detection (fraud, crime, intelligence, monitoring, and surveillance) to fine-grain
    micro-sector rotation visualization with Voronoi diagrams in finance. [Chapter
    13](part0538.html#G12EK0-4d291c9fed174a6992fd24938c2f9c77), *Spark Streaming and
    Machine Learning Library* provides a more detailed coverage for streaming.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 流式KMeans是KMeans实现的特殊情况，其中数据可以几乎实时到达并根据需要分类到一个簇（硬分类）。应用程序广泛，可以从几乎实时的异常检测（欺诈、犯罪、情报、监视和监控）到金融中细粒度微部门旋转可视化与Voronoi图。[第13章](part0538.html#G12EK0-4d291c9fed174a6992fd24938c2f9c77)，*Spark
    Streaming和机器学习库*提供了更详细的流式覆盖。
- en: For a reference to Voronoi diagrams, see the following URL: [https://en.wikipedia.org/wiki/Voronoi_diagram](https://en.wikipedia.org/wiki/Voronoi_diagram)
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Voronoi图的参考，请参阅以下网址：[https://en.wikipedia.org/wiki/Voronoi_diagram](https://en.wikipedia.org/wiki/Voronoi_diagram)
- en: 'Currently there are other algorithms besides streaming KMeans in the Spark
    Machine Library, as shown in the following figure:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，在Spark机器库中除了流式KMeans之外，还有其他算法，如下图所示：
- en: '![](img/00187.jpeg)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00187.jpeg)'
- en: See also
  id: totrans-464
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for Streaming KMeans can be found at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关流式KMeans的文档可以在以下网址找到：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)
- en: Documentation for Streaming KMeans Model can be found at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest)
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关流式KMeans模型的文档可以在以下网址找到：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest)
- en: "Documentation for Streaming Test--very useful for data generation--can be found\
    \ at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel\uFEFF\
    ](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)"
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: "流式测试的文档--用于数据生成--可以在以下网址找到：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel\uFEFF\
    ](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)"
