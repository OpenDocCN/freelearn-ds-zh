- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Connecting to Databases
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接到数据库
- en: In the previous chapters we focused entirely on data stored in individual files,
    but most of the real-world, work-based applications center around data stored
    in databases. Companies tend to store their data in the cloud, and therefore,
    being able to perform analyses on this data is a critical skill. In this chapter,
    we will explore how to access and use data stored in popular databases such as
    Snowflake and BigQuery. For each database, we’ll connect to the database, write
    SQL queries, and then make an example app.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们完全专注于存储在单个文件中的数据，但大多数真实世界的基于工作的应用程序集中在存储在数据库中的数据上。公司倾向于将其数据存储在云中，因此能够对这些数据进行分析是一项关键技能。在本章中，我们将探讨如何访问和使用存储在流行数据库（如Snowflake和BigQuery）中的数据。对于每个数据库，我们将连接到数据库，编写SQL查询，然后创建一个示例应用程序。
- en: Whether you are looking to perform ad hoc analysis on large datasets or build
    data-driven applications, the ability to efficiently retrieve and manipulate data
    from databases is essential. By the end of this chapter, you will have a strong
    understanding of how to use Streamlit to connect to and interact with databases,
    empowering you to extract insights and make data-driven decisions with confidence.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您是希望对大型数据集执行即席分析，还是构建数据驱动的应用程序，高效地从数据库中检索和操作数据的能力都是必不可少的。通过本章结束时，您将对如何使用Streamlit连接到数据库并与之交互有深入理解，从而使您能够自信地提取见解并做出数据驱动的决策。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Connecting to Snowflake with Streamlit
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Streamlit连接到Snowflake
- en: Connecting to BigQuery with Streamlit
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Streamlit连接到BigQuery
- en: Adding user input to queries
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加用户输入到查询中
- en: Organizing queries
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织查询
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following is a list of software and hardware installations that are required
    for this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章所需的软件和硬件安装列表：
- en: '**Snowflake account**: To get a Snowflake account, go to ([https://signup.snowflake.com/](https://signup.snowflake.com/))
    and start a free trial.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Snowflake账户**：要获得Snowflake账户，请访问（[https://signup.snowflake.com/](https://signup.snowflake.com/)），并开始免费试用。'
- en: '**Snowflake Python Connector**: The Snowflake Python Connector allows you to
    run queries from Python. If you installed the requirements for this book, then
    you already have the library. If not, `pip install` `snowflake-connector-python`
    to get started.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Snowflake Python连接器**：Snowflake Python连接器允许您从Python运行查询。如果您已安装了本书的要求，则已经拥有该库。如果没有，请运行`pip
    install snowflake-connector-python`开始安装。'
- en: '**BigQuery account**: To get a BigQuery account, go to ([https://console.cloud.google.com/bigquery](https://console.cloud.google.com/bigquery))
    and start a free trial.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BigQuery账户**：要获取BigQuery账户，请访问（[https://console.cloud.google.com/bigquery](https://console.cloud.google.com/bigquery)），并开始免费试用。'
- en: '**BigQuery Python Connector**: BigQuery also has a Python Connector that works
    the same way as the Snowflake Python Connector does! It also is in the requirements
    file that you installed at the beginning of the book, but you can also pip install
    `google-cloud-bigquery` if you do not have the library yet.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BigQuery Python连接器**：BigQuery也有一个Python连接器，其工作方式与Snowflake Python连接器相同！它也包含在您在本书开始时安装的要求文件中，但如果您尚未安装该库，您也可以运行`pip
    install google-cloud-bigquery`来安装。'
- en: Now that we have everything set up, let’s begin!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了一切，让我们开始吧！
- en: Connecting to Snowflake with Streamlit
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Streamlit连接到Snowflake
- en: To connect to any database within Streamlit, we mostly need to think about how
    to connect to that service **in** Python and then add some Streamlit-specific
    functions (like caching!) to improve the user experience. Luckily, Snowflake has
    invested a lot of time in making it incredibly easy to connect to Snowflake from
    Python; all you need to do is specify your account info and the Snowflake Python
    connector does the rest.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Streamlit中连接到任何数据库，我们主要需要考虑如何在Python中连接到该服务，然后添加一些特定于Streamlit的功能（如缓存！）来提高用户体验。幸运的是，Snowflake花费了大量时间使从Python连接到Snowflake变得非常简单；您只需指定您的账户信息，Snowflake
    Python连接器会完成其余操作。
- en: 'In this chapter, we’ll create and work in a new folder called `database_examples`
    and add a `streamlit_app.py` file, along with a Streamlit `secrets` file to get
    started:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将创建并在一个名为`database_examples`的新文件夹中工作，并添加一个名为`streamlit_app.py`的文件，以及一个Streamlit
    `secrets`文件来开始：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Within the `secrets.toml` file, we need to add our username, password, account,
    and warehouse. Our username and password are the ones we added when we signed
    up for our Snowflake account, the warehouse is the virtual computer that Snowflake
    uses to run the query (the default one is called `COMPUTE_WH`), and your account
    identifier is the only one left! To find your account identifier, the easiest
    way to find up to date info is through this link ([https://docs.snowflake.com/en/user-guide/admin-account-identifier](https://docs.snowflake.com/en/user-guide/admin-account-identifier)).
    Now that we have all the info we need, we can add them to our secrets file! Our
    file should look like the following, with your info instead of mine.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `secrets.toml` 文件中，我们需要添加用户名、密码、账户和仓库信息。我们的用户名和密码是在注册 Snowflake 账户时添加的，仓库是
    Snowflake 用来执行查询的虚拟计算机（默认的仓库叫做 `COMPUTE_WH`），而账户标识符是最后一个需要填写的！要查找你的账户标识符，最简单的办法是通过这个链接查看最新信息（[https://docs.snowflake.com/en/user-guide/admin-account-identifier](https://docs.snowflake.com/en/user-guide/admin-account-identifier)）。现在我们有了所有需要的信息，可以将它们添加到我们的
    secrets 文件中！我们的文件应该如下所示，内容是你的信息而不是我的。
- en: 'Now that we have the account info from the result of the SQL query above, we
    have all the info we need, and we can add it to our `secrets` file! Our file should
    look like the following, with your info instead of mine:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们从上面的 SQL 查询结果中获取了账户信息，我们有了所有需要的信息，可以将它们添加到 `secrets` 文件中！我们的文件应该如下所示，内容是你的信息而不是我的：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now we can start making our Streamlit app. Our first step is going to create
    our Snowflake connection, run a basic SQL query, and then output that to our Streamlit
    app:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始创建我们的 Streamlit 应用了。我们的第一步是创建 Snowflake 连接，运行一个基本的 SQL 查询，然后将结果输出到 Streamlit
    应用中：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This code does a few things; first, it uses the Snowflake Python Connector to
    programmatically connect to our Snowflake account using the secrets in our `secrets`
    file, then it runs the SQL query that just returns `1`, and finally, it shows
    that output in our app.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码做了几件事：首先，它使用 Snowflake Python 连接器，通过 `secrets` 文件中的秘密信息编程连接到我们的 Snowflake
    账户，然后它运行 SQL 查询，仅返回 `1`，最后它将在我们的应用中显示该输出。
- en: 'Our app should now look like the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的应用应该如下所示：
- en: '![](img/B18444_09_01.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18444_09_01.png)'
- en: 'Figure 9.1: Snowflake Query Result'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：Snowflake 查询结果
- en: 'Every time we run this app it will reconnect to Snowflake. This isn’t a great
    user experience, as it will make our app slower. In the past we would have cached
    this by wrapping it in a function and caching it with `st.cache_data`, but that
    will actually not work here as the connection is not data. Instead, we should
    cache it with `st.cache_resource`, similar to how we dealt with the HuggingFace
    model earlier in this book. Our session initialization code should now look like
    this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 每次运行这个应用时，它都会重新连接到 Snowflake。这不是一个理想的用户体验，因为它会使应用变得更慢。过去我们会通过将其包装在函数中并使用 `st.cache_data`
    来缓存，但在这里这样做不起作用，因为连接不是数据。相反，我们应该使用 `st.cache_resource` 来缓存它，类似于我们在本书前面处理 HuggingFace
    模型的方式。我们的会话初始化代码现在应该像这样：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, our app should look like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的应用应该是这样的：
- en: '![](img/B18444_09_02.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18444_09_02.png)'
- en: 'Figure 9.2: SQL GROUPBY'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：SQL GROUPBY
- en: 'Now, we also want to cache the result of the data to speed up our app and reduce
    the cost. This is something we’ve done before; we can wrap the query call in a
    function and use `st.cache_data` to cache it! It should look like this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们还想缓存数据结果，以加快应用速度并降低成本。这是我们之前做过的事情；我们可以将查询调用包装在一个函数中，并使用 `st.cache_data`
    来缓存它！它应该像这样：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our last step for this app is to dress up the appearance a bit. Right now it’s
    fairly basic, so we can add a graph, a title, and also what column we should use
    to graph as the user. Also, we will make sure our results are of the type `float`
    (which is roughly a non-integer number), as a good, general practice:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这个应用的最后一步是稍微打扮一下外观。现在它比较基础，因此我们可以添加一个图表、一个标题，并且让用户选择用于作图的列。另外，我们还会确保结果是 `float`
    类型（大致是非整数的数字），这是一个好的通用实践：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now our app is interactive, and it shows a great graph! It will look like the
    following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的应用程序是互动式的，并且显示了一个很棒的图表！它将如下所示：
- en: '![](img/B18444_09_03.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18444_09_03.png)'
- en: 'Figure 9.3: The TCP-H final app'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：TCP-H 最终应用
- en: That is it for our section on connecting to Snowflake with Streamlit! There
    are currently Snowflake products in preview that let you create Streamlit apps
    directly inside of Snowflake. If you want access to products like these, reach
    out to your Snowflake admin and they should be able to help you get access!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以上就是我们关于使用 Streamlit 连接 Snowflake 的章节内容！目前，Snowflake 有一些预览版产品可以让你直接在 Snowflake
    内创建 Streamlit 应用。如果你想使用这些产品，可以联系你的 Snowflake 管理员，他们应该能帮你获取访问权限！
- en: Now, on to BigQuery!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，开始使用 BigQuery！
- en: Connecting to BigQuery with Streamlit
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Streamlit 连接 BigQuery
- en: 'The first step to getting BigQuery connected to your Streamlit app is to gather
    the authentication information necessary from BigQuery. There is a wonderful Quickstart
    doc that Google keeps (and maintains!) that you should follow, which can be found
    here: [https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries](https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries).
    This link will help you sign up for a free account, and create a project. After
    you create your project, you need to create a service account ([https://console.cloud.google.com/apis/credentials](https://console.cloud.google.com/apis/credentials))
    and download the credentials as a JSON file. Once you have this file, you have
    all the data needed and can return to this chapter.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将 BigQuery 连接到 Streamlit 应用的第一步是获取从 BigQuery 所需的认证信息。Google 提供了一份非常棒的快速入门文档，你应该按照文档操作，文档链接如下：[https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries](https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries)。这个链接将帮助你注册免费账户，并创建一个项目。创建项目后，你需要创建一个服务账号（[https://console.cloud.google.com/apis/credentials](https://console.cloud.google.com/apis/credentials)），并将凭证下载为
    JSON 文件。一旦你获得了这个文件，你就拥有了所有需要的数据，可以回到本章继续操作。
- en: 'For this section, we will create a new file in our `database_example` folder
    called `bigquery_app.py`, and we will add a new section to the `secrets.toml`
    file we already created. First, we can add to the `secrets.toml` file and finally,
    let you create and view your service account credentials using this link ([https://console.cloud.google.com/apis/credentials](https://console.cloud.google.com/apis/credentials)).
    Go ahead and paste your service account credentials into a new section of your
    `secrets.toml` file like so:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将在 `database_example` 文件夹内创建一个新的文件，命名为 `bigquery_app.py`，并且我们将向已创建的
    `secrets.toml` 文件中添加一个新部分。首先，我们可以编辑 `secrets.toml` 文件，最后，你可以通过这个链接创建和查看你的服务账号凭证（[https://console.cloud.google.com/apis/credentials](https://console.cloud.google.com/apis/credentials)）。请将你的服务账号凭证粘贴到
    `secrets.toml` 文件的新部分，格式如下：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we need to create and open our new app file, called `bigquery_app.py`,
    and connect to BigQuery from there:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要创建并打开一个新的应用文件，命名为 `bigquery_app.py`，并从那里连接到 BigQuery：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, when we want to run a query, we can use the client variable that we created
    with our authentication to run it! To show an example, Google kindly provides
    a free dataset that stores how often people download Python libraries. We can
    write a quick query of that dataset that counts the last 5 days of Streamlit downloads
    in our app, as shown below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们想要运行查询时，可以使用我们通过认证创建的客户端变量来执行它！为了展示一个例子，Google 慷慨地提供了一个免费数据集，记录了人们下载 Python
    库的频率。我们可以编写一个查询，计算我们应用中过去 5 天的 Streamlit 下载量，查询代码如下：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When we run this app, we get the following result:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个应用时，得到的结果如下：
- en: '![](img/B18444_09_04.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18444_09_04.png)'
- en: 'Figure 9.4: The BigQuery result'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：BigQuery 查询结果
- en: 'In this case, I ran the query around 8pm PST on March 29^(th), which means
    that parts of the world had already moved on to March 30^(th) and started downloading
    libraries. This is the reason for the big drop on the 30^(th)! Next, as an improvement,
    we can graph the downloads over time with `st.line_chart()`, as we have done quite
    a few times in this book:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我大约在 3 月 29 日的太平洋标准时间 8 点运行了查询，这意味着世界某些地方已经进入了 3 月 30 日，并开始下载库。这就是 30
    日下载量大幅下降的原因！接下来，作为改进，我们可以通过 `st.line_chart()` 来绘制下载量随时间变化的图表，就像我们在本书中做过的几次一样：
- en: '![](img/B18444_09_05.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18444_09_05.png)'
- en: 'Figure 9.5: The BigQuery graph'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：BigQuery 图表
- en: 'As you will notice, it takes quite a while to run these queries. This is because
    we are caching neither the result nor the connection. Let’s add some functions
    to do that into our app:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，运行这些查询需要一些时间。这是因为我们既没有缓存结果，也没有缓存连接。让我们向应用中添加一些功能来实现这个目的：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And the bottom of our app will use the new `get_dataframe_from_sql` that we’ve
    just created:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用的底部将使用我们刚刚创建的 `get_dataframe_from_sql`：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: And that is it! Now you know how to get data from BigQuery and cache the results
    and the authentication process. This will be extremely useful as you start using
    Streamlit in work environments, as data rarely lives entirely in .`csv` files
    and instead exists in cloud databases. This next section will cover a couple more
    strategies to work with queries and databases in Streamlit.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！现在你知道如何从BigQuery获取数据并缓存结果以及认证过程了。随着你开始在工作环境中使用Streamlit，这将非常有用，因为数据很少完全存储在`.csv`文件中，而是存在于云数据库中。接下来的部分将介绍更多的策略，以便在Streamlit中处理查询和数据库。
- en: Adding user input to queries
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向查询添加用户输入
- en: One of the major benefits of using Streamlit is making user interactivity extremely
    easy, and we want to enable this while we write the apps that connect to databases.
    So far, we have written queries that we convert into DataFrames, and on top of
    these DataFrames, we can add our typical Streamlit widgets to further filter,
    group by, and then graph our data. However, this situation will only truly work
    on relatively small datasets, and often, we will have to change the underlying
    query for better performance in our apps. Let’s prove this point with an example.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Streamlit的一个主要好处是使用户交互变得极为简单，我们希望在编写连接数据库的应用程序时启用这一功能。到目前为止，我们编写了将查询转换为DataFrame的代码，并且在这些DataFrame上，我们可以添加典型的Streamlit小部件来进一步过滤、分组和绘制数据。然而，这种方法仅适用于相对较小的数据集，通常我们必须更改底层查询，以便在应用程序中获得更好的性能。让我们通过一个例子来证明这一点。
- en: 'Let us return to our Streamlit app in `bigquery_app.py`. We had a relatively
    arbitrary lookback period for our app, where we simply pulled the last 5 days
    in our query. What if we wanted to let the user define the lookback period? If
    we insisted on not changing the query and filtering after the query ran, then
    we would have to pull all the data from the `bigquery-public-data.pypi.file_downloads`
    table, which would be extremely slow and cost a huge amount of money. Instead,
    we can do the following to add a slider that changes the underlying query:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到`bigquery_app.py`中的Streamlit应用程序。我们为应用程序设置了一个相对任意的回溯期，在查询中仅提取了过去5天的数据。如果我们想让用户定义回溯期怎么办？如果我们坚持不改变查询，而是在查询执行后进行过滤，那么我们就不得不从`bigquery-public-data.pypi.file_downloads`表中提取所有数据，这将非常慢并且花费大量的金钱。相反，我们可以通过以下方法添加一个滑块来更改底层查询：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this situation, we added a slider that has appropriate minimum and maximum
    values, inputting the result of the slider into our query. This will cause the
    query to rerun every time the slider is moved, but it is still much more efficient
    than pulling the entire dataset. Now our app should look like this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们添加了一个滑块，设定了适当的最小值和最大值，并将滑块的结果输入到查询中。每当滑块移动时，查询都会重新执行，但这比提取整个数据集要高效得多。现在我们的应用程序应该是这样的：
- en: '![](img/B18444_09_06.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18444_09_06.png)'
- en: 'Figure 9.6: Dynamic SQL'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6：动态SQL
- en: We could have just as easily added dynamic SQL to our Snowflake queries with
    the same method, but this shows a wonderful example of it with BigQuery.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过同样的方法，轻松地在Snowflake查询中添加动态SQL，但这展示了在BigQuery中使用它的一个很好的例子。
- en: One word of warning here is to **never** use text input as input into a database
    query. If you allow freeform text as an input and put that into your queries,
    you functionally give your users the same access to your database that you have.
    You can use any of the other Streamlit widgets you would like without the same
    ramification because you have a guarantee of the output of widgets like `st.slider`,
    which will always return a number and never a malicious query.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一点警告，**绝对不要**将文本输入用作数据库查询的输入。如果你允许用户输入自由格式的文本并将其放入查询中，你实际上就赋予了用户与你一样的数据库访问权限。你可以使用Streamlit的其他小部件，而不必担心同样的后果，因为像`st.slider`这样的部件的输出是有保证的，它始终返回数字而非恶意查询。
- en: Now that we have learned about adding user input to our queries, we can head
    over to our last section, organizing queries in Streamlit apps.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何将用户输入添加到查询中，我们可以进入最后一部分，组织Streamlit应用程序中的查询。
- en: Organizing queries
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组织查询
- en: 'As you create more and more Streamlit apps that rely on database queries, your
    Streamlit apps often tend to get extremely long and will include long queries
    stored as strings. This tends to make apps harder to read, and less understandable
    when collaborating with others. It is not uncommon for the Streamlit Data Team
    to have half a dozen 30-line queries powering one Streamlit app that we created!
    There are two strategies to improve this setup:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你创建越来越多依赖数据库查询的 Streamlit 应用，你的应用往往会变得非常长，并且会包含作为字符串存储的长查询。这会使应用变得更难阅读，并且在与他人协作时也会更加难以理解。对于
    Streamlit 数据团队来说，常常会有半打 30 行的查询来支撑一个我们创建的 Streamlit 应用！有两种策略可以改善这种设置：
- en: Creating downstream tables with a tool like `dbt`
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用像 `dbt` 这样的工具创建下游表格
- en: Storing queries in separate files
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将查询存储在独立的文件中
- en: We will really only cover the first of these, creating downstream tables, briefly.
    As we noticed in the last example, every time the user changed the slider, the
    query would rerun in the app. This can get rather inefficient! We could use a
    tool like dbt, which is a very popular tool that lets us schedule SQL queries,
    to create a smaller table that already had the larger table filtered down to contain
    only the last 30 days of Streamlit data inside `bigquery-public-data.pypi.file_downloads`.
    This way, our query would be fewer lines and would not crowd our app, and it would
    also be more cost-effective and fast! We use this tip very often in the Streamlit
    Data Team, and we often have smaller downstream tables created in dbt that power
    our Streamlit apps.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要地只介绍其中的第一个，创建下游表格。如我们在上一个例子中所注意到的，每当用户更改滑块时，查询都会在应用中重新运行。这可能会变得相当低效！我们可以使用像
    dbt 这样的工具，它是一个非常流行的工具，可以让我们安排 SQL 查询，来创建一个较小的表格，这个表格已经把较大的表格筛选到只包含 `bigquery-public-data.pypi.file_downloads`
    中最后 30 天的 Streamlit 数据。这样，我们的查询行数会减少，也不会让应用变得拥挤，同时也更加经济高效！我们在 Streamlit 数据团队中非常常用这个技巧，我们经常在
    dbt 中创建较小的下游表格来支持我们的 Streamlit 应用。
- en: 'The second option is to store our queries in entirely separate files, and then
    import them into our apps. To do this, create a new file called `queries.py` in
    the same directory as our Streamlit app. Inside this file, we want to create a
    function that returns the `pypi` data query that we have already created, with
    the input to the function being the day filter we need for our app. It should
    look like this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选项是将我们的查询存储在完全独立的文件中，然后将它们导入到我们的应用中。为此，在与我们的 Streamlit 应用相同的目录中创建一个名为 `queries.py`
    的新文件。在这个文件中，我们需要创建一个函数，返回我们已经创建的 `pypi` 数据查询，函数的输入是我们应用所需的日期筛选。它应该是这样的：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, inside our Streamlit app file, we can import this function from our file
    and use it like so (I omitted the two cached functions for brevity):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们的 Streamlit 应用文件中，我们可以从文件中导入这个函数，并像这样使用它（为了简便，我省略了两个缓存函数）：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Perfect! Now our app is much smaller, and the Streamlit sections are logically
    separated from the query sections of our app. We consistently use strategies like
    this on the Streamlit Data Team, and we recommend strategies like this to folks
    who develop Streamlit apps in production.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！现在我们的应用变得更小了，Streamlit 部分与查询部分在应用中逻辑分离。我们在 Streamlit 数据团队中始终使用这样的策略，并且我们向开发生产环境中
    Streamlit 应用的人推荐这样的策略。
- en: Summary
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This concludes *Chapter 9*, *Connecting to Databases*. In this chapter, we learned
    a whole host of things, from connecting to Snowflake and BigQuery data in Streamlit
    to how to cache our queries and our database connections, saving us money and
    improving the user experience. In the next chapter, we will focus on improving
    job applications in Streamlit.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了*第 9 章*，*连接数据库*。在本章中，我们学习了很多内容，从在 Streamlit 中连接 Snowflake 和 BigQuery 数据，到如何缓存我们的查询和数据库连接，帮助我们节省成本并改善用户体验。在下一章中，我们将重点讨论如何在
    Streamlit 中优化工作应用程序。
- en: Learn more on Discord
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入这本书的 Discord 社区——你可以在这里分享反馈、向作者提问，并了解新版本的发布——请扫描下面的二维码：
- en: '[https://packt.link/sl](https://packt.link/sl)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/sl](https://packt.link/sl)'
- en: '![](img/QR_Code13440134443835796.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code13440134443835796.png)'
