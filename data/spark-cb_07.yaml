- en: Chapter 7. Supervised Learning with MLlib – Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。使用MLlib进行监督学习 - 回归
- en: 'This chapter is divided into the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分为以下几个部分：
- en: Using linear regression
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用线性回归
- en: Understanding the cost function
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解成本函数
- en: Doing linear regression with lasso
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用套索进行线性回归
- en: Doing ridge regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行岭回归
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'The following is Wikipedia''s definition of supervised learning:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是维基百科对监督学习的定义：
- en: '*"Supervised learning is the machine learning task of inferring a function
    from labeled training data."*'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“监督学习是从标记的训练数据中推断函数的机器学习任务。”*'
- en: 'Supervised learning has two steps:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习有两个步骤：
- en: Train the algorithm with training dataset; it is like giving questions and their
    answers first
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练数据集训练算法；这就像是先提出问题和它们的答案
- en: Use test dataset to ask another set of questions to the trained algorithm
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用测试数据集向训练好的算法提出另一组问题。
- en: 'There are two types of supervised learning algorithms:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种监督学习算法：
- en: '**Regression**: This predicts continuous value output, such as house price.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：这预测连续值输出，比如房价。'
- en: '**Classification**: This predicts discreet valued output (0 or 1) called label,
    such as whether an e-mail is a spam or not. Classification is not limited to two
    values; it can have multiple values such as marking an e-mail important, not important,
    urgent, and so on (0, 1, 2…).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：这预测离散值输出（0或1）称为标签，比如一封电子邮件是否是垃圾邮件。分类不仅限于两个值；它可以有多个值，比如标记一封电子邮件为重要、不重要、紧急等等（0,
    1, 2…）。'
- en: Note
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We are going to cover regression in this chapter and classification in the next.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍回归，下一章将介绍分类。
- en: 'As an example dataset for regression, we will use the recently sold house data
    of the City of Saratoga, CA, as a training set to train the algorithm. Once the
    algorithm is trained, we will ask it to predict the house price by the given size
    of that house. The following figure illustrates the workflow:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为回归的示例数据集，我们将使用加利福尼亚州萨拉托加市最近售出的房屋数据作为训练集来训练算法。一旦算法训练好了，我们将要求它根据房屋的尺寸来预测房价。下图说明了工作流程：
- en: '![Introduction](img/3056_07_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![介绍](img/3056_07_01.jpg)'
- en: Hypothesis, for what it does, may sound like a misnomer here, and you may think
    that prediction function may be a better name, but the word hypothesis is used
    for historic reasons.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的假设，对于它的作用来说，可能听起来像一个误称，你可能会认为预测函数可能是一个更好的名字，但是假设这个词是出于历史原因而使用的。
- en: If we use only one feature to predict the outcome, it is called **bivariate
    analysis**. When we have multiple features, it is called **multivariate analysis**.
    In fact, we can have as many features as we like. One such algorithm, **support
    vector machines** (**SVM**), which we will cover in the next chapter, in fact,
    allows you to have an infinite number of features.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只使用一个特征来预测结果，就称为**双变量分析**。当我们有多个特征时，就称为**多变量分析**。事实上，我们可以有任意多个特征。其中一种算法，**支持向量机**（**SVM**），我们将在下一章中介绍，实际上允许你拥有无限数量的特征。
- en: This chapter will cover how we can do supervised learning using MLlib, Spark's
    machine learning library.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍如何使用MLlib，Spark的机器学习库进行监督学习。
- en: Note
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Mathematical explanations have been provided in as simple a way as possible,
    but you can feel free to skip math and directly go to *How to do it...* section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数学解释已尽可能简单地提供，但你可以随意跳过数学，直接转到*如何做……*部分。
- en: Using linear regression
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用线性回归
- en: Linear regression is the approach to model the value of a response variable
    *y*, based on one or more predictor variables or feature *x*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是一种基于一个或多个预测变量或特征*x*来建模响应变量*y*值的方法。
- en: Getting ready
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Let''s use some housing data to predict the price of a house based on its size.
    The following are the sizes and prices of houses in the City of Saratoga, CA,
    in early 2014:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一些房屋数据来预测房屋的价格，基于它的大小。以下是2014年初加利福尼亚州萨拉托加市房屋的大小和价格：
- en: '| House size (sq ft) | Price |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 房屋大小（平方英尺） | 价格 |'
- en: '| --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 2100 | $ 1,620,000 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 2100 | $ 1,620,000 |'
- en: '| 2300 | $ 1,690,000 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 2300 | $ 1,690,000 |'
- en: '| 2046 | $ 1,400,000 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 2046 | $ 1,400,000 |'
- en: '| 4314 | $ 2,000,000 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 4314 | $ 2,000,000 |'
- en: '| 1244 | $ 1,060,000 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 1244 | $ 1,060,000 |'
- en: '| 4608 | $ 3,830,000 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 4608 | $ 3,830,000 |'
- en: '| 2173 | $ 1,230,000 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 2173 | $ 1,230,000 |'
- en: '| 2750 | $ 2,400,000 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 2750 | $ 2,400,000 |'
- en: '| 4010 | $ 3,380,000 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 4010 | $ 3,380,000 |'
- en: '| 1959 | $ 1,480,000 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 1959 | $ 1,480,000 |'
- en: 'Here''s a graphical representation of the same:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个相同的图形表示：
- en: '![Getting ready](img/3056_07_04.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![准备工作](img/3056_07_04.jpg)'
- en: How to do it…
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Start the Spark shell:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the statistics and related classes:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入统计和相关类：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create the `LabeledPoint` array with the house price as the label:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`LabeledPoint`数组，房价作为标签：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create an RDD of the preceding data:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建上述数据的RDD：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Train a model using this data using 100 iterations. Here, step size has been
    kept small to account for very large values of response variables, that is, the
    house price. The fourth parameter is a fraction of the dataset to use per iteration,
    and the last parameter is the initial set of weights to be used (weights of different
    features):'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些数据训练模型，进行100次迭代。这里，步长被保持得很小，以适应响应变量的非常大的值，也就是房价。第四个参数是每次迭代使用的数据集的一部分，最后一个参数是要使用的初始权重集（不同特征的权重）：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Predict the price for a house with 2,500 sq ft:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测一个2500平方英尺的房屋的价格：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: House size is just one predictor variable. House price depends upon other variables,
    such as lot size, age of the house, and so on. The more variables you have, the
    better your prediction will be.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 房屋大小只是一个预测变量。房价取决于其他变量，比如地块大小，房屋年龄等等。你拥有的变量越多，你的预测就会越准确。
- en: Understanding cost function
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解成本函数
- en: Cost function or loss function is a very important function in machine learning
    algorithms. Most algorithms have some form of cost function and the goal is to
    minimize that. Parameters, which affect cost function, such as `stepSize` in the
    last recipe, need to be set by hand. Therefore, understanding the whole concept
    of cost function is very important.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数或损失函数在机器学习算法中非常重要。大多数算法都有某种形式的成本函数，目标是最小化它。影响成本函数的参数，比如上一个步骤中的`stepSize`，需要手动设置。因此，理解成本函数的整个概念非常重要。
- en: In this recipe, we are going to analyze cost function for linear regression.
    Linear regression is a simple algorithm to understand and it will help readers
    understand the role of cost functions for even complex algorithms.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们将分析线性回归的成本函数。线性回归是一个简单的算法，可以帮助读者理解成本函数对于复杂算法的作用。
- en: Let's go back to linear regression. The goal is to find the best-fitting line
    so that the mean square of error is minimum. Here, we are referring error as the
    difference between the value as per the best-fitting line and the actual value
    of the response variable for the training dataset.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到线性回归。目标是找到最佳拟合线，使得误差的均方最小。这里，我们将误差定义为最佳拟合线的值与训练数据集中响应变量的实际值之间的差异。
- en: 'For a simple case of single predicate variable, the best-fit line can be written
    as:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个自变量的简单情况，最佳拟合线可以写成：
- en: '![Understanding cost function](img/3056_07_02.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_02.jpg)'
- en: 'This function is also called **hypothesis function**, and can be written as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数也被称为**假设函数**，可以写成：
- en: '![Understanding cost function](img/3056_07_03.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_03.jpg)'
- en: 'The goal of the linear regression is to find the best-fit line. On this line,
    θ[0] represents intercept on the *y* axis and θ[1] represents the slope of the
    line as obvious from the following equation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的目标是找到最佳拟合线。在这条线上，θ[0]代表*y*轴上的截距，θ[1]代表线的斜率，如下方程所示：
- en: '![Understanding cost function](img/3056_07_05.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_05.jpg)'
- en: 'We have to choose θ[0] and θ[1] in a way that *h(x)* is closest to *y* for
    the training dataset. So, for the *i* ^(th) data point, the square of distance
    between the line and data point is:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须选择θ[0]和θ[1]，使得*h(x)*对于训练数据集中的*y*最接近。因此，对于第*i*个数据点，线与数据点之间的距离的平方为：
- en: '![Understanding cost function](img/3056_07_06.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_06.jpg)'
- en: 'To put it in words, this is the square of the difference between the predicted
    house price and the actual price the house got sold for. Now, let''s take average
    of this value across the training dataset:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，这是预测房价与房屋实际售价之间的差的平方。现在，让我们计算训练数据集中这个值的平均值：
- en: '![Understanding cost function](img/3056_07_07.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_07.jpg)'
- en: The preceding equation is called the cost function *J* for linear regression.
    The goal is to minimize this cost function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程被称为线性回归的成本函数*J*。目标是最小化这个成本函数。
- en: '![Understanding cost function](img/3056_07_08.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_08.jpg)'
- en: This cost function is also called **squared error function**. Both θ[0] and
    theta θ[1] follow convex curve independently if they are plotted against *J*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个成本函数也被称为**平方误差函数**。如果它们分别针对*J*绘制，θ[0]和θ[1]都会遵循凸曲线。
- en: 'Let''s take a very simple example of dataset of three values, (1,1), (2,2),
    and (3,3), to make the calculation easy:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个非常简单的数据集的例子，包括三个值，(1,1), (2,2), 和 (3,3)，以便计算更容易：
- en: '![Understanding cost function](img/3056_07_09.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_09.jpg)'
- en: 'Let''s assume θ[1] is 0, that is, the best-fit line parallel to the *x* axis.
    In the first case, assume that the best-fit line is the *x* axis, that is, *y=0*.
    The following will be the value of the cost function:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 假设θ[1]为0，也就是说，最佳拟合线与*x*轴平行。在第一种情况下，假设最佳拟合线是*x*轴，也就是*y=0*。那么，成本函数的值将如下：
- en: '![Understanding cost function](img/3056_07_10.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_10.jpg)'
- en: 'Now, let''s move this line slightly up to *y=1*. The following will be the
    value of the cost function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们把这条线稍微移动到*y=1*。那么，成本函数的值将如下：
- en: '![Understanding cost function](img/3056_07_11.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_11.jpg)'
- en: 'Now, let''s move this line further up to *y=2*. Then, the following will be
    the value of the cost function:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们把这条线进一步移动到*y=2*。那么，成本函数的值将如下：
- en: '![Understanding cost function](img/3056_07_12.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_12.jpg)'
- en: 'Now, when we move this line further up to *y=3*, the following will be the
    value of the cost function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们把这条线进一步移动到*y=3*，成本函数的值将如下：
- en: '![Understanding cost function](img/3056_07_13.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_13.jpg)'
- en: 'Now, let''s move this line further up to *y=4*. The following will be the value
    of the cost function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们把这条线进一步移动到*y=4*。那么，成本函数的值将如下：
- en: '![Understanding cost function](img/3056_07_14.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_14.jpg)'
- en: 'So, you saw that the cost function value first decreased, and then increased
    again like this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你看到成本函数的值先减少，然后再次增加，就像这样：
- en: '![Understanding cost function](img/3056_07_15.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_15.jpg)'
- en: Now, let's repeat the exercise by making θ[0] 0 and using different values of
    θ[1].
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过将θ[0]设为0并使用不同的θ[1]值来重复这个练习。
- en: 'In the first case, assume the best-fit line is the *x* axis, that is, *y=0*.
    The following will be the value of the cost function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，假设最佳拟合线是*x*轴，也就是*y=0*。那么，成本函数的值将如下：
- en: '![Understanding cost function](img/3056_07_16.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_16.jpg)'
- en: 'Now, let''s use a slope of 0.5\. The following will be the value of the cost
    function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用斜率为0.5。那么，成本函数的值将如下：
- en: '![Understanding cost function](img/3056_07_17.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_17.jpg)'
- en: 'Now, let''s use a slope of 1\. The following will be the value of the cost
    function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用斜率为1。那么，成本函数的值将如下：
- en: '![Understanding cost function](img/3056_07_18.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_18.jpg)'
- en: 'Now, when we use a slope of 1.5, the following will be the value of the cost
    function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们使用斜率为1.5时，以下将是成本函数的值：
- en: '![Understanding cost function](img/3056_07_19.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_19.jpg)'
- en: 'Now, let''s use a slope of 2.0\. The following will be the value of the cost
    function:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用斜率为2.0。以下将是成本函数的值：
- en: '![Understanding cost function](img/3056_07_20.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_20.jpg)'
- en: As you can see in both the graphs, the minimum value of *J* is when slope or
    gradient of curve is 0.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在两个图中所见，当斜率或曲线的梯度为0时，*J*的最小值是。
- en: '![Understanding cost function](img/3056_07_21.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_21.jpg)'
- en: When both θ[0] and θ[1] are mapped to a 3D space, it becomes like the shape
    of a bowl with the minimum value of the cost function being at the bottom of it.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当θ[0]和θ[1]都映射到3D空间时，它就像一个碗的形状，成本函数的最小值在其底部。
- en: This approach to arrive at this minimum is called **gradient descent**. In Spark,
    the implementation is stochastic gradient descent.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 到达最小值的这种方法称为**梯度下降**。在Spark中，实现是随机梯度下降。
- en: Doing linear regression with lasso
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用套索进行线性回归
- en: The lasso is a shrinkage and selection method for linear regression. It minimizes
    the usual sum of squared errors, with a bound on the sum of the absolute values
    of the coefficients. It is based on the original lasso paper found at [http://statweb.stanford.edu/~tibs/lasso/lasso.pdf](http://statweb.stanford.edu/~tibs/lasso/lasso.pdf).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 套索是线性回归的收缩和选择方法。它最小化了通常的平方误差和系数绝对值之和的边界。它基于原始套索论文，可在[http://statweb.stanford.edu/~tibs/lasso/lasso.pdf](http://statweb.stanford.edu/~tibs/lasso/lasso.pdf)找到。
- en: 'The least square method we used in the last recipe is also called **ordinary
    least squares** (**OLS**). OLS has two challenges:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一个示例中使用的最小二乘法也称为**普通最小二乘法**（**OLS**）。OLS有两个挑战：
- en: '**Prediction accuracy**: Predictions made using OLS usually have low forecast
    bias and high variance. Prediction accuracy can be improved by shrinking some
    coefficients (or even making them zero). There will be some increase in bias,
    but overall prediction accuracy will improve.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测准确性**：使用OLS进行的预测通常具有较低的预测偏差和较高的方差。通过缩小一些系数（甚至使它们为零），可以提高预测准确性。偏差会有所增加，但整体预测准确性会提高。'
- en: '**Interpretation**: With a large number of predictors, it is desirable to find
    a subset of them that exhibits the strongest effect (correlation).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释**：对于预测变量的数量较多，希望找到其中表现最强的子集（相关性）。'
- en: Note
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Bias versus variance
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差与方差
- en: 'There are two primary reasons behind prediction error: bias and variance. The
    best way to understand bias and variance is to look at a case where we are doing
    predictions on the same dataset multiple times.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 预测误差背后有两个主要原因：偏差和方差。理解偏差和方差的最佳方法是看一个情况，我们在同一数据集上多次进行预测。
- en: Bias is an estimate of how far the predicted results are from the actual values,
    and variance is an estimate of the difference in predicted values among different
    predictions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差是预测结果与实际值之间的估计差距，方差是不同预测值之间的差异的估计。
- en: Generally, adding more features helps to reduce bias, as can easily be understood.
    If, in building a prediction model, we have left out some features with significant
    correlation, it would lead to significant error.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，添加更多的特征有助于减少偏差，这是很容易理解的。如果在构建预测模型时，我们遗漏了一些具有显著相关性的特征，这将导致显著的误差。
- en: If your model has high variance, you can remove features to reduce it. A bigger
    dataset also helps to reduce variance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型方差很高，可以删除特征以减少它。更大的数据集也有助于减少方差。
- en: Here, we are going to use a simple dataset, which is ill-posed. An ill-posed
    dataset is a dataset where the sample data size is smaller than the number of
    predictors.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用一个简单的数据集，这是一个不适当的数据集。不适当的数据集是指样本数据量小于预测变量的数量。
- en: '| y | x0 | x1 | x2 | x3 | x4 | x5 | x6 | x7 | x8 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| y | x0 | x1 | x2 | x3 | x4 | x5 | x6 | x7 | x8 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 5 | 3 | 1 | 2 | 1 | 3 | 2 | 2 | 1 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 5 | 3 | 1 | 2 | 1 | 3 | 2 | 2 | 1 |'
- en: '| 2 | 9 | 8 | 8 | 9 | 7 | 9 | 8 | 7 | 9 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 9 | 8 | 8 | 9 | 7 | 9 | 8 | 7 | 9 |'
- en: You can easily guess that, here, out of nine predictors, only two have a strong
    correlation with *y*, that is, *x0* and *x1*. We will use this dataset with the
    lasso algorithm to see its validity.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以很容易地猜到，在这里，九个预测变量中，只有两个与*y*有强相关性，即*x0*和*x1*。我们将使用这个数据集和套索算法来验证其有效性。
- en: How to do it…
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Start the Spark shell:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE6]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Import the statistics and related classes:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入统计和相关类：
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create the `LabeledPoint` array with the house price as the label:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建带有房价作为标签的`LabeledPoint`数组：
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create an RDD of the preceding data:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个RDD的前述数据：
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Train a model using this data using 100 iterations. Here, the step size and
    regularization parameter have been set by hand:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些数据训练一个模型，使用100次迭代。在这里，步长和正则化参数已经手动设置：
- en: '[PRE10]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Check how many predictors have their coefficients set to zero:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查有多少预测变量的系数被设置为零：
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you can see, six out of nine predictors have got their coefficients set
    to zero. This is the primary feature of lasso: any predictor it thinks is not
    useful, it literally moves them out of equation by setting their coefficients
    to zero.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，九个预测变量中有六个的系数被设置为零。这是套索的主要特征：它认为不实用的任何预测变量，通过将它们的系数设置为零，从方程中移除它们。
- en: Doing ridge regression
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行岭回归
- en: An alternate way to lasso to improve prediction quality is ridge regression.
    While in lasso, a lot of features get their coefficients set to zero and, therefore,
    eliminated from an equation, in ridge, predictors or features are penalized, but
    are never set to zero.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 改进预测质量的套索的另一种方法是岭回归。在套索中，许多特征的系数被设置为零，因此从方程中消除，在岭回归中，预测变量或特征受到惩罚，但永远不会被设置为零。
- en: How to do it…
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Start the Spark shell:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Import the statistics and related classes:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入统计和相关类：
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create the `LabeledPoint` array with the house price as the label:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建带有房价作为标签的`LabeledPoint`数组：
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create an RDD of the preceding data:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含上述数据的RDD：
- en: '[PRE15]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Train a model using this data using 100 iterations. Here, the step size and
    regularization parameter have been set by hand :'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些数据训练一个模型，进行100次迭代。在这里，步长和正则化参数已经手动设置：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Check how many predictors have their coefficients set to zero:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查有多少预测变量的系数被设为零：
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see, unlike lasso, ridge regression does not assign any predictor
    coefficients zero, but it does make some very close to zero.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，与套索不同，岭回归不会将任何预测变量的系数设为零，但它确实使一些系数非常接近于零。
