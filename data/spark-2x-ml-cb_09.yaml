- en: Optimization - Going Down the Hill with Gradient Descent
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化-使用梯度下降下山
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Optimizing a quadratic cost function and finding the minima using just math
    to gain insight
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过数学优化二次成本函数并找到最小值来获得洞察
- en: Coding a quadratic cost function optimization using Gradient Descent (GD) from
    scratch
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始编码二次成本函数优化，使用梯度下降（GD）
- en: Coding Gradient Descent optimization to solve Linear regression from scratch
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码梯度下降优化以从头解决线性回归
- en: Normal equations as an alternative to solve Linear regression in Spark 2.0
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中，正规方程作为解决线性回归的替代方法
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Understanding how optimization works is fundamental for a successful career
    in machine learning. We picked the **Gradient Descent** (**GD**) method for an
    end-to-end deep dive to demonstrate the inner workings of an optimization technique.
    We will develop the concept using three recipes that walk the developer from scratch
    to a fully developed code to solve an actual problem with real-world data. The
    fourth recipe explores an alternative to GD using Spark and normal equations (limited
    scaling for big data problems) to solve a regression problem.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 了解优化的工作原理对于成功的机器学习职业至关重要。我们选择了**梯度下降**（**GD**）方法进行端到端的深入挖掘，以演示优化技术的内部工作原理。我们将使用三个配方来开发这个概念，从头开始到完全开发的代码，以解决实际问题和真实世界数据。第四个配方探讨了使用Spark和正规方程（大数据问题的有限扩展）来解决回归问题的GD的替代方法。
- en: Let's get started. How does a machine learn anyway? Does it really learn from
    its mistakes? What does it mean when the machine finds a solution using optimization?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。机器到底是如何学习的？它真的能从错误中学习吗？当机器使用优化找到解决方案时，这意味着什么？
- en: 'At a high level, machines learn based on one of the following five techniques:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，机器学习基于以下五种技术之一：
- en: '**Error based learning**: In this technique, we search the domain space for
    a combination of parameter values (weights) that minimize the total error (predicted
    versus actual) over the training data.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于错误的学习**：在这种技术中，我们搜索领域空间，寻找最小化训练数据上总误差（预测与实际）的参数值组合（权重）。'
- en: '**Information theory learning**: This method uses concepts such as entropy
    and information gain that are found in classical Shannon Information theory. The
    tree-based ML system, rooted classically in the ID3 algorithm, fits well in this
    category. The ensemble tree models will be the crowning achievement of this category.
    We will discuss tree models in [Chapter 10](part0460.html#DMM2O0-4d291c9fed174a6992fd24938c2f9c77),
    *Building Machine Learning Systems with Decision Tree and Ensemble Models*.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息论学习**：这种方法使用经典香农信息论中的熵和信息增益等概念。基于树的ML系统，在ID3算法中经典地根植于这一类别。集成树模型将是这一类别的巅峰成就。我们将在[第10章](part0460.html#DMM2O0-4d291c9fed174a6992fd24938c2f9c77)中讨论树模型，*使用决策树和集成模型构建机器学习系统*。'
- en: '**Probability Space Learning**: This branch of machine learning is based on
    Bayes theorem ([https://en.wikipedia.org/wiki/Bayes''_theorem)](https://en.wikipedia.org/wiki/Bayes''_theorem)).
    The most well-known method in machine learning is Naïve Bayes (multiple variations).
    Naïve Bayes culminates with an introduction of the Bayes Network which allows
    for more control over the model.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率空间学习**：这个机器学习分支基于贝叶斯定理（[https://en.wikipedia.org/wiki/Bayes''_theorem)](https://en.wikipedia.org/wiki/Bayes''_theorem)）。机器学习中最著名的方法是朴素贝叶斯（多种变体）。朴素贝叶斯以引入贝叶斯网络而告终，这允许对模型进行更多控制。'
- en: '**Similarity measure learning***:* This method works by attempting to define
    a measure of similarity and then fitting an observation''s grouping based on that
    measure. The best known method is KNN (nearest neighbor), which is the standard
    in any ML toolkit. Spark ML implements K-means++ with parallelism referred to
    as K-Means|| (K Means Parallel).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似度测量学习**：这种方法通过尝试定义相似度度量，然后根据该度量来拟合观察的分组。最著名的方法是KNN（最近邻），这是任何ML工具包中的标准。
    Spark ML实现了带有并行性的K-means++，称为K-Means||（K均值并行）。'
- en: '**Genetic algorithms (GA) and evolutionary learning**: This can be viewed as
    Darwin''s theory (Origin of the Species) applied to optimization and machine learning.
    The idea behind GA is to use a recursive generative algorithm to create a set
    of initial candidates and then use feedback (fitness landscape) to eliminate distant
    candidates, fold similar ones while randomly introducing mutations (numerical
    or symbolic jitter) to unlikely candidates, and then repeat until the solution
    is found.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗传算法（GA）和进化学习**：这可以被视为达尔文的理论（物种的起源）应用于优化和机器学习。 GA背后的想法是使用递归生成算法创建一组初始候选者，然后使用反馈（适应性景观）来消除远距离的候选者，折叠相似的候选者，同时随机引入突变（数值或符号抖动）到不太可能的候选者，然后重复直到找到解决方案。'
- en: Some of the data scientist and ML engineers prefer to think of optimization
    as maximizing the log likelihood rather than minimizing the cost function - they
    are really the two sides of the same coin! In this chapter, we will focus on error-based
    learning, especially **Gradient Descent**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据科学家和ML工程师更喜欢将优化视为最大化对数似然，而不是最小化成本函数-它们实际上是同一枚硬币的两面！在本章中，我们将专注于基于错误的学习，特别是**梯度下降**。
- en: To provide a solid understanding, we will deep dive into Gradient Descent (GD)
    by going through three GD recipes as they apply to optimization. We will then
    provide Spark's Normal Equation recipe as an alternative to numerical optimization
    methods, such as Gradient Descent (GD) or the **Limited-memory Broyden-Fletcher-
    Goldfarb-Shanno** (**LBFGS**) algorithm.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供扎实的理解，我们将深入研究梯度下降（GD），通过三个GD配方来了解它们如何应用于优化。然后，我们将提供Spark的正规方程配方作为数值优化方法的替代方法，例如梯度下降（GD）或**有限内存的Broyden-Fletcher-Goldfarb-Shanno**（**LBFGS**）算法。
- en: Apache Spark provides an excellent coverage for all categories. The following
    figure depicts a taxonomy that will guide you through your journey in the field
    of numerical optimization, which is fundamental to achieving excellence in ML.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark为所有类别提供了出色的覆盖范围。以下图表描述了一个分类法，将指导您在数值优化领域的旅程，这对于在机器学习中取得卓越成就至关重要。
- en: '![](img/00188.gif)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00188.gif)'
- en: How do machines learn using an error-based system?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器如何使用基于错误的系统学习？
- en: Machines learn pretty much the same way we do--they learn from their mistakes.
    They first start by making an initial guess (random weights for parameters). Second,
    they use their model (for example, GLM, RRN, isotonic regression) to make a prediction
    (for example, a number). Third, they look at what the answer should have been
    (training set). Fourth, they measure the difference between actual versus predicted
    answers using a variety of techniques (such as least squares, similarity, and
    so on).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的学习方式与我们大致相同-它们从错误中学习。首先，它们首先进行初始猜测（参数的随机权重）。其次，它们使用自己的模型（例如GLM、RRN、等温回归）进行预测（例如一个数字）。第三，它们查看答案应该是什么（训练集）。第四，它们使用各种技术（如最小二乘法、相似性等）来衡量实际与预测答案之间的差异。
- en: Once all these mechanics are in place, they keep repeating the process over
    the entire training dataset, while trying to come up with a combination of parameters
    that has the minimal error when they consider the entire training dataset. What
    makes this interesting is that each branch of machine learning uses math or domain
    known facts to avoid a brute-force combinatorics approach which will not terminate
    in real-world settings.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有这些机制都就位，它们将在整个训练数据集上重复这个过程，同时试图提出一种参数组合，当考虑整个训练数据集时具有最小误差。有趣的是，机器学习的每个分支都使用数学或领域已知事实，以避免蛮力组合方法，这种方法在现实世界的环境中不会终止。
- en: The error-based ML optimization is a branch of mathematical programming (MP)
    which is implemented algorithmically, but with limited accuracy (varying accuracy
    of 10^(-2) to 10^(-6)). Most, if not all, the methods in this category take advantage
    of simple calculus facts like first derivative (slope), such as the GD technique,
    and second derivative (curvature), such as the BFGS technique, to minimize a cost
    function. In the case of BFGS, the invisible hands are the updater function (L1
    updater), rank (second rank update), approximating the final answer/solution using
    a Hessian free technique ([https://en.wikipedia.org/wiki/Hessian_matrix](https://en.wikipedia.org/wiki/Hessian_matrix))
    without the actual second derivative matrix.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基于错误的机器学习优化是数学规划（MP）的一个分支，它是通过算法实现的，但精度有限（精度变化为10^(-2)到10^(-6)）。这一类别中的大多数方法，如果不是全部，都利用简单的微积分事实，如一阶导数（斜率）（例如GD技术）和二阶导数（曲率）（例如BFGS技术），以最小化成本函数。在BFGS的情况下，隐形的手是更新器函数（L1更新器）、秩（二阶秩更新器），使用无Hessian矩阵的Hessian自由技术来近似最终答案/解决方案([https://en.wikipedia.org/wiki/Hessian_matrix](https://en.wikipedia.org/wiki/Hessian_matrix))。
- en: 'The following figure depicts some of the facilities that touch on optimization
    in Spark:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了Spark中涉及优化的一些设施：
- en: '**![](img/00189.gif)**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/00189.gif)**'
- en: The functions to do SGD and LBFGS optimization are available by themselves in
    Spark. To utilize them, you should be able to write and supply your own cost function.
    The functions, such as `runMiniBatchSGD()`, are not only marked private, but also
    require good understanding of the implementation of both algorithms.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中有用于执行SGD和LBFGS优化的函数。要使用它们，您应该能够编写并提供自己的成本函数。这些函数，如`runMiniBatchSGD()`，不仅标记为私有，而且需要对两种算法的实现有很好的理解。
- en: 'Since this is a cookbook and we cannot go deep into optimization theory, for
    background and reference we recommend the following books from our library:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一本食谱，我们无法深入研究优化理论，因此我们建议从我们的图书馆中参考以下书籍：
- en: '**Optimization (2013)**:[https://www.amazon.com/Optimization-Springer-Texts-Statistics-Kenneth/dp/1461458374/ref=sr_1_8?ie=UTF8&qid=1485744639&sr=8-8&keywords=optimization](https://www.amazon.com/Optimization-Springer-Texts-Statistics-Kenneth/dp/1461458374/ref=sr_1_8?ie=UTF8&qid=1485744639&sr=8-8&keywords=optimization)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化（2013）**：[https://www.amazon.com/Optimization-Springer-Texts-Statistics-Kenneth/dp/1461458374/ref=sr_1_8?ie=UTF8&qid=1485744639&sr=8-8&keywords=optimization](https://www.amazon.com/Optimization-Springer-Texts-Statistics-Kenneth/dp/1461458374/ref=sr_1_8?ie=UTF8&qid=1485744639&sr=8-8&keywords=optimization)'
- en: '**Optimization for Machine Learning (2011)**:[https://www.amazon.com/Optimization-Machine-Learning-Information-Processing/dp/026201646X/ref=sr_1_1?ie=UTF8&qid=1485744817&sr=8-1&keywords=optimization+for+machine+learning](https://www.amazon.com/Optimization-Machine-Learning-Information-Processing/dp/026201646X/ref=sr_1_1?ie=UTF8&qid=1485744817&sr=8-1&keywords=optimization+for+machine+learning)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习优化（2011）**：[https://www.amazon.com/Optimization-Machine-Learning-Information-Processing/dp/026201646X/ref=sr_1_1?ie=UTF8&qid=1485744817&sr=8-1&keywords=optimization+for+machine+learning](https://www.amazon.com/Optimization-Machine-Learning-Information-Processing/dp/026201646X/ref=sr_1_1?ie=UTF8&qid=1485744817&sr=8-1&keywords=optimization+for+machine+learning)'
- en: '**Convex Optimization (2004)**:[https://www.amazon.com/Convex-Optimization-Stephen-Boyd/dp/0521833787/ref=pd_sim_14_2?_encoding=UTF8&psc=1&refRID=7T88DJY5ZWBEREGJ4WT4](https://www.amazon.com/Convex-Optimization-Stephen-Boyd/dp/0521833787/ref=pd_sim_14_2?_encoding=UTF8&psc=1&refRID=7T88DJY5ZWBEREGJ4WT4)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**凸优化（2004）**：[https://www.amazon.com/Convex-Optimization-Stephen-Boyd/dp/0521833787/ref=pd_sim_14_2?_encoding=UTF8&psc=1&refRID=7T88DJY5ZWBEREGJ4WT4](https://www.amazon.com/Convex-Optimization-Stephen-Boyd/dp/0521833787/ref=pd_sim_14_2?_encoding=UTF8&psc=1&refRID=7T88DJY5ZWBEREGJ4WT4)'
- en: '**Genetic Algorithm in Search, Optimization and Machine Learning (1989) - a
    classic!**:[https://www.amazon.com/Genetic-Algorithms-Optimization-Machine-Learning/dp/0201157675/ref=sr_1_5?s=books&ie=UTF8&qid=1485745151&sr=1-5&keywords=genetic+programming](https://www.amazon.com/Genetic-Algorithms-Optimization-Machine-Learning/dp/0201157675/ref=sr_1_5?s=books&ie=UTF8&qid=1485745151&sr=1-5&keywords=genetic+programming)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗传算法在搜索、优化和机器学习中（1989）-经典！**：[https://www.amazon.com/Genetic-Algorithms-Optimization-Machine-Learning/dp/0201157675/ref=sr_1_5?s=books&ie=UTF8&qid=1485745151&sr=1-5&keywords=genetic+programming](https://www.amazon.com/Genetic-Algorithms-Optimization-Machine-Learning/dp/0201157675/ref=sr_1_5?s=books&ie=UTF8&qid=1485745151&sr=1-5&keywords=genetic+programming)'
- en: '**Swarm Intelligence from Natural to Artificial Systems (1999)**:[https://www.amazon.com/Swarm-Intelligence-Artificial-Institute-Complexity/dp/0195131592/ref=sr_1_3?s=books&ie=UTF8&qid=1485745559&sr=1-3&keywords=swarm+intelligence](https://www.amazon.com/Swarm-Intelligence-Artificial-Institute-Complexity/dp/0195131592/ref=sr_1_3?s=books&ie=UTF8&qid=1485745559&sr=1-3&keywords=swarm+intelligence)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《从自然到人工系统的群体智能》（1999）**：[https://www.amazon.com/Swarm-Intelligence-Artificial-Institute-Complexity/dp/0195131592/ref=sr_1_3?s=books&ie=UTF8&qid=1485745559&sr=1-3&keywords=swarm+intelligence](https://www.amazon.com/Swarm-Intelligence-Artificial-Institute-Complexity/dp/0195131592/ref=sr_1_3?s=books&ie=UTF8&qid=1485745559&sr=1-3&keywords=swarm+intelligence)'
- en: Optimizing a quadratic cost function and finding the minima using just math
    to gain insight
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过数学来优化二次成本函数并找到最小值
- en: In this recipe, we will explore the fundamental concept behind mathematical
    optimization using simple derivatives before introducing Gradient Descent (first
    order derivative) and L-BFGS, which is a Hessian free quasi-Newton method.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将在介绍梯度下降（一阶导数）和L-BFGS（一种无Hessian的拟牛顿方法）之前，探索数学优化背后的基本概念。
- en: We will examine a sample quadratic cost/error function and show how to find
    the minimum or maximum with just math.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究一个样本二次成本/误差函数，并展示如何仅通过数学找到最小值或最大值。
- en: '![](img/00190.jpeg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00190.jpeg)'
- en: We will use both the closed form (vertex formula) and derivative method (slope)
    to find the minima, but we will defer to later recipes in this chapter to introduce
    numerical optimization techniques, such Gradient Descent and its application to
    regression.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用顶点公式和导数方法来找到最小值，但我们将在本章的后续教程中介绍数值优化技术，如梯度下降及其在回归中的应用。
- en: How to do it...
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s assume we have a quadratic cost function and we find its minima:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们有一个二次成本函数，我们找到它的最小值：
- en: '![](img/00191.gif)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00191.gif)'
- en: The cost function in statistical machine learning algorithms acts as a proxy
    for the level of difficulty, energy spent, or total error as we move around in
    our search space.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在统计机器学习算法中，成本函数充当我们在搜索空间中移动时的难度级别、能量消耗或总误差的代理。
- en: The first thing we do is to graph the function and inspect it visually.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是绘制函数并进行直观检查。
- en: '![](img/00192.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00192.jpeg)'
- en: Upon visual inspection, we see that ![](img/00193.gif)  is a concave function
    with its minima at (2,1).
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过直观检查，我们看到 ![](img/00193.gif) 是一个凹函数，其最小值在 (2,1) 处。
- en: Our next step would be to find the minima by optimizing the function. Some examples
    of presenting the cost or error function in machine learning could be squared
    error, Euclidian distance, MSSE, or any other similarity measure that can capture
    how far we are from an optimal numeric answer.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的下一步将是通过优化函数来找到最小值。在机器学习中，呈现成本或误差函数的一些示例可能是平方误差、欧几里得距离、MSSE，或者任何其他能够捕捉我们离最佳数值答案有多远的相似度度量。
- en: The next step is to search for best parameter values that minimize errors (for
    example, cost) in our ML technique. For example, by optimizing a linear regression
    cost function (sum of squared errors), we arrive at best values for its parameter.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是寻找最小化误差（例如成本）的最佳参数值。例如，通过优化线性回归成本函数（平方误差的总和），我们得到其参数的最佳值。
- en: 'Derivative method: Set the first derivative to zero and solve'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导数方法：将一阶导数设为0并解出
- en: 'Vertex method: Use closed algebraic form'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顶点方法：使用封闭代数形式
- en: First, we solve for minima using the derivative method by computing the first
    derivative, setting it to zero, and solving for *x* and *y*.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们通过计算一阶导数，将其设为0，并解出 *x* 和 *y* 来使用导数方法求解最小值。
- en: '![](img/00194.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00194.jpeg)'
- en: 'Given f(x) = 2x² - 8x +9 as our cost/error function, the derivative can be
    computed as:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 f(x) = 2x² - 8x +9 作为我们的成本/误差函数，导数可以计算如下：
- en: '![](img/00195.jpeg)![](img/00196.gif)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00195.jpeg)![](img/00196.gif)'
- en: '[Power rule: ![](img/00197.gif)]'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[幂规则：![](img/00197.gif)]'
- en: '![](img/00198.gif) [We set the derivative equal to 0 and solve for![](img/00199.gif)]'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00198.gif) [我们将导数设为0并解出![](img/00199.gif)]'
- en: '![](img/00200.jpeg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00200.jpeg)'
- en: We now verify the minima using the vertex formula method. To compute the minima
    using the algebraic method please see the following steps.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用顶点公式方法验证最小值。要使用代数方法计算最小值，请参见以下步骤。
- en: 'Given the function, ![](img/00201.gif), the vertex can be found at:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定函数 ![](img/00201.gif)，顶点可以在以下位置找到：
- en: '![](img/00202.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00202.jpeg)'
- en: 'Let''s compute the minima using the vertex algebraic formula:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用顶点代数公式来计算最小值：
- en: '![](img/00203.gif)![](img/00204.gif)![](img/00205.gif)![](img/00206.gif)![](img/00207.gif)![](img/00208.gif)2(2)2
    + (-8) (2) +9![](img/00209.gif)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00203.gif)![](img/00204.gif)![](img/00205.gif)![](img/00206.gif)![](img/00207.gif)![](img/00208.gif)2(2)2
    + (-8) (2) +9![](img/00209.gif)'
- en: As the last step, we check the result of steps 4 and 5 to make sure that our
    answer using a closed algebraic form yielding the minimum of (2, 1), is consistent
    with the derivative method which also yields (2, 1).
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为最后一步，我们检查步骤4和5的结果，以确保我们使用封闭代数形式得出的最小值 (2, 1) 与导数方法得出的 (2, 1) 一致。
- en: In the last step, we show a pictorial view of *f(x)* in the left panel along
    with its derivative on the right panel, so you can visually inspect the answer
    for yourself.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最后一步，我们在左侧面板中展示 *f(x)* 的图形，右侧面板中展示其导数，这样您可以直观地检查答案。
- en: '![](img/00210.jpeg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00210.jpeg)'
- en: As you can see, a casual inspection depicts that the minima vertex is at (2,1)
    on the left hand side *{ x=2, f(x)=1 }* while the right hand side chart shows
    the derivative of the function with respect to *X* (only the parameter) with its
    minima at *X=2*. As seen in the previous steps, we set the derivative of the function
    to zero and solve for *X* which results in number 2\. You can also visually inspect
    the two panels and equations to make sure *X=2* is true and makes sense in both
    cases.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如您所看到的，随意检查显示最小值顶点在左侧为 (2,1) *{ x=2, f(x)=1 }*，而右侧图表显示函数关于 *X*（仅参数）的导数在 *X=2*
    处取得最小值。如前面的步骤所示，我们将函数的导数设为零并解出 *X*，结果为数字2。您还可以直观地检查两个面板和方程，以确保 *X=2* 在两种情况下都是正确的并且有意义。
- en: How it works...
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We have two techniques at our disposal for finding the minima of a quadratic
    function without using a numerical method. In real-life statistical machine learning
    optimization, we use the derivatives to find the minima of a convex function.
    If the function is convex (or the optimization is bonded), there is only one local
    minima, so the work is much simpler than non-linear/non-convex problems that are
    present in deep learning.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种技术可以用来找到二次函数的最小值，而不使用数值方法。在现实生活中的统计机器学习优化中，我们使用导数来找到凸函数的最小值。如果函数是凸的（或者优化是有界的），那么只有一个局部最小值，所以工作比在深度学习中出现的非线性/非凸问题要简单得多。
- en: 'Using the derivative method in the preceding recipe:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配方中使用导数方法：
- en: First, we found the derivative by applying the derivative rules (for example,
    exponent).
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们通过应用导数规则（例如指数）找到了导数。
- en: Second, we took advantage of the fact that, for a given simple quadratic function
    (convex optimization), the minima occurs when the slope of the first derivative
    is zero.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，我们利用了这样一个事实，对于给定的简单二次函数（凸优化），当第一导数的斜率为零时，最小值出现。
- en: Third, we simply found the derivative by following and applying mechanical calculus
    rules.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，我们简单地通过遵循和应用机械微积分规则找到了导数。
- en: Fourth, we set the derivative of the function to zero ![](img/00211.gif)and
    solved for x
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四，我们将函数的导数设置为零！[](img/00211.gif)并解出x
- en: Fifth, we used the x value and plugged it into the original equation to find
    y. Using steps 1 through 5, we ended up with the minima at point (2, 1).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第五，我们使用x值并将其代入原方程以找到y。通过步骤1到5，我们最终得到了点（2，1）处的最小值。
- en: There's more...
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Most statistical machine learning algorithms define and search a domain space
    while using a cost or error function to arrive at a best numerically approximated
    solution (for example, parameters of a regression). The point at which the function
    has its minima (minimizing cost/error) or its maxima (maximizing log likelihood)
    is where the best solution with minimal error (the best approximation) exists.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数统计机器学习算法在定义和搜索域空间时使用成本或误差函数来得到最佳的数值近似解（例如，回归的参数）。函数达到最小值（最小化成本/误差）或最大值（最大化对数似然）的点是最佳解（最佳近似）存在的地方，误差最小。
- en: A quick refresher for differentiation rules can be found at: [https://en.wikipedia.org/wiki/Differentiation_rules](https://en.wikipedia.org/wiki/Differentiation_rules) [and ](https://en.wikipedia.org/wiki/Differentiation_rules)[https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf](https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在以下网址找到微分规则的快速复习：[https://en.wikipedia.org/wiki/Differentiation_rules](https://en.wikipedia.org/wiki/Differentiation_rules) [和](https://en.wikipedia.org/wiki/Differentiation_rules)[https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf](https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf)
- en: A more mathematical write up for minimizing quadratic functions can be found
    at: [http://www.cis.upenn.edu/~cis515/cis515-11-sl12.pdf](http://www.cis.upenn.edu/~cis515/cis515-11-sl12.pdf)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在以下网址找到有关最小化二次函数的更多数学写作：[http://www.cis.upenn.edu/~cis515/cis515-11-sl12.pdf](http://www.cis.upenn.edu/~cis515/cis515-11-sl12.pdf)
- en: A scientific write up for quadratic function optimization and forms from MIT
    can be found at: [https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec4_quad_form.pdf](https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec4_quad_form.pdf)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在MIT找到有关二次函数优化和形式的科学写作：[https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec4_quad_form.pdf](https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec4_quad_form.pdf)
- en: See also
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: A lengthy write up for quadratic equation from UCSC can be found at: [https://people.ucsc.edu/~miglior/chapter%20pdf/Ch08_SE.pdf](https://people.ucsc.edu/~miglior/chapter%20pdf/Ch08_SE.pdf)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在UCSC找到有关二次方程的详细写作：[https://people.ucsc.edu/~miglior/chapter%20pdf/Ch08_SE.pdf](https://people.ucsc.edu/~miglior/chapter%20pdf/Ch08_SE.pdf)
- en: 'Quadratic functions can be expressed as one of the following forms:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二次函数可以表示为以下形式之一：
- en: '| **Quadratic function ax² + bx + c form** | **Standard form of quadratic function**
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **二次函数ax² + bx + c形式** | **二次函数的标准形式** |'
- en: '| ![](img/00212.gif) | ![](img/00213.gif) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/00212.gif) | ![](img/00213.gif) |'
- en: Where *a, b* and *c* are real numbers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*a，b*和*c*是实数。
- en: 'The following figure provides a quick reference to minima/maxima and the parameters
    regulating the convex/concave look and feel of the function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下图提供了最小值/最大值和参数的快速参考，这些参数调节了函数的凸/凹外观和感觉：
- en: '![](img/00214.jpeg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00214.jpeg)'
- en: Coding a quadratic cost function optimization using Gradient Descent (GD) from
    scratch
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始编写使用梯度下降（GD）的二次成本函数优化
- en: In this recipe, we will code an iterative numerical optimization technique called
    gradient descent (GD) to find the minimum of a quadratic function *f(x) = 2x²
    - 8x +9*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将编写一个名为梯度下降（GD）的迭代数值优化技术，以找到二次函数*f(x) = 2x² - 8x +9*的最小值。
- en: The focus here shifts from using math to solve for the minima (setting the first
    derivative to zero) to an iterative numerical method called Gradient Descent (GD)
    which starts with a guess and then gets closer to the solution in each iteration
    using a cost/error function as the guideline.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重点从使用数学来解决最小值（将第一导数设置为零）转移到了一种名为梯度下降（GD）的迭代数值方法，该方法从一个猜测开始，然后在每次迭代中使用成本/误差函数作为指导方针逐渐接近解决方案。
- en: How to do it...
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: Set up the path using the package directive: `package spark.ml.cookbook.chapter9`.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用包指令设置路径：`package spark.ml.cookbook.chapter9`。
- en: Import the necessary packages.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包。
- en: The `scala.util.control.Breaks` will allow us to break out of the program. We
    use this during the debugging phase only when the program fails to converge or
    gets stuck in a never ending process (for example, when the step size is too large).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`scala.util.control.Breaks`将允许我们跳出程序。我们仅在调试阶段使用它，当程序无法收敛或陷入永无止境的过程中（例如，当步长过大时）。'
- en: '[PRE0]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This step defines the actual quadratic function that we are trying to minimize:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这一步定义了我们试图最小化的实际二次函数：
- en: '[PRE1]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This step defines the derivative of the function. It is what is referred to
    as the gradient at point x. It is the first order derivative of function *f(x)
    = 2x^2 - 8x + 9*.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这一步定义了函数的导数。这被称为点x处的梯度。这是函数*f(x) = 2x^2 - 8x + 9*的一阶导数。
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this step, we setup a random starting point (set to 13 here). This would
    become our initial starting point on the *x* axis.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们设置一个随机起始点（这里设置为13）。这将成为我们在*x*轴上的初始起始点。
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We proceed to set up the actual minima calculated from the previous recipe,
    *O**ptimizing a quadratic cost function and finding the minima using just math
    to gain insight*, so we can compute our estimate with each iteration versus actual.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们继续设置从上一个配方*优化二次成本函数并仅使用数学来获得洞察力找到最小值*计算出的实际最小值，以便我们可以计算我们的估计与实际的每次迭代。
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This point is trying to act as the labels that you would provide during the
    training phase of an ML algorithm. In a real-life setting, we would have a training
    dataset with labels and would let the algorithm train and adjust its parameters
    accordingly.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点试图充当您在ML算法的训练阶段提供的标签。在现实生活中，我们会有一个带有标签的训练数据集，并让算法进行训练并相应地调整其参数。
- en: 'Set up bookkeeping variables and declare `ArrayBuffer` data structures to store
    the cost (error) plus the estimated minima for inspection and graphing:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置记录变量并声明`ArrayBuffer`数据结构以存储成本（错误）加上估计的最小值，以便进行检查和绘图：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The internal control variables for the gradient descent algorithm get set in
    this step:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度下降算法的内部控制变量在这一步中设置：
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `stepSize`, also referred to as the learning rate, guides the program in
    how much to move each time, while tolerance helps the algorithm to stop when we
    get sufficiently close to the minima.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`stepSize`，也被称为学习率，指导程序每次移动多少，而容差帮助算法在接近最小值时停止。'
- en: 'We first set up a loop to iterate and stop when we are close enough to the
    minima, based on the desired tolerance:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先设置一个循环来迭代，并在接近最小值时停止，基于期望的容差：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We update the minima each time and call on the function to calculate and return
    the derivative value at the current updated point:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们每次更新最小值并调用函数计算并返回当前更新点的导数值：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We decide how much to move by first taking the derivative value returned by
    the last step and multiplying that by the step size (that is, we scale it). We
    then proceed to update the current minima and decrease it by the movement (derivative
    value x step size):'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们决定移动多少，首先通过取上一步返回的导数值，然后将其乘以步长（即，我们对其进行缩放）。然后我们继续更新当前最小值并减少它的移动（导数值x步长）：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We compute our cost function value (error) by using a very simple square distance
    formula. In real life, the actual minima will be derived from training, but here
    we use the value from the previous recipe, *Optimizing a quadratic cost function
    and finding the minima using just math to gain insight*.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过使用一个非常简单的平方距离公式来计算我们的成本函数值（错误）。在现实生活中，实际的最小值将从训练中得出，但在这里我们使用上一个配方*优化二次成本函数并仅使用数学来获得洞察力找到最小值*中的值。
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We produce some intermediate output results so you can observe the behavior
    of currentMinimum at each iteration:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们生成一些中间输出结果，以便您观察每次迭代时currentMinimum的行为：
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output will be as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following statement is included as a reminder that, regardless of how an
    optimization algorithm is implemented, it should always provide means for exiting
    from a non-converging algorithm (that is, it should guard against user input and
    edge cases):'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下声明包括一个提醒，即使优化算法如何实现，它也应始终提供退出非收敛算法的手段（即，它应防范用户输入和边缘情况）：
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output cost and minima vectors that we collected in each iteration for
    later analysis and graphing are:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在每次迭代中收集的成本和最小值向量的输出，以供以后分析和绘图：
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We define and set the variables that are the final minima and the actual function
    value *f(minima)*. They act as (X,Y) for where the minima is located:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义并设置最终最小值和实际函数值*f(minima)*的变量。它们充当最小值的(X,Y)位置：
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We print our final results that match our computation in the recipe, *Optimizing
    a quadratic cost function and finding the minima using just math to gain insight*,
    using the iterative method. The final output should read as our minimum is located
    as (2,1), which can be either visually or computationally checked via the recipe,
    *Optimizing a quadratic cost function and finding the minima using just math to
    gain insight*.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印出最终结果，与我们在配方中的计算匹配，*优化二次成本函数并仅使用数学来获得洞察力找到最小值*，使用迭代方法。最终输出应该是我们的最小值位于(2,1)，可以通过视觉或计算通过配方*优化二次成本函数并仅使用数学来获得洞察力找到最小值*进行检查。
- en: '[PRE17]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE18]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The process finished with the exit code 0
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程以退出码0完成
- en: How it works...
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The Gradient Descent technique takes advantage of the fact that the gradient
    of the function (first derivative in this case) points to the direction of the
    descent. Conceptually, Gradient Descent (GD) optimizes a cost or error function
    to search for the best parameter for the model. The following figure demonstrates
    the iterative nature of the gradient descent:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降技术利用了函数的梯度（在这种情况下是一阶导数）指向下降方向的事实。概念上，梯度下降（GD）优化成本或错误函数以搜索模型的最佳参数。下图展示了梯度下降的迭代性质：
- en: '![](img/00215.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00215.jpeg)'
- en: We started the recipe by defining the step size (the learning rate), tolerance,
    the function to be differentiated, and the function's first derivative, and then
    proceeded to iterate and get closer to the target minima of zero from the initial
    guess (13 in this case).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过定义步长（学习率）、容差、要进行微分的函数以及函数的一阶导数来开始配方，然后继续迭代并从初始猜测（在这种情况下为13）接近目标最小值0。
- en: At each iteration, we calculated the gradient of the point (the first derivative
    at that point) and then scaled it using the step size to regulate the amount of
    each move. Since we are descending, we subtracted the scaled gradient from the
    old point to find the next point closer to the solution (to minimize the error).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们计算了点的梯度（该点的一阶导数），然后使用步长对其进行缩放，以调节每次移动的量。由于我们在下降，我们从旧点中减去了缩放的梯度，以找到下一个更接近解决方案的点（以最小化误差）。
- en: There is some confusion as to whether the gradient value should be added or
    subtracted to arrive at the new point, which we try to clarify next. The guiding
    principle should be whether the slope is negative or positive. To move in the
    right direction, you must move in the direction of the first derivative (gradient).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 关于梯度值是应该加还是减以到达新点存在一些混淆，我们将在下面尝试澄清。指导原则应该是斜率是负还是正。为了朝着正确的方向移动，您必须朝着第一导数（梯度）的方向移动。
- en: 'The following table and figure provide a guideline for the GD update step:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格和图表提供了GD更新步骤的指南：
- en: '| ![](img/00216.gif) ***< 0****Negative Gradient* | ![](img/00217.gif) ***>
    0****Positive Gradient* |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/00216.gif) ***< 0****负梯度* | ![](img/00217.gif) ***> 0****正梯度* |'
- en: '| ![](img/00218.gif) | ![](img/00219.gif) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/00218.gif) | ![](img/00219.gif) |'
- en: '![](img/00220.jpeg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00220.jpeg)'
- en: The following figure depicts the inner workings of a single step (the negative
    slope depicted) in which we either subtract or add the gradient from the starting
    point to arrive at the next point that will put us one step closer to the minimum
    of the quadratic function. For example, in this recipe, we start from 13 and after
    200+ iterations (depends on the learning rate), we end up at the minima of (2,1),
    which matches the solution found in the recipe, *Optimizing a quadratic cost function
    and finding the minima using just math to gain insight* of this chapter.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了单个步骤（负斜率）的内部工作，我们要么从起始点减去梯度，要么加上梯度，以到达下一个点，使我们离二次函数的最小值更近一步。例如，在这个配方中，我们从13开始，经过200多次迭代（取决于学习率），最终到达（2,1）的最小值，这与本章中的配方*优化二次成本函数并仅使用数学来获得洞察*中找到的解决方案相匹配。
- en: '![](img/00221.jpeg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00221.jpeg)'
- en: To understand the steps better, let's try to follow a step from the left-hand
    side of the preceding graph for a simple ![](img/00222.gif) function. In this
    case, we are on the left-hand side of the curve (the original guess was a negative
    number) and we are trying to climb down and increase X with each iteration in
    the direction of the gradient (the first derivative)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这些步骤，让我们尝试从前图的左侧跟随一个步骤，对于一个简单的![](img/00222.gif)函数。在这种情况下，我们位于曲线的左侧（原始猜测是负数），并且我们试图在每次迭代中向下爬升并增加X，朝着梯度（一阶导数）的方向。
- en: 'The following steps will walk you through the next figure to demonstrate the
    core concept and the steps in the recipe:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将引导您浏览下一个图表，以演示核心概念和配方中的步骤：
- en: Calculate the derivative at the given point--the gradient.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在给定点计算导数--梯度。
- en: Use the gradient from step 1 and scale it by step size--the amount of the move.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用步骤1中的梯度，并按步长进行缩放--移动的量。
- en: 'Find the new position by subtracting the amount of movement:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过减去移动量找到新位置：
- en: '**Negative gradient case**: In the following figure, we subtract the negative
    gradient (effectively adding the gradient) to the original point so end up climbing
    down toward the minima of ![](img/00223.gif)at zero. The graph depicted in the
    figure matches this case.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负梯度情况**：在下图中，我们减去负梯度（有效地加上梯度）到原始点，以便向下爬升到![](img/00223.gif)的最小值0。图中所示的曲线符合这种情况。'
- en: '**Positive gradient case**: If we were on the other side of the curve with
    the positive gradient, we then subtract the positive gradient number from the
    previous position (effectively subtracting the gradient) to climb down toward
    the minima. The code in this recipe matches this case, in which we are trying
    to start from a positive number 13 (the initial guess) and move toward the minima
    at 0 in an iterative fashion.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正梯度情况**：如果我们在曲线的另一侧，梯度为正，那么我们从先前位置减去正梯度数（有效减小梯度）以向下爬向最小值。本配方中的代码符合这种情况，我们试图从正数13（初始猜测）开始，并以迭代方式向0的最小值移动。'
- en: Update the parameters and move to the new point.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新参数并移动到新点。
- en: We keep repeating these steps untill we converge to a solution and, hence, minimize
    the function.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不断重复这些步骤，直到收敛到解决方案，从而最小化函数。
- en: '![](img/00224.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00224.jpeg)'
- en: It is important to note that Gradient Descent (GD) and its variations use the
    first order derivative, which means that they are curvature ignorant, while the
    second order derivative algorithms such as Newton or quasi-Newton (BFGS, LBFGS)
    methods use both gradient and curvature with or without a Hessian matrix (a partial
    directives matrix with respect to each variable).
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重要的是要注意，梯度下降（GD）及其变体使用一阶导数，这意味着它们忽略曲率，而牛顿或拟牛顿（BFGS，LBFGS）方法等二阶导数算法使用梯度和曲率，有时还使用海森矩阵（相对于每个变量的部分导数矩阵）。
- en: The alternative to GD would be to search the entire domain space for the optimal
    setting, which is neither practical, nor will it ever terminate in a practical
    sense, due to the size and scale of real-life big data ML problems.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: GD的替代方案将是在整个域空间中搜索最佳设置，这既不切实际，也永远不会在实际意义上终止，因为真实大数据机器学习问题的规模和范围。
- en: There's more...
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The step size or learning rate is very important to master when you first start
    with GD. If the step size is too small, it results in computational wastage and
    gives the appearance that the gradient descent is not converging to a solution.
    While setting the step size is trivial for demos and small projects, setting it
    to a wrong value can lead to a high computational loss on large ML projects. On
    the other hand, if the step size is too large, we end up with a ping-pong situation
    or moving away from convergences that usually shows up as a blown up error curve,
    meaning that the error increases rather than decreasing with each iteration.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当你刚开始使用GD时，掌握步长或学习率非常重要。如果步长太小，会导致计算浪费，并给人一种梯度下降不收敛到解决方案的错觉。虽然对于演示和小项目来说设置步长是微不足道的，但将其设置为错误的值可能会导致大型ML项目的高计算损失。另一方面，如果步长太大，我们就会陷入乒乓情况或远离收敛，通常表现为误差曲线爆炸，意味着误差随着每次迭代而增加，而不是减少。
- en: In our experience, it is best to look at the error versus iteration chart and
    use the knee to pinpoint the right value. The alternative is to try .01, .001,......0001
    and see how the convergence proceeds with each iteration (steps too small or too
    large). It is helpful to remember that the step size is just a scaling factor,
    since the actual gradient at the point might be too large for the movement (it
    will jump over the minimum).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，最好查看误差与迭代图表，并使用拐点来确定正确的值。另一种方法是尝试 .01, .001,......0001，并观察每次迭代的收敛情况（步长太小或太大）。值得记住的是，步长只是一个缩放因子，因为在某一点的实际梯度可能太大而无法移动（它会跳过最小值）。
- en: 'To summarize:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：
- en: If the step size is too small, then you have slow convergence.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果步长太小，收敛速度就会很慢。
- en: If the step size is too large, you will end up skipping the minima (over-shooting),
    resulting in either slow computation or a ping-pong effect (getting stuck).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果步长太大，你会跳过最小值（过冲），导致计算缓慢或出现乒乓效应（卡住）。
- en: The following figure depicts variations based on different step sizes to demonstrate
    the points mentioned previously.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了基于不同步长的变化，以演示前面提到的要点。
- en: '**Scenario 1**: Step size = .01 - The step size is appropriate - just a bit
    too small but it gets the job done in around 200 iterations. We don''t like to
    see anything under 200 because it must be general purpose enough to survive in
    real life.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**场景1**：步长= .01 - 步长适中 - 只是稍微有点小，但在大约200次迭代中完成了任务。我们不希望看到任何少于200的情况，因为它必须足够通用以在现实生活中生存。'
- en: '**Scenario 2**: Step size = .001 - The step size is too small and results in
    slow convergence. While it does not seem to be that bad (1,500+ iterations), it
    might be considered too fine grained.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**场景2**：步长= .001 - 步长太小，导致收敛速度缓慢。虽然看起来并不那么糟糕（1,500+次迭代），但可能被认为太细粒度了。'
- en: '**Scenario 3**: Step size = .05 - The step size is too large. In this case,
    the algorithm gets stuck and keeps going back and forth without ever converging.
    It can''t be emphasized enough that you must think of an exit policy in case this
    occurs in real life (the nature and distribution of data changes a lot, so be
    prepared).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**场景3**：步长= .05 - 步长太大了。在这种情况下，算法会陷入困境，来回徘徊而无法收敛。不能再强调了，你必须考虑在现实生活中出现这种情况时的退出策略（数据的性质和分布会发生很大变化，所以要有所准备）。'
- en: '**Scenario 4**: Step size = .06 - The step size is too large resulting in non-convergence
    and blow up. The error curve blows up (it increases in a non-linear way) meaning
    that errors get larger with each iteration rather than getting smaller. In practice,
    we see more of this case (scenario 4) than the previous scenario, but both can
    happen so you should be ready for both. As you can see, a small .01 difference
    in step size between scenarios 3 and 4 made a difference in how the GD behaved.
    This is the same problem (optimization) that makes algorithmic trading difficult.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**场景4**：步长= .06 - 步长太大，导致不收敛和爆炸。误差曲线爆炸（以非线性方式增加），意味着误差随着每次迭代而变大，而不是变小。在实践中，我们看到这种情况（场景4）比之前的情况更多，但两种情况都可能发生，所以你应该为两种情况做好准备。正如你所看到的，场景3和场景4之间步长的微小差异造成了梯度下降行为的不同。这也是使算法交易困难的相同问题（优化）。'
- en: '![](img/00225.jpeg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00225.jpeg)'
- en: It is worth mentioning that, for this type of smooth convex optimization problem,
    the local minima are often the same as the global minima. You can think of local
    minima/maxima as extreme values within a given range. For the same function, the
    global minima/maxima refers to the global or the most absolute value in the entire
    range of the function.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，对于这种光滑凸优化问题，局部最小值通常与全局最小值相同。你可以将局部最小值/最大值视为给定范围内的极值。对于同一函数，全局最小值/最大值指的是函数整个范围内的全局或最绝对值。
- en: See also
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'Stochastic Gradient Descent: There are multiple variations of Gradient Descent
    (GD), with Stochastic Gradient Descent (SGD) being the most talked about. Apache
    Spark supports the Stochastic Gradient Descent (SGD) variation, in which we update
    the parameters with a subset of training data - which is a bit challenging since
    we need to update the parameters simultaneously. There are two main differences
    between SGD and GD. The first difference is that SGD is an online learning/optimization
    technique while GD is more of an offline learning/optimization technique. The
    second difference between SGD versus GD is the speed of convergence due to not
    needing to examine the entire dataset before updating any parameter. This difference
    is depicted in the following figure:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降：梯度下降（GD）有多种变体，其中随机梯度下降（SGD）是最受关注的。Apache Spark支持随机梯度下降（SGD）变体，其中我们使用训练数据的子集来更新参数
    - 这有点具有挑战性，因为我们需要同时更新参数。SGD与GD之间有两个主要区别。第一个区别是SGD是一种在线学习/优化技术，而GD更多是一种离线学习/优化技术。SGD与GD之间的第二个区别是由于不需要在更新任何参数之前检查整个数据集，因此收敛速度更快。这一区别在下图中有所体现：
- en: '![](img/00226.jpeg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00226.jpeg)'
- en: We can set the batch-window size in Apache Spark to make the algorithm more
    responsive to massive datasets (there is no need to traverse the whole dataset
    at once). There will be some randomness associated with SGD, but overall it is
    the "de facto" method used these days. It is a lot faster and can converge much
    faster.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在Apache Spark中设置批处理窗口大小，以使算法对大规模数据集更具响应性（无需一次遍历整个数据集）。SGD会有一些与之相关的随机性，但总体上它是当今使用的“事实标准”方法。它速度更快，收敛速度更快。
- en: In both GD and SGD cases, you search for the best parameter for the model by
    updating the original parameters. The difference is that, in the core GD, you
    have to run through all your data points to do a single update for a parameter
    in a given iteration as opposed to SGD, in which you look at each single (or mini-batch)
    sample from the training dataset to update the parameters.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在GD和SGD的情况下，您通过更新原始参数来寻找模型的最佳参数。不同之处在于，在核心GD中，您必须遍历所有数据点以在给定迭代中对参数进行单次更新，而在SGD中，您需要查看来自训练数据集的每个单个（或小批量）样本以更新参数。
- en: 'For a short general purpose write up, a good place to start is with the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简短的通用写作，一个好的起点是以下内容：
- en: GD :[https://en.wikipedia.org/wiki/Gradient_descent](https://en.wikipedia.org/wiki/Gradient_descent)
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GD :[https://en.wikipedia.org/wiki/Gradient_descent](https://en.wikipedia.org/wiki/Gradient_descent)
- en: SGD:[https://en.wikipedia.org/wiki/Stochastic_gradient_descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SGD:[https://en.wikipedia.org/wiki/Stochastic_gradient_descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
- en: 'A more mathematical treatment from CMU, Microsoft, and the Journal of Statistical
    software can be found at:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在CMU、微软和统计软件杂志中找到更多数学处理：
- en: 'CMU: [https://www.cs.cmu.edu/~ggordon/10725-F12/slides/05-gd-revisited.pdf](https://www.cs.cmu.edu/~ggordon/10725-F12/slides/05-gd-revisited.pdf)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CMU: [https://www.cs.cmu.edu/~ggordon/10725-F12/slides/05-gd-revisited.pdf](https://www.cs.cmu.edu/~ggordon/10725-F12/slides/05-gd-revisited.pdf)'
- en: '[MS :](https://www.cs.cmu.edu/~ggordon/10725-F12/slides/05-gd-revisited.pdf)
    [http://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf](http://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MS :](https://www.cs.cmu.edu/~ggordon/10725-F12/slides/05-gd-revisited.pdf)
    [http://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf](http://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf)'
- en: Jstat:[https://arxiv.org/pdf/1509.06459v1.pdf](https://arxiv.org/pdf/1509.06459v1.pdf)
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jstat:[https://arxiv.org/pdf/1509.06459v1.pdf](https://arxiv.org/pdf/1509.06459v1.pdf)
- en: Coding Gradient Descent optimization to solve Linear Regression from scratch
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写梯度下降优化来解决线性回归问题
- en: In this recipe, we will explore how to code Gradient Descent to solve a Linear
    Regression problem. In the previous recipe, we demonstrated how to code GD to
    find the minimum of a quadratic function.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将探讨如何编写梯度下降来解决线性回归问题。在上一个示例中，我们演示了如何编写GD来找到二次函数的最小值。
- en: This recipe demonstrates a more realistic optimization problem in which we optimize
    (minimize) the least square cost function to solve the linear regression problem
    in Scala on Apache Spark 2.0+. We will use real data and run our algorithm and
    compare the result to a tier-1 commercially available statistic software to demonstrate
    accuracy and speed.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例演示了一个更现实的优化问题，我们通过Scala在Apache Spark 2.0+上优化（最小化）最小二乘成本函数来解决线性回归问题。我们将使用真实数据运行我们的算法，并将结果与一流的商业统计软件进行比较，以展示准确性和速度。
- en: How to do it...
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We start by downloading the file from Princeton University which contains the
    following data:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从普林斯顿大学下载包含以下数据的文件：
- en: '![](img/00227.jpeg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00227.jpeg)'
- en: 'Source: Princeton University'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：普林斯顿大学
- en: Download source: [http://data.princeton.edu/wws509/datasets/#salary](http://data.princeton.edu/wws509/datasets/#salary).
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载源码：[http://data.princeton.edu/wws509/datasets/#salary](http://data.princeton.edu/wws509/datasets/#salary).
- en: 'To keep things simple, we then select the `yr` and `sl` to study how the number
    of years in rank influences the salary. To cut down on data wrangling code, we
    save those two columns in a file (`Year_Salary.csv`), as depicted in the following
    table, to study their linear relationship:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了简化问题，我们选择`yr`和`sl`来研究年级对薪水的影响。为了减少数据整理代码，我们将这两列保存在一个文件中（`Year_Salary.csv`），如下表所示，以研究它们的线性关系：
- en: '![](img/00228.gif)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00228.gif)'
- en: We visually inspect the data by using a scatter plot from the IBM SPSS package.
    It cannot be emphasized enough that a visual inspection should be the first step
    in any data science project.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用IBM SPSS软件的散点图来直观地检查数据。不能再次强调，视觉检查应该是任何数据科学项目的第一步。
- en: '![](img/00229.jpeg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00229.jpeg)'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'We use the import package to place the code in the desired place:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用import包将代码放在所需的位置：
- en: '`package spark.ml.cookbook.chapter9`.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`package spark.ml.cookbook.chapter9`.'
- en: 'The first four statements import the necessary packages for the JFree chart
    package so we can graph the GD error and convergence in the same code base. The
    fifth import takes care of `ArrayBuffer`, which we use to store intermediate results:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 前四个语句导入了JFree图表包的必要包，以便我们可以在同一代码库中绘制GD错误和收敛。第五个导入处理`ArrayBuffer`，我们用它来存储中间结果：
- en: '[PRE19]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define the data structure to hold intermediate results as we minimize errors
    and converge to a solution for the slope (`mStep`) and intercept (`bStep`):'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据结构以保存中间结果，因为我们最小化错误并收敛到斜率（`mStep`）和截距（`bStep`）的解决方案：
- en: '[PRE20]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Define the functions for graphing via the JFree chart. The first one just displays
    the chart and the second one sets the chart properties. This is a boiler-plate
    code that you can customize based on your preference:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义通过JFree图表进行绘图的函数。第一个函数只显示图表，第二个函数设置图表属性。这是一个模板代码，您可以根据自己的喜好进行自定义：
- en: '[PRE21]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This function computes the error based on the least square principle which we
    minimize to find the best fitting solution. The function finds the difference
    between what we predict and what the actual value (the salary) is available via
    the training data. After finding the difference, it squares them to compute the
    total error. The pow() function is a Scala math function to compute the square.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该函数基于最小二乘原理计算错误，我们最小化该错误以找到最佳拟合解决方案。该函数找到我们预测的值与训练数据中实际值（薪水）之间的差异。找到差异后，对其进行平方以计算总错误。pow()函数是一个Scala数学函数，用于计算平方。
- en: '![](img/00230.jpeg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00230.jpeg)'
- en: 'Source: Wikipedia'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：维基百科
- en: '[PRE22]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The next function computes two gradients (the first derivative) of the *f(x)=
    b + mx* and averages them over the domain (all points). It is the same process
    as in the second recipe, except that we need partial derivatives (gradient) because
    we are minimizing two parameters `m` and `b` (slope and intercept) and not just
    one.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个函数计算*f(x)= b + mx*的两个梯度（一阶导数），并在整个定义域（所有点）上对它们进行平均。这与第二个配方中的过程相同，只是我们需要偏导数（梯度），因为我们要最小化两个参数`m`和`b`（斜率和截距），而不仅仅是一个参数。
- en: In the last two lines, we scale the gradient by multiplying it by the learning
    rate (step size). The reason we do that is to make sure we don't end up with large
    step sizes and overshoot the minimum, resulting in either a ping-pong scenario
    or error blow up, as discussed in the previous recipe.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后两行中，我们通过学习率（步长）将梯度进行缩放。我们这样做的原因是为了确保我们不会得到很大的步长，并超过最小值，导致出现乒乓情景或错误膨胀，正如前面的配方中所讨论的那样。
- en: '[PRE23]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This function reads and parses the CSV file:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该函数读取并解析CSV文件：
- en: '[PRE24]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The following is a wrapper function that loops for N number of iterations and
    calls on the `step_gradient()` function to compute the gradient at a given point.
    We then proceed to store the results from every step one by one for processing
    later (for example, graphing).
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是一个包装函数，它循环N次迭代，并调用`step_gradient()`函数来计算给定点的梯度。然后，我们继续逐步存储每一步的结果，以便以后处理（例如绘图）。
- en: Noteworthy is the use of `Tuple2()` to hold the return value from the `step_gradient()`
    function.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是使用`Tuple2()`来保存`step_gradient()`函数的返回值。
- en: In the last steps of the function we call on the `compute_error_for_line_given_points()`
    function to compute the error for a given combination of the slope and intercept
    and store it in `gradientStepError`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数的最后几步中，我们调用`compute_error_for_line_given_points()`函数来计算给定斜率和截距组合的错误，并将其存储在`gradientStepError`中。
- en: '[PRE25]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The last and final step is the main program, which sets up the initial starting
    point for the slope, intercept, number of iterations, and learning rate. We purposely
    choose a smaller learning rate and larger number of iterations to demonstrate
    accuracy and speed.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是主程序，它设置了斜率、截距、迭代次数和学习率的初始起点。我们故意选择了较小的学习率和较大的迭代次数，以展示准确性和速度。
- en: First, we start by the initialization of the key controlling variables for GD
    (learning rate, number of iterations, and starting point).
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从初始化GD的关键控制变量开始（学习率、迭代次数和起始点）。
- en: Second, we proceed to show the starting point (0,0) and call on `compute_error_for_line_given_points()`
    to show the starting error. It should be noted that the error should be lower
    after we are done running through the GD and display the result in the final step.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，我们继续显示起始点（0,0），并调用`compute_error_for_line_given_points()`来显示起始错误。值得注意的是，经过GD运行后，错误应该更低，并在最后一步显示结果。
- en: Third, we set up necessary calls and structures for the JFree chart to display
    two charts that depict the slope, intercept, and error behavior as we merge toward
    an optimized solution (the best combination of the slope and intercept to minimize
    the error).
  id: totrans-219
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三，我们为JFree图表设置必要的调用和结构，以显示两个图表，描述斜率、截距和错误的行为，当我们朝着优化解决方案（最小化错误的最佳斜率和截距组合）合并时。
- en: '[PRE26]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The following is the output for this recipe.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是此配方的输出。
- en: 'First we display the starting point of 0,0 with the error of 6.006 and then
    allow the algorithm to run and display the result after completing the number
    of iterations:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们显示起始点为0,0，错误为6.006，然后允许算法运行，并在完成迭代次数后显示结果：
- en: '![](img/00231.jpeg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00231.jpeg)'
- en: Noteworthy is the starting and ending error number and how it reduced over time
    due to optimization.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是起始和结束的错误数字以及由于优化而随时间减少。
- en: We used IBM SPSS as a control point to show that the GD algorithm that we put
    together matches the result (almost 1:1) produced by the SPSS package - it is
    almost exact!
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用IBM SPSS作为控制点，以显示我们组合的GD算法与SPSS软件生成的结果（几乎1:1）几乎完全相同！
- en: 'The following figure shows the output from IBM SPSS for comparing the results:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了IBM SPSS的输出，用于比较结果：
- en: '![](img/00232.jpeg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00232.jpeg)'
- en: In the last step, two charts are produced by the program side by side.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最后一步，程序并排生成了两个图表。
- en: 'The following figure shows how slope *(m)* and intercept (*b*) converge toward
    the best combination that minimizes the error as we run through the iterations:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了斜率（*m*）和截距（*b*）是如何朝着最小化错误的最佳组合收敛的，当我们通过迭代运行时。
- en: '![](img/00233.jpeg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00233.jpeg)'
- en: The following figure shows how slope (*m*) and intercept (*b*) converge toward
    the best combination that minimizes the error as we run through the iterations.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了斜率（*m*）和截距（*b*）是如何朝着最小化错误的最佳组合收敛的，当我们通过迭代运行时。
- en: '![](img/00234.jpeg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00234.jpeg)'
- en: How it works...
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Gradient Descent is an iterative numerical method that starts from an initial
    guess and then asks itself how badly am I doing by looking at an error function
    that is the squared distance of predicted versus actual data in the training file.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种迭代的数值方法，它从一个初始猜测开始，然后通过查看一个错误函数来询问自己，我做得有多糟糕，这个错误函数是训练文件中预测数据与实际数据的平方距离。
- en: In this program, we selected a simple linear line *f(x) = b + mx* equation as
    our model. To optimize and come up with the best combination of slope m, intercept
    b for our model, we had 52 actual pairs of data (age, salary) that we can plug
    into our linear model (*Predicted Salary = Slope x Age + Intercept*). In short,
    we wanted to find the best combination of the slope and intercept that helped
    us fit a linear line that minimizes the squared distance. The squared function
    gives us all positive values and lets us concentrate on the magnitude of the error
    only.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个程序中，我们选择了一个简单的线性方程*f(x) = b + mx*作为我们的模型。为了优化并找出最佳的斜率m和截距b的组合，我们有52对实际数据(年龄，工资)可以代入我们的线性模型(*预测工资=斜率x年龄+截距*)。简而言之，我们想要找到最佳的斜率和截距的组合，帮助我们拟合一个最小化平方距离的线性线。平方函数给我们所有正值，并让我们只关注错误的大小。
- en: '`ReadCSV()`: Reads and parses the data file in to our datasets:'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadCSV()`: 读取和解析数据文件到我们的数据集中：'
- en: '*(x[1], y[1]), (x[2], y[2]), (x[3], y[4]), ... (x[52], y[52])*'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '*(x[1], y[1]), (x[2], y[2]), (x[3], y[4]), ... (x[52], y[52])*'
- en: '`Compute_error_for_line_given_points()`: This function implements the cost
    or error function. We use a linear model (equation of a line) to predict, and
    then measure, the squared distance from the actual number. After adding up the
    errors, we average and return the total error:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Compute_error_for_line_given_points()`: 这个函数实现了成本或错误函数。我们使用一个线性模型(一条直线的方程)来预测，然后测量与实际数字的平方距离。在添加错误后，我们平均并返回总错误：'
- en: '![](img/00235.gif)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00235.gif)'
- en: 'y[i] = mx[i] + b : for all data pair (*x, y)*'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: y[i] = mx[i] + b：对于所有数据对(*x, y)*
- en: '![](img/00236.jpeg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00236.jpeg)'
- en: 'Noteworthy code within the function: the first line of code calculates the
    squared distance between predicted (*m * x + b*) and the actual (*y*). The second
    line of code averages it and returns it:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 函数内值得注意的代码：第一行代码计算了预测值(*m * x + b*)与实际值(*y*)之间的平方距离。第二行代码对其进行平均并返回：
- en: '![](img/00237.gif)*totalError += math.pow(y - (m * x + b), 2)**....**return
    totalError / points.length*'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00237.gif)*totalError += math.pow(y - (m * x + b), 2)**....**return
    totalError / points.length*'
- en: The following figure shows the basic concept of least squares. In short, we
    take the distance between what the actual training data was for a point versus
    what our model predicts, and square them and then add them up. The reason we square
    them is to avoid using the absolute value function `abs()` which is not computationally
    desirable. The squared difference has better mathematical properties by providing
    a continuously differentiable property which is preferred when you want to minimize
    it.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了最小二乘法的基本概念。简而言之，我们取实际训练数据与我们的模型预测之间的距离，然后对它们进行平方，然后相加。我们平方的原因是为了避免使用绝对值函数`abs()`，这在计算上是不可取的。平方差具有更好的数学性质，提供了连续可微的性质，这在想要最小化它时是更可取的。
- en: '![](img/00238.jpeg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00238.jpeg)'
- en: '`step_gradient()`: This function is where the gradient (the first derivative)
    is calculated using the current point we are iterating over (*x[i],y[i]).* It
    should be noted that, unlike the previous recipe, we have two parameters so we
    need to calculate partial derivatives for the intercept (`b_gradient`) and then
    the slope (`m_gradient`). We then need to divide by the number of points to average.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`step_gradient()`: 这个函数是计算梯度(一阶导数)的地方，使用我们正在迭代的当前点(*x[i],y[i])*。需要注意的是，与之前的方法不同，我们有两个参数，所以我们需要计算截距(`b_gradient`)和斜率(`m_gradient`)的偏导数。然后我们需要除以点的数量来求平均。'
- en: '![](img/00239.jpeg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00239.jpeg)'
- en: 'Using a partial derivative with respect to Intercept(*b*):'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用对截距(*b*)的偏导数：
- en: '*b_gradient += -(2 / N) * (y - ((m_current * x) + b_current))*'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '*b_gradient += -(2 / N) * (y - ((m_current * x) + b_current))*'
- en: 'Using a partial derivative with respect to Slope(*m*):'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用对斜率(*m*)的偏导数：
- en: '*m_gradient += -(2 / N) * x * (y - ((m_current * x) + b_current))*'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*m_gradient += -(2 / N) * x * (y - ((m_current * x) + b_current))*'
- en: The last step is to scale the calculated gradient by the learning-rate (step
    size) and then move to a newly estimated position for the slope (m_current) and
    intercept (b_current):*result(0) = b_current - (learningRate * b_gradient)**result(1)
    = m_current - (learningRate * m_gradient)*
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一步是通过学习率(步长)来缩放计算出的梯度，然后移动到斜率(m_current)和截距(b_current)的新估计位置：*result(0) =
    b_current - (learningRate * b_gradient)**result(1) = m_current - (learningRate
    * m_gradient)*
- en: '`gradient_descent_runner()`: This is the driver that executes the `step_gradient()`
    and `compute_error_for_line_given_points()` for the number of iterations defined:'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_descent_runner()`: 这是执行`step_gradient()`和`compute_error_for_line_given_points()`的驱动程序，执行定义的迭代次数：'
- en: '[PRE27]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: There's more...
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: While this recipe was able to handle real-life data and match the estimation
    from a commercial package, in practice you need to implement Stochastic Gradient
    Descent.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个方法能够处理现实生活中的数据并与商业软件的估计相匹配，但在实践中，您需要实现随机梯度下降。
- en: Spark 2.0 offers Stochastic Gradient Descent (SGD) with a mini-batch window
    (for efficiency control).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0提供了带有小批量窗口的随机梯度下降(SGD)。
- en: Spark provides two approaches toward utilizing SGD. The first alternative is
    to use a standalone optimization technique in which you pass in your optimization
    function. See the following links: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.optimization.Optimizer](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.optimization.Optimizer) and [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.optimization.GradientDescent](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.optimization.GradientDescent)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了两种利用SGD的方法。第一种选择是使用独立的优化技术，您可以通过传入优化函数来使用。参见以下链接：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.optimization.Optimizer](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.optimization.Optimizer)和[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.optimization.GradientDescent](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.optimization.GradientDescent)
- en: 'The second alternative would be to use specialized APIs that have SGD built-in
    already as their optimization technique:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择是使用已经内置了SGD的专门API作为它们的优化技术：
- en: '`LogisticRegressionWithSGD()`'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LogisticRegressionWithSGD()`'
- en: '`StreamingLogisticRegressionWithSGD()`'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamingLogisticRegressionWithSGD()`'
- en: '`LassoWithSGD()`'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LassoWithSGD()`'
- en: '`LinearRegressionWithSGD()`'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LinearRegressionWithSGD()`'
- en: '`RidgeRegressionWithSGD()`'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RidgeRegressionWithSGD()`'
- en: '`SVMWithSGD()`'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SVMWithSGD()`'
- en: As of Spark 2.0, all RDD-based regression is in maintenance mode only.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 截至Spark 2.0，所有基于RDD的回归只处于维护模式。
- en: See also
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Optimization with Spark 2.0: [https://spark.apache.org/docs/latest/mllib-optimization.html#stochastic-gradient-descent-sgd](https://spark.apache.org/docs/latest/mllib-optimization.html#stochastic-gradient-descent-sgd)
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0的优化：[https://spark.apache.org/docs/latest/mllib-optimization.html#stochastic-gradient-descent-sgd](https://spark.apache.org/docs/latest/mllib-optimization.html#stochastic-gradient-descent-sgd)
- en: Normal equations as an alternative for solving Linear Regression in Spark 2.0
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正规方程作为解决Spark 2.0中线性回归的替代方法
- en: In this recipe, we present an alternative to Gradient Descent (GD) and LBFGS
    by using Normal Equations to solve linear regression. In the case of normal equations,
    you are setting up your regression as a matrix of features and vector of labels
    (dependent variables) while trying to solve it by using matrix operations such
    as inverse, transpose, and so on.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们提供了使用正规方程来解决线性回归的梯度下降（GD）和LBFGS的替代方法。在正规方程的情况下，您正在将回归设置为特征矩阵和标签向量（因变量），同时尝试通过使用矩阵运算（如逆、转置等）来解决它。
- en: The emphasis here is to highlight Spark's facility for using Normal Equations
    to solve Linear Regression and not the details of the model or generated coefficients.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 重点在于强调Spark使用正规方程来解决线性回归的便利性，而不是模型或生成系数的细节。
- en: How to do it...
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: We use the same housing dataset which we extensively covered in [Chapter 5](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77),
    *Practical Machine Learning with Regression and Classification in Spark 2.0 -
    Part I* and [Chapter 6](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77),
    *Practical Machine Learning with Regression and Classification in Spark 2.0 -
    Part II*, which relate various attributes (for example number of rooms, and so
    on) to the price of the house.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用了在[第5章](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77)和[第6章](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77)中广泛涵盖的相同的房屋数据集，这些章节分别是*Spark
    2.0中的回归和分类的实际机器学习-第I部分*和*Spark 2.0中的回归和分类的实际机器学习-第II部分*，它们将各种属性（例如房间数量等）与房屋价格相关联。
- en: The data is available as `housing8.csv` under the `Chapter 9` data directory.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可在`Chapter 9`数据目录下的`housing8.csv`中找到。
- en: 'We use the package directive to take care of the placement:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用package指令来处理放置：
- en: '[PRE28]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We then import the necessary libraries:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后导入必要的库：
- en: '[PRE29]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Reduce the extra output generated by Spark by setting the Logger information
    level to `Level.ERROR`:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Spark生成的额外输出减少，将Logger信息级别设置为`Level.ERROR`：
- en: '[PRE30]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Set up SparkSession with the appropriate attributes:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用适当的属性设置SparkSession：
- en: '[PRE31]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Read the input file and parse it into a dataset:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取输入文件并将其解析为数据集：
- en: '[PRE32]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Display the following dataset contents, but limit them to first three rows
    for inspection:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示以下数据集内容，但限制为前三行以供检查：
- en: '![](img/00240.gif)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00240.gif)'
- en: 'We create a LinearRegression object and set the number of iterations, ElasticNet,
    and Regularization parameters. The last step is to set the right solver methods
    by choosing `setSolver("normal")`:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个LinearRegression对象，并设置迭代次数、ElasticNet和正则化参数。最后一步是通过选择`setSolver("normal")`来设置正确的求解器方法：
- en: '[PRE33]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Please be sure to set the ElasticNet parameter to 0.0 for the "normal" solver
    to work.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保将ElasticNet参数设置为0.0，以便"normal"求解器正常工作。
- en: 'Fit the `LinearRegressionModel` to the data using:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下内容将`LinearRegressionModel`拟合到数据中：
- en: '[PRE34]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following output is generated when you run the program:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 运行程序时会生成以下输出：
- en: '[PRE35]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Readers can output more information, but the model summary was covered in [Chapter
    5](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77), *Practical Machine
    Learning with Regression and Classification in Spark 2.0 - Part I* and [Chapter
    6](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77), *Practical Machine
    Learning with Regression and Classification in Spark 2.0 - Part II* via other
    techniques.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可以输出更多信息，但模型摘要已在[第5章](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77)和[第6章](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77)中进行了覆盖，*Spark
    2.0中的回归和分类的实际机器学习-第I部分*和*Spark 2.0中的回归和分类的实际机器学习-第II部分*，通过其他技术。
- en: How it works...
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We are ultimately trying to solve the following equation for linear regression
    using the closed form formula:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终尝试使用封闭形式公式解决线性回归的以下方程：
- en: '![](img/00241.jpeg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00241.jpeg)'
- en: Spark provides an out-of-the box fully parallel method for solving this equation
    by allowing you to set the `setSolver("normal")`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Spark通过允许您设置`setSolver("normal")`提供了一个完全并行的解决这个方程的方法。
- en: There's more...
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: If you fail to set the ElasticNet parameter to 0.0, you will get an error because
    L2 regularization is used when solving through normal equations in Spark (as of
    this writing).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未将ElasticNet参数设置为0.0，则会出现错误，因为在Spark中通过正规方程求解时使用了L2正则化（截至目前）。
- en: Documentation for Spark 2.0 related to isotonic regression can be found at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression) and [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionModel)
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Spark 2.0中等稳定回归的文档可以在以下网址找到：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression) 和 [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionModel)
- en: The model summary can be found at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionSummary](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionSummary)
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要可以在以下链接找到：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionSummary](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionSummary)
- en: "[\uFEFF](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionSummary)"
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: "[\uFEFF](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionSummary)"
- en: See also
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Also refer to the following table:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以参考以下表格：
- en: '| Iterative methods(SGD, LBFGS) | Closed formNormal Equation |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 迭代方法（SGD，LBFGS） | 闭合形式正规方程 |'
- en: '| Choosing learning Rate | No parameter |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 选择学习率 | 无参数 |'
- en: '| Iterations can be large | Does not iterate |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 迭代次数可能很大 | 不迭代 |'
- en: '| Good performance on large feature sets | Slow and impractical on large feature
    sets |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 在大特征集上表现良好 | 在大特征集上速度慢且不实用 |'
- en: '| Error prone: getting stuck due to poor parameter selection | (x^Tx)^(-1)
    is computationally expensive - in the order of n³ |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 容易出错：由于参数选择不当而卡住 | (x^Tx)^(-1)的计算代价高 - 复杂度为n³ |'
- en: Here is a quick reference on configuration of the LinearRegression object, but
    be sure to see [Chapter 5](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part I* and [Chapter
    6](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77), *Practical Machine
    Learning with Regression and Classification in Spark 2.0 - Part II* for more details.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于LinearRegression对象配置的快速参考，但是请查看[第5章](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77)，*Spark
    2.0中的回归和分类的实用机器学习-第I部分*和[第6章](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77)，*Spark
    2.0中的回归和分类的实用机器学习-第II部分*以获取更多细节。
- en: 'L1: Lasso regression'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1：套索回归
- en: 'L2: Ridge regression'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2：岭回归
- en: 'L1 - L2: Elastic net in which you can adjust the dial'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 - L2：弹性网络，可以调整参数
- en: 'The following link is a write-up from Columbia University that explains normal
    equations as they relate to solving Linear Regression problems:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接是哥伦比亚大学的一篇文章，解释了正规方程与解决线性回归问题的关系：
- en: '[http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11](http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11](http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11)'
- en: '[Octave (](http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11)
    [https://www.gnu.org/software/octave/](https://www.gnu.org/software/octave/)[)
    from GNU is a popular matrix manipulation software and you should have it in your
    toolkit.](http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11)'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GNU的Octave（](http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11)
    [https://www.gnu.org/software/octave/](https://www.gnu.org/software/octave/)[)是一种流行的矩阵操作软件，你应该在工具包中拥有它。](http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11)'
- en: The following link contains a quick tutorial to get you started: [http://www.lauradhamilton.com/tutorial-linear-regression-with-octave](http://www.lauradhamilton.com/tutorial-linear-regression-with-octave)
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下链接包含一个快速教程，帮助你入门：[http://www.lauradhamilton.com/tutorial-linear-regression-with-octave](http://www.lauradhamilton.com/tutorial-linear-regression-with-octave)
