- en: Spark for Big Data Analytics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于大数据分析的Spark
- en: As the use of Hadoop and related technologies in the respective ecosystem gained
    prominence, a few obvious and salient deficiencies of the Hadoop operational model
    became apparent. In particular, the ingrained reliance on the MapReduce paradigm,
    and other facets related to MapReduce, made a truly functional use of the Hadoop
    ecosystem possible only for major firms that were invested deeply in the respective
    technologies.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Hadoop及其相关技术在各自生态系统中的使用日益突出，Hadoop操作模型的一些明显和显著的缺陷变得明显。特别是对MapReduce范式的根深蒂固的依赖以及与MapReduce相关的其他方面，使得Hadoop生态系统的真正功能性使用仅对深度投资于相关技术的主要公司可能。
- en: At the **UC Berkeley Electrical Engineering and Computer Sciences** (**EECS**)
    Annual Research Symposium of 2011, a vision for a new research group at the university
    was announced during a presentation by Prof. Ian Stoica ([https://amplab.cs.berkeley.edu/about/](https://amplab.cs.berkeley.edu/about/)).
    It laid out the foundation of what was to become a pivotal unit that would profoundly
    change the landscape of Big Data. The **AMPLab**, launched in February 2011, aimed
    to deliver a scalable and unified solution by integrating Algorithms, Machines,
    and People that could cater to future needs without requiring any major re-engineering
    efforts.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在2011年的**加州大学伯克利分校电气工程和计算机科学**（**EECS**）年度研究研讨会上，Ian Stoica教授在一次演讲中宣布了该大学一个新的研究小组的愿景（[https://amplab.cs.berkeley.edu/about/](https://amplab.cs.berkeley.edu/about/)）。它奠定了一个将深刻改变大数据格局的关键单位的基础。AMPLab于2011年2月成立，旨在通过整合算法、机器和人员提供可扩展和统一的解决方案，以满足未来的需求，而无需进行任何重大的重新设计工作。
- en: The most well-known and most widely used project to evolve from the AMPLab initiative
    was Spark, arguably a superior alternative - or more precisely, *extension* -
    of the Hadoop ecosystem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从AMPLab计划发展出来的最著名和最广泛使用的项目是Spark，可以说是Hadoop生态系统的一个更优秀的替代方案，或者更准确地说是*扩展*。
- en: 'In this chapter, we will visit some of the salient characteristics of Spark
    and end with a real-world tutorial on how to use Spark. The topics we will cover
    are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍Spark的一些显著特点，并以一个真实世界的教程结束，介绍如何使用Spark。我们将涵盖的主题包括：
- en: The advent of Spark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的出现
- en: Theoretical concepts in Spark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark中的理论概念
- en: Core components of Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的核心组件
- en: The Spark architecture
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark架构
- en: Spark solutions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark解决方案
- en: Spark tutorial
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark教程
- en: The advent of Spark
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的出现
- en: When the first release of Spark became available in 2014, Hadoop had already
    enjoyed several years of growth since 2009 onwards in the commercial space. Although
    Hadoop solved a major hurdle in analyzing large terabyte-scale datasets efficiently,
    using distributed computing methods that were broadly accessible, it still had
    shortfalls that hindered its wider acceptance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当Spark的第一个版本于2014年发布时，自2009年以来，Hadoop在商业领域已经经历了数年的增长。尽管Hadoop解决了高效分析大规模数据集的主要障碍，使用广泛可访问的分布式计算方法，但仍存在阻碍其更广泛接受的缺陷。
- en: Limitations of Hadoop
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop的限制
- en: 'A few of the common limitations with Hadoop were as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的一些常见限制如下：
- en: '**I/O Bound operations**: Due to the reliance on local disk storage for saving
    and retrieving data, any operation performed in Hadoop incurred an I/O overhead.
    The problem became more acute in cases of larger datasets that involved thousands
    of blocks of data across hundreds of servers. To be fair, the ability to co-ordinate
    concurrent I/O operations (via HDFS) formed the foundation of distributed computing
    in Hadoop world. However, leveraging the capability and *tuning* the Hadoop cluster
    in an efficient manner across different use cases and datasets required an immense
    and perhaps disproportionate level of expertise. Consequently, the I/O bound nature
    of workloads became a deterrent factor for using Hadoop against extremely large
    datasets. As an example, machine learning use cases that required hundreds of
    iterative operations meant that the system would incur an I/O overhead for each
    pass of the iteration.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**I/O绑定操作**：由于依赖本地磁盘存储来保存和检索数据，Hadoop中执行的任何操作都会产生I/O开销。在涉及数千个数据块跨越数百台服务器的大型数据集的情况下，问题变得更加严重。公平地说，通过HDFS协调并发I/O操作的能力构成了Hadoop世界中分布式计算的基础。然而，有效地利用这种能力并在不同的用例和数据集中调整Hadoop集群需要极大且可能是不成比例的专业知识水平。因此，工作负载的I/O绑定特性成为使用Hadoop处理极大数据集的阻碍因素。例如，需要数百次迭代操作的机器学习用例意味着系统会在每次迭代中产生I/O开销。'
- en: '**MapReduce programming (MR) Model**: As discussed in the earlier parts of
    this book, all operations in Hadoop require expressing problems in terms of the
    MapReduce Programming Model - namely, the user would have to express the problem
    in terms of key-value pairs where each pair can be independently computed. In
    Hadoop, coding efficient MapReduce programs, mainly in Java, was non-trivial,
    especially for those new to Java or to Hadoop (or both).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MapReduce编程（MR）模型**：正如本书前面部分所讨论的，Hadoop中的所有操作都需要用MapReduce编程模型来表达问题，即用户必须以每对键值独立计算的方式来表达问题。在Hadoop中，编写高效的MapReduce程序，特别是对于那些对Java或Hadoop（或两者）都不熟悉的人来说，是非常困难的。'
- en: '**Non-MR Use Cases**: Due to the reliance on MapReduce, other more common and
    simpler concepts such as filters, joins, and so on would have to also be expressed
    in terms of a MapReduce program. Thus, a join across two files across a primary
    key would have to adopt a key-value pair approach. This meant that operations,
    both simple and complex, were hard to achieve without significant programming
    efforts.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非MR使用案例**：由于依赖MapReduce，其他更常见和更简单的概念，如过滤器、连接等，也必须以MapReduce程序的形式表达。因此，跨主键在两个文件之间进行连接必须采用键值对方法。这意味着简单和复杂的操作都很难在没有重大编程工作的情况下实现。'
- en: '**Programming APIs**: The use of Java as the central programming language across
    Hadoop meant that to be able to properly administer and use Hadoop, developers
    had to have a strong knowledge of Java and related topics such as JVM tuning,
    Garbage Collection, and others. This also meant that developers in other popular
    languages such as R, Python, and Scala had very little recourse for re-using or
    at least implementing their solution in the language they knew best.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程API：在Hadoop中将Java作为中心编程语言的使用意味着，为了能够正确地管理和使用Hadoop，开发人员必须对Java和相关主题（如JVM调优、垃圾收集等）有很强的了解。这也意味着，其他流行语言（如R、Python和Scala）的开发人员几乎没有办法重用或至少在他们最擅长的语言中实现他们的解决方案。
- en: On the whole, even though the Hadoop world had championed the Big Data revolution,
    it fell short of being able to democratize the use of the technology for Big Data
    on a broad scale.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总的来说，尽管Hadoop世界曾经领导了大数据革命，但它未能使大数据技术在广泛范围内得到民主化使用。
- en: The team at AMPLab recognized these shortcomings early on, and set about creating
    Spark to address these and, in the process, hopefully develop a new, superior
    alternative.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: AMPLab团队早早意识到了这些缺点，并着手创建Spark来解决这些问题，并希望开发一种新的、更优越的替代方案。
- en: Overcoming the limitations of Hadoop
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 克服Hadoop的局限性
- en: We'll now look at some of the limitations discussed in the earlier section and
    understand how Spark addresses these areas, by virtue of which it provides a superior
    alternative to the Hadoop ecosystem.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看一下前一节讨论的一些限制，并了解Spark如何通过这些方面来解决这些问题，从而提供了Hadoop生态系统的一个更优越的替代方案。
- en: A key difference to bear in mind at the onset is that Spark does NOT need Hadoop
    in order to operate. In fact, the underlying backend from which Spark accesses
    data can be technologies such as HBase, Hive and Cassandra in addition to HDFS.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要牢记的一个关键区别是，Spark不需要Hadoop才能运行。事实上，Spark访问数据的底层后端可以是诸如HBase、Hive和Cassandra以及HDFS等技术。
- en: This means that organizations that wish to leverage a standalone Spark system
    can do so without building a separate Hadoop infrastructure if one does not already
    exist.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着希望利用独立的Spark系统的组织可以在没有已有的Hadoop基础设施的情况下这样做。
- en: 'The Spark solutions are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的解决方案如下：
- en: '**I/O Bound operations**: Unlike Hadoop, Spark can store and access data stored
    in *memory*, namely RAM - which, as discussed earlier, is 1,000+ times faster
    than reading data from a disk. With the emergence of SSD drives, the standard
    in today''s enterprise systems, the difference has gone down significantly. Recent
    NVMe drives can deliver up to 3-5 GB (Giga Bytes) of bandwidth per second. Nevertheless,
    RAM, which averages about 25-30 GB per second in read speed, is still 5-10x faster
    compared to reading from the newer storage technologies. As a result, being able
    to store data in RAM provides a 5x or more improvement to the time it takes to
    read data for Spark operations. This is a significant improvement over the Hadoop
    operating model which relies on disk read for all operations. In particular, tasks
    that involve iterative operations as in machine learning benefit immensely from
    the Spark''s facility to store and read data from memory.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: I/O绑定操作：与Hadoop不同，Spark可以存储和访问存储在内存中的数据，即RAM - 正如前面讨论的，这比从磁盘读取数据快1000多倍。随着SSD驱动器的出现，成为当今企业系统的标准，差异已经显著减少。最近的NVMe驱动器可以提供每秒3-5GB（千兆字节）的带宽。然而，RAM的读取速度平均约为25-30GB每秒，仍然比从较新的存储技术中读取快5-10倍。因此，能够将数据存储在RAM中，可以使Spark操作读取数据的时间提高5倍或更多。这是对依赖于磁盘读取所有操作的Hadoop操作模型的显著改进。特别是，涉及迭代操作的任务，如机器学习，受益于Spark能够存储和从内存中读取数据的功能。
- en: '**MapReduce programming (MR) Model**: While MapReduce is the primary programming
    model through which users can benefit from a Hadoop platform, Spark does not have
    the same requirement. This is particularly helpful for more complex use cases
    such as quantitative analysis involving calculations that cannot be easily *parallelized,*
    such as machine learning algorithms. By decoupling the programming model from
    the platform, Spark allows users to write and execute code written in various
    languages without forcing any specific programming model as a pre-requisite.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapReduce编程（MR）模型：虽然MapReduce是用户可以从Hadoop平台中受益的主要编程模型，但Spark并没有相同的要求。这对于更复杂的用例特别有帮助，比如涉及无法轻松并行化的计算的定量分析，比如机器学习算法。通过将编程模型与平台解耦，Spark允许用户编写和执行用各种语言编写的代码，而不强制任何特定的编程模型作为先决条件。
- en: '**Non-MR use cases**: Spark SQL, Spark Streaming and other components of the
    Spark ecosystem provide a rich set of functionalities that allow users to perform
    common tasks such as SQL joins, aggregations, and related database-like operations
    without having to leverage other, external solutions. Spark SQL queries are generally
    executed against data stored in Hive (JSON is another option), and the functionality
    is also available in other Spark APIs such as R and Python.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非MR用例：Spark SQL、Spark Streaming和Spark生态系统的其他组件提供了丰富的功能，允许用户执行常见任务，如SQL连接、聚合和相关的类似数据库的操作，而无需利用其他外部解决方案。Spark
    SQL查询通常针对存储在Hive中的数据（JSON是另一个选项）执行，并且该功能也可用于其他Spark API，如R和Python。
- en: '**Programming APIs**: The most commonly used APIs in Spark are Python, Scala
    and Java. For R programmers, there is a separate package called `SparkR` that
    permits direct access to Spark data from R. This is a major differentiating factor
    between Hadoop and Spark, and by exposing APIs in these languages, Spark becomes
    immediately accessible to a much larger community of developers. In Data Science
    and Analytics, Python and R are the most prominent languages of choice, and hence,
    any Python or R programmer can leverage Spark with a much simpler learning curve
    relative to Hadoop. In addition, Spark also includes an interactive shell for
    ad-hoc analysis.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编程API**：Spark中最常用的API是Python、Scala和Java。对于R程序员，还有一个名为`SparkR`的单独包，允许直接从R访问Spark数据。这是Hadoop和Spark之间的一个主要区别，通过在这些语言中公开API，Spark立即对更大的开发者社区可用。在数据科学和分析中，Python和R是最突出的选择语言，因此，任何Python或R程序员都可以利用Spark，相对于Hadoop，学习曲线更简单。此外，Spark还包括一个用于临时分析的交互式shell。'
- en: Theoretical concepts in Spark
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark中的理论概念
- en: 'The following are the core concepts in Spark:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Spark中的核心概念：
- en: Resilient distributed datasets
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性分布式数据集
- en: Directed acyclic graphs
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有向无环图
- en: SparkContext
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkContext
- en: Spark DataFrames
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark DataFrames
- en: Actions and transformations
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作和转换
- en: Spark deployment options
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark部署选项
- en: Resilient distributed datasets
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性分布式数据集
- en: '**Resilient distributed datasets**, more commonly known as **RDD**s, are the
    primary data structure used in Spark. RDDs are essentially a collection of records
    that are stored across a Spark cluster in a distributed manner. RDDs are *immutable*,
    which is to say, they cannot be altered once created. RDDs that are stored across
    nodes can be accessed in parallel, and hence support parallel operations natively.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集**，更常被称为**RDD**，是Spark中使用的主要数据结构。RDD本质上是一个记录的集合，以分布式的方式存储在Spark集群中。RDD是*不可变*的，也就是说，一旦创建就无法更改。存储在节点上的RDD可以并行访问，因此本身支持并行操作。'
- en: The user does not need to write separate code to get the benefits of parallelization
    but can get the benefits of *actions and transformations* of data simply by running
    specific commands that are native to the Spark platform. Because RDDs can be also
    stored in memory, as an additional benefit, the parallel operations can act on
    the data directly in memory without incurring expensive I/O access penalties.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 用户无需编写单独的代码来获得并行化的好处，只需运行特定的命令即可获得与Spark平台本身相关的*操作和转换*的好处。由于RDD也可以存储在内存中，作为额外的好处，并行操作可以直接在内存中对数据进行操作，而不会产生昂贵的I/O访问惩罚。
- en: Directed acyclic graphs
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有向无环图
- en: In computer science and mathematics parlance, a directed acyclic graph represents
    pairs of nodes (also known as **vertices**) connected with edges (or **lines**)
    that are unidirectional. Namely, given Node A and Node B, the edge can connect
    A à B or B à A but not both. In other words, there isn't a circular relationship
    between any pair of nodes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学和数学术语中，有向无环图表示一对节点（也称为**顶点**）之间用边（或**线**）连接的图，这些边是单向的。也就是说，给定节点A和节点B，边可以连接A到B或B到A，但不能同时连接。换句话说，任何一对节点之间没有循环关系。
- en: Spark leverages the concept of DAG to build an internal workflow that delineates
    the different stages of processing in a Spark job. Conceptually, this is akin
    to creating a virtual flowchart of the series of steps needed to obtain a certain
    output. For instance, if the required output involves producing a count of words
    in a document, the intermediary steps map-shuffle-reduce can be represented as
    a series of actions that lead to the final result. By maintaining such a **map**,
    Spark is able to keep track of the dependencies involved in the operation. More
    specifically, RDDs are the **nodes**, and transformations, which are discussed
    later in this section, are the **edges** of the DAG.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Spark利用DAG的概念来构建内部工作流程，以划分Spark作业中不同阶段的处理。从概念上讲，这类似于创建一份虚拟流程图，展示了获得特定输出所需的一系列步骤。例如，如果所需的输出涉及在文档中生成单词计数，中间步骤map-shuffle-reduce可以表示为一系列导致最终结果的操作。通过维护这样的**map**，Spark能够跟踪操作中涉及的依赖关系。更具体地说，RDD是**节点**，而稍后在本节中讨论的转换是DAG的**边缘**。
- en: SparkContext
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkContext
- en: A SparkContext is the entry point for all Spark operations and means by which
    the application connects to the resources of the Spark cluster. It initializes
    an instance of Spark and can thereafter be used to create RDDs, perform actions
    and transformations on the RDDs, and extract data and other Spark functionalities.
    A SparkContext also initializes various properties of the process, such as the
    application name, number of cores, memory usage parameters, and other characteristics.
    Collectively, these properties are contained in the object SparkConf, which is
    passed to SparkContext as a parameter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SparkContext是所有Spark操作的入口点，也是应用程序连接到Spark集群资源的方式。它初始化了一个Spark实例，然后可以用于创建RDD，对RDD执行操作和转换，提取数据和其他Spark功能。SparkContext还初始化了进程的各种属性，如应用程序名称、核心数、内存使用参数和其他特性。这些属性集中在SparkConf对象中，作为参数传递给SparkContext。
- en: '`SparkSession` is the new abstraction through which users initiate their connection
    to Spark. It is a superset of the functionality provided in `SparkContext` prior
    to Spark 2.0.0\. However, practitioners still use `SparkSession` and `SparkContext`
    interchangeably to mean one and the same entity; namely, the primary mode of interacting
    with `Spark.SparkSession` has essentially combined the functionalities of both
    SparkContext and `HiveContext`.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession`是用户启动与Spark连接的新抽象。它是Spark 2.0.0之前`SparkContext`提供的功能的超集。然而，实践者仍然可以互换使用`SparkSession`和`SparkContext`来表示同一个实体；即与`Spark`交互的主要方式。`SparkSession`本质上结合了`SparkContext`和`HiveContext`的功能。'
- en: Spark DataFrames
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark DataFrames
- en: A DataFrame in Spark is the raw data organized into rows and columns. This is
    conceptually similar to CSV files or SQL tables. Using R, Python and other Spark
    APIs, the user can interact with a DataFrame using common Spark commands used
    for filtering, aggregating, and more generally manipulating the data. The data
    contained in DataFrames are physically located across the multiple nodes of the
    Spark cluster. However, by representing them in a **DataFrame** they appear to
    be a cohesive unit of data without exposing the complexity of the underlying operations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，DataFrame是组织成行和列的原始数据。这在概念上类似于CSV文件或SQL表。使用R、Python和其他Spark API，用户可以使用常见的Spark命令与DataFrame交互，用于过滤、聚合和更一般的数据操作。DataFrame中包含的数据实际上位于Spark集群的多个节点上。然而，通过在**DataFrame**中表示它们，它们看起来像是一个统一的数据单元，而不暴露底层操作的复杂性。
- en: Note that DataFrames are not the same as Datasets, another common term used
    in Spark. Datasets refer to the actual data that is held across the Spark cluster.
    A DataFrame is the tabular representation of the Dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，DataFrame和Dataset不是Spark中常用的术语。Dataset指的是存储在Spark集群中的实际数据。DataFrame是Dataset的表格表示。
- en: Starting with Spark 2.0, the DataFrame and Dataset APIs were merged and a DataFrame
    in essence now represents a Dataset of Row. That said, DataFrame still remains
    the primary abstraction for users who want to leverage Python and R for interacting
    with Spark data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark 2.0开始，DataFrame和Dataset API被合并，DataFrame现在本质上代表了一组行的Dataset。也就是说，DataFrame仍然是想要利用Python和R与Spark数据交互的用户的主要抽象。
- en: Actions and transformations
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作和转换
- en: 'There are 2 types of Spark operations:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Spark操作有两种类型：
- en: Transformations
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换
- en: Actions
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作
- en: '**Transformations** specify general data manipulation operations such as filtering
    data, joining data, performing aggregations, sampling data, and so on. Transformations
    do not return any result when the line containing the transformation operation
    in the code is executed. Instead, the command, upon execution, supplements Spark''s
    internal DAG with the corresponding operation request. Examples of common transformations
    include: `map`, `filter`, `groupBy`, `union`, `coalesce`, and many others.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**转换**指定一般的数据操作，如过滤数据、连接数据、执行聚合、抽样数据等。当执行代码中包含转换操作的行时，转换不会返回任何结果。相反，命令在执行时会向Spark的内部DAG添加相应的操作请求。常见的转换示例包括：`map`、`filter`、`groupBy`、`union`、`coalesce`等等。'
- en: '**Actions**, on the other hand, return results. Namely, they execute the series
    of transformations (if any) that the user may have specified on the corresponding
    RDD and produce an output. In other words, actions trigger the execution of the
    steps in the DAG. Common Actions include: `reduce`, `collect`, `take`, `aggregate`,
    `foreach`, and many others.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**操作**，另一方面，返回结果。换句话说，它们执行用户可能在相应的RDD上指定的一系列转换（如果有的话），并产生输出。换句话说，操作触发DAG中步骤的执行。常见的操作包括：`reduce`、`collect`、`take`、`aggregate`、`foreach`等等。'
- en: Note that RDDs are immutable. They cannot be changed; transformations and actions
    will always produce new RDDs, but never modify existing ones.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，RDD是不可变的。它们不能被改变；转换和操作总是会产生新的RDD，但永远不会修改现有的RDD。
- en: Spark deployment options
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark部署选项
- en: 'Spark can be deployed in various modes. The most important ones are:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以以各种模式部署。最重要的是：
- en: '**Standalone mode**: As an independent cluster not dependent upon any external
    cluster manager'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立模式**：作为一个独立的集群，不依赖于任何外部集群管理器'
- en: '**Amazon EC2**: On EC2 instances of Amazon Web Services where it can access
    data from S3'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon EC2**：在亚马逊网络服务的EC2实例上，可以从S3访问数据'
- en: '**Apache YARN**: The Hadoop ResourceManager'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache YARN**：Hadoop ResourceManager'
- en: Other options include **Apache Mesos** and **Kubernetes.**
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其他选项包括**Apache Mesos**和**Kubernetes**。
- en: Further details can be found at the Spark documentation website, [https://spark.apache.org/docs/latest/index.html](https://spark.apache.org/docs/latest/index.html).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详细信息可以在Spark文档网站找到，[https://spark.apache.org/docs/latest/index.html](https://spark.apache.org/docs/latest/index.html)。
- en: Spark APIs
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark API
- en: The Spark platform is easily accessible through Spark APIs available in Python,
    Scala, R, and Java. Together they make working with data in Spark simple and broadly
    accessible. During the inception of the Spark project, it only supported Scala/Java
    as the primary API. However, since one of the overarching objectives of Spark
    was to provide an easy interface to a diverse set of developers, the Scala API
    was followed by a Python and R API.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Spark平台可以通过Python、Scala、R和Java中可用的Spark API轻松访问。它们一起使得在Spark中处理数据变得简单且广泛可访问。在Spark项目初始阶段，它只支持Scala/Java作为主要API。然而，由于Spark的一个主要目标是为多样化的开发者提供一个简单的接口，Scala
    API之后又跟着Python和R API。
- en: In Python, the PySpark package has become a widely used standard for writing
    Spark applications by the Python developer community. In R, users interact with
    Spark via the SparkR package. This is useful for R developers who may also be
    interested in working with data stored in a Spark ecosystem. Both of these languages
    are very prevalent in the Data Science community, and hence, the introduction
    of the Python and R APIs set the groundwork for democratizing **Big Data** Analytics
    on Spark for analytical use cases.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，PySpark包已经成为Python开发者社区编写Spark应用程序的广泛标准。在R中，用户通过SparkR包与Spark进行交互。这对于可能也对在Spark生态系统中存储的数据进行操作的R开发者来说是有用的。这两种语言在数据科学社区中非常普遍，因此，引入Python和R
    API为分析用例上的**大数据**分析在Spark上的民主化奠定了基础。
- en: Core components in Spark
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的核心组件
- en: 'The following components are quite important in Spark:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下组件在Spark中非常重要：
- en: Spark Core
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Core
- en: Spark SQL
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Spark Streaming
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: GraphX
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GraphX
- en: MLlib
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLlib
- en: Spark Core
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Core
- en: Spark Core provides fundamental functionalities in Spark, such as working with
    RDDs, performing actions, and transformations, in addition to more administrative
    tasks such as storage, high availability, and other topics.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Core在Spark中提供了基本功能，如使用RDD、执行操作和转换，以及更多的管理任务，如存储、高可用性和其他主题。
- en: Spark SQL
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Spark SQL provides the user with the ability to query data stored in Apache
    Hive using standard SQL commands. This adds an additional level of accessibility
    by providing developers with a means to interact with datasets via the Spark SQL
    interface using common SQL terminologies. The platform hosting the underlying
    data is not limited to Apache Hive, but can also include JSON, Parquet, and others.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL为用户提供了使用标准SQL命令查询存储在Apache Hive中的数据的能力。这通过提供开发人员通过Spark SQL接口使用常见的SQL术语与数据集交互，增加了额外的可访问性。托管底层数据的平台不仅限于Apache
    Hive，还可以包括JSON、Parquet等。
- en: Spark Streaming
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: The streaming component of Spark allows users to interact with streaming data
    such as web-related content and others. It also includes enterprise characteristics
    such as high availability. Spark can read data from various middleware and data
    streaming services such as Apache Kafka, Apache Flume, and Cloud based solutions
    from vendors such as Amazon Web Services.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的流处理组件允许用户与流数据进行交互，如与网络相关的内容等。它还包括高可用性等企业特性。Spark可以从各种中间件和数据流服务中读取数据，如Apache
    Kafka、Apache Flume和云服务供应商如亚马逊网络服务。
- en: GraphX
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GraphX
- en: The GraphX component of Spark supports graph-based operations, similar to technologies
    such as graph databases that support specialized data structures. These make it
    easy to use, access, and represent inter-connected points of data, such as social
    networks. Besides analytics, the Spark GraphX platform supports graph algorithms
    that are useful for business use cases that require relationships to be represented
    at scale. As an example, credit card companies use Graph based databases similar
    to the GraphX component of Spark to build recommendation engines that detect users
    with similar characteristics. These characteristics may include buying habits,
    location, demographics, and other qualitative and quantitative factors. Using
    Graph systems in these cases allows companies to build networks with nodes representing
    individuals and edges representing relationship metrics to find common features
    amongst them.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的GraphX组件支持基于图的操作，类似于支持专门数据结构的图数据库技术。这使得使用、访问和表示数据的相互连接点变得容易，如社交网络。除了分析，Spark
    GraphX平台还支持图算法，这些算法对于需要在规模上表示关系的业务用例非常有用。例如，信用卡公司使用类似于Spark的GraphX组件的基于图的数据库来构建检测具有相似特征的用户的推荐引擎。这些特征可能包括购买习惯、位置、人口统计学和其他定性和定量因素。在这些情况下使用图系统允许公司构建网络，其中节点代表个体，边代表关系度量，以找到它们之间的共同特征。
- en: MLlib
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLlib
- en: 'MLlib is one of the flagship components of the Spark ecosystem. It provides
    a scalable, high-performance interface to perform resource intensive machine learning
    tasks in Spark. Additionally, MLlib can natively connect to HDFS, HBase, and other
    underlying storage systems supported in Spark. Due to this versatility, users
    do not need to rely on a pre-existing Hadoop environment to start using the algorithms
    built into MLlib. Some of the supported algorithms in MLlib include:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib是Spark生态系统的旗舰组件之一。它提供了一个可扩展的、高性能的接口，用于在Spark中执行资源密集型的机器学习任务。此外，MLlib可以原生连接到HDFS、HBase和其他在Spark中支持的底层存储系统。由于这种多功能性，用户不需要依赖预先存在的Hadoop环境来开始使用内置到MLlib中的算法。MLlib中支持的一些算法包括：
- en: '**Classification**: logistic regression'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：逻辑回归'
- en: '**Regression**: generalized linear regression, survival regression and others'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：广义线性回归、生存回归等'
- en: Decision trees, random forests, and gradient-boosted trees
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树、随机森林和梯度提升树
- en: '**Recommendation**: Alternating least squares'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐**：交替最小二乘法'
- en: '**Clustering**: K-means, Gaussian mixtures and others'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：K均值、高斯混合和其他'
- en: '**Topic modeling**: Latent Dirichlet allocation'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题建模**：潜在狄利克雷分配'
- en: '**Apriori**: Frequent Itemsets, Association Rules'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apriori**：频繁项集、关联规则'
- en: 'ML workflow utilities include:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ML工作流程实用程序包括：
- en: '**Feature transformations**: Standardization, normalization and others'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征转换**：标准化、归一化等'
- en: ML Pipeline construction
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML Pipeline构建
- en: Model evaluation and hyper-parameter tuning
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估和超参数调整
- en: '**ML persistence**: Saving and loading models and Pipelines'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ML持久性**：保存和加载模型和管道'
- en: The architecture of Spark
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的架构
- en: 'Spark consists of 3 primary architectural components:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Spark由3个主要的架构组件组成：
- en: The SparkSession / SparkContext
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkSession/SparkContext
- en: The Cluster Manager
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群管理器
- en: The Worker Nodes (that hosts executor processes)
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点（托管执行器进程）
- en: The **SparkSession/SparkContext**, or more generally the Spark Driver, is the
    entry point for all Spark applications as discussed earlier. The SparkContext
    will be used to create RDDs and perform operations against RDDs. The SparkDriver
    sends instructions to the worker nodes to schedule tasks.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**SparkSession/SparkContext**，或者更一般地说，Spark Driver，是所有Spark应用程序的入口点，如前所述。SparkContext将用于创建RDD并对RDD执行操作。SparkDriver发送指令到工作节点以安排任务。'
- en: The **Cluster manager** is conceptually similar to Resource Managers in Hadoop
    and indeed, one of the supported solutions is YARN. Other Cluster Managers include
    Mesos. Spark can also operate in a Standalone mode in which case YARN/Mesos are
    not required. Cluster Managers co-ordinate communications between the Worker Nodes,
    manage the nodes (such as starting, stopping, and so on), and perform other administration
    tasks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**集群管理器**在概念上类似于Hadoop中的资源管理器，事实上，支持的解决方案之一是YARN。其他集群管理器包括Mesos。Spark也可以在独立模式下运行，在这种情况下不需要YARN/Mesos。集群管理器协调工作节点之间的通信，管理节点（如启动、停止等），并执行其他管理任务。'
- en: '**Worker nodes** are servers where Spark applications are hosted. Each application
    gets its own unique **executor process**, namely, processes that perform the actual
    action and transformation tasks. By assigning dedicated executor processes, Spark
    ensures that an issue in any particular application does not impact other applications.
    Worker Nodes consist of the Executor, the JVM, and the Python/R/other application
    process required by the Spark application. Note that in the case of Hadoop, the
    Worker Node and Data Nodes are one and the same:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**工作节点**是托管Spark应用程序的服务器。每个应用程序都有自己独特的**执行器进程**，即执行实际操作和转换任务的进程。通过分配专用的执行器进程，Spark确保任何特定应用程序中的问题不会影响其他应用程序。工作节点由执行器、JVM和Spark应用程序所需的Python/R/其他应用程序进程组成。请注意，在Hadoop的情况下，工作节点和数据节点是一样的：'
- en: '![](img/820c15f2-b785-4035-9227-69df5fcfba24.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/820c15f2-b785-4035-9227-69df5fcfba24.png)'
- en: Spark solutions
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark解决方案
- en: Spark is directly available from [spark.apache.org](http://spark.apache.org/)
    as an open-source solution. **Databricks** is the leading provider of the commercial
    solution of Spark. For those who are familiar with programming in Python, R, Java,
    or Scala, the time required to start using Spark is minimal due to efficient interfaces,
    such as the PySpark API that allows users to work in Spark using just Python.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Spark直接可从[spark.apache.org](http://spark.apache.org/)作为开源解决方案获得。**Databricks**是Spark商业解决方案的领先提供商。对于熟悉Python、R、Java或Scala编程的人来说，由于高效的接口（如PySpark
    API），开始使用Spark所需的时间很短。
- en: Cloud-based Spark platforms, such as the Databricks Community Edition, provide
    an easy and simple means to work on Spark without the additional work of installing
    and configuring Spark. Hence, users who wish to use Spark for programming and
    related tasks can get started much more rapidly without spending time on administrative
    tasks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 基于云的Spark平台，如Databricks Community Edition，提供了一种简单易行的方式来使用Spark，而不需要安装和配置Spark。因此，希望使用Spark进行编程和相关任务的用户可以更快地开始，而不需要花时间在管理任务上。
- en: Spark practicals
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark实践
- en: In this section, we will create an account on Databricks' Community Edition
    and complete a hands-on exercise that will walk the reader through the basics
    of actions, transformations, and RDD concepts in general.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在Databricks的Community Edition上创建一个帐户，并完成一个实际操作的练习，引导读者了解操作、转换和RDD概念的基础知识。
- en: Signing up for Databricks Community Edition
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注册Databricks Community Edition
- en: 'The following steps outline the process of signing up for the **Databricks
    Community Edition**:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤概述了注册**Databricks Community Edition**的过程：
- en: 'Go to [https://databricks.com/try-databricks](https://databricks.com/try-databricks):'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到[https://databricks.com/try-databricks](https://databricks.com/try-databricks)：
- en: '![](img/b32b5600-60df-4eb1-8bc7-fbef99fc7302.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b32b5600-60df-4eb1-8bc7-fbef99fc7302.png)'
- en: 'Click on the START TODAY button and enter your information:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**立即开始**按钮并输入您的信息：
- en: '![](img/24ce51e3-d0a6-4572-a80c-226886b59f14.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24ce51e3-d0a6-4572-a80c-226886b59f14.png)'
- en: 'Confirm that you have read and agree to the terms in the popup menu (scroll
    down to the bottom for the **Agree** button):'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认您已阅读并同意弹出菜单中的条款（向下滚动到底部找到**同意**按钮）：
- en: '![](img/08c99e13-c3ff-49b6-932a-8786ce3112d8.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08c99e13-c3ff-49b6-932a-8786ce3112d8.png)'
- en: 'Check your email for a confirmation email from Databricks and click on the
    link to confirm your account:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查您的电子邮件，确认来自Databricks的确认电子邮件，并点击链接确认您的账户：
- en: '![](img/fa9ccec1-86a7-4b4e-898b-ebcf8fe525a7.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa9ccec1-86a7-4b4e-898b-ebcf8fe525a7.png)'
- en: 'Once you click on the link to confirm your account, you''ll be taken to a login
    screen where you can log on using the email address and password you used to sign
    up for the account:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击链接确认您的账户后，您将被带到一个登录界面，在那里您可以使用注册账户时使用的电子邮件地址和密码登录：
- en: '![](img/b77a0d6c-f43a-4646-a1f2-6cac2abd0f82.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b77a0d6c-f43a-4646-a1f2-6cac2abd0f82.png)'
- en: 'After logging in, click on Cluster to set up a Spark cluster, as shown in the
    following figure:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，点击集群设置一个Spark集群，如下图所示：
- en: '![](img/ec9a4931-343a-4856-8453-81359dc3cdc2.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec9a4931-343a-4856-8453-81359dc3cdc2.png)'
- en: 'Enter `Packt_Exercise` as the Cluster Name and click on the Create Cluster
    button at the top of the page:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入`Packt_Exercise`作为集群名称，然后点击页面顶部的创建集群按钮：
- en: '![](img/4c630ae4-a0ae-42e5-8e38-9be37c9b24a5.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c630ae4-a0ae-42e5-8e38-9be37c9b24a5.png)'
- en: This will initiate the process of starting up a Spark Cluster on which we will
    execute our Spark commands using an iPython notebook. An iPython Notebook is the
    name given to a commonly used IDE - a web-based development application used for
    writing and testing Python code. The notebook can also support other languages
    through the use of kernels, but for the purpose of this exercise, we will focus
    on the Python kernel.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将启动启动一个Spark集群的过程，我们将在其中使用iPython笔记本执行我们的Spark命令。iPython Notebook是一个常用的IDE的名称，它是一个用于编写和测试Python代码的基于Web的开发应用程序。笔记本还可以通过内核支持其他语言，但在本练习中，我们将专注于Python内核。
- en: 'After a while, the Status will change from Pending to Running:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间后，状态将从待定变为运行：
- en: '![](img/c97289e0-9db2-4a45-849f-3062f7f69040.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c97289e0-9db2-4a45-849f-3062f7f69040.png)'
- en: 'Status changes to Running after a few minutes:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后状态变为运行：
- en: '![](img/c7149a7e-15b2-4c4f-99ce-332c20841de1.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7149a7e-15b2-4c4f-99ce-332c20841de1.png)'
- en: 'Click on **Workspace** (on the left hand bar) and select **options**, **Users**
    | (`Your userid`) and click on the drop-down arrow next to your email address.
    Select Create | Notebook:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**工作区**（在左侧栏）并选择**选项**，**用户** | (`您的用户ID`)，然后点击您的电子邮件地址旁边的下拉箭头。选择创建 | 笔记本：
- en: '![](img/972a70c7-9846-48d0-83bf-c64f87eb3e3f.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/972a70c7-9846-48d0-83bf-c64f87eb3e3f.png)'
- en: 'In the popup screen, enter `Packt_Exercise` as the name of the notebook and
    click on the Create button:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在弹出屏幕中，输入`Packt_Exercise`作为笔记本的名称，然后点击创建按钮：
- en: '![](img/05754e5e-5029-4729-bff4-41b883ced6c8.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05754e5e-5029-4729-bff4-41b883ced6c8.png)'
- en: 'Once you click on the **Create** button, you''ll be taken directly to the Notebook
    as shown in the following screenshot. This is the Spark Notebook, where you''ll
    be able to execute the rest of the code given in the next few sections. The code
    should be typed in the cells of the notebook as shown. After entering your code,
    press *Shift + Enter* to execute the corresponding cell:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击**创建**按钮后，您将直接进入笔记本，如下面的屏幕截图所示。这是Spark笔记本，您将能够执行接下来几个部分中给出的其余代码。应在笔记本的单元格中输入代码，如所示。输入代码后，按*Shift
    + Enter*执行相应的单元格：
- en: '![](img/fa59787f-7c2f-4ed5-a0e6-62c84d6f76a9.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa59787f-7c2f-4ed5-a0e6-62c84d6f76a9.png)'
- en: For the next few exercises, you can copy-paste the text into the cells of the
    Notebook. Alternatively, you can also import the notebook and load it directly
    in your workspace. If you do so, you'll not need to type in the commands (although
    typing in the commands will provide more hands-on familiarity).
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的几个练习中，您可以将文本复制粘贴到笔记本的单元格中。或者，您还可以导入笔记本并直接在工作区中加载它。如果这样做，您将不需要输入命令（尽管输入命令将提供更多的实践熟悉度）。
- en: 'An alternative approach to copy-pasting commands: You can import the notebook
    by clicking on Import as shown in the following screenshot:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制粘贴命令的另一种方法：您可以通过单击以下屏幕截图中显示的导入来导入笔记本：
- en: '![](img/8e1545cd-bf1c-43a7-9309-4eb40b781d71.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e1545cd-bf1c-43a7-9309-4eb40b781d71.png)'
- en: 'Enter the following **URL** in the popup menu (select **URL** as the **Import
    from** option):'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在弹出菜单中输入以下**URL**（选择**URL**作为**从**选项**导入**）：
- en: '![](img/db82dd15-c748-4433-bf0e-96f8a1d820d8.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db82dd15-c748-4433-bf0e-96f8a1d820d8.png)'
- en: 'The notebook will then show up under your email ID. Click on the name of the
    notebook to load it:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后笔记本将显示在您的电子邮件ID下。单击笔记本的名称加载它：
- en: '![](img/e3ab20dd-cf19-40cd-9e8e-e32c3faed7e5.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3ab20dd-cf19-40cd-9e8e-e32c3faed7e5.png)'
- en: Spark exercise - hands-on with Spark (Databricks)
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark练习-亲身体验Spark（Databricks）
- en: This notebook is based on tutorials conducted by Databricks ([https://databricks.com/](https://databricks.com/)).
    The tutorial will be conducted using the Databricks' Community Edition of Spark,
    available to sign up to at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
    Databricks is a leading provider of the commercial and enterprise supported version
    of Spark.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本笔记本是基于Databricks进行的教程（[https://databricks.com/](https://databricks.com/)）。该教程将使用Databricks的Spark社区版进行，可在[https://databricks.com/try-databricks](https://databricks.com/try-databricks)注册。Databricks是Spark的商业和企业支持版本的领先提供商。
- en: In this tutorial, we will introduce a few basic commands used in Spark. Users
    are encouraged to try out more extensive Spark tutorials and notebooks that are
    available on the web for more detailed examples.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将介绍一些在Spark中使用的基本命令。鼓励用户尝试更广泛的Spark教程和笔记本，这些教程和笔记本可以在网络上找到更详细的示例。
- en: Documentation for Spark's Python API can be found at [https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.sql](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.sql).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的Python API文档可以在[https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.sql](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.sql)找到。
- en: The data for this book was imported into the Databricks' Spark Platform. For
    more information on importing data, go to **Importing Data** - **Databricks**
    ([https://docs.databricks.com/user-guide/importing-data.html](https://docs.databricks.com/user-guide/importing-data.html)).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的数据已导入Databricks的Spark平台。有关导入数据的更多信息，请转到**导入数据** - **Databricks** ([https://docs.databricks.com/user-guide/importing-data.html](https://docs.databricks.com/user-guide/importing-data.html))。
- en: '[PRE0]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Summary
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we read about some of the core features of Spark, one of the
    most prominent technologies in the Big Data landscape today. Spark has matured
    rapidly since its inception in 2014, when it was released as a Big Data solution
    that alleviated many of the shortcomings of Hadoop, such as I/O contention and
    others.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了Spark的一些核心特性，这是当今大数据领域中最突出的技术之一。自2014年发布以来，Spark已迅速成熟，当时它作为一个大数据解决方案发布，缓解了Hadoop的许多缺点，如I/O争用等。
- en: Today, Spark has several components, including dedicated ones for streaming
    analytics and machine learning, and is being actively developed. Databricks is
    the leading provider of the commercially supported version of Spark and also hosts
    a very convenient cloud-based Spark environment with limited resources that any
    user can access at no charge. This has dramatically lowered the barrier to entry
    as users do not need to install a complete Spark environment to learn and use
    the platform.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，Spark有几个组件，包括专门用于流式分析和机器学习的组件，并且正在积极开发中。Databricks是Spark商业支持版本的领先提供商，还托管了一个非常方便的基于云的Spark环境，用户可以免费访问有限资源。这大大降低了用户的准入门槛，因为用户无需安装完整的Spark环境来学习和使用该平台。
- en: In the next chapter, we will begin our discussion on machine learning. Most
    of the text, until this section, has focused on the management of large scale
    data. Making use of the data effectively and gaining *insights* from the data
    is always the final aim. In order to do so, we need to employ the advanced algorithmic
    techniques that have become commonplace today. The next chapter will discuss the
    basic tenets of machine learning, and thereafter we will delve deeper into the
    subject area in the subsequent chapter.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始讨论机器学习。直到这一部分，大部分文本都集中在大规模数据的管理上。有效利用数据并从数据中获得*洞察力*始终是最终目标。为了做到这一点，我们需要采用今天已经变得司空见惯的先进算法技术。下一章将讨论机器学习的基本原则，之后我们将在随后的章节中更深入地探讨这一主题领域。
