- en: PySpark and SparkR
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark和SparkR
- en: 'In this chapter, we will discuss two other popular APIs: PySpark and SparkR
    for writing Spark code in Python and R programming languages respectively. The
    first part of this chapter will cover some technical aspects while working with
    Spark using PySpark. Then we will move to SparkR and see how to use it with ease.
    The following topics will be discussed throughout this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论另外两个流行的API：PySpark和SparkR，分别用于使用Python和R编程语言编写Spark代码。本章的第一部分将涵盖在使用PySpark时的一些技术方面。然后我们将转向SparkR，并看看如何轻松使用它。本章将在整个过程中讨论以下主题：
- en: Introduction to PySpark
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark介绍
- en: Installation and getting started with PySpark
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和开始使用PySpark
- en: Interacting with DataFrame APIs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与DataFrame API交互
- en: UDFs with PySpark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PySpark的UDFs
- en: Data analytics using PySpark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PySpark进行数据分析
- en: Introduction to SparkR
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkR介绍
- en: Why SparkR?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么要使用SparkR？
- en: Installation and getting started with SparkR
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和开始使用SparkR
- en: Data processing and manipulation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据处理和操作
- en: Working with RDD and DataFrame using SparkR
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SparkR处理RDD和DataFrame
- en: Data visualization using SparkR
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SparkR进行数据可视化
- en: Introduction to PySpark
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark介绍
- en: Python is one of the most popular and general purpose programming languages
    with a number of exciting features for data processing and machine learning tasks.
    To use Spark from Python, PySpark was initially developed as a lightweight frontend
    of Python to Apache Spark and using Spark's distributed computation engine. In
    this chapter, we will discuss a few technical aspects of using Spark from Python
    IDE such as PyCharm.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Python是最受欢迎的通用编程语言之一，具有许多令人兴奋的特性，可用于数据处理和机器学习任务。为了从Python中使用Spark，最初开发了PySpark作为Python到Apache
    Spark的轻量级前端，并使用Spark的分布式计算引擎。在本章中，我们将讨论使用Python IDE（如PyCharm）从Python中使用Spark的一些技术方面。
- en: Many data scientists use Python because it has a rich variety of numerical libraries
    with a statistical, machine learning, or optimization focus. However, processing
    large-scale datasets in Python is usually tedious as the runtime is single-threaded.
    As a result, data that fits in the main memory can only be processed. Considering
    this limitation and for getting the full flavor of Spark in Python, PySpark was
    initially developed as a lightweight frontend of Python to Apache Spark and using
    Spark's distributed computation engine. This way, Spark provides APIs in non-JVM
    languages like Python.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据科学家使用Python，因为它具有丰富的数值库，具有统计、机器学习或优化的重点。然而，在Python中处理大规模数据集通常很麻烦，因为运行时是单线程的。因此，只能处理适合主内存的数据。考虑到这一限制，并为了在Python中获得Spark的全部功能，PySpark最初被开发为Python到Apache
    Spark的轻量级前端，并使用Spark的分布式计算引擎。这样，Spark提供了非JVM语言（如Python）的API。
- en: The purpose of this PySpark section is to provide basic distributed algorithms
    using PySpark. Note that PySpark is an interactive shell for basic testing and
    debugging and is not supposed to be used for a production environment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个PySpark部分的目的是提供使用PySpark的基本分布式算法。请注意，PySpark是用于基本测试和调试的交互式shell，不应该用于生产环境。
- en: Installation and configuration
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和配置
- en: There are many ways of installing and configuring PySpark on Python IDEs such
    as PyCharm, Spider, and so on. Alternatively, you can use PySpark if you have
    already installed Spark and configured the `SPARK_HOME`. Thirdly, you can also
    use PySpark from the Python shell. Below we will see how to configure PySpark
    for running standalone jobs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多安装和配置PySpark在Python IDEs如PyCharm，Spider等的方法。或者，如果您已经安装了Spark并配置了`SPARK_HOME`，您可以使用PySpark。第三，您也可以从Python
    shell使用PySpark。接下来我们将看到如何配置PySpark来运行独立的作业。
- en: By setting SPARK_HOME
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过设置SPARK_HOME
- en: 'At first, download and place the Spark distribution at your preferred place,
    say `/home/asif/Spark`. Now let''s set the `SPARK_HOME` as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，下载并将Spark分发放在您喜欢的位置，比如`/home/asif/Spark`。现在让我们设置`SPARK_HOME`如下：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now let''s set `PYTHONPATH` as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们设置`PYTHONPATH`如下：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now we need to add the following two paths to the environmental path:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要将以下两个路径添加到环境路径中：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, let''s refresh the current terminal so that the newly modified `PATH`
    variable is used:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们刷新当前终端，以便使用新修改的`PATH`变量：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'PySpark depends on the `py4j` Python package. It helps the Python interpreter
    to dynamically access the Spark object from the JVM. This package can be installed
    on Ubuntu as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark依赖于`py4j` Python包。它帮助Python解释器动态访问来自JVM的Spark对象。可以在Ubuntu上安装此软件包，方法如下：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Alternatively, the default `py4j`, which is already included in Spark (`$SPARK_HOME/python/lib`),
    can be used too.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，也可以使用默认的`py4j`，它已经包含在Spark中（`$SPARK_HOME/python/lib`）。
- en: Using Python shell
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python shell
- en: 'Like Scala interactive shell, an interactive shell is also available for Python.
    You can execute Python code from Spark root folder as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与Scala交互式shell一样，Python也有一个交互式shell。您可以从Spark根文件夹执行Python代码，如下所示：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If the command went fine, you should observer the following screen on Terminal
    (Ubuntu):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令执行正常，您应该在终端（Ubuntu）上观察到以下屏幕：
- en: '![](img/00375.jpeg)**Figure 1**: Getting started with PySpark shell'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00375.jpeg)**图1**：使用PySpark shell入门'
- en: Now you can enjoy Spark using the Python interactive shell. This shell might
    be sufficient for experimentations and developments. However, for production level,
    you should use a standalone application.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以使用Python交互式shell来使用Spark。这个shell可能足够用于实验和开发。但是，对于生产级别，您应该使用独立的应用程序。
- en: 'PySpark should be available in the system path by now. After writing the Python
    code, one can simply run the code using the Python command, then it runs in local
    Spark instance with default configurations:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark现在应该在系统路径中可用。编写Python代码后，可以简单地使用Python命令运行代码，然后它将在本地Spark实例中以默认配置运行：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that the current distribution of Spark is only Python 2.7+ compatible.
    Hence, we will have been strict on this.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当前版本的Spark仅兼容Python 2.7+。因此，我们将严格遵守这一点。
- en: 'Furthermore, it is better to use the `spark-submit` script if you want to pass
    the configuration values at runtime. The command is pretty similar to the Scala
    one:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您想在运行时传递配置值，最好使用`spark-submit`脚本。该命令与Scala的命令非常相似：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The configuration values can be passed at runtime, or alternatively, they can
    be changed in the `conf/spark-defaults.conf` file. After configuring the Spark
    config file, the changes also get reflected while running PySpark applications
    using a simple Python command.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 配置值可以在运行时传递，或者可以在`conf/spark-defaults.conf`文件中进行更改。在配置Spark配置文件之后，运行PySpark应用程序时，这些更改也会反映出来，只需使用简单的Python命令。
- en: However, unfortunately, at the time of this writing, there's no pip install
    advantage for using PySpark. But it is expected to be available in the Spark 2.2.0
    release (for more, refer to [https://issues.apache.org/jira/browse/SPARK-1267](https://issues.apache.org/jira/browse/SPARK-1267)).
    The reason why there is no pip install for PySpark can be found in the JIRA ticket
    at [https://issues.apache.org/jira/browse/SPARK-1267](https://issues.apache.org/jira/browse/SPARK-1267).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不幸的是，在撰写本文时，使用PySpark没有pip安装优势。但预计在Spark 2.2.0版本中将可用（有关更多信息，请参阅[https://issues.apache.org/jira/browse/SPARK-1267](https://issues.apache.org/jira/browse/SPARK-1267)）。为什么PySpark没有pip安装的原因可以在JIRA票证[https://issues.apache.org/jira/browse/SPARK-1267](https://issues.apache.org/jira/browse/SPARK-1267)中找到。
- en: By setting PySpark on Python IDEs
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过在Python IDEs上设置PySpark
- en: We can also configure and run PySpark from Python IDEs such as PyCharm. In this
    section, we will show how to do it. If you're a student, you can get the free
    licensed copy of PyCharm once you register using your university/college/institute
    email address at [https://www.jetbrains.com/student/](https://www.jetbrains.com/student/).
    Moreover, there's also a community (that is, free) edition of PyCharm, so you
    don't need to be a student in order to use it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在Python IDEs（如PyCharm）中配置和运行PySpark。在本节中，我们将展示如何操作。如果您是学生，您可以在[https://www.jetbrains.com/student/](https://www.jetbrains.com/student/)上使用您的大学/学院/研究所电子邮件地址注册后获得PyCharm的免费许可副本。此外，PyCharm还有一个社区（即免费）版本，因此您不需要是学生才能使用它。
- en: 'Recently PySpark has been published with Spark 2.2.0 PyPI (see [https://pypi.python.org/pypi/pyspark](https://pypi.python.org/pypi/pyspark)/.
    This has been a long time coming (previous releases included pip installable artifacts
    that for a variety of reasons couldn''t be published to PyPI). So if you (or your
    friends) want to be able to work with PySpark locally on your laptop you''ve got
    an easier path getting started, just execute the following command:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，PySpark已经发布了Spark 2.2.0 PyPI（请参阅[https://pypi.python.org/pypi/pyspark](https://pypi.python.org/pypi/pyspark)）。这是一个漫长的过程（以前的版本包括pip可安装的构件，由于各种原因无法发布到PyPI）。因此，如果您（或您的朋友）希望能够在笔记本电脑上本地使用PySpark，您可以更容易地开始，只需执行以下命令：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'However, if you are using Windos 7, 8 or 10, you should install pyspark manually.
    For exmple using PyCharm, you can do it as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您使用的是Windos 7、8或10，您应该手动安装pyspark。例如，使用PyCharm，您可以按照以下步骤操作：
- en: '![](img/00281.jpeg)**Figure 2:** Installing PySpark on Pycharm IDE on Windows
    10'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00281.jpeg)**图2**：在Windows 10上的Pycharm IDE上安装PySpark'
- en: 'At first, you should create a Python script with Project interpreter as Python
    2.7+. Then you can import pyspark along with other required models as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您应该创建一个带有项目解释器的Python脚本，解释器为Python 2.7+。然后，您可以按照以下方式导入pyspark以及其他所需的模块：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now that if you''re a Windows user, Python also needs to have the Hadoop runtime;
    you should put the `winutils.exe` file in the `SPARK_HOME/bin` folder. Then create
    a environmental variable as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您是Windows用户，Python还需要具有Hadoop运行时；您应该将`winutils.exe`文件放在`SPARK_HOME/bin`文件夹中。然后按以下方式创建环境变量：
- en: 'Select your python file | Run | Edit configuration | Create an environmental
    variable whose key is `HADOOP_HOME` and the value is the `PYTHON_PATH` for example
    for my case it''s `C:\Users\admin-karim\Downloads\spark-2.1.0-bin-hadoop2.7`.
    Finally, press OK then you''re done:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 选择您的python文件 | 运行 | 编辑配置 | 创建一个环境变量，其键为`HADOOP_HOME`，值为`PYTHON_PATH`，例如对于我的情况，它是`C:\Users\admin-karim\Downloads\spark-2.1.0-bin-hadoop2.7`。最后，按下OK，然后您就完成了：
- en: '![](img/00110.jpeg)**Figure 3:** Setting Hadoop runtime env on Pycharm IDE
    on Windows 10'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00110.jpeg)**图3**：在Windows 10上的Pycharm IDE上设置Hadoop运行时环境'
- en: 'That''s all you need. Now if you start writing Spark code, you should at first
    place the imports in the `try` block as follows (just for example):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是您需要的全部。现在，如果您开始编写Spark代码，您应该首先将导入放在`try`块中，如下所示（仅供参考）：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And the `catch` block can be placed as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`catch`块可以放在以下位置：'
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Refer to the following figure that shows importing and placing Spark packages
    in the PySpark shell:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下图，显示在PySpark shell中导入和放置Spark包：
- en: '![](img/00256.jpeg)**Figure 4**: Importing and placing Spark packages in PySpark
    shell'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00256.jpeg)**图4**：在PySpark shell中导入和放置Spark包'
- en: 'If these blocks execute successfully, you should observe the following message
    on the console:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些块成功执行，您应该在控制台上观察到以下消息：
- en: '![](img/00005.jpeg)**Figure 5**: PySpark package has been imported successfully'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00005.jpeg)**图5**：PySpark包已成功导入'
- en: Getting started with PySpark
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用PySpark
- en: 'Before going deeper, at first, we need to see how to create the Spark session.
    It can be done as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入之前，首先我们需要看一下如何创建Spark会话。可以按照以下步骤完成：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now under this code block, you should place your codes, for example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在这个代码块下，您应该放置您的代码，例如：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code demonstrates how to compute principal components on a RowMatrix
    and use them to project the vectors into a low-dimensional space. For a clearer
    picture, refer to the following code that shows how to use the PCA algorithm on
    PySpark:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码演示了如何在RowMatrix上计算主要成分，并将它们用于将向量投影到低维空间。为了更清晰地了解情况，请参考以下代码，该代码显示了如何在PySpark上使用PCA算法：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00188.jpeg)**Figure 6**: PCA result after successful execution of the
    Python script'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00188.jpeg)**图6**：Python脚本成功执行后的PCA结果'
- en: Working with DataFrames and RDDs
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DataFrames和RDDs
- en: 'SparkDataFrame is a distributed collection of rows under named columns. Less
    technically, it can be considered as a table in a relational database with column
    headers. Furthermore, PySpark DataFrame is similar to Python pandas. However,
    it also shares some mutual characteristics with RDD:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: SparkDataFrame是具有命名列的分布式行集合。从技术上讲，它可以被视为具有列标题的关系数据库中的表。此外，PySpark DataFrame类似于Python
    pandas。但是，它还与RDD共享一些相同的特征：
- en: '**Immutable**: Just like an RDD, once a DataFrame is created, it can''t be
    changed. We can transform a DataFrame to an RDD and vice versa after applying
    transformations.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不可变：就像RDD一样，一旦创建了DataFrame，就无法更改。在应用转换后，我们可以将DataFrame转换为RDD，反之亦然。
- en: '**Lazy Evaluations:** Its nature is a lazy evaluation. In other words, a task
    is not executed until an action is performed.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**惰性评估**：其性质是惰性评估。换句话说，任务直到执行操作才会被执行。'
- en: '**Distributed:** Both the RDD and DataFrame are distributed in nature.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式：RDD和DataFrame都具有分布式特性。
- en: Just like Java/Scala's DataFrames, PySpark DataFrames are designed for processing
    a large collection of structured data; you can even handle petabytes of data.
    The tabular structure helps us understand the schema of a DataFrame, which also
    helps optimize execution plans on SQL queries. Additionally, it has a wide range
    of data formats and sources.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与Java/Scala的DataFrame一样，PySpark DataFrame专为处理大量结构化数据而设计；甚至可以处理PB级数据。表格结构帮助我们了解DataFrame的模式，这也有助于优化SQL查询的执行计划。此外，它具有广泛的数据格式和来源。
- en: You can create RDDs, datasets, and DataFrames in a number of ways using PySpark.
    In the following subsections, we will show some examples of doing that.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用PySpark以多种方式创建RDD、数据集和DataFrame。在接下来的小节中，我们将展示一些示例。
- en: Reading a dataset in Libsvm format
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以Libsvm格式读取数据集
- en: 'Let''s see how to read data in LIBSVM format using the read API and the `load()`
    method by specifying the format of the data (that is, `libsvm`) as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用读取API和`load()`方法以指定数据格式（即`libsvm`）来以LIBSVM格式读取数据：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding MNIST dataset can be downloaded from [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2).
    This will essentially return a DataFrame and the content can be seen by calling
    the `show()` method as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2)下载前述的MNIST数据集。这将返回一个DataFrame，可以通过调用`show()`方法查看内容如下：
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00068.gif)**Figure 7**: A snap of the handwritten dataset in LIBSVM
    format'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00068.gif)**图7**：LIBSVM格式手写数据集的快照'
- en: 'You can also specify other options such as how many features of the raw dataset
    you want to give to your DataFrame as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以指定其他选项，例如原始数据集中要给DataFrame的特征数量如下：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now if you want to create an RDD from the same dataset, you can use the MLUtils
    API from `pyspark.mllib.util` as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您想从相同的数据集创建RDD，可以使用`pyspark.mllib.util`中的MLUtils API如下：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now you can save the RDD in your preferred location as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以按以下方式将RDD保存在首选位置：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Reading a CSV file
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取CSV文件
- en: 'Let''s start with loading, parsing, and viewing simple flight data. At first,
    download the NYC flights dataset as a CSV from [https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv](https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv).
    Now let''s load and parse the dataset using `read.csv()` API of PySpark:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载、解析和查看简单的航班数据开始。首先，从[https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv](https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv)下载NYC航班数据集作为CSV。现在让我们使用PySpark的`read.csv()`
    API加载和解析数据集：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This is pretty similar to reading the libsvm format. Now you can see the resulting
    DataFrame''s structure as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这与读取libsvm格式非常相似。现在您可以查看生成的DataFrame的结构如下：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00338.gif)**Figure 8**: Schema of the NYC flight dataset'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00338.gif)**图8**：NYC航班数据集的模式'
- en: 'Now let''s see a snap of the dataset using the `show()` method as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用`show()`方法查看数据集的快照如下：
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now let''s view the sample of the data as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们查看数据的样本如下：
- en: '![](img/00370.gif)**Figure 9**: Sample of the NYC flight dataset'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00370.gif)**图9**：NYC航班数据集的样本'
- en: Reading and manipulating raw text files
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取和操作原始文本文件
- en: 'You can read a raw text data file using the `textFile()` method. Suppose you
    have the logs of some purchase:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`textFile()`方法读取原始文本数据文件。假设您有一些购买的日志：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now reading and creating RDD is pretty straightforward using the `textFile()`
    method as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用`textFile()`方法读取和创建RDD非常简单如下：
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As you can see, the structure is not that readable. So we can think of giving
    a better structure by converting the texts as DataFrame. At first, we need to
    collect the header information as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，结构并不那么可读。因此，我们可以考虑通过将文本转换为DataFrame来提供更好的结构。首先，我们需要收集标题信息如下：
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now filter out the header and make sure the rest looks correct as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在过滤掉标题，并确保其余部分看起来正确如下：
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We still have the RDD but with a bit better structure of the data. However,
    converting it into DataFrame will provide a better view of the transactional data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然有RDD，但数据结构稍微好一些。但是，将其转换为DataFrame将提供更好的事务数据视图。
- en: 'The following code creates a DataFrame by specifying the `header.split` is
    providing the names of the columns:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码通过指定`header.split`来创建DataFrame，提供列的名称：
- en: '[PRE27]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00208.gif)**Figure 10**: Sample of the transactional data'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00208.gif)**图10**：事务数据的样本'
- en: 'Now you could save this DataFrame as a view and make a SQL query. Let''s do
    a query with this DataFrame now:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以将此DataFrame保存为视图并进行SQL查询。现在让我们对此DataFrame进行查询：
- en: '[PRE28]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00051.gif)**Figure 11**: Query result on the transactional data using
    Spark SQL'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00051.gif)**图11**：使用Spark SQL对事务数据进行查询的结果'
- en: Writing UDF on PySpark
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PySpark上编写UDF
- en: Like Scala and Java, you can also work with **User Defined Functions** (aka.
    **UDF**) on PySpark. Let's see an example in the following. Suppose we want to
    see the grade distribution based on the score for some students who have taken
    courses at a university.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与Scala和Java一样，您也可以在PySpark上使用**用户定义的函数**（也称为**UDF**）。让我们在下面看一个例子。假设我们想要根据一些在大学上课程的学生的分数来查看成绩分布。
- en: 'We can store them in two separate arrays as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将它们存储在两个单独的数组中，如下所示：
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now let''s declare an empty array for storing the data about courses and students
    so that later on both can be appended to this array as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们声明一个空数组，用于存储有关课程和学生的数据，以便稍后可以将两者都附加到此数组中，如下所示：
- en: '[PRE30]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Note that for the preceding code to work, please import the following at the
    beginning of the file:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了使前面的代码工作，请在文件开头导入以下内容：
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now let's create a DataFrame from these two objects toward converting corresponding
    grades against each one's score. For this, we need to define an explicit schema.
    Let's suppose that in your planned DataFrame, there would be three columns named
    `Student`, `Course`, and `Score`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从这两个对象创建一个DataFrame，以便将相应的成绩转换为每个成绩的分数。为此，我们需要定义一个显式模式。假设在您计划的DataFrame中，将有三列名为`Student`，`Course`和`Score`。
- en: 'At first, let''s import necessary modules:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的模块：
- en: '[PRE32]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now the schema can be defined as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模式可以定义如下：
- en: '[PRE33]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now let''s create an RDD from the Raw Data as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从原始数据创建一个RDD，如下所示：
- en: '[PRE34]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now let''s convert the RDD into the DataFrame as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将RDD转换为DataFrame，如下所示：
- en: '[PRE35]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00311.gif)**Figure 12**: Sample of the randomly generated score for
    students in subjects'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00311.gif)**图12**：随机生成的学科学生分数样本'
- en: 'Well, now we have three columns. However, we need to convert the score into
    grades. Say you have the following grading schema:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在我们有了三列。但是，我们需要将分数转换为等级。假设您有以下分级模式：
- en: '*90~100=> A*'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*90~100=> A*'
- en: '*80~89 => B*'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*80~89 => B*'
- en: '*60~79 => C*'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*60~79 => C*'
- en: '*0~59 => D*'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*0~59 => D*'
- en: 'For this, we can create our own UDF such that this will convert the numeric
    score to grade. It can be done in several ways. Following is an example of doing
    so:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以创建自己的UDF，使其将数字分数转换为等级。可以用几种方法来做。以下是一个这样做的例子：
- en: '[PRE36]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now we can have our own UDF as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以有自己的UDF如下：
- en: '[PRE37]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The second argument in the `udf()` method is the return type of the method
    (that is, `scoreToCategory`). Now you can call this UDF to convert the score into
    grade in a pretty straightforward way. Let''s see an example of it:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`udf()`方法中的第二个参数是方法的返回类型（即`scoreToCategory`）。现在您可以调用此UDF以一种非常直接的方式将分数转换为等级。让我们看一个例子：'
- en: '[PRE38]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The preceding line will take score as input for all entries and convert the
    score to a grade. Additionally, a new DataFrame with a column named `Grade` will
    be added.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 前一行将接受分数作为所有条目的输入，并将分数转换为等级。此外，将添加一个名为`Grade`的新DataFrame列。
- en: 'The output is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00354.gif)**Figure 13**: Assigned grades'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00354.gif)**图13**：分配的成绩'
- en: 'Now we can use the UDF with the SQL statement as well. However, for that, we
    need to register this UDF as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们也可以在SQL语句中使用UDF。但是，为此，我们需要将此UDF注册如下：
- en: '[PRE39]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The preceding line will register the UDF as a temporary function in the database
    by default. Now we need to create a team view to allow executing SQL queries:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 前一行将默认情况下在数据库中将UDF注册为临时函数。现在我们需要创建一个团队视图，以允许执行SQL查询：
- en: '[PRE40]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now let''s execute an SQL query on the view `score` as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对视图`score`执行SQL查询，如下所示：
- en: '[PRE41]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00153.gif)**Figure 14**: Query on the students score and corresponding
    grades'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00153.gif)**图14**：关于学生分数和相应成绩的查询'
- en: 'The complete source code for this example is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的完整源代码如下：
- en: '[PRE42]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: A more detailed discussion on using UDF can be found at [https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html.](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用UDF的更详细讨论可以在[https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html)找到。
- en: Now let's do some analytics tasks on PySpark. In the next section, we will show
    an example using the k-means algorithm for a clustering task using PySpark.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在PySpark上进行一些分析任务。在下一节中，我们将展示使用k-means算法进行聚类任务的示例。
- en: Let's do some analytics with k-means clustering
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们使用k-means聚类进行一些分析
- en: Anomalous data refers to data that is unusual from normal distributions. Thus,
    detecting anomalies is an important task for network security, anomalous packets
    or requests can be flagged as errors or potential attacks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 异常数据是指与正态分布不同寻常的数据。因此，检测异常是网络安全的重要任务，异常的数据包或请求可能被标记为错误或潜在攻击。
- en: 'In this example, we will use the KDD-99 dataset (can be downloaded here: [http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)
    ). A number of columns will be filtered out based on certain criteria of the data
    points. This will help us understand the example. Secondly, for the unsupervised
    task; we will have to remove the labeled data. Let''s load and parse the dataset
    as simple texts. Then let''s see how many rows there are in the dataset:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们将使用KDD-99数据集（可以在此处下载：[http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)）。将根据数据点的某些标准过滤出许多列。这将帮助我们理解示例。其次，对于无监督任务；我们将不得不删除标记的数据。让我们将数据集加载并解析为简单的文本。然后让我们看看数据集中有多少行：
- en: '[PRE43]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This essentially returns an RDD. Let''s see how many rows in the dataset are
    using the `count()` method as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这本质上返回一个RDD。让我们看看数据集中有多少行，使用`count()`方法如下所示：
- en: '[PRE44]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'So, the dataset is pretty big with lots of features. Since we have parsed the
    dataset as simple texts, we should not expect to see the better structure of the
    dataset. Thus, let''s work toward converting the RDD into DataFrame as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，数据集非常大，具有许多特征。由于我们已将数据集解析为简单文本，因此不应期望看到数据集的更好结构。因此，让我们朝着将RDD转换为DataFrame的方向努力：
- en: '[PRE45]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then let''s see some selected columns in the DataFrame as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们看一下DataFrame中的一些选定列：
- en: '[PRE46]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00080.gif)**Figure 15**: Sample of the KKD cup 99 dataset'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00080.gif)**图15**：KKD杯99数据集样本'
- en: 'Thus, this dataset is already labeled. This means that the types of malicious
    cyber behavior have been assigned to a row where the label is the last column
    (that is, `_42`). The first five rows off the DataFrame are labeled normal. This
    means that these data points are normal. Now this is the time that we need to
    determine the counts of the labels for the entire dataset for each type of labels:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个数据集已经被标记。这意味着恶意网络行为的类型已被分配到标签为最后一列（即`_42`）的行中。DataFrame的前五行被标记为正常。这意味着这些数据点是正常的。现在是我们需要确定整个数据集中每种标签的计数的时候了：
- en: '[PRE47]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00134.gif)**Figure 16**: Available labels (attack types) in the KDD
    cup dataset'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00134.gif)**图16**：KDD杯数据集中可用的标签（攻击类型）'
- en: We can see that there are 23 distinct labels (behavior for data objects). The
    most data points belong to Smurf. This is an abnormal behavior also known as DoS
    packet floods. The Neptune is the second highest abnormal behavior. The *normal*
    events are the third most occurring types of events in the dataset. However, in
    a real network dataset, you will not see any such labels.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到有23个不同的标签（数据对象的行为）。大多数数据点属于Smurf。这是一种异常行为，也称为DoS数据包洪水。Neptune是第二高的异常行为。*正常*事件是数据集中第三种最常发生的事件类型。然而，在真实的网络数据集中，你不会看到任何这样的标签。
- en: Also, the normal traffic will be much higher than any anomalous traffic. As
    a result, identifying the anomalous attack or anomaly from the large-scale unlabeled
    data would be tedious. For simplicity, let's ignore the last column (that is,
    labels) and think that this dataset is unlabeled too. In that case, the only way
    to conceptualize the anomaly detection is using unsupervised learning algorithms
    such as k-means for clustering.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正常流量将远远高于任何异常流量。因此，从大规模未标记的数据中识别异常攻击或异常将是费时的。为简单起见，让我们忽略最后一列（即标签），并认为这个数据集也是未标记的。在这种情况下，唯一可以概念化异常检测的方法是使用无监督学习算法，如k-means进行聚类。
- en: 'Now let''s work toward clustering the data points for this. One important thing
    about K-means is that it only accepts numeric values for modeling. However, our
    dataset also contains some categorical features. Now we can assign the categorical
    features binary values of 1 or 0 based on whether they are *TCP* or not. This
    can be done as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始对数据点进行聚类。关于K-means的一个重要事项是，它只接受数值值进行建模。然而，我们的数据集还包含一些分类特征。现在我们可以根据它们是否为*TCP*，为分类特征分配二进制值1或0。可以按如下方式完成：
- en: '[PRE48]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Thus, our dataset is almost ready. Now we can prepare our training and test
    set to training the k-means model with ease:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的数据集几乎准备好了。现在我们可以准备我们的训练集和测试集，轻松地训练k-means模型：
- en: '[PRE49]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output is as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE50]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: However, some standardization is also required since we converted some categorical
    features to numeric features. Standardization can improve the convergence rate
    during the optimization process and can also prevent features with very large
    variances exerting an influence during model training.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们将一些分类特征转换为数值特征，因此还需要进行一些标准化。标准化可以提高优化过程中的收敛速度，还可以防止具有非常大方差的特征在模型训练过程中产生影响。
- en: 'Now we will use StandardScaler, which is a feature transformer. It helps us
    standardize features by scaling them to unit variance. It then sets the mean to
    zero using column summary statistics in the training set samples:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用StandardScaler，这是一个特征转换器。它通过将特征缩放到单位方差来帮助我们标准化特征。然后使用训练集样本中的列汇总统计将均值设置为零：
- en: '[PRE51]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now let''s compute the summary statistics by fitting the preceding transformer
    as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过拟合前面的转换器来计算汇总统计信息：
- en: '[PRE52]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now the problem is the data that we have for training the k-means does not
    have a normal distribution. Thus, we need to normalize each feature in the training
    set to have the unit standard deviation. To make this happen, we need to further
    transform the preceding standardizer model as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在问题是，我们用于训练k-means的数据没有正态分布。因此，我们需要对训练集中的每个特征进行标准化，使其具有单位标准差。为实现这一点，我们需要进一步转换前面的标准化模型，如下所示：
- en: '[PRE53]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Well done! Now the training set is finally ready to train the k-means model.
    As we discussed in the clustering chapter, the trickiest thing in the clustering
    algorithm is finding the optimal number of clusters by setting the value of K
    so that the data objects get clustered automatically.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！现在训练集终于准备好训练k-means模型了。正如我们在聚类章节中讨论的那样，聚类算法中最棘手的事情是通过设置K的值找到最佳聚类数，使数据对象能够自动聚类。
- en: 'One Naive approach considered a brute force is setting `K=2` and observing
    the results and trying until you get an optimal one. However, a much better approach
    is the Elbow approach, where we can keep increasing the value of `K` and compute
    the **Within Set Sum of Squared Errors** (**WSSSE**) as the clustering cost. In
    short, we will be looking for the optimal `K` values that also minimize the WSSSE.
    Whenever a sharp decrease is observed, we will get to know the optimal value for
    `K`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一个天真的方法是采用蛮力法，设置`K=2`并观察结果，直到获得最佳结果。然而，一个更好的方法是肘部法，我们可以不断增加`K`的值，并计算**集合内平方误差和**（**WSSSE**）作为聚类成本。简而言之，我们将寻找最小化WSSSE的最佳`K`值。每当观察到急剧下降时，我们将知道最佳的`K`值：
- en: '[PRE54]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output is as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE55]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'In this case, 30 is the best value for k. Let''s check the cluster assignments
    for each data point when we have 30 clusters. The next test would be to run for
    `k` values of 30, 35, and 40\. Three values of k are not the most you would test
    in a single run, but only used for this example:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，30是k的最佳值。让我们检查每个数据点的簇分配，当我们有30个簇时。下一个测试将是运行`k`值为30、35和40。三个k值不是您在单次运行中测试的最多值，但仅用于此示例：
- en: '[PRE56]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output is as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00100.jpeg)**Figure 17**: Final cluster centers for each attack type
    (abridged)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.jpeg)**图17：**每种攻击类型的最终簇中心（摘要）'
- en: 'Now let''s compute and print the total cost for the overall clustering as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算并打印整体聚类的总成本如下：
- en: '[PRE57]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The output is as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE58]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Finally, the WSSSE of our k-means model can be computed and printed as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的k均值模型的WSSSE可以计算并打印如下：
- en: '[PRE59]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output is as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE60]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Your results might be slightly different. This is due to the random placement
    of the centroids when we first begin the clustering algorithm. Performing this
    many times allows you to see how points in your data change their value of k or
    stay the same. The full source code for this solution is given in the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 您的结果可能略有不同。这是由于在我们首次开始聚类算法时，质心的随机放置。多次执行可以让您看到数据中的点如何改变其k值或保持不变。此解决方案的完整源代码如下所示：
- en: '[PRE61]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: A more comprehensive discussion on this topic can be found at [https://github.com/jadianes/kdd-cup-99-spark](https://github.com/jadianes/kdd-cup-99-spark).
    Also, interested readers can refer to the main and latest documentation on PySpark
    APIs at [http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此主题的更全面讨论，请参阅[https://github.com/jadianes/kdd-cup-99-spark](https://github.com/jadianes/kdd-cup-99-spark)。此外，感兴趣的读者可以参考PySpark
    API的主要和最新文档，网址为[http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/)。
- en: Well, now it's time to move to SparkR, another Spark API to work with population
    statistical programming language called R.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候转向SparkR，这是另一个与名为R的流行统计编程语言一起使用的Spark API。
- en: Introduction to SparkR
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkR简介
- en: R is one of the most popular statistical programming languages with a number
    of exciting features that support statistical computing, data processing, and
    machine learning tasks. However, processing large-scale datasets in R is usually
    tedious as the runtime is single-threaded. As a result, only datasets that fit
    in someone's machine memory can be processed. Considering this limitation and
    for getting the full flavor of Spark in R, SparkR was initially developed at the
    AMPLab as a lightweight frontend of R to Apache Spark and using Spark's distributed
    computation engine.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: R是最流行的统计编程语言之一，具有许多令人兴奋的功能，支持统计计算、数据处理和机器学习任务。然而，在R中处理大规模数据集通常很繁琐，因为运行时是单线程的。因此，只有适合机器内存的数据集才能被处理。考虑到这一限制，并为了充分体验R中Spark的功能，SparkR最初在AMPLab开发，作为R到Apache
    Spark的轻量级前端，并使用Spark的分布式计算引擎。
- en: This way it enables the R programmer to use Spark from RStudio for large-scale
    data analysis from the R shell. In Spark 2.1.0, SparkR provides a distributed
    data frame implementation that supports operations such as selection, filtering,
    and aggregation. This is somewhat similar to R data frames like `dplyr` but can
    be scaled up for large-scale datasets.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这样可以使R程序员从RStudio使用Spark进行大规模数据分析。在Spark 2.1.0中，SparkR提供了一个支持选择、过滤和聚合等操作的分布式数据框实现。这与R数据框（如`dplyr`）有些类似，但可以扩展到大规模数据集。
- en: Why SparkR?
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择SparkR？
- en: 'You can write Spark codes using SparkR too that supports distributed machine
    learning using MLlib. In summary, SparkR inherits many benefits from being tightly
    integrated with Spark including the following:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用SparkR编写支持MLlib的分布式机器学习的Spark代码。总之，SparkR从与Spark紧密集成中继承了许多好处，包括以下内容：
- en: '**Supports various data sources API**: SparkR can be used to read in data from
    a variety of sources including Hive tables, JSON files, RDBMS, and Parquet files.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持各种数据源API**：SparkR可以用于从各种来源读取数据，包括Hive表、JSON文件、关系型数据库和Parquet文件。'
- en: '**DataFrame optimizations**: SparkR DataFrames also inherit all of the optimizations
    made to the computation engine in terms of code generation, memory management,
    and so on. From the following graph, it can be observed that the optimization
    engine of Spark enables SparkR competent with Scala and Python:'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据框优化：SparkR数据框也继承了计算引擎的所有优化，包括代码生成、内存管理等。从下图可以观察到，Spark的优化引擎使得SparkR能够与Scala和Python竞争力十足：
- en: '![](img/00108.jpeg)**Figure 18:** SparkR DataFrame versus Scala/Python DataFrame'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00108.jpeg)**图18：**SparkR数据框与Scala/Python数据框'
- en: '**Scalability:** Operations executed on SparkR DataFrames get automatically
    distributed across all the cores and machines available on the Spark cluster.
    Thus, SparkR DataFrames can be used on terabytes of data and run on clusters with
    thousands of machines.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性：**在SparkR数据框上执行的操作会自动分布到Spark集群上所有可用的核心和机器上。因此，SparkR数据框可以用于大量数据，并在具有数千台机器的集群上运行。'
- en: Installing and getting started
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和入门
- en: The best way of using SparkR is from RStudio. Your R program can be connected
    to a Spark cluster from RStudio using R shell, Rescript, or other R IDEs.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SparkR的最佳方式是从RStudio开始。您可以使用R shell、Rescript或其他R IDE将您的R程序连接到Spark集群。
- en: '**Option 1.** Set `SPARK_HOME` in the environment (you can check [https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.getenv.html](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.getenv.html)),
    load the SparkR package, and call `sparkR.session` as follows. It will check for
    the Spark installation, and, if not found, it will be downloaded and cached automatically:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**选项1.** 在环境中设置`SPARK_HOME`（您可以查看[https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.getenv.html](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.getenv.html)），加载SparkR包，并调用`sparkR.session`如下。它将检查Spark安装，如果找不到，将自动下载和缓存：'
- en: '[PRE62]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '**Option 2.** You can also manually configure SparkR on RStudio. For doing
    so, create an R script and execute the following lines of R code on RStudio:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**选项2.** 您还可以在RStudio上手动配置SparkR。为此，请在R脚本中执行以下R代码行：'
- en: '[PRE63]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now load the SparkR library as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在加载SparkR库如下：
- en: '[PRE64]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, like Scala/Java/PySpark, the entry point to your SparkR program is the
    SparkR session that can be created by calling `sparkR.session` as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，就像Scala/Java/PySpark一样，您的SparkR程序的入口点是通过调用`sparkR.session`创建的SparkR会话，如下所示：
- en: '[PRE65]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Furthermore, if you want, you could also specify certain Spark driver properties.
    Normally, these application properties and runtime environment cannot be set programmatically,
    as the driver JVM process would have been started; in this case, SparkR takes
    care of this for you. To set them, pass them as you would pass other configuration
    properties in the `sparkConfig` argument to `sparkR.session()` as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您愿意，还可以指定特定的Spark驱动程序属性。通常，这些应用程序属性和运行时环境无法以编程方式设置，因为驱动程序JVM进程已经启动；在这种情况下，SparkR会为您处理这些设置。要设置它们，将它们传递给`sparkR.session()`的`sparkConfig`参数，如下所示：
- en: '[PRE66]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'In addition, the following Spark driver properties can be set in `sparkConfig`
    with `sparkR.session` from RStudio:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，以下Spark驱动程序属性可以在RStudio中使用`sparkConfig`和`sparkR.session`进行设置：
- en: '![](img/00146.gif)**Figure 19**: Spark driver properties can be set in `sparkConfig`
    with `sparkR.session` from RStudio'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00146.gif)**图19**：可以在RStudio中使用`sparkConfig`和`sparkR.session`设置Spark驱动程序属性'
- en: Getting started
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'Let''s start with loading, parsing, and viewing simple flight data. At first,
    download the NY flights dataset as a CSV from [https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv](https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv).
    Now let''s load and parse the dataset using `read.csv()` API of R:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载、解析和查看简单的航班数据开始。首先，从[https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv](https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv)下载NY航班数据集作为CSV。现在让我们使用R的`read.csv()`
    API加载和解析数据集：
- en: '[PRE67]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Now let''s view the structure of the dataset using `View()` method of R as
    follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用R的`View()`方法查看数据集的结构如下：
- en: '[PRE68]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '![](img/00217.jpeg)**Figure 20**: A snap of the NYC flight dataset'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00217.jpeg)**图20**：NYC航班数据集的快照'
- en: 'Now let''s create the Spark DataFrame from the R DataFrame as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从R DataFrame创建Spark DataFrame如下：
- en: '[PRE69]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Let''s see the structure by exploring the schema of the DataFrame:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过探索DataFrame的模式来查看结构：
- en: '[PRE70]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output is as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00197.gif)**Figure 21**: The schema of the NYC flight dataset'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00197.gif)**图21**：NYC航班数据集的模式'
- en: 'Now let''s see the first 10 rows of the DataFrame:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看DataFrame的前10行：
- en: '[PRE71]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output is as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00222.gif)**Figure 22**: The first 10 rows of the NYC flight dataset'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00222.gif)**图22**：NYC航班数据集的前10行'
- en: So, you can see the same structure. However, this is not scalable since we loaded
    the CSV file using standard R API. To make it faster and scalable, like in Scala,
    we can use external data source APIs.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以看到相同的结构。但是，这不可扩展，因为我们使用标准R API加载了CSV文件。为了使其更快速和可扩展，就像在Scala中一样，我们可以使用外部数据源API。
- en: Using external data source APIs
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用外部数据源API
- en: 'As mentioned earlier, we can create DataFrame using external data source APIs
    as well. For the following example, we used `com.databricks.spark.csv` API as
    follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们也可以使用外部数据源API来创建DataFrame。在以下示例中，我们使用`com.databricks.spark.csv` API如下：
- en: '[PRE72]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Let''s see the structure by exploring the schema of the DataFrame:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过探索DataFrame的模式来查看结构：
- en: '[PRE73]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The output is as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00132.gif)**Figure 23**: The same schema of the NYC flight dataset
    using external data source API'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00132.gif)**图23**：使用外部数据源API查看NYC航班数据集的相同模式'
- en: 'Now let''s see the first 10 rows of the DataFrame:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看DataFrame的前10行：
- en: '[PRE74]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The output is as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00003.jpeg)**Figure 24**: Same sample data from NYC flight dataset
    using external data source API'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00003.jpeg)**图24**：使用外部数据源API的NYC航班数据集的相同样本数据'
- en: So, you can see the same structure. Well done! Now it's time to explore something
    more, such as data manipulation using SparkR.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以看到相同的结构。干得好！现在是时候探索更多内容了，比如使用SparkR进行数据操作。
- en: Data manipulation
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据操作
- en: 'Show the column names in the SparkDataFrame as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 显示SparkDataFrame中的列名如下：
- en: '[PRE75]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Show the number of rows in the SparkDataFrame as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 显示SparkDataFrame中的行数如下：
- en: '[PRE76]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Filter flights data whose destination is only Miami and show the first six
    entries as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤目的地仅为迈阿密的航班数据，并显示前六个条目如下：
- en: '[PRE77]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The output is as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00285.jpeg)**Figure 25**: Flights with destination Miami only'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00285.jpeg)**图25**：目的地仅为迈阿密的航班'
- en: 'Select specific columns. For example, let''s select all the flights that are
    going to Iowa that are delayed. Also, include the origin airport names:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 选择特定列。例如，让我们选择所有前往爱荷华州的延误航班。还包括起飞机场名称：
- en: '[PRE78]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The output is as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00269.gif)**Figure 26**: All the flights that are going to Iowa that
    are delayed'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00269.gif)**图26**：所有前往爱荷华州的延误航班'
- en: 'We can even use it to chain data frame operations. To show an example, at first,
    group the flights by date and then find the average daily delay. Then, finally,
    write the result into a SparkDataFrame as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以使用它来链接数据框操作。举个例子，首先按日期分组航班，然后找到平均每日延误。最后，将结果写入SparkDataFrame如下：
- en: '[PRE79]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now print the computed DataFrame:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打印计算出的DataFrame：
- en: '[PRE80]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The output is as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00300.gif)**Figure 27**: Group the flights by date and then find the
    average daily delay'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00300.gif)**图27**：按日期分组航班，然后找到平均每日延误'
- en: 'Let''s see another example that aggregates average arrival delay for the entire
    destination airport:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看另一个示例，对整个目的地机场的平均到达延误进行聚合：
- en: '[PRE81]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Even more complex aggregation can be performed. For example, the following
    code aggregates the average, maximum, and minimum delay per each destination airport.
    It also shows the number of flights that land in those airports:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以执行更复杂的聚合。例如，以下代码对每个目的地机场的平均、最大和最小延误进行了聚合。它还显示了降落在这些机场的航班数量：
- en: '[PRE82]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The output is as follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00290.gif)**Figure 28**: Maximum and minimum delay per each destination
    airport'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00290.gif)**图28**：每个目的地机场的最大和最小延误'
- en: Querying SparkR DataFrame
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询SparkR DataFrame
- en: 'Similar to Scala, we can perform a SQL query on the DataFrame once it is saved
    as `TempView` using the `createOrReplaceTempView()` method. Let''s see an example
    of that. At first, let''s save the fight DataFrame (that is, `flightDF`) as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 与Scala类似，一旦将DataFrame保存为`TempView`，我们就可以对其执行SQL查询，使用`createOrReplaceTempView()`方法。让我们看一个例子。首先，让我们保存航班DataFrame（即`flightDF`）如下：
- en: '[PRE83]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now let''s select destination and destinations of all the flights with their
    associated carrier information as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们选择所有航班的目的地和目的地的承运人信息如下：
- en: '[PRE84]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The output is as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00293.jpeg)**Figure 29**: All the flights with their associated carrier
    information'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00293.jpeg)**图29**：所有航班的目的地和承运人信息'
- en: 'Now let''s make the SQL a bit more complex, such as finding the destination''s
    airport of all the flights that are at least 120 minutes delayed as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使SQL复杂一些，比如找到所有至少延误120分钟的航班的目的地机场如下：
- en: '[PRE85]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The preceding code segment queries and shows the name of the airports of all
    the flights that are delayed by at least 2 hours:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码段查询并显示了所有至少延误2小时的航班的机场名称：
- en: '![](img/00302.jpeg)**Figure 30**: Destination airports of all the flights that
    are delayed by at least 2 hours'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00302.jpeg)**图30**：所有至少延误2小时的航班的目的地机场'
- en: 'Now let''s do a more complex query. Let''s find the origins of all the flights
    to Iowa that are delayed by at least 2 hours. Finally, sort them by arrival delay
    and limit the count up to 20 as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进行更复杂的查询。让我们找到所有飞往爱荷华的航班的起点，至少延误2小时。最后，按到达延误排序，并将计数限制为20如下：
- en: '[PRE86]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The preceding code segment queries and shows the name of the airports of all
    the flights that are delayed by at least 2 hours to Iowa:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码段查询并显示了所有至少延误2小时到爱荷华的航班的机场名称：
- en: '![](img/00308.jpeg)**Figure 31**: Origins of all the flights that are delayed
    by at least 2 hours where the destination is Iowa'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00308.jpeg)**图31**：所有航班的起点都至少延误2小时，目的地是爱荷华'
- en: Visualizing your data on RStudio
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在RStudio上可视化您的数据
- en: 'In the previous section, we have seen how to load, parse, manipulate, and query
    the DataFrame. Now it would be great if we could show the data for better visibility.
    For example, what could be done for the airline carriers? I mean, is it possible
    to find the most frequent carriers from the plot? Let''s give `ggplot2` a try.
    At first, load the library for the same:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们已经看到了如何加载、解析、操作和查询DataFrame。现在如果我们能够展示数据以便更好地看到就更好了。例如，对航空公司可以做些什么？我的意思是，是否可能从图表中找到最频繁的航空公司？让我们试试`ggplot2`。首先，加载相同的库：
- en: '[PRE87]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Now we already have the SparkDataFrame. What if we directly try to use our SparkSQL
    DataFrame class in `ggplot2`?
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了SparkDataFrame。如果我们直接尝试在`ggplot2`中使用我们的SparkSQL DataFrame类会怎么样？
- en: '[PRE88]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Obviously, it doesn''t work that way because the `ggplot2` function doesn''t
    know how to deal with those types of distributed data frames (the Spark ones).
    Instead, we need to collect the data locally and convert it back to a traditional
    R data frame as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这样是行不通的，因为`ggplot2`函数不知道如何处理这些类型的分布式数据框架（Spark的数据框架）。相反，我们需要在本地收集数据并将其转换回传统的R数据框架如下：
- en: '[PRE89]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Now let''s have a look at what we got using the `str()` method as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用`str()`方法查看我们得到了什么：
- en: '[PRE90]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The output is as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE91]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'This is good because when we collect results from a SparkSQL DataFrame, we
    get a regular R `data.frame`. It is also very convenient since we can manipulate
    it as needed. And now we are ready to create the `ggplot2` object as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，因为当我们从SparkSQL DataFrame中收集结果时，我们得到一个常规的R `data.frame`。这也非常方便，因为我们可以根据需要对其进行操作。现在我们准备创建`ggplot2`对象如下：
- en: '[PRE92]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Finally, let''s give the plot a proper representation as a bar diagram as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们给图表一个适当的表示，作为条形图如下：
- en: '[PRE93]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The output is as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00314.jpeg)**Figure 32**: Most frequent carriers are UA, B6, EV, and
    DL'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00314.jpeg)**图32**：最频繁的航空公司是UA、B6、EV和DL'
- en: 'From the graph, it is clear that the most frequent carriers are UA, B6, EV,
    and DL. This gets clearer from the following line of code in R:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中可以清楚地看出，最频繁的航空公司是UA、B6、EV和DL。这在R中的以下代码行中更清晰：
- en: '[PRE94]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The output is as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00317.gif)**Figure 33:** Most most frequent carriers are UA, B6, EV,
    and DL'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00317.gif)**图33**：最频繁的航空公司是UA、B6、EV和DL'
- en: 'The full source code of the preceding analysis is given in the following to
    understand the flow of the code:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 前面分析的完整源代码如下，以了解代码的流程：
- en: '[PRE95]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Summary
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we showed some examples of how to write your Spark code in
    Python and R. These are the most popular programming languages in the data scientist
    community.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了如何在Python和R中编写您的Spark代码的一些示例。这些是数据科学家社区中最流行的编程语言。
- en: We covered the motivation of using PySpark and SparkR for big data analytics
    with almost similar ease with Java and Scala. We discussed how to install these
    APIs on their popular IDEs such as PyCharm for PySpark and RStudio for SparkR.
    We also showed how to work with DataFrames and RDDs from these IDEs. Furthermore,
    we discussed how to execute Spark SQL queries from PySpark and SparkR. Then we
    also discussed how to perform some analytics with visualization of the dataset.
    Finally, we saw how to use UDFs with PySpark with examples.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了使用PySpark和SparkR进行大数据分析的动机，几乎与Java和Scala同样简单。我们讨论了如何在流行的IDE（如PyCharm和RStudio）上安装这些API。我们还展示了如何从这些IDE中使用DataFrames和RDDs。此外，我们还讨论了如何从PySpark和SparkR中执行Spark
    SQL查询。然后，我们还讨论了如何对数据集进行可视化分析。最后，我们看到了如何使用UDFs来进行PySpark的示例。
- en: 'Thus, we have discussed several aspects for two Spark''s APIs; PySpark and
    SparkR. There are much more to explore. Interested readers should refer to their
    websites for more information:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们讨论了两个Spark的API：PySpark和SparkR的几个方面。还有更多内容可以探索。感兴趣的读者应该参考它们的网站获取更多信息。
- en: 'PySpark: [http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PySpark: [http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/)'
- en: "SparkR: [https://spark.apache.org/docs/latest/sparkr.html\uFEFF](https://spark.apache.org/docs/latest/sparkr.html)"
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: "SparkR: [https://spark.apache.org/docs/latest/sparkr.html\uFEFF](https://spark.apache.org/docs/latest/sparkr.html)"
