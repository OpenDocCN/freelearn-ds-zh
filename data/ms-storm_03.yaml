- en: Storm Parallelism and Data Partitioning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm并行性和数据分区
- en: 'In the first two chapters, we have covered the introduction to Storm, the installation
    of Storm, and developing a sample topology. In this chapter, we are focusing on
    distribution of the topology on multiple Storm machines/nodes. This chapter covers
    the following points:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们已经介绍了Storm的概述、Storm的安装以及开发一个示例拓扑。在本章中，我们将专注于将拓扑分布在多个Storm机器/节点上。本章涵盖以下内容：
- en: Parallelism of topology
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拓扑的并行性
- en: How to configure parallelism at the code level
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在代码级别配置并行性
- en: Different types of stream groupings in a Storm cluster
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm集群中不同类型的流分组
- en: Guaranteed message processing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息处理保证
- en: Tick tuple
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tick tuple
- en: Parallelism of a topology
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拓扑的并行性
- en: Parallelism means the distribution of jobs on multiple nodes/instances where
    each instance can work independently and can contribute to the processing of data.
    Let's first look at the processes/components that are responsible for the parallelism
    of a Storm cluster.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 并行性意味着将作业分布在多个节点/实例上，每个实例可以独立工作并有助于数据的处理。让我们首先看一下负责Storm集群并行性的进程/组件。
- en: Worker process
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作进程
- en: A Storm topology is executed across multiple supervisor nodes in the Storm cluster.
    Each of the nodes in the cluster can run one or more JVMs called **worker processes**,
    which are responsible for processing a part of the topology.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Storm拓扑在Storm集群中的多个监督节点上执行。集群中的每个节点可以运行一个或多个称为**工作进程**的JVM，负责处理拓扑的一部分。
- en: A worker process is specific to one of the specific topologies and can execute
    multiple components of that topology. If multiple topologies are being run at
    the same time, none of them will share any of the workers, thus providing some
    degree of isolation between topologies.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 工作进程特定于特定的拓扑，并且可以执行该拓扑的多个组件。如果同时运行多个拓扑，它们中的任何一个都不会共享任何工作进程，因此在拓扑之间提供了一定程度的隔离。
- en: Executor
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行器
- en: Within each worker process, there can be multiple threads executing parts of
    the topology. Each of these threads is called an **executor**. An executor can
    execute only one of the components, that is, any spout or bolt in the topology.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个工作进程中，可以有多个线程执行拓扑的部分。这些线程中的每一个都被称为**执行器**。执行器只能执行拓扑中的一个组件，即拓扑中的任何spout或bolt。
- en: Each executor, being a single thread, can execute only tasks assigned to it
    serially. The number of executors defined for a spout or bolt can be changed dynamically
    while the topology is running, which means that you can easily control the degree
    of parallelism of various components in your topology.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每个执行器作为一个单独的线程，只能按顺序执行分配给它的任务。在拓扑运行时，可以动态更改为spout或bolt定义的执行器数量，这意味着您可以轻松控制拓扑中各个组件的并行度。
- en: Task
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: This is the most granular unit of task execution in Storm. Each task is an instance
    of a spout or bolt. When defining a Storm topology, you can specify the number
    of tasks for each spout and bolt. Once defined, the number of tasks cannot be
    changed for a component at runtime. Each task can be executed alone or with another
    task of the same type, or another instance of the same spout or bolt.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Storm中任务执行的最细粒度单位。每个任务都是spout或bolt的一个实例。在定义Storm拓扑时，可以为每个spout和bolt指定任务的数量。一旦定义，组件的任务数量就不能在运行时更改。每个任务可以单独执行，也可以与相同类型的另一个任务或相同spout或bolt的另一个实例一起执行。
- en: The following diagram depicts the relationship between a worker process, executors,
    and tasks. In the following diagram, there are two executors for each component,
    with each hosting a different number of tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了工作进程、执行器和任务之间的关系。在下图中，每个组件有两个执行器，每个执行器承载不同数量的任务。
- en: 'Also, as you can see, there are two executors and eight tasks defined for one
    component (each executor is hosting four tasks). If you are not getting enough
    performance out of this configuration, you can easily change the number of executors
    for the component to four or eight to increase performance and the tasks will
    be uniformly distributed between all executors of that component. The following
    diagrams show the relationship between executor, task, and worker:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以看到为一个组件定义了两个执行器和八个任务（每个执行器承载四个任务）。如果您对这个配置没有获得足够的性能，您可以轻松地将组件的执行器数量更改为四个或八个，以增加性能，并且任务将在该组件的所有执行器之间均匀分布。以下图表显示了执行器、任务和工作进程之间的关系：
- en: '![](img/00021.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00021.jpeg)'
- en: Configure parallelism at the code level
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在代码级别配置并行性
- en: Storm provides an API to set the number of worker processes, number of executors,
    and number of tasks at the code level. The following section shows how we can
    configure parallelism at the code level.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Storm提供了一个API来在代码级别设置工作进程的数量、执行器的数量和任务的数量。以下部分展示了我们如何在代码级别配置并行性。
- en: 'We can set the number of worker processes at the code level by using the `setNumWorkers`
    method of the `org.apache.storm.Config` class. Here is the code snippet to show
    these settings in practice:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`org.apache.storm.Config`类的`setNumWorkers`方法在代码级别设置工作进程的数量。以下是代码片段，展示了这些设置的实际应用：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the previous chapter, we configured the number of workers as three. Storm
    will assign the three workers for the `SampleStormTopology` and `SampleStormClusterTopology`
    topology.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们将工作进程的数量配置为三。Storm将为`SampleStormTopology`和`SampleStormClusterTopology`拓扑分配三个工作进程。
- en: 'We can set the number of executors at the code level by passing the `parallelism_hint`
    argument in the `setSpout(args,args,parallelism_hint)` or `setBolt(args,args,parallelism_hint)`
    methods of the `org.apache.storm.topology.TopologyBuilder` class. Here is the
    code snippet to show these settings in practice:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在`org.apache.storm.topology.TopologyBuilder`类的`setSpout(args,args,parallelism_hint)`或`setBolt(args,args,parallelism_hint)`方法中传递`parallelism_hint`参数来在代码级别设置执行器的数量。以下是代码片段，展示了这些设置的实际应用：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the previous chapter, we set `parallelism_hint=2` for `SampleSpout` and `parallelism_hint=4`
    for `SampleBolt`. At the time of execution, Storm will assign two executors for
    `SampleSpout` and four executors for `SampleBolt`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们为`SampleSpout`设置了`parallelism_hint=2`，为`SampleBolt`设置了`parallelism_hint=4`。在执行时，Storm将为`SampleSpout`分配两个执行器，为`SampleBolt`分配四个执行器。
- en: 'We can configure the number of tasks that can execute inside the executors.
    Here is the code snippet to show these settings in practice:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以配置在执行器内部可以执行的任务数量。以下是展示这些设置的代码片段：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we have configured the two executors and four tasks of
    `SampleSpout`. For `SampleSpout`, Storm will assign two tasks per executor. By
    default, Storm will run one task per executor if the user does not set the number
    of tasks at the code level.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们已经配置了`SampleSpout`的两个执行器和四个任务。对于`SampleSpout`，Storm将为每个执行器分配两个任务。默认情况下，如果用户在代码级别不设置任务数量，Storm将为每个执行器运行一个任务。
- en: Worker process, executor, and task distribution
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Worker进程、执行器和任务分布
- en: 'Let''s assume the numbers of worker processes set for the topology is three,
    the number of executors for `SampleSpout` is three, and the number of executors
    for `SampleBolt` is three. Also, the number of tasks for `SampleBolt` is to be
    six, meaning that each `SampleBolt` executor will have two tasks. The following
    diagram shows what the topology would look like in operation:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设为拓扑设置的worker进程数量为三，`SampleSpout`的执行器数量为三，`SampleBolt`的执行器数量为三。此外，`SampleBolt`的任务数量为六，这意味着每个`SampleBolt`执行器将有两个任务。以下图表显示了拓扑在运行时的样子：
- en: '![](img/00022.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00022.jpeg)'
- en: Rebalance the parallelism of a topology
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新平衡拓扑的并行性
- en: As explained in the previous chapter, one of the key features of Storm is that
    it allows us to modify the parallelism of a topology at runtime. The process of
    updating a topology parallelism at runtime is called **rebalance**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中已经解释过，Storm的一个关键特性是它允许我们在运行时修改拓扑的并行性。在运行时更新拓扑并行性的过程称为**rebalance**。
- en: 'There are two ways to rebalance the topology:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种重新平衡拓扑的方式：
- en: Using Storm Web UI
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Storm Web UI
- en: Using Storm CLI
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Storm CLI
- en: 'The Storm Web UI was covered in the previous chapter. This section covers how
    we can rebalance the topology using the Storm CLI tool. Here are the commands
    that we need to execute on Storm CLI to rebalance the topology:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中介绍了Storm Web UI。本节介绍了如何使用Storm CLI工具重新平衡拓扑。以下是我们需要在Storm CLI上执行的命令：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `rebalance` command will first deactivate the topology for the duration
    of the message timeout and then redistribute the workers evenly around the Storm
    cluster. After a few seconds or minutes, the topology will revert to the previous
    state of activation and restart the processing of input streams.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`rebalance`命令将首先在消息超时期间停用拓扑，然后在Storm集群中均匀重新分配worker。几秒钟或几分钟后，拓扑将恢复到之前的激活状态，并重新开始处理输入流。'
- en: Rebalance the parallelism of a SampleStormClusterTopology topology
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新平衡SampleStormClusterTopology拓扑的并行性
- en: 'Let''s first check the numbers of worker processes that are running in the
    Storm cluster by running the `jps` command on the supervisor machine:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先通过在supervisor机器上运行`jps`命令来检查Storm集群中运行的worker进程的数量：
- en: 'Run the `jps` command on supervisor-1:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在supervisor-1上运行`jps`命令：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Two worker processes are assigned to the supervisor-1 machine.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 两个worker进程分配给supervisor-1机器。
- en: 'Now, run the `jps` command on supervisor-2:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在supervisor-2上运行`jps`命令：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: One worker process is assigned to the supervisor-2 machine.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个worker进程分配给supervisor-2机器。
- en: A total of three worker processes are running on the Storm cluster.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Storm集群上运行着三个worker进程。
- en: 'Let''s try reconfiguring `SampleStormClusterTopology` to use two worker processes,
    `SampleSpout` to use four executors, and `SampleBolt` to use four executors:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试重新配置`SampleStormClusterTopology`，使用两个worker进程，`SampleSpout`使用四个执行器，`SampleBolt`使用四个执行器：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Rerun the `jps` commands on the supervisor machines to view the number of worker
    processes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 重新运行supervisor机器上的`jps`命令，查看worker进程的数量。
- en: 'Run the `jps` command on supervisor-1:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在supervisor-1上运行`jps`命令：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run the `jps` command on supervisor-2:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在supervisor-2上运行`jps`命令：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this case, two worker processes are shown previously. The first worker process
    is assigned to supervisor-1 and the other one is assigned to supervisor-2\. The
    distribution of workers may vary depending on the number of topologies running
    on the system and the number of slots available on each supervisor. Ideally, Storm
    tries to distribute the load uniformly between all the nodes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，之前显示了两个worker进程。第一个worker进程分配给supervisor-1，另一个分配给supervisor-2。worker的分布可能会根据系统上运行的拓扑数量和每个supervisor上可用的插槽数量而有所不同。理想情况下，Storm会尝试在所有节点之间均匀分配负载。
- en: Different types of stream grouping in the Storm cluster
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm集群中不同类型的流分组
- en: When defining a topology, we create a graph of computation with the number of
    bolt-processing streams. At a more granular level, each bolt executes multiple
    tasks in the topology. Thus, each task of a particular bolt will only get a subset
    of the tuples from the subscribed streams.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义拓扑时，我们创建了一个计算图，其中包含了多个bolt处理流。在更细粒度的层面上，每个bolt在拓扑中执行多个任务。因此，特定bolt的每个任务只会从订阅的流中获取一部分元组。
- en: Stream grouping in Storm provides complete control over how this partitioning
    of tuples happens among the many tasks of a bolt subscribed to a stream. Grouping
    for a bolt can be defined on the instance of `org.apache.storm.topology.InputDeclarer`
    returned when defining bolts using the `org.apache e.storm.topology.TopologyBuilder.setBolt`
    method.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Storm中的流分组提供了对如何在订阅流的许多任务之间对元组进行分区的完全控制。可以在使用`org.apache e.storm.topology.TopologyBuilder.setBolt`方法定义bolt时，通过`org.apache.storm.topology.InputDeclarer`的实例来定义bolt的分组。
- en: Storm supports the following types of stream groupings.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Storm支持以下类型的流分组。
- en: Shuffle grouping
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Shuffle分组
- en: Shuffle grouping distributes tuples in a uniform, random way across the tasks.
    An equal number of tuples will be processed by each task. This grouping is ideal
    when you want to distribute your processing load uniformly across the tasks and
    where there is no requirement for any data-driven partitioning. This is one of
    the most commonly used groupings in Storm.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Shuffle分组以均匀随机的方式在任务之间分发元组。每个任务将处理相等数量的元组。当您希望在任务之间均匀分配处理负载，并且不需要任何数据驱动的分区时，这种分组是理想的。这是Storm中最常用的分组之一。
- en: Field grouping
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字段分组
- en: 'This grouping enables you to partition a stream on the basis of some of the
    fields in the tuples. For example, if you want all the tweets from a particular
    user to go to a single task, then you can partition the tweet stream using field
    grouping by username in the following manner:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此分组使您能够根据元组中的某些字段对流进行分区。例如，如果您希望特定用户的所有推文都发送到一个任务，则可以使用字段分组按用户名对推文流进行分区：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As a result of the field grouping being *hash (fields) % (no. of tasks)*, it
    does not guarantee that each of the tasks will get tuples to process. For example,
    if you have applied a field grouping on a field, say *X*, with only two possible
    values, *A* and *B*, and created two tasks for the bolt, then it might be possible
    that both *hash (A) % 2* and *hash (B) % 2* return equal values, which will result
    in all the tuples being routed to a single task and the other being completely
    idle.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于字段分组是*hash（字段）%（任务数）*，它不能保证每个任务都会获得要处理的元组。例如，如果您对字段应用了字段分组，比如*X*，只有两个可能的值，*A*和*B*，并为bolt创建了两个任务，那么*hash（A）%2*和*hash（B）%2*可能返回相等的值，这将导致所有元组都被路由到一个任务，另一个任务完全空闲。
- en: 'Another common usage of field grouping is to join streams. Since partitioning
    happens solely on the basis of field values, and not the stream type, we can join
    two streams with any common join fields. The name of the fields needs not be the
    same. For example, in the order processing domain, we can join the `Order` stream
    and the `ItemScanned` stream to see when an order is completed:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 字段分组的另一个常见用途是连接流。由于分区仅基于字段值而不是流类型，因此我们可以使用任何公共连接字段连接两个流。字段的名称不需要相同。例如，在订单处理领域，我们可以连接`Order`流和`ItemScanned`流以查看何时完成订单：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Since joins on streams vary from application to application, you'll make your
    own definition of a join, say joins over a time window, that can be achieved by
    composing field groupings.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于流上的连接因应用程序而异，您将自己定义连接的定义，比如在时间窗口上进行连接，可以通过组合字段分组来实现。
- en: All grouping
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所有分组
- en: All grouping is a special grouping that does not partition the tuples but replicates
    them to all the tasks, that is, each tuple will be sent to each of the bolt's
    tasks for processing.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所有分组是一种特殊的分组，不会对元组进行分区，而是将它们复制到所有任务中，也就是说，每个元组将被发送到bolt的每个任务进行处理。
- en: One common use case of all grouping is for sending signals to bolts. For example,
    if you are doing some kind of filtering on the streams, you can pass or change
    the filter parameters to all the bolts by sending them those parameters over a
    stream that is subscribed by all the bolt's tasks with an all grouping. Another
    example is to send a reset message to all the tasks in an aggregation bolt.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所有分组的一个常见用例是向bolt发送信号。例如，如果您对流进行某种过滤，可以通过向所有bolt的任务发送这些参数的流来传递或更改过滤参数，并使用所有分组进行订阅。另一个例子是向聚合bolt中的所有任务发送重置消息。
- en: Global grouping
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全局分组
- en: Global grouping does not partition the stream but sends the complete stream
    to the bolt's task, the smallest ID. A general use case of this is when there
    needs to be a reduce phase in your topology where you want to combine the results
    from previous steps in the topology into a single bolt.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 全局分组不会对流进行分区，而是将完整的流发送到具有最小ID的bolt任务。这种情况的一般用例是在拓扑中需要减少阶段的情况，其中您希望将拓扑中以前步骤的结果合并到单个bolt中。
- en: Global grouping might seem redundant at first, as you can achieve the same results
    by defining the parallelism for the bolt as one if you only have one input stream.
    However, when you have multiple streams of data coming through a different path,
    you might want only one of the streams to be reduced and others to be parallel
    processes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 全局分组乍看起来可能是多余的，因为如果只有一个输入流，您可以通过将bolt的并行度定义为1来实现相同的结果。但是，当您有多个数据流通过不同路径传入时，您可能希望只有一个流被减少，而其他流被并行处理。
- en: 'For example, consider the following topology. In this, you might want to combine
    all the tuples coming from **Bolt C** in a single **Bolt D** task, while you might
    still want parallelism for tuples coming from **Bolt E** to **Bolt D**:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下拓扑。在这种情况下，您可能希望将来自**Bolt C**的所有元组组合在一个**Bolt D**任务中，而您可能仍希望将来自**Bolt
    E**到**Bolt D**的元组并行处理：
- en: '![](img/00023.jpeg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00023.jpeg)'
- en: Direct grouping
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直接分组
- en: In direct grouping, the emitter decides where each tuple will go for processing.
    For example, say we have a log stream and we want to process each log entry to
    be processed by a specific bolt task on the basis of the type of resource. In
    this case, we can use direct grouping.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在直接分组中，发射器决定每个元组将在哪里进行处理。例如，假设我们有一个日志流，我们希望根据资源类型将每个日志条目处理为特定的bolt任务。在这种情况下，我们可以使用直接分组。
- en: Direct grouping can only be used with direct streams. To declare a stream as
    a direct stream, use the `backtype.storm.topology.OutputFieldsDeclarer.declareStream`
    method, which takes a `boolean` parameter. Once you have a direct stream to emit
    to, use `backtype.storm.task.OutputCollector.emitDirect` instead of emit methods
    to emit it. The `emitDirect` method takes a `taskId` parameter to specify the
    task. You can get the number of tasks for a component using the `backtype.storm.task.TopologyContext.getComponentTasks`
    method.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 直接分组只能与直接流一起使用。要声明一个流为直接流，请使用`backtype.storm.topology.OutputFieldsDeclarer.declareStream`方法，该方法带有一个`boolean`参数。一旦有了要发射的直接流，请使用`backtype.storm.task.OutputCollector.emitDirect`而不是emit方法来发射它。`emitDirect`方法带有一个`taskId`参数来指定任务。您可以使用`backtype.storm.task.TopologyContext.getComponentTasks`方法获取组件的任务数。
- en: Local or shuffle grouping
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地或shuffle分组
- en: If the tuple source and target bolt tasks are running in the same worker, using
    this grouping will act as a shuffle grouping only between the target tasks running
    on the same worker, thus minimizing any network hops, resulting in increased performance.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果tuple源和目标bolt任务在同一个worker中运行，使用此分组将仅在同一worker上运行的目标任务之间起到洗牌分组的作用，从而最大程度地减少任何网络跳数，提高性能。
- en: If there are no target bolt tasks running on the source worker process, this
    grouping will act similar to the shuffle grouping mentioned earlier.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果源worker进程上没有运行目标bolt任务，这种分组将类似于前面提到的shuffle分组。
- en: None grouping
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: None分组
- en: None grouping is used when you don't care about the way tuples are partitioned
    among various tasks. As of Storm 0.8, this is equivalent to using shuffle grouping.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当您不关心tuple在各个任务之间如何分区时，可以使用None分组。从Storm 0.8开始，这相当于使用shuffle分组。
- en: Custom grouping
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义分组
- en: If none of the preceding groupings fit your use case, you can define your own
    custom grouping by implementing the `backtype.storm.grouping.CustomStreamGrouping`
    interface.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的分组都不适合您的用例，您可以通过实现`backtype.storm.grouping.CustomStreamGrouping`接口来定义自己的自定义分组。
- en: 'Here is a sample custom grouping that partitions the stream on the basis of
    the category in the tuples:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个基于tuple中的类别对流进行分区的示例自定义分组：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following diagram represents the Storm groupings graphically:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表以图形方式表示了Storm分组：
- en: '![](img/00024.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00024.jpeg)'
- en: Guaranteed message processing
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保证消息处理
- en: 'In a Storm topology, a single tuple being emitted by a spout can result in
    a number of tuples being generated in the later stages of the topology. For example,
    consider the following topology:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在Storm拓扑中，spout发出的单个tuple可能会导致拓扑后期生成多个tuple。例如，考虑以下拓扑：
- en: '![](img/00025.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00025.jpeg)'
- en: Here, **Spout A** emits a tuple **T(A)**, which is processed by **bolt B** and
    **bolt C**, which emit tuple **T(AB)** and **T(AC)** respectively. So, when all
    the tuples produced as a result of tuple **T(A)**--namely, the tuple tree **T(A)**,
    **T(AB)**, and **T(AC)**--are processed, we say that the tuple has been processed
    completely.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**Spout A**发出一个tuple **T(A)**，由**bolt B**和**bolt C**处理，它们分别发出tuple **T(AB)**和**T(AC)**。因此，当作为tuple
    **T(A)**结果产生的所有tuple--即tuple树**T(A)**，**T(AB)**和**T(AC)**--都被处理时，我们说该tuple已完全处理。
- en: When some of the tuples in a tuple tree fail to process either due to some runtime
    error or a timeout that is configurable for each topology, then Storm considers
    that to be a failed tuple.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当tuple树中的一些tuple由于某些运行时错误或每个拓扑可配置的超时而未能处理时，Storm将视其为失败的tuple。
- en: 'Here are the six steps that are required by Storm to guarantee message processing:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Storm需要以下六个步骤来保证消息处理：
- en: Tag each tuple emitted by a spout with a unique message ID. This can be done
    by using the `org.apache.storm.spout.SpoutOutputColletor.emit` method, which takes
    a `messageId` argument. Storm uses this message ID to track the state of the tuple
    tree generated by this tuple. If you use one of the emit methods that doesn't
    take a `messageId` argument, Storm will not track it for complete processing.
    When the message is processed completely, Storm will send an acknowledgement with
    the same `messageId` that was used while emitting the tuple.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用唯一的消息ID标记spout发出的每个tuple。这可以通过使用`org.apache.storm.spout.SpoutOutputColletor.emit`方法来实现，该方法带有一个`messageId`参数。Storm使用此消息ID来跟踪由此tuple生成的tuple树的状态。如果您使用不带`messageId`参数的emit方法之一，Storm将不会跟踪它以进行完全处理。当消息完全处理时，Storm将使用发出tuple时使用的相同`messageId`发送确认。
- en: A generic pattern implemented by spouts is that they read a message from a messaging
    queue, say RabbitMQ, produce the tuple into the topology for further processing,
    and then dequeue the message once it receives the acknowledgement that the tuple
    has been processed completely.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: spout实现的通用模式是，它们从消息队列（例如RabbitMQ）中读取消息，将tuple生成到拓扑中进行进一步处理，然后一旦收到tuple已完全处理的确认，就将消息出队。
- en: When one of the bolts in the topology needs to produce a new tuple in the course
    of processing a message, for example, **bolt B** in the preceding topology, then
    it should emit the new tuple anchored with the original tuple that it got from
    the spout. This can be done by using the overloaded emit methods in the `org.apache.storm.task.OutputCollector`
    class that takes an anchor tuple as an argument. If you are emitting multiple
    tuples from the same input tuple, then anchor each outgoing tuple.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当拓扑中的一个bolt在处理消息过程中需要生成一个新的tuple时，例如前面拓扑中的**bolt B**，那么它应该发出新的tuple，并用它从spout获取的原始tuple进行关联。这可以通过使用`org.apache.storm.task.OutputCollector`类中带有anchor
    tuple参数的重载emit方法来实现。如果您从同一个输入tuple发出多个tuple，则要为每个输出的tuple进行关联。
- en: Whenever you are done with processing a tuple in the execute method of your
    bolt, send an acknowledgment using the `org.apache.storm.task.OutputCollector.ack`
    method. When the acknowledgement reaches the emitting spout, you can safely mark
    the message as being processed and dequeue it from the message queue, if any.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每当您在bolt的execute方法中处理完一个tuple时，使用`org.apache.storm.task.OutputCollector.ack`方法发送确认。当确认到达发射的spout时，您可以安全地将消息标记为已处理，并从消息队列中出队（如果有的话）。
- en: Similarly, if there is some problem in processing a tuple, a failure signal
    should be sent back using the `org.apache.storm.task.OutputCollector.fail` method
    so that Storm can replay the failed message.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，如果在处理元组时出现问题，应该使用`org.apache.storm.task.OutputCollector.fail`方法发送失败信号，以便Storm可以重放失败的消息。
- en: One of the general patterns of processing in Storm bolts is to process a tuple
    in, emit new tuples, and send an acknowledgement at the end of the execute method.
    Storm provides the `org.apache.storm.topology.base.BasicBasicBolt` class that
    automatically sends the acknowledgement at the end of the execute method. If you
    want to signal a failure, throw `org.apache.storm.topology.FailedException` from
    the execute method.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Storm bolt中处理的一般模式之一是在execute方法的末尾处理一个元组，发出新的元组，并在execute方法的末尾发送确认。Storm提供了`org.apache.storm.topology.base.BasicBasicBolt`类，它会在execute方法的末尾自动发送确认。如果要发出失败信号，请在execute方法中抛出`org.apache.storm.topology.FailedException`。
- en: This model results in at-least-once message processing semantics, and your application
    should be ready to handle a scenario when some of the messages will be processed
    multiple times. Storm also provides exactly-once message processing semantics,
    which we will discuss in [Chapter 5](part0102.html#318PC0-6149dd15e07b443593cc93f2eb31ee7b),
    *Trident Topology and Uses*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型导致至少一次消息处理语义，并且你的应用程序应该准备好处理一些消息会被多次处理的情况。Storm还提供了一次消息处理语义，我们将在[第5章](part0102.html#318PC0-6149dd15e07b443593cc93f2eb31ee7b)
    *Trident Topology and Uses*中讨论。
- en: Even though you can achieve some guaranteed message processing in Storm using
    the methods mentioned here, it is always a point to ponder whether you actually
    require it or not, as you can gain a large performance boost by risking some of
    the messages not being completely processed by Storm. This is a trade-off that
    you can think of when designing your application.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以通过这里提到的方法在Storm中实现一些消息处理的保证，但你是否真正需要它，这总是一个需要考虑的问题，因为你可以通过冒一些消息不被Storm完全处理来获得很大的性能提升。这是在设计应用程序时可以考虑的一个权衡。
- en: Tick tuple
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tick元组
- en: In some use cases, a bolt needs to cache the data for a few seconds before performing
    some operation, such as cleaning the cache after every 5 seconds or inserting
    a batch of records into a database in a single request.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些用例中，一个bolt需要在执行某些操作之前缓存数据几秒钟，比如在每5秒清理缓存或者在单个请求中插入一批记录到数据库中。
- en: The tick tuple is the system-generated (Storm-generated) tuple that we can configure
    at each bolt level. The developer can configure the tick tuple at the code level
    while writing a bolt.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: tick元组是系统生成（由Storm生成）的元组，我们可以在每个bolt级别进行配置。开发人员可以在编写bolt时在代码级别配置tick元组。
- en: 'We need to overwrite the following method in the bolt to enable the tick tuple:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在bolt中重写以下方法以启用tick元组：
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding code, we have configured the tick tuple time to 10 seconds.
    Now, Storm will start generating a tick tuple after every 10 seconds.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们已经将tick元组的时间配置为10秒。现在，Storm将在每10秒开始生成一个tick元组。
- en: 'Also, we need to add the following code in the execute method of the bolt to
    identify the type of tuple:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要在bolt的execute方法中添加以下代码以识别元组的类型：
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If the output of the `isTickTuple()` method is true, then the input tuple is
    a tick tuple. Otherwise, it is a normal tuple emitted by the previous bolt.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`isTickTuple()`方法的输出为true，则输入元组是一个tick元组。否则，它是由前一个bolt发出的普通元组。
- en: Be aware that tick tuples are sent to bolts/spouts just like regular tuples,
    which means they will be queued behind other tuples that a bolt/spout is about
    to process via its `execute()` or `nextTuple()` method, respectively. As such,
    the time interval you configure for tick tuples is, in practice, served on a best-effort
    basis. For instance, if a bolt is suffering from high execution latency--for example,
    due to being overwhelmed by the incoming rate of regular, non-tick tuples--then
    you will observe that the periodic activities implemented in the bolt will get
    triggered later than expected.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，tick元组会像普通元组一样发送到bolt/spout，这意味着它们将排在bolt/spout即将通过其`execute()`或`nextTuple()`方法处理的其他元组之后。因此，你为tick元组配置的时间间隔在实践中是尽力而为的。例如，如果一个bolt受到高执行延迟的影响--例如，由于被常规非tick元组的传入速率压倒--那么你会观察到在bolt中实现的周期性活动会比预期触发得晚。
- en: Summary
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we have shed some light on how we can define the parallelism
    of Storm, how we can distribute jobs between multiple nodes, and how we can distribute
    data between multiple instances of a bolt. The chapter also covered two important
    features: guaranteed message processing and the tick tuple.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经介绍了如何定义Storm的并行性，如何在多个节点之间分发作业，以及如何在多个bolt实例之间分发数据。本章还涵盖了两个重要特性：消息处理的保证和tick元组。
- en: In the next chapter, we are covering the Trident high-level abstraction over
    Storm. Trident is mostly used to solve the real-time transaction problem, which
    can't be solved through plain Storm.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍Storm上的Trident高级抽象。Trident主要用于解决实时事务问题，这是无法通过普通的Storm解决的。
