- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: The continued growth in data coupled with the need to make increasingly complex
    decisions against that data is creating massive hurdles that prevent organizations
    from deriving insights in a timely manner using traditional analytical approaches.
    The field of big data has become so related to these frameworks that its scope
    is defined by what these frameworks can handle. Whether you're scrutinizing the
    clickstream from millions of visitors to optimize online ad placements, or sifting
    through billions of transactions to identify signs of fraud, the need for advanced
    analytics, such as machine learning and graph processing, to automatically glean
    insights from enormous volumes of data is more evident than ever.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的持续增长与需要在这些数据上做出日益复杂决策的需求正在带来巨大的挑战，阻碍了组织通过传统分析方法及时获取洞察力。大数据领域已与这些框架紧密相关，其范围由这些框架能处理的内容来定义。无论你是在分析数百万访客的点击流以优化在线广告投放，还是在筛查数十亿笔交易以识别欺诈迹象，对于高级分析（如机器学习和图形处理）的需求——从大量数据中自动获取洞察力——比以往任何时候都更加迫切。
- en: Apache Spark, the de facto standard for big data processing, analytics, and
    data sciences across all academia and industries, provides both machine learning
    and graph processing libraries, allowing companies to tackle complex problems
    easily with the power of highly scalable and clustered computers. Spark's promise
    is to take this a little further to make writing distributed programs using Scala
    feel like writing regular programs for Spark. Spark will be great in giving ETL
    pipelines huge boosts in performance and easing some of the pain that feeds the
    MapReduce programmer's daily chant of despair to the Hadoop gods.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark，作为大数据处理、分析和数据科学的事实标准，已被广泛应用于所有学术界和行业，它提供了机器学习和图形处理库，帮助企业利用高度可扩展的集群计算轻松解决复杂问题。Spark的承诺是将这一过程进一步推进，让使用Scala编写分布式程序的感觉，像是为Spark编写常规程序一样。Spark将在大幅提升ETL管道性能方面发挥巨大作用，减轻MapReduce程序员每天对Hadoop神明的哀鸣。
- en: In this book, we used Spark and Scala for the endeavor to bring state-of-the-art
    advanced data analytics with machine learning, graph processing, streaming, and
    SQL to Spark, with their contributions to MLlib, ML, SQL, GraphX, and other libraries.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们使用Spark和Scala的组合，致力于将最先进的机器学习、图形处理、流处理和SQL等大数据分析技术带到Spark，并探讨它们在MLlib、ML、SQL、GraphX等库中的应用。
- en: We started with Scala and then moved to the Spark part, and finally, covered
    some advanced topics for big data analytics with Spark and Scala. In the appendix,
    we will see how to extend your Scala knowledge for SparkR, PySpark, Apache Zeppelin,
    and in-memory Alluxio. This book isn't meant to be read from cover to cover. Skip
    to a chapter that looks like something you're trying to accomplish or that simply
    ignites your interest.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Scala开始，然后转向Spark部分，最后覆盖了Spark和Scala的大数据分析高级主题。在附录中，我们将看到如何将你的Scala知识扩展到SparkR、PySpark、Apache
    Zeppelin以及内存中的Alluxio。本书并非按从头到尾的顺序阅读，跳到你正在尝试实现的目标或激发你兴趣的章节即可。
- en: Happy reading!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你阅读愉快！
- en: What this book covers
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容
- en: '[Chapter 1](part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c), *Introduction
    to Scala*, will teach big data analytics using the Scala-based APIs of Spark.
    Spark itself is written with Scala and naturally, as a starting point, we will
    discuss a brief introduction to Scala, such as the basic aspects of its history,
    purposes, and how to install Scala on Windows, Linux, and Mac OS. After that,
    the Scala web framework will be discussed in brief. Then, we will provide a comparative
    analysis of Java and Scala. Finally, we will dive into Scala programming to get
    started with Scala.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c)，*Scala简介*，将教授使用基于Scala的Spark
    API进行大数据分析。Spark本身是用Scala编写的，因此作为起点，我们将简要介绍Scala的历史、用途以及如何在Windows、Linux和Mac OS上安装Scala。接下来，我们将简要讨论Scala的Web框架。然后，我们将进行Java和Scala的对比分析。最后，我们将深入Scala编程，开始学习Scala。'
- en: '[Chapter 2](part0058.html#1NA0K1-21aec46d8593429cacea59dbdcd64e1c), *Object-Oriented
    Scala*, says that the object-oriented programming (OOP) paradigm provides a whole
    new layer of abstraction. In short, this chapter discusses some of the greatest
    strengths of OOP languages: discoverability, modularity, and extensibility. In
    particular, we will see how to deal with variables in Scala; methods, classes,
    and objects in Scala; packages and package objects; traits and trait linearization;
    and Java interoperability.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[第2章](part0058.html#1NA0K1-21aec46d8593429cacea59dbdcd64e1c)，*面向对象的Scala*，说明面向对象编程（OOP）范式提供了一种全新的抽象层。简而言之，本章讨论了OOP语言的一些最大优势：可发现性、模块化和可扩展性。特别地，我们将看到如何在Scala中处理变量；Scala中的方法、类和对象；包和包对象；特质和特质线性化；以及Java互操作性。'
- en: '[Chapter 3](part0093.html#2OM4A1-21aec46d8593429cacea59dbdcd64e1c), *Functional
    Programming Concepts*, showcases the functional programming concepts in Scala.
    More specifically, we will learn several topics, such as why Scala is an arsenal
    for the data scientist, why it is important to learn the Spark paradigm, pure
    functions, and higher-order functions (HOFs). A real-life use case using HOFs
    will be shown too. Then, we will see how to handle exceptions in higher-order
    functions outside of collections using the standard library of Scala. Finally,
    we will look at how functional Scala affects an object''s mutability.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](part0093.html#2OM4A1-21aec46d8593429cacea59dbdcd64e1c)，*函数式编程概念*，展示了Scala中的函数式编程概念。更具体地，我们将学习几个主题，如为什么Scala是数据科学家的武器库，为什么学习Spark范式很重要，纯函数和高阶函数（HOFs）。还将展示一个使用高阶函数的实际用例。接着，我们将看到如何在不使用集合的情况下，通过Scala的标准库来处理高阶函数中的异常。最后，我们将了解函数式Scala如何影响对象的可变性。'
- en: '[Chapter4](part0117.html#3FIHQ1-21aec46d8593429cacea59dbdcd64e1c), *Collection
    APIs*, introduces one of the features that attract most Scala users--the Collections
    API. It''s very powerful and flexible, and has lots of operations coupled. We
    will also demonstrate the capabilities of the Scala Collection API and how it
    can be used in order to accommodate different types of data and solve a wide range
    of different problems. In this chapter, we will cover Scala collection APIs, types
    and hierarchy, some performance characteristics, Java interoperability, and Scala
    implicits.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[第4章](part0117.html#3FIHQ1-21aec46d8593429cacea59dbdcd64e1c)，*集合API*，介绍了吸引大多数Scala用户的一个特性——集合API。它功能强大且灵活，具有丰富的操作组合。我们还将展示Scala集合API的功能以及如何使用它来处理不同类型的数据，并解决各种各样的问题。在这一章中，我们将涵盖Scala集合API、类型和层次结构、一些性能特性、Java互操作性和Scala隐式转换。'
- en: '[Chapter 5](part0148.html#4D4J81-21aec46d8593429cacea59dbdcd64e1c), *Tackle
    Big Data - Spark Comes to the Party,* outlines data analysis and big data; we
    see the challenges that big data poses, how they are dealt with by distributed
    computing, and the approaches suggested by functional programming. We introduce
    Google''s MapReduce, Apache Hadoop, and finally, Apache Spark, and see how they
    embraced this approach and these techniques. We will look into the evolution of
    Apache Spark: why Apache Spark was created in the first place and the value it
    can bring to the challenges of big data analytics and processing.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](part0148.html#4D4J81-21aec46d8593429cacea59dbdcd64e1c)，*应对大数据 - Spark
    登场*，概述了数据分析和大数据；我们看到了大数据所带来的挑战，分布式计算如何应对这些挑战，以及函数式编程提出的方法。我们介绍了Google的MapReduce、Apache
    Hadoop，最后是Apache Spark，看看它们是如何采用这一方法和技术的。我们将探讨Apache Spark的演变：为什么最初创建了Apache Spark，它能为大数据分析和处理的挑战带来什么价值。'
- en: '[Chapter 6](part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c), *Start
    Working with Spark - REPL and RDDs,* covers how Spark works; then, we introduce
    RDDs, the basic abstractions behind Apache Spark, and see that they are simply
    distributed collections exposing Scala-like APIs. We will look at the deployment
    options for Apache Spark and run it locally as a Spark shell. We will learn the
    internals of Apache Spark, what RDDs are, DAGs and lineages of RDDs, Transformations,
    and Actions.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[第6章](part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c)，*开始使用Spark - REPL和RDDs*，介绍了Spark的工作原理；接着，我们介绍了RDDs，它是Apache
    Spark背后的基本抽象，并看到它们只是暴露类似Scala的API的分布式集合。我们将探讨Apache Spark的部署选项，并作为Spark shell在本地运行它。我们将学习Apache
    Spark的内部结构，RDD是什么，RDD的DAG和谱系，转换和操作。'
- en: '[Chapter 7](part0212.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c), *Special
    RDD Operations,* focuses on how RDDs can be tailored to meet different needs,
    and how these RDDs provide new functionalities (and dangers!) Moreover, we investigate
    other useful objects that Spark provides, such as broadcast variables and Accumulators.
    We will learn aggregation techniques, shuffling.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[第七章](part0212.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c)，*特殊的 RDD 操作*，重点介绍了如何根据不同需求定制
    RDD，以及这些 RDD 如何提供新的功能（以及潜在的风险！）。此外，我们还将探讨 Spark 提供的其他有用对象，如广播变量和累加器。我们将学习聚合技术和数据洗牌。'
- en: '[Chapter 8](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c), *Introduce
    a Little Structure - SparkSQL,* teaches how to use Spark for the analysis of structured
    data as a higher-level abstraction of RDDs and how Spark SQL''s APIs make querying
    structured data simple yet robust. Moreover, we introduce datasets and look at
    the differences between datasets, DataFrames, and RDDs. We will also learn to
    join operations and window functions to do complex data analysis using DataFrame
    APIs.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[第八章](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c)，*引入一些结构 - SparkSQL*，讲解了如何使用
    Spark 作为 RDD 的高级抽象来分析结构化数据，以及如何通过 Spark SQL 的 API 简单且强大地查询结构化数据。此外，我们介绍了数据集，并对数据集、DataFrame
    和 RDD 之间的差异进行了比较。我们还将学习如何通过 DataFrame API 进行连接操作和窗口函数，来进行复杂的数据分析。'
- en: '[Chapter 9](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c), *Stream
    Me Up, Scotty - Spark Streaming,* takes you through Spark Streaming and how we
    can take advantage of it to process streams of data using the Spark API. Moreover,
    in this chapter, the reader will learn various ways of processing real-time streams
    of data using a practical example to consume and process tweets from Twitter.
    We will look at integration with Apache Kafka to do real-time processing. We will
    also look at structured streaming, which can provide real-time queries to your
    applications.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[第九章](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c)，*流式处理 - Spark
    Streaming*，带领你了解 Spark Streaming，以及如何利用 Spark API 处理数据流。此外，本章中，读者将学习如何通过实践示例，使用
    Twitter 上的推文进行实时数据流处理。我们还将探讨与 Apache Kafka 的集成，实现实时处理。我们还会了解结构化流处理，能够为你的应用提供实时查询。'
- en: '[Chapter 10](part0326.html#9MSNC1-21aec46d8593429cacea59dbdcd64e1c), *Everything
    is Connected - GraphX,* in this chapter, we learn how many real-world problems
    can be modeled (and resolved) using graphs. We will look at graph theory using
    Facebook as an example, Apache Spark''s graph processing library GraphX, VertexRDD
    and EdgeRDDs, graph operators, aggregateMessages, TriangleCounting, the Pregel
    API, and use cases such as the PageRank algorithm.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[第十章](part0326.html#9MSNC1-21aec46d8593429cacea59dbdcd64e1c)，*万物互联 - GraphX*，本章中，我们将学习如何使用图模型来解决许多现实世界的问题。我们将通过
    Facebook 举例，学习图论、Apache Spark 的图处理库 GraphX、VertexRDD 和 EdgeRDD、图操作符、aggregateMessages、TriangleCounting、Pregel
    API 以及 PageRank 算法等应用场景。'
- en: '[Chapter 11](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c), *Learning
    Machine Learning - Spark MLlib and ML*, the purpose of this chapter is to provide
    a conceptual introduction to statistical machine learning. We will focus on Spark''s
    machine learning APIs, called Spark MLlib and ML. We will then discuss how to
    solve classification tasks using decision trees and random forest algorithms and
    regression problem using linear regression algorithm. We will also show how we
    could benefit from using one-hot encoding and dimensionality reductions algorithms
    in feature extraction before training a classification model. In later sections,
    we will show a step-by-step example of developing a collaborative filtering-based
    movie recommendation system.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[第十一章](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c)，*学习机器学习 - Spark
    MLlib 和 ML*，本章的目的是提供统计机器学习的概念性介绍。我们将重点介绍 Spark 的机器学习 API，称为 Spark MLlib 和 ML。接着，我们将讨论如何使用决策树和随机森林算法解决分类任务，以及使用线性回归算法解决回归问题。我们还将展示如何通过使用独热编码和降维算法，在训练分类模型前进行特征提取。此外，在后续部分，我们将通过一个逐步示例，展示如何开发基于协同过滤的电影推荐系统。'
- en: '[Chapter 12](part0383.html#BD87E1-21aec46d8593429cacea59dbdcd64e1c), *Advanced
    Machine Learning Best Practices*, provides theoretical and practical aspects of
    some advanced topics of machine learning with Spark. We will see how to tune machine
    learning models for optimized performance using grid search, cross-validation,
    and hyperparameter tuning. In a later section, we will cover how to develop a
    scalable recommendation system using ALS, which is an example of a model-based
    recommendation algorithm. Finally, a topic modelling application will be demonstrated
    as a text clustering technique'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[第12章](part0383.html#BD87E1-21aec46d8593429cacea59dbdcd64e1c)，*高级机器学习最佳实践*，提供了关于Spark机器学习的一些高级主题的理论和实践方面的内容。我们将了解如何使用网格搜索、交叉验证和超参数调优来优化机器学习模型的性能。在后续部分，我们将讨论如何使用ALS开发可扩展的推荐系统，ALS是基于模型的推荐算法的一个例子。最后，我们还将展示一个主题建模应用，这是文本聚类技术的一个实例。'
- en: '[Chapter 13](part0413.html#C9ROA1-21aec46d8593429cacea59dbdcd64e1c), *My Name
    is Bayes, Naive Bayes,* states that machine learning in big data is a radical
    combination that has created great impact in the field of research, in both academia
    and industry. Big data imposes great challenges on ML, data analytics tools, and
    algorithms to find the real value. However, making a future prediction based on
    these huge datasets has never been easy. Considering this challenge, in this chapter,
    we will dive deeper into ML and find out how to use a simple yet powerful method
    to build a scalable classification model and concepts such as multinomial classification,
    Bayesian inference, Naive Bayes, decision trees, and a comparative analysis of
    Naive Bayes versus decision trees.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[第13章](part0413.html#C9ROA1-21aec46d8593429cacea59dbdcd64e1c)，*我的名字是贝叶斯，朴素贝叶斯*，指出大数据中的机器学习是一种激进的结合，已经在学术界和工业界的研究领域产生了巨大影响。大数据对机器学习、数据分析工具和算法带来了巨大的挑战，以帮助我们找到真正的价值。然而，基于这些庞大的数据集进行未来预测从未如此简单。考虑到这一挑战，本章将深入探讨机器学习，并研究如何使用一种简单而强大的方法构建可扩展的分类模型，涉及多项式分类、贝叶斯推理、朴素贝叶斯、决策树等概念，并对朴素贝叶斯与决策树进行比较分析。'
- en: '[Chapter 14](part0434.html#CTSK41-21aec46d8593429cacea59dbdcd64e1c), *Time
    to Put Some Order - Cluster Your Data with Spark MLlib,* gets you started on how
    Spark works in cluster mode with its underlying architecture. In previous chapters,
    we saw how to develop practical applications using different Spark APIs. Finally,
    we will see how to deploy a full Spark application on a cluster, be it with a
    pre-existing Hadoop installation or without.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[第14章](part0434.html#CTSK41-21aec46d8593429cacea59dbdcd64e1c)，*是时候整理一些秩序——使用Spark
    MLlib对数据进行聚类*，帮助你了解Spark如何在集群模式下工作及其底层架构。在前几章中，我们已经看到如何使用不同的Spark API开发实际应用。最后，我们将看到如何在集群上部署一个完整的Spark应用，无论是使用预先存在的Hadoop安装还是没有。'
- en: '[Chapter 15](part0458.html#DKP1K1-21aec46d8593429cacea59dbdcd64e1c), *Text
    Analytics Using Spark ML,* outlines the wonderful field of text analytics using
    Spark ML. Text analytics is a wide area in machine learning and is useful in many
    use cases, such as sentiment analysis, chat bots, email spam detection, natural
    language processing, and many many more. We will learn how to use Spark for text
    analysis with a focus on use cases of text classification using a 10,000 sample
    set of Twitter data. We will also look at LDA, a popular technique to generate
    topics from documents without knowing much about the actual text, and will implement
    text classification on Twitter data to see how it all comes together.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[第15章](part0458.html#DKP1K1-21aec46d8593429cacea59dbdcd64e1c)，*使用Spark ML进行文本分析*，概述了使用Spark
    ML进行文本分析这一美妙领域。文本分析是机器学习中的一个广泛领域，应用场景非常广泛，如情感分析、聊天机器人、电子邮件垃圾邮件检测、自然语言处理等。我们将学习如何使用Spark进行文本分析，重点讨论文本分类的应用，使用一万条Twitter数据样本集进行分析。我们还将研究LDA，这是一种流行的技术，用于从文档中生成主题，而无需深入了解实际文本，并将实现基于Twitter数据的文本分类，看看如何将这些内容结合起来。'
- en: '[Chapter 16](part0480.html#E9OE01-21aec46d8593429cacea59dbdcd64e1c), *Spark
    Tuning,* digs deeper into Apache Spark internals and says that while Spark is
    great in making us feel as if we are using just another Scala collection, we shouldn''t
    forget that Spark actually runs in a distributed system. Therefore, throughout
    this chapter, we will cover how to monitor Spark jobs, Spark configuration, common
    mistakes in Spark app development, and some optimization techniques.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[第16章](part0480.html#E9OE01-21aec46d8593429cacea59dbdcd64e1c)，*Spark调优*，深入探讨了Apache
    Spark的内部机制，指出尽管Spark在使用时让我们感觉就像在使用另一个Scala集合，但我们不应忘记Spark实际上运行在分布式系统中。因此，在本章中，我们将介绍如何监控Spark作业、Spark配置、Spark应用开发中的常见错误，以及一些优化技术。'
- en: '[Chapter 17](part0511.html#F7AFE1-21aec46d8593429cacea59dbdcd64e1c), *Time
    to Go to ClusterLand - Deploying Spark on a Cluster,* explores how Spark works
    in cluster mode with its underlying architecture. We will see Spark architecture
    in a cluster, the Spark ecosystem and cluster management, and how to deploy Spark
    on standalone, Mesos, Yarn, and AWS clusters. We will also see how to deploy your
    app on a cloud-based AWS cluster.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[第17章](part0511.html#F7AFE1-21aec46d8593429cacea59dbdcd64e1c)，*进入ClusterLand
    - 在集群上部署Spark*，探讨了Spark在集群模式下的工作原理及其底层架构。我们将了解Spark在集群中的架构、Spark生态系统和集群管理，以及如何在独立集群、Mesos、Yarn和AWS集群上部署Spark。我们还将了解如何在基于云的AWS集群上部署应用程序。'
- en: '[Chapter 18](part0550.html#GCGLC1-21aec46d8593429cacea59dbdcd64e1c), *Testing
    and Debugging Spark,* explains how difficult it can be to test an application
    if it is distributed; then, we see some ways to tackle this. We will cover how
    to do testing in a distributed environment, and testing and debugging Spark applications.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[第18章](part0550.html#GCGLC1-21aec46d8593429cacea59dbdcd64e1c)，*测试和调试Spark*，解释了在分布式应用程序中进行测试的难度；然后，我们将介绍一些解决方法。我们将讲解如何在分布式环境中进行测试，以及如何测试和调试Spark应用程序。'
- en: '[Chapter 19](part0571.html#H0HH61-21aec46d8593429cacea59dbdcd64e1c), *PySpark
    & SparkR,* covers the other two popular APIs for writing Spark code using R and
    Python, that is, PySpark and SparkR. In particular, we will cover how to get started
    with PySpark and interacting with DataFrame APIs and UDFs with PySpark, and then
    we will do some data analytics using PySpark. The second part of this chapter
    covers how to get started with SparkR. We will also see how to do data processing
    and manipulation, and how to work with RDD and DataFrames using SparkR, and finally,
    some data visualization using SparkR.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[第19章](part0571.html#H0HH61-21aec46d8593429cacea59dbdcd64e1c)，*PySpark & SparkR*，介绍了使用R和Python编写Spark代码的另外两个流行API，即PySpark和SparkR。特别是，我们将介绍如何开始使用PySpark，并与DataFrame
    API和UDF进行交互，然后我们将使用PySpark进行一些数据分析。本章的第二部分介绍了如何开始使用SparkR。我们还将了解如何使用SparkR进行数据处理和操作，如何使用SparkR处理RDD和DataFrame，最后是使用SparkR进行一些数据可视化。'
- en: '[Appendix A](part0593.html#HLGTI1-21aec46d8593429cacea59dbdcd64e1c), *Accelerating
    Spark with Alluxio*, shows how to use Alluxio with Spark to increase the speed
    of processing. Alluxio is an open source distributed memory storage system useful
    for increasing the speed of many applications across platforms, including Apache
    Spark. We will explore the possibilities of using Alluxio and how Alluxio integration
    will provide greater performance without the need to cache the data in memory
    every time we run a Spark job.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[附录A](part0593.html#HLGTI1-21aec46d8593429cacea59dbdcd64e1c)，*通过Alluxio加速Spark*，展示了如何将Alluxio与Spark结合使用，以提高处理速度。Alluxio是一个开源的分布式内存存储系统，对于提高跨平台应用程序的速度非常有用，包括Apache
    Spark。在本章中，我们将探讨使用Alluxio的可能性，以及Alluxio的集成如何提供更高的性能，而不需要每次运行Spark任务时都将数据缓存到内存中。'
- en: '[Appendix B](part0612.html#I7KO81-21aec46d8593429cacea59dbdcd64e1c), *Interactive
    Data Analytics with Apache Zepp**e**lin*, says that from a data science perspective,
    interactive visualization of your data analysis is also important. Apache Zeppelin
    is a web-based notebook for interactive and large-scale data analytics with multiple
    backends and interpreters. In this chapter, we will discuss how to use Apache
    Zeppelin for large-scale data analytics using Spark as the interpreter in the
    backend.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[附录B](part0612.html#I7KO81-21aec46d8593429cacea59dbdcd64e1c)，*使用Apache Zeppelin进行互动数据分析*，指出从数据科学的角度来看，数据分析的交互式可视化也非常重要。Apache
    Zeppelin是一个基于Web的笔记本，用于交互式和大规模数据分析，支持多种后端和解释器。在本章中，我们将讨论如何使用Apache Zeppelin进行大规模数据分析，使用Spark作为后端的解释器。'
- en: What you need for this book
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书所需的工具
- en: 'All the examples have been implemented using Python version 2.7 and 3.5 on
    an Ubuntu Linux 64 bit, including the TensorFlow library version 1.0.1\. However,
    in the book, we showed the source code with only Python 2.7 compatible. Source
    codes that are Python 3.5+ compatible can be downloaded from the Packt repository.
    You will also need the following Python modules (preferably the latest versions):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所有示例均使用Python版本2.7和3.5在Ubuntu Linux 64位系统上实现，包括TensorFlow库版本1.0.1。然而，在书中，我们仅展示了兼容Python
    2.7的源代码。兼容Python 3.5+的源代码可以从Packt仓库下载。您还需要以下Python模块（最好是最新版本）：
- en: Spark 2.0.0 (or higher)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0.0（或更高版本）
- en: Hadoop 2.7 (or higher)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 2.7（或更高版本）
- en: Java (JDK and JRE) 1.7+/1.8+
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java（JDK和JRE）1.7+/1.8+
- en: Scala 2.11.x (or higher)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala 2.11.x（或更高版本）
- en: Python 2.7+/3.4+
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 2.7+/3.4+
- en: R 3.1+ and RStudio 1.0.143 (or higher)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 3.1+ 和 RStudio 1.0.143（或更高版本）
- en: Eclipse Mars, Oxygen, or Luna (latest)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eclipse Mars、Oxygen 或 Luna（最新版本）
- en: Maven Eclipse plugin (2.9 or higher)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maven Eclipse 插件（2.9 或更高版本）
- en: Maven compiler plugin for Eclipse (2.3.2 or higher)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于 Eclipse 的 Maven 编译插件（2.3.2 或更高版本）
- en: Maven assembly plugin for Eclipse (2.4.1 or higher)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maven assembly 插件用于 Eclipse（2.4.1 或更高版本）
- en: '**Operating system:** Linux distributions are preferable (including Debian,
    Ubuntu, Fedora, RHEL, and CentOS) and to be more specific, for Ubuntu it is recommended
    to have a complete 14.04 (LTS) 64-bit (or later) installation, VMWare player 12,
    or Virtual box. You can run Spark jobs on Windows (XP/7/8/10) or Mac OS X (10.4.7+).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**操作系统：** 推荐使用 Linux 发行版（包括 Debian、Ubuntu、Fedora、RHEL 和 CentOS），具体来说，推荐在 Ubuntu
    上安装完整的 14.04（LTS）64 位（或更高版本）系统，VMWare Player 12 或 VirtualBox。你可以在 Windows（XP/7/8/10）或
    Mac OS X（10.4.7 及更高版本）上运行 Spark 作业。'
- en: '**Hardware configuration:** Processor Core i3, Core i5 (recommended), or Core
    i7 (to get the best results). However, multicore processing will provide faster
    data processing and scalability. You will need least 8-16 GB RAM (recommended)
    for a standalone mode and at least 32 GB RAM for a single VM--and higher for cluster.
    You will also need enough storage for running heavy jobs (depending on the dataset
    size you will be handling), and preferably at least 50 GB of free disk storage
    (for standalone word missing and for an SQL warehouse).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**硬件配置：** 推荐使用 Core i3、Core i5（推荐）或 Core i7 处理器（以获得最佳效果）。不过，多核处理器将提供更快的数据处理速度和更好的扩展性。你至少需要
    8-16 GB 的内存（推荐）用于独立模式，至少需要 32 GB 内存用于单个虚拟机——集群模式则需要更高的内存。你还需要足够的存储空间来运行大型作业（具体取决于你处理的数据集大小），并且最好至少有
    50 GB 的可用磁盘存储（用于独立模式的缺失和 SQL 数据仓库）。'
- en: Who this book is for
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适合谁阅读
- en: Anyone who wishes to learn how to perform data analysis by harnessing the power
    of Spark will find this book extremely useful. No knowledge of Spark or Scala
    is assumed, although prior programming experience (especially with other JVM languages)
    will be useful in order to pick up the concepts quicker. Scala has been observing
    a steady rise in adoption over the past few years, especially in the fields of
    data science and analytics. Going hand in hand with Scala is Apache Spark, which
    is programmed in Scala and is widely used in the field of analytics. This book
    will help you leverage the power of both these tools to make sense of big data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 任何希望通过利用 Spark 的强大功能来进行数据分析的人，都会发现本书极为有用。本书假设读者没有 Spark 或 Scala 的基础，虽然具备一定的编程经验（特别是其他
    JVM 语言的经验）将有助于更快地掌握概念。Scala 在过去几年中一直在稳步增长，特别是在数据科学和分析领域。与 Scala 密切相关的是 Apache
    Spark，它是用 Scala 编写的，并且广泛应用于分析领域。本书将帮助你充分利用这两种工具的力量，理解大数据。
- en: Conventions
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 约定
- en: 'In this book, you will find a number of text styles that distinguish between
    different kinds of information. Here are some examples of these styles and an
    explanation of their meaning. Code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles are shown as follows: "The next lines of code read the link and assign
    it to the to the `BeautifulSoup` function."'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，你会发现一些文本样式，用于区分不同类型的信息。以下是这些样式的示例和它们的含义解释。文中的代码词、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟URL、用户输入和
    Twitter 账户名的显示方式如下：“下一行代码读取链接并将其传递给 `BeautifulSoup` 函数。”
- en: 'A block of code is set as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块设置如下：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望将你的注意力引导到代码块的某一部分时，相关的行或项将以粗体显示：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出将以以下方式呈现：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**New terms**and **important words** are shown in bold. Words that you see
    on the screen, for example, in menus or dialog boxes, appear in the text like
    this: "Clicking the Next button moves you to the next screen."'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**新术语** 和 **重要词汇** 以粗体显示。你在屏幕上看到的词汇，例如在菜单或对话框中，文本中会以这种方式呈现：“点击下一步按钮会将你带到下一个界面。”'
- en: Warnings or important notes appear like this.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 警告或重要注意事项将以这样的方式出现。
- en: Tips and tricks appear like this.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 提示和技巧将以这种方式出现。
- en: Reader feedback
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读者反馈
- en: Feedback from our readers is always welcome. Let us know what you think about
    this book-what you liked or disliked. Reader feedback is important for us as it
    helps us develop titles that you will really get the most out of. To send us general
    feedback, simply e-mail `feedback@packtpub.com`, and mention the book's title
    in the subject of your message. If there is a topic that you have expertise in
    and you are interested in either writing or contributing to a book, see our author
    guide at [www.packtpub.com/authors](http://www.packtpub.com/authors).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终欢迎读者反馈。请告诉我们你对这本书的看法——你喜欢或不喜欢的部分。读者的反馈对我们非常重要，它帮助我们开发出你真正能从中受益的书籍。如果你有任何建议，请通过电子邮件`feedback@packtpub.com`联系我们，并在邮件主题中注明书名。如果你在某个领域有专业知识，并且有兴趣为书籍写作或贡献内容，请查看我们的作者指南：[www.packtpub.com/authors](http://www.packtpub.com/authors)。
- en: Customer support
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户支持
- en: Now that you are the proud owner of a Packt book, we have a number of things
    to help you to get the most from your purchase.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经拥有了一本Packt书籍，我们为你准备了多项内容，帮助你最大化地利用这次购买。
- en: Downloading the example code
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码
- en: 'You can download the example code files for this book from your account at
    [http://www.packtpub.com](http://www.packtpub.com). If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you. You can download the
    code files by following these steps:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从你在[http://www.packtpub.com](http://www.packtpub.com)的账户中下载本书的示例代码文件。如果你是在其他地方购买的此书，你可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册以直接通过电子邮件接收文件。你可以按照以下步骤下载代码文件：
- en: Log in or register to our website using your e-mail address and password.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你的电子邮件地址和密码登录或注册我们的网站。
- en: Hover the mouse pointer on the SUPPORT tab at the top.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将鼠标指针悬停在顶部的“支持”标签上。
- en: Click on Code Downloads & Errata.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“代码下载与勘误”。
- en: Enter the name of the book in the Search box.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索框中输入书名。
- en: Select the book for which you're looking to download the code files.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你希望下载代码文件的书籍。
- en: Choose from the drop-down menu where you purchased this book from.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从下拉菜单中选择你购买这本书的来源。
- en: Click on Code Download.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“代码下载”。
- en: 'Once the file is downloaded, please make sure that you unzip or extract the
    folder using the latest version of:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下载文件后，请确保使用最新版本的工具解压或提取文件夹：
- en: WinRAR / 7-Zip for Windows
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于Windows的WinRAR / 7-Zip
- en: Zipeg / iZip / UnRarX for Mac
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于Mac的Zipeg / iZip / UnRarX
- en: 7-Zip / PeaZip for Linux
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于Linux的7-Zip / PeaZip
- en: The code bundle for the book is also hosted on GitHub at [https://github.com/PacktPublishing/Scala-and-Spark-for-Big-Data-Analytics](https://github.com/PacktPublishing/Scala-and-Spark-for-Big-Data-Analytics).
    We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的代码包也托管在GitHub上，地址为：[https://github.com/PacktPublishing/Scala-and-Spark-for-Big-Data-Analytics](https://github.com/PacktPublishing/Scala-and-Spark-for-Big-Data-Analytics)。我们还有其他来自我们丰富书籍和视频目录的代码包，地址为：[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)。快去看看吧！
- en: Downloading the color images of this book
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载本书的彩色图片
- en: We also provide you with a PDF file that has color images of the screenshots/diagrams
    used in this book. The color images will help you better understand the changes
    in the output. You can download this file from [https://www.packtpub.com/sites/default/files/downloads/ScalaandSparkforBigDataAnalytics_ColorImages.pdf](https://www.packtpub.com/sites/default/files/downloads/ScalaandSparkforBigDataAnalytics_ColorImages.pdf)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还为你提供了一份包含本书中截图/图表彩色图片的PDF文件。这些彩色图片将帮助你更好地理解输出结果中的变化。你可以从[https://www.packtpub.com/sites/default/files/downloads/ScalaandSparkforBigDataAnalytics_ColorImages.pdf](https://www.packtpub.com/sites/default/files/downloads/ScalaandSparkforBigDataAnalytics_ColorImages.pdf)下载此文件。
- en: Errata
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 勘误
- en: Although we have taken every care to ensure the accuracy of our content, mistakes
    do happen. If you find a mistake in one of our books-maybe a mistake in the text
    or the code-we would be grateful if you could report this to us. By doing so,
    you can save other readers from frustration and help us improve subsequent versions
    of this book. If you find any errata, please report them by visiting [http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details of your errata. Once your errata are verified, your submission will
    be accepted and the errata will be uploaded to our website or added to any list
    of existing errata under the Errata section of that title. To view the previously
    submitted errata, go to [https://www.packtpub.com/books/content/support](https://www.packtpub.com/books/content/support)
    and enter the name of the book in the search field. The required information will
    appear under the Errata section.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已尽一切努力确保内容的准确性，但错误还是可能发生。如果你在我们的书中发现错误——可能是文本或代码中的错误——我们将非常感激你能向我们报告。这样做，你不仅可以帮助其他读者避免困扰，还能帮助我们改进本书的后续版本。如果你发现任何勘误，请访问[http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)报告，选择你的书籍，点击“勘误提交表单”链接，填写勘误详情。一旦你的勘误得到验证，提交将被接受，并且勘误将被上传到我们的网站或加入该书籍的现有勘误列表中。要查看之前提交的勘误，请访问[https://www.packtpub.com/books/content/support](https://www.packtpub.com/books/content/support)，在搜索框中输入书名，所需信息将显示在勘误部分。
- en: Piracy
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 盗版
- en: Piracy of copyrighted material on the Internet is an ongoing problem across
    all media. At Packt, we take the protection of our copyright and licenses very
    seriously. If you come across any illegal copies of our works in any form on the
    Internet, please provide us with the location address or website name immediately
    so that we can pursue a remedy. Please contact us at `copyright@packtpub.com`
    with a link to the suspected pirated material. We appreciate your help in protecting
    our authors and our ability to bring you valuable content.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网版权材料的盗版问题在所有媒体中都是一个持续存在的问题。在Packt，我们非常重视保护我们的版权和许可。如果你在互联网上发现我们作品的任何非法复制品，请立即向我们提供该位置地址或网站名称，以便我们采取相应措施。请通过`copyright@packtpub.com`与我们联系，并提供涉嫌盗版材料的链接。感谢你在保护我们的作者和我们提供有价值内容的能力方面的帮助。
- en: Questions
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: If you have a problem with any aspect of this book, you can contact us at `questions@packtpub.com`,
    and we will do our best to address the problem.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在本书的任何方面遇到问题，可以通过`questions@packtpub.com`与我们联系，我们将尽力解决问题。
