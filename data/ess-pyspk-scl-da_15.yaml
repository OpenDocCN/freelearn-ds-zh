- en: 'Chapter 12: Spark SQL Primer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章：Spark SQL 入门
- en: In the previous chapter, you learned about data visualizations as a powerful
    and key tool of data analytics. You also learned about various Python visualization
    libraries that can be used to visualize data in pandas DataFrames. An equally
    important and ubiquitous and essential skill in any data analytics professional's
    repertoire is **Structured Query Language** or **SQL**. **SQL** has existed as
    long as the field of data analytics has existed, and even with the advent of big
    data, data science, and **machine learning** (**ML**), SQL is still proving to
    be indispensable.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你了解了数据可视化作为数据分析的强大工具。你还学习了可以用于可视化 pandas DataFrame 数据的各种 Python 可视化库。另一个同样重要、普遍且必不可少的技能是**结构化查询语言**（**SQL**）。**SQL**
    自数据分析领域诞生以来一直存在，即使在大数据、数据科学和**机器学习**（**ML**）兴起的今天，SQL 仍然被证明是不可或缺的。
- en: This chapter introduces you to the basics of SQL and looks at how SQL can be
    applied in a distributed computing setting via Spark SQL. You will learn about
    the various components that make up Spark SQL, including the storage, metastore,
    and the actual query execution engine. We will look at the differences between
    **Hadoop Hive** and Spark SQL, and finally, end with some techniques for improving
    the performance of Spark SQL queries.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将向你介绍 SQL 的基础知识，并探讨如何通过 Spark SQL 在分布式计算环境中应用 SQL。你将了解构成 Spark SQL 的各种组件，包括存储、元数据存储和实际的查询执行引擎。我们将比较**Hadoop
    Hive**和 Spark SQL 之间的差异，最后介绍一些提高 Spark SQL 查询性能的技巧。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Introduction to SQL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL 简介
- en: Introduction to Spark SQL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL 简介
- en: Spark SQL language reference
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL 语言参考
- en: Optimizing Spark SQL performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化 Spark SQL 性能
- en: Some of the areas covered in this chapter include the usefulness of SQL as a
    language for slicing and dicing of data, the individual components of Spark SQL,
    and how they come together to create a powerful distributed SQL engine on Apache
    Spark. You will look at a Spark SQL language reference to help with your data
    analytics needs and some techniques to optimize the performance of your Spark
    SQL queries.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涉及的内容包括 SQL 作为数据切片和切割语言的实用性、Spark SQL 的各个组件，以及它们如何结合起来在 Apache Spark 上创建一个强大的分布式
    SQL 引擎。你将查阅 Spark SQL 语言参考，以帮助你的数据分析需求，并学习一些优化 Spark SQL 查询性能的技巧。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Here is what you''ll need for this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所需的内容如下：
- en: In this chapter, we will be using Databricks Community Edition to run our code
    (https://community.cloud.databricks.com). Sign-up instructions can be found at
    [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 Databricks Community Edition 运行我们的代码 (https://community.cloud.databricks.com)。注册说明可以在
    [https://databricks.com/try-databricks](https://databricks.com/try-databricks)
    找到。
- en: The code and data used in this chapter can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter12](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter12).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章使用的代码和数据可以从 [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter12](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter12)
    下载。
- en: Introduction to SQL
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SQL 简介
- en: '**SQL** is a declarative language for storing, manipulating, and querying data
    stored in relational databases, also called **relational database management systems**
    (**RDBMSes**). A relational database contains data in tables, which in turn contain
    rows and columns. In the real world, entities have relationships among themselves,
    and a relational database tries to mimic these real-world relationships as relationships
    between tables. Thus, in relational databases, individual tables contain data
    related to individual entities, and these tables might be related.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**SQL** 是一种声明式语言，用于存储、操作和查询存储在关系型数据库中的数据，也称为**关系数据库管理系统**（**RDBMSes**）。关系型数据库中的数据以表格形式存储，表格包含行和列。在现实世界中，实体之间存在关系，关系型数据库试图将这些现实世界中的关系模拟为表格之间的关系。因此，在关系型数据库中，单个表格包含与特定实体相关的数据，这些表格可能存在关联。'
- en: SQL is a declarative programming language that helps you specify which rows
    and columns you want to retrieve from a given table and specify constraints to
    filter out any data. An RDBMS contains a query optimizer that turns a SQL declaration
    into a query plan and executes it on the database engine. The query plan is finally
    translated into an execution plan for the database engine to read table rows and
    columns into memory and filter them based on the provided constraints.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: SQL是一种声明式编程语言，帮助你指定想要从给定表中检索的行和列，并指定约束以过滤掉任何数据。RDBMS包含一个查询优化器，它将SQL声明转换为查询计划，并在数据库引擎上执行。查询计划最终被转换为数据库引擎的执行计划，用于读取表中的行和列到内存中，并根据提供的约束进行过滤。
- en: The SQL language includes subsets for defining schemas—called **Data Definition
    Language** (**DDL**)—and modifying and querying data—called **Data Manipulation
    Language** (**DML**), as discussed in the following sections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: SQL语言包括定义模式的子集——称为**数据定义语言**（**DDL**）——以及修改和查询数据的子集——称为**数据操作语言**（**DML**），如以下章节所述。
- en: DDL
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DDL
- en: '`CREATE`, `ALTER`, `DROP`, `TRUNCATE`, and so on. The following SQL query represents
    a DDL SQL statement:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`CREATE`、`ALTER`、`DROP`、`TRUNCATE`等。以下SQL查询表示一个DDL SQL语句：'
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The previous SQL statement represents a typical command to create a new table
    within a specific database and a schema with a few columns and its data types
    defined. A database is a collection of data and log files, while a schema is a
    logical grouping within a database.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个SQL语句表示在特定数据库中创建新表的典型命令，并定义了几个列及其数据类型。数据库是数据和日志文件的集合，而模式是数据库中的逻辑分组。
- en: DML
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DML
- en: '`SELECT`, `UPDATE`, `INSERT`, `DELETE`, `MERGE`, and so on. An example DML
    query is shown here:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`SELECT`、`UPDATE`、`INSERT`、`DELETE`、`MERGE`等。以下是一个DML查询示例：'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The previous query results in `column2` being aggregated by each distinct value
    of `column1` after filtering rows based on the constraint specified on `column3`,
    and finally, the results being sorted by the aggregated value.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个查询结果通过对`column1`的每个不同值进行聚合，在基于`column3`上指定的约束过滤行后，最终根据聚合值对结果进行排序。
- en: Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although SQL generally adheres to certain standards set by the **American National
    Standards Institute** (**ANSI**), each RDBMS vendor has a slightly different implementation
    of the SQL standards, and you should refer to the specific RDBMS's documentation
    for the correct syntax.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SQL通常遵循**美国国家标准协会**（**ANSI**）设定的某些标准，但每个RDBMS供应商对SQL标准的实现略有不同，您应参考特定RDBMS的文档以获取正确的语法。
- en: The previous SQL statement represents standard DDL and DML queries; however,
    there might be subtle implementation details between each RDBMS's implementation
    of the SQL standard. Similarly, Apache Spark also has its own implementation of
    the ANSI SQL 2000 standard.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个SQL语句表示标准的DDL和DML查询；然而，每个RDBMS在SQL标准的实现上可能会有细微的差异。同样，Apache Spark也有自己对ANSI
    SQL 2000标准的实现。
- en: Joins and sub-queries
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接与子查询
- en: 'Tables within relational databases contain data that is related, and it is
    often required to join the data between various tables to produce meaningful analytics.
    Thus, SQL supports operations such as joins and sub-queries for users to be able
    to combine data across tables, as shown in the following SQL statement:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据库中的表包含相关数据，通常需要在不同表之间进行连接，以生成有意义的分析。因此，SQL支持诸如连接和子查询等操作，使用户能够跨表合并数据，如以下SQL语句所示：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the previous SQL query, we join two tables using a common key column and
    produce columns from both the tables after the `JOIN` operation. Similarly, sub-queries
    are queries within queries that can occur in a `SELECT`, `WHERE`, or `FROM` clause
    that lets you combine data from multiple tables. A specific implementation of
    these SQL queries within Spark SQL will be explored in the following sections.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的SQL查询中，我们使用一个公共键列连接了两个表，并在`JOIN`操作后从这两个表中生成了列。类似地，子查询是查询中的查询，可以出现在`SELECT`、`WHERE`或`FROM`子句中，它允许你从多个表中合并数据。接下来的章节将探讨在Spark
    SQL中这些SQL查询的具体实现。
- en: Row-based versus columnar storage
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于行存储与列存储
- en: 'Databases physically store data in one of two ways, either in a row-based manner
    or in a columnar fashion. Each has its own advantages and disadvantages, depending
    on its use case. In row-based storage, all the values are stored together, and
    in columnar storage, all the values of a column are stored contiguously on a physical
    storage medium, as shown in the following screenshot:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库以两种方式之一物理存储数据，要么以行存储方式，要么以列存储方式。每种方式都有其优缺点，取决于使用场景。在行存储中，所有的值一起存储，而在列存储中，单个列的所有值会连续存储在物理存储介质上，如下图所示：
- en: '![Figure 12.1 – Row-based versus columnar storage'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.1 – 行存储与列存储'
- en: '](img/B16736_12_1.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_12_1.jpg)'
- en: Figure 12.1 – Row-based versus columnar storage
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 – 行存储与列存储
- en: As depicted in the previous screenshot, in row-based storage, an entire row
    with all its column values is stored together on physical storage. This makes
    it easier to find an individual row and retrieve all its columns from storage
    in a fast and efficient manner. Columnar storage, on the other hand, stores all
    the values of an individual column contiguously on physical storage, which makes
    retrieving an individual column fast and efficient.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面截图所示，在行存储中，整行及其所有列值会被一起存储在物理存储介质上。这使得查找单个行并快速高效地从存储中检索其所有列变得更容易。而列存储则将单个列的所有值连续存储在物理存储介质上，这使得检索单个列既快速又高效。
- en: Row-based storage is more popular with transactional systems, where quickly
    retrieving an individual transactional record or a row is more important. On the
    other hand, analytical systems typically deal with aggregates of rows and only
    need to retrieve a few columns per query. Thus, it is more efficient to choose
    columnar storage while designing analytical systems. Columnar storage also offers
    a better data compression ratio, thus making optimal use of available storage
    space when storing huge amounts of historical data. Analytical storage systems
    including **data warehouses** and **data lakes** prefer columnar storage over
    row-based storage. Popular big data file formats such as **Parquet** and **Optimized
    Row Columnar** (**ORC**) are also columnar.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 行存储在事务系统中更为流行，在这些系统中，快速检索单个事务记录或行更为重要。另一方面，分析系统通常处理的是行的聚合，只需要每个查询检索少量的列。因此，在设计分析系统时，选择列存储更加高效。列存储还提供了更好的数据压缩比，从而在存储大量历史数据时，能够更有效地利用可用存储空间。包括
    **数据仓库** 和 **数据湖** 在内的分析存储系统更倾向于使用列存储而非行存储。流行的大数据文件格式如 **Parquet** 和 **优化行列存储**（**ORC**）也采用列存储。
- en: The ease of use and ubiquity of SQL has led the creators of many non-relational
    data processing frameworks such as Hadoop and Apache Spark to adopt subsets or
    variations of SQL in creating Hadoop Hive and Spark SQL. We will explore Spark
    SQL in detail in the following section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 的易用性和普遍性促使许多非关系型数据处理框架的创建者，如 Hadoop 和 Apache Spark，采纳 SQL 的子集或变体来创建 Hadoop
    Hive 和 Spark SQL。我们将在接下来的章节中详细探讨 Spark SQL。
- en: Introduction to Spark SQL
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL 简介
- en: '**Spark SQL** brings native support for SQL to Apache Spark and unifies the
    process of querying data stored both in Spark DataFrames and in external data
    sources. Spark SQL unifies DataFrames and relational tables and makes it easy
    for developers to intermix SQL commands with querying external data for complex
    analytics. With the release of **Apache Spark 1.3**, Spark DataFrames powered
    by Spark SQL became the de facto abstraction of Spark for expressing data processing
    code, while **resilient distributed datasets** (**RDDs**) still remain Spark''s
    core abstraction method, as shown in the following diagram:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark SQL** 为 Apache Spark 提供了原生的 SQL 支持，并统一了查询存储在 Spark DataFrames 和外部数据源中的数据的过程。Spark
    SQL 将 DataFrames 和关系表统一，使得开发者可以轻松地将 SQL 命令与外部数据查询结合，进行复杂的分析。随着 **Apache Spark
    1.3** 的发布，Spark SQL 驱动的 Spark DataFrames 成为表达数据处理代码的事实标准抽象方式，而 **弹性分布式数据集**（**RDDs**）仍然是
    Spark 的核心抽象方法，如下图所示：'
- en: '![Figure 12.2 – Spark SQL architecture'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.2 – Spark SQL 架构'
- en: '](img/B16736_12_2.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_12_2.jpg)'
- en: Figure 12.2 – Spark SQL architecture
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 – Spark SQL 架构
- en: As shown in the previous diagram, you can see that most of Spark's components
    now leverage Spark SQL and DataFrames. Spark SQL provides more information about
    the structure of the data and the computation being performed, and the **Spark
    SQL engine** uses this extra information to perform additional optimizations to
    the query. With Spark SQL, all of Spark's components—including **Structured Streaming**,
    **DataFrames**, **Spark ML**, and **GraphFrames**—and all its programming **application**
    **programming interfaces** (**APIs**)—including **Scala**, **Java**, **Python**,
    **R**, and **SQL**—use the same execution engine to express computations. This
    unification makes it easy for you to switch back and forth between different APIs
    and lets you choose the right API for the task at hand. Certain data processing
    operations, such as joining multiple tables, are expressed much more easily in
    SQL, and developers can easily mix **SQL** with **Scala**, **Java**, or **Python**
    code.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图所示，你可以看到现在大多数 Spark 组件都利用了 Spark SQL 和 DataFrames。Spark SQL 提供了关于数据结构和正在执行的计算的更多信息，**Spark
    SQL 引擎**利用这些额外的信息对查询进行进一步的优化。使用 Spark SQL，Spark 的所有组件——包括**结构化流**、**DataFrames**、**Spark
    ML**和**GraphFrames**——以及所有的编程**应用程序编程接口**（**APIs**）——包括**Scala**、**Java**、**Python**、**R**和**SQL**——都使用相同的执行引擎来表达计算。这种统一性使你可以轻松地在不同的
    API 之间切换，并让你根据当前任务选择合适的 API。某些数据处理操作，如连接多个表格，在 SQL 中表达起来更容易，开发者可以轻松地将**SQL**与**Scala**、**Java**或**Python**代码混合使用。
- en: Spark SQL also brings a powerful new optimization framework called **Catalyst**,
    which can automatically transform any data processing code, whether expressed
    using Spark DataFrames or using Spark SQL, to execute more efficiently. We will
    explore the **Catalyst** optimizer in the following section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 还引入了一个强大的新优化框架，名为**Catalyst**，它可以自动将任何数据处理代码，不论是使用 Spark DataFrames
    还是使用 Spark SQL 表达的，转换为更高效的执行方式。我们将在接下来的章节中深入探讨**Catalyst**优化器。
- en: Catalyst optimizer
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Catalyst 优化器
- en: A **SQL query optimizer** in an RDBMS is a process that determines the most
    efficient way for a given SQL query to process data stored in a database. The
    SQL optimizer tries to generate the most optimal execution for a given SQL query.
    The optimizer typically generates multiple query execution plans and chooses the
    optimal one among them. It typically takes into consideration factors such as
    the **central processing unit** (**CPU**), **input/output** (**I/O**), and any
    available statistics on the tables being queried to choose the most optimal query
    execution plan. The optimizer based on the chosen query execution plan chooses
    to re-order, merge, and process a query in any order that yields the optimal results.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**SQL 查询优化器**在关系型数据库管理系统（RDBMS）中是一个过程，它确定给定 SQL 查询处理存储在数据库中的数据的最有效方式。SQL 优化器尝试生成给定
    SQL 查询的最优执行方式。优化器通常会生成多个查询执行计划，并从中选择最优的一个。它通常会考虑因素如**中央处理单元**（**CPU**）、**输入/输出**（**I/O**）以及查询的表格的任何可用统计信息，以选择最优的查询执行计划。优化器根据选择的查询执行计划，决定以任何顺序重新排序、合并和处理查询，以获得最优结果。'
- en: 'The Spark SQL engine also comes equipped with a query optimizer named **Catalyst**.
    **Catalyst** is based on **functional programming** concepts, like the rest of
    Spark''s code base, and uses Scala''s programming language features to build a
    robust and extensible query optimizer. Spark''s Catalyst optimizer generates an
    optimal execution plan for a given Spark SQL query by following a series of steps,
    as depicted in the following diagram:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 引擎也配备了一个名为**Catalyst**的查询优化器。**Catalyst**基于**函数式编程**的概念，像 Spark 的其他代码库一样，利用
    Scala 编程语言的特性来构建一个强大且可扩展的查询优化器。Spark 的 Catalyst 优化器通过一系列步骤为给定的 Spark SQL 查询生成一个最优执行计划，如下图所示：
- en: '![Figure 12.3 – Spark''s Catalyst optimizer'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.3 – Spark 的 Catalyst 优化器'
- en: '](img/B16736_12_03.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_12_03.jpg)'
- en: Figure 12.3 – Spark's Catalyst optimizer
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3 – Spark 的 Catalyst 优化器
- en: As shown in the previous diagram, the Catalyst optimizer first generates a logical
    plan after resolving references, then optimizes the logical plan based on standard
    rule-based optimization techniques. It then generates a set of physical execution
    plans using the optimized logical plan and chooses the best physical plan, and
    finally, generates **Java virtual machine** (**JVM**) bytecode using the best
    possible physical plan. This process allows Spark SQL to translate user queries
    into the best possible data processing code without the developer having any nuanced
    understanding of the microscopic inner workings of Spark's distributed data processing
    paradigm. Moreover, Spark SQL DataFrame APIs in the Java, Scala, Python, and R
    programming languages all go through the same Catalyst optimizer. Thus, Spark
    SQL or any data processing written using DataFrame APIs irrespective of the programming
    language yields comparable performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，Catalyst 优化器首先在解析引用后生成逻辑计划，然后基于标准的规则优化技术优化逻辑计划。接着，使用优化后的逻辑计划生成一组物理执行计划，并选择最佳的物理计划，最终使用最佳的物理计划生成**Java
    虚拟机**（**JVM**）字节码。这个过程使得 Spark SQL 能够将用户查询转化为最佳的数据处理代码，而开发者无需了解 Spark 分布式数据处理范式的微观细节。此外，Spark
    SQL DataFrame API 在 Java、Scala、Python 和 R 编程语言中的实现都经过相同的 Catalyst 优化器。因此，无论使用哪种编程语言编写的
    Spark SQL 或任何数据处理代码，性能都是相当的。
- en: Tip
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: There are a few exceptions where PySpark DataFrame code might not be comparable
    to Scala or Java code in performance. One such example is when using non-vectorized
    **user-defined functions** (**UDFs**) in PySpark DataFrame operations. Catalyst
    doesn't have any visibility into UDFs in Python and will not be able to optimize
    the code. Thus, these should be replaced with either Spark SQL's built-in function
    or the UDFs defined in Scala or Java.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，PySpark DataFrame 的代码性能可能无法与 Scala 或 Java 代码相比。一个例子是当在 PySpark DataFrame
    操作中使用非矢量化的**用户定义函数**（**UDFs**）时。Catalyst 无法查看 Python 中的 UDF，因此无法优化代码。因此，应该将其替换为
    Spark SQL 的内置函数或在 Scala 或 Java 中定义的 UDF。
- en: A seasoned data engineer with a thorough understanding of the RDD API can possibly
    write a little more optimized code than the Catalyst optimizer; however, by letting
    Catalyst handle the code-generation complexity, developers can focus their valuable
    time on actual data processing tasks, thus making them even more efficient. After
    gaining an understanding of the inner workings of the Spark SQL engine, it would
    be useful to understand the kinds of data sources that Spark SQL can work with.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一位经验丰富的数据工程师，如果对 RDD API 有深入了解，可能写出比 Catalyst 优化器更优化的代码；然而，通过让 Catalyst 处理代码生成的复杂性，开发者可以将宝贵的时间集中在实际的数据处理任务上，从而提高效率。在深入了解
    Spark SQL 引擎的内部工作原理后，理解 Spark SQL 可以处理的数据源类型会非常有用。
- en: Spark SQL data sources
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark SQL 数据源
- en: Since Spark DataFrame API and SQL API are both based on the same Spark SQL engine
    powered by the Catalyst optimizer, they also support the same set of data sources.
    A few prominent Spark SQL data sources are presented here.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Spark DataFrame API 和 SQL API 都基于由 Catalyst 优化器提供支持的相同 Spark SQL 引擎，它们也支持相同的数据源。这里展示了一些突出的
    Spark SQL 数据源。
- en: File data source
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文件数据源
- en: 'File-based data sources such as Parquet, ORC, Delta, and so on are supported
    by Spark SQL out of the box, as shown in the following SQL query example:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 原生支持基于文件的数据源，例如 Parquet、ORC、Delta 等，以下是一个 SQL 查询示例：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the previous SQL statement, data is directly queried from a `delta.` prefix.
    The same SQL construct can also be used with a Parquet file location on the data
    lake.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 SQL 语句中，数据直接从 `delta.` 前缀查询。同样的 SQL 语法也可以用于数据湖中的 Parquet 文件位置。
- en: 'Other file types such as **JavaScript Object Notation** (**JSON**) and **comma-separated
    values** (**CSV**) would require first registering a table or a view with the
    metastore, as these files are not self-describing and lack any inherent schema
    information. An example of using a CSV file with Spark SQL is presented in the
    following SQL query:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其他文件类型，如**JavaScript 对象表示法**（**JSON**）和**逗号分隔值**（**CSV**），需要先在元数据存储中注册表或视图，因为这些文件不自描述，缺乏固有的模式信息。以下是使用
    CSV 文件与 Spark SQL 的示例 SQL 查询：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the previous SQL statement, we first create a temporary view by using a CSV
    file on the data lake using a CSV data source. We also use `OPTIONS` to specify
    the CSV file has a header row and to infer a schema from the file itself.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 SQL 语句中，我们首先使用数据湖中的 CSV 文件通过 CSV 数据源创建一个临时视图。我们还使用 `OPTIONS` 来指定 CSV 文件包含标题行，并从文件本身推断出模式。
- en: Note
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A metastore is an RDBMS database where Spark SQL persists metadata information
    such as databases, tables, columns, and partitions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 元存储是一个 RDBMS 数据库，Spark SQL 将数据库、表、列和分区等元数据信息持久化存储在其中。
- en: You could also create a permanent table instead of a temporary view if the table
    needs to persist across cluster restarts and will be reused later.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果表需要在集群重启后保持持久化，并且将来会被重用，您也可以创建一个永久表而不是临时视图。
- en: JDBC data source
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JDBC 数据源
- en: 'Existing RDBMS databases can also be registered with the metastore via **Java
    Database Connectivity** (**JDBC**) and used as a data source in Spark SQL. An
    example is presented in the following code block:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的 RDBMS 数据库也可以通过 **Java 数据库连接**（**JDBC**）与元存储进行注册，并作为 Spark SQL 的数据源。以下代码块展示了一个示例：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the previous code block, we create a temporary view using the `jdbc` data
    source and specify database connectivity options such as the database **Uniform
    Resource Locator** (**URL**), table name, username, password, and so on.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们使用 `jdbc` 数据源创建了一个临时视图，并指定了数据库连接选项，如数据库 **统一资源定位符**（**URL**）、表名、用户名、密码等。
- en: Hive data source
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hive 数据源
- en: '**Apache Hive** is a data warehouse in the Hadoop ecosystem that can be used
    to read, write, and manage datasets on a Hadoop filesystem or a data lake using
    SQL. Spark SQL can be used with Apache Hive, including a Hive metastore, Hive
    **Serializer/Deserializer** (**SerDes**), and Hive UDFs. Spark SQL supports most
    Hive features such as the Hive query language, Hive expressions, user-defined
    aggregate functions, window functions, joins, unions, sub-queries, and so on.
    However, features such as Hive **atomicity, consistency, isolation, durability**
    (**ACID**) table updates, Hive I/O formats, and certain Hive-specific optimizations
    are not supported. A full list of the supported and unsupported features can be
    found in Databricks'' public documentation here: [https://docs.databricks.com/spark/latest/spark-sql/compatibility/hive.html](https://docs.databricks.com/spark/latest/spark-sql/compatibility/hive.html).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Hive** 是 Hadoop 生态系统中的一个数据仓库，可以使用 SQL 读取、写入和管理存储在 Hadoop 文件系统或数据湖中的数据集。Spark
    SQL 可以与 Apache Hive 一起使用，包括 Hive 元存储、Hive **序列化/反序列化器**（**SerDes**）以及 Hive UDF。Spark
    SQL 支持大多数 Hive 特性，如 Hive 查询语言、Hive 表达式、用户定义的聚合函数、窗口函数、连接、并集、子查询等。然而，像 Hive **原子性、一致性、隔离性、持久性**（**ACID**）表更新、Hive
    I/O 格式以及某些 Hive 特定优化等特性是不支持的。支持和不支持的特性完整列表可以在 Databricks 的公共文档中找到，链接如下：[https://docs.databricks.com/spark/latest/spark-sql/compatibility/hive.html](https://docs.databricks.com/spark/latest/spark-sql/compatibility/hive.html)。'
- en: Now that you have gained an understanding of Spark SQL components such as the
    Catalyst optimizer and its data sources, we can delve into Spark SQL-specific
    syntax and functions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经了解了 Spark SQL 组件，如 Catalyst 优化器及其数据源，我们可以深入探讨 Spark SQL 特定的语法和函数。
- en: Spark SQL language reference
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL 语言参考
- en: Being a part of the overarching Hadoop ecosystem, Spark has traditionally been
    Hive-compliant. While the Hive query language diverges greatly from ANSI SQL standards,
    Spark 3.0 Spark SQL can be made ANSI SQL-compliant using a `spark.sql.ansi.enabled`
    configuration. With this configuration enabled, Spark SQL uses an ANSI SQL-compliant
    dialect instead of a Hive dialect.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 Hadoop 生态系统的一部分，Spark 一直以来都与 Hive 兼容。尽管 Hive 查询语言与 ANSI SQL 标准有很大差异，Spark
    3.0 的 Spark SQL 可以通过配置 `spark.sql.ansi.enabled` 来实现 ANSI SQL 兼容。启用此配置后，Spark SQL
    将使用 ANSI SQL 兼容的方言，而不是 Hive 方言。
- en: Even with ANSI SQL compliance enabled, Spark SQL may not entirely conform to
    ANSI SQL dialect, and in this section, we will explore some of the prominent DDL
    and DML syntax of Spark SQL.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 即使启用了 ANSI SQL 兼容，Spark SQL 可能仍无法完全符合 ANSI SQL 方言，本节将探讨一些 Spark SQL 的突出 DDL
    和 DML 语法。
- en: Spark SQL DDL
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark SQL DDL
- en: 'The syntax for creating a database and a table using Spark SQL is presented
    as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark SQL 创建数据库和表的语法如下所示：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the previous code block, we do the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们执行了以下操作：
- en: First, we create a database if it doesn't already exist, using the `CREATE DATABASE`
    command. With this command, options such as the physical warehouse location on
    persistent storage and other database properties can also be specified as options.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，如果数据库不存在，我们使用 `CREATE DATABASE` 命令创建一个数据库。通过此命令，还可以指定一些选项，如持久存储中的物理仓库位置和其他数据库属性。
- en: Then, we create a table using `delta` as the data source and specify the location
    of the data. Here, data at the specified location already exists, so there is
    no need to specify any schema information such as column names and their data
    types. However, to create an empty table structure, columns and their data types
    need to be specified.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们使用 `delta` 作为数据源创建一个表，并指定数据的位置。这里，指定位置的数据已经存在，因此无需指定任何模式信息，如列名及其数据类型。然而，为了创建一个空的表结构，仍然需要指定列及其数据类型。
- en: 'To change certain properties of an existing table such as renaming the table,
    altering or dropping columns, or ammending table partition information, the `ALTER`
    command can be used, as shown in the following code sample:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改现有表的某些属性，例如重命名表、修改或删除列，或修改表分区信息，可以使用 `ALTER` 命令，如下所示的代码示例所示：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the previous code sample, we rename the table in the first SQL statement.
    The second SQL statement alters the table and adds a new column of the `String`
    type. Only changing column comments and adding new columns are supported in Spark
    SQL. The following code sample presents Spark SQL syntax for dropping or deleting
    artifacts altogether:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们在第一个 SQL 语句中重命名了表。第二个 SQL 语句修改了表并添加了一个新的 `String` 类型的列。在 Spark SQL
    中，仅支持更改列注释和添加新列。以下代码示例展示了 Spark SQL 语法，用于完全删除或删除对象：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the previous code sample, the `TRUNCATE` command deletes all the rows of
    the table and leaves the table structure and schema intact. The `DROP TABLE` command
    deletes the table along with its schema, and the `DROP DATABASE` command deletes
    the entire database itself.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，`TRUNCATE` 命令删除了表中的所有行，并保持表结构和模式不变。`DROP TABLE` 命令删除了表及其模式，而 `DROP
    DATABASE` 命令则删除整个数据库。
- en: Spark DML
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark DML
- en: 'Data manipulation involves adding, changing, and deleting data from tables.
    Some examples of this are presented in the following code statements:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据操作涉及从表中添加、修改和删除数据。以下代码语句展示了一些示例：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The previous SQL statement inserts data into an existing table using results
    from another SQL query. Similarly, the `INSERT OVERWRITE` command can be used
    to overwrite existing data and then load new data into a table. The following
    SQL statement can be used to selectively delete data from a table:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的 SQL 语句通过另一个 SQL 查询的结果将数据插入到现有表中。类似地，`INSERT OVERWRITE` 命令可用于覆盖现有数据，然后将新数据加载到表中。以下
    SQL 语句可用于选择性地删除表中的数据：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The previous SQL statement deletes selective data from the table based on a
    filter condition. Though `SELECT` statements are not necessary, they are quintessential
    in data analysis. The following SQL statement depicts the use of `SELECT` statements
    for data analysis using Spark SQL:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的 SQL 语句根据筛选条件从表中删除选择性的数据。虽然 `SELECT` 语句不是必须的，但它们在数据分析中至关重要。以下 SQL 语句展示了使用
    `SELECT` 语句进行 Spark SQL 数据分析：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The previous SQL statement performs an **inner join** of two tables based on
    a common key and calculates the average salary of each employee by year. The results
    of this query give insights into employee salary changes over the years and can
    be easily scheduled to be refreshed periodically.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的 SQL 语句根据公共键对两个表进行**内连接**，并按年份计算每位员工的平均薪资。此查询的结果提供了员工薪资随年份变化的见解，并且可以轻松安排定期刷新。
- en: 'This way, using the powerful distributed SQL engine of Apache Spark and its
    expressive Spark SQL language, you can perform complex data analysis in a fast
    and efficient manner without having to learn any new programming languages. A
    complete Spark SQL reference guide for an exhaustive list of supported data types,
    function libraries, and SQL syntax can be found in Apache Spark''s public documentation
    here: [https://spark.apache.org/docs/latest/sql-ref-syntax.html](https://spark.apache.org/docs/latest/sql-ref-syntax.html).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，使用 Apache Spark 强大的分布式 SQL 引擎及其富有表现力的 Spark SQL 语言，您可以在无需学习任何新编程语言的情况下，以快速高效的方式执行复杂的数据分析。有关支持的数据类型、函数库和
    SQL 语法的完整参考指南，可以在 Apache Spark 的公共文档中找到：[https://spark.apache.org/docs/latest/sql-ref-syntax.html](https://spark.apache.org/docs/latest/sql-ref-syntax.html)。
- en: Though Spark SQL's **Catalyst** optimizer does most of the heavy lifting, it's
    useful to know a few techniques to further tune Spark SQL's performance, and a
    few prominent ones are presented in the following section.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Spark SQL 的**Catalyst**优化器承担了大部分优化工作，但了解一些进一步调整 Spark SQL 性能的技巧还是很有帮助的，以下部分将介绍几个显著的技巧。
- en: Optimizing Spark SQL performance
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化 Spark SQL 性能
- en: In the previous section, you learned how the Catalyst optimizer optimizes user
    code by running the code through a set of optimization steps until an optimal
    execution plan is derived. To take advantage of the Catalyst optimizer, it is
    recommended to use Spark code that leverages the Spark SQL engine—that is, Spark
    SQL and DataFrame APIs—and avoid using RDD-based Spark code as much as possible.
    The Catalyst optimizer has no visibility into UDFs, thus users could end up writing
    sub-optimal code that might degrade performance. Thus, it is recommended to use
    built-in functions instead of UDFs or to define functions in Scala and Java and
    then use them in SQL and Python APIs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你了解了 Catalyst 优化器是如何通过将代码运行经过一系列优化步骤，直到得出最佳执行计划，从而优化用户代码的。为了充分利用 Catalyst
    优化器，推荐使用利用 Spark SQL 引擎的 Spark 代码——即 Spark SQL 和 DataFrame API——并尽量避免使用基于 RDD
    的 Spark 代码。Catalyst 优化器无法识别 UDF（用户定义函数），因此用户可能会编写出次优的代码，从而导致性能下降。因此，推荐使用内置函数，而不是
    UDF，或者在 Scala 和 Java 中定义函数，然后在 SQL 和 Python API 中使用这些函数。
- en: Though Spark SQL supports file-based formats such as CSV and JSON, it is recommended
    to use serialized data formats such as Parquet, AVRO, and ORC. Semi-structured
    formats such as CSV or JSON incur performance costs, firstly during the schema
    inference phase, as they cannot present their schema readily to the Spark SQL
    engine. Secondly, they do not support any data filtering features such as **Predicate
    Pushdown**, thus entire files must be loaded into memory before any data can be
    filtered out at the source. Being inherently uncompressed file formats, CSV and
    JSON also consume more memory compared to binary compressed formats such as Parquet.
    Even traditional relational databases are preferred over using semi-structured
    data formats as they support Predicate Pushdown, and some data processing responsibility
    can be delegated down to the databases.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Spark SQL 支持基于文件的格式，如 CSV 和 JSON，但推荐使用序列化数据格式，如 Parquet、AVRO 和 ORC。半结构化格式（如
    CSV 或 JSON）会带来性能开销，首先是在模式推断阶段，因为它们无法将模式直接提供给 Spark SQL 引擎。其次，它们不支持诸如**谓词下推**（Predicate
    Pushdown）之类的数据过滤功能，因此必须将整个文件加载到内存中，才能在源头过滤掉数据。作为天生的无压缩文件格式，CSV 和 JSON 相比于 Parquet
    等二进制压缩格式，也消耗更多的内存。甚至传统的关系型数据库比使用半结构化数据格式更为推荐，因为它们支持谓词下推，并且可以将一些数据处理任务委托给数据库。
- en: For iterative workloads such as ML, where the same dataset is accessed multiple
    times, it is useful to cache the dataset in memory so that subsequent scans of
    the table or DataFrame happen in memory, improving query performance greatly.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于诸如机器学习（ML）等迭代工作负载，数据集被多次访问的情况下，将数据集缓存到内存中非常有用，这样后续对表或DataFrame的扫描就会在内存中进行，从而大大提高查询性能。
- en: 'Spark comes with various `BROADCAST`, `MERGE`, `SHUFFLE_HASH`, and so on. However,
    the Spark SQL engine might sometimes not be able to predict the strategy for a
    given query. This can be mitigated by passing in **hints** to the Spark SQL query,
    as shown in the following code block:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了各种 `BROADCAST`、`MERGE`、`SHUFFLE_HASH` 等。然而，Spark SQL 引擎有时可能无法预测某个查询的策略。可以通过将**提示**传递给
    Spark SQL 查询来缓解这个问题，如下代码块所示：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the previous code block, we are passing in a `SELECT` clause. This specifies
    that the smaller table is broadcasted to all the worker nodes, which should improve
    join performance and thus the overall query performance. Similarly, `COALESCE`
    and `REPARTITION` hints can also be passed to Spark SQL queries; these hints reduce
    the number of output files, thus improving performance.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们传入了一个`SELECT`子句。这指定了较小的表会被广播到所有的工作节点，这应该能够提高连接操作的性能，从而提升整体查询性能。同样，`COALESCE`和`REPARTITION`提示也可以传递给
    Spark SQL 查询；这些提示减少了输出文件的数量，从而提升了性能。
- en: Note
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: SQL hints, query hints, or optimizer hints are additions to standard SQL statements
    used to nudge the SQL execution engine to choose a particular physical execution
    plan that the developer thinks is optimal. SQL hints have traditionally been supported
    by all RDBMS engines and are now supported by Spark SQL as well as for certain
    kinds of queries, as discussed previously.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 提示、查询提示或优化器提示是标准 SQL 语句的补充，用于提示 SQL 执行引擎选择开发者认为最优的特定物理执行计划。SQL 提示在所有关系型数据库管理系统（RDBMS）引擎中通常都得到支持，现在
    Spark SQL 也支持某些类型的查询，如前所述。
- en: 'While the Catalyst optimizer does an excellent job of producing the best possible
    physical query execution plan, it can still be thrown off by stale statistics
    on the table. Starting with Spark 3.0, `spark.sql.adaptive.enabled` configuration.
    These are just a few of the Spark SQL performance-tuning techniques available,
    and detailed descriptions of each can be found in the Apache Spark public documentation
    here: [https://spark.apache.org/docs/latest/sql-performance-tuning.html](https://spark.apache.org/docs/latest/sql-performance-tuning.html).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Catalyst 优化器在生成最佳物理查询执行计划方面表现出色，但仍然可能会因为表上的陈旧统计数据而受到影响。从 Spark 3.0 开始，`spark.sql.adaptive.enabled`
    配置项得以引入。这些只是 Spark SQL 性能调优技术中的一部分，每项技术的详细描述可以在 Apache Spark 官方文档中找到，链接如下：[https://spark.apache.org/docs/latest/sql-performance-tuning.html](https://spark.apache.org/docs/latest/sql-performance-tuning.html)。
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about SQL as a declarative language that has been
    universally accepted as the language for structured data analysis because of its
    ease of use and expressiveness. You learned about the basic constructions of SQL,
    including the DDL and DML dialects of SQL. You were introduced to the Spark SQL
    engine as the unified distributed query engine that powers both Spark SQL and
    DataFrame APIs. SQL optimizers, in general, were introduced, and Spark's very
    own query optimizer Catalyst was also presented, along with its inner workings
    as to how it takes a Spark SQL query and converts it into Java JVM bytecode. A
    reference to the Spark SQL language was also presented, along with the most important
    DDL and DML statements, with examples. Finally, a few performance optimizations
    techniques were also discussed to help you get the best out of Spark SQL for all
    your data analysis needs. In the next chapter, we will extend our Spark SQL knowledge
    and see how external data analysis tools such as **business intelligence** (**BI**)
    tools and SQL analysis tools can also leverage Apache Spark's distributed SQL
    engine to process and visualize massive amounts of data in a fast and efficient
    manner.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 SQL 作为一种声明式语言，它因易用性和表达能力而被普遍接受为结构化数据分析的语言。你了解了 SQL 的基本构造，包括 SQL 的 DDL
    和 DML 方言。你还介绍了 Spark SQL 引擎，这是一个统一的分布式查询引擎，支持 Spark SQL 和 DataFrame API。一般来说，介绍了
    SQL 优化器，Spark 自己的查询优化器 Catalyst 也做了介绍，并说明了它如何将 Spark SQL 查询转换为 Java JVM 字节码。还介绍了
    Spark SQL 语言的参考资料，以及最重要的 DDL 和 DML 语句，并提供了示例。最后，我们讨论了一些性能优化技术，帮助你在数据分析过程中充分发挥
    Spark SQL 的优势。在下一章，我们将进一步拓展 Spark SQL 知识，探讨外部数据分析工具，如 **商业智能**（**BI**）工具和 SQL
    分析工具，如何利用 Apache Spark 的分布式 SQL 引擎，以快速高效地处理和可视化大量数据。
