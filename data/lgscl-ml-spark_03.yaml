- en: Chapter 3. Understanding the Problem by Understanding the Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。通过了解数据来了解问题
- en: 'This chapter will cover in details of the DataFrame, Datasets, and **Resilient
    Distributed Dataset** (**RDD**) APIs for working with structured data targeting
    to provide a basic understanding of machine learning problems with the available
    data. At the end of the chapter you will be able to apply basic to complex data
    manipulation with ease. Some comparisons will be made available with basic abstractions
    in Spark using RDD, DataFrame, and Dataset based data manipulation to show both
    gains in terms of programming and performance. In addition, we will guide you
    on the right track so that you will be able to use Spark to persist an RDD or
    data objects in memory, allowing it to be reused efficiently across the parallel
    operations in the later stage. In a nutshell, the following topics will be covered
    throughout this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将详细介绍DataFrame、Datasets和**Resilient Distributed Dataset**（**RDD**）API，用于处理结构化数据，旨在提供对可用数据进行机器学习问题的基本理解。在本章结束时，您将能够轻松应用从基本到复杂的数据操作。将提供一些比较，使用RDD、DataFrame和Dataset进行数据操作的基本抽象，以展示在编程和性能方面的收益。此外，我们将指导您走上正确的道路，以便您能够使用Spark将RDD或数据对象持久化在内存中，从而在后期的并行操作中高效地重复使用。简而言之，本章将涵盖以下主题：
- en: Analyzing and preparing your data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析和准备您的数据
- en: Resilient Distributed Dataset (RDD) basics
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Resilient Distributed Dataset（RDD）基础知识
- en: Dataset basics
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集基础知识
- en: Dataset from string and typed class
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自字符串和类型类的数据集
- en: Spark and data scientists, workflow
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark和数据科学家，工作流程
- en: Deeper into Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入Spark
- en: Analyzing and preparing your data
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析和准备您的数据
- en: In practice, several factors affect the success of **machine learning** (**ML**)
    applications on a given task. Therefore, the representation and quality of the
    experimental dataset is first and foremost considered as the first class entities.
    It is always advisable to have better data. For example, irrelevant and redundant
    data, data features with null values or noisy data result in unreliable source
    of information. The bad properties in datasets make the knowledge discovery process
    during the machine learning model training phase more tedious and time inefficient.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，有几个因素会影响给定任务上机器学习（ML）应用的成功。因此，实验数据集的表示和质量首先被视为一流实体。拥有更好的数据总是明智的。例如，不相关和冗余的数据，具有空值或嘈杂数据的数据特征会导致不可靠的信息来源。数据集中的不良属性使得在机器学习模型训练阶段的知识发现过程更加繁琐和时间低效。
- en: As a result, the data preprocessing will contribute a considerable amount of
    computational time across the total ML workflow steps. As we stated in the previous
    chapter, unless you know your available data, it would be difficult to understand
    the problem itself. Moreover, knowing the data will help you to formulate your
    problem. In parallel, and more importantly, before trying to apply an ML algorithm
    to a problem, first you have to identify if the problem is really a machine learning
    problem and whether an ML algorithm could directly be applied to solve the problem.
    The next step that you need to take is to know the machine learning classes. More
    technically, you need to know if an identified problem falls under classification,
    clustering, rule retraction, or regression classes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据预处理将在总体ML工作流程步骤中占据相当大的计算时间。正如我们在上一章中所述，除非您了解可用数据，否则很难理解问题本身。此外，了解数据将帮助您制定问题。同时，更重要的是，在尝试将ML算法应用于问题之前，首先您必须确定问题是否真的是一个机器学习问题，以及ML算法是否可以直接应用于解决问题。您需要采取的下一步是了解机器学习类别。更具体地说，您需要知道已识别的问题是否属于分类、聚类、规则重构或回归类别。
- en: For the sake of simplicity, we assume you have a machine learning problem. Now
    you need to do some data pre-processing that includes some steps like data cleaning,
    normalization, transformation, feature extraction, and selection. The product
    of a data pre-processing workflow step is the final training set that is typically
    used to build/train the ML model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们假设您有一个机器学习问题。现在，您需要进行一些数据预处理，包括一些步骤，如数据清理、归一化、转换、特征提取和选择。数据预处理工作流程步骤的产物通常用于构建/训练ML模型的最终训练集。
- en: In the previous chapter, we also argued that a machine learning algorithm learns
    from the data and activities during the model building and feed backing. It is
    critical that you feed your algorithm with the right data for the problem you
    want to solve. Even if you have good data (or well-structured data to be more
    precise), you need to make sure that the data is in an appropriate scale, with
    a well-known format to be parsed by the programming languages and, most importantly,
    if the most meaningful features are also included.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们还论证了机器学习算法是从数据和模型构建和反馈活动中学习的。关键是，您需要为您想要解决的问题为算法提供正确的数据。即使您拥有良好的数据（或者更准确地说是结构良好的数据），您也需要确保数据处于适当的规模，并且具有编程语言可以解析的良好格式，最重要的是，是否还包括最有意义的特征。
- en: In this section, you will learn how to prepare your data so that your machine-learning
    algorithm becomes spontaneous towards best performance. The overall data processing
    is a huge topic; however, we will try to cover essential techniques to make some
    large scale machine learning applications in *[Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines")*, *Building Scalable
    Machine Learning Pipelines*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何准备数据，使您的机器学习算法对最佳性能变得自发。总体数据处理是一个庞大的主题；然而，我们将尝试覆盖一些基本技术，以便在*[第6章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "第6章。构建可扩展的机器学习管道")*，*构建可扩展的机器学习管道*中进行一些大规模的机器学习应用。
- en: Data preparation process
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备过程
- en: 'If you are more focused and disciplined during the data handling and preparation
    steps, you are likely to get more consistent and better results in the first place.
    However, the data preparation is a tedious process consisting of several steps.
    Nevertheless, the process for getting data ready for a machine learning algorithm
    can be summarized in three steps:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在数据处理和准备步骤中更加专注和纪律，您很可能会在第一时间获得更一致和更好的结果。然而，数据准备是一个包含多个步骤的繁琐过程。然而，为了让数据准备好用于机器学习算法，可以总结为三个步骤：
- en: Data selection
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据选择
- en: Data pre-processing
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Data transformation
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转换
- en: Data selection
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据选择
- en: 'This step will focus on selecting the subset of all available datasets that
    you will be using and working with within your machine learning application development
    and deployment. There is always a strong urge to include all the available data
    in machine learning application development since more data will provide more
    features. In other words, by holding the well-known aphorism, *more is better*.
    However, essentially, this might not be true in all cases. You need to consider
    what data you need to have before you actually answer the question. The ultimate
    goal is to provide a solution of a particular hypothesis. You might be doing some
    assumptions about the data as well in the first place. Although it is difficult,
    if you are a domain expert of that problem, you can make some assumption to know
    at least some insights before applying your ML algorithms. However, be careful
    to record those assumptions so that you can test them at a later stage when required.
    We will present some common question to help you out in thinking through the data
    selection process:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步将专注于选择您将在机器学习应用程序开发和部署中使用和处理的所有可用数据集的子集。在机器学习应用程序开发中，总是有一种强烈的冲动，即包含所有可用数据，因为更多的数据将提供更多的特征。换句话说，按照众所周知的格言，*越多越好*。然而，实际上，在所有情况下，这可能并不正确。在实际回答问题之前，您需要考虑需要哪些数据。最终目标是提供特定假设的解决方案。在一开始，您可能对数据做出一些假设。虽然这很困难，但如果您是该问题的领域专家，您可以在应用ML算法之前做出一些假设以至少了解一些见解。但是，要小心记录这些假设，以便在需要时在以后的阶段进行测试。我们将提出一些常见问题，以帮助您思考数据选择过程：
- en: The first question would be, *what is the extent of the data you have available?*
    For example, the extent could be the throughout time, database tables, connected
    system files, and so on. Therefore, the better practice is to ensure that you
    have a clear understanding and low-level structure of everything that you can
    use, or holding informally the available resources (while of course including
    the available data and computational resources).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个问题是，*您可用的数据范围是多少？*例如，范围可能是整个时间、数据库表、连接的系统文件等。因此，最好的做法是确保您清楚地了解并低级结构化您可以使用的一切，或者非正式地持有可用资源（当然包括可用的数据和计算资源）。
- en: The second question is a little bit weird! *What data are not yet available
    but important to solve the problem?* In this case, you might have to wait for
    the data to be available or alternatively you can at least generate or simulate
    these types of data using some generator or software.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个问题有点奇怪！*哪些数据尚未可用，但对解决问题很重要？*在这种情况下，您可能需要等待数据可用，或者您可以至少使用一些生成器或软件生成或模拟这些类型的数据。
- en: The third question might be: *what data don't you need to address the problem?*
    That means again the redundancies so excluding these redundant or unwanted data
    is almost always easier than including it altogether. You might be wondering whether
    or not to note down the data you excluded and why? We think it should be yes since
    you might need some trivial data in the later stages.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个问题可能是：*您不需要哪些数据来解决问题？*这意味着再次排除冗余数据，因此排除这些冗余或不需要的数据几乎总是比全部包含它更容易。您可能会想知道是否需要记录排除的数据以及原因？我们认为应该是的，因为您可能在以后的阶段需要一些琐碎的数据。
- en: Moreover, in practice in this case small problems or games, toy competition
    data will already have been selected for you; therefore, you don't need to be
    worried at all!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在实践中，对于小问题或游戏，玩具竞赛数据已经为您选择好了；因此，您无需担心！
- en: Data pre–processing
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'After you have selected the data you will be working with, you need to consider
    how you could use the data and the proper utilization required. This pre-processing
    step will address some steps or techniques for getting the selected data into
    a form that you can work and apply during your model building and validation steps.
    The three most common data pre-processing steps that are used are formatting,
    cleaning, and sampling the data:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择要处理的数据后，您需要考虑如何使用数据和所需的适当利用。这个预处理步骤将解决一些步骤或技术，以便将所选数据转换为您可以在模型构建和验证步骤中使用和应用的形式。最常用的三个数据预处理步骤是格式化、清理和抽样数据：
- en: '**Formatting**: The selected data may not be in a good shape so might not be
    suitable for you to work with directly. Very often, your data might be in a raw
    data format (a flat file format such as a text format or a less used proprietary
    format) and if you are lucky enough then data might be in a relational database.
    If this is the case, then it would better be to apply some conversion steps (that
    is, converting a relational database to its format for example, since using Spark
    you cannot make any conversion). As already stated, the beauty of Spark is its
    support for diverse file formats. Therefore, we will be able to take advantage
    in the following sections.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**格式化**：所选数据可能不够完善，因此可能不适合直接使用。您的数据很可能是原始数据格式（如文本格式或较少使用的专有格式的平面文件格式），如果您足够幸运，数据可能是在关系数据库中。如果是这种情况，最好应用一些转换步骤（即，例如将关系数据库转换为其格式，因为使用Spark您无法进行任何转换）。正如已经说明的，Spark的美妙之处在于其对多种文件格式的支持。因此，我们将能够在接下来的部分中利用这一点。'
- en: '**Cleaning**: Very often the data you will be using comes with many unwanted
    records or sometimes with missing entries against a record. This cleaning process
    deals with the removal or fixing of missing data. There may be always some trivial
    data objects that are insignificant or incomplete and addressing them should be
    the first priority. Consequently, these instances may need to be removed, ignored
    or deleted from the datasets to get rid of this problem. Additionally, if the
    privacy or security is a concern because of the presence of the sensitive information
    against some attributes, those attributes need to be anonymized or removed from
    the data entirely (if appropriate).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清洗**：您将要使用的数据往往带有许多不需要的记录，有时甚至有缺失的记录。这个清洗过程涉及到删除或修复缺失的数据。可能总会有一些微不足道或不完整的数据对象，处理它们应该是首要任务。因此，这些实例可能需要从数据集中删除、忽略或删除以摆脱这个问题。此外，如果由于某些属性中存在敏感信息的存在而导致隐私或安全成为问题，那么这些属性需要被匿名化或从数据中完全删除（如果适用）。'
- en: '**Sampling**: The third step would be the sampling over the top of the formatted
    and cleaned datasets. Sampling is often required since there might be a time when the
    available data size is large or a number of records are huge. However, we argue
    to use the data as much as possible. Another reason is that more data can result
    in a longer execution time during the whole machine learning process. If this
    is the case, this also increases the running times of the algorithms and requires
    a more powerful computational infrastructure. Therefore, you can take a smaller
    representative sample of the selected data that may be much faster for exploring
    and prototyping the machine learning solution before considering the whole dataset.
    It is obvious that whatever the machine learning tools you apply for your machine
    learning application development and commercialization, data will influence the
    pre-processing you will be required to perform.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽样**：第三步将是在格式化和清理的数据集上进行抽样。由于可用数据量可能很大或记录数量很多，因此通常需要抽样。然而，我们建议尽可能使用数据。另一个原因是更多的数据可能导致整个机器学习过程的执行时间更长。如果是这种情况，这也会增加算法的运行时间，并需要更强大的计算基础设施。因此，您可以对所选数据进行较小的代表性样本，这样在考虑整个数据集之前，探索和原型化机器学习解决方案可能会更快。显然，无论您为机器学习应用开发和商业化应用的机器学习工具，数据都将影响您需要执行的预处理。'
- en: Data transformation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据转换
- en: 'After selecting appropriate data sources and pre-processing those data, the
    final step is to transform the processed data. Your specific ML algorithm and
    knowledge of the problem domain will be influenced in this step. Three common
    data transformations techniques are scaling attributes, decompositions and attribute
    aggregations. This step is also commonly referred to as feature engineering that
    will be discussed in more details in the next chapter:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择适当的数据源并对这些数据进行预处理后，最后一步是转换已处理的数据。您特定的机器学习算法和对问题领域的了解将在这一步中受到影响。三种常见的数据转换技术是属性缩放、分解和属性聚合。这一步通常也被称为特征工程，在下一章中将更详细地讨论：
- en: '**Scaling**: The pre-processed data may contain attributes with a mixture of
    scales for various quantities and units, for example dollars, kilograms, and sales
    volume. However, the machine-learning methods have the data attributes within
    the same scale such as between 0 and 1 for the smallest and largest value for
    a given feature. Therefore, consider any feature scaling you may need to perform
    the proper scaling of the processed data.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩放**：预处理的数据可能包含具有各种数量和单位的混合比例的属性，例如美元、千克和销售量。然而，机器学习方法要求数据属性在相同的比例内，例如对于给定特征的最小值和最大值之间的0到1之间。因此，考虑您可能需要执行适当的特征缩放来对已处理的数据进行适当的缩放。'
- en: '**Decomposition**: The data might have some features that represent a complex
    concept that provides a more powerful response from the machine learning algorithms
    when you split the datasets into the fundamental parts. For example, consider
    a day that is composed of 24 hours, 1,440 minutes, and 86,400 seconds that in
    turn could be split out further. Probably some specific hours or only the hours
    in a day are relevant to the problem which to be investigated and resolved. Therefore,
    consider an appropriate feature extraction and selection to perform the proper
    decomposition of the processed data.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分解**：数据可能具有一些代表复杂概念的特征，当您将数据集分解为基本部分时，可以使机器学习算法产生更强大的响应。例如，考虑一天由24小时、1,440分钟和86,400秒组成，这些时间又可以进一步分解。可能一些特定的小时或一天中的小时对于需要调查和解决的问题是相关的。因此，考虑适当的特征提取和选择来执行已处理数据的适当分解。'
- en: '**Aggregation**: Often segregated or scattered features may be trivial on their.
    However, those features can be aggregated into a single feature that would be
    more meaningful to the problem you are trying to solve. For example, several data
    instances can be presented in an online shopping website for each time a customer
    logged on the site. These data objects could be aggregated into a count for the
    number of logins by discarding additional instances. Therefore, consider appropriate
    feature aggregation to process the data properly.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合**：通常，分散或零散的特征可能在其自身上是微不足道的。然而，这些特征可以被聚合成一个更有意义的特征，对您试图解决的问题更有意义。例如，一些数据实例可以在在线购物网站上呈现，每次客户登录网站时都会产生数据对象。这些数据对象可以通过丢弃额外的实例来聚合成登录次数的计数。因此，考虑适当的特征聚合来正确处理数据。'
- en: Apache Spark has its distributed data structures includes RDD, DataFrame, and
    Datasets by which you can perform the data pre-processing efficiently. These data
    structures have different advantages and performance for processing the data.
    In the next sections, we will describe those data structures individually and
    also show examples of how to process the large Dataset using them.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark具有其分布式数据结构，包括RDD、DataFrame和Datasets，您可以使用这些数据结构高效地进行数据预处理。这些数据结构在处理数据时具有不同的优势和性能。在接下来的章节中，我们将分别描述这些数据结构，并展示如何使用它们处理大型数据集的示例。
- en: Resilient Distributed Dataset basics
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性分布式数据集基础知识
- en: In [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark"), *Introduction to Data
    Analytics with Spark*, we have described the Resilient Distributed Datasets in
    brief including the data transformation and action as well as the caching mechanism.
    We also stated that RDDs are basically an immutable collection of records that
    can only be created by operations such as map, filter, group by, and so on. In
    this chapter, we are going to use this native data structure of Spark for data
    manipulation and data pre-processing for a practical machine learning application
    commonly referred to as spam-filtering. Spark provides another two higher label
    APIs such as DataFrame and Datasets for data manipulation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d "第1章。使用Spark进行数据分析简介")中，*使用Spark进行数据分析简介*，我们简要描述了弹性分布式数据集，包括数据转换和操作以及缓存机制。我们还指出RDD基本上是一个不可变的记录集合，只能通过map、filter、group
    by等操作来创建。在本章中，我们将使用Spark的这种本机数据结构进行数据操作和数据预处理，用于一个常被称为垃圾邮件过滤的实际机器学习应用。Spark还提供了另外两个更高级别的API，如DataFrame和Datasets，用于数据操作。
- en: We will, however, show all the APIs including RDD here because you might need
    this API to handle more complex data manipulation. We have referred to some commonly
    used definitions regarding Spark actions and operations from the Spark programming
    guide.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们将展示包括RDD在内的所有API，因为您可能需要这个API来处理更复杂的数据操作。我们已经从Spark编程指南中引用了一些关于Spark操作和操作的常用定义。
- en: As we already discussed some basics of RDD operations using action and transformations.
    The RDDs can be created by both stable storages such as the **Hadoop Distributed
    File System** (**HDFS**) and by transformations on existing RDDs. Spark periodically
    logs those transformations while creating RDDs over a set of transformations,
    rather than actual data, therefore, technically the original RDD and the Datasets
    do not get changed.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经讨论过使用操作和转换的RDD操作的一些基础知识。RDD可以通过稳定的存储（如Hadoop分布式文件系统（HDFS））和对现有RDD的转换来创建。Spark定期记录这些转换，而不是实际数据，因此从技术上讲，原始RDD和数据集不会发生变化。
- en: A transformed Dataset can be created from an existing one; however, the reverse
    is not possible in Spark. After finishing a computation on the Dataset, an action
    returns a value to the driver program. For example, according to the Spark programming
    guidelines, the map is a transformation that passes each Dataset element using
    a function and returns a brand new RDD that represents and holds the results.
    In contrast, reduce is also an action that aggregates all the elements of an RDD
    by using a function and returns a brand new RDD too as the final result to the
    driver program.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从现有数据集创建一个转换后的数据集；但是在Spark中不可能反过来。在数据集上完成计算后，操作将返回一个值给驱动程序。例如，根据Spark编程指南，map是一种通过函数传递每个数据集元素并返回一个全新的RDD来表示和保存结果的转换。相反，reduce也是一种通过函数聚合RDD的所有元素并返回一个全新的RDD作为最终结果返回给驱动程序的操作。
- en: More technically, suppose we have a text file that contains a sequence of number
    separated by commas (that is, a CSV file). Now after reading the same you will
    have an RDD and consequently, you might want to count the frequencies of each
    number. For doing this, you need to convert the RDD into key value pairs, where
    the key is the number and the value will be the frequency of each number.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，假设我们有一个包含以逗号分隔的数字序列的文本文件（即CSV文件）。现在在读取完毕后，您将拥有一个RDD，因此您可能希望计算每个数字的频率。为此，您需要将RDD转换为键值对，其中键是数字，值将是每个数字的频率。
- en: On the other hand, you might need to collect the result in the driver program
    by doing some operations. In the next few sections, we will provide more details
    on some useful topics such as transformations and actions by showing some examples
    based on a practical machine learning problem.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，您可能需要通过一些操作在驱动程序中收集结果。在接下来的几节中，我们将通过基于实际机器学习问题的一些示例，提供有关转换和操作等一些有用主题的更多细节。
- en: Reading the Datasets
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取数据集
- en: For reading Datasets from different data sources like a local filesystem, HDFS,
    Cassandra, HBase, and more, Spark provides different APIs that are easy to use.
    It supports the different representation of data including text file, sequence
    file, Hadoop input format, CSV, TSV, TXT, MD, JSON, and other data formats. The
    input API or methods support running on compressed files, directories and wildcards.
    For example, *Table 1* shows the list of reading formats. The `textFile()` method
    reads different file formats such as `.txt` and `.gz` from the directory `/my/directory:`
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从不同的数据源（如本地文件系统、HDFS、Cassandra、HBase等）读取数据集，Spark提供了易于使用的不同API。它支持包括文本文件、序列文件、Hadoop输入格式、CSV、TSV、TXT、MD、JSON和其他数据格式在内的不同数据表示。输入API或方法支持在压缩文件、目录和通配符上运行。例如，*表1*显示了读取格式的列表。`textFile()`方法从目录`/my/directory`中读取不同的文件格式，如`.txt`和`.gz`。
- en: '| textFile("/my/directory"),textFile("/my/directory/*.txt"),textFile("/my/directory/*.gz").
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| textFile("/my/directory"),textFile("/my/directory/*.txt"),textFile("/my/directory/*.gz").
    |'
- en: 'Table 1: Reading files formats'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：读取文件格式
- en: Reading from files
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从文件中读取
- en: You might need to read a Dataset from local or HDFS. The following code show
    the different methods for creating RDDs from a given Dataset stored on your local
    machine or in HDFS.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要从本地或HDFS读取数据集。以下代码展示了从存储在本地机器或HDFS上的给定数据集创建RDD的不同方法。
- en: 'However, before reading and writing with Spark, we need to create the Spark
    entry point by means of a Spark session that can be instantiated as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在使用Spark进行读取和写入之前，我们需要通过Spark会话创建Spark入口点，可以通过以下方式实例化：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, the Spark SQL warehouse is set to as `E:/Exp/` path. You should set your
    path accordingly based on OS types you are on. Well, now we have our Spark session
    as variable `spark`, let's see how to use it with ease for reading from a text
    file.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Spark SQL仓库设置为`E:/Exp/`路径。您应该根据您所在的操作系统类型设置您的路径。好了，现在我们有了变量`spark`作为我们的Spark会话，让我们看看如何轻松地使用它来从文本文件中读取。
- en: Reading from a text file
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从文本文件中读取
- en: 'It uses `textFile()` methods of `SparkContext()` and returns an RDD of a string
    containing a collection of lines. In [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark"), *Introduction to Data
    Analytics with Spark*, we explained what SparkContext is. Nevertheless, Spark
    Context is the entry point of a Spark application. Suppose we have a Dataset called
    `1.txt` containing some tweets data as unstructured texts. You can download the
    data from the Packt materials and store under the `project_path/input/test/` directory,
    defined as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用`SparkContext()`的`textFile()`方法，并返回一个包含行集合的字符串的RDD。在[第1章](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "第1章。使用Spark进行数据分析的介绍")中，我们解释了什么是SparkContext。尽管如此，Spark Context是Spark应用程序的入口点。假设我们有一个名为`1.txt`的数据集，其中包含一些推文数据作为非结构化文本。您可以从Packt材料中下载数据，并将其存储在`project_path/input/test/`目录下，定义如下：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here we have created RDDs of a string that is stored with the variable `distFile`
    in two partitions. However, to work with Java, the RDDs have to be converted into
    JavaRDD. Let''s do it by calling the `toJavaRDD()` method as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经创建了一个存储在变量`distFile`中的字符串的两个分区的RDD。但是，要使用Java，RDD必须转换为JavaRDD。通过调用`toJavaRDD()`方法来实现：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Reading multiple text files from a directory
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从目录中读取多个文本文件
- en: 'It will return RDD as (filename and content) pairs. Suppose we have multiple
    files stored to read in the directory `csvFiles/` is defined as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 它将返回RDD作为（文件名和内容）对。假设我们有多个文件存储在`csvFiles/`目录中以供读取，定义如下：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Please note, when the data objects in an RDD do not hold in the main memory
    or HDD, we need to perform a partition on the RDD to increase the parallelism.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当RDD中的数据对象不存储在主内存或HDD中时，我们需要对RDD执行分区以增加并行性。
- en: Reading from existing collections
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从现有集合中读取
- en: 'The second source for creating an RDD is from the collections of your driver
    program such as list a containing integers. Before going deeper into this, let''s
    initialize Spark in an alternative way as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 创建RDD的第二个来源是从驱动程序程序的集合中，例如包含整数的列表。在深入讨论之前，让我们以另一种方式初始化Spark，如下所示：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here Java Spark context is available as a variable `sc`. This time, we have
    created the Spark context so we will be able to create the Java RDDs of string
    without using the `toJavaRDD()` method.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里Java Spark上下文可用作变量`sc`。这一次，我们已经创建了Spark上下文，因此我们将能够创建字符串的Java RDD，而无需使用`toJavaRDD()`方法。
- en: 'Now, you can do it by using the parallelized method of Spark Context as shown
    here:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用Spark Context的并行化方法来实现：
- en: '**Reading list of integers**: It returns a parallelized RDD of integers:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**读取整数列表**：它返回一个并行化的整数RDD：'
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Reading list of pairs**: It returns a parallelized `pairRDD` of the list
    of pairs (integer, string):'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**读取成对列表**：它返回一个并行化的`pairRDD`，其中包含成对的整数和字符串：'
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Pre–processing with RDD
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD的预处理
- en: To continue the discussion about the data pre-processing that we started in
    the previous section, we will show an example of a machine learning problem and
    how to pre-process the Dataset using RDD in this section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续讨论我们在上一节开始的数据预处理，我们将在本节中展示一个机器学习问题的示例，以及如何使用RDD对数据集进行预处理。
- en: 'We are considering the **Spam filter** application that is a popular example
    of s supervised learning problem. The problem is to predict and identify the spam
    messages from the incoming e-mails (please refer to *Table 2*). As usual, to train
    the model, you have to train a model by using the historical data (the historical
    e-mail that you have received over a couple of days, hours or months an even year).
    The final output of pre-processing tasks is to make the feature vectors or extract
    the features including its labels or classes. Typically, you might be doing the
    following steps:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在考虑的是**垃圾邮件过滤器**应用程序，这是一个受欢迎的监督学习问题的典型示例。问题是从传入的电子邮件中预测和识别垃圾邮件消息（请参阅*表2*）。通常，为了训练模型，您必须使用历史数据（您在几天、几小时或几个月甚至一年内收到的历史电子邮件）来训练模型。预处理任务的最终输出是制作特征向量或提取包括其标签或类别的特征。通常，您可能会执行以下步骤：
- en: '**Stop word removal**: The text file might contain some word that is useless
    or redundant for the feature vector such as *and*, *the*, and *of*, since these
    are very common in all forms of English sentences. Another reason is they are
    not very meaningful in deciding spam or ham status or they may contain trivial
    significance. These words, therefore, need to be filtered from the e-mail Dataset
    before moving the next step.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停用词去除**：文本文件可能包含一些对于特征向量无用或多余的单词，例如*and*、*the*和*of*，因为这些单词在所有形式的英语句子中都非常常见。另一个原因是它们在决定垃圾邮件或正常邮件状态时没有太多意义，或者它们可能包含微不足道的意义。因此，在进行下一步之前，需要从电子邮件数据集中过滤掉这些单词。'
- en: '**Lemmatization**: Some words possessing the same meaning but with different
    endings, need to be readjusted in order to make them consistent across the data
    set and if they all carry the same form will be easier to make them transform
    into feature vectors. For example, *attached*, *attachment*, and *attach* could
    all be represented and later on interpreted as e-mail *attachments*. The `SMSSpamCollection`
    Dataset was downloaded from the UCI ML repositories at [https://archive.ics.uci.edu/ml/machine-learning-databases/00228/](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词形还原**：一些具有相同含义但不同结尾的单词需要被调整，以使它们在整个数据集中保持一致，如果它们都具有相同的形式，将更容易将它们转换为特征向量。例如，*attached*、*attachment*和*attach*都可以表示为电子邮件*attachments*。`SMSSpamCollection`数据集可以从UCI
    ML存储库[https://archive.ics.uci.edu/ml/machine-learning-databases/00228/](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/)下载。'
- en: 'Please note, all the words in the email body are usually converted to lowercase
    for simplicity in this phase. Now, let''s take a look at the following table:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个阶段，电子邮件正文中的所有单词通常都会被转换为小写。现在，让我们来看一下下表：
- en: '| ham: What you doing? how are you?ham: Ok lar... Joking wif u oni.ham: dun
    say so early hor... U c already then say.ham: MY NO. IN LUTON 0125698789 RING
    ME IF UR AROUND! H*spam: FreeMsg: Txt: CALL to No: 86888 & claim your reward of
    3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStopham:
    Siva is in hostel aha.ham: Cos i was out shopping wif darren jus now n i called
    him 2 ask wat present he wan lor. Then he started guessing who i was wif n he
    finally guessed darren lor.spam: Sunshine Quiz! Win a super Sony DVD recorder
    if you can name the capital of Australia? Text MQUIZ to 82277\. B |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| ham: 你在干什么？你好吗？ham: 好的啦...只是和你开玩笑。ham: 不要这么早说...你已经看到了然后说。ham: 我的号码在卢顿0125698789，如果你在附近给我打电话！H*spam:
    免费信息：短信：拨打86888号码，领取您的3小时通话时间奖励，现在可以在您的手机上使用！订阅6英镑/月，包括3小时16次停止？txtStop ham: 西瓦在宿舍哈哈。ham:
    因为我刚才和达伦出去购物，我给他打电话问他想要什么礼物。然后他开始猜我和谁在一起，最后他猜到了是达伦。spam: 阳光测验！如果你能说出澳大利亚的首都，就赢得一个超级索尼DVD录像机！发送MQUIZ到82277\.
    B |'
- en: 'Table 2: Test file for training set containing ham and spam messages'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：包含正常邮件和垃圾邮件消息的训练集的测试文件
- en: '**Removal of non-words**: Numbers and punctuation have to be removed too. However,
    we will not show here all the possible transformation for pre-processing data
    due to page limitation and brevity, but we will try to show some basic transformations
    and actions for pre-processing segment of the Dataset that contains some labels
    data as spam or ham presented in *Table 2*. Where the Dataset or e-mails are labelled
    as ham or spam followed by the message or e-mails. Ham means non-spam and spams
    are identified as junk emails messages.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去除非单词**：数字和标点也必须被去除。然而，由于页面限制和简洁性，我们不会在这里展示所有可能的预处理数据的转换，但我们将尝试展示一些基本的预处理数据的转换和操作，其中包含一些标签数据，如*表2*中所示。数据集或电子邮件被标记为正常邮件或垃圾邮件，后面跟着消息或电子邮件。正常邮件意味着非垃圾邮件，垃圾邮件被识别为垃圾邮件消息。'
- en: 'The overall pre-processing using RDD can be described using the following steps. The
    first step we need is to prepare the feature vectors using RDD operations and
    transformations. The remaining steps are given as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RDD进行的整体预处理可以用以下步骤描述。我们需要的第一步是使用RDD操作和转换准备特征向量。其余步骤如下：
- en: '**Reading Dataset**: The following code is for reading Dataset that creates
    a `linesRDD` of strings from the `SMSSpamCollection` Dataset. Please download
    this Dataset from the Packt materials and store it in your disk or HDFS in  `Project+path/input/`
    directory. A detailed description of this Dataset will be provided later on:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**读取数据集**：以下代码用于读取数据集，从`SMSSpamCollection`数据集创建一个字符串的`linesRDD`。请从Packt材料中下载这个数据集，并将其存储在您的磁盘或HDFS中的`Project+path/input/`目录中。稍后将提供有关此数据集的详细描述：'
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: However, the `linesRDD` contains both the spam as well as the ham messages.
    Therefore, we need to separate the spam and ham message from the files.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`linesRDD`包含了垃圾邮件和正常邮件消息。因此，我们需要从文件中分离垃圾邮件和正常邮件。
- en: '**Filter out the spam messages**: To filter the data from existing RDDs, Spark
    provides a method called `filter()`, which returns a new Dataset containing only
    the selected elements. In the following code you can see we have passed `new Function()`
    as a parameter that takes two arguments of the type String and Boolean of the`filter()`
    method. Basically, Spark APIs heavily rely on passing functions to the driver
    program for running on the cluster. There are two ways to create a function that
    includes:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤垃圾邮件**：为了从现有的RDD中过滤数据，Spark提供了一个名为`filter()`的方法，它返回一个只包含所选元素的新数据集。在下面的代码中，您可以看到我们已经将`new
    Function()`作为参数传递给了`filter()`方法，它接受两个类型为String和Boolean的参数。基本上，Spark API在集群上运行时大量依赖于将函数传递给驱动程序。有两种方法可以创建包含函数的函数，包括：'
- en: Implementing the function interfaces either creating anonymous inner class or
    named one and passing an instance of it to Spark
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现函数接口，无论是创建匿名内部类还是命名内部类，并将其实例传递给Spark
- en: Using lambda expressions (you will have to have Java 8 installed to take advantage
    of lambda expressions though)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用lambda表达式（尽管您需要安装Java 8才能利用lambda表达式）
- en: 'The following code segments that we have used explain the concept of anonymous
    class as a parameter that contains a  `call()` method that returns `true` if the
    line contains the word `spam`:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '以下代码片段说明了我们使用的匿名类概念作为包含`call()`方法的参数，如果行包含单词`spam`，则返回`true`： '
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Filter out the ham messages: similarly, we can filter out the ham messages
    as shown here:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤掉正常邮件：同样，我们可以过滤掉正常邮件，如下所示：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Split the words from the lines**: To extract the features and labels from
    each line, we need to split those using space or tab characters. After that, we
    can have the lines without the spam or ham words.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从行中分割单词**：为了从每行中提取特征和标签，我们需要使用空格或制表符来分割它们。之后，我们可以得到不包含垃圾邮件或正常邮件单词的行。'
- en: 'The following code segments show the separation of spam and ham features from
    the lines. We have used the `map` transformation that returns a new RDD formed
    by passing each line of the existing RDD through a function `call`. Here the `call`
    method always returns a single item. You will find a difference with `flatMap`
    in the later section:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码段显示了从行中分离垃圾邮件和正常邮件特征。我们使用了`map`转换，它返回一个新的RDD，通过将现有RDD的每一行传递给`call`函数来形成。这里`call`方法总是返回一个单个项目。您将在后面的部分中发现与`flatMap`的区别：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Output: `ham.collect()`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：`ham.collect()`：
- en: '![Pre–processing with RDD](img/00052.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![RDD的预处理](img/00052.jpeg)'
- en: 'Figure 1: A snapshot of the spam RDD'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：垃圾邮件RDD的快照
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Output: `ham.collect()`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：`ham.collect()`：
- en: '![Pre–processing with RDD](img/00066.jpeg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![RDD的预处理](img/00066.jpeg)'
- en: 'Figure 2: A snapshot of the ham RDD'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：正常邮件RDD的快照
- en: '**Split the words from the lines of spam RDD**: After we get the feature lines
    against the spam and ham RDDs separately, we have to split the words for makinga
     feature vector in the future. The following codes do this split with a space
    by returning the wordlist RDD. The call method returns a list of words for each
    line:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从垃圾邮件RDD的行中拆分单词**：在我们分别得到垃圾邮件和正常邮件RDD的特征行之后，我们需要拆分单词以便在将来创建特征向量。以下代码通过使用空格进行拆分并返回单词列表RDD。调用方法返回每行的单词列表：'
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Make label and feature pair RDD**: Now we have two RDDs for spam and ham.
    We want to label them by 1.0 or 0.0 for spam and ham words or features respectively.
    For ease of use, we can again create a new RDD-containing tuple of a label and
    features or wordlist for each line. In the following code we have used `Tuple2`
    for making a pair. You can also use `JavaPairRDD` for making a pair of labels
    and features:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建标签和特征对RDD**：现在我们有了垃圾邮件和正常邮件的两个RDD。我们想要用1.0或0.0来标记它们，分别表示垃圾邮件和正常邮件的单词或特征。为了方便使用，我们可以再次创建一个新的RDD，每行包含一个标签和特征或单词列表的元组。在下面的代码中，我们使用了`Tuple2`来创建一对。您也可以使用`JavaPairRDD`来创建标签和特征的一对：'
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Union of the two RDDs**: Now we have two labels for the Dataset in *Table
    2*, the feature pair RDD of spam and ham. Now to make the training Dataset, we
    can join these two RDDs into one. Spark has `union()`method for doing this that
    returns a new RDD, containing the union of the Dataset and the argument or another
    Dataset:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**两个RDD的并集**：现在我们在*表2*中有数据集的两个标签，即垃圾邮件和正常邮件的特征对RDD。现在要创建训练数据集，我们可以将这两个RDD合并成一个。Spark有一个`union()`方法可以做到这一点，它返回一个新的RDD，包含数据集和参数或另一个数据集的并集：'
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Counting all the lines from the preceding operations, are called a transformation.
    This returns a new Dataset from existing one in the worker nodes in each case.
    If you want to bring the return in the driver program or print the results that
    would be called an action operation. Spark supports several built-in methods as
    actions. The `count()` method counts the number of elements in the Dataset:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于前面的所有操作，称为转换。这在每种情况下都会从现有的工作节点中返回一个新的数据集。如果您想要将返回结果带回驱动程序或打印结果，那将被称为动作操作。Spark支持几种内置方法作为动作。`count()`方法用于计算数据集中的元素数量：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Printing the RDD**: The `collect()` and `take()` are also action method that
    are used to print or collect the Dataset as an array in the driver program, where,
    `take()` takes an argument of, say *n* that returns the first n elements of that
    Dataset. The following code segments print the first 10 elements or tuples out
    of the train set:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**打印RDD**：`collect()`和`take()`也是用于打印或收集数据集作为驱动程序中的数组的动作方法，其中，`take()`接受一个参数，比如*n*，返回数据集的前n个元素。以下代码段打印出训练集中的前10个元素或元组：'
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Save the result in the local filesystem**: Sometimes you might need to save
    the RDD in the filesystem as text. You can use the following code for saving your
    RDDs straight away:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将结果保存在本地文件系统中**：有时您可能需要将RDD保存在文件系统中。您可以使用以下代码直接保存您的RDD：'
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Getting insight from the SMSSpamCollection dataset
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从SMSSpamCollection数据集中获取见解
- en: 'The following source code shows the basic ham and spam statistics:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下源代码显示了基本的正常邮件和垃圾邮件统计信息：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding code generates the following spam and ham counts:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下垃圾邮件和正常邮件计数：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This means that out of 5,578 emails `747` e-mails are spam and `4,831` e-mails
    are labelled as ham or non-spam. In other words, the spam and ham ratio is 13.40%
    and 86.6%.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在5578封电子邮件中，有747封是垃圾邮件，4831封被标记为正常邮件或非垃圾邮件。换句话说，垃圾邮件和正常邮件的比例分别为13.40%和86.6%。
- en: Working with the key/value pair
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理键值对
- en: This subsection describes the key/value pair that is frequently needed in the
    data analytics, especially in the text processing.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节描述了在数据分析中经常需要的键值对，特别是在文本处理中。
- en: mapToPair()
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: mapToPair()
- en: 'This method will return a Dataset of (K, V) pair where K is key and V is value.
    For example, if you have an RDD with a list of integer then you want to count
    the number of duplicate entries in the list then the first task is to map each
    number to 1\. After that you can do the reduce operation on it. The code produces
    the output and cache as shown here:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法将返回一个(K，V)对的数据集，其中K是键，V是值。例如，如果您有一个包含整数列表的RDD，然后想要计算列表中重复条目的数量，那么第一步是将每个数字映射为1。之后您可以对其进行reduce操作。该代码产生了如下所示的输出和缓存：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: More about transformation
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多关于转换的内容
- en: In this section, you can see more about transformation including some differences
    between similar types of methods. Mainly `map` and `flatMap`, `groupByKey`, `reduceByKey`
    and `aggregateByKey`, `sortByKey` and `sortBy` will be discussed in this section.
    However, interested readers can refer to the Spark programming guidelines for
    RDD operation in [2].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您可以了解更多关于转换的内容，包括一些类似方法之间的差异。主要将讨论`map`和`flatMap`，`groupByKey`，`reduceByKey`和`aggregateByKey`，`sortByKey`和`sortBy`。然而，有兴趣的读者可以参考[2]中的Spark编程指南了解RDD操作。
- en: map and flatMap
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: map和flatMap
- en: 'The `flatMap` is similar to the map we have showed in the preceding examples,
    but each input item or each time calling the `call()` method of the anonymous
    class can be mapped to zero or more output items. So ,the `call()` function returns
    a Sequence rather than a single item like a map. For example, for input of following
    RDD, the output should be as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatMap`类似于我们在前面的示例中展示的`map`，但是每个输入项或每次调用匿名类的`call()`方法都可以映射到零个或多个输出项。因此，`call()`函数返回的是一个序列，而不是像`map`那样返回单个项。例如，对于以下RDD的输入，输出应该如下所示：'
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: For the previous example, you could not do the map operation because the `call()`
    method of map return only one object rathers than a sequence of the objects.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前面的示例，您无法执行映射操作，因为`map`的`call()`方法只返回一个对象，而不是一系列对象。
- en: groupByKey, reduceByKey, and aggregateByKey
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: groupByKey，reduceByKey和aggregateByKey
- en: 'In order to perform some operation while pre-processing your Dataset you might
    need to do some aggregation such as sum and average based on key values. Spark
    provides some methods for doing these kinds of operations. Let''s say, you have
    the following pairs of RDD and you want to group the values based on the keys
    and do some aggregations:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在预处理数据集时执行一些操作，您可能需要根据键值进行一些汇总，例如求和和平均值。Spark提供了一些方法来执行这些类型的操作。假设您有以下RDD对，并且您想根据键对值进行分组并进行一些汇总：
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The aggregations you want to do can be done by the three methods of Spark including
    `groupByKey`, `reduceByKey`, and `aggregateByKey`. But they differ in term of
    performance, efficiency and flexibility to do an operation such as counting, computing
    summary statistics, finding unique elements from a data set and so on. The `groupByKey`
    method returns a Dataset of (k, `Iterable<v>`) pairs where k is the key and `Iterable<v>`
    is the sequence of values of the key k. The output of previous Dataset using this
    method is given as follows and it shows the collection values of each key:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要执行的汇总可以通过Spark的三种方法来完成，包括`groupByKey`，`reduceByKey`和`aggregateByKey`。但它们在性能、效率和灵活性方面有所不同，可以执行诸如计数、计算摘要统计信息、从数据集中找到唯一元素等操作。`groupByKey`方法返回一个(k，`Iterable<v>`)对的数据集，其中k是键，`Iterable<v>`是键k的值序列。使用此方法的先前数据集的输出如下所示，显示了每个键的集合值：
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![groupByKey, reduceByKey, and aggregateByKey](img/00080.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![groupByKey，reduceByKey和aggregateByKey](img/00080.jpeg)'
- en: 'Figure 3: Pairs using groupBykey'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用groupBykey的对
- en: In order to make the sum of values of each unique key, `groupByKey` is inefficient
    in terms of performance because it does not perform the map side in combination.
    You have to make more transformation to do this summation explicitly. So, it increases
    the network I/O and shuffle size. Better performance can be gained by `reduceByKey`
    or `aggregateByKey` because they perform the map side combination.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对每个唯一键的值进行求和，`groupByKey`在性能上是低效的，因为它不执行组合的映射。您必须进行更多的转换来明确进行这种求和。因此，它会增加网络I/O和洗牌大小。通过`reduceByKey`或`aggregateByKey`可以获得更好的性能，因为它们执行映射端的组合。
- en: The methods return the Dataset with the result of each key aggregation such
    as summations of the values of each key. The following code show the operation
    of those methods which return the Dataset of (k,v) pairs where values (v) of the
    keys are aggregated by the given functions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法返回具有每个键聚合结果的数据集，例如每个键的值的总和。以下代码显示了这些方法的操作，它们返回由给定函数聚合键的值的(k,v)对的数据集。
- en: 'The `reduceByKey` takes one function that reduces the values of each key while
    the `aggregateByKey` takes two functions where the first function is for specifying
    how the aggregation will take place inside each partition and the second function
    is for specifying how the aggregation will take place between the partitions:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey`需要一个函数，用于减少每个键的值，而`aggregateByKey`需要两个函数，其中第一个函数用于指定如何在每个分区内进行聚合，第二个函数用于指定如何在分区之间进行聚合：'
- en: 'Code: `reduceByKey()`:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码：`reduceByKey()`：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Code: `aggregateByKey()`:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码：`aggregateByKey()`：
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For both cases, the output will be as shown here:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种情况，输出将如下所示：
- en: 'Output: `counts.collect()`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`counts.collect()`的输出：'
- en: '![groupByKey, reduceByKey, and aggregateByKey](img/00088.jpeg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![groupByKey，reduceByKey和aggregateByKey](img/00088.jpeg)'
- en: 'Figure 4: RDD using count'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用计数的RDD
- en: sortByKey and sortBy
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: sortByKey和sortBy
- en: 'Sorting is a common operation in data pre-processing. Spark provides two methods
    that transform one Dataset to another sorted paired Dataset that includes `sortByKey`
    and `sortBy`. For instance, we have a Dataset as shown here:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 排序是数据预处理中的常见操作。Spark提供了两种方法，可以将一个数据集转换为另一个排序的配对数据集，包括`sortByKey`和`sortBy`。例如，我们有一个如下所示的数据集：
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `sortByKey()` method performs on (k,v) pairs and returns (k,v) pairs sorted
    by keys in ascending or descending order. You can also customize the sorting by
    providing a comparator as parameters. The following code shows the sorting by
    key of the preceding Dataset:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`sortByKey()`方法在(k,v)对上执行，并按键的升序或降序返回(k,v)对。您还可以通过提供比较器来自定义排序。以下代码显示了对前面数据集的键进行排序：'
- en: 'Code: `sortByKey()`:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码：`sortByKey()`：
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![sortByKey and sortBy](img/00019.jpeg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![sortByKey和sortBy](img/00019.jpeg)'
- en: 'Figure 5: Pairs using sortByKey'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：使用sortByKey的对
- en: 'The `sortBy()` method takes a function as a parameter where you can specify
    the sorting method either by key or by value. The following code shows the sorting
    by values of the preceding Dataset:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`sortBy()`方法接受一个函数作为参数，您可以在其中指定排序方法，无论是按键还是按值。以下代码显示了对前面数据集的值进行排序：'
- en: 'Code: `sortBy()`:'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码：`sortBy()`：
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Output: `sortedRDD.collect()`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`sortedRDD.collect()`的输出：'
- en: '![sortByKey and sortBy](img/00023.jpeg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![sortByKey和sortBy](img/00023.jpeg)'
- en: 'Figure 6: Pairs using sortBy'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：使用sortBy的对
- en: Dataset basics
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集基础知识
- en: As discussed in [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark"), *Introduction to Data
    Analytics with Spark* that in Spark 2.0.0 release, the DataFrame remains the primary
    computation abstraction for the Scala, Python and R, however, while using Java
    the same will be replaced with Dataset. Consequently, Dataset of type Row will
    be used throughout this book.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[第1章](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d "第1章。使用Spark进行数据分析简介")中讨论的，*使用Spark进行数据分析简介*，在Spark
    2.0.0发布中，DataFrame仍然是Scala、Python和R的主要计算抽象，但在使用Java时，将使用Dataset替换。因此，本书中将始终使用类型为Row的Dataset。
- en: The Dataset is a distributed collection of data is structured the Rows. This
    is this is one of the more convenient ways for interacting with Spark SQL module.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Dataset是一个分布式的数据集合，结构化为行。这是与Spark SQL模块交互的更方便的方式之一。
- en: In other words, it can be considered as an equivalent entity to a tabular data
    like a **Relational Database** (**RDB**) format.. The Like the other data abstractions
    like DataFrame and RDD, the Dataset can also be created from various data sources
    like structured data files (TSV, CSV, JSON, and TXT), Hive tables, secondary storages,
    external databases, or existing RDDs and DataFrames. However, upon the Spark 2.0.0
    release, the Java based computation does not support the DataFrame but you are
    developing your applications using Python, Scala or R, still you will be able
    making use of the DataFrames.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它可以被视为类似于关系数据库（RDB）格式的等价实体。与DataFrame和RDD等其他数据抽象一样，Dataset也可以从各种数据源创建，如结构化数据文件（TSV、CSV、JSON和TXT）、Hive表、辅助存储、外部数据库或现有的RDD和DataFrame。然而，在Spark
    2.0.0发布后，基于Java的计算不支持DataFrame，但如果您使用Python、Scala或R开发应用程序，仍然可以使用DataFrame。
- en: In the next few sections, you will find the operations and actions using Dataset
    and how to create a Dataset from different sources.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，您将找到使用Dataset的操作和操作，以及如何从不同的来源创建Dataset。
- en: Reading datasets to create the Dataset
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取数据集以创建Dataset
- en: As mentioned above, the Dataset is a component of Spark SQL module introduced
    from the Spark 1.5.0 release. Therefore, all the entry point of all functionally
    starts from the initialization of Spark `SQLContext` . Basically, Spark SQL is
    used for executing SQL queries written either as a basic SQL syntax or HiveQL.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，Dataset是从Spark 1.5.0版本开始引入的Spark SQL模块的一个组件。因此，所有功能的入口都始于初始化Spark `SQLContext`。基本上，Spark
    SQL用于执行SQL查询，可以使用基本的SQL语法或HiveQL编写。
- en: 'A Dataset object will be returning when running SQL within another programming
    language. The following code segment will initialize the `SQLContext` within Spark
    Context. On the other hand, you might require having the `HiveContext` initialized
    for reading a data set from the Hive. You can also create a different context
    like `HiveContext` which provides a superset of basic functionalities of `SQLContext`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一种编程语言中运行SQL时，将返回一个Dataset对象。以下代码段将在Spark上下文中初始化`SQLContext`。另一方面，您可能需要初始化`HiveContext`以从Hive中读取数据集。您还可以创建一个不同的上下文，如`HiveContext`，它提供了`SQLContext`基本功能的超集：
- en: '[PRE30]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Reading from the files
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从文件中读取
- en: 'For example, you have a JSON file as shown here. Now you want to read this
    file using SQL context which basically returns a DataFrame which you can perform
    all the basic SQL operations and other DSL operations of Spark:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您有一个如下所示的JSON文件。现在您想要使用SQL上下文读取此文件，基本上返回一个DataFrame，您可以执行所有基本的SQL操作和其他Spark的DSL操作：
- en: '[PRE31]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Reading from the Hive
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从Hive中读取
- en: 'The following code connects with Hive context where one table is created and
    people JSON file is loaded into hive create. The output of the DataFrame will
    be the same as above:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码连接到Hive上下文，其中创建了一个表，并将people JSON文件加载到Hive中。DataFrame的输出将与上述相同：
- en: '[PRE32]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Pre-processing with Dataset
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Dataset进行预处理
- en: 'In the previous section, we have described the pre-processing with RDD for
    a practical machine learning application. Now we will do the same example using
    **DataFrame** (**DF**) API. You will find it very easy to manipulate the `SMSSpamCollection`
    Dataset (see at [http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)).
    We will show the same example by tokenizing the spam and ham messages for preparing
    a training set:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们已经描述了在实际机器学习应用程序中使用RDD进行预处理。现在我们将使用DataFrame（DF）API进行相同的示例。您会发现很容易操作`SMSSpamCollection`数据集（请参阅[http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)）。我们将展示相同的示例，通过对垃圾邮件和正常消息进行标记化，以准备一个训练集：
- en: '**Reading a Dataset:** You can read that Dataset using the Spark session variable
    `spark`that we have to initialize before using it. After reading the file as Dataset
    the output will be a tabular format of a single column. The default name of this
    column is `value`:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**读取数据集**：您可以使用初始化之前的Spark会话变量`spark`来读取该数据集。读取文件作为Dataset后，输出将是单列的表格格式。此列的默认名称是`value`：'
- en: '[PRE33]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Output:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '![Pre-processing with Dataset](img/00101.jpeg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![使用Dataset进行预处理](img/00101.jpeg)'
- en: 'Figure 7: A snapshot of the SMS spam dataset'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：SMS垃圾邮件数据集的快照
- en: '**Create Row RDD from existing Dataset**: From the preceding output you can
    see one column containing all the lines together. In order to make two columns
    such as label and features, we have to split it. Since Dataset is immutable you
    cannot modify the existing columns or Dataset. So you have to create new Dataset
    using the existing Dataset. Here the code converts the Dataset to RDD that is
    the collection of Row dataset. The row is an interface, which represents one row
    of output from a relational operator. You can create a new Row using `RowFactory`
    class of Spark:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从现有数据集创建Row RDD**：从前面的输出中，您可以看到一个包含所有行在一起的列。为了使两列，如标签和特征，我们必须将其拆分。由于Dataset是不可变的，您无法修改现有的列或Dataset。因此，您必须使用现有的Dataset创建新的Dataset。这里的代码将Dataset转换为Row数据集的集合。Row是一个接口，表示来自关系运算符的输出的一行。您可以使用Spark的`RowFactory`类创建一个新的Row：'
- en: '[PRE34]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Create new row RDD from an existing row RDD**: After having the Row RDD you
    can perform normal map operation which is all contains Row Dataset but having
    two values. The following code split the each row and returns a new one:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从现有的行RDD创建新的行RDD：在拥有行RDD之后，您可以执行常规的map操作，其中包含所有包含两个值的行数据集。以下代码拆分每一行并返回一个新的行：
- en: '[PRE35]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '**Create Dataset from Row RDD**: Now you have Row RDD, which contains two values
    for each Row. For creating a DF, you have to define the column names or schemas
    and its data types. There are two methods to define including inferring the schema
    using reflection and programmatically specify the schema. The methods are as follows:'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从行RDD创建数据集：现在您有了包含每行两个值的行RDD。要创建DF，您必须定义列名或模式及其数据类型。有两种方法可以定义，包括使用反射推断模式和以编程方式指定模式。方法如下：
- en: The 1st method basically uses the POJO classes and fields names will be the
    schema
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种方法基本上使用POJO类和字段名称将成为模式
- en: 'The 2nd method create list of StruchFields by defining the datatypes and create
    the structype. For this example, we have used the 2nd method for creating DF from
    existing row RDD as shown here:'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种方法通过定义数据类型并创建structype来创建StruchFields列表。在本例中，我们使用了第二种方法来从现有行RDD创建DF，如下所示：
- en: '[PRE36]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![Pre-processing with Dataset](img/00040.jpeg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![使用数据集进行预处理](img/00040.jpeg)'
- en: 'Figure 8: Schema of the collection'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：集合的模式
- en: '**Adding a new column**: Now that we have the DF of two columns. But we want
    to add new columns which convert the `labledSting` to `labedDouble` and `featureString`
    to `featureTokens`. You can do it similarly as previous code. After adding to
    new fields create new schema. Then create new DF after having normal map transformation
    in existing DF. The following code gives output of new DF having four columns:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加新列：现在我们有了两列的DF。但是我们想要添加新列，将`labledSting`转换为`labedDouble`，将`featureString`转换为`featureTokens`。您可以像以前的代码一样进行操作。在添加新字段后，创建新模式。然后在现有DF中进行常规map转换后创建新DF。以下代码给出了具有四列的新DF的输出：
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![Pre-processing with Dataset](img/00058.jpeg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![使用数据集进行预处理](img/00058.jpeg)'
- en: 'Figure 9: The dataset after adding a new column'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：添加新列后的数据集
- en: '**Some Dataset operations**: For data manipulation DF provides domain specific
    language in Java, Scala and others. You can do select, counting, filter, `groupBy`
    and so on operations into a DF. The following codes show some operations on the
    above DF:'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些数据集操作：对于数据操作，DF提供了Java、Scala等领域特定语言。您可以对DF进行选择、计数、过滤、`groupBy`等操作。以下代码展示了对上述DF的一些操作：
- en: '[PRE38]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![Pre-processing with Dataset](img/00053.jpeg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![使用数据集进行预处理](img/00053.jpeg)'
- en: 'Figure 10: Dataset showing the label and features'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：显示标签和特征的数据集
- en: '[PRE39]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![Pre-processing with Dataset](img/00009.jpeg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![使用数据集进行预处理](img/00009.jpeg)'
- en: 'Figure 11: Dataset showing that the label has been converted into double value'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：数据集显示标签已转换为双值
- en: '[PRE40]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![Pre-processing with Dataset](img/00001.jpeg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![使用数据集进行预处理](img/00001.jpeg)'
- en: 'Figure 12: showing the Dataset statistics after manipulations'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：显示操作后的数据集统计信息
- en: More about Dataset manipulation
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多关于数据集操作
- en: This section will describe how to use SQL queries on DF and different way to
    create Datasets across the datasets. Mainly running the SQL queries on DataFrame
    and the creating DataFrame from the JavaBean will be discussed in this section.
    However, interested readers can refer Spark programing guidelines for SQL operation
    in [3].
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述如何在DF上使用SQL查询以及在数据集之间创建不同方式的数据集。主要讨论在DataFrame上运行SQL查询以及从JavaBean创建DataFrame。然而，有兴趣的读者可以参考Spark编程指南中有关SQL操作的内容[3]。
- en: Running SQL queries on Dataset
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在数据集上运行SQL查询
- en: 'The `SQLContext` of Spark has `sql` method enables applications to run SQL
    queries. This method returns a DataFrame as a result:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`SQLContext`具有`sql`方法，使应用程序能够运行SQL查询。该方法返回一个DataFrame作为结果：
- en: '[`FilternewColumnsAddedDF.createOrReplaceTempView`(`SMSSpamCollection`)]:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`FilternewColumnsAddedDF.createOrReplaceTempView`(`SMSSpamCollection`)]:'
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following is the output of the preceding code:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是上述代码的输出：
- en: '![Running SQL queries on Dataset](img/00077.jpeg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![在数据集上运行SQL查询](img/00077.jpeg)'
- en: 'Figure 13: using SQL query to retrieve same result as Figure 11'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：使用SQL查询检索与图11相同的结果
- en: 'Count:'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计数：
- en: '[PRE42]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Output:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '![Running SQL queries on Dataset](img/00031.jpeg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![在数据集上运行SQL查询](img/00031.jpeg)'
- en: 'Figure 14: Showing the Dataset statistics'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：显示数据集统计信息
- en: Creating Dataset from the Java Bean
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Java Bean创建数据集
- en: 'You can create Dataset from a Java Bean; where you don''t need to define the
    schemas programmatically. For example, you can see **Plain Old Java Object** (**POJO**)
    named as Bean in the following code:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从Java Bean创建数据集；在这种情况下，您不需要以编程方式定义模式。例如，您可以在以下代码中看到名为Bean的**普通旧Java对象**（**POJO**）：
- en: '[PRE43]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create DF:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 创建DF：
- en: '[PRE44]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The following output is as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出如下：
- en: '![Creating Dataset from the Java Bean](img/00064.jpeg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![从Java Bean创建数据集](img/00064.jpeg)'
- en: 'Figure 15: Corresponding feature and label string'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：相应的特征和标签字符串
- en: Dataset from string and typed class
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从字符串和类型化类创建数据集
- en: As already mentioned that the Dataset is a typed and immutable collection of
    objects. Datasets are basically mapped to a relational schema. With the Dataset
    abstraction, a new concept has been brought in Spark called an encoder. The encoder
    helps in entity conversion for example conversion between the JVM objects and
    the corresponding tabular representation. You will find this API quite similar
    to RDDs transformations such as `map, mapToPair, flatMap` or `filter.`
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，数据集是一组对象的类型化和不可变集合。数据集基本上映射到关系模式。使用数据集抽象，Spark引入了一个新概念，称为编码器。编码器有助于实体转换，例如JVM对象与相应的表格表示之间的转换。您会发现这个API与RDD的转换非常相似，比如`map、mapToPair、flatMap`或`filter`。
- en: We will show the spam filter example using Datasets API in the following section.
    It reads the text file using and returns a Dataset as a tabular format. Then perform
    map transformation like RDDs for making (label, tokens) columns with adding an
    additional encoder parameter. Here, we have used the bean encoder with `SMSSpamTokenizedBean`
    class.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中展示使用数据集API的垃圾邮件过滤示例。它使用文本文件读取并返回数据集作为表格格式。然后执行类似RDD的映射转换，以添加额外的编码器参数（标签、令牌列）。在这里，我们使用了`SMSSpamTokenizedBean`类的bean编码器。
- en: 'In this sub-section, we will show how to create Dataset from string and typed
    class `SMSSpamTokenizedBean`. Let''s create the Spark session at first place as
    follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将展示如何从字符串和类型类`SMSSpamTokenizedBean`创建数据集。首先让我们创建Spark会话，如下所示：
- en: '[PRE45]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now create a new Dataset of type String from the `smm` filtering Dataset that
    means `Dataset<String>` and show the result as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从`smm`过滤数据集创建一个新的String类型的数据集，即`Dataset<String>`，并显示结果如下：
- en: '[PRE46]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Here is the output of the preceding code:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '![Dataset from string and typed class](img/00010.jpeg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![从字符串和类型类创建的数据集](img/00010.jpeg)'
- en: 'Figure 16: Showing the snapshot of the spam filtering dataset using Dataset'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：显示使用数据集进行垃圾邮件过滤的快照
- en: 'Now let''s create a second Dataset from the typed class `SMSSpamTokenizedBean`
    by mapping the Dataset of string we created immediate before as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过将我们之前创建的字符串数据集映射为`SMSSpamTokenizedBean`类型的第二个数据集，来创建第二个数据集，如下所示：
- en: '[PRE47]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now let''s print the Dataset along with its schema as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打印数据集及其模式，如下所示：
- en: '[PRE48]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The following output is:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出为：
- en: '![Dataset from string and typed class](img/00089.jpeg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![从字符串和类型类创建的数据集](img/00089.jpeg)'
- en: 'Figure 17: Showing the token and label and the lower side the schema'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：显示令牌和标签，下方是模式
- en: 'Now if you would like to convert this typed Dataset as type Row then you can
    use the `toDF()` method and to further create a temporary view out of the new
    `Dataset<Row>` you can use the `createOrReplaceTempView()` method with ease as
    follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您想将此类型的数据集转换为Row类型，那么您可以使用`toDF()`方法，并且可以轻松地使用`createOrReplaceTempView()`方法创建新的`Dataset<Row>`的临时视图，如下所示：
- en: '[PRE49]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Similarly, might want to view the same Dataset by calling show `method()` as
    follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可能希望通过调用`show()`方法查看相同的数据集，如下所示：
- en: '[PRE50]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Output:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '![Dataset from string and typed class](img/00128.jpeg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![从字符串和类型类创建的数据集](img/00128.jpeg)'
- en: 'Figure 18: Corresponding labels and tokens. Labels are converted into double
    value'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：相应的标签和令牌。标签转换为双值
- en: 'Now let''s explore the typed class `SMSSpamTokenizedBean`. The class works
    as a Java tokenized bean class for the labeling the texts. More technically, the
    class takes the input then it sets the labels and after that gets the labels.
    Secondly, it also sets and gets the token for spam filtering. Including the setter
    and methods, here is the class:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索类型类`SMSSpamTokenizedBean`。该类作为用于标记文本的Java标记化bean类。更具体地说，该类接受输入，然后设置标签，然后获取标签。其次，它还设置和获取用于垃圾邮件过滤的令牌。包括setter和方法，以下是该类：
- en: '[PRE51]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Comparison between RDD, DataFrame and Dataset
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD、DataFrame和Dataset之间的比较
- en: There are some objectives to bring Dataset as a new Data Structure of Spark.
    Although RDD API is very flexible, it is sometimes harder to optimize the processing.
    On the other hand, the DataFrame API is very easier to optimize but it lacks some
    of the nice features of RDD. So, the goal of the Datasets is to allow the users
    to easily express transformations on objects and also providing the advantages
    (performance and robustness) of the Spark SQL execution engine.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集作为Spark的新数据结构带来一些目标。虽然RDD API非常灵活，但有时很难优化处理。另一方面，DataFrame API很容易优化，但缺少RDD的一些好特性。因此，数据集的目标是允许用户轻松表达对象上的转换，并提供Spark
    SQL执行引擎的优势（性能和鲁棒性）。
- en: The Dataset can perform many operations such as sorting or shuffling without
    de-serializing of an object. For doing this it requires an explicit Encoder that
    is used to serialize the object into a binary format. It is capable of mapping
    the schema of a given object (Bean) to the Spark SQL type system. On the other
    hand, RDDs are based on run-time reflection based serialisation and the operations
    that change the types of object of a Dataset also need an encoder for the new
    type.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以执行许多操作，如排序或洗牌，而无需对对象进行反序列化。为此，它需要一个显式的编码器，用于将对象序列化为二进制格式。它能够将给定对象（Bean）的模式映射到Spark
    SQL类型系统。另一方面，RDD基于运行时反射的序列化，而改变数据集对象类型的操作也需要新类型的编码器。
- en: Spark and data scientists workflow
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark和数据科学家的工作流程
- en: As already stated that, a common task for a data scientist is to select the
    data, data pre-processing (formatting, cleaning and sampling) and data transformation
    (scaling, decomposition and aggregation) the raw data into a format that can be
    passed into machine learning models to build the models. As the size of the experimental
    datasets increases, the traditional single-node databases will not be feasible
    to handle these kinds of datasets, therefore, you need to switch a big data processing
    computing like Spark. Fortunately, we have the Spark to be an excellent option
    as a scalable distributed computing system to coup with your datasets.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 正如已经说明的，数据科学家的一个常见任务是选择数据、数据预处理（格式化、清理和抽样）和数据转换（缩放、分解和聚合）原始数据，以便将其传递到机器学习模型中构建模型。随着实验数据集的增加，传统的单节点数据库将无法处理这些类型的数据集，因此，您需要切换到像Spark这样的大数据处理计算。幸运的是，我们有Spark作为可扩展的分布式计算系统，可以处理您的数据集。
- en: '![Spark and data scientists workflow](img/00090.jpeg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![Spark和数据科学家的工作流程](img/00090.jpeg)'
- en: 'Figure-19: Data scientist''s workflow for using the Spark'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：数据科学家使用Spark的工作流程
- en: Now let's move to the exact point, as a data scientist at first you will have
    to read the Dataset available in diverse formats. Then reading the Datasets will
    provide you with the concept of RDDs, DataFrames and Datasets that we already
    describe. You can cache the Dataset into the main memory; you can transform the
    read data sets from the DataFrame, SQL or as Datasets. And finally, you will perform
    an action to dump your data to the disks, computing nodes or clusters. The step
    what we describe here essentially forms a workflow that you will follow for the
    basic data processing using Spark that showed in *Figure 1*.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来到确切的点，作为一名数据科学家，首先你将不得不阅读以各种格式提供的数据集。然后阅读数据集将为你提供我们已经描述的RDDs、DataFrames和Datasets的概念。你可以将数据集缓存到主内存中；你可以从DataFrame、SQL或Datasets转换读取的数据集。最后，你将执行一个操作，将数据转储到磁盘、计算节点或集群。我们在这里描述的步骤基本上形成了一个工作流程，你将按照这个工作流程进行使用Spark进行基本数据处理，如*图1*所示。
- en: Deeper into Spark
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解Spark
- en: In this section, we will show you the advanced features of Spark including the
    use of shared variables (both the broadcast variables and accumulators) and their
    underlying concept will be discussed. However, we will discuss the data partition
    in later chapters.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将展示Spark的高级特性，包括使用共享变量（广播变量和累加器），并讨论它们的基本概念。然而，我们将在后面的章节中讨论数据分区。
- en: Shared variables
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共享变量
- en: 'The concept of shared variables in the context programming is not new. The
    variables that are required to use by many functions, and methods in parallel
    are called shared variables. Spark has some mechanism to use or implement the
    shared variables. In spark, the functions are passed to a spark operation like
    a map or reduce is executed on remote cluster nodes. The codes or functions work
    as a separate copy of variables on the nodes and no updates of the results are
    propagated back to the driver program. However, Spark provides two types of shared
    variables for two common usage patterns: broadcast variables and accumulators.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在编程上下文中，共享变量的概念并不新鲜。需要许多函数和方法并行使用的变量称为共享变量。Spark有一些机制来使用或实现共享变量。在Spark中，传递给Spark操作的函数（如map或reduce）在远程集群节点上执行。代码或函数在节点上作为变量的独立副本工作，结果的更新不会传播回驱动程序。然而，Spark提供了两种常见用法模式的共享变量：广播变量和累加器。
- en: Broadcast variables
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 广播变量
- en: 'Broadcast variables provide the facility to persist a read-only to be variable
    cached on local machine rather than sending a copy to the computing nodes or driver
    program. Providing the copy of large input Dataset to every node in an efficient
    manner of spark. It also reduces the communication cost because Spark uses an
    efficient broadcast. Broadcast variables can be created from a variable `v` by
    calling `SparkContext.broadcast(v)`. The following code shows this:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark API中有许多类型和方法需要了解。然而，更多和更详细的讨论超出了本书的范围。
- en: '[PRE52]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Accumulators
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 累加器
- en: 'Accumulators are another shared variable can be used to implement counters
    (as in MapReduce) or sums. Spark provides the supports for accumulators to be
    of numeric types only. However, you can also add support for new data types using
    existing techniques [1]. It is created from an initial value say `val` by calling:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 累加器是另一种共享变量，可以用于实现计数器（如MapReduce中）或求和。Spark只支持累加器为数值类型。然而，你也可以使用现有技术为新的数据类型添加支持[1]。通过调用以下初始值为`val`的方法创建：
- en: '[PRE53]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The following code shows the uses of accumulator for adding the elements of
    an array:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了使用累加器将数组元素相加的用法：
- en: '[PRE54]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: There are many types and method in the Spark APIs needed to be known. However,
    more and details discussion is out of the scope of this book.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: Spark编程指南：[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html)。
- en: Tip
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Interested readers should refer Spark and related materials on the following
    web pages:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣的读者应该参考以下网页上关于Spark和相关材料的内容：
- en: 'Spark programming guide: [http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html).'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了RDDs、Dataset和DataFrame API进行了基本的数据操作。我们还学习了如何通过这些API进行一些复杂的数据操作。我们试图专注于数据操作，以理解一个实际的机器学习问题——垃圾邮件过滤。除此之外，我们还展示了如何从不同的来源读取数据。分析和准备你的数据，以理解垃圾邮件过滤作为一个例子。
- en: 'Spark RDD operation: [http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations).'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: Spark RDD操作：[http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations)。
- en: 'Spark SQL operation: [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html).'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL操作：[http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)。
- en: Summary
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this Chapter, we applied the basic data manipulations with RDDs, Dataset
    and DataFrame APIs. We also learn how to do some complex data manipulation through
    these APIs. We tried to focus on data manipulations, to understand a practical
    machine learning problem Spam-filtering. In addition to these, we showed how to
    read the data from different sources. Analyzing and preparing your data to understand
    the spam filtering as an example.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量提供了将只读变量持久化缓存在本地机器上的功能，而不是将副本发送到计算节点或驱动程序。以一种高效的方式将大型输入数据集的副本提供给Spark中的每个节点。它还减少了通信成本，因为Spark使用了高效的广播。广播变量可以通过调用`SparkContext.broadcast(v)`从变量`v`创建。以下代码显示了这一点：
- en: However, we did not develop any complete machine learning application, since
    our target was just to show you the basic data manipulation on the experimental
    Datasets. We intended to develop complete ML application in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们并没有开发任何完整的机器学习应用程序，因为我们的目标只是向您展示实验数据集上的基本数据操作。我们打算在[第6章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "第6章。构建可扩展的机器学习管道")*构建可扩展的机器学习管道*中开发完整的ML应用程序。
- en: Which features should be used to create a predictive model is not only a vital
    question but also a difficult question that may require deep knowledge of the
    problem domain to be answered. It is possible to automatically select those features
    in data that are most useful or most relevant for the problem someone is working
    on. Considering these questions, the next chapter covers the feature engineering
    in detail, explaining the reasons why to apply it along with some best practices
    in feature engineering. Some topics which are still unclear will be clearer in
    the next chapter.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 创建预测模型应该使用哪些特征不仅是一个重要的问题，而且可能是一个需要深入了解问题领域才能回答的困难问题。可以自动选择数据中对某人正在处理的问题最有用或最相关的特征。考虑到这些问题，下一章将详细介绍特征工程，解释为什么要应用它以及一些特征工程的最佳实践。一些仍不清楚的主题将在下一章中更清晰。
