- en: 5\. Decision Trees and Random Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 决策树与随机森林
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: 'In this chapter, we''ll shift our focus to another type of machine learning
    model that has taken data science by storm in recent years: tree-based models.
    In this chapter, after learning about trees individually, you''ll then learn how
    models made up of many trees, called random forests, can improve the overfitting
    associated with individual trees. After reading this chapter, you will be able
    to train decision trees for machine learning purposes, visualize trained decision
    trees, and train random forests and visualize the results.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点介绍近年来在数据科学中风靡一时的另一类机器学习模型：基于树的模型。在本章中，在单独学习决策树后，你将学习由多棵树组成的模型（即随机森林），它们如何改善单棵树所产生的过拟合问题。读完本章后，你将能够为机器学习训练决策树、可视化训练好的决策树，并训练随机森林并可视化结果。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In the last two chapters, we have gained a thorough understanding of the workings
    of logistic regression. We have also gotten a lot of experience with using the
    scikit-learn package in Python to create logistic regression models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的两章中，我们已经深入理解了逻辑回归的工作原理，并且已经积累了大量使用 Python 中的 scikit-learn 包来创建逻辑回归模型的经验。
- en: 'In this chapter, we will introduce a powerful type of predictive model that
    takes a completely different approach from the logistic regression model: **decision
    trees**. Decision trees and the models based on them are some of the most performant
    models available today for general machine learning applications. The concept
    of using a tree process to make decisions is simple, and therefore, decision tree
    models are easy to interpret. However, a common criticism of decision trees is
    that they overfit to the training data. In order to remedy this issue, researchers
    have developed **ensemble methods**, such as **random forests**, that combine
    many decision trees to work together and make better predictions than any individual
    tree could.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一种强大的预测模型，这种模型与逻辑回归模型采用完全不同的方法：**决策树**。决策树及其基础上的模型是目前可用于一般机器学习应用的最具表现力的模型之一。使用树状过程进行决策的概念简单明了，因此，决策树模型易于理解。然而，决策树的一个常见批评是它们容易对训练数据过拟合。为了解决这个问题，研究人员开发了**集成方法**，如**随机森林**，通过将多棵决策树结合在一起，协同工作，做出比任何单棵树更好的预测。
- en: We will see that decision trees and random forests can improve the quality of
    the predictive modeling of the case study data beyond what we have achieved so
    far with logistic regression.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到，决策树和随机森林可以提升案例研究数据的预测建模质量，超越我们目前使用逻辑回归所取得的成果。
- en: Decision Trees
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: '**Decision trees** and the machine learning models that are based on them,
    in particular, **random forests** and **gradient boosted trees**, are fundamentally
    different types of models than Generalized Linear Models (GLMs), such as logistic
    regression. GLMs are rooted in the theories of classical statistics, which have
    a long history. The mathematics behind linear regression was originally developed
    at the beginning of the 19th century, by Legendre and Gauss. Because of this,
    the normal distribution is also known as the Gaussian distribution.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**及其基础上的机器学习模型，特别是**随机森林**和**梯度提升树**，与广义线性模型（GLM），如逻辑回归，是根本不同的模型类型。GLM
    源自经典统计学理论，这些理论有着悠久的历史。线性回归背后的数学最初由勒让德和高斯在19世纪初提出。因此，正态分布也被称为高斯分布。'
- en: In contrast, while the idea of using a tree process to make decisions is relatively
    simple, the popularity of decision trees as mathematical models has come about
    more recently. The mathematical procedures that we currently use for formulating
    decision trees in the context of predictive modeling were published in the 1980s.
    The reason for this more recent development is that the methods used to grow decision
    trees rely on computational power – that is, the ability to crunch a lot of numbers
    quickly. We take such capabilities for granted nowadays, but they weren't widely
    available until more recently in the history of mathematics.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，虽然使用树状过程进行决策的想法相对简单，决策树作为数学模型的流行是在最近才兴起的。我们目前用于制定决策树的数学方法是在1980年代发布的。之所以出现这种较新的发展，是因为用于生长决策树的方法依赖于计算能力——即快速处理大量数字的能力。如今我们理所当然地拥有这种能力，但在数学历史上，直到近代才广泛可用。
- en: So, what is meant by a decision tree? We can illustrate the basic concept using
    a practical example. Imagine that you are considering whether or not to venture
    outdoors on a certain day. The only information you will base your decision on
    involves the weather and, in particular, whether the sun is shining and how warm
    it is. If it is sunny, your tolerance for cool temperatures is increased, and
    you will go outside if the temperature is at least 10 °C.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，决策树是什么意思呢？我们可以通过一个实际的例子来说明基本概念。假设你正在考虑是否在某一天外出。你做决定时唯一依赖的信息是天气，特别是阳光是否明媚以及气温有多暖和。如果是晴天，你对凉爽气温的耐受性会提高，只要气温至少为
    10°C，你就会外出。
- en: 'However, if it''s cloudy, you require somewhat warmer temperatures and will
    only go outside if the temperature is 15 °C or more. Your decision-making process
    could be represented by the following tree:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果是阴天，你需要稍微温暖一些的气温，并且只有当气温达到 15°C 或更高时，你才会外出。你的决策过程可以通过以下树状图表示：
- en: '![Figure 5.1: A decision tree for deciding whether to go outside given the
    weather'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1：根据天气决定是否外出的决策树'
- en: '](img/B16392_05_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16392_05_01.jpg)'
- en: 'Figure 5.1: A decision tree for deciding whether to go outside given the weather'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：根据天气决定是否外出的决策树
- en: As you can see, decision trees have an intuitive structure and mimic the way
    that logical decisions might be made by humans. Therefore, they are a highly **interpretable**
    type of mathematical model, which can be a particularly desirable property depending
    on the audience. For example, the client for a data science project may be especially
    interested in a clear understanding of how a model works. Decision trees are a
    good way of delivering on this requirement, as long as their performance is sufficient.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，决策树具有直观的结构，并模拟了人类可能做出逻辑决策的方式。因此，它们是一个高度**可解释**的数学模型类型，这在某些受众中可能是一个特别理想的特性。例如，数据科学项目的客户可能特别关注如何清晰地理解一个模型是如何工作的。只要其性能足够，决策树是满足这一要求的好方法。
- en: The Terminology of Decision Trees and Connections to Machine Learning
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树术语及其与机器学习的关系
- en: Looking at the tree in *Figure 5.1*, we can begin to become familiar with some
    of the terminology of decision trees. Because there are two levels of decisions
    being made, based on cloud conditions at the first level and temperature at the
    second level, we say that this decision tree has a **depth** of two. Here, both
    **nodes** at the second level are temperature-based decisions, but the kinds of
    decisions could be different within a level; for example, we could base our decision
    on whether or not it was raining if it was not sunny.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 看图中的*图 5.1*，我们可以开始熟悉一些决策树的术语。因为在第一层基于云层条件，第二层基于气温做出决策，所以我们说这棵决策树的**深度**为二。这里，第二层的两个**节点**都是基于气温做出的决策，但在同一层次内，决策的种类可能不同；例如，如果不是晴天，我们也可以根据是否下雨来做决定。
- en: In the context of machine learning, the quantities that are used to make decisions
    at the nodes (in other words, to **split** the nodes) are the features. The features
    in the example in *Figure 5.1* are a binary categorical feature for whether it's
    sunny, and a continuous feature for temperature. While we have only illustrated
    each feature being used once in a given branch of the tree, the same feature could
    be used multiple times in a branch. For example, we may choose to go outside on
    a sunny day with a temperature of at least 10 °C, but not if it were more than
    40 °C – that's too hot! In this case, node 4 of *Figure 5.1* would be split on
    the condition "Is the temperature greater than 40 °C?" where "stay in" is the
    outcome if the answer is "yes," but "go outside" is the outcome if the answer
    is "no," meaning that the temperature is between 10 °C and 40 °C. Decision trees
    are therefore able to capture non-linear effects of the features, as opposed to
    a linear relationship that might assume that the hotter it was, the more likely
    we would be to go outside, regardless of how hot it was.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，用于在节点处做决策（换句话说，**分裂**节点）的量是特征。在*图 5.1*中的示例中，特征包括是否晴天的二元分类特征和温度的连续特征。虽然我们在树的给定分支中只展示了每个特征被使用一次，但同一个特征也可以在一个分支中被多次使用。例如，我们可能选择在阳光明媚的日子里，温度至少为
    10 °C 时外出，但如果温度超过 40 °C，就不出去了——那太热了！在这种情况下，*图 5.1*中的节点 4 将根据“温度是否大于 40 °C？”这一条件进行分裂，如果答案是“是”，结果是“待在室内”，如果答案是“否”，则结果是“外出”，这意味着温度在
    10 °C 到 40 °C 之间。因此，决策树能够捕捉特征的非线性效应，而不是假设温度越高，我们越可能外出的一种线性关系，无论温度有多高。
- en: Consider the way that trees are typically represented, such as in *Figure 5.1*.
    The branches grow downward based on the binary decisions that can split the nodes
    into two more nodes. These binary decisions can be thought of as "if, then" rules.
    In other words, if a certain condition is met, do this, otherwise, do something
    else. The decision being made in our example tree is analogous to the concept
    of the response variable in machine learning. If we made a decision tree for the
    case study problem of credit default, the decisions would instead be predictions
    of the binary response values, which are "this account defaults" or "this account
    doesn't default." A tree that answers a binary yes/no type of question is a **classification
    tree**. However, decision trees are quite versatile and can also be used for multiclass
    classification and regression.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑树通常是如何表示的，例如在*图 5.1*中。分支基于二元决策向下生长，这些二元决策可以将节点分裂成两个子节点。这些二元决策可以被视为“如果，那么”的规则。换句话说，如果某个条件满足，就做这个，否则做别的事情。我们示例树中的决策类似于机器学习中的响应变量的概念。如果我们为信用违约的案例研究问题做一个决策树，决策将会是预测二元响应值，即“此账户违约”或“此账户不违约”。回答二元是/否问题的树被称为**分类树**。然而，决策树非常多功能，也可以用于多类分类和回归问题。
- en: The terminal nodes at the bottom of the tree are called **leaves**, or leaf
    nodes. In our example, the leaves are the final decisions as to whether to go
    outside or stay in. There are four leaves on our tree, although you can imagine
    that if the tree only had a depth of one, where we made our decision based only
    on cloud conditions, there would be two leaves; and nodes 2 and 3 in *Figure 5.1*
    would be leaf nodes with "go outside" and "stay in" as the decisions, respectively.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 树的最底层节点被称为**叶子**，或叶节点。在我们的示例中，叶子是最终的决策，即是否外出或待在室内。我们的树上有四个叶子，尽管你可以想象，如果树的深度只有一层，其中的决策仅基于云层情况，那么将会有两个叶子；在*图
    5.1*中，节点 2 和节点 3 将是叶节点，分别以“外出”和“待在室内”作为决策。
- en: In our example, every node at every level before the final level is split. This
    is not strictly necessary as you may go outside on any sunny day, regardless of
    the temperature. In this case, node 2 will not be split, so this branch of the
    tree will end on the first level with a "yes" decision. Your decision on cloudy
    days, however, may involve temperature, meaning this branch can extend to a further
    level. In the case that every node before the final level is split, consider how
    quickly the number of leaves grows with the number of levels.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，每个层次上的每个节点都被分裂了。在严格意义上，这并非必要，因为你可能会选择在任何阳光明媚的日子外出，无论温度如何。在这种情况下，节点 2
    将不会被分裂，因此该分支会在第一层次以“是”的决策结束。然而，在阴天的情况下，你的决策可能会涉及温度，这意味着该分支可以扩展到更深的层次。如果每个节点在最终层次之前都被分裂，考虑一下随着层数增加，叶子数量增长的速度。
- en: For example, what would happen if we grew the decision tree in *Figure 5.1*
    down through an additional level, perhaps with a wind speed feature, to factor
    in wind chill for the four combinations of cloud conditions and temperature. Each
    of the four nodes that are now leaves, nodes numbered from four to seven in *Figure
    5.1*, would be split into two more leaf nodes, based on wind speed in each case.
    Then, there would be *4 × 2 = 8* leaf nodes. In general, it should be clear that
    in a tree with n levels, where every node before the final level is split, there
    will be *2n* leaf nodes. This is important to bear in mind as **maximum depth**
    is one of the hyperparameters that you can set for a decision tree classifier
    in scikit-learn. We'll now explore this in the following exercise.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们将*图 5.1*中的决策树再向下生长一个额外的层级，或许增加一个风速特征，以便考虑四种云层条件和温度的风寒效应会发生什么情况。现在作为叶子的四个节点，编号从四到七的节点*图
    5.1*，将会基于每种情况的风速被拆分成两个更多的叶节点。然后，叶节点将变为*4 × 2 = 8*个。一般来说，应该清楚的是，在一个有 n 层的树中，若每个最终层之前的节点都被拆分，那么将会有*2n*个叶节点。考虑到这一点是很重要的，因为**最大深度**是你可以为决策树分类器设置的超参数之一。接下来我们将在以下练习中探讨这一点。
- en: 'Exercise 5.01: A Decision Tree in Scikit-Learn'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.01：在 Scikit-Learn 中使用决策树
- en: 'In this exercise, we will use the case study data to grow a decision tree,
    where we specify the maximum depth. We''ll also use some handy functionality to
    visualize the decision tree, in the form of the `graphviz` package. Perform the
    following steps to complete the exercise:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用案例研究数据来生长一棵决策树，其中我们指定最大深度。我们还将使用一些便捷的功能来可视化决策树，使用的是 `graphviz` 包。请按以下步骤完成练习：
- en: Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The Jupyter notebook for this exercise can be found at [https://packt.link/IUt7d](https://packt.link/IUt7d).
    Before you begin the exercise, please ensure that you have followed the instructions
    in the *Preface* regarding setting up your environment and importing the necessary
    libraries.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的 Jupyter notebook 可在 [https://packt.link/IUt7d](https://packt.link/IUt7d)
    找到。在开始练习之前，请确保你已按照*前言*中的说明设置好环境并导入必要的库。
- en: 'Load several of the packages that we''ve been using, and an additional one,
    `graphviz`, so that we can visualize decision trees:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载我们一直在使用的几个包，并额外加载一个包 `graphviz`，以便我们可以可视化决策树：
- en: '[PRE0]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the cleaned case study data:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载清理后的案例研究数据：
- en: '[PRE1]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The location of the cleaned data may differ depending on where you saved it.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 清理后的数据的位置可能因你保存数据的位置而有所不同。
- en: 'Get a list of column names of the DataFrame:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据框的列名列表：
- en: '[PRE2]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Make a list of columns to remove that aren''t features or the response variable:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个列出要移除的不是特征或响应变量的列的列表：
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Use a list comprehension to remove these column names from our list of features
    and the response variable:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用列表推导式从我们的特征列表和响应变量中移除这些列名：
- en: '[PRE4]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This should output the list of features and the response variable:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该输出特征列表和响应变量：
- en: '[PRE5]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now the list of features is prepared. Next, we will make some imports from scikit-learn.
    We want to make a train/test split, which we are already familiar with. We also
    want to import the decision tree functionality.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在特征列表已准备好。接下来，我们将从 scikit-learn 导入一些库。我们需要进行训练/测试集拆分，这是我们已经熟悉的操作。我们还需要导入决策树功能。
- en: 'Run this code to make imports from scikit-learn:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码从 scikit-learn 进行导入：
- en: '[PRE6]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `tree` library of scikit-learn contains decision tree-related classes.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: scikit-learn 的 `tree` 库包含与决策树相关的类。
- en: 'Split the data into training and testing sets using the same random seed that
    we have used throughout the book:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们在全书中使用的相同随机种子，将数据拆分为训练集和测试集：
- en: '[PRE7]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here, we use all but the last element of the list to get the names of the features,
    but not the response variable: `features_response[:-1]`. We use this to select
    columns from the DataFrame, and then retrieve their values using the `.values`
    method. We also do something similar for the response variable, but specify the
    column name directly. In making the train/test split, we''ve used the same random
    seed as in previous work, as well as the same split size. This way, we can directly
    compare the work we will do in this chapter with previous results. Also, we continue
    to reserve the same "unseen test set" from the model development process.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用列表中的所有元素（除了最后一个）来获取特征名称，而不是响应变量：`features_response[:-1]`。我们用它从DataFrame中选择列，然后使用`.values`方法检索它们的值。我们对响应变量也做类似的操作，但直接指定列名。在进行训练/测试数据划分时，我们使用与之前相同的随机种子以及相同的划分比例。这样，我们可以直接将本章所做的工作与以前的结果进行比较。此外，我们继续保留与模型开发过程中的“未见测试集”相同的数据集。
- en: Now we are ready to instantiate the decision tree class.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们准备实例化决策树类。
- en: 'Instantiate the decision tree class by setting the `max_depth` parameter to
    `2`:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将`max_depth`参数设置为`2`来实例化决策树类：
- en: '[PRE8]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We have used the `DecisionTreeClassifier` class because we have a classification
    problem. Since we specified `max_depth=2`, when we grow the decision tree using
    the case study data, the tree will grow to a depth of at most `2`. Let's now train
    this model.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用了`DecisionTreeClassifier`类，因为这是一个分类问题。由于我们指定了`max_depth=2`，当我们使用案例研究数据生长决策树时，树的最大深度将为`2`。现在让我们训练这个模型。
- en: 'Use this code to fit the decision tree model and grow the tree:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码拟合决策树模型并生长树：
- en: '[PRE9]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This should display the following output:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应显示以下输出：
- en: '[PRE10]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we have fit this decision tree model, we can use the `graphviz` package
    to display a graphical representation of the tree.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经拟合了这个决策树模型，我们可以使用`graphviz`包来显示树的图形表示。
- en: 'Export the trained model in a format that can be read by the `graphviz` package
    using this code:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将训练好的模型导出为`graphviz`包可以读取的格式：
- en: '[PRE11]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, we''ve provided a number of options for the `.export_graphviz` method.
    First, we need to say which trained model we''d like to graph, which is `dt`.
    Next, we say we don''t want an output file: `out_file=None`. Instead, we provide
    the `dot_data` variable to hold the output of this method. The rest of the options
    are set as follows:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们为`.export_graphviz`方法提供了多个选项。首先，我们需要指定要绘制的训练模型，即`dt`。接下来，我们说不需要输出文件：`out_file=None`。相反，我们提供了`dot_data`变量来保存此方法的输出。其余选项设置如下：
- en: '`filled=True`: Each node will be filled with a color.'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`filled=True`：每个节点将填充颜色。'
- en: '`rounded=True`: The nodes will appear with rounded edges as opposed to rectangles.'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`rounded=True`：节点将呈现圆角边缘，而不是矩形。'
- en: '`feature_names=features_response[:-1]`: The names of the features from our
    list will be used as opposed to generic names such as `X[0]`.'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`feature_names=features_response[:-1]`：我们列表中的特征名称将被使用，而不是像`X[0]`这样的通用名称。'
- en: '`proportion=True`: The proportion of training samples in each node will be
    displayed (we''ll discuss this more later).'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`proportion=True`：每个节点中训练样本的比例将显示（我们稍后会进一步讨论）。'
- en: '`class_names=[''Not defaulted'', ''Defaulted'']`: The name of the predicted
    class will be displayed for each node.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`class_names=[''Not defaulted'', ''Defaulted'']`：每个节点将显示预测类的名称。'
- en: What is the output of this method?
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个方法的输出是什么？
- en: If you examine the contents of `dot_data`, you will see that it is a long text
    string. The `graphviz` package can interpret this text string to create a visualization.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果检查`dot_data`的内容，你会发现它是一个长文本字符串。`graphviz`包可以解析这个文本字符串并创建可视化效果。
- en: 'Use the `.Source` method of the `graphviz` package to create an image from
    `dot_data` and display it:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`graphviz`包的`.Source`方法从`dot_data`创建图像并显示它：
- en: '[PRE12]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output should look like this:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![Figure 5.2: A decision tree plot from graphviz'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图5.2：来自graphviz的决策树图'
- en: '](img/B16392_05_02.jpg)'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_05_02.jpg)'
- en: 'Figure 5.2: A decision tree plot from graphviz'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.2：来自graphviz的决策树图
- en: The graphical representation of the decision tree in *Figure 5.2* should be
    rendered directly in your Jupyter notebook.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 决策树的图形表示应直接呈现在你的Jupyter笔记本中，如*图5.2*所示。
- en: Note
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'Alternatively, you could save the output of `.export_graphviz` to disk by providing
    a file path to the `out_file` keyword argument. To turn this output file into
    an image file, for example, a `.png` file that you could use in a presentation,
    you could run this code at the command line, substituting in the filenames as
    appropriate: `$ dot -Tpng <exported_file_name> -o <image_file_name_you_want>.png`.'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，你可以通过为`out_file`关键字参数提供文件路径，将`.export_graphviz`的输出保存到磁盘。例如，要将这个输出文件转换为图像文件，如`.png`文件，以便在演示中使用，你可以在命令行运行以下代码，并根据需要替换文件名：`$
    dot -Tpng <exported_file_name> -o <image_file_name_you_want>.png`。
- en: For further details on the options relating to `.export_graphviz`, you should
    consult the scikit-learn documentation ([https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html)).
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于`.export_graphviz`选项的更多细节，你应参考scikit-learn文档（[https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html)）。
- en: The visualization in *Figure 5.2* contains a lot of information about how the
    decision tree was trained, and how it can be used to make predictions. We will
    discuss the training process in more detail later, but suffice to say that training
    a decision tree works by starting with all the training samples in the initial
    node at the top of the tree, and then splitting these into two groups based on
    a `PAY_1 <= 1.5` in the first node.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*图5.2*中的可视化包含了许多有关决策树训练和如何使用它进行预测的信息。我们稍后会更详细地讨论训练过程，但简而言之，训练决策树的过程是从树顶部初始节点的所有训练样本开始，然后根据第一个节点中的`PAY_1
    <= 1.5`将这些样本分成两组。'
- en: All the samples where the value of the `PAY_1` feature is less than or equal
    to the cut point of `1.5` will be represented as `True` under this Boolean condition.
    As shown in *Figure 5.2*, these samples get sorted into the left side of the tree,
    following the arrow that says `True` next to it.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有`PAY_1`特征值小于或等于`1.5`的样本将在此布尔条件下表示为`True`。如*图5.2*所示，这些样本会根据旁边写着`True`的箭头被排序到树的左侧。
- en: As you can see in the graph, each node that is split contains the splitting
    criteria on the first line of text. The next line relates to `gini`, which we
    will discuss shortly.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如你在图表中看到的，每个被拆分的节点包含拆分标准的第一行文本。下一行与`gini`有关，我们稍后会讨论。
- en: The following line contains information about the proportion of samples in each
    node. In the top node, we are starting with all the samples (`samples = 100.0%`).
    Following the first split, 89.5% of the samples get sorted into the node on the
    left, while the remaining 10.5% go into the node on the right. This information
    is shown directly in the visualization and reflects how the training data was
    used to create the tree. Let's confirm this by examining the training data.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下一行包含每个节点中样本比例的信息。在顶部节点，我们从所有样本（`samples = 100.0%`）开始。第一次拆分后，89.5%的样本被排序到左侧节点，剩余的10.5%进入右侧节点。这些信息直接显示在可视化中，反映了如何使用训练数据来创建树。让我们通过检查训练数据来确认这一点。
- en: 'To confirm the proportion of training samples where the `PAY_1` feature is
    less than or equal to `1.5`, first identify the index of this feature in the list
    of `features_response[:-1]` feature names:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要确认训练样本中`PAY_1`特征小于或等于`1.5`的比例，首先识别该特征在`features_response[:-1]`特征名称列表中的索引：
- en: '[PRE13]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This code should output the following:'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码应输出如下内容：
- en: '[PRE14]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, observe the shape of the training data:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，观察训练数据的形状：
- en: '[PRE15]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This should give you the following output:'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应为你提供以下输出：
- en: '[PRE16]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To confirm the fraction of samples following the first split of the decision
    tree, we need to know the proportion of samples, where the `PAY_1` feature meets
    the Boolean condition, that was used to make this split. To do this, we can use
    the index of the `PAY_1` feature in the training data, corresponding to the index
    in the list of feature names, and the number of samples in the training data,
    which is the number of rows we observed from `.shape`.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要确认决策树第一次拆分后的样本比例，我们需要知道满足`PAY_1`特征布尔条件的样本比例，这些样本被用于进行此拆分。为此，我们可以使用训练数据中`PAY_1`特征的索引，这对应于特征名称列表中的索引，并使用训练数据中的样本数量，这个数量是我们从`.shape`观察到的行数。
- en: 'Use this code to confirm the proportion of samples after the first split of
    the decision tree:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此代码确认决策树第一次拆分后的样本比例：
- en: '[PRE17]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output should be as follows:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE18]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: By applying a logical condition to the column of the training data corresponding
    to the `PAY_1` feature, and then taking the sum of this, we calculated the number
    of samples meeting this condition. Then, by dividing by the total number of samples,
    we converted this to a proportion. We can see that the proportion we directly
    calculated from the training data is equal to the proportion displayed in the
    left node following the first split in *Figure 5.2*.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过对训练数据中与`PAY_1`特征对应的列应用逻辑条件，然后计算满足该条件的样本数量，再除以样本总数，我们将其转换为比例。我们可以看到，从训练数据直接计算出的比例与*图
    5.2*中第一次分裂后的左节点显示的比例相等。
- en: Following the first split, the samples contained in each of the two nodes on
    the first level are split again. As further splits are made beyond the first split,
    smaller and smaller proportions of the training data will be assigned to any given
    node in the subsequent levels of a branch, as can be seen in *Figure 5.2*.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在第一次分裂之后，第一层中每个节点包含的样本会再次被分裂。随着进一步的分裂，树枝后续层级中任何给定节点的训练数据所占比例会越来越小，这一点可以在*图 5.2*中看到。
- en: Now we want to interpret the remaining lines of text in the nodes in *Figure
    5.2*. The lines starting with `value` give the class fractions of the response
    variable for the samples contained in each node. For example, in the top node,
    we see `value = [0.777, 0.223]`. These are simply the class fractions for the
    overall training set, which you can confirm in the following step.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们要解释节点中其余文本行的含义，这些节点出现在*图 5.2*中。以`value`开头的行给出了每个节点中样本的响应变量类别比例。例如，在顶部节点中，我们看到`value
    = [0.777, 0.223]`。这些只是整体训练集的类别比例，你可以在下一步中验证这些比例。
- en: 'Calculate the class fraction in the training set with this code:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码计算训练集中的类别比例：
- en: '[PRE19]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output should be as follows:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE20]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This is equal to the second member of the pair of numbers following `value`
    in the top node; the first number is simply one minus this, in other words, the
    fraction of negative training samples. In each subsequent node, the class fractions
    of the samples that are contained in that node are displayed. The class fractions
    are also how the nodes are colored: those with a higher proportion of the negative
    class than the positive class are orange, with darker orange signifying higher
    proportions, while those with a higher proportion of the positive class have a
    similar scheme using a blue color.'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这等同于在顶部节点中`value`后面那对数字的第二个数；第一个数字就是减去该数字后的结果，换句话说，就是负类训练样本的比例。在每个后续节点中，都会显示该节点中样本的类别比例。类别比例也决定了节点的颜色：负类比例高于正类的节点为橙色，较深的橙色表示比例越高，而正类比例较高的节点则采用类似的蓝色配色方案。
- en: Finally, the line starting with `class` indicates how the decision tree would
    make predictions from a given node, if that node were a leaf node. Decision trees
    for classification make predictions by determining which leaf node a sample will
    be sorted into, given the values of the features, and then predicting the class
    of the majority of the training samples in that leaf node. This strategy means
    that the tree structure and the class proportions in the leaf nodes are pieces
    of information that are needed to make a prediction.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，以`class`开头的行表示如果某个节点是叶节点，决策树如何根据给定的节点进行预测。分类的决策树通过确定样本根据特征值会被划分到哪个叶节点，然后预测该叶节点中大多数训练样本的类别来进行预测。这一策略意味着，树结构和叶节点中的类别比例是做出预测所需的信息。
- en: For example, if we've made no splits and we are forced to make a prediction
    knowing nothing but the class fractions for the overall training data, we will
    simply choose the majority class. Since most people don't default, the class on
    the top node is `Not defaulted`. However, the class fractions in the nodes of
    deeper levels are different, leading to different predictions. How does scikit-learn
    decide the structure of the tree? We'll discuss the training process in the following
    section.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，如果我们没有进行任何分裂，且只能在不知道其他信息的情况下，仅凭整体训练数据的类别比例做出预测，那么我们将选择多数类别。由于大多数人不会违约，顶部节点的类别为`未违约`。然而，深层节点中的类别比例不同，导致不同的预测。scikit-learn是如何决定树的结构的呢？我们将在接下来的部分讨论训练过程。
- en: '**Importance of max_depth**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**max_depth的重要性**'
- en: 'Recall that the only hyperparameter we specified in this exercise was `max_depth`,
    that is, the maximum depth to which the decision tree can be grown during the
    model training process. It turns out that this is one of the most important hyperparameters.
    Without placing a limit on the depth, the tree will be grown until one of the
    other limitations, specified by other hyperparameters, takes effect. This can
    lead to very deep trees, with very many nodes. For example, consider how many
    leaf nodes there could be in a tree with a depth of 20\. This would be *220* leaf
    nodes, which is over 1 million! Do we even have 1 million training samples to
    sort into all these nodes? In this case, we do not. It would clearly be impossible
    to grow such a tree, with every node before the final level being split, using
    this training data. However, if we remove the `max_depth` limit and rerun the
    model training of this exercise, observe the effect:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在本练习中我们指定的唯一超参数是`max_depth`，也就是在模型训练过程中决策树可以生长的最大深度。事实证明，这是最重要的超参数之一。如果没有对深度进行限制，树将继续生长，直到其他由其他超参数指定的限制起作用。这可能导致非常深的树，并且节点数量非常多。例如，考虑一棵深度为20的树，它可能有多少叶节点呢？这将是*220*个叶节点，超过100万个！我们甚至有足够的训练样本来将所有这些节点填充吗？在这种情况下，我们没有。显然，使用这些训练数据生长这样的树是不可能的，因为在最终层之前的每个节点都会被分裂。然而，如果我们移除`max_depth`限制并重新运行本练习的模型训练，观察效果：
- en: '![Figure 5.3: A portion of the decision tree grown with no maximum depth'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3：没有最大深度限制的决策树的一部分](img/B16392_05_03.jpg)'
- en: '](img/B16392_05_03.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16392_05_03.jpg)'
- en: 'Figure 5.3: A portion of the decision tree grown with no maximum depth'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：没有最大深度限制的决策树的一部分
- en: Here, we have shown a portion of the decision tree that is grown with the default
    options, which include `max_depth=None`, meaning no limit in terms of the depth
    of the tree. The entire tree is about twice as wide as the portion shown here.
    There are so many nodes that they only appear as very small orange or blue patches;
    the exact interpretation of each node is not important as we are just trying to
    illustrate how large trees can potentially be. It should be clear that without
    hyperparameters to govern the tree-growing process, extremely large and complex
    trees may result.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了一个使用默认选项生成的决策树的一部分，默认选项包括`max_depth=None`，意味着树的深度没有限制。整个树大约是这里展示部分的两倍宽。树的节点非常多，以至于它们只作为非常小的橙色或蓝色斑点出现；每个节点的具体解释并不重要，因为我们只是想说明树的规模可能会非常大。可以清楚地看出，如果没有超参数来控制树的生长过程，可能会生成极其庞大且复杂的树。
- en: 'Training Decision Trees: Node Impurity'
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练决策树：节点不纯度
- en: At this point, you should have an understanding of how a decision tree makes
    predictions using features, and the class fractions of training samples in the
    leaf nodes. Now, we will learn how decision trees are trained. The training process
    involves selecting features to split nodes on, and the thresholds at which to
    make splits, for example `PAY_1 <= 1.5` for the first split in the tree of the
    previous exercise. Computationally, this means the samples in each node must be
    sorted on the values of each feature to consider a split for, and splits between
    each successive pair of sorted feature values are considered. All features may
    be considered, or only a subset as we will learn about shortly.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，你应该已经了解了决策树是如何利用特征进行预测的，以及叶节点中训练样本的类别分布。现在，我们将学习决策树是如何训练的。训练过程涉及选择特征来对节点进行分裂，并决定分裂的阈值，例如在前面练习中的树的第一次分裂是`PAY_1
    <= 1.5`。从计算角度来看，这意味着每个节点中的样本必须根据每个特征的值进行排序，以考虑分裂，并且在排序后的特征值之间的每对连续值都会被考虑作为潜在的分裂点。所有特征都可以被考虑，或者如我们稍后将要学习的那样，仅考虑一部分特征。
- en: '**How Are the Splits Decided during the Training Process?**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**在训练过程中，如何决定分裂？**'
- en: Given that the method of prediction is to take the majority class of a leaf
    node, it makes sense that we'd like to find leaf nodes that are primarily from
    one class or the other; choosing the majority class will be a more accurate prediction,
    the closer a node is to containing just one class. In the perfect case, the training
    data can be split so that every leaf node contains entirely positive or entirely
    negative samples. Then, we will have a high level of confidence that a new sample,
    once sorted into one of these nodes, will be either positive or negative. In practice,
    this rarely, if ever, happens. However, this illustrates the goal of training
    decision trees – that is, to make splits so that the next two nodes after the
    split have a higher **purity**, or, in other words, are closer to containing either
    only positive or only negative samples.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预测方法是选择叶节点的多数类，因此我们希望找到主要来自某一类的叶节点；选择多数类将是更准确的预测，节点越接近只包含某一类，其预测越准确。在理想情况下，训练数据可以被划分，使得每个叶节点完全包含正类或完全包含负类样本。然后，我们就可以高信心地认为，一旦新样本被分配到其中一个节点，它将是正类或负类。然而，在实践中，这种情况很少发生，几乎不会发生。然而，这说明了训练决策树的目标——也就是做出分裂，使得分裂后的两个节点具有更高的
    **纯度**，换句话说，更接近只包含正类或负类样本。
- en: 'In practice, decision trees are actually trained using the inverse of purity,
    or **node impurity**. This is some measure of how far the node is from having
    100% of the training samples belonging to one class and is analogous to the concept
    of a cost function, which signifies how far a given solution is from a theoretical
    perfect solution. The most intuitive concept of node impurity is the **misclassification
    rate**. Adopting a widely used notation (for example, [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html))
    for the proportion of samples in each node belonging to a certain class, we can
    define *p*mk as the proportion of samples belonging to the *k*th class in the
    *m*th node. In a binary classification problem, there are only two classes: *k*
    = 0 and *k* = 1\. For a given node *m*, the misclassification rate is simply the
    proportion of the less common class in that node, since all these samples will
    be misclassified when the majority class in that node is taken as the prediction.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，决策树实际上是使用纯度的逆，即 **节点不纯度** 进行训练的。这是衡量节点中训练样本距离完全属于某一类的程度的一个指标，类似于代价函数的概念，表示给定解决方案与理论上完美解决方案的差距。节点不纯度的最直观概念是
    **误分类率**。采用广泛使用的符号（例如， [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html)）表示每个节点中属于某一类的样本比例，我们可以定义
    *p*mk 为第 *m* 个节点中属于第 *k* 类的样本比例。在二分类问题中，只有两类：*k* = 0 和 *k* = 1。对于给定的节点 *m*，误分类率就是该节点中较少见类别的比例，因为当该节点中的多数类作为预测类别时，所有这些样本都会被误分类。
- en: 'Let''s visualize the misclassification rate as a way to start thinking about
    how decision trees are trained. Programmatically, we consider possible class fractions,
    *p*m0, between 0.01 and 0.99 of the negative class, *k* = 0, in a node, *m*, using
    NumPy''s `linspace` function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将误分类率可视化，作为开始思考决策树训练方式的一种方法。在程序中，我们使用 NumPy 的 `linspace` 函数考虑负类 *k* = 0 在节点
    *m* 中可能的类比例 *p*m0，范围从 0.01 到 0.99：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, the fraction of the positive class for this node is one minus *p*m0:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这个节点的正类比例是 1 减去 *p*m0：
- en: '![Figure 5.4: Equation for calculating the positive class fraction for node
    m0'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.4：计算节点 m0 的正类比例的公式'
- en: '](img/B16392_05_04.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16392_05_04.jpg)'
- en: 'Figure 5.4: Equation for calculating the positive class fraction for node m0'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：计算节点 m0 的正类比例的公式
- en: 'Now, the misclassification rate for this node will be whatever the smaller
    class fraction is, between *p*m0 and *p*m1\. We can find the smaller of the corresponding
    elements between two arrays with the same shape in NumPy by using the `minimum` function:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个节点的误分类率将是 *p*m0 和 *p*m1 之间较小的类比例。我们可以使用 NumPy 的 `minimum` 函数来找到两个形状相同的数组中对应元素的较小值：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: What does the misclassification rate look like plotted against the possible
    class fractions of the negative class?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 误分类率与负类可能的类比例绘制出来是什么样的？
- en: 'We can plot this using the following code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码绘制这个图：
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You should obtain this graph:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到这个图：
- en: '![Figure 5.5: The misclassification rate for a node'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.5：一个节点的误分类率'
- en: '](img/B16392_05_05.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16392_05_05.jpg)'
- en: 'Figure 5.5: The misclassification rate for a node'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5：节点的误分类率
- en: Now, it's clear that the closer the class fraction of the negative class, *p*m0,
    is to 0 or 1, the lower the misclassification rate will be. How is this information
    used when growing decision trees? Consider the process that might be followed.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，很明显，负类的类分数*p*m0越接近0或1，误分类率就越低。那么在构建决策树时，如何利用这些信息呢？考虑一下可能遵循的过程。
- en: Every time a node is split when growing a decision tree, two new nodes are created.
    Since the prediction from either of these new nodes is simply the majority class,
    an important goal will be to reduce the misclassification rate. Therefore, we
    will want to find a feature, from all the possible features, and a value of this
    feature at which to make a cut point, so that the misclassification rate in the
    two new nodes will be as low as possible when averaging over all the classes.
    This is very close to the actual process that is used to train decision trees.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 每次在构建决策树时进行节点划分时，都会创建两个新节点。由于这两个新节点的预测值只是多数类，因此一个重要的目标是减少误分类率。因此，我们需要找到一个特征和该特征的一个值作为切分点，使得在所有类别上取平均后，两个新节点的误分类率尽可能低。这与实际训练决策树时使用的过程非常接近。
- en: 'Continuing for the moment with the idea of minimizing the misclassification
    rate, the decision tree training algorithm goes about node splitting by considering
    all the features, although the algorithm may possibly only consider a randomly
    selected subset if you set the `max_features` hyperparameter to anything less
    than the total number of features. We''ll discuss possible reasons for doing this
    later. In either case, the algorithm then considers each possible threshold for
    every candidate feature and chooses the one that results in the lowest impurity,
    calculated as the average impurity across the two possible new nodes, weighted
    by the number of samples in each node. The node splitting process is shown in
    *Figure 5.6*. This process is repeated until a stopping criterion of the tree,
    such as `max_depth`, is reached:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 继续讨论最小化误分类率的思路，决策树训练算法通过考虑所有特征进行节点划分，尽管如果你将`max_features`超参数设置为少于特征总数的值，算法可能只会考虑一个随机选择的特征子集。稍后我们将讨论为什么要这么做。在任何情况下，算法会考虑每个候选特征的所有可能阈值，并选择那个能使得不纯度最低的阈值，不纯度的计算方式是通过加权每个节点的样本数量，计算两个新节点的平均不纯度。节点划分过程如*图
    5.6*所示。该过程会一直重复，直到树的停止准则（如`max_depth`）达到：
- en: '![Figure 5.6: How to select a feature and threshold in order to split a node'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.6：如何选择特征和阈值来划分节点'
- en: '](img/B16392_05_06.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16392_05_06.jpg)'
- en: 'Figure 5.6: How to select a feature and threshold in order to split a node'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：如何选择特征和阈值来划分节点
- en: While the misclassification rate is an intuitive measure of impurity, it happens
    that there are better measures that can be used to find splits during the model
    training process. The two options that are available in scikit-learn for the impurity
    calculation, which you can specify with the `criterion` keyword argument, are
    the **Gini impurity** and the **cross-entropy** options. Here, we will describe
    these options mathematically and show how they compare with the misclassification
    rate.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然误分类率是一个直观的衡量不纯度的指标，但实际上还有更好的指标可以用来在模型训练过程中找到最佳分裂。scikit-learn 提供了两种可供选择的计算不纯度的方法，你可以通过`criterion`关键字参数来指定，分别是**基尼不纯度**和**交叉熵**。在这里，我们将从数学上描述这些方法，并展示它们与误分类率的比较。
- en: 'Gini impurity is calculated for a node *m* using the following formula:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度通过以下公式计算节点*m*的不纯度：
- en: '![Figure 5.7: Equation for calculating Gini impurity'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7：计算基尼不纯度的公式'
- en: '](img/B16392_05_07.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16392_05_07.jpg)'
- en: 'Figure 5.7: Equation for calculating Gini impurity'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：计算基尼不纯度的公式
- en: 'Here, the summation is taken over all classes. In the case of a binary classification
    problem, there are only two classes, and we can write this programmatically as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，求和是对所有类别进行的。在二分类问题中，只有两个类别，我们可以像下面这样编写程序：
- en: '[PRE24]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Cross-entropy is calculated using this formula:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵通过以下公式计算：
- en: '![Figure 5.8: Equation for calculating cross-entropy'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.8：计算交叉熵的公式'
- en: '](img/B16392_05_08.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16392_05_08.jpg)'
- en: 'Figure 5.8: Equation for calculating cross-entropy'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：计算交叉熵的公式
- en: 'Using this code, we can calculate the cross-entropy:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这段代码，我们可以计算交叉熵：
- en: '[PRE25]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In order to add Gini impurity and cross-entropy to our plot of misclassification
    rate and see how they compare, we just need to include the following lines of
    code after we plot the misclassification rate:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将 Gini 不纯度和交叉熵添加到我们的误分类率图中并查看它们的比较，我们只需要在绘制误分类率后添加以下代码行：
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The final plot should appear as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的图形应如下所示：
- en: '![Figure 5.9: The misclassification rate, Gini impurity, and cross-entropy'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.9：误分类率、Gini 不纯度和交叉熵'
- en: '](img/B16392_05_09.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16392_05_09.jpg)'
- en: 'Figure 5.9: The misclassification rate, Gini impurity, and cross-entropy'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：误分类率、Gini 不纯度和交叉熵
- en: Note
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you''re reading the print version of this book, you can download and browse
    the color versions of some of the images in this chapter by visiting the following
    link:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在阅读本书的纸质版，你可以通过访问以下链接下载并浏览本章中某些图像的彩色版本：
- en: '[https://packt.link/mQ4Xn](https://packt.link/mQ4Xn)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mQ4Xn](https://packt.link/mQ4Xn)'
- en: Like the misclassification rate, both the Gini impurity and cross-entropy are
    highest when the class fractions are equal at 0.5, and they decrease as the node
    becomes purer – in other words, when they contain a higher proportion of just
    one of the classes. However, the Gini impurity is somewhat steeper than the misclassification
    rate in certain regions of the class fraction, which enables it to more effectively
    find the best split. Cross-entropy looks even steeper. So, which one is better
    for your work? This is the kind of question that does not have a concrete answer
    across all datasets. You should consider both impurity metrics in a cross-validation
    search for hyperparameters in order to determine the appropriate one. Note that
    in scikit-learn, Gini impurity can be specified with the `criterion` argument
    using the `'gini'` string, while cross-entropy is just referred to as `'entropy'`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与误分类率类似，Gini 不纯度和交叉熵在类别比例为 0.5 时最大，随着节点变得更加纯净——换句话说，当它们包含的类别比例更高时——它们会逐渐降低。然而，Gini
    不纯度在某些类别比例区域比误分类率变化得更陡峭，这使得它能够更有效地找到最佳分裂点。交叉熵看起来变化得更加陡峭。那么，哪一个更适合你的工作呢？这是一个在所有数据集上都没有明确答案的问题。你应该在交叉验证超参数的过程中同时考虑这两种不纯度度量，以确定最合适的一个。需要注意的是，在
    scikit-learn 中，Gini 不纯度可以通过 `criterion` 参数使用 `'gini'` 字符串来指定，而交叉熵则简单地称为 `'entropy'`。
- en: 'Features Used for the First Splits: Connections to Univariate Feature Selection
    and Interactions'
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于第一次分裂的特征：与单变量特征选择和交互的关系
- en: We can begin to get an impression of how important various features are to decision
    tree models, based on the small tree shown in *Figure 5.2*. Notice that `PAY_1`
    was the feature chosen for the first split. This means that it was the best feature
    in terms of decreasing node impurity on the node containing all of the training
    samples. Recall our experience with univariate feature selection in *Chapter 3*,
    *Details of Logistic Regression and Feature Exploration*, where `PAY_1` was the
    top-selected feature from the F-test. So, the appearance of this feature in the
    first split of the decision tree makes sense given our previous analysis.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据*图 5.2*所示的小树开始了解不同特征对决策树模型的重要性。注意，`PAY_1`是第一次分裂时选择的特征。这意味着它是在减少包含所有训练样本的节点不纯度方面表现最好的特征。回想我们在*第
    3 章*“逻辑回归和特征探索的细节”中的单变量特征选择经验，其中`PAY_1`是通过 F 检验选出的最佳特征。因此，考虑到我们之前的分析，`PAY_1`出现在决策树的第一次分裂中是合理的。
- en: In the second level of the tree, there is another split on `PAY_1`, as well
    as a split on `BILL_AMT_1`. `BILL_AMT_1` was not listed among the top features
    in univariate feature selection. However, it may be that there is an important
    interaction between `BILL_AMT_1` and `PAY_1`, which would not be found up by univariate
    methods. In particular, from the splits chosen by the decision tree, it seems
    that those accounts with both a value of 2 or greater for `PAY_1`, and a `BILL_AMT_1`
    of greater than 568, are especially at risk of default. This combined effect of
    `PAY_1` and `BILL_AMT_1` is an interaction and may also be why we were able to
    improve logistic regression performance by including interaction terms in the
    activity of the previous chapter.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在树的第二层，`PAY_1`上有另一个分裂，同时也有在`BILL_AMT_1`上的分裂。`BILL_AMT_1`在单变量特征选择中并没有列为重要特征。然而，可能是`BILL_AMT_1`与`PAY_1`之间存在一个重要的交互作用，而这种交互作用在单变量方法中无法发现。特别是，从决策树选择的分裂来看，似乎那些`PAY_1`值为2或更大的账户，并且`BILL_AMT_1`大于568的账户，尤其容易违约。`PAY_1`和`BILL_AMT_1`的这种组合效应是一种交互作用，这也可能是我们通过在前一章的活动中包含交互项来改善逻辑回归性能的原因。
- en: 'Training Decision Trees: A Greedy Algorithm'
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练决策树：一种贪婪算法
- en: There is no guarantee that a decision tree trained by the process described
    previously will be the best possible decision tree for finding leaf nodes with
    the lowest impurity. This is because the algorithm used to train decision trees
    is what is called a greedy algorithm. In this context, this means that at each
    opportunity to split a node, the algorithm is looking for the best possible split
    at that point in time, without any regard for the fact that the opportunities
    for later splits are being affected.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 没有保证通过前述过程训练得到的决策树是找到最低不纯度叶节点的最佳决策树。这是因为训练决策树所使用的算法是一种所谓的贪婪算法。在这种情况下，这意味着在每次分裂节点的机会中，算法都会寻找当前时刻最佳的分裂，而不会考虑后续分裂机会受到影响的事实。
- en: 'For example, consider the following hypothetical scenario: the best initial
    split for the training data of the case study involves `PAY_1`, as we''ve seen
    in *Figure 5.2*. But what if we instead split on `BILL_AMT_1`, and then make subsequent
    splits on `PAY_1` in the next level? Even though the initial split on `BILL_AMT_1`
    is not the best one available at first, it is possible that the end result will
    be better if the tree is grown this way. The algorithm has no way of finding solutions
    like this if they exist, since it only considers the best possible split at each
    node and not possible future splits.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下假设场景：案例研究的训练数据的最佳初始分裂涉及`PAY_1`，正如我们在*图 5.2*中所看到的。但是，如果我们改为在`BILL_AMT_1`上进行分裂，然后在下一级做`PAY_1`的后续分裂呢？即使初始在`BILL_AMT_1`上的分裂不是最优的，最终结果可能会更好，如果树是这样生长的。如果存在这样的解决方案，算法是无法找到的，因为它只考虑每个节点的最佳分裂，而不考虑未来可能的分裂。
- en: The reason why we still use greedy tree-growing algorithms is that it takes
    substantially longer to consider all possible splits in a way that enables the
    truly optimal tree to be found. Despite this shortcoming of the decision tree
    training process, there are methods that you can use to reduce the possible harmful
    effects of the greedy algorithm. Instead of searching for the best split at each
    node, the `splitter` keyword argument to the decision tree class can be set to
    `random` in order to choose a random feature to make a split on. However, the
    default is `best`, which searches all features for the best split. Another option,
    which we've already discussed, is to limit the number of features that will be
    searched at each splitting opportunity using the `max_features` keyword. Finally,
    you can also use ensembles of decision trees, such as random forests, which we
    will describe shortly. Note that all these options, in addition to possibly avoiding
    the ill-effects of the greedy algorithm, are also options for addressing the overfitting
    that decision trees are often criticized for.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然使用贪心的树生长算法的原因是，考虑所有可能的分裂方式需要的时间相当长，这样才能找到真正最优的树。尽管决策树训练过程中有这一缺陷，但仍然有方法可以减少贪心算法可能带来的不利影响。你可以将`splitter`关键字参数设置为`random`，以便在每个节点选择一个随机特征进行分裂，而不是寻找最优的分裂。默认值是`best`，它会搜索所有特征以找到最佳分裂。另一个我们已经讨论过的选项是，通过`max_features`关键字限制在每次分裂时将搜索的特征数量。最后，你还可以使用决策树的集成方法，如随机森林，我们稍后会介绍。请注意，所有这些选项除了可能避免贪心算法的副作用外，也是解决决策树常被批评的过拟合问题的选项。
- en: 'Training Decision Trees: Different Stopping Criteria and Other Options'
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练决策树：不同的停止标准与其他选项
- en: We have already reviewed using the `max_depth` parameter as a limit to how deep
    a tree will grow. However, there are several other options available in scikit-learn
    as well. These are mainly related to how many samples are present in a leaf node,
    or how much the impurity can be decreased by further splitting nodes. As discussed
    previously, you may be limited by the size of your dataset in terms of how deep
    you can grow a tree. And it may not make sense to grow trees deeper, especially
    if the splitting process is no longer finding nodes with substantially higher
    purity.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经回顾了使用`max_depth`参数来限制树的生长深度。然而，scikit-learn中还有几个其他可选项。这些选项主要与叶子节点中样本的数量相关，或者进一步分裂节点时如何减少不纯度。如前所述，树的生长深度可能会受到数据集大小的限制。如果分裂过程不再找到具有显著更高纯度的节点，那么继续加深树的深度可能没有意义。
- en: 'We summarize all of the keyword arguments that you can supply to the `DecisionTreeClassifier`
    class in scikit-learn here:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里总结了你可以提供给`DecisionTreeClassifier`类的所有关键字参数，适用于scikit-learn：
- en: '![Figure 5.10: The complete list of options for the decision tree classifier
    in scikit-learn'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.10：scikit-learn中决策树分类器的完整选项列表'
- en: '](img/B16392_05_10.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16392_05_10.jpg)'
- en: 'Figure 5.10: The complete list of options for the decision tree classifier
    in scikit-learn'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10：scikit-learn中决策树分类器的完整选项列表
- en: 'Using Decision Trees: Advantages and Predicted Probabilities'
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用决策树：优势与预测概率
- en: While decision trees are simple in concept, they have several practical advantages.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管决策树在概念上很简单，但它们具有多个实际优势。
- en: '**No Need to Scale Features**'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**无需缩放特征**'
- en: Consider the reasons why we needed to scale features for logistic regression.
    One reason is that, for some of the solution algorithms based on gradient descent,
    it is necessary that the features are on the same scale in order to quickly find
    a minimum of the cost function. Another is that when we are using L1 or L2 regularization
    to penalize coefficients, all the features must be on the same scale so that they
    are penalized equally. With decision trees, the node splitting algorithm considers
    each feature individually and, therefore, it doesn't matter whether the features
    are on the same scale.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们为什么需要对特征进行缩放以应用逻辑回归。一个原因是，对于一些基于梯度下降的解决算法，特征必须在相同的尺度上，以便快速找到成本函数的最小值。另一个原因是，当我们使用L1或L2正则化来惩罚系数时，所有特征必须在相同的尺度上，这样才能均等地对它们进行惩罚。而对于决策树，节点分裂算法会单独考虑每个特征，因此特征是否在相同尺度上并不重要。
- en: '**Non-Linear Relationships and Interactions**'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**非线性关系与交互作用**'
- en: 'Because each successive split in a decision tree is performed on a subset of
    the training samples resulting from the previous split(s), decision trees can
    describe complex non-linear relationships of a single feature, as well as interactions
    between features. Consider our discussion previously in the *Features Used for
    the First Splits: Connections to Univariate Feature Selection and Interactions*
    section. Also, as a hypothetical example with synthetic data, consider the following
    dataset for classification:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因为决策树中的每个后续分裂都是在先前分裂产生的训练样本子集上进行的，所以决策树能够描述单个特征的复杂非线性关系，以及特征之间的交互作用。回想我们之前在*首次分裂所使用的特征：与单变量特征选择和交互作用的关联*部分中的讨论。另外，作为一个假设的例子，考虑以下分类的合成数据集：
- en: '![Figure 5.11: An example classification dataset, with the classes shown in
    red and blue (if reading in black and white, please refer to the GitHub repository
    for a color version of this figure; the blue dots are on the inside circle)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.11：一个示例分类数据集，类别以红色和蓝色显示（如果是黑白阅读，请参阅 GitHub 仓库获取该图的彩色版本；蓝色点位于内圆圈内）'
- en: '](img/B16392_05_11.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16392_05_11.jpg)'
- en: 'Figure 5.11: An example classification dataset, with the classes shown in red
    and blue (if reading in black and white, please refer to the GitHub repository
    for a color version of this figure; the blue dots are on the inside circle)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：一个示例分类数据集，类别以红色和蓝色显示（如果是黑白阅读，请参阅 GitHub 仓库获取该图的彩色版本；蓝色点位于内圆圈内）
- en: We know from *Chapter 3*, *Details of Logistic Regression and Feature Exploration*,
    that logistic regression has a linear decision boundary. So, how do you think
    logistic regression would cope with a dataset like that shown in *Figure 5.11*?
    Where would you draw a line to separate the blue and red classes? It should be
    clear that without engineering additional features, a logistic regression is not
    likely to be a good classifier for this data. Now think about the set of "if,
    then" rules of a decision tree, which could be used with the features represented
    on the *x* and *y* axes of *Figure 5.11*. Do you think a decision tree will be
    effective with this data?
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从*第 3 章*，*逻辑回归和特征探索的详细信息*中了解到，逻辑回归具有线性决策边界。那么，你认为逻辑回归如何处理像*图 5.11*中展示的数据集呢？你会在哪里画一条线来分隔蓝色和红色类别？应该很明显，在没有额外工程特征的情况下，逻辑回归不太可能是这个数据的好分类器。现在想一想决策树的“如果，那么”的规则，它可以与*图
    5.11*中*X*和*Y*轴上表示的特征一起使用。你认为决策树对这组数据有效吗？
- en: 'Here, we plot in the background the predicted probabilities of class membership
    using red and blue, for both of these models:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们在背景中绘制了这两个模型的类别成员预测概率，使用红色和蓝色表示：
- en: '![Figure 5.12: Decision tree and logistic regression predictions'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.12：决策树和逻辑回归预测'
- en: '](img/B16392_05_12.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16392_05_12.jpg)'
- en: 'Figure 5.12: Decision tree and logistic regression predictions'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12：决策树和逻辑回归预测
- en: In *Figure 5.12*, the predicted probabilities for both models are colored so
    that darker red corresponds to a higher predicted probability for the red class,
    and darker blue for the blue class. We can see that the decision tree can isolate
    the blue class in the middle of the circle of red points. This is because, by
    using thresholds for the *x* and *y* coordinates in the node-splitting process,
    a decision tree can mathematically model the fact that the location of the blue
    and red classes depends on both the *x* and *y* coordinates together (interactions),
    and that the likelihood of either class is not a linearly increasing or decreasing
    function of *x* or *y* (non-linearities). Consequently, the decision tree approach
    is able to get most classifications right.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 5.12*中，两个模型的预测概率已经上色，深红色表示红色类别的较高预测概率，深蓝色表示蓝色类别的较高预测概率。我们可以看到，决策树能够将蓝色类别从红色点的中间圈分隔出来。这是因为，通过在节点分裂过程中使用*X*和*Y*坐标的阈值，决策树可以在数学上模拟蓝色和红色类别的位置依赖于*X*和*Y*坐标（交互作用），并且每个类别的可能性不是*X*或*Y*的线性增减函数（非线性）。因此，决策树方法能够正确分类大多数数据。
- en: Note
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The code to generate *Figures 5.11* and *5.12* can be found in the reference notebook:
    [https://packt.link/9W4WN](https://packt.link/9W4WN).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 生成*图 5.11*和*图 5.12*的代码可以在参考笔记本中找到：[https://packt.link/9W4WN](https://packt.link/9W4WN)。
- en: However, the logistic regression has a linear decision boundary, which will
    be the straight line between the lightest blue and red patches in the background.
    The logistic regression decision boundary goes right through the middle of the
    data and doesn't provide a useful classifier. This shows the power of decision
    trees "out of the box," without the need for engineering non-linear or interaction
    features.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，逻辑回归具有线性决策边界，这将是背景中最浅蓝色和红色区域之间的直线。逻辑回归的决策边界穿过数据的中间，并未提供一个有效的分类器。这展示了决策树“开箱即用”的强大功能，而无需工程化非线性或交互特征。
- en: '**Predicted Probabilities**'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测概率**'
- en: We know that logistic regression produces probabilities as raw output. However,
    a decision tree makes predictions based on the majority class of the leaf nodes.
    So, where would predicted probabilities come from, like those shown in *Figure
    5.12*? In fact, decision trees do offer the `.predict_proba` method in scikit-learn
    to calculate predicted probabilities. The probability is based on the proportion
    of the majority class in the leaf node used for a given prediction. If 75% of
    the samples in a leaf node belonged to the positive class, for example, the prediction
    for that node would be the positive class and the predicted probability will be
    0.75\. The predicted probabilities from decision trees are not considered to be
    as statistically rigorous as those from generalized linear models, but they are
    still commonly used to measure the performance of models by methods that depend
    on varying the threshold for classification, such as the ROC curve or the precision-recall
    curve.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道逻辑回归输出的是概率。然而，决策树是根据叶节点中的多数类来做出预测的。那么，像*图5.12*中所示的预测概率从哪里来呢？实际上，决策树在scikit-learn中确实提供了`.predict_proba`方法来计算预测概率。该概率基于用于给定预测的叶节点中多数类的比例。例如，如果一个叶节点中75%的样本属于正类，那么该节点的预测将是正类，预测的概率将是0.75。来自决策树的预测概率在统计上不如广义线性模型的预测概率严谨，但它们仍然被广泛用于通过变化分类阈值来衡量模型性能的方法，如ROC曲线或精确度-召回曲线。
- en: Note
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We are focusing here on decision trees for classification because of the nature
    of the case study. However, decision trees can also be used for regression, making
    them a versatile method. The tree-growing process is similar for regression as
    it is for classification, except that instead of seeking to reduce node impurity,
    a regression tree seeks to minimize other metrics such as the **Mean Squared Error**
    (**MSE**) or **Mean Absolute Error** (**MAE**) of the predictions, where the prediction
    for a node may be the average or median of the samples in the node, respectively.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们专注于分类决策树，因为案例研究的性质。然而，决策树也可以用于回归，这使它成为一种多功能的方法。决策树的生长过程对于回归和分类是类似的，唯一的区别是，回归树不是寻求减少节点的不纯度，而是寻求最小化其他指标，如**均方误差**（**MSE**）或**平均绝对误差**（**MAE**），其中节点的预测可能是该节点中样本的平均值或中位数。
- en: A More Convenient Approach to Cross-Validation
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更便捷的交叉验证方法
- en: 'In *Chapter 4*, *The Bias-Variance Trade-Off*, we gained a deep understanding
    of cross-validation by writing our own function to do it, using the `KFold` class
    to generate the training and testing indices. This was helpful to get a thorough
    understanding of how the process works. However, scikit-learn offers a convenient
    class that can do more of the heavy lifting for us: `GridSearchCV`. `GridSearchCV`
    can take as input a model that we want to find optimal hyperparameters for, such
    as a decision tree or a logistic regression, and a "grid" of hyperparameters that
    we want to perform cross-validation over. For example, in a logistic regression,
    we may want to get the average cross-validation score over all the folds for different
    values of the regularization parameter, **C**. With decision trees, we may want
    to explore different depths of trees.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4章*，*偏差-方差权衡*中，我们通过编写自己的函数来进行交叉验证，深入理解了交叉验证的概念，使用`KFold`类来生成训练和测试索引。这对于全面理解这个过程如何工作非常有帮助。然而，scikit-learn提供了一个便捷的类，可以为我们做更多繁重的工作：`GridSearchCV`。`GridSearchCV`可以作为输入，接受我们想要寻找最优超参数的模型，如决策树或逻辑回归，以及我们希望进行交叉验证的超参数“网格”。例如，在逻辑回归中，我们可能希望获得不同正则化参数**C**值下，所有折叠的平均交叉验证得分。在决策树中，我们可能希望探索不同的树深度。
- en: You can also search multiple parameters at once, for example, if we wanted to
    try different depths of trees and different numbers of `max_features` to consider
    at each node split.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以一次性搜索多个参数，例如，如果我们想尝试不同的树深度和不同数量的`max_features`来考虑每个节点的分裂。
- en: '`GridSearchCV` does what is called an exhaustive grid search over all the possible
    combinations of parameters that we supply. This means that if we supplied five
    different values for each of the two hyperparameters, the cross-validation procedure
    would be run 5 x 5 = 25 times. If you are searching many values of many hyperparameters,
    the number of cross-validation runs can grow very quickly. In these cases, you
    may wish to use `RandomizedSearchCV`, which searches a random subset of hyperparameter
    combinations from the universe of all possibilities in the grid you supply.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV`执行的是所谓的“穷举式网格搜索”，遍历我们提供的所有可能的参数组合。这意味着，如果我们为每个超参数提供五个不同的值，那么交叉验证过程将运行
    5 x 5 = 25 次。如果你在搜索许多超参数的多个值时，交叉验证的运行次数会迅速增加。在这种情况下，你可能会想使用`RandomizedSearchCV`，它会从你提供的网格中搜索超参数组合的一个随机子集。 '
- en: '`GridSearchCV` can speed up your work by streamlining the cross-validation
    process. You should be familiar with the concepts of cross-validation from the
    previous chapter, so we proceed directly to listing all the options available
    for `GridSearchCV`.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV`通过简化交叉验证过程，可以加速你的工作。你应该已经了解了前一章中交叉验证的概念，因此我们直接列出`GridSearchCV`可用的所有选项。'
- en: 'In the following exercise, we will get hands-on practice using `GridSearchCV`
    with the case study data, in order to search hyperparameters for a decision tree
    classifier. Here are the options for `GridSearchCV`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将通过实际操作使用`GridSearchCV`，结合案例研究数据，来搜索决策树分类器的超参数。以下是`GridSearchCV`的选项：
- en: '![Figure 5.13: The options for GridSearchCV'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.13：GridSearchCV 的选项'
- en: '](img/B16392_05_13.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16392_05_13.jpg)'
- en: 'Figure 5.13: The options for GridSearchCV'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13：GridSearchCV 的选项
- en: In the following exercise, we'll make use of the **standard error of the mean**
    to create error bars. We'll average the model performance metric across the testing
    folds, and the error bars will help us visualize how variable model performance
    is across the folds.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将利用**均值的标准误差**来创建误差条。我们将对模型性能指标在测试折中的平均值进行计算，误差条将帮助我们可视化模型性能在各个折中的变化程度。
- en: 'The standard error of the mean is also known as the standard deviation of the
    sampling distribution of the sample mean. That is a long name, but the concept
    isn''t too complicated. The idea behind this is that the population of model performance
    metrics that we wish to make error bars for represents one possible way of sampling
    a theoretical, larger population of similar samples, for example if more data
    were available and we used it to have more testing folds. If we could take repeated
    samples from the larger population, each of these sampling events would result
    in a slightly different mean (the sample mean). Constructing a distribution of
    these means (the sampling distribution of the sample mean) from repeated sampling
    events would allow us to know the variance of this sampling distribution, which
    would be useful as a measure of uncertainty in the sample mean. It turns out this
    variance (let''s call it ![1](img/B16392_05_equation1.png), where ![2](img/B16392_05_equation2.png)
    indicates this is the variance of the sample mean) depends on the number of observations
    in our sample (n): it is inversely proportional to sample size, but also directly
    proportional to the variance of the larger, unobserved population ![3](img/B16392_05_equation3.png).
    If you''re working with standard deviation of the sample mean, simply take the
    square root of both sides: ![4](img/B16392_05_equation4.png). While we don''t
    know the true value of ![5](img/B16392_05_equation5.png) since we don''t observe
    the theoretical population, we can estimate it with the variance of the population
    of testing folds that we do observe.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 平均值的标准误差也被称为样本均值的抽样分布的标准差。这个名字很长，但概念并不复杂。其背后的思想是，我们希望为模型性能度量创建误差条的总体，代表了从一个理论上较大的类似样本群体中抽取样本的一种可能方式，例如如果有更多数据可用并用它进行更多的测试折叠。如果我们能从较大的总体中进行反复抽样，每次抽样事件会导致略微不同的均值（样本均值）。通过反复抽样事件构建这些均值的分布（样本均值的抽样分布）可以让我们知道这个抽样分布的方差，这将作为样本均值的不确定性度量。事实证明，这个方差（我们称之为![1](img/B16392_05_equation1.png)，其中![2](img/B16392_05_equation2.png)表示这是样本均值的方差）取决于我们样本中的观察次数（n）：它与样本大小成反比，但也与更大、未观察的总体方差![3](img/B16392_05_equation3.png)成正比。如果您在处理样本均值的标准差，简单地对两边取平方根：![4](img/B16392_05_equation4.png)。虽然我们不知道![5](img/B16392_05_equation5.png)的真实值，因为我们无法观察到理论总体，但我们可以通过观察到的测试折叠的总体方差来估计它。
- en: This is a key concept in statistics called the **Central Limit Theorem**.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这是统计学中的一个关键概念，称为**中心极限定理**。
- en: 'Exercise 5.02: Finding Optimal Hyperparameters for a Decision Tree'
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.02：为决策树寻找最优超参数
- en: 'In this exercise, we will use `GridSearchCV` to tune the hyperparameters for
    a decision tree model. You will learn about a convenient way of searching different
    hyperparameters with scikit-learn. Perform the following steps to complete the exercise:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用`GridSearchCV`来调优决策树模型的超参数。您将学习一种使用 scikit-learn 搜索不同超参数的便捷方法。请执行以下步骤来完成练习：
- en: Note
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Before you begin this exercise, you need import the necessary packages and
    load the cleaned dataframe. You can refer to the following Jupyter notebook for
    the prerequisite steps: [https://packt.link/SKuoB](https://packt.link/SKuoB).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本练习之前，您需要导入必要的包并加载已清理的数据框。您可以参考以下 Jupyter notebook 来了解前提步骤：[https://packt.link/SKuoB](https://packt.link/SKuoB)。
- en: 'Import the `GridSearchCV` class with this code:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码导入`GridSearchCV`类：
- en: '[PRE27]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The next step is to define the hyperparameters that we want to search using
    cross-validation. We will find the best maximum depth of tree, using the `max_depth`
    parameter. Deeper trees have more node splits, which partition the training set
    into smaller and smaller subspaces using the features. While we don't know the
    best maximum depth ahead of time, it is helpful to consider some limiting cases
    when considering the range of parameters to use for the grid search.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下一步是定义我们希望使用交叉验证进行搜索的超参数。我们将通过`max_depth`参数找到树的最佳最大深度。深度较大的树会有更多的节点分裂，这些分裂使用特征将训练集划分为越来越小的子空间。虽然我们无法预先知道最佳的最大深度，但在考虑用于网格搜索的参数范围时，考虑一些极限情况是有帮助的。
- en: We know that one is the minimum depth, consisting of a tree with just one split.
    As for the largest depth, you can consider how many samples you have in your training
    data, or, more appropriately in this case, how many samples will be in the training
    fold for each split of the cross-validation. We will perform a 4-fold cross-validation
    like we did in the previous chapter. So, how many samples will be in each training
    fold, and how does this relate to the depth of the tree?
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们知道，1是最小深度，由只有一个分割的树构成。至于最大深度，你可以考虑你的训练数据中有多少个样本，或者更适当地，在这种情况下，考虑交叉验证每次分割时，训练折叠中有多少个样本。我们将像上一章一样执行4折交叉验证。那么，每个训练折叠中将有多少样本，这与树的深度有什么关系？
- en: 'Find the number of samples in the training data using this code:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码查找训练数据中的样本数量：
- en: '[PRE28]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output should be as follows:'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE29]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: With 21,331 training samples and 4-fold cross-validation, there will be three-fourths
    of the samples, or about 16,000 samples, in each training fold.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在21,331个训练样本和4折交叉验证的情况下，每个训练折叠中将有三分之四的样本，即大约16,000个样本。
- en: '`max_depth` hyperparameter. We will explore a range of depths from 1 up to
    12.'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`max_depth` 超参数。我们将探索从1到12的深度范围。'
- en: 'Define a dictionary with the key being the hyperparameter name and the value being
    the list of values of this hyperparameter that we want to search in cross-validation:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个字典，键为超参数名称，值为我们想要在交叉验证中搜索的该超参数的值列表：
- en: '[PRE30]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In this case, we are only searching one hyperparameter. However, you could define
    a dictionary with multiple key-value pairs to search over multiple hyperparameters
    simultaneously.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，我们只搜索一个超参数。然而，你可以定义一个字典，包含多个键值对，来同时搜索多个超参数。
- en: 'If you are running all the exercises for this chapter in a single notebook,
    you can reuse the decision tree object, `dt`, from earlier. If not, you need to
    create a decision tree object for the hyperparameter search:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你在一个笔记本中运行本章的所有练习，可以重用之前的决策树对象`dt`。如果没有，你需要为超参数搜索创建一个决策树对象：
- en: '[PRE31]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now we want to instantiate the `GridSearchCV` class.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们想要实例化`GridSearchCV`类。
- en: 'Instantiate the `GridSearchCV` class using these options:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些选项实例化`GridSearchCV`类：
- en: '[PRE32]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note here that we use the ROC AUC metric (`scoring='roc_auc'`), that we do 4-fold
    cross-validation `(cv=4`), and that we calculate training scores (`return_train_score=True`)
    to assess the bias-variance trade-off.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们使用的是ROC AUC指标（`scoring='roc_auc'`），进行4折交叉验证（`cv=4`），并计算训练分数（`return_train_score=True`）来评估偏差-方差权衡。
- en: Once the cross-validation object is defined, we can simply use the `.fit` method
    on it as we would with a model object. This encapsulates essentially all the functionality
    of the cross-validation loop we wrote in the previous chapter.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦交叉验证对象被定义，我们可以像使用模型对象一样，简单地对其使用`.fit`方法。这基本上封装了我们在上一章中编写的交叉验证循环的所有功能。
- en: 'Perform 4-fold cross-validation to search for the optimal maximum depth using
    this code:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码执行4折交叉验证，搜索最优的最大深度：
- en: '[PRE33]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output should be as follows:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![Figure 5.14: The cross-validation fitting output'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图5.14：交叉验证拟合输出'
- en: '](img/B16392_05_14.jpg)'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_05_14.jpg)'
- en: 'Figure 5.14: The cross-validation fitting output'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.14：交叉验证拟合输出
- en: All the options that we specified are printed as output. Additionally, there
    is some output information regarding how many cross-validation fits were performed.
    We had 4 folds and 7 hyperparameters, meaning 4 x 7 = 28 fits are performed. The
    amount of time this took is also displayed. You can control how much output you
    get from this procedure with the `verbose` keyword argument; larger numbers mean
    more output.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们指定的所有选项都会作为输出打印出来。此外，还有一些关于执行了多少次交叉验证拟合的输出信息。我们有4个折叠和7个超参数，意味着执行了4 x 7 = 28次拟合。还显示了这所花费的时间。你可以通过`verbose`关键字参数控制从该过程中获得的输出量；较大的数字意味着更多的输出。
- en: Now it's time to examine the results of the cross-validation procedure. Among
    the methods that are available on the fitted `GridSearchCV` object is `.cv_results_`.
    This is a dictionary containing the names of results as keys and the results themselves
    as values. For example, the `mean_test_score` key holds the average testing score
    across the folds for each of the seven hyperparameters. You could directly examine
    this output by running `cv.cv_results_` in a code cell. However, this is not easy
    to read. Dictionaries with this kind of structure can be used immediately in the
    creation of a pandas DataFrame, which makes looking at the results a little easier.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在是时候查看交叉验证过程的结果了。在已拟合的 `GridSearchCV` 对象上，有一个方法是 `.cv_results_`。这是一个字典，字典的键是结果的名称，值是结果本身。例如，`mean_test_score`
    键包含了每个超参数的平均测试得分。你可以通过在代码单元中运行 `cv.cv_results_` 来直接查看这个输出。然而，这样查看输出不太方便。具有这种结构的字典可以直接用于创建
    pandas DataFrame，这样查看结果会稍微容易一些。
- en: 'Run the following code to create and examine a pandas DataFrame of cross-validation
    results:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码来创建并查看一个 pandas DataFrame，该 DataFrame 显示了交叉验证的结果：
- en: '[PRE34]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output should look like this:'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![Figure 5.15: First several columns of the cross-validation results DataFrame'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.15：交叉验证结果 DataFrame 的前几列](img/B16392_05_15.jpg)'
- en: '](img/B16392_05_15.jpg)'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_05_15.jpg)'
- en: 'Figure 5.15: First several columns of the cross-validation results DataFrame'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.15：交叉验证结果 DataFrame 的前几列
- en: The DataFrame has one row for each combination of hyperparameters in the grid.
    Since we are only searching one hyperparameter here, there is one row for each
    of the seven values that we searched for. You can see a lot of output for each
    row, such as the mean and standard deviation of the time in seconds that each
    of the four folds took for both training (fitting) and testing (scoring). The
    hyperparameter values that were searched are also shown. In *Figure 5.16*, we
    can see the ROC AUC score for the testing data of the first fold (index 0). What
    are the rest of the columns in the results DataFrame?
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DataFrame 中每一行代表网格中每一组超参数的组合。由于我们这里只搜索一个超参数，因此每一行代表我们搜索的七个值之一。你可以看到每一行的输出信息，包括每一折训练（拟合）和测试（评分）所用时间的均值和标准差，单位为秒。搜索的超参数值也会显示出来。在*图
    5.16*中，我们可以看到第一折（索引 0）的测试数据的 ROC AUC 分数。那么结果 DataFrame 中其余的列包含了什么内容呢？
- en: 'View the names of the remaining columns in the results DataFrame using this code:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码查看结果 DataFrame 中剩余列的名称：
- en: '[PRE35]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output should be as follows:'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE36]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The columns in the cross-validation results DataFrame include the testing scores
    for each fold, their average and standard deviation, and the same information
    for the training scores.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 交叉验证结果 DataFrame 中的列包括每一折的测试得分、它们的平均值和标准差，以及训练得分的相同信息。
- en: Generally speaking, the "best" combination of hyperparameters is that with the
    highest average testing score. This is an estimation of how well the model, fitted
    using these hyperparameters, could perform when scored on new data. Let's make
    a plot showing how the average testing score varies with the `max_depth` hyperparameter.
    We will also show the average training scores on the same plot, to see how bias
    and variance change as we allow deeper and more complex trees to be grown during
    model fitting.
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一般来说，“最佳”超参数组合是具有最高平均测试得分的组合。这是对使用这些超参数拟合的模型，在新数据上评分时可能表现如何的估计。我们绘制一张图，展示平均测试得分如何随
    `max_depth` 超参数变化。我们还将在同一张图上展示平均训练得分，以查看随着我们允许在模型拟合过程中生长更深、更复杂的树，偏差和方差是如何变化的。
- en: We include the standard errors of the 4-fold training and testing scores as
    error bars, using the Matplotlib `errorbar` function. This gives you an indication
    of how variable the scores are across the folds.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将 4 折的训练和测试得分的标准误差作为误差条，使用 Matplotlib 的 `errorbar` 函数。这将显示得分在各折之间的变异情况。
- en: 'Execute the following code to create an error bar plot of training and testing
    scores for each value of `max_depth` that was examined in cross-validation:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下代码，创建一个关于每个 `max_depth` 值的训练和测试得分的误差条图，这些值是在交叉验证中进行检查的：
- en: '[PRE37]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The plot should appear as follows:'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该图应如下所示：
- en: '![Figure 5.16: An error bar plot of training and testing scores across the
    four folds'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.16：跨四折的训练和测试得分的误差条图](img/B16392_05_16.jpg)'
- en: '](img/B16392_05_16.jpg)'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_05_15.jpg)'
- en: 'Figure 5.16: An error bar plot of training and testing scores across the four
    folds'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16：跨四折的训练和测试得分的误差条图
- en: Note that standard errors are calculated as the standard deviation divided by
    the square root of the number of folds. The standard errors of the training and
    testing scores are shown as vertical lines at each value of `max_depth` that was
    tried; the distance above and below the average score is 1 standard error. Whenever
    making error bar plots, it's best to ensure that the units of the error measurement
    are the same as the units of the *y* axis. In this case they are, since standard
    error has the same units as the underlying data, as opposed to variance, for example,
    which has squared units.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，标准误差是通过标准差除以折叠数量的平方根来计算的。训练和测试评分的标准误差以垂直线的形式显示在每个尝试过的`max_depth`值处；平均分数上下的距离表示1个标准误差。在制作误差条图时，最好确保误差测量的单位与*y*轴的单位相同。在本例中，它们是相同的，因为标准误差的单位与底层数据相同，而不是像方差那样的平方单位。
- en: The error bars indicate how variable the scores are across folds. If there were
    a large amount of variation across the folds, it would indicate that the nature
    of the data across the folds was different in a way that affected the ability
    of our model to describe it. This could be concerning because it would indicate
    that we may not have enough data to train a model that would reliably perform
    on new data. However, in our case here, there is not much variability between
    the folds, so this is not an issue.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 错误条表示评分在不同折叠之间的变动情况。如果各个折叠之间的变异性很大，这表明数据在不同折叠之间的性质存在差异，从而影响了我们模型的拟合能力。这可能是个问题，因为这意味着我们可能没有足够的数据来训练一个能在新数据上稳定表现的模型。然而，在我们这里，折叠之间的变异性并不大，因此这不是问题。
- en: What about the general trends of the training and testing scores across the
    different values of `max_depth`? We can see that as we grow deeper and deeper
    trees, the model fits the training data better and better. As noted previously,
    if we grew trees deep enough so that each leaf node had just one training sample,
    we would create a model that is very specific to the training data. In fact, it
    would fit the training data perfectly. We could say that such a model had extremely
    high **variance**.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，不同`max_depth`值下训练和测试评分的总体趋势如何呢？我们可以看到，随着树的深度越来越大，模型对训练数据的拟合越来越好。如前所述，如果我们把树长得足够深，使得每个叶节点只有一个训练样本，我们将会创建一个非常针对训练数据的模型。事实上，它将完美地拟合训练数据。我们可以说，这样的模型具有极高的**方差**。
- en: But this performance on the training set does not necessarily translate over
    to the testing set. In *Figure 5.16* it's apparent that increasing `max_depth`
    only increases testing scores up to a point, after which deeper trees in fact
    have lower testing performance. This is another example of how we can leverage
    the **bias-variance trade-off** to create a better predictive model – similar
    to how we used a regularized logistic regression. Shallower trees have more **bias**,
    since they are not fitting the training data as well. But this is fine because
    if we accept some bias, we will have better performance on the testing data, which
    is the metric we ultimately care about.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，训练集上的表现不一定会转化到测试集上。在*图5.16*中，我们可以明显看到，增加`max_depth`只会在某个点之前提高测试评分，而超过该点后，树的深度增加反而导致测试表现下降。这是另一个例子，说明我们如何利用**偏差-方差权衡**来创建更好的预测模型——类似于我们使用正则化的逻辑回归。较浅的树具有更高的**偏差**，因为它们无法很好地拟合训练数据。但这是可以接受的，因为如果我们接受一定的偏差，我们将在测试数据上获得更好的表现，而测试数据才是我们最终关心的指标。
- en: In this case, we would select `max_depth` = 6\. You could also perform a more
    thorough search by trying every integer between 2 and 12, instead of going by
    2s, as we've done here. In general, it is a good idea to perform as thorough a
    search of parameter space as you can, up to the limits of the computational time
    that you have. In this case, it would lead to the same result.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们选择`max_depth` = 6。你也可以通过尝试2到12之间的每个整数来进行更彻底的搜索，而不是像我们这里那样每次跳2个值。一般来说，最好尽可能深入地探索参数空间，直到你有的计算时间为止。在本例中，这将导致相同的结果。
- en: '**Comparison between Models**'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型比较**'
- en: At this point, we've calculated a 4-fold cross-validation of several different
    machine learning models on the case study data. So, how are we doing? What's our
    best so far? In the last chapter, we got an average testing ROC AUC of 0.718 with
    logistic regression, and 0.740 by engineering interaction features in a logistic
    regression. Here, with a decision tree, we can achieve 0.745\. So, we are making
    gains in model performance. Now, let's, explore another type of model, based on
    decision tress, to see whether we can push performance even higher.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经对案例研究数据进行了几种不同机器学习模型的4折交叉验证。那么，我们的表现如何呢？到目前为止，我们的最佳表现是什么？在上一章中，我们使用逻辑回归得到了平均测试ROC
    AUC为0.718，通过在逻辑回归中工程化交互特征得到了0.740。而在这里，使用决策树，我们可以达到0.745。所以，我们在模型性能上有所提升。现在，让我们探索另一种基于决策树的模型，看看是否能够进一步提高性能。
- en: 'Random Forests: Ensembles of Decision Trees'
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林：决策树的集成
- en: As we saw in the previous exercise, decision trees are prone to overfitting.
    This is one of the principal criticisms of their usage, despite the fact that
    they are highly interpretable. We were able to limit this overfitting, to an extent,
    however, by limiting the maximum depth to which the tree could be grown.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的练习中看到的，决策树容易出现过拟合问题。这是它们使用的主要批评之一，尽管它们具有高度的可解释性。然而，我们通过限制树的最大深度，能够在一定程度上限制这种过拟合。
- en: Building on the concepts of decision trees, machine learning researchers have
    leveraged multiple trees as the basis for more complex procedures, resulting in
    some of the most powerful and widely used predictive models. In this chapter,
    we will focus on random forests of decision trees. Random forests are examples
    of what are called ensemble models, because they are formed by combining other,
    simpler models. By combining the predictions of many models, it is possible to
    improve upon the deficiencies of any given one of them. This is sometimes called
    combining many weak learners to make a strong learner.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 基于决策树的概念，机器学习研究人员利用多棵树作为更复杂过程的基础，最终形成了一些最强大且最广泛使用的预测模型。在本章中，我们将重点介绍决策树的随机森林。随机森林是所谓的集成模型的例子，因为它们是通过组合其他更简单的模型形成的。通过结合多个模型的预测，可以改善任何给定模型的缺陷。这有时被称为将多个弱学习者结合成一个强学习者。
- en: Once you understand decision trees, the concept behind random forests is fairly
    simple. That is because random forests are just ensembles of many decision trees;
    all the models in this kind of ensemble have the same mathematical form. So, how
    many decision tree models will be included in a random forest? This is one of
    the hyperparameters, `n_estimators`, that needs to be specified when building
    a random forest model. Generally speaking, the more trees, the better. As the
    number of trees increases, the variance of the overall ensemble will decrease.
    This should result in the random forest model having better generalization to
    new data, which will be reflected in increased testing scores. However, there
    will be a point of diminishing returns after which increasing the number of trees
    does not result in a substantial improvement in model performance.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了决策树，随机森林背后的概念就非常简单。那是因为随机森林只是许多决策树的集成；这种集成中的所有模型都具有相同的数学形式。那么，一个随机森林中会包括多少个决策树模型呢？这是构建随机森林模型时需要指定的超参数之一，`n_estimators`。一般来说，树木越多越好。随着树木数量的增加，整个集成的方差将减少。这应该导致随机森林模型对新数据的泛化能力更强，反映在测试分数的提高上。然而，在某个点之后，增加树木的数量将不再显著提高模型的性能。
- en: 'So, how do random forests reduce the high variance (overfitting) issue that
    affects decision trees? The answer to this question lies in what is different
    about the different trees in the forest. There are two main ways in which the
    trees are different, one of which we are already familiar with:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，随机森林是如何减少影响决策树的高方差（过拟合）问题的呢？这个问题的答案在于森林中不同树的不同之处。树之间的差异主要有两种方式，其中一种我们已经熟悉：
- en: The number of features considered at each split
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次划分时考虑的特征数量
- en: The training samples used to grow different trees
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于生长不同树的训练样本
- en: '**The Number of Features Considered at Each Split**'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**每次划分时考虑的特征数量**'
- en: 'We are already familiar with this option from the `DecisionTreeClassifier`
    class: `max_features`. In our previous usage of this class, we left `max_features`
    at its default value of `None`, which meant that all features were considered
    at each split. By using all the features to fit the training data, overfitting
    is possible. By limiting the number of features considered at each split, some
    of the decision trees in a random forest will potentially find better splits.
    This is because, although they are still greedily searching for the best split,
    they are doing it with a limited selection of features. This may make certain
    splits possible later in the tree that may not have been found if all features
    were being searched at each split.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经熟悉了`DecisionTreeClassifier`类中的这个选项：`max_features`。在之前使用该类时，我们将`max_features`保持在默认值`None`，这意味着每次分割时会考虑所有特征。通过使用所有特征来拟合训练数据，可能会导致过拟合。通过限制每次分割时考虑的特征数量，随机森林中的某些决策树可能会找到更好的分割点。这是因为，尽管它们仍在贪心地寻找最佳分割，但它们是在有限的特征选择下进行的。这可能会使得某些分割在树的后续部分变得可能，而如果在每次分割时都搜索所有特征，可能就无法找到这些分割点。
- en: There is a `max_features` option in the `RandomForestClassifier` class in scikit-learn
    just as there is for the `DecisionTreeClassifier` class and the options are similar.
    However, for the random forest, the default setting is `'auto'`, which means the
    algorithm will only search a random selection of the square root of the number
    of possible features at each split, for example, a random selection of √9 = 3
    features from a total of 9 possible features. Because each tree in the forest
    will likely choose different random selections of features to split as the trees
    are being grown, the trees in the forest will not be the same.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中的`RandomForestClassifier`类中有一个`max_features`选项，就像在`DecisionTreeClassifier`类中一样，这两个选项是类似的。然而，对于随机森林，默认设置是`'auto'`，这意味着算法每次分割时只会搜索可能特征数量的平方根的随机选择。例如，从总共有9个可能特征中，随机选择√9
    = 3个特征。由于森林中的每棵树在生长过程中可能会选择不同的特征随机分割，因此森林中的树木不会完全相同。
- en: '**The Samples Used to Grow Different Trees**'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**用于生成不同树木的样本**'
- en: The other way that the trees in a random forest differ from each other is that
    they are usually grown with different training samples. To do this, a statistical
    procedure known as bootstrapping is used, which means generating new synthetic
    datasets from the original data. The synthetic datasets are created by randomly
    selecting samples from the original dataset using replacement. Here, "replacement"
    means that if we select a certain sample, we will continue to consider it for
    selection, that is, it is "replaced" in the original dataset after we've sampled
    it. The number of samples in the synthetic datasets are the same as those in the
    original dataset, but some samples may be repeated because of replacement, while
    others may not be present at all.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林中的树木彼此之间的另一种区别是它们通常使用不同的训练样本进行生长。为此，需要使用一种叫做自助抽样（bootstrapping）的统计方法，这意味着从原始数据中生成新的合成数据集。合成数据集通过从原始数据集中随机选择样本来创建，并允许重复选择。这里的“重复选择”意味着，如果我们选择了某个样本，我们将继续考虑该样本用于选择，也就是说，它在被采样后会被“替换”到原始数据集中。合成数据集中的样本数量与原始数据集中的样本数量相同，但由于替换机制，一些样本可能会重复，而另一些样本则可能完全不在其中。
- en: The procedure of using random sampling to create synthetic datasets, and training
    models on them separately, is called bagging, which is short for bootstrapped
    aggregation. Bagging can, in fact, be used with any machine learning model, not
    just decision trees, and scikit-learn offers functionality to do this for both
    classification (`BaggingClassifier`) and regression (`BaggingRegressor`) problems.
    In the case of random forest, bagging is turned on by default and the `bootstrap`
    option is set to `True`. But if you want all the trees in the forest to be grown
    using all of the training data, you can set this option to `False`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机抽样创建合成数据集并分别对其进行训练的过程称为袋装法（bagging），即自助聚合（bootstrapped aggregation）的简称。事实上，袋装法可以与任何机器学习模型一起使用，而不仅仅是决策树，scikit-learn提供了该功能，用于分类问题（`BaggingClassifier`）和回归问题（`BaggingRegressor`）。对于随机森林来说，袋装法默认开启，且`bootstrap`选项被设置为`True`。但是，如果你希望森林中的所有树都使用全部训练数据进行生长，可以将该选项设置为`False`。
- en: Now you should have a good understanding of what a random forest is. As you
    can see, if you are already familiar with decision trees, understanding random
    forests does not involve much additional knowledge. A reflection of this fact
    is that the hyperparameters available for the `RandomForestClassifier` class in
    scikit-learn are mostly the same as those for the `DecisionTreeClassifier` class.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该对随机森林有了一个较好的理解。正如你所见，如果你已经熟悉决策树，那么理解随机森林并不涉及太多额外的知识。这个事实的体现是，`scikit-learn`中的`RandomForestClassifier`类的超参数大多数与`DecisionTreeClassifier`类的超参数相同。
- en: 'In addition to `n_estimators` and `bootstrap`, which we discussed previously,
    there are only two new options beyond what''s available for decision trees:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们之前讨论过的`n_estimators`和`bootstrap`，还有两个新选项，它们超出了决策树的可用选项：
- en: '`oob_score`, a `bool`: This option controls whether or not to calculate an
    `True` to calculate the OOB score or `False` (the default) not to.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`oob_score`，一个`bool`值：此选项控制是否计算OOB分数，`True`表示计算，`False`（默认值）表示不计算。'
- en: '`warm_start`, a `bool`: This is `False` by default – if you set this to `True`,
    then reusing the same random forest model object will cause additional trees to
    be added to the already generated forest.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warm_start`，一个`bool`值：默认值为`False`——如果将其设置为`True`，则重新使用相同的随机森林模型对象会在已生成的森林中添加额外的树。'
- en: '`max_samples`, an `int` or `float`: Controls how many samples are used to train
    each tree in the forest, when using the bootstrapping procedure. The default is
    to use the same number as the original dataset.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_samples`，一个`int`或`float`值：控制在使用自助法程序训练每棵树时使用多少样本。默认值是使用与原始数据集相同的样本数。'
- en: '**Other Kinds of Ensemble Models**'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他类型的集成模型**'
- en: Random forest, as we now know, is an example of a bagging ensemble. Another
    kind of ensemble is a **boosting** ensemble. The general idea of boosting is to
    use successive new models of the same type and to train them on the errors of
    previous models. This way, successive models learn where earlier models didn't
    do well and correct these errors. Boosting has enjoyed successful application
    with decision trees and is available in scikit-learn and another popular Python
    package called XGBoost. We will discuss boosting in the next chapter.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们现在所知道的，随机森林是袋装集成的一种示例。另一种集成方法是**提升**集成。提升的一般思路是使用一系列相同类型的新模型，并且将它们训练在前一个模型的错误上。通过这种方式，后续模型学习到早期模型未能解决的问题，并对这些错误进行修正。提升在决策树中获得了成功的应用，并且在`scikit-learn`和另一个流行的Python库XGBoost中都可以使用。我们将在下一章讨论提升。
- en: '**Stacking** ensembles are a somewhat more advanced kind of ensemble, where
    the different models (estimators) within the ensemble do not need to be of the
    same type as they do in bagging and boosting. For example, you could build a stacking
    ensemble with a random forest and a logistic regression. The predictions of the
    different members of the ensemble are combined for a final prediction using yet
    another model (the **stacker**), which considers the predictions of the **stacked**
    models as features.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆叠**集成方法是一种较为高级的集成方法，在这种方法中，集成中的不同模型（估计器）不需要像在袋装法和提升法中那样是相同类型的。例如，你可以通过随机森林和逻辑回归来构建一个堆叠集成。集成中不同成员的预测会通过另一个模型（**堆叠器**）结合起来进行最终预测，该模型将**堆叠**模型的预测作为特征来考虑。'
- en: 'Random Forest: Predictions and Interpretability'
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林：预测与可解释性
- en: Since a random forest is just a collection of decision trees, somehow the predictions
    of all those trees must be combined to create the prediction of the random forest.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机森林只是决策树的集合，因此，必须以某种方式将所有这些树的预测结合起来，以得出随机森林的预测。
- en: 'After model training, classification trees will take an input sample and produce
    a predicted class, for example, whether or not a credit account in our case study
    problem will default. One intuitive approach to combining the predictions of these
    trees into the ultimate prediction of the forest is to take a majority vote. That
    is, whatever the most common prediction of all the trees is becomes the prediction
    of the forest, for a given sample. This was the approach taken in the publication
    first describing random forests ([https://scikit-learn.org/stable/modules/ensemble.html#forest](https://scikit-learn.org/stable/modules/ensemble.html#forest)).
    However, scikit-learn uses a somewhat different approach: adding up the predicted
    probabilities for each class and then choosing the one with the highest probability
    sum. This captures more information from each tree than just the predicted class.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练后，分类树将接受一个输入样本并生成一个预测类别，例如，在我们的案例研究问题中，预测信用账户是否会违约。将这些树的预测组合成森林的最终预测的一种直观方法是采用多数投票。也就是说，无论所有树的最常见预测是什么，它就成为该样本的森林预测。这是最初描述随机森林的文献中采用的方法（[https://scikit-learn.org/stable/modules/ensemble.html#forest](https://scikit-learn.org/stable/modules/ensemble.html#forest)）。然而，scikit-learn使用的是一种稍有不同的方法：将每个类别的预测概率相加，然后选择具有最高概率和的类别。这比仅仅选择预测类别捕获了更多来自每棵树的信息。
- en: '**Interpretability of Random Forests**'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林的可解释性**'
- en: One of the main advantages of decision trees is that it is straightforward to
    see how any individual prediction is made. You can trace the decision path for
    any sample through the series of "if, then" rules used to make a prediction and
    know exactly how it came to have that prediction. By contrast, imagine that you
    have a random forest consisting of 1,000 trees. This would mean there are 1,000
    sets of rules like this, which are much harder to communicate to human beings
    than one set of rules!
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的主要优势之一是它能够直观地看到每个单独预测的生成过程。你可以通过一系列“如果， 那么”的规则追溯任何样本的决策路径，准确知道它是如何得出该预测的。相反，假设你有一个包含1,000棵树的随机森林，这意味着有1,000组这样的规则，它们比单一规则更难以向人类传达！
- en: That being said, there are various methods that can be used to understand how
    random forests make predictions. A simple way to interpret how a random forest
    works, and which is available in scikit-learn, is to observe the **feature importances**.
    Feature importances of a random forest are a measure of how useful each of the
    features was when growing the trees in the forest. This usefulness is measured
    by a combination of the fraction of training samples that were split using each
    feature, and the decrease in node impurity that resulted.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，仍然有多种方法可以用来理解随机森林是如何做出预测的。一种简单的方法是观察**特征重要性**，这种方法可以在scikit-learn中使用。随机森林的特征重要性是衡量在生成森林中的树木时，每个特征的有用程度。这种有用性是通过结合每个特征在训练样本中被用于分裂的比例，以及由此导致的节点杂质降低来度量的。
- en: Because of the feature importance calculation, which can be used to rank features
    by how impactful they are within the random forest model, random forests can also
    be used for feature selection.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 由于特征重要性计算，可以通过该计算按特征在随机森林模型中的影响力对特征进行排序，因此，随机森林还可以用于特征选择。
- en: 'Exercise 5.03: Fitting a Random Forest'
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.03：拟合随机森林
- en: 'In this exercise, we will extend our efforts with decision trees by using the
    random forest model with cross-validation on the training data from the case study.
    We will observe the effect of increasing the number of trees in the forest and
    examine the feature importance that can be calculated using a random forest model.
    Perform the following steps to complete the exercise:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将通过使用随机森林模型并在案例研究中的训练数据上进行交叉验证，来扩展我们在决策树方面的工作。我们将观察增加森林中树木数量的效果，并检查可以使用随机森林模型计算的特征重要性。请执行以下步骤来完成此练习：
- en: Note
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The Jupyter notebook for this exercise can be found at [https://packt.link/VSz2T](https://packt.link/VSz2T).
    This notebook contains the prerequisite steps of importing the necessary libraries
    and loading the cleaned dataframe. Please execute these steps before you begin
    this exercise.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的Jupyter笔记本可以在[https://packt.link/VSz2T](https://packt.link/VSz2T)找到。此笔记本包含了导入必要库和加载清洗后的数据框的前提步骤。请在开始本练习之前执行这些步骤。
- en: 'Import the random forest classifier model class as follows:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式导入随机森林分类器模型类：
- en: '[PRE38]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Instantiate the class using these options:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些选项实例化类：
- en: '[PRE39]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: For this exercise, we'll use mainly the default options. However, note that
    we will set `max_depth =` 3\. Here, we are only going to explore the effect of
    using different numbers of trees, which we will illustrate with relatively shallow
    trees for the sake of shorter runtimes. To find the best model performance, we'd
    typically try more trees and deeper depths of trees.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于这个练习，我们主要使用默认选项。但请注意，我们将设置`max_depth = 3`。在这里，我们只探索使用不同数量的树的效果，因此选择相对较浅的树以便更短的运行时间。要找到最佳模型性能，通常会尝试更多的树和更深的树。
- en: We also set `random_state` for consistent results across runs.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还为了保证运行结果的一致性设置了`random_state`。
- en: 'Create a parameter grid for this exercise in order to search the numbers of
    trees, ranging from 10 to 100 by 10s:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建参数网格以便在范围从10到100的树中搜索：
- en: '[PRE40]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We use Python's `range()` function to create an iterator for the integer values
    we want, and then convert them to a list using `list()`.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用Python的`range()`函数创建所需整数值的迭代器，然后使用`list()`将其转换为列表。
- en: 'Instantiate a grid search cross-validation object for the random forest model
    using the parameter grid from the previous step. Otherwise, you can use the same
    options that were used for the cross-validation of the decision tree:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用先前步骤中的参数网格实例化随机森林模型的网格搜索交叉验证对象。否则，您可以使用与决策树交叉验证相同的选项：
- en: '[PRE41]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Fit the cross-validation object as follows:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将交叉验证对象拟合如下：
- en: '[PRE42]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The fitting procedure should output the following:'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 拟合过程应该输出以下内容：
- en: '![Figure 5.17: The output from the cross-validation of the random forest'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图5.17：随机森林交叉验证的输出'
- en: across different numbers of trees
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在不同数量的树上
- en: '](img/B16392_05_17.jpg)'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_05_17.jpg)'
- en: 'Figure 5.17: The output from the cross-validation of the random forest across
    different numbers of trees'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.17：随机森林在不同数量的树上的交叉验证输出
- en: You may have noticed that, although we are only cross-validating over 10 hyperparameter
    values, comparable to the 7 values that we examined for the decision tree in the
    previous exercise, this cross-validation took noticeably longer. Consider how
    many trees we are growing in this case. For the last hyperparameter, `n_estimators
    = 100`, we have grown a total of 400 trees across all the cross-validation splits.
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可能已经注意到，尽管我们只在10个超参数值上进行交叉验证，与前一练习中决策树所检验的7个值相比，这次的交叉验证花费的时间明显更长。考虑一下我们在这种情况下生成了多少棵树。对于最后一个超参数`n_estimators
    = 100`，我们在所有交叉验证的拆分中总共生成了400棵树。
- en: How long has model fitting taken across the various numbers of trees that we
    just tried? What gains in terms of cross-validation testing performance have we
    made by using more trees? These are good things to examine using plots. First,
    we'll pull the cross-validation results out into a pandas DataFrame, as we've
    done before.
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们刚才尝试的各种树的模型拟合时间是多长？通过使用更多的树，我们在交叉验证测试性能方面获得了多大的提升？这些都是通过绘图来检查的好方法。首先，我们将交叉验证结果提取到一个pandas
    DataFrame中，就像以前做过的那样。
- en: 'Put the cross-validation results into a pandas DataFrame:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将交叉验证结果放入pandas DataFrame中：
- en: '[PRE43]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: You can examine the whole DataFrame in the accompanying Jupyter notebook. Here,
    we move directly to creating plots of the quantities of interest. We'll make a
    line plot, with symbols, of the mean fit time across the folds for each hyperparameter,
    contained in the `mean_fit_time` column, as well as an error bar plot of testing
    scores, which we've already done for decision trees. Both plots will be against
    the number of trees on the *x* axis.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以在附带的Jupyter笔记本中查看整个DataFrame。这里，我们直接转向创建感兴趣的量的图形。我们将制作一条线图，其中包含每个超参数的平均拟合时间（包含在`mean_fit_time`列中的符号），以及一个测试分数的误差条形图，这些我们已经为决策树做过。两个图都将根据*
    x *轴上的树数量进行绘制。
- en: 'Create two subplots of the mean training time and mean testing scores with
    standard error:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个子图，分别显示平均训练时间和平均测试分数及标准误差：
- en: '[PRE44]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Here, we've used `plt.subplots` to create two axes at once, within a figure,
    in a one-row-by-two-column configuration. We then access the axes objects by indexing
    the array of `axs` axes returned from this operation in order to create plots.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用`plt.subplots`一次创建了一个figure内的两个轴，配置为一行两列。然后，我们通过对返回的`axs`轴数组进行索引来访问轴对象，以便创建绘图。
- en: 'The output should look similar to this plot:'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应该类似于此图：
- en: '![Figure 5.18: The mean fitting time and testing scores for different numbers'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图5.18：不同数量的树的平均拟合时间和测试分数'
- en: of trees in the forest
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 森林中的树的数量
- en: '](img/B16392_05_18.jpg)'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_05_18.jpg)'
- en: 'Figure 5.18: The mean fitting time and testing scores for different numbers
    of trees in the forest'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.18：不同数量树木的森林的平均拟合时间和测试分数
- en: Note
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注
- en: Your results may differ due to the differences in the platform or if you set
    a different random seed.
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于平台差异或设置了不同的随机种子，你的结果可能会有所不同。
- en: There are several things to note regarding these visualizations. First of all,
    we can see that by using a random forest, we have increased model performance
    on the cross-validation testing folds above that of any of our previous efforts.
    While we haven't made an attempt to tune the random forest hyperparameters to
    achieve the best model performance we can, this is a promising result and indicates
    that a random forest will be a valuable addition to our modeling efforts.
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于这些可视化，还有几个要注意的点。首先，我们可以看到，通过使用随机森林，我们在交叉验证测试折叠上的模型表现超过了我们之前的任何尝试。虽然我们还没有调整随机森林的超参数以获得最佳的模型性能，但这是一个有前景的结果，表明随机森林将是我们建模工作中一个有价值的补充。
- en: However, along with these higher model testing scores, notice that there is
    also more variability between the folds than what we saw with the decision tree;
    this variability is visible as larger standard errors in model testing scores
    across the folds. While this indicates that there is a wider range in model performance
    that might be expected from using this model, you are encouraged to examine the
    model testing scores of the folds directly in the pandas DataFrame in the Jupyter
    notebook. You should see that even the lowest score from an individual fold is
    still higher than the average testing score from the decision tree, indicating
    that it will be better to use a random forest.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，随着这些更高模型测试分数的提升，请注意，折叠之间的变异性也比我们在决策树中看到的更大；这种变异性在模型测试分数的标准误差中表现得更加明显。虽然这表明使用该模型时，模型性能的范围可能更广，但我们建议直接在
    Jupyter notebook 中的 pandas DataFrame 中查看各个折叠的模型测试分数。你会看到，即使是个别折叠的最低分数，仍然高于决策树的平均测试分数，这表明使用随机森林会更好。
- en: So, what about the other questions that we set out to explore with this visualization?
    We are interested in seeing how long it takes to fit random forest models with
    various numbers of trees, and what the gains in model performance are from using
    more trees. The subplot on the left of *Figure 5.18* shows that there is a fairly
    linear increase in training time as more trees are added to the forest. This is
    probably to be expected; we are simply adding to the amount of computation to
    be done in the training procedure by adding more trees.
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 那么，我们用这个可视化来探讨的其他问题如何呢？我们感兴趣的是查看拟合不同数量树木的随机森林模型需要多长时间，以及使用更多树木时模型性能的提升。*图 5.18*
    左侧的小图显示，随着树木数量的增加，训练时间呈现出相当线性的增长。这可能是可以预期的；通过增加更多树木，我们实际上是在增加训练过程中需要进行的计算量。
- en: But is this additional computational time worth it in terms of increased model
    performance? The subplot on the right of *Figure 5.18* shows that beyond about
    20 trees, it's not clear that adding more trees reliably improves testing performance.
    While the model with 50 trees has the highest score, the fact that adding more
    trees actually decreases the testing score somewhat indicates that the gain in
    ROC AUC for 50 trees may just be due to randomness, as adding more trees theoretically
    should increase model performance. Based on this reasoning, if we were limited
    to `max_depth = 3`, we may choose a forest of 20 or perhaps 50 trees and proceed.
    However, we will explore the parameter space more fully in the activity at the
    end of this chapter.
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 但是，考虑到模型性能的提升，这额外的计算时间是否值得呢？*图 5.18* 右侧的小图显示，超过大约 20 棵树后，添加更多树木并不一定能可靠地提高测试性能。虽然拥有
    50 棵树的模型得分最高，但添加更多树木实际上使测试分数略有下降，这表明 50 棵树的 ROC AUC 增益可能只是由于随机性，因为理论上增加更多树木应该提高模型性能。基于这个推理，如果我们将
    `max_depth = 3` 作为限制，我们可能会选择 20 棵或许 50 棵树的森林继续进行。但在本章最后的活动中，我们将更全面地探索这个参数空间。
- en: Finally, note that we have not shown the training ROC AUC metrics here. If you
    were to plot these or look them up in the results DataFrame, you'd see that the
    training scores are higher than the testing scores, indicating that some amount
    of overfitting is happening. While this may be the case, it's still true that
    the cross-validation testing scores for this random forest model are higher than
    those that we've observed for any other model. Based on this result, we would
    likely choose the random forest model at this point.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，请注意，我们没有在这里显示训练集的ROC AUC指标。如果你绘制这些或在结果数据框中查看，你会发现训练分数高于测试分数，这表明某些过拟合现象正在发生。尽管如此，仍然可以确定这个随机森林模型的交叉验证测试分数比我们观察到的任何其他模型的分数要高。基于这个结果，我们很可能会选择此时的随机森林模型。
- en: For a few additional insights into what we can access using our fitted cross-validation
    object, let's take a look at the best hyperparameters and feature importance.
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了更深入地了解我们可以通过拟合的交叉验证对象访问哪些内容，让我们看看最佳超参数和特征重要性。
- en: 'Use this code to see the best hyperparameters from cross-validation:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码查看交叉验证中最好的超参数：
- en: '[PRE45]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This should be the output:'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该是输出结果：
- en: '[PRE46]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Here, best just means the hyperparameters that resulted in the highest average
    model testing score.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，best仅仅指的是那些导致最高平均模型测试分数的超参数。
- en: 'Run this code to create a DataFrame of the feature names and importances, and
    then show a horizontal bar plot sorted by importance:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行此代码以创建特征名称和重要性的数据框，然后显示按重要性排序的横向条形图：
- en: '[PRE47]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The plot should look like this:'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图表应该是这样的：
- en: '![Figure 5.19: Feature importance from a random forest'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.19：来自随机森林的特征重要性'
- en: '](img/B16392_05_19.jpg)'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_05_19.jpg)'
- en: 'Figure 5.19: Feature importance from a random forest'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19：来自随机森林的特征重要性
- en: In this code, we've created a dictionary with feature importances and used this
    along with the feature names as an index to create a DataFrame. The feature importances
    came from the `best_estimator_` method of the fitted cross-validation object,
    so it refers to the model with the highest average testing score (in other words,
    the model with 50 trees). This is a way to access the random forest model object,
    which was trained on all the training data, using the best hyperparameters found
    by the cross-validation grid search. `feature_importances_` is a method that can
    be used on fitted random forest models.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们创建了一个包含特征重要性的字典，并将它与特征名称作为索引一起使用，以创建一个数据框。特征重要性来自拟合后的交叉验证对象的`best_estimator_`方法，所以它指的是具有最高平均测试分数的模型（换句话说，就是具有50棵树的模型）。这是一种访问随机森林模型对象的方法，该对象已经在所有训练数据上进行了训练，并使用了交叉验证网格搜索找到的最佳超参数。`feature_importances_`是可以在已拟合的随机森林模型上使用的方法。
- en: After accessing all these attributes, we plot them on a horizontal bar chart,
    which is a convenient way to look at feature importances. Notice that the top
    five most important features from the random forest are the same as the top five
    chosen by an ANOVA F-test in *Chapter 3*, *Details of Logistic Regression and
    Feature Exploration*, although they are in a somewhat different order. This is
    good confirmation between the different methods.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在访问了所有这些属性后，我们将它们绘制在一个横向条形图上，这是一种方便查看特征重要性的方法。请注意，来自随机森林的前五个最重要特征与*第3章*中通过ANOVA
    F检验选择的前五个特征相同，尽管它们的顺序有所不同。这是不同方法之间的一种良好确认。
- en: Checkerboard Graph
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 棋盘图
- en: Before moving on to the activity, we illustrate a visualization technique in
    Matplotlib. Plotting a two-dimensional grid with colored squares or other shapes
    on it can be useful when you want to show three dimensions of data. Here, color
    illustrates the third dimension. For example, you may want to visualize model
    testing scores over a grid of two hyperparameters, as we'll do in *Activity 5.01*,
    *Cross-Validation Grid Search with Random Forest*.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续进行活动之前，我们展示了一种在Matplotlib中进行可视化的技术。绘制一个二维网格，并在其上用彩色方块或其他形状表示数据，当你想展示数据的三个维度时，这种方法很有用。这里，颜色表示第三维度。例如，您可能想要在两个超参数的网格上可视化模型的测试分数，就像我们将在*活动
    5.01*中做的那样，*使用随机森林的交叉验证网格搜索*。
- en: 'The first step in the process is to create grids of *x* and *y* coordinates.
    The NumPy `meshgrid` function can be used to do this. This function takes one-dimensional
    arrays of *x* and *y* coordinates and creates the mesh grid with all the possible
    pairs from both. The points in the mesh grid will be the corners of each square
    on the checkerboard plot. Here is how the code looks for a 4 x 4 grid of colored
    patches. Since we are specifying the corners, we require a 5 x 5 grid of points.
    We also show the arrays of the *x* and *y* coordinates:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 过程的第一步是创建 *x* 和 *y* 坐标的网格。可以使用 NumPy 的 `meshgrid` 函数来完成这项工作。该函数接受一维的 *x* 和 *y*
    坐标数组，并创建所有可能的坐标对的网格。网格中的点将是棋盘图中每个方块的角落。下面是一个 4 x 4 彩色块网格的代码示例。由于我们指定了角落，所以我们需要一个
    5 x 5 的点网格。我们还展示了 *x* 和 *y* 坐标的数组：
- en: '[PRE48]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE49]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The grid of data to plot on this mesh should have a 4 x 4 shape. We make a
    one-dimensional array of integers between 1 and 16, and reshape it to a two-dimensional,
    4 x 4 grid:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在此网格上绘制的数据应该具有 4 x 4 的形状。我们创建了一个包含从 1 到 16 的整数的一维数组，并将其重塑为一个二维的 4 x 4 网格：
- en: '[PRE50]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This outputs the following:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE51]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We can plot the `z_example` data on the `xx_example, yy_example` mesh grid
    with the following code. Notice that we use `pcolormesh` to make the plot with
    the `jet` colormap, which gives a rainbow color scale. We add a `colorbar`, which
    needs to be passed the `pcolor_ex` object returned by `pcolormesh` as an argument,
    so the interpretation of the color scale is clear:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码在 `xx_example, yy_example` 网格上绘制 `z_example` 数据。请注意，我们使用 `pcolormesh`
    来生成图表，并采用 `jet` 色图，这会给出彩虹色的颜色刻度。我们还添加了一个 `colorbar`，它需要传入 `pcolor_ex` 对象（由 `pcolormesh`
    返回）作为参数，这样颜色刻度的解释就会清晰：
- en: '[PRE52]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![Figure 5.20: A pcolormesh plot of consecutive integers'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.20：连续整数的 pcolormesh 图'
- en: '](img/B16392_05_20.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16392_05_20.jpg)'
- en: 'Figure 5.20: A pcolormesh plot of consecutive integers'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.20：连续整数的 pcolormesh 图
- en: 'Activity 5.01: Cross-Validation Grid Search with Random Forest'
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 5.01：使用随机森林进行交叉验证网格搜索
- en: 'In this activity, you will conduct a grid search over the number of trees in
    the forest (`n_estimators`) and the maximum depth of a tree (`max_depth`) for
    a random forest model on the case study data. You will then create a visualization
    showing the average testing score for the grid of hyperparameters that you searched
    over. Perform the following steps to complete the activity:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你将对随机森林模型在案例研究数据上进行网格搜索，搜索的超参数包括森林中的树木数量（`n_estimators`）和树的最大深度（`max_depth`）。然后，你将创建一个可视化，展示你搜索过的超参数网格的平均测试得分。请按照以下步骤完成此活动：
- en: Create a dictionary representing the grid for the `max_depth` and `n_estimators`
    hyperparameters that will be searched. Include depths of 3, 6, 9, and 12, and
    10, 50, 100, and 200 trees. Leave the other hyperparameters at their defaults.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个字典，表示将要搜索的 `max_depth` 和 `n_estimators` 超参数的网格。包括深度 3、6、9 和 12，以及树木数量 10、50、100
    和 200。将其他超参数保持为默认值。
- en: Instantiate a `GridSearchCV` object using the same options that we have had
    previously in this chapter, but with the dictionary of hyperparameters created
    in step 1 here. Set `verbose=2` to see the output for each fit performed. You
    can reuse the same random forest model object, `rf`, that we have been using or
    create a new one.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用本章前面提到的相同选项实例化一个 `GridSearchCV` 对象，但使用步骤 1 中创建的超参数字典。设置 `verbose=2`，以查看每次拟合过程的输出。你可以重用我们之前使用的随机森林模型对象
    `rf`，或者创建一个新的。
- en: Fit the `GridSearchCV` object on the training data.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据上拟合 `GridSearchCV` 对象。
- en: Put the results of the grid search in a pandas DataFrame.
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将网格搜索的结果放入 pandas DataFrame 中。
- en: 'Create a `pcolormesh` visualization of the mean testing score for each combination
    of hyperparameters. You should obtain a visualization similar to the following:![Figure
    5.21: Results of cross-validation of a random forest'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `pcolormesh` 可视化，显示每个超参数组合的平均测试得分。你应该得到类似于以下的可视化效果：![图 5.21：随机森林交叉验证结果
- en: over a grid with two hyperparameters
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在具有两个超参数的网格上
- en: '](img/B16392_05_21.jpg)'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_05_21.jpg)'
- en: 'Figure 5.21: Results of cross-validation of a random forest over a grid with
    two hyperparameters'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.21：随机森林在具有两个超参数的网格上的交叉验证结果
- en: Conclude which set of hyperparameters to use.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定使用哪组超参数。
- en: Note
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The Jupyter notebook containing the Python code for this activity can be found
    at [https://packt.link/D0OBc](https://packt.link/D0OBc). Detailed step-wise solution
    to this activity can be found via [this link](B16925_Solution_ePub.xhtml#_idTextAnchor157).
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含此活动的 Python 代码的 Jupyter 笔记本可以在[https://packt.link/D0OBc](https://packt.link/D0OBc)找到。此活动的详细分步解决方案可以通过[这个链接](B16925_Solution_ePub.xhtml#_idTextAnchor157)找到。
- en: Summary
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've learned how to use decision trees and the ensemble models
    called random forests that are made up of many decision trees. Using these simply
    conceived models, we were able to make better predictions than we could with logistic
    regression, judging by the cross-validation ROC AUC score. This is often the case
    for many real-world problems. Decision trees are robust to a lot of the potential
    issues that can prevent logistic regression models from good performance, such
    as non-linear relationships between features and the response variable, and the
    presence of complicated interactions among features.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用决策树和由多个决策树组成的集成模型——随机森林。通过使用这些简单构思的模型，我们能够比使用逻辑回归做出更好的预测，这从交叉验证的
    ROC AUC 分数中可以看出。这种情况在许多实际问题中都适用。决策树对于很多可能影响逻辑回归模型性能的潜在问题具有较强的鲁棒性，例如特征与响应变量之间的非线性关系，以及特征之间复杂交互的存在。
- en: Although a single decision tree is prone to overfitting, the random forest ensemble
    method has been shown to reduce this high-variance issue. Random forests are built
    by training many trees. The decreased variance of the ensemble of trees is achieved
    by increasing the bias of the individual trees in the forest, by only training
    them on a portion of the available training set (bootstrapped aggregation or bagging),
    and by only considering a reduced number of features at each node split.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管单棵决策树容易出现过拟合问题，但随机森林集成方法已被证明能够减少这一高方差问题。随机森林通过训练多棵树来构建。通过仅在部分可用训练集上训练每棵树（自助聚合或袋ging），并且在每个节点分裂时只考虑减少的特征数量，树的集成可以减少方差，同时增加单棵树的偏差。
- en: Now that we have tried several different machine learning approaches to modeling
    the case study data, we found that some work better than others; for example,
    a random forest with tuned hyperparameters provides the highest average cross-validation
    ROC AUC score of 0.776, as we saw in *Activity 5*, *Cross-Validation Grid Search
    with Random Forest*.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经尝试了几种不同的机器学习方法来建模案例数据，发现有些方法效果更好；例如，经过调优的超参数的随机森林提供了最高的平均交叉验证 ROC AUC
    分数 0.776，正如我们在*活动 5*中看到的，*使用随机森林进行交叉验证网格搜索*。
- en: In the next chapter, we'll learn about another type of ensemble method, called
    gradient boosting, which is often used in conjunction with decision trees. Gradient
    boosting has yielded some of the best performance of all machine learning models
    for binary classification use cases. We'll also learn a powerful method for explaining
    and interpreting the predictions of gradient boosted ensembles of trees, using
    **SHapely Additive exPlanation** (**SHAP**) values.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习另一种集成方法——梯度提升，它通常与决策树结合使用。梯度提升在所有机器学习模型中为二分类问题提供了一些最佳性能。我们还将学习一种强大的方法，通过**SHapely加法解释**（**SHAP**）值来解释和解读梯度提升树集成的预测。
