- en: Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: '**Reinforcement learning** (**RL**) is the third major section of machine learning
    after supervised and unsupervised learning. These techniques have gained a lot
    of traction in recent years in the application of artificial intelligence. In
    reinforcement learning, sequential decisions are to be made rather than one shot
    decision making, which makes it difficult to train the models in a few cases.
    In this chapter, we would be covering various techniques used in reinforcement
    learning with practical examples to support with. Though covering all topics are
    beyond the scope of this book, but we did cover the most important fundamentals
    here for a reader to create enough enthusiasm on this subject. Topics discussed
    in this chapter are:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）是继监督学习和无监督学习之后的第三大机器学习领域。这些技术在近年来在人工智能应用中获得了很大的关注。在强化学习中，需要做出顺序决策，而不是一次性决策，这在某些情况下使得训练模型变得困难。在本章中，我们将涵盖强化学习中使用的各种技术，并提供实际示例支持。虽然涵盖所有主题超出了本书的范围，但我们确实在这里涵盖了这个主题的最重要基础知识，以激发读者对这一主题产生足够的热情。本章讨论的主题包括：'
- en: Markov decision process
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: Bellman equations
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: Dynamic programming
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态规划
- en: Monte Carlo methods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法
- en: Temporal difference learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间差分学习
- en: Reinforcement learning basics
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习基础知识
- en: 'Before we deep dive into the details of reinforcement learning, I would like
    to cover some of the basics necessary for understanding the various nuts and bolts
    of RL methodologies. These basics appear across various sections of this chapter,
    which we will explain in detail whenever required:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究强化学习的细节之前，我想介绍一些理解RL方法的各种要素所必需的基础知识。这些基础知识将出现在本章的各个部分中，我们将在需要时详细解释：
- en: '**Environment:** This is any system that has states, and mechanisms to transition
    between states. For example, the environment for a robot is the landscape or facility
    it operates.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境：** 这是具有状态和状态之间转换机制的任何系统。例如，机器人的环境是其操作的景观或设施。'
- en: '**Agent:** This is an automated system that interacts with the environment.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理：** 这是与环境交互的自动化系统。'
- en: '**State:** The state of the environment or system is the set of variables or
    features that fully describe the environment.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态：** 环境或系统的状态是完全描述环境的变量或特征集。'
- en: '**Goal or absorbing state or terminal state:** This is the state that provides
    a higher discounted cumulative reward than any other state. A high cumulative
    reward prevents the best policy from being dependent on the initial state during
    training. Whenever an agent reaches its goal, we will finish one episode.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标或吸收状态或终止状态：** 这是提供比任何其他状态更高折现累积奖励的状态。高累积奖励可以防止最佳策略在训练过程中依赖于初始状态。每当代理达到目标时，我们将完成一个回合。'
- en: '**Action:** This defines the transition between states. The agent is responsible
    for performing, or at least recommending an action. Upon execution of the action,
    the agent collects a reward (or punishment) from the environment.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作：** 这定义了状态之间的转换。代理负责执行或至少推荐一个动作。执行动作后，代理从环境中收集奖励（或惩罚）。'
- en: '**Policy:** This defines the action to be selected and executed for any state
    of the environment. In other words, policy is the agent''s behavior; it is a map
    from state to action. Policies could be either deterministic or stochastic.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略：** 这定义了在环境的任何状态下要选择和执行的动作。换句话说，策略是代理的行为；它是从状态到动作的映射。策略可以是确定性的，也可以是随机的。'
- en: '**Best policy:** This is the policy generated through training. It defines
    the model in Q-learning and is constantly updated with any new episode.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最佳策略：** 这是通过训练生成的策略。它定义了Q学习中的模型，并且会随着任何新的回合不断更新。'
- en: '**Rewards**: This quantifies the positive or negative interaction of the agent
    with the environment. Rewards are usually immediate earnings made by the agent
    reaching each state.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励：** 这量化了代理与环境的积极或消极交互。奖励通常是代理到达每个状态时获得的即时收益。'
- en: '**Returns or value function**: A value function (also called returns) is a
    prediction of future rewards of each state. These are used to evaluate the goodness/badness
    of the states, based on which, the agent will choose/act on for selecting the
    next best state:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回报或值函数：** 值函数（也称为回报）是对每个状态未来奖励的预测。这些用于评估状态的好坏，基于这一点，代理将选择/行动以选择下一个最佳状态：'
- en: '![](img/97df1bcf-51c9-4747-a6fe-23daa16edd22.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97df1bcf-51c9-4747-a6fe-23daa16edd22.jpg)'
- en: '**Episode:** This defines the number of steps necessary to reach the goal state
    from an initial state. Episodes are also known as trials.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回合**：这定义了从初始状态到目标状态所需的步骤数。回合也称为试验。'
- en: '**Horizon:** This is the number of future steps or actions used in the maximization
    of the reward. The horizon can be infinite, in which case, the future rewards
    are discounted in order for the value of the policy to converge.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视野**：这是在最大化奖励过程中所考虑的未来步骤或动作的数量。视野可以是无限的，在这种情况下，未来的奖励会被折扣，以便策略的价值能够收敛。'
- en: '**Exploration versus Exploitation:** RL is a type of trial and error learning.
    The goal is to find the best policy; and at the same time, remain alert to explore
    some unknown policies. A classic example would be treasure hunting: if we just
    go to the locations greedily (exploitation), we fail to look for other places
    where hidden treasure might also exist (exploration). By exploring the unknown
    states, and by taking chances, even when the immediate rewards are low and without
    losing the maximum rewards, we might achieve greater goals. In other words, we
    are escaping the local optimum in order to achieve a global optimum (which is
    exploration), rather than just a short-term focus purely on the immediate rewards
    (which is exploitation). Here are a couple of examples to explain the difference:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索与利用**：强化学习（RL）是一种试错学习方法。目标是找到最佳策略；同时，要保持警觉，探索一些未知的策略。一个经典的例子就是寻宝：如果我们只是贪婪地去已知的位置（利用），就会忽视其他可能藏有宝藏的地方（探索）。通过探索未知的状态，尽管即时奖励较低且没有失去最大奖励，我们仍然可能实现更大的目标。换句话说，我们是在逃离局部最优解，以便达到全局最优解（即探索），而不是仅仅专注于即时奖励的短期目标（即利用）。以下是几个例子来解释二者的区别：'
- en: '**Restaurant selection**: By exploring unknown restaurants once in a while,
    we might find a much better one than our regular favorite restaurant:'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**餐馆选择**：通过偶尔探索一些未知的餐馆，我们可能会发现比我们常去的最喜欢的餐馆更好的餐馆：'
- en: '**Exploitation**: Going to your favorite restaurant'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用**：去你最喜欢的餐馆'
- en: '**Exploration**: Trying a new restaurant'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索**：尝试一家新的餐馆'
- en: '**Oil drilling example:** By exploring new untapped locations, we may get newer
    insights that are more beneficial that just exploring the same place:'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**油井钻探示例**：通过探索新的未开发地点，我们可能会获得比仅仅探索相同地点更有益的新见解：'
- en: '**Exploitation**: Drill for oil at best known location'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用**：在已知最佳地点钻探石油'
- en: '**Exploration**: Drill at a new location'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索**：在新地点钻探'
- en: '**State-Value versus State-Action Function:** In action-value, Q represents
    the expected return (cumulative discounted reward) an agent is to receive when
    taking Action *A* in State *S*, and behaving according to a certain policy π(a|s)
    afterwards (which is the probability of taking an action in a given state).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态值与状态-动作函数**：在动作值中，Q代表一个智能体在状态*S*下采取动作*A*并根据某一策略π(a|s)（即在给定状态下采取某一动作的概率）之后预期获得的回报（累计折扣奖励）。'
- en: 'In state-value, the value is the expected return an agent is to receive from
    being in state *s* behaving under a policy *π(a|s)*. More specifically, the state-value
    is an expectation over the action-values under a policy:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在状态值中，值是智能体在状态*s*下根据策略*π(a|s)*行为所预期获得的回报。更具体地说，状态值是基于策略下各动作值的期望：
- en: '![](img/9b1b62a2-aae2-4d68-bea1-4ca30750e064.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b1b62a2-aae2-4d68-bea1-4ca30750e064.jpg)'
- en: '**On-policy versus off-policy TD control:** An off-policy learner learns the
    value of the optimal policy independently of the agent''s actions. Q-learning
    is an off-policy learner. An on-policy learner learns the value of the policy
    being carried out by the agent, including the exploration steps.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略内学习与策略外学习的时间差分控制**：策略外学习者独立于智能体的行动学习最优策略的值。Q学习是一个策略外学习者。策略内学习者则学习智能体执行的策略值，包括探索步骤。'
- en: '**Prediction and control problems:** Prediction talks about how well I do,
    based on the given policy: meaning, if someone has given me a policy and I implement
    it, how much reward I will get for that. Whereas, in control, the problem is to
    find the best policy so that I can maximize the reward.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测与控制问题**：预测是指根据给定策略评估我的表现：也就是说，如果有人给我一个策略，我执行它后能获得多少奖励。而在控制中，问题是找到最佳策略，以便我能够最大化奖励。'
- en: '**Prediction:** Evaluation of the values of states for a given policy.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测**：评估在给定策略下各状态的值。'
- en: For the uniform random policy, what is the value function for all states?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于均匀随机策略，所有状态的价值函数是多少？
- en: '**Control:** Optimize the future by finding the best policy.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制：** 通过找到最佳策略来优化未来。'
- en: What is the optimal value function over all possible policies, and what is the
    optimal policy?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最优价值函数是什么，如何在所有可能的策略中找到最优策略？
- en: Usually, in reinforcement learning, we need to solve the prediction problem
    first, in order to solve the control problem after, as we need to figure out all
    the policies to figure out the best or optimal one.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在强化学习中，我们需要先解决预测问题，之后才能解决控制问题，因为我们需要找出所有策略，才能找出最佳或最优的策略。
- en: '**RL Agent Taxonomy:** An RL agent includes one or more of the following components:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RL智能体分类：** 一个RL智能体包括以下一个或多个组件：'
- en: '**Policy:** Agent''s behavior function (map from state to action); Policies
    can be either deterministic or stochastic'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略：** 智能体的行为函数（从状态到动作的映射）；策略可以是确定性的或随机的'
- en: '**Value function:** How good is each state (or) prediction of expected future
    reward for each state'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值函数：** 每个状态的好坏（或）每个状态的未来奖励预期值预测'
- en: '**Model:** Agent''s representation of the environment. A model predicts what
    the environment will do next:'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型：** 智能体对环境的表征。模型预测环境接下来会做什么：'
- en: '**Transitions:** p predicts the next state (that is, dynamics):'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转移：** p预测下一个状态（即动态）：'
- en: '![](img/88cda7e7-bfe8-482d-ad09-c50612f77308.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88cda7e7-bfe8-482d-ad09-c50612f77308.jpg)'
- en: '**Rewards:** R predicts the next (immediate) reward'
  id: totrans-44
  prefs:
  - PREF_UL
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励：** R预测下一个（即时）奖励'
- en: '![](img/036b8b26-fd87-4bd1-8172-8231499871f4.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/036b8b26-fd87-4bd1-8172-8231499871f4.jpg)'
- en: 'Let us explain the various categories possible in RL agent taxonomy, based
    on combinations of policy and value, and model individual components with the
    following maze example. In the following maze, you have both the start and the
    goal; the agent needs to reach the goal as quickly as possible, taking a path
    to gain the total maximum reward and the minimum total negative reward. Majorly
    five categorical way this problem can be solved:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过基于策略和值的组合以及用以下迷宫示例来解释RL智能体分类中的各种可能类别。在以下迷宫中，你既有起点，也有目标；智能体需要尽快到达目标，选择一条路径以获得最大的总奖励和最小的总负奖励。这个问题主要可以通过五种类别的方式来解决：
- en: Value based
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于价值
- en: Policy based
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于策略
- en: Actor critic
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Actor critic
- en: Model free
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无模型
- en: Model based
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于模型
- en: '![](img/20718ee8-c5f3-4aa4-b4ed-6ff3e9ec9b60.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20718ee8-c5f3-4aa4-b4ed-6ff3e9ec9b60.png)'
- en: Category 1 - value based
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类别 1 - 基于价值
- en: Value function does look like the right-hand side of the image (the sum of discounted
    future rewards) where every state has some value. Let's say, the state one step
    away from the goal has a value of -1; and two steps away from the goal has a value
    of -2\. In a similar way, the starting point has a value of -16\. If the agent
    gets stuck in the wrong place, the value could be as much as -24\. In fact, the
    agent does move across the grid based on the best possible values to reach its
    goal. For example, the agent is at a state with a value of -15\. Here, it can
    choose to move either north or south, so it chooses to move north due to the high
    reward, which is -14 rather, than moving south, which has a value of -16\. In
    this way, the agent chooses its path across the grid until it reaches the goal.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数看起来像图像的右侧（折扣未来奖励的总和），其中每个状态都有一个值。假设距离目标一步的状态值为-1；距离目标两步的状态值为-2。类似地，起始点的值为-16。如果智能体卡在错误的位置，值可能达到-24。事实上，智能体确实根据最佳值在网格中移动，以到达目标。例如，智能体处于值为-15的状态。在这里，它可以选择向北或向南移动，因此由于高奖励，它选择向北移动（-14），而不是向南移动（值为-16）。通过这种方式，智能体选择它在网格中的路径，直到到达目标。
- en: '**Value Function**: Only values are defined at all states'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值函数**：在所有状态下仅定义值'
- en: '**No Policy (Implicit)**: No exclusive policy is present; policies are chosen
    based on the values at each state'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无策略（隐式）：** 没有专门的策略；策略根据每个状态的值来选择'
- en: '![](img/273b4f5e-d2bc-440c-a62b-eecfd30159cd.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/273b4f5e-d2bc-440c-a62b-eecfd30159cd.png)'
- en: Category 2 - policy based
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类别 2 - 基于策略
- en: The arrows in the following image represent what an agent chooses as the direction
    of the next move while in any of these states. For example, the agent first moves
    east and then north, following all the arrows until the goal has been reached.
    This is also known as mapping from states to actions. Once we have this mapping,
    an agent just needs to read it and behave accordingly.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图中的箭头表示智能体在这些状态中选择的下一个移动方向。例如，智能体首先向东移动，然后向北移动，沿着所有箭头直到目标被达成。这也被称为从状态到动作的映射。一旦我们有了这个映射，智能体只需要读取它并相应地行动。
- en: '**Policy**: Policies or arrows that get adjusted to reach the maximum possible
    future rewards. As the name suggests, only policies are stored and optimized to
    maximize rewards.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**：策略或箭头，通过调整这些策略来达到最大可能的未来奖励。顾名思义，只有策略被存储并优化，以最大化奖励。'
- en: '**No value function**: No values exist for the states.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无价值函数**：状态没有对应的价值。'
- en: '![](img/eba78992-d0d0-4236-8fda-8d8ee466cbc8.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eba78992-d0d0-4236-8fda-8d8ee466cbc8.png)'
- en: Category 3 - actor-critic
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三类 - Actor-Critic
- en: 'In Actor-Critic, we have both policy and value functions (or a combination
    of value-based and policy-based). This method is the best of both worlds:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Actor-Critic 中，我们有策略和价值函数（或价值基和策略基的结合）。这种方法融合了两者的优点：
- en: Policy
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略
- en: Value Function
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价值函数
- en: Category 4 - model-free
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四类 - 无模型
- en: 'In RL, a fundamental distinction is if it is model-based or model-free. In
    model-free, we do not explicitly model the environment, or we do not know the
    entire dynamics of a complete environment. Instead, we just go directly to the
    policy or value function to gain the experience and figure out how the policy
    affects the reward:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，一个基本的区分是是否基于模型或无模型。在无模型方法中，我们并未显式地建模环境，或者我们不了解完整环境的所有动态。相反，我们直接通过策略或价值函数来获得经验，并了解策略如何影响奖励：
- en: Policy and/or value function
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略和/或价值函数
- en: No model
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无模型
- en: Category 5 - model-based
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五类 - 基于模型
- en: 'In model-based RL, we first build the entire dynamics of the environment:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于模型的强化学习中，我们首先建立整个环境的动态：
- en: Policy and/or value function
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略和/或价值函数
- en: Model
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型
- en: After going through all the above categories, the following Venn diagram shows
    the entire landscape of the taxonomy of an RL agent at one single place. If you
    pick up any paper related to reinforcement learning, those methods can fit in
    within any section of this landscape.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 经过上述所有类别后，以下维恩图展示了强化学习智能体的分类法的整体框架。如果你拿起任何关于强化学习的论文，这些方法都可以适应这个框架中的任意部分。
- en: '![](img/c859d161-fdc0-4d4f-af65-fb4ddfec3627.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c859d161-fdc0-4d4f-af65-fb4ddfec3627.png)'
- en: Fundamental categories in sequential decision making
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序决策中的基本类别
- en: 'There are two fundamental types of problems in sequential decision making:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序决策中有两种基本的类型问题：
- en: '**Reinforcement learning** (for example, autonomous helicopter, and so on):'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习**（例如，自主直升机等）：'
- en: Environment is initially unknown
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境最初是未知的
- en: Agent interacts with the environment and obtain policies, rewards, values from
    the environment
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体与环境交互并从环境中获得策略、奖励和价值
- en: Agent improves its policy
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体改善其策略
- en: '**Planning** (for example, chess, Atari games, and so on):'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划**（例如，象棋、Atari 游戏等）：'
- en: Model of environment or complete dynamics of environment is known
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境模型或完整的环境动态已知
- en: Agent performs computation with its model (without any external interaction)
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体通过其模型进行计算（无需任何外部交互）
- en: Agent improves its policy
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体改善其策略
- en: These are the type of problems also known as reasoning, searching, introspection,
    and so on
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些问题也被称为推理、搜索、自省等问题
- en: Though the preceding two categories can be linked together as per the given
    problem, but this is basically a broad view of the two types of setups.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前述的两类可以根据具体问题结合在一起，但这基本上是两种设置类型的宏观视角。
- en: Markov decision processes and Bellman equations
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程与贝尔曼方程
- en: '**Markov decision process** (**MDP**) formally describes an environment for
    reinforcement learning. Where:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程**（**MDP**）正式描述了强化学习中的环境。其定义如下：'
- en: Environment is fully observable
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境是完全可观察的
- en: Current state completely characterizes the process (which means the future state
    is entirely dependent on the current state rather than historic states or values)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前状态完全表征过程（这意味着未来状态完全依赖于当前状态，而不是历史状态或历史值）
- en: Almost all RL problems can be formalized as MDPs (for example, optimal control
    primarily deals with continuous MDPs)
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎所有的强化学习问题都可以形式化为 MDP（例如，最优控制主要处理连续的 MDP）
- en: '**Central idea of MDP:** MDP works on the simple Markovian property of a state;
    for example, *S[t+1]* is entirely dependent on latest state *S[t]* rather than
    any historic dependencies. In the following equation, the current state captures
    all the relevant information from the history, which means the current state is
    a sufficient statistic of the future:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**MDP 的核心思想：** MDP 基于状态的简单马尔可夫性属性工作；例如，*S[t+1]* 完全依赖于最新的状态 *S[t]*，而不是任何历史依赖关系。在以下方程中，当前状态捕获了来自历史的所有相关信息，这意味着当前状态是未来的充分统计量：'
- en: '![](img/923c18bd-709d-4898-8198-f2dea3196190.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/923c18bd-709d-4898-8198-f2dea3196190.jpg)'
- en: 'An intuitive sense of this property can be explained with the autonomous helicopter
    example: the next step is for the helicopter to move either to the right, left,
    to pitch, or to roll, and so on, entirely dependent on the current position of
    the helicopter, rather than where it was five minutes before.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特性可以通过自主直升机示例来直观解释：下一步，直升机将向右、向左、俯仰或滚动，等等，完全取决于直升机当前的位置，而不是五分钟前的位置。
- en: '**Modeling of MDP:** RL problems models the world using MDP formulation as
    a five tuple (*S, A, {P[sa]}, y, R*)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**MDP的建模：** 强化学习问题通过MDP的五元组(*S, A, {P[sa]}, y, R*)来建模世界'
- en: '*S* - Set of States (set of possible orientations of the helicopter)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S* - 状态集（直升机可能的所有朝向）'
- en: '*A* - Set of Actions (set of all possible positions that can pull the control
    stick)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A* - 动作集（可以拉动控制杆的所有可能位置的集合）'
- en: '*P[sa]* - State transition distributions (or state transition probability distributions)
    provide transitions from one state to another and the respective probabilities
    needed for the Markov process:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P[sa]* - 状态转移分布（或状态转移概率分布）提供从一个状态到另一个状态的转移及所需的相应概率，供马尔可夫过程使用：'
- en: '![](img/db779c49-a499-4e9b-a735-39c6f3337401.jpg)![](img/4f549720-521a-45c0-9ebd-aa388abc1c26.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db779c49-a499-4e9b-a735-39c6f3337401.jpg)![](img/4f549720-521a-45c0-9ebd-aa388abc1c26.jpg)'
- en: 'γ - Discount factor:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: γ - 折扣因子：
- en: '![](img/f9570224-0dcc-4670-bad4-c9a0f269444c.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9570224-0dcc-4670-bad4-c9a0f269444c.jpg)'
- en: 'R - Reward function (maps set of states to real numbers, either positive or
    negative):'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R - 奖励函数（将状态集映射为实数，可以是正数或负数）：
- en: '![](img/52688e9d-6663-4453-ae5b-f429b7d6eef4.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52688e9d-6663-4453-ae5b-f429b7d6eef4.jpg)'
- en: Returns are calculated by discounting the future rewards until terminal state
    is reached.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 返回是通过折扣未来奖励计算的，直到到达终止状态为止。
- en: '**Bellman Equations for MDP:** Bellman equations are utilized for the mathematical
    formulation of MDP, which will be solved to obtain the optimal policies of the
    environment. Bellman equations are also known as **dynamic programming equations**
    and are a necessary condition for the optimality associated with the mathematical
    optimization method that is known as dynamic programming. Bellman equations are
    linear equations which can be solvable for the entire environment. However, the
    time complexity for solving these equations is *O (n³)*, which becomes computationally
    very expensive when the number of states in an environment is large; and sometimes,
    it is not feasible to explore all the states because the environment itself is
    very large. In those scenarios, we need to look at other ways of solving problems.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝尔曼方程在MDP中的应用：** 贝尔曼方程用于MDP的数学公式化，解决这些方程可以获得环境的最优策略。贝尔曼方程也被称为**动态规划方程**，它是与数学优化方法——动态规划——相关的最优性所必需的条件。贝尔曼方程是线性方程，可以解出整个环境的解。然而，求解这些方程的时间复杂度是
    *O (n³)*，当环境中的状态数非常大时，计算开销会变得非常昂贵；有时，由于环境本身非常大，探索所有状态也不可行。在这种情况下，我们需要寻找其他问题求解方法。'
- en: 'In Bellman equations, value function can be decomposed into two parts:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝尔曼方程中，价值函数可以分解为两部分：
- en: Immediate reward *R[t+1]*, from the successor state you will end up with
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '从后续状态你将得到的即时奖励 *R[t+1]* '
- en: 'Discounted value of successor states *yv(S[t+1])* you will get from that timestep
    onwards:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从那个时间步开始，你将获得的后续状态的折现值 *yv(S[t+1])*：
- en: '![](img/74cb4e86-c22d-4b73-9204-3fefb68e908f.jpg)![](img/b0aef024-1db2-439c-bc0c-dec7e0b7ea20.jpg)![](img/6828d717-c803-4810-8a1d-8ed77e5462d8.jpg)![](img/eee8b434-ddc4-4fe4-8231-33e4479447a7.jpg)![](img/b05a1d44-b098-45b7-96dd-b8f2a8996362.jpg)![](img/865ad437-11e1-4a35-9d51-65873a08a4d0.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74cb4e86-c22d-4b73-9204-3fefb68e908f.jpg)![](img/b0aef024-1db2-439c-bc0c-dec7e0b7ea20.jpg)![](img/6828d717-c803-4810-8a1d-8ed77e5462d8.jpg)![](img/eee8b434-ddc4-4fe4-8231-33e4479447a7.jpg)![](img/b05a1d44-b098-45b7-96dd-b8f2a8996362.jpg)![](img/865ad437-11e1-4a35-9d51-65873a08a4d0.jpg)'
- en: '**Grid world example of MDP:** Robot navigation tasks live in the following
    type of grid world. An obstacle is shown the cell (2,2), through which the robot
    can''t navigate. We would like the robot to move to the upper-right cell (4,3)
    and when it reaches that position, the robot will get a reward of +1\. The robot
    should avoid the cell (4,2), as, if it moved into that cell, it would receive
    a-1 reward.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**MDP的网格世界示例：** 机器人导航任务生活在以下类型的网格世界中。一个障碍物位于单元格（2,2），机器人无法穿越该单元格。我们希望机器人移动到右上角的单元格（4,3），当它到达该位置时，机器人将获得+1的奖励。机器人应该避免进入单元格（4,2），因为如果进入该单元格，它将获得-1的奖励。'
- en: '![](img/01aa79ca-4cde-4eb6-bfea-81a7fc272b62.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01aa79ca-4cde-4eb6-bfea-81a7fc272b62.png)'
- en: 'Robot can be in any of the following positions:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人可以处于以下任何位置：
- en: '*11 States* - (except cell (2,2), in which we have got an obstacle for the
    robot)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*11个状态* - （除了 (2,2) 这个格子，那里有一个障碍物阻挡了机器人）'
- en: A = {N-north, S-south, E-east, W-west}
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A = {N-北，S-南，E-东，W-西}
- en: In the real world, robot movements are noisy, and a robot may not be able to
    move exactly where it has been asked to. Examples might include that some of its
    wheels slipped, its parts were loosely connected, it had incorrect actuators,
    and so on. When asked to move by 1 meter, it may not be able to move exactly 1
    meter; instead, it may move 90-105 centimeters, and so on.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，机器人的运动是嘈杂的，机器人可能无法精确地移动到它被要求到达的地方。例如，它的一些轮子可能打滑，部件可能连接松动，执行器可能不正确，等等。当要求它移动
    1 米时，机器人可能无法精确地移动 1 米；它可能只会移动 90-105 厘米，等等。
- en: 'In a simplified grid world, stochastic dynamics of a robot can be modeled as
    follows. If we command the robot to go north, there is a 10% chance that the robot
    could drag towards the left and a 10 % chance that it could drag towards the right.
    Only 80 percent of the time it may actually go north. When a robot bounces off
    the wall (including obstacles) and just stays at the same position, nothing happens:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个简化的网格世界中，机器人的随机动态可以如下建模。如果我们命令机器人向北移动，机器人有 10% 的概率会被拉向左侧，10% 的概率会被拉向右侧。只有
    80% 的概率它才会真正向北移动。当机器人碰到墙壁（包括障碍物）并停留在原地时，什么也不会发生：
- en: '![](img/941f1293-81d5-498e-93ef-7b181cda00b5.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/941f1293-81d5-498e-93ef-7b181cda00b5.png)'
- en: 'Every state in this grid world example is represented by (x, y) coordinates.
    Let''s say it is at state (3,1) and we asked the robot to move north, then the
    state transition probability matrices are as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网格世界示例中的每个状态都由 (x, y) 坐标表示。假设机器人位于状态 (3,1)，如果我们让它向北移动，则状态转移概率矩阵如下：
- en: '![](img/02519596-96cd-4180-b594-c978f3eb763f.jpg)![](img/63f2607c-00dd-45aa-8fb6-c0f155aa777d.jpg)![](img/399f84ac-a979-417a-8946-c114a76ea574.jpg)![](img/6c3be965-fa7b-42ad-9a3d-39ea351993a5.jpg)![](img/09268d1d-7300-4bcb-a092-ff56615f9e03.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02519596-96cd-4180-b594-c978f3eb763f.jpg)![](img/63f2607c-00dd-45aa-8fb6-c0f155aa777d.jpg)![](img/399f84ac-a979-417a-8946-c114a76ea574.jpg)![](img/6c3be965-fa7b-42ad-9a3d-39ea351993a5.jpg)![](img/09268d1d-7300-4bcb-a092-ff56615f9e03.jpg)'
- en: The probability of staying in the same position is 0 for the robot.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人停留在原地的概率为 0。
- en: 'As we know, that sum of all the state transition probabilities sums up to 1:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，所有状态转移概率的总和等于 1：
- en: '![](img/d3c6be8f-f270-454a-a54e-3baf328a9a86.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3c6be8f-f270-454a-a54e-3baf328a9a86.jpg)'
- en: 'Reward function:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数：
- en: '![](img/0188acae-45ca-4cf2-81cd-cf8c22722737.jpg)![](img/71493e81-cbf5-4b56-9791-1b67a42e5382.jpg)![](img/58dac8d9-b8d7-40e4-8902-689333af3cff.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0188acae-45ca-4cf2-81cd-cf8c22722737.jpg)![](img/71493e81-cbf5-4b56-9791-1b67a42e5382.jpg)![](img/58dac8d9-b8d7-40e4-8902-689333af3cff.jpg)'
- en: For all the other states, there are small negative reward values, which means
    it charges the robot for battery or fuel consumption when running around the grid,
    which creates solutions that do not waste moves or time while reaching the goal
    of reward +1, which encourages the robot to reach the goal as quickly as possible
    with as little fuel used as possible.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有其他状态，有一些小的负奖励值，这意味着在网格中跑来跑去时会扣除机器人的电池或燃料消耗，这就产生了不浪费移动或时间的解决方案，同时尽可能快速地达到奖励
    +1 的目标，这鼓励机器人以最少的燃料消耗尽可能快速地达到目标。
- en: The world ends when the robot reaches either +1 or -1 states. No more rewards
    are possible after reaching any of these states; these can be called absorbing
    states. These are zero-cost absorbing states and the robot stays there forever.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器人到达 +1 或 -1 状态时，世界结束。到达这些状态后将不再有任何奖励；这些状态可以称为吸收状态。这些是零成本的吸收状态，机器人会永远停留在那里。
- en: 'MDP working model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 工作模型：
- en: At state *S[0]*
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在状态 *S[0]*
- en: Choose *a[0]*
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择 *a[0]*
- en: Get to *S[1] ~ P*[*s0*, a0]
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到达 *S[1] ~ P*[*s0*, a0]
- en: Choose *a[1]*
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择 *a[1]*
- en: Get to *S[2] ~ P*[*s1*, *a1*]
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到达 *S[2] ~ P*[*s1*, *a1*]
- en: and so on ....
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以此类推….
- en: 'After a while, it takes all the rewards and sums up to obtain:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间后，它会获得所有奖励并将其累加：
- en: '![](img/e5cd5a27-28c1-47dd-9376-6c8fc379dba5.jpg)![](img/3d2fc2bd-25eb-48d6-a360-153642a335d7.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5cd5a27-28c1-47dd-9376-6c8fc379dba5.jpg)![](img/3d2fc2bd-25eb-48d6-a360-153642a335d7.jpg)'
- en: Discount factor models an economic application, in which one dollar earned today
    is more valuable than one dollar earned tomorrow.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣因子模型化了一种经济应用，其中今天赚到的一美元比明天赚到的一美元更有价值。
- en: 'The robot needs to choose actions over time (a[0], a[1], a[2, ....]) to maximize
    the expected payoff:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人需要在一段时间内选择行动（a[0]，a[1]，a[2]，......）以最大化预期回报：
- en: '![](img/ab87b8ba-c9da-409e-9a9f-23520ed62b1c.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab87b8ba-c9da-409e-9a9f-23520ed62b1c.jpg)'
- en: 'Over the period, a reinforcement learning algorithm learns a policy which is
    a mapping of actions for each state, which means it is a recommended action, which
    the robot needs to take based on the state in which it exists:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，一个强化学习算法学习一个策略，该策略是每个状态下的行动映射，意味着这是一个推荐的行动，机器人需要根据其所处的状态来采取行动：
- en: '![](img/ac60d23d-5e7a-4425-b650-cb68a836a197.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac60d23d-5e7a-4425-b650-cb68a836a197.jpg)'
- en: '**Optimal Policy for Grid World:** Policy maps from states to actions, which
    means that, if you are in a particular state, you need to take this particular
    action. The following policy is the optimal policy which maximizes the expected
    value of the total payoff or sum of discounted rewards. Policy always looks into
    the current state rather than previous states, which is the Markovian property:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**网格世界的最优策略：** 策略是从状态到行动的映射，这意味着如果你处于某个特定状态，你需要采取这一特定行动。以下策略是最优策略，它最大化了总回报或折扣奖励的期望值。策略总是根据当前状态来决策，而不是之前的状态，这就是马尔可夫性质：'
- en: '![](img/77bc9123-6017-4a05-b603-72c8782e13be.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77bc9123-6017-4a05-b603-72c8782e13be.png)'
- en: 'One tricky thing to look at is at the position (3,1): optimal policy shows
    to go left (West) rather than going (north), which may have a fewer number of
    states; however, we have an even riskier state that we may step into. So, going
    left may take longer, but it safely arrives at the destination without getting
    into negative traps. These types of things can be obtained from computing, which
    do not look obvious to humans, but a computer is very good at coming up with these
    policies:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个问题是位置（3,1）：最优策略显示应向左（西）走，而不是向北走，虽然向北走可能涉及的状态数较少；但是，我们也可能进入一个更危险的状态。因此，向左走可能需要更长时间，但可以安全到达目的地而不会陷入负面陷阱。这些类型的结论可以通过计算获得，虽然对人类而言不明显，但计算机在提出这些策略时非常擅长：
- en: 'Define: *V^π, V*, π**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：*V^π, V*, π**
- en: '*V^π* = For any given policy π, value function is *V^π : S -> R* such that
    *V^π (S)* is expected total payoff starting in state S, and execute π'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*V^π* = 对于任何给定的策略π，价值函数为 *V^π : S -> R*，使得 *V^π (S)* 是从状态S开始，执行π后的期望总回报'
- en: '![](img/2e06c006-31da-4b3f-bc50-98ac530295f7.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e06c006-31da-4b3f-bc50-98ac530295f7.jpg)'
- en: '**Random policy for grid world:** The following is an example of a random policy
    and its value functions. This policy is a rather bad policy with negative values.
    For any policy, we can write down the value function for that particular policy:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**网格世界的随机策略：** 以下是一个随机策略及其价值函数的示例。这个策略是一个相当糟糕的策略，具有负值。对于任何策略，我们都可以写出该策略的价值函数：'
- en: '![](img/bf01ee52-fe48-4591-bc7b-097498609169.png)![](img/7e0c0c6c-5af6-495d-a087-199d2b79b60e.jpg)![](img/ca313eed-3330-4634-92bf-1a9dc2384f66.jpg)![](img/3b2db0f4-74d5-4f46-b1b5-d2ed7fa080aa.jpg)![](img/7cb27a71-2014-4494-80e6-c0643be76761.jpg)![](img/2f6ff475-d699-49c6-8e8e-f126b98603cd.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf01ee52-fe48-4591-bc7b-097498609169.png)![](img/7e0c0c6c-5af6-495d-a087-199d2b79b60e.jpg)![](img/ca313eed-3330-4634-92bf-1a9dc2384f66.jpg)![](img/3b2db0f4-74d5-4f46-b1b5-d2ed7fa080aa.jpg)![](img/7cb27a71-2014-4494-80e6-c0643be76761.jpg)![](img/2f6ff475-d699-49c6-8e8e-f126b98603cd.jpg)'
- en: In simple English, Bellman equations illustrate that the value of the current
    state is equal to the immediate reward and discount factor applied to the expected
    total payoff of new states (*S'*) multiplied by their probability to take action
    (policy) into those states.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，Bellman 方程说明了当前状态的价值等于即时奖励和折扣因子应用于新状态（*S'*)的期望总回报，这些回报根据它们进入这些状态的概率进行加权。
- en: Bellman equations are used to solve value functions for a policy in close form,
    given fixed policy, how to solve the value function equations.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Bellman 方程用于求解策略的价值函数的闭式解，给定固定策略，如何求解价值函数方程。
- en: Bellman equations impose a set of linear constraints on value functions. It
    turns out that we solve the value function at any state *S* by solving a set of
    linear equations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Bellman 方程对价值函数施加了一组线性约束。事实证明，通过求解一组线性方程，我们可以在任何状态 *S* 下求解其价值函数。
- en: '**Example of Bellman equations with a grid world problem:**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bellman 方程在网格世界问题中的示例：**'
- en: The chosen policy for cell *(3,1)* is to move north. However, we have stochasticity
    in the system that about 80 percent of the time it moves in the said direction,
    and *20%* of the time it drifts sideways, either left (10 percent) or right (10
    percent).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为单元格 *(3,1)* 选择的策略是向北移动。然而，我们的系统存在随机性，大约80%的时间它会朝着指定方向移动，而20%的时间它会偏离，向左（10%）或向右（10%）偏移。
- en: '![](img/994596ff-4652-4fe9-922a-feed7fd009af.jpg)![](img/4d2fdf17-c7b3-4874-8674-795915340535.png)![](img/84f35a41-5cfc-4808-bf9b-300e4c122e1e.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/994596ff-4652-4fe9-922a-feed7fd009af.jpg)![](img/4d2fdf17-c7b3-4874-8674-795915340535.png)![](img/84f35a41-5cfc-4808-bf9b-300e4c122e1e.jpg)'
- en: 'Similar equations can be written for all the 11 states of the MDPs within the
    grid. We can obtain the following metrics, from which we will solve all the unknown
    values, using a system of linear equation methods:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 可以为网格中的所有11个MDP状态写出类似的方程。我们可以从中获得以下度量值，利用线性方程组的方法来解决所有未知值：
- en: 11 equations
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 11个方程
- en: 11 unknown value function variables
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 11个未知值函数变量
- en: 11 constraints
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 11个约束条件
- en: This is solving an `n` variables with `n` equations problem, for which we can
    find the exact form of a solution using a system of equations easily to get an
    exact solution for V (π) for the entire closed form of the grid, which consists
    of all the states.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是解决一个`n`个变量与`n`个方程的问题，我们可以通过使用方程组轻松找到一个解决方案的确切形式，进而得到整个网格的V (π)的精确解，网格包含了所有的状态。
- en: Dynamic programming
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态规划
- en: Dynamic programming is a sequential way of solving complex problems by breaking
    them down into sub-problems and solving each of them. Once it solves the sub-problems,
    then it puts those subproblem solutions together to solve the original complex
    problem. In the reinforcement learning world, Dynamic Programming is a solution
    methodology to compute optimal policies given a perfect model of the environment
    as a Markov Decision Process (MDP).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划是一种通过将复杂问题分解为子问题并逐一解决它们来顺序求解问题的方法。一旦子问题解决，它就会将这些子问题的解组合起来解决原始的复杂问题。在强化学习中，动态规划是一种在环境的完美模型作为马尔可夫决策过程（MDP）下计算最优策略的方法论。
- en: 'Dynamic programming holds good for problems which have the following two properties.
    MDPs, in fact, satisfy both properties, which makes DP a good fit for solving
    them by solving Bellman Equations:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划适用于具有以下两个性质的问题。事实上，MDP满足这两个性质，这使得动态规划非常适合通过求解贝尔曼方程来解决它们：
- en: Optimal substructure
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最优子结构
- en: Principle of optimality applies
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最优性原理适用
- en: Optimal solution can be decomposed into sub-problems
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最优解可以分解成子问题
- en: Overlapping sub-problems
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子问题重叠
- en: Sub-problems recur many times
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子问题重复多次
- en: Solutions can be cached and reused
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解可以被缓存和重用
- en: MDP satisfies both the properties - luckily!
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MDP满足这两个性质——幸运的是！
- en: Bellman equations have recursive decomposition of state-values
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝尔曼方程具有状态值的递归分解
- en: Value function stores and reuses solutions
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值函数存储并重用解决方案
- en: Though, classical DP algorithms are of limited utility in reinforcement learning,
    both because of their assumptions of a perfect model and high computational expense.
    However, it is still important, as they provide an essential foundation for understanding
    all the methods in the RL domain.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，经典的动态规划算法在强化学习中的应用有限，原因在于它们假设了一个完美的模型，并且计算开销较大。然而，它们依然很重要，因为它们为理解强化学习领域的所有方法提供了必要的基础。
- en: Algorithms to compute optimal policy using dynamic programming
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用动态规划计算最优策略的算法
- en: 'Standard algorithms to compute optimal policies for MDP utilizing Dynamic Programming
    are as follows, and we will be covering both in detail in later sections of this
    chapter:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 计算MDP最优策略的标准算法利用了动态规划，以下是相关算法，我们将在本章后续部分详细讲解：
- en: '**Value Iteration algorithm:** An iterative algorithm, in which state values
    are iterated until it reaches optimal values; and, subsequently, optimum values
    are utilized to determine the optimal policy'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值迭代算法：** 一种迭代算法，其中状态值不断迭代直到达到最优值；随后，最优值被用于确定最优策略'
- en: '**Policy Iteration algorithm:** An iterative algorithm, in which policy evaluation
    and policy improvements are utilized alternatively to reach optimal policy'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略迭代算法：** 一种迭代算法，其中策略评估和策略改进交替进行，以达到最优策略'
- en: '**Value Iteration algorithm:** Value Iteration algorithms are easy to compute
    for the very reason of applying iteratively on only state values. First, we will
    compute the optimal value function *V**, then plug those values into the optimal
    policy equation to determine the optimal policy. Just to give the size of the
    problem, for 11 possible states, each state can have four policies (N-north, S-south,
    E-east, W-west), which gives an overall 11⁴ possible policies. The value iteration
    algorithm consists of the following steps:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**值迭代算法：** 值迭代算法之所以容易计算，是因为它仅对状态值进行迭代计算。首先，我们将计算最优值函数 *V*，然后将这些值代入最优策略方程，以确定最优策略。为了说明问题的规模，对于
    11 个可能的状态，每个状态可以有四个策略（N-北，S-南，E-东，W-西），因此总共有 11⁴ 种可能的策略。值迭代算法包括以下步骤：'
- en: Initialize *V(S) = 0* for all states S
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 *V(S) = 0* 对于所有状态 S
- en: 'For every S, update:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个 S，更新：
- en: '![](img/c9d080c2-be67-4ae3-af04-e0b4c8d7058b.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9d080c2-be67-4ae3-af04-e0b4c8d7058b.jpg)'
- en: 'By repeatedly computing step 2, we will eventually converge to optimal values
    for all the states:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过反复计算步骤 2，我们最终会收敛到所有状态的最优值：
- en: '![](img/6e4c7025-ccd4-467c-83f2-acac295a8f01.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e4c7025-ccd4-467c-83f2-acac295a8f01.jpg)'
- en: There are two ways of updating the values in step 2 of the algorithm
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法的步骤 2 中，有两种更新值的方法
- en: '**Synchronous update** - By performing synchronous update (or Bellman backup
    operator) we will perform RHS computing and substitute LHS of the equation represented
    as follows:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步更新** - 通过执行同步更新（或贝尔曼备份操作符），我们将执行右侧计算并替换方程的左侧，如下所示：'
- en: '![](img/ae39ce46-6137-4a13-9820-90e986fd4ab1.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae39ce46-6137-4a13-9820-90e986fd4ab1.jpg)'
- en: '**Asynchronous update** - Update the values of the states one at a time rather
    than updating all the states at the same time, in which states will be updated
    in a fixed order (update state number 1, followed by 2, and so on.). During convergence,
    asynchronous updates are a little faster than synchronous updates.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步更新** - 一次更新一个状态的值，而不是同时更新所有状态，在这种情况下，状态会按固定顺序更新（先更新状态 1，再更新状态 2，以此类推）。在收敛过程中，异步更新比同步更新稍快。'
- en: '**Illustration of value iteration on grid world example:** The application
    of the Value iteration on a grid world is explained in the following image, and
    the complete code for solving a real problem is provided at the end of this section.
    After applying the previous value iteration algorithm on MDP using Bellman equations,
    we''ve obtained the following optimal values V* for all the states (Gamma value
    chosen as *0.99*):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**值迭代在网格世界示例中的说明：** 值迭代在网格世界中的应用在下图中进行了说明，解决实际问题的完整代码会在本节末尾提供。在使用贝尔曼方程对 MDP
    应用之前的值迭代算法后，我们得到了以下所有状态的最优值 V*（Gamma 值选择为 *0.99*）：'
- en: '![](img/803ddf4b-5afc-4c68-a2e6-3e1f8ac21182.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/803ddf4b-5afc-4c68-a2e6-3e1f8ac21182.png)'
- en: 'When we plug these values in to our policy equation, we obtain the following
    policy grid:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这些值代入到我们的策略方程时，我们得到以下的策略网格：
- en: '![](img/7988638f-e34f-4639-aee5-3703faad8c84.jpg)![](img/399adc98-5126-4423-b205-12f0027853fd.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7988638f-e34f-4639-aee5-3703faad8c84.jpg)![](img/399adc98-5126-4423-b205-12f0027853fd.png)'
- en: 'Here, at position (3,1) we would like to prove mathematically why an optimal
    policy suggests taking going left (west) rather than moving up (north):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，在位置 (3,1) 我们想通过数学证明为什么最优策略建议向左（西）而不是向上（北）移动：
- en: '![](img/eee54233-072a-4ab6-8025-b6baced8efe0.jpg)![](img/01ddc3cb-9371-489a-af1f-d6a6aa7f7163.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eee54233-072a-4ab6-8025-b6baced8efe0.jpg)![](img/01ddc3cb-9371-489a-af1f-d6a6aa7f7163.jpg)'
- en: Due to the wall, whenever the robot tries to move towards South (downwards side),
    it will remain in the same place, hence we assigned the value of the current position
    0.71 for a probability of 0.1.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 由于墙壁，每当机器人尝试向南（下方）移动时，它会停留在原地，因此我们为当前位置分配了 0.71 的值，概率为 0.1。
- en: 'Similarly, for north, we calculated the total payoff as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，对于北，我们计算了如下的总收益：
- en: '![](img/b8aa1f41-aa36-4f09-ba60-dc1b22f0305d.jpg)![](img/7473b012-bdea-4528-a114-b53f1e4d13fc.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8aa1f41-aa36-4f09-ba60-dc1b22f0305d.jpg)![](img/7473b012-bdea-4528-a114-b53f1e4d13fc.jpg)'
- en: So, it would be optimal to move towards the west rather than north, and therefore
    the optimal policy is chosen to do so.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，向西而非向北移动会是最优的选择，因此最优策略选择了这种方式。
- en: '**Policy Iteration Algorithm:** Policy iterations are another way of obtaining
    optimal policies for MDP in which policy evaluation and policy improvement algorithms
    are applied iteratively until the solution converges to the optimal policy. Policy
    Iteration Algorithm consists of the following steps:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略迭代算法：** 策略迭代是获得 MDP 最优策略的另一种方法，在这种方法中，策略评估和策略改进算法被反复应用，直到解决方案收敛到最优策略。策略迭代算法包括以下步骤：'
- en: Initialize random policy π
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化随机策略 π
- en: Repeatedly do the following until convergence happens
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复执行以下操作，直到收敛发生
- en: 'Solve Bellman equations for the current policy for obtaining V^π for using
    system of linear equations:'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对当前策略求解贝尔曼方程，以使用线性方程组获得 V^π：
- en: '![](img/129df1ca-f1a4-45dc-8821-555ebf29aed2.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/129df1ca-f1a4-45dc-8821-555ebf29aed2.jpg)'
- en: 'Update the policy as per the new value function to improve the policy by pretending
    the new value is an optimal value using argmax formulae:'
  id: totrans-204
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据新的价值函数更新策略，通过将新值假设为最优值，使用 argmax 公式改进策略：
- en: '![](img/f2cc7a6b-b385-42ac-9ff6-27e37de8abfa.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2cc7a6b-b385-42ac-9ff6-27e37de8abfa.jpg)'
- en: 'By repeating these steps, both value and policy will converge to optimal values:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过重复这些步骤，值和策略将会收敛到最优值：
- en: '![](img/98961d5f-f6e5-44f4-8614-d75b289e7716.jpg)![](img/6579aed2-6181-4017-8628-a14997334d8e.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98961d5f-f6e5-44f4-8614-d75b289e7716.jpg)![](img/6579aed2-6181-4017-8628-a14997334d8e.jpg)'
- en: Policy iterations tend to do well with smaller problems. If an MDP has an enormous
    number of states, policy iterations will be computationally expensive. As a result,
    large MDPs tend to use value iterations rather than policy iterations.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代通常适用于较小的问题。如果一个 MDP 的状态数量非常庞大，策略迭代会在计算上非常昂贵。因此，大型 MDP 通常使用值迭代而不是策略迭代。
- en: '**What if we don''t know exact state transition probabilities in real life
    examples** *P[s,a]* **?**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果我们在现实生活中不知道确切的状态转移概率** *P[s,a]* **该怎么办？**'
- en: 'We need to estimate the probabilities from the data by using the following
    simple formulae:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用以下简单公式从数据中估计概率：
- en: '![](img/c4845d43-ffbf-4f8b-bc88-89eaf86a1a7d.jpg)![](img/9ff1e5fa-788d-43b7-91b8-e7abda3a9ea5.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4845d43-ffbf-4f8b-bc88-89eaf86a1a7d.jpg)![](img/9ff1e5fa-788d-43b7-91b8-e7abda3a9ea5.jpg)'
- en: If for some states no data is available, which leads to 0/0 problem, we can
    take a default probability from uniform distributions.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某些状态没有数据可用，导致出现 0/0 问题，我们可以从均匀分布中获取一个默认概率。
- en: Grid world example using value and policy iteration algorithms with basic Python
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基本 Python 的值迭代和策略迭代算法的网格世界示例
- en: 'The classic grid world example has been used to illustrate value and policy
    iterations with Dynamic Programming to solve MDP''s Bellman equations. In the
    following grid, the agent will start at the south-west corner of the grid in (1,1)
    position and the goal is to move towards the north-east corner, to position (4,3).
    Once it reaches the goal, the agent will get a reward of +1\. During the journey,
    it should avoid the danger zone (4,2), because this will give out a negative penalty
    of reward -1\. The agent cannot get into the position where the obstacle (2,2)
    is present from any direction. Goal and danger zones are the terminal states,
    which means the agent continues to move around until it reaches one of these two
    states. The reward for all the other states would be -0.02\. Here, the task is
    to determine the optimal policy (direction to move) for the agent at every state
    (11 states altogether), so that the agent''s total reward is the maximum, or so
    that the agent can reach the goal as quickly as possible. The agent can move in
    4 directions: north, south, east and west.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的网格世界示例被用来通过动态规划说明值迭代和策略迭代，以求解 MDP 的贝尔曼方程。在以下网格中，代理从网格的西南角（1,1）位置开始，目标是向东北角（4,3）移动。一旦到达目标，代理将获得
    +1 的奖励。在途中，它应该避免进入危险区（4,2），因为这会导致 -1 的负奖励。代理不能从任何方向进入有障碍物（2,2）的位置。目标区和危险区是终止状态，意味着代理会继续移动，直到到达这两个状态之一。其他所有状态的奖励为
    -0.02。在这里，任务是为代理在每个状态（共 11 个状态）确定最优策略（移动方向），以使代理的总奖励最大化，或者使代理尽可能快地到达目标。代理可以朝四个方向移动：北、南、东和西。
- en: '![](img/4b027216-c27a-401d-88e6-e587ff48c87c.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b027216-c27a-401d-88e6-e587ff48c87c.png)'
- en: The complete code was written in the Python programming language with class
    implementation. For further reading, please refer to object oriented programming
    in Python to understand class, objects, constructors, and so on.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码使用Python编程语言编写，带有类实现。欲了解更多，请参考Python中的面向对象编程，了解类、对象、构造函数等。
- en: 'Import the `random` package for generating moves in any of the N, E, S, W directions:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`random`包以生成任何N、E、S、W方向的动作：
- en: '[PRE0]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following `argmax` function calculated the maximum state among the given
    states, based on the value for each state:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`argmax`函数根据每个状态的值计算给定状态中的最大状态：
- en: '[PRE1]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To add two vectors at component level, the following code has been utilized
    for:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要在分量级别上添加两个向量，以下代码已被用于：
- en: '[PRE2]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Orientations provide what the increment value would be, which needs to be added
    to the existing position of the agent; orientations can be applied on the *x*-axis
    or *y*-axis:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 方位提供了需要加到代理当前位置的增量值；方位可以作用于*x*轴或*y*轴：
- en: '[PRE3]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following function is used to turn the agent in the right direction, as
    we know at every command the agent moves in that direction about 80% of the time,
    whilst 10% of the time it would move right, and 10% it would move left.:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数用于将代理转向正确的方向，因为我们知道在每个命令下，代理大约80%的时间会按该方向移动，10%的时间会向右移动，10%的时间会向左移动：
- en: '[PRE4]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The Markov decision process is defined as a class here. Every MDP is defined
    by an initial position, state, transition model, reward function, and gamma values.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程在这里定义为一个类。每个MDP由初始位置、状态、转移模型、奖励函数和gamma值定义。
- en: '[PRE5]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Returns a numeric reward for the state:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 返回状态的数值奖励：
- en: '[PRE6]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Transition model with from a state and an action returns a list of (probability,
    result-state) pairs for each state:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 转移模型从某个状态和动作返回每个状态的（概率，结果状态）对的列表：
- en: '[PRE7]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Set of actions that can be performed at a particular state:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在特定状态下执行的动作集：
- en: '[PRE8]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Class `GridMDP` is created for modeling a 2D grid world with grid values at
    each state, terminal positions, initial position, and gamma value (discount):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridMDP`类用于建模一个二维网格世界，其中每个状态有网格值、终端位置、初始位置和折扣值（gamma）：'
- en: '[PRE9]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following code is used for reversing the grid, as we would like to see
    *row 0* at the bottom instead of at the top:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于反转网格，因为我们希望将*第0行*放在底部，而不是顶部：
- en: '[PRE10]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following `__init__` command is a constructor used within the grid class
    for initializing parameters:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`__init__`命令是一个构造函数，用于在网格类中初始化参数：
- en: '[PRE11]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'State transitions provide randomly 80% toward the desired direction and 10%
    for left and right. This is to model the randomness in a robot which might slip
    on the floor, and so on:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 状态转移在80%的情况下随机朝向期望方向，10%朝左，10%朝右。这是为了模拟机器人在地板上可能滑动的随机性，等等：
- en: '[PRE12]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Returns the state that results from going in the direction, subject to where
    that state is in the list of valid states. If the next state is not in the list,
    like hitting the wall, then the agent should remain in the same state:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 返回执行指定方向后产生的状态，前提是该状态在有效状态列表中。如果下一个状态不在列表中，例如撞墙，则代理应保持在同一状态：
- en: '[PRE13]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Convert a mapping from (x, y) to v into [[..., v, ...]] grid:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 将从(x, y)到v的映射转换为[[...，v，...]]网格：
- en: '[PRE14]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Convert orientations into arrows for better graphical representations:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 将方位转换为箭头，以便更好地进行图形表示：
- en: '[PRE15]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following code is used for solving an MDP, using value iterations, and
    returns optimum state values:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于通过值迭代求解MDP，并返回最优状态值：
- en: '[PRE16]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Given an MDP and a utility function `STS`, determine the best policy, as a
    mapping from state to action:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个MDP和一个效用函数`STS`，确定最佳策略，即从状态到动作的映射：
- en: '[PRE17]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The expected utility of doing `a` in state `s`, according to the MDP and STS:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 根据MDP和STS，执行动作`a`在状态`s`中的预期效用：
- en: '[PRE18]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following code is used to solve an MDP using policy iterations by alternatively
    performing policy evaluation and policy improvement steps:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于通过策略迭代求解MDP，通过交替执行策略评估和策略改进步骤：
- en: '[PRE19]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following code is used to return an updated utility mapping `U` from each
    state in the MDP to its utility, using an approximation (modified policy iteration):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于返回从每个MDP状态到其效用的更新效用映射`U`，使用近似（修改后的策略迭代）：
- en: '[PRE20]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following is the input grid of a 4 x 3 grid environment that presents the
    agent with a sequential decision-making problem:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个4 x 3网格环境的输入网格，呈现出代理的顺序决策问题：
- en: '[PRE21]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following code is for performing a value iteration on the given sequential
    decision-making environment:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于在给定的顺序决策环境中执行价值迭代：
- en: '[PRE22]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/4daaf65e-72f1-4d5c-81a9-9411ca3e6166.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4daaf65e-72f1-4d5c-81a9-9411ca3e6166.png)'
- en: 'The code for policy iteration is:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代的代码如下：
- en: '[PRE23]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](img/e01853b9-4c08-4972-a213-c7f30ddb2b43.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e01853b9-4c08-4972-a213-c7f30ddb2b43.png)'
- en: From the preceding output with two results, we can conclude that both value
    and policy iterations provide the same optimal policy for an agent to move across
    the grid to reach the goal state in the quickest way possible. When the problem
    size is large enough, it is computationally advisable to go for value iteration
    rather than policy iteration, as in policy iterations, we need to perform two
    steps at every iteration of the policy evaluation and policy improvement.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出结果来看，两个结果表明，价值迭代和策略迭代为智能体提供了相同的最优策略，使其能在最短时间内通过网格到达目标状态。当问题规模足够大时，从计算角度考虑，选择价值迭代更为可取，因为在策略迭代中，每次迭代都需要进行两个步骤：策略评估和策略改进。
- en: Monte Carlo methods
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法
- en: Using **Monte Carlo** (**MC**) methods, we will compute the value functions
    first and determine the optimal policies. In this method, we do not assume complete
    knowledge of the environment. MC require only experience, which consists of sample
    sequences of states, actions, and rewards from actual or simulated interactions
    with the environment. Learning from actual experiences is striking because it
    requires no prior knowledge of the environment's dynamics, but still attains optimal
    behavior. This is very similar to how humans or animals learn from actual experience
    rather than any mathematical model. Surprisingly, in many cases, it is easy to
    generate experience sampled according to the desired probability distributions,
    but infeasible to obtain the distributions in explicit form.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**蒙特卡洛**（**MC**）方法，我们首先计算价值函数，并确定最优策略。在这种方法中，我们不假设对环境有完全的了解。蒙特卡洛方法只需要经验，这些经验包括来自与环境实际或模拟互动的状态、动作和回报的样本序列。从实际经验中学习非常具有意义，因为它不依赖于对环境动态的先验知识，但仍能获得最优行为。这与人类或动物从实际经验中学习而非依赖任何数学模型的方式非常相似。令人惊讶的是，在许多情况下，根据所需的概率分布生成样本经验是容易的，但获得这些分布的显式形式却不可行。
- en: Monte Carlo methods solve the reinforcement learning problem based on averaging
    the sample returns over each episode. This means that we assume experience is
    divided into episodes, and that all episodes eventually terminate, no matter what
    actions are selected. Values are estimated and policies are changed only after
    the completion of each episode. MC methods are incremental in an episode-by-episode
    sense, but not in a step-by-step (which is an online learning, and which we will
    cover the same in Temporal Difference learning section) sense.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法通过对每个回合中的样本回报进行平均来解决强化学习问题。这意味着我们假设经验被划分为多个回合，并且所有回合最终都会终止，无论选择什么动作。只有在每个回合完成后，才会估算价值并改变策略。蒙特卡洛方法是按回合逐步增量的，而不是按步骤增量（这属于在线学习，我们将在时间差学习部分进行相同讨论）。
- en: Monte Carlo methods sample and average returns for each state-action pair over
    the episode. However, within the same episode, the return after taking an action
    in one stage depends on the actions taken in later states. Because all the action
    selections are undergoing learning, the problem becomes non-stationary from the
    point of view of the earlier state. In order to handle this non-stationarity,
    we adapt the idea of policy iteration from dynamic programming, in which, first,
    we compute the value function for a fixed arbitrary policy; and, later, we improve
    the policy.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法通过对每个状态-动作对在整个回合中的回报进行采样和平均。然而，在同一回合内，采取某一动作后的回报依赖于后续状态中采取的动作。由于所有的动作选择都在进行学习，从早期状态的角度来看，问题变得非平稳。为了处理这种非平稳性，我们借用了动态规划中的策略迭代思想，其中，首先计算固定任意策略的价值函数；然后，再改进策略。
- en: Monte Carlo prediction
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛预测
- en: 'As we know, Monte Carlo methods predict the state-value function for a given
    policy. The value of any state is the expected return or expected cumulative future
    discounted rewards starting from that state. These values are estimated in MC
    methods simply to average the returns observed after visits to that state. As
    more and more values are observed, the average should converge to the expected
    value based on the law of large numbers. In fact, this is the principle applicable
    in all Monte Carlo methods. The Monte Carlo Policy Evaluation Algorithm consist
    of the following steps:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，蒙特卡洛方法预测给定策略的状态值函数。任何状态的值是从该状态开始的预期回报或预期累计未来折扣奖励。这些值在蒙特卡洛方法中通过简单地对访问该状态后的回报进行平均来估算。随着越来越多的值被观察到，平均值应该根据大数法则收敛到预期值。实际上，这是所有蒙特卡洛方法适用的原理。蒙特卡洛策略评估算法包括以下步骤：
- en: 'Initialize:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化：
- en: '![](img/5ee2d232-f6ef-433f-8537-9f55928bdc94.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ee2d232-f6ef-433f-8537-9f55928bdc94.jpg)'
- en: 'Repeat forever:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无限重复：
- en: Generate an episode using π
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用π生成一个回合
- en: 'For each state *s* appearing in the episode:'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对回合中出现的每个状态*s*：
- en: G return following the first occurrence of *s*
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: G 回报，跟随第一次出现的*s*
- en: Append *G* to Returns(s)
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将*G*附加到Returns(s)
- en: V(s)  average(Returns(s))
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: V(s)  平均值（Returns(s)）
- en: The suitability of Monte Carlo prediction on grid-world problems
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛预测在网格世界问题上的适用性
- en: The following diagram has been plotted for illustration purposes. However, practically,
    Monte Carlo methods cannot be easily used for solving grid-world type problems,
    due to the fact that termination is not guaranteed for all the policies. If a
    policy was ever found that caused the agent to stay in the same state, then the
    next episode would never end. Step-by-step learning methods like (**State-Action-Reward-State-Action**
    (**SARSA**), which we will be covering in a later part of this chapter in TD Learning
    Control) do not have this problem because they quickly learn during the episode
    that such policies are poor, and switch to something else.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 下图用于说明目的。然而，实际上，由于并非所有策略都能保证终止，蒙特卡洛方法不能轻易用于解决网格世界类型的问题。如果发现某个策略导致代理停留在同一状态，那么下一个回合将永远不会结束。像**（状态-动作-奖励-状态-动作）**（**SARSA**，我们将在本章后面讨论的时间差分学习控制方法中讲解）这样的逐步学习方法没有这个问题，因为它们在回合过程中迅速学习到这些策略是差的，并会切换到其他策略。
- en: '![](img/03344b7c-3d21-4779-8282-22fb609f9bb7.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03344b7c-3d21-4779-8282-22fb609f9bb7.png)'
- en: Modeling Blackjack example of Monte Carlo methods using Python
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python建模二十一点游戏的蒙特卡洛方法
- en: 'The objective of the popular casino card game Blackjack is to obtain cards,
    the sum of whose numerical values is as great as possible, without exceeding the
    value of 21\. All face cards (king, queen, and jack) count as 10, and an ace can
    count as either 1 or as 11, depending upon the way the player wants to use it.
    Only the ace has this flexibility option. All the other cards are valued at face
    value. The game begins with two cards dealt with both dealer and players. One
    of the dealer''s cards is face up and the other is face down. If the player has
    a ''Natural 21'' from these first two cards (an ace and a 10-card), the player
    wins unless the dealer also has a Natural, in which case the game is a draw. If
    the player does not have a natural, then he can ask for additional cards, one
    by one (hits), until he either stops (sticks) or exceeds 21 (goes bust). If the
    player goes bust, he loses; if the player sticks, then it''s the dealer''s turn.
    The dealer hits or sticks according to a fixed strategy without choice: the dealer
    usually sticks on any sum of 17 or greater, and hits otherwise. If the dealer
    goes bust, then the player automatically wins. If he sticks, the outcome would
    be either win, lose, or draw, determined by whether the dealer or the player''s
    sum total is closer to 21.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 流行赌场扑克牌游戏“二十一点”的目标是获得一组牌，这些牌的点数总和尽可能大，但不得超过21点。所有的面牌（国王、皇后和杰克）都算作10点，而A牌可以根据玩家的需求算作1点或11点，只有A牌具有这种灵活性选项。其他所有牌按面值计算。游戏开始时，庄家和玩家各发两张牌，其中庄家的其中一张牌是面朝上的，另一张是面朝下的。如果玩家从这两张牌中得到“自然21点”（即一张A牌和一张10点牌），那么玩家获胜，除非庄家也有自然21点，在这种情况下，游戏为平局。如果玩家没有自然21点，则可以要求继续抽牌，一张一张地抽（称为“要牌”），直到他选择停牌（不再要牌）或超过21点（称为“爆牌”）。如果玩家爆牌，则玩家失败；如果玩家选择停牌，则轮到庄家。庄家根据固定策略选择要牌或停牌，无法选择：通常庄家在总点数为17点或更高时选择停牌，低于17点时选择要牌。如果庄家爆牌，则玩家自动获胜。如果庄家停牌，则游戏结果将是胜利、失败或平局，取决于庄家和玩家的点数哪个更接近21点。
- en: '![](img/a2357e08-6d21-41db-b26a-257ee659ea18.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2357e08-6d21-41db-b26a-257ee659ea18.png)'
- en: The Blackjack problem can be formulated as an episodic finite MDP, in which
    each game of Blackjack is an episode. Rewards of +1, -1, and 0 are given for winning,
    losing, and drawing for each episode respectively at the terminal state and the
    remaining rewards within the state of game are given the value as 0 with no discount
    (gamma = 1). Therefore, the terminal rewards are also the returns for this game.
    We draw the cards from an infinite deck so that no traceable pattern exists. The
    entire game is modeled in Python in the following code.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 黑杰克问题可以被建模为一个周期性有限的马尔可夫决策过程（MDP），其中每一局黑杰克游戏为一个回合。每个回合的奖励分别为+1（胜利）、-1（失败）和0（平局），这些奖励会在游戏结束时给出，游戏状态中的其余奖励则为0，不进行折扣（gamma
    = 1）。因此，终端奖励也就是游戏的回报。我们从一个无限的牌堆中抽取卡牌，以确保没有可追踪的规律。以下是用Python语言建模的整个游戏代码。
- en: 'The following snippets of code have taken inspiration from *Shangtong Zhang*''s
    Python codes for RL, and are published in this book with permission from the student
    of *Richard S. Sutton*, the famous author of *Reinforcement : Learning: An Introduction*
    (details provided in the *Further reading* section).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '以下代码片段灵感来自*Shangtong Zhang*的强化学习Python代码，并已获得*Richard S. Sutton*著作《Reinforcement
    Learning: An Introduction》学生的许可，在本书中发布（详细信息请见*进一步阅读*部分）。'
- en: 'The following package is imported for array manipulation and visualization:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 以下包用于数组操作和可视化：
- en: '[PRE24]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'At each turn, the player or dealer can take one of the actions possible: either
    to hit or to stand. These are the only two states possible :'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个回合中，玩家或庄家可以采取两种可能的行动：抽牌或停牌。这是唯一的两种可能状态：
- en: '[PRE25]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The policy for player is modeled with 21 arrays of values, as the player will
    get bust after going over the value of 21:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 玩家策略使用21个值的数组来建模，因为玩家总分超过21时将爆掉：
- en: '[PRE26]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The player has taken the policy of stick if he gets a value of either 20 or
    21, or else he will keep hitting the deck to draw a new card:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如果玩家的总分为20或21点，他将选择停牌；否则，玩家将继续抽牌：
- en: '[PRE27]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Function form of target policy of a player:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 玩家目标策略的函数形式：
- en: '[PRE28]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Function form of behavior policy of a player:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 玩家行为策略的函数形式：
- en: '[PRE29]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Fixed policy for the dealer is to keep hitting the deck until value is 17 and
    then stick between 17 to 21:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 庄家的固定策略是持续抽牌直到达到17点，然后在17到21点之间停牌：
- en: '[PRE30]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following function is used for drawing a new card from the deck with replacement:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数用于从牌堆中抽取一张新牌，并进行替换：
- en: '[PRE31]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Let's play the game!
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始游戏吧！
- en: '[PRE32]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Sum of the player, player''s trajectory and whether player uses ace as 11:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 玩家总分、玩家轨迹以及玩家是否将王牌算作11点的情况：
- en: '[PRE33]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Dealer status of drawing cards:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 庄家抽牌状态：
- en: '[PRE34]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Generate a random initial state:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个随机初始状态：
- en: '[PRE35]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Initializing the player''s cards:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化玩家的手牌：
- en: '[PRE36]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If the sum of a player''s cards is less than 12, always hit the deck for drawing
    card:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果玩家的总分小于12点，始终要抽一张牌：
- en: '[PRE37]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'If the player''s sum is larger than 21, he must hold at least one ace, but
    two aces are also possible. In that case, he will use ace as 1 rather than 11\.
    If the player has only one ace, then he does not have a usable ace any more:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果玩家的总分大于21点，他必须至少拥有一张王牌，但也可以有两张王牌。在这种情况下，他将会把王牌当作1点来计算，而不是11点。如果玩家只有一张王牌，那么他将不再有可用的王牌：
- en: '[PRE38]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Initializing the dealer cards:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化庄家手牌：
- en: '[PRE39]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Initialize the game state:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化游戏状态：
- en: '[PRE40]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Initializing the dealer''s sum:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化庄家的总分：
- en: '[PRE41]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The game starts from here, as the player needs to draw extra cards from here
    onwards:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 游戏从这里开始，因为玩家需要从这里开始抽取额外的牌：
- en: '[PRE42]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Get action based on the current sum of a player:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据当前玩家的总分选择行动：
- en: '[PRE43]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Tracking the player''s trajectory for importance sampling:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跟踪玩家轨迹以便进行重要性采样：
- en: '[PRE44]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Get new a card if the action is to hit the deck:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果选择抽牌（hit the deck），则抽一张新牌：
- en: '[PRE45]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Player busts here if the total sum is greater than 21, the game ends, and he
    gets a reward of -1\. However, if he has an ace at his disposable, he can use
    it to save the game, or else he will lose.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果玩家的总分大于21点，玩家爆掉，游戏结束，玩家获得奖励-1。不过，如果玩家手上有王牌，他可以将王牌用作11点来挽救游戏，否则将会失败。
- en: '[PRE46]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now it''s the dealer''s turn. He will draw cards based on a sum: if he reaches
    17, he will stop, otherwise keep on drawing cards. If the dealer also has ace,
    he can use it to achieve the bust situation, otherwise, he goes bust:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在轮到庄家行动。他将根据总分来决定是否抽牌：如果他达到17点，就停牌，否则继续抽牌。如果庄家也有王牌，他可以使用王牌来达到爆掉的情况，否则庄家会爆掉：
- en: '[PRE47]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now we compare the player''s sum with the dealer''s sum to decide who wins
    without going bust:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将玩家的总和与庄家的总和进行比较，决定谁在不爆掉的情况下获胜：
- en: '[PRE48]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The following code illustrates the Monte Carlo sample with *On-Policy*:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了使用*On-Policy*的蒙特卡洛采样：
- en: '[PRE49]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The following code discusses Monte Carlo with Exploring Starts, in which all
    the returns for each state-action pair are accumulated and averaged, irrespective
    of what policy was in force when they were observed:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码讨论了带有探索起始的蒙特卡洛方法，其中每个状态-动作对的所有回报都会被累积并平均，不管观察到时使用的是何种策略：
- en: '[PRE50]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Behavior policy is greedy, which gets `argmax` of the average returns (s, a):'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 行为策略是贪心策略，获取平均回报(s, a)的`argmax`：
- en: '[PRE51]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Play continues for several episodes and, at each episode, randomly initialized
    state, action, and update values of state-action pairs:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏会持续若干回合，每一回合随机初始化状态、动作，并更新状态-动作对的值：
- en: '[PRE52]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Update values of state-action pairs:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 更新状态-动作对的值：
- en: '[PRE53]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Print the state value:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 打印状态值：
- en: '[PRE54]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'On-Policy results with or without a usable ace for 10,000 and 500,000 iterations:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 使用或不使用可用A牌的*On-Policy*结果，经过10,000和500,000次迭代：
- en: '[PRE55]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Optimized or Monte Carlo control of policy iterations:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代的优化或蒙特卡洛控制：
- en: '[PRE56]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '![](img/288909aa-3398-46a7-9096-a541ae6a6d7d.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![](img/288909aa-3398-46a7-9096-a541ae6a6d7d.png)'
- en: From the previous diagram, we can conclude that a usable ace in a hand gives
    much higher rewards even at the low player sum combinations, whereas for a player
    without a usable ace, values are pretty distinguished in terms of earned reward
    if those values are less than 20.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以得出结论：即使在低玩家总和的组合下，手中有可用A牌也能带来更高的奖励，而对于没有可用A牌的玩家，如果回报少于20，这些值之间的差异就非常明显。
- en: '[PRE57]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![](img/3ae682af-301f-4a67-b74f-7b46025b5e1d.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ae682af-301f-4a67-b74f-7b46025b5e1d.png)'
- en: From the optimum policies and state values, we can conclude that, with a usable
    ace at our disposal, we can hit more than stick, and also that the state values
    for rewards are much higher compared with when there is no ace in a hand. Though
    the results we are talking about are obvious, we can see the magnitude of the
    impact of holding an ace in a hand.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 从最优策略和状态值中，我们可以得出结论：如果有可用的A牌，我们可以打得比停牌更多，而且奖励的状态值也远高于没有A牌时的情况。尽管我们讨论的结果是显而易见的，但我们可以看到持有A牌对结果的影响程度。
- en: Temporal difference learning
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时序差分学习：
- en: '**Temporal Difference** (**TD**) learning is the central and novel theme of
    reinforcement learning. TD learning is the combination of both **Monte Carlo**
    (**MC**) and **Dynamic Programming** (**DP**) ideas. Like Monte Carlo methods,
    TD methods can learn directly from the experiences without the model of the environment.
    Similar to Dynamic Programming, TD methods update estimates based in part on other
    learned estimates, without waiting for a final outcome, unlike MC methods, in
    which estimates are updated after reaching the final outcome only.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '**时序差分**（**TD**）学习是强化学习的核心和创新主题。TD学习结合了**蒙特卡洛**（**MC**）和**动态规划**（**DP**）的思想。与蒙特卡洛方法类似，TD方法可以直接从经验中学习，无需环境模型。类似于动态规划，TD方法基于其他已学习的估计值来更新估计，而不需要等待最终结果，这与MC方法不同，后者只有在达到最终结果后才更新估计。'
- en: '![](img/4256712b-f076-4482-8960-92d182c9c460.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4256712b-f076-4482-8960-92d182c9c460.png)'
- en: TD prediction
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时序差分预测：
- en: Both TD and MC use experience to solve *z* prediction problem. Given some policy
    π, both methods update their estimate *v* of *v*[π]  for the non-terminal states
    *S[t]* occurring in that experience. Monte Carlo methods wait until the return
    following the visit is known, then use that return as a target for *V(S[t])*.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: TD和MC都使用经验来解决*z*预测问题。给定某个策略π，两者都更新它们对非终结状态*S[t]*的估计*v*，该状态在经验中发生。蒙特卡洛方法会等到访问后的回报已知后，再将该回报作为*V(S[t])*的目标。
- en: '![](img/6bffd9d6-a6a8-4337-b394-6d9e39c06077.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6bffd9d6-a6a8-4337-b394-6d9e39c06077.jpg)'
- en: The preceding method can be called as a constant - *α MC*, where MC must wait
    until the end of the episode to determine the increment to *V(S[t])* (only then
    is *G[t]* known).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法可以被称为常数 - *α MC*，其中MC必须等到回合结束后才能确定对*V(S[t])*的增量（只有到那时*G[t]*才会知道）。
- en: 'TD methods need to wait only until the next timestep. At time *t+1*, they immediately
    form a target and make a useful update using the observed reward *R[t+1]* and
    the estimate *V(S[t+1])*. The simplest TD method, known as *TD(0)*, is:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: TD方法只需要等到下一个时间步。到达*t+1*时，它们立即形成目标并使用观察到的奖励*R[t+1]*和估计的*V(S[t+1])*进行有用的更新。最简单的TD方法是*TD(0)*，其公式为：
- en: '![](img/b9301f34-ddc2-48b2-b7a5-c3cbe1febc08.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9301f34-ddc2-48b2-b7a5-c3cbe1febc08.jpg)'
- en: Target for MC update is *G[t]*, whereas the target for the TD update is *R[t+1 ]+
    y V(S[t+1])*.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: MC 更新的目标是 *G[t]*，而 TD 更新的目标是 *R[t+1] + y V(S[t+1])*。
- en: In the following diagram, a comparison has been made between TD with MC methods.
    As we've written in equation TD(0), we use one step of real data and then use
    the estimated value of the value function of next state. In a similar way, we
    can also use two steps of real data to get a better picture of the reality and
    estimate value function of the third stage. However, as we increase the steps,
    which eventually need more and more data to perform parameter updates, the more
    time it will cost.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们对比了 TD 和 MC 方法。如我们在方程 TD(0) 中所写，我们使用一步的实际数据，然后使用下一个状态的价值函数的估计值。同样，我们也可以使用两步的实际数据，以便更好地理解现实，并估计第三阶段的价值函数。然而，随着步骤的增加，最终需要越来越多的数据来进行参数更新，这将消耗更多的时间。
- en: When we take infinite steps until it touches the terminal point for updating
    parameters in each episode, TD becomes the Monte Carlo method.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们采取无限步数，直到触及终止点以更新每个回合的参数时，TD 就变成了蒙特卡罗方法。
- en: '![](img/579d6e42-de4f-47c1-8fea-88eca610019f.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](img/579d6e42-de4f-47c1-8fea-88eca610019f.png)'
- en: 'TD (0) for estimating *v* algorithm consists of the following steps:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 用于估计 *v* 的 TD (0) 算法包含以下步骤：
- en: 'Initialize:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化：
- en: '![](img/fe4f7713-158f-4708-a82a-06e72dde76f3.jpg)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe4f7713-158f-4708-a82a-06e72dde76f3.jpg)'
- en: 'Repeat (for each episode):'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复（对于每个回合）：
- en: Initialize *S*
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化 *S*
- en: 'Repeat (for each step of episode):'
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复（对于回合的每一步）：
- en: A <- action given by π for S
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: A <- 由 π 给定的动作，针对 S
- en: Take action A, observe *R,S'*
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行动作 A，观察 *R,S'*
- en: '![](img/645d3a19-7c01-4d55-9b6c-850d4cdd6a27.jpg)'
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/645d3a19-7c01-4d55-9b6c-850d4cdd6a27.jpg)'
- en: '![](img/5c3bcd7e-fdba-4a49-b716-b8099ced3401.jpg)'
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/5c3bcd7e-fdba-4a49-b716-b8099ced3401.jpg)'
- en: Until *S* is terminal.
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到 *S* 是终止状态。
- en: Driving office example for TD learning
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 驾驶办公室示例用于 TD 学习
- en: 'In this simple example, you travel from home to the office every day and you
    try to predict how long it will take to get to the office in the morning. When
    you leave your home, you note that time, the day of the week, the weather (whether
    it is rainy, windy, and so on) any other parameter which you feel is relevant.
    For example, on Monday morning you leave at exactly 8 a.m. and you estimate it
    takes 40 minutes to reach the office. At 8:10 a.m., and you notice that a VIP
    is passing, and you need to wait until the complete convoy has moved out, so you
    re-estimate that it will take 45 minutes from then, or a total of 55 minutes.
    Fifteen minutes later you have completed the highway portion of your journey in
    good time. Now you enter a bypass road and you now reduce your estimate of total
    travel time to 50 minutes. Unfortunately, at this point, you get stuck behind
    a bunch of bullock carts and the road is too narrow to pass. You end up having
    to follow those bullock carts until you turn onto the side street where your office
    is located at 8:50\. Seven minutes later, you reach your office parking. The sequence
    of states, times, and predictions are as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的示例中，你每天从家里出发到办公室，试图预测早晨到达办公室需要多长时间。当你离开家时，你记录下这个时间、星期几、天气情况（是否下雨、刮风等）以及你认为相关的其他参数。例如，在星期一早晨，你在早上8点整离开，并估计到达办公室需要40分钟。到了8:10，你注意到一位贵宾正在经过，你需要等到完整的车队离开，因此你重新估计从那时起需要45分钟，总共需要55分钟。15分钟后，你顺利完成了高速公路部分的行程。现在你进入了一个绕行的道路，你将总行程时间的估计减少到50分钟。不幸的是，这时你被一堆牛车堵住了，路太窄无法通过。你不得不跟着那些牛车走，直到你转入一条侧街，在8:50到达了办公室。七分钟后，你到达了办公室停车场。状态、时间和预测的顺序如下：
- en: '![](img/cf127b34-153f-4784-801e-809df5455da9.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf127b34-153f-4784-801e-809df5455da9.png)'
- en: Rewards in this example are the elapsed time at each leg of the journey and
    we are using a discount factor (gamma, *v = 1*), so the return for each state
    is the actual time to go from that state to the destination (office). The value
    of each state is the predicted time to go, which is the second column in the preceding
    table, also known the current estimated value for each state encountered.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，奖励是每段行程所用的时间，我们使用了折扣因子（gamma，*v = 1*），因此每个状态的回报就是从该状态到目的地（办公室）的实际时间。每个状态的价值是预计的行驶时间，即前表中的第二列，也被称为每个遇到的状态的当前估计值。
- en: '![](img/04b114de-7dbf-4065-9501-1fbcd65f3120.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04b114de-7dbf-4065-9501-1fbcd65f3120.png)'
- en: In the previous diagram, Monte Carlo is used to plot the predicted total time
    over the sequence of events. Arrows always show the change in predictions recommended
    by the constant-α MC method. These are errors between the estimated value in each
    stage and the actual return (57 minutes). In the MC method, learning happens only
    after finishing, for which it needs to wait until 57 minutes passed. However,
    in reality, you can estimate before reaching the final outcome and correct your
    estimates accordingly. TD works on the same principle, at every stage it tries
    to predict and correct the estimates accordingly. So, TD methods learn immediately
    and do not need to wait until the final outcome. In fact, that is how humans predict
    in real life. Because of these many positive properties, TD learning is considered
    as novel in reinforcement learning.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，蒙特卡洛方法被用来绘制在事件序列中的预测总时间。箭头始终表示由常数-α MC 方法推荐的预测变化。这些是每个阶段的估计值与实际回报（57
    分钟）之间的误差。在 MC 方法中，学习仅在结束后进行，因此它需要等待直到 57 分钟过去。然而，实际上，您可以在达到最终结果之前进行估计，并相应地修正您的估计。TD
    也遵循相同的原理，在每个阶段它尝试进行预测并相应地修正估计。因此，TD 方法立即学习，不需要等到最终结果。事实上，这也是人类在现实生活中的预测方式。由于这些积极的特性，TD
    学习被认为是强化学习中的创新方法。
- en: SARSA on-policy TD control
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SARSA 在策略上的 TD 控制
- en: '**State-action-reward-state-action** (**SARSA**) is an on-policy TD control
    problem, in which policy will be optimized using policy iteration (GPI), only
    time TD methods used for evaluation of predicted policy. In the first step, the
    algorithm learns a SARSA function. In particular, for an on-policy method we estimate
    *q[π] (s, a)* for the current behavior policy π and for all states (s) and actions
    (a), using the TD method for learning v[π.]'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '**状态-动作-奖励-状态-动作**（**SARSA**）是一个在策略的 TD 控制问题，其中策略将使用策略迭代（GPI）进行优化，仅在 TD 方法用于评估预测策略时。首先，算法学习一个
    SARSA 函数。特别地，对于一种策略方法，我们估计当前行为策略 π 下所有状态（s）和动作（a）对应的 *q[π] (s, a)*，并使用 TD 方法学习
    v[π]。'
- en: 'Now, we consider transitions from state-action pair to state-action pair, and
    learn the values of state-action pairs:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们考虑从状态-动作对到状态-动作对的转换，并学习状态-动作对的值：
- en: '![](img/4a39bee3-683a-4610-8214-ba9d78ccc614.jpg)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a39bee3-683a-4610-8214-ba9d78ccc614.jpg)'
- en: This update is done after every transition from a non-terminal state *S[t]*.
    If *S[t+1]* is terminal, then *Q (S[t+1,] A[t+1])* is defined as zero. This rule
    uses every element of the quintuple of events (*S[t]*, *A[t]*, *Rt*, *St[+1]*,
    *A[t+1]*), which make up a transition from one state-action pair to the next.
    This quintuple gives rise to the name SARSA for the algorithm.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 此更新在每次从非终结状态 *S[t]* 转换后进行。如果 *S[t+1]* 是终结状态，则 *Q (S[t+1,] A[t+1])* 定义为零。此规则使用事件五元组的每个元素（*S[t]*，*A[t]*，*Rt*，*St[+1]*，*A[t+1]*），它们构成从一个状态-动作对到下一个的转换。这个五元组使得算法被命名为
    SARSA。
- en: 'As in all on-policy methods, we continually estimate q[π] for the behavior
    policy π, and at the same time change π toward greediness with respect to q[π.] The
    algorithm for computation of SARSA is given as follows:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有策略方法一样，我们不断估计行为策略 π 的 q[π]，并同时将 π 向贪心策略调整，以符合 q[π]。SARSA 计算的算法如下：
- en: 'Initialize:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化：
- en: '![](img/dad25125-2e1c-48c5-89c9-97034a05a1ca.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dad25125-2e1c-48c5-89c9-97034a05a1ca.jpg)'
- en: 'Repeat (for each episode):'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复（对于每个回合）：
- en: Initialize S
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化 S
- en: Choose A from S using policy derived from Q (for example, ε- greedy)
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用从 Q 中得出的策略从 S 中选择 A（例如，ε-贪心策略）
- en: 'Repeat (for each step of episode):'
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复（对于每一步的回合）：
- en: Take action *A*, observe *R,S'*
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行动作 *A*，观察 *R, S'*
- en: Choose *A'* from using *S'* policy derived from Q (for example, ε - greedy)
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用从 Q 中得出的策略从 *S'* 中选择 *A'*（例如，ε-贪心策略）
- en: '![](img/d83543f0-d7b0-4856-b065-c1f49e9bb8ac.jpg)'
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/d83543f0-d7b0-4856-b065-c1f49e9bb8ac.jpg)'
- en: '![](img/3d8f822d-6b84-44fa-8274-4fe9881c379b.jpg)'
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/3d8f822d-6b84-44fa-8274-4fe9881c379b.jpg)'
- en: Until *S* is terminal
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到 *S* 是终结状态
- en: Q-learning - off-policy TD control
  id: totrans-408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning - 非策略 TD 控制
- en: 'Q-learning is the most popular method used in practical applications for many
    reinforcement learning problems. The off-policy TD control algorithm is known
    as Q-learning. In this case, the learned action-value function, Q directly approximates
    ![](img/55c095cc-5895-4255-91b6-9c22cdb1caf7.png), the optimal action-value function,
    independent of the policy being followed. This approximation simplifies the analysis
    of the algorithm and enables early convergence proofs. The policy still has an
    effect, in that it determines which state-action pairs are visited and updated.
    However, all that is required for correct convergence is that all pairs continue
    to be updated. As we know, this is a minimal requirement in the sense that any
    method guaranteed to find optimal behavior in the general case must require it.
    An algorithm of convergence is shown in the following steps:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning是目前在许多强化学习问题中应用最广泛的方法。策略外的TD控制算法被称为Q-learning。在这种情况下，学习到的动作-价值函数Q直接逼近最优的动作-价值函数，独立于所遵循的策略。这个逼近简化了算法的分析，并支持早期收敛性的证明。策略仍然有影响，因为它决定了哪些状态-动作对会被访问和更新。然而，对于正确的收敛性而言，只需要所有对都继续被更新。正如我们所知，这是一个最小要求，因为任何能够保证找到最优行为的算法在一般情况下都必须满足这个要求。收敛算法的步骤如下：
- en: 'Initialize:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化：
- en: '![](img/c5488bbc-43aa-4794-9ecb-fc8ce42c4efb.jpg)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5488bbc-43aa-4794-9ecb-fc8ce42c4efb.jpg)'
- en: 'Repeat (for each episode):'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复（每个回合）：
- en: Initialize S
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化S
- en: 'Repeat (for each step of episode):'
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复（每个步骤都进行）：
- en: Choose A from S using policy derived from Q (for example, ε - greedy)
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从S中选择A，使用从Q中推导的策略（例如，ε - 贪婪策略）
- en: Take action A, observe *R,S'*
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采取动作A，观察*R,S'*
- en: '![](img/751ba8b1-8f2c-4399-8418-a3dd70935dc5.jpg)'
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/751ba8b1-8f2c-4399-8418-a3dd70935dc5.jpg)'
- en: '![](img/15d47def-acaf-4609-91ef-4f56531bccd4.jpg)'
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/15d47def-acaf-4609-91ef-4f56531bccd4.jpg)'
- en: Until *S* is terminal
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到*S*为终止状态
- en: Cliff walking example of on-policy and off-policy of TD control
  id: totrans-420
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TD控制的策略内和策略外的悬崖行走示例
- en: A cliff walking grid-world example is used to compare SARSA and Q-learning,
    to highlight the differences between on-policy (SARSA) and off-policy (Q-learning)
    methods. This is a standard undiscounted, episodic task with start and end goal
    states, and with permitted movements in four directions (north, west, east and
    south). The reward of -1 is used for all transitions except the regions marked
    *The Cliff*, stepping on this region will penalize the agent with reward of -100
    and sends the agent instantly back to the start position.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 使用悬崖行走网格世界示例来比较SARSA和Q-learning，突出策略内（SARSA）和策略外（Q-learning）方法之间的区别。这是一个标准的非折扣、回合制任务，具有起始和结束目标状态，并且允许四个方向（北、南、西、东）移动。除了标记为*悬崖*的区域，所有的转移都使用-1作为奖励，踩到这个区域会使智能体受到-100的惩罚，并立即将其送回起始位置。
- en: '![](img/f23a61ae-c0ed-4be1-bcf5-92f3becb9e2e.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f23a61ae-c0ed-4be1-bcf5-92f3becb9e2e.png)'
- en: 'The following snippets of code have taken inspiration from Shangtong Zhang''s
    Python codes for RL and are published in this book with permission from the student
    of *Richard S. Sutton*, the famous author of *Reinforcement Learning: An Introduction*
    (details provided in the *Further reading* section):'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段灵感来自于Shangtong Zhang的强化学习Python代码，并已获得*Richard S. Sutton*（《强化学习：导论》一书的著名作者）的学生的许可，在本书中发布（更多详细信息请参阅*进一步阅读*部分）：
- en: '[PRE58]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '![](img/a1ed3b55-f95f-4692-9127-ac7a3bb988cb.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1ed3b55-f95f-4692-9127-ac7a3bb988cb.png)'
- en: After an initial transient, Q-learning learns the value of optimal policy to
    walk along the optimal path, in which the agent travels right along the edge of
    the cliff. Unfortunately, this will result in occasionally falling off the cliff
    because of ε-greedy action selection. Whereas SARSA, on the other hand, takes
    the action selection into account and learns the longer and safer path through
    the upper part of the grid. Although Q-learning learns the value of the optimal
    policy, its online performance is worse than that of the SARSA, which learns the
    roundabout and safest policy. Even if we observe the following sum of rewards
    displayed in the following diagram, SARSA has a less negative sum of rewards during
    the episode than Q-learning.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步的瞬态之后，Q-learning学习最优策略的价值，沿着最优路径行走，在此过程中智能体会沿着悬崖边缘行走。不幸的是，这会因为ε-贪婪动作选择导致偶尔从悬崖上掉下去。而SARSA则考虑到动作选择，学习通过网格上部的更长且更安全的路径。尽管Q-learning学习最优策略的价值，但其在线表现不如SARSA，后者学习了绕行的最安全策略。即便我们观察下图所显示的奖励总和，SARSA在整个过程中获得的奖励总和也比Q-learning要少负。
- en: '![](img/6c28f559-985f-438f-a861-5a1e7566ecb2.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c28f559-985f-438f-a861-5a1e7566ecb2.png)'
- en: Further reading
  id: totrans-428
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'There are many classic resources available for reinforcement learning, and
    we encourage the reader to go through them:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有许多经典的强化学习资源可供使用，我们鼓励读者进行学习：
- en: 'R.S. Sutton and A.G. Barto, *Reinforcement Learning: An Introduction*. *MIT
    Press*, Cambridge, MA, USA, 1998'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R.S. Sutton 和 A.G. Barto, *强化学习：导论*，*MIT Press*，剑桥，美国，1998年
- en: '*RL Course* by *David Silver* from YouTube: [https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RL 课程* 由 *David Silver* 在 YouTube 上提供：[https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)'
- en: '*Machine Learning* (Stanford) by *Andrew NG* form YouTube (Lectures 16- 20): [https://www.youtube.com/watch?v=UzxYlbK2c7E&list=PLA89DCFA6ADACE599](https://www.youtube.com/watch?v=UzxYlbK2c7E&list=PLA89DCFA6ADACE599)'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习*（斯坦福大学）由 *Andrew NG* 在 YouTube 上提供（第16-20讲）：[https://www.youtube.com/watch?v=UzxYlbK2c7E&list=PLA89DCFA6ADACE599](https://www.youtube.com/watch?v=UzxYlbK2c7E&list=PLA89DCFA6ADACE599)'
- en: '*Algorithms for reinforcement learning* by *Csaba* from *Morgan & Claypool*
    Publishers'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*强化学习算法*，*Csaba* 著，*Morgan & Claypool* 出版社'
- en: '*Artificial Intelligence: A Modern Approach* 3^(rd) Edition, by *Stuart Russell*
    and *Peter Norvig*, *Prentice Hall*'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人工智能：现代方法* 第3版，*Stuart Russell* 和 *Peter Norvig* 著，*Prentice Hall* 出版'
- en: Summary
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you've learned various reinforcement learning techniques, like
    Markov decision process, Bellman equations, dynamic programming, Monte Carlo methods,
    Temporal Difference learning, including both on-policy (SARSA) and off-policy
    (Q-learning), with Python examples to understand its implementation in a practical
    way. You also learned how Q-learning is being used in many practical applications
    nowadays, as this method learns from trial and error by interacting with environments.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了各种强化学习技术，如马尔可夫决策过程、贝尔曼方程、动态规划、蒙特卡罗方法、时序差分学习，包括基于策略（SARSA）和非基于策略（Q-learning）的算法，并通过
    Python 示例来理解其在实际中的实现。你还了解了 Q-learning 如何在许多实际应用中使用，因为这种方法通过与环境互动并通过试错学习。
- en: Finally, *Further reading* has been provided for you if you would like to pursue
    reinforcement learning full-time. We wish you all the best!
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你希望全职深入学习强化学习，我们为你提供了*进一步阅读*的资源。我们祝你一切顺利！
