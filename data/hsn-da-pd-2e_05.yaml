- en: '*Chapter 3*: Data Wrangling with Pandas'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第3章*：使用 Pandas 进行数据整理'
- en: In the previous chapter, we learned about the main `pandas` data structures,
    how to create `DataFrame` objects with our collected data, and various ways to
    inspect, summarize, filter, select, and work with `DataFrame` objects. Now that
    we are well versed in the initial data collection and inspection stage, we can
    begin our foray into the world of data wrangling.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了主要的`pandas`数据结构，如何用收集到的数据创建`DataFrame`对象，以及检查、总结、筛选、选择和处理`DataFrame`对象的各种方法。现在，我们已经熟练掌握了初步数据收集和检查阶段，可以开始进入数据整理的世界。
- en: As mentioned in [*Chapter 1*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015),
    *Introduction to Data Analysis*, preparing data for analysis is often the largest
    portion of the job time-wise for those working with data, and often the least
    enjoyable. On the bright side, `pandas` is well equipped to help with these tasks,
    and, by mastering the skills presented in this book, we will be able to get to
    the more interesting parts sooner.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第1章*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015)，《*数据分析简介*》中所提到的，准备数据进行分析通常是从事数据工作的人耗费最多时间的部分，而且往往是最不令人愉快的部分。幸运的是，`pandas`非常适合处理这些任务，通过掌握本书中介绍的技能，我们将能够更快地进入更有趣的部分。
- en: It should be noted that data wrangling isn't something we do merely once in
    our analysis; it is highly likely that we will do some data wrangling and move
    on to another analysis task, such as data visualization, only to find that we
    need to do additional data wrangling. The more familiar we are with the data,
    the better we will be able to prepare the data for our analysis. It's crucial
    to form an intuition of what types our data should be, what format we need our
    data to be in for the visualization that would best convey what we are looking
    to show, and the data points we should collect for our analysis. This comes with
    experience, so we must practice the skills that will be covered in this chapter
    on our own data every chance we get.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，数据整理并非我们在分析中只做一次的工作；很可能在完成一次数据整理并转向其他分析任务（如数据可视化）后，我们会发现仍然需要进行额外的数据整理。我们对数据越熟悉，就越能为分析做好准备。形成一种直觉，了解数据应该是什么类型、我们需要将数据转换成什么格式来进行可视化，以及我们需要收集哪些数据点来进行分析，这一点至关重要。这需要经验积累，因此我们必须在每次处理自己数据时，实践本章中将涉及的技能。
- en: Since this is a very large topic, our coverage of data wrangling will be split
    between this chapter and [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*. In this chapter, we will get an overview of data
    wrangling before exploring the `requests` library. Then, we will discuss data
    wrangling tasks that deal with preparing data for some initial analyses and visualizations
    (which we will learn about in [*Chapter 5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106),
    *Visualizing Data with Pandas and Matplotlib*, and [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*). We will address some more
    advanced aspects of data wrangling that relate to aggregations and combining datasets
    in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082), *Aggregating
    Pandas DataFrames*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个非常庞大的话题，关于数据整理的内容将在本章和[*第4章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)，《*聚合
    Pandas 数据框*》中分开讲解。本章将概述数据整理，然后探索`requests`库。接着，我们将讨论一些数据整理任务，这些任务涉及为初步分析和可视化准备数据（我们将在[*第5章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106)，《*使用
    Pandas 和 Matplotlib 进行数据可视化*》以及[*第6章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125)，《*使用
    Seaborn 绘图和定制技巧*》中学习到的内容）。我们将针对与聚合和数据集合并相关的更高级的数据整理内容，在[*第4章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)，《*聚合
    Pandas 数据框*》中进行讲解。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding data wrangling
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据整理
- en: Exploring an API to find and collect temperature data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 API 查找并收集温度数据
- en: Cleaning data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理数据
- en: Reshaping data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据重塑
- en: Handling duplicate, missing, or invalid data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理重复、缺失或无效数据
- en: Chapter materials
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章材料
- en: 'The materials for this chapter can be found on GitHub at [https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_03](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_03).
    There are five notebooks that we will work through, each numbered according to
    when they will be used, and two directories, `data/` and `exercises/`, which contain
    all the CSV files necessary for the aforementioned notebooks and end-of-chapter
    exercises, respectively. The following files are in the `data/` directory:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的材料可以在 GitHub 上找到，链接为 [https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_03](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_03)。这里有五个笔记本，我们将按顺序进行学习，并有两个目录，`data/`
    和 `exercises/`，分别包含了上述笔记本和章节末尾练习所需的所有 CSV 文件。`data/` 目录中包含以下文件：
- en: '![Figure 3.1 – Breakdown of the datasets used in this chapter'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.1 – 本章使用的数据集解析'
- en: '](img/Figure_3.1_B16834.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.1_B16834.jpg)'
- en: Figure 3.1 – Breakdown of the datasets used in this chapter
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 本章使用的数据集解析
- en: We will begin in the `1-wide_vs_long.ipynb` notebook by discussing wide versus
    long format data. Then, we will collect daily temperature data from the NCEI API,
    which can be found at [https://www.ncdc.noaa.gov/cdo-web/webservices/v2](https://www.ncdc.noaa.gov/cdo-web/webservices/v2),
    in the `2-using_the_weather_api.ipynb` notebook. The documentation for the **Global
    Historical Climatology Network – Daily** (**GHCND**) dataset we will be using
    can be found at [https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf](https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 `1-wide_vs_long.ipynb` 笔记本中开始，讨论宽格式和长格式数据的区别。接下来，我们将在 `2-using_the_weather_api.ipynb`
    笔记本中从 NCEI API 获取每日气温数据，API 地址为 [https://www.ncdc.noaa.gov/cdo-web/webservices/v2](https://www.ncdc.noaa.gov/cdo-web/webservices/v2)。我们将使用的
    **全球历史气候网络–每日**（**GHCND**）数据集的文档可以在 [https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf](https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf)
    找到。
- en: Important note
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The NCEI is part of the **National Oceanic and Atmospheric Administration**
    (**NOAA**). As indicated by the URL for the API, this resource was created when
    the NCEI was called the NCDC. Should the URL for this resource change in the future,
    search for *NCEI weather API* to find the updated one.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: NCEI 是 **国家海洋和大气管理局**（**NOAA**）的一部分。如 API 的 URL 所示，该资源是在 NCEI 被称为 NCDC 时创建的。如果将来该资源的
    URL 发生变化，可以搜索 *NCEI weather API* 来找到更新后的 URL。
- en: In the `3-cleaning_data.ipynb` notebook, we will learn how to perform an initial
    round of cleaning on the temperature data and some financial data, which was collected
    using the `stock_analysis` package that we will build in [*Chapter 7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146),
    *Financial Analysis – Bitcoin and the Stock Market*. Afterward, we will walk through
    ways to reshape our data in the `4-reshaping_data.ipynb` notebook. Finally, in
    the `5-handling_data_issues.ipynb` notebook, we will learn about some strategies
    for dealing with duplicate, missing, or invalid data using some dirty data that
    can be found in `data/dirty_data.csv`. The text will indicate when it's time to
    switch between notebooks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `3-cleaning_data.ipynb` 笔记本中，我们将学习如何对温度数据和一些财务数据进行初步清理，这些财务数据是通过我们将在 [*第七章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146)
    中构建的 `stock_analysis` 包收集的，*财务分析——比特币与股市*。然后，我们将在 `4-reshaping_data.ipynb` 笔记本中探讨如何重塑数据。最后，在
    `5-handling_data_issues.ipynb` 笔记本中，我们将学习如何使用一些脏数据（可在 `data/dirty_data.csv` 中找到）处理重复、缺失或无效数据的策略。文本中将指示何时切换笔记本。
- en: Understanding data wrangling
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据处理
- en: 'Like any professional field, data analysis is filled with buzzwords, and it
    can often be difficult for newcomers to understand the lingo—the topic of this
    chapter is no exception. When we perform **data wrangling**, we are taking our
    input data from its original state and putting it in a format where we can perform
    meaningful analysis on it. **Data manipulation** is another way to refer to this
    process. There is no set list of operations; the only goal is that the data post-wrangling
    is more useful to us than when we started. In practice, there are three common
    tasks involved in the data wrangling process:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何专业领域一样，数据分析充满了术语，初学者往往很难理解这些行话——本章的主题也不例外。当我们进行**数据清洗**时，我们将输入数据从其原始状态转化为可以进行有意义分析的格式。**数据操作**是指这一过程的另一种说法。没有固定的操作清单；唯一的目标是，经过清洗后的数据对我们来说比开始时更有用。在实践中，数据清洗过程通常涉及以下三项常见任务：
- en: Data cleaning
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据清理
- en: Data transformation
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转换
- en: Data enrichment
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据丰富化
- en: 'It should be noted that there is no inherent order to these tasks, and it is
    highly probable that we will perform each many times throughout the data wrangling
    process. This idea brings up an interesting conundrum: if we need to wrangle our
    data to prepare it for our analysis, isn''t it possible to wrangle it in such
    a way that we tell the data what to say instead of us learning what it is saying?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，这些任务没有固定的顺序，我们很可能会在数据清洗过程中多次执行每一项。这一观点引出了一个有趣的难题：如果我们需要清洗数据以为分析做准备，难道不能以某种方式清洗数据，让它告诉我们它在说什么，而不是我们去了解它在说什么吗？
- en: '"If you torture the data long enough, it will confess to anything."'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: “如果你对数据施加足够的压力，它就会承认任何事情。”
- en: — Ronald Coase, winner of a Nobel Prize in Economics
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: — 罗纳德·科斯，诺贝尔经济学奖得主
- en: Those working with data will find it is very easy to distort the truth by manipulating
    the data. However, it is our duty to do our best to avoid deceit by keeping the
    effect our actions have on the data's integrity in mind, and by explaining the
    process we took to draw our conclusions to the people who consume our analyses,
    so that they too may make their own judgments.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从事数据工作的人会发现，通过操作数据很容易扭曲事实。然而，我们的责任是尽最大努力避免欺骗，通过时刻关注我们操作对数据完整性产生的影响，并且向分析使用者解释我们得出结论的过程，让他们也能作出自己的判断。
- en: Data cleaning
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清理
- en: 'Once we have collected our data, brought it into a `DataFrame` object, and
    used the skills we discussed in [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035),
    *Working with Pandas DataFrames*, to familiarize ourselves with the data, we will
    need to perform some data cleaning. An initial round of data cleaning will often
    give us the bare minimum we need to start exploring our data. Some essential data
    cleaning tasks to master include the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们收集到数据，将其导入`DataFrame`对象，并运用在[*第2章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035)中讨论的技巧熟悉数据后，我们将需要进行一些数据清理。初步的数据清理通常能为我们提供开始探索数据所需的最基本条件。一些必须掌握的基本数据清理任务包括：
- en: Renaming
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重命名
- en: Sorting and reordering
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排序与重新排序
- en: Data type conversions
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据类型转换
- en: Handling duplicate data
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理重复数据
- en: Addressing missing or invalid data
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失或无效数据
- en: Filtering to the desired subset of data
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 筛选出所需的数据子集
- en: Data cleaning is the best starting point for data wrangling, since having the
    data stored as the correct data types and easy-to-reference names will open up
    many avenues for exploration, such as summary statistics, sorting, and filtering.
    Since we covered filtering in [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035),
    *Working with Pandas DataFrames*, we will focus on the other topics from the preceding
    list in this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理是数据清洗的最佳起点，因为将数据存储为正确的数据类型和易于引用的名称将为许多探索途径提供便利，如总结统计、排序和筛选。由于我们在[*第2章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035)中已经讲解过筛选内容，*与Pandas
    DataFrame的合作*，本章将重点讨论上一节提到的其他主题。
- en: Data transformation
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据转换
- en: Frequently, we will reach the data transformation stage after some initial data
    cleaning, but it is entirely possible that our dataset is unusable in its current
    shape, and we must restructure it before attempting to do any data cleaning. In
    **data transformation**, we focus on changing our data's structure to facilitate
    our downstream analyses; this usually involves changing which data goes along
    the rows and which goes down the columns.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 经常在一些初步数据清洗之后，我们会进入数据转换阶段，但完全有可能我们的数据集在当前的形式下无法使用，我们必须在进行任何数据清理之前对其进行重构。在**数据转换**中，我们专注于改变数据的结构，以促进后续分析；这通常涉及改变哪些数据沿行展示，哪些数据沿列展示。
- en: 'Most data we will find is either **wide format** or **long format**; each of
    these formats has its merits, and it''s important to know which one we will need
    for our analysis. Often, people will record and present data in wide format, but
    there are certain visualizations that require the data to be in long format:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遇到的大多数数据都是**宽格式**或**长格式**；这两种格式各有其优点，了解我们需要哪一种格式进行分析是非常重要的。通常，人们会以宽格式记录和呈现数据，但有些可视化方法需要数据采用长格式：
- en: '![Figure 3.2 – (Left) wide format versus (right) long format'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.2 – (左)宽格式与(右)长格式'
- en: '](img/Figure_3.2_B16834.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.2_B16834.jpg)'
- en: Figure 3.2 – (Left) wide format versus (right) long format
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – (左)宽格式与(右)长格式
- en: Wide format is preferred for analysis and database design, while long format
    is considered poor design because each column should be its own data type and
    have a singular meaning. However, in cases where new fields will be added (or
    old ones removed) from a table in a relational database, rather than having to
    alter all the tables each time, the database's maintainers may decide to use the
    long format. This allows them to provide a fixed schema for users of the database,
    while being able to update the data it contains as needed. When building an API,
    the long format may be chosen if flexibility is required. Perhaps the API will
    provide a generic response format (for instance, date, field name, and field value)
    that can support various tables from a database. This may also have to do with
    making the response easier to form, depending on how the data is stored in the
    database the API uses. Since we will find data in both of these formats, it's
    important we understand how to work with both of them and go from one to the other.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 宽格式更适合分析和数据库设计，而长格式被认为是一种不良设计，因为每一列应该代表一个数据类型并具有单一含义。然而，在某些情况下，当需要在关系型数据库中添加新字段（或删除旧字段）时，数据库管理员可能会选择使用长格式，这样可以避免每次都修改所有表格。这样，他们可以为数据库用户提供固定的模式，同时根据需要更新数据库中的数据。在构建API时，如果需要灵活性，可能会选择长格式。比如，API可能会提供一种通用的响应格式（例如日期、字段名和字段值），以支持来自数据库的各种表格。这也可能与如何根据API所使用的数据库存储数据来简化响应的构建过程相关。由于我们将遇到这两种格式的数据，理解如何处理它们并在两者之间转换是非常重要的。
- en: 'Now, let''s navigate to the `1-wide_vs_long.ipynb` notebook to see some examples.
    First, we will import `pandas` and `matplotlib` (to help illustrate the strengths
    and weaknesses of each format when it comes to visualizations, which we will discuss
    in [*Chapter 5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106), *Visualizing
    Data with Pandas and Matplotlib*, and [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*) and read in the CSV files
    containing wide and long format data:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们导航到`1-wide_vs_long.ipynb`笔记本，查看一些示例。首先，我们将导入`pandas`和`matplotlib`（这些将帮助我们说明每种格式在可视化方面的优缺点，相关内容将在[*第5章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106)《使用Pandas和Matplotlib进行数据可视化》和[*第6章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125)《使用Seaborn和自定义技巧进行绘图》中讨论），并读取包含宽格式和长格式数据的CSV文件：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The wide data format
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 宽格式数据
- en: With wide format data, we represent measurements of variables with their own
    columns, and each row represents an observation of those variables. This makes
    it easy for us to compare variables across observations, get summary statistics,
    perform operations, and present our data; however, some visualizations don't work
    with this data format because they may rely on the long format to split, size,
    and/or color the plot content.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于宽格式数据，我们用各自的列来表示变量的测量值，每一行代表这些变量的一个观测值。这使得我们可以轻松地比较不同观测中的变量，获取汇总统计数据，执行操作并展示我们的数据；然而，一些可视化方法无法使用这种数据格式，因为它们可能依赖于长格式来拆分、调整大小和/或着色图表内容。
- en: 'Let''s look at the top six observations from the wide format data in `wide_df`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看`wide_df`中宽格式数据的前六条记录：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Each column contains the top six observations of a specific class of temperature
    data in degrees Celsius—maximum temperature (**TMAX**), minimum temperature (**TMIN**),
    and temperature at the time of observation (**TOBS**)—at a daily frequency:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每列包含特定类别温度数据的前六条观测记录，单位是摄氏度——最高温度 (**TMAX**)、最低温度 (**TMIN**)、和观测时温度 (**TOBS**)，其频率为每日：
- en: '![Figure 3.3 – Wide format temperature data'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.3 – 宽格式温度数据'
- en: '](img/Figure_3.3_B16834.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.3_B16834.jpg)'
- en: Figure 3.3 – Wide format temperature data
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 宽格式温度数据
- en: 'When working with wide format data, we can easily grab summary statistics on
    this data by using the `describe()` method. Note that while older versions of
    `pandas` treated `datetimes` as categorical, `pandas` is moving toward treating
    them as numeric, so we pass `datetime_is_numeric=True` to suppress the warning:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理宽格式数据时，我们可以通过使用`describe()`方法轻松获得该数据的总结统计信息。请注意，虽然旧版本的`pandas`将`datetimes`视为分类数据，但`pandas`正朝着将其视为数值型数据的方向发展，因此我们传入`datetime_is_numeric=True`来抑制警告：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With hardly any effort on our part, we get summary statistics for the dates,
    maximum temperature, minimum temperature, and temperature at the time of observation:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎不费力气，我们就能获得日期、最高温度、最低温度和观测时温度的总结统计信息：
- en: '![Figure 3.4 – Summary statistics for the wide format temperature data'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.4 – 宽格式温度数据的总结统计'
- en: '](img/Figure_3.4_B16834.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.4_B16834.jpg)'
- en: Figure 3.4 – Summary statistics for the wide format temperature data
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 宽格式温度数据的总结统计
- en: 'As we discussed previously, the summary data in the preceding table is easy
    to obtain and is informative. This format can easily be plotted with `pandas`
    as well, provided we tell it exactly what we want to plot:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，前面表格中的总结数据容易获得，并且富有信息。这种格式也可以用`pandas`轻松绘制，只要我们告诉它确切需要绘制的内容：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Pandas plots the daily maximum temperature, minimum temperature, and temperature
    at the time of observation as their own lines on a single line plot:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas`将每日最高温度、最低温度和观测时温度绘制成单一折线图的三条线：'
- en: '![Figure 3.5 – Plotting the wide format temperature data'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.5 – 绘制宽格式温度数据'
- en: '](img/Figure_3.5_B16834.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.5_B16834.jpg)'
- en: Figure 3.5 – Plotting the wide format temperature data
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 绘制宽格式温度数据
- en: Important note
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Don't worry about understanding the visualization code right now; it's here
    just to illustrate how each of these data formats can make certain tasks easier
    or harder. We will cover visualizations with `pandas` and `matplotlib` in [*Chapter
    5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106), *Visualizing Data with Pandas
    and Matplotlib*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在不用担心理解可视化代码，它的目的是仅仅展示这些数据格式如何使某些任务更简单或更困难。我们将在[*第5章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106)中讲解如何使用`pandas`和`matplotlib`进行数据可视化，*使用
    Pandas 和 Matplotlib 可视化数据*。
- en: The long data format
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长格式数据
- en: Long format data will have a row for each observation of a variable; this means
    that, if we have three variables being measured daily, we will have three rows
    for each day we record observations. The long format setup can be achieved by
    turning the variable column names into a single column, where the data is the
    variable name, and putting their values in a separate column.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 长格式数据每一行代表一个变量的观测值；这意味着，如果我们有三个每天测量的变量，每天的观测将有三行数据。长格式的设置可以通过将变量的列名转化为单一列来实现，这一列的值就是变量名称，然后将变量的值放在另一列。
- en: 'We can look at the top six rows of the long format data in `long_df` to see
    the difference between wide format and long format data:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看`long_df`中长格式数据的前六行，看看宽格式数据和长格式数据之间的差异：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Notice that we now have three entries for each date, and the **datatype** column
    tells us what the data in the **value** column is for that row:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，现在我们为每个日期有三条记录，并且**数据类型**列告诉我们**值**列中数据的含义：
- en: '![Figure 3.6 – Long format temperature data'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.6 – 长格式温度数据'
- en: '](img/Figure_3.6_B16834.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.6_B16834.jpg)'
- en: Figure 3.6 – Long format temperature data
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 长格式温度数据
- en: 'If we try to get summary statistics, like we did with the wide format data,
    the result isn''t as helpful:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们像处理宽格式数据一样尝试获取总结统计，结果将不那么有用：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The **value** column shows us summary statistics, but this is summarizing the
    daily maximum temperatures, minimum temperatures, and temperatures at the time
    of observation. The maximum will be the maximum of the daily maximum temperatures
    and the minimum will be the minimum of the daily minimum temperatures. This means
    that this summary data is not very helpful:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**value** 列展示了汇总统计数据，但这是对每日最高气温、最低气温和观测时气温的汇总。最大值是每日最高气温的最大值，最小值是每日最低气温的最小值。这意味着这些汇总数据并不十分有用：'
- en: '![Figure 3.7 – Summary statistics for the long format temperature data'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7 – 长格式温度数据的汇总统计'
- en: '](img/Figure_3.7_B16834.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.7_B16834.jpg)'
- en: Figure 3.7 – Summary statistics for the long format temperature data
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 长格式温度数据的汇总统计
- en: 'This format is not very easy to digest and certainly shouldn''t be how we present
    data; however, it makes it easy to create visualizations where our plotting library
    can color lines by the name of the variable, size the points by the values of
    a certain variable, and perform splits for faceting. Pandas expects its data for
    plotting to be in wide format, so, to easily make the same plot that we did with
    the wide format data, we must use another plotting library, called `seaborn`,
    which we will cover in [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这种格式并不容易理解，当然也不应该是我们展示数据的方式；然而，它使得创建可视化变得容易，利用我们的绘图库可以根据变量的名称为线条着色、根据某个变量的值调整点的大小，并且进行分面。Pandas
    期望绘图数据为宽格式数据，因此，为了轻松绘制出我们用宽格式数据绘制的相同图表，我们必须使用另一个绘图库，叫做 `seaborn`，我们将在[*第 6 章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125)中讲解，*使用
    Seaborn 绘图与定制技术*：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Seaborn can subset based on the `datatype` column to give us individual lines
    for the daily maximum temperature, minimum temperature, and temperature at the
    time of observation:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Seaborn 可以根据 `datatype` 列对数据进行子集化，给我们展示每日最高气温、最低气温和观测时的气温的单独线条：
- en: '![Figure 3.8 – Plotting the long format temperature data'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.8 – 绘制长格式温度数据'
- en: '](img/Figure_3.8_B16834.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.8_B16834.jpg)'
- en: Figure 3.8 – Plotting the long format temperature data
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – 绘制长格式温度数据
- en: 'Seaborn lets us specify the column to use for `hue`, which colored the lines
    in *Figure 3.8* by the temperature type. We aren''t limited to this, though; with
    long format data, we can easily facet our plots:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Seaborn 允许我们指定用于 `hue` 的列，这样可以根据温度类型为*图 3.8*中的线条着色。不过，我们并不局限于此；对于长格式数据，我们可以轻松地对图表进行分面：
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Seaborn can use long format data to create subplots for each distinct value
    in the `datatype` column:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Seaborn 可以使用长格式数据为 `datatype` 列中的每个不同值创建子图：
- en: '![Figure 3.9 – Plotting subsets of the long format temperature data'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.9 – 绘制长格式温度数据的子集'
- en: '](img/Figure_3.9_B16834.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.9_B16834.jpg)'
- en: Figure 3.9 – Plotting subsets of the long format temperature data
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 绘制长格式温度数据的子集
- en: Important note
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: While it is possible to create a plot similar to the preceding one with `pandas`
    and `matplotlib` using subplots, more complicated combinations of facets will
    make using `seaborn` infinitely easier. We will cover `seaborn` in [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用 `pandas` 和 `matplotlib` 的子图可以创建类似于之前图表的图形，但更复杂的分面组合将使得使用 `seaborn` 变得极其简便。我们将在[*第
    6 章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125)中讲解 `seaborn`，*使用 Seaborn
    绘图与定制技术*。
- en: In the *Reshaping data* section, we will cover how to transform our data from
    wide to long format by melting, and from long to wide format by pivoting. Additionally,
    we will learn how to transpose data, which flips the columns and the rows.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在*重塑数据*部分，我们将讲解如何通过“melting”将数据从宽格式转换为长格式，以及如何通过“pivoting”将数据从长格式转换为宽格式。此外，我们还将学习如何转置数据，即翻转列和行。
- en: Data enrichment
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据丰富
- en: Once we have our cleaned data in the format we need for our analysis, we may
    find the need to enrich the data a bit. **Data enrichment** improves the quality
    of the data by adding to it in one way or another. This process becomes very important
    in modeling and in machine learning, where it forms part of the **feature engineering**
    process (which we will touch on in [*Chapter 10*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217),
    *Making Better Predictions – Optimizing Models*).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了格式化良好的清理数据用于分析，我们可能会发现需要稍微丰富一下数据。**数据丰富**通过某种方式向数据添加更多内容，从而提升数据的质量。这个过程在建模和机器学习中非常重要，它是**特征工程**过程的一部分（我们将在[*第10章*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217)中提到，*做出更好的预测——优化模型*）。
- en: 'When we''re looking to enrich our data, we can either **merge** new data with
    the original data (by appending new rows or columns) or use the original data
    to create new data. The following are ways to enhance our data using the original
    data:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要丰富数据时，我们可以**合并**新数据与原始数据（通过附加新行或列）或使用原始数据创建新数据。以下是使用原始数据增强数据的几种方法：
- en: '**Adding new columns**: Using functions on the data from existing columns to
    create new values.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加新列**：使用现有列中的数据，通过函数计算出新值。'
- en: '**Binning**: Turning continuous data or discrete data with many distinct values
    into buckets, which makes the column discrete while letting us control the number
    of possible values in the column.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分箱**：将连续数据或具有许多不同值的离散数据转换为区间，这使得列变为离散的，同时让我们能够控制列中可能值的数量。'
- en: '**Aggregating**: Rolling up the data and summarizing it.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合**：将数据汇总并概括。'
- en: '**Resampling**: Aggregating time series data at specific intervals.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重采样**：在特定的时间间隔内聚合时间序列数据。'
- en: Now that we understand what data wrangling is, let's collect some data to work
    with. Note that we will cover data cleaning and transformation in this chapter,
    while data enrichment will be covered in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了数据整理的概念，接下来让我们收集一些数据来进行操作。请注意，在本章中我们将讨论数据清理和转换，而数据丰富将在[*第4章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)中讨论，内容包括*聚合Pandas数据框*。
- en: Exploring an API to find and collect temperature data
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索API以查找并收集温度数据
- en: "In [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035), *Working\
    \ with Pandas DataFrames*, we worked on data collection and how \Lto perform an\
    \ initial inspection and filtering of the data; this usually gives us ideas of\
    \ things that need to be addressed before we move further in our analysis. Since\
    \ this chapter builds on those skills, we will get to practice some of them here\
    \ as well. To begin, we will start by exploring the weather API that's provided\
    \ by the NCEI. Then, in the next section, we will learn about data wrangling using\
    \ temperature data that was previously obtained from this API."
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第2章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035)中，*使用Pandas数据框*，我们处理了数据收集以及如何进行初步检查和筛选数据；这通常会给我们一些启示，告诉我们在进一步分析之前需要解决的事项。由于本章内容建立在这些技能的基础上，我们也将在这里练习其中的一些技能。首先，我们将开始探索NCEI提供的天气API。接下来，在下一节中，我们将学习如何使用之前从该API获取的温度数据进行数据整理。
- en: Important note
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'To use the NCEI API, you will have to request a token by filling out this form
    with your email address: [https://www.ncdc.noaa.gov/cdo-web/token](https://www.ncdc.noaa.gov/cdo-web/token).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用NCEI API，您需要填写此表格并提供您的电子邮件地址以申请一个令牌：[https://www.ncdc.noaa.gov/cdo-web/token](https://www.ncdc.noaa.gov/cdo-web/token)。
- en: 'For this section, we will be working in the `2-using_the_weather_api.ipynb`
    notebook to request temperature data from the NCEI API. As we learned in [*Chapter
    2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035),*Working with Pandas DataFrames*,
    we can use the `requests` library to interact with APIs. In the following code
    block, we import the `requests` library and create a convenience function for
    making the requests to a specific endpoint, sending our token along. To use this
    function, we need to provide a token, as indicated in bold:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在`2-using_the_weather_api.ipynb`笔记本中请求NCEI API的温度数据。正如我们在[*第2章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035)中学到的，*使用Pandas数据框*，我们可以使用`requests`库与API进行交互。在下面的代码块中，我们导入了`requests`库，并创建了一个便捷函数来向特定端点发出请求，并附带我们的令牌。要使用此函数，我们需要提供一个令牌，具体如下所示（以粗体显示）：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Tip
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'This function is making use of `format()` method: `''api/v2/{}''.format(endpoint)`.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数使用了`format()`方法：`'api/v2/{}'.format(endpoint)`。
- en: 'To use the `make_request()` function, we need to learn how to form our request.
    The NCEI has a helpful getting started page ([https://www.ncdc.noaa.gov/cdo-web/webservices/v2#gettingStarted](https://www.ncdc.noaa.gov/cdo-web/webservices/v2#gettingStarted))
    that shows us how to form requests; we can progress through the tabs on the page
    to figure out what filters we want on our query. The `requests` library takes
    care of turning our dictionary of search parameters (passed in as `payload`) into
    a `2018-08-28` for `start` and `2019-04-15` for `end`, we will get `?start=2018-08-28&end=2019-04-15`),
    just like the examples on the website. This API provides many different endpoints
    for exploring what is offered and building up our ultimate request for the actual
    dataset. We will start by figuring out the ID of the dataset we want to query
    for (`datasetid`) using the `datasets` endpoint. Let''s check which datasets have
    data within the date range of October 1, 2018 through today:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`make_request()`函数，我们需要学习如何构建请求。NCEI提供了一个有用的入门页面（[https://www.ncdc.noaa.gov/cdo-web/webservices/v2#gettingStarted](https://www.ncdc.noaa.gov/cdo-web/webservices/v2#gettingStarted)），该页面向我们展示了如何构建请求；我们可以通过页面上的选项卡逐步确定查询中需要哪些过滤条件。`requests`库负责将我们的搜索参数字典（作为`payload`传递）转换为`2018-08-28`的`start`和`2019-04-15`的`end`，最终得到`?start=2018-08-28&end=2019-04-15`，就像网站上的示例一样。这个API提供了许多不同的端点，供我们探索所提供的内容，并构建我们最终的实际数据集请求。我们将从使用`datasets`端点查找我们想查询的数据集ID（`datasetid`）开始。让我们检查哪些数据集在2018年10月1日至今天之间有数据：
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Remember that we check the `status_code` attribute to make sure the request
    was successful. Alternatively, we can use the `ok` attribute to get a Boolean
    indicator if everything went as expected:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们需要检查`status_code`属性，以确保请求成功。或者，我们可以使用`ok`属性来获取布尔指示，查看是否一切按预期进行：
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Tip
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The API limits us to 5 requests per second and 10,000 requests per day. If we
    exceed these limits, the status code will indicate a client error (meaning that
    the error appears to have been caused by us). Client errors have status codes
    in the 400s; for example, 404, if the requested resource can't be found, or 400,
    if the server can't understand our request (or refuses to process it). Sometimes,
    the server has an issue on its side when processing our request, in which case
    we see status codes in the 500s. You can find a listing of common status codes
    and their meanings at [https://restfulapi.net/http-status-codes/](https://restfulapi.net/http-status-codes/).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: API限制我们每秒最多5个请求，每天最多10,000个请求。如果超过这些限制，状态码将显示客户端错误（意味着错误似乎是由我们引起的）。客户端错误的状态码通常在400系列；例如，404表示请求的资源无法找到，400表示服务器无法理解我们的请求（或拒绝处理）。有时，服务器在处理我们的请求时遇到问题，在这种情况下，我们会看到500系列的状态码。您可以在[https://restfulapi.net/http-status-codes/](https://restfulapi.net/http-status-codes/)找到常见状态码及其含义的列表。
- en: 'Once we have our response, we can use the `json()` method to get the payload.
    Then, we can use dictionary methods to determine which part we want to look at:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到响应，就可以使用`json()`方法获取有效载荷。然后，我们可以使用字典方法来确定我们想查看的部分：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `metadata` portion of the JSON payload tells us information about the result,
    while the `results` section contains the actual results. Let''s see how much data
    we got back, so that we know whether we can print the results or whether we should
    try to limit the output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`metadata`部分的JSON有效载荷告诉我们有关结果的信息，而`results`部分包含实际的结果。让我们看看我们收到了多少数据，这样我们就知道是否可以打印结果，或者是否应该尝试限制输出：'
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We got back 11 rows, so let''s see what fields are in the `results` portion
    of the JSON payload. The `results` key contains a list of dictionaries. If we
    select the first one, we can look at the keys to see what fields the data contains.
    We can then reduce the output to the fields we care about:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收到了11行数据，因此让我们看看`results`部分的JSON有效载荷包含哪些字段。`results`键包含一个字典列表。如果我们选择第一个字典，可以查看键以了解数据包含哪些字段。然后，我们可以将输出缩减到我们关心的字段：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For our purposes, we want to look at the IDs and names of the datasets, so
    let''s use a list comprehension to look at those only:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们希望查看数据集的ID和名称，因此让我们使用列表推导式仅查看这些：
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The first entry in the result is what we are looking for. Now that we have
    a value for `datasetid` (`GHCND`), we proceed to identify one for `datacategoryid`,
    which we need to request temperature data. We do so using the `datacategories`
    endpoint. Here, we can print the JSON payload since it isn''t that large (only
    nine entries):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 结果中的第一个条目就是我们要找的。现在我们有了`datasetid`的值（`GHCND`），我们继续识别一个`datacategoryid`，我们需要使用`datacategories`端点请求温度数据。在这里，我们可以打印JSON负载，因为它并不是很大（仅九个条目）：
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Based on the previous result, we know that we want a value of `TEMP` for `datacategoryid`.
    Next, we use this to identify the data types we want by using the `datatypes`
    endpoint. We will use a list comprehension once again to only print the names
    and IDs; this is still a rather large list, so the output has been abbreviated:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 根据先前的结果，我们知道我们想要`TEMP`的值作为`datacategoryid`。接下来，我们使用此值通过`datatypes`端点识别我们想要的数据类型。我们将再次使用列表推导式仅打印名称和ID；这仍然是一个相当大的列表，因此输出已经被缩短。
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We are looking for the `TAVG`, `TMAX`, and `TMIN` data types. Now that we have
    everything we need to request temperature data for all locations, we need to narrow
    it down to a specific location. To determine a value for `locationcategoryid`,
    we must use the `locationcategories` endpoint:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在寻找`TAVG`、`TMAX`和`TMIN`数据类型。现在我们已经准备好请求所有位置的温度数据，我们需要将其缩小到特定位置。要确定`locationcategoryid`的值，我们必须使用`locationcategories`端点：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Note that we can use `pprint` from the Python standard library ([https://docs.python.org/3/library/pprint.html](https://docs.python.org/3/library/pprint.html))
    to print our JSON payload in an easier-to-read format:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们可以使用来自Python标准库的`pprint`（[https://docs.python.org/3/library/pprint.html](https://docs.python.org/3/library/pprint.html)）来以更易读的格式打印我们的JSON负载：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We want to look at New York City, so, for the `locationcategoryid` filter, `CITY`
    is the proper value. The notebook we are working in has a function to search for
    a field by name using **binary search** on the API; binary search is a more efficient
    way of searching through an ordered list. Since we know that the fields can be
    sorted alphabetically, and the API gives us metadata about the request, we know
    how many items the API has for a given field and can tell whether we have passed
    the one we are looking for.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要查看纽约市，因此对于`locationcategoryid`过滤器，`CITY`是正确的值。我们正在使用API上的**二分搜索**来搜索字段的笔记本；二分搜索是一种更有效的有序列表搜索方法。因为我们知道字段可以按字母顺序排序，并且API提供了有关请求的元数据，我们知道API对于给定字段有多少项，并且可以告诉我们是否已经通过了我们正在寻找的项。
- en: 'With each request, we grab the middle entry and compare its location in the
    alphabet with our target; if the result comes before our target, we look at the
    half of the data that''s greater than what we just got; otherwise, we look at
    the smaller half. Each time, we are slicing the data in half, so when we grab
    the middle entry to test, we are moving closer to the value we seek (see *Figure
    3.10*):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 每次请求时，我们获取中间条目并将其与我们的目标字母顺序比较；如果结果在我们的目标之前出现，我们查看大于我们刚获取的数据的一半；否则，我们查看较小的一半。每次，我们都将数据切成一半，因此当我们获取中间条目进行测试时，我们越来越接近我们寻找的值（见*图3.10*）：
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is a **recursive** implementation of the algorithm, meaning that we call
    the function itself from inside; we must be very careful when we do this to define
    a **base condition** so that it will eventually stop and not enter an infinite
    loop. It is possible to implement this iteratively. See the *Further reading*
    section at the end of this chapter for additional reading on binary search and
    **recursion**.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这是算法的**递归**实现，这意味着我们从内部调用函数自身；我们在这样做时必须非常小心，以定义一个**基本条件**，以便最终停止并避免进入无限循环。可以以迭代方式实现此功能。有关二分搜索和**递归**的更多信息，请参阅本章末尾的*进一步阅读*部分。
- en: Important note
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: In a traditional implementation of binary search, it is trivial to find the
    length of the list that we are searching. With the API, we have to make one request
    to get the count; therefore, we must ask for the first entry (offset of 1) to
    orient ourselves. This means that we make an extra request here compared to what
    we would have needed if we knew how many locations were in the list before starting.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的二分搜索实现中，查找我们搜索的列表的长度是微不足道的。使用API，我们必须发出一个请求来获取计数；因此，我们必须请求第一个条目（偏移量为1）来确定方向。这意味着与我们开始时所需的相比，我们在这里多做了一个额外的请求。
- en: 'Now, let''s use the binary search implementation to find the ID for New York
    City, which will be the value we will use for `locationid` in subsequent queries:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用二分查找实现来查找纽约市的 ID，这将作为后续查询中 `locationid` 的值：
- en: '[PRE20]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'By using binary search here, we find **New York** in **8** requests, despite
    it being close to the middle of 1,983 entries! For comparison, using linear search,
    we would have looked at 1,254 entries before finding it. In the following diagram,
    we can see how binary search eliminates sections of the list of locations systematically,
    which is represented by black on the number line (white means it is still possible
    that the desired value is in that section):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在这里使用二分查找，我们只用了 **8** 次请求就找到了 **纽约**，尽管它位于 1,983 个条目的中间！做个对比，使用线性查找，我们在找到它之前会查看
    1,254 个条目。在下面的图示中，我们可以看到二分查找是如何系统性地排除位置列表中的部分内容的，这在数轴上用黑色表示（白色表示该部分仍可能包含所需值）：
- en: '![Figure 3.10 – Binary search to find New York City'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.10 – 二分查找定位纽约市'
- en: '](img/Figure_3.10_B16834.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.10_B16834.jpg)'
- en: Figure 3.10 – Binary search to find New York City
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 二分查找定位纽约市
- en: Tip
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Some APIs (such as the NCEI API) restrict the number of requests we can make
    within certain periods of time, so it's important to be smart about our requests.
    When searching a very long ordered list, think of binary search.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 API（如 NCEI API）限制我们在某些时间段内可以进行的请求次数，因此我们必须聪明地进行请求。当查找一个非常长的有序列表时，想一想二分查找。
- en: 'Optionally, we can drill down to the ID of the station that is collecting the
    data. This is the most granular level. Using binary search again, we can grab
    the station ID for the Central Park station:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，我们可以深入挖掘收集数据的站点 ID。这是最细粒度的层次。再次使用二分查找，我们可以获取中央公园站点的站点 ID：
- en: '[PRE21]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let''s request NYC''s temperature data in Celsius for October 2018, recorded
    from Central Park. For this, we will use the `data` endpoint and provide all the
    parameters we picked up throughout our exploration of the API:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们请求 2018 年 10 月来自中央公园的纽约市温度数据（摄氏度）。为此，我们将使用 `data` 端点，并提供在探索 API 过程中收集的所有参数：
- en: '[PRE22]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Lastly, we will create a `DataFrame` object; since the `results` portion of
    the JSON payload is a list of dictionaries, we can pass it directly to `pd.DataFrame()`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将创建一个 `DataFrame` 对象；由于 JSON 数据负载中的 `results` 部分是字典列表，我们可以直接将其传递给 `pd.DataFrame()`：
- en: '[PRE23]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We get back long format data. The **datatype** column is the temperature variable
    being measured, and the **value** column contains the measured temperature:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了长格式的数据。**datatype** 列是正在测量的温度变量，**value** 列包含测得的温度：
- en: '![Figure 3.11 – Data retrieved from the NCEI API'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.11 – 从 NCEI API 获取的数据'
- en: '](img/Figure_3.11_B16834.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.11_B16834.jpg)'
- en: Figure 3.11 – Data retrieved from the NCEI API
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 – 从 NCEI API 获取的数据
- en: Tip
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: We can use the previous code to turn any of the JSON responses we worked with
    in this section into a `DataFrame` object, if we find that easier to work with.
    However, it should be stressed that JSON payloads are pretty much ubiquitous when
    it comes to APIs (and, as Python users, we should be familiar with dictionary-like
    objects), so it won't hurt to get comfortable with them.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用之前的代码将本节中处理过的任何 JSON 响应转换为 `DataFrame` 对象，如果我们觉得这样更方便。但需要强调的是，JSON 数据负载几乎在所有
    API 中都很常见（作为 Python 用户，我们应该熟悉类似字典的对象），因此，熟悉它们不会有什么坏处。
- en: 'We asked for `TAVG`, `TMAX`, and `TMIN`, but notice that we didn''t get `TAVG`.
    This is because the Central Park station isn''t recording average temperature,
    despite being listed in the API as offering it—real-world data is dirty:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们请求了 `TAVG`、`TMAX` 和 `TMIN`，但注意到我们没有得到 `TAVG`。这是因为尽管中央公园站点在 API 中列出了提供平均温度的选项，但它并没有记录该数据——现实世界中的数据是有缺陷的：
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Time for plan B: let''s use LaGuardia Airport as the station instead of Central
    Park for the remainder of this chapter. Alternatively, we could have grabbed data
    for all the stations that cover New York City; however, since this would give
    us multiple entries per day for some of the temperature measurements, we won''t
    do so here—we would need skills that will be covered in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*, to work with that data.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 计划 B 的时候到了：让我们改用拉瓜迪亚机场作为本章剩余部分的站点。或者，我们本可以获取覆盖整个纽约市的所有站点的数据；不过，由于这会导致一些温度测量数据每天有多个条目，我们不会在这里这么做——要处理这些数据，我们需要一些将在
    [*第 4 章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082) 中介绍的技能，*聚合 Pandas DataFrames*。
- en: The process of collecting the weather data from the LaGuardia Airport station
    is the same as with the Central Park station, but in the interest of brevity,
    we will read in the data for LaGuardia in the next notebook when we discuss cleaning
    the data. Note that the bottom cells of the current notebook contain the code
    that's used to collect this data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 从LaGuardia机场站收集天气数据的过程与从中央公园站收集数据的过程相同，但为了简洁起见，我们将在下一个笔记本中讨论清洗数据时再读取LaGuardia的数据。请注意，当前笔记本底部的单元格包含用于收集这些数据的代码。
- en: Cleaning data
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清洗数据
- en: 'Let''s move on to the `3-cleaning_data.ipynb` notebook for our discussion of
    data cleaning. As usual, we will begin by importing `pandas` and reading in our
    data. For this section, we will be using the `nyc_temperatures.csv` file, which
    contains the maximum daily temperature (`TMAX`), minimum daily temperature (`TMIN`),
    and the average daily temperature (`TAVG`) from the LaGuardia Airport station
    in New York City for October 2018:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们转到`3-cleaning_data.ipynb`笔记本讨论数据清洗。和往常一样，我们将从导入`pandas`并读取数据开始。在这一部分，我们将使用`nyc_temperatures.csv`文件，该文件包含2018年10月纽约市LaGuardia机场站的每日最高气温（`TMAX`）、最低气温（`TMIN`）和平均气温（`TAVG`）：
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We retrieved long format data from the API; for our analysis, we want wide
    format data, but we will address that in the *Pivoting DataFrames* section, later
    in this chapter:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从API获取的是长格式数据；对于我们的分析，我们需要宽格式数据，但我们将在本章稍后的*数据透视*部分讨论这个问题：
- en: '![Figure 3.12 – NYC temperature data'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.12 – 纽约市温度数据'
- en: '](img/Figure_3.12_B16834.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.12_B16834.jpg)'
- en: Figure 3.12 – NYC temperature data
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 – 纽约市温度数据
- en: 'For now, we will focus on making little tweaks to the data that will make it
    easier for us to use: renaming columns, converting each column into the most appropriate
    data type, sorting, and reindexing. Often, this will be the time to filter the
    data down, but we did that when we worked on requesting data from the API; for
    a review of filtering with `pandas`, refer to [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035),
    *Working with Pandas DataFrames*.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将专注于对数据进行一些小的调整，使其更易于使用：重命名列、将每一列转换为最合适的数据类型、排序和重新索引。通常，这也是过滤数据的时机，但我们在从API请求数据时已经进行了过滤；有关使用`pandas`过滤的回顾，请参考[*第2章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035)，*使用Pandas
    DataFrame*。
- en: Renaming columns
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重命名列
- en: 'Since the API endpoint we used could return data of any units and category,
    it had to call that column `value`. We only pulled temperature data in Celsius,
    so all our observations have the same units. This means that we can rename the
    `value` column so that it''s clear what data we are working with:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的API端点可以返回任何单位和类别的数据，因此它将该列命名为`value`。我们只提取了摄氏度的温度数据，因此所有观测值的单位都相同。这意味着我们可以重命名`value`列，以便明确我们正在处理的数据：
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `DataFrame` class has a `rename()` method that takes a dictionary mapping
    the old column name to the new column name. In addition to renaming the `value`
    column, let''s rename the `attributes` column to `flags` since the API documentation
    mentions that that column contains flags for information about data collection:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame`类有一个`rename()`方法，该方法接收一个字典，将旧列名映射到新列名。除了重命名`value`列外，我们还将`attributes`列重命名为`flags`，因为API文档提到该列包含有关数据收集的信息标志：'
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Most of the time, `pandas` will return a new `DataFrame` object; however, since
    we passed in `inplace=True`, our original dataframe was updated instead. Always
    be careful with in-place operations, as they might be difficult or impossible
    to undo. Our columns now have their new names:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，`pandas`会返回一个新的`DataFrame`对象；然而，由于我们传递了`inplace=True`，原始数据框被直接更新了。使用原地操作时要小心，因为它们可能难以或不可能撤销。我们的列现在已经有了新名字：
- en: '[PRE28]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Tip
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Both `Series` and `Index` objects can also be renamed using their `rename()`
    methods. Simply pass in the new name. For example, if we have a `Series` object
    called `temperature` and we want to rename it `temp_C`, we can run `temperature.rename('temp_C')`.
    The variable will still be called `temperature`, but the name of the data in the
    series itself will now be `temp_C`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`Series`和`Index`对象也可以使用它们的`rename()`方法重命名。只需传入新名称。例如，如果我们有一个名为`temperature`的`Series`对象，并且我们想将其重命名为`temp_C`，我们可以运行`temperature.rename(''temp_C'')`。变量仍然叫做`temperature`，但Series本身的数据名称将变为`temp_C`。'
- en: 'We can also do transformations on the column names with `rename()`. For instance,
    we can put all the column names in uppercase:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`rename()`对列名进行转换。例如，我们可以将所有列名转换为大写：
- en: '[PRE29]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This method even lets us rename the values of the index, although this is something
    we don't have use for now since our index is just numbers. However, for reference,
    we would simply change `axis='columns'` in the preceding code to `axis='rows'`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法甚至允许我们重命名索引的值，尽管目前我们还不需要这样做，因为我们的索引只是数字。然而，作为参考，只需将前面代码中的 `axis='columns'`
    改为 `axis='rows'` 即可。
- en: Type conversion
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类型转换
- en: Now that the column names are indicative of the data they contain, we can check
    what types of data they hold. We should have formed an intuition as to what the
    data types should be after looking at the first few rows when we inspected the
    dataframe with the `head()` method previously. With type conversion, we aim to
    reconcile what the current data types are with what we believe they should be;
    we will be changing how our data is represented.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，列名已经能够准确指示它们所包含的数据，我们可以检查它们所持有的数据类型。在之前使用 `head()` 方法查看数据框的前几行时，我们应该已经对数据类型有了一些直观的理解。通过类型转换，我们的目标是将当前的数据类型与我们认为应该是的类型进行对比；我们将更改数据的表示方式。
- en: Note that, sometimes, we may have data that we believe should be a certain type,
    such as a date, but it is stored as a string; this could be for a very valid reason—data
    could be missing. In the case of missing data encoded as text (for example, `?`
    or `N/A`), `pandas` will store it as a string when reading it in to allow for
    this data. It will be marked as `object` when we use the `dtypes` attribute on
    our dataframe. If we try to convert (or **cast**) these columns, we will either
    get an error or our result won't be what we expected. For example, if we have
    strings of decimal numbers, but try to convert the column into integers, we will
    get an error since Python knows they aren't integers; however, if we try to convert
    decimal numbers into integers, we will lose any information after the decimal
    point.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，有时我们可能会遇到认为应该是某种类型的数据，比如日期，但它实际上存储为字符串；这可能有很合理的原因——数据可能丢失了。在这种情况下，存储为文本的缺失数据（例如
    `?` 或 `N/A`）会被 `pandas` 在读取时作为字符串处理。使用 `dtypes` 属性查看数据框时，它将被标记为 `object` 类型。如果我们尝试转换（或**强制转换**）这些列，要么会出现错误，要么结果不符合预期。例如，如果我们有小数点数字的字符串，但尝试将其转换为整数，就会出现错误，因为
    Python 知道它们不是整数；然而，如果我们尝试将小数数字转换为整数，就会丢失小数点后的任何信息。
- en: 'That being said, let''s examine the data types in our temperature data. Note
    that the `date` column isn''t actually being stored as a datetime:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们检查一下温度数据中的数据类型。请注意，`date` 列实际上并没有以日期时间格式存储：
- en: '[PRE30]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can use the `pd.to_datetime()` function to convert it into a datetime:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `pd.to_datetime()` 函数将其转换为日期时间：
- en: '[PRE31]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This is much better. Now, we can get useful information when we summarize the
    `date` column:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在好多了。现在，当我们总结 `date` 列时，可以得到有用的信息：
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Dealing with dates can be tricky since they come in many different formats
    and time zones; fortunately, `pandas` has more methods we can use for dealing
    with converting datetime objects. For example, when working with a `DatetimeIndex`
    object, if we need to keep track of time zones, we can use the `tz_localize()`
    method to associate our datetimes with a time zone:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 处理日期可能会比较棘手，因为它们有很多不同的格式和时区；幸运的是，`pandas` 提供了更多我们可以用来处理转换日期时间对象的方法。例如，在处理 `DatetimeIndex`
    对象时，如果我们需要跟踪时区，可以使用 `tz_localize()` 方法将我们的日期时间与时区关联：
- en: '[PRE33]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This also works with `Series` and `DataFrame` objects that have an index of
    type `DatetimeIndex`. We can read in the CSV file again and, this time, specify
    that the `date` column will be our index and that we should parse any dates in
    the CSV file into datetimes:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这同样适用于具有 `DatetimeIndex` 类型索引的 `Series` 和 `DataFrame` 对象。我们可以再次读取 CSV 文件，这次指定
    `date` 列为索引，并将 CSV 文件中的所有日期解析为日期时间：
- en: '[PRE34]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We had to read the file in again for this example because we haven''t learned
    how to change the index of our data yet (covered in the *Reordering, reindexing,
    and sorting data* section, later this chapter). Note that we have added the Eastern
    Standard Time offset (-05:00 from UTC) to the datetimes in the index:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们不得不重新读取文件，因为我们还没有学习如何更改数据的索引（将在本章稍后的*重新排序、重新索引和排序数据*部分讲解）。请注意，我们已经将东部标准时间偏移（UTC-05:00）添加到了索引中的日期时间：
- en: '![Figure 3.13 – Time zone-aware dates in the index'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.13 – 索引中的时区感知日期'
- en: '](img/Figure_3.13_B16834.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.13_B16834.jpg)'
- en: Figure 3.13 – Time zone-aware dates in the index
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 – 索引中的时区感知日期
- en: 'We can use the `tz_convert()` method to change the time zone into a different
    one. Let''s change our data into UTC:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`tz_convert()`方法将时区转换为其他时区。让我们将数据转换为UTC时区：
- en: '[PRE35]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, the offset is UTC (+00:00), but note that the time portion of the date
    is now 5 AM; this conversion took into account the -05:00 offset:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，偏移量是UTC（+00:00），但请注意，日期的时间部分现在是上午5点；这次转换考虑了-05:00的偏移：
- en: '![Figure 3.14 – Converting data into another time zone'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.14 – 将数据转换为另一个时区'
- en: '](img/Figure_3.14_B16834.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.14_B16834.jpg)'
- en: Figure 3.14 – Converting data into another time zone
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14 – 将数据转换为另一个时区
- en: 'We can also truncate datetimes with the `to_period()` method, which comes in
    handy if we don''t care about the full date. For example, if we wanted to aggregate
    our data by month, we could truncate our index to just the month and year and
    then perform the aggregation. Since we will cover aggregation in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*, we will just do the truncation here. Note that
    we first remove the time zone information to avoid a warning from `pandas` that
    the `PeriodArray` class doesn''t have time zone information, and therefore it
    will be lost. This is because the underlying data for a `PeriodIndex` object is
    stored as a `PeriodArray` object:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用`to_period()`方法截断日期时间，如果我们不关心完整的日期，这个方法非常有用。例如，如果我们想按月汇总数据，我们可以将索引截断到仅包含月份和年份，然后进行汇总。由于我们将在[*第4章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)《聚合Pandas
    DataFrame》中讨论聚合方法，我们这里只做截断。请注意，我们首先去除时区信息，以避免`pandas`的警告，提示`PeriodArray`类没有时区信息，因此会丢失。这是因为`PeriodIndex`对象的底层数据是存储为`PeriodArray`对象：
- en: '[PRE36]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can use the `to_timestamp()` method to convert our `PeriodIndex` object
    into a `DatetimeIndex` object; however, the datetimes all start at the first of
    the month now:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`to_timestamp()`方法将我们的`PeriodIndex`对象转换为`DatetimeIndex`对象；然而，所有的日期时间现在都从每月的第一天开始：
- en: '[PRE37]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Alternatively, we can use the `assign()` method to handle any type conversions
    by passing the column names as named parameters and their new values as the value
    for that argument to the method call. In practice, this will be more beneficial
    since we can perform many tasks in one call and use the columns we create in that
    call to calculate additional columns. For example, let''s cast the `date` column
    to a datetime and add a new column for the temperature in Fahrenheit (`temp_F`).
    The `assign()` method returns a new `DataFrame` object, so we must remember to
    assign it to a variable if we want to keep it. Here, we will create a new one.
    Note that our original conversion of the dates modified the column, so, in order
    to illustrate that we can use `assign()`, we need to read our data in once more:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用`assign()`方法来处理任何类型转换，通过将列名作为命名参数传递，并将其新值作为该参数的值传递给方法调用。在实践中，这样做会更有益，因为我们可以在一次调用中执行许多任务，并使用我们在该调用中创建的列来计算额外的列。例如，我们将`date`列转换为日期时间，并为华氏温度（`temp_F`）添加一个新列。`assign()`方法返回一个新的`DataFrame`对象，因此如果我们想保留它，必须记得将其分配给一个变量。在这里，我们将创建一个新的对象。请注意，我们原始的日期转换已经修改了该列，因此为了说明我们可以使用`assign()`，我们需要再次读取数据：
- en: '[PRE38]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We now have datetimes in the `date` column and a new column, `temp_F`:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在`date`列中有日期时间，并且有了一个新列`temp_F`：
- en: '![Figure 3.15 – Simultaneous type conversion and column creation'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.15 – 同时进行类型转换和列创建'
- en: '](img/Figure_3.15_B16834.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.15_B16834.jpg)'
- en: Figure 3.15 – Simultaneous type conversion and column creation
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15 – 同时进行类型转换和列创建
- en: 'Additionally, we can use the `astype()` method to convert one column at a time.
    As an example, let''s say we only cared about the temperatures at every whole
    number, but we don''t want to round. In this case, we simply want to chop off
    the information after the decimal. To accomplish this, we can cast the floats
    as integers. This time, we will use `temp_F` column to create the `temp_F_whole`
    column, even though `df` doesn''t have this column before we call `assign()`.
    It is very common (and useful) to use lambda functions with `assign()`:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以使用`astype()`方法一次转换一列。例如，假设我们只关心每个整数的温度，但不想进行四舍五入。在这种情况下，我们只是想去掉小数点后的信息。为此，我们可以将浮动值转换为整数。此次，我们将使用`temp_F`列创建`temp_F_whole`列，即使在调用`assign()`之前，`df`中并没有这个列。结合`assign()`使用lambda函数是非常常见（且有用）的：
- en: '[PRE39]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Note that we can refer to columns we''ve just created if we use a lambda function.
    It''s also important to mention that we don''t have to know whether to convert
    the column into a float or an integer: we can use `pd.to_numeric()`, which will
    convert the data into floats if it sees decimals. If all the numbers are whole,
    they will be converted into integers (obviously, we will still get errors if the
    data isn''t numeric at all):'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果我们使用lambda函数，我们可以引用刚刚创建的列。还需要提到的是，我们不必知道是将列转换为浮动数值还是整数：我们可以使用`pd.to_numeric()`，如果数据中有小数，它会将数据转换为浮动数；如果所有数字都是整数，它将转换为整数（显然，如果数据根本不是数字，仍然会出现错误）：
- en: '![Figure 3.16 – Creating columns with lambda functions'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.16 – 使用 lambda 函数创建列'
- en: '](img/Figure_3.16_B16834.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.16_B16834.jpg)'
- en: Figure 3.16 – Creating columns with lambda functions
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.16 – 使用 lambda 函数创建列
- en: Lastly, we have two columns with data currently being stored as strings that
    can be represented in a better way for this dataset. The `station` and `datatype`
    columns only have one and three distinct values, respectively, meaning that we
    aren't being efficient with our memory use since we are storing them as strings.
    We could potentially have issues with analyses further down the line. Pandas has
    the ability to define columns as `pandas` and other packages will be able to handle
    this data, provide meaningful statistics on them, and use them properly. Categorical
    variables can take on one of a few values; for example, blood type would be a
    categorical variable—people can only have one of A, B, AB, or O.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有两列当前存储为字符串的数据，可以用更适合此数据集的方式来表示。`station`和`datatype`列分别只有一个和三个不同的值，这意味着我们在内存使用上并不高效，因为我们将它们存储为字符串。这样可能会在后续的分析中出现问题。Pandas能够将列定义为类别，并且其他包可以处理这些数据，提供有意义的统计信息，并正确使用它们。类别变量可以取几个值中的一个；例如，血型就是一个类别变量——人们只能有A型、B型、AB型或O型中的一种。
- en: 'Going back to the temperature data, we only have one value for the `station`
    column and only three distinct values for the `datatype` column (`TAVG`, `TMAX`,
    `TMIN`). We can use the `astype()` method to cast these into categories and look
    at the summary statistics:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 回到温度数据，我们的`station`列只有一个值，`datatype`列只有三个不同的值（`TAVG`、`TMAX`、`TMIN`）。我们可以使用`astype()`方法将它们转换为类别，并查看汇总统计信息：
- en: '[PRE40]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The summary statistics for categories are just like those for strings. We can
    see the number of non-null entries (**count**), the number of unique values (**unique**),
    the mode (**top**), and the number of occurrences of the mode (**freq**):'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 类别的汇总统计信息与字符串的汇总统计类似。我们可以看到非空条目的数量（**count**）、唯一值的数量（**unique**）、众数（**top**）以及众数的出现次数（**freq**）：
- en: '![Figure 3.17 – Summary statistics for the categorical columns'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.17 – 类别列的汇总统计'
- en: '](img/Figure_3.17_B16834.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.17_B16834.jpg)'
- en: Figure 3.17 – Summary statistics for the categorical columns
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.17 – 类别列的汇总统计
- en: 'The categories we just made don''t have any order to them, but `pandas` does
    support this:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建的类别没有顺序，但是`pandas`确实支持这一点：
- en: '[PRE41]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: When the columns in our dataframe are stored in the appropriate type, it opens
    up additional avenues for exploration, such as calculating statistics, aggregating
    the data, and sorting the values. For example, depending on our data source, it's
    possible that the numeric data is represented as a string, in which case attempting
    to sort on the values will reorder the contents lexically, meaning the result
    could be 1, 10, 11, 2, rather than 1, 2, 10, 11 (numerical sort). Similarly, if
    we have dates represented as strings in a format other than YYYY-MM-DD, sorting
    on this information may result in non-chronological order; however, by converting
    the date strings with `pd.to_datetime()`, we can chronologically sort dates that
    are provided in any format. Type conversion makes it possible to reorder both
    the numeric data and the dates according to their values, rather than their initial
    string representations.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的数据框中的列被存储为适当的类型时，它为探索其他领域打开了更多的可能性，比如计算统计数据、汇总数据和排序值。例如，取决于我们的数据源，数字数据可能被表示为字符串，在这种情况下，如果尝试按值进行排序，排序结果将按字典顺序重新排列，意味着结果可能是1、10、11、2，而不是1、2、10、11（数字排序）。类似地，如果日期以除YYYY-MM-DD格式以外的字符串表示，排序时可能会导致非按时间顺序排列；但是，通过使用`pd.to_datetime()`转换日期字符串，我们可以按任何格式提供的日期进行按时间排序。类型转换使得我们可以根据数值而非初始的字符串表示，重新排序数字数据和日期。
- en: Reordering, reindexing, and sorting data
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新排序、重新索引和排序数据
- en: 'We will often find the need to sort our data by the values of one or many columns.
    Say we wanted to find the days that reached the highest temperatures in New York
    City during October 2018; we could sort our values by the `temp_C` (or `temp_F`)
    column in descending order and use `head()` to select the number of days we wanted
    to see. To accomplish this, we can use the `sort_values()` method. Let''s look
    at the top 10 days:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要根据一个或多个列的值对数据进行排序。例如，如果我们想找到 2018 年 10 月在纽约市达到最高温度的日期；我们可以按 `temp_C`（或
    `temp_F`）列降序排序，并使用 `head()` 选择我们想查看的天数。为了实现这一点，我们可以使用 `sort_values()` 方法。让我们看看前
    10 天：
- en: '[PRE42]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This shows us that on October 7th and October 10th the temperature reached
    its highest value during the month of October 2018, according to the LaGuardia
    station. We also have ties between October 2nd and 4th, October 1st and 9th, and
    October 5th and 8th, but notice that the dates aren''t always sorted—the 10th
    is after the 7th, but the 4th comes before the 2nd:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 LaGuardia 站的数据，这表明 2018 年 10 月 7 日和 10 月 10 日的温度达到了 2018 年 10 月的最高值。我们还在
    10 月 2 日和 4 日、10 月 1 日和 9 日、10 月 5 日和 8 日之间存在平局，但请注意，日期并不总是按顺序排列——10 日排在 7 日之后，但
    4 日排在 2 日之前：
- en: '![Figure 3.18 – Sorting the data to find the warmest days'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.18 – 排序数据以找到最温暖的天数'
- en: '](img/Figure_3.18_B16834.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.18_B16834.jpg)'
- en: Figure 3.18 – Sorting the data to find the warmest days
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.18 – 排序数据以找到最温暖的天数
- en: 'The `sort_values()` method can be used with a list of column names to break
    ties. The order in which the columns are provided will determine the sort order,
    with each subsequent column being used to break ties. As an example, let''s make
    sure the dates are sorted in ascending order when breaking ties:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`sort_values()` 方法可以与列名列表一起使用，以打破平局。提供列的顺序将决定排序顺序，每个后续的列将用于打破平局。例如，确保在打破平局时按升序排列日期：'
- en: '[PRE43]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Since we are sorting in ascending order, in the case of a tie, the date that
    comes earlier in the year will be above the later one. Notice how October 2nd
    is now above October 4th, despite both having the same temperature reading:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们按升序排序，在平局的情况下，年份较早的日期会排在年份较晚的日期之前。请注意，尽管 10 月 2 日和 4 日的温度读数相同，但现在 10 月 2
    日排在 10 月 4 日之前：
- en: '![Figure 3.19 – Sorting the data with multiple columns to break ties'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.19 – 使用多个列进行排序以打破平局'
- en: '](img/Figure_3.19_B16834.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.19_B16834.jpg)'
- en: Figure 3.19 – Sorting the data with multiple columns to break ties
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.19 – 使用多个列进行排序以打破平局
- en: Tip
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: In `pandas`, the index is tied to the rows—when we drop rows, filter, or do
    anything that returns only some of the rows, our index may look out of order (as
    we saw in the previous examples). At the moment, the index just represents the
    row number in our data, so we may be interested in changing the values so that
    we have the first entry at index `0`. To have `pandas` do so automatically, we
    can pass `ignore_index=True` to `sort_values()`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `pandas` 中，索引与行相关联——当我们删除行、筛选或执行任何返回部分行的操作时，我们的索引可能会看起来不按顺序（正如我们在之前的示例中看到的）。此时，索引仅代表数据中的行号，因此我们可能希望更改索引的值，使第一个条目出现在索引
    `0` 位置。为了让 `pandas` 自动执行此操作，我们可以将 `ignore_index=True` 传递给 `sort_values()`。
- en: 'Pandas also provides an additional way to look at a subset of the sorted values;
    we can use `nlargest()` to grab the `n` rows with the largest values according
    to specific criteria and `nsmallest()` to grab the `n` smallest rows, without
    the need to sort the data beforehand. Both accept a list of column names or a
    string for a single column. Let''s grab the top 10 days by average temperature
    this time:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 还提供了一种额外的方式来查看排序值的子集；我们可以使用 `nlargest()` 按照特定标准抓取具有最大值的 `n` 行，使用 `nsmallest()`
    抓取具有最小值的 `n` 行，无需事先对数据进行排序。两者都接受列名列表或单列的字符串。让我们这次抓取按平均温度排序的前 10 天：
- en: '[PRE44]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We get the warmest days (on average) in October:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们找到了 10 月份最温暖的天数（平均温度）：
- en: '![Figure 3.20 – Sorting to find the 10 warmest days on average'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.20 – 排序以找到平均温度最高的 10 天'
- en: '](img/Figure_3.20_B16834.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.20_B16834.jpg)'
- en: Figure 3.20 – Sorting to find the 10 warmest days on average
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.20 – 排序以找到平均温度最高的 10 天
- en: 'We aren''t limited to sorting values; if we wish, we can even order the columns
    alphabetically and sort the rows by their index values. For these tasks, we can
    use the `sort_index()` method. By default, `sort_index()` will target the rows
    so that we can do things such as order the index after performing an operation
    that shuffles it. For instance, the `sample()` method will give us randomly selected
    rows, which will lead to a jumbled index, so we can use `sort_index()` to order
    them afterward:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅限于对值进行排序；如果需要，我们甚至可以按字母顺序排列列，并按索引值对行进行排序。对于这些任务，我们可以使用`sort_index()`方法。默认情况下，`sort_index()`会针对行进行操作，以便我们在执行打乱操作后对索引进行排序。例如，`sample()`方法会随机选择若干行，这将导致索引混乱，所以我们可以使用`sort_index()`对它们进行排序：
- en: '[PRE45]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Tip
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: If we need the result of `sample()` to be reproducible, we can pass in a `random_state`
    argument). The seed initializes a pseudorandom number generator, so, provided
    that the same seed is used, the results will be the same.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望`sample()`的结果是可重复的，可以传入`random_state`参数。种子初始化一个伪随机数生成器，只要使用相同的种子，结果就会是相同的。
- en: 'When we want to target columns, we must pass in `axis=1`; rows will be the
    default (`axis=0`). Note that this argument is present in many `pandas` methods
    and functions (including `sample()`), so it''s important to understand what it
    means. Let''s use this knowledge to sort the columns of our dataframe alphabetically:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要操作列时，必须传入`axis=1`；默认是操作行（`axis=0`）。请注意，这个参数在许多`pandas`方法和函数（包括`sample()`）中都存在，因此理解其含义非常重要。我们可以利用这一点按字母顺序对数据框的列进行排序：
- en: '[PRE46]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Having our columns in alphabetical order can come in handy when using `loc[]`
    because we can specify a range of columns with similar names; for example, we
    could now use `df.loc[:,''station'':''temp_F_whole'']` to easily grab all of our
    temperature columns, along with the station information:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 将列按字母顺序排列在使用`loc[]`时很有用，因为我们可以指定一系列具有相似名称的列；例如，现在我们可以使用`df.loc[:,'station':'temp_F_whole']`轻松获取所有温度列及站点信息：
- en: '![Figure 3.21 – Sorting the columns by name'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.21 – 按名称对列进行排序'
- en: '](img/Figure_3.21_B16834.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.21_B16834.jpg)'
- en: Figure 3.21 – Sorting the columns by name
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.21 – 按名称对列进行排序
- en: Important note
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Both `sort_index()` and `sort_values()` return new `DataFrame` objects. We must
    pass in `inplace=True` to update the dataframe we are working with.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`sort_index()`和`sort_values()`都会返回新的`DataFrame`对象。我们必须传入`inplace=True`来更新正在处理的数据框。'
- en: 'The `sort_index()` method can also help us get an accurate answer when we''re
    testing two dataframes for equality. Pandas will check that, in addition to having
    the same data, both have the same values for the index when it compares the rows.
    If we sort our dataframe by temperature in Celsius and check whether it is equal
    to the original dataframe, `pandas` tells us that it isn''t. We must sort the
    index to see that they are the same:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`sort_index()`方法还可以帮助我们在测试两个数据框是否相等时获得准确的答案。Pandas会检查，在数据相同的情况下，两个数据框的索引值是否也相同。如果我们按摄氏温度对数据框进行排序，并检查其是否与原数据框相等，`pandas`会告诉我们它们不相等。我们必须先对索引进行排序，才能看到它们是相同的：'
- en: '[PRE47]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Sometimes, we don''t care too much about the numeric index, but we would like
    to use one (or more) of the other columns as the index instead. In this case,
    we can use the `set_index()` method. Let''s set the `date` column as our index:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们并不关心数字索引，但希望使用其他列中的一个（或多个）作为索引。在这种情况下，我们可以使用`set_index()`方法。让我们将`date`列设置为索引：
- en: '[PRE48]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Notice that the `date` column has moved to the far left where the index goes,
    and we no longer have the numeric index:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`date`列已移到最左侧，作为索引的位置，我们不再有数字索引：
- en: '![Figure 3.22 – Setting the date column as the index'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.22 – 将日期列设置为索引'
- en: '](img/Figure_3.22_B16834.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.22_B16834.jpg)'
- en: Figure 3.22 – Setting the date column as the index
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.22 – 将日期列设置为索引
- en: Tip
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: We can also provide a list of columns to use as the index. This will create
    a `MultiIndex` object, where the first element in the list is the outermost level
    and the last is the innermost. We will discuss this further in the *Pivoting DataFrames*
    section.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以提供一个列的列表，将其作为索引使用。这将创建一个`MultiIndex`对象，其中列表中的第一个元素是最外层级，最后一个是最内层级。我们将在*数据框的透视*部分进一步讨论这一点。
- en: 'Setting the index to a datetime lets us take advantage of datetime slicing
    and indexing, which we briefly discussed in [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035),
    *Working with Pandas DataFrames*. As long as we provide a date format that `pandas`
    understands, we can grab the data. To select all of 2018, we can use `df.loc[''2018'']`;
    for the fourth quarter of 2018, we can use `df.loc[''2018-Q4'']`; and for October,
    we can use `df.loc[''2018-10'']`. These can also be combined to build ranges.
    Note that `loc[]` is optional when using ranges:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 将索引设置为日期时间格式，让我们能够利用日期时间切片和索引功能，正如我们在[*第2章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035)《处理
    Pandas 数据框》中简要讨论的那样。只要我们提供`pandas`能够理解的日期格式，就可以提取数据。要选择2018年的所有数据，我们可以使用`df.loc['2018']`；要选择2018年第四季度的数据，我们可以使用`df.loc['2018-Q4']`；而要选择10月的数据，我们可以使用`df.loc['2018-10']`。这些也可以组合起来构建范围。请注意，在使用范围时，`loc[]`是可选的：
- en: '[PRE49]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This gives us the data from October 11, 2018 through October 12, 2018 (inclusive
    of both endpoints):'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了从2018年10月11日到2018年10月12日（包括这两个端点）的数据：
- en: '![Figure 3.23 – Selecting date ranges'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.23 – 选择日期范围'
- en: '](img/Figure_3.23_B16834.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.23_B16834.jpg)'
- en: Figure 3.23 – Selecting date ranges
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.23 – 选择日期范围
- en: 'We can use the `reset_index()` method to restore the `date` column:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`reset_index()`方法恢复`date`列：
- en: '[PRE50]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Our index now starts at `0`, and the dates are now in a column called `date`.
    This is especially useful if we have data that we don''t want to lose in the index,
    such as the date, but need to perform an operation as if it weren''t in the index:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的索引从`0`开始，日期被放在一个叫做`date`的列中。如果我们有一些数据不想丢失在索引中，比如日期，但需要像没有在索引中一样进行操作，这种做法尤其有用：
- en: '![Figure 3.24 – Resetting the index'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.24 – 重置索引'
- en: '](img/Figure_3.24_B16834.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.24_B16834.jpg)'
- en: Figure 3.24 – Resetting the index
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.24 – 重置索引
- en: In some cases, we may have an index we want to continue to use, but we need
    to align it to certain values. For this purpose, we have the `reindex()` method.
    We provide it with an index to align our data to, and it adjusts the index accordingly.
    Note that this new index isn't necessarily part of the data—we simply have an
    index and want to match the current data up to it.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能有一个想要继续使用的索引，但需要将其对齐到某些特定的值。为此，我们可以使用`reindex()`方法。我们提供一个要对齐数据的索引，它会相应地调整索引。请注意，这个新索引不一定是数据的一部分——我们只是有一个索引，并希望将当前数据与之匹配。
- en: 'As an example, we will turn to the S&P 500 stock data in the `sp500.csv` file.
    It contains the `date` column as the index and parsing the dates:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，我们将使用`sp500.csv`文件中的S&P 500股票数据。它将`date`列作为索引并解析日期：
- en: '[PRE51]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let''s see what our data looks like and mark the day of the week for each row
    in order to understand what the index contains. We can easily isolate the date
    part from an index of type `DatetimeIndex`. When isolating date parts, `pandas`
    will give us the numeric representation of what we are looking for; if we are
    looking for the string version, we should look to see whether there is already
    a method before writing our own conversion function. In this case, it''s `day_name()`:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据的样子，并为每一行标注星期几，以便理解索引包含的内容。我们可以轻松地从类型为`DatetimeIndex`的索引中提取日期部分。在提取日期部分时，`pandas`会给出我们所需的数值表示；如果我们需要字符串版本，我们应该先看看是否已经有现成的方法，而不是自己编写转换函数。在这种情况下，方法是`day_name()`：
- en: '[PRE52]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Tip
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: We can also do this with a series, but first, we need to access the `dt` attribute.
    For example, if we had a `date` column in the `sp` dataframe, we could grab the
    month with `sp.date.dt.month`. You can find the full list of what can be accessed
    at [https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过系列来做这件事，但首先，我们需要访问`dt`属性。例如，如果在`sp`数据框中有一个`date`列，我们可以通过`sp.date.dt.month`提取月份。你可以在[https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties)找到可以访问的完整列表。
- en: 'Since the stock market is closed on the weekend (and holidays), we only have
    data for weekdays:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 由于股市在周末（和假期）关闭，我们只有工作日的数据：
- en: '![Figure 3.25 – S&P 500 OHLC data'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.25 – S&P 500 OHLC 数据'
- en: '](img/Figure_3.25_B16834.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.25_B16834.jpg)'
- en: Figure 3.25 – S&P 500 OHLC data
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.25 – S&P 500 OHLC 数据
- en: 'If we were analyzing the performance of a group of assets in a portfolio that
    included the S&P 500 and something that trades on the weekend, such as bitcoin,
    we would need to have values for every day of the year for the S&P 500\. Otherwise,
    when looking at the daily value of our portfolio we would see huge drops every
    day the market was closed. To illustrate this, let''s read in the bitcoin data
    from the `bitcoin.csv` file and combine the S&P 500 and bitcoin data into a portfolio.
    The bitcoin data also contains OHLC data and volume traded, but it comes with
    a column called `market_cap` that we don''t need, so we have to drop that first:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在分析一个包括标准普尔 500 指数和像比特币这样在周末交易的资产组合，我们需要为标准普尔 500 指数的每一天提供数据。否则，在查看我们投资组合的每日价值时，我们会看到市场休市时每天的巨大跌幅。为了说明这一点，让我们从
    `bitcoin.csv` 文件中读取比特币数据，并将标准普尔 500 指数和比特币的数据结合成一个投资组合。比特币数据还包含 OHLC 数据和交易量，但它有一列叫做
    `market_cap` 的数据，我们不需要，因此我们首先需要删除这列：
- en: '[PRE53]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'To analyze the portfolio, we will need to aggregate the data by day; this is
    a topic for [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082), *Aggregating
    Pandas DataFrames*, so, for now, don''t worry too much about how this aggregation
    is being performed—just know that we are summing up the data by day. For example,
    each day''s closing price will be the sum of the closing price of the S&P 500
    and the closing price of bitcoin:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 要分析投资组合，我们需要按天汇总数据；这是[*第 4 章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)的内容，*汇总
    Pandas DataFrame*，所以现在不用过于担心汇总是如何执行的——只需知道我们按天将数据进行求和。例如，每天的收盘价将是标准普尔 500 指数收盘价和比特币收盘价的总和：
- en: '[PRE54]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, if we examine our portfolio, we will see that we have values for every
    day of the week; so far, so good:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们检查我们的投资组合，我们会看到每周的每一天都有数据；到目前为止，一切正常：
- en: '![ Figure 3.26 – Portfolio of the S&P 500 and bitcoin'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.26 – 标准普尔 500 指数和比特币的投资组合'
- en: '](img/Figure_3.26_B16834.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.26_B16834.jpg)'
- en: Figure 3.26 – Portfolio of the S&P 500 and bitcoin
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.26 – 标准普尔 500 指数和比特币的投资组合
- en: 'However, there is a problem with this approach, which is much easier to see
    with a visualization. Plotting will be covered in depth in [*Chapter 5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106),
    *Visualizing Data with Pandas and Matplotlib*, and [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*, so don''t worry about the
    details for now:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法有一个问题，通过可视化展示会更容易看出。绘图将在[*第 5 章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106)中详细介绍，*使用
    Pandas 和 Matplotlib 可视化数据*，以及[*第 6 章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125)，*使用
    Seaborn 绘图与自定义技术*，因此暂时不用担心细节：
- en: '[PRE55]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Notice how there is a cyclical pattern here? It is dropping every day the market
    is closed because the aggregation only had bitcoin data to sum for those days:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里有一个周期性模式吗？它在每个市场关闭的日子都会下降，因为汇总时只能用比特币数据来填充那些天：
- en: '![Figure 3.27 – Portfolio closing price without accounting for stock market
    closures'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.27 – 未考虑股市休市的投资组合收盘价'
- en: '](img/Figure_3.27_B16834.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.27_B16834.jpg)'
- en: Figure 3.27 – Portfolio closing price without accounting for stock market closures
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.27 – 未考虑股市休市的投资组合收盘价
- en: 'Clearly, this is a problem; an asset''s value doesn''t drop to zero whenever
    the market is closed. If we want `pandas` to fill the missing data in for us,
    we will need to reindex the S&P 500 data with bitcoin''s index using the `reindex()`
    method, passing one of the following strategies to the `method` parameter:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这是一个问题；资产的价值不会因为市场关闭而降至零。如果我们希望 `pandas` 为我们填补缺失的数据，我们需要使用 `reindex()` 方法将标准普尔
    500 指数的数据与比特币的索引重新对齐，并传递以下策略之一给 `method` 参数：
- en: '`''ffill''`: This method brings values forward. In the previous example, this
    fills the days the market was closed with the data for the last time the market
    was open before those days.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''ffill''`：该方法将值向前填充。在前面的示例中，这会将股市休市的那几天填充为股市上次开盘时的数据。'
- en: '`''bfill''`: This method backpropagates the values, which will result in carrying
    future results to past dates, meaning that this isn''t the right option here.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''bfill''`：该方法将值向后填充，这将导致将未来的数据传递到过去的日期，这意味着在这里并不是正确的选择。'
- en: '`''nearest''`: This method fills according to the rows closest to the missing
    ones, which in this case will result in Sundays getting the data for the following
    Mondays, and Saturdays getting the data from the previous Fridays.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''nearest''`：该方法根据最接近缺失行的行来填充，在这个例子中，这将导致周日获取下一个周一的数据，而周六获取前一个周五的数据。'
- en: 'Forward-filling seems to be the best option, but since we aren''t sure, we
    will see how this works on a few rows of the data first:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 前向填充似乎是最好的选择，但由于我们不确定，我们首先会查看数据的几行，看看它的效果：
- en: '[PRE56]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Notice any issues with this? Well, the volume traded (`volume`) column makes
    it seem like the days we used forward-filling for are actually days when the market
    is open:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到这个有问题吗？嗯，`volume`（成交量）列让它看起来像我们用前向填充的那些天，实际上是市场开放的日期：
- en: '![Figure 3.28 – Forward-filling dates with missing values'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.28 – 前向填充缺失值的日期'
- en: '](img/Figure_3.28_B16834.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.28_B16834.jpg)'
- en: Figure 3.28 – Forward-filling dates with missing values
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.28 – 前向填充缺失值的日期
- en: Tip
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The `compare()` method will show us the values that differ across identically-labeled
    dataframes (same index and columns); we can use it to isolate the changes in our
    data when forward-filling here. There is an example in the notebook.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '`compare()` 方法将展示在标记相同的数据框中（相同的索引和列）不同的值；我们可以使用它来在这里进行前向填充时，隔离数据中的变化。在笔记本中有一个示例。'
- en: 'Ideally, we only want to maintain the value of the stock when the stock market
    is closed—the volume traded should be zero. In order to handle the `NaN` values
    in a different manner for each column, we will turn to the `assign()` method.
    To fill any `NaN` values in the `volume` column with `0`, we will use the `fillna()`
    method, which we will see more of in the *Handling duplicate, missing, or invalid
    data* section, later in this chapter. The `fillna()` method also allows us to
    pass in a method instead of a value, so we can forward-fill the `close` column,
    which was the only column that made sense from our previous attempt. Lastly, we
    can use the `np.where()` function for the remaining columns, which allows us to
    build a vectorized `if...else`. It takes the following form:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们只希望在股市关闭时保持股票的值——成交量应该为零。为了以不同的方式处理每列中的 `NaN` 值，我们将使用 `assign()` 方法。为了用
    `0` 填充 `volume`（成交量）列中的任何 `NaN` 值，我们将使用 `fillna()` 方法，这部分我们将在本章稍后的 *处理重复、缺失或无效数据*
    部分详细介绍。`fillna()` 方法还允许我们传入一个方法而不是一个值，这样我们就可以对 `close`（收盘价）列进行前向填充，而这个列是我们之前尝试中唯一合适的列。最后，我们可以对其余的列使用
    `np.where()` 函数，这使得我们可以构建一个向量化的 `if...else`。它的形式如下：
- en: '[PRE57]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '`pandas`, we should avoid writing loops in favor of vectorized operations for
    better performance. NumPy functions are designed to work on arrays, so they are
    perfect candidates for high-performance `pandas` code. This will make it easy
    for us to set any `NaN` values in the `open`, `high`, or `low` columns to the
    value in the `close` column for the same day. Since these come after the `close`
    column gets worked on, we will have the forward-filled value for `close` to use
    for the other columns where necessary:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas` 中，我们应避免编写循环，而应该使用向量化操作以提高性能。NumPy 函数设计用于操作数组，因此它们非常适合用于高性能的 `pandas`
    代码。这将使我们能够轻松地将 `open`（开盘价）、`high`（最高价）或 `low`（最低价）列中的任何 `NaN` 值替换为同一天 `close`（收盘价）列中的值。由于这些操作发生在处理完
    `close` 列之后，我们将可以使用前向填充的 `close` 值来填充其他列中需要的地方：'
- en: '[PRE58]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'On Saturday, January 7th and Sunday, January 8th, we now have volume traded
    at zero. The OHLC prices are all equal to the closing price on Friday, the 6th:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在1月7日星期六和1月8日星期日，我们现在的成交量为零。OHLC（开盘、最高、最低、收盘）价格都等于1月6日星期五的收盘价：
- en: '![Figure 3.29 – Reindexing the S&P 500 data with specific strategies per column'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.29 – 根据每列的特定策略重新索引S&P 500数据'
- en: '](img/Figure_3.29_B16834.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.29_B16834.jpg)'
- en: Figure 3.29 – Reindexing the S&P 500 data with specific strategies per column
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.29 – 根据每列的特定策略重新索引S&P 500数据
- en: Tip
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Here, we use `np.where()` to both introduce a function we will see throughout
    this book and to make it easier to understand what is going on, but note that
    `np.where(x.open.isnull(), x.close, x.open)` can be replaced with the `combine_first()`
    method, which (for this use case) is equivalent to `x.open.combine_first(x.close)`.
    We will use the `combine_first()` method in the *Handling duplicate, missing,
    or invalid data* section, later this chapter.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `np.where()` 来引入一个我们将在本书中反复看到的函数，并且让它更容易理解发生了什么，但请注意，`np.where(x.open.isnull(),
    x.close, x.open)` 可以被 `combine_first()` 方法替代，在这个用例中它等同于 `x.open.combine_first(x.close)`。我们将在本章稍后的
    *处理重复、缺失或无效数据* 部分中使用 `combine_first()` 方法。
- en: 'Now, let''s recreate the portfolio with the reindexed S&P 500 data and use
    a visualization to compare it with the previous attempt (again, don''t worry about
    the plotting code, which will be covered in [*Chapter 5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106),
    *Visualizing Data with Pandas and Matplotlib*, and [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*):'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用重新索引后的标准普尔500数据重建投资组合，并使用可视化将其与之前的尝试进行比较（再次说明，不用担心绘图代码，这部分内容将在[*第5章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106)“*使用Pandas和Matplotlib可视化数据*”以及[*第6章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125)“*使用Seaborn和自定义技术绘图*”中详细讲解）：
- en: '[PRE59]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The orange dashed line is our original attempt at studying the portfolio (without
    reindexing), while the blue solid line is the portfolio we just built with reindexing
    and different filling strategies per column. Keep this strategy in mind for the
    exercises in [*Chapter 7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146), *Financial
    Analysis – Bitcoin and the Stock Market*:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 橙色虚线是我们最初尝试研究投资组合的结果（未重新索引），而蓝色实线是我们刚刚使用重新索引以及为每列采用不同填充策略构建的投资组合。请在[*第7章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146)“*金融分析——比特币与股市*”的练习中牢记此策略：
- en: '![Figure 3.30 – Visualizing the effect of reindexing'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.30 – 可视化重新索引的效果](img/Figure_3.30_B16834.jpg)'
- en: '](img/Figure_3.30_B16834.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.30_B16834.jpg)'
- en: Figure 3.30 – Visualizing the effect of reindexing
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.30 – 可视化重新索引的效果
- en: Tip
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'We can also use the `reindex()` method to reorder the rows. For example, if
    our data is stored in `x`, then `x.reindex([32, 20, 11])` will return a new `DataFrame`
    object of three rows: 32, 20, and 11 (in that order). This can be done along the
    columns with `axis=1` (the default is `axis=0` for rows).'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`reindex()`方法重新排序行。例如，如果我们的数据存储在`x`中，那么`x.reindex([32, 20, 11])`将返回一个新的`DataFrame`对象，包含三行：32、20和11（按此顺序）。也可以通过`axis=1`沿列进行此操作（默认值是`axis=0`，用于行）。
- en: Now, let's turn our attention to reshaping data. Recall that we had to first
    filter the temperature data by the `datatype` column and then sort to find the
    warmest days; reshaping the data will make this easier, and also make it possible
    for us to aggregate and summarize the data.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向数据重构。回想一下，我们首先需要通过`datatype`列筛选温度数据，然后进行排序以找出最热的日子；重构数据将使这一过程更加简便，并使我们能够聚合和总结数据。
- en: Reshaping data
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据重构
- en: Data isn't always given to us in the format that's most convenient for our analysis.
    Therefore, we need to be able to restructure data into both wide and long formats,
    depending on the analysis we want to perform. For many analyses, we will want
    wide format data so that we can look at the summary statistics easily and share
    our results in that format.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并不总是以最方便我们分析的格式提供给我们。因此，我们需要能够根据我们想要进行的分析，将数据重构为宽格式或长格式。对于许多分析，我们希望使用宽格式数据，以便能够轻松查看汇总统计信息，并以这种格式分享我们的结果。
- en: 'However, this isn''t always as black and white as going from long format to
    wide format or vice versa. Consider the following data from the *Exercises* section:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据重构并不总是像从长格式到宽格式或反之那样简单。请考虑以下来自*练习*部分的数据：
- en: '![Figure 3.31 – Data with some long and some wide format columns'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.31 – 一些列为长格式，一些列为宽格式的数据](img/Figure_3.31_B16834.jpg)'
- en: '](img/Figure_3.31_B16834.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.31_B16834.jpg)'
- en: Figure 3.31 – Data with some long and some wide format columns
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.31 – 一些列为长格式，一些列为宽格式的数据
- en: It's possible to have data where some of the columns are in wide format (`describe()`
    on this data aren't helpful unless we first filter on `pandas`—we would need `seaborn`.
    Alternatively, we could restructure the data for that visualization.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 有时数据的某些列可能是宽格式的（对这些数据使用`describe()`并没有帮助，除非我们先使用`pandas`进行筛选——我们需要使用`seaborn`。另外，我们也可以将数据重构为适合该可视化的格式。
- en: 'Now that we understand the motivation for restructuring data, let''s move to
    the next notebook, `4-reshaping_data.ipynb`. We will begin by importing `pandas`
    and reading in the `long_data.csv` file, adding the temperature in Fahrenheit
    column (`temp_F`), and performing some of the data cleaning we just learned about:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了重构数据的动机，接下来让我们转到下一个笔记本`4-reshaping_data.ipynb`。我们将从导入`pandas`并读取`long_data.csv`文件开始，添加华氏温度列（`temp_F`），并进行一些我们刚刚学到的数据清理操作：
- en: '[PRE60]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Our long format data looks like this:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的长格式数据如下所示：
- en: '![Figure 3.32 – Long format temperature data'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.32 – 长格式的温度数据](img/Figure_3.32_B16834.jpg)'
- en: '](img/Figure_3.32_B16834.jpg)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.32_B16834.jpg)'
- en: Figure 3.32 – Long format temperature data
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.32 – 长格式的温度数据
- en: In this section, we will discuss transposing, pivoting, and melting our data.
    Note that after reshaping the data, we will often revisit the data cleaning tasks
    as things may have changed, or we may need to change things we couldn't access
    easily before. For example, we will want to perform some type conversion if the
    values were all turned into strings in long format, but in wide format some columns
    are clearly numeric.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论数据的转置、透视和熔化。请注意，重塑数据后，我们通常会重新访问数据清理任务，因为数据可能已发生变化，或者我们可能需要修改之前无法轻易访问的内容。例如，如果在长格式下，所有值都变成了字符串，我们可能需要进行类型转换，但在宽格式下，一些列显然是数字类型的。
- en: Transposing DataFrames
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转置数据框（DataFrames）
- en: 'While we will be pretty much only working with wide or long formats, `pandas`
    provides ways to restructure our data as we see fit, including taking the **transpose**
    (flipping the rows with the columns), which we may find useful to make better
    use of our display area when we''re printing parts of our dataframe:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们大多数时候只会使用宽格式或长格式，`pandas`提供了重构数据的方式，我们可以根据需要调整，包括执行**转置**（将行和列交换），这在打印数据框部分内容时可能有助于更好地利用显示区域：
- en: '[PRE61]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Notice that the index is now in the columns, and that the column names are
    in the index:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，索引现在已转换为列，而列名已变为索引：
- en: '![Figure 3.33 – Transposed temperature data'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.33 – 转置后的温度数据'
- en: '](img/Figure_3.33_B16834.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.33_B16834.jpg)'
- en: Figure 3.33 – Transposed temperature data
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.33 – 转置后的温度数据
- en: It may not be immediately apparent how useful this can be, but we will see this
    a quite few times throughout this book; for example, to make content easier to
    display in [*Chapter 7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146), *Financial
    Analysis – Bitcoin and the Stock Market*, and to build a particular visualization
    for machine learning in [*Chapter 9*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188),
    *Getting Started with Machine Learning in Python*.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能一开始不太显而易见有多有用，但在本书中，我们会看到多次使用这种方法；例如，在[*第7章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146)中，*财务分析——比特币与股票市场*，以及在[*第9章*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188)中，*Python中的机器学习入门*，我们都会使用它来让内容更容易显示，并为机器学习构建特定的可视化。
- en: Pivoting DataFrames
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据框透视
- en: 'We `pivot()` method performs this restructuring of our `DataFrame` object.
    To pivot, we need to tell `pandas` which column currently holds the values (with
    the `values` argument) and the column that contains what will become the column
    names in wide format (the `columns` argument). Optionally, we can provide a new
    index (the `index` argument). Let''s pivot into wide format, where we have a column
    for each of the temperature measurements in Celsius and use the dates as the index:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`pivot()`方法执行了这种数据框（`DataFrame`）对象的重构。为了进行数据透视，我们需要告诉`pandas`哪个列包含当前的值（通过`values`参数），以及哪个列将成为宽格式下的列名（通过`columns`参数）。我们还可以选择提供新的索引（通过`index`参数）。让我们将数据透视为宽格式，其中每一列都代表一个摄氏温度测量，并使用日期作为索引：
- en: '[PRE62]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'In our starting dataframe, there was a `datatype` column that contained only
    `TMAX`, `TMIN`, or `TOBS` as strings. Now, these are column names because we passed
    in `columns=''datatype''`. By passing in `index=''date''`, the `date` column became
    our index, without needing to run `set_index()`. Finally, the values for each
    combination of `date` and `datatype` are the corresponding temperatures in Celsius
    since we passed in `values=''temp_C''`:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们初始的数据框中，有一个`datatype`列，其中只包含`TMAX`、`TMIN`或`TOBS`作为字符串。现在，这些已经变成了列名，因为我们传入了`columns='datatype'`。通过传入`index='date'`，`date`列成为了我们的索引，而无需运行`set_index()`。最后，对于每个`date`和`datatype`的组合，`temp_C`列中的值是对应的摄氏温度：
- en: '![Figure 3.34 – Pivoting the long format temperature data into wide format'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.34 – 将长格式温度数据透视为宽格式'
- en: '](img/Figure_3.34_B16834.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.34_B16834.jpg)'
- en: Figure 3.34 – Pivoting the long format temperature data into wide format
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.34 – 将长格式温度数据透视为宽格式
- en: 'As we discussed at the beginning of this chapter, with the data in wide format,
    we can easily get meaningful summary statistics with the `describe()` method:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章开头讨论的那样，数据处于宽格式时，我们可以轻松地使用`describe()`方法获取有意义的汇总统计：
- en: '[PRE63]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We can see that we have 31 observations for all three temperature measurements
    and that this month has a wide range of temperatures (highest daily maximum of
    26.7°C and lowest daily minimum of -1.1°C):'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于所有三种温度测量，我们有31个观测值，并且本月的温度变化范围很广（最高日最高气温为26.7°C，最低日最低气温为-1.1°C）：
- en: '![Figure 3.35 – Summary statistics on the pivoted temperature data'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.35 – 透视后的温度数据汇总统计'
- en: '](img/Figure_3.35_B16834.jpg)'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.35_B16834.jpg)'
- en: Figure 3.35 – Summary statistics on the pivoted temperature data
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.35 – 透视温度数据的汇总统计
- en: 'We lost the temperature in Fahrenheit, though. If we want to keep it, we can
    provide multiple columns to `values`:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们失去了华氏温度。如果我们想保留它，可以将多个列提供给`values`：
- en: '[PRE64]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'However, we now get an extra level above the column names. This is called a
    **hierarchical index**:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们现在在列名上方增加了一个额外的层级。这被称为**层级索引**：
- en: '![Figure 3.36 – Pivoting with multiple value columns'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.36 – 使用多个值列进行透视'
- en: '](img/Figure_3.36_B16834.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.36_B16834.jpg)'
- en: Figure 3.36 – Pivoting with multiple value columns
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.36 – 使用多个值列进行透视
- en: 'With this hierarchical index, if we want to select `TMIN` in Fahrenheit, we
    will first need to select `temp_F` and then `TMIN`:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个层级索引，如果我们想选择华氏温度中的`TMIN`，我们首先需要选择`temp_F`，然后选择`TMIN`：
- en: '[PRE65]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Important note
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In cases where we need to perform an aggregation as we pivot (due to duplicate
    values in the index), we can use the `pivot_table()` method, which we will discuss
    in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082), *Aggregating
    Pandas DataFrames*.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要在透视时进行聚合（因为索引中存在重复值）的情况下，我们可以使用`pivot_table()`方法，我们将在[*第4章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)中讨论，*聚合
    Pandas 数据框*。
- en: 'We have been working with a single index throughout this chapter; however,
    we can create an index from any number of columns with `set_index()`. This gives
    us an index of type `MultiIndex`, where the outermost level corresponds to the
    first element in the list provided to `set_index()`:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们一直使用单一索引；然而，我们可以使用`set_index()`从任意数量的列创建索引。这将给我们一个`MultiIndex`类型的索引，其中最外层对应于提供给`set_index()`的列表中的第一个元素：
- en: '[PRE66]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Notice that we now have two levels in the index—`date` is the outermost level
    and `datatype` is the innermost:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，现在我们在索引中有两个层级——`date`是最外层，`datatype`是最内层：
- en: '![Figure 3.37 – Working with a multi-level index'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.37 – 操作多级索引'
- en: '](img/Figure_3.37_B16834.jpg)'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.37_B16834.jpg)'
- en: Figure 3.37 – Working with a multi-level index
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.37 – 操作多级索引
- en: 'The `pivot()` method expects the data to only have one column to set as the
    index; if we have a multi-level index, we should use the `unstack()` method instead.
    We can use `unstack()` on `multi_index_df` and get a similar result to what we
    had previously. Order matters here because, by default, `unstack()` will move
    the innermost level of the index to the columns; in this case, that means we will
    keep the `date` level in the index and move the `datatype` level to the column
    names. To unstack a different level, simply pass in the index of the level to
    unstack, where 0 is the leftmost and -1 is the rightmost, or the name of the level
    (if it has one). Here, we will use the default:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '`pivot()`方法期望数据只有一列可以设置为索引；如果我们有多级索引，我们应该改用`unstack()`方法。我们可以在`multi_index_df`上使用`unstack()`，并得到与之前类似的结果。顺序在这里很重要，因为默认情况下，`unstack()`会将索引的最内层移到列中；在这种情况下，这意味着我们将保留`date`层级在索引中，并将`datatype`层级移到列名中。要解开不同层级，只需传入要解开的层级的索引，0表示最左侧，-1表示最右侧，或者传入该层级的名称（如果有）。这里，我们将使用默认设置：'
- en: '[PRE67]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'With `multi_index_df`, we had `datatype` as the innermost level of the index,
    so, after using `unstack()`, it is along the columns. Note that we once again
    have a hierarchical index in the columns. In [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*, we will discuss a way to squash this back into
    a single level of columns:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在`multi_index_df`中，我们将`datatype`作为索引的最内层，因此，在使用`unstack()`后，它出现在列中。注意，我们再次在列中有了层级索引。在[*第4章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)中，*聚合
    Pandas 数据框*，我们将讨论如何将其压缩回单一层级的列：
- en: '![Figure 3.38 – Unstacking a multi-level index to pivot data'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.38 – 解开多级索引以转换数据'
- en: '](img/Figure_3.38_B16834.jpg)'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.38_B16834.jpg)'
- en: Figure 3.38 – Unstacking a multi-level index to pivot data
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.38 – 解开多级索引以转换数据
- en: 'The `unstack()` method has the added benefit of allowing us to specify how
    to fill in missing values that come into existence upon reshaping the data. To
    do so, we can use the `fill_value` parameter. Imagine a case where we have been
    given the data for `TAVG` for October 1, 2018 only. We could append this to `long_df`
    and set our index to the `date` and `datatype` columns, as we did previously:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '`unstack()`方法的一个额外好处是，它允许我们指定如何填充在数据重塑过程中出现的缺失值。为此，我们可以使用`fill_value`参数。假设我们只得到2018年10月1日的`TAVG`数据。我们可以将其附加到`long_df`，并将索引设置为`date`和`datatype`列，正如我们之前所做的：'
- en: '[PRE68]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We now have four temperature measurements for October 1, 2018, but only three
    for the remaining days:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了 2018 年 10 月 1 日的四个温度测量值，但剩余的日期只有三个：
- en: '![Figure 3.39 – Introducing an additional temperature measurement into the
    data'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.39 – 向数据中引入额外的温度测量'
- en: '](img/Figure_3.39_B16834.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.39_B16834.jpg)'
- en: Figure 3.39 – Introducing an additional temperature measurement into the data
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.39 – 向数据中引入额外的温度测量
- en: 'Using `unstack()`, as we did previously, will result in `NaN` values for most
    of the `TAVG` data:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `unstack()`，正如我们之前所做的，将会导致大部分 `TAVG` 数据变为 `NaN` 值：
- en: '[PRE69]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Take a look at the `TAVG` columns after we unstack:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下我们解压栈后的 `TAVG` 列：
- en: '![Figure 3.40 – Unstacking can lead to null values'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.40 – 解压栈可能会导致空值'
- en: '](img/Figure_3.40_B16834.jpg)'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.40_B16834.jpg)'
- en: Figure 3.40 – Unstacking can lead to null values
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.40 – 解压栈可能会导致空值
- en: 'To address this, we can pass in an appropriate `fill_value`. However, we are
    restricted to passing in a value for this, not a strategy (as we saw when we discussed
    reindexing), so while there is no good value for this case, we can use `-40` to
    illustrate how this works:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这个问题，我们可以传入适当的 `fill_value`。然而，我们仅能为此传入一个值，而非策略（正如我们在讨论重建索引时所看到的），因此，虽然在这个案例中没有合适的值，我们可以使用
    `-40` 来说明这种方法是如何工作的：
- en: '[PRE70]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The `NaN` values have now been replaced with `-40.0`. However, note that now
    both `temp_C` and `temp_F` have the same temperature reading. Actually, this is
    the reason we picked `-40` for `fill_value`; it is the temperature at which both
    Fahrenheit and Celsius are equal, so we won''t confuse people with them both being
    the same number; for example, `0` (since 0°C = 32°F and 0°F = -17.78°C). Since
    this temperature is also much colder than the temperatures measured in New York
    City and is below `TMIN` for all the data we have, it is more likely to be deemed
    a data entry error or a signal that data is missing compared to if we had used
    `0`. Note that, in practice, it is better to be explicit about the missing data
    if we are sharing this with others and leave the `NaN` values:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`NaN` 值已经被 `-40.0` 替换。然而，值得注意的是，现在 `temp_C` 和 `temp_F` 都有相同的温度读数。实际上，这就是我们选择
    `-40` 作为 `fill_value` 的原因；这是摄氏度和华氏度相等的温度，因此我们不会因为它们是相同的数字而混淆人们；比如 `0`（因为 0°C =
    32°F 和 0°F = -17.78°C）。由于这个温度也远低于纽约市的测量温度，并且低于我们所有数据的 `TMIN`，它更可能被认为是数据输入错误或数据缺失的信号，而不是如果我们使用了
    `0` 的情况。请注意，实际上，如果我们与他人共享数据时，最好明确指出缺失的数据，并保留 `NaN` 值：
- en: '![Figure 3.41 – Unstacking with a default value for missing combinations'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.41 – 使用默认值解压栈以填补缺失的组合'
- en: '](img/Figure_3.41_B16834.jpg)'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.41_B16834.jpg)'
- en: Figure 3.41 – Unstacking with a default value for missing combinations
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.41 – 使用默认值解压栈以填补缺失的组合
- en: To summarize, `unstack()` should be our method of choice when we have a multi-level
    index and would like to move one or more of the levels to the columns; however,
    if we are simply using a single index, the `pivot()` method's syntax is likely
    to be easier to specify correctly since it's more apparent which data will end
    up where.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，当我们有一个多级索引并希望将其中一个或多个级别移动到列时，`unstack()` 应该是我们的首选方法；然而，如果我们仅使用单一索引，`pivot()`
    方法的语法可能更容易正确指定，因为哪个数据会出现在何处更加明确。
- en: Melting DataFrames
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 熔化数据框
- en: 'To go from wide format to long format, we need to `wide_data.csv` file:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 要从宽格式转为长格式，我们需要 `wide_data.csv` 文件：
- en: '[PRE71]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Our wide data contains a column for the date and a column for each temperature
    measurement we have been working with:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的宽格式数据包含一个日期列和每个温度测量列：
- en: '![Figure 3.42 – Wide format temperature data'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.42 – 宽格式温度数据'
- en: '](img/Figure_3.42_B16834.jpg)'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.42_B16834.jpg)'
- en: Figure 3.42 – Wide format temperature data
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.42 – 宽格式温度数据
- en: 'We can use the `melt()` method for flexible reshaping—allowing us to turn this
    into long format, similar to what we got from the API. Melting requires that we
    specify the following:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `melt()` 方法进行灵活的重塑—使我们能够将其转为长格式，类似于从 API 获取的数据。重塑需要我们指定以下内容：
- en: Which column(s) uniquely identify a row in the wide format data with the `id_vars`
    argument
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪一列（们）能唯一标识宽格式数据中的一行，使用 `id_vars` 参数
- en: Which column(s) contain(s) the variable(s) with the `value_vars` argument
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪一列（们）包含 `value_vars` 参数的变量（们）
- en: Optionally, we can also specify how to name the column containing the variable
    names in the long format data (`var_name`) and the name for the column containing
    their values (`value_name`). By default, these will be `variable` and `value`,
    respectively.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，我们还可以指定如何命名包含变量名的列（`var_name`）和包含变量值的列（`value_name`）。默认情况下，这些列名将分别为 `variable`
    和 `value`。
- en: 'Now, let''s use the `melt()` method to turn the wide format data into long
    format:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 `melt()` 方法将宽格式数据转换为长格式：
- en: '[PRE72]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The `date` column was the identifier for our rows, so we provided that as `id_vars`.
    We turned the values in the `TMAX`, `TMIN`, and `TOBS` columns into a single column
    with the temperatures (`value_vars`) and used their column names as the values
    for a measurement column (`var_name=''measurement''`). Lastly, we named the values
    column (`value_name=''temp_C''`). We now have just three columns; the date, the
    temperature reading in Celsius (`temp_C`), and a column indicating which temperature
    measurement is in that row''s `temp_C` cell (`measurement`):'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '`date` 列是我们行的标识符，因此我们将其作为 `id_vars` 提供。我们将 `TMAX`、`TMIN` 和 `TOBS` 列中的值转换为一个单独的列，列中包含温度（`value_vars`），并将它们的列名作为测量列的值（`var_name=''measurement''`）。最后，我们将值列命名为（`value_name=''temp_C''`）。现在我们只有三列；日期、摄氏度的温度读数（`temp_C`），以及一个列，指示该行的
    `temp_C` 单元格中的温度测量（`measurement`）：'
- en: '![Figure 3.43 – Melting the wide format temperature data'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.43 – 将宽格式的温度数据转化为长格式](img/Figure_3.43_B16834.jpg)'
- en: '](img/Figure_3.43_B16834.jpg)'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.43 – 将宽格式的温度数据转化为长格式](img/Figure_3.43_B16834.jpg)'
- en: Figure 3.43 – Melting the wide format temperature data
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.43 – 将宽格式的温度数据转化为长格式
- en: 'Just as we had an alternative way of pivoting data with the `unstack()` method,
    we also have another way of melting data with the `stack()` method. This method
    will pivot the columns into the innermost level of the index (resulting in an
    index of type `MultiIndex`), so we need to double-check our index before calling
    it. It also lets us drop row/column combinations that result in no data, if we
    choose. We can do the following to get a similar output to the `melt()` method:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们有另一种方法通过 `unstack()` 方法对数据进行透视一样，我们也有另一种方法通过 `stack()` 方法对数据进行熔化。该方法会将列透视到索引的最内层（导致生成
    `MultiIndex` 类型的索引），因此我们在调用该方法之前需要仔细检查索引。它还允许我们在选择时删除没有数据的行/列组合。我们可以通过以下方式获得与
    `melt()` 方法相似的输出：
- en: '[PRE73]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Notice that the result came back as a `Series` object, so we will need to create
    the `DataFrame` object once more. We can use the `to_frame()` method and pass
    in a name to use for the column once it is a dataframe:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，结果返回的是一个 `Series` 对象，因此我们需要重新创建 `DataFrame` 对象。我们可以使用 `to_frame()` 方法，并传入一个名称，用于在数据框中作为列名：
- en: '[PRE74]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Now, we have a dataframe with a multi-level index, containing `date` and `datatype`,
    with `values` as the only column. Notice, however, that only the `date` portion
    of our index has a name:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个包含多层索引的 DataFrame，其中包含 `date` 和 `datatype`，`values` 作为唯一列。然而，注意到的是，只有索引中的
    `date` 部分有名称：
- en: '![Figure 3.44 – Stacking to melt the temperature data into long format'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.44 – 堆叠数据以将温度数据转化为长格式](img/Figure_3.44_B16834.jpg)'
- en: '](img/Figure_3.44_B16834.jpg)'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.44 – 堆叠数据以将温度数据转化为长格式](img/Figure_3.44_B16834.jpg)'
- en: Figure 3.44 – Stacking to melt the temperature data into long format
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.44 – 堆叠数据以将温度数据转化为长格式
- en: 'Initially, we used `set_index()` to set the index to the `date` column because
    we didn''t want to melt that; this formed the first level of the multi-level index.
    Then, the `stack()` method moved the `TMAX`, `TMIN`, and `TOBS` columns into the
    second level of the index. However, this level was never named, so it shows up
    as `None`, but we know that the level should be called `datatype`:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们使用 `set_index()` 将索引设置为 `date` 列，因为我们不想对其进行熔化；这形成了多层索引的第一层。然后，`stack()`
    方法将 `TMAX`、`TMIN` 和 `TOBS` 列移动到索引的第二层。然而，这一层从未命名，因此显示为 `None`，但我们知道这一层应该命名为 `datatype`：
- en: '[PRE75]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'We can use the `set_names()` method to address this:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `set_names()` 方法来处理这个问题：
- en: '[PRE76]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Now that we understand the basics of data cleaning and reshaping, we will walk
    through an example of how these techniques can be combined when working with data
    that contains various issues.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了数据清洗和重塑的基础知识，接下来我们将通过一个示例来展示如何将这些技巧结合使用，处理包含各种问题的数据。
- en: Handling duplicate, missing, or invalid data
  id: totrans-452
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理重复、缺失或无效数据
- en: 'So far, we have discussed things we could change with the way the data was
    represented with zero ramifications. However, we didn''t discuss a very important
    part of data cleaning: how to deal with data that appears to be duplicated, invalid,
    or missing. This is separated from the rest of the data cleaning discussion because
    it is an example where we will do some initial data cleaning, then reshape our
    data, and finally look to handle these potential issues; it is also a rather hefty
    topic.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的是可以改变数据表示方式而不会带来后果的事情。然而，我们还没有讨论数据清理中一个非常重要的部分：如何处理看起来是重复的、无效的或缺失的数据。这部分与数据清理的其他内容分开讨论，因为它是一个需要我们做一些初步数据清理、重塑数据，并最终处理这些潜在问题的例子；这也是一个相当庞大的话题。
- en: 'We will be working in the `5-handling_data_issues.ipynb` notebook and using
    the `dirty_data.csv` file. Let''s start by importing `pandas` and reading in the
    data:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 `5-handling_data_issues.ipynb` 笔记本中工作，并使用 `dirty_data.csv` 文件。让我们先导入 `pandas`
    并读取数据：
- en: '[PRE77]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The `dirty_data.csv` file contains wide format data from the weather API that
    has been altered to introduce many common data issues that we will encounter in
    the wild. It contains the following fields:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '`dirty_data.csv` 文件包含来自天气 API 的宽格式数据，经过修改以引入许多我们在实际中会遇到的常见数据问题。它包含以下字段：'
- en: '`PRCP`: Precipitation in millimeters'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PRCP`: 降水量（毫米）'
- en: '`SNOW`: Snowfall in millimeters'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SNOW`: 降雪量（毫米）'
- en: '`SNWD`: Snow depth in millimeters'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SNWD`: 雪深（毫米）'
- en: '`TMAX`: Maximum daily temperature in Celsius'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TMAX`: 日最高温度（摄氏度）'
- en: '`TMIN`: Minimum daily temperature in Celsius'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TMIN`: 日最低温度（摄氏度）'
- en: '`TOBS`: Temperature at the time of observation in Celsius'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TOBS`: 观察时的温度（摄氏度）'
- en: '`WESF`: Water equivalent of snow in millimeters'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WESF`: 雪的水当量（毫米）'
- en: This section is divided into two parts. In the first part, we will discuss some
    tactics to uncover issues within a dataset, and in the second part, we will learn
    how to mitigate some of the issues present in this dataset.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 本节分为两部分。在第一部分，我们将讨论一些揭示数据集问题的策略，在第二部分，我们将学习如何减轻数据集中的一些问题。
- en: Finding the problematic data
  id: totrans-465
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找问题数据
- en: 'In [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035), *Working
    with Pandas DataFrames*, we learned the importance of examining our data when
    we get it; it''s not a coincidence that many of the ways to inspect the data will
    help us find these issues. Examining the results of calling `head()` and `tail()`
    on the data is always a good first step:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第二章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035)《与 Pandas DataFrames
    一起工作》中，我们学习了获取数据时检查数据的重要性；并非巧合的是，许多检查数据的方法将帮助我们发现这些问题。调用 `head()` 和 `tail()` 来检查数据的结果始终是一个好的第一步：
- en: '[PRE78]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'In practice, `head()` and `tail()` aren''t as robust as the rest of what we
    will discuss in this section, but we can still get some useful information by
    starting here. Our data is in wide format, and at a quick glance, we can see that
    we have some potential issues. Sometimes, the `station` field is recorded as `?`,
    while other times, it has a station ID. We have values of negative infinity (`-inf`)
    for snow depth (`SNWD`), along with some very hot temperatures for `TMAX`. Lastly,
    we can observe many `NaN` values in several columns, including the `inclement_weather`
    column, which appears to also contain Boolean values:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，`head()` 和 `tail()` 并不像我们将在本节讨论的其他方法那样强大，但我们仍然可以通过从这里开始获取一些有用的信息。我们的数据是宽格式的，快速浏览一下，我们可以看到一些潜在的问题。有时，`station`
    字段记录为 `?`，而有时则记录为站点 ID。我们有些雪深度 (`SNWD`) 的值是负无穷大 (`-inf`)，同时 `TMAX` 也有一些非常高的温度。最后，我们可以看到多个列中有许多
    `NaN` 值，包括 `inclement_weather` 列，这列似乎还包含布尔值：
- en: '![Figure 3.45 – Dirty data'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.45 – 脏数据'
- en: '](img/Figure_3.45_B16834.jpg)'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.45_B16834.jpg)'
- en: Figure 3.45 – Dirty data
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.45 – 脏数据
- en: 'Using `describe()`, we can see if we have any missing data and look at the
    5-number summary to spot potential issues:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `describe()`，我们可以查看是否有缺失数据，并通过五数概括（5-number summary）来发现潜在的问题：
- en: '[PRE79]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The `SNWD` column appears to be useless, and the `TMAX` column seems unreliable.
    For perspective, the temperature of the Sun''s photosphere is around 5,505°C,
    so we certainly wouldn''t expect to observe those air temperatures in New York
    City (or anywhere on Earth, for that matter). This likely means that the `TMAX`
    column was set to a nonsensical, large number when it wasn''t available. The fact
    that it is so large is actually what helps identify it using the summary statistics
    we get from `describe()`. If unknowns were encoded with another value, say 40°C,
    we couldn''t be sure it wasn''t actual data:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '`SNWD`列似乎没什么用，而`TMAX`列看起来也不可靠。为了提供参考，太阳光球的温度大约是5,505°C，因此我们肯定不会在纽约市（或者地球上的任何地方）看到如此高的气温。这很可能意味着当`TMAX`列的数据不可用时，它被设置为一个荒谬的大数字。它之所以能通过`describe()`函数得到的总结统计结果被识别出来，正是因为这个值过大。如果这些未知值是通过其他值来编码的，比如40°C，那么我们就不能确定这是不是实际的数据：'
- en: '![Figure 3.46 – Summary statistics for the dirty data'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.46 – 脏数据的总结统计'
- en: '](img/Figure_3.46_B16834.jpg)'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.46_B16834.jpg)'
- en: Figure 3.46 – Summary statistics for the dirty data
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.46 – 脏数据的总结统计
- en: 'We can use the `info()` method to see if we have any missing values and check
    that our columns have the expected data types. In doing so, we immediately see
    two issues: we have 765 rows, but for five of the columns, we have many fewer
    non-null entries. This output also shows us that the data type of the `inclement_weather`
    column is not Boolean, though we may have thought so from the name. Notice that
    the `?` value that we saw for the `station` column when we used `head()` doesn''t
    show up here—it''s important to inspect our data from many different angles:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`info()`方法查看是否有缺失值，并检查我们的列是否具有预期的数据类型。这样做时，我们立即发现两个问题：我们有765行数据，但其中五列的非空值条目远少于其他列。该输出还告诉我们`inclement_weather`列的数据类型不是布尔值，尽管从名称上我们可能以为是。注意，当我们使用`head()`时，`station`列中看到的`?`值在这里没有出现——从多个角度检查数据非常重要：
- en: '[PRE80]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now, let''s track down those null values. Both `Series` and `DataFrame` objects
    provide two methods to do so: `isnull()` and `isna()`. Note that if we use the
    method on the `DataFrame` object, the result will tell us which rows have all
    null values, which isn''t what we want in this case. Here, we want to examine
    the rows that have null values in the `SNOW`, `SNWD`, `TOBS`, `WESF`, or `inclement_weather`
    columns. This means that we will need to combine checks for each of the columns
    with the `|` (bitwise OR) operator:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们找出这些空值。`Series`和`DataFrame`对象提供了两个方法来实现这一点：`isnull()`和`isna()`。请注意，如果我们在`DataFrame`对象上使用该方法，结果将告诉我们哪些行完全为空值，而这并不是我们想要的。在这种情况下，我们想检查那些在`SNOW`、`SNWD`、`TOBS`、`WESF`或`inclement_weather`列中含有空值的行。这意味着我们需要结合每列的检查，并使用`|`（按位或）运算符：
- en: '[PRE81]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'If we look at the `shape` attribute of our `contain_nulls` dataframe, we will
    see that every single row contains some null data. Looking at the top 10 rows,
    we can see some `NaN` values in each of these rows:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看`contain_nulls`数据框的`shape`属性，我们会看到每一行都包含一些空数据。查看前10行时，我们可以看到每行中都有一些`NaN`值：
- en: '![Figure 3.47 – Rows in the dirty data with nulls'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.47 – 脏数据中的含有空值的行'
- en: '](img/Figure_3.47_B16834.jpg)'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.47_B16834.jpg)'
- en: Figure 3.47 – Rows in the dirty data with nulls
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.47 – 脏数据中的含有空值的行
- en: Tip
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: By default, the `sort_values()` method that we discussed earlier in this chapter
    will put any `NaN` values last. We can change this behavior (to put them first)
    by passing in `na_position='first'`, which can also be helpful when looking for
    patterns in the data when the sort columns have null values.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，我们在本章早些时候讨论的`sort_values()`方法会将任何`NaN`值放在最后。如果我们希望将它们放在最前面，可以通过传递`na_position='first'`来改变这种行为，这在数据排序列含有空值时，查找数据模式也会很有帮助。
- en: 'Note that we can''t check whether the value of the column is equal to `NaN`
    because `NaN` is not equal to anything:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们无法检查列的值是否等于`NaN`，因为`NaN`与任何值都不相等：
- en: '[PRE82]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'We must use the aforementioned options (`isna()`/`isnull()`):'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须使用上述选项（`isna()`/`isnull()`）：
- en: '[PRE83]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Note that `inf` and `-inf` are actually `np.inf` and `-np.inf`. Therefore,
    we can find the number of rows with `inf` or `-inf` values by doing the following:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`inf`和`-inf`实际上是`np.inf`和`-np.inf`。因此，我们可以通过以下方式找到包含`inf`或`-inf`值的行数：
- en: '[PRE84]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'This only tells us about a single column, though, so we could write a function
    that will use a dictionary comprehension to return the number of infinite values
    per column in our dataframe:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅告诉我们关于单一列的信息，因此我们可以编写一个函数，使用字典推导式返回每列中无限值的数量：
- en: '[PRE85]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Using our function, we find that the `SNWD` column is the only column with
    infinite values, but the majority of the values in the column are infinite:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的函数，我们发现`SNWD`列是唯一一个具有无限值的列，但该列中的大多数值都是无限的：
- en: '[PRE86]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Before we can decide on how to handle the infinite values of snow depth, we
    should look at the summary statistics for snowfall (`SNOW`), which forms a big
    part of determining the snow depth (`SNWD`). To do so, we can make a dataframe
    with two series, where one contains the summary statistics for the snowfall column
    when the snow depth is `np.inf`, and the other for when it is `-np.inf`. In addition,
    we will use the `T` attribute to transpose the data for easier viewing:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们决定如何处理雪深的无限值之前，我们应该查看降雪（`SNOW`）的总结统计信息，因为这在确定雪深（`SNWD`）时占据了很大一部分。为此，我们可以创建一个包含两列的数据框，其中一列包含当雪深为`np.inf`时的降雪列的总结统计信息，另一列则包含当雪深为`-np.inf`时的总结统计信息。此外，我们将使用`T`属性对数据进行转置，以便更容易查看：
- en: '[PRE87]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The snow depth was recorded as negative infinity when there was no snowfall;
    however, we can''t be sure this isn''t just a coincidence going forward. If we
    are just going to be working with this fixed date range, we can treat that as
    having a depth of `0` or `NaN` because it didn''t snow. Unfortunately, we can''t
    really make any assumptions with the positive infinity entries. They most certainly
    aren''t that, but we can''t decide what they should be, so it''s probably best
    to leave them alone or not look at this column:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有降雪时，雪深被记录为负无穷；然而，我们不能确定这是不是只是巧合。如果我们只是处理这个固定的日期范围，我们可以将其视为雪深为`0`或`NaN`，因为没有降雪。不幸的是，我们无法对正无穷的条目做出任何假设。它们肯定不是真正的无穷大，但我们无法决定它们应该是什么，因此最好还是将其忽略，或者不看这一列：
- en: '![Figure 3.48 – Summary statistics for snowfall when snow depth is infinite'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 3.48 – 当雪深为无穷大时的降雪总结统计信息'
- en: '](img/Figure_3.48_B16834.jpg)'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.48_B16834.jpg)'
- en: Figure 3.48 – Summary statistics for snowfall when snow depth is infinite
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 3.48 – 当雪深为无穷大时的降雪总结统计信息
- en: 'We are working with a year of data, but somehow, we have 765 rows, so we should
    check why. The only columns we have yet to inspect are the `date` and `station`
    columns. We can use the `describe()` method to see the summary statistics for
    them:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在处理一年的数据，但不知为何我们有765行数据，所以我们应该检查一下原因。我们尚未检查的唯一列是`date`和`station`列。我们可以使用`describe()`方法查看它们的总结统计信息：
- en: '[PRE88]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'In 765 rows of data, the `date` column only has 324 unique values (meaning
    that some dates are missing), with some dates being present as many as eight times
    (`station` column, with the most frequent being `?` when we used `head()` earlier
    (*Figure 3.45*), we know that is the other value; however, we can use `unique()`
    to see all the unique values if we hadn''t. We also know that `?` occurs 367 times
    (765 - 398), without the need to use `value_counts()`:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 在765行数据中，`date`列只有324个唯一值（意味着某些日期缺失），有些日期出现了多达八次（`station`列，最常见的值为`?`，当我们之前使用`head()`查看时（*Figure
    3.45*），我们知道那是另一个值；不过如果我们没有使用`unique()`，我们可以查看所有唯一值）。我们还知道`?`出现了367次（765 - 398），无需使用`value_counts()`：
- en: '![Figure 3.49 – Summary statistics for the non-numeric columns in the dirty
    data'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 3.49 – 脏数据中非数值列的总结统计信息'
- en: '](img/Figure_3.49_B16834.jpg)'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.49_B16834.jpg)'
- en: Figure 3.49 – Summary statistics for the non-numeric columns in the dirty data
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 3.49 – 脏数据中非数值列的总结统计信息
- en: In practice, we may not know why the station is sometimes recorded as `?`—it
    could be intentional to show that they don't have the station, an error in the
    recording software, or an accidental omission that got encoded as `?`. How we
    deal with this would be a judgment call, as we will discuss in the next section.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，我们可能不知道为什么有时站点被记录为`?`——这可能是故意的，表示他们没有该站点，或者是记录软件的错误，或者是由于意外遗漏导致被编码为`?`。我们如何处理这种情况将是一个判断问题，正如我们将在下一部分讨论的那样。
- en: 'Upon seeing that we had 765 rows of data and two distinct values for the station
    ID, we might have assumed that each day had two entries—one per station. However,
    this would only account for 730 rows, and we also now know that we are missing
    some dates. Let''s see whether we can find any duplicate data that could account
    for this. We can use the result of the `duplicated()` method as a Boolean mask
    to find the duplicate rows:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看到有765行数据和两个不同的站点ID值时，我们可能会假设每天有两条记录——每个站点一条。然而，这样只能解释730行数据，而且我们现在也知道一些日期缺失了。我们来看看能否找到任何可能的重复数据来解释这个问题。我们可以使用`duplicated()`方法的结果作为布尔掩码来查找重复的行：
- en: '[PRE89]'
  id: totrans-512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Depending on what we are trying to achieve, we may handle duplicates differently.
    The rows that are returned can be modified with the `keep` argument. By default,
    it is `''first''`, and, for each row that is present more than once, we will get
    only the additional rows (besides the first). However, if we pass in `keep=False`,
    we will get all the rows that are present more than once, not just each additional
    appearance they make:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们试图实现的目标不同，我们可能会以不同的方式处理重复项。返回的行可以用`keep`参数进行修改。默认情况下，它是`'first'`，对于每个出现超过一次的行，我们只会得到额外的行（除了第一个）。但是，如果我们传入`keep=False`，我们将得到所有出现超过一次的行，而不仅仅是每个额外的出现：
- en: '[PRE90]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'There is also a `subset` argument (first positional argument), which allows
    us to focus just on the duplicates of certain columns. Using this, we can see
    that when the `date` and `station` columns are duplicated, so is the rest of the
    data because we get the same result as before. However, we don''t know if this
    is actually a problem:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个`subset`参数（第一个位置参数），它允许我们只专注于某些列的重复项。使用这个参数，我们可以看到当`date`和`station`列重复时，其余数据也重复了，因为我们得到了与之前相同的结果。然而，我们不知道这是否真的是一个问题：
- en: '[PRE91]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Now, let''s examine a few of the duplicated rows:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查一些重复的行：
- en: '[PRE92]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Just looking at the first five rows shows us that some rows are repeated at
    least three times. Remember that the default behavior of `duplicated()` is to
    not show the first occurrence, which means that rows **1** and **2** have another
    matching value in the data (same for rows **5** and **6**):'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 只看前五行就能看到一些行至少重复了三次。请记住，`duplicated()`的默认行为是不显示第一次出现，这意味着第1行和第2行在数据中有另一个匹配值（第5行和第6行也是如此）：
- en: '![Figure 3.50 – Examining the duplicate data'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.50 – 检查重复数据'
- en: '](img/Figure_3.50_B16834.jpg)'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.50_B16834.jpg)'
- en: Figure 3.50 – Examining the duplicate data
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.50 – 检查重复数据
- en: Now that we know how to find problems in our data, let's learn about some ways
    we can try to address them. Note that there is no panacea here, and it will often
    come down to knowing the data we are working with and making judgment calls.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何在我们的数据中找到问题了，让我们学习一些可以尝试解决它们的方法。请注意，这里没有万能药，通常都要了解我们正在处理的数据并做出判断。
- en: Mitigating the issues
  id: totrans-524
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓解问题
- en: We are in an unsatisfactory state with our data, and while we can work to make
    it better, the best plan of action isn't always evident. Perhaps the easiest thing
    we can do when faced with this class of data issues is to remove the duplicate
    rows. However, it is crucial that we evaluate the ramifications such a decision
    may have on our analysis. Even in cases where it appears that the data we are
    working with was collected from a larger dataset that had additional columns,
    thus making all our data distinct, we can't be sure that removing these columns
    is the reason the remaining data was duplicated—we would need to consult the source
    of the data and any available documentation.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据处于一个不理想的状态，尽管我们可以努力改善它，但最佳的行动计划并不总是明显的。也许当面对这类数据问题时，我们可以做的最简单的事情就是删除重复的行。然而，评估这样一个决定可能对我们的分析产生的后果至关重要。即使在看起来我们处理的数据是从一个有额外列的更大数据集中收集而来，从而使我们所有的数据都是不同的，我们也不能确定移除这些列是导致剩余数据重复的原因——我们需要查阅数据的来源和任何可用的文档。
- en: 'Since we know that both stations will be for New York City, we may decide to
    drop the `station` column—they may have just been collecting different data. If
    we then decide to remove duplicate rows using the `date` column and keep the data
    for the station that wasn''t `?`, in the case of duplicates, we will lose all
    data we have for the `WESF` column because the `?` station is the only one reporting
    `WESF` measurements:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道两个站点都属于纽约市，我们可能决定删除`station`列——它们可能只是在收集不同的数据。如果我们然后决定使用`date`列删除重复行，并保留非`?`站点的数据，在重复的情况下，我们将失去所有关于`WESF`列的数据，因为`?`站点是唯一报告`WESF`测量值的站点：
- en: '[PRE93]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'One satisfactory solution in this case may be to carry out the following actions:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个令人满意的解决方案可能是执行以下操作：
- en: 'Perform type conversion on the `date` column:'
  id: totrans-529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`date`列进行类型转换：
- en: '[PRE94]'
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Save the `WESF` column as a series:'
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`WESF`列保存为一个系列：
- en: '[PRE95]'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Sort the dataframe by the `station` column in descending order to put the station
    with no ID (`?`) last:'
  id: totrans-533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按`station`列降序排序数据框，以将没有ID（`?`）的站点放在最后：
- en: '[PRE96]'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Remove rows that are duplicated based on the date, keeping the first occurrences,
    which will be ones where the `station` column has an ID (if that station has measurements).
    Note that `drop_duplicates()` can be done in-place, but if what we are trying
    to do is complicated, it''s best not to start out with the in-place operation:'
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除基于日期的重复行，保留首次出现的行，即那些`station`列有ID的行（如果该站点有测量数据）。需要注意的是，`drop_duplicates()`可以原地操作，但如果我们要做的事情比较复杂，最好不要一开始就使用原地操作：
- en: '[PRE97]'
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Drop the `station` column and set the index to the `date` column (so that it
    matches the `WESF` data):'
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃`station`列，并将索引设置为`date`列（这样它就能与`WESF`数据匹配）：
- en: '[PRE98]'
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Update the `WESF` column using the `combine_first()` method to `?`). Since
    both `df_deduped` and `station_qm_wesf` are using the date as the index, the values
    are properly matched to the appropriate date:'
  id: totrans-539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`combine_first()`方法更新`WESF`列为`?`。由于`df_deduped`和`station_qm_wesf`都使用日期作为索引，因此值将正确匹配到相应的日期：
- en: '[PRE99]'
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'This may sound a little complicated, but that''s largely because we haven''t
    learned about aggregation yet. In [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*, we will look at another way to go about this.
    Let''s take a look at the result using the aforementioned implementation:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来有些复杂，但主要是因为我们还没有学习聚合的内容。在[*第4章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)《聚合Pandas
    DataFrame》中，我们将学习另一种方法来处理这个问题。让我们通过上述实现查看结果：
- en: '[PRE100]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'We are now left with 324 rows—one for each unique date in our data. We were
    able to save the `WESF` column by putting it alongside the data from the other
    station:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们剩下324行——每行代表数据中的一个独特日期。我们通过将`WESF`列与其他站点的数据并排放置，成功保存了`WESF`列：
- en: '![Figure 3.51 – Using data wrangling to keep the information in the WESF column'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.51 – 使用数据整理保持WESF列中的信息'
- en: '](img/Figure_3.51_B16834.jpg)'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.51_B16834.jpg)'
- en: Figure 3.51 – Using data wrangling to keep the information in the WESF column
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.51 – 使用数据整理保持WESF列中的信息
- en: Tip
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: We could have also specified to keep the last entry instead of the first or
    drop all duplicates with the `keep` argument, just like when we checked for duplicates
    with `duplicated()`. Keep this in mind as the `duplicated()` method can be useful
    in giving the results of a dry run on a deduplication task.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以指定保留最后一项数据而不是第一项，或者使用`keep`参数丢弃所有重复项，就像我们使用`duplicated()`检查重复项时一样。记住，`duplicated()`方法可以帮助我们在去重任务中做干运行，以确认最终结果。
- en: Now, let's deal with the null data. We can choose to drop it, replace it with
    some arbitrary value, or impute it using surrounding data. Each of these options
    has its ramifications. If we drop the data, we are going about our analysis with
    only part of the data; if we end up dropping half the rows, this is going to have
    a big impact. When changing the values of the data, we may be affecting the outcome
    of our analysis.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们处理空值数据。我们可以选择丢弃空值、用任意值替换，或使用周围的数据进行填充。每个选项都有其影响。如果我们丢弃数据，我们的分析将只基于部分数据；如果我们丢弃了大部分行，这将对结果产生很大影响。更改数据值时，我们可能会影响分析的结果。
- en: 'To drop all the rows with any null data (this doesn''t have to be true for
    all the columns of the row, so be careful), we use the `dropna()` method; in our
    case, this leaves us with just 4 rows:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除包含任何空值的行（这不必在行的所有列中都为真，因此要小心），我们使用`dropna()`方法；在我们的例子中，这会留下4行数据：
- en: '[PRE101]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'We can change the default behavior to only drop a row if all the columns are
    null with the `how` argument, except this doesn''t get rid of anything:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`how`参数更改默认行为，仅在所有列都为空时才删除行，但这样做并不会删除任何内容：
- en: '[PRE102]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Thankfully, we can also use a subset of columns to determine what to drop.
    Say we wanted to look at snow data; we would most likely want to make sure that
    our data had values for `SNOW`, `SNWD`, and `inclement_weather`. This can be achieved
    with the `subset` argument:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们还可以使用部分列来确定需要丢弃的内容。假设我们想查看雪的数据；我们很可能希望确保数据中包含`SNOW`、`SNWD`和`inclement_weather`的值。这可以通过`subset`参数来实现：
- en: '[PRE103]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Note that this operation can also be performed along the columns, and that
    we can provide a threshold for the number of null values that must be observed
    to drop the data with the `thresh` argument. For example, if we say that at least
    75% of the rows must be null to drop the column, we will drop the `WESF` column:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此操作也可以沿着列进行，并且我们可以通过`thresh`参数设置必须观察到的空值数量阈值，以决定是否丢弃数据。例如，如果我们要求至少75%的行必须为空才能丢弃该列，那么我们将丢弃`WESF`列：
- en: '[PRE104]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: Since we have a lot of null values, we will likely be more interested in keeping
    these values, and perhaps finding a better way to represent them. If we replace
    the null data, we must exercise caution when deciding what to fill in instead;
    filling in all the values we don't have with some other value may yield strange
    results later on, so we must think about how we will use this data first.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有很多空值，我们可能更关心保留这些空值，并可能找到一种更好的方式来表示它们。如果我们替换空数据，必须在决定填充什么内容时谨慎；用其他值填充我们没有的数据可能会导致后续产生奇怪的结果，因此我们必须首先思考如何使用这些数据。
- en: 'To fill in null values with other data, we use the `fillna()` method, which
    gives us the option of specifying a value or a strategy for how to perform the
    filling. We will discuss filling with a single value first. The `WESF` column
    contains mostly null values, but since it is a measurement in milliliters that
    takes on the value of `NaN` when there is no water equivalent of snowfall, we
    can fill in the nulls with zeros. Note that this can be done in-place (again,
    as a general rule of thumb, we should use caution with in-place operations):'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 要用其他数据填充空值，我们使用`fillna()`方法，它允许我们指定一个值或填充策略。我们首先讨论用单一值填充的情况。`WESF`列大多是空值，但由于它是以毫升为单位的测量，当没有降雪的水当量时，它的值为`NaN`，因此我们可以用零来填充这些空值。请注意，这可以就地完成（同样，作为一般规则，我们应该谨慎使用就地操作）：
- en: '[PRE105]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'The `WESF` column no longer contains `NaN` values:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '`WESF`列不再包含`NaN`值：'
- en: '![Figure 3.52 – Filling in null values in the WESF column'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.52 – 填充WESF列中的空值'
- en: '](img/Figure_3.52_B16834.jpg)'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.52_B16834.jpg)'
- en: Figure 3.52 – Filling in null values in the WESF column
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.52 – 填充WESF列中的空值
- en: At this point, we have done everything we can without distorting the data. We
    know that we are missing dates, but if we reindex, we don't know how to fill in
    the resulting `NaN` values. With the weather data, we can't assume that because
    it snowed one day that it will snow the next, or that the temperature will be
    the same. For this reason, note that the following examples are just for illustrative
    purposes only—just because we can do something doesn't mean we should. The right
    solution will most likely depend on the domain and the problem we are looking
    to solve.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经做了所有不扭曲数据的工作。我们知道缺少日期，但如果重新索引，我们不知道如何填充结果中的`NaN`值。对于天气数据，我们不能假设某天下雪了就意味着第二天也会下雪，或者温度会相同。因此，请注意，以下示例仅供说明用途——只是因为我们能做某件事，并不意味着我们应该这样做。正确的解决方案很可能取决于领域和我们希望解决的问题。
- en: 'That being said, let''s try to address some of the remaining issues with the
    temperature data. We know that when `TMAX` is the temperature of the Sun, it must
    be because there was no measured value, so let''s replace it with `NaN`. We will
    also do so for `TMIN`, which currently uses -40°C for its placeholder, despite
    the coldest temperature ever recorded in NYC being -15°F (-26.1°C) on February
    9, 1934 ([https://www.weather.gov/media/okx/Climate/CentralPark/extremes.pdf](https://www.weather.gov/media/okx/Climate/CentralPark/extremes.pdf)):'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们试着解决一些剩余的温度数据问题。我们知道当`TMAX`表示太阳的温度时，必须是因为没有测量值，因此我们将其替换为`NaN`。对于`TMIN`，目前使用-40°C作为占位符，尽管纽约市记录的最低温度是1934年2月9日的-15°F（-26.1°C）([https://www.weather.gov/media/okx/Climate/CentralPark/extremes.pdf](https://www.weather.gov/media/okx/Climate/CentralPark/extremes.pdf))，我们也会进行同样的处理：
- en: '[PRE106]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'We will also make an assumption that the temperature won''t change drastically
    from day to day. Note that this is actually a big assumption, but it will allow
    us to understand how the `fillna()` method works when we provide a strategy through
    the `method` parameter: `''ffill''` to forward-fill or `''bfill''` to back-fill.
    Notice we don''t have the `''nearest''` option, like we did when we were reindexing,
    which would have been the best option; so, to illustrate how this works, let''s
    use forward-filling:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会假设温度在不同日期之间不会剧烈变化。请注意，这其实是一个很大的假设，但它将帮助我们理解当我们通过`method`参数提供策略时，`fillna()`方法如何工作：`'ffill'`向前填充或`'bfill'`向后填充。注意我们没有像在重新索引时那样使用`'nearest'`选项，这本来是最佳选择；因此，为了说明这种方法的工作原理，我们将使用向前填充：
- en: '[PRE107]'
  id: totrans-569
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Take a look at the `TMAX` and `TMIN` columns on January 1st and 4th. Both are
    `NaN` on the 1st because we don''t have data before then to bring forward, but
    the 4th now has the same values as the 3rd:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 查看1月1日和4日的`TMAX`和`TMIN`列。1日的值都是`NaN`，因为我们没有之前的数据可以向前填充，但4日现在的值和3日相同：
- en: '![Figure 3.53 – Forward-filling null values'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.53 – 向前填充空值'
- en: '](img/Figure_3.53_B16834.jpg)'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.53_B16834.jpg)'
- en: Figure 3.53 – Forward-filling null values
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.53 – 向前填充空值
- en: 'If we want to handle the nulls and infinite values in the `SNWD` column, we
    can use the `np.nan_to_num()` function; it turns `NaN` into 0 and `inf`/`-inf`
    into very large positive/negative finite numbers, making it possible for machine
    learning models (discussed in [*Chapter 9*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188)*,
    Getting Started with Machine Learning in Python*) to learn from this data:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想处理`SNWD`列中的空值和无限值，我们可以使用`np.nan_to_num()`函数；它会将`NaN`转换为0，将`inf`/`-inf`转换为非常大的正/负有限数字，从而使得机器学习模型（在[*第9章*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188)*,
    用Python入门机器学习*中讨论）可以从这些数据中学习：
- en: '[PRE108]'
  id: totrans-575
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'This doesn''t make much sense for our use case though. For instances of `-np.inf`,
    we may choose to set `SNWD` to 0 since we saw there was no snowfall on those days.
    However, we don''t know what to do with `np.inf`, and the large positive numbers,
    arguably, make this more confusing to interpret:'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这对于我们的用例来说意义不大。对于`-np.inf`的情况，我们可以选择将`SNWD`设置为0，因为我们已经看到这些天没有降雪。然而，我们不知道该如何处理`np.inf`，而且较大的正数无疑使得数据更难以解读：
- en: '![Figure 3.54 – Replacing infinite values'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.54 – 替换无限值'
- en: '](img/Figure_3.54_B16834.jpg)'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.54_B16834.jpg)'
- en: Figure 3.54 – Replacing infinite values
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.54 – 替换无限值
- en: 'Depending on the data we are working with, we may choose to use the `clip()`
    method as an alternative to the `np.nan_to_num()` function. The `clip()` method
    makes it possible to cap values at a specific minimum and/or maximum threshold.
    Since the snow depth can''t be negative, let''s use `clip()` to enforce a lower
    bound of zero. To show how the upper bound works, we will use the snowfall (`SNOW`)
    as an estimate:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们处理的数据，我们可能选择使用`clip()`方法作为`np.nan_to_num()`函数的替代方法。`clip()`方法使得我们能够将值限制在特定的最小值和/或最大值阈值内。由于雪深不能为负值，我们可以使用`clip()`强制设定下限为零。为了展示上限如何工作，我们将使用降雪量（`SNOW`）作为估算值：
- en: '[PRE109]'
  id: totrans-581
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'The values of `SNWD` for January 1st through 3rd are now `0` instead of `-inf`,
    while the values of `SNWD` for January 4th and 5th went from `inf` to that day''s
    value for `SNOW`:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 1月1日至3日的`SNWD`值现在为`0`，而不是`-inf`，1月4日和5日的`SNWD`值从`inf`变为当日的`SNOW`值：
- en: '![Figure 3.55 – Capping values at thresholds'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.55 – 将值限制在阈值内'
- en: '](img/Figure_3.55_B16834.jpg)'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.55_B16834.jpg)'
- en: Figure 3.55 – Capping values at thresholds
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.55 – 将值限制在阈值内
- en: Our last strategy is imputation. When we replace a missing value with a new
    value derived from the data, using summary statistics or data from other observations,
    it is called **imputation**. For example, we can impute with the mean to replace
    temperature values. Unfortunately, if we are only missing values for the end of
    the month of October, and we replace them with the mean of the values from the
    rest of the month, this is likely to be skewed toward the extreme values, which
    are the warmer temperatures at the beginning of October, in this case. Like everything
    else that was discussed in this section, we must exercise caution and think about
    any potential consequences or side effects of our actions.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一个策略是插补。当我们用从数据中得出的新值替代缺失值时，使用汇总统计数据或其他观测值的数据，这种方法叫做**插补**。例如，我们可以用平均值来替代温度值。不幸的是，如果我们仅在十月月底缺失数据，而我们用剩余月份的平均值替代，这可能会偏向极端值，在这个例子中是十月初的较高温度。就像本节中讨论的其他内容一样，我们必须小心谨慎，考虑我们的行为可能带来的任何后果或副作用。
- en: 'We can combine imputation with the `fillna()` method. As an example, let''s
    fill in the `NaN` values for `TMAX` and `TMIN` with their medians and `TOBS` with
    the average of `TMIN` and `TMAX` (after imputing them):'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将插补与`fillna()`方法结合使用。例如，我们可以用`TMAX`和`TMIN`的中位数来填充`NaN`值，用`TMIN`和`TMAX`的平均值来填充`TOBS`（在插补后）：
- en: '[PRE110]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Notice from the changes to the data for January 1st and 4th that the median
    maximum and minimum temperatures were 14.4°C and 5.6°C, respectively. This means
    that when we impute `TOBS` and also don''t have `TMAX` and `TMIN` in the data,
    we get 10°C:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 从1月1日和4日数据的变化可以看出，最大和最小温度的中位数分别是14.4°C和5.6°C。这意味着，当我们插补`TOBS`且数据中没有`TMAX`和`TMIN`时，我们得到10°C：
- en: '![Figure 3.56 – Imputing missing values with summary statistics'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.56 – 使用汇总统计数据插补缺失值'
- en: '](img/Figure_3.56_B16834.jpg)'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.56_B16834.jpg)'
- en: Figure 3.56 – Imputing missing values with summary statistics
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.56 – 使用汇总统计数据插补缺失值
- en: 'If we want to run the same calculation on all the columns, we should use the
    `apply()` method instead of `assign()`, since it saves us the redundancy of having
    to write the same calculation for each of the columns. For example, let''s fill
    in all the missing values with the rolling 7-day median of their values, setting
    the number of periods required for the calculation to zero to ensure that we don''t
    introduce extra null values. We will cover rolling calculations and `apply()`
    in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082), *Aggregating
    Pandas DataFrames*, so this is just a preview:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想对所有列进行相同的计算，我们应该使用`apply()`方法，而不是`assign()`，因为这样可以避免为每一列重复写相同的计算。例如，让我们用滚动7天中位数填充所有缺失值，并将计算所需的周期数设置为零，以确保我们不会引入额外的空值。我们将在[*第4章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)《聚合Pandas数据框》中详细讲解滚动计算和`apply()`方法，所以这里只是一个预览：
- en: '[PRE111]'
  id: totrans-594
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'It''s kind of hard to tell where our imputed values are here—temperatures can
    fluctuate quite a bit day to day. We know that January 4th had missing data from
    our previous attempt; our imputed temperatures are colder that day than those
    around it with this strategy. In reality, it was slightly warmer that day (around
    -3°C):'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 这里很难看出我们的填补值在哪里——温度会随着每天的变化波动相当大。我们知道1月4日的数据缺失，正如我们之前的尝试所示；使用这种策略时，我们填补的温度比周围的温度要低。在实际情况下，那天的温度稍微温暖一些（大约-3°C）：
- en: '![Figure 3.57 – Imputing missing values with the rolling median'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.57 – 使用滚动中位数填补缺失值'
- en: '](img/Figure_3.57_B16834.jpg)'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.57_B16834.jpg)'
- en: Figure 3.57 – Imputing missing values with the rolling median
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.57 – 使用滚动中位数填补缺失值
- en: Important note
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It's important to exercise caution when imputing. If we pick the wrong strategy
    for the data, we can make a real mess of things.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行填补时，必须小心谨慎。如果我们选择了错误的策略，可能会弄得一团糟。
- en: 'Another way of imputing missing data is to have `pandas` calculate what the
    values should be with the `interpolate()` method. By default, it will perform
    linear interpolation, making the assumption that all the rows are evenly spaced.
    Our data is daily data, although some days are missing, so it is just a matter
    of reindexing first. Let''s combine this with the `apply()` method to interpolate
    all of our columns at once:'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种填补缺失数据的方法是让`pandas`使用`interpolate()`方法计算出这些值。默认情况下，它会执行线性插值，假设所有行是均匀间隔的。我们的数据是每日数据，虽然有些天的数据缺失，因此只需要先重新索引即可。我们可以将其与`apply()`方法结合，来一次性插值所有列：
- en: '[PRE112]'
  id: totrans-602
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Check out January 9th, which we didn''t have previously—the values for `TMAX`,
    `TMIN`, and `TOBS` are the average of the values for the day prior (January 8th)
    and the day after (January 10th):'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 看看1月9日，这是我们之前没有的数据——`TMAX`、`TMIN`和`TOBS`的值是前一天（1月8日）和后一天（1月10日）的平均值：
- en: '![Figure 3.58 – Interpolating missing values'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.58 – 插值缺失值'
- en: '](img/Figure_3.58_B16834.jpg)'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_3.58_B16834.jpg)'
- en: Figure 3.58 – Interpolating missing values
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.58 – 插值缺失值
- en: Different strategies for interpolation can be specified via the `method` argument;
    be sure to check out the `interpolate()` method documentation to view the available
    options.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过`method`参数指定不同的插值策略；务必查看`interpolate()`方法的文档，了解可用的选项。
- en: Summary
  id: totrans-608
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Congratulations on making it through this chapter! Data wrangling may not be
    the most exciting part of the analytics workflow, but we will spend a lot of time
    on it, so it's best to be well versed in what `pandas` has to offer.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了本章！数据整理可能不是分析流程中最令人兴奋的部分，但我们将花费大量时间在这上面，因此最好熟悉`pandas`提供的功能。
- en: In this chapter, we learned more about what data wrangling is (aside from a
    data science buzzword) and got some firsthand experience with cleaning and reshaping
    our data. Utilizing the `requests` library, we once again practiced working with
    APIs to extract data of interest; then, we used `pandas` to begin our introduction
    to data wrangling, which we will continue in the next chapter. Finally, we learned
    how to deal with duplicate, missing, and invalid data points in various ways and
    discussed the ramifications of those decisions.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们进一步了解了数据整理的概念（不仅仅是数据科学的流行术语），并亲身体验了如何清洗和重塑数据。通过使用`requests`库，我们再次练习了如何使用API提取感兴趣的数据；然后，我们使用`pandas`开始了数据整理的介绍，下一章我们将继续深入探讨这一主题。最后，我们学习了如何处理重复、缺失和无效的数据点，并讨论了这些决策的后果。
- en: Building on these concepts, in the next chapter, we will learn how to aggregate
    dataframes and work with time series data. Be sure to complete the end-of-chapter
    exercises before moving on.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些概念，在下一章中，我们将学习如何聚合数据框并处理时间序列数据。在继续之前，请务必完成本章末的练习。
- en: Exercises
  id: totrans-612
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Complete the following exercises using what we have learned so far in this
    book and the data in the `exercises/` directory:'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们到目前为止在本书中学到的知识和`exercises/`目录中的数据完成以下练习：
- en: 'We want to look at data for the `stock_analysis` package we will build in [*Chapter
    7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146), *Financial Analysis – Bitcoin
    and the Stock Market*). Combine them into a single file and store the dataframe
    of the FAANG data as `faang` for the rest of the exercises:'
  id: totrans-614
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望查看我们将在[*第7章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146)中构建的`stock_analysis`包的数据，*金融分析
    - 比特币与股票市场*）。将它们合并成一个文件，并将FAANG数据的数据框存储为`faang`，以便进行接下来的练习：
- en: a) Read in the `aapl.csv`, `amzn.csv`, `fb.csv`, `goog.csv`, and `nflx.csv`
    files.
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 读取`aapl.csv`、`amzn.csv`、`fb.csv`、`goog.csv`和`nflx.csv`文件。
- en: b) Add a column to each dataframe, called `ticker`, indicating the ticker symbol
    it is for (Apple's is AAPL, for example); this is how you look up a stock. In
    this case, the filenames happen to be the ticker symbols.
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 向每个数据框中添加一个名为`ticker`的列，表示它对应的股票代码（例如，苹果的股票代码是AAPL）；这是查询股票的方式。在这个例子中，文件名恰好是股票代码。
- en: c) Append them together into a single dataframe.
  id: totrans-617
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 将它们合并成一个单一的数据框。
- en: d) Save the result in a CSV file called `faang.csv`.
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 将结果保存为名为`faang.csv`的CSV文件。
- en: With `faang`, use type conversion to cast the values of the `date` column into
    datetimes and the `volume` column into integers. Then, sort by `date` and `ticker`.
  id: totrans-619
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`faang`，通过类型转换将`date`列的值转换为日期时间格式，并将`volume`列的值转换为整数。然后，按`date`和`ticker`进行排序。
- en: Find the seven rows in `faang` with the lowest value for `volume`.
  id: totrans-620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出`faang`中`volume`值最小的七行数据。
- en: 'Right now, the data is somewhere between long and wide format. Use `melt()`
    to make it completely long format. Hint: `date` and `ticker` are our ID variables
    (they uniquely identify each row). We need to melt the rest so that we don''t
    have separate columns for `open`, `high`, `low`, `close`, and `volume`.'
  id: totrans-621
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，数据介于长格式和宽格式之间。使用`melt()`将其完全转化为长格式。提示：`date`和`ticker`是我们的ID变量（它们唯一标识每一行）。我们需要将其他列进行转化，以避免有单独的`open`、`high`、`low`、`close`和`volume`列。
- en: Suppose we found out that on July 26, 2018 there was a glitch in how the data
    was recorded. How should we handle this? Note that there is no coding required
    for this exercise.
  id: totrans-622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们发现2018年7月26日数据记录出现了故障。我们应该如何处理这个问题？注意，此练习不需要编写代码。
- en: The `covid19_cases.csv` file.
  id: totrans-623
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`covid19_cases.csv`文件。'
- en: b) Create a `date` column using the data in the `dateRep` column and the `pd.to_datetime()`
    function.
  id: totrans-624
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 使用`dateRep`列中的数据和`pd.to_datetime()`函数创建一个`date`列。
- en: c) Set the `date` column as the index and sort the index.
  id: totrans-625
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 将`date`列设置为索引并对索引进行排序。
- en: 'd) Replace all occurrences of `United_States_of_America` and `United_Kingdom`
    with `USA` and `UK`, respectively. Hint: the `replace()` method can be run on
    the dataframe as a whole.'
  id: totrans-626
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 将所有出现的`United_States_of_America`和`United_Kingdom`分别替换为`USA`和`UK`。提示：`replace()`方法可以在整个数据框上运行。
- en: e) Using the `countriesAndTerritories` column, filter the cleaned COVID-19 cases
    data down to Argentina, Brazil, China, Colombia, India, Italy, Mexico, Peru, Russia,
    Spain, Turkey, the UK, and the USA.
  id: totrans-627
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 使用`countriesAndTerritories`列，将清洗后的COVID-19数据筛选为阿根廷、巴西、中国、哥伦比亚、印度、意大利、墨西哥、秘鲁、俄罗斯、西班牙、土耳其、英国和美国的数据。
- en: f) Pivot the data so that the index contains the dates, the columns contain
    the country names, and the values are the case counts (the `cases` column). Be
    sure to fill in `NaN` values with `0`.
  id: totrans-628
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f) 将数据透视，使索引包含日期，列包含国家名称，值为病例数（即`cases`列）。确保将`NaN`值填充为`0`。
- en: 'In order to determine the case totals per country efficiently, we need the
    aggregation skills we will learn about in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*, so the ECDC data in the `covid19_cases.csv` file
    has been aggregated for us and saved in the `covid19_total_cases.csv` file. It
    contains the total number of cases per country. Use this data to find the 20 countries
    with the largest COVID-19 case totals. Hints: when reading in the CSV file, pass
    in `index_col=''cases''`, and note that it will be helpful to transpose the data
    before isolating the countries.'
  id: totrans-629
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了高效地确定每个国家的病例总数，我们需要在[*第 4 章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)中学习到的聚合技能，即*聚合
    Pandas 数据框*，因此 ECDC 数据在 `covid19_cases.csv` 文件中已为我们进行了聚合，并保存在 `covid19_total_cases.csv`
    文件中。该文件包含每个国家的病例总数。使用这些数据查找 COVID-19 病例总数最多的 20 个国家。提示：在读取 CSV 文件时，传入 `index_col='cases'`，并注意，在提取国家信息之前，先转置数据将会很有帮助。
- en: Further reading
  id: totrans-630
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'Check out the following resources for more information on the topics that were
    covered in this chapter:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下资源，了解更多关于本章涵盖的主题信息：
- en: '*A Quick-Start Tutorial on Relational Database Design*: [https://www.ntu.edu.sg/home/ehchua/programming/sql/relational_database_design.html](https://www.ntu.edu.sg/home/ehchua/programming/sql/relational_database_design.html)'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关系型数据库设计快速入门教程*：[https://www.ntu.edu.sg/home/ehchua/programming/sql/relational_database_design.html](https://www.ntu.edu.sg/home/ehchua/programming/sql/relational_database_design.html)'
- en: '*Binary search*: [https://www.khanacademy.org/computing/computer-science/algorithms/binary-search/a/binary-search](https://www.khanacademy.org/computing/computer-science/algorithms/binary-search/a/binary-search)'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*二分查找*：[https://www.khanacademy.org/computing/computer-science/algorithms/binary-search/a/binary-search](https://www.khanacademy.org/computing/computer-science/algorithms/binary-search/a/binary-search)'
- en: '*How Recursion Works—explained with flowcharts and a video*: [https://www.freecodecamp.org/news/how-recursion-works-explained-with-flowcharts-and-a-video-de61f40cb7f9/](https://www.freecodecamp.org/news/how-recursion-works-explained-with-flowcharts-and-a-video-de61f40cb7f9/)'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*递归如何工作—通过流程图和视频讲解*：[https://www.freecodecamp.org/news/how-recursion-works-explained-with-flowcharts-and-a-video-de61f40cb7f9/](https://www.freecodecamp.org/news/how-recursion-works-explained-with-flowcharts-and-a-video-de61f40cb7f9/)'
- en: '*Python f-strings*: [https://realpython.com/python-f-strings/](https://realpython.com/python-f-strings/)'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Python f-strings*：[https://realpython.com/python-f-strings/](https://realpython.com/python-f-strings/)'
- en: '*Tidy Data (article by Hadley Wickham)*: [https://www.jstatsoft.org/article/view/v059i10](https://www.jstatsoft.org/article/view/v059i10)'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*整洁数据（Hadley Wickham 文章）*：[https://www.jstatsoft.org/article/view/v059i10](https://www.jstatsoft.org/article/view/v059i10)'
- en: '*5 Golden Rules for Great Web API Design*: [https://www.toptal.com/api-developers/5-golden-rules-for-designing-a-great-web-api](https://www.toptal.com/api-developers/5-golden-rules-for-designing-a-great-web-api)'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*伟大的 Web API 设计的 5 个黄金法则*：[https://www.toptal.com/api-developers/5-golden-rules-for-designing-a-great-web-api](https://www.toptal.com/api-developers/5-golden-rules-for-designing-a-great-web-api)'
