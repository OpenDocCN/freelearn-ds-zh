- en: Common Recipes for Implementing a Robust Machine Learning System
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现强大机器学习系统的常见配方
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Spark's basic statistical API to help you build your own algorithms
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的基本统计API，帮助您构建自己的算法
- en: ML pipelines for real-life machine learning applications
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于现实生活机器学习应用的ML管道
- en: Normalizing data with Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行数据归一化
- en: Splitting data for training and testing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拆分用于训练和测试的数据
- en: Common operations with the new Dataset API
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用新的Dataset API的常见操作
- en: Creating and using RDD versus DataFrame versus Dataset from a text file in Spark
    2.0
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中从文本文件创建和使用RDD与DataFrame与Dataset
- en: LabeledPoint data structure for Spark ML
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark ML的LabeledPoint数据结构
- en: Getting access to Spark cluster in Spark 2.0+
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0+中访问Spark集群
- en: Getting access to Spark cluster pre-Spark 2.0
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0之前访问Spark集群
- en: Getting access to SparkContext vis-a-vis SparkSession object in Spark 2.0
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中通过SparkSession对象获取SparkContext的访问权限
- en: New model export and PMML markup in Spark 2.0
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中进行新模型导出和PMML标记
- en: Regression model evaluation using Spark 2.0
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行回归模型评估
- en: Binary classification model evaluation using Spark 2.0
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行二元分类模型评估
- en: Multilabel classification model evaluation using Spark 2.0
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行多标签分类模型评估
- en: Multiclass classification model evaluation using Spark 2.0
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行多类别分类模型评估
- en: Using the Scala Breeze library to do graphics in Spark 2.0
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scala Breeze库在Spark 2.0中进行图形处理
- en: Introduction
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In every line of business ranging from running a small business to creating
    and managing a mission critical application, there are a number of tasks that
    are common and need to be included as a part of almost every workflow that is
    required during the course of executing the functions. This is true even for building
    robust machine learning systems. In Spark machine learning, some of these tasks
    range from splitting the data for model development (train, test, validate) to
    normalizing input feature vector data to creating ML pipelines via the Spark API.
    We provide a set of recipes in this chapter to enable the reader to think about
    what is actually required to implement an end-to-end machine learning system.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在从运行小型企业到创建和管理关键任务应用的各行业中，都有一些常见的任务需要作为几乎每个工作流程的一部分包含在内。即使是构建强大的机器学习系统也是如此。在Spark机器学习中，这些任务包括从拆分数据进行模型开发（训练、测试、验证）到归一化输入特征向量数据以通过Spark
    API创建ML管道。本章提供了一组配方，以使读者思考实际上需要实施端到端机器学习系统的内容。
- en: This chapter attempts to demonstrate a number of common tasks which are present
    in any robust Spark machine learning system implementation. To avoid redundant
    references these common tasks in every recipe covered in this book, we have factored
    out such common tasks as short recipes in this chapter, which can be leveraged
    as needed while reading the other chapters. These recipes can either stand alone
    or be included as pipeline subtasks in a larger system. Please note that these
    common recipes are emphasized in the larger context of machine learning algorithms
    in later chapters, while also including them as independent recipes in this chapter
    for completeness.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章试图演示任何强大的Spark机器学习系统实现中存在的一些常见任务。为了避免在本书的每个配方中重复引用这些常见任务，我们在本章中将这些常见任务作为简短的配方因子出来，这些配方可以在阅读其他章节时根据需要加以利用。这些配方可以作为独立的任务或作为更大系统中的管道子任务。请注意，这些常见配方在后面的章节中强调了机器学习算法的更大背景，同时也在本章中作为独立的配方包括在内，以确保完整性。
- en: Spark's basic statistical API to help you build your own algorithms
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的基本统计API，帮助您构建自己的算法
- en: In this recipe, we cover Spark's multivariate statistical summary (that is,
    *Statistics.colStats*) such as correlation, stratified sampling, hypothesis testing,
    random data generation, kernel density estimators, and much more, which can be
    applied to extremely large datasets while taking advantage of both parallelism
    and resiliency via RDDs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们涵盖了Spark的多元统计摘要（即*Statistics.colStats*），如相关性、分层抽样、假设检验、随机数据生成、核密度估计等，这些可以应用于极大的数据集，同时利用RDD的并行性和弹性。
- en: How to do it...
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以访问Spark会话以及`log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`，以减少Spark的日志输出：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器模式初始化Spark会话，从而使Spark集群的入口点可用：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s retrieve the Spark session underlying the SparkContext to use when generating
    RDDs:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检索SparkContext下面的Spark会话，以在生成RDD时使用：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we create a RDD with the handcrafted data to illustrate usage of summary
    statistics:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用手工制作的数据创建一个RDD，以说明摘要统计的用法：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We use Spark''s statistics objects by invoking the method `colStats()` and
    passing the RDD as an argument:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过调用`colStats()`方法并将RDD作为参数传递来使用Spark的统计对象：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `colStats()` method will return a `MultivariateStatisticalSummary`, which
    contains the computed summary statistics:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`colStats()`方法将返回一个`MultivariateStatisticalSummary`，其中包含计算的摘要统计信息：'
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We close the program by stopping the Spark session:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止Spark会话来关闭程序：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How it works...
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We created an RDD from dense vector data followed by the generation of summary
    statistics on it using the statistics object. Once the `colStats()` method returned,
    we retrieved summary statistics such as the mean, variance, minimum, maximum,
    and so on.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从密集向量数据创建了一个RDD，然后使用统计对象对其进行了摘要统计。一旦`colStats()`方法返回，我们就可以获取摘要统计，如均值、方差、最小值、最大值等。
- en: There's more...
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: It cannot be emphasized enough how efficient the statistical API is on large
    datasets. These APIs will provide you with basic elements to implement any statistical
    learning algorithm from scratch. Based on our research and experience with half
    versus full matrix factorization, we encourage you to first read the source code
    and make sure that there isn't an equivalent functionality already implemented
    in Spark before implementing your own.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 无法强调足够大型数据集上的统计API有多么高效。这些API将为您提供实现任何统计学习算法的基本元素。根据我们对半矩阵分解和全矩阵分解的研究和经验，我们鼓励您首先阅读源代码，并确保在实现自己的功能之前，Spark中没有已经实现的等效功能。
- en: 'While we only demonstrate a basic statistics summary here, Spark comes equipped
    out of the box with:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在这里只展示了基本的统计摘要，但Spark默认配备了以下功能：
- en: 'Correlation: `Statistics.corr(seriesX, seriesY, "type of correlation")`:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关性：`Statistics.corr(seriesX, seriesY, "type of correlation")`：
- en: Pearson (default)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearson（默认）
- en: Spearman
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spearman
- en: 'Stratified sampling - RDD API:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层抽样 - RDD API：
- en: With a replacement RDD
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用替换RDD
- en: Without a replacement - requires an additional pass
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需替换 - 需要额外的传递
- en: 'Hypothesis testing:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设检验：
- en: Vector - `Statistics.chiSqTest( vector )`
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量 - `Statistics.chiSqTest( vector )`
- en: Matrix - `Statistics.chiSqTest( dense matrix )`
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵 - `Statistics.chiSqTest( dense matrix )`
- en: '**Kolmogorov-Smirnov** (**KS**) test for equality - one or two-sided:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kolmogorov-Smirnov**（**KS**）相等性检验 - 单侧或双侧：'
- en: '`Statistics.kolmogorovSmirnovTest(RDD, "norm", 0, 1)`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Statistics.kolmogorovSmirnovTest(RDD, "norm", 0, 1)`'
- en: 'Random data generator - `normalRDD()`:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机数据生成器 - `normalRDD()`：
- en: Normal - can specify a parameter
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正态 - 可以指定参数
- en: Lots of option plus `map()`s to generate any distribution
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多选项加上`map()`来生成任何分布
- en: Kernel density estimator - `KernelDensity().estimate( data )`
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核密度估计器 - `KernelDensity().estimate( data )`
- en: A quick reference to the *Goodness of fit* concept in statistics can be found
    at [https://en.wikipedia.org/wiki/Goodness_of_fit](https://en.wikipedia.org/wiki/Goodness_of_fit)
    link.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，可以在[https://en.wikipedia.org/wiki/Goodness_of_fit](https://en.wikipedia.org/wiki/Goodness_of_fit)链接中找到对*拟合优度*概念的快速参考。
- en: See also
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'Documentation for more multivariate statistical summary:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 更多多元统计摘要的文档：
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary)'
- en: ML pipelines for real-life machine learning applications
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于实际机器学习应用的ML流水线
- en: This is the first of two recipes which cover the ML pipeline in Spark 2.0\.
    For a more advanced treatment of ML pipelines with additional details such as
    API calls and parameter extraction, see later chapters in this book.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是两个覆盖Spark 2.0中ML流水线的食谱中的第一个。有关ML流水线的更高级处理，例如API调用和参数提取等其他详细信息，请参见本书的后续章节。
- en: In this recipe, we attempt to have a single pipeline that can tokenize text,
    use HashingTF (an old trick) to map term frequencies, run a regression to fit
    a model, and then predict which group a new term belongs to (for example, news
    filtering, gesture classification, and so on).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们尝试创建一个单一的流水线，可以对文本进行标记化，使用HashingTF（一种旧技巧）来映射词项频率，运行回归来拟合模型，然后预测一个新词项属于哪个组（例如，新闻过滤，手势分类等）。
- en: How to do it...
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以便Spark会话可以访问集群和`log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器模式初始化Spark会话，从而为Spark集群提供入口点：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s create a training set DataFrame with several random text documents:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个包含几个随机文本文档的训练集DataFrame：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create a tokenizer to parse the text documents into individual terms:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个标记化器来解析文本文档为单独的词项：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create a HashingTF for transforming terms into feature vectors:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个HashingTF来将词项转换为特征向量：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Create a logistic regression class to generate a model to predict which group
    a new text document belongs to:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个逻辑回归类来生成一个模型，以预测一个新的文本文档属于哪个组：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we construct a data pipeline with an array of three stages:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们构建一个包含三个阶段的数据流水线：
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we train the model so we can make predictions later:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们训练模型，以便稍后进行预测：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s create a test dataset to validate our trained model:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个测试数据集来验证我们训练好的模型：
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we transform the test set using the trained model, generating predictions:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用训练好的模型转换测试集，生成预测：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/00084.gif)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00084.gif)'
- en: 'We close the program by stopping the Spark session:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止Spark会话来关闭程序：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How it works...
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this section, we investigated constructing a simple machine learning pipeline
    with Spark. We began with creating a DataFrame comprised of two groups of text
    documents and then proceeded to set up a pipeline.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们研究了如何使用Spark构建一个简单的机器学习流水线。我们首先创建了一个由两组文本文档组成的DataFrame，然后设置了一个流水线。
- en: First, we created a tokenizer to parse text documents into terms followed by
    the creation of the HashingTF to convert the terms into features. Then, we created
    a logistic regression object to predict which group a new text document belongs
    to.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建了一个分词器，将文本文档解析为术语，然后创建了HashingTF来将术语转换为特征。然后，我们创建了一个逻辑回归对象，以预测新文本文档属于哪个组。
- en: Second, we constructed the pipeline by passing an array of arguments to it,
    specifying three stages of execution. You will notice each subsequent stage provides
    the result as a specified column, while using the previous stage's output column
    as the input.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们通过向其传递参数数组来构建管道，指定三个执行阶段。您会注意到每个后续阶段都将结果提供为指定的列，同时使用前一阶段的输出列作为输入。
- en: Finally, we trained the model by invoking `fit()` on the pipeline object and
    defining a set of test data for verification. Next, we transformed the test set
    with the model, producing which of the defined two groups the text documents in
    the test set belong to.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过在管道对象上调用`fit()`并定义一组测试数据来训练模型以进行验证。接下来，我们使用模型转换测试集，确定测试集中的文本文档属于定义的两个组中的哪一个。
- en: There's more...
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The pipeline in Spark ML was inspired by scikit-learn in Python, which is referenced
    here for completeness:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML中的管道受到了Python中scikit-learn的启发，这里提供了参考：
- en: '[http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)'
- en: 'ML pipelines make it easy to combine multiple algorithms used to implement
    a production task in Spark. It would be unusual to see a use case in a real-life
    situation that is made of a single algorithm. Often a number of cooperating ML
    algorithms work together to achieve a complex use case. For example, in LDA-based
    systems (for example, news briefings) or human emotion detection, there are a
    number of steps before and after the core system to be implemented as a single
    pipe to produce any meaningful and production-worthy system. See the following
    link for a real-life use case requiring a pipeline to implement a robust system:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ML管道使得在Spark中组合多个算法以实现生产任务变得容易。在现实情况下，很少会看到由单个算法组成的用例。通常，一些合作的ML算法一起工作以实现复杂的用例。例如，在基于LDA的系统（例如新闻简报）或人类情感检测中，核心系统之前和之后有许多步骤，需要作为单个管道来实现任何有意义且值得投入生产的系统。请参阅以下链接，了解需要使用管道来实现强大系统的真实用例：
- en: '[https://www.thinkmind.org/index.php?view=article&articleid=achi_2013_15_50_20241](https://www.thinkmind.org/index.php?view=article&articleid=achi_2013_15_50_20241)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.thinkmind.org/index.php?view=article&articleid=achi_2013_15_50_20241](https://www.thinkmind.org/index.php?view=article&articleid=achi_2013_15_50_20241)'
- en: See also
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'Documentation for more multivariate statistical summary:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 更多多元统计摘要的文档：
- en: Pipeline docs are available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline)
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline)找到
- en: 'Pipeline model that is useful when we load and save the `.load()`, `.save()
    methods`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在加载和保存`.load()`、`.save()`方法时有用的管道模型：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel)
- en: Pipeline stage information is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道阶段信息可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage)找到
- en: HashingTF, a nice old trick to map a sequence to their term frequency in text
    analytics is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HashingTF，一种将序列映射到文本分析中的词频的老技巧，可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF)找到
- en: Normalizing data with Spark
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark对数据进行归一化
- en: In this recipe, we demonstrate normalizing (scaling) the data prior to importing
    the data into an ML algorithm. There are a good number of ML algorithms such as
    **Support Vector Machine** (**SVM**) that work better with scaled input vectors
    rather than with the raw values.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们演示了在将数据导入ML算法之前对数据进行归一化（缩放）。有很多ML算法，比如**支持向量机**（**SVM**），它们使用缩放后的输入向量而不是原始值效果更好。
- en: How to do it...
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Go to the UCI Machine Learning Repository and download the [http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data) file.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到UCI机器学习库并下载[http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)文件。
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包，以便Spark会话可以访问集群和`log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define a method to parse wine data into a tuple:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个将葡萄酒数据解析为元组的方法：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器模式初始化Spark会话，从而为Spark集群提供入口点：
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Import `spark.implicits`, therefore adding in behavior with only an `import`:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`spark.implicits`，因此只需一个`import`即可添加行为：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s load the wine data into memory, taking only the first four columns and
    converting the latter three into a new feature vector:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将葡萄酒数据加载到内存中，仅取前四列，并将后三列转换为新的特征向量：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we generate a DataFrame with two columns:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们生成一个包含两列的DataFrame：
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, we will print out the DataFrame schema and display data contained within
    the DataFrame:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将打印DataFrame模式并显示其中包含的数据：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](img/00085.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00085.jpeg)'
- en: 'Finally, we generate the scaling model and transform the feature into a common
    range between a negative and positive one displaying the results:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们生成了缩放模型，并将特征转换为一个在负一和正一之间的常见范围，显示结果：
- en: '[PRE31]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](img/00086.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00086.jpeg)'
- en: 'We close the program by stopping the Spark session:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止Spark会话来关闭程序：
- en: '[PRE32]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: How it works...
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this example, we explored feature scaling which is a critical step in most
    machine learning algorithms such as **classifiers**. We started out by loading
    the wine data files, extracted an identifier, and used the next three columns
    to create a feature vector.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们探讨了特征缩放，这是大多数机器学习算法（如**分类器**）中的关键步骤。我们首先加载了葡萄酒数据文件，提取了一个标识符，并使用接下来的三列创建了一个特征向量。
- en: Then, we created a `MinMaxScaler` object, configuring a minimum and maximum
    range to scale our values into. We invoked the scaling model by executing the
    `fit()` method on the scaler class, and then we used the model to scale the values
    in our DataFrame.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建了一个`MinMaxScaler`对象，配置了一个最小和最大范围，以便将我们的值缩放到。我们通过在缩放器类上执行`fit()`方法来调用缩放模型，然后使用模型来缩放DataFrame中的值。
- en: Finally, we displayed the resulting DataFrame and we noticed feature vector
    values ranges are between negative 1 and positive 1.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们显示了生成的DataFrame，并注意到特征向量值的范围在负1和正1之间。
- en: There's more...
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The roots of normalizing and scaling can be better understood by examining
    the concept of **unit vectors** in introductory linear algebra. Please see the
    following links for some common references for unit vectors:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 通过研究**单位向量**的概念，可以更好地理解归一化和缩放的根源。请参阅以下链接以获取一些关于单位向量的常见参考资料：
- en: You can refer to unit vectors at [https://en.wikipedia.org/wiki/Unit_vector](https://en.wikipedia.org/wiki/Unit_vector)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以参考[https://en.wikipedia.org/wiki/Unit_vector](https://en.wikipedia.org/wiki/Unit_vector)中的单位向量
- en: For scalar, you can refer to [https://en.wikipedia.org/wiki/Scalar_(mathematics)](https://en.wikipedia.org/wiki/Scalar_(mathematics))
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于标量，您可以参考[https://en.wikipedia.org/wiki/Scalar_(mathematics)](https://en.wikipedia.org/wiki/Scalar_(mathematics))
- en: In the case of input sensitive algorithms, such as SVM, it is recommended that
    the algorithm be trained on scaled values (for example, range from 0 to 1) of
    the features rather than the absolute values as represented by the original vector.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入敏感的算法，例如SVM，建议对特征的缩放值（例如，范围从0到1）进行训练，而不是原始向量表示的绝对值。
- en: See also
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `MinMaxScaler` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`MinMaxScaler`的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler)找到'
- en: We want to emphasize that `MinMaxScaler` is an extensive API that extends the
    `Estimator` (a concept from the ML pipeline) and when used correctly can lead
    to achieving coding efficiency and high accuracy results.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想强调`MinMaxScaler`是一个广泛的API，它扩展了`Estimator`（来自ML管道的概念），正确使用时可以实现编码效率和高准确性结果。
- en: Splitting data for training and testing
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为训练和测试拆分数据
- en: In this recipe, you will learn to use Spark's API to split your available input
    data into different datasets that can be used for training and validation phases.
    It is common to use an 80/20 split, but other variations of splitting the data
    can be considered as well based on your preference.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，您将学习使用Spark的API将可用的输入数据拆分成可用于训练和验证阶段的不同数据集。通常使用80/20的拆分，但也可以根据您的偏好考虑拆分数据的其他变化。
- en: How to do it...
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Go to the UCI Machine Learning Repository and download the [http://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip](http://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip) file.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到UCI机器学习库并下载[http://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip](http://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip)文件。
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE33]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark会话所需的包，以便访问集群和`log4j.Logger`，以减少Spark产生的输出量：
- en: '[PRE34]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE35]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器模式初始化Spark会话，从而为Spark集群提供入口点：
- en: '[PRE36]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We begin with loading a data file by way of the Spark session''s `csv()` method
    to parse and load data into a dataset:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先通过Spark会话的`csv()`方法加载数据文件，以解析和加载数据到数据集中：
- en: '[PRE37]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now, we count how many items the CSV loader parsed and loaded into memory. We
    will need this value later to reconcile data splitting.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们计算CSV加载程序解析并加载到内存中的项目数。我们稍后需要这个值来调和数据拆分。
- en: '[PRE38]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we utilize the dataset''s `randomSplit` method to split the data into
    two buckets with allocations of 80% and 20% of data each:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们利用数据集的`randomSplit`方法将数据分成两个桶，每个桶分配80%和20%的数据：
- en: '[PRE39]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The `randomSplit` method returns an array with two sets of data, the first
    set with 80% of data being the training set and the next with 20% being the testing
    set:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`randomSplit`方法返回一个包含两组数据的数组，第一组数据占80%的训练集，下一组占20%的测试集：'
- en: '[PRE40]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s generate counts for both training and testing sets:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们为训练集和测试集生成计数：
- en: '[PRE41]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we reconcile the values, and notice that the original row count is `415606`
    and the final summation of the training and testing sets equals `415606`:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们对值进行调和，并注意到原始行数为`415606`，训练集和测试集的最终总和等于`415606`：
- en: '[PRE42]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We close the program by stopping the Spark session:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止Spark会话来关闭程序：
- en: '[PRE43]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: How it works...
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We began by loading the data file `newsCorpora.csv` and then by way of the `randomSplit()`
    method attached to the dataset object, we split the dataset.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载数据文件`newsCorpora.csv`，然后通过附加到数据集对象的`randomSplit()`方法来拆分数据集。
- en: There's more...
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: To validate the result, we must set up a Delphi technique in which the test
    data is absolutely unknown to the model. See Kaggle competitions for details at [https://www.kaggle.com/competitions](https://www.kaggle.com/competitions).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证结果，我们必须建立一个Delphi技术，其中测试数据对模型是完全未知的。有关详细信息，请参阅Kaggle竞赛[https://www.kaggle.com/competitions](https://www.kaggle.com/competitions)。
- en: 'Three types of datasets are needed for a robust ML system:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的ML系统需要三种类型的数据集：
- en: '**Training dataset**: This is used to fit a model to sample'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据集**：用于将模型拟合到样本'
- en: '**Validation dataset**: This is used to estimate the delta or prediction error
    for the fitted model (trained by training set)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证数据集**：用于估计由训练集拟合的模型的增量或预测误差'
- en: '**Test dataset**: This is used to assess the model generalization error once
    a final model is selected'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试数据集**：用于在选择最终模型后评估模型的泛化误差'
- en: See also
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `randomSplit()` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD@randomSplit(weights:Array%5BDouble%5D):Array%5Borg.apache.spark.api.java.JavaRDD%5BT%5D%5D](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD@randomSplit(weights:Array%5BDouble%5D):Array%5Borg.apache.spark.api.java.JavaRDD%5BT%5D%5D).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`randomSplit()`的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD@randomSplit(weights:Array%5BDouble%5D):Array%5Borg.apache.spark.api.java.JavaRDD%5BT%5D%5D](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD@randomSplit(weights:Array%5BDouble%5D):Array%5Borg.apache.spark.api.java.JavaRDD%5BT%5D%5D)找到。'
- en: The `randomSplit()` is a method call within an RDD. While the number of RDD
    method calls can be overwhelming, mastering this Spark concept and API is a must.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`randomSplit()`是RDD内的一个方法调用。虽然RDD方法调用的数量可能令人不知所措，但掌握这个Spark概念和API是必须的。'
- en: 'API signature is as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: API签名如下：
- en: '[PRE44]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Randomly splits this RDD with the provided weights.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提供的权重随机拆分此RDD。
- en: Common operations with the new Dataset API
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新的Dataset API的常见操作
- en: In this recipe, we cover the Dataset API, which is the way forward for data
    wrangling in Spark 2.0 and beyond. In [Chapter 3](part0116.html#3EK180-4d291c9fed174a6992fd24938c2f9c77),
    *Spark's Three Data Musketeers for Machine Learning - Perfect Together* we covered
    three detailed recipes for dataset, and in this chapter we cover some of the common,
    repetitive operations that are required to work with these new API sets. Additionally,
    we demonstrate the query plan generated by the Spark SQL Catalyst optimizer.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们涵盖了Dataset API，这是Spark 2.0及更高版本中数据处理的未来方向。在[第3章](part0116.html#3EK180-4d291c9fed174a6992fd24938c2f9c77)，*Spark的三大数据武士-机器学习的完美组合*中，我们涵盖了三个详细的数据集示例，本章中我们涵盖了一些使用这些新API集的常见重复操作。此外，我们演示了由Spark
    SQL Catalyst优化器生成的查询计划。
- en: How to do it...
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'We will use a JSON data file named `cars.json`, which has been created for
    this example:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用一个名为`cars.json`的JSON数据文件，该文件是为本示例创建的：
- en: '[PRE45]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Set up the package location where the program will reside:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE46]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Import the necessary packages for the Spark session to get access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以便Spark会话可以访问集群和`log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE47]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Define a Scala `case class` to model the data:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala `case class`来建模数据：
- en: '[PRE48]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE49]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建模式初始化Spark会话，从而为Spark集群提供入口点：
- en: '[PRE50]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Import `spark.implicits`, therefore adding in behavior with only an `import`:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`spark.implicits`，因此只需一个`import`就可以添加行为：
- en: '[PRE51]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let''s create a dataset from a Scala list and print out the results:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从Scala列表创建一个数据集并打印结果：
- en: '[PRE52]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![](img/00087.jpeg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00087.jpeg)'
- en: 'Next, we will load a CSV into memory and transform it into a dataset of type
    `Team`:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载一个CSV文件到内存中，并将其转换为`Team`类型的数据集：
- en: '[PRE53]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![](img/00088.jpeg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00088.jpeg)'
- en: 'Now we demonstrate a transversal of the teams dataset by use of the `map` function,
    yielding a new dataset of city names:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们通过使用`map`函数对团队数据集进行横向遍历，生成一个新的城市名称数据集：
- en: '[PRE54]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![](img/00089.jpeg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00089.jpeg)'
- en: 'Display the execution plan for retrieving city names:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示检索城市名称的执行计划：
- en: '[PRE55]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Finally, we save the `teams` dataset to a JSON file:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将`teams`数据集保存为JSON文件：
- en: '[PRE56]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We close the program by stopping the Spark session:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止Spark会话来关闭程序：
- en: '[PRE57]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: How it works...
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we created a dataset from a Scala list and displayed the output to validate
    the creation of the dataset as expected. Second, we loaded a **comma-separated
    value** (**CSV**) file into memory, transforming it into a dataset of type `Team`.
    Third, we executed the `map()` function over our dataset to build a list of team
    city names and printed out the execution plan used to generate the dataset. Finally,
    we persisted the `teams` dataset we previously loaded into a JSON formatted file
    for future use.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从Scala列表创建了一个数据集，并显示输出以验证数据集的创建是否符合预期。其次，我们将一个**逗号分隔值**（**CSV**）文件加载到内存中，将其转换为类型为`Team`的数据集。第三，我们在数据集上执行`map()`函数，构建了一个团队城市名称列表，并打印出用于生成数据集的执行计划。最后，我们将之前加载的`teams`数据集持久化到一个JSON格式的文件中，以备将来使用。
- en: There's more...
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Please take a note of some interesting points on datasets:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意一些关于数据集的有趣点：
- en: Datasets use *lazy* evaluation
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集使用*延迟*评估
- en: Datasets take advantage of the Spark SQL Catalyst optimizer
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集利用了Spark SQL Catalyst优化器
- en: Datasets take advantage of the tungsten off-heap memory management
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集利用了钨的非堆内存管理
- en: There are plenty of systems that will remain pre-Spark 2.0 for the next 2 year
    so you must still learn and master RDDs and DataFrame for practical reasons.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在接下来的两年中，仍将有许多系统保持在Spark 2.0之前，因此出于实际原因，您仍必须学习和掌握RDD和DataFrame。
- en: See also
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for Dataset is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)找到。
- en: Creating and using RDD versus DataFrame versus Dataset from a text file in Spark
    2.0
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0中从文本文件创建和使用RDD与DataFrame与Dataset
- en: 'In this recipe, we explore the subtle differences in creating RDD, DataFrame,
    and Dataset from a text file and their relationship to each other via a short
    sample code:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探讨了从文本文件创建RDD、DataFrame和Dataset以及它们之间关系的微妙差异，通过一个简短的示例代码：
- en: '[PRE58]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Assume `spark` is the session name
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`spark`是会话名称
- en: How to do it...
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE59]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以便Spark会话可以访问集群和`log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE60]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We also define a `case class` to host the data used:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还定义了一个`case class`来存储使用的数据：
- en: '[PRE61]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE62]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器模式初始化Spark会话，从而为Spark集群提供入口点：
- en: '[PRE63]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: In the following block, we let Spark *create a dataset* object from a text file.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们让Spark从文本文件中*创建数据集*对象。
- en: 'The text file contains very simple data (each line contains an ID and name
    separated by a comma):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 文本文件包含非常简单的数据（每行包含逗号分隔的ID和名称）：
- en: '[PRE64]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We read the file in and parse the data in the file. The dataset object is created
    by Spark. We confirm the type in the console and then display the data:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取文件并解析文件中的数据。数据集对象由Spark创建。我们在控制台中确认类型，然后显示数据：
- en: '[PRE65]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'From the console output:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE66]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '![](img/00090.jpeg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00090.jpeg)'
- en: 'Now we create an RDD with the same data file, in a very similar way as the
    preceding step:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用与前一步非常相似的方式创建了一个包含相同数据文件的RDD：
- en: '[PRE67]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We then confirm that it is an RDD and display the data in the console:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们确认它是一个RDD，并在控制台中显示数据：
- en: '[PRE68]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Note that the method is very similar but different.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，方法非常相似但不同。
- en: 'From the console output:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE69]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'DataFrame is another common data structure utilized by Spark communities. We
    show a similar way to create a DataFrame using the similar method based on the
    same data file:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DataFrame是Spark社区常用的另一种数据结构。我们展示了使用相同的方法基于相同的数据文件创建DataFrame的类似方式：
- en: '[PRE70]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: We then confirm that it is a DataFrame.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们确认它是一个DataFrame。
- en: '[PRE71]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Note that `DataFrame = Dataset[Row]`, so the type is Dataset.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`DataFrame = Dataset[Row]`，因此类型是Dataset。
- en: 'From the console output:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE72]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '![](img/00091.jpeg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00091.jpeg)'
- en: 'We close the program by stopping the Spark session:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止Spark会话来关闭程序：
- en: '[PRE73]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: How it works...
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'We create an RDD, DataFrame, and Dataset object using a similar method from
    the same text file and confirm the type using the `getClass` method:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用相同的文本文件使用类似的方法创建了一个RDD、DataFrame和Dataset对象，并使用`getClass`方法确认了类型：
- en: '[PRE74]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Please note that they are very similar and sometimes confusing. Spark 2.0 has
    transformed DataFrame into an alias for `Dataset[Row]`, making it truly a dataset.
    We showed the preceding methods to let the user pick an example to create their
    own datatype flavor.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意它们非常相似，有时会令人困惑。Spark 2.0已经将DataFrame转换为`Dataset[Row]`的别名，使其真正成为一个数据集。我们展示了前面的方法，让用户选择一个示例来创建他们自己的数据类型。
- en: There's more...
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Documentation for datatypes is available at [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型的文档可在[http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)找到。
- en: If you are unsure as to what kind of data structure you have at hand (sometimes
    the difference is not obvious), use the `getClass` method to verify.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不确定手头有什么样的数据结构（有时差异并不明显），请使用`getClass`方法进行验证。
- en: Spark 2.0 has transformed DataFrame into an alias for `Dataset[Row]`. While
    RDD and Dataram remain fully viable for near future, it is best to learn and code
    new projects using the dataset.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0已经将DataFrame转换为`Dataset[Row]`的别名。虽然RDD和Dataram在不久的将来仍然是完全可行的，但最好学习并编写新项目时使用数据集。
- en: See also
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Documentation for RDD and Dataset is available at the following websites:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: RDD和Dataset的文档可在以下网站找到：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)'
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)'
- en: LabeledPoint data structure for Spark ML
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark ML的LabeledPoint数据结构
- en: '**LabeledPoint** is a data structure that has been around since the early days
    for packaging a feature vector along with a label so it can be used in unsupervised
    learning algorithms. We demonstrate a short recipe that uses LabeledPoint, the
    **Seq** data structure, and DataFrame to run a logistic regression for binary
    classification of the data. The emphasis here is on LabeledPoint, and the regression
    algorithms are covered in more depth in [Chapter 5](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part I* and
    [Chapter 6](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part II*.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '**LabeledPoint**是一个自从早期就存在的数据结构，用于打包特征向量和标签，以便在无监督学习算法中使用。我们演示了一个简短的配方，使用LabeledPoint、**Seq**数据结构和DataFrame来运行数据的二元分类的逻辑回归。这里重点是LabeledPoint，回归算法在[第5章](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77)和[第6章](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77)中有更深入的介绍，*Spark
    2.0中的回归和分类的实用机器学习-第I部分*和*Spark 2.0中的回归和分类的实用机器学习-第II部分*。'
- en: How to do it...
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE75]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以使SparkContext能够访问集群：
- en: '[PRE76]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Create Spark''s configuration and SparkContext so we can have access to the
    cluster:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和SparkContext，以便我们可以访问集群：
- en: '[PRE77]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'We create the LabeledPoint, using the `SparseVector` and `DenseVector`. In
    the following code blocks, the first four LabeledPoints are created by the `DenseVector`,
    the last two LabeledPoints are created by the `SparseVector`:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`SparseVector`和`DenseVector`创建LabeledPoint。在以下代码块中，前四个LabeledPoints是由`DenseVector`创建的，最后两个LabeledPoints是由`SparseVector`创建的：
- en: '[PRE78]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: The DataFrame objects are created from the preceding LabeledPoint.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame对象是从前面的LabeledPoint创建的。
- en: We verify the raw data count and process data count.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们验证原始数据计数和处理数据计数。
- en: 'You can operate a `show()` function call to the DataFrame created:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以对创建的DataFrame使用`show()`函数调用：
- en: '[PRE79]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: You will see the following in the console:![](img/00092.jpeg)
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将在控制台中看到以下内容：![](img/00092.jpeg)
- en: 'We create a simple LogisticRegression model from the data structure we just
    created:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从刚刚创建的数据结构中创建一个简单的LogisticRegression模型：
- en: '[PRE80]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'In the console, it will show the following `model` parameters:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台中，它将显示以下`model`参数：
- en: '[PRE81]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'We then close the program by stopping the Spark session:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过停止Spark会话来关闭程序：
- en: '[PRE82]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: How it works...
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We used a LabeledPoint data structure to model features and drive training of
    a logistics regression model. We began by defining a group of LabeledPoints, which
    are used to create a DataFrame for further processing. Then, we created a logistic
    regression object and passed LabeledPoint DataFrame as an argument to it so we
    could train our model. Spark ML APIs are designed to work well with the LabeledPoint
    format and require minimal intervention.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个LabeledPoint数据结构来建模特征并驱动逻辑回归模型的训练。我们首先定义了一组LabeledPoints，用于创建进一步处理的DataFrame。然后，我们创建了一个逻辑回归对象，并将LabeledPoint
    DataFrame作为参数传递给它，以便训练我们的模型。Spark ML API被设计为与LabeledPoint格式良好配合，并且需要最少的干预。
- en: There's more...
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'A LabeledPoint is a popular structure used to package data as a `Vector` +
    a `Label` which can be purposed for supervised machine learning algorithms. A
    typical layout of the LabeledPoint is given here:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: LabeledPoint是一种常用的结构，用于将数据打包为`Vector` + `Label`，可用于监督式机器学习算法。LabeledPoint的典型布局如下：
- en: '[PRE83]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Please note that not only dense but also sparse vectors can be used with LabeledPoint,
    which will make a huge difference in efficiency especially if you have a large
    and sparse dataset housed in the driver during testing and development.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，不仅稠密向量，而且稀疏向量也可以与LabeledPoint一起使用，这将在效率上产生巨大的差异，特别是在测试和开发过程中，如果您有一个大型稀疏数据集存储在驱动程序中。
- en: See also
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: LabeledPoint API documentation is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.LabeledPoint](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.LabeledPoint)
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LabeledPoint API文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.LabeledPoint](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.LabeledPoint)找到
- en: DenseVector API documentation is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.DenseVector](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.DenseVector)
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseVector API文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.DenseVector](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.DenseVector)找到
- en: SparseVector API documentation is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.SparseVector](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.SparseVector)
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparseVector API文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.SparseVector](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.SparseVector)找到
- en: Getting access to Spark cluster in Spark 2.0
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0中访问Spark集群
- en: In this recipe, we demonstrate how to get access to a Spark cluster using a
    single point access named `SparkSession`. Spark 2.0 abstracts multiple contexts
    (such as SQLContext, HiveContext) into a single entry point, `SparkSession`, which
    allows you to get access to all Spark subsystems in a unified way.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们演示了如何使用名为`SparkSession`的单一访问点来访问Spark集群。Spark 2.0将多个上下文（如SQLContext、HiveContext）抽象为一个统一的入口点`SparkSession`，这样可以以统一的方式访问所有Spark子系统。
- en: How to do it...
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE84]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Import the necessary packages for SparkContext to get access to the cluster.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入SparkContext所需的包，以便访问集群。
- en: In Spark 2.x, `SparkSession` is more commonly used instead.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Spark 2.x中，更常用的是`SparkSession`。
- en: '[PRE85]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Create Spark''s configuration and `SparkSession` so we can have access to the
    cluster:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和`SparkSession`，以便我们可以访问集群：
- en: '[PRE86]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The preceding code utilizes the `master()` function to set the cluster type
    to `local`. A comment is provided to show how to run the local cluster running
    on a specific port.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码利用`master()`函数将集群类型设置为`local`。提供了一个注释，显示如何在特定端口上运行本地集群。
- en: The `-D` option value will be overridden by the cluster master parameter set
    in the code if both exist.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两者都存在，`-D`选项值将被代码中设置的集群主参数覆盖。
- en: In a `SparkSession` object, we typically use the `master()` function, while
    pre-Spark 2.0, in the `SparkConf` object, uses the `setMaster()` function.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在`SparkSession`对象中，我们通常使用`master()`函数，而在Spark 2.0之前，在`SparkConf`对象中，使用`setMaster()`函数。
- en: 'The following are the three sample ways to connect to a cluster in different
    modes:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是连接到不同模式的集群的三种示例方式：
- en: 'Running in `local` mode:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`local`模式下运行：
- en: '[PRE87]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Running in cluster mode:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集群模式下运行：
- en: '[PRE88]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Passing the master value in:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 传递主值：
- en: '[PRE89]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '![](img/00093.jpeg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00093.jpeg)'
- en: 'We read a CSV file in and parse the CSV file into Spark using the following
    code:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取一个CSV文件，并使用以下代码将CSV文件解析为Spark：
- en: '[PRE90]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'We show the DataFrame in the console:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中显示DataFrame：
- en: '[PRE91]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: And you will see the following in the console:![](img/00094.jpeg)
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将在控制台中看到以下内容：![](img/00094.jpeg)
- en: 'We then close the program by stopping the Spark session:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过停止Spark会话来关闭程序：
- en: '[PRE92]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: How it works...
  id: totrans-355
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this example, we show how to connect to a Spark cluster using local and remote
    options for an application. First, we create a `SparkSession` object which will
    grant us access to a Spark cluster by specifying whether the cluster is local
    or remote using the `master()` function. You can also specify the master location
    by passing a JVM argument when starting your client program. In addition, you
    can configure an application name and a working data directory. Next, you invoked
    the `getOrCreate()` method to create a new `SparkSession` or hand you a reference
    to an already existing session. Finally, we execute a small sample program to
    prove our `SparkSession` object creation is valid.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们展示了如何使用本地和远程选项连接到Spark集群。首先，我们创建一个`SparkSession`对象，通过使用`master()`函数指定集群是本地还是远程，从而获得对Spark集群的访问。您还可以通过在启动客户端程序时传递JVM参数来指定主位置。此外，您还可以配置应用程序名称和工作数据目录。接下来，您调用了`getOrCreate()`方法来创建一个新的`SparkSession`或将一个引用交给您一个已经存在的会话。最后，我们执行一个小的示例程序来证明我们的`SparkSession`对象创建是有效的。
- en: There's more...
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: A Spark session has numerous parameters and APIs that can be set and exercised,
    but it is worth consulting the Spark documentation since some of the methods/parameters
    are marked with the status Experimental or left blank - for non-experimental statuses
    (15 minimum as of our last examination).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: Spark会话有许多可以设置和使用的参数和API，但值得咨询Spark文档，因为其中一些方法/参数标有实验状态或留空-用于非实验状态（截至我们上次检查为止，至少有15个）。
- en: Another change to be aware of is to use `spark.sql.warehouse.dir` for the location
    of the tables. Spark 2.0 uses `spark.sql.warehouse.dir` to set warehouse locations
    to store tables rather than `hive.metastore.warehouse.dir`. The default value
    for `spark.sql.warehouse.dir` is `System.getProperty("user.dir")`.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意的一点是使用`spark.sql.warehouse.dir`来设置表的位置。Spark 2.0使用`spark.sql.warehouse.dir`来设置存储表的仓库位置，而不是`hive.metastore.warehouse.dir`。`spark.sql.warehouse.dir`的默认值是`System.getProperty("user.dir")`。
- en: Also see `spark-defaults.conf` for more details.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以查看`spark-defaults.conf`以获取更多详细信息。
- en: 'Also noteworthy are the following:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是：
- en: 'Some of our favorite and interesting APIs from the Spark 2.0 documentation:'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从Spark 2.0文档中选择了一些我们喜欢和有趣的API：
- en: '[PRE93]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The version of Spark on which this application is running:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 此应用程序运行的Spark版本：
- en: 'Def **sql**(sqlText: String): [DataFrame](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/package.html#DataFrame=org.apache.spark.sql.Dataset%5Borg.apache.spark.sql.Row%5D)'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Def **sql**(sqlText: String): [DataFrame](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/package.html#DataFrame=org.apache.spark.sql.Dataset%5Borg.apache.spark.sql.Row%5D)'
- en: Executes a SQL query using Spark, returning the result as a [DataFrame](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/package.html#DataFrame=org.apache.spark.sql.Dataset%5Borg.apache.spark.sql.Row%5D)
    - **Preferred Spark 2.0**
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark执行SQL查询，将结果作为[DataFrame](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/package.html#DataFrame=org.apache.spark.sql.Dataset%5Borg.apache.spark.sql.Row%5D)返回-
    **首选Spark 2.0**
- en: 'Val **sqlContext**: [SQLContext](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/SQLContext.html)'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Val **sqlContext**：[SQLContext](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/SQLContext.html)
- en: A wrapped version of this session in the form of a [SQLContext](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/SQLContext.html),
    for backward compatibility.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '[SQLContext](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/SQLContext.html)的包装版本，用于向后兼容。'
- en: 'lazy val **conf**: [RuntimeConfig](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/RuntimeConfig.html)'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 懒惰的val **conf**：[RuntimeConfig](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/RuntimeConfig.html)
- en: Runtime configuration interface for Spark.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的运行时配置界面。
- en: 'lazy val **catalog**: [Catalog](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/catalog/Catalog.html)'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 懒惰的val **catalog**：[Catalog](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/catalog/Catalog.html)
- en: Interface through which the user may create, drop, alter, or query underlying
    databases, tables, functions, and so on.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以通过它创建、删除、更改或查询底层数据库、表、函数等的接口。
- en: '**Def newSession(): SparkSession**'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Def newSession(): SparkSession**'
- en: Starts a new session with isolated SQL configurations and temporary tables;
    registered functions are isolated, but share the underlying [SparkContext](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/SparkContext.html)
    and cached data.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 使用隔离的SQL配置和临时表启动一个新会话；注册的函数是隔离的，但共享底层的[SparkContext](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/SparkContext.html)和缓存数据。
- en: 'Def **udf**: [UDFRegistration](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/UDFRegistration.html)'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Def **udf**: [UDFRegistration](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/UDFRegistration.html)'
- en: A collection of methods for registering user-defined functions (UDF).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 一组用于注册用户定义函数（UDF）的方法。
- en: We can create both DataFrame and Dataset directly via the Spark session. It
    works, but is marked as experimental in Spark 2.0.0.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过Spark会话直接创建DataFrame和Dataset。这是有效的，但在Spark 2.0.0中被标记为实验性的。
- en: If you are going to do any SQL related work, SparkSession is now the entry point
    to Spark SQL. SparkSession is the first object that you have to create in order
    to create Spark SQL applications.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您要进行任何与SQL相关的工作，现在SparkSession是访问Spark SQL的入口点。SparkSession是您必须创建的第一个对象，以创建Spark
    SQL应用程序。
- en: See also
  id: totrans-379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `SparkSession` API documents is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession` API文档的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession)上找到。'
- en: Getting access to Spark cluster pre-Spark 2.0
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0之前获取对Spark集群的访问
- en: This is a *pre-Spark 2.0 recipe*, but it will be helpful for developers who
    want to quickly compare and contrast the cluster access for porting pre-Spark
    2.0 programs to Spark 2.0's new paradigm.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个*Spark 2.0之前的配方*，但对于想要快速比较和对比将Spark 2.0之前的程序移植到Spark 2.0的新范式的开发人员来说是有帮助的。
- en: How to do it...
  id: totrans-383
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE94]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入SparkContext所需的包以访问集群：
- en: '[PRE95]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Create Spark''s configuration and SparkContext so we can have access to the
    cluster:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和SparkContext，以便我们可以访问集群：
- en: '[PRE96]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: The preceding code utilizes the `setMaster()` function to set the cluster master
    location. As you can see, we are running the code in `local` mode.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码利用`setMaster()`函数来设置集群主位置。正如您所看到的，我们是在`local`模式下运行代码。
- en: The `-D` option value will be overridden by the cluster master parameter set
    in the code if both exist).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两者都存在，那么`-D`选项值将被代码中设置的集群主参数覆盖）。
- en: 'The following are the three sample ways to connect to the cluster in different
    modes:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是连接到不同模式下集群的三种示例方式：
- en: 'Running in local mode:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地模式下运行：
- en: '[PRE97]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Running in cluster mode:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集群模式下运行：
- en: '[PRE98]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Passing the master value in:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下方式传递主值：
- en: '[PRE99]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '![](img/00095.jpeg)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00095.jpeg)'
- en: 'We use the preceding SparkContext to read a CSV file in and parse the CSV file
    into Spark using the following code:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用上述SparkContext来读取CSV文件并使用以下代码将CSV文件解析为Spark：
- en: '[PRE100]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'We take the sample result and print them in the console:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获取样本结果并在控制台中打印它们：
- en: '[PRE101]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: And you will see the following in the console:![](img/00096.jpeg)
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后您将在控制台中看到以下内容：![](img/00096.jpeg)
- en: 'We then close the program by stopping the SparkContext:'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过停止SparkContext来关闭程序：
- en: '[PRE102]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: How it works...
  id: totrans-408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this example, we show how to connect to a Spark cluster using the local and
    remote modes prior to Spark 2.0\. First, we create a `SparkConf` object and configure
    all the required parameters. We will specify the master location, application
    name, and working data directory. Next, we create a SparkContext passing the `SparkConf`
    as an argument to access a Spark cluster. Also, you can specify the master location
    my passing a JVM argument when starting your client program. Finally, we execute
    a small sample program to prove our SparkContext is functioning correctly.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们展示了如何在Spark 2.0之前使用本地和远程模式连接到Spark集群。首先，我们创建一个`SparkConf`对象并配置所有必需的参数。我们将指定主位置、应用程序名称和工作数据目录。接下来，我们创建一个SparkContext，传递`SparkConf`作为参数来访问Spark集群。此外，您还可以在启动客户端程序时通过传递JVM参数来指定主位置。最后，我们执行一个小的示例程序来证明我们的SparkContext正常运行。
- en: There's more...
  id: totrans-410
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Prior to Spark 2.0, getting access to a Spark cluster was done via **SparkContext**.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0之前，通过**SparkContext**获取对Spark集群的访问。
- en: The access to the subsystems such as SQL was per-specific names context (for
    example, SQLContext**)**.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 对子系统的访问，如SQL，是特定名称上下文（例如，SQLContext**）。
- en: Spark 2.0 changed how we gain access to a cluster by creating a single unified
    access point (namely, `SparkSession`).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0通过创建一个统一的访问点（即`SparkSession`）来改变我们访问集群的方式。
- en: See also
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for SparkContext is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext).
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: SparkContext的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)上找到。
- en: Getting access to SparkContext vis-a-vis SparkSession object in Spark 2.0
  id: totrans-416
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0中通过SparkSession对象访问SparkContext
- en: In this recipe, we demonstrate how to get hold of SparkContext using a SparkSession
    object in Spark 2.0\. This recipe will demonstrate the creation, usage, and back
    and forth conversion of RDD to Dataset. The reason this is important is that even
    though we prefer Dataset going forward, we must still be able to use and augment
    the legacy (pre-Spark 2.0) code mostly utilizing RDD.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们演示了如何使用SparkSession对象在Spark 2.0中获取SparkContext。这个示例将演示从RDD到数据集的创建、使用和来回转换。这是重要的原因是，即使我们更喜欢数据集，我们仍然必须能够使用和增强大部分使用RDD的遗留（Spark
    2.0之前）代码。
- en: How to do it...
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE103]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark会话所需的必要包，以访问集群和`log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE104]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE105]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建模式初始化Spark会话，从而为Spark集群提供入口点：
- en: '[PRE106]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'We first show how to use `sparkContext` to create RDD. The following code samples
    were very common in Spark 1.x:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先展示了如何使用`sparkContext`创建RDD。以下代码示例在Spark 1.x中非常常见：
- en: '[PRE107]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'We get the `SparkContext` object:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获取`SparkContext`对象：
- en: '[PRE108]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'We first create `rdd1` from the `makeRDD` method and display the RDD in the
    console:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从`makeRDD`方法创建了`rdd1`并在控制台中显示了RDD：
- en: '[PRE109]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: We then use the `parallelize` method to generate `rdd2`, and display the data
    in the RDD in the console.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用`parallelize`方法生成了`rdd2`，并在控制台中显示了RDD中的数据。
- en: 'From the console output:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE110]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Now we show the way to use the `session` object to create the dataset:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们展示了使用`session`对象创建数据集的方法：
- en: '[PRE111]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: We generated `dataset1` and `dataset2` using different methods.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用不同的方法生成了`dataset1`和`dataset2`。
- en: 'From the console output:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: 'For dataset1:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 对于dataset1：
- en: '![](img/00097.jpeg)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00097.jpeg)'
- en: 'For dataset2:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 对于dataset2：
- en: '![](img/00098.jpeg)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00098.jpeg)'
- en: 'We show the way to retrieve the underlying RDD from the dataset:'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们展示了如何从数据集中检索基础RDD：
- en: '[PRE112]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'From the console output:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE113]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'The following block shows a way to convert RDD to Dataset object:'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块显示了将RDD转换为数据集对象的方法：
- en: '[PRE114]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'From the console output:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '![](img/00099.jpeg)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00099.jpeg)'
- en: 'We close the program by stopping the Spark session:'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止Spark会话来关闭程序：
- en: '[PRE115]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: How it works...
  id: totrans-455
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We created RDD using the SparkContext; this was widely used in Spark 1.x. We
    also demonstrated a way to create Dataset in Spark 2.0 using the Session object.
    The conversion back and forth is necessary to deal with pre-Spark 2.0 code in
    production today.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用SparkContext创建了RDD；这在Spark 1.x中被广泛使用。我们还演示了在Spark 2.0中使用Session对象创建数据集的方法。在生产中，来回转换是必要的，以处理Spark
    2.0之前的代码。
- en: The technical message from this recipe is that while DataSet is the preferred
    method of data wrangling going forward, we can always use the API to go back and
    forth to RDD and vice versa.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的技术信息是，虽然DataSet是未来数据处理的首选方法，但我们始终可以使用API来回转换为RDD，反之亦然。
- en: There's more...
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: More about the datatypes can be found at [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html).
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 有关数据类型的更多信息，请访问[http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)。
- en: See also
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Documentation for SparkContext and SparkSession is available at the following
    websites:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: SparkContext和SparkSession的文档可在以下网站找到：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)'
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession)'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession)'
- en: New model export and PMML markup in Spark 2.0
  id: totrans-464
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0中的新模型导出和PMML标记
- en: In this recipe, we explore the model export facility available in Spark 2.0
    to use **Predictive Model Markup Language** (**PMML**). This standard XML-based
    language allows you to export and run your models on other systems (some limitations
    apply). You can explore the *There's more...* section for more information.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探索了Spark 2.0中可用的模型导出功能，以使用**预测模型标记语言**（**PMML**）。这种标准的基于XML的语言允许您在其他系统上导出和运行您的模型（某些限制适用）。您可以在*还有更多...*部分中获取更多信息。
- en: How to do it...
  id: totrans-466
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE116]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入SparkContext所需的必要包以访问集群：
- en: '[PRE117]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Create Spark''s configuration and SparkContext:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和SparkContext：
- en: '[PRE118]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'We read the data from a text file; the data file contains a sample dataset
    for a KMeans model:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从文本文件中读取数据；数据文件包含KMeans模型的样本数据集：
- en: '[PRE119]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'We set up the parameters for the KMeans model, and train the model using the
    preceding datasets and parameters:'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了KMeans模型的参数，并使用前述数据集和参数训练模型：
- en: '[PRE120]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: We have effectively created a simple KMeans model (by setting the number of
    clusters to 2) from the data structure we just created.
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经有效地从我们刚刚创建的数据结构中创建了一个简单的KMeans模型（通过将聚类数设置为2）。
- en: '[PRE121]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'In the console, it will show the following model:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台中，它将显示以下模型：
- en: '[PRE122]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'We then export the PMML to an XML file in the data directory:'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将PMML导出到数据目录中的XML文件中：
- en: '[PRE123]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '![](img/00100.jpeg)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.jpeg)'
- en: 'We then close the program by stopping the Spark session:'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过停止Spark会话来关闭程序：
- en: '[PRE124]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: How it works...
  id: totrans-487
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: After you spend time to train a model, the next step will be to persist the
    model for future use. In this recipe, we began by training a KMeans model to generate
    model info for persistence in later steps. Once we have the trained model, we
    invoke the `toPMML()` method on the model converting it into PMML for storage.
    The invocation of the method generates an XML document, then the XML document
    text can easily be persisted to a file.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型后，下一步将是保存模型以供将来使用。在这个示例中，我们首先训练了一个KMeans模型，以生成后续步骤中的持久性模型信息。一旦我们有了训练好的模型，我们在模型上调用`toPMML()`方法，将其转换为PMML进行存储。该方法的调用会生成一个XML文档，然后XML文档文本可以轻松地持久化到文件中。
- en: There's more...
  id: totrans-489
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'PMML is a standard developed by the **Data Mining Group** (**DMG**). The standard
    enables inter-platform interoperability by letting you build on one system and
    then deploy to another system in production. The PMML standard has gained momentum
    and has been adopted by most vendors. At its core, the standard is based on an
    XML document with the following:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: PMML是由数据挖掘组（DMG）制定的标准。该标准通过允许您在一个系统上构建，然后在生产中部署到另一个系统，实现了跨平台的互操作性。PMML标准已经获得了动力，并已被大多数供应商采用。在其核心，该标准基于一个XML文档，其中包括以下内容：
- en: Header with general information
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有一般信息的标题
- en: Dictionary describing field level definitions used by the third component (the
    model)
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述第三个组件（模型）使用的字段级定义的字典
- en: Model structure and parameters
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型结构和参数
- en: 'As of this writing, the Spark 2.0 Machine Library support for PMML exporting
    is currently limited to:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，Spark 2.0机器库对PMML导出的支持目前仅限于：
- en: Linear Regression
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Logistic Regression
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Ridge Regression
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 岭回归
- en: Lasso
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 套索
- en: SVM
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM
- en: KMeans
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KMeans
- en: 'You can export the model to the following file types in Spark:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Spark中将模型导出为以下文件类型：
- en: 'Local filesystem:'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地文件系统：
- en: '[PRE125]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'Distributed filesystem:'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式文件系统：
- en: '[PRE126]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'Output stream--acting as a pipe:'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出流--充当管道：
- en: '[PRE127]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: See also
  id: totrans-508
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `PMMLExportable` API documents at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.pmml.PMMLExportable](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.pmml.PMMLExportable).
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '`PMMLExportable` API文档的文档在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.pmml.PMMLExportable](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.pmml.PMMLExportable)。'
- en: Regression model evaluation using Spark 2.0
  id: totrans-510
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行回归模型评估
- en: In this recipe, we explore how to evaluate a regression model (a regression
    decision tree in this example). Spark provides the **RegressionMetrics** facility
    which has basic statistical facilities such as **Mean Squared Error** (**MSE**),
    R-Squared, and so on, right out of the box.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探讨了如何评估回归模型（在本例中是回归决策树）。Spark提供了**RegressionMetrics**工具，它具有基本的统计功能，如**均方误差**（MSE），R-Squared等。
- en: The objective in this recipe is to understand the evaluation metrics provided
    by Spark out of the box. It is best to concentrate on step 8 since we cover regression
    in more detail in [Chapter 5](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part I* and
    [Chapter 6](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part II* and
    throughout the book.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的目标是了解Spark开箱即用提供的评估指标。最好集中在第8步，因为我们在[第5章](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77)中更详细地介绍了回归，*Spark
    2.0中的实用机器学习-回归和分类第一部分*和[第6章](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77)，*Spark
    2.0中的实用机器学习-回归和分类第二部分*以及整本书中。
- en: How to do it...
  id: totrans-513
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE128]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以便SparkContext可以访问集群：
- en: '[PRE129]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Create Spark''s configuration and SparkContext:'
  id: totrans-519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和SparkContext：
- en: '[PRE130]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: We utilize the Wisconsin breast cancer dataset as an example dataset for the
    regression model.
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们利用威斯康星州乳腺癌数据集作为回归模型的示例数据集。
- en: The **Wisconsin breast cancer** dataset was obtained from the University of
    Wisconsin Hospital from Dr. William H Wolberg. The dataset was gained periodically
    as Dr.Wolberg reported his clinical cases.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 威斯康星州乳腺癌数据集是从威斯康星大学医院的William H Wolberg博士那里获得的。数据集是定期获得的，因为Wolberg博士报告了他的临床病例。
- en: More details on the dataset can be found in [Chapter 9](part0437.html#D0O5Q0-4d291c9fed174a6992fd24938c2f9c77), *Optimization
    - Going Down the Hill with Gradient Descent*.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 有关数据集的更多详细信息可以在[第9章](part0437.html#D0O5Q0-4d291c9fed174a6992fd24938c2f9c77)中找到，*优化-使用梯度下降进行下坡*。
- en: '[PRE131]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: We load the data into Spark and filter the missing values in the data.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据加载到Spark中，并过滤数据中的缺失值。
- en: 'We split the dataset in the ratio of 70:30 to create two datasets, one used
    for training the model, and the other for testing the model:'
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据集按70:30的比例分割成两个数据集，一个用于训练模型，另一个用于测试模型：
- en: '[PRE132]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'We set up the parameters and using the `DecisionTree` model, after the training
    dataset, we use the test dataset to do the prediction:'
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置参数并使用`DecisionTree`模型，在训练数据集之后，我们使用测试数据集进行预测：
- en: '[PRE133]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'We instantiate the `RegressionMetrics` object and start the evaluation:'
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实例化`RegressionMetrics`对象并开始评估：
- en: '[PRE134]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'We print out the statistics value in the console:'
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中打印出统计值：
- en: '[PRE135]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'From the console output:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE136]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'We then close the program by stopping the Spark session:'
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过停止Spark会话来关闭程序：
- en: '[PRE137]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: How it works...
  id: totrans-538
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we explored the generation of regression metrics to help us
    evaluate our regression model. We began to load a breast cancer data file and
    then split it in a 70/30 ratio to create training and test datasets. Next, we
    trained a `DecisionTree` regression model and utilized it to make predictions
    on our test set. Finally, we took the predictions and generated regression metrics
    which gave us the squared error, R-squared, mean absolute error, and explained
    variance.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探讨了生成回归度量标准来帮助我们评估回归模型。我们开始加载一个乳腺癌数据文件，然后将其按70/30的比例分割，以创建训练和测试数据集。接下来，我们训练了一个`DecisionTree`回归模型，并利用它对我们的测试集进行了预测。最后，我们拿到了预测结果，并生成了回归度量标准，这给了我们平方误差，R平方，平均绝对误差和解释的方差。
- en: There's more...
  id: totrans-540
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We can use `RegressionMetrics()` to produce the following statistical measures:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`RegressionMetrics()`来生成以下统计量：
- en: MSE
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MSE
- en: RMSE
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSE
- en: R-squared
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R平方
- en: MAE
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAE
- en: Explained variance
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释的方差
- en: Documentation on regression validation is available at [https://en.wikipedia.org/wiki/Regression_validation](https://en.wikipedia.org/wiki/Regression_validation).
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 有关回归验证的文档可在[https://en.wikipedia.org/wiki/Regression_validation](https://en.wikipedia.org/wiki/Regression_validation)上找到。
- en: R-Squared/coefficient of determination is available at [https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination).
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: R平方/决定系数可在[https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)上找到。
- en: See also
  id: totrans-549
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The Wisconsin breast cancer dataset could be downloaded at [ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum](ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum)
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威斯康星州乳腺癌数据集可以在[ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum](ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum)上下载
- en: Regression metrics documents are available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics)
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归度量标准文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics)上找到。
- en: Binary classification model evaluation using Spark 2.0
  id: totrans-552
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行二元分类模型评估
- en: In this recipe, we demonstrate the use of the `BinaryClassificationMetrics`
    facility in Spark 2.0 and its application to evaluating a model that has a binary
    outcome (for example, a logistic regression).
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们演示了在Spark 2.0中使用`BinaryClassificationMetrics`工具以及其应用于评估具有二元结果的模型（例如逻辑回归）。
- en: The purpose here is not to showcase the regression itself, but to demonstrate
    how to go about evaluating it using common metrics such as **receiver operating
    characteristic** (**ROC**), Area Under ROC Curve, thresholds, and so on.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目的不是展示回归本身，而是演示如何使用常见的度量标准（如**接收器操作特征**（**ROC**），ROC曲线下面积，阈值等）来评估它。
- en: We recommend that you concentrate on step 8 since we cover regression in more
    detail in [Chapter 5](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part I* and
    [Chapter 6](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part II*.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您专注于第8步，因为我们在[第5章](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77)中更详细地介绍了回归，*使用Spark
    2.0进行回归和分类的实际机器学习-第I部分*和[第6章](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77)中更详细地介绍了回归和分类的实际机器学习-第II部分*。
- en: How to do it...
  id: totrans-556
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE138]'
  id: totrans-559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-560
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以使SparkContext能够访问集群：
- en: '[PRE139]'
  id: totrans-561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'Create Spark''s configuration and SparkContext:'
  id: totrans-562
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和SparkContext：
- en: '[PRE140]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'We download the dataset, originally from the UCI, and modify it to fit the
    need for the code:'
  id: totrans-564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们下载了数据集，最初来自UCI，并对其进行修改以适应代码的需要：
- en: '[PRE141]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: The dataset is a modified dataset. The original adult dataset has 14 features,
    among which six are continuous and eight are categorical. In this dataset, continuous
    features are discretized into quantiles, and each quantile is represented by a
    binary feature. We modified the data to fit the purpose of the code. Details of
    the dataset feature can be found at the [http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)
    UCI site.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是一个修改后的数据集。原始的成年人数据集有14个特征，其中六个是连续的，八个是分类的。在这个数据集中，连续特征被离散化为分位数，并且每个分位数由一个二进制特征表示。我们修改了数据以适应代码的目的。数据集特征的详细信息可以在[http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)
    UCI网站上找到。
- en: 'We split the dataset into training and test parts in a ratio of 60:40 random
    split, then get the model:'
  id: totrans-567
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据集随机分成60:40的训练和测试部分，然后得到模型：
- en: '[PRE142]'
  id: totrans-568
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: 'We create the prediction using the model created by the training dataset:'
  id: totrans-569
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用训练数据集创建模型进行预测：
- en: '[PRE143]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'We create the `BinaryClassificationMetrics` object from the predication, and
    start the evaluation on the metrics:'
  id: totrans-571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从预测中创建`BinaryClassificationMetrics`对象，并开始对指标进行评估：
- en: '[PRE144]'
  id: totrans-572
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'We print out the precision by `Threashold` in the console:'
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中按`Threashold`打印出精度：
- en: '[PRE145]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'From the console output:'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE146]'
  id: totrans-576
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'We print out the `recallByThreshold` in the console:'
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中打印出`recallByThreshold`：
- en: '[PRE147]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: 'From the console output:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE148]'
  id: totrans-580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: 'We print out the `fmeasureByThreshold` in the console:'
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中打印出`fmeasureByThreshold`：
- en: '[PRE149]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: 'From the console output:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE150]'
  id: totrans-584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'From the console output:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE151]'
  id: totrans-586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: 'We print out the `Area Under Precision Recall Curve` in the console:'
  id: totrans-587
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中打印出`Area Under Precision Recall Curve`：
- en: '[PRE152]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'From the console output:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE153]'
  id: totrans-590
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: 'We print out the Area Under ROC curve in the console:'
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中打印出ROC曲线下面积：
- en: '[PRE154]'
  id: totrans-592
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: 'From the console output:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE155]'
  id: totrans-594
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: 'We then close the program by stopping the Spark session:'
  id: totrans-595
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过停止Spark会话来关闭程序：
- en: '[PRE156]'
  id: totrans-596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: How it works...
  id: totrans-597
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we investigated the evaluation of metrics for binary classification.
    First, we loaded the data, which is in the `libsvm` format, and split it in the
    ratio of 60:40, resulting in the creation of a training and a test set of data.
    Next, we trained a logistic regression model followed by generating predictions
    from our test set.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们调查了二元分类指标的评估。首先，我们加载了数据，它是以`libsvm`格式，然后将其按60:40的比例分割，从而创建了训练和测试数据集。接下来，我们训练了一个逻辑回归模型，然后从我们的测试集生成了预测。
- en: Once we had our predictions, we created a binary classification metrics object.
    Finally, we retrieved the true positive rate, positive predictive value, receiver
    operating curve, area under receiver operating curve, area under precision recall
    curve, and F-measure to evaluate our model for fitness.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了预测结果，我们创建了一个二元分类指标对象。最后，我们检索真正率、阳性预测值、接收器操作曲线、接收器操作曲线下面积、精度-召回曲线下面积和F-度量来评估我们的模型适应性。
- en: There's more...
  id: totrans-600
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Spark provides the following metrics to facilitate evaluation:'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供以下指标以便进行评估：
- en: TPR - True Positive Rate
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TPR - 真正率
- en: PPV - Positive Predictive Value
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PPV - 阳性预测值
- en: F - F-Measure
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F - F-度量
- en: ROC - Receiver Operating Curve
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROC - 接收器操作曲线
- en: AUROC - Area Under Receiver Operating Curve
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUROC - 接收器操作曲线下面积
- en: AUORC - Area Under Precision-Recall Curve
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUORC - 精度-召回曲线下面积
- en: 'The following links should provide a good introductory material for the metrics:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接应提供有关指标的良好入门材料：
- en: '[https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)'
- en: '[https://en.wikipedia.org/wiki/Sensitivity_and_specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Sensitivity_and_specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)'
- en: '[https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score)'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score)'
- en: See also
  id: totrans-612
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Documentation for the original dataset information is available at the following
    links:'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集信息的文档可在以下链接找到：
- en: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html)'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html)'
- en: '[http://archive.ics.uci.edu/ml/datasets.html](http://archive.ics.uci.edu/ml/datasets.html)'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://archive.ics.uci.edu/ml/datasets.html](http://archive.ics.uci.edu/ml/datasets.html)'
- en: Documentation for binary classification metrics is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics).
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类指标的文档可在以下链接找到：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics)。
- en: Multiclass classification model evaluation using Spark 2.0
  id: totrans-617
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行多类分类模型评估
- en: In this recipe, we explore `MulticlassMetrics`, which allows you to evaluate
    a model that classifies the output to more than two labels (for example, red,
    blue, green, purple, do-not-know). It highlights the use of confusion matrix (`confusionMatrix`)
    and model accuracy.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探讨了`MulticlassMetrics`，它允许您评估将输出分类到两个以上标签的模型（例如，红色、蓝色、绿色、紫色、不知道）。它突出了混淆矩阵（`confusionMatrix`）和模型准确性的使用。
- en: How to do it...
  id: totrans-619
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-621
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE157]'
  id: totrans-622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-623
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以便SparkContext可以访问集群：
- en: '[PRE158]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: 'Create Spark''s configuration and SparkContext:'
  id: totrans-625
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和SparkContext：
- en: '[PRE159]'
  id: totrans-626
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: 'We download the dataset, originally from the UCI, and modify it to fit the
    need of the code:'
  id: totrans-627
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们下载了最初来自UCI的数据集，并对其进行修改以适应代码的需要：
- en: '[PRE160]'
  id: totrans-628
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: The dataset is a modified dataset. The original Iris Plant dataset has four
    features. We modified the data to fit the purpose of the code. Details of the
    dataset features can be found at the UCI site.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是一个修改后的数据集。原始的鸢尾花植物数据集有四个特征。我们修改了数据以适应代码的目的。数据集特征的详细信息可以在UCI网站上找到。
- en: 'We split the dataset into training and test parts in a ratio of 60% versus
    40% random split, then get the model:'
  id: totrans-630
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据集随机分成60%的训练部分和40%的测试部分，然后得到模型：
- en: '[PRE161]'
  id: totrans-631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: 'We compute the raw score on the test dataset:'
  id: totrans-632
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在测试数据集上计算原始分数：
- en: '[PRE162]'
  id: totrans-633
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: 'We create the `MulticlassMetrics` object from the predication, and start the
    evaluation on the metrics:'
  id: totrans-634
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从预测中创建了`MulticlassMetrics`对象，并开始对指标进行评估：
- en: '[PRE163]'
  id: totrans-635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: 'We print out the confusion matrix in the console:'
  id: totrans-636
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中打印出混淆矩阵：
- en: '[PRE164]'
  id: totrans-637
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'From the console output:'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE165]'
  id: totrans-639
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'We print out the overall statistics in the console:'
  id: totrans-640
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中打印出总体统计信息：
- en: '[PRE166]'
  id: totrans-641
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: 'From the console output:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE167]'
  id: totrans-643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: 'We print out the precision by label value in the console:'
  id: totrans-644
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中按标签值打印出精度：
- en: '[PRE168]'
  id: totrans-645
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: 'From the console output:'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE169]'
  id: totrans-647
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: 'We print out the recall by label in the console:'
  id: totrans-648
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中按标签打印出召回率：
- en: '[PRE170]'
  id: totrans-649
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: 'From the console output:'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE171]'
  id: totrans-651
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: 'We print out the false positive rate by label in the console:'
  id: totrans-652
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中按标签打印出假阳性率：
- en: '[PRE172]'
  id: totrans-653
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: 'From the console output:'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE173]'
  id: totrans-655
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: 'We print out the F-measure by label in the console:'
  id: totrans-656
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中按标签打印出F-度量：
- en: '[PRE174]'
  id: totrans-657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: 'From the console output:'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE175]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: 'We print out the weighted statistics value in the console:'
  id: totrans-660
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中打印出加权统计值：
- en: '[PRE176]'
  id: totrans-661
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: 'From the console output:'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE177]'
  id: totrans-663
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: 'We then close the program by stopping the Spark session:'
  id: totrans-664
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过停止Spark会话来关闭程序：
- en: '[PRE178]'
  id: totrans-665
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: How it works...
  id: totrans-666
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we explored generating evaluation metrics for a multi-classification
    model. First, we loaded the Iris data into memory and split it in a ratio 60:40\.
    Second, we trained a logistic regression model with the number of classifications
    set to three. Third, we made predictions with the test dataset and utilized `MultiClassMetric`
    to generate evaluation measurements. Finally, we evaluated metrics such as the
    model accuracy, weighted precision, weighted recall, weighted F1 score, weighted
    false positive rate, and so on.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们探索了为多类别模型生成评估指标。首先，我们将Iris数据加载到内存中，并将其按比例60:40拆分。其次，我们训练了一个逻辑回归模型，分类数设置为三。第三，我们对测试数据集进行了预测，并利用`MultiClassMetric`生成评估测量。最后，我们评估了诸如模型准确度、加权精度、加权召回率、加权F1分数、加权假阳性率等指标。
- en: There's more...
  id: totrans-668
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: While the scope of the book does not allow for a complete treatment of the confusion
    matrix, a short explanation and a link are provided as a quick reference.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本书的范围不允许对混淆矩阵进行完整处理，但提供了简短的解释和链接作为快速参考。
- en: 'The confusion matrix is just a fancy name for an error matrix. It is mostly
    used in unsupervised learning to visualize the performance. It is a layout that
    captures actual versus predicted outcomes with an identical set of labels in two
    dimensions:'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵只是一个错误矩阵的花哨名称。它主要用于无监督学习来可视化性能。它是一个布局，以两个维度捕获实际与预测结果的相同标签集：
- en: '**Confusion Matrix**'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: '**混淆矩阵**'
- en: '![](img/00101.jpeg)'
  id: totrans-672
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00101.jpeg)'
- en: To get a quick introduction to the confusion matrix in unsupervised and supervised
    statistical learning systems, see [https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix).
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速了解无监督和监督统计学习系统中的混淆矩阵，请参阅[https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix)。
- en: See also
  id: totrans-674
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Documentation for original dataset information is available at the following
    websites:'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集信息的文档可在以下网站找到：
- en: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html)'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html)'
- en: '[http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)'
- en: 'Documentation for multiclass classification metrics is available at:'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 多类别分类指标的文档可在以下网址找到：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)'
- en: Multilabel classification model evaluation using Spark 2.0
  id: totrans-680
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行多标签分类模型评估
- en: In this recipe, we explore multilabel classification `MultilabelMetrics` in
    Spark 2.0 which should not be mixed up with the previous recipe dealing with multiclass
    classification `MulticlassMetrics`. The key to exploring this recipe is to concentrate
    on evaluation metrics such as Hamming loss, accuracy, f1-measure, and so on, and
    what they measure.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们探讨了Spark 2.0中的多标签分类`MultilabelMetrics`，这不应该与处理多类别分类`MulticlassMetrics`的先前教程混淆。探索这个教程的关键是集中在评估指标，如汉明损失、准确度、f1度量等，以及它们的测量。
- en: How to do it...
  id: totrans-682
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-683
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-684
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE179]'
  id: totrans-685
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  id: totrans-686
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以便SparkContext可以访问集群：
- en: '[PRE180]'
  id: totrans-687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: 'Create Spark''s configuration and SparkContext:'
  id: totrans-688
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和SparkContext：
- en: '[PRE181]'
  id: totrans-689
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: 'We create the dataset for the evaluation model:'
  id: totrans-690
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为评估模型创建数据集：
- en: '[PRE182]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: 'We create the `MultilabelMetrics` object from the predication, and start the
    evaluation on the metrics:'
  id: totrans-692
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从预测中创建`MultilabelMetrics`对象，并开始对指标进行评估：
- en: '[PRE183]'
  id: totrans-693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: 'We print out the overall statistics summary in the console:'
  id: totrans-694
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中打印出总体统计摘要：
- en: '[PRE184]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: 'From the console output:'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE185]'
  id: totrans-697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: 'We print out the individual label value in the console:'
  id: totrans-698
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中打印出各个标签值：
- en: '[PRE186]'
  id: totrans-699
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: 'From the console output:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '[PRE187]'
  id: totrans-701
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: 'We print out the micro statistics value in the console:'
  id: totrans-702
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中打印出微观统计值：
- en: '[PRE188]'
  id: totrans-703
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: 'We print out the Hamming loss and subset accuracy from the metrics in the console:'
  id: totrans-704
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在控制台中打印出指标中的汉明损失和子集准确度：
- en: '[PRE189]'
  id: totrans-705
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: We then close the program by stopping the Spark session.
  id: totrans-706
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过停止Spark会话来关闭程序。
- en: '[PRE190]'
  id: totrans-707
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: How it works...
  id: totrans-708
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we investigated generating evaluation metrics for the multilabel
    classification model. We began with manually creating a dataset for the model
    evaluation. Next, we passed our dataset as an argument to the `MultilabelMetrics`
    and generated evaluation metrics. Finally, we printed out various metrics such
    as micro recall, micro precision, micro f1-measure, Hamming loss, subset accuracy,
    and so on.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们调查了为多标签分类模型生成评估指标。我们首先手动创建了一个用于模型评估的数据集。接下来，我们将我们的数据集作为参数传递给`MultilabelMetrics`并生成评估指标。最后，我们打印出各种指标，如微观召回率、微观精度、微观f1度量、汉明损失、子集准确度等。
- en: There's more...
  id: totrans-710
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Note that the multilabel and multiclass classifications sound similar, but they
    are two different things.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，多标签和多类别分类听起来相似，但它们是两回事。
- en: All multilabel `MultilabelMetrics()` method is trying to accomplish is to map
    a number of inputs (x) to a binary vector (y) rather than numerical values in
    a typical classification system.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 所有多标签`MultilabelMetrics()`方法试图做的只是将多个输入（x）映射到二进制向量（y），而不是典型分类系统中的数值。
- en: 'The important metrics associated with the multilabel classification are (see
    the preceding code):'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 与多标签分类相关的重要指标是（参见前面的代码）：
- en: Accuracy
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确度
- en: Hamming loss
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汉明损失
- en: Precision
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精度
- en: Recall
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 召回
- en: F1
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1
- en: 'A full explanation of each parameter is out of scope, but the following link
    provides a short treatment for the multilabel metrics:'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 每个参数的详细解释超出了范围，但以下链接提供了多标签指标的简短处理：
- en: '[https://en.wikipedia.org/wiki/Multi-label_classification](https://en.wikipedia.org/wiki/Multi-label_classification)'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Multi-label_classification](https://en.wikipedia.org/wiki/Multi-label_classification)'
- en: See also
  id: totrans-721
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Documentation for multilabel classification metrics:'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类指标的文档：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MultilabelMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MultilabelMetrics)'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MultilabelMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MultilabelMetrics)'
- en: Using the Scala Breeze library to do graphics in Spark 2.0
  id: totrans-724
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用Scala Breeze库进行图形处理
- en: In this recipe, we will use the functions `scatter()` and `plot()` from the
    Scala Breeze linear algebra library (part of) to draw a scatter plot from a two-dimensional
    data. Once the results are computed on the Spark cluster, either the actionable
    data can be used in the driver for drawing or a JPEG or GIF can be generated in
    the backend and pushed forward for efficiency and speed (popular with GPU-based
    analytical databases such as MapD)
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用Scala Breeze线性代数库（部分）的`scatter()`和`plot()`函数来绘制二维数据的散点图。一旦在Spark集群上计算出结果，可以在驱动程序中使用可操作数据进行绘制，也可以在后端生成JPEG或GIF并推动效率和速度（在基于GPU的分析数据库中很受欢迎，如MapD）。
- en: How to do it...
  id: totrans-726
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: First, we need to download the necessary ScalaNLP library. Download the JAR
    from the Maven repository available at [https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar](https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar).
  id: totrans-727
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要下载必要的ScalaNLP库。从Maven仓库下载JAR文件，网址为[https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar](https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar)。
- en: 'Place the JAR in the `C:\spark-2.0.0-bin-hadoop2.7\examples\jars` directory
    on a Windows machine:'
  id: totrans-728
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Windows机器上的`C:\spark-2.0.0-bin-hadoop2.7\examples\jars`目录中放置JAR文件：
- en: In macOS, please put the JAR in its correct path. For our setting examples,
    the path is `/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/examples/jars/`.
  id: totrans-729
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在macOS中，请将JAR文件放在正确的路径下。对于我们的设置示例，路径为`/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/examples/jars/`。
- en: 'The following is the sample screenshot showing the JARs:'
  id: totrans-730
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是显示JAR文件的示例截图：
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-731
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-732
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE191]'
  id: totrans-733
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-734
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以便Spark会话可以访问集群和`log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE192]'
  id: totrans-735
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-736
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE193]'
  id: totrans-737
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: 'Initialize a Spark session by specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-738
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用构建器模式指定配置来初始化Spark会话，从而为Spark集群提供入口点：
- en: '[PRE194]'
  id: totrans-739
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: 'Now we create the figure object, and set the parameter for the figure:'
  id: totrans-740
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们创建图形对象，并设置图形的参数：
- en: '[PRE195]'
  id: totrans-741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: We create a dataset from random numbers, and display the dataset.
  id: totrans-742
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从随机数创建一个数据集，并显示数据集。
- en: The dataset will be used later.
  id: totrans-743
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集将在以后使用。
- en: '[PRE196]'
  id: totrans-744
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: 'From the console output:'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出：
- en: '![](img/00102.jpeg)'
  id: totrans-746
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00102.jpeg)'
- en: We collect the dataset, and set up the *x* and *y* axis.
  id: totrans-747
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们收集数据集，并设置*x*和*y*轴。
- en: For the photo part, we convert the datatype to double, and derive the value
    to `y2`.
  id: totrans-748
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于照片部分，我们将数据类型转换为double，并将值派生为`y2`。
- en: 'We use the Breeze library''s scatter method to put the data into the chart,
    and plot the diagonal line with the plot method from Breeze:'
  id: totrans-749
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用Breeze库的scatter方法将数据放入图表中，并使用Breeze的plot方法绘制对角线：
- en: '[PRE197]'
  id: totrans-750
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: We set the label for both the *x* axis and *y* axis and refresh the figure object.
  id: totrans-751
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为*x*轴和*y*轴设置标签，并刷新图形对象。
- en: 'The following is the generated Breeze chart:'
  id: totrans-752
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是生成的Breeze图表：
- en: '![](img/00103.jpeg)'
  id: totrans-753
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00103.jpeg)'
- en: 'We close the program by stopping the Spark session:'
  id: totrans-754
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止Spark会话来关闭程序：
- en: '[PRE198]'
  id: totrans-755
  prefs: []
  type: TYPE_PRE
  zh: '[PRE198]'
- en: How it works...
  id: totrans-756
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we created a dataset in Spark from random numbers. We then created
    a Breeze figure and set up the basic parameters. We derived *x*, *y* data from
    the created dataset.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们从随机数中创建了一个Spark数据集。然后创建了一个Breeze图，并设置了基本参数。我们从创建的数据集中派生了*x*和*y*数据。
- en: We used Breeze's `scatter()` and `plot()` functions to do graphics using the
    Breeze library.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Breeze库的`scatter()`和`plot()`函数来使用Breeze库进行图形处理。
- en: There's more...
  id: totrans-759
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: One can use Breeze as an alternative to more complicated and powerful charting
    libraries such as JFreeChart, demonstrated in the previous chapter. The ScalaNLP
    project tends to be optimized with Scala goodies such as implicit conversions
    that make the coding relatively easier.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Breeze作为更复杂和强大的图表库（如前一章中演示的JFreeChart）的替代方案。ScalaNLP项目倾向于使用Scala好用的功能，比如隐式转换，使编码相对容易。
- en: The Breeze graphics JAR file can be downloaded at [http://central.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar](http://central.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar).
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: Breeze图形JAR文件可以在[http://central.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar](http://central.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar)下载。
- en: More about Breeze graphics can be found at [https://github.com/scalanlp/breeze/wiki/Quickstart](https://github.com/scalanlp/breeze/wiki/Quickstart).
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Breeze图形的更多信息，请访问[https://github.com/scalanlp/breeze/wiki/Quickstart](https://github.com/scalanlp/breeze/wiki/Quickstart)。
- en: The API document (please note, the API document is not necessarily up-to-date)
    can be found at [http://www.scalanlp.org/api/breeze/#package](http://www.scalanlp.org/api/breeze/#package).
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: API文档（请注意，API文档未必是最新的）可在[http://www.scalanlp.org/api/breeze/#package](http://www.scalanlp.org/api/breeze/#package)找到。
- en: Note that once you are in the root package, you need click on Breeze to see
    the details.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一旦您进入根包，您需要点击Breeze查看详细信息。
- en: See also
  id: totrans-765
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: For more information on Breeze, see the original material on GitHub at [https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze).
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Breeze的更多信息，请参阅GitHub上的原始材料[https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze)。
- en: Note that once you are in the root package, you need to click on Breeze to see
    the details.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一旦您进入根包，您需要点击Breeze查看详细信息。
- en: For more information regarding the Breeze API documentation, please download
    the [https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12-javadoc.jar](https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12-javadoc.jar)
    JAR.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Breeze API文档的更多信息，请下载[https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12-javadoc.jar](https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12-javadoc.jar)
    JAR。
