- en: Interactive Data Analytics with Apache Zeppelin
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Apache Zeppelin 进行交互式数据分析
- en: From a data science perspective, interactive visualization of your data analysis
    is also important. Apache Zeppelin, is a web-based notebook for interactive and
    large-scale data analytics with multiple backends and interpreters, such as Spark,
    Scala, Python, JDBC, Flink, Hive, Angular, Livy, Alluxio, PostgreSQL, Ignite,
    Lens, Cassandra, Kylin, Elasticsearch, JDBC, HBase, BigQuery, Pig, Markdown, Shell,
    and even more.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据科学的角度来看，数据分析的交互式可视化同样重要。Apache Zeppelin 是一个基于 Web 的笔记本，用于交互式和大规模的数据分析，支持多种后端和解释器，如
    Spark、Scala、Python、JDBC、Flink、Hive、Angular、Livy、Alluxio、PostgreSQL、Ignite、Lens、Cassandra、Kylin、Elasticsearch、JDBC、HBase、BigQuery、Pig、Markdown、Shell
    等。
- en: 'There is no doubt about Spark''s ability to handle large-scale datasets in
    a scalable and fast way. However, one thing in Spark is missing--there is no real-time
    or interactive visualization support with it. Considering the aforementioned exciting
    features of Zeppelin, in this chapter, we will discuss how to use Apache Zeppelin
    for large-scale data analytics using Spark as the interpreter in the backend.
    In summary, the following topics will be covered:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 在可扩展和快速处理大规模数据集方面的能力是毋庸置疑的。然而，Spark 中有一点是缺失的——它没有实时或交互式的可视化支持。考虑到 Zeppelin
    所具备的上述令人兴奋的特点，本章将讨论如何使用 Apache Zeppelin 进行大规模数据分析，后端使用 Spark 作为解释器。总结来说，以下主题将被涵盖：
- en: Introduction to Apache Zeppelin
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Zeppelin 简介
- en: Installation and getting started
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装与入门
- en: Data ingestion
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取
- en: Data analytics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分析
- en: Data visualization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Data collaboration
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据协作
- en: Introduction to Apache Zeppelin
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Zeppelin 简介
- en: Apache Zeppelin is a web-based notebook that enables you to do data analytics
    in an interactive way. Using Zeppelin, you can make beautiful data-driven, interactive,
    and collaborative documents with SQL, Scala, and more. The Apache Zeppelin interpreter
    concept allows any language/data-processing backend to be plugged into Zeppelin.
    Currently, Apache Zeppelin supports many interpreters, such as Apache Spark, Python,
    JDBC, Markdown, and Shell. Apache Zeppelin is a relatively new technology from
    the Apache Software Foundation, which enables data scientists, engineers, and
    practitioners to take the advantage of the data exploration, visualization, sharing,
    and collaboration features.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Zeppelin 是一个基于 Web 的笔记本，能够以交互方式进行数据分析。使用 Zeppelin，您可以制作美观的、数据驱动的、交互式的、协作式的文档，支持
    SQL、Scala 等语言。Apache Zeppelin 的解释器概念允许将任何语言/数据处理后端插件集成到 Zeppelin 中。目前，Apache Zeppelin
    支持多种解释器，如 Apache Spark、Python、JDBC、Markdown 和 Shell 等。Apache Zeppelin 是 Apache
    软件基金会推出的相对较新的技术，能够帮助数据科学家、工程师和从业人员利用数据探索、可视化、共享和协作功能。
- en: Installation and getting started
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装与入门
- en: 'Since using the other interpreters is not the goal of this book, but using
    Spark on Zeppelin is, all the code will be written using Scala. In this section,
    therefore, we will show how to configure Zeppelin using the binary package that
    contains only the Spark interpreter. Apache Zeppelin officially supports and is
    tested on the following environments:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书的目标并不是使用其他解释器，而是使用 Spark 在 Zeppelin 上，因此所有的代码将使用 Scala 编写。因此，在本节中，我们将展示如何使用只包含
    Spark 解释器的二进制包配置 Zeppelin。Apache Zeppelin 官方支持并在以下环境上进行测试：
- en: '| **Requirements** | **Value/Version** | **Other Requirements** |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **要求** | **值/版本** | **其他要求** |'
- en: '| Oracle JDK | 1.7 or higher | Set `JAVA_HOME` |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| Oracle JDK | 1.7 或更高版本 | 设置 `JAVA_HOME` |'
- en: '| OS | macOS 10.X+ Ubuntu 14.X+'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '| 操作系统 | macOS 10.X+ Ubuntu 14.X+ |'
- en: CentOS 6.X+
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: CentOS 6.X+
- en: Windows 7 Pro SP1+ | - |
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Windows 7 Pro SP1+ | - |
- en: Installation and configuration
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装与配置
- en: As shown in the preceding table, Java is required to execute Spark codes on
    Zeppelin. Therefore, if not set up, install and set up Java on any of the aforementioned
    platforms. Alternatively, you can refer to [Chapter 1](part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c),
    *Introduction to Scala*, to learn how to set up Java on your machine.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如前表所示，在 Zeppelin 上执行 Spark 代码需要 Java。因此，如果尚未设置，请在上述平台上安装并配置 Java。或者，您可以参考[第
    1 章](part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c)，*Scala 入门*，了解如何在您的机器上设置
    Java。
- en: 'The latest release of Apache Zeppelin can be downloaded from [https://zeppelin.apache.org/download.html](https://zeppelin.apache.org/download.html).
    Each release comes with three options:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从[https://zeppelin.apache.org/download.html](https://zeppelin.apache.org/download.html)下载最新版本的
    Apache Zeppelin。每个版本提供三种选项：
- en: '**Binary package with all the interpreters**: It contains support for many
    interpreters. For example, Spark, JDBC, Pig, Beam, Scio, BigQuery, Python, Livy,
    HDFS, Alluxio, Hbase, Scalding, Elasticsearch, Angular, Markdown, Shell, Flink,
    Hive, Tajo, Cassandra, Geode, Ignite, Kylin, Lens, Phoenix, and PostgreSQL are
    currently supported in Zeppelin.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**包含所有解释器的二进制包**：它包含对许多解释器的支持。例如，目前 Zeppelin 支持 Spark、JDBC、Pig、Beam、Scio、BigQuery、Python、Livy、HDFS、Alluxio、Hbase、Scalding、Elasticsearch、Angular、Markdown、Shell、Flink、Hive、Tajo、Cassandra、Geode、Ignite、Kylin、Lens、Phoenix
    和 PostgreSQL 等。'
- en: '**Binary package with the Spark interpreter**: It usually contains only the
    Spark interpreter. It also contains the interpreter net-install script.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**包含 Spark 解释器的二进制包**：它通常只包含 Spark 解释器，同时也包含解释器的 net-install 脚本。'
- en: '**Source**: You can also build Zeppelin with all the latest changes from the
    GitHub repo (more to follow).'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**源代码**：你也可以从 GitHub 仓库构建包含所有最新更改的 Zeppelin（更多内容即将发布）。'
- en: 'To show how to install and configure Zeppelin, we have downloaded the binary
    package from the following site mirror:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示如何安装和配置 Zeppelin，我们已从以下站点镜像下载了二进制包：
- en: '[http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.7.1/zeppelin-0.7.1-bin-netinst.tgz](http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.7.1/zeppelin-0.7.1-bin-netinst.tgz)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.7.1/zeppelin-0.7.1-bin-netinst.tgz](http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.7.1/zeppelin-0.7.1-bin-netinst.tgz)'
- en: Once you have downloaded it, unzip it somewhere in your machine. Suppose that
    the path where you have unzipped the file is `/home/Zeppelin/`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦下载完成，将其解压到机器的某个位置。假设你解压的路径是 `/home/Zeppelin/`。
- en: Building from source
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从源代码构建
- en: 'You can also build Zeppelin with all the latest changes from the GitHub repo.
    If you want to build from source, you must first install the following tools:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以从 GitHub 仓库构建包含所有最新更改的 Zeppelin。如果你想从源代码构建，必须先安装以下工具：
- en: 'Git: any version'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Git：任何版本
- en: 'Maven: 3.1.x or higher'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maven：3.1.x 或更高版本
- en: 'JDK: 1.7 or higher'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDK：1.7 或更高版本
- en: 'npm: the latest version'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: npm：最新版本
- en: 'libfontconfig: the latest version'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: libfontconfig：最新版本
- en: 'If you haven''t installed Git and Maven yet, check the build requirement instructions
    from [http://zeppelin.apache.org/docs/0.8.0-SNAPSHOT/install/build.html#build-requirements](http://zeppelin.apache.org/docs/0.8.0-SNAPSHOT/install/build.html#build-requirements).
    However, due to page limitation, we have not discussed all the steps in detail.
    If you''re interested, you should refer to this URL for more details: [http://zeppelin.apache.org/docs/snapshot/install/build.html](http://zeppelin.apache.org/docs/snapshot/install/build.html).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有安装 Git 和 Maven，请查看 [http://zeppelin.apache.org/docs/0.8.0-SNAPSHOT/install/build.html#build-requirements](http://zeppelin.apache.org/docs/0.8.0-SNAPSHOT/install/build.html#build-requirements)
    中的构建要求说明。然而，由于页面限制，我们没有详细讨论所有步骤。如果你有兴趣，应该参考这个 URL 获取更多详细信息：[http://zeppelin.apache.org/docs/snapshot/install/build.html](http://zeppelin.apache.org/docs/snapshot/install/build.html)。
- en: Starting and stopping Apache Zeppelin
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动和停止 Apache Zeppelin
- en: 'On all Unix-like platforms (for example, Ubuntu, macOS, and so on), use the
    following command:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有类 Unix 平台（例如，Ubuntu、macOS 等）上，使用以下命令：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If the preceding command is successfully executed, you should observe the following
    logs on the terminal:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的命令执行成功，你应该在终端看到以下日志：
- en: '![](img/00061.jpeg)**Figure 1**: Starting Zeppelin from the Ubuntu terminal'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00061.jpeg)**图 1**：从 Ubuntu 终端启动 Zeppelin'
- en: 'If you are on Windows, use the following command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 Windows，请使用以下命令：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After Zeppelin has started successfully, go to `http://localhost:8080` with
    your web browser and you will see that Zeppelin is running. More specifically,
    you''ll see the following view on your browser:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Zeppelin 成功启动后，使用你的浏览器访问 `http://localhost:8080`，你将看到 Zeppelin 正在运行。更具体地说，你将会在浏览器中看到以下界面：
- en: '![](img/00069.jpeg)**Figure 2**: Zeppelin is running on http://localhost:8080'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00069.jpeg)**图 2**：Zeppelin 正在 http://localhost:8080 上运行'
- en: Congratulations; you have successfully installed Apache Zeppelin! Now, let's
    move on to Zeppelin and get started with our data analytics once we have configured
    the preferred interpreter.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你；你已经成功安装了 Apache Zeppelin！现在，让我们继续进入 Zeppelin 并开始进行数据分析，一旦我们配置好首选的解释器。
- en: 'Now, to stop Zeppelin from the command line, issue the following command:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要从命令行停止 Zeppelin，请输入以下命令：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Creating notebooks
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建笔记本
- en: Once you are on `http://localhost:8080/`, you can explore different options
    and menus that help you understand how to get familiar with Zeppelin. You can
    find more on Zeppelin and its user-friendly UI at [https://zeppelin.apache.org/docs/0.7.1/quickstart/explorezeppelinui.html](https://zeppelin.apache.org/docs/0.7.1/quickstart/explorezeppelinui.html)
    (you can refer to the latest quick start documentation too, based on the available
    versions).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你进入`http://localhost:8080/`，你可以探索不同的选项和菜单，帮助你了解如何熟悉Zeppelin。你可以在[https://zeppelin.apache.org/docs/0.7.1/quickstart/explorezeppelinui.html](https://zeppelin.apache.org/docs/0.7.1/quickstart/explorezeppelinui.html)找到更多关于Zeppelin及其用户友好界面的信息（你也可以根据可用版本参考最新的快速入门文档）。
- en: 'Now, let''s first create a sample notebook and get started. As shown in the
    following figure, you can create a new notebook by clicking on the Create new
    note option:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们首先创建一个示例笔记本并开始操作。如以下图所示，你可以通过点击“创建新笔记”选项来创建一个新的笔记本：
- en: '![](img/00082.jpeg)**Figure 3**: Creating a sample Zeppelin notebook'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00082.jpeg)**图 3**：创建一个示例Zeppelin笔记本'
- en: As shown in the previous figure, the default interpreter is selected as Spark.
    In the drop-down list, you will also see only Spark, since we have download the
    Spark-only binary package for Zeppelin.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，默认的解释器被选择为Spark。在下拉列表中，你也只会看到Spark，因为我们已经为Zeppelin下载了仅包含Spark的二进制包。
- en: Configuring the interpreter
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置解释器
- en: Every interpreter belongs to an interpreter group. An interpreter group is a
    unit of start/stop interpreters. By default, every interpreter belongs to a single
    group, but the group might contain more interpreters. For example, the Spark interpreter
    group includes Spark support, pySpark, Spark SQL, and the dependency loader. If
    you want to execute an SQL statement on Zeppelin, you should specify the interpreter
    type using the `%` sign; for example, for using SQL, you should use `%sql`; for
    mark-down, use `%md`, and so on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 每个解释器都属于一个解释器组。解释器组是启动/停止解释器的单位。默认情况下，每个解释器都属于一个单独的组，但一个组可能包含更多的解释器。例如，Spark解释器组包括Spark支持、pySpark、Spark
    SQL和依赖加载器。如果你想在Zeppelin上执行SQL语句，你应该使用`%`符号指定解释器类型；例如，使用SQL时应使用`%sql`，使用markdown时应使用`%md`，等等。
- en: 'For more information, refer to the following image:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参见以下图片：
- en: '![](img/00086.jpeg)**Figure 4**: The interpreter properties for using Spark
    on Zeppelin Data ingestion'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00086.jpeg)**图 4**：在Zeppelin中使用Spark的解释器属性 数据摄取'
- en: 'Well, once you have created the notebook, you can start writing Spark code
    directly in the code section. For this simple example, we will use the bank dataset,
    which is publicly available for research and can be downloaded from [https://archive.ics.uci.edu/ml/machine-learning-databases/00222/](https://archive.ics.uci.edu/ml/machine-learning-databases/00222/),
    courtesy of S. Moro, R. Laureano, and P. Cortez, Using Data Mining for Bank Direct
    Marketing: An Application of the CRISP-DM Methodology. The dataset contains data
    such as age, job title, marital status, education, if s/he is a defaulter, bank
    balance, housing, if borrower loaned from the bank, and so on, about the customer
    of the bank in a CSV format. A sample of the dataset is given as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，一旦你创建了笔记本，你就可以直接在代码部分编写Spark代码。对于这个简单的示例，我们将使用银行数据集，它是公开可用的用于研究，并可以从[https://archive.ics.uci.edu/ml/machine-learning-databases/00222/](https://archive.ics.uci.edu/ml/machine-learning-databases/00222/)下载，感谢S.
    Moro、R. Laureano和P. Cortez提供的《使用数据挖掘进行银行直销：CRISP-DM方法的应用》。该数据集包含了诸如年龄、工作职位、婚姻状况、教育程度、是否为违约者、银行余额、住房情况、借款人是否从银行借款等客户信息，数据格式为CSV。数据集的样例如下所示：
- en: '![](img/00094.jpeg)**Figure 5**: A sample of the bank dataset'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00094.jpeg)**图 5**：银行数据集示例'
- en: 'Now, let''s first load the data on the Zeppelin notebook:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们首先在Zeppelin笔记本中加载数据：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Upon the execution of this line of code, create a new paragraph and name it
    as the data ingestion paragraph:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此行代码后，创建一个新段落，并将其命名为数据摄取段落：
- en: '![](img/00098.jpeg)**Figure 6**: Data ingesting paragraph'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00098.jpeg)**图 6**：数据摄取段落'
- en: If you see the preceding image carefully, the code worked and we did not need
    to define the Spark context. The reason is that it is already defined there as
    `sc`. You don't even need to define Scala implicitly. We will see an example of
    this later.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察前面的图片，你会发现代码已经生效，并且我们无需定义Spark上下文。原因是它已经在那里定义为`sc`。你甚至不需要隐式地定义Scala。我们稍后将看到一个示例。
- en: Data processing and visualization
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处理与可视化
- en: 'Now, let''s create a case class that will tell us how to pick the selected
    fields from the dataset:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个案例类，它将告诉我们如何从数据集中选择字段：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, split each line, filter out the header (starts with `age`), and map it
    into the `Bank` case class, as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，拆分每一行，过滤掉标题（以`age`开头），并将其映射到`Bank`案例类，如下所示：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, convert to DataFrame and create a temporal table:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，转换为 DataFrame 并创建临时表：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following screenshot shows that all the code snippets were executed successfully
    without showing any errors:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了所有代码片段已成功执行，没有出现任何错误：
- en: '![](img/00102.jpeg)**Figure 7**: Data process paragraph'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00102.jpeg)**图 7**：数据处理段落'
- en: 'To make it more transparent, let''s see the status marked in green color (in
    the top-right corner of the image), as follows, after the code has been executed
    for each case:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其更透明，让我们查看在代码执行每个案例后，右上角（绿色标记）的状态，如下所示：
- en: '![](img/00114.jpeg)**Figure 8**: A successful execution of Spark code in each
    paragraph'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00114.jpeg)**图 8**：每个段落成功执行 Spark 代码的结果'
- en: 'Now let''s load some data to play with the following SQL command:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载一些数据，通过以下 SQL 命令进行操作：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that the preceding line of code is a pure SQL statement that selects the
    names of all the customers whose age is greater than or equal to 45 (that is,
    age distribution). Finally, it counts the number for the same customer group.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上面那行代码是一个纯 SQL 语句，选择所有年龄大于或等于 45 岁的客户的名字（即，年龄分布）。最后，它会统计同一客户组的数量。
- en: 'Now let''s see how the preceding SQL statement works on the temp view (that
    is, `bank`):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看前面的 SQL 语句如何作用于临时视图（即`bank`）：
- en: '![](img/00126.jpeg)**Figure 9**: SQL query that selects the names of all the
    customers with age distribution [Tabular]'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00126.jpeg)**图 9**：SQL 查询，选择所有客户的名字和年龄分布 [表格]'
- en: Now you can select graph options, such as histogram, pie-chart, bar chart, and
    so on, from the tab near the table icon (in the result section). For example,
    using histogram, you can see the corresponding count for `age group >=45`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以从表格图标旁边的选项卡中选择图形选项，例如直方图、饼图、条形图等（在结果部分）。例如，使用直方图，您可以查看`年龄组 >=45`的对应计数。
- en: '![](img/00092.jpeg)**Figure 10**: SQL query that selects the names of all the
    customers with age distribution [Histogram]'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00092.jpeg)**图 10**：SQL 查询，选择所有客户的名字和年龄分布 [直方图]'
- en: 'This is how it looks using a pie-chart:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用饼图的效果：
- en: '![](img/00328.jpeg)**Figure 11**: SQL query that selects the names all the
    customers with age distribution [pie-chart]'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00328.jpeg)**图 11**：SQL 查询，选择所有客户的名字和年龄分布 [饼图]'
- en: Fantastic! We are now almost ready to do more complex data analytics problems
    using Zeppelin.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们现在几乎准备好使用 Zeppelin 进行更复杂的数据分析问题了。
- en: Complex data analytics with Zeppelin
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Zeppelin 进行复杂数据分析
- en: In this section, we will see how to perform more complex analytics using Zeppelin.
    At first, we will formalize the problem, and then, will explore the dataset that
    will be used. Finally, we will apply some visual analytics and machine learning
    techniques.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何使用 Zeppelin 执行更复杂的分析。首先，我们将形式化问题，然后探索将要使用的数据集。最后，我们将应用一些可视化分析和机器学习技术。
- en: The problem definition
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题定义
- en: 'In this section, we will build a spam classifier for classifying the raw text
    as spam or ham. We will also show how to evaluate such a model. We will try to
    focus using and working with the DataFrame API. In the end, the spam classifier
    model will help you distinguish between spam and ham messages. The following image
    shows a conceptual view of two messages (spam and ham respectively):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个垃圾邮件分类器，将原始文本分类为垃圾邮件或正常邮件。我们还将展示如何评估这样的模型。我们将尝试重点使用并与 DataFrame
    API 一起工作。最后，垃圾邮件分类器模型将帮助你区分垃圾邮件和正常邮件。以下图片展示了两个消息（分别为垃圾邮件和正常邮件）的概念视图：
- en: '[![](img/00333.jpeg)](https://blog.codecentric.de/files/2016/06/ham-vs-spam.png)**Figure
    12**: Spam and Ham example'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/00333.jpeg)](https://blog.codecentric.de/files/2016/06/ham-vs-spam.png)**图
    12**：垃圾邮件与正常邮件示例'
- en: We power some basic machine learning techniques to build and evaluate such a
    classifier for this kind of problem. In particular, the logistic regression algorithm
    will be used for this problem.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用一些基本的机器学习技术，构建并评估这种问题的分类器。特别地，将使用逻辑回归算法来解决这个问题。
- en: Dataset descripting and exploration
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集描述与探索
- en: 'The spam data set that we downloaded from [https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)
    consists of 5,564 SMS, which have been classified by hand as either ham or spam.
    Only 13.4% of these SMSes are spam. This means that the dataset is skewed and
    provides only a few examples of spam. This is something to keep in mind, as it
    can introduce bias while training models:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 [https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)
    下载的垃圾短信数据集包含 5,564 条短信，这些短信经过人工分类为正常短信（ham）或垃圾短信（spam）。其中只有 13.4% 的短信是垃圾短信。这意味着数据集是偏斜的，只有少量的垃圾短信样本。需要注意的是，这可能在训练模型时引入偏差：
- en: '![](img/00336.jpeg)**Figure 13**: A snap of the SMS dataset'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00336.jpeg)**图 13**：SMS 数据集快照'
- en: So, what does this data look like? As you might have seen, social media text
    can really get dirty, containing slang words, misspelled words, missing whitespaces,
    abbreviated words, such as *u*, *urs*, *yrs*, and so on, and, often, a violation
    of grammar rules. It sometimes even contains trivial words in the messages. Thus,
    we need to take care of these issues as well. In the following steps, we will
    encounter these issues for a better interpretation of the analytics.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些数据看起来怎么样呢？正如你可能看到的，社交媒体文本常常很“脏”，包含俚语、拼写错误、缺失的空格、缩写词，例如 *u*、*urs*、*yrs*
    等等，并且经常违反语法规则。有时，消息中甚至包含琐碎的词语。因此，我们也需要处理这些问题。在接下来的步骤中，我们将遇到这些问题，以便更好地解读分析结果。
- en: '**Step 1\. Load the required packages and APIs on Zeppelin** - Let''s load
    the required packages and APIs and create the first paragraph, before we ingest
    the dataset on Zeppelin:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1\. 在 Zeppelin 上加载所需的包和 API** - 在我们将数据集导入 Zeppelin 之前，让我们加载所需的包和 API，并创建第一个段落：'
- en: '![](img/00345.jpeg)**Figure 14**: Package/APIs load paragraph'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00345.jpeg)**图 14**：包/API 加载段落'
- en: '**Step 2\. Load and parse the dataset** - We''ll use the CSV parsing library
    by Databricks (that is, `com.databricks.spark.csv`) to read the data into the
    DataFrame:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2\. 加载并解析数据集** - 我们将使用 Databricks 提供的 CSV 解析库（即 `com.databricks.spark.csv`）将数据读取到
    DataFrame 中：'
- en: '![](img/00351.jpeg)**Figure 15**: Data ingesting/load paragraph'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00351.jpeg)**图 15**：数据导入/加载段落'
- en: '**Step 3\. Using** `StringIndexer` **to create numeric labels** - Since the
    labels in the original DataFrame are categorical, we will have to convert them
    back so that we can feed them to or use them in the machine learning models:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3\. 使用** `StringIndexer` **创建数值标签** - 由于原始 DataFrame 中的标签是分类的，我们需要将其转换为数值类型，以便可以将其输入到机器学习模型中：'
- en: '![](img/00357.jpeg)**Figure 16**: The StringIndexer paragraph, and the output
    shows the raw labels, original texts, and corresponding labels.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00357.jpeg)**图 16**：StringIndexer 段落，输出显示了原始标签、原始文本和对应的标签。'
- en: '**Step 4\. Using** `RegexTokenizer` **to create a bag of words** - We''ll use
    `RegexTokenizer` to remove unwanted words and create a bag of words:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4\. 使用** `RegexTokenizer` **创建词袋** - 我们将使用 `RegexTokenizer` 来删除不需要的词语，并创建一个词袋：'
- en: '![](img/00363.jpeg)**Figure 17**: The RegexTokenizer paragraph, and the output
    shows the raw labels, original texts, corresponding labels, and tokens'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00363.jpeg)**图 17**：RegexTokenizer 段落，输出显示了原始标签、原始文本、对应的标签和标记'
- en: '**Step 5\. Removing stop words and creating a filtered** **DataFrame** - We''ll
    remove stop words and create a filtered DataFrame for visual analytics. Finally,
    we show the DataFrame:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5\. 去除停用词并创建过滤后的** **DataFrame** - 我们将去除停用词并创建一个过滤后的 DataFrame 以便进行可视化分析。最后，我们展示该
    DataFrame：'
- en: '![](img/00329.jpeg)**Figure 18**: StopWordsRemover paragraph and the output
    shows the raw labels, original texts, corresponding labels, tokens, and filtered
    tokens without the stop words'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00329.jpeg)**图 18**：StopWordsRemover 段落，输出显示了原始标签、原始文本、对应标签、标记和去除停用词后的标记'
- en: '**Step 6\. Finding spam messages/words and their frequency** - Let''s try to
    create a DataFrame containing only the spam words, along with their respective
    frequency, to understand the context of the messages in the dataset. We can create
    a paragraph on Zeppelin:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 6\. 查找垃圾短信/词语及其频率** - 让我们尝试创建一个 DataFrame，其中只包含垃圾词及其各自的频率，以便理解数据集中消息的上下文。我们可以在
    Zeppelin 上创建一个段落：'
- en: '![](img/00349.jpeg)**Figure 19**: Spam tokens with a frequency paragraph'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00349.jpeg)**图 19**：带有频率的垃圾词标记段落'
- en: 'Now, let''s see them in the graph using SQL queries. The following query selects
    all the tokens with frequencies of more than 100\. Then, we sort the tokens in
    a descending order of their frequency. Finally, we use the dynamic forms to limit
    the number of records. The first one is just a raw tabular format:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用SQL查询在图表中查看它们。以下查询选择所有频率大于100的标记。然后，我们按频率的降序排列标记。最后，我们使用动态表单限制记录数。第一个只是一个原始的表格格式：
- en: '![](img/00128.jpeg)**Figure 20**: Spam tokens with a frequency visualization
    paragraph [Tabular]'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00128.jpeg)**图 20**：垃圾邮件标记及其频率可视化段落 [表格]'
- en: 'Then, we''ll use a bar diagram, which provides more visual insights. We can
    now see that the most frequent words in the spam messages are call and free, with
    a frequency of 355 and 224 respectively:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用条形图，它提供了更多的视觉洞察。现在我们可以看到，垃圾邮件中最频繁的单词是“call”和“free”，它们的频率分别为355和224：
- en: '![](img/00096.jpeg)**Figure 21**: Spam tokens with a frequency visualization
    paragraph [Histogram]'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00096.jpeg)**图 21**：垃圾邮件标记及其频率可视化段落 [直方图]'
- en: 'Finally, using the pie chart provides much better and wider visibility, especially
    if you specify the column range:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用饼图提供了更好的可视化效果，特别是当你指定列范围时：
- en: '![](img/00145.jpeg)**Figure 22**: Spam tokens with a frequency visualization
    paragraph [Pie chart]'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00145.jpeg)**图 22**：垃圾邮件标记及其频率可视化段落 [饼图]'
- en: '**Step 7\. Using HashingTF for term frequency** - Use `HashingTF` to generate
    the term frequency of each filtered token, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 7. 使用HashingTF进行词频** - 使用`HashingTF`生成每个过滤标记的词频，如下所示：'
- en: '![](img/00251.jpeg)**Figure 23**: HashingTF paragraph, and the output shows
    the raw labels, original texts, corresponding labels, tokens, filtered tokens,
    and corresponding term-frequency for each row'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00251.jpeg)**图 23**：HashingTF段落，输出显示原始标签、原始文本、对应标签、标记、过滤后的标记和每行的对应词频'
- en: '**Step 8\. Using IDF for Term frequency-inverse document frequency (TF-IDF)**
    - TF-IDF is a feature vectorization method widely used in text mining to reflect
    the importance of a term to a document in the corpus:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 8. 使用IDF进行词频-逆文档频率（TF-IDF）** - TF-IDF是一种广泛用于文本挖掘的特征向量化方法，用于反映一个术语在语料库中对文档的重要性：'
- en: '![](img/00085.jpeg)**Figure 24**: IDF paragraph, and the output shows the raw
    labels, original texts, corresponding labels, tokens, filtered tokens, term-frequency,
    and the corresponding IDFs for each row'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00085.jpeg)**图 24**：IDF段落，输出显示原始标签、原始文本、对应标签、标记、过滤后的标记、词频和每行的对应IDF'
- en: '**Bag of words:** The bag of words assigns a value of `1` for every occurrence
    of a word in a sentence. This is probably not ideal, as each category of the sentence,
    most likely, has the same frequency of *the*, *and*, and other words; whereas
    words such as *viagra* and *sale* probably should have an increased importance
    in figuring out whether or not the text is spam.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**词袋模型：** 词袋模型为每个单词在句子中的出现分配`1`的值。这可能并不理想，因为句子中的每个类别通常会有相同频率的*the*、*and*等词，而像*viagra*和*sale*这样的词可能在判断文本是否为垃圾邮件时应该具有更高的权重。'
- en: '**TF-IDF:** This is the acronym for Text Frequency – Inverse Document Frequency.
    This term is essentially the product of text frequency and inverse document frequency
    for each word. This is commonly used in the bag of words methodology in NLP or
    text analytics.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**TF-IDF：** 这是“文本频率 - 逆文档频率”的缩写。这个术语本质上是每个单词的文本频率和逆文档频率的乘积。它通常用于自然语言处理（NLP）或文本分析中的词袋方法。'
- en: '**Using TF-IDF:** Let''s take a look at word frequency. Here, we consider the
    frequency of a word in an individual entry, that is, term. The purpose of calculating
    text frequency (TF) is to find terms that appear to be important in each entry.
    However, words such as *the* and *and* may appear very frequently in every entry.
    We want to downweigh the importance of these words, so we can imagine that multiplying
    the preceding TF by the inverse of the whole document frequency might help find
    important words. However, since a collection of texts (a corpus) may be quite
    large, it is common to take the logarithm of the inverse document frequency. In
    short, we can imagine that high values of TF-IDF might indicate words that are
    very important to determining what a document is about. Creating the TF-IDF vectors
    requires us to load all the text into memory and count the occurrences of each
    word before we can start training our model.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 TF-IDF：** 让我们看看词频。这里我们考虑的是单个条目中的词频，即术语。计算文本频率（TF）的目的是找出在每个条目中似乎很重要的术语。然而，像
    *the* 和 *and* 这样的词在每个条目中可能会非常频繁地出现。我们希望减少这些词的权重，因此我们可以想象，将前面的 TF 乘以文档频率的倒数可能有助于找出重要的词语。然而，由于文本集合（语料库）可能非常庞大，通常会对倒数文档频率取对数。简而言之，我们可以想象，TF-IDF
    的高值可能表示在确定文档内容时非常重要的词语。创建 TF-IDF 向量需要将所有文本加载到内存中，并在开始训练模型之前计算每个单词的出现次数。'
- en: '**Step 9\. Using VectorAssembler to generate raw features for the Spark ML
    pipeline** - As you saw in the previous step, we have only the filtered tokens,
    labels, TF, and IDF. However, there are no associated features that can be fed
    into any ML models. Thus, we need to use the Spark VectorAssembler API to create
    features based on the properties in the previous DataFrame, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 9. 使用 VectorAssembler 为 Spark ML 管道生成原始特征** - 如你在上一步中看到的，我们只有过滤后的标记、标签、TF
    和 IDF。然而，当前没有可以输入任何 ML 模型的相关特征。因此，我们需要使用 Spark 的 VectorAssembler API，基于前面 DataFrame
    中的属性来创建特征，具体如下：'
- en: '![](img/00101.jpeg)**Figure 25**: The VectorAssembler paragraph that shows
    using VectorAssembler for feature creations'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00101.jpeg)**图 25**：使用 VectorAssembler 进行特征创建的段落'
- en: '**Step 10\. Preparing the training and test set** - Now we need to prepare
    the training and test set. The training set will be used to train the Logistic
    Regression model in Step 11*,* and the test set will be used to evaluate the model
    in Step 12\. Here, I make it 75% for the training and 25% for the test. You can
    adjust it accordingly:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 10. 准备训练集和测试集** - 现在我们需要准备训练集和测试集。训练集将用于在步骤 11 中训练逻辑回归模型，而测试集将用于在步骤 12
    中评估模型。在这里，我将训练集比例设为 75%，测试集比例设为 25%。你可以根据需要调整：'
- en: '![](img/00117.jpeg)**Figure 26**: Preparing training/test set paragraph'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00117.jpeg)**图 26**：准备训练/测试集段落'
- en: '**Step 11\. Training binary logistic regression model** - Since, the problem
    itself is a binary classification problem, we can use a binary logistic regression
    classifier, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 11. 训练二分类逻辑回归模型** - 由于这个问题本身是一个二分类问题，我们可以使用二分类逻辑回归分类器，具体如下：'
- en: '![](img/00133.jpeg)**Figure 27**: LogisticRegression paragraph that shows how
    to train the logistic regression classifier with the necessary labels, features,
    regression parameters, elastic net param, and maximum iterations'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00133.jpeg)**图 27**：逻辑回归段落，展示了如何使用必要的标签、特征、回归参数、弹性网参数和最大迭代次数来训练逻辑回归分类器'
- en: Note that, here, for better results, we have iterated the training for 200 times.
    We have set the regression parameter and elastic net params a very low -i.e. 0.0001
    for making the training more intensive.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了获得更好的结果，我们将训练迭代了 200 次。我们将回归参数和弹性网参数设置得非常低——即 0.0001，以使训练更加密集。
- en: '**Step 12\. Model evaluation** - Let''s compute the raw prediction for the
    test set. Then, we instantiate the raw prediction using the binary classifier
    evaluator, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 12. 模型评估** - 让我们计算测试集的原始预测结果。然后，我们使用二分类评估器来实例化原始预测，具体如下：'
- en: '**![](img/00335.jpeg)****Figure 28**: Model evaluator paragraph'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/00335.jpeg)****图 28**：模型评估器段落'
- en: 'Now let''s compute the accuracy of the model for the test set, as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算模型在测试集上的准确度，具体如下：
- en: '![](img/00346.jpeg)**Figure 29**: Accuracy calculation paragraph'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00346.jpeg)**图 29**：准确度计算段落'
- en: 'This is pretty impressive. However, if you were to go with the model tuning
    using cross-validation, for example, you could gain even higher accuracy. Finally,
    we will compute the confusion matrix to get more insight:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当令人印象深刻。然而，如果你选择使用交叉验证进行模型调优，例如，你可能会获得更高的准确度。最后，我们将计算混淆矩阵以获得更多的洞察：
- en: '![](img/00350.jpeg)**Figure 30**: Confusion paragraph shows the number of correct
    and incorrect predictions summarized with count values and broken down by each
    class'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00350.jpeg)**图 30**：混淆矩阵段落展示了正确和错误预测的数量，通过每个类别进行汇总和拆解'
- en: Data and results collaborating
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据和结果协作
- en: 'Furthermore, Apache Zeppelin provides a feature for publishing your notebook
    paragraph results. Using this feature, you can show the Zeppelin notebook paragraph
    results on your own website. It''s very straightforward; just use the `<iframe>`
    tag on your page. If you want to share the link of your Zeppelin notebook, the
    first step to publish your paragraph result is Copy a paragraph link. After running
    a paragraph in your Zeppelin notebook, click the gear button located on the right-hand.
    Then, click Link this paragraph in the menu, as shown in the following image:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Apache Zeppelin 提供了一个发布笔记本段落结果的功能。使用这个功能，你可以在自己的网站上展示 Zeppelin 笔记本段落的结果。这非常简单；只需在你的页面上使用
    `<iframe>` 标签。如果你想分享 Zeppelin 笔记本的链接，发布段落结果的第一步是复制段落链接。在 Zeppelin 笔记本中运行一个段落后，点击右侧的齿轮按钮。然后，在菜单中点击“Link
    this paragraph”选项，如下图所示：
- en: '![](img/00355.jpeg)**Figure 31**: Linking the paragraph'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00355.jpeg)**图 31**：链接段落'
- en: 'Then, just copy the provided link, as shown here:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，只需复制提供的链接，如下所示：
- en: '![](img/00358.jpeg)**Figure 32**: Getting the link for paragraph sharing with
    collaborators'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00358.jpeg)**图 32**：获取段落链接以便与合作者共享'
- en: 'Now, even if you want to publish the copied paragraph, you may use the `<iframe>`
    tag on your website. Here is an example:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，即使你想发布已复制的段落，也可以在自己的网站上使用 `<iframe>` 标签。以下是一个示例：
- en: '[PRE8]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, you can show off your beautiful visualization results on your website.
    This is more or less the end of our data analytics journey with Apache Zeppelin.
    For more inforamtion and related updates, you should visit the official website
    of Apache Zeppelin at [https://zeppelin.apache.org/](https://zeppelin.apache.org/);
    you can even subscribe to Zeppelin users at [users-subscribe@zeppelin.apache.org](mailto:users-subscribe@zeppelin.apache.org).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以在自己的网站上展示漂亮的可视化结果。这基本上标志着我们在 Apache Zeppelin 中的数据分析旅程的结束。有关更多信息和相关更新，请访问
    Apache Zeppelin 的官方网站 [https://zeppelin.apache.org/](https://zeppelin.apache.org/)；你甚至可以通过
    [users-subscribe@zeppelin.apache.org](mailto:users-subscribe@zeppelin.apache.org)
    订阅 Zeppelin 用户。
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Apache Zeppelin is a web-based notebook that enables you to do data analytics
    in an interactive way. Using Zeppelin, you can make beautiful data-driven, interactive,
    and collaborative documents with SQL, Scala, and more. It is gaining more popularity
    by the day, since more features are being added to recent releases. However, due
    to page limitations, and to make you more focused on using Spark only, we have
    shown examples that are only suitable for using Spark with Scala. However, you
    can write your Spark code in Python and test your notebook with similar ease.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Zeppelin 是一个基于 Web 的笔记本，能够以交互式方式进行数据分析。使用 Zeppelin，你可以制作美观的数据驱动、交互式和协作型文档，支持
    SQL、Scala 等语言。随着更多新特性不断加入到最近的版本中，它正日益受到欢迎。然而，由于页面限制，并且为了让你更专注于仅使用 Spark，我们展示的示例仅适用于使用
    Spark 和 Scala。然而，你也可以用 Python 编写 Spark 代码，并且可以同样轻松地测试你的笔记本。
- en: In this chapter, we discussed how to use Apache Zeppelin for large-scale data
    analytics using Spark in the backend as the interpreter. We saw how to install
    and get started with Zeppelin. We then saw how to ingest your data and parse and
    analyse it for better visibility. Then, we saw how to visualize it for better
    insights. Finally, we saw how to share the Zeppelin notebook with collaborators.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何使用 Apache Zeppelin 进行大规模数据分析，Spark 在后台作为解释器。我们了解了如何安装并开始使用 Zeppelin。接着，我们看到如何获取数据并解析分析，以便更好地可视化。然后，我们看到如何通过可视化提供更深入的洞察。最后，我们了解了如何与合作者共享
    Zeppelin 笔记本。
