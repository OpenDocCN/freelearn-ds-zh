- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Advanced Sequential Modeling Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级顺序建模算法
- en: An algorithm is a sequence of instructions that, if followed, will solve a problem.
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 算法是一个指令序列，按照该序列执行可以解决一个问题。
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Unknown
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —未知
- en: In the last chapter we looked into the core principles of sequential models.
    It provided an introductory overview of their techniques and methodologies. The
    sequential modeling algorithms discussed in the last chapter had two basic restrictions.
    First, the output sequence was required to have the same number of elements as
    the input sequence. Second, those algorithms can process only one element of an
    input sequence at a time. If the input sequence is a sentence, it means that the
    sequential algorithms discussed so far can “*attend*,” or process, only one word
    at a time. To be able to better mimic the processing capabilities of the human
    brain, we need much more than that. We need complex sequential models that process
    an output with different lengths to the input, and which can attend to more than
    one word of a sentence at the same time, removing this information bottleneck.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了顺序模型的核心原理。它提供了这些技术和方法的入门概述。上一章讨论的顺序建模算法有两个基本限制。首先，输出序列必须与输入序列有相同数量的元素。其次，这些算法一次只能处理输入序列中的一个元素。如果输入序列是一句句子，那么到目前为止讨论的顺序算法只能一次“*关注*”或处理一个单词。为了更好地模拟人脑的处理能力，我们需要的不仅仅是这些。我们需要复杂的顺序模型，能够处理长度与输入不同的输出，并且能够同时关注句子中的多个单词，从而打破这一信息瓶颈。
- en: In this chapter, we will delve deeper into the advanced aspects of sequential
    models to understand the creation of complex configurations. We’ll start by breaking
    down key elements, such as autoencoders and **Sequence-to-Sequence** (**Seq2Seq**)
    models. Next, we will look at attention mechanisms and transformers, which are
    pivotal in the development of **Large Language Models** (**LLMs**), which we will
    then study.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将更深入地探讨顺序模型的高级内容，了解如何创建复杂的配置。我们将从分解关键元素开始，例如自编码器和**序列到序列**（**Seq2Seq**）模型。接下来，我们将介绍注意力机制和变换器，它们在**大语言模型**（**LLMs**）的发展中起着至关重要的作用，随后我们将对其进行学习。
- en: By the end of this chapter, you will have gained a comprehensive understanding
    of these advanced structures and their significance in the realm of machine learning.
    We will also provide insights into the practical applications of these models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将全面了解这些高级结构及其在机器学习领域的重要性。我们还将提供这些模型的实际应用的深入见解。
- en: 'The following topics are covered in this chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Introduction to autoencoders
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器介绍
- en: Seq2Seq models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seq2Seq模型
- en: Attention mechanisms
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制
- en: ­Transformers
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器
- en: LLMs
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大语言模型
- en: Deep and wide architectures
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度和广度架构
- en: First, let’s explore an overview of advanced sequential models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们概述一下高级顺序模型。
- en: The evolution of advanced sequential modeling techniques
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级顺序建模技术的演变
- en: In *Chapter 10*, *Understanding Sequential Models*, we touched upon the foundational
    aspects of sequential models. While they serve numerous use cases, they face challenges
    in grasping and producing the complex intricacies of human language.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第10章*，《*理解顺序模型*》中，我们讨论了顺序模型的基础知识。尽管它们有许多应用场景，但在理解和生成复杂的人类语言细节方面仍然面临挑战。
- en: We’ll begin our journey by discussing **autoencoders**. Introduced in the early
    2010s, autoencoders provided a refreshing approach to data representation. They
    marked a significant evolution in **natural language processing** (**NLP**), transforming
    how we thought about data encoding and decoding. But the momentum in NLP didn’t
    stop there. By the mid-2010s, **Seq2Seq** models entered the scene, bringing forth
    innovative methodologies for tasks such as language translation. These models
    could adeptly transform one sequence form into another, heralding an era of advanced
    sequence processing.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从讨论**自编码器**开始。自编码器是在2010年代初期提出的，为数据表示提供了一种新颖的方式。它们在**自然语言处理**（**NLP**）中标志着一个重要的演变，彻底改变了我们对数据编码和解码的思考方式。但NLP的进展并未止步于此。到2010年代中期，**Seq2Seq**模型开始出现，带来了用于任务（如语言翻译）的一些创新方法。这些模型能够巧妙地将一种序列形式转化为另一种，开启了先进序列处理的新时代。
- en: However, with the rise in data complexity, the NLP community felt the need for
    more sophisticated tools. This led to the 2015 unveiling of the **attention mechanism**.
    This elegant solution provided models the ability to selectively concentrate on
    specific portions of input data, enabling them to manage longer sequences more
    efficiently. Essentially, it allowed models to weigh the importance of different
    data segments, amplifying the relevant and diminishing the less pertinent.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着数据复杂度的增加，NLP社区感受到了对更精密工具的需求。这促成了2015年**注意力机制**的发布。这个优雅的解决方案赋予了模型选择性地关注输入数据特定部分的能力，使其能更高效地处理更长的序列。实质上，它允许模型对不同数据段的重要性进行加权，从而放大相关信息，减小不相关信息。
- en: Building on this foundation, 2017 saw the advent of the **transformer** architecture.
    Fully leveraging the capabilities of attention mechanisms, transformers set new
    benchmarks in NLP.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，2017年迎来了**变换器**架构的出现。充分利用注意力机制的能力，变换器在NLP中树立了新的标杆。
- en: These advancements culminated in the development of **Large Language Models**
    (**LLMs**). Trained on vast and diverse textual data, LLMs can both understand
    and generate nuanced human language expressions. Their unparalleled prowess is
    evident in their widespread applications, from healthcare diagnostics to algorithmic
    trading in finance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些进展最终促成了**大语言模型**（**LLMs**）的发展。经过海量且多样化文本数据的训练，LLMs能够理解并生成细腻的人类语言表达。它们无与伦比的能力在广泛应用中得到了体现，从医疗诊断到金融中的算法交易。
- en: In the sections that follow, we’ll unpack the intricacies of autoencoders—from
    their early beginnings to their central role in today’s advanced sequential models.
    Prepare to deep dive into the mechanisms, applications, and evolutions of these
    transformative tools.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将深入探讨自动编码器的复杂性——从它们的早期起源到今天在先进序列模型中的核心作用。准备好深入了解这些变革性工具的机制、应用和演变吧。
- en: Exploring autoencoders
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索自动编码器
- en: Autoencoders occupy a unique niche in the landscape of neural network architectures,
    playing a pivotal role in the narrative of advanced sequential models. Essentially,
    an autoencoder is designed to create a network where the output mirrors its input,
    implying a compression of the input data into a more succinct, lower-dimensional
    latent representation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器在神经网络架构的领域中占据了独特的地位，在高级序列模型的叙事中扮演着关键角色。基本上，自动编码器旨在创建一个输出与输入相似的网络，意味着将输入数据压缩成更简洁、低维的潜在表示。
- en: 'The autoencoder structure can be conceptualized as a dual-phase process: the
    **encoding** phase and the **decoding** phase.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器结构可以被概念化为一个双阶段过程：**编码**阶段和**解码**阶段。
- en: 'Consider the following diagram:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下图示：
- en: '![A picture containing text, clock  Description automatically generated](img/B18046_11_01.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, clock  Description automatically generated](img/B18046_11_01.png)'
- en: 'Figure 11.1: Autoencoder architecture'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1：自动编码器架构
- en: 'In this diagram we make the following assumptions:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图示中，我们做出以下假设：
- en: '*x* corresponds to the input data'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x* 对应于输入数据'
- en: '*h* is the compressed form of our data'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h* 是我们数据的压缩形式'
- en: '*r* denotes the output, a recreation or approximation of *x*'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*r* 表示输出，即 *x* 的重建或近似值'
- en: 'We can see that the two phases are represented by *f* and *g*. Let’s look at
    them in more detail:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这两个阶段分别由 *f* 和 *g* 表示。让我们更详细地看一下它们：
- en: '**Encoding** (*f*): Described mathematically as *h* = *f*(*x*). In this stage,
    the input, represented as *x*, transforms into a condensed, hidden representation
    labeled *h*.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码**（*f*）：用数学形式表示为 *h* = *f*(*x*)。在此阶段，输入数据* x *被转化为一个简化的、隐藏的表示，称为 *h*。'
- en: '**Decoding** (*g*): During this phase, represented as *r* = *g*(*h*), the compacted
    *h* is unraveled, aiming to reproduce the initial input.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码**（*g*）：在此阶段，用 *r* = *g*(*h*) 表示，紧凑的 *h* 被展开，旨在重建最初的输入。'
- en: When training an autoencoder, the goal is to perfect *h*, ensuring it encapsulates
    the essence of the input data. In achieving a high-quality *h*, we ensure the
    recreated output *r* mirrors the original *x* with minimal loss. The objective
    is not just to reproduce but also to train an *h* that’s streamlined and efficient
    in this reproduction task.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练自动编码器时，目标是完善 *h*，确保它能 encapsulate 输入数据的本质。在实现高质量的 *h* 时，我们确保重建的输出 *r* 能尽量少的损失地再现原始的
    *x*。目标不仅是重建，还要训练出一个精简且高效的 *h*，以完成这个重建任务。
- en: Coding an autoencoder
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写自动编码器
- en: 'The **Modified National Institute of Standards and Technology** (**MNIST**)
    dataset is a renowned database of handwritten digits, consisting of 28x28 pixel
    grayscale images representing numbers from 0 to 9\. It has been widely used as
    a benchmark for machine learning algorithms. More information and access to the
    dataset can be found at the official MNIST website. For those interested in accessing
    the dataset, it’s available at the official MNIST repository hosted by Yann LeCun:
    [yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/). Please note that
    an account may be required to download the dataset.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**美国国家标准与技术研究所**（**MNIST**）数据集是一个著名的手写数字数据库，包含28x28像素的灰度图像，表示从0到9的数字。它已广泛用作机器学习算法的基准。更多信息以及数据集的访问可以通过官方MNIST网站获得。对于有兴趣访问数据集的人，它可以在Yann
    LeCun主办的官方MNIST存储库中找到：[yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)。请注意，下载数据集可能需要创建账户。'
- en: 'In this section, we’ll employ an autoencoder to reproduce these handwritten
    digits. The unique feature of autoencoders is their training mechanism: the *input*
    and the *target output* are the same image. Let’s break this down.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用自编码器重建这些手写数字。自编码器的独特之处在于其训练机制：*输入*和*目标输出*是相同的图像。我们来详细解析一下。
- en: 'First, there is the **training** **phase**, where the following steps occur:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是**训练**阶段，期间将进行以下步骤：
- en: The MNIST images are provided to the autoencoder.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MNIST图像被提供给自编码器。
- en: The encoder segment compresses these images into a condensed latent representation.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器部分将这些图像压缩成浓缩的潜在表示。
- en: The decoder segment then tries to restore the original image from this representation.
    By iterating over this process, the autoencoder acquires the nuances of compressing
    and reconstructing, capturing the core patterns of the handwritten digits.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器部分随后会尝试从这个表示中恢复原始图像。通过反复迭代这个过程，自编码器掌握了压缩和重构的细节，捕捉到手写数字的核心模式。
- en: 'Second, there is the **reconstruction phase**:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是**重构阶段**：
- en: With the model trained, when we feed it new images of handwritten digits, the
    autoencoder will first encode them into its internal representation.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练好的模型在接收到新的手写数字图像时，自编码器会首先将其编码成内部表示。
- en: Then, decoding this representation will yield a reconstructed image, which,
    if the training was successful, should closely match the original.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，解码这个表示将得到一个重构的图像，如果训练成功，它应该与原始图像非常相似。
- en: With the autoencoder effectively trained on the MNIST dataset, it becomes a
    powerful tool to process and reconstruct handwritten digit images.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNIST数据集上有效训练后的自编码器，成为一个强大的工具，用来处理和重构手写数字图像。
- en: Setting up the environment
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境设置
- en: 'Before diving into the code, essential libraries must be imported. TensorFlow
    will be our primary tool, but for data handling, libraries like NumPy may be pivotal:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入代码之前，必须导入必要的库。TensorFlow将是我们的主要工具，但在数据处理方面，像NumPy这样的库可能至关重要：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Data preparation
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Next, we’ll segregate the dataset into training and test segments and then
    normalize them:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把数据集划分为训练集和测试集，然后对它们进行归一化处理：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that the division by `255.0` is to normalize our grayscale image data,
    a step that optimizes the learning process.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`255.0`的除法操作是为了对灰度图像数据进行归一化，这是优化学习过程的一个步骤。
- en: Model architecture
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型架构
- en: 'Designing the autoencoder involves making decisions about the layers, their
    sizes, and activation functions. Here, the model is defined using TensorFlow’s
    `Sequential` and `Dense` classes:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 设计自编码器涉及到关于层次、尺寸和激活函数的决策。在这里，模型是通过TensorFlow的`Sequential`和`Dense`类定义的：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Flattening the 28x28 images gives us a 1D array of 784 elements, hence the input
    shape.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将28x28的图像展平后，我们得到一个包含784个元素的一维数组，因此输入的形状为该数组的形状。
- en: Compilation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编译
- en: 'After the model is defined, it’s compiled with a specified loss function and
    optimizer. Binary cross-entropy is chosen due to the binary nature of our grayscale
    images:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 模型定义完成后，将使用指定的损失函数和优化器进行编译。由于我们的灰度图像是二值性质，因此选择了二元交叉熵作为损失函数：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Training
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练
- en: 'The training phase is initiated with the `fit` method. Here, the model learns
    the nuances of the MNIST handwritten digits:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 训练阶段通过`fit`方法启动。在这里，模型学习MNIST手写数字的细节：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Prediction
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测
- en: 'With a trained model, predictions (both encoding and decoding) can be executed
    as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练好的模型，可以执行以下预测操作（包括编码和解码）：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Visualization
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化
- en: 'Let us now visually compare the original images with their reconstructed counterparts.
    The following script showcases a visualization procedure that displays two rows
    of images:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们直观地比较原始图像与其重建后的对比图像。以下脚本展示了一个可视化过程，显示了两排图像：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following screenshot shows the outputted reconstructed images:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了输出的重建图像：
- en: '![A black square with white numbers  Description automatically generated](img/B18046_11_02.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![A black square with white numbers  Description automatically generated](img/B18046_11_02.png)'
- en: 'Figure 11.2: The original test images (top row) and the post-reconstruction
    by the autoencoder (bottom row)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：原始测试图像（上排）和自编码器重建后的图像（下排）
- en: The top row presents the original test images, while the bottom row exhibits
    the post-reconstruction images made by the autoencoder. Through this side-by-side
    comparison, we can discern the efficacy of our model in preserving the intrinsic
    features of the input.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最上排展示了原始测试图像，而下排展示了通过自编码器重建后的图像。通过这种并排比较，我们可以辨别出模型在保持输入的内在特征方面的有效性。
- en: Let us now discuss Seq2Seq models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论Seq2Seq模型。
- en: Understanding the Seq2Seq model
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Seq2Seq模型
- en: 'Following our exploration of autoencoders, another groundbreaking architecture
    in the realm of advanced sequential models is the **Seq2Seq** model. Central to
    many state-of-the-art natural language processing tasks, the Seq2Seq model exhibits
    a unique capability: transforming an input sequence into an output sequence that
    may differ in length. This flexibility allows it to excel in challenges like machine
    translation, where the source and target sentences can naturally differ in size.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索自编码器之后，另一个在高级序列模型领域中的突破性架构是**Seq2Seq**模型。Seq2Seq模型在许多最先进的自然语言处理任务中占据重要地位，展现了一种独特的能力：将输入序列转换为可能在长度上不同的输出序列。这种灵活性使其在诸如机器翻译等挑战中表现出色，因为源语言和目标语言的句子长度可以自然地不同。
- en: 'Refer to *Figure 11.3*, which visualizes the core components of a Seq2Seq model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅*图11.3*，它展示了Seq2Seq模型的核心组成部分：
- en: '![Diagram  Description automatically generated](img/B18046_11_03.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B18046_11_03.png)'
- en: 'Figure 11.3: Illustration of the Seq2Seq model architecture'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：Seq2Seq模型架构示意图
- en: 'Broadly, there are three main elements:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，有三个主要元素：
- en: '**Encoder**: Processes the input sequence'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：处理输入序列'
- en: '**Thought vector**: A bridge between the encoder and decoder'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**思维向量**：编码器和解码器之间的桥梁'
- en: '**Decoder**: Generates the output sequence'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：生成输出序列'
- en: Let us explore them one by one.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一个一个地探索它们。
- en: Encoder
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器
- en: 'The encoder is shown as ![Icon  Description automatically generated](img/B18046_11_09.png)
    in *Figure 11.3*. As we can observe, it is an input **Recurrent Neural Network**
    (**RNN**) that processes the input sequence. The input sentence in this case is
    a three-word sentence: *Is Ottawa cold?* It can be represented as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器在*图11.3*中显示为![Icon  Description automatically generated](img/B18046_11_09.png)。正如我们所观察到的，它是一个输入**循环神经网络**（**RNN**），用于处理输入序列。此处的输入句子是一个三词句子：*Is
    Ottawa cold?* 它可以表示为：
- en: X = {x^(<1>), x^(<2>),… …., x^(<L1>)}
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: X = {x^(<1>), x^(<2>),… …., x^(<L1>)}
- en: The encoder traverses through this sequence until it encounters an **End-Of-Sentence**
    (<**EOS**>) token, indicating the conclusion of the input. It will be positioned
    at timestep *L1*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器遍历该序列，直到遇到**句子结束**（<**EOS**>）标记，表示输入的结束。它将位于时间步*L1*。
- en: Thought vector
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维向量
- en: Throughout the encoding phase, the RNN updates its hidden state, denoted by
    h^(<t>). The final hidden state, captured at the end of the sequence h^(<L1>),
    is relayed to the decoder. This final state is termed the **thought vector**,
    coined by Geoffrey Hinton in 2015\. This compact representation captures the essence
    of the input sequence. The thought vector is shown as ![Icon  Description automatically
    generated](img/B18046_11_10.png) in *Figure 11.3*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码阶段，RNN更新其隐藏状态，表示为h^(<t>)。序列结束时捕获的最终隐藏状态h^(<L1>)会传递给解码器。这个最终状态被称为**思维向量**，由Geoffrey
    Hinton于2015年提出。这个紧凑的表示捕捉了输入序列的精髓。思维向量在*图11.3*中显示为![Icon  Description automatically
    generated](img/B18046_11_10.png)。
- en: Decoder or writer
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器或写入器
- en: 'Upon completion of the encoding process, a `<GO>` token signals the decoder
    to commence. Using the encoder’s final hidden state h^(<L1>) as its initial input,
    the decoder, an output RNN, begins constructing the output sequence, Y = {y^(<1>),
    y^(<2>),… …., y^(<L2>)}. In the context of *Figure 11.3*, this output sequence
    translates to the sentence: *Yes*, *it is*.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码过程完成后，`<GO>`符号表示解码器开始工作。使用编码器的最后一个隐藏状态h^(<L1>)作为其初始输入，解码器作为输出RNN，开始构建输出序列Y
    = {y^(<1>), y^(<2>),… …., y^(<L2>)}。在*图11.3*的背景下，这个输出序列转化为句子：*是的*，*它是*。
- en: Special tokens in Seq2Seq
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Seq2Seq中的特殊符号
- en: 'While `<EOS>` and `<GO>` are essential tokens within the Seq2Seq paradigm,
    there are others worth noting:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`<EOS>`和`<GO>`是Seq2Seq范式中的重要符号，但还有一些其他值得注意的符号：
- en: '`<UNK>`: Standing for *unknown*, this token replaces infrequent words, ensuring
    the vocabulary remains manageable.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<UNK>`：代表*未知*，这个符号用来替换不常见的单词，确保词汇表保持可管理。'
- en: '`<PAD>`: Used for padding shorter sequences, this token standardizes sequence
    lengths during training, enhancing the model’s efficacy.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<PAD>`：用于填充较短的序列，这个符号在训练过程中标准化序列长度，从而提升模型的效率。'
- en: A salient feature of the Seq2Seq model is its ability to handle variable sequence
    lengths, meaning input and output sequences can inherently differ in size. This
    flexibility, combined with its sequential nature, makes Seq2Seq a pivotal architecture
    in the advanced modeling landscape, bridging our journey from autoencoders to
    more complex, nuanced sequential processing systems.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Seq2Seq模型的一个显著特点是能够处理可变序列长度，这意味着输入和输出序列在大小上本质上可以有所不同。这种灵活性，再加上其顺序特性，使得Seq2Seq成为高级建模领域中的一个重要架构，架起了从自编码器到更复杂、更精细的序列处理系统的桥梁。
- en: Having traversed the foundational realms of autoencoders and delved deep into
    the Seq2Seq models, we now need to understand the limitations of the encoder-decoder
    framework.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在经历了自编码器的基础领域并深入研究了Seq2Seq模型之后，我们现在需要理解编码器-解码器框架的局限性。
- en: The information bottleneck dilemma
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息瓶颈难题
- en: As we have learned, the heart of traditional Seq2Seq models is the thought vector,
    h^(<L1>) . This is the last hidden state from the encoder, which serves as the
    bridge to the decoder. This vector is tasked with encapsulating the entirety of
    the input sequence, *X*. The simplicity of this mechanism is both its strength
    and its weakness. This weakness is highlighted when sequences grow longer; compressing
    vast amounts of information into a fixed-size representation becomes increasingly
    formidable. This is termed the **information bottleneck**. No matter the richness
    or complexity of the input, the fixed-length memory constraint means only so much
    can be relayed from the encoder to the decoder.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所学到的，传统Seq2Seq模型的核心是思维向量h^(<L1>)。这是来自编码器的最后一个隐藏状态，作为连接编码器和解码器的桥梁。这个向量负责封装整个输入序列，*X*。该机制的简单性既是其优势也是其弱点。当序列变得更长时，这种弱点尤为突出；将大量信息压缩成固定大小的表示变得越来越困难。这被称为**信息瓶颈**。无论输入的丰富性或复杂性如何，固定长度的记忆限制意味着从编码器传递到解码器的信息量是有限的。
- en: To learn how this problem has been addressed, we need to shift our focus from
    Seq2Seq models to the attention mechanism.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解这个问题是如何被解决的，我们需要将焦点从Seq2Seq模型转移到注意力机制上。
- en: Understanding the attention mechanism
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解注意力机制
- en: 'Following the challenges presented by the fixed-length memory in traditional
    Seq2Seq models, 2014 marked a revolutionary step forward. Dzmitry Bahdanau, KyungHyun
    Cho, and Yoshua Bengio proposed a transformative solution: the **attention mechanism**.
    Unlike earlier models that tried (often in vain) to condense entire sequences
    into limited memory spaces, attention mechanisms enabled models to hone in on
    specific, relevant parts of the input sequence. Picture it as a magnifying glass
    over only the most critical data at each decoding step.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统Seq2Seq模型中的固定长度记忆带来的挑战之后，2014年标志着一次革命性的进步。Dzmitry Bahdanau、KyungHyun Cho
    和 Yoshua Bengio提出了一种变革性解决方案：**注意力机制**。与早期模型试图（常常徒劳地）将整个序列压缩到有限的内存空间不同，注意力机制使得模型能够专注于输入序列中具体且相关的部分。可以把它想象成在每个解码步骤中仅对最关键数据进行放大。
- en: What is attention in neural networks?
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络中的注意力是什么？
- en: Attention, as the adage goes, is where focus goes. In the realm of NLP and particularly
    in the training of LLMs, attention has garnered significant emphasis. Traditionally,
    neural networks processed input data in a fixed sequence, potentially missing
    out on the relevance of context. Enter attention—a mechanism that weighs the importance
    of different input data, focusing more on what’s relevant.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 正如俗话所说，注意力集中之处，就是焦点所在。在自然语言处理（NLP）领域，尤其是在大语言模型（LLM）的训练中，注意力受到了极大的关注。传统上，神经网络按固定顺序处理输入数据，可能会错过上下文的重要性。而注意力机制的引入，则是为了解决这个问题——它能够衡量不同输入数据的重要性，更加关注相关内容。
- en: Basic idea
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本概念
- en: Just as humans pay more attention to salient parts of an image or text, attention
    mechanisms allow neural models to focus on more relevant parts of the input data.
    It effectively tells the model where to “look” next.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 就像人类会将更多注意力放在图像或文本的显著部分一样，注意力机制使神经模型能够专注于输入数据中更相关的部分。它有效地指引模型下一步“看”哪里。
- en: Example
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: 'Inspired by my recent journey to Egypt, which felt like a voyage back in time,
    consider the expressive and symbolic language of ancient Egypt: hieroglyphs.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 受到我最近一次前往埃及的旅行启发，这次旅行仿佛是一次穿越时空的旅行，让我们思考古埃及的表现力和象征性语言：象形文字。
- en: Hieroglyphs were much more than mere symbols; they were an intricate fusion
    of art and language, representing multifaceted meanings. This system, with its
    vast array of symbols, exemplifies the foundational principles of attention mechanisms
    in neural networks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 象形文字不仅仅是符号；它们是艺术与语言的复杂融合，代表着多重含义。这个系统，通过其丰富的符号阵列，展示了神经网络注意力机制的基础原则。
- en: '![A group of pyramids in a desert  Description automatically generated](img/B18046_11_04.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![沙漠中的一群金字塔  描述自动生成](img/B18046_11_04.png)'
- en: 'Figure 11.4: Giza’s prominent pyramids - Khufu and Khafre, accompanied by inscriptions
    in the age-old Egyptian script, “hieroglyphs” (photographs taken by the author)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4：吉萨的著名金字塔——胡夫金字塔和哈夫拉金字塔，旁边是古老埃及象形文字的铭文，“象形文字”（照片由作者拍摄）
- en: 'For instance, an Egyptian scribe wishes to convey news about an anticipated
    grand festival by the Nile. Out of the thousands of hieroglyphs available:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，一位埃及抄写员希望传达关于尼罗河边即将举行的盛大节日的消息。在成千上万的象形文字中：
- en: '![A white symbol on a black background  Description automatically generated](img/B18046_11_11.png)
    The *Ankh* hieroglyph, symbolizing life, captures the festival’s vibrancy and
    celebratory spirit.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![黑色背景上的白色符号  描述自动生成](img/B18046_11_11.png) *安卡*象形文字，象征生命，捕捉了节日的生动气氛和庆祝精神。'
- en: '![A black pole with a number one  Description automatically generated](img/B18046_11_12.png)
    The *Was* symbol, resembling a staff, hints at authority or the Pharaoh’s pivotal
    role in the celebrations.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![带有数字一的黑色杆  描述自动生成](img/B18046_11_12.png) *瓦斯*符号，形似权杖，暗示着权威或法老在庆典中的关键角色。'
- en: '![A black zigzag lines on a white background  Description automatically generated](img/B18046_11_13.png)
    An illustration of the *Nile*, central to Egyptian culture, pinpoints the festival’s
    venue.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![白色背景上的黑色锯齿线  描述自动生成](img/B18046_11_13.png) *尼罗河*的插图，象征着埃及文化的中心，标示出节庆的举办地点。'
- en: However, to communicate the festival’s grandeur and importance, not all symbols
    would hold equal weight. The scribe would have to emphasize or repeat specific
    hieroglyphs to draw attention to the most crucial aspects of the message.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了传达节日的宏伟和重要性，并非所有符号的权重都相同。抄写员必须强调或重复特定的象形文字，以引起人们对信息最关键部分的注意。
- en: This selective emphasis is parallel to neural network attention mechanisms.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这种选择性强调与神经网络的注意力机制相似。
- en: Three key aspects of attention mechanisms
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力机制的三个关键方面
- en: 'In neural networks, especially within NLP tasks, attention mechanisms play
    a crucial role in filtering and focusing on relevant information. Here, we’ll
    distill the primary facets of attention into three main components: contextual
    relevance, symbol efficiency, and prioritized focus:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络，尤其是NLP任务中，注意力机制在过滤和聚焦相关信息方面起着至关重要的作用。在这里，我们将注意力的主要方面提炼为三个关键组成部分：上下文相关性、符号效率和优先关注：
- en: '**Contextual relevance**:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文相关性**：'
- en: '**Overview**: At its core, attention aims to allocate more importance to certain
    parts of the input data that are deemed more relevant to the task at hand.'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述**：本质上，注意力旨在将更多的重视分配给那些被认为与当前任务更相关的输入数据部分。'
- en: '**Deep dive**: Take a simple input like *“The grand Nile festival.”* In this
    context, attention mechanisms might assign higher weights to the words *“Nile”*
    and *“grand.”* This isn’t because of their general significance but due to their
    task-specific importance. Instead of treating every word or input with uniform
    importance, attention differentiates and adjusts the model’s focus based on context.'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深入探讨**：以一个简单的输入例如*“宏伟的尼罗河节”*为例。在这种情况下，注意力机制可能会给“尼罗河”和“宏伟”这两个词赋予更高的权重。这并非因为它们的普遍意义，而是由于它们在任务中的特定重要性。注意力机制并非将每个词或输入都视为同等重要，而是根据上下文区分并调整模型的聚焦点。'
- en: '**In practice**: Think of this as a spotlight. Just as a spotlight on a stage
    illuminates specific actors during crucial moments while dimming others, attention
    shines a light on specific input data that holds more contextual value.'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实际应用**：可以把这看作是一束聚光灯。就像聚光灯在舞台上照亮特定演员在关键时刻的表现，而其他演员则被暗淡处理，注意力也会把光照在更具上下文价值的输入数据上。'
- en: '**Symbol efficiency**:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**符号效率**：'
- en: '**Overview**: The ability of attention to condense vast amounts of information
    into digestible, critical segments.'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述**：注意力机制能够将大量信息浓缩成易于理解的关键片段。'
- en: '**Deep dive**: Hieroglyphs can encapsulate complex narratives or ideas within
    singular symbols. Analogously, attention mechanisms, by assigning varied weights,
    can determine which segments of the data contain maximal information and should
    be processed preferentially.'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深入探讨**：象形文字可以通过单一符号承载复杂的叙事或思想。类比地，注意力机制通过分配不同的权重，能够判断数据的哪些部分包含最多的信息，并应优先处理这些部分。'
- en: '**In practice**: Consider compressing a large document into a succinct summary.
    The summary retains only the most critical information, mirroring the function
    of attention mechanisms that extract and prioritize the most pertinent data from
    a larger input.'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实际应用**：考虑将一篇大文档压缩成简洁的摘要。摘要仅保留最关键的信息，这类似于注意力机制从更大的输入中提取并优先处理最相关的数据。'
- en: '**Prioritized focus**:'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优先聚焦**：'
- en: '**Overview**: Attention mechanisms don’t distribute their focus uniformly.
    They are designed to prioritize segments of input data based on their perceived
    relevance to the task.'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述**：注意力机制不会均匀分配它们的聚焦点。它们的设计是根据输入数据与任务的相关性来优先考虑某些片段。'
- en: '**Deep dive**: Drawing inspiration from our hieroglyph example, just as an
    Egyptian scribe might emphasize the “*Ankh*” symbol when wanting to convey the
    idea of life or celebration, attention mechanisms will adjust their focus (or
    weights) to specific parts of the input that are more relevant.'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深入探讨**：从象形文字的例子中汲取灵感，就像古埃及的抄写员在表达生命或庆祝的概念时会强调“*安卡*”符号一样，注意力机制也会根据任务的相关性调整对输入特定部分的聚焦（或权重）。'
- en: '**In practice**: It’s akin to reading a research paper. While the entire document
    holds value, one might focus more on the abstract, conclusion, or specific data
    points that align with their current research needs.'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实际应用**：这就像阅读一篇研究论文。虽然整个文档都有价值，但人们可能更关注摘要、结论或与自己当前研究需求相关的特定数据点。'
- en: Thus, the attention mechanism in neural networks emulates the selective focus
    humans naturally employ when processing information. By understanding the nuances
    of how attention prioritizes and processes data, we can better design and interpret
    neural models.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络中的注意力机制模仿了人类在处理信息时自然使用的选择性聚焦方式。通过理解注意力如何优先处理数据的细微差别，我们可以更好地设计和解读神经模型。
- en: A deeper dive into attention mechanisms
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更深入探讨注意力机制
- en: Attention mechanisms can be thought of as an evolved form of communication,
    much like hieroglyphs were in ancient times. Traditionally, an encoder sought
    to distill an entire input sequence into one encapsulating hidden state. This
    is analogous to an Egyptian scribe trying to convey an entire event using a single
    hieroglyph. While possible, it’s challenging and may not capture the event’s full
    essence.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制可以被看作是一种进化的沟通方式，就像古代的象形文字一样。传统上，编码器试图将整个输入序列浓缩成一个概括性的隐藏状态。这就像古埃及的抄写员试图通过一个单一的象形文字来表达整个事件。虽然这在理论上是可能的，但它很具挑战性，并且可能无法完整捕捉事件的全部精髓。
- en: Now, with the enhanced encoder-decoder approach, we have the luxury of generating
    a hidden state for every step, offering a richer tapestry of data for the decoder.
    But referencing every single hieroglyph (or state) at once would be chaotic, like
    a scribe using every symbol available to describe a single event by the Nile.
    That’s where attention comes in.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，采用增强的编码器-解码器方法，我们可以为每个步骤生成一个隐藏状态，从而为解码器提供更丰富的数据。但是，如果同时引用每一个象形符号（或状态），那将会是混乱的，类似于一位抄写员用所有可用的符号来描述尼罗河旁的一个事件。这就是注意力机制的作用所在。
- en: Attention allows the decoder to prioritize. Just as a scribe might focus on
    the “Ankh” hieroglyph to signify life and vitality, or the “Was” staff to represent
    power, or even depict the Nile itself to pinpoint a location, the decoder assigns
    varying weightage to each encoder state. It decides which parts of the input sequence
    (or which hieroglyphs) deserve more emphasis. Using our translation example, when
    converting “*Transformers are great!*” to “*Transformatoren sind grossartig!*”,
    the mechanism emphasizes aligning “*great*” with “*grossartig*,” ensuring the
    core sentiment remains intact.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制使解码器能够进行优先级排序。就像一位抄写员可能专注于“Ankh”符号来象征生命和活力，或者“Was”权杖代表权力，甚至描绘尼罗河本身来指示位置一样，解码器会为每个编码器状态分配不同的权重。它决定序列中的哪些部分（或哪些象形符号）值得更多的关注。以我们的翻译示例为例，当将“*Transformers
    are great!*”翻译为“*Transformatoren sind grossartig!*”时，机制会强调“*great*”与“*grossartig*”的对齐，确保核心情感不变。
- en: This selective focus, whether in neural network attention mechanisms or hieroglyphic
    storytelling, ensures precision and clarity in the conveyed message.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这种选择性聚焦，无论是在神经网络的注意力机制中，还是在象形文字叙事中，都能确保传达信息的准确性和清晰度。
- en: '![enc-dec-attn](img/B18046_11_05.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![enc-dec-attn](img/B18046_11_05.png)'
- en: 'Figure 11.5: RNNs employing an encoder-decoder structure enhanced with an attention
    mechanism'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：采用增强注意力机制的编码器-解码器结构的RNN
- en: The challenges of attention mechanisms
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力机制的挑战
- en: While incorporating attention with RNNs offers notable improvements, it’s not
    a silver bullet. One significant hurdle is computational cost. The act of transferring
    multiple hidden states from encoder to decoder demands substantial processing
    power.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管将注意力机制与RNN结合可以带来显著的改进，但这并不是万能的解决方案。其中一个重大障碍是计算成本。将多个隐藏状态从编码器传输到解码器的过程需要大量的计算能力。
- en: However, as with all technological progress, solutions continually emerge. One
    such advancement is the introduction of **self-attention**, a cornerstone of transformer
    architectures. This innovative variant refines the attention process, making it
    more efficient and scalable.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如所有技术进步一样，解决方案不断涌现。其中一个进展是**自注意力**的引入，这是变压器架构的基石。这种创新的变体优化了注意力过程，使其更加高效和可扩展。
- en: Delving into self-attention
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨自注意力机制
- en: Let’s consider again the ancient art of hieroglyphs, where symbols were chosen
    intentionally to convey complex messages. Self-attention operates in a similar
    manner, determining which parts of a sequence are vital and should be emphasized.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑古老的象形文字艺术，其中符号的选择是有意为之，用来传达复杂的消息。自注意力的运作方式类似，确定序列中哪些部分是关键并应当被强调。
- en: Illustrated in *Figure 11.6* is the beauty of integrating self-attention within
    sequential models. Think of the bottom layer, churning with bidirectional RNNs,
    as the foundational stones of a pyramid. They generate what we call the **context
    vector** (**c2**), a summary, much like a hieroglyph would for an event.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.6*展示了在顺序模型中整合自注意力的美妙之处。想象底层通过双向RNN运作，像是金字塔的基石。它们生成我们所称的**上下文向量**（**c2**），类似于象形文字对事件的总结。'
- en: Each step or word in a sequence has its **weightage**, symbolized as *α*. These
    weights interact with the context vector, emphasizing the importance of certain
    elements over others.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 序列中的每一步或每个单词都有其**权重**，用*α*表示。这些权重与上下文向量相互作用，强调某些元素的重要性，而不是其他元素。
- en: 'Imagine a scenario wherein the input *X*[k] represents a distinct sentence,
    denoted as *k*, which spans a length of *L1*. This can be mathematically articulated
    as:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个场景，其中输入*X*[k]表示一个独立的句子，记作*k*，该句子的长度为*L1*。这可以用数学方式表示为：
- en: '![](img/B18046_11_002.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_11_002.png)'
- en: 'Here, every element, ![](img/B18046_11_003.png), represents a word or token
    from sentence *k*: the superscript <t> indicates its specific position or timestep
    within that sentence.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个元素，![](img/B18046_11_003.png)，代表句子*k*中的一个单词或符号：上标<t>表示它在句子中的特定位置或时间步。
- en: Attention weights
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力权重
- en: In the realm of self-attention, attention weights play a pivotal role, acting
    like a compass pointing to which words are essential. They assign an “importance
    score” to each word when generating the context vector.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在自注意力机制中，注意力权重起着至关重要的作用，像一个指向重要单词的指南针。它们在生成上下文向量时，给每个单词分配一个“重要性分数”。
- en: 'To bring this into perspective, consider our earlier translation example: “*Transformers
    are great!*” translated to “*Transformatoren sind grossartig!*”. When focusing
    on the word “*Transformers*”, the attention weights might break down like this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这更有意义，考虑我们之前的翻译示例：“*Transformers are great!*” 翻译为 “*Transformatoren sind
    grossartig!*”。当聚焦于“*Transformers*”时，注意力权重可能是这样分布的：
- en: 'α[2,1]: Measures the relationship between “*Transformers*” and the beginning
    of the sentence. A high value here indicates that the word “*Transformers*” significantly
    relies on the beginning for its context.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: α[2,1]：衡量“*Transformers*”与句子开头之间的关系。此处的高值表示“*Transformers*”在其上下文中显著依赖句子开头。
- en: 'α[2,2]: Reflects how much “*Transformers*” emphasizes its inherent meaning.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: α[2,2]：反映了“*Transformers*”对其内在意义的强调程度。
- en: 'α[2,3] and α[2,4]: These weigh how much “*Transformers*” takes into context
    the words “*are*” and “*great!*”, respectively. High scores here mean that “*Transformers*”
    is deeply influenced by these surrounding words.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: α[2,3] 和 α[2,4]：衡量“*Transformers*”在上下文中对“*are*”和“*great!*”这两个词的依赖程度。高分值表示“*Transformers*”深受这些周围词汇的影响。
- en: During training, these attention weights are constantly adjusted and fine-tuned.
    This ongoing refinement ensures our model understands the intricate dance between
    words in a sentence, capturing both the explicit and subtle connections.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，这些注意力权重会不断地被调整和精细化。这种持续的优化确保我们的模型理解句子中单词之间错综复杂的关系，捕捉到显性和隐性之间的联系。
- en: '![A diagram of a complex structure  Description automatically generated](img/B18046_11_06.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![A diagram of a complex structure  Description automatically generated](img/B18046_11_06.png)'
- en: 'Figure 11.6: Integrating self-attention in sequential models'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6：在序列模型中整合自注意力机制
- en: Before we delve deeper into the mechanisms of self-attention, it’s vital to
    understand the key pieces that come together in *Figure 11.6*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨自注意力机制之前，理解组成*图 11.6*中的关键部分是至关重要的。
- en: 'Encoder: bidirectional RNNs'
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器：双向 RNN
- en: In the last chapter we investigated the main architectural building blocks of
    unidirectional RNN and its variants. **Bidirectional RNNs** were invented to address
    that need (Schuster and Paliwal, 1997). We also identified a deficiency in unidirectional
    RNNs, as they are only capable of carrying the context in one direction.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们研究了单向 RNN 及其变种的主要架构组件。**双向 RNN**的发明旨在解决这一需求（Schuster 和 Paliwal，1997）。我们还发现了单向
    RNN 的一个缺陷，即它们只能向一个方向传递上下文。
- en: 'For an input sequence, say *X*, the bidirectional RNN first reads it from the
    start to the end, and then from the end back to the start. This dual approach
    helps capture information based on preceding and succeeding elements. For each
    timestep, we get two hidden states: ![](img/B18046_11_004.png) for the forward
    direction and ![](img/B18046_11_005.png)for the backward one. These states are
    merged into a single one for that timestep, represented by:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个输入序列，假设是 *X*，双向 RNN 首先从头到尾读取它，然后再从尾到头读取。这种双重方法帮助捕捉基于前后元素的信息。对于每个时间步，我们得到两个隐藏状态：一个是
    ![](img/B18046_11_004.png)（正向）方向的，另一个是 ![](img/B18046_11_005.png)（反向）方向的。这些状态被合并成该时间步的单一状态，表示为：
- en: '![](img/B18046_11_006.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_11_006.png)'
- en: For instance, if ![](img/B18046_11_007.png) and ![](img/B18046_11_008.png)are
    64-dimensional vectors, the resulting h^(<t2>) is 128-dimensional. This combined
    state is a detailed representation of the sequence context from both directions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，如果 ![](img/B18046_11_007.png) 和 ![](img/B18046_11_008.png) 是64维向量，那么结果的
    h^(<t2>) 将是128维的。这个合并后的状态是来自两个方向的序列上下文的详细表示。
- en: Thought vector
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维向量
- en: The thought vector, here symbolized as C[k], is a representation of the input
    X[k]. As we have learned, its creation is an attempt to capture the sequencing
    patterns, context, and state of each element in X[k].
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 思维向量，这里用 C[k] 表示，是输入 X[k] 的一种表征。如我们所学，它的创建旨在捕捉 X[k] 中每个元素的顺序模式、上下文和状态。
- en: 'In our preceding diagram it is defined as:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们前面的图示中，它被定义为：
- en: '![](img/B18046_11_009.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_11_009.png)'
- en: Where ![](img/B18046_11_010.png) are attention weights for timestep *t* that
    are refined during training.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B18046_11_010.png) 是在训练过程中针对时间步 *t* 精炼后的注意力权重。
- en: 'Using the summation notation, it can be expressed as:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用求和符号，它可以表示为：
- en: '![](img/B18046_11_011.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_11_011.png)'
- en: 'Decoder: regular RNNs'
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器：常规的RNN
- en: '*Figure 11.5* shows the decoder connected through the thought vector to the
    encoder.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.5* 显示了通过思维向量连接的解码器与编码器。'
- en: 'The output of the decoder for a certain sentence k is represented by:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 某个句子k的解码器输出表示为：
- en: '![](img/B18046_11_012.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_11_012.png)'
- en: Note that the output has a length of *L2*, which is different from the length
    of the input sequence, which was *L1*.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出的长度是 *L2*，与输入序列的长度 *L1* 不同。
- en: Training versus inference
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练与推理
- en: 'In the training data for a certain input sequence *k*, we have the expected
    output vector representing the ground truth, which is represented by a vector
    Y[k]. This is:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个输入序列 *k* 的训练数据中，我们有一个表示地面真实的期望输出向量，这个向量用Y[k]表示。它是：
- en: '![](img/B18046_11_013.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_11_013.png)'
- en: 'At each timestep, the decoder’s RNN gets three inputs:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步，解码器的RNN接收三个输入：
- en: '![](img/B18046_11_014.png): The previous hidden state'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18046_11_014.png)：前一个隐藏状态'
- en: 'C[k]: The thought vector for sequence *k*'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C[k]：序列 *k* 的思维向量
- en: '![](img/B18046_11_015.png): The previous word in the ground truth vector Y[k]'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18046_11_015.png)：地面真实向量Y[k]中的前一个词'
- en: However, during inference, as there’s no prior ground truth available, the decoder’s
    RNN uses the prior output word, ![](img/B18046_11_016.png), instead.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在推理过程中，由于没有先前的真实值可用，解码器的RNN会使用先前的输出词，![](img/B18046_11_016.png)，代替。
- en: 'Now that we have learned how self-attention addresses the challenges faced
    by attention mechanisms and the basic operations it involves, we can move our
    attention to the next major advancement in sequential modeling: transformers.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了自注意力如何解决注意力机制面临的挑战以及它涉及的基本操作，我们可以将注意力转向序列建模中的下一个重大进展：变换器（transformers）。
- en: 'Transformers: the evolution in neural networks after self-attention'
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器：自注意力之后神经网络的演进
- en: 'Our exploration into self-attention revealed its powerful capability to reinterpret
    sequence data, providing each word with a contextual understanding based on its
    relationships with other words. This principle set the stage for an evolutionary
    leap in neural network designs: the **transformer** architecture.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对自注意力的探索揭示了它强大的能力，可以重新解释序列数据，基于与其他词的关系为每个词提供上下文理解。这个原则为神经网络设计的进化跃进奠定了基础：**变换器**架构。
- en: Introduced by the Google Brain team in their 2017 paper, *Attention is All You
    Need* ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)),
    the transformer architecture is built upon the very essence of self-attention.
    Before its advent, RNNs were the go-to. Picture RNNs as diligent librarians reading
    an English sentence to translate it into German, word by word, ensuring the context
    is relayed from one word to the next. They’re reliable for short texts but can
    stumble when sentences get too long, misplacing the essence of earlier words.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器架构由谷歌大脑团队在他们2017年的论文《*Attention is All You Need*》([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))中提出，基于自注意力的本质。变换器问世之前，RNN是首选。可以将RNN看作是勤奋的图书管理员，逐字地读取英语句子并将其翻译成德语，确保上下文从一个词传递到下一个词。它们在处理短文本时非常可靠，但当句子变得过长时，可能会发生错位，丢失早期词汇的本意。
- en: '![transformer-self-attn](img/B18046_11_07.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![transformer-self-attn](img/B18046_11_07.png)'
- en: 'Figure 11.7: Encoder-decoder architecture of the original transformer'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：原始变换器的编码器-解码器架构
- en: Transformers are a fresh approach to sequence data. Instead of a linear, word-by-word
    progression, transformers, armed with advanced attention mechanisms, comprehend
    an entire sequence in a single glance. It’s like instantly grasping the sentiment
    of a whole paragraph rather than piecing it together word by word. This holistic
    view ensures a richer, all-encompassing understanding, celebrating the nuanced
    interplay between words.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器是处理序列数据的一种全新方法。与线性逐字的进程不同，变换器借助先进的注意力机制，可以一眼理解整个序列。这就像瞬间抓住整段文字的情感，而不是逐字拼凑。这种全局视角确保了更丰富、全面的理解，庆祝了词与词之间微妙的相互作用。
- en: Self-attention is central to the transformer’s efficiency. While we touched
    upon this in the previous section, it’s worth noting how pivotal it is here. Each
    layer of the network, through self-attention, can resonate with every other part
    of the input data. As depicted in *Figure 11.7*, the transformer architecture
    employs self-attention for both its encoder and decoder segments, which then feed
    into neural networks (also known as **feedforward neural networks (FFNNs)**).
    Beyond being more trainable, this setup has catalyzed many of the recent breakthroughs
    in NLP.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力是变换器高效性的核心。虽然我们在前面已经提到过这一点，但值得注意的是它在这里的重要性。网络的每一层通过自注意力机制可以与输入数据的其他部分产生共鸣。如*图11.7*所示，变换器架构在其编码器和解码器部分都使用自注意力机制，然后这些部分再传递给神经网络（也称为**前馈神经网络（FFNNs）**）。除了更易于训练外，这一架构也促进了NLP领域的许多最新突破。
- en: 'To illustrate, consider *Ancient Egypt: An Enthralling Overview of Egyptian
    History*, by Billy Wellman. Within it, the relationships between early pharaohs
    like Ramses and Cleopatra and pyramid construction are vast and intricate. Traditional
    models might stumble with such expansive content.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，考虑*《古埃及：埃及历史的迷人概述》*，作者比利·韦尔曼。在书中，像拉美西斯和克娄巴特拉这样的早期法老与金字塔建设之间的关系既庞大又复杂。传统模型可能在面对如此庞大的内容时会遇到困难。
- en: Why transformers shine
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么变换器如此出色
- en: The transformer architecture, with its self-attention mechanism, emerges as
    a promising solution. When encountering a term like “*pyramids*,” the model can,
    using self-attention, assess its relevance to terms like “*Ramses*” or “*Cleopatra*,”
    irrespective of their position. This ability to attend to various input parts
    demonstrates why transformers are pivotal in modern NLP.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器架构凭借其自注意力机制，成为一个有前景的解决方案。当遇到像“*金字塔*”这样的术语时，模型可以通过自注意力机制评估它与“*拉美西斯*”或“*克娄巴特拉*”等术语的相关性，而不管它们的位置如何。这种对不同输入部分进行关注的能力，展示了变换器在现代NLP中的重要性。
- en: A Python code breakdown
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个Python代码解析
- en: 'Here’s a simplified version of how the self-attention mechanism can be implemented:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是自注意力机制实现的简化版本：
- en: '[PRE7]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE8]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This code is a basic representation, and the real transformer model uses a more
    optimized and detailed approach, especially when scaling for larger sequences.
    But the essence is the dynamic weighting of different words in the sequence, allowing
    the model to bring in contextual understanding.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是一个基本表示，真实的变换器模型采用了更优化且更详细的方法，尤其是在扩展到更大序列时。但本质上是动态加权序列中的不同单词，允许模型引入上下文理解。
- en: Understanding the output
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解输出
- en: The first row, `[0.09003057 1.57521038 0.57948752]`, corresponds to the weighted
    combination of the V matrix for the first word in the query (in this case, represented
    by the first row of the Q matrix). This means when our model encounters the word
    represented by this query, it will focus 9% on the first word, 57.5% on the second
    word, and 57.9% on the third word from the V matrix to derive contextual understanding.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一行，`[0.09003057 1.57521038 0.57948752]`，对应查询中第一个单词的V矩阵加权组合（在这种情况下，表示为Q矩阵的第一行）。这意味着当我们的模型遇到由这个查询表示的单词时，它会将9%的关注放在第一个单词，57.5%的关注放在第二个单词，57.9%的关注放在第三个单词，从V矩阵中提取上下文理解。
- en: The second row, `[0.86681333 0.14906291 1.10143419]`, is the attention result
    for the second word in the query. It focuses 86.6% on the first word, 14.9% on
    the second, and 110.1% on the third from the V matrix.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二行，`[0.86681333 0.14906291 1.10143419]`，是查询中第二个单词的注意力结果。它对V矩阵中第一个单词、第二个单词和第三个单词的关注度分别为86.6%、14.9%和110.1%。
- en: The third row, `[0.4223188 0.73304361 1.26695639]`, is for the third word in
    the query. It has attention weights of 42.2%, 73.3%, and 126.7%, respectively,
    for the words in the V matrix.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三行，`[0.4223188 0.73304361 1.26695639]`，对应查询中的第三个单词。它对V矩阵中各个单词的注意力权重分别为42.2%、73.3%和126.7%。
- en: Having reviewed transformers, their place in sequential modeling, their code,
    and their output, we can consider the next major development in NLP. Next, let
    us look at LLMs.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了变换器、它们在序列建模中的地位、它们的代码及其输出之后，我们可以考虑NLP领域的下一个重大进展。接下来，我们来看一下LLMs。
- en: LLMs
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs
- en: '**LLMs** are the next evolutionary step after transformers in the world of
    NLP. They’re not just beefed-up older models; they represent a quantum leap. These
    models can handle vast amounts of text data and perform tasks previously thought
    to be reserved for human minds.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLMs**是NLP领域中继变换器（transformers）之后的下一个进化步骤。它们不仅仅是增强版的旧模型；它们代表了一个飞跃。这些模型可以处理大量文本数据，并执行曾经被认为只有人类大脑才能完成的任务。'
- en: Simply put, LLMs can produce text, answer questions, and even code. Picture
    chatting with software and it replying just like a human, catching subtle hints
    and recalling earlier parts of the conversation. That’s what LLMs offer.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，LLMs可以生成文本、回答问题，甚至编写代码。想象一下和软件聊天，它像人类一样回复，捕捉微妙的暗示并记得之前对话的内容。这正是LLMs所能提供的。
- en: '**Language models** (**LMs**) have always been the backbone of NLP, helping
    in tasks ranging from machine translation to more modern tasks like text classification.
    While the early LMs relied on RNNs and **Long Short-Term Memory** (**LSTM**) structures,
    today’s NLP achievements are primarily due to deep learning techniques, especially
    transformers.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言模型**（**LMs**）一直是自然语言处理（NLP）的支柱，帮助完成从机器翻译到更现代的文本分类等任务。早期的LMs依赖于RNNs和**长短期记忆**（**LSTM**）结构，而今天NLP的成就主要得益于深度学习技术，特别是transformer模型。'
- en: The hallmark of LLMs? Their capacity to read and learn from vast quantities
    of text. Training one from scratch is a serious undertaking, requiring powerful
    computers and lots of time. Depending on factors like the model’s size and the
    amount of training data—say, from giant sources like Wikipedia or the Common Crawl
    dataset—it could take weeks or even months to train an LLM.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的标志性特征？它们能够阅读并从大量文本中学习。从零开始训练一个LLM是一项艰巨的任务，需要强大的计算机和大量的时间。根据模型的大小和训练数据的量——例如来自像维基百科或Common
    Crawl数据集这样的庞大来源——训练一个LLM可能需要几周甚至几个月的时间。
- en: Dealing with long sequences is a known challenge for LLMs. Earlier models, built
    on RNNs and LSTMs, faced issues with lengthy sequences, often losing vital details,
    which hampered their performance. This is where we start to see the role of **attention**
    come into play. Attention mechanisms act as a torch, illuminating essential sections
    of long inputs. For example, in a text about car advancements, attention makes
    sure the model recognizes and focuses on the major breakthroughs, no matter where
    they appear in the text.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 处理长序列是LLMs面临的已知挑战。早期基于RNNs和LSTMs的模型在处理长序列时常常会丢失重要细节，这影响了它们的性能。这时我们开始看到**注意力**的作用。注意力机制就像一盏手电筒，照亮长输入中的重要部分。例如，在一篇关于汽车进展的文章中，注意力确保模型能够识别并聚焦于主要突破，无论这些突破出现在文本的哪里。
- en: Understanding attention in LLMs
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解LLMs中的注意力机制
- en: Attention mechanisms have become foundational in the neural network domain,
    particularly evident in LLMs. Training these mammoth models, laden with millions
    or even billions of parameters, is not a walk in the park. At their core, attention
    mechanisms are like highlighters, emphasizing key details. For instance, when
    processing a lengthy text on NLP’s evolution, LLMs can understand the overall
    theme, but attention ensures they don’t miss the critical milestones. Transformers
    utilize this attention feature, aiding LLMs in handling vast text stretches and
    ensuring **contextual** consistency.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制已经成为神经网络领域的基础，尤其在LLMs中表现得尤为突出。训练这些庞大的模型，包含数百万甚至数十亿个参数，并非易事。从本质上讲，注意力机制就像高光笔，强调关键细节。例如，在处理一篇关于NLP发展的长篇文章时，LLMs能够理解整体主题，但注意力确保它们不会错过重要的里程碑。transformer模型利用这一注意力特性，帮助LLMs处理庞大的文本片段，并确保**上下文**的一致性。
- en: For LLMs, context is everything. For example, if an LLM crafts a story starting
    with a cat, attention ensures that as the tale unfolds, the context remains. So
    instead of introducing unrelated sounds like “*barking*,” the story would naturally
    lean toward “*purring*” or “*meowing*.”
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLMs来说，**上下文**至关重要。例如，如果一个LLM编写了一个以猫为开头的故事，注意力机制确保随着故事的发展，上下文保持一致。因此，故事不会引入像“*狗吠声*”这样的无关声音，而是自然地倾向于“*猫叫声*”或“*喵喵声*”。
- en: Training an LLM resembles running a supercomputer continuously for months, purely
    to process vast text quantities. And when the initial training is done, it’s only
    the beginning. Think of it like owning a high-end vehicle—you’d need periodic
    maintenance. Similarly, LLMs need frequent updates and refinements based on new
    data.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个LLM就像是连续运行超级计算机数月，仅仅为了处理大量的文本数据。而且，当初始训练完成时，这只是个开始。可以把它想象成拥有一辆高端汽车——你需要定期进行维护。同样，LLMs也需要基于新数据进行频繁的更新和调整。
- en: Even after training an LLM, the work isn’t over. For these models to remain
    effective, they need to keep learning. Imagine teaching someone English grammar
    rules and then throwing in slang or idioms—they need to adapt to these irregularities
    for a complete understanding.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 即使训练完一个LLM，工作也远未结束。为了保持这些模型的有效性，它们需要不断学习。想象一下，教某人英语语法规则，然后再加入俚语或习语——他们需要适应这些不规则用法，才能全面理解。
- en: Highlighting a historical shift, between 2017 and 2018, there was a notable
    change in the LLM landscape. Firms, including OpenAI, began leveraging unsupervised
    pretraining, paving the way for more streamlined models for tasks, like sentiment
    analysis.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 强调一个历史性转折点，2017年至2018年间，LLM（大规模语言模型）领域发生了显著变化。包括OpenAI在内的公司开始利用无监督预训练，为情感分析等任务的更简化模型铺平了道路。
- en: 'Exploring the powerhouses of NLP: GPT and BERT'
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索NLP的强大引擎：GPT和BERT
- en: '**Universal Language Model Fine-Tuning** (**ULMFiT**) set the stage for a new
    era in NLP. This method pioneered the reuse of pre-trained LSTM models, adapting
    them to a variety of NLP tasks, which saved both computational resources and time.
    Let’s break down its process:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用语言模型微调**（**ULMFiT**）为自然语言处理（NLP）开启了新时代。这种方法开创了预训练LSTM模型的再利用，并将其适应于多种NLP任务，从而节省了计算资源和时间。让我们来分解一下它的过程：'
- en: '**Pretraining**: This is akin to teaching a child the basics of a language.
    Using extensive datasets like Wikipedia, the model learns the foundational structures
    and grammar of the language. Imagine this as equipping a student with general
    knowledge textbooks.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练**：这类似于教一个孩子语言的基础。通过使用像Wikipedia这样的广泛数据集，模型学习语言的基本结构和语法。可以把它看作是为学生提供通识教材。'
- en: '**Domain adaptation**: The model then delves into specialized areas or genres.
    If the first step was about learning grammar, this step is like introducing the
    model to different genres of literature - from thrillers to scientific journals.
    It still predicts words, but now within specific contexts.'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**领域适应**：模型然后深入到特定领域或类型。如果第一步是学习语法，那么这一步就像是将模型引入不同的文学类型——从惊悚小说到科学期刊。它仍然在预测词汇，但现在是在特定的上下文中进行。'
- en: '**Fine-tuning**: Finally, the model is honed for specific tasks, such as detecting
    emotions or sentiments in a given text. This is comparable to training a student
    to write essays or analyze texts in depth.'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调**：最后，模型被精细调整，以应对特定任务，例如检测给定文本中的情绪或情感。这相当于训练学生写作论文或深入分析文本。'
- en: '2018’s LLM pioneers: GPT and BERT'
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2018年LLM先驱：GPT和BERT
- en: '2018 saw the rise of two standout models: GPT and BERT. Let us look into them
    in more detail.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年见证了两款突出模型的崛起：GPT和BERT。让我们更详细地了解它们。
- en: Generative Pre-trained Transformer (GPT)
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成预训练变换器（GPT）
- en: Inspired by ULMFiT, GPT is a model that leans on the decoder side of the transformer
    architecture. Visualize the vastness of human literature. If traditional models
    are trained with a fixed set of books, GPT is like giving a scholar access to
    an entire library, including the BookCorpus - a rich dataset with diverse, unpublished
    books. This allows GPT to draw insights from genres ranging from fiction to history.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 受ULMFiT启发，GPT是一个依赖于变换器架构解码器部分的模型。想象人类文学的浩瀚。如果传统模型是通过固定的一组书籍来训练，那么GPT就像是给学者提供了整个图书馆的访问权限，包括BookCorpus——一个包含多样化未出版书籍的丰富数据集。这使得GPT可以从小说到历史等多种类型中汲取见解。
- en: 'Here’s an analogy: traditional models might know the plots of Shakespeare’s
    plays. GPT, with its extensive learning, would understand not only the plots but
    also the cultural context, character nuances, and the evolution of Shakespeare’s
    writing style over time.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个类比：传统模型可能知道莎士比亚剧本的情节。GPT通过广泛的学习，不仅理解情节，还能掌握文化背景、人物细微之处以及莎士比亚写作风格随时间的演变。
- en: Its focus on the decoder makes GPT a master of generating text that’s both relevant
    and coherent, like a seasoned author drafting a novel.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 它对解码器的专注使GPT成为生成既相关又连贯文本的大师，就像一位经验丰富的作者在起草小说。
- en: BERT (Bidirectional Encoder Representations from Transformers)
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BERT（双向编码器表示从变换器）
- en: BERT revamped traditional language modeling with its “masked language modeling”
    technique. Unlike models that just predict the next word in a sentence, BERT fills
    in intentionally blanked-out or “masked” words, enhancing its contextual understanding.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: BERT通过其“掩蔽语言模型”技术彻底改变了传统的语言建模。与仅预测句子中下一个单词的模型不同，BERT会填补故意空缺或“掩蔽”的词，从而增强了其对上下文的理解。
- en: Let’s put this into perspective. In a sentence like “*She went to Paris to visit
    the ___*,” conventional models might predict words that fit after “*the*,” such
    as “*museum*.” BERT, given “*She went to Paris to visit the masked*,” would aim
    to deduce that “*masked*” could be replaced by “*Eiffel Tower*,” understanding
    the broader context of a trip to Paris.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这个变化的背景。在像“*她去巴黎参观___*”这样的句子中，传统模型可能会预测符合“*the*”之后的词，比如“*博物馆*”。而BERT在遇到“*她去巴黎参观masked*”时，会试图推断出“*masked*”应该被“*埃菲尔铁塔*”替代，理解到巴黎之行的更广泛背景。
- en: BERT’s approach offers a more rounded view of language, capturing the essence
    of words based on what precedes and follows them, elevating its language comprehension
    prowess.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的方法提供了更全面的语言理解，基于前后文捕捉单词的本质，提升了其语言理解能力。
- en: The key to success when training an LLM lies in combining ‘deep’ and ‘wide’
    learning architectures. Think of the ‘deep’ part as a specialist deeply focused
    on a subject, while the ‘wide’ approach is like a jack of all trades, understanding
    a bit of everything.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大语言模型（LLM）成功的关键在于结合“深度”和“广度”学习架构。可以把“深度”部分看作是专注于某一领域的专家，而“广度”方法则像是通才，了解各个领域的基础知识。
- en: Using deep and wide models to create powerful LLMs
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用深度和广度模型来创建强大的大语言模型（LLMs）
- en: 'LLMs are intricately designed to excel at a rather specific task: predicting
    the next word in a sequence. It might seem simple at first, but to achieve this
    with high accuracy, models often draw inspiration from certain aspects of human
    learning.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型的设计非常精细，旨在在一个相对具体的任务上表现出色：预测序列中的下一个词汇。起初看似简单，但为了高精度地完成这一任务，模型往往借鉴了人类学习中的某些方面。
- en: The human brain, a marvel of nature, processes information by recognizing and
    abstracting common patterns from the surrounding environment. On top of this foundational
    understanding, humans then enhance their knowledge by memorizing specific instances
    or exceptions that don’t fit the usual patterns. Think of it as understanding
    a rule and then learning the outliers to that rule.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 人脑，作为自然界的奇迹，通过识别和抽象周围环境中的常见模式来处理信息。在此基础理解的基础上，人类还通过记忆那些不符合常规模式的特定实例或例外来增强他们的知识。可以将其理解为：首先了解一条规则，然后学习该规则的例外情况。
- en: To infuse machines with this dual-layered learning approach, we need thoughtful
    machine learning architectures. A rudimentary method might involve training models
    solely on generalized patterns, sidelining the exceptions. However, to truly excel,
    especially in tasks like predicting the next word, models must be adept at grasping
    both the common patterns and the unique exceptions that punctuate a language.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让机器具备这种双层次学习方法，我们需要深思熟虑的机器学习架构。一个初步的方法可能仅仅是基于普遍的模式来训练模型，而忽略了例外情况。然而，要真正做到卓越，尤其是在诸如预测下一个词汇这样的任务中，模型必须能够掌握既能捕捉常见模式，又能识别语言中偶尔出现的独特例外。
- en: While LLMs are not designed to fully emulate human intelligence (which is multifaceted
    and not solely about predicting sequences), they do borrow from human learning
    strategies to become proficient at their specific tasks.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大语言模型并非旨在完全模拟人类智能（人类智能是多面的，不仅仅是关于预测序列），但它们确实借鉴了人类的学习策略，以便在其特定任务上变得更加熟练。
- en: 'LLMs are designed to understand and generate language by detecting patterns
    in vast amounts of text data. Consider the following basic linguistic guidelines:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型的设计旨在通过检测大量文本数据中的模式来理解和生成语言。可以考虑以下基本的语言学准则：
- en: Ancient Egyptian hieroglyphs provide a fascinating example. In this early writing
    system, a symbol might represent a word, sound, or even a concept. For instance,
    while a single hieroglyph could denote the word “*river*,” a combination of hieroglyphs
    could convey a deeper meaning like “*the life-giving Nile River*.”
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 古埃及象形文字提供了一个有趣的例子。在这个早期的书写系统中，一个符号可能代表一个单词、一个声音，甚至是一个概念。例如，虽然一个单一的象形文字可以表示“*河流*”一词，但多个象形文字的组合可以传达更深的含义，如“*生命之源——尼罗河*”。
- en: Now, consider how questions are formed. Typically, they begin with auxiliary
    verbs. However, indirect inquiries, such as “*I wonder if the Nile will flood
    this year*” diverge from this conventional pattern.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，考虑一下问题是如何形成的。通常，问题的构建始于助动词。然而，间接的询问，例如“*我想知道尼罗河今年是否会泛滥*”，则偏离了这种常规模式。
- en: To effectively predict the next word or phrase in a sequence, LLMs must master
    both the prevailing language norms and their occasional outliers.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地预测一个序列中的下一个词汇或短语，大语言模型（LLMs）必须掌握既有的语言规范及其偶尔的例外。
- en: Bottom of Form
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表单底部
- en: Thus, combining deep and wide models (*Figure 11.8*) has been shown to improve
    the performance of models on a wide range of tasks. Deep models are characterized
    by having many hidden layers and are adept at learning complex relationships between
    input and output.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，结合深度和广度模型（*图 11.8*）已被证明能提升模型在广泛任务上的表现。深度模型的特点是拥有许多隐藏层，能够学习输入与输出之间复杂的关系。
- en: In contrast, wide models are designed to learn simple patterns in the data.
    By combining the two, it is possible to capture both the complex relationships
    and the simple patterns, leading to more robust and flexible models.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，宽度模型旨在学习数据中的简单模式。通过将两者结合，可以同时捕捉复杂关系和简单模式，从而得到更强大、更灵活的模型。
- en: '![A diagram of a network  Description automatically generated](img/B18046_11_08.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的网络图示](img/B18046_11_08.png)'
- en: 'Figure 11.8: Architecture of deep and wide models'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8：深度与宽度模型的架构
- en: Incorporating exceptions into the training process is crucial for better generalization
    of models to new and unseen data. For example, a language model that is trained
    only on data that includes one meaning of a word may struggle to recognize other
    meanings when it encounters them in new data. By incorporating exceptions, the
    model can learn to recognize multiple meanings of a word, which can improve its
    performance on a variety of NLP tasks.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中融入例外对于模型在面对新数据时更好地泛化至关重要。例如，只有在包含某个词义的数据上训练的语言模型，可能在遇到新数据时无法识别该词的其他含义。通过融入例外，模型可以学习识别一个词的多种含义，从而提升其在各种自然语言处理任务中的表现。
- en: Deep architectures are typically used for tasks that require learning complex,
    hierarchical abstract representations of data. The features that exhibit generalizable
    patterns are called dense features. When we use deep architectures to formulate
    the rules, we call it learning by generalization. To build a wide and deep network,
    we connect the sparse features directly to the output node.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 深度架构通常用于需要学习数据的复杂层次抽象表示的任务。表现出可泛化模式的特征称为密集特征。当我们使用深度架构来制定规则时，我们称之为通过泛化进行学习。为了构建一个宽深网络，我们将稀疏特征直接连接到输出节点。
- en: In the field of machine learning, combining deep and wide models has been identified
    as an important approach to building more flexible and robust models that can
    capture both complex relationships and simple patterns in data. Deep models excel
    at learning complex, hierarchical abstract representations of data, by having
    many hidden layers, where each layer processes the data and learns different features
    at different levels of abstraction. In contrast, wide models have a minimum number
    of hidden layers and are typically used for tasks that require learning simple,
    non-linear relationships in the data without creating any layer of abstraction.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，结合深度和宽度模型已被确定为构建更灵活、更强大的模型的重要方法，这些模型可以同时捕捉数据中的复杂关系和简单模式。深度模型擅长学习数据的复杂层次抽象表示，通过多个隐藏层，每一层处理数据并在不同层次上学习不同的特征。相对而言，宽度模型具有最少的隐藏层，通常用于需要学习数据中简单非线性关系的任务，而不会创建任何抽象层。
- en: Such patterns are represented through sparse features. When the wide part of
    the model has one or zero hidden layers, it can be used to memorize the examples
    and formulate exceptions. Thus, when wide architectures are used to formulate
    rules, we call it learning by **memorization**.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模式通过稀疏特征来表示。当模型的宽部分具有一个或零个隐藏层时，它可以用来记住示例并制定例外。因此，当宽架构用于制定规则时，我们称之为通过**记忆**进行学习。
- en: The deep and wide models can use the deep neural network to generalize patterns.
    Typically, this portion of the model will take lots of time to train. The wide
    partition and efforts to capture all the exceptions to these generalizations in
    real time are a part of the constant algorithmic learning process.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 深度和宽度模型可以利用深度神经网络来泛化模式。通常，这部分模型需要大量的时间进行训练。宽度部分以及在实时捕捉这些泛化的所有例外的努力，都是持续算法学习过程的一部分。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we discussed advanced sequential models, which are advanced
    techniques designed to process input sequences, especially when the length of
    output sequences may differ from that of the input. Autoencoders, a type of neural
    network architecture, are particularly adept at compressing data. They work by
    encoding input data into a smaller representation and then decoding it back to
    resemble the original input. This process can be useful in tasks like image denoising,
    where noise from an image is filtered out to produce a clearer version.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了先进的序列模型，这些技术专门用于处理输入序列，尤其是在输出序列的长度可能与输入序列不同的情况下。自编码器是一种神经网络架构，特别擅长压缩数据。它们通过将输入数据编码成更小的表示，然后再解码回来，以便与原始输入相似。这个过程可以用于图像去噪等任务，在这些任务中，图像中的噪声被过滤掉，以生成更清晰的版本。
- en: Another influential model is the Seq2Seq model. It’s designed to handle tasks
    where input and output sequences have varying lengths, making it ideal for applications
    like machine translation. However, traditional Seq2Seq models face the **information
    bottleneck** challenge, wherein the entire context of an input sequence needs
    to be captured in a single, fixed-size representation. Addressing this, the attention
    mechanism was introduced, allowing models to focus on different parts of the input
    sequence dynamically. The transformer architecture, introduced in the paper *Attention
    is All You Need*, utilizes this mechanism, revolutionizing the processing of sequence
    data. Transformers, unlike their predecessors, can attend to all positions in
    a sequence simultaneously, capturing intricate relationships within the data.
    This innovation paved the way for LLMs, which have gained prominence for their
    human-like text-generation capabilities.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有影响力的模型是 Seq2Seq 模型。它被设计用来处理输入和输出序列长度不一致的任务，使其在诸如机器翻译等应用中非常理想。然而，传统的 Seq2Seq
    模型面临着**信息瓶颈**问题，即需要将输入序列的整个上下文捕捉到一个固定大小的表示中。为了解决这个问题，引入了注意力机制，使模型能够动态地聚焦于输入序列的不同部分。论文《Attention
    is All You Need》中提出的 Transformer 架构正是利用了这一机制，彻底改变了序列数据的处理方式。与其前身不同，Transformer
    可以同时关注序列中的所有位置，从而捕捉数据中的复杂关系。这一创新为大型语言模型（LLM）的发展铺平了道路，这些模型因其类人文本生成能力而广受关注。
- en: In the next chapter we will look into how to use recommendation engines.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨如何使用推荐引擎。
- en: Learn more on Discord
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解更多内容请访问 Discord
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入本书的 Discord 社区——在这里你可以分享反馈、向作者提问，并了解新版本的发布——请扫描以下二维码：
- en: '[https://packt.link/WHLel](https://packt.link/WHLel)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/WHLel](https://packt.link/WHLel)'
- en: '![](img/QR_Code1955211820597889031.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1955211820597889031.png)'
