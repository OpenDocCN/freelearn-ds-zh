- en: Using Spark SQL with SparkR
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL与SparkR
- en: Many data scientists use R to perform exploratory data analysis, data visualization,
    data munging, data processing, and machine learning tasks. SparkR is an R package
    that enables practitioners to work with data by leveraging the Apache Spark's
    distributed processing capabilities. In this chapter, we will cover SparkR (an
    R frontend package) that leverages Spark's engine to perform data analysis at
    scale. We will also describe the key elements of SparkR's design and implementation.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据科学家使用R进行探索性数据分析、数据可视化、数据整理、数据处理和机器学习任务。SparkR是一个R包，通过利用Apache Spark的分布式处理能力，使从业者能够处理数据。在本章中，我们将介绍SparkR（一个R前端包），它利用Spark引擎进行大规模数据分析。我们还将描述SparkR设计和实现的关键要素。
- en: 'More specifically, in this chapter, you will learn the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，在本章中，您将学习以下主题：
- en: What is SparkR?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是SparkR？
- en: Understanding the SparkR architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解SparkR架构
- en: Understanding SparkR DataFrames
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解SparkR的DataFrame
- en: Using SparkR for **Exploratory Data Analysis** (**EDA**) and data munging tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SparkR进行探索性数据分析（EDA）和数据整理任务
- en: Using SparkR for data visualization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SparkR进行数据可视化
- en: Using SparkR for machine learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SparkR进行机器学习
- en: Introducing SparkR
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍SparkR
- en: R is a language and environment for statistical computing and data visualization.
    It is one of the most popular tools used by statisticians and data scientists.
    R is open source and provides a dynamic interactive environment with a rich set
    of packages and powerful visualization features. It is an interpreted language
    that includes extensive support for numerical computing, with data types for vectors,
    matrices, arrays, and libraries for performing numerical operations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: R是一种用于统计计算和数据可视化的语言和环境。它是统计学家和数据科学家使用最广泛的工具之一。R是开源的，提供了一个动态交互环境，具有丰富的包和强大的可视化功能。它是一种解释性语言，包括对数值计算的广泛支持，具有用于向量、矩阵、数组的数据类型，以及用于执行数值操作的库。
- en: R provides support for structured data processing using DataFrames. R DataFrames
    make data manipulation simpler and more convenient. However, R's dynamic design
    limits the extent of possible optimizations. Additionally, interactive data analysis
    capabilities and overall scalability are also limited, as the R runtime is single
    threaded and can only process Datasets that fit in a single machine's memory.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: R提供了对使用DataFrame进行结构化数据处理的支持。R的DataFrame使数据操作更简单、更方便。然而，R的动态设计限制了可能的优化程度。此外，交互式数据分析能力和整体可伸缩性也受到限制，因为R运行时是单线程的，只能处理适合单台机器内存的数据集。
- en: For more details on R, refer to the *R project website* at [https://www.r-project.org/about.html](https://www.r-project.org/about.html).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有关R的更多详细信息，请参阅[R项目网站](https://www.r-project.org/about.html)。
- en: SparkR addresses these shortcomings to enable data scientists to work with data
    at scale in a distributed environment. SparkR is an R package that provides a
    light-weight frontend so you can use Apache Spark from R. It combines Spark's
    distributed processing features, easy connectivity to varied data sources, and
    off-memory data structures with R's dynamic environment, interactivity, packages,
    and visualization features.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR解决了这些缺点，使数据科学家能够在分布式环境中处理大规模数据。SparkR是一个R包，提供了一个轻量级的前端，让您可以从R中使用Apache
    Spark。它结合了Spark的分布式处理功能、易于连接各种数据源的特性以及内存外数据结构，与R的动态环境、交互性、包和可视化功能。
- en: Traditionally, data scientists have been using R with other frameworks, such
    as Hadoop MapReduce, Hive, Pig, and so on. However, with SparkR, they can avoid
    using multiple big data tools and platforms, and working in multiple different
    languages to accomplish their objectives. SparkR allows them to do their work
    in R and use Spark's distributed computation model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，数据科学家一直在使用R与其他框架，如Hadoop MapReduce、Hive、Pig等。然而，有了SparkR，他们可以避免使用多个大数据工具和平台，以及在多种不同的语言中工作来实现他们的目标。SparkR使他们可以在R中进行工作，并利用Spark的分布式计算模型。
- en: SparkR interfaces are similar to R and R packages rather than the Python/Scala/Java
    interfaces we have encountered thus far. SparkR implements a distributed dataframe
    that supports operations, such as statistical computations, selection of columns,
    SQL execution, filtering rows, performs aggregations, and so on, on large Datasets.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR接口类似于R和R包，而不是我们迄今为止遇到的Python/Scala/Java接口。SparkR实现了一个分布式DataFrame，支持对大型数据集进行统计计算、列选择、SQL执行、行过滤、执行聚合等操作。
- en: SparkR supports conversion to/from local R DataFrames. The tight integration
    of SparkR with the Spark project enables SparkR to reuse other Spark modules,
    including Spark SQL, MLlib, and so on. Additionally, the Spark SQL data sources
    API enables reading input from a variety of sources, such as HDFS, HBase, Cassandra,
    and file formats, such as CSV, JSON, Parquet, Avro, and so on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR支持将本地R DataFrame转换为SparkR。SparkR与Spark项目的紧密集成使SparkR能够重用其他Spark模块，包括Spark
    SQL、MLlib等。此外，Spark SQL数据源API使其能够从各种来源读取输入，如HDFS、HBase、Cassandra，以及CSV、JSON、Parquet、Avro等文件格式。
- en: In the next section, we present a brief overview of the SparkR architecture.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将简要介绍SparkR架构。
- en: Understanding the SparkR architecture
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解SparkR架构
- en: SparkR's distributed DataFrame enables programming syntax that is familiar to
    R users. The high-level DataFrame API integrates the R API with the optimized
    SQL execution engine in Spark.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR的分布式DataFrame使编程语法对R用户来说非常熟悉。高级DataFrame API将R API与Spark中优化的SQL执行引擎集成在一起。
- en: 'SparkR''s architecture primarily consists of two components: an R to JVM binding
    on the driver that enables R programs to submit jobs to a Spark cluster and support
    for running R on the Spark executors.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR的架构主要由两个组件组成：驱动程序上的R到JVM绑定，使R程序能够向Spark集群提交作业，并支持在Spark执行器上运行R。
- en: '![](img/00223.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00223.jpeg)'
- en: SparkR's design consists of support for launching R processes on Spark executor
    machines. However, there is an overhead associated with serializing the query
    and deserializing the results after they have been computed. As the amount of
    data transferred between R and the JVM increases, these overheads can become more
    significant as well. However, caching can enable efficient interactive query processing
    in SparkR.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR的设计包括支持在Spark执行器机器上启动R进程。然而，序列化查询和在计算后反序列化结果会带来一些开销。随着在R和JVM之间传输的数据量增加，这些开销也会变得更加显著。然而，缓存可以实现在SparkR中高效的交互式查询处理。
- en: 'For a detailed description of SparkR design and implementation, refer: "SparkR:
    Scaling R Programs with Spark" by Shivaram Venkataraman1, Zongheng Yang, *et al,*
    available at: [https://cs.stanford.edu/~matei/papers/2016/sigmod_sparkr.pdf.](https://cs.stanford.edu/~matei/papers/2016/sigmod_sparkr.pdf)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '有关SparkR设计和实现的详细描述，请参阅："SparkR: Scaling R Programs with Spark" by Shivaram
    Venkataraman1, Zongheng Yang, *et al,*，可在[https://cs.stanford.edu/~matei/papers/2016/sigmod_sparkr.pdf](https://cs.stanford.edu/~matei/papers/2016/sigmod_sparkr.pdf)上找到。'
- en: In the next section, we next present an overview of the distributed DataFrame
    component of SparkR - the Spark DataFrames.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍SparkR的分布式DataFrame组件Spark DataFrames的概述。
- en: Understanding SparkR DataFrames
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解SparkR DataFrames
- en: The main component of SparkR is a distributed DataFrame called **SparkR DataFrames**.
    The Spark DataFrame API is similar to local R DataFrames but scales to large Datasets
    using Spark's execution engine and the relational query optimizer. It is a distributed
    collection of data organized into columns similar to a relational database table
    or an R DataFrame.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR的主要组件是一个名为**SparkR DataFrames**的分布式DataFrame。Spark DataFrame API类似于本地R
    DataFrames，但使用Spark的执行引擎和关系查询优化器扩展到大型数据集。它是一个分布式的数据集合，以列的形式组织，类似于关系数据库表或R DataFrame。
- en: Spark DataFrames can be created from many different data sources, such as data
    files, databases, R DataFrames, and so on. After the data is loaded, developers
    can use familiar R syntax for performing various operations, such as filtering,
    aggregations, and merges. SparkR performs a lazy evaluation on DataFrame operations.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrames可以从许多不同的数据源创建，例如数据文件、数据库、R DataFrames等。数据加载后，开发人员可以使用熟悉的R语法执行各种操作，如过滤、聚合和合并。SparkR对DataFrame操作执行延迟评估。
- en: Furthermore, SparkR supports many functions on DataFrames, including statistical
    functions.  We can also use libraries such as magrittr to chain commands. Developers
    can execute SQL queries on SparkR DataFrames using the SQL commands. Finally,
    SparkR DataFrames can be converted into a local R DataFrame by using the collect
    operator.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，SparkR支持对DataFrames进行许多函数操作，包括统计函数。我们还可以使用诸如magrittr之类的库来链接命令。开发人员可以使用SQL命令在SparkR
    DataFrames上执行SQL查询。最后，可以使用collect运算符将SparkR DataFrames转换为本地R DataFrame。
- en: In the next section, we introduce typical SparkR programming operations used
    in EDA and data munging tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍在EDA和数据整理任务中使用的典型SparkR编程操作。
- en: Using SparkR for EDA and data munging tasks
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SparkR进行EDA和数据整理任务
- en: In this section, we will use Spark SQL and SparkR for preliminary exploration
    of our Datasets. The examples in this chapter use several publically available
    Datasets to illustrate the operations and can be run in the SparkR shell.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Spark SQL和SparkR对我们的数据集进行初步探索。本章中的示例使用几个公开可用的数据集来说明操作，并且可以在SparkR
    shell中运行。
- en: The entry point into SparkR is the SparkSession. It connects the R program to
    a Spark cluster. If you are working in the SparkR shell, the SparkSession is already
    created for you.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR的入口点是SparkSession。它将R程序连接到Spark集群。如果您在SparkR shell中工作，SparkSession已经为您创建。
- en: 'At this time, start SparkR shell, as shown:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，启动SparkR shell，如下所示：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can install the required libraries, such as ggplot2, in your SparkR shell,
    as shown:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在SparkR shell中安装所需的库，例如ggplot2，如下所示：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Reading and writing Spark DataFrames
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取和写入Spark DataFrames
- en: SparkR supports operating on a variety of data sources through the Spark DataFrames
    interface. SparkR's DataFrames supports a number of methods to read input, perform
    structured data analysis, and write DataFrames to the distributed storage.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR通过Spark DataFrames接口支持对各种数据源进行操作。SparkR的DataFrames支持多种方法来读取输入，执行结构化数据分析，并将DataFrames写入分布式存储。
- en: The `read.df` method can be used for creating Spark DataFrames from a variety
    of data sources. We will need to specify the path to the input data file and the
    type of data source. The data sources API natively supports formats, such as CSV,
    JSON, and Parquet.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`read.df`方法可用于从各种数据源创建Spark DataFrames。我们需要指定输入数据文件的路径和数据源的类型。数据源API原生支持格式，如CSV、JSON和Parquet。'
- en: 'A complete list of functions can be found in the API docs available at: [http://spark.apache.org/docs/latest/api/R/](http://spark.apache.org/docs/latest/api/R/).
    For the initial set of code examples, we will use the Dataset from [Chapter 3](part0045.html#1AT9A0-e9cbc07f866e437b8aa14e841622275c),
    *Using Spark SQL for Data Exploration,* that contains data related to the direct
    marketing campaigns (phone calls) of a Portuguese banking institution.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在API文档中找到完整的函数列表：[http://spark.apache.org/docs/latest/api/R/](http://spark.apache.org/docs/latest/api/R/)。对于初始的一组代码示例，我们将使用[第3章](part0045.html#1AT9A0-e9cbc07f866e437b8aa14e841622275c)中包含与葡萄牙银行机构的直接营销活动（电话营销）相关数据的数据集。
- en: The input file is in **Comma-Separated values** (**CSV**) format contains a
    header and the fields are delimited by a semicolon. The input file can be any
    of the Spark data sources; for example, if it is JSON or Parquet format, then
    we just need to change the source parameter to `json` or `parquet`, respectively.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文件以**逗号分隔值**（**CSV**）格式呈现，包含标题，并且字段由分号分隔。输入文件可以是Spark的任何数据源；例如，如果是JSON或Parquet格式，则只需将源参数更改为`json`或`parquet`。
- en: 'We can create a `SparkDataFrame` by loading our input CSV file using `read.df`,
    as shown:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`read.df`加载输入的CSV文件来创建`SparkDataFrame`，如下所示：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Similarly, we can write the DataFrame to the distributed storage using `write.df`.
    We specify the output DataFrame name and format in the source parameter (as in
    the `read.df` function).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以使用`write.df`将DataFrame写入分布式存储。我们在source参数中指定输出DataFrame名称和格式（与`read.df`函数中一样）。
- en: 'The data sources API can be used to save the Spark DataFrames into multiple
    different file formats. For example, we can save the Spark DataFrame created in
    the previous step to a Parquet file using `write.df`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据源API可用于将Spark DataFrames保存为多种不同的文件格式。例如，我们可以使用`write.df`将上一步创建的Spark DataFrame保存为Parquet文件：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `read.df` and `write.df` functions are used to bring data from the storage
    to the workers and write data from the workers to the storage, respectively. It
    does not bring this data into the R process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`read.df`和`write.df`函数用于将数据从存储传输到工作节点，并将数据从工作节点写入存储，分别。它不会将这些数据带入R进程。'
- en: Exploring structure and contents of Spark DataFrames
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Spark DataFrames的结构和内容
- en: In this section, we explore the dimensions, schema, and data contained in the
    Spark DataFrames.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探索了Spark DataFrames中包含的维度、模式和数据。
- en: 'First, we cache the Spark DataFrame for performance using either the cache
    or the persist function. We can also specify storage level options, such as `DISK_ONLY`,
    `MEMORY_ONLY`, `MEMORY_AND_DISK`, and so on, as shown here:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用cache或persist函数对Spark DataFrame进行性能缓存。我们还可以指定存储级别选项，例如`DISK_ONLY`、`MEMORY_ONLY`、`MEMORY_AND_DISK`等，如下所示：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can list the columns and associated data types of the Spark DataFrames by
    typing the name of the DataFrame, as shown here:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过输入DataFrame的名称列出Spark DataFrames的列和关联的数据类型，如下所示：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Spark DataFrames[age:`int`, job:`string`, marital:`string`, education:`string`,
    default:`string`, housing:`string`, loan:`string`, contact:`string`, month:`string`,
    day_of_week:`string`, duration:`int`, campaign:`int`, pdays:`int`, previous:`int`,
    poutcome:`string`, emp.var.rate:`double`, cons.price.idx:`double`, cons.conf.idx:double,
    euribor3m:`double`, nr.employed:`double`, y:`string`]
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrames[age:`int`, job:`string`, marital:`string`, education:`string`,
    default:`string`, housing:`string`, loan:`string`, contact:`string`, month:`string`,
    day_of_week:`string`, duration:`int`, campaign:`int`, pdays:`int`, previous:`int`,
    poutcome:`string`, emp.var.rate:`double`, cons.price.idx:`double`, cons.conf.idx:double,
    euribor3m:`double`, nr.employed:`double`, y:`string`]
- en: 'SparkR can automatically infer the schema from the input file''s header row.
    We can print the DataFrame schema, as shown here:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR可以自动从输入文件的标题行推断模式。我们可以打印DataFrame模式，如下所示：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/00224.gif)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00224.gif)'
- en: 'We can also display the names of the columns in our DataFrame using the `names`
    function, as shown here:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`names`函数显示DataFrame中列的名称，如下所示：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/00225.gif)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00225.gif)'
- en: 'Next, we display a few sample values (from each of the columns) and records
    from the Spark DataFrame, as shown here:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们显示Spark DataFrame中的一些样本值（从每个列中）和记录，如下所示：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/00226.gif)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00226.gif)'
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/00227.gif)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00227.gif)'
- en: 'We can display the dimensions of the DataFrame, as shown. Executing dim after
    the cache or persist function with the `MEMORY_ONLY` option is a good way to ensure
    the DataFrame is loaded and kept in memory for faster operations:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以显示DataFrame的维度，如下所示。在使用`MEMORY_ONLY`选项的cache或persist函数后执行dim是确保DataFrame加载并保留在内存中以进行更快操作的好方法：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can also use the count or the `nrow` function to compute the number of rows
    in our DataFrame:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用count或`nrow`函数计算DataFrame中的行数：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Additionally, we can use the `distinct` function to get the number of distinct
    values contained in the specified column:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以使用`distinct`函数获取指定列中包含的不同值的数量：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Running basic operations on Spark DataFrames
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark DataFrames上运行基本操作
- en: 'In this section, we use SparkR to execute some basic operations on Spark DataFrames,
    including aggregations, splits, and sampling. For example, we can select columns
    of interest from the DataFrame. Here, we select only the `education` column from
    the DataFrame:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们使用SparkR在Spark DataFrames上执行一些基本操作，包括聚合、拆分和抽样。例如，我们可以从DataFrame中选择感兴趣的列。在这里，我们只选择DataFrame中的`education`列：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Alternatively, we can also specify the column name, as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以指定列名，如下所示：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can use the subset function to select rows meeting certain conditions, for
    example, rows with `marital` status `married`, as shown here:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用subset函数选择满足某些条件的行，例如，`marital`状态为`married`的行，如下所示：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/00228.gif)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00228.gif)'
- en: 'We can use the filter function to only retain rows with `education` level `basic.4y`,
    as shown here:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用filter函数仅保留`education`水平为`basic.4y`的行，如下所示：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/00229.gif)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00229.gif)'
- en: 'SparkR DataFrames support a number of common aggregations on the data after
    grouping. For example, we can compute a histogram of the `marital` status values
    in the Dataset, as follows. Here, we use the `n` operator to count the number
    of times each marital status appears:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR DataFrames支持在分组后对数据进行一些常见的聚合。例如，我们可以计算数据集中`marital`状态值的直方图，如下所示。在这里，我们使用`n`运算符来计算每个婚姻状态出现的次数：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can also sort the output from the aggregation to get the most common set
    of marital statuses, as shown here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对聚合的输出进行排序，以获取最常见的婚姻状态集，如下所示：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Next, we use the `magrittr` package to pipeline functions instead of nesting
    them, as follows.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`magrittr`包来进行函数的管道处理，而不是嵌套它们，如下所示。
- en: 'First, install the `magrittr` package using the `install.packages` command,
    if the package is not installed already:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`install.packages`命令安装`magrittr`包，如果该包尚未安装：
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note that when loading and attaching a new package in R, it is possible to
    have a name conflict, where a function is masking another function. Depending
    on the load order of the two packages, some functions from the package loaded
    first are masked by those in the package loaded after. In such cases, we need
    to prefix such calls with the package name:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在R中加载和附加新包时，可能会出现名称冲突，其中一个函数掩盖了另一个函数。根据两个包的加载顺序，先加载的包中的一些函数会被后加载的包中的函数掩盖。在这种情况下，我们需要使用包名作为前缀来调用这些函数：
- en: '[PRE21]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We pipeline the `filter`, `groupBy`, and the `summarize` functions, as shown
    in the following example:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面的示例中使用 `filter`、`groupBy` 和 `summarize` 函数进行流水线处理：
- en: '[PRE22]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/00230.gif)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00230.gif)'
- en: 'Next, we create a local DataFrame from the distributed Spark version we have
    been working with so far. We use the `collect` function to move Spark DataFrame
    to a local/R DataFrame on the Spark driver, as shown. Typically, you would summarize
    or take a sample of your data before moving it to a local DataFrame:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从迄今为止使用的分布式 Spark 版本创建一个本地 DataFrame。我们使用 `collect` 函数将 Spark DataFrame
    移动到 Spark 驱动程序上的本地/R DataFrame，如所示。通常，在将数据移动到本地 DataFrame 之前，您会对数据进行汇总或取样：
- en: '[PRE23]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can create a `sample` from our DataFrame and move it to a local DataFrame,
    as shown. Here, we take 10% of the input records and create a local DataFrame
    from it:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从我们的 DataFrame 创建一个 `sample` 并将其移动到本地 DataFrame，如下所示。在这里，我们取输入记录的 10% 并从中创建一个本地
    DataFrame：
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: SparkR also provides a number of functions that can directly be applied to the
    columns for data processing and aggregations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 还提供了许多可以直接应用于数据处理和聚合的列的函数。
- en: 'For example, we can add a new column to our DataFrame containing a new column
    with the call duration converted from seconds to minutes, as shown here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以向我们的 DataFrame 添加一个新列，该列包含从秒转换为分钟的通话持续时间，如下所示：
- en: '[PRE25]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following is the output obtained:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是获得的输出：
- en: '![](img/00231.gif)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00231.gif)'
- en: Executing SQL statements on Spark DataFrames
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Spark DataFrames 上执行 SQL 语句
- en: A Spark DataFrames can also be registered as a temporary view in Spark SQL that
    allows us to run SQL queries over its data. The `sql` function enables applications
    to run SQL queries programmatically and return the result as a Spark DataFrame.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrames 也可以在 Spark SQL 中注册为临时视图，这允许我们在其数据上运行 SQL 查询。`sql` 函数使应用程序能够以编程方式运行
    SQL 查询，并将结果作为 Spark DataFrame 返回。
- en: 'First, we register the Spark DataFrame as a temporary view:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将 Spark DataFrame 注册为临时视图：
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we execute the SQL statements using the `sql` function. For example,
    we select the `education`, `age`, `marital`, `housing`, and `loan` columns for
    customers aged between 13 and 19 years, as shown here:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `sql` 函数执行 SQL 语句。例如，我们选择年龄在 13 到 19 岁之间的客户的 `education`、`age`、`marital`、`housing`
    和 `loan` 列，如下所示：
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](img/00232.gif)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00232.gif)'
- en: Merging SparkR DataFrames
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合并 SparkR DataFrames
- en: We can explicitly specify the columns that SparkR should merge the DataFrames
    on using the operation parameters `by` and `by.x`/`by.y`. The merge operation
    determines how SparkR should merge DataFrames based on the values, `all.x` and
    `all.y`, which indicate which rows in `x` and `y` should be included in the join,
    respectively. For example, we can specify an `inner join` (default) by explicitly
    specifying `all.x = FALSE`, `all.y = FALSE`, or a left `outer join` with `all.x
    = TRUE`, `all.y = FALSE`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以明确指定 SparkR 应该使用操作参数 `by` 和 `by.x`/`by.y` 在哪些列上合并 DataFrames。合并操作根据值 `all.x`
    和 `all.y` 确定 SparkR 应该如何基于 `x` 和 `y` 的行来合并 DataFrames。例如，我们可以通过明确指定 `all.x = FALSE`，`all.y
    = FALSE` 来指定一个 `inner join`（默认），或者通过 `all.x = TRUE`，`all.y = FALSE` 来指定一个左 `outer
    join`。
- en: For more details on join and merge operations, refer to [https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/merging.md.](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/merging.md)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 join 和 merge 操作的更多详细信息，请参阅 [https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/merging.md.](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/merging.md)
- en: Alternatively, we can also use the `join` operation to merge the DataFrames
    by row.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以使用 `join` 操作按行合并 DataFrames。
- en: 'For the following example, we use the crimes Dataset available at: [https://archive.ics.uci.edu/ml/Datasets/Communities+and+Crime+Unnormalized](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们使用位于 [https://archive.ics.uci.edu/ml/Datasets/Communities+and+Crime+Unnormalized](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized)
    的犯罪数据集。
- en: 'As before, we read in the input Dataset, as shown here:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们读取输入数据集，如下所示：
- en: '[PRE28]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we select specific columns related to the type of crime and rename the
    default column names to more meaningful names, as shown here:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择与犯罪类型相关的特定列，并将默认列名重命名为更有意义的名称，如下所示：
- en: '[PRE29]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](img/00233.gif)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00233.gif)'
- en: '[PRE30]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](img/00234.gif)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00234.gif)'
- en: 'Next, we read in a Dataset containing the names of US states, as shown here:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们读取包含美国州名的数据集，如下所示：
- en: '[PRE31]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We list out the columns of the two DataFrames using the names function. The
    common column between the two DataFrames is the "code" column (containing state
    codes):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 names 函数列出了两个 DataFrames 的列。两个 DataFrames 之间的共同列是 "code" 列（包含州代码）：
- en: '[PRE32]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we perform an inner join using the common column:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用共同列执行内部连接：
- en: '[PRE33]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![](img/00235.gif)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00235.gif)'
- en: 'Here, we perform an inner join based on specifying the expression explicitly:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们根据明确指定的表达式执行内部连接：
- en: '[PRE34]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](img/00236.gif)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00236.gif)'
- en: For the following example, we use the tennis tournament match statistics Dataset
    available at: [http://archive.ics.uci.edu/ml/Datasets/Tennis+Major+Tournament+Match+Statistics.](http://archive.ics.uci.edu/ml/datasets/Tennis+Major+Tournament+Match+Statistics)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们使用位于 [http://archive.ics.uci.edu/ml/Datasets/Tennis+Major+Tournament+Match+Statistics.](http://archive.ics.uci.edu/ml/datasets/Tennis+Major+Tournament+Match+Statistics)
    的网球锦标赛比赛统计数据集。
- en: 'The following is the output obtained:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是获得的输出：
- en: '![](img/00237.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00237.jpeg)'
- en: Using User Defined Functions (UDFs)
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用用户定义函数（UDFs）
- en: In SparkR, several types of **User Defined Functions** (**UFDs**) are supported.
    For example, we can run a given function on a large Dataset using `dapply` or
    `dapplyCollect`. The `dapply` function applies a function to each partition of
    a Spark DataFrame. The output of the function should be a data.frame.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SparkR 中，支持多种类型的**用户定义函数**（**UDFs**）。例如，我们可以使用 `dapply` 或 `dapplyCollect`
    在大型数据集上运行给定的函数。`dapply` 函数将函数应用于 Spark DataFrame 的每个分区。函数的输出应该是一个 data.frame。
- en: 'The schema specifies the row format of the resulting a Spark DataFrame:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 模式指定了生成的 Spark DataFrame 的行格式：
- en: '[PRE35]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](img/00238.gif)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00238.gif)'
- en: Similar to dapply, the `dapplyCollect` function applies the function to each
    partition of a Spark DataFrames and collects the result back. The output of the
    function should be a `data.frame` and the schema parameter is not required. Note
    that `dapplyCollect` can fail if the output of UDF cannot be transferred to the
    driver or fit in the driver's memory.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于dapply，`dapplyCollect`函数将函数应用于Spark DataFrames的每个分区，并收集结果。函数的输出应为`data.frame`，不需要schema参数。请注意，如果UDF的输出无法传输到驱动程序或适合驱动程序的内存中，则`dapplyCollect`可能会失败。
- en: 'We can use `gapply` or `gapplyCollect` to run a given function on a large Dataset grouping
    by input columns. In the following example, we determine a set of top duration
    values:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`gapply`或`gapplyCollect`来对大型数据集进行分组运行给定函数。在下面的示例中，我们确定一组顶部持续时间值：
- en: '[PRE36]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](img/00239.gif)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00239.gif)'
- en: The `gapplyCollect` similarly applies a function to each partition of a Spark
    DataFrames but also collects the result back to an `R data.frame`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`gapplyCollect`类似地将一个函数应用于Spark DataFrames的每个分区，但也将结果收集回到`R data.frame`中。'
- en: In the next section, we introduce SparkR functions to compute summary statistics
    for our example Datasets.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们介绍SparkR函数来计算示例数据集的摘要统计信息。
- en: Using SparkR for computing summary statistics
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SparkR计算摘要统计信息
- en: 'The describe (or summary) operation creates a new DataFrame that contains count,
    mean, max, mean, and standard deviation values for a specified DataFrame or a
    list of numerical columns:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 描述（或摘要）操作创建一个新的DataFrame，其中包含指定DataFrame或数值列列表的计数、平均值、最大值、平均值和标准偏差值：
- en: '[PRE37]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/00240.gif)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00240.gif)'
- en: 'Computing these values on a large Dataset can be computationally expensive.
    Hence, we present the individual computation of these statistical measures here:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型数据集上计算这些值可能会非常昂贵。因此，我们在这里呈现这些统计量的单独计算：
- en: '[PRE38]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we create a DataFrame that lists the minimum and maximum values and the
    range width:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个列出最小值和最大值以及范围宽度的DataFrame：
- en: '[PRE39]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, we compute the sample variance and standard deviation as shown here:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算样本方差和标准偏差，如下所示：
- en: '[PRE40]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The operation `approxQuantile` returns approximate quantiles for a DataFrame
    column. We specify the quantiles to be approximated using the probabilities parameter
    and the acceptable error by the `relativeError` parameter. We define a new DataFrame,
    `df1`, that drops missing values for `age` and then computes the approximate `Q1`,
    `Q2`, and `Q3` values, as shown here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 操作`approxQuantile`返回DataFrame列的近似分位数。我们使用概率参数和`relativeError`参数来指定要近似的分位数。我们定义一个新的DataFrame，`df1`，删除`age`的缺失值，然后计算近似的`Q1`、`Q2`和`Q3`值，如下所示：
- en: '[PRE41]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can measure the magnitude and the direction of the skew in the distribution
    of a column by using the `skewness` operation. In the following example, we measure
    the skewness for the `age` column:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`skewness`操作来测量列分布的偏斜程度和方向。在下面的示例中，我们测量`age`列的偏斜度：
- en: '[PRE42]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Similarly, we can measure the kurtosis of a column. Here, we measure the kurtosis
    for the `age` column:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以测量列的峰度。在这里，我们测量`age`列的峰度：
- en: '[PRE43]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we compute the sample covariance and correlation between two DataFrame
    columns. Here, we compute the covariance and correlation between the `age` and
    `duration` columns:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算两个DataFrame列之间的样本协方差和相关性。在这里，我们计算`age`和`duration`列之间的协方差和相关性：
- en: '[PRE44]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Next, we create a relative frequency table for the job column. The relative
    frequency for each distinct job category value is shown in the Percentage column:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为工作列创建一个相对频率表。每个不同的工作类别值的相对频率显示在百分比列中：
- en: '[PRE45]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](img/00241.gif)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00241.gif)'
- en: 'Finally, we create a contingency table between two categorical columns with
    the operation `crosstab`. In the following example, we create a contingency table
    for the job and marital columns:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`crosstab`操作在两个分类列之间创建一个列联表。在下面的示例中，我们为工作和婚姻列创建一个列联表：
- en: '[PRE46]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![](img/00242.gif)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00242.gif)'
- en: In the next section, we use SparkR to execute various data visualization tasks.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们使用SparkR执行各种数据可视化任务。
- en: Using SparkR for data visualization
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SparkR进行数据可视化
- en: The SparkR extension of the ggplot2 package, `ggplot2.SparkR`, allows SparkR
    users to build powerful visualizations.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ggplot2包的SparkR扩展`ggplot2.SparkR`允许SparkR用户构建强大的可视化。
- en: 'In this section, we use various plots to visualize our data. Additionally,
    we also present examples of plotting of data on maps and visualizing graphs:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用各种图表来可视化我们的数据。此外，我们还展示了在地图上绘制数据和可视化图表的示例：
- en: '[PRE47]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Refer to the ggplot website for different options available to improve the displays
    of each of your plots at [http://docs.ggplot2.org](http://docs.ggplot2.org).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考ggplot网站，了解如何改进每个图表的显示的不同选项，网址为[http://docs.ggplot2.org](http://docs.ggplot2.org)。
- en: 'In the next step, we plot a basic bar graph that gives frequency counts for
    the different marital statuses in the data:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们绘制一个基本的条形图，显示数据中不同婚姻状态的频率计数：
- en: '[PRE48]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](img/00243.jpeg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00243.jpeg)'
- en: 'In the following example, we plot a histogram for the age column and several
    bar graphs that give frequency counts for the education, marital status, and job
    values:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们为年龄列绘制了一个直方图，并绘制了教育、婚姻状况和工作值的频率计数的几个条形图：
- en: '[PRE49]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](img/00244.jpeg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00244.jpeg)'
- en: 'The following expression creates a bar graph that describes the proportional
    frequency of education levels grouped over marital types:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表达式创建了一个条形图，描述了按婚姻类型分组的教育水平的比例频率：
- en: '[PRE50]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![](img/00245.jpeg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00245.jpeg)'
- en: 'The following expression plots a histogram that gives frequency counts across
    binned age values in the data:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表达式绘制了一个直方图，显示了数据中分箱的年龄值的频率计数：
- en: '[PRE51]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![](img/00246.jpeg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00246.jpeg)'
- en: 'The following expression returns a frequency polygon equivalent to the histogram
    plotted previously:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表达式返回了一个与先前绘制的直方图相当的频率多边形：
- en: '[PRE52]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![](img/00247.jpeg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00247.jpeg)'
- en: 'The following expression gives a boxplot of call duration values across types
    of marital statuses:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表达式给出了一个箱线图，描述了不同婚姻状态下通话持续时间值的比例频率：
- en: '[PRE53]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![](img/00248.jpeg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00248.jpeg)'
- en: 'The following expression facets age histograms across different levels of education:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表达式在不同教育水平上展示了年龄直方图：
- en: '[PRE54]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![](img/00249.jpeg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00249.jpeg)'
- en: 'In the following example, we show several box plots simultaneously for different
    columns:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，我们同时展示了不同列的几个箱线图：
- en: '[PRE55]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](img/00250.jpeg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00250.jpeg)'
- en: When building expressions or functions in SparkR, we should avoid computationally
    expensive operations. For example, even though the collect operation in SparkR
    allows us to leverage ggplot2 features, we should collect data as sparingly as
    possible, since we need to ensure the operation results fit into a single node's
    available memory.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建SparkR中的表达式或函数时，我们应该避免计算昂贵的操作。例如，即使在SparkR中collect操作允许我们利用ggplot2的特性，我们也应该尽量节约地收集数据，因为我们需要确保操作结果适合单个节点的可用内存。
- en: 'The problem with the following scatterplot is overplotting. The points are
    plotted on top of one another, distorting the visual appearance of the plot. We
    can adjust the value of the alpha parameter to use transparent points:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下散点图的问题是过度绘制。点被绘制在彼此之上，扭曲了图形的视觉效果。我们可以调整alpha参数的值来使用透明点：
- en: '[PRE56]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '![](img/00251.jpeg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00251.jpeg)'
- en: 'To display as a two-dimensional set of panels or to wrap the panels into multiple
    rows, we use `facet_wrap`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要将面板显示为二维面板集或将面板包装成多行，我们使用`facet_wrap`：
- en: '[PRE57]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![](img/00252.jpeg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00252.jpeg)'
- en: 'The adjusted alpha-value improves the visualization of scatterplots; however,
    we can summarize the points to average values and plot them to get a much clearer
    visualization, as shown in the following example:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的alpha值改善了散点图的可视化效果；然而，我们可以将点总结为平均值并绘制它们，以获得更清晰的可视化效果，如下例所示：
- en: '[PRE58]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '![](img/00253.jpeg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00253.jpeg)'
- en: 'In the next example, we create a Density plot and overlay a line passing through
    the mean. Density plots are a good way to view the distribution of a variable;
    for example, in our example, we plot the call duration values:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个例子中，我们创建了一个密度图，并叠加了通过平均值的线。密度图是查看变量分布的好方法；例如，在我们的例子中，我们绘制了通话持续时间的值：
- en: '[PRE59]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '![](img/00254.jpeg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00254.jpeg)'
- en: In the next section, we will present an example of plotting values on a map.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将展示一个在地图上绘制值的例子。
- en: Visualizing data on a map
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在地图上可视化数据
- en: 'In this section, we describe how to merge two data sets and plot the results
    on a map:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述如何合并两个数据集并在地图上绘制结果：
- en: '[PRE60]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The Dataset we want to visualize is the average number of arsons by state,
    as computed here:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要可视化的数据集是按州计算的平均纵火次数，如下所示：
- en: '[PRE61]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Next, we read `states.csv` Dataset into an R DataFrame:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将`states.csv`数据集读入R DataFrame：
- en: '[PRE62]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Next, we replace the state code with the state name using a `factor` variable:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`factor`变量将州代码替换为州名：
- en: '[PRE63]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'To create a map of the United States with the states colored according to the
    average number of arsons per state, we can use the `ggplot2''s map_data` function:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个美国地图，根据每个州的平均纵火次数着色，我们可以使用`ggplot2`的`map_data`函数：
- en: '[PRE64]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, we merge the Dataset with the map and use `ggplot` to display the
    map, as shown here:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将数据集与地图合并，并使用`ggplot`显示地图，如下所示：
- en: '[PRE65]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '![](img/00255.jpeg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00255.jpeg)'
- en: For more on plotting on geographical maps, refer to the *Exploring geographical
    data using SparkR and ggplot2* by Jose A. Dianes at [https://www.codementor.io/spark/tutorial/exploratory-geographical-data-using-sparkr-and-ggplot2](https://www.codementor.io/spark/tutorial/exploratory-geographical-data-using-sparkr-and-ggplot2).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在地理地图上绘图的更多信息，请参阅Jose A. Dianes的*使用SparkR和ggplot2探索地理数据* [https://www.codementor.io/spark/tutorial/exploratory-geographical-data-using-sparkr-and-ggplot2](https://www.codementor.io/spark/tutorial/exploratory-geographical-data-using-sparkr-and-ggplot2)。
- en: In the next section, we present an example of graph visualization.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将展示一个图形可视化的例子。
- en: Visualizing graph nodes and edges
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化图形节点和边缘
- en: It is key to visualize graphs to get a sense of the overall structural properties.
    In this section, we will plot several graphs in the SparkR shell.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化图形对于了解整体结构特性非常重要。在本节中，我们将在SparkR shell中绘制几个图形。
- en: For more details, refer to *Static and dynamic network visualization with R*
    by Katherine Ognynova at [http://kateto.net/network-visualization](http://kateto.net/network-visualization).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详情，请参阅Katherine Ognynova的*使用R进行静态和动态网络可视化* [http://kateto.net/network-visualization](http://kateto.net/network-visualization)。
- en: For the following example, we use a Dataset containing the network of interactions
    on the stack exchange website Ask Ubuntu available at: [https://snap.stanford.edu/data/sx-askubuntu.html](https://snap.stanford.edu/data/sx-askubuntu.html).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，我们使用一个包含在stack exchange网站Ask Ubuntu上的交互网络的数据集：[https://snap.stanford.edu/data/sx-askubuntu.html](https://snap.stanford.edu/data/sx-askubuntu.html)。
- en: 'We create a local DataFrame from ten percent sample of the data and create
    a plot of the graph, as shown here:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从数据的百分之十的样本中创建一个本地DataFrame，并创建一个图形的绘制，如下所示：
- en: '[PRE66]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '![](img/00256.jpeg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00256.jpeg)'
- en: 'We can obtain a clearer visualization in this example by reducing the sample
    size further and removing certain edges, such as loops, as shown here:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 通过进一步减少样本量并移除某些边缘，例如循环，我们可以在这个例子中获得更清晰的可视化效果，如下所示：
- en: '[PRE67]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![](img/00257.jpeg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00257.jpeg)'
- en: In the next section, we explore using SparkR for machine learning tasks.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将探讨如何使用SparkR进行机器学习任务。
- en: Using SparkR for machine learning
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SparkR进行机器学习
- en: SparkR supports a growing list of machine learning algorithms, such as **Generalized
    Linear Model** (**glm**), Naive Bayes Model, K-Means Model, Logistic Regression
    Model, **Latent Dirichlet Allocation** (**LDA**) Model, Multilayer Perceptron
    Classification Model, Gradient Boosted Tree Model for Regression and Classification,
    Random Forest Model for Regression and Classification, **Alternating Least Squares**
    (**ALS**) matrix factorization Model, and so on.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR支持越来越多的机器学习算法，例如**广义线性模型**（**glm**），朴素贝叶斯模型，K均值模型，逻辑回归模型，**潜在狄利克雷分配**（**LDA**）模型，多层感知分类模型，用于回归和分类的梯度提升树模型，用于回归和分类的随机森林模型，**交替最小二乘**（**ALS**）矩阵分解模型等等。
- en: SparkR uses Spark MLlib to train the model. The summary and predict functions
    are used to print a summary of the fitted model and make predictions on new data,
    respectively. The `write.ml`/`read.ml` operations can be used to save/load the
    fitted models. SparkR also supports a subset of the available R formula operators
    for model fitting, such as `~`, `.`, `:`, `+`, and `-`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR使用Spark MLlib来训练模型。摘要和predict函数分别用于打印拟合模型的摘要和对新数据进行预测。`write.ml`/`read.ml`操作可用于保存/加载拟合的模型。SparkR还支持一些可用的R公式运算符，如`~`、`.`、`:`、`+`和`-`。
- en: 'For the following examples, we use a wine quality Dataset available at [https://archive.ics.uci.edu/ml/Datasets/Wine+Quality](https://archive.ics.uci.edu/ml/datasets/Wine+Quality):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的示例中，我们使用[https://archive.ics.uci.edu/ml/Datasets/Wine+Quality](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)上可用的葡萄酒质量数据集：
- en: '[PRE68]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '![](img/00258.gif)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00258.gif)'
- en: '[PRE69]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We create the training and test DataFrames using the sample function, as shown
    here:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用sample函数创建训练和测试DataFrames，如下所示：
- en: '[PRE70]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Next, we fit a logistic regression model against a SparkDataFrame, as shown
    here:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对SparkDataFrame拟合逻辑回归模型，如下所示：
- en: '[PRE71]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Following, we use the summary function to print a summary of the fitted model:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用summary函数打印拟合模型的摘要：
- en: '[PRE72]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '![](img/00259.gif)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00259.gif)'
- en: 'Next, we use the predict function to make predictions on the test DataFrame:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用predict函数对测试DataFrame进行预测：
- en: '[PRE73]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '![](img/00260.gif)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00260.gif)'
- en: only showing top 5 rows
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 仅显示前5行
- en: 'Next, we count the number of mismatches between the labels and the predicted
    values:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算标签和预测值之间的不匹配数量：
- en: '[PRE74]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'In the following example, we fit a Random Forest Classification model on a
    Spark DataFrames. We then use the `summary` function to get a summary of the fitted
    Random Forest model and the `predict` function to make predictions on the test
    data, as shown here:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们在Spark DataFrames上拟合了一个随机森林分类模型。然后我们使用`summary`函数获取拟合的随机森林模型的摘要和`predict`函数对测试数据进行预测，如下所示：
- en: '[PRE75]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '![](img/00261.gif)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00261.gif)'
- en: '[PRE76]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '![](img/00262.gif)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00262.gif)'
- en: '[PRE77]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Similar to the previous examples, we fit a generalized linear model in the
    following example:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的示例类似，我们在以下示例中拟合广义线性模型：
- en: '[PRE78]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '![](img/00263.gif)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00263.gif)'
- en: '[PRE79]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '![](img/00264.gif)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00264.gif)'
- en: '[PRE80]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '![](img/00265.gif)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00265.gif)'
- en: 'Next, we present an example of clustering, where we fit a multivariate Gaussian
    mixture model against a Spark DataFrames:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍一个聚类的例子，我们将针对Spark DataFrames拟合一个多变量高斯混合模型：
- en: '[PRE81]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '![](img/00266.gif)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00266.gif)'
- en: 'Next, we perform a two-sided **Kolmogorov-Smirnov** (**KS**) test for data
    sampled from a continuous distribution. We compare the largest difference between
    the empirical cumulative distribution of the data and the theoretical distribution
    to test the null hypothesis that the sample data comes from that theoretical distribution.
    In the following example, we illustrate the test on the `fixed_acidity` column
    against the normal distribution:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对从连续分布中抽样的数据执行双侧Kolmogorov-Smirnov（KS）检验。我们比较数据的经验累积分布和理论分布之间的最大差异，以检验样本数据是否来自该理论分布的零假设。在下面的示例中，我们对`fixed_acidity`列针对正态分布进行了测试：
- en: '[PRE82]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Kolmogorov-Smirnov test summary:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: Kolmogorov-Smirnov检验摘要：
- en: '[PRE83]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Finally, in the following example, we perform distributed training of multiple
    models using `spark.lapply`. The results of all the computations must fit in a
    single machine''s memory:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在下面的示例中，我们使用`spark.lapply`进行多模型的分布式训练。所有计算的结果必须适合单台机器的内存：
- en: '[PRE84]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'We pass a read-only list of arguments for the family parameter of the generalized
    linear model, as shown here:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递一个只读参数列表用于广义线性模型的family参数，如下所示：
- en: '[PRE85]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The following statement returns a list of the model summaries:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 以下语句返回模型摘要的列表：
- en: '[PRE86]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Finally, we can print the summary of both the models, as shown here:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以打印两个模型的摘要，如下所示：
- en: '[PRE87]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The summary of model 1 is:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 模型1的摘要是：
- en: '![](img/00267.gif)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00267.gif)'
- en: 'The summary of model 2 is:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 模型2的摘要是：
- en: '![](img/00268.gif)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00268.gif)'
- en: Summary
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced SparkR. We covered SparkR architecture and SparkR
    DataFrames API. Additionally, we provided code examples for using SparkR for EDA
    and data munging tasks, data visualization, and machine learning.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了SparkR。我们涵盖了SparkR架构和SparkR DataFrames API。此外，我们提供了使用SparkR进行探索性数据分析和数据整理任务、数据可视化和机器学习的代码示例。
- en: In the next chapter, we will build Spark applications using a mix of Spark modules.
    We will present examples of applications that combine Spark SQL with Spark Streaming,
    Spark Machine Learning, and so on.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用Spark模块的混合构建Spark应用程序。我们将展示将Spark SQL与Spark Streaming、Spark机器学习等结合的应用程序示例。
