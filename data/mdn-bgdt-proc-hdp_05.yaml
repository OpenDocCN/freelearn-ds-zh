- en: Data Modeling in Hadoop
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop中的数据建模
- en: So far, we've learned how to create a Hadoop cluster and how to load data into
    it. In the previous chapter, we learned about various data ingestion tools and
    techniques. As we know by now, there are various open source tools available in
    the market, but there is a single silver bullet tool that can take on all our
    use cases. Each data ingestion tool has certain unique features; they can prove
    to be very productive and useful in typical use cases. For example, Sqoop is more
    useful when used to import and export Hadoop data from and to an RDBMS.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何创建Hadoop集群以及如何将数据加载到其中。在上一章中，我们学习了各种数据摄取工具和技术。正如我们所知，市场上有很多开源工具，但并没有一个能够应对所有用例的万能工具。每个数据摄取工具都有其独特的功能；在典型用例中，它们可以证明是非常高效和有用的。例如，Sqoop在用于从关系型数据库管理系统（RDBMS）导入和导出Hadoop数据时更为有用。
- en: 'In this chapter, we will learn how to store and model data in Hadoop clusters.
    Like data ingestion tools, there are various data stores available. These data
    stores support different data models—that is, columnar data storage, key value
    pairs, and so on; and they support various file formats, such as ORC, Parquet,
    and AVRO, and so on. There are very popular data stores, widely used in production
    these days, for example, Hive, HBase, Cassandra, and so on. We will learn more
    about the following two data stores and data modeling techniques:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何在Hadoop集群中存储和建模数据。与数据摄取工具类似，有各种数据存储可供选择。这些数据存储支持不同的数据模型——即列式数据存储、键值对等；并且支持各种文件格式，如ORC、Parquet和AVRO等。目前，有一些非常流行的数据存储，在生产环境中被广泛使用，例如Hive、HBase、Cassandra等。我们将更深入地了解以下两个数据存储和数据建模技术：
- en: Apache Hive
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Hive
- en: Apache HBase
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache HBase
- en: 'First, we will start with basic concepts and then we will learn how we can
    apply modern data modeling techniques for faster data access. In a nutshell, we
    will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从基本概念开始，然后我们将学习如何应用现代数据建模技术以实现更快的数据访问。简而言之，本章将涵盖以下主题：
- en: Apache Hive and RDBMS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Hive和关系型数据库管理系统（RDBMS）
- en: Supported datatypes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持的数据类型
- en: Hive architecture and how it works
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive架构及其工作原理
- en: Apache Hive
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Hive
- en: Hive is a data processing tool in Hadoop. As we have learned in the previous
    chapter, data ingestion tools load data and generate HDFS files in Hadoop; we
    need to query that data based on our business requirements. We can access the
    data using MapReduce programming. But data access with MapReduce is extremely
    slow. To access a few lines of HDFS files, we have to write separate mapper, reducer,
    and driver code. So, in order to avoid this complexity, Apache introduced Hive.
    Hive supports an SQL-like interface that helps access the same lines of HDFS files
    using SQL commands. Hive was initially developed by Facebook but was later taken
    over by Apache.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Hive是Hadoop中的一个数据处理工具。正如我们在上一章所学的，数据摄取工具在Hadoop中加载数据并生成HDFS文件；我们需要根据业务需求查询这些数据。我们可以使用MapReduce编程来访问数据。但是，使用MapReduce访问数据非常慢。为了访问HDFS文件中的一小部分数据，我们必须编写单独的mapper、reducer和driver代码。因此，为了避免这种复杂性，Apache引入了Hive。Hive支持类似SQL的接口，它可以帮助使用SQL命令访问相同的HDFS文件行。Hive最初由Facebook开发，但后来被Apache接管。
- en: Apache Hive and RDBMS
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Hive和RDBMS
- en: I mentioned that Hive provides an SQL-like interface. Bearing this in mind,
    the question that arises is: *is Hive the same as RDBMS on Hadoop?* The answer
    is *no*. Hive is not a database. Hive does not store any data. Hive stores table
    information as a part of metadata, which is called schema, and points to files
    on HDFS. Hive accesses data stored on HDFS files using an SQL-like interface called
    **HiveQL** (**HQL**). Hive supports SQL commands to access and modify data in
    HDFS. Hive is not a tool for OLTP. It does not provide any row-level insert, update,
    or delete. The current version of Hive (version 0.14), does support insert, update,
    and delete with full ACID properties, but that feature is not efficient. Also,
    this feature does not support all file formats. For example, the update supports
    only ORC file format. Basically, Hive is designed for batch processing and does
    not support transaction processing like RDBMS does. Hence, Hive is better suited
    for data warehouse applications for providing data summarization, query, and analysis.
    Internally, Hive SQL queries are converted into MapReduce by its compiler. Users
    need not worry about writing any complex mapper and reducer code. Hive supports
    query structured data only. It is very complex to access unstructured data using
    Hive SQL. You may have to write your own custom functions for that. Hive supports
    various file formats such as text files, sequence files, ORC, and Parquet, which
    provide significant data compression.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到Hive提供了一个类似SQL的接口。考虑到这一点，随之而来的问题是：*Hive是否与Hadoop上的RDBMS相同？* 答案是*不是*。Hive不是一个数据库。Hive不存储任何数据。Hive将表信息作为元数据的一部分存储，这被称为模式，并指向HDFS上的文件。Hive使用一个称为**HiveQL**（**HQL**）的类似SQL的接口来访问存储在HDFS文件上的数据。Hive支持SQL命令来访问和修改HDFS中的数据。Hive不是一个OLTP工具。它不提供任何行级别的插入、更新或删除。当前版本的Hive（版本0.14）支持具有完整ACID属性的插入、更新和删除，但这个功能效率不高。此外，这个功能不支持所有文件格式。例如，更新只支持ORC文件格式。基本上，Hive是为批量处理设计的，不支持像RDBMS那样的事务处理。因此，Hive更适合数据仓库应用，提供数据汇总、查询和分析。内部，Hive
    SQL查询由其编译器转换为MapReduce。用户无需担心编写任何复杂的mapper和reducer代码。Hive只支持查询结构化数据。使用Hive SQL访问非结构化数据非常复杂。你可能需要为该功能编写自己的自定义函数。Hive支持各种文件格式，如文本文件、序列文件、ORC和Parquet，这些格式提供了显著的数据压缩。
- en: Supported datatypes
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持的数据类型
- en: 'The following datatypes are supported by Hive version 0.14:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Hive版本0.14支持以下数据类型：
- en: '| **Datatype group** | **Datatype** | **Format** |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| **数据类型组** | **数据类型** | **格式** |'
- en: '| String | `STRING` | `column_name STRING` |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 字符串 | `STRING` | `column_name STRING` |'
- en: '| `VARCHAR` | `column_name VARCHAR(max_length)` |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| `VARCHAR` | `column_name VARCHAR(max_length)` |'
- en: '| `CHAR` | `column_name CHAR(length)` |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| `CHAR` | `column_name CHAR(length)` |'
- en: '| Numeric | `TINYINT` | `column_name TINYINT` |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 数值 | `TINYINT` | `column_name TINYINT` |'
- en: '| `SMALLINT` | `column_name SMALLINT` |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `SMALLINT` | `column_name SMALLINT` |'
- en: '| `INT` | `column_name INT` |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| `INT` | `column_name INT` |'
- en: '| `BIGINT` | `column_name BIGINT` |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| `BIGINT` | `column_name BIGINT` |'
- en: '| `FLOAT` | `column_name FLOAT` |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| `FLOAT` | `column_name FLOAT` |'
- en: '| `DOUBLE` | `column_name DOUBLE` |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `DOUBLE` | `column_name DOUBLE` |'
- en: '| `DECIMAL` | `column_name DECIMAL[(precision[,scale])]` |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `DECIMAL` | `column_name DECIMAL[(precision[,scale])]` |'
- en: '| Date/time type | `TIMESTAMP` | `column_name TIMESTAMP` |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 日期/时间类型 | `TIMESTAMP` | `column_name TIMESTAMP` |'
- en: '| `DATE` | `column_name DATE` |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| `DATE` | `column_name DATE` |'
- en: '| `INTERVAL` | `column_name INTERVAL year to month` |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| `INTERVAL` | `column_name INTERVAL year to month` |'
- en: '| Miscellaneous type | `BOOLEAN` | `column_name BOOLEAN` |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 杂项类型 | `BOOLEAN` | `column_name BOOLEAN` |'
- en: '| `BINARY` | `column_name BINARY` |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| `BINARY` | `column_name BINARY` |'
- en: '| Complex type | `ARRAY` | `column_name ARRAY < type >` |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 复杂数据类型 | `ARRAY` | `column_name ARRAY < type >` |'
- en: '| `MAPS` | `column_name MAP < primitive_type, type >` |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| `MAPS` | `column_name MAP < primitive_type, type >` |'
- en: '| `STRUCT` | `column_name STRUCT < name : type [COMMENT ''comment_string'']
    >` |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| `STRUCT` | `column_name STRUCT < name : type [COMMENT ''comment_string'']
    >` |'
- en: '| `UNION` | `column_name UNIONTYPE <int, double, array, string>` |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| `UNION` | `column_name UNIONTYPE <int, double, array, string>` |'
- en: How Hive works
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hive的工作原理
- en: Hive databases are comprised of tables which are made up of partitions. Data
    can be accessed via a simple query language and Hive supports overwriting or appending
    of data. Within a particular database, data in tables is serialized and each table
    has a corresponding HDFS directory. Each table can be sub-divided into partitions
    that determine how data is distributed within subdirectories of the table directory.
    Data within partitions can be further broken down into buckets.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Hive数据库由表组成，这些表由分区构成。数据可以通过简单的查询语言访问，并且Hive支持数据的覆盖或追加。在特定数据库中，表中的数据是序列化的，每个表都有一个对应的HDFS目录。每个表可以进一步细分为分区，这些分区决定了数据如何在表目录的子目录中分布。分区内的数据可以进一步细分为桶。
- en: Hive architecture
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hive架构
- en: 'The following is a representation of Hive architecture:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对Hive架构的表示：
- en: '![](img/7dcc7f8c-71f0-4b1d-b245-8e0e317aa65a.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7dcc7f8c-71f0-4b1d-b245-8e0e317aa65a.png)'
- en: 'The preceding diagram shows that Hive architecture is divided into three parts—that
    is, clients, services, and metastore. The Hive SQL is executed as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示显示Hive架构分为三个部分——即客户端、服务和元存储。Hive SQL的执行方式如下：
- en: '**Hive SQL query**: A Hive query can be submitted to the Hive server using
    one of these ways: WebUI, JDBC/ODBC application, and Hive CLI. For a thrift-based
    application, it will provide a thrift client for communication.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hive SQL查询**：可以使用以下方式之一将Hive查询提交给Hive服务器：WebUI、JDBC/ODBC应用程序和Hive CLI。对于基于thrift的应用程序，它将提供一个thrift客户端用于通信。'
- en: '**Query execution**: Once the Hive server receives the query, it is compiled,
    converted into an optimized query plan for better performance, and converted into
    a MapReduce job. During this process, the Hive Server interacts with the metastore
    for query metadata.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询执行**：一旦Hive服务器接收到查询，它就会被编译，转换为优化查询计划以提高性能，并转换为MapReduce作业。在这个过程中，Hive服务器与元存储交互以获取查询元数据。'
- en: '**Job execution**: The MapReduce job is executed on the Hadoop cluster.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作业执行**：MapReduce作业在Hadoop集群上执行。'
- en: Hive data model management
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hive数据模型管理
- en: 'Hive handles data in the following four ways:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Hive以以下四种方式处理数据：
- en: Hive tables
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive表
- en: Hive table partition
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive表分区
- en: Hive partition bucketing
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive分区桶
- en: Hive views
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive视图
- en: We will see each one of them in detail in the following sections.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几节中详细探讨每一个。
- en: Hive tables
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hive表
- en: 'A Hive table is very similar to any RDBMS table. The table is divided into
    rows and columns. Each column (field) is defined with a proper name and datatype.
    We have already seen all the available datatypes in Hive in the *Supported datatypes*
    section. A Hive table is divided into two types:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Hive表与任何RDBMS表非常相似。表被分为行和列。每个列（字段）都使用适当的名称和数据类型进行定义。我们已经在*支持的数据类型*部分中看到了Hive中所有可用的数据类型。Hive表分为两种类型：
- en: Managed tables
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理表
- en: External tables
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部表
- en: We will learn about both of these types in the following sections.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几节中学习这两种类型。
- en: Managed tables
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理表
- en: 'The following is a sample command to define a Hive managed table:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个定义Hive管理表的示例命令：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When the preceding query is executed, Hive creates the table and the metadata
    is updated in the metastore accordingly. But the table is empty. So, data can
    be loaded into this table by executing the following command:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行前面的查询时，Hive会创建表，并且元数据会相应地更新到元存储中。但是表是空的。因此，可以通过执行以下命令将数据加载到这个表中：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After executing the previous command, the data is moved from `<hdfs_folder_name>`
    to the Hive table''s default location `/user/hive/warehouse/<managed_table_name`.
    This default folder, `/user/hive/warehouse`, is defined in `hive-site.xml` and
    can be changed to any folder. Now, if we decide to drop the table, we can do so
    by issuing the following command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的命令后，数据将从`<hdfs_folder_name>`移动到Hive表的默认位置`/user/hive/warehouse/<managed_table_name>`。这个默认文件夹`/user/hive/warehouse`在`hive-site.xml`中定义，可以被更改为任何文件夹。现在，如果我们决定删除表，可以通过以下命令执行：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `/user/hive/warehouse/<managed_table_name` folder will be dropped and the
    metadata stored in the metastore will be deleted.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`/user/hive/warehouse/<managed_table_name>`文件夹将被删除，并且存储在元存储中的元数据将被删除。'
- en: External tables
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部表
- en: 'The following is a sample command to define a Hive external table:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个定义Hive外部表的示例命令：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'When the preceding query is executed, Hive creates the table and the metadata
    is updated in the metastore accordingly. But, again, the table is empty. So, data
    can be loaded into this table by executing the following command:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行前面的查询时，Hive将创建表，并在metastore中相应地更新元数据。但是，表仍然是空的。因此，可以通过执行以下命令将数据加载到该表中：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This command will not move any file to any folder but, instead, creates a pointer
    to the folder location, and it is updated in the metadata in the metastore. The
    file remains at the same location (`<hdfs_folder_name>`) of the query. Now, if
    we decide to drop the table, we can do so by issuing the following command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令不会将任何文件移动到任何文件夹，而是创建一个指向文件夹位置的指针，并在metastore的元数据中进行更新。文件将保持在查询相同的位置（《hdfs_folder_name》）。现在，如果我们决定删除表，我们可以通过以下命令执行：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The folder `/user/hive/warehouse/<managed_table_name` will not be dropped and
    only the metadata stored in the metastore will be deleted. The file remains in
    the same location—`<hdfs_folder_name>`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 文件夹`/user/hive/warehouse/<managed_table_name>`不会被删除，只会删除存储在metastore中的元数据。文件将保持在相同的位置——`<hdfs_folder_name>`。
- en: Hive table partition
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hive表分区
- en: 'Partitioning a table means dividing a table into different parts based on a
    value of a partition key. A partition key can be any column, for example, date,
    department, country, and so on. As data is stored in parts, the query response
    time becomes faster. Instead of scanning the whole table, partition creates subfolders
    within the main table folders. Hive will scan only a specific part or parts of
    the table based on the query''s `WHERE` clause. Hive table partition is similar
    to any RDBMS table partition. The purpose is also the same. As we keep inserting
    data into a table, the table becomes bigger in data size. Let''s say we create
    an `ORDERS` table as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表分区意味着根据分区键的值将表划分为不同的部分。分区键可以是任何列，例如日期、部门、国家等。由于数据存储在部分中，查询响应时间会更快。分区不是扫描整个表，而是在主表文件夹内创建子文件夹。Hive将根据查询的`WHERE`子句仅扫描表的具体部分或部分。Hive表分区类似于任何RDBMS表分区。其目的也是相同的。随着我们不断向表中插入数据，表的数据量会越来越大。假设我们创建了一个如下所示的`ORDERS`表：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will load the following sample file `ORDERS_DATA` table as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下方式加载以下示例文件`ORDERS_DATA`表：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we load `orders.txt` to the `/tmp` HDFS folder:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将`orders.txt`加载到`/tmp` HDFS文件夹中：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Load the `ORDERS_DATA` table as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下方式加载`ORDERS_DATA`表：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s assume we want to insert cities data in an `ORDERS_DATA` table. Each
    city orders data is of 1 TB in size. So the total data size of the `ORDERS_DATA`
    table will be 15 TB (there are 15 cities in the table). Now, if we write the following
    query to get all orders booked in `Los Angeles`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想在`ORDERS_DATA`表中插入城市数据。每个城市的订单数据大小为1 TB。因此，`ORDERS_DATA`表的总数据大小将为15 TB（表中共有15个城市）。现在，如果我们编写以下查询以获取在`洛杉矶`预订的所有订单：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The query will run very slowly as it has to scan the entire table. The obvious
    idea is that we can create 10 different `orders` tables for each city and store
    `orders` data in the corresponding city of the `ORDERS_DATA` table. But instead
    of that, we can partition the `ORDERS_PART` table as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 查询将运行得非常慢，因为它必须扫描整个表。显然的想法是，我们可以为每个城市创建10个不同的`orders`表，并将`orders`数据存储在`ORDERS_DATA`表的相应城市中。但不是这样，我们可以按照以下方式对`ORDERS_PART`表进行分区：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, Hive organizes the tables into partitions for grouping similar types of
    data together based on a column or partition key. Let''s assume that we have 10
    `orders` files for each city, that is, `Orders1.txt` to `Orders10.txt`. The following
    example shows how to load each monthly file to each corresponding partition:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Hive根据列或分区键将表组织成分区，以便将相似类型的数据分组在一起。假设我们为每个城市有10个`orders`文件，即`Orders1.txt`到`Orders10.txt`。以下示例显示了如何将每个月度文件加载到相应的分区中：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Partitioning the data can greatly improve the performance of queries because
    the data is already separated into files based on the column value, which can
    decrease the number of mappers and greatly decrease the amount of shuffling and
    sorting of data in the resulting MapReduce job.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分区可以显著提高查询性能，因为数据已经根据列值分离到不同的文件中，这可以减少映射器的数量，并大大减少MapReduce作业中数据的洗牌和排序量。
- en: Hive static partitions and dynamic partitions
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hive静态分区和动态分区
- en: 'If you want to use a static partition in Hive, you should set the property
    as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在Hive中使用静态分区，应设置以下属性：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the preceding example, we have seen that we have to insert each monthly
    order file to each static partition individually. Static partition saves time
    in loading data compared to dynamic partition. We have to individually add a partition
    to the table and move the file into the partition of the table. If we have a lot
    partitions, writing a query to load data in each partition may become cumbersome.
    We can overcome this with a dynamic partition. In dynamic partitions, we can insert
    data into a partition table with a single SQL statement but still load data in
    each partition. Dynamic partition takes more time in loading data compared to
    static partition. When you have large data stored in a table, dynamic partition
    is suitable. If you want to partition a number of columns but you don''t know
    how many columns they are, then dynamic partition is also suitable. Here are the
    hive dynamic partition properties you should allow:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们已经看到我们必须将每个月的订单文件分别插入到每个静态分区中。与动态分区相比，静态分区在加载数据时可以节省时间。我们必须单独向表中添加一个分区并将文件移动到表的分区中。如果我们有很多分区，编写一个查询来加载数据到每个分区可能会变得繁琐。我们可以通过动态分区来克服这一点。在动态分区中，我们可以使用单个SQL语句将数据插入到分区表中，但仍可以加载数据到每个分区。与静态分区相比，动态分区在加载数据时花费更多时间。当你有一个表中有大量数据存储时，动态分区是合适的。如果你想要对多个列进行分区，但你不知道它们有多少列，那么动态分区也是合适的。以下是你应该允许的hive动态分区属性：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is an example of dynamic partition. Let''s say we want to load
    data from the `ORDERS_PART` table to a new table called `ORDERS_NEW`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个动态分区的示例。假设我们想要从`ORDERS_PART`表加载数据到一个名为`ORDERS_NEW`的新表中：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Load data into the `ORDER_NEW` table from the `ORDERS_PART` table. Here, Hive
    will load all partitions of the `ORDERS_NEW` table dynamically:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从`ORDERS_PART`表加载数据到`ORDER_NEW`表。在这里，Hive将动态地加载数据到`ORDERS_NEW`表的全部分区：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let''s see how many partitions are created in `ORDERS_NEW`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`ORDERS_NEW`中创建了多少个分区：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now it is very clear when to use static and dynamic partitions. Static partitioning
    can be used when the partition column values are known well in advance before
    loading data into a hive table. In the case of dynamic partitions, partition column
    values are known only during loading of the data into the hive table.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在很清楚何时使用静态分区和动态分区。当在将数据加载到Hive表之前，分区列的值已知得很好时，可以使用静态分区。在动态分区的情况下，分区列的值仅在将数据加载到Hive表期间知道。
- en: Hive partition bucketing
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hive分区桶
- en: Bucketing is a technique of decomposing a large dataset into more manageable
    groups. Bucketing is based on the hashing function. When a table is bucketed,
    all the table records with the same column value will go into the same bucket.
    Physically, each bucket is a file in a table folder just like a partition. In
    a partitioned table, Hive can group the data in multiple folders. But partitions
    prove effective when they are of a limited number and when the data is distributed
    equally among all of them. If there are a large number of partitions, then their
    use becomes less effective. So in that case, we can use bucketing. We can create
    a number of buckets explicitly during table creation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 桶分区是一种将大型数据集分解成更易管理的小组的技术。桶分区基于哈希函数。当一个表被桶分区时，具有相同列值的所有表记录将进入同一个桶。在物理上，每个桶是表文件夹中的一个文件，就像分区一样。在一个分区表中，Hive可以在多个文件夹中分组数据。但是，当分区数量有限且数据在它们之间均匀分布时，分区证明是有效的。如果有大量分区，那么它们的使用效果就会降低。因此，在这种情况下，我们可以使用桶分区。我们可以在创建表时显式创建多个桶。
- en: How Hive bucketing works
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hive桶分区的工作原理
- en: 'The following diagram shows the working of Hive bucketing in detail:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表详细展示了Hive桶分区的原理：
- en: '![](img/9496a468-bbc4-4711-8027-95b31f9076d7.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9496a468-bbc4-4711-8027-95b31f9076d7.png)'
- en: 'If we decide to have three buckets in a table for a column, (`Ord_city`) in
    our example, then Hive will create three buckets with numbers 0-2 (*n-1*). During
    record insertion time, Hive will apply the Hash function to the `Ord_city` column
    of each record to decide the hash key. Then Hive will apply a modulo operator
    to each hash value. We can use bucketing in non-partitioned tables also. But we
    will get the best performance when the bucketing feature is used with a partitioned
    table. Bucketing has two key benefits:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们决定在一个表中的列（在我们的例子中是`Ord_city`）有三个桶，那么Hive将创建三个编号为0-2（*n-1*）的桶。在记录插入时，Hive将对每个记录的`Ord_city`列应用哈希函数以决定哈希键。然后Hive将对每个哈希值应用模运算符。我们也可以在非分区表中使用桶分区。但是，当与分区表一起使用桶分区功能时，我们将获得最佳性能。桶分区有两个关键好处：
- en: '**Improved query performance**: During joins on the same bucketed columns,
    we can specify the number of buckets explicitly. Since each bucket is of equal
    size of data, map-side joins perform better on a bucketed table than a non-bucketed
    table. In a map-side join, the left-hand side table bucket will exactly know the
    dataset in the right-hand side bucket to perform a table join efficiently.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进查询性能**：在相同分桶列上的连接操作中，我们可以明确指定桶的数量。由于每个桶的数据大小相等，因此分桶表上的map-side连接比非分桶表上的连接性能更好。在map-side连接中，左侧表桶将确切知道右侧桶中的数据集，以便高效地执行表连接。'
- en: '**Improved sampling**: Because the data is already split up into smaller chunks.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进采样**：因为数据已经被分割成更小的块。'
- en: Let's consider our `ORDERS_DATA` table example. It is partitioned in the `CITY`
    column. It may be possible that all of the cities do not have an equal distribution
    of orders. Some cities may have more orders than others. In that case, we will
    have lopsided partitions. This will affect query performance. Queries with cities
    that have more orders will be slower than for cities with fewer orders. We can
    solve this problem by bucketing the table. Buckets in the table are defined by
    the `CLUSTER` clause in the table DDL. The following examples explain the bucketing
    feature in detail.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑我们的`ORDERS_DATA`表示例。它按`CITY`列进行分区。可能所有城市订单的分布并不均匀。一些城市可能比其他城市有更多的订单。在这种情况下，我们将有倾斜的分区。这将影响查询性能。对于订单较多的城市的查询将比订单较少的城市慢。我们可以通过分桶表来解决这个问题。表中的桶由表DDL中的`CLUSTER`子句定义。以下示例详细解释了分桶功能。
- en: Creating buckets in a non-partitioned table
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在非分区表中创建桶
- en: 'First, we will create a `ORDERS_BUCK_non_partition` table:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个`ORDERS_BUCK_non_partition`表：
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To refer to all Hive `SET` configuration parameters, please use this URL:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要引用所有Hive `SET`配置参数，请使用此URL：
- en: '[https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties](https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties](https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties).'
- en: 'Load the newly created non-partitioned bucket table:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 加载新创建的非分区桶表：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following command shows that Hive has created four buckets (folders), `00000[0-3]_0`,
    in the table:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令显示Hive在表中创建了四个桶（文件夹），`00000[0-3]_0`：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Creating buckets in a partitioned table
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在分区表中创建桶
- en: 'First, we will create a bucketed partition table. Here, the table is partitioned into
    four buckets on the `Ord_city` column, but subdivided into `Ord_zip` columns:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个分桶分区表。在这里，表按`Ord_city`列分为四个桶，但按`Ord_zip`列进一步细分：
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Load the bucketed partitioned table with another partitioned table (`ORDERS_PART`)
    with a dynamic partition:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用动态分区将分桶分区表加载到另一个分桶分区表（`ORDERS_PART`）中：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Hive views
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hive视图
- en: A Hive view is a logical table. It is just like any RDBMS view. The concept
    is the same. When a view is created, Hive will not store any data into it. When
    a view is created, Hive freezes the metadata. Hive does not support the materialized
    view concept of any RDBMS. The basic purpose of a view is to hide the query complexity.
    At times, HQL contains complex joins, subqueries, or filters. With the help of
    view, the entire query can be flattened out in a virtual table.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Hive视图是一个逻辑表。它就像任何RDBMS视图一样。概念是相同的。当创建视图时，Hive不会将其中的任何数据存储进去。当创建视图时，Hive会冻结元数据。Hive不支持任何RDBMS的物化视图概念。视图的基本目的是隐藏查询复杂性。有时，HQL包含复杂的连接、子查询或过滤器。借助视图，整个查询可以简化为一个虚拟表。
- en: When a view is created on an underlying table, any changes to that table, or
    even adding or deleting the table, are invalidated in the view. Also, when a view
    is created, it only changes the metadata. But when that view is accessed by a
    query, it triggers the MapReduce job. A view is a purely logical object with no
    associated storage (no support for materialized views is currently available in
    Hive). When a query references a view, the view's definition is evaluated in order
    to produce a set of rows for further processing by the query. (This is a conceptual
    description. In fact, as part of query optimization, Hive may combine the view's
    definition with the queries, for example, pushing filters from the query down
    into the view.)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当在底层表上创建视图时，对该表的任何更改，甚至添加或删除该表，都会在视图中失效。此外，当创建视图时，它只更改元数据。但是，当查询访问该视图时，它将触发MapReduce作业。视图是一个纯粹的逻辑对象，没有关联的存储（Hive中目前不支持物化视图）。当查询引用视图时，将评估视图的定义以生成一组行供查询进一步处理。（这是一个概念性描述。实际上，作为查询优化的部分，Hive可能会将视图的定义与查询结合起来，例如，将查询中的过滤器推入视图。）
- en: A view's schema is frozen at the time the view is created; subsequent changes
    to underlying tables (for example, adding a column) will not be reflected in the
    view's schema. If an underlying table is dropped or changed in an incompatible
    fashion, subsequent attempts to query the invalid view will fail. Views are read-only
    and may not be used as the target of `LOAD`/`INSERT`/`ALTER` for changing metadata.
    A view may contain `ORDER BY` and `LIMIT` clauses. If a referencing query also
    contains these clauses, the query-level clauses are evaluated after the view clauses
    (and after any other operations in the query). For example, if a view specifies
    `LIMIT 5` and a referencing query is executed as (`select * from v LIMIT 10`),
    then at most five rows will be returned.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 视图的模式在创建视图时被冻结；对底层表（例如，添加列）的后续更改不会反映在视图的模式中。如果底层表被删除或以不兼容的方式更改，后续尝试查询无效视图将失败。视图是只读的，不能用作`LOAD`/`INSERT`/`ALTER`更改元数据的目标。视图可能包含`ORDER
    BY`和`LIMIT`子句。如果引用查询也包含这些子句，则查询级别的子句将在视图子句（以及查询中的任何其他操作）之后评估。例如，如果视图指定`LIMIT 5`，并且引用查询以（`select
    * from v LIMIT 10`）执行，则最多返回五行。
- en: Syntax of a view
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视图的语法
- en: 'Let''s see a few examples of views:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看几个视图的示例：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'I will demonstrate the advantages of views using the following few examples.
    Let''s assume we have two tables, `Table_X` and `Table_Y`, with the following
    schema: `Table_XXCol_1` string, `XCol_2` string, `XCol_3` string, `Table_YYCol_1` string,
    `YCol_2` string, `YCol_3` string, and `YCol_4` string. To create a view exactly
    like the base tables, use the following code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我将通过以下几个示例演示视图的优势。假设我们有两个表，`Table_X`和`Table_Y`，具有以下模式：`Table_XXCol_1`字符串，`XCol_2`字符串，`XCol_3`字符串，`Table_YYCol_1`字符串，`YCol_2`字符串，`YCol_3`字符串，和`YCol_4`字符串。要创建与基础表完全相同的视图，请使用以下代码：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To create a view on selective columns of base tables, use the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要在基础表的选定列上创建视图，请使用以下语法：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To create a view to filter values of columns of base tables, we can use:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个用于筛选基础表列值的视图，我们可以使用：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To create a view to hide query complexities:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个用于隐藏查询复杂性的视图：
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Hive indexes
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hive索引
- en: 'The main purpose of the indexing is to search through the records easily and
    speed up the query. The goal of Hive indexing is to improve the speed of query
    lookup on certain columns of a table. Without an index, queries with predicates
    like `WHERE tab1.col1 = 10` load the entire table or partition and process all
    the rows. But if an index exists for `col1`, then only a portion of the file needs
    to be loaded and processed. The improvement in query speed that an index can provide
    comes at the cost of additional processing to create the index and disk space
    to store the index. There are two types of indexes:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 索引的主要目的是便于快速搜索记录并加速查询。Hive索引的目标是提高对表特定列的查询查找速度。如果没有索引，带有如`WHERE tab1.col1 =
    10`这样的谓词的查询将加载整个表或分区并处理所有行。但是，如果为`col1`存在索引，则只需加载和处理文件的一部分。索引可以提供的查询速度提升是以创建索引的额外处理和存储索引的磁盘空间为代价的。有两种类型的索引：
- en: Compact index
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紧凑索引
- en: Bitmap index
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位图索引
- en: The main difference is in storing mapped values of the rows in the different
    blocks.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于存储不同块中行的映射值。
- en: Compact index
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 紧凑索引
- en: In HDFS, the data is stored in blocks. But scanning which data is stored in
    which block is time consuming. Compact indexing stores the indexed column's value
    and its `blockId`. So the query will not go to the table. Instead, the query will
    directly go to the compact index, where the column value and `blockId` are stored.
    No need to scan all the blocks to find data! So, while performing a query, it
    will first check the index and then go directly into that block.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HDFS 中，数据存储在块中。但是扫描存储在哪个块中的数据是耗时的。紧凑索引存储索引列的值及其 `blockId`。因此，查询将不会访问表。相反，查询将直接访问存储列值和
    `blockId` 的紧凑索引。无需扫描所有块以找到数据！所以，在执行查询时，它将首先检查索引，然后直接进入该块。
- en: Bitmap index
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 位图索引
- en: 'Bitmap indexing stores the combination of indexed column value and list of
    rows as a bitmap. Bitmap indexing is commonly used for columns with distinct values.
    Let''s review a few examples: Base table, `Table_XXCol_1` Integer, `XCol_2` string,
    `XCol_3` integer, and `XCol_4` string. Create an index:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 位图索引将索引列的值和行列表的组合存储为位图。位图索引通常用于具有不同值的列。让我们回顾几个示例：基础表，`Table_XXCol_1` 整数，`XCol_2`
    字符串，`XCol_3` 整数，和 `XCol_4` 字符串。创建索引：
- en: '[PRE28]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The preceding index is empty because it is created with the `DEFERRED REBUILD`
    clause, regardless of whether or not the table contains any data. After this index
    is created, the `REBUILD` command needs to be used to build the index structure.
    After creation of the index, if the data in the underlying table changes, the
    `REBUILD` command must be used to bring the index up to date. Create the index
    and store it in a text file:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的索引为空，因为它使用了 `DEFERRED REBUILD` 子句创建，无论表是否包含任何数据。在此索引创建后，需要使用 `REBUILD` 命令来构建索引结构。在索引创建后，如果基础表中的数据发生变化，必须使用
    `REBUILD` 命令来更新索引。创建索引并将其存储在文本文件中：
- en: '[PRE29]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create a bitmap index:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 创建位图索引：
- en: '[PRE30]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: JSON documents using Hive
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Hive 的 JSON 文档
- en: 'JSON, is a minimal readable format for structuring data. It is used primarily
    to transmit data between a server and web application as an alternative to XML.
    JSON is built on two structures:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 是结构化数据的最小可读格式。它主要用于在服务器和 Web 应用程序之间传输数据，作为 XML 的替代方案。JSON 建立在两种结构之上：
- en: A collection of name/value pairs. In various languages, this is realized as
    an object, record, struct, dictionary, hash table, keyed list, or associative
    array.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名称/值对的集合。在不同的语言中，这通常实现为对象、记录、结构、字典、散列表、键列表或关联数组。
- en: An ordered list of values. In most languages, this is realized as an array,
    vector, list, or sequence.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值的有序列表。在大多数语言中，这通常实现为数组、向量、列表或序列。
- en: Please read more on JSON at the following URL: [http://www.json.org/](http://www.json.org/).
    [](http://www.json.org/)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请在以下网址了解更多关于 JSON 的信息：[http://www.json.org/](http://www.json.org/). [](http://www.json.org/)
- en: Example 1 – Accessing simple JSON documents with Hive (Hive 0.14 and later versions)
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 1 – 使用 Hive 访问简单的 JSON 文档（Hive 0.14 及更高版本）
- en: 'In this example, we will see how to query simple JSON documents using HiveQL.
    Let''s assume we want to access the following `Sample-Json-simple.json` file in
    `HiveSample-Json-simple.json`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将看到如何使用 HiveQL 查询简单的 JSON 文档。假设我们想访问以下 `Sample-Json-simple.json` 文件在
    `HiveSample-Json-simple.json`：
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'View the `Sample-Json-simple.json` file:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 `Sample-Json-simple.json` 文件：
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Load `Sample-Json-simple.json` into HDFS:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `Sample-Json-simple.json` 加载到 HDFS：
- en: '[PRE33]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create an external Hive table, `simple_json_table`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个外部 Hive 表，`simple_json_table`：
- en: '[PRE34]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now verify the records:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在验证记录：
- en: '[PRE35]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Example 2 – Accessing nested JSON documents with Hive (Hive 0.14 and later versions)
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 2 – 使用 Hive 访问嵌套 JSON 文档（Hive 0.14 及更高版本）
- en: 'We will see how to query Nested JSON documents using HiveQL. Let''s assume
    we want to access the following `Sample-Json-complex.json` file in `HiveSample-Json-complex.json`:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到如何使用 HiveQL 查询嵌套 JSON 文档。假设我们想访问以下 `Sample-Json-complex.json` 文件在 `HiveSample-Json-complex.json`：
- en: '[PRE36]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Load `Sample-Json-simple.json` into HDFS:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `Sample-Json-simple.json` 加载到 HDFS：
- en: '[PRE37]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Create an external Hive table, `json_nested_table`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个外部 Hive 表，`json_nested_table`：
- en: '[PRE38]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Verify the records:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 验证记录：
- en: '[PRE39]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Example 3 – Schema evolution with Hive and Avro (Hive 0.14 and later versions)
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 3 – 使用 Hive 和 Avro 进行模式演变（Hive 0.14 及更高版本）
- en: In production, we have to change the table structure to address new business
    requirements. The table schema has to change to add/delete/rename table columns.
    Any of these changes affect downstream ETL jobs adversely. In order avoid these,
    we have to make corresponding changes to ETL jobs and target tables.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，我们必须更改表结构以应对新的业务需求。表模式必须更改以添加/删除/重命名表列。这些更改中的任何一项都会对下游 ETL 作业产生不利影响。为了避免这些，我们必须对
    ETL 作业和目标表进行相应的更改。
- en: 'Schema evolution allows you to update the schema used to write new data while
    maintaining backwards compatibility with the schemas of your old data. Then you
    can read it all together as if all of the data has one schema. Please read more
    on Avro serialization at the following URL: [https://avro.apache.org/](https://avro.apache.org/).
    In the following example, I will demonstrate how Avro and Hive tables absorb the
    changes of source table''s schema changes without ETL job failure. We will create
    a customer table in the MySQL database and load it to the target Hive external
    table using Avro files. Then we will add one more column to the source tables
    to see how a Hive table absorbs that change without any errors. Connect to MySQL
    to create a source table (`customer`):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 架构演变允许你在保持与旧数据架构向后兼容的同时更新用于写入新数据的架构。然后你可以将所有数据一起读取，就像所有数据具有一个架构一样。请阅读以下 URL
    上的 Avro 序列化更多信息：[https://avro.apache.org/](https://avro.apache.org/)。在以下示例中，我将演示
    Avro 和 Hive 表如何在不失败 ETL 作业的情况下吸收源表架构更改。我们将创建一个 MySQL 数据库中的客户表，并使用 Avro 文件将其加载到目标
    Hive 外部表中。然后我们将向源表添加一个额外的列，以查看 Hive 表如何无错误地吸收该更改。连接到 MySQL 创建源表（`customer`）：
- en: '[PRE40]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Insert records into the `customer` table:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 向 `customer` 表中插入记录：
- en: '[PRE41]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'On Hadoop, run the following `sqoop` command to import the `customer` table
    and store data in Avro files into HDFS:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hadoop 上，运行以下 `sqoop` 命令以导入 `customer` 表并将数据存储在 Avro 文件中到 HDFS：
- en: '[PRE42]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Verify the target HDFS folder:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 验证目标 HDFS 文件夹：
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create a Hive external table to access Avro files:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 Hive 外部表以访问 Avro 文件：
- en: '[PRE44]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Verify the Hive `customer` table:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 验证 Hive `customer` 表：
- en: '[PRE45]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Perfect! We have no errors. We successfully imported the source `customer`
    table to the target Hive table using Avro serialization. Now, we add one column
    to the source table and import it again to verify that we can access the target
    Hive table without any schema changes. Connect to MySQL and add one more column:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！我们没有错误。我们成功地将源 `customer` 表导入目标 Hive 表，使用了 Avro 序列化。现在，我们在源表中添加一个列，并再次导入以验证我们可以在没有任何架构更改的情况下访问目标
    Hive 表。连接到 MySQL 并添加一个额外的列：
- en: '[PRE46]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now insert rows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在插入行：
- en: '[PRE47]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'On Hadoop, run the following `sqoop` command to import the `customer` table
    so as to append the new address column and data. I have used the `append`  and
    `where "cust_id > 4"` parameters to import only the new rows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hadoop 上，运行以下 `sqoop` 命令以导入 `customer` 表，以便追加新的地址列和数据。我使用了 `append` 和 `where
    "cust_id > 4"` 参数来仅导入新行：
- en: '[PRE48]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Verify the HDFS folder:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 验证 HDFS 文件夹：
- en: '[PRE49]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, let''s verify that our target Hive table is still able to access old and
    new Avro files:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们验证我们的目标 Hive 表是否仍然能够访问旧的和新的 Avro 文件：
- en: '[PRE50]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Great! No errors. Still, it''s business as usual; now we will add one new column
    to the Hive table to see the newly added Avro files:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！没有错误。尽管如此，一切照旧；现在我们将向 Hive 表添加一个新列，以查看新添加的 Avro 文件：
- en: '[PRE51]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Verify the Hive table for new data:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 验证 Hive 表中的新数据：
- en: '[PRE52]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Awesome! Take a look at customer IDs `5` and `6`. We can see the newly added
    column (`cust_state`) with values. You can experiment the delete column and replace
    column feature with the same technique. Now we have a fairly good idea about how
    to access data using Apache Hive. In the next section, we will learn about accessing
    data using HBase, which is a NoSQL data store.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！看看客户 ID `5` 和 `6`。我们可以看到新添加的列（`cust_state`）及其值。您可以使用相同的技术进行删除列和替换列的实验。现在我们相当清楚地了解了如何使用
    Apache Hive 访问数据。在下一节中，我们将学习如何使用 HBase 访问数据，HBase 是一个 NoSQL 数据存储。
- en: Apache HBase
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache HBase
- en: We have just learned about Hive, which is a database where users can access
    data using SQL commands. But there are certain databases where users cannot use
    SQL commands. Those databases are known as **NoSQL data stores**. HBase is a NoSQL
    database. So, what is actually meant by NoSQL? NoSQL means not only SQL. In NoSQL
    data stores like HBase, the main features of RDBMS, such as validation and consistency,
    are relaxed. Also, another important difference between RDBMS or SQL databases
    and NoSQL databases is schema on write versus schema on read. In schema on write,
    the data is validated at the time of writing to the table, whereas schema on read
    supports validation of data at the time of reading it. In this way, NoSQL data
    stores support storage of huge data velocity due to the relaxation of basic data
    validation at the time of writing data. There are about 150 NoSQL data stores
    in the market today. Each of these NoSQL data stores has some unique features
    to offer. Some popular NoSQL data stores are HBase, Cassandra, MongoDB, Druid,
    Apache Kudu, and Accumulo, and so on.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了Hive，这是一个用户可以使用SQL命令访问数据的数据库。但是，也有一些数据库用户不能使用SQL命令。这些数据库被称为**NoSQL数据存储**。HBase就是一个NoSQL数据库。那么，NoSQL实际上意味着什么呢？NoSQL不仅仅意味着SQL。在HBase这样的NoSQL数据存储中，关系型数据库管理系统（RDBMS）的主要特性，如验证和一致性，被放宽了。另外，RDBMS或SQL数据库与NoSQL数据库之间的重要区别在于写入时模式与读取时模式。在写入时模式中，数据在写入表时进行验证，而读取时模式支持在读取数据时进行验证。通过放宽在写入数据时的基本数据验证，NoSQL数据存储能够支持存储大量数据速度。目前市场上大约有150种NoSQL数据存储。每种NoSQL数据存储都提供一些独特的特性。一些流行的NoSQL数据存储包括HBase、Cassandra、MongoDB、Druid、Apache
    Kudu和Accumulo等。
- en: You can get a detailed list of all types of NoSQL databases at [http://nosql-database.org/](http://nosql-database.org/).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[http://nosql-database.org/](http://nosql-database.org/)上获取所有类型NoSQL数据库的详细列表。
- en: HBase is a popular NoSQL database used by many big companies such as Facebook,
    Google, and so on.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: HBase是一个被许多大型公司如Facebook、Google等广泛使用的流行NoSQL数据库。
- en: Differences between HDFS and HBase
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS与HBase之间的差异
- en: The following explains the key difference between HDFS and HBase. Hadoop is
    built on top of HDFS, which has support for storing large volumes (petabytes)
    of datasets. These datasets are accessed using batch jobs, by using MapReduce
    algorithms. In order to find a data element in such a huge dataset, the entire
    dataset needs to be scanned. HBase, on the other hand, is built on top of HDFS
    and provides fast record lookups (and updates) for large tables. HBase internally
    puts your data in indexed StoreFiles that exist on HDFS for high-speed lookup.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下解释了HDFS和HBase之间的关键差异。Hadoop建立在HDFS之上，支持存储大量（PB级）数据集。这些数据集通过批处理作业，使用MapReduce算法进行访问。为了在如此庞大的数据集中找到数据元素，需要扫描整个数据集。另一方面，HBase建立在HDFS之上，并为大型表提供快速的记录查找（和更新）。HBase内部将数据存储在HDFS上存在的索引StoreFiles中，以实现高速查找。
- en: Differences between Hive and HBase
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hive与HBase之间的差异
- en: HBase is a database management system; it supports both transaction processing
    and analytical processing. Hive is a data warehouse system, which can be used
    only for analytical processing. HBase supports low latency and random data access
    operations. Hive only supports batch processing, which leads to high latency.
    HBase does not support any SQL interface to interact with the table data. You
    may have to write Java code to read and write data to HBase tables. At times,
    Java code becomes very complex to process data sets involving joins of multiple
    data sets. But Hive supports very easy access with SQL, which makes it very easy
    to read and write data to its tables. In HBase, data modeling involves flexible
    data models and column-oriented data storage, which must support data denormalization.
    The columns of HBase tables are decided at the time of writing data into the tables.
    In Hive, the data model involves tables with a fixed schema like an RDBMS data
    model.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: HBase是一个数据库管理系统；它支持事务处理和分析处理。Hive是一个数据仓库系统，只能用于分析处理。HBase支持低延迟和随机数据访问操作。Hive仅支持批处理，这导致高延迟。HBase不支持任何SQL接口与表数据进行交互。您可能需要编写Java代码来读取和写入HBase表中的数据。有时，处理涉及多个数据集连接的数据集的Java代码可能非常复杂。但Hive支持使用SQL进行非常容易的访问，这使得读取和写入其表中的数据变得非常简单。在HBase中，数据建模涉及灵活的数据模型和列式数据存储，必须支持数据反规范化。HBase表的列在将数据写入表时确定。在Hive中，数据模型涉及具有固定模式（如RDBMS数据模型）的表。
- en: Key features of HBase
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HBase的关键特性
- en: 'The following are a few key features of HBase:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些HBase的关键特性：
- en: '**Sorted rowkeys**: In HBase, data processing is down with three basic operations/APIs:
    get, put, and scan. All three of these APIs access data using rowkeys to ensure
    smooth data access. As scans are done over a range of rows, HBase lexicographically
    orders rows according to their rowkeys. Using these sorted rowkeys, a scan can
    be defined simply from its start and stop rowkeys. This is extremely powerful
    to get all relevant data in a single database call. The application developer
    can design a system to access recent datasets by querying recent rows based on
    their timestamp as all rows are stored in a table in sorted order based on the
    latest timestamp.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排序行键**：在HBase中，数据处理通过三个基本操作/API进行：get、put和scan。这三个API都使用行键访问数据，以确保数据访问的流畅。由于扫描是在行键的范围内进行的，因此HBase根据行键的字典顺序对行进行排序。使用这些排序行键，可以从其起始和终止行键简单地定义扫描。这在单个数据库调用中获取所有相关数据方面非常强大。应用程序开发者可以设计一个系统，通过根据其时间戳查询最近的行来访问最近的数据集，因为所有行都是根据最新的时间戳在表中按顺序存储的。'
- en: '**Control data sharding**: HBase Table rowkey strongly influences data sharding.
    Table data is sorted in ascending order by rowkey, column families, and column
    key. A solid rowkey design is very important to ensure data is evenly distributed
    across the Hadoop cluster. As rowkeys determine the sort order of a table''s row,
    each region in the table ends up being responsible for the physical storage of
    a part of the row key space.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制数据分片**：HBase表的行键强烈影响数据分片。表数据按行键、列族和列键的升序排序。良好的行键设计对于确保数据在Hadoop集群中均匀分布非常重要。由于行键决定了表行的排序顺序，因此表中的每个区域最终负责行键空间的一部分的物理存储。'
- en: '**Strong consistency**: HBase favors consistency over availability. It also
    supports ACID-level semantics on a per row basis. It, of course, impacts the write
    performance, which will tend to be slower. Overall, the trade-off plays in favor
    of the application developer, who will have the guarantee that the data store
    always the right value of the data.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强一致性**：HBase更倾向于一致性而不是可用性。它还支持基于每行的ACID级语义。当然，这会影响写性能，可能会变慢。总体而言，权衡有利于应用程序开发者，他们将保证数据存储始终具有数据的正确值。'
- en: '**Low latency processing**: HBase supports fast, random reads and writes to
    all data stored.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低延迟处理**：HBase支持对存储的所有数据进行快速、随机的读取和写入。'
- en: '**Flexibility**: HBase supports any type—structured, semi-structured, unstructured.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**：HBase支持任何类型——结构化、半结构化、非结构化。'
- en: '**Reliability**: HBase table data block is replicated multiple times to ensure
    protection against data loss. HBase also supports fault tolerance. The table data
    is always available for processing even in case of failure of any regional server.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可靠性**：HBase表数据块被复制多次，以确保防止数据丢失。HBase还支持容错。即使在任何区域服务器故障的情况下，表数据始终可用于处理。'
- en: HBase data model
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HBase数据模型
- en: 'These are the key components of an HBase data model:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是HBase数据模型的关键组件：
- en: '**Table**: In HBase, data is stored in a logical object, called **table**,
    that has multiple rows.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表**：在HBase中，数据存储在逻辑对象中，称为**表**，该表具有多个行。'
- en: '**Row**: A row in HBase consists of a row key and one or more columns. The
    row key sorts rows. The goal is to store data in such a way that related rows
    are near each other. The row key can be a combination of one of more columns.
    The row key is like the primary key of the table, which must be unique. HBase
    uses row keys to find data in a column. For example, `customer_id` can be a row
    key for the `customer` table.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行**：HBase中的行由一个行键和一个或多个列组成。行键用于排序行。目标是按这种方式存储数据，使得相关的行彼此靠近。行键可以是多个列的组合。行键类似于表的主键，必须是唯一的。HBase使用行键在列中查找数据。例如，`customer_id`可以是`customer`表的行键。'
- en: '**Column**: A column in HBase consists of a column family and a column qualifier.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列**：HBase中的列由一个列族和一个列限定符组成。'
- en: '**Column qualifier**: It is the column name of a table.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列限定符**：它是表的列名。'
- en: '**Cell**: This is a combination of row, column family, and column qualifier,
    and contains a value and a timestamp which represents the value''s version.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单元格**：这是行、列族和列限定符的组合，包含一个值和一个时间戳，该时间戳表示值的版本。'
- en: '**Column family**: It is a collection of columns that are co-located and stored
    together, often for performance reasons. Each column family has a set of storage
    properties, such as cached, compression, and data encodation.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列族**：它是一组位于同一位置并存储在一起的列的集合，通常出于性能考虑。每个列族都有一组存储属性，例如缓存、压缩和数据编码。'
- en: Difference between RDBMS table and column - oriented data store
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDBMS表与列式数据存储之间的差异
- en: 'We all know how data is stored in any RDBMS table. It looks like this:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都知道数据在任何关系型数据库管理系统（RDBMS）表中是如何存储的。它看起来像这样：
- en: '| **ID** | `Column_1` | `Column_2` | `Column_3` | `Column_4` |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| **ID** | `Column_1` | `Column_2` | `Column_3` | `Column_4` |'
- en: '| 1 | A | 11 | P | XX |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 1 | A | 11 | P | XX |'
- en: '| 2 | B | 12 | Q | YY |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 2 | B | 12 | Q | YY |'
- en: '| 3 | C | 13 | R | ZZ |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 3 | C | 13 | R | ZZ |'
- en: '| 4 | D | 14 | S | XX1 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 4 | D | 14 | S | XX1 |'
- en: 'The column ID is used as a unique/primary key of the table to access data from
    other columns of the table. But in a column-oriented data store like HBase, the
    same table is divided into key and value and is stored like this:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 列ID用作表的唯一/主键，以便从表的其它列访问数据。但在像HBase这样的列式数据存储中，相同的表被分为键和值，并按如下方式存储：
- en: '| **Key** | **Value** |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| **Key** | **Value** |'
- en: '| **Row** | **Column** | **Column Value** |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| **行** | **列** | **列值** |'
- en: '| 1 | `Column_1` | `A` |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `Column_1` | `A` |'
- en: '| 1 | `Column_2` | `11` |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `Column_2` | `11` |'
- en: '| 1 | `Column_3` | `P` |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `Column_3` | `P` |'
- en: '| 1 | `Column_4` | `XX` |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `Column_4` | `XX` |'
- en: '| 2 | `Column_1` | `B` |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `Column_1` | `B` |'
- en: '| 2 | `Column_2` | `12` |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `Column_2` | `12` |'
- en: '| 2 | `Column_3` | `Q` |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `Column_3` | `Q` |'
- en: '| 2 | `Column_4` | `YY` |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `Column_4` | `YY` |'
- en: '| 3 | `Column_1` | `C` |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `Column_1` | `C` |'
- en: '| 3 | `Column_2` | `13` |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `Column_2` | `13` |'
- en: '| 3 | `Column_3` | `R` |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `Column_3` | `R` |'
- en: '| 3 | `Column_4` | `ZZ` |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `Column_4` | `ZZ` |'
- en: 'In HBase, each table is a sorted map format, where each key is sorted in ascending
    order. Internally, each key and value is serialized and stored on the disk in
    byte array format. Each column value is accessed by its corresponding key. So,
    in the preceding table, we define a key, which is a combination of two columns,
    *row + column*. For example, in order to access the `Column_1` data element of
    row 1, we have to use a key, row 1 + `column_1`. That''s the reason the row key
    design is very crucial in HBase. Before creating the HBase table, we have to decide
    a column family for each column. A column family is a collection of columns, which
    are co-located and stored together, often for performance reasons. Each column
    family has a set of storage properties, such as cached, compression, and data
    encodation. For example, in a typical `CUSTOMER` table, we can define two column
    families, namely `cust_profile` and `cust_address`. All columns related to the
    customer address are assigned to the column family `cust_address`; all other columns,
    namely `cust_id`, `cust_name`, and `cust_age`, are assigned to the column family
    `cust_profile`. After assigning the column families, our sample table will look
    like the following:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在HBase中，每个表都是一个排序映射格式，其中每个键按升序排序。内部，每个键和值都被序列化并以字节数组格式存储在磁盘上。每个列值通过其对应键进行访问。因此，在前面的表中，我们定义了一个键，它是两个列的组合，*行
    + 列*。例如，为了访问第1行的`Column_1`数据元素，我们必须使用一个键，即行1 + `column_1`。这就是为什么行键设计在HBase中非常关键。在创建HBase表之前，我们必须为每个列决定一个列族。列族是一组列的集合，这些列位于同一位置并一起存储，通常出于性能原因。每个列族都有一组存储属性，例如缓存、压缩和数据编码。例如，在一个典型的`CUSTOMER`表中，我们可以定义两个列族，即`cust_profile`和`cust_address`。所有与客户地址相关的列都分配给列族`cust_address`；所有其他列，即`cust_id`、`cust_name`和`cust_age`，都分配给列族`cust_profile`。分配列族后，我们的示例表将如下所示：
- en: '| **Key** | **Value** |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| **Key** | **Value** |'
- en: '| **Row** | **Column** | **Column family** | **Value** | **Timestamp** |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| **行** | **列** | **列族** | **值** | **时间戳** |'
- en: '| 1 | `Column_1` | `cf_1` | `A` | `1407755430` |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `Column_1` | `cf_1` | `A` | `1407755430` |'
- en: '| 1 | `Column_2` | `cf_1` | `11` | `1407755430` |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `Column_2` | `cf_1` | `11` | `1407755430` |'
- en: '| 1 | `Column_3` | `cf_1` | `P` | `1407755430` |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `Column_3` | `cf_1` | `P` | `1407755430` |'
- en: '| 1 | `Column_4` | `cf_2` | `XX` | `1407755432` |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `Column_4` | `cf_2` | `XX` | `1407755432` |'
- en: '| 2 | `Column_1` | `cf_1` | `B` | `1407755430` |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `Column_1` | `cf_1` | `B` | `1407755430` |'
- en: '| 2 | `Column_2` | `cf_1` | `12` | `1407755430` |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `Column_2` | `cf_1` | `12` | `1407755430` |'
- en: '| 2 | `Column_3` | `cf_1` | `Q` | `1407755430` |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `Column_3` | `cf_1` | `Q` | `1407755430` |'
- en: '| 2 | `Column_4` | `cf_2` | `YY` | `1407755432` |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `Column_4` | `cf_2` | `YY` | `1407755432` |'
- en: '| 3 | `Column_1` | `cf_1` | `C` | `1407755430` |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `Column_1` | `cf_1` | `C` | `1407755430` |'
- en: '| 3 | `Column_2` | `cf_1` | `13` | `1407755430` |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `Column_2` | `cf_1` | `13` | `1407755430` |'
- en: '| 3 | `Column_3` | `cf_1` | `R` | `1407755430` |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `Column_3` | `cf_1` | `R` | `1407755430` |'
- en: '| 3 | `Column_4` | `cf_2` | `ZZ` | `1407755432` |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `Column_4` | `cf_2` | `ZZ` | `1407755432` |'
- en: While inserting data into a table, HBase will automatically add a timestamp
    for each version of the cell.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 当向表中插入数据时，HBase将为每个单元格的每个版本自动添加时间戳。
- en: HBase architecture
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HBase架构
- en: 'If we want to read data from an HBase table, we have to give an appropriate
    row ID, and HBase will perform a lookup based on the given row ID. HBase uses
    the following sorted nested map to return the column value of the row ID: row
    ID a column family, a column at timestamp, and value. HBase is always deployed
    on Hadoop. The following is a typical installation:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要从HBase表中读取数据，我们必须提供一个合适的行ID，HBase将根据提供的行ID进行查找。HBase使用以下排序嵌套映射来返回行ID的列值：行ID、列族、列在时间戳和值。HBase始终部署在Hadoop上。以下是一个典型的安装：
- en: '![](img/2b1fd55d-59df-48b3-b0a5-d5aacf0c836e.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b1fd55d-59df-48b3-b0a5-d5aacf0c836e.png)'
- en: It is a master server of the HBase cluster and is responsible for the administration,
    monitoring, and management of RegionServers, such as assignment of regions to
    RegionServer, region splits, and so on. In a distributed cluster, the HMaster
    typically runs on the Hadoop NameNode.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 它是HBase集群的主服务器，负责RegionServer的管理、监控和管理，例如将region分配给RegionServer、region拆分等。在分布式集群中，HMaster通常运行在Hadoop
    NameNode上。
- en: 'ZooKeeper is a coordinator of HBase cluster. HBase uses ZooKeeper as a distributed
    coordination service to maintain server state in the cluster. ZooKeeper maintains
    which servers are alive and available, and provides server failure notification. RegionServer
    is responsible for management of regions. RegionServer is deployed on DataNode.
    It serves data for reads and writes. RegionServer is comprised of the following
    additional components:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ZooKeeper是HBase集群的协调器。HBase使用ZooKeeper作为分布式协调服务来维护集群中的服务器状态。ZooKeeper维护哪些服务器是活跃的和可用的，并提供服务器故障通知。RegionServer负责region的管理。RegionServer部署在DataNode上。它提供读写服务。RegionServer由以下附加组件组成：
- en: 'Regions: HBase tables are divided horizontally by row key range into regions.
    A region contains all rows in the table between the region''s start key and end
    key. **Write-ahead logging** (**WAL**) is used to store new data that has not
    yet stored on disk.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Regions：HBase表通过行键范围水平分割成region。一个region包含表中从region的起始键到结束键之间的所有行。**预写日志**（**WAL**）用于存储尚未存储在磁盘上的新数据。
- en: 'MemStore is a write cache. It stores new data that has not yet been written
    to disk. It is sorted before writing to disk. There is one MemStore per column
    family per region. Hfile stores the rows as sorted key/values on disk/HDFS:'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MemStore是一个写缓存。它存储尚未写入磁盘的新数据。在写入磁盘之前进行排序。每个region每个列族有一个MemStore。Hfile在磁盘/HDFS上以排序的键/值形式存储行：
- en: '![](img/79ed20fa-ef32-4b86-9fc6-d0cb11127b8f.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79ed20fa-ef32-4b86-9fc6-d0cb11127b8f.png)'
- en: HBase architecture in a nutshell
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HBase架构概述
- en: The HBase cluster is comprised of one active master and one or more backup master
    servers
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBase集群由一个活动主服务器和一个或多个备份主服务器组成
- en: The cluster has multiple RegionServers
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该集群包含多个RegionServer
- en: The HBase table is always large and rows are divided into partitions/shards
    called **regions**
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBase表始终很大，行被分割成称为**region**的分区/碎片
- en: Each RegionServer hosts one or many regions
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个RegionServer托管一个或多个region
- en: The HBase catalog is known as META table, which stores the locations of table
    regions
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBase目录被称为META表，它存储表region的位置
- en: ZooKeeper stores the locations of the META table
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZooKeeper存储META表的位置
- en: During a write, the client sends the put request to the HRegionServer
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在写入过程中，客户端将put请求发送到HRegionServer
- en: Data is written to WAL
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据写入WAL
- en: Then data is pushed into MemStore and an acknowledgement is sent to the client
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后将数据推入MemStore并向客户端发送确认
- en: Once enough data is accumulated in MemStore, it flushes data to the Hfile on
    HDFS
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦MemStore中积累足够的数据，它就会将数据刷新到HDFS上的Hfile
- en: The HBase compaction process activates periodically to merge multiple HFiles
    into one Hfile (called **compaction**)
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBase的压缩过程定期激活，将多个HFile合并成一个Hfile（称为**压缩**）
- en: HBase rowkey design
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HBase行键设计
- en: 'Rowkey design is a very crucial part of HBase table design. During key design,
    proper care must be taken to avoid hotspotting. In case of poorly designed keys,
    all of the data will be ingested into just a few nodes, leading to cluster imbalance.
    Then, all the reads have to be pointed to those few nodes, resulting in slower
    data reads. We have to design a key that will help load data equally to all nodes
    of the cluster. Hotspotting can be avoided by the following techniques:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 行键设计是HBase表设计的一个非常关键的部分。在键设计期间，必须采取适当的措施以避免热点。在设计不良的键的情况下，所有数据都将被摄入到少数几个节点中，导致集群不平衡。然后，所有读取都必须指向这些少数节点，导致数据读取速度变慢。我们必须设计一个键，以帮助将数据均匀地加载到集群的所有节点上。可以通过以下技术避免热点：
- en: '**Key salting**: It means adding an arbitrary value at the beginning of the
    key to make sure that the rows are distributed equally among all the table regions.
    Examples are `aa-customer_id`, `bb-customer_id`, and so on.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密钥加盐**：这意味着在密钥的开头添加一个任意值，以确保行在所有表区域中均匀分布。例如，`aa-customer_id`、`bb-customer_id`等。'
- en: '**Key hashing**: The key can be hashed and the hashing value can be used as
    a rowkey, for example, `HASH(customer_id)`.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密钥哈希**：密钥可以被哈希，哈希值可以用作行键，例如，`HASH(customer_id)`。'
- en: '**Key with reverse timestamp**: In this technique, you have to define a regular
    key and then attach a reverse timestamp to it. The timestamp has to be reversed
    by subtracting it from any arbitrary maximum value and then attached to the key.
    For example, if `customer_id` is your row ID, the new key will be `customer_id`
    + reverse timestamp.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向时间戳密钥**：在这种技术中，您必须定义一个常规密钥，然后将其与一个反向时间戳相关联。时间戳必须通过从任意最大值中减去它来反转，然后附加到密钥上。例如，如果`customer_id`是您的行ID，则新的密钥将是`customer_id`
    + 反向时间戳。'
- en: 'The following are the guidelines while designing a HBase table:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 设计HBase表时的以下是一些指南：
- en: Define no more than two column families per table
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个表定义不超过两个列族
- en: Keep column family names as small as possible
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能保持列族名称尽可能小
- en: Keep column names as small as possible
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能保持列名尽可能小
- en: Keep the rowkey length as small as possible
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能保持行键长度尽可能小
- en: Do not set the row version at a high level
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要将行版本设置在较高水平
- en: The table should not have more than 100 regions
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表不应超过100个区域
- en: Example 4 – loading data from MySQL table to HBase table
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例4 – 从MySQL表加载数据到HBase表
- en: 'We will use the same `customer` table that we created earlier:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们之前创建的相同的`customer`表：
- en: '[PRE53]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Start HBase and create a `customer` table in HBase:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 启动HBase并在HBase中创建一个`customer`表：
- en: '[PRE54]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Load MySQL `customer` table data in HBase using Sqoop:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Sqoop在HBase中加载数据库`customer`表数据：
- en: '[PRE55]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Verify the HBase table:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 验证HBase表：
- en: '[PRE56]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: You must see all 11 rows in the HBase table.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须看到HBase表中的所有11行。
- en: Example 5 – incrementally loading data from MySQL table to HBase table
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例5 – 从MySQL表增量加载数据到HBase表
- en: '[PRE57]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Insert a new customer and update the existing one:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 插入一个新客户并更新现有客户：
- en: '[PRE58]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Example 6 – Load the MySQL customer changed data into the HBase table
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例6 – 将MySQL客户更改数据加载到HBase表中
- en: 'Here, we have used the `InsUpd_on` column as our ETL date:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`InsUpd_on`列作为我们的ETL日期：
- en: '[PRE59]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Example 7 – Hive HBase integration
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例7 – Hive与HBase集成
- en: 'Now, we will access the HBase `customer` table using the Hive external table:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用Hive外部表访问HBase `customer`表：
- en: '[PRE60]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw how data is stored and accessed using a Hadoop SQL interface
    called Hive. We studied various partitioning and indexing strategies in Hive.
    The working examples helped us to understand JSON data access and schema evolution
    using Avro in Hive. In the second section of the chapter, we studied a NoSQL data
    store called HBase and its difference with respect to RDBMS. The row design of
    the HBase table is very crucial to balancing reads and writes to avoid region
    hotspots. One has to keep in mind the HBase table design best practices discussed
    in this chapter. The working example shows the easier paths of data ingestions
    into an HBase table and its integration with Hive.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何使用称为Hadoop SQL接口的Hive存储和访问数据。我们研究了Hive中的各种分区和索引策略。工作示例帮助我们理解了在Hive中使用Avro访问JSON数据和模式演变。在第2节中，我们研究了名为HBase的NoSQL数据存储以及它与RDBMS的区别。HBase表的行设计对于平衡读写以避免区域热点至关重要。必须记住本章中讨论的HBase表设计最佳实践。工作示例展示了数据导入HBase表和其与Hive集成的简单路径。
- en: In the next chapter, we will take a look at tools and techniques for designing
    real-time data analytics.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨设计实时数据分析的工具和技术。
