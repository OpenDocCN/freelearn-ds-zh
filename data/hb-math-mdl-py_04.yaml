- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Gradient Descent
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: One optimization algorithm that lays the foundation for machine learning models
    is **gradient descent** (**GD**). GD is a simple and effective tool useful to
    train such models. Gradient descent, as the name suggests, involves “going downhill.”
    We choose a direction across a landscape and take whichever step gets us downhill.
    The step size depends on the slope (gradient) of the hill. In **machine learning**
    (**ML**) models, gradient descent estimates the error gradient, helping to minimize
    the cost function. Very few optimization methods are as computationally efficient
    as gradient descent. GD also lays the foundation for the optimization of deep
    learning models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一种为机器学习模型奠定基础的优化算法是**梯度下降**（**GD**）。GD是一个简单且有效的工具，适用于训练此类模型。顾名思义，梯度下降涉及“下坡”。我们选择一个方向穿越地形，并采取每一步让我们下坡的方式。步长取决于坡度（梯度）。在**机器学习**（**ML**）模型中，梯度下降估计误差梯度，帮助最小化成本函数。很少有优化方法能像梯度下降那样在计算效率上表现得如此出色。GD还为深度学习模型的优化奠定了基础。
- en: In problems where the parameters cannot be calculated analytically by use of
    linear algebra and must be searched by optimization, GD finds its best use. The
    algorithm works iteratively by moving in the direction of the steepest descent.
    At each iteration, the model parameters, such as coefficients in linear regression
    and weights in neural networks, are updated. The model continues to update its
    parameters until the cost function converges or reaches its minimum value (the
    bottom of the slope in *Figure 4**.1a*).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些无法通过线性代数解析计算参数，且必须通过优化搜索的难题中，GD找到了它的最佳应用。该算法通过迭代的方式沿着最陡的下降方向移动。在每次迭代中，模型参数（如线性回归中的系数和神经网络中的权重）都会被更新。模型会不断更新其参数，直到成本函数收敛或达到最小值（即*图
    4.1a*中的斜坡底部）。
- en: '![Figure 4.1a: Gradient descent](img/Figure_04_01_B18943.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1a：梯度下降](img/Figure_04_01_B18943.jpg)'
- en: 'Figure 4.1a: Gradient descent'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1a：梯度下降
- en: The size of a step taken in each iteration is called the learning rate (a function
    derivative is scaled by the learning rate at each iteration). With a learning
    rate that is too low, the model may reach the maximum permissible number of iterations
    before reaching the bottom, whereas it may not converge or may diverge (the so-called
    exploding gradient problem) completely if the learning rate is too high. Selecting
    the most appropriate learning rate is crucial in achieving a model with the best
    possible accuracy, as seen in *Figure 4**.1b*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代中所采取的步长称为学习率（每次迭代时，函数的导数都会按学习率缩放）。如果学习率过低，模型可能会在达到底部之前就达到最大允许迭代次数，而如果学习率过高，则可能无法收敛或完全发散（即所谓的爆炸梯度问题）。选择最合适的学习率对于获得最佳精度的模型至关重要，如*图
    4.1b*所示。
- en: '![Figure 4.1b: Learning rates in gradient descent](img/Figure_04_02_B18943.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1b：梯度下降中的学习率](img/Figure_04_02_B18943.jpg)'
- en: 'Figure 4.1b: Learning rates in gradient descent'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1b：梯度下降中的学习率
- en: For GD to work, the objective or cost function must be differentiable (meaning
    the first derivative exists at each point in the domain of a univariate function)
    and convex (where two points on the function can be connected by a line segment
    without crossing). The second derivative of a convex function is always positive.
    Examples of convex and non-convex functions are shown in *Figure 4**.2*. GD is
    a first-order optimization algorithm.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使梯度下降（GD）有效，目标或成本函数必须是可微分的（意味着在单变量函数的定义域中的每个点都有一阶导数）且是凸的（即函数上的两点可以通过一条不相交的线段连接）。凸函数的二阶导数总是为正。凸函数和非凸函数的例子如*图
    4.2*所示。GD是一种一阶优化算法。
- en: '![Figure 4.2: Example of convex (L) and non-convex (R) function](img/Figure_04_03_B18943.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2：凸函数（L）和非凸函数（R）的示例](img/Figure_04_03_B18943.jpg)'
- en: 'Figure 4.2: Example of convex (L) and non-convex (R) function'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：凸函数（L）和非凸函数（R）的示例
- en: In a multivariate function, the gradient is a vector of derivatives in each
    direction in the domain. Such functions have saddle points (quasi-convex or semi-convex)
    where the algorithm may get stuck and obtaining a minimum is not guaranteed. This
    is where second-order optimization algorithms are brought in to escape the saddle
    point and reach the global minimum. The GD algorithm finds its use in control
    as well as mechanical engineering, apart from ML and DL. The following sections
    compare the algorithm with other optimization algorithms used in ML and **deep
    learning** (**DL**) models and specifically examines some commonly used gradient
    descent optimizers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在多变量函数中，梯度是该函数在定义域中每个方向上的导数向量。此类函数可能具有鞍点（准凸或半凸），在这些点上，算法可能会陷入困境，无法保证找到最小值。为了突破鞍点并达到全局最小值，引入了二阶优化算法。除了机器学习（ML）和深度学习（**DL**）之外，梯度下降算法也在控制工程和机械工程中有应用。以下各节将梯度下降算法与其他用于ML和**深度学习**（**DL**）模型的优化算法进行比较，并专门探讨一些常用的梯度下降优化器。
- en: 'This chapter covers the following topics:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Gradient descent variants
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降法的变种
- en: Gradient descent optimizers
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降优化器
- en: Gradient descent variants
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降法的变种
- en: The workings of the gradient descent algorithm to optimize a simple linear regression
    model (*y = mx + c*) is elaborated with Python code in this section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过Python代码详细说明了梯度下降算法在优化简单线性回归模型（*y = mx + c*）中的工作原理。
- en: Application of gradient descent
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降法的应用
- en: 'Keeping the number of iterations the same, the algorithm is run for three different
    learning rates resulting in three models, hence three **MSE** (**mean squared
    error**) values. MSE is the calculated loss or cost function in linear regression:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在保持迭代次数相同的情况下，算法会分别在三种不同的学习率下运行，产生三种模型，因此得到三种**MSE**（**均方误差**）值。MSE是线性回归中计算的损失或代价函数：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Two other models are trained with two different learning rates, one higher
    and another lower than model 1, as seen here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两个模型分别使用两种不同的学习率进行训练，一个比模型1的学习率高，另一个低，如下所示：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Upon executing the code, the linear regression models obtained (*Figure 4**.3*)
    show how carefully the parameter (learning rate) should be chosen to attain optimal
    performance or the best accuracy of the ML model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，得到的线性回归模型（*图4.3*）展示了如何谨慎选择参数（学习率），以达到最佳性能或最优的ML模型准确度。
- en: '![Figure 4.3: Gradient descent for a linear regression model](img/Figure_04_04_B18943.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3：线性回归模型的梯度下降](img/Figure_04_04_B18943.jpg)'
- en: 'Figure 4.3: Gradient descent for a linear regression model'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：线性回归模型的梯度下降
- en: There are GD variants (*Figure 4**.4*) that differ in the data size used to
    compute the gradient of the objective function. A trade-off between the accuracy
    of the parameter (coefficient or weight) and the time taken to do it is made depending
    on the amount of data. The variants are **batch gradient descent** (**BGD**),
    **mini-batch gradient descent**, and **stochastic gradient descent** (**SGD**),
    which we will now discuss in the following subsection.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 存在不同的梯度下降法变种（*图4.4*），它们在计算目标函数的梯度时使用的数据量不同。根据数据量的多少，在参数（系数或权重）的准确性与计算所需时间之间需要做出权衡。梯度下降法的变种包括**批量梯度下降**（**BGD**）、**小批量梯度下降**和**随机梯度下降**（**SGD**），我们将在以下小节中讨论它们。
- en: Mini-batch gradient descent and stochastic gradient descent
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小批量梯度下降与随机梯度下降
- en: BGD, also known as vanilla gradient descent, is simply gradient descent and
    computes the gradient for the entire training data to perform one update (in one
    step) and thus can be very slow. Common examples of ML models that are optimized
    using BGD are linear regression and logistic regression for smaller datasets.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: BGD，也称为原始梯度下降法，仅仅是梯度下降，它对整个训练数据计算梯度，进行一次更新（一步更新），因此可能非常慢。使用BGD优化的常见ML模型包括用于较小数据集的线性回归和逻辑回归。
- en: For bigger datasets, we generally use mini-batch GD, which allows the splitting
    of training data into mini-batches that can be processed individually. After each
    mini-batch is processed, the parameters are updated and this continues until the
    entire dataset has iteratively been processed. One full cycle through the data
    is called an epoch. A number of steps are taken to reach the global minimum, which
    introduces some variance into the optimization process. This variant of GD is
    usually used for modeling problems where efficiency is as important as accuracy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更大的数据集，我们通常使用小批量梯度下降（mini-batch GD），它允许将训练数据分割成可以单独处理的小批量。处理完每个小批量后，更新参数，直到整个数据集被迭代处理完毕。通过这种方式，一次完整的数据集循环称为一个周期。为了达到全局最小值，需要执行若干步骤，这在优化过程中引入了一些方差。这种
    GD 的变体通常用于效率与精度同等重要的建模问题。
- en: SGD performs frequent parameter updates (for each training example) with a high
    level of variance that causes the cost function to fluctuate heavily. This enables
    it to jump to a new and potentially better local minimum. Upon slowly decreasing
    the learning rate, SGD shows convergence behavior similar to BGD. SGD is computationally
    faster than BGD as it considers one example at a time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 对每个训练样例执行频繁的参数更新，具有较高的方差，导致成本函数波动较大。这使得它能够跳到一个新的、潜在更好的局部最小值。通过缓慢减小学习率，SGD
    表现出类似于 BGD 的收敛行为。SGD 的计算速度比 BGD 快，因为它一次考虑一个样本。
- en: '![Figure 4.4: Gradient descent variants](img/Figure_04_05_B18943.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4: 梯度下降变种](img/Figure_04_05_B18943.jpg)'
- en: 'Figure 4.4: Gradient descent variants'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4.4: 梯度下降变种'
- en: SGD is typically the algorithm of choice for training a neural network. A key
    challenge with SGD while minimizing the highly non-convex error functions common
    in neural networks is avoiding getting trapped in their numerous suboptimal local
    minima. These saddle points make it very hard for SGD to escape, as the gradient
    is close to zero in all dimensions. In the next section, we outline some GD optimizers
    that deal with such challenges.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 通常是训练神经网络的选择算法。在最小化神经网络中常见的高度非凸误差函数时，SGD 的一个关键挑战是避免陷入其众多次优局部最小值中。这些鞍点使得
    SGD 很难逃脱，因为梯度在所有维度上接近零。在下一节中，我们将概述一些处理此类挑战的梯度下降优化器。
- en: Gradient descent optimizers
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降优化器
- en: The optimizers discussed here are widely used to train DL models depending on
    the degree of the non-convexity of the error or cost function.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论的优化器广泛用于训练依赖于误差或成本函数非凸性程度的深度学习模型。
- en: Momentum
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动量
- en: The momentum method uses a moving average gradient instead of a gradient at
    each time step and reduces the back-and-forth oscillations (fluctuations of the
    cost function) caused by SGD. This process focuses on the steepest descent path.
    *Figure 4**.5a* shows movement with no momentum by creating oscillations in SGD
    while *Figure 4**.5b* shows movement in the relevant direction by accumulating
    velocity with damped oscillations and closer to the optimum.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 动量法使用移动平均梯度而不是每个时间步的梯度，并减少由 SGD 引起的前后振荡（成本函数波动）。此过程侧重于最陡下降路径。*图 4**.5a* 显示了没有动量时
    SGD 中的振荡，而 *图 4**.5b* 显示了在相关方向上累积速度并减少阻尼振荡，接近最优解。
- en: '![Figure 4.5a: SGD with no momentum](img/Figure_04_06_B18943.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5a: 没有动量的 SGD](img/Figure_04_06_B18943.jpg)'
- en: 'Figure 4.5a: SGD with no momentum'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4.5a: 没有动量的 SGD'
- en: '![Figure 4.5b: SGD with momentum](img/Figure_04_07_B18943.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5b: 具有动量的 SGD](img/Figure_04_07_B18943.jpg)'
- en: 'Figure 4.5b: SGD with momentum'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4.5b: 具有动量的 SGD'
- en: The momentum term reduces updates for dimensions whose gradients change directions
    and as a result, faster convergence is achieved.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 动量项减少了梯度方向变化的维度更新，从而实现更快的收敛。
- en: Adagrad
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Adagrad
- en: The `adagrad` optimizer is used when dealing with sparse data as the algorithm
    performs small updates of parameters based on features that occur often. In `adagrad`,
    different or “adaptive” learning rates are used for every update at every time
    step (*Figure 4**.6*). The algorithm uses larger learning rates for infrequent
    features and smaller ones for more frequent features. The major advantage of using
    this optimizer is that the learning rate is not set manually. And when the learning
    rate shrinks to almost zero, the model gains no new knowledge.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`adagrad` 优化器在处理稀疏数据时使用，因为该算法基于频繁出现的特征对参数进行小的更新。在 `adagrad` 中，每次更新的学习率是不同的或“自适应”的（*图
    4**.6*）。该算法为不常见的特征使用较大的学习率，为更常见的特征使用较小的学习率。使用此优化器的主要优势是学习率不需要手动设置。当学习率缩小到接近零时，模型将不再获得新的知识。'
- en: '![Figure 4.6: Adagrad optimizer](img/Figure_04_08_B18943.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6：Adagrad 优化器](img/Figure_04_08_B18943.jpg)'
- en: 'Figure 4.6: Adagrad optimizer'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：Adagrad 优化器
- en: RMSprop
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RMSprop
- en: The RMSprop optimizer is similar to the `adagrad` optimizer and hence known
    as **leaky adagrad**, only it uses a different method for parameter updates. The
    RMSprop algorithm restricts oscillations in the vertical direction so that it
    can take larger steps in the horizontal direction (*Figure 4**.7*). The algorithm
    adaptively scales the learning rate in each dimension by using an exponentially
    weighted average of the gradient that allows it to focus on the most recent gradients.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: RMSprop 优化器类似于`adagrad`优化器，因此也被称为**泄漏的 adagrad**，只是它使用不同的参数更新方法。RMSprop 算法限制了垂直方向的振荡，使其可以在水平方向上迈出更大的步伐（*图
    4**.7*）。该算法通过使用梯度的指数加权平均值来自适应地缩放每个维度的学习率，从而使其能够聚焦于最近的梯度。
- en: '![Figure 4.7: RMSprop optimizer](img/Figure_04_09_B18943.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7：RMSprop 优化器](img/Figure_04_09_B18943.jpg)'
- en: 'Figure 4.7: RMSprop optimizer'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：RMSprop 优化器
- en: Adam
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Adam
- en: The **adaptive moment estimation** (**adam**) optimizer inherits the advantages
    of both the momentum and RMSprop optimization algorithms (*Figure 4**.8*). It
    combines the ideas of a moving average gradient and an adaptive learning rate.
    These two respectively represent the estimates of the first moment (mean) and
    second moment (variance) of the gradient of the cost function, hence the name.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**自适应矩估计**（**adam**）优化器继承了动量和 RMSprop 优化算法的优点（*图 4**.8*）。它结合了移动平均梯度和自适应学习率的思想。这两者分别表示代价函数梯度的一阶矩（均值）和二阶矩（方差）的估计，因此得名。'
- en: '![Figure 4.8: Adam optimizer](img/Figure_04_10_B18943.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8：Adam 优化器](img/Figure_04_10_B18943.jpg)'
- en: 'Figure 4.8: Adam optimizer'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：Adam 优化器
- en: 'It has been empirically observed that the adam optimizer is effective and works
    better than SGD in practice. It has become the default optimizer of choice to
    train DL models. For further reading, check out the following MachineLearningMastery
    article: [https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 经实验证明，Adam 优化器在实践中比 SGD 更有效，表现更好。它已经成为训练深度学习模型的默认优化器。欲了解更多，请阅读以下《机器学习精粹》文章：[https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)。
- en: Summary
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about a foundational optimization algorithm and
    its variants used in training ML and DL models. An application of the optimization
    technique in Python to a linear regression problem was also elaborated on. Both
    the cost function and its gradient, and how to update the gradient to converge
    to the optimal point, are mathematical concepts every data scientist must understand
    thoroughly; optimizing a cost function is the basis of achieving an optimal model
    for a problem or predictions. Different ways can be used to estimate the gradients
    depending on the behavior of the cost function.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了一个基础的优化算法及其在训练机器学习和深度学习模型中的变体。还详细讲解了如何在 Python 中将优化技术应用于线性回归问题。无论是代价函数及其梯度，还是如何更新梯度以收敛到最优点，都是每个数据科学家必须彻底理解的数学概念；优化代价函数是为问题或预测实现最优模型的基础。根据代价函数的行为，可以使用不同的方法来估算梯度。
- en: In the following chapter, we will explore another fundamental algorithm, known
    as **support vector machines** (**SVMs**). Although SVMs can be used for regression
    problems, they are more widely used for classification tasks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨另一种基础算法，称为**支持向量机**（**SVMs**）。尽管 SVM 可以用于回归问题，但它更广泛地用于分类任务。
