- en: A Developer&#x27;s Approach to Data Cleaning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发人员如何处理数据清理
- en: This chapter discusses how a developer might understand and approach the topic
    of **data cleaning** using several common statistical methods.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了开发人员如何运用几种常见的统计方法理解并处理**数据清理**这一话题。
- en: 'In this chapter, we''ve broken things into the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将内容分为以下几个话题：
- en: Understanding basic data cleaning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基础数据清理
- en: Using R to detect and diagnose common data issues, such as missing values, special
    values, outliers, inconsistencies, and localization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用R检测和诊断常见的数据问题，例如缺失值、特殊值、离群值、不一致性和本地化问题
- en: Using R to address advanced statistical situations, such as transformation,
    deductive correction, and deterministic imputation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用R解决高级统计问题，例如数据转换、推导修正和确定性填补
- en: Understanding basic data cleaning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解基础数据清理
- en: The importance of having clean (and therefore reliable) data in any statistical
    project cannot be overstated. Dirty data, even with sound statistical practice,
    can be unreliable and can lead to producing results that suggest courses of action
    that are incorrect or that may even cause harm or financial loss. It has been
    stated that a data scientist spends nearly 90 percent of his or her time in the
    process of cleaning data and only 10 percent on the actual modeling of the data
    and deriving results from it.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何统计项目中，拥有干净（因此可靠）的数据至关重要。脏数据，即使在正确的统计实践下，也可能是不可靠的，可能导致得出错误的行动建议，甚至可能造成伤害或经济损失。曾有人指出，数据科学家几乎90%的时间都花在清理数据上，只有10%的时间用于实际建模和从数据中得出结果。
- en: So, just what is data cleaning?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，数据清理到底是什么呢？
- en: Data cleaning is also referred to as data cleansing or data scrubbing and involves
    both the processes of detecting as well as addressing errors, omissions, and inconsistencies
    within a population of data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理也称为数据清洗或数据去污，涉及到检测和解决数据集中的错误、遗漏和不一致性问题。
- en: This may be done interactively with data wrangling tools, or in batch mode through
    scripting. We will use R in this book as it is well-fitted for data science since
    it works with even very complex datasets, allows handling of the data through
    various modeling functions, and even provides the ability to generate visualizations
    to represent data and prove theories and assumptions in just a few lines of code.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过数据整理工具进行交互式处理，或者通过脚本批量处理。本书将使用R语言，因为它非常适合数据科学，能够处理非常复杂的数据集，支持通过各种建模功能对数据进行处理，甚至可以通过几行代码生成可视化数据，来证明理论和假设。
- en: During cleansing, you first use logic to examine and evaluate your data pool
    to establish a level of quality for the data. The level of data quality can be
    affected by the way the data is entered, stored, and managed. Cleansing can involve
    correcting, replacing, or just removing data points or entire actual records.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在清理过程中，首先使用逻辑来检查和评估数据池，以建立数据的质量水平。数据质量的高低可以受到数据录入、存储和管理方式的影响。清理过程可能涉及纠正、替换或直接删除数据点或整个实际记录。
- en: Cleansing should not be confused with validating as they differ from each other.
    A validation process is a pass or fails process, usually occurring as the data
    is captured (time of entry), rather than an operation performed later on the data
    in preparation for an intended purpose.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 清理过程不应与验证过程混淆，它们是不同的。验证过程通常在数据捕获时（即录入时）进行，是一个通过或不通过的过程，而不是在数据处理后，为某个特定目的进行的操作。
- en: As a data developer, one should not be new to the concept of data cleaning or
    the importance of improving the level of quality of data. A data developer will
    also agree that the process of addressing data quality requires a routine and
    regular review and evaluation of the data, and in fact, most organizations have
    enterprise tools and/or processes (or at least policies) in place to routinely
    preprocess and cleanse the enterprise data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据开发人员，不应对数据清理的概念或提升数据质量的重要性感到陌生。数据开发人员还会同意，数据质量的处理过程需要定期的常规评审和评估，实际上，大多数组织都有企业级工具和/或流程（或者至少有相关政策）来定期预处理和清理企业数据。
- en: There is quite a list of both free and paid tools to sample, if you are interested,
    including iManageData, Data Manager, DataPreparator (Trifecta) Wrangler, and so
    on. From a statistical perspective, the top choices would be R, Python, and Julia.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣的话，市面上有许多免费的和付费的工具可以尝试，包括 iManageData、Data Manager、DataPreparator（Trifecta）Wrangler
    等。从统计学的角度来看，最受欢迎的选择包括 R、Python 和 Julia。
- en: Before you can address specific issues within your data, you need to detect
    them. Detecting them requires that you determine what would qualify as an issue
    or error, given the context of your objective (more on this later in this section).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在你处理数据中的具体问题之前，你需要先检测到它们。检测这些问题要求你根据目标的背景，确定什么可以被视为问题或错误（后面这一节会有更多的内容）。
- en: Common data issues
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见的数据问题
- en: 'We can categorize data difficulties into several groups. The most generally
    accepted groupings (of data issues) include:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将数据难题分为几个组。最普遍接受的（数据问题）分类包括：
- en: '**Accuracy**: There are many varieties of data inaccuracies and the most common
    examples include poor math, out-of-range values, invalid values, duplication,
    and more.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：数据不准确有很多种类型，最常见的例子包括数学错误、超出范围的值、无效值、重复等。'
- en: '**Completeness**: Data sources may be missing values from particular columns,
    missing entire columns, or even missing complete transactions.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整性**：数据源可能会缺少某些列的值，缺少整列数据，甚至缺少完整的交易。'
- en: '**Update status**: As part of your quality assurance, you need to establish
    the cadence of data refresh or update, as well as have the ability to determine
    when the data was last saved or updated. This is also referred to as latency.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新状态**：作为质量保证的一部分，你需要建立数据刷新或更新的节奏，并且能够判断数据最后一次保存或更新的时间。这也被称为延迟。'
- en: '**Relevance**: It is identification and elimination of information that you
    don''t need or care about, given your objectives. An example would be removing
    sales transactions for pickles if you are intending on studying personal grooming
    products.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关性**：这涉及识别并消除在特定目标下不需要或不相关的信息。例如，如果你的研究目标是个人护理产品，那么去除关于腌黄瓜的销售交易就是一个例子。'
- en: '**Consistency**: It is common to have to cross-reference or translate information
    from data sources. For example, recorded responses to a patient survey may require
    translation to a single consistent indicator to later make processing or visualizing
    easier.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**：通常需要交叉参考或转换数据源中的信息。例如，记录的病人调查反馈可能需要转换为一个一致的指标，以便以后简化处理或可视化。'
- en: '**Reliability**: This is chiefly concerned with making sure that the method
    of data gathering leads to consistent results. A common data assurance process
    involves establishing baselines and ranges, and then routinely verifying that
    the data results fall within the established expectations. For example, districts
    that typically have a mix of both registered Democrat and Republican voters would
    warrant the investigation if the data suddenly was 100 percent single partied.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可靠性**：这主要关注确保数据收集方法能产生一致的结果。常见的数据保障流程包括建立基准线和范围，然后定期验证数据结果是否符合预期。例如，通常有混合的注册民主党和共和党选民的地区，如果数据突然变为100%的单一党派，就需要进行调查。'
- en: '**Appropriateness**: Data is considered appropriate if it is suitable for the
    intended purpose; this can be subjective. For example, it is considered a fact
    that holiday traffic affects purchasing habits (an increase in US Flags Memorial
    day week does not indicate an average or expected weekly behavior).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用性**：当数据适合预定的用途时，就被认为是合适的；这可能是主观的。例如，假设假期交通影响了购买习惯（美国国旗在阵亡将士纪念日一周的销售量增加并不表示这是一个平均或预期的每周行为）。'
- en: '**Accessibility**: Data of interest may be watered down in a sea of data you
    are not interested in, thereby reducing the quality of the interesting data since
    it would be mostly inaccessible. This is particularly common in big data projects.
    Additionally, security may play a role in the quality of your data. For example,
    particular computers might be excluded from captured logging files or certain
    health-related information may be hidden and not part of shared patient data.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可访问性**：有趣的数据可能会被大量无关数据所淹没，从而降低了有趣数据的质量，因为它变得难以访问。这在大数据项目中尤其常见。此外，安全性也可能影响数据的质量。例如，某些计算机可能会被排除在捕获的日志文件之外，或某些与健康相关的信息可能被隐藏，未包含在共享的病人数据中。'
- en: Contextual data issues
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文数据问题
- en: A lot of the previously mentioned data issues can be automatically detected
    and even corrected. The issues may have been originally caused by user entry errors,
    by corruption in transmission or storage, or by different definitions or understandings
    of similar entities in different data sources. In data science, there is more
    to think about.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 很多前面提到的数据问题可以被自动检测到，甚至可以被修正。这些问题最初可能是由用户输入错误、传输或存储过程中的损坏，或者由不同数据源中相似实体的不同定义或理解所引起的。在数据科学中，还有更多需要考虑的事项。
- en: During data cleaning, a data scientist will attempt to identify patterns within
    the data, based on a hypothesis or assumption about the context of the data and
    its intended purpose. In other words, any data that the data scientist determines
    to be either obviously disconnected with the assumption or objective of the data
    or obviously inaccurate will then be addressed. This process is reliant upon the
    data scientist's judgment and his or her ability to determine which points are
    valid and which are not.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据清洗过程中，数据科学家会尝试根据对数据背景及其预期用途的假设或假定，识别数据中的模式。换句话说，任何数据科学家认为明显与数据假设或目标无关，或明显不准确的数据都会被处理。这个过程依赖于数据科学家的判断力以及他或她确定哪些数据点有效，哪些无效的能力。
- en: When relying on human judgment, there is always a chance that valid data points,
    not sufficiently accounted for in the data scientist's hypothesis/assumption,
    are overlooked or incorrectly addressed. Therefore, it is a common practice to
    maintain appropriately labeled versions of your cleansed data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖人类判断时，总有可能忽视或错误处理那些没有充分考虑在数据科学家假设/假定中的有效数据点。因此，保持适当标签的清洗数据版本是一种常见的做法。
- en: Cleaning techniques
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清洗技术
- en: Typically, the data cleansing process evolves around identifying those data
    points that are outliers, or those data points that stand out for not following
    the pattern within the data that the data scientist sees or is interested in.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据清洗过程围绕识别那些异常数据点展开，或者识别那些由于不符合数据科学家所看到或感兴趣的数据模式而显得突出的数据点。
- en: The data scientists use various methods or techniques for identifying the outliers
    in the data. One approach is plotting the data points and then visually inspecting
    the resultant plot for those data points that lie far outside the general distribution.
    Another technique is programmatically eliminating all points that do not meet
    the data scientist's mathematical control limits (limits based upon the objective
    or intention of the statistical project).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家使用各种方法或技术来识别数据中的异常值。一种方法是绘制数据点图，然后通过视觉检查结果图，找出那些远离整体分布的数据点。另一种技术是通过编程方式删除所有不符合数据科学家数学控制限制的数据点（这些限制基于统计项目的目标或意图）。
- en: 'Other data cleaning techniques include:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据清洗技术包括：
- en: '**Validity checking**: Validity checking involves applying rules to the data
    to determine if it is valid or not. These rules can be global; for example, a
    data scientist could perform a uniqueness check if specific unique keys are part
    of the data pool (for example, social security numbers cannot be duplicated),
    or case level, as when a combination of field values must be a certain value.
    The validation may be strict (such as removing records with missing values) or
    fuzzy (such as correcting values that partially match existing, known values).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有效性检查**：有效性检查涉及应用规则来判断数据是否有效。这些规则可以是全局性的；例如，如果数据池中包含特定唯一键（例如社会安全号码不能重复），数据科学家可以进行唯一性检查；也可以是案例级的，例如当某些字段值的组合必须是特定值时。验证可以是严格的（例如删除缺失值的记录），也可以是模糊的（例如修正与现有已知值部分匹配的值）。'
- en: '**Enhancement**: This is a technique where data is made complete by adding
    related information. The additional information can be calculated by using the
    existing values within the data file or it can be added from another source. This
    information could be used for reference, comparison, contrast, or show tendencies.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强**：这是一种通过添加相关信息使数据完整的技术。额外的信息可以通过使用数据文件中的现有值计算得出，也可以从其他来源添加。这些信息可以用于参考、比较、对比或展示趋势。'
- en: '**Harmonization**: With data harmonization, data values are converted, or mapped
    to other more desirable values.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化**：在数据标准化过程中，数据值会被转换或映射到其他更为理想的值。'
- en: '**Standardization**: This involves changing a reference dataset to a new standard.
    For example, use of standard codes.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化**：这涉及将参考数据集转换为新的标准。例如，使用标准代码。'
- en: '**Domain expertise**: This involves removing or modifying data values in a
    data file based upon the data scientist''s prior experience or best judgment.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域专业知识**：这涉及根据数据科学家的经验或最佳判断，删除或修改数据文件中的数据值。'
- en: We will go through an example of each of these techniques in the next sections
    of this chapter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的接下来的部分中逐一讲解这些技术的例子。
- en: R and common data issues
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R与常见数据问题
- en: Let's start this section with some background on R. R is a language and environment
    that is easy to learn, very flexible in nature, and very focused on statistical
    computing, making it a great choice for manipulating, cleaning, summarizing, producing
    probability statistics, and so on.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些关于R的背景知识开始。R是一种易学的语言和环境，天生非常灵活，专注于统计计算，因而成为处理、清洗、汇总、生成概率统计等的绝佳选择。
- en: 'In addition, here are a few more reasons to use R for data cleaning:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些使用R进行数据清洗的其他原因：
- en: It is used by a large number of data scientists so it's not going away anytime
    soon
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它被大量数据科学家使用，所以它不会很快消失
- en: R is platform independent, so what you create will run almost anywhere
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R是平台独立的，因此你所创建的内容几乎可以在任何地方运行
- en: R has awesome help resources--just Google it, you'll see!
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R有非常棒的帮助资源——只需谷歌一下，你就能找到！
- en: Outliers
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常值
- en: 'The simplest explanation for what outliers are might be is to say that outliers
    are those data points that just don''t fit the rest of your data. Upon observance,
    any data that is either very high, very low, or just unusual (within the context
    of your project), is an outlier. As part of data cleansing, a data scientist would
    typically identify the outliers and then address the outliers using a generally
    accepted method:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对异常值最简单的解释是，异常值就是那些与其他数据点不相符的数据点。观察时，任何特别高、特别低，或在你项目的上下文中显得特别不寻常的数据，都可以视为异常值。作为数据清洗的一部分，数据科学家通常会识别异常值，然后采用公认的方法来处理这些异常值：
- en: Delete the outlier values or even the actual variable where the outliers exist
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除异常值，甚至删除包含异常值的实际变量
- en: Transform the values or the variable itself
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换值或变量本身
- en: Let's look at a real-world example of using R to identify and then address data
    outliers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个使用R识别并解决数据异常值的真实世界例子。
- en: In the world of gaming, slot machines (a gambling machine operated by inserting
    coins into a slot and pulling a handle which determines the payoff) are quite
    popular. Most slot machines today are electronic and therefore are programmed
    in such a way that all their activities are continuously tracked. In our example,
    investors in a casino want to use this data (as well as various supplementary
    data) to drive adjustments to their profitability strategy. In other words, what
    makes for a profitable slot machine? Is it the machine's theme or its type? Are
    newer machines more profitable than older or retro machines? What about the physical
    location of the machine? Are lower denomination machines more profitable? We try
    to find our answers using the outliers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏世界中，老虎机（是一种通过将硬币插入插槽并拉动手柄来操作的赌博机器，手柄决定了支付的结果）非常受欢迎。如今大多数老虎机是电子的，因此它们被编程成可以持续追踪所有活动的方式。在我们的例子中，赌场的投资者希望利用这些数据（以及各种补充数据）来推动他们的盈利策略调整。换句话说，什么样的老虎机更能带来盈利？是机器的主题还是类型？新型机器比旧型或复古机器更有利可图吗？机器的物理位置如何？低面额的机器更有利可图吗？我们尝试通过异常值来寻找答案。
- en: We are given a collection or pool of gaming data (formatted as a comma-delimited
    or CSV text file), which includes data points such as the location of the slot
    machine, its denomination, month, day, year, machine type, age of the machine,
    promotions, coupons, weather, and coin-in (which is the total amount inserted
    into the machine less pay-outs). The first step for us as a data scientist is
    to review (sometimes called **profile**) the data, where we'll determine if any
    outliers exist. The second step will be to address those outliers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到一组游戏数据（格式为逗号分隔或CSV文本文件），其中包括老虎机的位置、面额、月份、日期、年份、机器类型、机器年龄、促销活动、优惠券、天气和投币金额（即插入机器的总金额减去支付出的金额）。作为数据科学家的第一步，我们需要审查（有时称为**分析**）数据，查看是否存在异常值。第二步是处理这些异常值。
- en: Step 1 – Profiling the data
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 1 – 数据分析
- en: 'R makes this step very simple. Although there are many ways to program a solution,
    let us try to keep the lines of the actual program code or script to a minimum.
    We can begin by defining our CSV file as a variable in our R session (named `MyFile`)
    and then reading our file into an R `data.frame` (named `MyData`):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: R 使这一步变得非常简单。虽然有很多种方式可以编写解决方案，但我们尽量保持实际程序代码或脚本的行数最小。我们可以从在 R 会话中定义 CSV 文件作为一个变量（命名为
    `MyFile`）开始，然后将文件读取到 R 的 `data.frame`（命名为 `MyData`）中：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In statistics, a `boxplot` is a simple way to gain information regarding the
    shape, variability, and centre (or median) of a statistical dataset, so we''ll
    use the `boxplot` with our data to see if we can identify both the median `Coin-in`
    and if there are any outliers. To do this, we can ask R to plot the `Coin-in`
    value for each slot machine in our file, using the `boxplot` function:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，`boxplot` 是一种简单的方式，可以了解统计数据集的形状、变异性和中心（或中位数），因此我们将使用 `boxplot` 来查看我们的数据，看看能否识别出
    `Coin-in` 的中位数，并确定是否存在离群值。为此，我们可以请求 R 绘制文件中每台老虎机的 `Coin-in` 值，使用 `boxplot` 函数：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`Coin-in` is the 11th column in our file so I am referring to it explicitly
    as a parameter of the function `boxplot`. I''ve also added optional parameters
    (again, continuing the effort to stay minimal) which add headings to the visualization.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`Coin-in` 是文件中的第 11 列，因此我在函数 `boxplot` 中明确将其作为参数。为了保持简洁，我还添加了可选的参数（继续简化代码），这些参数为可视化图形添加了标题。'
- en: 'Executing the previous script yields us the following visual. Note both the
    median (shown by the line that cuts through the box in the `boxplot`) as well
    as the four outliers:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 执行之前的脚本后，我们得到了以下图形。注意中位数（通过 `boxplot` 中穿过盒子的线表示）以及四个离群值：
- en: '![](img/eb9f236a-3856-46ce-9471-b8851f952c24.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb9f236a-3856-46ce-9471-b8851f952c24.png)'
- en: Step 2 – Addressing the outliers
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2 – 处理离群值
- en: Now that we see the outliers do exist within our data, we can address them so
    that they do not adversely affect our intended study. Firstly, we know that it
    is illogical to have a negative `Coin-in` value since machines cannot dispense
    more coins that have been inserted in them. Given this rule, we can simply drop
    any records from the file that have negative `Coin-in` values. Again, R makes
    it easy as we'll use the `subset` function to create a new version of our `data.frame`,
    one that only has records (or cases) with non-negative `Coin-in` values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到数据中确实存在离群值，我们可以处理它们，避免它们对我们的研究产生不利影响。首先，我们知道负的 `Coin-in` 值是不合逻辑的，因为机器不能发放超过已经投入的硬币。根据这一规则，我们可以简单地删除文件中所有
    `Coin-in` 值为负的记录。再次地，R 使这变得很简单，我们将使用 `subset` 函数来创建一个新的 `data.frame`，该数据框只包含那些
    `Coin-in` 值非负的记录（或案例）。
- en: 'We''ll call our `subset` data frame `noNegs`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个 `subset` 数据框命名为 `noNegs`：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we''ll replot to make sure we''ve dropped our negative outlier:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将重新绘制图形，确保已经删除了负的离群值：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This produces a new `boxplot`, as shown in the following screenshot:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了一个新的 `boxplot`，如下面的截图所示：
- en: '![](img/3b83e3a1-b730-4441-b64d-805707d21dc8.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b83e3a1-b730-4441-b64d-805707d21dc8.png)'
- en: 'We can use the same approach to drop our extreme positive `Coin-in` values
    (those greater than $1,500) by creating yet another `subset` and then replotting:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的方法来删除极端的正 `Coin-in` 值（大于 $1,500 的值），通过创建另一个 `subset`，然后重新绘制：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It is well-advised, as you work through various iterations of your data, that
    you save off most (if not just the most significant) versions of your data. You
    can use the R function `write.csv`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据的不同迭代过程中，建议你保存大部分（如果不是所有的话）版本的数据显示，特别是保存最重要的版本。你可以使用 R 的 `write.csv` 函数：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Most data scientists adopt a common naming convention to be used through the
    project (if not for all the projects). The names of your files should be as explicit
    as possible to save you time later. In addition, especially when working with
    big data, you need to be mindful of disk space.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据科学家采用统一的命名约定，贯穿整个项目（如果不是所有项目的话）。你的文件名应该尽可能明确，以便以后节省时间。此外，尤其是在处理大数据时，你还需要注意磁盘空间。
- en: 'The output of the preceding code is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出结果如下：
- en: '![](img/874a7a47-f170-4c53-a966-8861ec97c0e5.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/874a7a47-f170-4c53-a966-8861ec97c0e5.png)'
- en: Domain expertise
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 领域专业知识
- en: Moving on, another data cleaning technique is referred to as cleaning data based
    upon domain expertise. This doesn't need to be complicated. The point of this
    technique is simply using information not found in the data. For example, previously
    we excluded cases with negative `Coin-in` values since we know it is impossible
    to have a negative `Coin-in` amount. Another example might be the time when Hurricane
    Sandy hit the northeast United States. During that period of time, the cases of
    most machines had very low (if not zero) `Coin-in` amounts. A data scientist would
    probably remove all the data cases during a specific time period, based on that
    information.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，另一种数据清理技巧是基于领域知识进行数据清理。这并不复杂。这种技巧的关键是利用数据中没有的额外信息。例如，之前我们排除了具有负值的 `Coin-in`
    数据，因为我们知道负的 `Coin-in` 数值是不可能的。另一个例子可能是当飓风桑迪袭击美国东北部时。在那段时间里，大多数机器的 `Coin-in` 数值非常低（如果不是零的话）。数据科学家可能会基于这些信息，移除特定时间段的数据。
- en: Validity checking
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有效性检查
- en: As I mentioned earlier in this chapter, cross-validation is when a data scientist
    applies rules to data in a data pool.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在本章之前提到的，交叉验证是指数据科学家对数据池中的数据应用规则。
- en: Validity checking is the most common form of statistical data cleansing and
    is a process that both the data developer and the data scientist will most likely
    be (at least somewhat) familiar with.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有效性检查是最常见的统计数据清理形式，是数据开发人员和数据科学家最有可能（至少有些）熟悉的过程。
- en: 'There can be any number of validity rules used to clean the data, and these
    rules will depend upon the intended purpose or objective of the data scientist.
    Examples of these rules include: data-typing (for example, a field must be a numeric),
    range limitations (where numbers or dates must fall within a certain range), required
    (a value cannot be empty or missing), uniqueness (a field, or a combination of
    fields, must be unique within the data pool), set-member (this is when values
    must be a member of a discreet list), foreign-key (certain values found within
    a case must be defined as member of or meeting a particular rule), regular expression
    patterning (which simply means verifying that a value is formatted in a prescribed
    format), and cross-field validation (where combinations of fields within a case
    must meet a certain criteria).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 清理数据时可以使用任何数量的有效性规则，这些规则将依赖于数据科学家的预期目的或目标。这些规则的示例包括：数据类型（例如，一个字段必须是数值类型）、范围限制（数字或日期必须落在某个范围内）、必填（值不能为空或缺失）、唯一性（字段或字段组合必须在数据池内唯一）、集合成员（即值必须是某个离散列表的成员）、外键（某些值在某个案例中必须定义为某个规则的成员或符合该规则）、正则表达式匹配（即验证值是否符合预定的格式）、以及跨字段验证（即一个案例中的多个字段组合必须符合某个特定条件）。
- en: 'Let''s look at a few examples of the preceding, starting with data-typing (also
    known as **coercion**). R offers six coercion functions to make it easy:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些前述的示例，首先从数据类型转换（也叫 **强制转换**）开始。R 提供了六个强制转换函数，方便使用：
- en: '`as.numeric`'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`as.numeric`'
- en: '`as.integer`'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`as.integer`'
- en: '`as.character`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`as.character`'
- en: '`as.logical`'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`as.logical`'
- en: '`as.factor`'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`as.factor`'
- en: '`as.ordered`'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`as.ordered`'
- en: '`as.Date`'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`as.Date`'
- en: 'These functions, along with a little R knowledge, can make the effort of converting
    a value in a data pool pretty straightforward. For example, using the previous
    GammingData as an example, we might discover that a new gamming results file was
    generated and the age value was saved as a string (or text value). To clean it,
    we need to convert the value to a numeric data type. We can use the following
    single line of R code to quickly convert those values in the file:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数，加上一点 R 知识，可以使得将数据池中的一个值转换变得相当简单。例如，使用之前的 GammingData 为例，我们可能会发现生成了一个新的游戏结果文件，且年龄值被保存为字符串（或文本值）。为了清理这些数据，我们需要将值转换为数值类型。我们可以使用以下一行
    R 代码快速转换文件中的值：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'One point: using this simple approach, should any value be unable to be converted,
    it will be set to an **NA** value. In type conversion, the real work is understanding
    what type a value needs to be, and, of course, what data types are valid; R has
    a wide variety of data types, including scalars, vectors (numerical, character,
    logical), matrices, data frames, and lists.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一点提示：使用这种简单的方法，如果任何值无法转换，它将被设置为 **NA** 值。在类型转换中，真正的工作是理解一个值需要转换成什么类型，当然，还有哪些数据类型是有效的；R
    语言有多种数据类型，包括标量、向量（数值型、字符型、逻辑型）、矩阵、数据框和列表。
- en: Another area of data cleaning we'll look at here is the process of regular expression
    patterning. In practice, especially when working with data that is collected (or
    mined) from multiple sources, the data scientist surely encounters either field
    that are not in the desired format (for the objective at hand) or, field values
    that are inconsistently formatted (which potentially can yield incorrect results).
    Some examples can be dates, social security numbers, and telephone numbers. With
    dates, depending on the source, you may have to re-type (as described previously),
    but more often than not, you'll also need to reformat the values into a format
    that is usable, given your objective.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里查看的另一个数据清理领域是正则表达式模式的处理。在实际操作中，特别是当处理来自多个来源的数据时，数据科学家无疑会遇到一些字段，它们的格式不符合要求（例如目标任务的格式），或者字段值格式不一致（这可能会导致错误的结果）。一些例子包括日期、社会保障号码和电话号码。对于日期，视来源而定，你可能需要重新输入（如前所述），但通常情况下，你还需要将这些值重新格式化成一个可用的格式，符合你的目标。
- en: Re-typing a date is important so that R knows to use the value as an actual
    date and you can use the various R data functions correctly.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 重新输入日期是很重要的，这样 R 才知道使用该值作为实际日期，并且你可以正确地使用各种 R 数据函数。
- en: 'A common example is when data contains cases with dates that are perhaps formatted
    as `YYYY/MM/DD` and you want to perform a time series analysis showing a sum week
    to week, or some other operation that requires using the date value but perhaps
    requiring the date to be reformatted, or you just need it to be a true R date
    object type. So, let''s assume a new Gamming file—this one with just two columns
    of data: `Date` and `Coinin`. This file is a dump of collected `Coinin` values
    for a single slot machine, day by day.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的例子是，当数据包含日期字段，格式可能是`YYYY/MM/DD`，你想进行时间序列分析，展示每周的总和，或者进行其他操作，这些操作需要使用日期值，但可能需要重新格式化日期，或者你仅仅需要它作为一个真正的
    R 日期对象类型。假设我们有一个新的 Gamming 文件——这个文件只有两列数据：`Date` 和 `Coinin`。这个文件是一个收集的 `Coinin`
    值的转储，针对一个老虎机的每一天数据。
- en: 'The records (or cases) in our new file look like the following screenshot:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们新文件中的记录（或案例）看起来像以下截图：
- en: '![](img/16b90af7-9697-4826-9fb1-d4d877b3e992.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16b90af7-9697-4826-9fb1-d4d877b3e992.png)'
- en: 'A variety of cleaning examples can be used by the data scientist. Starting
    with verifying what data types each of these data points are. We can use the R
    function class to verify our file''s data types. First (as we did in the previous
    example), we read our CSV file into an R data frame object:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家可以使用多种清理方法。首先，验证这些数据点的每种数据类型。我们可以使用 R 函数 `class` 来验证文件的数据类型。首先（正如我们在前面的例子中做的），我们将
    CSV 文件读入一个 R 数据框对象中：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we can use the `class` function, as shown in the following screenshot:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用 `class` 函数， 如下截图所示：
- en: '![](img/8e56f465-ba3f-460c-9db9-1f09c36913eb.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e56f465-ba3f-460c-9db9-1f09c36913eb.png)'
- en: You can see in the preceding screenshot that we used `class` to display our
    data types.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，你可以看到我们使用了 `class` 来显示我们的数据类型。
- en: '`MyData` is our data frame holding our gaming data, `Date` is of type `factor`,
    and `Coinin` is an `integer`. So, the data frame and the integer should make sense
    to you, but take note that R sets our dates up for what it calls a `factor`. Factors
    are categorical variables that are beneficial in summary statistics, plots, and
    regressions, but not so much as date values. To remedy this, we can use the R
    functions `substr` and `paste` as shown next:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`MyData` 是我们的数据框，包含我们的游戏数据，`Date` 是 `factor` 类型，`Coinin` 是 `integer` 类型。所以，数据框和整数类型对你来说应该是清晰的，但请注意，R
    会将日期设置为所谓的 `factor` 类型。因子（factor）是分类变量，在总结统计、绘图和回归分析中非常有用，但作为日期值并不适合。为了修正这个问题，我们可以使用
    R 函数 `substr` 和 `paste`，如下面所示：'
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This reformats the value of our `Data` field in all our cases in one simple
    line of script by pulling apart the field into three segments (the month, day,
    and year) and then pasting the segments back together in the order we want (with
    a/as the separator (`sep`)), as shown in the following screenshot:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这行简单的脚本，我们可以将所有记录中 `Data` 字段的值重新格式化。它将字段拆分成三个部分（即月、日和年），然后按照我们想要的顺序将这些部分重新拼接起来（使用
    `/` 作为分隔符 (`sep`)），如下所示的截图：
- en: '![](img/9d21a628-e05a-44e9-80b8-67815525d812.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d21a628-e05a-44e9-80b8-67815525d812.png)'
- en: 'We find that this line of script converts our `Data` field to type `character`
    and, finally, we can use them `as.Date` function to re-data type our values to
    an R `Date` type:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，这行脚本将我们的 `Data` 字段转换为 `character` 类型，最后我们可以使用 `as.Date` 函数将我们的值重新转为 R 中的
    `Date` 类型：
- en: '![](img/e97e3be7-be72-4bb4-bb36-660405cd5858.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e97e3be7-be72-4bb4-bb36-660405cd5858.png)'
- en: With a little trial and error, you can reformat a string or character data point
    exactly how you want it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一些反复试验，你可以将字符串或字符数据点格式化成你想要的样子。
- en: Enhancing data
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强数据
- en: Data cleaning through enhancement is another common technique where data is
    made complete (and perhaps more valuable) by adding related information, facts,
    and/or figures. The source of this additional data could be calculations using
    information already in the data or added from another source. There are a variety
    of reasons why a data scientist may take the time to enhance data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增强进行数据清洗是另一种常见的技术，它通过添加相关的信息、事实和/或数据，使数据变得完整（也许更有价值）。这些附加数据的来源可以是使用数据中已有信息的计算，或者是从另一个来源添加的数据。数据科学家可能会花时间增强数据的原因有很多种。
- en: 'Based upon the purpose or objective at hand, the information the data scientist
    adds might be used for reference, comparison, contrast, or show tendencies. Typical
    use cases include:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 根据手头的目的或目标，数据科学家添加的信息可能用于参考、比较、对比或展示趋势。典型的使用场景包括：
- en: Derived fact calculation
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 派生事实计算
- en: Indicating the use of calendar versus fiscal year
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指明使用日历年与财政年度
- en: Converting time zones
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换时区
- en: Currency conversions
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 货币转换
- en: Adding current versus previous period indicators
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加当前与上一时期的指标
- en: Calculating values such as the total units shipped per day
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每天运送的总单位数
- en: Maintaining slowly changing dimensions
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护缓慢变化的维度
- en: As a data scientist, you should always use scripting to enhance your data, as
    this approach is much better than editing a data file directly since it is less
    prone to errors and maintains the integrity of the original file. Also, creating
    scripts allows you to reapply the enhancements to multiple files and/or new versions
    of files received, without having to redo the same work.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，你应始终使用脚本来增强数据，因为这种方法比直接编辑数据文件要好得多，因为它更不容易出错，并且能保持原始文件的完整性。此外，创建脚本还允许你将增强功能应用于多个文件和/或新版本的文件，而无需重新做同样的工作。
- en: 'For a working example, let us again go back to our `GammingData`. Assume we''re
    receiving files of the `Coinin` amounts by slot machine and our gaming company
    now runs casinos outside of the continental United States. These locations are
    sending us files to be included in our statistical analysis and we''ve now discovered
    that these international files are providing the `Coinin` amounts in their local
    currencies. To be able to correctly model the data, we''ll need to convert those
    amounts to US dollars. Here is the scenario:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个工作示例，让我们再次回到我们的`GammingData`。假设我们正在接收每台老虎机的`Coinin`金额文件，并且我们的博彩公司现在在美国本土以外的地方运营赌场。这些地点向我们发送文件，以便将其纳入我们的统计分析，并且我们现在发现这些国际文件提供的`Coinin`金额是以当地货币表示的。为了能够正确地建模数据，我们需要将这些金额转换为美元。以下是这个场景：
- en: 'File Source: Great Britain'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 文件来源：英国
- en: 'Currency used: GBP or Great British Pound'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的货币：GBP 或英镑
- en: 'The formula to convert our GBP values to USD is simply the amount multiplied
    by an exchange rate. So, in R:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的GBP值转换为美元的公式很简单，就是将金额乘以汇率。因此，在R语言中：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The previous line of code will accomplish what we want; however, the data scientist
    is left to determine which currency needs to be converted (GBP) and what the exchange
    rate to be used is. Not a huge deal, but one might want to experiment with creating
    a user-defined function that determines the rate to be used, as shown next:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上一行代码将完成我们想要的操作；然而，数据科学家需要决定需要转换哪种货币（GBP）以及使用的汇率是多少。虽然这不是一个大问题，但可能有人想尝试创建一个用户定义的函数来确定要使用的汇率，如下所示：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Although the preceding code snippet is rather simplistic, it illustrates the
    point of creating logic that we can reuse later:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的代码片段相当简单，但它说明了创建我们可以稍后重复使用的逻辑这一点：
- en: '![](img/9abc7426-e90c-497a-8909-aabf86a3900a.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9abc7426-e90c-497a-8909-aabf86a3900a.png)'
- en: 'Finally, to make things better still, save off your function (in an R file)
    so that it can always be used:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了进一步改善，保存你的函数（在R文件中），这样它可以随时使用：
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Of course, in the best of all worlds, we might modify the function to look up
    the rate in a table or a file, based upon the country code, so that the rates
    can be changed to reflect the most current value and to de-couple the data from
    the program code.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在最理想的情况下，我们可能会修改该函数，通过查找表格或文件中的汇率来获取汇率，基于国家代码，这样汇率就可以反映最新的值，并将数据与程序代码解耦。
- en: Harmonization
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协同化
- en: With data harmonization, the data scientist converts, translates, or maps data
    values to other more desirable values, based upon the overall objective or purpose
    of the analysis to be performed. The most common examples of this can be gender
    or country code. For example, if your file has gender coded as `1`s and `0`s or
    `M` and `F`, you might want to convert the data points to be consistently coded
    as `MALE` or `FEMALE`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据统一化过程中，数据科学家会根据分析目标或目的，将数据值转换、翻译或映射到其他更理想的值。最常见的例子是性别或国家代码。例如，如果你的文件中性别编码为`1`和`0`，或者`M`和`F`，你可能想将数据点统一编码为`MALE`或`FEMALE`。
- en: 'With country codes, the data scientist may want to plot summations for regions:
    North America, South America, and Europe rather than USA, CA, MX, BR, CH, GB,
    FR, and DE individually. In this case, he or she would be creating aggregated
    values:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用国家代码时，数据科学家可能希望按区域绘制汇总数据：北美、南美和欧洲，而不是单独显示USA、CA、MX、BR、CH、GB、FR和DE。在这种情况下，他或她会创建汇总值：
- en: North America = USA + CA + MX
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 北美 = 美国 + 加拿大 + 墨西哥
- en: South America = BR + CH
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 南美 = 巴西 + 瑞士
- en: Europe = GB + FR + DE
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 欧洲 = 英国 + 法国 + 德国
- en: 'To make a point, perhaps the data scientist has stitched together multiple
    survey files, all containing gender, called `gender.txt`, but in various codes
    (`1`, `0`, `M`, `F`, `Male`, and `Female`). If we tried to use the R function
    table, we would see the following undesirable result:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，也许数据科学家将多个调查文件拼接在一起，这些文件都包含性别信息，文件名为`gender.txt`，但性别使用了不同的编码（`1`、`0`、`M`、`F`、`Male`和`Female`）。如果我们尝试使用R函数`table`，我们会看到以下不希望出现的结果：
- en: '![](img/f5f4ff68-574c-4b49-b9ca-5bf531893c45.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5f4ff68-574c-4b49-b9ca-5bf531893c45.png)'
- en: 'And if we visualize this with the best of expectations:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们带着最好的期待来可视化这个过程：
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We see the following screenshot:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到以下截图：
- en: '![](img/e31b73ea-1d20-48f5-88e9-ae58b6c5b863.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e31b73ea-1d20-48f5-88e9-ae58b6c5b863.png)'
- en: 'Once again, to solve the inconsistent coding of the data point gender, I''ve
    borrowed the concept from the example in the previous section and created a simple
    function to help us with our recoding:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了解决数据点性别编码不一致的问题，我借用了上一节中的示例概念，并创建了一个简单的函数来帮助我们重新编码：
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This time, I've added the `toupper` function so that we don't have to worry
    about the case, as well as `substr` to handle values that are longer than a single
    character.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我添加了`toupper`函数，以便我们不必担心字母大小写，同时使用`substr`处理长度超过一个字符的值。
- en: I am assuming the argument value will be either `0`,`1`,`m`,`M`,`f`,`F`,`Male`,
    or `Female`, otherwise an error will occur.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设参数值将是`0`、`1`、`m`、`M`、`f`、`F`、`Male`或`Female`，否则将会出现错误。
- en: 'Since R categorizes the `Gender` value as data type `factor`, I found it was
    difficult to easily make use of the simple function, so I decided to create a
    new R data frame object to hold our harmonized data. I''ve also decided to use
    a looping process to read through each case (record) in our file and convert it
    to `Male` or `Female`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于R将`Gender`值归类为数据类型`factor`，我发现很难轻松使用简单的函数，因此我决定创建一个新的R数据框对象来存储我们的统一数据。我还决定使用循环过程，逐一读取文件中的每个记录，并将其转换为`Male`或`Female`：
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we can enjoy a more appropriate visualization by writing:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过编写代码来享受更合适的可视化效果：
- en: '[PRE16]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/43d1b057-00cd-41af-a0ea-938b3b4de4c6.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43d1b057-00cd-41af-a0ea-938b3b4de4c6.png)'
- en: Standardization
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化
- en: Most mainstream data scientists have noted the importance of standardizing data
    variables (changing reference data to a standard) as part of the data cleaning
    process before beginning a statistical study or analysis project. This is important,
    as, without standardization, the data points measured using different scales will
    most likely not contribute equally to the analysis.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数主流数据科学家已注意到，在开始统计研究或分析项目之前，标准化数据变量（将参考数据更改为标准数据）是数据清理过程中的一个重要步骤。这很重要，因为如果没有标准化，使用不同尺度测量的数据点很可能不会对分析贡献相同。
- en: If you consider that a data point within a range between 0 and 100 will outweigh
    a variable within a range between 0 and 1, you can understand the importance of
    data standardization. Using these variables without standardization in effect
    gives the variable with the larger range a larger weight in the analysis. To address
    this concern and equalize the variables, the data scientists try to transform
    the data into a comparable scale.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑到一个在0到100范围内的数据点会比一个在0到1范围内的变量具有更大的权重，那么你可以理解数据标准化的重要性。没有标准化地使用这些变量，实际上会让范围更大的变量在分析中占据更大的权重。为了解决这个问题并使变量平衡，数据科学家会尝试将数据转换成一个可比的尺度。
- en: Centring (of the data points) is the most common example of data standardization
    (there are many others though). To center a data point, the data scientist would
    subtract the mean of all the data points from each individual data point in the
    file.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 居中（数据点）是数据标准化最常见的示例（当然，还有其他很多例子）。为了居中一个数据点，数据科学家会将所有数据点的均值从文件中的每个数据点中减去。
- en: Instead of doing the mathematics, R provides the `scale` function. This is a
    function whose default method centers and/or scales a column of numeric data in
    a file in one line of code. Let's look at a simple example.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: R语言提供了`scale`函数，省去了手动计算的麻烦。这是一个默认方法可以在一行代码中完成列的居中和/或缩放的函数。让我们看一个简单的例子。
- en: 'Back to our slot machines! In our gaming files, you may recall that there is
    a field named `Coinin` that contains a numeric value indicating the total dollars
    put into the machine. This is considered a measurement of the machine profitability.
    This seems like an obvious data point to use in our profitability analysis. However,
    these amounts may be misleading since there are machines of different denominations
    (in other words, some machines accept nickels while others accept dimes or dollars).
    Perhaps this difference in machine denominations creates an unequal scale. We
    can use the `scale` function to address this situation. First, we see in the following
    screenshot, the values of `Coin.in`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的老虎机！在我们的游戏文件中，你可能记得有一个名为`Coinin`的字段，其中包含一个数值，表示投放到机器中的总金额。这被视为机器盈利能力的衡量标准。这个数据点看起来是我们盈利分析中一个显而易见的选项。然而，这些金额可能具有误导性，因为存在不同面额的机器（换句话说，有些机器接受五分镍币，而有些则接受十分硬币或美元）。或许机器面额的不同造成了不平等的尺度。我们可以使用`scale`函数来解决这个问题。首先，我们可以在下图中看到`Coin.in`的值：
- en: '![](img/6bfc6e92-9f22-48e4-a624-e806aacd79d3.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6bfc6e92-9f22-48e4-a624-e806aacd79d3.png)'
- en: 'We can then write the following line of code to center our `Coin.in` data points:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以写出以下代码来居中我们的`Coin.in`数据点：
- en: '[PRE17]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The value of center determines how column centring is to be performed. Using
    center is `TRUE` causes centring to be done by subtracting the column means (omitting
    NAs) of `Coin.in` from their corresponding columns. The value of `scale` determines
    how column scaling is performed (after centring). If the scale is `TRUE`, then
    scaling is done by dividing the (centered) columns of `Coin.in` by their standard
    deviations if a center is `TRUE`, and the root mean square otherwise.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`center`的值决定了如何执行列的居中。使用`TRUE`值时，居中是通过从相应列中减去`Coin.in`的列均值（忽略NA值）来完成的。`scale`的值决定了如何执行列的缩放（在居中之后）。如果`scale`为`TRUE`，则通过将（已居中的）`Coin.in`列除以它们的标准差来进行缩放，如果`center`为`TRUE`，否则除以均方根。'
- en: 'We see the difference in the following screenshot:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下图中看到差异：
- en: '![](img/6c565dfb-7d93-4fcf-9f46-5b39e0a9b468.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c565dfb-7d93-4fcf-9f46-5b39e0a9b468.png)'
- en: Transformations
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换
- en: A thought-provoking type of data cleaning, which may be a new concept for a
    data developer, is **data transformation**. Data transformation is a process where
    the data scientist actually changes what you might expect to be valid data values
    through some mathematical operation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一种引人深思的数据清理类型，可能对数据开发者来说是一个新概念，那就是**数据转换**。数据转换是一个过程，其中数据科学家通过一些数学操作实际改变你可能认为是有效数据值的内容。
- en: Performing data transformation maps data from an original format into the format
    expected by an appropriate application or a format more convenient for a particular
    assumption or purpose. This includes value conversions or translation functions,
    as well as normalizing of numeric values to conform to the minimum and maximum
    values.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换的执行是将数据从原始格式映射到适当应用所期望的格式，或更适合特定假设或目的的格式。这包括值转换或翻译函数，以及将数值标准化以符合最小和最大值。
- en: 'As we''ve used R earlier in this chapter, we can see that the syntax of a very
    simple example of this process is simple. For example, a data scientist may decide
    to transform a given value to the square root of the value:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章前面使用 R 的示例一样，我们可以看到这个过程的一个非常简单的例子，语法也非常简单。例如，数据科学家可能决定将一个给定值转换为该值的平方根：
- en: '[PRE18]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The preceding code example informs R to create a new variable (or column in
    the `data.dat` dataset) named `trans_Y` that is equal to the square root of the
    original response variable `Y`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码示例告诉 R 创建一个新的变量（或 `data.dat` 数据集中的一列）`trans_Y`，该变量等于原始响应变量 `Y` 的平方根。
- en: While R can support just about any mathematical operation you can think of or
    have a need for, the syntax is not always intuitive. R even provides the generic
    function `transform`**,** but as of this writing, it only works with data frames.
    `transform.default` converts its first argument to a data frame and then calls
    `transform.data.frame`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 R 可以支持几乎所有你能想到或需要的数学运算，但其语法并不总是直观的。R 甚至提供了通用函数 `transform`**，**但截至本文写作时，它仅适用于数据框。`transform.default`
    将其第一个参数转换为数据框，然后调用 `transform.data.frame`。
- en: But why would you do this? Well, one reason is relationships like that wouldn't
    work well. For example, if you were experimenting with values that were different
    by orders of magnitude, it would be difficult to deal or work with them. Practical
    examples can be a comparison of the physical body weight between species or various
    sound frequencies and their effects on the atmosphere. In these examples, the
    use of log transformations enables the data scientist to graph values in a way
    that permits one to see the differences among the data points at lower values.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么要这么做呢？一个原因是像这样的关系可能不太奏效。例如，如果你正在处理数量级差异很大的值，那么处理这些值会变得困难。在实际应用中，比如比较不同物种的体重或各种声音频率及其对大气的影响。这些例子中，使用对数变换使得数据科学家能够以一种方式绘制值的图表，从而能看到在较小值之间的数据点差异。
- en: Another example is the transformation of test scores (to the distance the score
    is from a mean score).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是对测试成绩的变换（将成绩转换为距离均值的差值）。
- en: Finally, various widely used statistical methods assume normality or a normal
    distribution shape. In cases where the data scientist observes something other
    than normalcy, data transformations can be used. Data transformations such as
    log or exponential, or power transformations are typically used in an attempt
    to make the distribution of data scores that are non-normal in shape more normal.
    These data transformations can also help the data scientist bring extreme outliers
    closer to the rest of the data values; and that reduces the impact of the outliers
    on the estimates of summary statistics, such as the sample mean or correlation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，许多广泛使用的统计方法假设数据服从正态分布或呈正态分布形态。在数据科学家观察到的数据与正常分布不符的情况下，可以使用数据变换。数据变换如对数、指数或幂变换通常用于尝试使非正态形态的分布数据变得更接近正态分布。这些数据变换也有助于数据科学家将极端离群值拉近与其他数据值的距离，从而减少离群值对总结统计量（如样本均值或相关性）的影响。
- en: Deductive correction
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演绎推理
- en: With deductive reasoning, one uses known information, assumptions, or generally
    accepted rules to reach a conclusion. In statistics, a data scientist uses this
    concept (in an attempt) to repair inconsistencies and/or missing values within
    a data population.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在演绎推理中，人们使用已知的信息、假设或普遍接受的规则来得出结论。在统计学中，数据科学家使用这一概念（尝试）修复数据集中存在的不一致性和/或缺失值。
- en: To the data developer, examples of deductive correction include the idea of
    converting a string or text value to a numeric data type or flipping a sign from
    negative to positive (or vice versa). Practical examples of these instances are
    overcoming storage limitations such as when survey information is always captured
    and stored as text or when accounting needs to represent a numeric dollar value
    as an expense. In these cases, a review of the data may take place (in order to
    deduce what corrections—also known as **statistical data****editing**—need to
    be performed), or the process may be automated to affect all incoming data from
    a particular data source.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据开发人员，推理修正的例子包括将字符串或文本值转换为数值数据类型，或将符号从负数翻转为正数（或反之）。这些情况的实际例子包括克服存储限制，例如，当调查信息总是以文本形式捕获并存储，或会计需要将数值的美元金额表示为支出。在这些情况下，可能会进行数据审查（以推断需要执行哪些修正——也称为**统计数据**编辑——），或者该过程可以自动化，以影响来自特定数据源的所有传入数据。
- en: Other deductive corrections routinely performed by the data scientists include
    the corrections of input typing errors, rounding errors, sign errors, and value
    interchanges.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家常规执行的其他推理修正包括输入打字错误、四舍五入误差、符号错误和数值交换错误的修正。
- en: There are R packages and libraries available, such as the `deducorrect` package,
    which focus on the correction of rounding, typing, and sign errors, and include
    three data cleaning algorithms (`correctRounding`, `correctTypos`, and `correctSigns`).
    However, the data scientists mostly want to specially custom script a solution
    for the project at hand.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些R语言包和库可用，例如`deducorrect`包，专注于修正四舍五入、打字和符号错误，并包括三种数据清洗算法（`correctRounding`、`correctTypos`和`correctSigns`）。然而，数据科学家通常希望为当前项目特别定制编写解决方案。
- en: Deterministic imputation
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定性填充
- en: We have been discussing the topic of the data scientists deducing or determining
    how to address or correct a dirty data issue, such as missing, incorrect, incomplete,
    or inconsistent values within a data pool.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在讨论数据科学家推导或确定如何处理或修正脏数据问题的主题，例如数据池中的缺失、错误、不完整或不一致的值。
- en: When data is missing (or incorrect, incomplete, or inconsistent) within a data
    pool, it can make handling and analysis difficult and can introduce bias to the
    results of the analysis performed on the data. This leads us to imputation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据池中的数据缺失（或错误、不完整、不一致）时，会使处理和分析变得困难，并且可能引入偏差，影响对数据进行的分析结果。这就引出了填充方法。
- en: In data statistics, imputation is when, through a data cleansing procedure,
    the data scientist replaces missing (or otherwise specified) data with other values.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据统计中，填充是指通过数据清洗过程，数据科学家用其他值替换缺失（或以其他方式指定的）数据。
- en: Because missing data can create problems in analyzing data, imputation is seen
    as a way to avoid the dangers involved with simply discarding or removing altogether
    the cases with missing values. In fact, some statistical packages default to discarding
    any case that has a missing value, which may introduce bias or affect the representativeness
    of the results. Imputation preserves all the cases within the data pool by replacing
    the missing data with an estimated value based on other available information.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺失数据可能在分析数据时产生问题，填充被视为避免仅仅丢弃或完全删除缺失值案例所带来的风险的方式。实际上，一些统计软件包默认丢弃任何具有缺失值的案例，这可能引入偏差或影响结果的代表性。填充通过基于其他可用信息估算的值来替换缺失数据，从而保留数据池中的所有案例。
- en: Deterministic imputation is a process used by the data scientists in the process
    of assigning replacement values for missing, invalid, or inconsistent data that
    has failed edits. In other words, in a particular case, when specific values of
    all other fields are known and valid, if only one (missing) value will cause the
    record or case to satisfy or pass all the data scientist edits, that value will
    be imputed. Deterministic imputation is a conservation imputation theory in that
    it is aimed at cases that are simply identified (as mentioned previously) and
    may be the first situation that is considered in the automated editing and imputation
    of data.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性填充是数据科学家在为缺失、无效或不一致的数据分配替换值时使用的过程，这些数据在编辑过程中未通过。换句话说，在特定情况下，当所有其他字段的具体值已知且有效时，如果只有一个（缺失）值会使记录或案例满足或通过所有数据科学家的编辑，那么该值将被填充。确定性填充是一种保守的填充理论，它针对的是那些简单可识别的情况（如前所述），并且可能是自动化编辑和填充数据时考虑的首要情况。
- en: Currently, in the field of data science, imputation theory is gaining popularity
    and is continuing to be developed, and thus requires consistent attention to new
    information regarding the subject.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，在数据科学领域，填补理论正在获得越来越多的关注，并且持续发展，因此需要持续关注该领域的新信息。
- en: A few of the other well-known imputation theories attempting to deal with missing
    data include hot deck and cold deck imputation; listwise and pairwise deletion;
    mean imputation; regression imputation; last observation carried forward; stochastic
    imputation; and multiple imputations.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些著名的缺失数据填补理论包括热补法和冷补法；完全删除和配对删除；均值填补；回归填补；最后观察值前移法；随机填补；以及多重填补。
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we provided an overview of the fundamentals of the different
    kinds or types of statistical data cleansing. Then, using the R programming language,
    we illustrated various working examples, showing each of the best or commonly
    used data cleansing techniques.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了不同类型或种类的统计数据清洗的基本原理。然后，使用R编程语言，我们展示了各种实际示例，说明了每种最佳或常用的数据清洗技术。
- en: We also introduced the concepts of data transformation, deductive correction,
    and deterministic imputation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还介绍了数据转换、推理修正和确定性填补的概念。
- en: 'In the next chapter, we will dive deep into the topic of what data mining is
    and why it is important, and use R for the most common statistical data mining
    methods: dimensional reduction, frequent patterns, and sequences.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将深入探讨数据挖掘是什么以及为什么它很重要，并使用R进行最常见的统计数据挖掘方法：降维、频繁模式和序列。
