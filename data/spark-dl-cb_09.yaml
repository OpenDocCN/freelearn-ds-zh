- en: Predicting Apple Stock Market Cost with LSTM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LSTM预测苹果股票市场成本
- en: Stock market predictions have been going on for many years and it has spawned
    an entire industry of prognosticators. It shouldn't come as a surprise since it
    can turn a significant profit if predicted properly. Understanding when is a good
    time to buy or sell a stock is key to getting the upper hand on Wall Street. This
    chapter will focus on creating a deep learning model using LSTM on Keras to predict
    the stock market quote of AAPL.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来一直有股票市场预测，并且已经产生了整个预言家行业。这并不奇怪，因为如果预测正确，它可以带来可观的利润。了解何时是买入或卖出股票的好时机是在华尔街占据上风的关键。本章将专注于使用Keras上的LSTM创建深度学习模型来预测AAPL的股票市场报价。
- en: 'The following recipes will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下配方：
- en: Downloading stock market data for Apple
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载苹果的股票市场数据
- en: Exploring and visualizing stock market data for Apple
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索和可视化苹果的股票市场数据
- en: Preparing stock data for model performance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为模型性能准备股票数据
- en: Building the LSTM model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建LSTM模型
- en: Evaluating the LSTM model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估LSTM模型
- en: Downloading stock market data for Apple
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载苹果的股票市场数据
- en: There are many resources for downloading stock market data for Apple. For our
    purposes, we will be using the Yahoo! Finance website.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多资源可用于下载苹果的股票市场数据。对于我们的目的，我们将使用Yahoo! Finance网站。
- en: Getting ready
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This section will require initializing a Spark cluster that will be used for
    all recipes in this chapter. A Spark notebook can be initialized in the terminal
    using `sparknotebook`, as seen in the following screenshot:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将需要初始化一个Spark集群，该集群将用于本章中的所有配方。可以在终端使用`sparknotebook`初始化Spark笔记本，如下屏幕截图所示：
- en: '![](img/00266.jpeg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00266.jpeg)'
- en: 'A `SparkSession` can be initialized in a Jupyter notebook using the following
    script:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下脚本在Jupyter笔记本中初始化`SparkSession`：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to do it...
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: The following section walks through the steps for downloading historical stock
    market data for Apple.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将介绍下载苹果的历史股票市场数据的步骤。
- en: Visit the following website to track the daily historical adjusted closing stock
    value for Apple, which has a stock ticker value of AAPL: [https://finance.yahoo.com/quote/AAPL/history](https://finance.yahoo.com/quote/AAPL/history)
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问以下网站，跟踪苹果的每日历史调整收盘股票价值，其股票代码为AAPL：[https://finance.yahoo.com/quote/AAPL/history](https://finance.yahoo.com/quote/AAPL/history)
- en: 'Set and apply the following parameters to the Historical Data tab:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置并应用以下参数到历史数据选项卡：
- en: 'Time Period: Jan 01, 2000 - Apr 30, 2018.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 时间段：2000年1月1日至2018年4月30日。
- en: 'Show: Historical prices.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示：历史价格。
- en: 'Frequency: Daily.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 频率：每日。
- en: 'Download the dataset with the specified parameter to a `.csv` file by clicking
    on the Download Data link, as seen in the following screenshot:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过单击下载数据链接，使用指定参数将数据集下载到`.csv`文件中，如下屏幕截图所示：
- en: '![](img/00267.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00267.jpeg)'
- en: 'Download the file, `AAPL.csv`, and then upload the same dataset to a Spark
    dataframe using the following script:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载文件`AAPL.csv`，然后使用以下脚本将相同的数据集上传到Spark数据框中：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How it works...
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The following section explains how the stock market data is incorporated into
    a Jupyter notebook.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分解释了如何将股票市场数据纳入Jupyter笔记本。
- en: Yahoo! Finance is a great source for stock market quotes for publicly traded
    companies. The stock quote for Apple, AAPL, is traded on NASDAQ and the historical
    quotes can be captured for model development and analysis purposes. Yahoo! Finance
    gives you the option to capture stock quotes on a daily, weekly, or monthly snapshot.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yahoo! Finance是公开交易公司股票市场报价的重要来源。苹果的股票报价AAPL在纳斯达克交易，可以捕获历史报价以进行模型开发和分析。Yahoo!
    Finance提供了在每日、每周或每月快照上捕获股票报价的选项。
- en: The purpose of this chapter is to forecast stock at a daily level, as that would
    pull in the most amount of data into our training model. We can do this by tracing
    data back to January 1, 2000, all the way to April 30, 2018.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本章的目的是在每日级别预测股票，因为这将为我们的训练模型带来最多的数据。我们可以通过追溯数据到2000年1月1日，一直到2018年4月30日来实现这一点。
- en: Once our parameters are set for download, we receive a nicely formatted comma-separated
    value file from Yahoo! Finance that can be easily converted into a Spark dataframe
    with minimal issues.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们设置了下载参数，我们就会从Yahoo! Finance收到一个格式良好的逗号分隔值文件，可以很容易地转换为具有最少问题的Spark数据框。
- en: 'The dataframe will allow us to view the Date, Open, High, Low, Close, Adj Close,
    and Volume of the stock on a daily basis. The columns in the dataframe track the
    opening and closing stock values as well as the highest and lowest values traded
    during that day. The number of shares traded during the day is also captured.
    The output of the Spark dataframe, `df`, can be shown by executing `df.show()`,
    as you can see in the following screenshot:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据框将允许我们每天查看股票的日期、开盘价、最高价、最低价、收盘价、调整收盘价和成交量。数据框中的列跟踪开盘和收盘股票价值，以及当天交易的最高和最低价值。还捕获了当天交易的股票数量。Spark数据框的输出`df`可以通过执行`df.show()`来显示，如下面的屏幕截图所示：
- en: '![](img/00268.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00268.jpeg)'
- en: There's more...
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Python had stock market APIs that allowed you to automatically connect and pull
    back stock market quotes for publicly traded companies such as Apple.   You would
    be required to input parameters and retrieve the data that can be stored in a
    dataframe. However, as of April 2018, the *Yahoo! Finance* API is no longer operational
    and therefore not a reliable solution for extracting data for this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Python有股票市场API，允许您自动连接并拉取公开交易公司（如苹果）的股票市场报价。您需要输入参数并检索可以存储在数据框中的数据。然而，截至2018年4月，*Yahoo!
    Finance* API不再运作，因此不是提取本章数据的可靠解决方案。
- en: See also
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '`Pandas_datareader` is a very powerful library for extracting data from websites
    such as Yahoo! Finance. To learn more about the library and how it may connect
    back to Yahoo! Finance once it is back online, visit the following website:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pandas_datareader` 是一个非常强大的库，可以从网站上提取数据，例如 Yahoo! Finance。要了解更多关于该库以及它如何在恢复在线后与
    Yahoo! Finance 连接的信息，请访问以下网站：'
- en: '[https://github.com/pydata/pandas-datareader](https://github.com/pydata/pandas-datareader)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/pydata/pandas-datareader](https://github.com/pydata/pandas-datareader)'
- en: Exploring and visualizing stock market data for Apple
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索和可视化苹果股票市场数据
- en: Before any modeling and predictions are performed on the data, it is important
    to first explore and visualize the data at hand for any hidden gems.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据进行任何建模和预测之前，首先探索和可视化手头的数据是很重要的，以发现任何隐藏的宝藏。
- en: Getting ready
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will perform transformations and visualizations on the dataframe in this
    section. This will require importing the following libraries in Python:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对数据框进行转换和可视化。这将需要在 Python 中导入以下库：
- en: '`pyspark.sql.functions`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pyspark.sql.functions`'
- en: '`matplotlib`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`'
- en: How to do it...
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: The following section walks through the steps to explore and visualize the stock
    market data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将介绍探索和可视化股票市场数据的步骤。
- en: 'Transform the `Date` column in the dataframe by removing the timestamp using
    the following script:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本通过删除时间戳来转换数据框中的 `Date` 列：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create a for-cycle to add three additional columns to the dataframe. The loop
    breaks apart the `date` field into `year`, `month`, and `day`, as seen in the
    following script:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个循环来向数据框添加三个额外的列。循环将把 `date` 字段分解为 `year`、`month` 和 `day`，如下面的脚本所示：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Save a subset of the Spark dataframe to a `pandas` dataframe called `df_plot`
    using the following script: `df_plot = df.select('year', 'Adj Close').toPandas()`.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将 Spark 数据框的子集保存到名为 `df_plot` 的 `pandas` 数据框中：`df_plot = df.select('year',
    'Adj Close').toPandas()`.
- en: 'Graph and visualize the `pandas` dataframe, `df_plot`, inside of the notebook
    using the following script:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本在笔记本中绘制和可视化 `pandas` 数据框 `df_plot`：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Calculate the row and column count of our Spark dataframe using the following
    script: `df.toPandas().shape`.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本计算我们的 Spark 数据框的行和列数：`df.toPandas().shape`。
- en: Execute the following script to determine null values in the dataframe: `df.dropna().count()`.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本来确定数据框中的空值：`df.dropna().count()`。
- en: 'Execute the following script to pull back statistics on `Open`, `High`, `Low`,
    `Close`, and `Adj Close`:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本来获取 `Open`、`High`、`Low`、`Close` 和 `Adj Close` 的统计数据：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works...
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The following section explains the techniques used and insights gained from
    exploratory data analysis.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分解释了探索性数据分析所使用的技术和获得的见解。
- en: 'The date column in the dataframe is more of a date-time column with the time
    values all ending in 00:00:00\. This is unnecessary for what we will need during
    our modeling and therefore can be removed from the dataset. Luckily for us, PySpark
    has a `to_date` function that can do this quite easily. The dataframe, `df`, is
    transformed using the `withColumn()` function and now only shows the date column
    without the timestamp, as seen in the following screenshot:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据框中的日期列更像是一个带有时间值的日期时间列，所有时间值都以 00:00:00 结尾。这对于我们建模过程中的需求是不必要的，因此可以从数据集中删除。幸运的是，PySpark 有一个 `to_date` 函数可以很容易地做到这一点。数据框 `df` 使用 `withColumn()` 函数进行转换，现在只显示日期列而没有时间戳，如下面的屏幕截图所示：
- en: '![](img/00269.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00269.jpeg)'
- en: 'For analysis purposes, we want to extract the `day`, `month`, and `year` from
    the `date` column. We can do this by enumerating through a custom list, `date_breakdown`,
    to split the date by a `-` and then adding a new column for the year, month, and
    day using the `withColumn()` function. The updated dataframe with the newly added
    columns can be seen in the following screenshot:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了分析目的，我们想要从日期列中提取 `day`、`month` 和 `year`。我们可以通过枚举一个自定义列表 `date_breakdown` 来实现这一点，通过 `-` 分割日期，然后使用 `withColumn()` 函数为年、月和日添加新列。更新后的数据框中可以看到新添加的列，如下面的屏幕截图所示：
- en: '![](img/00270.jpeg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00270.jpeg)'
- en: One important takeaway is that `PySpark` also has a SQL function for dates that
    can extract the day, month, or year from a date timestamp. For example, if we
    were to add a month column to our dataframe, we would use the following script: `df.withColumn("month",f.month("date")).show()`.
    This is to highlight the fact that there are multiple ways to transform data within
    Spark.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的要点是，`PySpark` 也有一个用于日期的 SQL 函数，可以从日期时间戳中提取日、月或年。例如，如果我们要向数据框添加一个月份列，我们将使用以下脚本：`df.withColumn("month",f.month("date")).show()`。这是为了突出在
    Spark 中有多种方法可以转换数据。
- en: Spark dataframes are more limited in visualization features than `pandas` dataframes.
    Therefore, we will subset two columns from the Spark dataframe, `df`, and convert
    them into a `pandas` dataframe for plotting a line or time-series chart. The y-axis
    will be the adjusted close of the stock and the x-axis will be the year of the
    date.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 数据框的可视化功能比 `pandas` 数据框更有限。因此，我们将从 Spark 数据框 `df` 中提取两列，并将它们转换为 `pandas` 数据框，以绘制线形或时间序列图。y
    轴将是股票的调整收盘价，x 轴将是日期的年份。
- en: 'The pandas dataframe, df_plot, is ready to be plotted using matplotlib once
    some formatting features are set, such as the grid visibility, the figure size
    of the plot, and the labels for the title and axes. Additionally, we explicitly
    state that the index of the dataframe needs to point to the year column. Otherwise,
    the default index will appear on the x-axis and not the year. The final time-series
    plot can be seen in the following screenshot:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备好的 pandas 数据框 df_plot 可以在设置一些格式特性后使用 matplotlib 进行绘制，例如网格可见性、绘图的图形大小以及标题和轴的标签。此外，我们明确指出数据框的索引需要指向年份列。否则，默认索引将出现在
    x 轴上而不是年份。最终的时间序列图可以在下面的屏幕截图中看到：
- en: '![](img/00271.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00271.jpeg)'
- en: Apple has experienced extensive growth over the last 18 years. While a few years
    saw some downward dips, the overall trend has been a steady upward move with the
    last couple of year's stock quotes hovering between $150 and $175.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在过去的18年中，苹果经历了广泛的增长。虽然有几年出现了一些下跌，但总体趋势是稳步上升，过去几年的股票报价在150美元和175美元之间徘徊。
- en: 'We have made some changes to our dataframe so far, so it is important to get
    an inventory count of the rows and columns total as this will affect how the dataset
    is broken up for testing and training purposes later on in the chapter. As can
    be seen in the following screenshot, we have a total of 10 columns and 4,610 rows:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们对数据框进行了一些更改，因此重要的是要对行和列的总数进行清点，因为这将影响后面在本章中对数据集进行测试和训练的方式。如下截图所示，我们总共有10列和4,610行：
- en: '![](img/00272.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00272.jpeg)'
- en: When executing `df.dropna().count()`, we can see that the row count is still
    4,610, which is identical to the row count from the previous step, indicating
    that none of the rows have any null values.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当执行`df.dropna().count()`时，我们可以看到行数仍然是4,610，与上一步的行数相同，表明没有任何行具有空值。
- en: 'Finally, we can get a good read on the row count, mean, standard deviation,
    minimum, and maximum values of each of the columns that will be used in the model.
    This can help to identify whether there are anomalies in the data. One important
    thing to note is that each of the five fields that will be used in the model has
    a standard deviation higher than the mean value, indicating that the data is more
    spread out and not so clustered around the mean. The statistics for Open, High,
    Low, Close, and Adj Close can be seen in the following screenshot:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以得到每个将用于模型的列的行数、均值、标准差、最小值和最大值的良好读数。这可以帮助确定数据中是否存在异常。需要注意的一点是，将用于模型的五个字段的标准差都高于均值，表明数据更分散，而不是围绕均值聚集。可以在以下截图中看到Open、High、Low、Close和Adj
    Close的统计数据：
- en: '![](img/00273.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00273.jpeg)'
- en: There's more...
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: While dataframes in Spark do not have the same native visualization features
    that are found in `pandas` dataframes, there are companies that manage Spark for
    enterprise solutions that allow for advanced visualization capabilities through
    notebooks without having to use libraries such as `matplotlib`. Databricks is
    one such company that offers this feature.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Spark中的数据框没有`pandas`数据框中的本地可视化功能，但有些公司可以通过笔记本提供高级可视化功能，而无需使用诸如`matplotlib`之类的库。Databricks是一家提供此功能的公司之一。
- en: 'The following is an example of a visualization using the built-in features
    available in notebooks from Databricks:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用Databricks笔记本中内置功能的可视化示例：
- en: '![](img/00274.jpeg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00274.jpeg)'
- en: See also
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: To learn more about Databricks in general, visit the following website: [https://databricks.com/](https://databricks.com/).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关Databricks的更多信息，请访问以下网站：[https://databricks.com/](https://databricks.com/)。
- en: To learn more about visualizations in Databricks notebooks, visit the following
    website: [https://docs.databricks.com/user-guide/visualizations/index.html](https://docs.databricks.com/user-guide/visualizations/index.html).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解Databricks笔记本中的可视化更多信息，请访问以下网站：[https://docs.databricks.com/user-guide/visualizations/index.html](https://docs.databricks.com/user-guide/visualizations/index.html)。
- en: 'To learn more about accessing Databricks through a Microsoft Azure subscription,
    visit the following website:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何通过Microsoft Azure订阅访问Databricks的更多信息，请访问以下网站：
- en: '[https://azure.microsoft.com/en-us/services/databricks/](https://azure.microsoft.com/en-us/services/databricks/)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://azure.microsoft.com/en-us/services/databricks/](https://azure.microsoft.com/en-us/services/databricks/)'
- en: Preparing stock data for model performance
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为模型性能准备股票数据
- en: We are almost ready to build a prediction algorithm for the stock value performance
    of Apple. The remaining task at hand is to prepare the data in a manner that ensures
    the best possible predictive outcome.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好为苹果的股票价值表现构建预测算法了。手头剩下的任务是以确保最佳预测结果的方式准备数据。
- en: Getting ready
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will perform transformations and visualizations on the dataframe in this
    section. This will require importing the following libraries in Python:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对数据框执行转换和可视化。这将需要在Python中导入以下库：
- en: '`numpy`'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`MinMaxScaler()`'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MinMaxScaler()`'
- en: How to do it...
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: This section walks through the steps for preparing the stock market data for
    our model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍为我们的模型准备股票市场数据的步骤。
- en: 'Execute the following script to group the year column by the `Adj Close` count:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本将年份列按`Adj Close`计数分组：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Execute the following script to create two new dataframes for training and
    testing purposes:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本创建两个新的用于训练和测试的数据框：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Convert the two new dataframes  to `pandas` dataframes to get row and column
    counts with `toPandas()` using the following script:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将两个新数据框转换为`pandas`数据框，以获取行和列计数：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As we did previously with `df`, we visualize `trainDF` and `testDF` using the
    following script:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与`df`之前所做的一样，我们使用以下脚本可视化`trainDF`和`testDF`：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We create two new arrays, `trainArray` and `testArray`, based on the dataframes
    with the exception of the date columns using the following script:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据以下脚本创建两个新数组`trainArray`和`testArray`，除了日期列以外的数据框的数据：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In order to scale the arrays between 0 and 1, import `MinMaxScaler` from `sklearn` and
    create a function call, `MinMaxScale`, using the following script:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将数组缩放到0到1之间，从`sklearn`导入`MinMaxScaler`并创建一个函数调用`MinMaxScale`，使用以下脚本：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`MinMaxScaler` is then fit on the `trainArray` and used to create two new arrays
    that are scaled to fit using the following script:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后在`trainArray`上拟合`MinMaxScaler`并使用以下脚本创建两个新数组，以便进行缩放：
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Split both `testingArray` and `trainingArray`  into features, `x`, and label,
    `y`, using the following script:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将`testingArray`和`trainingArray`拆分为特征`x`和标签`y`：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Execute the following script to retrieve a final inventory of the shape of
    all four arrays:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本以检索所有四个数组的最终形状清单：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Execute the following script to plot the training array for the quotes `open`,
    `high`, `low`, and `close` :'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本来绘制报价`open`、`high`、`low`和`close`的训练数组：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Additionally, we plot the training array for `volume` using the following script:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们使用以下脚本绘制`volume`的训练数组：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works...
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: This section explains the transformations needed on the data to be used in the
    model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将解释数据在模型中使用时所需的转换。
- en: 'One of the first steps to building a model is splitting the data into a training
    and test dataset for model evaluation purposes. Our goal is to use all of the
    stock quotes from 2000 through 2016 to predict stock trends in 2017-2018\. We
    know from previous sections that we have a total of 4,610 days of stock quotes,
    but we don''t know exactly how many fall in each year. We can use the `groupBy()`
    function within the dataframe to get a unique count of stock quotes per year,
    as can be seen in the following screenshot:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立模型的第一步之一是将数据分割为训练和测试数据集，以进行模型评估。我们的目标是使用2000年至2016年的所有股票报价来预测2017年至2018年的股票趋势。我们知道从前面的部分我们有总共4,610天的股票报价，但我们不知道每年有多少。我们可以使用数据框中的`groupBy()`函数来获取每年股票报价的唯一计数，如下图所示：
- en: '![](img/00275.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00275.jpeg)'
- en: 2016 and 2017's combined data represents approximately 7% of the total data,
    which is a bit small for a testing dataset. However, for the purposes of this
    model, it should be sufficient. The remaining 93% of the dataset will be used
    for training purposes between 2000 and 2016\. Therefore, two dataframes are created
    using a filter to determine whether to include or exclude rows before or after
    2016.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2016年和2017年的合并数据大约占总数据的7%，这对于测试数据集来说有点小。但是，对于这个模型的目的来说，应该是足够的。剩下的93%的数据将用于2000年至2016年的训练。因此，使用筛选器创建了两个数据框，以确定是否包括或排除2016年之前或之后的行。
- en: 'We can now see that the test dataset, `testDF`, contains 333 rows and that
    the training dataset, `trainDF`, contains 4,277 rows. When both are combined,
    we reach our total row count from our original dataframe, `df`, of 4,610\. Finally,
    we see that `testDF` is comprised of 2017 and 2018 data only, which is 251 rows
    for 2017 and 82 rows for 2018 for a total of 333 rows, as can be seen in the following
    screenshot:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以看到测试数据集`testDF`包含333行，而训练数据集`trainDF`包含4,277行。当两者合并时，我们可以得到原始数据框`df`的总行数为4,610。最后，我们看到`testDF`仅由2017年和2018年的数据组成，2017年有251行，2018年有82行，总共333行，如下图所示：
- en: '![](img/00276.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00276.jpeg)'
- en: Please note that anytime we are converting a Spark dataframe to a `pandas` dataframe
    it may not always scale for big data.  While it will work for our specific example
    as we are using a relatively small dataset, the conversion to a `pandas` dataframe
    means that all of the data is loaded into the memory of the driver.  Once this
    conversion occurs, the data is not stored in the Spark worker nodes but is instead
    to the main driver node.  This is not optimal and may produce an out of memory
    error.  If you find that you need to convert to a `pandas` dataframe from Spark
    to visualize data it is recommended to pull a random sample from Spark or to aggregate
    the spark data to a more manageable dataset and then visualize in `pandas`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每当我们将Spark数据框转换为`pandas`数据框时，它可能不适用于大数据。虽然对于我们的特定示例它可以工作，因为我们使用的是相对较小的数据集，但是将数据转换为`pandas`数据框意味着所有数据都加载到驱动程序的内存中。一旦发生这种转换，数据就不会存储在Spark工作节点中，而是存储在主驱动节点中。这并不是最佳的做法，可能会产生内存不足的错误。如果您发现需要将Spark转换为`pandas`数据框来可视化数据，建议从Spark中提取一个随机样本，或者将Spark数据聚合到一个更易管理的数据集中，然后在`pandas`中进行可视化。
- en: 'Both testing and training dataframes can be visualized using `matplotlib` once
    a subset of the data is converted using `toPandas()` to leverage the built-in
    graphing capabilities of `pandas`. Visualizing the dataframes side by side showcases
    how the graphs appear to be similar when the y-axis for adjusted close is not
    scaled. In reality, we can see that `trainDF_plot` starts close to 0, but `testDF_plot`
    starts closer to 110, as seen in the following two screenshots:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦将数据的子集转换为`toPandas()`以利用`pandas`的内置绘图功能，就可以使用`matplotlib`可视化测试和训练数据框。将数据框并排可视化展示了当未缩放调整收盘价的y轴时，图表看起来相似。实际上，我们可以看到`trainDF_plot`从0开始，而`testDF_plot`从110开始，如下两个截图所示。
- en: '![](img/00277.jpeg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00277.jpeg)'
- en: '![](img/00278.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00278.jpeg)'
- en: 'Our stock values, as they stand, don''t lend themselves well to deep learning
    modeling because there isn''t a baseline for normalization or standardization.
    When working with neural networks, it is best to keep the values between 0 and
    1 to match outcomes found in sigmoid or step functions that are used for activation.
    In order for us to accomplish this, we must first convert our `pyspark` dataframes,
    `trainDF` and `testDF`, into `numpy` arrays, these being `trainArray` and `testArray`.
    As these are now arrays and not dataframes, we will not be using the date column
    as the neural network is only interested in numerical values. The first values
    in each can be seen in the following screenshot:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前我们的股票价值不适合深度学习建模，因为没有归一化或标准化的基线。在使用神经网络时，最好将值保持在0到1之间，以匹配Sigmoid或Step函数中的结果，这些函数用于激活。为了实现这一点，我们必须首先将`pyspark`数据框`trainDF`和`testDF`转换为`numpy`数组，即`trainArray`和`testArray`。由于这些现在是数组而不是数据框，我们将不再使用日期列，因为神经网络只对数值感兴趣。每个数组的第一个值可以在以下截图中看到：
- en: '![](img/00279.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00279.jpeg)'
- en: 'There are many ways to scale array values to a range between 0 and 1\. It involves
    using the following formula: `scaled array value = (array value - min array value)
    / (max array value - min array value)`. Fortunately, we do not need to manually
    make this calculation on arrays. We can leverage the `MinMaxScaler()` function
    from `sklearn` to scale down both arrays.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有许多方法可以将数组值缩放到0到1之间的范围。它涉及使用以下公式：`缩放后的数组值 = (数组值 - 最小数组值) / (最大数组值 - 最小数组值)`。幸运的是，我们不需要手动计算数组的值。我们可以利用`sklearn`中的`MinMaxScaler()`函数来缩放这两个数组。
- en: 'The `MinMaxScaler()` function is fit on the training array, `trainArray`, and
    is then applied to create two brand new arrays, `trainingArray` and `testingArray`,
    that are scaled to values between 0 and 1\. The first row for each array can be
    seen in the following screenshot:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MinMaxScaler()`函数适用于训练数组`trainArray`，然后应用于创建两个全新的数组`trainingArray`和`testingArray`，它们的值在0到1之间进行了缩放。每个数组的第一行可以在下面的截图中看到：'
- en: '![](img/00280.jpeg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00280.jpeg)'
- en: 'We are now ready to set our label and feature variables by slicing up the array
    into x and y for both testing and training purposes. The first five elements in
    the array are the features or the x values and the last element is the label or
    y value. The features are composed of the values from Open, High, Low, Close,
    and Volume. The label is composed of Adj Close. The breakout of the first row
    for `trainingArray` can be seen in the following screenshot:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备通过将数组切片为测试和训练目的的x和y来设置我们的标签和特征变量。数组中的前五个元素是特征或x值，最后一个元素是标签或y值。特征由Open、High、Low、Close和Volume的值组成。标签由Adj
    Close组成。`trainingArray`的第一行的拆分可以在下面的截图中看到：
- en: '![](img/00281.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00281.jpeg)'
- en: 'A final look at the shape of the four arrays that we will be using in the model
    can be used to confirm that we have 4,227 matrix rows of training data, 333 matrix
    rows of test data, 5 elements for features (`x`), and 1 element for the label
    (`y`), as can be seen in the following screenshot:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将查看我们在模型中将要使用的四个数组的形状，以确认我们有4,227个训练数据矩阵行，333个测试数据矩阵行，5个特征元素(`x`)和1个标签元素(`y`)，如下截图所示：
- en: '![](img/00282.jpeg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00282.jpeg)'
- en: 'The values for the training array, `xtrain`, for open, low, high, and close
    can be plotted using the newly adjusted scales between 0 and 1 for the quotes,
    as shown in the following screenshot:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练数组`xtrain`的open、low、high和close的值可以使用新调整的0到1之间的标度绘制报价，如下截图所示：
- en: '![](img/00283.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00283.jpeg)'
- en: 'Additionally, to volume can also be plotted with the scaled volume scores between
    0 and 1, as shown in the following screenshot:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，`volume`也可以使用0到1之间的缩放体积得分绘制，如下截图所示：
- en: '![](img/00284.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00284.jpeg)'
- en: There's more...
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: While we did use `MinMaxScaler` from `sklearn`, it is also important to understand
    that there is also a `MinMaxScaler` function that is available directly through
    `pyspark.ml.feature`. It works exactly the same way by rescaling each feature
    to a value between 0 and 1\. Had we used a machine learning library natively through
    PySpark in this chapter to make our prediction, we would have used `MinMaxScaler`
    from `pyspark.ml.feature`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们使用了来自`sklearn`的`MinMaxScaler`，但也很重要的是要了解，`pyspark.ml.feature`中也有一个`MinMaxScaler`函数可供使用。它的工作方式与`sklearn`完全相同，通过将每个特征重新缩放为0到1之间的值。如果我们在本章中使用了PySpark中的机器学习库来进行预测，我们将使用`pyspark.ml.feature`中的`MinMaxScaler`。
- en: See also
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'To learn more about `MinMaxScaler` from `sklearn`, visit the following website:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解来自`sklearn`的`MinMaxScaler`的更多信息，请访问以下网站：
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html.](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html.](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)'
- en: 'To learn more about `MinMaxScaler` from `pyspark`, visit the following website:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解来自`pyspark`的`MinMaxScaler`的更多信息，请访问以下网站：
- en: '[https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler.](https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler.](https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler)'
- en: Building the LSTM model
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建LSTM模型
- en: The data is now in a format compatible with model development in Keras for LSTM
    modeling. Therefore, we will spend this section setting up and configuring the
    deep learning model for predicting stock quotes for Apple in 2017 and 2018.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据以符合Keras用于LSTM建模的模型开发格式。因此，我们将在本节中设置和配置深度学习模型，以预测2017年和2018年苹果股票报价。
- en: Getting ready
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will perform model management and hyperparameter tuning of our model in
    this section. This will require importing the following libraries in Python:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对模型进行模型管理和超参数调整。这将需要在Python中导入以下库：
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How to do it...
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: This section walks through the steps to setting up and tuning the LSTM model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍设置和调整LSTM模型的步骤。
- en: 'Import the following libraries from `keras` using the following script:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本从`keras`导入以下库：
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Build a `Sequential` model using the following script:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本构建一个`Sequential`模型：
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Transform the testing and training data sets into three-dimensional arrays
    using the following script:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将测试和训练数据集转换为三维数组：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Fit the  `model` using a variable called `loss` with the following script:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本使用名为`loss`的变量来`fit`模型：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Create a new array, `predicted`, using the following script:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本创建一个新数组`predicted`：
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Combine the `predicted` and `ytest` arrays into a single unified array, `combined_array`,
    using the following script:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将`predicted`和`ytest`数组合并成一个统一的数组`combined_array`：
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: How it works...
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section explains how the LSTM neural network model is configured to train
    on our dataset.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了如何配置LSTM神经网络模型以在我们的数据集上进行训练。
- en: Most of the functionality from `keras` used to build the LSTM model will come
    from `models` and `layers`.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大部分用于构建LSTM模型的`keras`功能将来自`models`和`layers`。
- en: The `LSTM` model that has been built will be defined using a `Sequential` class
    that works well with time series that are sequence dependent. The LSTM model has
    an `input_shape = (1,5)` for one dependent variable and five independent variables
    in our training dataset. Only one `Dense` layer will be used to define the neural
    network as we are looking to keep the model simple. A loss function is required
    when compiling a model in keras, and since we are performing it on a recurrent
    neural network, a `mean_squared_error` calculation is best to determine how close
    the predicted value is to the actual value. Finally, an optimizer is also defined
    when the model is compiled to adjust the weights in the neural network. `adam`
    has given good results, especially when being used with recurrent neural networks.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建的`LSTM`模型将使用`Sequential`类进行定义，该类与依赖于序列的时间序列非常匹配。LSTM模型的`input_shape = (1,5)`，表示我们的训练数据集中有一个因变量和五个自变量。只使用一个`Dense`层来定义神经网络，因为我们希望保持模型简单。在keras中编译模型时需要一个损失函数，由于我们正在对递归神经网络进行操作，因此最好使用`mean_squared_error`计算来确定预测值与实际值的接近程度。最后，在编译模型时还需要定义一个优化器来调整神经网络中的权重。`adam`在递归神经网络中表现良好，尤其是在使用时。
- en: 'Our current arrays, `xtrain` and `xtest`, are currently two-dimensional arrays;
    however, to incorporate them into the LSTM model, they will need to be converted
    to three-dimensional arrays using `reshape()`, as shown in the following screenshot:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们当前的数组`xtrain`和`xtest`目前是二维数组；然而，为了将它们纳入LSTM模型中，它们需要使用`reshape()`转换为三维数组，如下面的屏幕截图所示：
- en: '![](img/00285.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00285.jpeg)'
- en: The LSTM model is fit with `xtrain` and `ytrain` and the batch size is set to
    10 with 100 epochs. The batch size is the setting that defines the number of objects
    that are trained together. We can go as low or as high as we like in terms of
    setting the batch size, keeping in mind that the lower the number of batches,
    the more memory is required. Additionally, an epoch is a measurement of how often
    the model goes through the entire dataset. Ultimately, these parameters can be
    tuned based on time and memory allotment.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM模型使用`xtrain`和`ytrain`进行拟合，批量大小设置为10，时期数设置为100。批量大小是定义一起训练的对象数量的设置。我们可以根据需要设置批量大小的大小，但要记住，批量数量越低，需要的内存就越多。此外，时期是模型遍历整个数据集的次数的度量。最终，这些参数可以根据时间和内存分配进行调整。
- en: 'The mean squared error loss in each epoch is captured and visualized. After
    the fifth or sixth epoch, we can see that the loss tapers off, as shown in the
    following screenshot:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时期的均方误差损失都被捕获并可视化。在第五或第六个时期之后，我们可以看到损失逐渐减小，如下面的屏幕截图所示：
- en: '![](img/00286.jpeg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00286.jpeg)'
- en: We can now create a new array, `predicted`, based on the fitted model applied
    on `xtest` and then combine it with `ytest` to compare them side by side for accuracy
    purposes.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以创建一个新数组`predicted`，基于应用于`xtest`的拟合模型，然后将其与`ytest`结合在一起，以便进行准确性比较。
- en: See also
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: To learn more about parameter tuning models within keras, visit the following
    website: [https://keras.io/models/model/](https://keras.io/models/model/)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于keras中参数调整模型的信息，请访问以下网站：[https://keras.io/models/model/](https://keras.io/models/model/)
- en: Evaluating the model
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Here''s the moment of truth: we are going to see if our model is able to give
    us a good prediction for the AAPL stock in 2017 and 2018.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在到了关键时刻：我们将看看我们的模型是否能够为2017年和2018年的AAPL股票提供良好的预测。
- en: Getting ready
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will perform a model evaluation using the mean squared error. Therefore,
    we will need to import the following library:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用均方误差进行模型评估。因此，我们需要导入以下库：
- en: '[PRE24]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How to do it...
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: This section walks through visualizing and calculating the predicted vs. actual
    stock quotes for Apple in 2017 and 2018.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了可视化和计算2017年和2018年苹果公司预测与实际股票报价的过程。
- en: 'Plot a side by side comparison of `Actual` versus `Predicted` stock to compare
    trends using the following script:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制`Actual`与`Predicted`股票的并排比较图，使用以下脚本：
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Calculate the mean squared error between the actual `ytest` versus `predicted` stock
    using the following script:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本计算实际`ytest`与`predicted`股票之间的均方误差：
- en: '[PRE26]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How it works...
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: This section explains the results of the LSTM model's evaluation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了LSTM模型评估的结果。
- en: 'From a graphical perspective, we can see that our predictions were close to
    the actual stock quotes from 2017-2018, as shown in the following screenshot:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从图形上看，我们可以看到我们的预测与2017年至2018年的实际股票报价非常接近，如下面的屏幕截图所示：
- en: '![](img/00287.jpeg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00287.jpeg)'
- en: 'Our model shows that the predicted values are closer to the actual values earlier
    on in the days for 2017 and 2018 than later on.  Overall, while it seems that
    our predicted and actual scores are very close, it would be best to get a mean
    squared error calculation to understand how much deviation is between the two.
    As we can see, we have a mean squared error of 0.05841 or approximately 5.8%:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的模型显示，对于2017年和2018年的前几天，预测值与实际值更接近。总的来说，虽然我们的预测值和实际得分似乎非常接近，但最好还是进行均方误差计算，以了解两者之间的偏差有多大。正如我们所看到的，我们的均方误差为0.05841，约为5.8%。
- en: '![](img/00288.jpeg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00288.jpeg)'
- en: See also
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'In order to learn more about how the mean squared error is calculated within
    sklearn, visit the following website:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于sklearn中如何计算均方误差的信息，请访问以下网站：
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)。'
