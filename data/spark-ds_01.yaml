- en: Chapter 1.  Big Data and Data Science – An Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。大数据和数据科学-介绍
- en: '*Big data is definitely a big deal!* It promises a wealth of opportunities
    by deriving hidden insights in huge data silos and by opening new avenues to excel
    in business. Leveraging **big data** through advanced analytics techniques has
    become a no-brainer for organizations to create and maintain their competitive
    advantage.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*大数据绝对是一件大事！*它承诺通过从庞大的数据堆中获取隐藏的见解，并开辟新的业务发展途径，为组织创造和保持竞争优势提供了丰富的机会。通过先进的分析技术利用大数据已经成为组织的必然选择。'
- en: This chapter explains what big data is all about, the various challenges with
    big data analysis and how **Apache Spark** pitches in as the de facto standard
    to address computational challenges and also serves as a data science platform.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章解释了大数据的全部内容，大数据分析的各种挑战，以及Apache Spark如何成为解决计算挑战的事实标准，并且还作为数据科学平台。
- en: 'The topics covered in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题如下：
- en: Big data overview - what is all the fuss about?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据概述-为什么如此重要？
- en: Challenges with big data analytics - why was it so difficult?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据分析的挑战-为什么如此困难？
- en: Evolution of big data analytics - the data analytics trend
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据分析的演变-数据分析趋势
- en: Spark for data analytics - the solution to big data challenges
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分析的Spark-解决大数据挑战的解决方案
- en: The Spark stack - all that makes it up for a complete big data solution
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark堆栈-构成完整大数据解决方案的一切
- en: Big data overview
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据概述
- en: Much has already been spoken and written about what big data is, but there is
    no specific standard as such to clearly define it. It is actually a relative term
    to some extent. Whether small or big, your data can be leveraged only if you can
    analyze it properly. To make some sense out of your data, the right set of analysis
    techniques is needed and selecting the right tools and techniques is of utmost
    importance in data analytics. However, when the data itself becomes a part of
    the problem and the computational challenges need to be addressed prior to performing
    data analysis, it becomes a big data problem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 关于大数据已经有很多言论和文章，但没有明确的标准来清晰地定义它。在某种程度上，这实际上是一个相对的术语。无论数据是小还是大，只有当你能够正确地分析它时，你才能利用它。为了从数据中得出一些意义，需要正确的分析技术，并且在数据分析中选择正确的工具和技术至关重要。然而，当数据本身成为问题的一部分，并且在执行数据分析之前需要解决计算挑战时，它就成为了一个大数据问题。
- en: A revolution took place in the World Wide Web, also referred to as Web 2.0,
    which changed the way people used the Internet. Static web pages became interactive
    websites and started collecting more and more data. Technological advancements
    in cloud computing, social media, and mobile computing created an explosion of
    data. Every digital device started emitting data and many other sources started
    driving the data deluge. The dataflow from every nook and corner generated varieties
    of voluminous data, at speed! The formation of big data in this fashion was a
    natural phenomenon, because this is how the World Wide Web had evolved and no
    explicit efforts were involved in specifics. This is about the past! If you consider
    the change that is happening now, and is going to happen in future, the volume
    and speed of data generation is beyond what one can anticipate. I am propelled
    to make such a statement because every device is getting smarter these days, thanks
    to the **Internet of Things** (**IoT**).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在万维网上发生了一场革命，也被称为Web 2.0，改变了人们使用互联网的方式。静态网页变成了互动网站，并开始收集越来越多的数据。云计算、社交媒体和移动计算的技术进步创造了数据爆炸。每个数字设备开始发出数据，许多其他来源开始驱动数据洪流。来自各个角落的数据流产生了各种大量的数据，速度之快！以这种方式形成大数据是一种自然现象，因为这就是万维网的演变方式，没有明确的特定努力。这是关于过去的事情！如果考虑到正在发生的变化，以及未来将会发生的变化，数据生成的数量和速度超出了人们的预期。我之所以要做出这样的表态，是因为如今每个设备都变得更加智能，这要感谢物联网（IoT）。
- en: The IT trend was such that the technological advancements also facilitated the
    data explosion. Data storage had experienced a paradigm shift with the advent
    of cheaper clusters of online storage pools and the availability of commodity
    hardware with bare minimal price. Storing data from disparate sources in its native
    form in a single data lake was rapidly gaining over carefully designed data marts
    and data warehouses. Usage patterns also shifted from rigid schema-driven, RDBMS-based
    approaches to schema-less, continuously available **NoSQL** data-store-driven
    solutions. As a result, the rate of data creation, whether structured, semi-structured,
    or unstructured, started accelerating like never before.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: IT趋势是技术进步也促进了数据爆炸。随着更便宜的在线存储集群和廉价的通用硬件的出现，数据存储经历了一次范式转变。将来自不同来源的数据以其原生形式存储在单一数据湖中，迅速取代了精心设计的数据集市和数据仓库。使用模式也从严格的基于模式的RDBMS方法转变为无模式、持续可用的NoSQL数据存储驱动的解决方案。因此，无论是结构化、半结构化还是非结构化的数据创建速度都加速了前所未有的速度。
- en: Organizations are very much convinced that not only can specific business questions
    be answered by leveraging big data; it also brings in opportunities to cover the
    uncovered possibilities in businesses and address the uncertainties associated
    with this. So, apart from the natural data influx, organizations started devising
    strategies to generate more and more data to maintain their competitive advantages
    and to be future ready. Here, an example would help to understand this better.
    Imagine sensors are installed on the machines of a manufacturing plant which are
    constantly emitting data, and hence the status of the machine parts, and a company
    is able to predict when the machine is going to fail. It lets the company prevent
    a failure or damage and avoid unplanned downtime, saving a lot of money.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 组织非常确信，利用大数据不仅可以回答特定的业务问题，还可以带来机会，以覆盖业务中未发现的可能性，并解决与此相关的不确定性。因此，除了自然的数据涌入外，组织开始制定策略，产生越来越多的数据，以保持其竞争优势并做好未来准备。举个例子来更好地理解这一点。想象一下，在制造工厂的机器上安装了传感器，这些传感器不断地发出数据，因此可以得知机器零部件的状态，公司能够预测机器何时会发生故障。这让公司能够预防故障或损坏，避免计划外停机，从而节省大量资金。
- en: Challenges with big data analytics
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据分析的挑战
- en: There are broadly two types of formidable challenges in the analysis of big
    data. The first challenge is the requirement for a massive computation platform,
    and once it is in place, the second challenge is to analyze and make sense out
    of huge data at scale.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据分析中，主要存在两种类型的严峻挑战。第一个挑战是需要一个庞大的计算平台，一旦建立起来，第二个挑战就是在规模上分析和理解大量数据。
- en: Computational challenges
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算挑战
- en: With the increase in data, the storage requirement for big data also grew more
    and more. Data management became a cumbersome task. The latency involved in accessing
    the disk storage due to the seek time became the major bottleneck even though
    the processing speed of the processor and the frequency of RAM were up to the
    mark.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据量的增加，大数据的存储需求也越来越大。数据管理变成了一项繁琐的任务。尽管处理器的处理速度和RAM的频率达到了标准，但由于寻道时间导致的访问磁盘存储的延迟成为了主要瓶颈。
- en: Fetching structured and unstructured data from across the gamut of business
    applications and data silos, consolidating them, and processing them to find useful
    business insights was challenging. There were only a few applications that could
    address any one area, or just a few areas of diversified business requirement.
    However, integrating those applications to address most of the business requirements
    in a unified way only increased the complexity.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从各种业务应用程序和数据孤岛中提取结构化和非结构化数据，对其进行整合并加工以找到有用的业务见解是具有挑战性的。只有少数应用程序能够解决任何一个领域，或者只能解决少数多样化的业务需求。然而，将这些应用程序集成在一起以统一方式解决大部分业务需求只会增加复杂性。
- en: To address these challenges, people turned to the distributed computing framework
    with distributed file system, for example, Hadoop and **Hadoop Distributed File
    System** (**HDFS**). This could eliminate the latency due to disk I/O, as the
    data could be read in parallel across the cluster of machines.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，人们转向了具有分布式文件系统的分布式计算框架，例如Hadoop和**Hadoop分布式文件系统**（**HDFS**）。这可以消除由于磁盘I/O而产生的延迟，因为数据可以在机器集群上并行读取。
- en: Distributed computing technologies had existed for decades before, but gained
    more prominence only after the importance of big data was realized in the industry.
    So, technology platforms such as Hadoop and HDFS or Amazon S3 became the industry
    standard. On top of Hadoop, many other solutions such as Pig, Hive, Sqoop, and
    others were developed to address different kinds of industry requirements such
    as storage, **Extract, Transform, and Load** (**ETL**), and data integration to
    make Hadoop a unified platform.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算技术在之前已存在几十年，但直到行业意识到大数据的重要性后才变得更加突出。因此，诸如Hadoop和HDFS或Amazon S3之类的技术平台成为了行业标准。除了Hadoop之外，还开发了许多其他解决方案，如Pig、Hive、Sqoop等，以满足不同类型的行业需求，如存储、**提取、转换和加载**（**ETL**）以及数据集成，从而使Hadoop成为一个统一的平台。
- en: Analytical challenges
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析挑战
- en: Analyzing data to find some hidden insights has always been challenging because
    of the additional intricacies involved in dealing with huge datasets. The traditional
    BI and OLAP solutions could not address most of the challenges that arose due
    to big data. As an example, if there were multiple dimensions to a dataset, say
    100, it got really difficult to compare these variables with one another to draw
    a conclusion because there would be around 100C2 combinations for it. Such cases
    required statistical techniques such as *correlation* and the like to find the
    hidden patterns.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 分析数据以发现一些隐藏的见解一直是具有挑战性的，因为处理大型数据集涉及到额外的复杂性。传统的BI和OLAP解决方案无法解决由大数据带来的大部分挑战。举个例子，如果数据集有多个维度，比如100个，那么很难将这些变量相互比较以得出结论，因为会有大约100C2种组合。这种情况需要使用统计技术，如*相关性*等，来发现隐藏的模式。
- en: Though there were statistical solutions to many problems, it got really difficult
    for data scientists or analytics professionals to slice and dice the data to find
    intelligent insights unless they loaded the entire dataset into a **DataFrame**
    in memory. The major roadblock was that most of the general-purpose algorithms
    for statistical analysis and machine learning were single-threaded and written
    at a time when datasets were usually not so huge and could fit in the RAM on a
    single computer. Those algorithms written in R or Python were no longer very useful
    in their native form to be deployed on a distributed computing environment because
    of the limitation of in-memory computation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多问题都有统计解决方案，但对于数据科学家或分析专业人员来说，除非他们将整个数据集加载到内存中的**DataFrame**中，否则很难对数据进行切片和切块以找到智能洞见。主要障碍在于，大多数用于统计分析和机器学习的通用算法都是单线程的，并且是在数据集通常不那么庞大且可以适应单台计算机的RAM中编写的时代编写的。这些用R或Python编写的算法在原始形式上在分布式计算环境中部署时不再非常有用，因为存在内存计算的限制。
- en: To address this challenge, statisticians and computer scientists had to work
    together to rewrite most of the algorithms that would work well in a distributed
    computing environment. Consequently, a library called **Mahout** for machine learning
    algorithms was developed on Hadoop for parallel processing. It had most of the
    common algorithms that were being used most often in the industry. Similar initiatives
    were taken for other distributed computing frameworks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一挑战，统计学家和计算机科学家不得不共同努力，重写大部分算法，使其在分布式计算环境中能够良好运行。因此，在Hadoop上开发了一个名为**Mahout**的库，用于机器学习算法的并行处理。它包含了行业中经常使用的大多数常见算法。其他分布式计算框架也采取了类似的举措。
- en: Evolution of big data analytics
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据分析的演变
- en: The previous section outlined how the computational and data analytics challenges
    were addressed for big data requirements. It was possible because of the convergence
    of several related trends such as low-cost commodity hardware, accessibility to
    big data, and improved data analytics techniques. Hadoop became a cornerstone
    in many large, distributed data processing infrastructures.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节概述了如何解决大数据需求的计算和数据分析挑战。这是可能的，因为多个相关趋势的融合，如低成本的通用硬件、大数据的可访问性和改进的数据分析技术。Hadoop成为许多大型分布式数据处理基础设施的基石。
- en: However, people soon started realizing the limitations of Hadoop. Hadoop solutions
    were best suited for only specific types of big data requirements such as ETL;
    it gained popularity for such requirements only.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，人们很快意识到Hadoop的局限性。Hadoop解决方案最适合特定类型的大数据需求，如ETL；它只在这些需求中才变得流行。
- en: There were scenarios when data engineers or analysts had to perform ad hoc queries
    on the data sets for interactive data analysis. Every time they ran a query on
    Hadoop, the data was read from the disk (HDFS-read) and loaded into the memory
    - which was a costly affair. Effectively, jobs were running at the speed of I/O
    transfers over the network and cluster of disks, instead of the speed of CPU and
    RAM.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有时数据工程师或分析师需要对数据集执行即席查询进行交互式数据分析。每次在Hadoop上运行查询时，数据都会从磁盘（HDFS读取）读取并加载到内存中，这是一项昂贵的工作。实际上，作业的运行速度取决于网络和磁盘集群上的I/O传输速度，而不是CPU和RAM的速度。
- en: 'The following is a pictorial representation of the scenario:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是情景的图示表示：
- en: '![Evolution of big data analytics](img/image_01_001.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![大数据分析的演变](img/image_01_001.jpg)'
- en: One more case where Hadoop's MapReduce model could not fit in well was with
    machine learning algorithms that were iterative in nature. Hadoop MapReduce was
    underperforming, with huge latency in iterative computation. Since MapReduce had
    a restricted programming model with forbidden communication between Map and Reduce
    workers, the intermediate results needed to be stored in a stable storage. So,
    those were pushed on to the HDFS, which in turn writes into the instead of saving
    in RAM and then loading back in the memory for the subsequent iteration, similarly
    for the rest of the iterations. The number of disk I/O was dependent on the number
    of iterations involved in an algorithm and this was topped with the serialization
    and deserialization overhead while saving and loading the data. Overall, it was
    computationally expensive and could not get the level of popularity compared to
    what was expected of it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的MapReduce模型无法很好地适应迭代性质的机器学习算法。Hadoop MapReduce在迭代计算中性能不佳，延迟巨大。由于Map和Reduce工作者之间禁止通信的受限编程模型，中间结果需要存储在稳定的存储器中。因此，这些结果被推送到HDFS，然后写入磁盘，而不是保存在RAM中，然后在后续迭代中重新加载到内存中，其他迭代也是如此。磁盘I/O的数量取决于算法中涉及的迭代次数，这还伴随着在保存和加载数据时的序列化和反序列化开销。总的来说，这是计算昂贵的，与预期相比，并没有达到预期的受欢迎程度。
- en: 'The following is a pictorial representation of this scenario:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这种情景的图示表示：
- en: '![Evolution of big data analytics](img/image_01_002.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![大数据分析的演变](img/image_01_002.jpg)'
- en: To address this, tailor-made solutions were developed, for example, Google's
    Pregel, which was an iterative graph processing algorithm and was optimized for
    inter-process communication and in-memory storage for the intermediate results
    to make it run faster. Similarly, many other solutions were developed or redesigned
    that would best suit some of the specific needs that the algorithms used were
    designed for.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，开发了定制解决方案，例如谷歌的Pregel，这是一种迭代图处理算法，针对进程间通信和中间结果的内存存储进行了优化，以使其运行更快。类似地，还开发或重新设计了许多其他解决方案，以最好地满足一些特定的算法使用的特定需求。
- en: Instead of redesigning all the algorithms, a general-purpose engine was needed
    that could be leveraged by most of the algorithms for in-memory computation on
    a distributed computing platform. It was also expected that such a design would
    result in faster execution of iterative computation and ad hoc data analysis.
    This is how the Spark project paved its way out at the AMPLab at UC Berkeley.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要重新设计所有算法，而是需要一个通用引擎，大多数算法可以利用它在分布式计算平台上进行内存计算。人们也期望这样的设计会导致迭代计算和临时数据分析的更快执行。这就是Spark项目在加州大学伯克利分校的AMPLab中开辟道路的方式。
- en: Spark for data analytics
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于数据分析的Spark
- en: Soon after the Spark project was successful in the AMP labs, it was made open
    source in 2010 and transferred to the Apache Software Foundation in 2013\. It
    is currently being led by Databricks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在AMPLab中，Spark项目成功之后，它于2010年开源，并于2013年转移到Apache软件基金会。目前由Databricks领导。
- en: 'Spark offers many distinct advantages over other distributed computing platforms,
    such as:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spark相对于其他分布式计算平台具有许多明显的优势，例如：
- en: A faster execution platform for both iterative machine learning and interactive
    data analysis
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于迭代机器学习和交互式数据分析的更快执行平台
- en: Single stack for batch processing, SQL queries, real-time stream processing,
    graph processing, and complex data analytics
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于批处理、SQL查询、实时流处理、图处理和复杂数据分析的单一堆栈
- en: Provides high-level API to develop a diverse range of distributed applications
    by hiding the complexities of distributed programming
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过隐藏分布式编程的复杂性，提供高级API来开发各种分布式应用程序
- en: Seamless support for various data sources such as RDBMS, HBase, Cassandra, Parquet,
    MongoDB, HDFS, Amazon S3, and so on
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对各种数据源的无缝支持，如RDBMS、HBase、Cassandra、Parquet、MongoDB、HDFS、Amazon S3等
- en: '![Spark for data analytics](img/image_01_003.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![用于数据分析的Spark](img/image_01_003.jpg)'
- en: 'The following is a pictorial representation of in-memory data sharing for iterative
    algorithms:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是迭代算法的内存数据共享的图示表示：
- en: '![Spark for data analytics](img/image_01_004.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![用于数据分析的Spark](img/image_01_004.jpg)'
- en: Spark hides the complexities in writing the core MapReduce jobs and provides
    most of the functionalities through simple function calls. Because of its simplicity,
    it is able to cater to wider and bigger audience groups such as data scientists,
    data engineers, statisticians, and R/Python/Scala/Java developers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Spark隐藏了编写核心MapReduce作业的复杂性，并通过简单的函数调用提供了大部分功能。由于其简单性，它能够满足更广泛和更大的受众群体，如数据科学家、数据工程师、统计学家和R/Python/Scala/Java开发人员。
- en: The Spark architecture broadly consists of a data storage layer, management
    framework, and API. It is designed to work on top of an HDFS filesystem, and thereby
    leverages the existing ecosystem. Deployment could be as a standalone server or
    on distributed computing frameworks such as Apache Mesos or YARN. An API is provided
    for Scala, the language in which Spark is written, along with Java, R and Python.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Spark架构主要包括数据存储层、管理框架和API。它旨在在HDFS文件系统之上工作，并因此利用现有的生态系统。部署可以作为独立服务器或在诸如Apache
    Mesos或YARN之类的分布式计算框架上进行。提供了Scala的API，这是Spark编写的语言，以及Java、R和Python。
- en: The Spark stack
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark堆栈
- en: Spark is a general-purpose cluster computing system that empowers other higher-level
    components to leverage its core engine. It is interoperable with Apache Hadoop,
    in the sense that it can read and write data from/to HDFS and can also integrate
    with other storage systems that are supported by the Hadoop API.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个通用的集群计算系统，它赋予其他更高级别的组件利用其核心引擎的能力。它与Apache Hadoop是可互操作的，可以从HDFS读取和写入数据，并且还可以与Hadoop
    API支持的其他存储系统集成。
- en: While it allows building other higher-level applications on top of it, it already
    has a few components built on top that are tightly integrated with its core engine
    to take advantage of the future enhancements at the core. These applications come
    bundled with Spark to cover the broader sets of requirements in the industry.
    Most of the real-world applications need to be integrated across projects to solve
    specific business problems that usually have a set of requirements. This is eased
    out with Apache Spark as it allows its higher level components to be seamlessly
    integrated, such as libraries in a development project.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它允许在其上构建其他更高级的应用程序，但它已经在其核心之上构建了一些组件，这些组件与其核心引擎紧密集成，以利用核心的未来增强。这些应用程序与Spark捆绑在一起，以满足行业中更广泛的需求。大多数现实世界的应用程序需要在项目之间进行集成，以解决通常具有一组要求的特定业务问题。Apache
    Spark可以简化这一点，因为它允许其更高级别的组件无缝集成，例如在开发项目中的库。
- en: 'Also, with Spark''s built-in support for Scala, Java, R and Python, a broader
    range of developers and data engineers are able to leverage the entire Spark stack:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于Spark内置支持Scala、Java、R和Python，更广泛的开发人员和数据工程师能够利用整个Spark堆栈：
- en: '![The Spark stack](img/image_01_005.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![Spark堆栈](img/image_01_005.jpg)'
- en: Spark core
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark核心
- en: The Spark core, in a way, is similar to the kernel of an operating system. It
    is the general execution engine, which is fast as well as fault tolerant. The
    entire Spark ecosystem is built on top of this core engine. It is mainly designed
    to do job scheduling, task distribution, and monitoring of jobs across worker
    nodes. It is also responsible for memory management, interacting with various
    heterogeneous storage systems, and various other operations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Spark核心在某种程度上类似于操作系统的内核。它是通用执行引擎，既快速又容错。整个Spark生态系统都是建立在这个核心引擎之上的。它主要设计用于作业调度、任务分发和跨工作节点的作业监控。它还负责内存管理，与各种异构存储系统的交互以及各种其他操作。
- en: The primary building block of Spark core is the **Resilient Distributed Dataset**
    (**RDD**), which is an immutable, fault-tolerant collection of elements. Spark
    can create RDDs from a variety of data sources such as HDFS, local filesystems,
    Amazon S3, other RDDs, NoSQL data stores such as Cassandra, and so on. They are
    resilient in the sense that they automatically rebuild on failure. RDDs are built
    through lazy parallel transformations. They may be cached and partitioned, and
    may or may not be materialized.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Spark核心的主要构建模块是**弹性分布式数据集**（**RDD**），它是一个不可变的、容错的元素集合。Spark可以从各种数据源（如HDFS、本地文件系统、Amazon
    S3、其他RDD、Cassandra等NoSQL数据存储）创建RDD。它们在失败时会自动重建，因此具有容错性。RDD是通过惰性并行转换构建的。它们可以被缓存和分区，也可以或者不可以被实现。
- en: The entire Spark core engine may be viewed as a set of simple operations on
    distributed datasets. All the scheduling and execution of jobs in Spark is done
    based on the methods associated with each RDD. Also, the methods associated with
    each RDD define their own ways of distributed in-memory computation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 整个Spark核心引擎可以被视为对分布式数据集进行简单操作的集合。Spark中所有作业的调度和执行都是基于与每个RDD相关联的方法完成的。此外，与每个RDD相关联的方法定义了它们自己的分布式内存计算方式。
- en: Spark SQL
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark SQL
- en: This module of Spark is designed to query, analyze, and perform operations on
    structured data. This is a very important component in the entire Spark stack
    because of the fact that most of the organizational data is structured, though
    unstructured data is growing rapidly. Acting as a distributed query engine, it
    enables Hadoop Hive queries to run up to 100 times faster on it without any modification.
    Apart from Hive, it also supports Apache Parquet, an efficient columnar storage,
    JSON, and other structured data formats. Spark SQL enables running SQL queries
    along with complex programs written in Python, Scala, and Java.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Spark模块旨在查询、分析和对结构化数据执行操作。这是整个Spark堆栈中非常重要的一个组件，因为大多数组织数据都是结构化的，尽管非结构化数据正在迅速增长。作为一个分布式查询引擎，它使Hadoop
    Hive查询在不进行任何修改的情况下可以运行得更快，最多可以提高100倍。除了Hive，它还支持Apache Parquet（一种高效的列存储）、JSON和其他结构化数据格式。Spark
    SQL使得可以在Python、Scala和Java中运行SQL查询以及复杂程序。
- en: Spark SQL provides a distributed programming abstraction called **DataFrames**,
    referred to as SchemaRDD before, which had fewer functions associated with it.
    DataFrames are distributed collections of named columns, analogous to SQL tables
    or Python's Pandas DataFrames. They can be constructed with a variety of data
    sources that have schemas with them such as Hive, Parquet, JSON, other RDBMS sources,
    and also from Spark RDDs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL提供了一个名为**数据框**的分布式编程抽象，之前称为SchemaRDD，它的相关函数较少。数据框是命名列的分布式集合，类似于SQL表或Python的Pandas数据框。它们可以使用具有模式的各种数据源构建，例如Hive、Parquet、JSON、其他RDBMS源，以及Spark
    RDD。
- en: Spark SQL can be used for ETL processing across different formats and then running
    ad hoc analysis. Spark SQL comes with an optimizer framework called Catalyst that
    can transform SQL queries for better efficiency.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL可用于跨不同格式的ETL处理，然后进行临时分析。Spark SQL配备了一个名为Catalyst的优化器框架，可以将SQL查询转换为更高效的形式。
- en: Spark streaming
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark流处理
- en: The processing window for the enterprise data is becoming shorter than ever.
    To address the real-time processing requirement of the industry, this component
    of Spark was designed, which is fault tolerant as well as scalable. Spark enables
    real-time data analytics on live streams of data by supporting data analysis,
    machine learning, and graph processing on them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 企业数据的处理窗口正在变得比以往任何时候都要短。为了满足行业的实时处理需求，设计了Spark的这个组件，它既具有容错性又可扩展。Spark通过支持对实时数据流进行数据分析、机器学习和图处理，实现了对实时数据流的实时数据分析。
- en: It provides an API called **Discretised Stream** (**DStream**) to manipulate
    the live streams of data. The live streams of data are sliced up into small batches
    of, say, *x* seconds. Spark treats each batch as an RDD and processes them as
    basic RDD operations. DStreams can be created out of live streams of data from
    HDFS, Kafka, Flume, or any other source which can stream data on the TCP socket.
    By applying some higher-level operations on DStreams, other DStreams can be produced.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供了一个名为**离散流**（**DStream**）的API，用于操作实时数据流。实时数据流被切分成小批次，比如说，*x*秒。Spark将每个批次视为RDD并对它们进行基本的RDD操作。DStreams可以从HDFS、Kafka、Flume或任何其他能够通过TCP套接字流式传输数据的源创建出来。通过在DStreams上应用一些高级操作，可以产生其他DStreams。
- en: The final result of Spark streaming can either be written back to the various
    data stores supported by Spark or can be pushed to any dashboard for visualization.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Spark流处理的最终结果可以被写回到Spark支持的各种数据存储中，也可以被推送到任何仪表板进行可视化。
- en: MLlib
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLlib
- en: MLlib is the built-in machine learning library in the Spark stack. This was
    introduced in Spark 0.8\. Its goal is to make machine learning scalable and easy.
    Developers can seamlessly use Spark SQL, Spark Streaming, and GraphX in their
    programming language of choice, be it Java, Python, or Scala. MLlib provides the
    necessary functions to perform various statistical analyses such as correlations,
    sampling, hypothesis testing, and so on. This component also has a broad coverage
    of applications and algorithms in classification, regression, collaborative filtering,
    clustering, and decomposition.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib是Spark堆栈中内置的机器学习库。它是在Spark 0.8中引入的。其目标是使机器学习变得可扩展和简单。开发人员可以无缝地在他们选择的编程语言（Java、Python或Scala）中使用Spark
    SQL、Spark流处理和GraphX。MLlib提供了执行各种统计分析（如相关性、抽样、假设检验等）所需的函数。此组件还涵盖了分类、回归、协同过滤、聚类和分解等领域的广泛应用和算法。
- en: The machine learning workflow involves collecting and preprocessing data, building
    and deploying the model, evaluating the results, and refining the model. In the
    real world, the preprocessing steps take up significant effort. These are typically
    multi-stage workflows involving expensive intermediate read/write operations.
    Often, these processing steps may be performed multiple times over a period of
    time. A new concept called **ML Pipelines** was introduced to streamline these
    preprocessing steps. A Pipeline is a sequence of transformations where the output
    of one stage is the input of another, forming a chain. The ML Pipeline leverages
    Spark and MLlib and enables developers to define reusable sequences of transformations.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作流程涉及收集和预处理数据，构建和部署模型，评估结果和改进模型。在现实世界中，预处理步骤需要大量的工作。这些通常是涉及昂贵的中间读/写操作的多阶段工作流程。通常情况下，这些处理步骤可能会在一段时间内多次执行。引入了一个新概念**ML
    Pipelines**来简化这些预处理步骤。管道是一个转换序列，其中一个阶段的输出是另一个阶段的输入，形成一个链。ML Pipeline利用了Spark和MLlib，使开发人员能够定义可重用的转换序列。
- en: GraphX
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphX
- en: GraphX is a thin-layered unified graph analytics framework on Spark. It was
    designed to be a general-purpose distributed dataflow framework in place of specialized
    graph processing frameworks. It is fault tolerant and also exploits in-memory
    computation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX是Spark上的一个薄层统一图分析框架。它旨在成为一个通用的分布式数据流框架，取代专门的图处理框架。它具有容错性，并且利用了内存计算。
- en: '**GraphX** is an embedded graph processing API for manipulating graphs (for
    example, social networks) and to do graph parallel computation (for example, Google''s
    Pregel). It combines the advantages of both graph-parallel and data-parallel systems
    on the Spark stack to unify exploratory data analysis, iterative graph computation,
    and ETL processing. It extends the RDD abstraction to introduce the **Resilient
    Distributed Graph** (**RDG**), which is a directed graph with properties associated
    to each of its vertices and edges.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX是一个嵌入式图处理API，用于操作图（例如社交网络）和进行图并行计算（例如Google的Pregel）。它结合了Spark堆栈上图并行和数据并行系统的优势，统一了探索性数据分析、迭代图计算和ETL处理。它扩展了RDD抽象，引入了**Resilient
    Distributed Graph**（**RDG**），这是一个带有每个顶点和边属性的有向图。
- en: GraphX includes a decently large collection of graph algorithms, such as PageRank,
    K-Core, Triangle Count, LDA, and so on.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX包括大量的图算法，如PageRank、K-Core、Triangle Count、LDA等。
- en: SparkR
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SparkR
- en: The SparkR project was started to integrate the statistical analysis and machine
    learning capability of R with the scalability of Spark. It addressed the limitation
    of R, which was its ability to process as much data as fitted in the memory of
    a single machine. R programs can now scale in a distributed setting through SparkR.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR项目旨在将R的统计分析和机器学习能力与Spark的可伸缩性相结合。它解决了R的局限性，即其能够处理的数据量受限于单台机器的内存。现在，R程序可以通过SparkR在分布式环境中扩展。
- en: SparkR is actually an R Package that provides an R shell to leverage Spark's
    distributed computing engine. With R's rich set of built-in packages for data
    analytics, data scientists can analyze large datasets interactively at scale.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR实际上是一个R包，提供了一个R shell来利用Spark的分布式计算引擎。借助R丰富的内置数据分析包，数据科学家可以交互式地分析大型数据集。
- en: Summary
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we briefly covered what big data is all about. We then discussed
    the computational and analytical challenges involved in big data analytics. Later,
    we looked at how the analytics space in the context of big data has evolved over
    a period of time and what the trend has been. We also covered how Spark addressed
    most of the big data analytics challenges and became a general-purpose unified
    analytics platform for data science as well as parallel computation. At the end
    of this chapter, we just gave you a heads-up on the Spark stack and its components.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要介绍了大数据的概念。然后，我们讨论了大数据分析中涉及的计算和分析挑战。后来，我们看了一下大数据背景下分析领域是如何随着时间的推移而发展的，趋势是什么。我们还介绍了Spark如何解决了大部分大数据分析挑战，并成为了数据科学和并行计算的通用统一分析平台。在本章的结尾，我们简要介绍了Spark堆栈及其组件。
- en: In the next chapter, we will learn about the Spark programming model. We will
    take a deep dive into the basic building block of Spark, which is the RDD. Also,
    we will learn how to program with the RDD API on Scala and Python.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习Spark编程模型。我们将深入了解Spark的基本构建块，即RDD。此外，我们将学习如何在Scala和Python上使用RDD API进行编程。
- en: References
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'Apache Spark overview:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark概述：
- en: '[http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)'
- en: '[https://databricks.com/spark/about](https://databricks.com/spark/about)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/spark/about](https://databricks.com/spark/about)'
- en: 'Apache Spark architecture:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark架构：
- en: '[http://lintool.github.io/SparkTutorial/slides/day1_context.pdf](http://lintool.github.io/SparkTutorial/slides/day1_context.pdf)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://lintool.github.io/SparkTutorial/slides/day1_context.pdf](http://lintool.github.io/SparkTutorial/slides/day1_context.pdf)'
