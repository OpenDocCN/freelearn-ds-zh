- en: Big Data Cleaning and Wrangling with Spark Notebooks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark笔记本进行大数据清洗和整理
- en: In this chapter, we will learn about big data cleaning and wrangling with Spark
    Notebooks. We will also look at how using Spark on a Notebook application allows
    us to use RDDs effectively. We will use Spark Notebooks for quick iteration of
    ideas and carry out sampling/filtering RDDs to pick out relevant data points.
    We will also learn how to split datasets and create new combinations with set
    operations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习使用Spark笔记本进行大数据清洗和整理。我们还将看看在笔记本应用程序上使用Spark如何有效地使用RDD。我们将使用Spark笔记本快速迭代想法，并进行抽样/过滤RDD以挑选出相关数据点。我们还将学习如何拆分数据集并使用集合操作创建新的组合。
- en: 'In this chapter, we will discuss the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Using Spark Notebooks for quick iteration of ideas
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark笔记本快速迭代想法
- en: Sampling/filtering RDDs to pick out relevant data points
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对RDD进行抽样/过滤以挑选出相关数据点
- en: Splitting datasets and creating some new combinations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拆分数据集并创建一些新的组合
- en: Using Spark Notebooks for quick iteration of ideas
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark笔记本快速迭代想法
- en: 'In this section, we will answer the following questions:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将回答以下问题：
- en: What are Spark Notebooks?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是Spark笔记本？
- en: How do you start Spark Notebooks?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何启动Spark笔记本？
- en: How do you use Spark Notebooks?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用Spark笔记本？
- en: Let's start with setting up a Jupyter Notebook-like environment for Spark. Spark
    Notebook is just an interactive and reactive data science environment that uses
    Scala and Spark.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从为Spark设置类似Jupyter Notebook的环境开始。Spark笔记本只是一个使用Scala和Spark的交互式和反应式数据科学环境。
- en: 'If we view the GitHub page ([https://github.com/spark-notebook/spark-notebook](https://github.com/spark-notebook/spark-notebook)),
    we can see that what the Notebooks do is actually very straightforward, as shown
    in the following screenshot:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看GitHub页面（[https://github.com/spark-notebook/spark-notebook](https://github.com/spark-notebook/spark-notebook)），我们会发现笔记本的功能实际上非常简单，如下截图所示：
- en: '![](img/edaecfa4-9892-4f8e-9831-a38e1520b220.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/edaecfa4-9892-4f8e-9831-a38e1520b220.png)'
- en: If we look at a Spark Notebook, we can see that they look very much like what
    Python developers use, which is Jupyter Notebooks. You have a text box allowing
    you to enter some code, and then you execute the code below the text box, which
    is similar to a Notebook format. This allows us to perform a reproducible analysis
    with Apache Spark and the big data ecosystem.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下Spark笔记本，我们会发现它们看起来非常像Python开发人员使用的Jupyter笔记本。您可以在文本框中输入一些代码，然后在文本框下方执行代码，这与笔记本格式类似。这使我们能够使用Apache
    Spark和大数据生态系统执行可重现的分析。
- en: 'So, we can use Spark Notebooks as it is, and all we need to do is go to the
    Spark Notebook website and click on Quick Start to get the Notebook started, as
    shown in the following screenshot:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以直接使用Spark笔记本，我们只需要转到Spark笔记本网站，然后点击“快速启动”即可启动笔记本，如下截图所示：
- en: '![](img/10274ddd-d867-401e-b359-00846d61a5b2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10274ddd-d867-401e-b359-00846d61a5b2.png)'
- en: 'We need to make sure that we are running Java 7\. We can see that the setup
    steps are also mentioned in the documentation, as shown in the following screenshot:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保我们正在运行Java 7。我们可以看到设置步骤也在文档中提到，如下截图所示：
- en: '![](img/87e2fc62-6f8d-45f8-a8bf-21c574afb867.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87e2fc62-6f8d-45f8-a8bf-21c574afb867.png)'
- en: 'The main website for Spark Notebook is `spark-notebook.io`, where we can see
    many options. A few of them have been shown in the following screenshot:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Spark笔记本的主要网站是`spark-notebook.io`，在那里我们可以看到许多选项。以下截图显示了其中一些选项：
- en: '![](img/8518e8f2-1513-4569-822d-39bc6d0f8814.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8518e8f2-1513-4569-822d-39bc6d0f8814.png)'
- en: 'We can download the TAR file and unzip it. You can use Spark Notebook, but
    we will be using Jupyter Notebook in this book. So, going back to the Jupyter
    environment, we can look at the PySpark-accompanying code files. In `Chapter 3` Notebook we
    have included a convenient way for us to set up the environment variables to get
    PySpark working with Jupyter, as shown in the following screenshot:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以下载TAR文件并解压缩。您可以使用Spark笔记本，但是在本书中我们将使用Jupyter Notebook。因此，回到Jupyter环境，我们可以查看PySpark附带的代码文件。在`第3章`笔记本中，我们已经包含了一个方便的方法来设置环境变量，以使PySpark与Jupyter一起工作，如下截图所示：
- en: '![](img/cb54d381-06e9-41f2-960c-41210cd0e36f.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb54d381-06e9-41f2-960c-41210cd0e36f.png)'
- en: First, we need to create two new environment variables in our environments.
    If you are using Linux, you can use Bash RC. If you are using Windows, all you
    need to do is to change and edit your system environment variables. There are
    multiple tutorials online to help you do this. What we want to do here is to edit
    or include the `PYSPARK_DRIVER_PYTHON` variable and point it to your Jupyter Notebook
    installation. If you are on Anaconda, you probably would be pointed to the Anaconda
    Jupyter Bash file. Since we are on WinPython, I have pointed it to my WinPython
    Jupyter Notebook Bash file. The second environment variable we want to export
    is simply `PYSPARK_DRIVER_PYTHON_OPTS`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在我们的环境中创建两个新的环境变量。如果您使用Linux，可以使用Bash RC。如果您使用Windows，您只需要更改和编辑系统环境变量。有多个在线教程可以帮助您完成此操作。我们要做的是编辑或包含`PYSPARK_DRIVER_PYTHON`变量，并将其指向您的Jupyter
    Notebook安装位置。如果您使用Anaconda，可能会指向Anaconda Jupyter Bash文件。由于我们使用的是WinPython，我已将其指向了我的WinPython
    Jupyter Notebook Bash文件。我们要导出的第二个环境变量只是`PYSPARK_DRIVER_PYTHON_OPTS`。
- en: 'One of the suggestions is that we include the Notebook folder and the Notebook
    app in the options, ask it not to open in the browser, and tell it what port to
    bind to. In practice, if you are on Windows and WinPython environments then you
    don''t really need this line here, and you can simply skip it. After this has
    been done, simply restart your PySpark from a command line. What will happen is
    that, instead of having the console that we have seen before, it directly launches
    into a Jupyter Notebook instance, and, furthermore, we can use Spark and SparkContext
    variables as in Jupyter Notebook. So, let''s test it out as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个建议是，我们在选项中包括笔记本文件夹和笔记本应用程序，要求它不要在浏览器中打开，并告诉它要绑定到哪个端口。 在实践中，如果您使用的是Windows和WinPython环境，那么您实际上不需要在这里使用这行代码，您可以直接跳过它。
    完成后，只需从命令行重新启动PySpark。 发生的情况是，与我们以前看到的控制台不同，它直接启动到Jupyter Notebook实例，并且我们可以像在Jupyter
    Notebook中一样使用Spark和SparkContext变量。 因此，让我们测试一下，如下所示：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We instantly get access to our `SparkContext` that tells us that Spark is version
    `2.3.3`, our `Master` is at `local`, and the `AppName` is the Python SparkShell
    (`PySparkShell`), as shown in the following code snippet:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立即获得了我们的`SparkContext`，告诉我们Spark的版本是`2.3.3`，我们的`Master`是`local`，`AppName`是Python
    SparkShell（`PySparkShell`），如下面的代码片段所示：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So, now we know how we create a Notebook-like environment in Jupyter. In the
    next section, we will look at sampling and filtering RDDs to pick out relevant
    data points.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们知道了如何在Jupyter中创建类似笔记本的环境。 在下一节中，我们将看一下对RDD进行抽样和过滤以挑选出相关数据点。
- en: Sampling/filtering RDDs to pick out relevant data points
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抽样/过滤RDD以挑选出相关数据点
- en: In this section, we will look at sampling and filtering RDDs to pick up relevant
    data points. This is a very powerful concept that allows us to circumvent the
    limitations of big data and perform our calculations on a particular sample.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看对RDD进行抽样和过滤以挑选出相关数据点。 这是一个非常强大的概念，它使我们能够规避大数据的限制，并在特定样本上执行我们的计算。
- en: 'Let''s now check how sampling not only speeds up our calculations, but also
    gives us a good approximation of the statistic that we are trying to calculate.
    To do this, we first import the `time` library as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们检查抽样不仅加速了我们的计算，而且还给了我们对我们试图计算的统计量的良好近似。 为此，我们首先导入`time`库，如下所示：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next thing we want to do is look at lines or data points in the KDD database
    that contains the word `normal`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要做的是查看KDD数据库中包含单词`normal`的行或数据点：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We need to create a sample of `raw_data`. We will store the sample into the `sample`,
    variable, and we''re sampling from `raw_data` without replacement. We''re sampling
    10% of the data, and we''re providing `42` as our random seed:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建`raw_data`的样本。 我们将样本存储到`sample`变量中，我们正在从`raw_data`中进行无替换的抽样。 我们正在抽样数据的10％，并且我们提供`42`作为我们的随机种子：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next thing to do is to chain some `map` and `filter` functions, as we do
    normally if we are dealing with unsampled datasets:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来要做的是链接一些`map`和`filter`函数，就像我们通常处理未抽样数据集一样：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we need to time how long it would take for us to count the number of
    rows in the sample:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要计算在样本中计算行数需要多长时间：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We issue the count statement here. As you know from the previous section, this
    is going to trigger all the calculations in PySpark as defined in `contains_normal_sample`,
    and we''re recording the time before the sample count happens. We are also recording
    the time after the sample count happens, so we can see how long it takes when
    we''re looking at a sample. Once this is done, let''s take a look at how long
    the `duration` was in the following code snippet:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里发布计数声明。 正如您从上一节中所知，这将触发PySpark中`contains_normal_sample`中定义的所有计算，并且我们记录了样本计数发生之前的时间。
    我们还记录了样本计数发生后的时间，这样我们就可以看到在查看样本时需要多长时间。 一旦完成了这一点，让我们来看看以下代码片段中`duration`持续了多长时间：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output will be as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It took us 23 seconds to run this operation over 10% of the data. Now, let''s
    look at what happens if we run the same transform over all of the data:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们花了23秒来运行这个操作，占数据的10％。 现在，让我们看看如果我们在所有数据上运行相同的转换会发生什么：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s take a look at the `duration` again:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看一下`duration`：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will provide the following output:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供以下输出：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There is a small difference, as we are comparing `36.5` seconds to `23.7` seconds.
    However, this difference becomes much larger as your dataset becomes much more
    varied, and the amount of data you're dealing with becomes much more complex.
    The great thing about this is that, if you are usually doing big data, verifying
    whether your answers make sense with a small sample of the data can help you catch
    bugs much earlier on.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个小差异，因为我们正在比较`36.5`秒和`23.7`秒。 但是，随着数据集变得更加多样化，以及您处理的数据量变得更加复杂，这种差异会变得更大。 这其中的好处是，如果您通常处理大数据，使用数据的小样本验证您的答案是否合理可以帮助您更早地捕捉错误。
- en: 'The last thing to look at is how we can use `takeSample`. All we need to do
    is use the following code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要看的是我们如何使用`takeSample`。 我们只需要使用以下代码：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As we''ve learned earlier, when we present the new functions we call `takeSample`,
    and it will give us `10` items with a random seed of `42`, which we will now put
    into memory. Now that this data is in memory, we can call the same `map` and `filter`
    functions using native Python methods as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前学到的，当我们呈现新函数时，我们调用`takeSample`，它将给我们`10`个具有随机种子`42`的项目，现在我们将其放入内存。 现在这些数据在内存中，我们可以使用本机Python方法调用相同的`map`和`filter`函数，如下所示：
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output will be as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We have now finished calculating our `contains_normal` function by bringing
    `data_in_memory`. This is a great illustration of the power of PySpark.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在通过将`data_in_memory`带入来计算我们的`contains_normal`函数。 这很好地说明了PySpark的强大之处。
- en: We originally took a sample of 10,000 data points, and it crashed the machine.
    So here, we will take these ten data points to see if it contains the word `normal`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初抽取了10,000个数据点的样本，这导致了机器崩溃。 因此，在这里，我们将取这十个数据点，看看它是否包含单词`normal`。
- en: We can see that the calculation is completed in the previous code block, and
    it took longer and used more memory than if we were doing it in PySpark. And that's
    why we use Spark, because Spark allows us to parallelize any big datasets and
    operate it on it using a parallel fashion, which means that we can do more with
    less memory and with less time. In the next section, we're going to talk about
    splitting datasets and creating new combinations with set operations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在前一个代码块中计算已经完成，它比在PySpark中进行计算花费了更长的时间并且使用了更多的内存。这就是为什么我们使用Spark，因为Spark允许我们并行处理任何大型数据集，并且以并行方式操作它，这意味着我们可以用更少的内存和更少的时间做更多的事情。在下一节中，我们将讨论拆分数据集并使用集合操作创建新的组合。
- en: Splitting datasets and creating some new combinations
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拆分数据集并创建一些新的组合
- en: In this section, we are going to look at splitting datasets and creating new
    combinations with set operations. We're going to learn subtracts, and Cartesian
    ones in particular.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看看如何拆分数据集并使用集合操作创建新的组合。我们将学习特别是减法和笛卡尔积。
- en: 'Let''s go back to `Chapter 3` of the Jupyter Notebook that we''ve been looking
    at lines in the datasets that contain the word `normal`. Let''s try to get all
    the lines that don''t contain the word `normal`. One way is to use the `filter`
    function to look at lines that don''t have `normal` in it. But, we can use something
    different in PySpark: a function called `subtract` to take the entire dataset
    and subtract the data that contains the word `normal`. Let''s have a look at the
    following snippet:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们一直在查看包含单词`normal`的数据集中的行的Jupyter笔记本的`第3章`。让我们尝试获取不包含单词`normal`的所有行。一种方法是使用`filter`函数查看不包含`normal`的行。但是，在PySpark中我们可以使用一些不同的东西：一个名为`subtract`的函数来取整个数据集并减去包含单词`normal`的数据。让我们看看以下片段：
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can then obtain interactions or data points that don''t contain the word
    `normal` by subtracting the `normal` ones from the entire sample as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以通过从整个样本中减去`normal`样本来获得不包含单词`normal`的交互或数据点如下：
- en: '[PRE16]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We take the `normal` sample and we subtract it from the entire sample, which
    is 10% of the entire dataset. Let''s issue some counts as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们取`normal`样本，然后从整个样本中减去它，这是整个数据集的10%。让我们按如下方式发出一些计数：
- en: '[PRE17]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will give us the following output:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供以下输出：
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As you can see, 10% of the dataset gives us `490705` data points, and within
    it, we have a number of data points containing the word `normal`. To find out
    its count, write the following code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，数据集的10%给我们`490705`个数据点，其中有一些包含单词`normal`的数据点。要找出它的计数，写下以下代码：
- en: '[PRE19]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will give us the following output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供以下输出：
- en: '[PRE20]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'So, here we have `97404` data points. If we count the on normal samples because
    we''re simply subtracting one sample from another, the count should be roughly
    just below 400,000 data points, because we have 490,000 data points minus 97,000
    data points, which should result in something like 390,000\. Let''s see what happens
    using the following code snippet:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这里有`97404`个数据点。如果我们计算正常样本，因为我们只是从另一个样本中减去一个样本，计数应该大约略低于400,000个数据点，因为我们有490,000个数据点减去97,000个数据点，这应该导致大约390,000。让我们看看使用以下代码片段会发生什么：
- en: '[PRE21]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will give us the following output:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供以下输出：
- en: '[PRE22]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As expected, it returned a value of `393301`, which validates our assumption
    that subtracting the data points containing `normal` gives us all the non-normal
    data points.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，它返回了`393301`的值，这验证了我们的假设，即减去包含`normal`的数据点会给我们所有非正常的数据点。
- en: 'Let''s now discuss the other function, called `cartesian`. This allows us to
    give all the combinations between the distinct values of two different features.
    Let''s see how this works in the following code snippet:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论另一个名为`cartesian`的函数。这允许我们给出两个不同特征的不同值之间的所有组合。让我们看看以下代码片段中这是如何工作的：
- en: '[PRE23]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here, we''re splitting the `line` function by using `,`. So, we will split
    the values that are comma-separated—for all the features that we come up with
    after splitting, we take the first feature, and we find all the distinct values
    of that column. We can repeat this for the second feature as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`,`来拆分`line`函数。因此，我们将拆分逗号分隔的值 - 对于拆分后得到的所有特征，我们取第一个特征，并找到该列的所有不同值。我们可以重复这个过程来获取第二个特征，如下所示：
- en: '[PRE24]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'And so, we now have two features. We can look at the actual items in `feature_1` and`feature_2` as
    follows, by issuing the `collect()` call that we saw earlier:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在有两个特征。我们可以查看`feature_1`和`feature_2`中的实际项目，如下所示，通过发出我们之前看到的`collect()`调用：
- en: '[PRE25]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s look at each one as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分别看一下如下：
- en: '[PRE26]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This will provide the following outcome:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供以下结果：
- en: '[PRE27]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'So, `f1` has three values; let''s check for `f2` as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，`f1`有三个值；让我们检查`f2`如下：
- en: '[PRE28]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This will provide us with the following output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供以下输出：
- en: '![](img/d4407ef2-4c27-4d7a-a633-41a19dd42187.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4407ef2-4c27-4d7a-a633-41a19dd42187.png)'
- en: '`f2` has a lot more values, and we can use the `cartesian` function to collect
    all the combinations between `f1` and `f2` as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`f2`有更多的值，我们可以使用`cartesian`函数收集`f1`和`f2`之间的所有组合如下：'
- en: '[PRE29]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This will give us the following output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供以下输出：
- en: '[PRE30]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This is how we use the `cartesian` function to find the Cartesian product between
    two features. In this chapter, we looked at Spark Notebooks; sampling, filtering,
    and splitting datasets; and creating new combinations with set operations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何使用`cartesian`函数找到两个特征之间的笛卡尔积。在本章中，我们看了Spark笔记本；抽样、过滤和拆分数据集；以及使用集合操作创建新的组合。
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at Spark Notebooks for quick iterations. We then
    used sampling or filtering to pick out relevant data points. We also learned how
    to split datasets and create new combinations with set operations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看了Spark笔记本进行快速迭代。然后我们使用抽样或过滤来挑选出相关的数据点。我们还学会了如何拆分数据集并使用集合操作创建新的组合。
- en: In the next chapter, we will cover aggregating and summarizing data into useful
    reports.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍将数据聚合和汇总为有用的报告。
