- en: Big Data With Hadoop
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Hadoop 处理大数据
- en: Hadoop has become the de facto standard in the world of big data, especially
    over the past three to four years. Hadoop started as a subproject of Apache Nutch
    in 2006 and introduced two key features related to distributed filesystems and
    distributed computing, also known as MapReduce, that caught on very rapidly among
    the open source community. Today, there are thousands of new products that have
    been developed leveraging the core features of Hadoop, and it has evolved into
    a vast ecosystem consisting of more than 150 related major products. Arguably,
    Hadoop was one of the primary catalysts that started the big data and analytics
    industry.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 已成为大数据领域的事实标准，尤其是在过去三到四年里。Hadoop 最初是 Apache Nutch 的一个子项目，诞生于 2006 年，介绍了与分布式文件系统和分布式计算（也称为
    MapReduce）相关的两个关键特性，这些特性在开源社区中迅速获得了广泛的关注。如今，已经有成千上万的新产品基于 Hadoop 的核心特性开发，并且它已演变成一个庞大的生态系统，包含了
    150 多个相关的主要产品。可以说，Hadoop 是启动大数据和分析行业的主要催化剂之一。
- en: 'In this chapter, we will discuss the background and core concepts of Hadoop,
    the components of the Hadoop platform, and delve deeper into the major products
    in the Hadoop ecosystem. We will learn about the core concepts of distributed
    filesystems and distributed processing and optimizations to improve the performance
    of Hadoop deployments. We''ll conclude with real-world hands-on exercises using
    the **Cloudera Distribution of Hadoop** (**CDH**). The topics we will cover are:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论 Hadoop 的背景及核心概念，Hadoop 平台的组件，并深入探讨 Hadoop 生态系统中的主要产品。我们将了解分布式文件系统和分布式处理的核心概念，以及提高
    Hadoop 部署性能的优化方法。最后，我们将通过使用**Cloudera 分发版 Hadoop**（**CDH**）进行实际操作练习。我们将涵盖的主题包括：
- en: The basics of Hadoop
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 基础知识
- en: The core components of Hadoop
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 的核心组件
- en: Hadoop 1 and Hadoop 2
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 1 和 Hadoop 2
- en: The Hadoop Distributed File System
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 分布式文件系统
- en: Distributed computing principles with MapReduce
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 MapReduce 的分布式计算原理
- en: The Hadoop ecosystem
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 生态系统
- en: Overview of the Hadoop ecosystem
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 生态系统概述
- en: Hive, HBase, and more
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive、HBase 等
- en: Hadoop Enterprise deployments
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 企业级部署
- en: In-house deployments
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部部署
- en: Cloud deployments
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云端部署
- en: Hands-on with Cloudera Hadoop
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Cloudera Hadoop 进行实践
- en: Using HDFS
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 HDFS
- en: Using Hive
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Hive
- en: MapReduce with WordCount
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MapReduce 进行 WordCount
- en: The fundamentals of Hadoop
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop 基础知识
- en: In 2006, Doug Cutting, the creator of Hadoop, was working at Yahoo!. He was
    actively engaged in an open source project called Nutch that involved the development
    of a large-scale web crawler. A web crawler at a high level is essentially software
    that can browse and index web pages, generally in an automatic manner, on the
    internet. Intuitively, this involves efficient management and computation across
    large volumes of data. In late January of 2006, Doug formally announced the start
    of Hadoop. The first line of the request, still available on the internet at [https://issues.apache.org/jira/browse/INFRA-700,](https://issues.apache.org/jira/browse/INFRA-700)
    was *The Lucene PMC has voted to split part of Nutch into a new subproject named
    Hadoop*. And thus, Hadoop was born.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 2006 年，Hadoop 的创始人 Doug Cutting 当时在 Yahoo! 工作。他积极参与了一个开源项目，名为 Nutch，该项目涉及开发一个大规模的网页爬虫。网页爬虫从高层次上讲，本质上是可以在互联网上自动浏览并索引网页的软件。直观地说，这涉及到对大量数据的高效管理和计算。在
    2006 年 1 月底，Doug 正式宣布了 Hadoop 的开始。请求的第一行，仍然可以在互联网上找到，地址为 [https://issues.apache.org/jira/browse/INFRA-700,](https://issues.apache.org/jira/browse/INFRA-700)，内容为
    *Lucene PMC 已投票决定将 Nutch 的一部分拆分为一个新的子项目，命名为 Hadoop*。于是，Hadoop 应运而生。
- en: 'At the onset, Hadoop had two core components : **Hadoop Distributed File System**
    (**HDFS**) and MapReduce. This was the first iteration of Hadoop, also now known
    as Hadoop 1\. Later, in 2012, a third component was added known as **YARN** (**Yet
    Another Resource Negotiator**) which decoupled the process of resource management
    and job scheduling. Before we delve into the core components in more detail, it
    would help to get an understanding of the fundamental premises of Hadoop:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，Hadoop 有两个核心组件：**Hadoop 分布式文件系统**（**HDFS**）和 MapReduce。这是 Hadoop 的第一个版本，现在也被称为
    Hadoop 1。后来，在 2012 年，加入了第三个组件，称为 **YARN**（**Yet Another Resource Negotiator**），它将资源管理和作业调度过程解耦。在我们更详细地探讨核心组件之前，了解
    Hadoop 的基本前提将有所帮助：
- en: '![](img/5dbca749-d341-4994-ad32-89f3820278d2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5dbca749-d341-4994-ad32-89f3820278d2.png)'
- en: Doug Cutting's post at [https://issues.apache.org/jira/browse/NUTCH-193](https://issues.apache.org/jira/browse/NUTCH-193)
    announced his intent to separate **Nutch Distributed FS** (**NDFS**) and MapReduce
    to a new subproject called Hadoop.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Doug Cutting在[https://issues.apache.org/jira/browse/NUTCH-193](https://issues.apache.org/jira/browse/NUTCH-193)的帖子中宣布了他将**Nutch分布式文件系统**（**NDFS**）和MapReduce分离到一个名为Hadoop的新子项目中的计划。
- en: The fundamental premise of Hadoop
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop的基本前提
- en: The fundamental premise of Hadoop is that instead of attempting to perform a
    task on a single large machine, the task can be subdivided into smaller segments
    that can then be delegated to multiple smaller machines. These so-called smaller
    machines would then perform the task on their own portion of the data. Once the
    smaller machines have completed their tasks to produce the results on the tasks
    they were allocated, the individual units of results would then be aggregated
    to produce the final result.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的基本前提是，与其尝试在单一大型机器上执行任务，不如将任务划分为更小的部分，然后将这些部分分配给多个较小的机器。这些所谓的较小机器将会在其各自的数据部分上执行任务。一旦这些较小的机器完成了各自分配的任务，产生的结果就会被汇总，最终得到总结果。
- en: 'Although, in theory, this may appear relatively simple, there are various technical
    considerations to bear in mind. For example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从理论上讲，这看起来相对简单，但有许多技术考虑因素需要牢记。例如：
- en: Is the network fast enough to collect the results from each individual server?
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络是否足够快，以便从每个单独的服务器收集结果？
- en: Can each individual server read data fast enough from the disk?
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个单独的服务器是否能从磁盘读取数据足够快？
- en: If one or more of the servers fail, do we have to start all over?
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个或多个服务器发生故障，我们是否需要从头开始？
- en: If there are multiple large tasks, how should they be prioritized?
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有多个大型任务，它们应该如何排序优先级？
- en: There are many more such considerations that must be considered when working
    with a distributed architecture of this nature.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理这种分布式架构时，还必须考虑更多此类技术因素。
- en: The core modules of Hadoop
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop的核心模块
- en: 'The core modules of Hadoop consist of:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的核心模块包括：
- en: '**Hadoop Common**: Libraries and other common helper utilities required by
    Hadoop'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop Common**：Hadoop所需的库和其他常用辅助工具'
- en: '**HDFS**: A distributed, highly-available, fault-tolerant filesystem that stores
    data'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HDFS**：一个分布式、高可用、容错的文件系统，用于存储数据'
- en: '**Hadoop MapReduce**: A programming paradigm involving distributed computing
    across commodity servers (or nodes)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop MapReduce**：一种涉及在普通服务器（或节点）上进行分布式计算的编程范式'
- en: '**Hadoop YARN**: A framework for job scheduling and resource management'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop YARN**：一个作业调度和资源管理框架'
- en: Of these core components, YARN was introduced in 2012 to address some of the
    shortcomings of the first release of Hadoop. The first version of Hadoop (or equivalently,
    the first model of Hadoop) used HDFS and MapReduce as its main components. As
    Hadoop gained in popularity, the need to use facilities beyond those provided
    by MapReduce became more and more important. This, along with some other technical
    considerations, led to the development of YARN.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些核心组件中，YARN于2012年引入，以解决Hadoop第一次发布时的一些不足。Hadoop的第一个版本（或者说是Hadoop的第一个模型）使用HDFS和MapReduce作为其主要组件。随着Hadoop的流行，使用超出MapReduce提供的功能的需求变得越来越重要。这以及其他一些技术因素促使了YARN的开发。
- en: Let's now look at the salient characteristics of Hadoop as itemized previously.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看之前列出的Hadoop的显著特点。
- en: Hadoop Distributed File System - HDFS
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop分布式文件系统 - HDFS
- en: The HDFS forms the underlying basis of all Hadoop installations. Files, or more
    generally data, is stored in HDFS and accessed by the nodes of Hadoop.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS构成了所有Hadoop安装的基础。文件，或者更一般地说，数据，是存储在HDFS中并由Hadoop的节点访问的。
- en: 'HDFS performs two main functions:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS执行两个主要功能：
- en: '**Namespaces**: Provides namespaces that hold cluster metadata, that is, the
    location of data in the Hadoop cluster'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名空间**：提供包含集群元数据的命名空间，即Hadoop集群中数据的位置'
- en: '**Data storage**: Acts as storage for data used in the Hadoop cluster'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据存储**：作为Hadoop集群中数据的存储'
- en: The filesystem is termed as distributed since the data is stored in chunks across
    multiple servers. An intuitive understanding of HDFS can be gained from a simple
    example, as follows. Consider a large book that consists of Chapters A - Z. In
    ordinary filesystems, the entire book would be stored as a single file on the
    disk. In HDFS, the book would be split into smaller chunks, say a chunk for Chapters
    A - H, another for I - P, and a third one for Q - Z. These chunks are then stored
    in separate racks (or bookshelves as with this analogy). Further, the chapters
    are replicated three times, such that there are three copies of each of the chapters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统被称为分布式的，因为数据被分散存储在多个服务器的块中。通过一个简单的例子可以直观地理解HDFS，具体如下：假设一本大书包含了A到Z的章节。在普通文件系统中，整本书会作为一个单一文件存储在磁盘上。而在HDFS中，这本书会被分成更小的块，比如A到H的章节为一个块，I到P为另一个块，Q到Z为第三个块。这些块会被存储在不同的机架上（就像这个类比中的书架一样）。此外，这些章节会被复制三次，以确保每个章节有三个副本。
- en: 'Suppose, further, the size of the entire book is 1 GB, and each chapter is
    approximately 350 MB:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设整本书的大小为1GB，每个章节大约为350MB：
- en: '![](img/46a63885-0e9f-4d87-a5d3-c9edb3be3e6a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46a63885-0e9f-4d87-a5d3-c9edb3be3e6a.png)'
- en: A bookshelf analogy for HDFS
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS的书架类比
- en: 'Storing the book in this manner achieves a few important objectives:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式存储书籍达到了几个重要目标：
- en: Since the book has been split into three parts by groups of chapters and each
    part has been replicated three times, it means that our process can read the book
    in parallel by querying the parts from different servers. This reduces I/O contention
    and is a very fitting example of the proper use of parallelism.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于这本书已经按章节分成了三部分，并且每部分被复制了三次，这意味着我们的进程可以通过从不同服务器查询各部分来并行读取这本书。这减少了I/O竞争，恰好展示了并行处理的正确使用示例。
- en: If any of the racks are not available, we can retrieve the chapters from any
    of the other racks as there are multiple copies of each chapter available on different
    racks.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果某些机架不可用，我们可以从其他任何机架中提取章节，因为每个章节在不同机架上都有多个副本。
- en: If a task I have been given only requires me to access a single chapter, for
    example, Chapter B, I need to access only the file corresponding to Chapters A-H.
    Since the size of the file corresponding to Chapters A-H is a third the size of
    the entire book, the time to access and read the file would be much smaller.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我被分配的任务只需要访问一个章节，例如B章节，我只需要访问与A到H章节相对应的文件。由于A到H章节的文件大小是整本书的三分之一，访问和读取该文件所需的时间将会大大减少。
- en: Other benefits, such as selective access rights to different chapter groups
    and so on, would also be possible with such a model.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种模型还可以实现其他好处，例如对不同章节组的选择性访问权限等。
- en: 'This may be an over-simplified analogy of the actual HDFS functionality, but
    it conveys the basic principle of the technology - that large files are subdivided
    into blocks (chunks) and spread across multiple servers in a high-availability
    redundant configuration. We''ll now look at the actual HDFS architecture in a
    bit more detail:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是对实际HDFS功能的一个过于简化的类比，但它传达了该技术的基本原理——即大型文件被划分为块（块）并分布在多个服务器上，以高可用性冗余配置进行存储。现在我们将更详细地了解实际的HDFS架构：
- en: '![](img/a9eace37-f3d1-4033-9f6c-9b85130a311b.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9eace37-f3d1-4033-9f6c-9b85130a311b.png)'
- en: 'The HDFS backend of Hadoop consists of:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的HDFS后端包括：
- en: '**NameNode**: This can be considered the master node. The NameNode contains
    cluster metadata and is aware of what data is stored in which location - in short,
    it holds the namespace. It stores the entire namespace in RAM and when a request
    arrives, provides information on which servers hold the data required for the
    task. In Hadoop 2, there can be more than one NameNode. A secondary NameNode can
    be created that acts as a helper node to the primary. As such, it is not a backup
    NameNode, but one that helps in keeping cluster metadata up to date.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NameNode**：可以认为这是主节点。NameNode包含集群元数据，并且知道数据存储在什么位置——简而言之，它保存了命名空间。它将整个命名空间存储在内存中，当请求到达时，会提供哪个服务器存储了所需数据的信息。在Hadoop
    2中，可以有多个NameNode。可以创建一个辅助的Secondary NameNode，它作为主节点的辅助节点来工作。因此，它不是备份NameNode，而是帮助保持集群元数据最新的节点。'
- en: '**DataNode**: The DataNodes are the individual servers that are responsible
    for storing chunks of the data and performing compute operations when they receive
    a new request. These are primarily commodity servers that are less powerful in
    terms of resource and capacity than the NameNode that stores the cluster metadata.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataNode**：DataNodes 是负责存储数据块并在接收到新请求时执行计算操作的独立服务器。这些通常是资源和容量较小的商品服务器，而不像存储集群元数据的
    NameNode 那样强大。'
- en: Data storage process in HDFS
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS 中的数据存储过程
- en: 'The following points should give a good idea of the data storage process:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几点有助于理解数据存储的过程：
- en: All data in HDFS is written in blocks, usually of size 128 MB. Thus, a single
    file of say size 512 MB would be split into four blocks (4 * 128 MB). These blocks
    are then written to DataNodes. To maintain redundancy and high availability, each
    block is replicated to create duplicate copies. In general, Hadoop installations
    have a replication factor of three, indicating that each block of data is replicated
    three times.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 中的所有数据都是以块的形式写入的，通常大小为 128 MB。因此，一个 512 MB 的文件会被拆分为四个块（4 * 128 MB）。这些块随后会被写入到
    DataNode。为了保持冗余性和高可用性，每个块都会被复制，形成多个副本。通常，Hadoop 安装的复制因子为三，这意味着每个数据块会被复制三次。
- en: This guarantees redundancy such that in the event one of the servers fails or
    stops responding, there would always be a second and even a third copy available.
    To ensure that this process works seamlessly, the DataNode places the replicas
    in independent servers and can also ensure that the blocks are placed on servers
    in different racks in a data center. This is due to the fact that even if all
    the replicas were on independent servers, but all the servers were on the same
    rack, a rack power failure would mean that no replica would be available.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了冗余性，在某个服务器发生故障或停止响应时，始终会有第二个甚至第三个副本可用。为了确保该过程无缝进行，DataNode 会将副本放在独立的服务器上，并且可以确保这些块被放置在数据中心不同机架上的服务器上。因为即使所有副本都在独立的服务器上，但如果这些服务器都在同一机架上，机架电力故障就会导致没有副本可用。
- en: 'The general process of writing data into HDFS is as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 写入数据到 HDFS 的一般过程如下：
- en: The NameNode receives a request to write a new file to HDFS.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NameNode 收到写入新文件到 HDFS 的请求。
- en: Since the data has to be written in blocks or chunks, the HDFS client (the entity
    that made the request) begins caching data into a local buffer and once the buffer
    reaches the allocated chunk size (for example, 128 MB), it informs the NameNode
    that it is ready to write the first block (chunk) of data.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于数据必须以块或块状单元的形式写入，HDFS 客户端（即发出请求的实体）开始将数据缓存到本地缓冲区，一旦缓冲区达到分配的块大小（例如 128 MB），它会通知
    NameNode，表示准备好写入第一个数据块（块）。
- en: The NameNode, based on information available to it about the state of the HDFS
    cluster, responds with information on the destination DataNode where the block
    needs to be stored.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NameNode 根据其所掌握的 HDFS 集群状态信息，回应并提供需要存储数据块的目标 DataNode 信息。
- en: The HDFS client writes data to the target DataNode and informs the NameNode
    once the write process for the block has completed.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HDFS 客户端将数据写入目标 DataNode，并在数据块写入完成后通知 NameNode。
- en: The target DataNode, subsequently, begins copying its copy of the block of data
    to a second DataNode, which will serve as a replica for the current block.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后，目标 DataNode 开始将其数据块的副本复制到第二个 DataNode，该节点将作为当前块的副本。
- en: Once the second DataNode completes the write process, it sends the block of
    data to the third DataNode.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦第二个 DataNode 完成写入过程，它会将数据块发送给第三个 DataNode。
- en: This process repeats until all the blocks corresponding to the data (or equivalently,
    the file) are copied across different nodes.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这一过程会一直重复，直到所有数据块（或等同的文件）都复制到不同的节点上。
- en: Note that the number of chunks will depend on the file size. The following image
    illustrated the distribution of the data across 5 datanodes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，块的数量将取决于文件的大小。下图展示了数据在 5 个数据节点之间的分布。
- en: '![](img/0750456b-a13d-47df-ba1d-d734014189af.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0750456b-a13d-47df-ba1d-d734014189af.png)'
- en: Master Node and Data Nodes
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点和数据节点
- en: 'The HDFS architecture in the first release of Hadoop, also known as Hadoop
    1, had the following characteristics:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 第一个版本中，HDFS 的架构（也被称为 Hadoop 1）具有以下特点：
- en: 'Single NameNode: Only one NameNode was available, and as a result it also acted
    as a single point of failure since it stored all the cluster metadata.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一 NameNode：在早期版本中只有一个 NameNode，这使得它成为单点故障的源头，因为它存储了所有集群的元数据。
- en: Multiple DataNodes that stored blocks of data, processed client requests, and
    performed I/O operations (create, read, delete, and so on) on the blocks.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储数据块、处理客户端请求并对数据块执行 I/O 操作（如创建、读取、删除等）的多个 DataNode。
- en: The HDFS architecture in the second release of Hadoop, also known as Hadoop
    2, provided all the benefits of the original HDFS design and also added some new
    features, most notably, the ability to have multiple NameNodes that can act as
    primary and secondary NameNodes. Other features included the facility to have
    multiple namespaces as well as HDFS Federation.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 第二版的 HDFS 架构，也称为 Hadoop 2，提供了原始 HDFS 设计的所有优点，并增加了一些新特性，最显著的是能够拥有多个 NameNode，它们可以作为主
    NameNode 和副 NameNode。其他特性包括可以拥有多个命名空间以及 HDFS Federation。
- en: 'HDFS Federation deserves special mention. The following excerpt from [http://hadoop.apache.org](http://hadoop.apache.org)
    explains the subject in a very precise manner:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HDFS Federation 值得特别提及。以下摘自[http://hadoop.apache.org](http://hadoop.apache.org)的内容以非常精确的方式解释了这一主题：
- en: The NameNodes are federated; the NameNodes are independent and do not require
    coordination with each other. The DataNodes are used as common storage for blocks
    by all the NameNodes. Each DataNode registers with all the NameNodes in the cluster.
    DataNodes send periodic heartbeats and block reports.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: NameNode 是分布式的；这些 NameNode 是独立的，并且不需要相互协调。DataNode 被所有 NameNode 用作块的公共存储。每个
    DataNode 都会向集群中的所有 NameNode 注册。DataNode 会定期发送心跳信号和块报告。
- en: The secondary NameNode is not a backup node in the sense that it cannot perform
    the same tasks as the NameNode in the event that the NameNode is not available.
    However, it makes the NameNode restart process much more efficient by performing
    housekeeping operations.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 副 NameNode 并不是一个备份节点，意味着当 NameNode 无法使用时，它不能执行与 NameNode 相同的任务。然而，它通过执行一些管理操作，使
    NameNode 的重启过程更加高效。
- en: These operations (such as merging HDFS snapshot data with information on data
    changes) are generally performed by the NameNode when it is restarted and can
    take a long time depending on the amount of changes since the last restart. The
    secondary NameNode can, however, perform these housekeeping operations whilst
    the primary NameNode is still in operation, such that in the event of a restart
    the primary NameNode can recover much faster. Since the secondary NameNode essentially
    performs a checkpoint on the HDFS data at periodic intervals, it is also known
    as the checkpoint node.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作（例如将 HDFS 快照数据与数据变更信息合并）通常由 NameNode 在重启时执行，且根据自上次重启以来的数据变更量，可能需要较长时间。然而，副
    NameNode 可以在主 NameNode 仍在运行时执行这些管理操作，从而确保在重启时主 NameNode 能够更快地恢复。由于副 NameNode 本质上会定期对
    HDFS 数据进行检查点操作，因此它也被称为检查点节点。
- en: Hadoop MapReduce
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop MapReduce
- en: MapReduce was one of the seminal features of Hadoop that was arguably the most
    instrumental in bringing it to prominence. MapReduce works on the principle of
    dividing larger tasks into smaller subtasks. Instead of delegating a single machine
    to compute a large task, a network of smaller machines can instead be used to
    complete the smaller subtasks. By distributing the work in this manner, the task
    can be completed much more efficiently relative to using a single-machine architecture.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 是 Hadoop 的标志性特性之一，它在推动 Hadoop 崛起方面起到了至关重要的作用。MapReduce 的工作原理是将较大的任务划分为更小的子任务。与其将单台机器分配给处理一个大任务，不如利用一组小型机器来完成这些小任务。通过这种方式分配工作，相比于单机架构，任务的完成效率会大大提高。
- en: This is not dissimilar to how we go about completing work in our day-to-day
    lives. An example will help to make this clearer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们日常工作中完成任务的方式并没有太大不同。一个例子有助于更清楚地说明这一点。
- en: An intuitive introduction to MapReduce
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce 的直观介绍
- en: Let's take the example of a hypothetical organization consisting of a CEO, directors,
    and managers. The CEO wants to know how many new hires have joined the company.
    The CEO sends a request to his or her directors to report back the number of hires
    in their departments. The directors in turn send a request to managers in their
    individual departments to provide the number of new hires. The managers provide
    the number to the directors, who in turn send the final value back to the CEO.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个虚拟的组织，包含 CEO、董事和经理。CEO 想要了解公司有多少新员工加入。CEO 向他的董事发送请求，要求他们汇报各自部门的新员工数量。董事则向各自部门的经理发送请求，要求经理提供新员工数量。经理将数字提供给董事，董事再将最终值反馈给
    CEO。
- en: 'This can be considered to be a real-world example of MapReduce. In this analogy,
    the task was finding the number of new hires. Instead of collecting all the data
    on his or her own, the CEO delegated it to the directors and managers who provided
    their own individual departmental numbers as illustrated in the following image:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以被视为 MapReduce 的一个现实世界的例子。在这个类比中，任务是找出新员工的数量。CEO 没有单独收集所有数据，而是将任务委托给了各个部门的董事和经理，他们提供了各自部门的个体数字，正如下面的图所示：
- en: '![](img/1d094fb0-9403-42f9-b7cc-04ad8f8e0747.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d094fb0-9403-42f9-b7cc-04ad8f8e0747.png)'
- en: The Concept of MapReduce
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 的概念
- en: In this rather simplistic scenario, the process of splitting a large task (find
    new hires in the entire company), into smaller tasks (new hires in each team),
    and then a final re-aggregation of the individual numbers, is analogous to how
    MapReduce works.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个相当简单的场景中，将一个大任务（在整个公司内寻找新员工）拆分成更小的任务（在每个团队内寻找新员工），然后再最终重新汇总这些个体数字，类似于 MapReduce
    的工作原理。
- en: A technical understanding of MapReduce
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce 的技术理解
- en: MapReduce, as the name implies, has a map phase and a reduce phase. A map phase
    is generally a function that is applied on each element of its input, thus modifying
    its original value.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce，如其名字所示，包含了 map 阶段和 reduce 阶段。map 阶段通常是对输入的每个元素应用的一个函数，从而修改其原始值。
- en: MapReduce generates key-value pairs as output.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 生成键值对作为输出。
- en: '**Key-value:** A key-value pair establishes a relationship. For example, if
    John is 20 years old, a simple key-value pair could be (John, 20). In MapReduce,
    the map operation produces such key-value pairs that have an entity and the value
    assigned to the entity.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**键值对：** 键值对建立了一种关系。例如，如果约翰 20 岁，一个简单的键值对可以是（John, 20）。在 MapReduce 中，map 操作会生成这样的键值对，包含一个实体和分配给该实体的值。'
- en: In practice, map functions can be complex and involve advanced functionalities.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 实际中，map 函数可以很复杂，并涉及高级功能。
- en: 'The reduce phase takes the key-value input from the map function and performs
    a summarization operation. For example, consider the output of a map operation
    that contains the ages of students in different grades in a school:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: reduce 阶段会接受来自 map 函数的键值输入，并执行汇总操作。例如，考虑一个 map 操作的输出，其中包含学校中不同年级学生的年龄：
- en: '| **Student name** | **Class** | **Age** |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **学生姓名** | **班级** | **年龄** |'
- en: '| John | Grade 1 | 7 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| John | Grade 1 | 7 |'
- en: '| Mary | Grade 2 | 8 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Mary | Grade 2 | 8 |'
- en: '| Jill | Grade 1 | 6 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Jill | Grade 1 | 6 |'
- en: '| Tom | Grade 3 | 10 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Tom | Grade 3 | 10 |'
- en: '| Mark | Grade 3 | 9 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Mark | Grade 3 | 9 |'
- en: We can create a simple key-value pair, taking for example the value of Class
    and Age (it can be anything, but I'm just taking these to provide the example).
    In this case, our key-value pairs would be (Grade 1, 7), (Grade 2, 8), (Grade
    1, 6), (Grade 3, 10), and (Grade 3, 9).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个简单的键值对，举例来说，取班级和年龄的值（可以是任何东西，但我只是取这些来提供示例）。在这种情况下，我们的键值对将是（Grade 1,
    7）、（Grade 2, 8）、（Grade 1, 6）、（Grade 3, 10）和（Grade 3, 9）。
- en: An operation that calculates the average of the ages of students in each grade
    could then be defined as a reduce operation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一个计算每个年级学生年龄平均值的操作可以被定义为一个 reduce 操作。
- en: More concretely, we can sort the output and then send the tuples corresponding
    to each grade to a different server.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们可以对输出进行排序，然后将对应于每个年级的元组发送到不同的服务器。
- en: For example, Server A would receive the tuples (Grade 1, 7) and (Grade 1, 6),
    Server B would receive the tuple (Grade 2, 8), Server C would receive the tuples
    (Grade 3, 10) and (Grade 3, 9). Each of the servers, A, B, and C, would then find
    the average of the tuples and report back (Grade 1, 6.5), (Grade 2, 8), and (Grade
    3, 9.5).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，服务器 A 会接收元组（Grade 1, 7）和（Grade 1, 6），服务器 B 会接收元组（Grade 2, 8），服务器 C 会接收元组（Grade
    3, 10）和（Grade 3, 9）。然后，A、B 和 C 服务器将分别计算这些元组的平均值并报告（Grade 1, 6.5）、（Grade 2, 8）和（Grade
    3, 9.5）。
- en: Observe that there was an intermediary step in this process that involved sending
    the output to a particular server and sorting the output to determine which server
    it should be sent to. And indeed, MapReduce requires a shuffle and sort phase,
    whereby the key-value pairs are sorted so that each reducer receives a fixed set
    of unique keys.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个过程中有一个中间步骤，涉及将输出发送到特定的服务器，并对输出进行排序，以确定应将其发送到哪个服务器。确实，MapReduce 需要一个 shuffle
    和 sort 阶段，在该阶段，键值对会被排序，以确保每个 reducer 接收到一组固定的唯一键。
- en: In this example, if say, instead of three servers there were only two, Server
    A could be assigned to computing averages for keys corresponding to Grades 1 and
    2, and Server B could be assigned to computing an average for Grade 3.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，如果说，假设不是有三台服务器，而是只有两台，服务器 A 可以被分配来计算与成绩 1 和 2 对应的键的平均值，服务器 B 则可以被分配来计算成绩
    3 的平均值。
- en: 'In Hadoop, the following process takes place during MapReduce:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hadoop 中，MapReduce 过程中会发生以下操作：
- en: The client sends a request for a task.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端发送任务请求。
- en: NameNode allocates DataNodes (individual servers) that will perform the map
    operation and ones that will perform the reduce operation. Note that the selection
    of the DataNode server is dependent upon whether the data that is required for
    the operation is *local to the server*. The servers where the data resides can
    only perform the map operation.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NameNode 分配将执行映射操作和归约操作的 DataNode（独立服务器）。注意，DataNode 服务器的选择取决于所需的数据是否 *本地存在于服务器上*。数据所在的服务器只能执行映射操作。
- en: DataNodes perform the map phase and produce key-value (k,v) pairs.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DataNode 执行映射阶段并生成键值对 (k,v)。
- en: As the mapper produces the (k,v) pairs, they are sent to these reduce nodes
    based on the *keys* the node is assigned to compute. The allocation of keys to
    servers is dependent upon a partitioner function, which could be as simple as
    a hash value of the key (this is default in Hadoop).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当映射器生成 (k,v) 对时，它们会根据节点分配的 *键* 被发送到这些 reduce 节点进行计算。键分配到服务器的方式取决于分区器函数，这个函数可以像计算键的哈希值一样简单（在
    Hadoop 中，这是默认的方式）。
- en: Once the reduce node receives its set of data corresponding to the keys it is
    responsible to compute on, it applies the reduce function and generates the final
    output.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 reduce 节点接收到与其负责计算的键相对应的数据集，它就会应用 reduce 函数并生成最终的输出。
- en: Hadoop maximizes the benefits of data locality. Map operations are performed
    by servers that hold the data locally, that is, on disk. More precisely, the map
    phase will be executed only by those servers that hold the blocks corresponding
    to the file. By delegating multiple individual nodes to perform computations independently,
    the Hadoop architecture can perform very large-scale data processing effectively.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 最大化了数据本地性的好处。映射操作由本地持有数据的服务器执行，即在磁盘上。更准确地说，映射阶段仅会由持有与文件对应块的服务器执行。通过委派多个独立的节点独立执行计算，Hadoop
    架构可以有效地进行大规模的数据处理。
- en: Block size and number of mappers and reducers
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 块大小以及映射器和归约器的数量
- en: An important consideration in the MapReduce process is an understanding of HDFS
    block size, that is, the size of the chunks into which the files have been split.
    A MapReduce task that needs to access a certain file will need to perform the
    map operation on each block representing the file. For example, given a 512 MB
    file and a 128 MB block size, four blocks would be needed to store the entire
    file. Hence, a MapReduce operation will at a minimum require four map tasks whereby
    each map operation would be applied to each subset of the data (that is, each
    of the four blocks).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MapReduce 过程中，一个重要的考虑因素是理解 HDFS 块大小，即文件被分割成的块的大小。需要访问某个文件的 MapReduce 任务需要对表示该文件的每个块执行映射操作。例如，给定一个
    512 MB 的文件和一个 128 MB 的块大小，存储整个文件需要四个块。因此，MapReduce 操作至少需要四个映射任务，其中每个映射操作应用于数据的每个子集（即四个块中的每一个）。
- en: If the file was very large, however, and required say, 10,000 blocks to store,
    this means we would have required 10,000 map operations. But, if we had only 10
    servers, then we'd have to send 1,000 map operations to each server. This might
    be sub-optimal as it can lead to a high penalty due to disk I/O operations and
    resource allocation settings on a per-map basis.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果文件非常大，并且需要 10,000 个块来存储，这意味着我们将需要进行 10,000 次映射操作。但如果我们只有 10 台服务器，那么每台服务器将需要执行
    1,000 次映射操作。这可能是次优的，因为它可能导致由于磁盘 I/O 操作和每次映射的资源分配设置而产生较高的惩罚。
- en: The number of reducers required is summarized very elegantly on Hadoop Wiki
    ([https://wiki.apache.org/hadoop/HowManyMapsAndReduces](https://wiki.apache.org/hadoop/HowManyMapsAndReduces)).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的 reducer 数量在 Hadoop Wiki 上有很好的总结 ([https://wiki.apache.org/hadoop/HowManyMapsAndReduces](https://wiki.apache.org/hadoop/HowManyMapsAndReduces))。
- en: 'The ideal reducers should be the optimal value that gets them closest to:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的 reducers 应该是最优值，使它们尽可能接近：
- en: '* A multiple of the block size * A task time between 5 and 15 minutes * Creates
    the fewest files possible'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '* 块大小的倍数 * 任务时间在 5 到 15 分钟之间 * 尽可能创建最少的文件'
- en: 'Anything other than that means there is a good chance your reducers are less
    than great. There is a tremendous tendency for users to use a REALLY high value
    ("More parallelism means faster!") or a REALLY low value ("I don''t want to blow
    my namespace quota!"). Both are equally dangerous, resulting in one or more of:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，意味着你的Reducer的性能很可能不太好。用户常常倾向于使用一个非常高的值（“更多并行意味着更快！”）或一个非常低的值（“我不想超出我的命名空间配额！”）。这两者都是非常危险的，可能导致以下情况之一：
- en: '* Terrible performance on the next phase of the workflow * Terrible performance
    due to the shuffle * Terrible overall performance because you''ve overloaded the
    namenode with objects that are ultimately useless * Destroying disk IO for no
    really sane reason * Lots of network transfers due to dealing with crazy amounts
    of CFIF/MFIF work'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '* 工作流的下一个阶段性能差 * 由于洗牌过程导致性能差 * 由于过载了NameNode并处理了最终无用的对象，导致整体性能差 * 不合理的破坏磁盘IO
    * 由于处理大量的CFIF/MFIF工作，导致大量网络传输'
- en: Hadoop YARN
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop YARN
- en: YARN was a module introduced in Hadoop 2\. In Hadoop 1, the process of managing
    jobs and monitoring them was performed by processes known as JobTracker and TaskTracker(s).
    NameNodes that ran the JobTracker daemon (process) would submit jobs to the DataNodes
    which ran TaskTracker daemons (processes).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: YARN是Hadoop 2中引入的一个模块。在Hadoop 1中，作业管理和监控是由名为JobTracker和TaskTracker的进程执行的。运行JobTracker守护进程（进程）的NameNode会将作业提交给运行TaskTracker守护进程（进程）的DataNode。
- en: 'The JobTracker was responsible for the co-ordination of all MapReduce jobs
    and served as a central administrator for managing processes, handling server
    failure, re-allocating to new DataNodes, and so on. The TaskTracker monitored
    the execution of jobs local to its own instance in the DataNode and provided feedback
    on the status to the JobTracker as shown in the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: JobTracker负责协调所有MapReduce作业，并作为中央管理员管理进程，处理服务器故障，重新分配给新的DataNode等。TaskTracker监控其本地DataNode实例上的作业执行情况，并向JobTracker提供状态反馈，如下所示：
- en: '![](img/77c0cfdf-fc44-4b4d-91fe-2d0709ee0109.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77c0cfdf-fc44-4b4d-91fe-2d0709ee0109.png)'
- en: JobTracker and TaskTrackers
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: JobTracker和TaskTrackers
- en: This design worked well for a long time, but as Hadoop evolved, the demands
    for more sophisticated and dynamic functionalities rose proportionally. In Hadoop
    1, the NameNode, and consequently the JobTracker process, managed both job scheduling
    and resource monitoring. In the event the NameNode failed, all activities in the
    cluster would cease immediately. Lastly, all jobs had to be represented in MapReduce
    terms - that is, all code would have to be written in the MapReduce framework
    in order to be executed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计在很长一段时间内都运作良好，但随着Hadoop的发展，对更复杂和动态功能的需求也相应增加。在Hadoop 1中，NameNode以及相应的JobTracker进程同时负责作业调度和资源监控。如果NameNode发生故障，集群中的所有活动将立即停止。最后，所有作业都必须以MapReduce的方式表示——也就是说，所有代码必须用MapReduce框架编写才能执行。
- en: 'Hadoop 2 alleviated all these concerns:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2解决了所有这些问题：
- en: The process of job management, scheduling, and resource monitoring was decoupled
    and delegated to a new framework/module called YARN
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业管理、调度和资源监控的过程被解耦并委托给一个名为YARN的新框架/模块。
- en: A secondary NameNode could be defined which would act as a helper for the primary
    NameNode
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以定义一个辅助的NameNode，作为主NameNode的辅助节点。
- en: Further, Hadoop 2.0 would accommodate frameworks beyond MapReduce
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，Hadoop 2.0将支持MapReduce以外的框架。
- en: Instead of fixed map and reduce slots, Hadoop 2 would leverage containers
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代替固定的map和reduce插槽，Hadoop 2将利用容器。
- en: In MapReduce, all data had to be read from disk, and this was fine for operations
    on large datasets but it was not optimal for operations on smaller datasets. In
    fact, any tasks that required very fast processing (low latency), were interactive
    in nature, or had multiple iterations (thus requiring multiple reads from the
    disk for the same data), would be extremely slow.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapReduce中，所有数据都必须从磁盘读取，这对于大数据集的操作是可行的，但对于小数据集的操作并不理想。实际上，任何需要非常快速处理（低延迟）、交互式的任务，或有多个迭代（因此需要多次从磁盘读取相同数据）的任务都会变得极为缓慢。
- en: By removing these dependencies, Hadoop 2 allowed developers to implement new
    programming frameworks that would support jobs with diverse performance requirements,
    such as low latency and interactive real-time querying, iterative processing required
    for machine learning, different topologies such as the processing of streaming
    data, optimizations such as in-memory data caching/processing, and so on.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过消除这些依赖关系，Hadoop 2允许开发人员实现支持具有不同性能需求的作业的新编程框架，例如低延迟和交互式实时查询、机器学习所需的迭代处理、流数据处理等不同拓扑结构、内存数据缓存/处理等优化。
- en: 'A few new terms became prominent:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一些新术语变得突出：
- en: '**ApplicationMaster**: Responsible for managing the resources needed by applications.
    For example, if a certain job required more memory, the ApplicationMaster would
    be responsible for securing the required resource. An application in this context
    refers to application execution frameworks such as MapReduce, Spark, and so on.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ApplicationMaster**：负责管理应用程序所需的资源。例如，如果某个作业需要更多内存，ApplicationMaster将负责确保获得所需的资源。这里所说的应用程序是指应用执行框架，如MapReduce、Spark等。'
- en: '**Containers**: The unit of resource allocation (for example, 1 GB of memory
    and four CPUs). An application may require several such containers to execute.
    The ResourceManager allocates containers for executing tasks. Once the allocation
    is complete, the ApplicationMaster requests DataNodes to start the allocated containers
    and takes over the management of the containers.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Containers**：资源分配的单位（例如，1GB内存和四个CPU）。一个应用程序可能需要多个这样的容器来执行。ResourceManager为执行任务分配容器。容器分配完成后，ApplicationMaster请求DataNodes启动已分配的容器，并接管容器的管理。'
- en: '**ResourceManager**: A component of YARN that had the primary role of allocating
    resources to applications and functioned as a replacement for the JobTracker.
    The ResourceManager process ran on the NameNode just as the JobTracker did.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ResourceManager**：YARN的一个组件，主要负责向应用程序分配资源，并作为JobTracker的替代者。ResourceManager进程与JobTracker一样运行在NameNode上。'
- en: '**NodeManagers**: A replacement for TaskTracker, NodeManagers were responsible
    for reporting the status of jobs to the ResourceManager (RM) and monitoring the
    resource utilization of containers.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NodeManagers**：是TaskTracker的替代者，NodeManagers负责向ResourceManager（RM）报告作业状态，并监控容器的资源利用情况。'
- en: 'The following image shows a high level view of the ResourceManager and NodeManagers
    in Hadoop 2.0:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了Hadoop 2.0中ResourceManager和NodeManagers的高级视图：
- en: '![](img/9debf45a-4304-4551-9c7b-ac04b702b13f.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9debf45a-4304-4551-9c7b-ac04b702b13f.png)'
- en: Hadoop 2.0
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2.0
- en: 'The prominent concepts inherent in Hadoop 2 have been illustrated in the next
    image:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2中固有的突出概念已在下图中展示：
- en: '![](img/86aed323-c6e4-4dbe-9272-3fc5e50d9441.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86aed323-c6e4-4dbe-9272-3fc5e50d9441.png)'
- en: Hadoop 2.0 Concepts
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2.0概念
- en: Job scheduling in YARN
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YARN中的作业调度
- en: It is not uncommon for large Hadoop clusters to have multiple jobs running concurrently.
    The allocation of resources when there are multiple jobs submitted from multiple
    departments becomes an important and indeed interesting topic. Which request should
    receive priority if say, two departments, A and B, submit a job at the same time
    but each request is for the maximum available resources? In general, Hadoop uses
    a **First-In-First-Out** (**FIFO**) policy. That is, whoever submits the job first
    gets to use the resources first. But what if A submitted the job first but completing
    A's job will take five hours whereas B's job will complete in five minutes?
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 大型Hadoop集群中，多个作业同时运行并不罕见。当多个部门提交多个作业时，资源分配成为一个重要且有趣的话题。例如，如果A部门和B部门同时提交作业，而每个请求都要求最大可用资源，应该优先处理哪个请求呢？通常，Hadoop使用**先进先出**（**FIFO**）策略。也就是说，谁先提交作业，谁就先使用资源。但如果A部门先提交了作业，但A的作业需要五个小时才能完成，而B的作业只需要五分钟呢？
- en: 'To deal with these nuances and variables in job scheduling, numerous scheduling
    methods have been implemented. Three of the more commonly used ones are:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对作业调度中的这些细节和变量，已经实现了多种调度方法。以下是三种常用的方法：
- en: '**FIFO**: As described above, FIFO scheduling uses a queue to priorities jobs.
    Jobs are executed in the order in which they are submitted.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FIFO**：如上所述，FIFO调度使用队列来优先处理作业。作业按照提交的顺序执行。'
- en: '**CapacityScheduler**: CapacityScheduler assigns a value on the number of jobs
    that can be submitted on a per-department basis, where a department can indicate
    a logical group of users. This is to ensure that each department or group can
    have access to the Hadoop cluster and be able to utilize a minimum number of resources.
    The scheduler also allows departments to scale up beyond their assigned capacity
    up to a maximum value set on a per-department basis if there are unused resources
    on the server. The model of CapacityScheduler thus provides a guarantee that each
    department can access the cluster on a deterministic basis.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容量调度器**：容量调度器根据每个部门可以提交的作业数量进行分配，其中一个部门可以表示一组逻辑用户。这是为了确保每个部门或组都能访问Hadoop集群并能够使用最低限度的资源。如果服务器上有未使用的资源，调度器还允许部门在其分配的容量之外进行扩展，直到达到每个部门设定的最大值。因此，容量调度器的模型提供了保证，确保每个部门可以基于确定性访问集群。'
- en: '**Fair Schedulers**: These schedulers attempt to evenly balance the utilization
    of resources across different apps. While an even balance might not be feasible
    at a certain given point in time, balancing allocation over time such that the
    averages are more or less similar can be achieved using Fair Schedulers.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平调度器**：这些调度器尝试在不同应用之间均衡地分配资源。虽然在某些时刻完全均衡可能不可行，但通过公平调度器，可以在一段时间内实现资源分配的均衡，使得各个应用的平均资源使用量更为接近。'
- en: These, and other schedulers, provide finely grained access controls (such as
    on a per-user or per-group basis) and primarily utilize queues in order to prioritize
    and allocate resources.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些以及其他调度器提供了细粒度的访问控制（例如按用户或按组分配），并主要利用队列来优先分配资源。
- en: Other topics in Hadoop
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop中的其他主题
- en: A few other aspects of Hadoop deserve special mention. As we have discussed
    the most important topics at length, this section provides an overview of some
    of the other subjects of interest.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的其他一些方面值得特别提及。由于我们已详细讨论了最重要的主题，本节提供了其他一些相关主题的概述。
- en: Encryption
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加密
- en: Data encryption is mandated by official regulations for various types of data.
    In the US, data that identifies patient information is required to be compliant
    with the rules set forth by HIPAA that dictate how such records should be stored.
    Data in HDFS can be encrypted whilst at rest (on disk) and/or while in transit.
    The keys that are used to decrypt the data are generally managed by **Key Management
    Systems** (**KMSs**).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加密是官方法规对各种数据类型的强制要求。在美国，涉及患者信息的数据需要符合HIPAA规定，这些规定决定了此类记录应该如何存储。HDFS中的数据可以在静态（磁盘上）和/或传输过程中进行加密。用于解密数据的密钥通常由**密钥管理系统**（**KMSs**）进行管理。
- en: User authentication
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户认证
- en: Hadoop can use the native user-authentication methods of the server. For example,
    in Linux-based machines, users can be authenticated based on the IDs defined in
    the system's `/etc/passwd` files. In other words, Hadoop inherits the user authentication
    set up on the server side.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop可以使用服务器的本地用户认证方法。例如，在基于Linux的机器上，用户可以根据系统的`/etc/passwd`文件中定义的ID进行认证。换句话说，Hadoop继承了服务器端设置的用户认证。
- en: User authentication via Kerberos, a cross-platform authentication protocol,
    is also commonly used in Hadoop clusters. Kerberos works based on a concept of
    tickets that grant privileges to users on a temporary as-needed basis. Tickets
    can be invalidated using Kerberos commands, thus restricting the users' rights
    to access resources on the cluster as needed.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Kerberos进行用户认证，Kerberos是一种跨平台认证协议，在Hadoop集群中也常常被使用。Kerberos基于票证的概念，票证授予用户临时的、按需的权限。票证可以使用Kerberos命令作废，从而根据需要限制用户访问集群资源的权限。
- en: Note that even if the user is permitted to access data (user authentication),
    he or she can still be limited in what data can be accessed due to another feature
    known as authorization. The term implies that even if the user can authenticate
    and log in to the system, the user may be restricted to only the data the user
    is authorized to access. This level of authorization is generally performed using
    native HDFS commands to change directory and file ownerships to the named users.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使用户被允许访问数据（用户认证），他或她仍然可能因为另一种称为授权的特性而受到限制，无法访问某些数据。该术语意味着，即使用户可以通过认证登录系统，用户可能只能访问其被授权访问的数据。此级别的授权通常通过本地HDFS命令来执行，修改目录和文件的所有权，将其分配给指定用户。
- en: Hadoop data storage formats
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop数据存储格式
- en: Since Hadoop involves storing very large-scale data, it is essential to select
    a storage type that is appropriate for your use cases. There are a few formats
    in which data can be stored in Hadoop, and the selection of the optimal storage
    format depends on your requirements in terms of read/write I/O speeds, how well
    the files can be compressed and decompressed on demand, and how easily the file
    can be split since the data will be eventually stored as blocks.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Hadoop涉及存储大规模数据，选择适合特定用例的存储类型至关重要。数据可以以几种格式存储在Hadoop中，选择最佳存储格式取决于读/写I/O速度、文件是否能够根据需求高效压缩和解压缩，以及文件是否容易拆分，因为数据最终将以块的形式存储。
- en: 'Some of the popular and commonly used storage formats are as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见和广泛使用的存储格式如下：
- en: '**Text/CSV**: These are plain text CSV files, similar to Excel files, but saved
    in plain text format. Since CSV files contain records on a per-line basis, it
    is naturally trivial to split the files up into blocks of data.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本/CSV**：这些是纯文本CSV文件，类似于Excel文件，但以纯文本格式保存。由于CSV文件按行存储记录，因此自然容易将文件拆分成数据块。'
- en: '**Avro**: Avro was developed to improve the efficient sharing of data across
    heterogeneous systems. It stores both the schema as well as the actual data in
    a single compact binary using data serialization. Avro uses JSON to store the
    schema and binary format for the data and serializes them into a single Avro Object
    Container File. Multiple languages such as Python, Scala, C/C++, and others have
    native APIs that can read Avro files and consequently, it is very portable and
    well suited for cross-platform data exchange.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Avro**：Avro旨在提高异构系统之间数据共享的效率。它使用数据序列化将模式和实际数据存储在一个紧凑的二进制格式中。Avro使用JSON存储模式，使用二进制格式存储数据，并将它们序列化为单一的Avro对象容器文件。多种语言（如Python、Scala、C/C++等）具有原生API，可以读取Avro文件，因此它非常便携，适用于跨平台的数据交换。'
- en: '**Parquet**: Parquet is a columnar data storage format. This helps to improve
    performance, sometimes significantly by permitting data storage and access on
    a per-column basis. Intuitively, if you were working on a 1 GB file with 100 columns
    and 1 million rows, and wanted to query data from only one of the 100 columns,
    being able to access just the individual column would be more efficient than having
    to access the entire file.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Parquet**：Parquet是一种列式数据存储格式。这有助于提高性能，有时通过允许按列存储和访问数据，从而显著提升效率。直观地说，如果你在处理一个1GB的文件，文件有100列和100万行，且你只想查询其中一列的数据，那么能够仅访问这一列会比必须访问整个文件更高效。'
- en: '**ORCFiles**: ORC stands for Optimized Row-Columnar. In a sense, it is a further
    layer of optimization over pure columnar formats such as Parquet. ORCFiles store
    data not only by columns, but also by rows, also known as stripes. A file with
    data in tabular format can thus be split into multiple smaller stripes where each
    stripe comprises of a subset of rows from the original file. By splitting data
    in this manner, if a user task requires access to only a small subsection of the
    data, the process can interrogate the specific stripe that holds the data.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ORCFiles**：ORC代表优化行列式。在某种意义上，它是纯列式格式（如Parquet）的一层额外优化。ORCFiles不仅按列存储数据，还按行存储数据，这些行被称为条带（stripes）。一个以表格格式存储的数据文件可以拆分成多个较小的条带，每个条带包含原始文件中的一部分行。通过这种方式拆分数据，如果用户任务只需要访问数据的一个小部分，进程可以查询包含该数据的特定条带。'
- en: '**SequenceFiles**: In SequenceFiles, data is represented as key-value pairs
    and stored in a binary serialized format. Due to serialization, data can be represented
    in a compact binary format that not only reduces the data size but consequently
    also improves I/O. Hadoop, and more concretely HDFS, is not efficient when there
    are multiple files of a small size, such as audio files. SequenceFiles solve this
    problem by allowing multiple small files to be stored as a single unit or SequenceFile.
    They are also very well suited for parallel operations that are splittable and
    are overall efficient for MapReduce jobs.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SequenceFiles**：在SequenceFiles中，数据以键值对的形式表示，并以二进制序列化格式存储。由于序列化，数据可以以紧凑的二进制格式表示，不仅减少了数据大小，而且提高了I/O效率。当存在多个小文件（例如音频文件）时，Hadoop，特别是HDFS，效率较低。SequenceFiles通过将多个小文件作为一个单元或SequenceFile存储来解决这个问题。它们还非常适合用于可拆分的并行操作，并且总体上对于MapReduce作业效率较高。'
- en: '**HDFS Snapshots:** HDFS Snapshots allow users to preserve data at a given
    point in time in a read-only mode. Users can create snapshots—in essence a replica
    of the data as it is at that point time—in in HDFS, such that they can be retrieved
    at a later stage as and when needed. This ensures that data can be recovered in
    the event that there is a file corruption or any other failure that affects the
    availability of data. In that regard, it can be also considered to be a backup.
    The snapshots are available in a .snapshot directory where they have been created.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HDFS快照：** HDFS快照允许用户在给定时间点以只读模式保留数据。用户可以在HDFS中创建快照，本质上是某一时刻的数据副本，以便稍后需要时可以恢复。这确保了在文件损坏或其他影响数据可用性的故障发生时，数据能够恢复。因此，它也可以被视为备份。这些快照会保存在创建它们的`.snapshot`目录中。'
- en: '**Handling of node failures:** Large Hadoop clusters can contain tens of thousands
    of nodes. Hence it is likely that there would be server failures on any given
    day. So that the NameNode is aware of the status of all nodes in the cluster,
    the DataNodes send a periodic heartbeat to the NameNode. If the NameNode detects
    that a server has failed, that is, it has stopped receiving heartbeats, it marks
    the server as failed and replicates all the data that was local to the server
    onto a new instance.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点故障处理：** 大型Hadoop集群可能包含数万台节点。因此，任何一天都会发生服务器故障。因此，为了让NameNode了解集群中所有节点的状态，DataNodes会定期向NameNode发送心跳信号。如果NameNode检测到某台服务器故障，即停止接收心跳信号，NameNode会将该服务器标记为失败，并将该服务器上的所有数据复制到新实例上。'
- en: New features expected in Hadoop 3
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop 3中预期的新功能
- en: At the time of writing this book, Hadoop 3 is in Alpha stage. Details on the
    new changes that will be available in Hadoop 3 can be found on the internet. For
    example, [http://hadoop.apache.org/docs/current/](http://hadoop.apache.org/docs/current/)
    provides the most up-to-date information on new changes to the architecture.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在写这本书时，Hadoop 3仍处于Alpha阶段。有关Hadoop 3的新变化的详细信息可以在互联网上找到。例如，[http://hadoop.apache.org/docs/current/](http://hadoop.apache.org/docs/current/)提供了有关架构变化的最新信息。
- en: The Hadoop ecosystem
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop生态系统
- en: This chapter should be titled as the Apache ecosystem. Hadoop, like all the
    other projects that will be discussed in this section, is an Apache project. Apache
    is used loosely as a short form for the open source projects that are supported
    by the Apache Software Foundation. It originally has its roots in the development
    of the Apache HTTP server in the early 90s, and today is a collaborative global
    initiative that comprises entirely of volunteers who participate in releasing
    open source software to the global technical community.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 本章应命名为Apache生态系统。Hadoop，和本节中将讨论的所有其他项目一样，都是一个Apache项目。Apache这个名称通常作为Apache软件基金会支持的开源项目的简称。它最初源于1990年代初期Apache
    HTTP服务器的开发，如今是一个全球性的协作项目，完全由志愿者组成，这些志愿者参与发布开源软件，服务于全球技术社区。
- en: Hadoop started out as, and still is, one of the projects in the Apache ecosystem.
    Due to its popularity, many other projects that are also part of Apache have been
    linked directly or indirectly to Hadoop as they support key functionalities in
    the Hadoop environment. That said, it is important to bear in mind that these
    projects can in most cases exist as independent products that can function without
    a Hadoop environment. Whether it would provide optimal functionality would be
    a separate topic.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop最初作为Apache生态系统中的一个项目起步，并且至今仍然是。由于其流行性，许多其他也属于Apache的项目直接或间接地与Hadoop关联，它们在Hadoop环境中支持关键功能。尽管如此，重要的是要记住，这些项目在大多数情况下可以作为独立产品存在，并且可以在没有Hadoop环境的情况下运行。它们是否提供最佳功能则是另一个话题。
- en: 'In this section, we''ll go over some of the Apache projects that have had a
    great deal of influence as well as an impact on the growth and usability of Hadoop
    as a standard IT enterprise solution, as detailed in the following figure:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些对Hadoop的增长和可用性产生重要影响的Apache项目，这些项目已成为标准的IT企业解决方案，具体内容见下图：
- en: '| **Product** | **Functionality** |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| **产品** | **功能** |'
- en: '| Apache Pig | Apache Pig, also known as Pig Latin, is a language specifically
    designed to represent MapReduce programs through concise statements that define
    workflows. Coding MapReduce programs in the traditional methods, such as with
    Java, can be quite complex, and Pig provides an easy abstraction to express a
    MapReduce workflow and complex **Extract-Transform-Load** (**ETL**) through the
    use of simple semantics. Pig programs are executed via the Grunt shell. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Apache Pig | Apache Pig，也被称为 Pig Latin，是一种专门设计用于通过简洁的语句表示 MapReduce 程序的语言，这些语句定义了工作流。以传统方法（如
    Java）编写 MapReduce 程序可能非常复杂，而 Pig 提供了一种简化的抽象，可以通过简单的语法表达 MapReduce 工作流和复杂的**提取-转换-加载**（**ETL**）过程。Pig
    程序通过 Grunt shell 执行。|'
- en: '| Apache HBase | Apache HBase is a distributed column-oriented database that
    sits on top of HDFS. It was modelled on Google''s BigTable whereby data is represented
    in a columnar format. HBase supports low-latency read-write across tables with
    billions of records and is well suited to tasks that require direct random access
    to data. More concretely, HBase indexes data in three dimensions - row, column,
    and timestamp. It also provides a means to represent data with an arbitrary number
    of columns as column values can be expressed as key-value pairs within the cells
    of an HBase table. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Apache HBase | Apache HBase 是一个分布式列式数据库，位于 HDFS 之上。它是基于 Google 的 BigTable
    模型设计的，其中数据以列式格式表示。HBase 支持在包含数十亿条记录的表之间进行低延迟的读写，非常适合需要直接随机访问数据的任务。更具体地说，HBase
    按照行、列和时间戳三个维度对数据进行索引。它还提供了一种表示具有任意数量列的数据的方法，因为列值可以在 HBase 表的单元格中作为键值对表示。|'
- en: '| Apache Hive | Apache Hive provides a SQL-like dialect to query data stored
    in HDFS. Hive stores data as serialized binary files in a folder-like structure
    in HDFS. Similar to tables in traditional database management systems, Hive stores
    data in tabular format in HDFS partitioned based on user-selected attributes.
    Partitions are thus subfolders of the higher-level directories or tables. There
    is a third level of abstraction provided by the concept of buckets, which reference
    files in the partitions of the Hive tables. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Apache Hive | Apache Hive 提供了类似 SQL 的方言来查询存储在 HDFS 中的数据。Hive 将数据存储为序列化的二进制文件，采用类似文件夹的结构存储在
    HDFS 中。与传统数据库管理系统中的表类似，Hive 将数据存储为基于用户选择的属性分区的表格格式，分区因此成为更高级目录或表的子文件夹。还有一个由桶（buckets）概念提供的第三层抽象，桶引用了
    Hive 表的分区中的文件。|'
- en: '| Apache Sqoop | Sqoop is used to extract data from traditional databases to
    HDFS. Large enterprises that have data stored in relational database management
    systems can thus use Sqoop to transfer data from their data warehouse to a Hadoop
    implementation. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Apache Sqoop | Sqoop 用于从传统数据库提取数据到 HDFS。大企业如果将数据存储在关系型数据库管理系统中，可以使用 Sqoop
    将数据从数据仓库转移到 Hadoop 实现中。|'
- en: '| Apache Flume | Flume is used for the management, aggregation, and analysis
    of large-scale log data. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Apache Flume | Flume 用于大规模日志数据的管理、汇聚和分析。|'
- en: '| Apache Kafka | Kafka is a publish/subscribe-based middleware system that
    can be used to analyze and subsequently persist (in HDFS) streaming data in real
    time. |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Apache Kafka | Kafka 是一个基于发布/订阅的中间件系统，可以用来实时分析并将流式数据持久化（在 HDFS 中）。|'
- en: '| Apache Oozie | Oozie is a workflow management system designed to schedule
    Hadoop jobs. It implements a key concept known as a **directed acyclic graph**
    (**DAG**), which will be discussed in our section on Spark. |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Apache Oozie | Oozie 是一个工作流管理系统，用于调度 Hadoop 作业。它实现了一个称为**有向无环图**（**DAG**）的关键概念，稍后将在我们关于
    Spark 的章节中讨论。|'
- en: '| Apache Spark | Spark is one of the most significant projects in Apache and
    was designed to address some of the shortcomings of the HDFS-MapReduce model.
    It started as a relatively small project at UC Berkeley and evolved rapidly to
    become one of the most prominent alternatives to using Hadoop for analytical tasks.
    Spark has seen a widespread adoption across the industry and comprises of various
    other subprojects that provide additional capabilities such as machine learning,
    streaming analytics, and others. |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Apache Spark | Spark 是 Apache 中最重要的项目之一，旨在解决 HDFS-MapReduce 模型的一些不足之处。它最初是加州大学伯克利分校的一个相对较小的项目，迅速发展成为
    Hadoop 在分析任务中最显著的替代方案之一。Spark 已在业界得到了广泛采用，并且包含了多个子项目，提供了额外的能力，如机器学习、流式分析等。|'
- en: Hands-on with CDH
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亲自操作 CDH
- en: In this section, we will utilize the CDH QuickStart VM to work through some
    of the topics that have been discussed in the current chapter. The exercises do
    not have to be necessarily performed in a chronological order and are not dependent
    upon the completion of any of the other exercises.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将利用CDH QuickStart虚拟机来处理当前章节中讨论的一些主题。练习不一定需要按时间顺序执行，也不依赖于其他练习的完成。
- en: 'We will complete the following exercises in this section:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中我们将完成以下练习：
- en: WordCount using Hadoop MapReduce
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hadoop MapReduce进行单词计数
- en: Working with the HDFS
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作HDFS
- en: Downloading and querying data with Apache Hive
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Hive下载并查询数据
- en: WordCount using Hadoop MapReduce
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Hadoop MapReduce进行单词计数
- en: In this exercise, we will be attempting to count the number of occurrences of
    each word in one of the longest novels ever written. For the exercise, we have
    selected the book *Artamène ou le Grand Cyrus* written by Georges and/or Madeleine
    de Scudéry between 1649-1653\. The book is considered to be the second longest
    novel ever written, per the related list on Wikipedia ([https://en.wikipedia.org/wiki/List_of_longest_novels](https://en.wikipedia.org/wiki/List_of_longest_novels)).
    The novel consists of 13,905 pages across 10 volumes and has close to two million
    words.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将尝试统计有史以来最厚重小说之一中每个单词的出现次数。为了进行练习，我们选择了由Georges和/或Madeleine de Scudéry于1649-1653年间所写的小说《Artamène
    ou le Grand Cyrus》。该小说被认为是有史以来第二长的小说，相关列表可参考维基百科（[https://en.wikipedia.org/wiki/List_of_longest_novels](https://en.wikipedia.org/wiki/List_of_longest_novels)）。这本小说包含了13,905页，分为10卷，近200万个单词。
- en: 'To begin, we need to launch the Cloudera Distribution of Hadoop Quickstart
    VM in VirtualBox and double-click on the Cloudera Quickstart VM instance:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在VirtualBox中启动Cloudera Distribution of Hadoop Quickstart虚拟机，并双击Cloudera
    Quickstart虚拟机实例：
- en: '![](img/bcae8e19-32d6-4d52-9798-b80487582a38.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bcae8e19-32d6-4d52-9798-b80487582a38.png)'
- en: 'It will take some time to start up as it initializes all the CDH-related processes
    such as the DataNode, NameNode, and so on:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 启动过程需要一些时间，因为它会初始化所有与CDH相关的进程，如DataNode、NameNode等：
- en: '![](img/6f3d9fb5-6d5a-48ed-9227-371380b5fac8.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f3d9fb5-6d5a-48ed-9227-371380b5fac8.png)'
- en: 'Once the process starts up, it will launch a default landing page that contains
    references to numerous tutorials related to Hadoop. We''ll be writing our MapReduce
    code in the Unix terminal for this section. Launch the terminal from the top-left
    menu, as shown in the following screenshot:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦过程启动，它将启动一个默认的登录页面，其中包含许多与Hadoop相关的教程。我们将在Unix终端中编写MapReduce代码。本节中的终端可以从左上角的菜单启动，如下图所示：
- en: '![](img/26b31ed5-02d4-41c1-a81a-e2184b90b038.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26b31ed5-02d4-41c1-a81a-e2184b90b038.png)'
- en: 'Now, we must follow these steps:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要按照以下步骤操作：
- en: Create a directory named `cyrus`. This is where we will store all the files
    which contain the text of the book.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`cyrus`的目录。这是我们存储包含书籍文本的所有文件的地方。
- en: Run `getCyrusFiles.sh` as shown in step 4\. This will download the book into
    the `cyrus` directory.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照步骤 4 中所示运行`getCyrusFiles.sh`。这将把书籍下载到`cyrus`目录中。
- en: Run `processCyrusFiles.sh` as shown. The book contains various Unicode and non-printable
    characters. Additionally, we would like to change all the words to lowercase in
    order to ignore double-counting words that are the same but have capitalizations.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照所示运行`processCyrusFiles.sh`。书中包含各种Unicode和不可打印字符。此外，我们希望将所有单词转换为小写，以避免对大小写不同但相同的单词进行重复计数。
- en: This will produce a file called `cyrusprint.txt`. This document contains the
    entire text of the book. We will be running our MapReduce code on this text file.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将生成一个名为`cyrusprint.txt`的文件。该文档包含了整本书的文本。我们将在此文本文件上运行我们的MapReduce代码。
- en: Prepare `mapper.py` and `reducer.py`. As the name implies, `mapper.py` runs
    the map part of the MapReduce process. Similarly, `reducer.py` runs the reduce
    part of the MapReduce process. The file `mapper.py` will split the document into
    words and assign a value of one to each word in the document. The file, `reducer.py`,
    will read in the sorted output of `mapper.py` and sum the occurrences of the same
    word (by first initializing the count of the word to one and incrementing it for
    each new occurrence of the word). The final output is a file containing the count
    of each word in the document.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备`mapper.py`和`reducer.py`。顾名思义，`mapper.py`负责运行MapReduce过程中的map部分。同样，`reducer.py`负责运行MapReduce过程中的reduce部分。`mapper.py`文件会将文档分割成单词，并为每个单词分配一个值1。`reducer.py`文件会读取`mapper.py`的排序输出，并对相同单词的出现次数进行求和（通过首先将该单词的计数初始化为1，并在每次该单词出现时递增）。最终输出是一个包含文档中每个单词计数的文件。
- en: 'The steps are as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Create `getCyrusFiles.sh` - this script will be used to retrieve the data from
    the web:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`getCyrusFiles.sh` - 该脚本将用于从网上获取数据：
- en: '[PRE0]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create `processCyrusFiles.sh` - this script will be used to concatenate and
    cleanse the files that were downloaded in the previous step:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`processCyrusFiles.sh` - 该脚本将用于连接并清理在上一步骤下载的文件：
- en: '[PRE1]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Change the permissions to 755 to make the `.sh` files executable at the command
    prompt:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权限更改为755，以便在命令提示符下使`.sh`文件可执行：
- en: '[PRE2]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Execute `getCyrusFiles.sh`:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行`getCyrusFiles.sh`：
- en: '[PRE3]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Execute `processCyrusFiles.sh`:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行`processCyrusFiles.sh`：
- en: '[PRE4]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Execute the following steps to copy the final file, named `cyrusprint.txt`,
    to HDFS, create the `mapper.py` and `reducer.py` scripts.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下步骤，将最终文件（命名为`cyrusprint.txt`）复制到HDFS，并创建`mapper.py`和`reducer.py`脚本：
- en: The files, `mapper.py` and `reducer.py`, are referenced on Glenn Klockwood's
    website ([http://www.glennklockwood.com/data-intensive/hadoop/streaming.html](http://www.glennklockwood.com/data-intensive/hadoop/streaming.html)),
    which provides a wealth of information on MapReduce and related topics.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapper.py`和`reducer.py`文件可以在Glenn Klockwood的网站上找到（[http://www.glennklockwood.com/data-intensive/hadoop/streaming.html](http://www.glennklockwood.com/data-intensive/hadoop/streaming.html)），该网站提供了关于MapReduce和相关主题的丰富信息。'
- en: 'The following code shows the contents of `mapper.py`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了`mapper.py`的内容：
- en: '[PRE5]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Execute the mapper and reducer scripts that will perform the MapReduce operations
    in order to produce the word count. You may see error messages as shown here,
    but for the purpose of this exercise (and for generating the results), you may
    disregard them:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行mapper和reducer脚本，这些脚本将执行MapReduce操作以生成单词计数。你可能会看到如下错误消息，但为了本练习的目的（以及生成结果），可以忽略这些错误：
- en: '[PRE6]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The results are stored in HDFS under the `/user/cloudera/output` directory
    in files prefixed with `part-` :'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果存储在HDFS的`/user/cloudera/output`目录下，文件以`part-`为前缀：
- en: '[PRE7]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To view the contents of the file use `hdfs dfs -cat` and provide the name of
    the file. In this case we are viewing the first 10 lines of the output:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看文件内容，使用`hdfs dfs -cat`命令并提供文件名。在这个例子中，我们查看输出的前10行：
- en: '[PRE8]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Analyzing oil import prices with Hive
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Hive 分析石油进口价格
- en: 'In this section, we will use Hive to analyze the import prices of oil in countries
    across the world from 1980-2016\. The data is available from the site of the **OECD**
    (**Organisation for Economic Co-operation and Development**) at the URL shown
    in the following screenshot:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Hive 来分析1980-2016年间世界各国的石油进口价格。这些数据可以通过**OECD**（**经济合作与发展组织**）的网站获取，网址如以下截图所示：
- en: '![](img/23401bea-8fb7-4edb-af54-e882c4086d69.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23401bea-8fb7-4edb-af54-e882c4086d69.png)'
- en: The actual CSV file is available at [https://stats.oecd.org/sdmx-json/data/DP_LIVE/.OILIMPPRICE.../OECD?contentType=csv&amp;detail=code&amp;separator=comma&amp;csv-lang=en](https://stats.oecd.org/sdmx-json/data/DP_LIVE/.OILIMPPRICE.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的CSV文件可以通过以下链接获取：[https://stats.oecd.org/sdmx-json/data/DP_LIVE/.OILIMPPRICE.../OECD?contentType=csv&amp;detail=code&amp;separator=comma&amp;csv-lang=en](https://stats.oecd.org/sdmx-json/data/DP_LIVE/.OILIMPPRICE.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en)。
- en: 'Since we''ll be loading the data in Hive, it makes sense to download the file
    into our home directory via the terminal in our Cloudera Quickstart CDH environment.
    The steps we''d execute are as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将在Hive中加载数据，因此通过终端在Cloudera Quickstart CDH环境中将文件下载到我们的主目录是有意义的。我们将执行以下步骤：
- en: 'Download the CSV file into the CDH environment:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将CSV文件下载到CDH环境中：
- en: '![](img/91b3f48c-3292-4ae8-bef9-e9b434b94fd9.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91b3f48c-3292-4ae8-bef9-e9b434b94fd9.png)'
- en: '[PRE9]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Clean the CSV file. Data cleansing is an area of core importance in data science.
    In practice, it is very common to receive files that will require some level of
    cleansing. This is due to the fact that there could be invalid characters or values
    in columns, missing data, missing or additional delimiters, and so on. We noted
    that various values were enclosed in double-quotes ("). In Hive, we can ignore
    the quotes by specifying the `quoteChar` property whilst creating the table. Since
    Linux also offers simple and easy ways to remove such characters, we used `sed`
    to remove the quotation marks:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理CSV文件。数据清理是数据科学中的核心领域。实际上，接收需要某种程度清理的文件是非常常见的。这是因为列中可能会有无效字符或值、缺失数据、缺少或多余的分隔符等等。我们注意到各种值被双引号（"）包围。在Hive中，我们可以通过在创建表时指定`quoteChar`属性来忽略这些引号。由于Linux也提供了简单易行的方法来移除这些字符，我们使用了`sed`命令来去除引号：
- en: '[PRE10]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Moreover, in our downloaded file, `oil.csv`, we observed that there were non-printable
    characters that could cause issues. We removed them by issuing the following command:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在我们下载的文件`oil.csv`中，我们发现存在非打印字符，这可能导致问题。我们通过执行以下命令将其移除：
- en: '[PRE11]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '(Source: [http://alvinalexander.com/blog/post/linux-unix/how-remove-non-printable-ascii-characters-file-unix](http://alvinalexander.com/blog/post/linux-unix/how-remove-non-printable-ascii-characters-file-unix))'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '(来源: [http://alvinalexander.com/blog/post/linux-unix/how-remove-non-printable-ascii-characters-file-unix](http://alvinalexander.com/blog/post/linux-unix/how-remove-non-printable-ascii-characters-file-unix))'
- en: 'Finally, we copied the new file (`oil_clean.csv`) to `oil.csv`. Since the `oil.csv`
    file already existed in the same folder, we were prompted with an overwrite message
    and we entered `yes`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将新文件（`oil_clean.csv`）复制到`oil.csv`。由于`oil.csv`文件已经存在于同一文件夹中，我们收到了覆盖提示并输入了`yes`：
- en: '[PRE12]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Log in to Cloudera Hue:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到Cloudera Hue：
- en: 'Click on Hue on the Bookmarks bar in the browser. This will bring up the Cloudera
    login screen. Log in using ID `cloudera` and password `cloudera`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器的书签栏中点击Hue。这将弹出Cloudera登录界面。使用ID `cloudera` 和密码 `cloudera` 登录：
- en: '![](img/e279c451-220f-434d-947b-c166b798f4cf.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e279c451-220f-434d-947b-c166b798f4cf.png)'
- en: 'Click on Hue from the drop-down menu on Quick Start at the top of the Hue login
    window:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Hue登录窗口顶部的快速启动下拉菜单中点击Hue：
- en: '![](img/ada22afe-0776-4378-9f95-a056ff1077f5.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ada22afe-0776-4378-9f95-a056ff1077f5.png)'
- en: 'Create the table schema, load the CSV file, `oil.csv`, and view the records:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建表架构，加载CSV文件`oil.csv`，并查看记录：
- en: '[PRE13]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/7663083b-9243-4b24-ad98-cc522f0faae4.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7663083b-9243-4b24-ad98-cc522f0faae4.png)'
- en: Load the oil file.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载油文件。
- en: Now that the table has been loaded into Hive, you can run miscellaneous Hive
    commands using HiveQL. A full set of these commands is available at [https://cwiki.apache.org/confluence/display/Hive/LanguageManual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual).
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，表已加载到Hive中，你可以使用HiveQL运行各种Hive命令。这些命令的完整集可以在[https://cwiki.apache.org/confluence/display/Hive/LanguageManual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)中找到。
- en: 'For instance, to find the maximum, minimum, and average value of oil prices
    in each country from 1980-2015 (the date range of the dataset), we can use familiar
    SQL operators. The query would be as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要查找1980-2015年间（数据集的日期范围）每个国家油价的最大值、最小值和平均值，我们可以使用熟悉的SQL操作符。查询将如下所示：
- en: '[PRE14]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here is the screenshot of the same:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是相同的截图：
- en: '![](img/e414b22d-47af-4f7d-a052-474c1f7cf74d.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e414b22d-47af-4f7d-a052-474c1f7cf74d.png)'
- en: In similar ways, we can use an array of other SQL commands. The Hive Manual
    provides an in-depth look into these commands and the various ways data can be
    saved, queried, and retrieved.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，我们可以使用其他SQL命令。Hive手册深入介绍了这些命令以及数据保存、查询和检索的各种方法。
- en: Hue includes a set of useful features such as data visualization, data download,
    and others that allow users to perform ad hoc analysis on the data.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Hue包含一系列有用的功能，如数据可视化、数据下载等，允许用户对数据进行临时分析。
- en: 'To access the visualization feature, click on the visualization icon underneath
    the grid icon in the results section, as shown in the following screenshot:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问可视化功能，请点击结果部分网格图标下方的可视化图标，如下图所示：
- en: '![](img/499fcfb2-f240-4123-97d1-2f1bb8dcdd62.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/499fcfb2-f240-4123-97d1-2f1bb8dcdd62.png)'
- en: 'Select Scatter. In Hue, this type of chart, also known more generally as a
    scatterplot, allows users to create multivariate charts very easily. Different
    values for the x and y axes, as well as scatter size and grouping, can be selected,
    as shown in the following screenshot:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 选择散点图。在Hue中，这种图表类型，也被更广泛地称为散点图，允许用户非常容易地创建多变量图表。可以选择x轴和y轴的不同数值，以及散点大小和分组，如下图所示：
- en: '![](img/259f4b41-4364-4376-b13e-befe47bd30a6.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/259f4b41-4364-4376-b13e-befe47bd30a6.png)'
- en: 'The following is a simple pie chart that can be constructed by selecting Pie
    in the drop-down menu:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是通过在下拉菜单中选择“饼图”来构建的一个简单饼图：
- en: '![](img/6106cd02-6123-4778-9133-00a18ac7e5d1.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6106cd02-6123-4778-9133-00a18ac7e5d1.png)'
- en: Joining tables in Hive
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Hive中连接表
- en: Hive supports advanced join functionalities. The following illustrates the process
    of using Left Join. As seen, the original table has data for each country represented
    by their three-letter country code. Since Hue supports map charts, we can add
    the values for latitude and longitude to overlay the oil pricing data on a world
    map.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Hive支持高级连接功能。以下示例展示了使用左连接的过程。如图所示，原始表格包含每个国家的数据，国家以三字母代码表示。由于Hue支持地图图表，我们可以添加纬度和经度值，将油价数据叠加到世界地图上。
- en: 'To do so, we''ll need to download a dataset containing the values for latitude
    and longitude:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们需要下载一个包含纬度和经度值的数据集：
- en: '[PRE15]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once the file has been downloaded and cleansed, define the schema and load
    the data in Hive:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 下载并清理完文件后，定义架构并将数据加载到Hive中：
- en: '[PRE16]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/c66e5c65-998c-425f-b7d3-0bbcb229eebb.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c66e5c65-998c-425f-b7d3-0bbcb229eebb.png)'
- en: 'Join the oil data with the lat/long data:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 将油价数据与纬度/经度数据连接：
- en: '[PRE17]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/6ff9ec7c-634e-43a5-bc35-4187a92c525b.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ff9ec7c-634e-43a5-bc35-4187a92c525b.png)'
- en: We can now proceed with creating geospatial visualizations. It would be useful
    to bear in mind that these are preliminary visualizations in Hue that provide
    a very convenient means to view data. More in-depth visualizations can be developed
    on geographical data using shapefiles, polygons, and other advanced charting methods.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以继续进行地理空间可视化。需要记住的是，这些是Hue中的初步可视化，它们提供了一种非常方便的方式来查看数据。更深入的可视化可以使用形状文件、多边形和其他高级图表方法来开发。
- en: 'Select Gradient Map from the drop-down menu and enter the appropriate values
    to create the chart, as shown in the following figure:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 从下拉菜单中选择渐变地图，并输入适当的值来创建图表，如下图所示：
- en: '![](img/2b47321a-b355-411b-b262-9a5ea24c77cd.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b47321a-b355-411b-b262-9a5ea24c77cd.png)'
- en: 'The next chart was developed using the Marker Map option in the drop-down menu.
    It uses the three-character country code in order to place markers and associated
    values on the respective regions, as shown in the following figure:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图表是使用下拉菜单中的“标记地图”选项开发的。它使用三字符的国家代码来在相应地区上放置标记和相关的数值，如下图所示：
- en: '![](img/08fb5f5c-9fd6-4352-9d0b-39d28dd86133.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08fb5f5c-9fd6-4352-9d0b-39d28dd86133.png)'
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter provided a technical overview of Hadoop. We discussed the core
    components and core concepts that are fundamental to Hadoop, such as MapReduce
    and HDFS. We also looked at the technical challenges and considerations of using
    Hadoop. While it may appear simple in concept, the inner workings and a formal
    administration of a Hadoop architecture can be fairly complex. In this chapter
    we highlighted a few of them.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了Hadoop的技术概述。我们讨论了Hadoop的核心组件和核心概念，如MapReduce和HDFS。我们还探讨了使用Hadoop时的技术挑战和注意事项。虽然从概念上看它可能显得简单，但Hadoop架构的内部工作原理和正式的管理过程可能相当复杂。本章中我们强调了其中的一些问题。
- en: We concluded with a hands-on exercise on Hadoop using the Cloudera Distribution.
    For this tutorial, we used the CDH Virtual Machine downloaded earlier from Cloudera's
    website.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用Cloudera分发版的Hadoop进行实践练习做了总结。对于本教程，我们使用了从Cloudera官网之前下载的CDH虚拟机。
- en: In the next chapter, we will look at NoSQL, an alternative or a complementary
    solution to Hadoop depending upon your individual and/or organization al needs.
    While Hadoop offers a far richer set of capabilities, if your intended use case(s)
    can be done with simply NoSQL solutions, the latter may be an easier choice in
    terms of the effort required.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍NoSQL，作为Hadoop的替代方案或补充解决方案，具体取决于您的个人和/或组织需求。虽然Hadoop提供了更丰富的功能集，但如果您的使用案例可以通过简单的NoSQL解决方案完成，后者可能在所需的工作量方面是一个更容易的选择。
