- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Utilizing the Feature Store
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特征存储
- en: In the last chapter, we briefly touched upon what a **feature store** is and
    how **Databricks Feature Store** is unique in its own way.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们简要介绍了什么是**特征存储**，以及**Databricks Feature Store**在其独特性方面的优势。
- en: This chapter will take a more hands-on approach and utilize Databricks Feature
    Store to register our first feature table and discuss concepts related to Databricks
    Feature Store.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将采取更为实践的方式，利用Databricks Feature Store注册我们的第一个特征表，并讨论与Databricks Feature Store相关的概念。
- en: 'We will be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Diving into feature stores and the problems they solve
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入了解特征存储及其解决的问题
- en: Discovering feature stores on the Databricks platform
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Databricks平台上发现特征存储
- en: Registering your first feature table in Databricks Feature Store
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Databricks Feature Store中注册你的第一个特征表
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the code is available on the GitHub repository [https://github.com/PacktPublishing/Practical-Machine-Learning-on-Databricks](https://github.com/PacktPublishing/Practical-Machine-Learning-on-Databricks)
    and is self-contained. To execute the notebooks, you can import the code repository
    directly into your Databricks workspace using **Repos**. We discussed Repos in
    the second chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码都可以在GitHub仓库[https://github.com/PacktPublishing/Practical-Machine-Learning-on-Databricks](https://github.com/PacktPublishing/Practical-Machine-Learning-on-Databricks)中找到，且是自包含的。要执行这些笔记本，你可以直接将代码仓库导入到你的Databricks工作区，使用**Repos**功能。我们在第二章中讨论过Repos。
- en: Working knowledge of **Delta** format is required. If you are new to Delta format,
    check out [https://docs.databricks.com/en/delta/index.html](https://docs.databricks.com/en/delta/index.html)
    and [https://docs.databricks.com/en/delta/tutorial.html](https://docs.databricks.com/en/delta/tutorial.html)
    before going forward.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 需要具备**Delta**格式的基本知识。如果你是Delta格式的新人，可以在继续之前查看[https://docs.databricks.com/en/delta/index.html](https://docs.databricks.com/en/delta/index.html)和[https://docs.databricks.com/en/delta/tutorial.html](https://docs.databricks.com/en/delta/tutorial.html)。
- en: Diving into feature stores and the problems they solve
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解特征存储及其解决的问题
- en: As more teams in the organization start to use AI and ML to solve various business
    use cases, it becomes necessary to have a centralized, reusable, and easily discoverable
    feature repository. This repository is called a feature store.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着组织中的更多团队开始使用人工智能（AI）和机器学习（ML）来解决各种业务应用场景，建立一个集中的、可重用的、易于发现的特征库变得尤为必要。这个库被称为特征存储（Feature
    Store）。
- en: All the curated features are in centralized, governed, access-controlled storage,
    such as a curated data lake. Different data science teams can be granted access
    to feature tables based on their needs. Like in enterprise data lakes, we can
    track data lineage; similarly, we can track the lineage of a feature table logged
    in Databricks Feature Store. We can also see all the downstream models that are
    consuming features from a registered feature table.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有经过整理的特征都存储在集中式、受管理、访问受控的存储中，例如整理过的数据湖。不同的数据科学团队可以根据需求被授予访问特征表的权限。像企业数据湖一样，我们可以追踪数据的血缘关系；同样，我们也可以追踪在Databricks
    Feature Store中注册的特征表的血缘关系。我们还可以查看所有消费已注册特征表中特征的下游模型。
- en: There are hundreds of data science teams tackling different business questions
    in large organizations. Each team may have its own domain knowledge and expertise.
    Performing feature engineering often requires heavy processing. Without a feature
    store, it becomes difficult for a new group of data scientists to reuse the features
    created and curated by another data science team.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型组织中，有数百个数据科学团队在解决不同的业务问题。每个团队可能都有自己的领域知识和专业技能。特征工程的实施通常需要大量的计算处理。如果没有特征存储，新加入的数据科学团队将很难重复使用其他团队创建和整理的特征。
- en: We can think of feature store workflows as being similar to ETL workflows that
    cater to a specific type of BI or analytics use case. The workflows that write
    data to feature store tables cater to a particular feature-engineering process
    that needs to be performed on the curated dataset in your data lake before training
    an ML model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将特征存储的工作流类比于ETL工作流，后者针对特定的BI或分析应用场景。将数据写入特征存储表的工作流针对的是特征工程过程，需要在数据湖中的整理数据集上执行这些过程，然后才能训练机器学习模型。
- en: You can schedule and monitor the execution of feature table workflows just like
    a regular ETL operation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像常规的ETL操作一样调度和监控特征表工作流的执行。
- en: Feature stores also solve the problem of *skew* between model training and inference
    code by providing a central repository of features across the organization. The
    same feature-engineering logic is used during model training and inference.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 特性存储还通过提供一个跨组织的特性中央仓库，解决了模型训练和推理代码之间的*偏差*问题。在模型训练和推理时使用相同的特性工程逻辑。
- en: Let’s look at how Feature Store has been built and integrated with the Databricks
    workspace.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看特性存储是如何构建并与 Databricks 工作空间集成的。
- en: Discovering feature stores on the Databricks platform
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Databricks 平台上发现特性存储
- en: Each Databricks workspace has its own feature store. At the time of writing
    this book, **Databricks Feature Store** only supports the Python API. The latest
    Python API reference is located at [https://docs.databricks.com/applications/machine-learning/feature-store/python-api.html](https://docs.databricks.com/applications/machine-learning/feature-store/python-api.html).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Databricks 工作空间都有自己的特性存储。在本书编写时，**Databricks 特性存储**仅支持 Python API。最新的 Python
    API 参考可以在 [https://docs.databricks.com/applications/machine-learning/feature-store/python-api.html](https://docs.databricks.com/applications/machine-learning/feature-store/python-api.html)
    找到。
- en: Databricks Feature Store is fully integrated with **Managed MLFlow** and other
    Databricks components. This allows models that are deployed by utilizing MLFlow
    to automatically retrieve the features at the time of training and inference.
    The exact steps involved in defining a feature table and using it with model training
    and inference are going to be covered in the following sections.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 特性存储与**管理的 MLFlow**及其他 Databricks 组件完全集成。这使得通过 MLFlow 部署的模型可以在训练和推理时自动检索特性。定义特性表并在模型训练和推理中使用它的具体步骤将在接下来的部分中介绍。
- en: Let’s look at some of the key concepts and terminology associated with Databricks
    Feature Store.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些与 Databricks 特性存储相关的关键概念和术语。
- en: Feature table
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特性表
- en: As the name suggests, a feature store stores features generated by data scientists
    after doing feature engineering for a particular problem.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名称所示，特性存储存储由数据科学家在进行特性工程时为特定问题生成的特性。
- en: 'These features may come from one or more clean and curated tables in the data
    lake. A feature table in Databricks contains two main components:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特性可能来自数据湖中一个或多个清理过的表。Databricks 中的特性表包含两个主要组件：
- en: '**Metadata**: The metadata tracks the source of the data utilized to create
    the feature table, which notebooks and scheduled jobs write data into the feature
    table, and at what frequency. The metadata also tracks downstream ML models utilizing
    the feature table. This provides lineage.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元数据**：元数据跟踪用于创建特性表的数据来源，哪些笔记本和定时任务将数据写入特性表，以及其频率。元数据还跟踪使用该特性表的下游 ML 模型。这提供了数据的溯源。'
- en: '**Generated feature data**: In the case of batch and streaming inference, the
    underlying generated feature DataFrame is written out as a Delta table to an offline
    feature store. Databricks manages this offline feature store for you. In contrast,
    the feature table is written out to a supported **relational database management
    system** (**RDBMS**) for an **online feature store**. The online feature store
    is not managed by Databricks and requires some additional steps to set up. There
    is a link in the *Further reading* section that you can refer to in order to set
    up an online feature store.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成的特性数据**：在批处理和流式推理的情况下，底层生成的特性 DataFrame 被写入作为 Delta 表到离线特性存储中。Databricks
    为你管理这个离线特性存储。相比之下，特性表被写入到支持的**关系数据库管理系统**（**RDBMS**）中，以用于**在线特性存储**。在线特性存储不由 Databricks
    管理，设置时需要一些额外的步骤。你可以在*进一步阅读*部分找到相关链接，以帮助你设置在线特性存储。'
- en: 'Let’s briefly understand the different types of inference patterns and how
    Databricks Feature Store can be beneficial in each scenario before moving forward:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们简要了解不同类型的推理模式，以及 Databricks 特性存储在每种场景中的优势。
- en: '**Batch inference**: Batch inference involves making predictions on a large
    set of data all at once, typically in intervals or scheduled runs. In Databricks,
    you can set up batch jobs using technologies such as **Apache Spark** to process
    and predict input data. Batch inference is well suited for scenarios where timely
    predictions are not critical and you can afford to wait for results. For instance,
    this could be used in customer segmentation, where predictions are made periodically.
    This scenario is supported by an offline feature store.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量推理**：批量推理涉及对大量数据进行一次性预测，通常是在间隔时间或按计划执行时进行。在Databricks中，你可以使用如**Apache Spark**等技术设置批处理作业，以处理和预测输入数据。批量推理适用于那些预测时效性不那么关键，可以等待结果的场景。例如，这可以用于客户细分，其中预测是定期进行的。此场景由离线特征存储支持。'
- en: Databricks Feature Store enhances batch inference by providing a centralized
    repository for feature data. Instead of recalculating features for every batch
    job, Feature Store allows you to store and manage pre-computed features. This
    reduces computation time, ensuring consistent and accurate features for your models
    during each batch run.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks特征存储通过提供集中式的特征数据存储库来增强批量推理。与其为每个批处理作业重新计算特征，特征存储允许你存储和管理预计算的特征。这减少了计算时间，确保在每次批处理运行中为你的模型提供一致且准确的特征。
- en: '**Streaming inference**: Streaming inference involves processing and making
    predictions on data as it arrives in real time, without waiting for the entire
    dataset to be collected. Databricks supports streaming data processing using tools
    such as Apache Spark’s **Structured Streaming**. Streaming inference is valuable
    when you need to respond quickly to changing data, such as in fraud detection
    where immediate action is crucial. This scenario is supported by an offline feature
    store.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式推理**：流式推理涉及在数据实时到达时进行处理和预测，而无需等待整个数据集的收集。Databricks通过如Apache Spark的**结构化流式处理**等工具支持流式数据处理。流式推理在需要快速响应变化数据的场景中非常有价值，例如在欺诈检测中，及时采取行动至关重要。此场景由离线特征存储支持。'
- en: Feature Store plays a key role in streaming scenarios by providing a reliable
    source of feature data. When new data streams in, Feature Store can supply the
    necessary features for predictions, ensuring consistent and up-to-date input for
    your models. This simplifies the streaming pipeline, as feature preparation is
    decoupled from the real-time inference process.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征存储在流式场景中发挥关键作用，提供可靠的特征数据源。当新数据流入时，特征存储可以提供预测所需的特征，确保为模型提供一致且最新的输入。这简化了流式管道，因为特征准备与实时推理过程解耦。
- en: '**Real-time inference**: Real-time inference takes streaming a step further
    by delivering instantaneous predictions as soon as new data arrives. This is essential
    in applications such as recommendation systems, where users expect immediate responses
    to their actions. This scenario requires an online feature store.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时推理**：实时推理将流式推理进一步发展，通过在新数据到达时立即提供预测。在推荐系统等应用中，这一点尤为重要，因为用户期望对其行为立即做出响应。此场景需要在线特征存储。'
- en: For real-time inference, Feature Store ensures that feature data is readily
    available for quick predictions. Feature Store’s integration into the real-time
    inference pipeline enables low-latency access to features, contributing to swift
    and accurate predictions. This is crucial in applications demanding rapid decision-making.
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于实时推理，特征存储确保特征数据随时可用，以便快速预测。特征存储与实时推理管道的集成使得可以低延迟访问特征，从而有助于快速和准确的预测。这对于需要迅速决策的应用至关重要。
- en: Each feature table has a primary key that uniquely defines a row of data. Databricks
    Feature Store allows defining composite keys as well.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特征表都有一个主键，唯一标识一行数据。Databricks特征存储也允许定义复合主键。
- en: New data can be written into the feature tables using regularly executed ETL
    pipelines in a batch fashion or a continuous style utilizing the Structured Streaming
    API ([https://docs.databricks.com/spark/latest/structured-streaming/index.html](https://docs.databricks.com/spark/latest/structured-streaming/index.html)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 新数据可以通过定期执行的ETL管道以批量方式或利用结构化流式API（[https://docs.databricks.com/spark/latest/structured-streaming/index.html](https://docs.databricks.com/spark/latest/structured-streaming/index.html)）以持续方式写入特征表中。
- en: 'The workflow to register a new feature table in Databricks is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在Databricks中注册新特征表的工作流如下：
- en: Create a database that will store our feature tables.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数据库，用于存储我们的特征表。
- en: Write feature-engineering logic as a function that returns an Apache Spark DataFrame
    ([https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html)).
    This DataFrame should also produce a unique primary key for each record in the
    DataFrame. The primary key can have more than one column as well.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征工程逻辑编写为一个函数，该函数返回一个Apache Spark数据框（[https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html)）。此数据框还应为数据框中的每条记录生成一个唯一的主键。主键可以包含多个列。
- en: Instantiate an object of `FeatureStoreClient` and use `create_table` (supported
    in DB ML Runtime 10.2 and above) to define a feature table in the feature store.
    At this point, there is no data stored in the feature table. If we initialize
    an additional `df` argument with the value of the feature-engineered DataFrame,
    we can skip *step 4*.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个 `FeatureStoreClient` 对象，并使用 `create_table`（在 DB ML Runtime 10.2 及以上版本支持）在特征存储中定义特征表。此时，特征表中没有存储数据。如果我们初始化一个额外的
    `df` 参数，并将其赋值为特征工程数据框，我们可以跳过 *步骤 4*。
- en: Use the `write_table` method to write the feature-engineered dataset into the
    defined feature table. The `write_table` method provides modes to either completely
    overwrite the existing feature table or update certain records based on the defined
    lookup key.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `write_table` 方法将特征工程数据集写入定义的特征表中。`write_table` 方法提供了两种模式：要么完全覆盖现有特征表，要么根据定义的查找键更新某些记录。
- en: The code example provided in this chapter will go through the aforementioned
    steps and make them clearer to understand. Before we dive deeper into the code,
    we need to understand some more concepts related to the feature store.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供的代码示例将通过上述步骤并使其更容易理解。在深入研究代码之前，我们需要了解一些与特征存储相关的概念。
- en: We will look more at reading from the feature table in the chapter on MLFlow.
    We will reuse the feature table we created in this chapter to predict bank customer
    churn.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在《MLFlow》一章中进一步探讨如何从特征表中读取数据。我们将重用本章中创建的特征表来预测银行客户流失。
- en: Offline store
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离线存储
- en: The Databricks offline feature store is backed by Delta tables and is utilized
    for model training, batch inferencing, and feature discovery.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 离线特征存储由 Delta 表支持，主要用于模型训练、批量推理和特征发现。
- en: 'Delta tables allow users to update feature values based on the primary key
    in this mode. Utilizing the Delta format also provides additional advantages in
    the context of ML:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Delta 表允许用户根据主键在此模式下更新特征值。利用 Delta 格式在 ML 的背景下也提供了额外的优势：
- en: '`timestampAsOf` to read certain historical data:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `timestampAsOf` 来读取某些历史数据：
- en: '[PRE0]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`history()` function in the Delta API ([https://docs.delta.io/latest/index.html](https://docs.delta.io/latest/index.html)):'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delta API 中的 `history()` 函数（[https://docs.delta.io/latest/index.html](https://docs.delta.io/latest/index.html)）：
- en: '[PRE1]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To load data from a particular version of the table, we can specify the `versionAsOf`
    option:'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要从特定版本的表中加载数据，我们可以指定 `versionAsOf` 选项：
- en: '[PRE2]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This way, you can now train models on different versions of your data and maintain
    the lineage.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你就可以在数据的不同版本上训练模型并保持数据血统。
- en: Online store
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线存储
- en: When using a feature store for real-time inference, the feature table needs
    to be stored in low-latency storage such as a relational database.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用特征存储进行实时推理时，特征表需要存储在低延迟存储中，如关系数据库。
- en: If you must have your features available both in online and offline feature
    stores, you can use your offline store as a streaming source to update your online
    store’s feature tables.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你必须在在线和离线特征存储中都能使用特征，你可以将离线存储作为流式数据源来更新在线存储的特征表。
- en: Training Set
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练集
- en: While training an ML model, you may want to combine data from multiple feature
    tables. Each feature table needs to have a unique ID(s) or primary key(s) that
    is used at the time of model training and inference to join and retrieve the relevant
    features from multiple feature tables to the **Training** **Set** construct.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 ML 模型时，你可能希望将来自多个特征表的数据结合起来。每个特征表都需要有唯一的 ID 或主键，这些 ID 或主键将在模型训练和推理时用于连接和提取来自多个特征表的相关特征到
    **训练集** 构建中。
- en: A training set makes use of an object called `FeatureLookup`, which takes as
    input the feature table name, feature names that we need to retrieve from the
    feature table, and a lookup key(s). The lookup key(s) are used to join the features
    from various feature tables if we define multiple `FeatureLookup` to generate
    a Training Set.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集使用一个名为`FeatureLookup`的对象，它的输入包括特征表名称、我们需要从特征表中检索的特征名称，以及查找键（s）。查找键（s）用于在我们定义多个`FeatureLookup`以生成训练集时，从不同的特征表中连接特征。
- en: In the notebook accompanying this chapter, we will go over example code that
    registers a fraud detection dataset as a feature table in Databricks Feature Store.
    In [*Chapter 5*](B17875_05.xhtml#_idTextAnchor085), *Create a Baseline Model Using
    Databricks AutoML*, in relation to the AutoML overview, we will take the feature
    table generated to build a churn prediction model and showcase various components
    of integrated MLFlow in the Databricks environment.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章附带的笔记本中，我们将通过示例代码展示如何将一个欺诈检测数据集注册为Databricks Feature Store中的特征表。在[*第5章*](B17875_05.xhtml#_idTextAnchor085)中，*使用Databricks
    AutoML创建基准模型*，我们将利用AutoML概述中的特征表，构建一个流失预测模型，并展示Databricks环境中集成的MLFlow的各种组件。
- en: Model packaging
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型打包
- en: The `FeatureStoreClient` API ([https://docs.databricks.com/en/dev-tools/api/python/latest/feature-store/client.html](https://docs.databricks.com/en/dev-tools/api/python/latest/feature-store/client.html))
    provides a method called `log_model` that allows ML models to retain the references
    to the features utilized to train the model. These features reside in Databricks
    Feature Store as feature tables. The ML model can retrieve the necessary features
    from the feature tables based on the primary key(s) provided at the time of inference.
    The feature values are retrieved in the batch and streaming inference mode from
    the offline store. The retrieved features are combined with any new feature provided
    during inference before making a prediction. In the real-time inference mode,
    feature values are retrieved from the online store.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`FeatureStoreClient` API（[https://docs.databricks.com/en/dev-tools/api/python/latest/feature-store/client.html](https://docs.databricks.com/en/dev-tools/api/python/latest/feature-store/client.html)）提供了一个名为`log_model`的方法，允许机器学习模型保留用于训练模型的特征的引用。这些特征存储在Databricks
    Feature Store中作为特征表。ML模型可以根据推理时提供的主键（s）从特征表中检索必要的特征。在批量和流式推理模式下，特征值从离线存储中检索。在实时推理模式下，特征值则从在线存储中检索。'
- en: Note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of writing this book, Databricks Feature Store only supports the
    Python language. You can use your favorite libraries, such as `sklearn` and pandas,
    to do feature engineering; however, before you write the table out as a feature
    table, it needs to be converted to a **PySpark** DataFrame. PySpark is a Python
    wrapper on top of the Spark distributed processing engine ([https://spark.apache.org/docs/latest/api/python/](https://spark.apache.org/docs/latest/api/python/)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写本书时，Databricks Feature Store仅支持Python语言。你可以使用你喜欢的库，如`sklearn`和pandas，进行特征工程；但是，在将表格写出为特征表之前，它需要转换为**PySpark**
    DataFrame。PySpark是Spark分布式处理引擎的Python封装（[https://spark.apache.org/docs/latest/api/python/](https://spark.apache.org/docs/latest/api/python/)）。
- en: Let’s dive into a hands-on example that will walk you through the process of
    registering your first feature table in Databricks Feature Store.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入一个动手实践的例子，带你一步步完成在Databricks Feature Store中注册第一个特征表的过程。
- en: The dataset we will work with comes from **Kaggle**, and we are going to register
    this dataset after doing some feature engineering in Databricks Feature Store.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集来自**Kaggle**，在进行一些特征工程后，我们将在Databricks Feature Store注册该数据集。
- en: Registering your first feature table in Databricks Feature Store
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Databricks Feature Store中注册你的第一个特征表
- en: Before we get started, the code needs to be downloaded from the Git repository
    accompanying this book ([https://github.com/debu-sinha/Practical_Data_Science_on_Databricks.git](https://github.com/debu-sinha/Practical_Data_Science_on_Databricks.git)).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，代码需要从与本书配套的Git仓库中下载（[https://github.com/debu-sinha/Practical_Data_Science_on_Databricks.git](https://github.com/debu-sinha/Practical_Data_Science_on_Databricks.git)）。
- en: We will use the Databricks repository feature to clone the GitHub repo.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Databricks的仓库功能来克隆GitHub仓库。
- en: 'To clone the code repository, complete the following steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要克隆代码仓库，完成以下步骤：
- en: 'Click on the **Repos** tab and select your username:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**Repos**标签并选择你的用户名：
- en: '![Figure 3.1 – A screenshot displaying the Repos tab](img/B17875_03_001.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – 显示 Repos 标签页的截图](img/B17875_03_001.jpg)'
- en: Figure 3.1 – A screenshot displaying the Repos tab
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 显示 Repos 标签页的截图
- en: Important note
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: In light of a recent user interface update, the 'Repos' section has been moved
    and can now be accessed by clicking on the 'Workspaces' icon, as illustrated in
    the following image.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最近的用户界面更新，'Repos' 部分已被移动，现在可以通过点击 'Workspaces' 图标访问，以下图所示。
- en: '![](img/B17875_03_000.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17875_03_000.png)'
- en: Despite this change, the workflow outlined in this chapter remains applicable.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管进行了此更改，本章中概述的工作流程依然适用。
- en: 'Right-click and add the repo:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 右键点击并添加仓库：
- en: '![Figure 3.2 – A screenshot displaying how to clone the code for this chapter
    (step 2)](img/B17875_03_002.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 显示如何克隆本章代码（步骤 2）的截图](img/B17875_03_002.jpg)'
- en: Figure 3.2 – A screenshot displaying how to clone the code for this chapter
    (step 2)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 显示如何克隆本章代码（步骤 2）的截图
- en: 'Paste the link into the **Git repo URL** field and click **Create**:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将链接粘贴到 **Git 仓库 URL** 字段中，然后点击 **创建**：
- en: '![Figure 3.3 – A screenshot displaying how to clone the code for this chapter
    (step 3)](img/B17875_03_003.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – 显示如何克隆本章代码（步骤 3）的截图](img/B17875_03_003.jpg)'
- en: Figure 3.3 – A screenshot displaying how to clone the code for this chapter
    (step 3)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 显示如何克隆本章代码（步骤 3）的截图
- en: 'In the cloned repository, click on `Chapter 03` and, within that, the `churn-analysis`
    notebook:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在克隆的仓库中，点击 `Chapter 03`，然后在其中选择 `churn-analysis` 笔记本：
- en: '![Figure 3.4 – A screenshot displaying how to clone the code for this chapter
    (step 4)](img/B17875_03_004.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 显示如何克隆本章代码（步骤 4）的截图](img/B17875_03_004.jpg)'
- en: Figure 3.4 – A screenshot displaying how to clone the code for this chapter
    (step 4)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 显示如何克隆本章代码（步骤 4）的截图
- en: When you first open the notebook, it will be in the detached stage. If you have
    not provisioned a cluster, please refer to the *Exploring clusters* section in
    [*Chapter 2*](B17875_02.xhtml#_idTextAnchor036).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次打开笔记本时，它将处于分离状态。如果你还没有配置集群，请参考 [*第 2 章*](B17875_02.xhtml#_idTextAnchor036)中的
    *探索集群* 部分。
- en: 'You can see the notebook in the following screenshot:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下截图中看到笔记本：
- en: '![Figure 3.5 – A screenshot displaying the initial state of the notebook](img/B17875_03_005.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 显示笔记本初始状态的截图](img/B17875_03_005.jpg)'
- en: Figure 3.5 – A screenshot displaying the initial state of the notebook
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 显示笔记本初始状态的截图
- en: 'After you have a cluster ready to use, you can attach it to the notebook:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置好集群后，你可以将其附加到笔记本：
- en: '![Figure 3.6 – A screenshot displaying the dropdown for attaching a notebook
    to a list of available clusters](img/B17875_03_006.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 显示将笔记本附加到可用集群列表的下拉菜单的截图](img/B17875_03_006.jpg)'
- en: Figure 3.6 – A screenshot displaying the dropdown for attaching a notebook to
    a list of available clusters
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 显示将笔记本附加到可用集群列表的下拉菜单的截图
- en: All the code has been tested on Databricks ML Runtime 10.4 LTS. I would recommend
    users have a cluster provisioned with ML Runtime 10.4 LTS or above.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码已在 Databricks ML Runtime 10.4 LTS 上进行测试。建议用户使用配置了 ML Runtime 10.4 LTS 或更高版本的集群。
- en: 'Select an option from the drop-down menu:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从下拉菜单中选择一个选项：
- en: '![Figure 3.7 – A screenshot displaying the notebook attached to a cluster in
    the ready state](img/B17875_03_007.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – 显示笔记本附加到集群并处于准备状态的截图](img/B17875_03_007.jpg)'
- en: Figure 3.7 – A screenshot displaying the notebook attached to a cluster in the
    ready state
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 显示笔记本附加到集群并处于准备状态的截图
- en: 'Now, you are ready to start executing the code in the notebook:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以开始执行笔记本中的代码了：
- en: 'The first few cells in the notebook describe the dataset we are working with
    and how to read it as a Spark DataFrame. We can also import the data as a pandas
    DataFrame. Databricks has a handy display function that visualizes the data loaded
    into a pandas or Spark DataFrame:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 笔记本中的前几个单元描述了我们正在使用的数据集以及如何将其读取为 Spark DataFrame。我们也可以将数据导入为 pandas DataFrame。Databricks
    提供了一个方便的显示功能，可以可视化加载到 pandas 或 Spark DataFrame 中的数据：
- en: '[PRE3]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The next block of code goes over the steps to create a feature table. The first
    step is to define a database that will store the feature tables that we define
    backed by the Delta tables. We also define a method for performing some basic
    feature engineering:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一段代码介绍了创建特征表的步骤。第一步是定义一个数据库，用来存储我们定义的、以 Delta 表为基础的特征表。我们还定义了一种方法来执行一些基本的特征工程：
- en: '[PRE4]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The following code block defines a basic function that performs feature engineering
    on the input Spark DataFrame.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的代码块定义了一个基本函数，用于对输入的 Spark DataFrame 执行特征工程。
- en: Before going forward, I would like to highlight a powerful library called `pyspark.pandas`.
    As an ML practitioner, you might be familiar with using the pandas ([https://pandas.pydata.org/docs/#](https://pandas.pydata.org/docs/#))
    library for manipulating data. It has one big drawback in that it’s not scalable.
    All the processing using the pandas API happens on a single machine and if your
    data cannot fit on a single machine, you will be stuck. This is where Apache Spark
    can help. Apache Spark is built to handle massive amounts of data as it chunks
    large amounts of data into individual units and distributes the processing on
    multiple nodes of a cluster. As the volume of data you want to process increases,
    you can simply add more nodes to the cluster, and if your data doesn’t have any
    skews, the performance of your data processing pipeline will remain the same.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在继续之前，我想强调一个强大的库称为 `pyspark.pandas`。作为机器学习实践者，你可能熟悉使用 pandas ([https://pandas.pydata.org/docs/#](https://pandas.pydata.org/docs/#))
    库来操作数据。它有一个很大的缺点，即它不具备可扩展性。使用 pandas API 进行的所有处理都在单台机器上进行，如果你的数据无法放入单台机器，你就会陷入困境。这就是
    Apache Spark 可以帮助的地方。Apache Spark 专为处理大量数据而构建，它将大量数据划分为单个单位，并在集群的多个节点上分发处理。随着要处理的数据量增加，你可以简单地向集群添加更多节点，如果你的数据没有任何倾斜，数据处理流水线的性能将保持不变。
- en: 'However, there is a big challenge: many ML practitioners are unfamiliar with
    the Spark libraries. This is the core reason for developing the `pyspark.pandas`
    ([https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html))
    library. This library aims to bridge the gap between the pandas library and Apache
    Spark.'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，有一个很大的挑战：许多机器学习从业者不熟悉 Spark 库。这正是开发 `pyspark.pandas` ([https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html))
    库的核心原因。该库旨在弥合 pandas 库和 Apache Spark 之间的差距。
- en: At the heart of the `pyspark.pandas` library lies its ability to offer a pandas-like
    API within the realm of Apache Spark. This innovation brings forth the best of
    both worlds, granting users access to a familiar DataFrame API while harnessing
    Spark’s scalability and performance prowess. For those well versed in pandas,
    the transition to `pyspark.pandas` is seamless, paving the way for streamlined
    adoption. However, it’s important to keep in mind that not all panda APIs are
    implemented in the `pyspark.pandas` API, leading to compatibility issues.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`pyspark.pandas` 库的核心在于它能够在 Apache Spark 的领域内提供类似 pandas 的 API。这一创新将两个世界的优势结合在一起，使用户在使用熟悉的
    DataFrame API 的同时，利用 Spark 的可扩展性和性能优势。对于熟悉 pandas 的用户来说，转向 `pyspark.pandas` 是无缝的，为简化采纳过程铺平了道路。然而，重要的是要记住，并非所有
    pandas API 都在 `pyspark.pandas` API 中实现，可能会导致兼容性问题。'
- en: If you really have a need to use certain pandas API functionality that is not
    yet available in `pyspark.pandas`, you can use a method called `toPandas()` ([https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html?highlight=topandas](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html?highlight=topandas)).
    As a best practice, try to use the PySpark/`pyspark.pandas` API before going for
    the pandas API. You can read more about best practices at [https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html?highlight=pandas#use-pandas-api-on-spark-directly-whenever-possible](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html?highlight=pandas#use-pandas-api-on-spark-directly-whenever-possible).
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你真的需要使用某些 pandas API 功能，而 `pyspark.pandas` 尚未提供，你可以使用一个名为 `toPandas()` 的方法（[https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html?highlight=topandas](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html?highlight=topandas)）。作为最佳实践，在使用
    pandas API 之前，尽量使用 PySpark/`pyspark.pandas` API。有关最佳实践的更多信息，请参阅 [https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html?highlight=pandas#use-pandas-api-on-spark-directly-whenever-possible](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html?highlight=pandas#use-pandas-api-on-spark-directly-whenever-possible)。
- en: '[PRE5]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The next code block performs feature engineering, utilizing the function defined
    in the previous section, and displays it:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个代码块执行特征工程，利用前一节定义的函数，并显示结果：
- en: '[PRE6]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we will initialize `FeatureStoreClient`, register the table, and define
    our feature table structure using the `create_table` function:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将初始化`FeatureStoreClient`，注册表格，并使用`create_table`函数定义特征表的结构：
- en: '[PRE7]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once we have defined the structure of our feature table, we can populate data
    in it using the `write_table` function:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们定义了特征表的结构，就可以使用`write_table`函数向表中填充数据：
- en: '[PRE8]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Instead of overwriting, you can choose `merge` as an option if you want to update
    only certain records.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你只想更新某些记录，可以选择`merge`作为选项，而不是覆盖原有数据。
- en: In practice, you can populate the feature table when you call the `create_table`
    method itself by passing in the source Spark DataFrame as the `feature_df` parameter.
    This approach can be useful when you have a DataFrame ready to initialize the
    feature table.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实际上，当你调用`create_table`方法并传入源 Spark DataFrame 作为`feature_df`参数时，可以填充特征表。当你已经有一个准备好的
    DataFrame 用来初始化特征表时，这种方法非常有用。
- en: 'Now we can explore our feature table in the integrated feature store UI. We
    can see who created the feature table and the data sources that populated the
    feature table. The feature store UI has a lot of important information about our
    feature table:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以在集成的特征存储 UI 中查看我们的特征表。我们可以看到谁创建了特征表，以及哪些数据源填充了特征表。特征存储 UI 提供了很多关于特征表的重要信息：
- en: '![Figure 3.8 – A screenshot displaying details of the feature store](img/B17875_03_008.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.8 – 显示特征存储的截图](img/B17875_03_008.jpg)'
- en: Figure 3.8 – A screenshot displaying details of the feature store
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – 显示特征存储的截图
- en: 'We can examine when a feature table was created, and by whom, through Databricks’
    user interface. This information is particularly valuable for tracking data provenance
    and understanding data lineage within the organization. Additionally, the UI displays
    other pertinent information such as the last time the table was refreshed, giving
    insights into how up to date the table’s data is:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过 Databricks 的用户界面检查特征表的创建时间以及创建者。此信息对于跟踪数据来源和了解组织内的数据流向非常有价值。此外，UI 还显示其他相关信息，例如表格最后一次刷新时间，帮助了解表格数据的最新状态：
- en: '![Figure 3.9 – A screenshot displaying details about the owner, primary key,
    creation, and last update date of the feature table](img/B17875_03_009.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.9 – 显示特征表的所有者、主键、创建日期和最后更新时间的截图](img/B17875_03_009.jpg)'
- en: Figure 3.9 – A screenshot displaying details about the owner, primary key, creation,
    and last update date of the feature table
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 显示特征表的所有者、主键、创建日期和最后更新时间的截图
- en: Furthermore, the UI provides details about the table’s partitions and primary
    keys. Partitions are crucial for query optimization, as they enable more efficient
    data retrieval by segregating the table into different subsets based on specific
    column values. Primary keys, on the other hand, serve as unique identifiers for
    each row in the table, ensuring data integrity and facilitating quick lookups.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，UI 还提供了有关表格分区和主键的详细信息。分区对于查询优化至关重要，它们通过根据特定列值将表格划分为不同的子集，从而实现更高效的数据检索。而主键则充当表中每一行的唯一标识符，确保数据完整性，并便于快速查找。
- en: 'In the production environment, if our feature table is populated regularly
    through a notebook, we can also visualize historical updates:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生产环境中，如果我们的特征表通过笔记本定期填充，我们还可以查看历史更新：
- en: '![Figure 3.10 – A screenshot displaying details about the source of the feature
    table](img/B17875_03_010.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10 – 显示特征表来源的截图](img/B17875_03_010.jpg)'
- en: Figure 3.10 – A screenshot displaying details about the source of the feature
    table
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 显示特征表来源的截图
- en: 'Lastly, we can also view the data types of every feature in the feature table:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们还可以查看特征表中每个特征的数据类型：
- en: '![Figure 3.11 – A screenshot displaying details about the various columns of
    the feature table and the data types](img/B17875_03_011.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.11 – 显示特征表各列及其数据类型的截图](img/B17875_03_011.jpg)'
- en: Figure 3.11 – A screenshot displaying details about the various columns of the
    feature table and the data types
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 – 显示特征表各列及其数据类型的截图
- en: The notebook is heavily documented and will walk you through all the steps required
    to get raw data, from importing to Databricks to writing out your first feature
    table.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本有详细的文档，指导你完成从导入到 Databricks 到编写第一个特征表所需的所有步骤。
- en: To understand the current limitations and other administrative options available
    with Databricks Feature Store, refer to [https://docs.databricks.com/en/machine-learning/feature-store/troubleshooting-and-limitations.html](https://docs.databricks.com/en/machine-learning/feature-store/troubleshooting-and-limitations.html).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解 Databricks Feature Store 的当前限制和其他可用的管理选项，请参考 [https://docs.databricks.com/en/machine-learning/feature-store/troubleshooting-and-limitations.html](https://docs.databricks.com/en/machine-learning/feature-store/troubleshooting-and-limitations.html)。
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we got a deeper understanding of feature stores, the problems
    they solve, and a detailed look into the feature store implementation within the
    Databricks environment. We also went through an exercise to register our first
    feature table. This will enable us to utilize the feature table to create our
    first ML model as we discussed in the MLFlow chapter.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们更深入地了解了特征库、它们解决的问题，以及在 Databricks 环境中实现特征库的详细过程。我们还进行了一个练习，注册了我们的第一个特征表。这将使我们能够利用特征表创建我们的第一个
    ML 模型，正如我们在 MLFlow 章节中所讨论的。
- en: Next, we will cover MLFlow in detail.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细讲解 MLFlow。
- en: Further reading
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Databricks, *Repos for Git* *Integration*: [https://docs.databricks.com/repos.html#repos-for-git-integration](https://docs.databricks.com/repos.html#repos-for-git-integration)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks, *Git 集成的 Repos*：[https://docs.databricks.com/repos.html#repos-for-git-integration](https://docs.databricks.com/repos.html#repos-for-git-integration)
- en: 'You can read more about the supported RDBMS here: [https://docs.databricks.com/applications/machine-learning/feature-store/concepts.html#online-store](https://docs.databricks.com/applications/machine-learning/feature-store/concepts.html#online-store)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在这里了解更多关于支持的 RDBMS 信息：[https://docs.databricks.com/applications/machine-learning/feature-store/concepts.html#online-store](https://docs.databricks.com/applications/machine-learning/feature-store/concepts.html#online-store)
- en: 'You can read more on how the feature tables are joined together with the training
    DataFrame here: [https://docs.databricks.com/applications/machine-learning/feature-store/feature-tables.html#create-a-trainingset-when-lookup-keys-do-not-match-the-primary-keys](https://docs.databricks.com/applications/machine-learning/feature-store/feature-tables.html#create-a-trainingset-when-lookup-keys-do-not-match-the-primary-keys)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在这里了解更多关于如何将特征表与训练 DataFrame 合并的信息：[https://docs.databricks.com/applications/machine-learning/feature-store/feature-tables.html#create-a-trainingset-when-lookup-keys-do-not-match-the-primary-keys](https://docs.databricks.com/applications/machine-learning/feature-store/feature-tables.html#create-a-trainingset-when-lookup-keys-do-not-match-the-primary-keys)
- en: 'Apache Spark, *Apache Arrow in* *PySpark*: [https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html](https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Apache Spark, *Apache Arrow 在* *PySpark 中的应用*: [https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html](https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html)'
- en: 'Databricks, *Convert PySpark DataFrames to and from pandas* *DataFrames*: (https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html#convert-pyspark-dataframes-to-and-from-pandas-dataframes)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks, *将 PySpark DataFrames 转换为 pandas* *DataFrames*：([https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html#convert-pyspark-dataframes-to-and-from-pandas-dataframes](https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html#convert-pyspark-dataframes-to-and-from-pandas-dataframes))
