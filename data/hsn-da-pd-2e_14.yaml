- en: '*Chapter 10*: Making Better Predictions – Optimizing Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*：做出更好的预测——优化模型'
- en: In the previous chapter, we learned how to build and evaluate our machine learning
    models. However, we didn't touch upon what we can do if we want to improve their
    performance. Of course, we could try out a different model and see if it performs
    better—unless there are requirements that we use a specific method for legal reasons
    or in order to be able to explain how it works. We want to make sure we use the
    best version of the model that we can, and for that, we need to discuss how to
    tune our models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们学习了如何构建和评估机器学习模型。然而，我们没有涉及如果我们想要提高模型性能时可以做些什么。当然，我们可以尝试使用不同的模型，看看它是否表现更好——除非有法律要求或需要能够解释其工作原理的要求，必须使用特定的方法。我们希望确保使用我们能做到的最佳版本的模型，为此，我们需要讨论如何调整我们的模型。
- en: This chapter will introduce techniques for the optimization of machine learning
    model performance using `scikit-learn`, as a continuation of the content in [*Chapter
    9*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188), *Getting Started with Machine
    Learning in Python*. Nonetheless, it should be noted that there is no panacea.
    It is entirely possible to try everything we can think of and still have a model
    with little predictive value; such is the nature of modeling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍使用`scikit-learn`优化机器学习模型性能的技术，作为[*第9章*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188)《Python中的机器学习入门》的延续。然而，需要注意的是，优化并非万能药。我们完全有可能尝试所有能想到的方法，仍然得到一个预测能力较差的模型；这正是建模的特点。
- en: Don't be discouraged though—if the model doesn't work, consider whether the
    data collected suffices to answer the question, and whether the algorithm chosen
    is appropriate for the task at hand. Often, subject matter expertise will prove
    crucial when building machine learning models, because it helps us determine which
    data points will be relevant, as well as to take advantage of known interactions
    between the variables collected.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 不过不要灰心——如果模型不起作用，考虑一下收集到的数据是否足够回答问题，以及所选择的算法是否适合当前任务。通常，学科领域的专业知识在构建机器学习模型时至关重要，因为它帮助我们确定哪些数据点是相关的，并利用已知的变量之间的相互作用。
- en: 'In particular, the following topics will be covered:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，将涵盖以下主题：
- en: Hyperparameter tuning with grid search
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网格搜索进行超参数调整
- en: Feature engineering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: Building ensemble models combining many estimators
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建集成模型，结合多个估计器
- en: Inspecting classification prediction confidence
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查分类预测置信度
- en: Addressing class imbalance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决类别不平衡问题
- en: Penalizing high regression coefficients with regularization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则化惩罚高回归系数
- en: Chapter materials
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章资料
- en: In this chapter, we will be working with three datasets. The first two come
    from data on wine quality donated to the UCI Machine Learning Data Repository
    ([http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php))
    by P. Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis, and contain information
    on the chemical properties of various wine samples along with a rating of the
    quality from a blind tasting session by a panel of wine experts. These files can
    be found in the `data/` folder inside this chapter's folder in the GitHub repository
    ([https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_10](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_10))
    as `winequality-red.csv` and `winequality-white.csv` for red and white wine, respectively.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用三个数据集。前两个数据集来自P. Cortez、A. Cerdeira、F. Almeida、T. Matos和J. Reis捐赠给UCI机器学习数据仓库的关于葡萄酒质量的数据（[http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)）。数据包含了各种葡萄酒样本的化学属性信息，以及来自一组葡萄酒专家盲品评审会对其质量的评分。这些文件可以在本章的GitHub仓库中的`data/`文件夹下找到，文件名为`winequality-red.csv`和`winequality-white.csv`，分别对应红葡萄酒和白葡萄酒。
- en: Our third dataset was collected using the Open Exoplanet Catalogue database,
    at [https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/](https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/),
    which provides data in XML format. The parsed planet data can be found in the
    `data/planets.csv` file. For the exercises, we will also be working with the star
    temperature data from [*Chapter 9*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188),
    *Getting Started with Machine Learning in Python*, which can be found in the `data/stars.csv`
    file.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第三个数据集是使用开放系外行星目录数据库收集的，网址是：[https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/](https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/)，该数据库提供XML格式的数据。解析后的行星数据可以在`data/planets.csv`文件中找到。对于练习，我们还将使用来自[*第9章*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188)的恒星温度数据，*Python中的机器学习入门*，可以在`data/stars.csv`文件中找到。
- en: 'For reference, the following data sources were used:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，以下数据源被使用：
- en: '*Open Exoplanet Catalogue database*, available at [https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure](https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*开放系外行星目录数据库*，可在[https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure](https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure)找到。'
- en: '*P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences
    by data mining from physicochemical properties. In Decision Support Systems, Elsevier,
    47(4):547-553, 2009.* Available online at [http://archive.ics.uci.edu/ml/datasets/Wine+Quality](http://archive.ics.uci.edu/ml/datasets/Wine+Quality).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P. Cortez, A. Cerdeira, F. Almeida, T. Matos 和 J. Reis. 通过物理化学性质的数据挖掘建模葡萄酒偏好。在《决策支持系统》，Elsevier，47(4):547-553，2009年。*
    可在线获取：[http://archive.ics.uci.edu/ml/datasets/Wine+Quality](http://archive.ics.uci.edu/ml/datasets/Wine+Quality)。'
- en: '*Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository (*[http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)*).
    Irvine, CA: University of California, School of Information and Computer Science.*'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dua, D. 和 Karra Taniskidou, E. (2017). UCI机器学习库 (*[http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)*)。加利福尼亚州尔湾市：加利福尼亚大学信息与计算机科学学院。*'
- en: We will be using the `red_wine.ipynb` notebook to predict red wine quality,
    `wine.ipynb` to distinguish between red and white wine based on their chemical
    properties, and the `planets_ml.ipynb` notebook to build a regression model to
    predict the year length of planets in Earth days.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`red_wine.ipynb`笔记本来预测红葡萄酒的质量，使用`wine.ipynb`根据葡萄酒的化学性质来区分红白葡萄酒，使用`planets_ml.ipynb`笔记本构建回归模型来预测行星的年份长度（以地球日为单位）。
- en: 'Before we get started, let''s handle our imports and read in our data:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，让我们处理导入并读取数据：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s also create our training and testing sets for the red wine quality,
    wine type by chemical properties, and planets models:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也为红葡萄酒质量、葡萄酒类型通过化学性质以及行星模型创建训练和测试集：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Important note
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Remember that we will be working in dedicated notebooks for each of the datasets,
    so while the setup code is all in the same code block to make it easier to follow
    in the book, make sure to work in the notebook corresponding to the data in question.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们将在每个数据集对应的专用笔记本中工作，因此，虽然设置代码都在同一个代码块中，以便在书中更容易跟随，但请确保在与相应数据相关的笔记本中工作。
- en: Hyperparameter tuning with grid search
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用网格搜索进行超参数调优
- en: No doubt you have noticed that we can provide various parameters to the model
    classes when we instantiate them. These model parameters are not derived from
    the data itself and are referred to as **hyperparameters**. Some examples of these
    are regularization terms, which we will discuss later in this chapter, and weights.
    Through the process of **model tuning**, we seek to optimize our model's performance
    by tuning these hyperparameters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 毋庸置疑，您已经注意到我们在实例化模型类时可以提供各种参数。这些模型参数不是由数据本身派生出来的，而是被称为**超参数**。其中一些示例是正则化项，我们将在本章后面讨论，另一些是权重。通过**模型调优**的过程，我们希望通过调整这些超参数来优化模型的性能。
- en: How can we know we are picking the best values to optimize our model's performance?
    One way is to use a technique called **grid search** to tune these hyperparameters.
    Grid search allows us to define a search space and test all combinations of hyperparameters
    in that space, keeping the ones that result in the best model. The scoring criterion
    we define will determine the best model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何知道自己选择的是优化模型性能的最佳值呢？一种方法是使用一种称为 **网格搜索** 的技术来调优这些超参数。网格搜索允许我们定义一个搜索空间，并测试该空间中所有超参数的组合，保留那些导致最佳模型的组合。我们定义的评分标准将决定最佳模型。
- en: 'Remember the elbow point method we discussed in *Chapter 9*, *Getting Started
    with Machine Learning in Python*, for finding a good value for *k* in k-means
    clustering? We can employ a similar visual method to find the best value for our
    hyperparameters. This will involve splitting our training data into `train_test_split()`.
    Here, we will use the red wine quality dataset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得我们在 *第 9 章* 中讨论的肘部法则吗？用于寻找 k-means 聚类中 *k* 的一个合适值？我们可以使用类似的可视化方法来找到最佳的超参数值。这将涉及将训练数据拆分成
    `train_test_split()`。在这里，我们将使用红酒质量数据集：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we can build the model multiple times for all the values of the hyperparameters
    we want to test, and score them based on the metric that matters most to us. Let''s
    try to find a good value for `C`, the inverse of the regularization strength,
    which determines the weight of the penalty term for logistic regression and is
    discussed more in-depth in the *Regularization* section toward the end of this
    chapter; we tune this hyperparameter to reduce overfitting:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以对所有想要测试的超参数值多次构建模型，并根据我们最关心的指标对它们进行评分。让我们尝试找到 `C` 的一个合适值，`C` 是正则化强度的倒数，它决定了逻辑回归中惩罚项的权重，并将在本章末的
    *正则化* 部分进行更深入的讨论；我们调节这个超参数来减少过拟合：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Tip
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Here, we are using `np.logspace()` to get our range of values to try for `C`.
    To use this function, we supply starting and stopping exponents to use with a
    base number (10, by default). So `np.logspace(-1, 1, num=10)` gives us 10 evenly
    spaced numbers between 10-1 and 101.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `np.logspace()` 来获得我们要尝试的 `C` 的取值范围。要使用此函数，我们提供起始和停止的指数，并与基数（默认值为 10）一起使用。所以
    `np.logspace(-1, 1, num=10)` 会给我们 10 个均匀分布的数字，范围从 10^-1 到 10^1。
- en: 'This is then plotted as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，结果绘制如下：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Using the resulting plot, we can pick the value that maximizes our performance:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成的图表，我们可以选择最大化我们性能的值：
- en: '![Figure 10.1 – Searching for the best hyperparameters'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.1 – 寻找最佳超参数'
- en: '](img/Figure_10.1_B16834.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.1_B16834.jpg)'
- en: Figure 10.1 – Searching for the best hyperparameters
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 寻找最佳超参数
- en: Scikit-learn provides the `GridSearchCV` class in the `model_selection` module
    for carrying out this exhaustive search much more easily. Classes that end with
    *CV* utilize **cross-validation**, meaning they divide up the training data into
    subsets, some of which will be the validation set for scoring the model (without
    needing the testing data until after the model is fit).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 提供了 `GridSearchCV` 类，该类位于 `model_selection` 模块中，可以更轻松地执行这种全面的搜索。以
    *CV* 结尾的类利用 **交叉验证**，这意味着它们将训练数据划分为子集，其中一些子集将作为验证集来评分模型（在模型拟合之前无需使用测试数据）。
- en: 'One common method of cross-validation is **k-fold cross-validation**, which
    splits the training data into *k* subsets and will train the model *k* times,
    each time leaving one subset out to use as the validation set. The score for the
    model will be the average across the *k* validation sets. Our initial attempt
    was 1-fold cross-validation. When *k*=3, this process looks like the following
    diagram:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的交叉验证方法是 **k 折交叉验证**，它将训练数据划分为 *k* 个子集，并将模型训练 *k* 次，每次留下一个子集作为验证集。模型的得分将是
    *k* 个验证集的平均值。我们最初的尝试是 1 折交叉验证。当 *k*=3 时，这个过程如下图所示：
- en: '![Figure 10.2 – Understanding k-fold cross-validation'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.2 – 理解 k 折交叉验证'
- en: '](img/Figure_10.2_B16834.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.2_B16834.jpg)'
- en: Figure 10.2 – Understanding k-fold cross-validation
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 理解 k 折交叉验证
- en: Tip
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: When working with classification problems, `scikit-learn` will implement stratified
    k-fold cross-validation. This ensures that the percentage of samples belonging
    to each class will be preserved across folds. Without stratification, it's possible
    some validation sets will see a disproportionately low (or high) amount of a given
    class, which can distort the results.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理分类问题时，`scikit-learn`将实现分层k折交叉验证。这确保了每个类的样本百分比将在折叠之间得到保持。如果没有分层，可能会出现某些验证集看到某个类的样本数量不成比例（过低或过高），这会扭曲结果。
- en: '`GridSearchCV` uses cross-validation to find the best hyperparameters in the
    search space, without the need to use the testing data. Remember, test data should
    not influence the training process in any way—neither when training the model
    nor when tuning hyperparameters—otherwise, the model will have issues generalizing.
    This happens because we would be picking the hyperparameters that give the best
    performance on the test set, thus leaving no way to test on unseen data, and overestimating
    our performance.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV`使用交叉验证来找到搜索空间中的最佳超参数，无需使用测试数据。请记住，测试数据不应以任何方式影响训练过程——无论是在训练模型时，还是在调整超参数时——否则模型将无法很好地进行泛化。这是因为我们可能会选择在测试集上表现最好的超参数，从而无法在未见过的数据上进行测试，并高估我们的性能。'
- en: In order to use `GridSearchCV`, we need to provide a model (or pipeline) and
    a search space, which will be a dictionary mapping the hyperparameter to tune
    (by name) to a list of values to try. Optionally, we can provide a scoring metric
    to use, as well as the number of folds to use with cross-validation. We can tune
    any step in the pipeline by prefixing the hyperparameter name with the name of
    that step, followed by two underscores. For instance, if we have a logistic regression
    step called `lr` and want to tune `C`, we use `lr__C` as the key in the search
    space dictionary. Note that if our model has any preprocessing steps, it's imperative
    that we use a pipeline.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用`GridSearchCV`，我们需要提供一个模型（或管道）和一个搜索空间，搜索空间将是一个字典，将要调优的超参数（按名称）映射到要尝试的值的列表。可选地，我们还可以提供要使用的评分指标，以及要用于交叉验证的折叠数。我们可以通过在超参数名称前加上步骤名称并用两个下划线分隔来调优管道中的任何步骤。例如，如果我们有一个名为`lr`的逻辑回归步骤，并且想要调优`C`，我们在搜索空间字典中使用`lr__C`作为键。请注意，如果我们的模型有任何预处理步骤，必须使用管道。
- en: 'Let''s use `GridSearchCV` for the red wine quality logistic regression, searching
    for whether or not to fit our model with an intercept and the best value for the
    inverse of the regularization strength (`C`). We will use the F1 score macro average
    as the scoring metric. Note that, due to the consistency of the API, `GridSearchCV`
    can be used to score, fit, and predict with the same methods as the underlying
    models. By default, the grid search will run in series, but `GridSearchCV` is
    capable of performing multiple searches in parallel, greatly speeding up this
    process:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`GridSearchCV`进行红酒质量的逻辑回归搜索，看看是否需要为我们的模型添加截距项，并寻找正则化强度的逆（`C`）的最佳值。我们将使用F1得分宏平均作为评分指标。请注意，由于API的一致性，`GridSearchCV`可以像基础模型一样使用相同的方法进行评分、拟合和预测。默认情况下，网格搜索将按顺序进行，但`GridSearchCV`能够并行执行多个搜索，从而大大加速这一过程：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once the grid search completes, we can isolate the best hyperparameters from
    the search space with the `best_params_` attribute. Notice that this result is
    different from our 1-fold cross-validation attempt because each of the folds has
    been averaged together to find the best hyperparameters overall, not just for
    a single fold:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网格搜索完成，我们可以通过`best_params_`属性从搜索空间中提取最佳超参数。请注意，这一结果与我们1折交叉验证的尝试不同，因为每个折叠的结果已经被平均起来，以找到整体最佳的超参数，而不仅仅是针对单一折叠的超参数：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Tip
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: We can also retrieve the best version of the pipeline from the grid search with
    the `best_estimator_` attribute. If we want to see the score the best estimator
    (model) had, we can grab it from the `best_score_` attribute; note that this will
    be the score we specified with the `scoring` argument.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过`best_estimator_`属性从网格搜索中检索最佳版本的管道。如果我们想查看最佳估计器（模型）的得分，可以从`best_score_`属性中获取；请注意，这将是我们在`scoring`参数中指定的得分。
- en: 'Our F1 score macro average is now higher than what we achieved in *Chapter
    9*, *Getting Started with Machine Learning in Python*:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的F1得分宏平均值现在已经高于我们在*第9章*，*用Python进行机器学习入门*中取得的成绩：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that the `cv` argument doesn't have to be an integer—we can provide one
    of the splitter classes mentioned at [https://scikit-learn.org/stable/modules/classes.html#splitter-classes](https://scikit-learn.org/stable/modules/classes.html#splitter-classes)
    if we want to use a method other than the default of k-fold for regression or
    stratified k-fold for classification. For example, when working with time series,
    we can use `TimeSeriesSplit` as the cross-validation object to work with successive
    samples and avoid shuffling. Scikit-learn shows how the cross-validation classes
    compare at [https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`cv`参数不一定非得是整数——如果我们希望使用除默认的k折回归交叉验证或分类分层k折交叉验证之外的方法，我们可以提供在[https://scikit-learn.org/stable/modules/classes.html#splitter-classes](https://scikit-learn.org/stable/modules/classes.html#splitter-classes)中提到的分割器类。例如，在处理时间序列时，我们可以使用`TimeSeriesSplit`作为交叉验证对象，以处理连续样本并避免打乱顺序。Scikit-learn在[https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html)展示了交叉验证类的比较。
- en: 'Let''s test out `RepeatedStratifiedKFold` on the red wine quality model instead
    of the default `StratifiedKFold`, which will repeat the stratified k-fold cross-validation
    10 times by default. All we have to do is change what we passed in as `cv` in
    the first `GridSearchCV` example to be a `RepeatedStratifiedKFold` object. Note
    that—despite using the same pipeline, search space, and scoring metric—we have
    different values for `best_params_` because our cross-validation process has changed:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在红酒质量模型上测试`RepeatedStratifiedKFold`，而不是默认的`StratifiedKFold`，它会默认重复进行10次分层k折交叉验证。我们要做的就是将第一个`GridSearchCV`示例中传递的`cv`参数改为`RepeatedStratifiedKFold`对象。请注意——尽管使用的是相同的管道、搜索空间和评分指标——由于交叉验证过程的变化，我们的`best_params_`值不同：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In addition to cross-validation, `GridSearchCV` allows us to specify the metric
    we want to optimize with the `scoring` parameter. This can be a string for the
    name of the score (as in the previous code blocks), provided that it is in the
    list at [https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values);
    otherwise, we can either pass the function itself or make our own using the `make_scorer()`
    function from `sklearn.metrics`. We can even provide a dictionary of scorers (in
    the form of `{name: function}`) for grid search, provided that we specify which
    one we want to use for optimization by passing its name to the `refit` parameter.
    Therefore, we can use grid search to find the hyperparameters that help us maximize
    our performance on the metrics we discussed in the previous chapter.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '除了交叉验证，`GridSearchCV`还允许我们通过`scoring`参数指定我们希望优化的指标。这可以是一个字符串，表示评分的名称（如前面代码块中的做法），前提是它在[https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)的列表中；否则，我们可以直接传递函数本身，或者使用`sklearn.metrics`中的`make_scorer()`函数自定义评分函数。我们甚至可以为网格搜索提供一个评分函数字典（格式为`{name:
    function}`），只要我们通过`refit`参数指定我们希望用来优化的评分函数的名称。因此，我们可以使用网格搜索来找到能够最大化我们在上一章讨论的指标上的性能的超参数。'
- en: Important note
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The time it takes to train our model should also be something we evaluate and
    look to optimize. If it takes us double the training time to get one more correct
    classification, it's probably not worth it. If we have a `GridSearchCV` object
    called `grid`, we can see the average fit time by running `grid.cv_results_['mean_fit_time']`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型所需的时间也是我们需要评估并优化的因素。如果我们需要两倍的训练时间才能获得一个额外的正确分类，可能就不值得这么做。如果我们有一个叫做`grid`的`GridSearchCV`对象，我们可以通过运行`grid.cv_results_['mean_fit_time']`来查看平均拟合时间。
- en: 'We can use `GridSearchCV` to search for the best parameters for any step in
    our pipeline. For example, let''s use grid search with a pipeline of preprocessing
    and linear regression on the planets data (similar to when we modeled planet year
    length in *Chapter 9*, *Getting Started with Machine Learning in Python*) while
    minimizing **mean absolute error** (**MAE**) instead of the default R2:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`GridSearchCV`在管道中的任何步骤上搜索最佳参数。例如，我们可以在行星数据集上使用预处理和线性回归的管道进行网格搜索（类似于我们在*第9章*，*《Python机器学习入门》*中建立行星年长度模型时的做法），同时最小化**平均绝对误差**（**MAE**），而不是默认的R2：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Note that we are using the negative of all the metrics except R2\. This is
    because `GridSearchCV` will attempt to maximize the score, and we want to minimize
    our errors. Let''s check the best parameters for the scaling and linear regression
    in this grid:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在使用除R2外所有指标的负值。这是因为`GridSearchCV`将尝试最大化得分，而我们希望最小化错误。让我们检查此网格中缩放和线性回归的最佳参数：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The tuned model''s MAE is more than 120 Earth days smaller than the MAE we
    got in *Chapter 9*, *Getting Started with Machine Learning in Python*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后模型的MAE比《Python机器学习入门》第9章中的MAE减少了120个地球日：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It's important to note that while a model may be fast to train, we shouldn't
    create a large, granular search space; in practice, it's better to start with
    a few different spread-out values, and then examine the results to see which areas
    warrant a more in-depth search. For instance, say we are looking to tune the `C`
    hyperparameter. On our first pass, we may look at the result of `np.logspace(-1,
    1)`. If we see that the best value for `C` is at either end of the spectrum, we
    can then look at values above/below the value. If the best value is in the range,
    we may look at a few values around it. This process can be performed iteratively
    until we don't see additional improvement. Alternatively, we could use `RandomizedSearchCV`,
    which will try 10 random combinations in the search space (by default) and find
    the best estimator (model). We can change this number with the `n_iter` argument.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，虽然模型可能训练速度很快，但我们不应创建大而精细的搜索空间；在实践中，最好从几个不同的分布值开始，然后检查结果，看哪些区域值得进行更深入的搜索。例如，假设我们要调整`C`超参数。在第一次尝试中，我们可以查看`np.logspace(-1,
    1)`的结果。如果我们发现`C`的最佳值在极端值之一，我们可以再查看该值以上/以下的值。如果最佳值在范围内，我们可以查看该值周围的几个值。可以迭代执行此过程，直到不再看到额外的改进。或者，我们可以使用`RandomizedSearchCV`，它将在搜索空间中尝试10个随机组合（默认情况下），并找到最佳估算器（模型）。我们可以使用`n_iter`参数更改此数字。
- en: Important note
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Since the process of tuning hyperparameters requires us to train our model multiple
    times, we must consider the time complexity of our models. Models that take a
    long time to train will be very costly to use with cross-validation. This will
    likely cause us to shrink our search space.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于调整超参数的过程要求我们多次训练模型，我们必须考虑模型的时间复杂度。训练时间长的模型在使用交叉验证时将非常昂贵。这可能会导致我们缩小搜索空间。
- en: Feature engineering
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: When trying to improve performance, we may also consider ways to provide the
    best **features** (model inputs) to our model through the process of **feature
    engineering**. The *Preprocessing data* section in *Chapter 9*, *Getting Started
    with Machine Learning in Python*, introduced us to **feature transformation**
    when we scaled, encoded, and imputed our data. Unfortunately, feature transformation
    may mute some elements of our data that we want to use in our model, such as the
    unscaled value of the mean of a specific feature. For this situation, we can create
    a new feature with this value; this and other new features are added during **feature
    construction** (sometimes called **feature creation**).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们试图提高性能时，我们也可以考虑通过**特征工程**的过程向模型提供最佳**特征**（模型输入）。在《Python机器学习入门》第9章的*数据预处理*部分，当我们对数据进行缩放、编码和填充时，介绍了**特征转换**。不幸的是，特征转换可能会使我们想在模型中使用的数据元素变得不那么显著，比如特定特征的未缩放平均值。针对这种情况，我们可以创建一个具有此值的新特征；这些以及其他新特征是在**特征构建**（有时称为**特征创建**）过程中添加的。
- en: '**Feature selection** is the process of determining which features to train
    the model on. This can be done manually or through another process, such as machine
    learning. When looking to choose features for our model, we want features that
    have an impact on our dependent variable without unnecessarily increasing the
    complexity of our problem. Models built with many features increase in complexity,
    but also, unfortunately, have a higher tendency to fit noise, because our data
    is sparse in such a high-dimensional space. This is referred to as the **curse
    of dimensionality**. When a model has learned the noise in the training data,
    it will have a hard time generalizing to unseen data; this is called **overfitting**.
    By restricting the number of features the model uses, feature selection can help
    address overfitting.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征选择**是确定哪些特征用于训练模型的过程。这可以通过手动操作或通过其他过程，如机器学习来完成。当我们选择模型的特征时，我们希望选择对因变量有影响的特征，同时避免不必要地增加问题的复杂性。使用许多特征构建的模型增加了复杂性，但不幸的是，这些模型更容易拟合噪声，因为我们的数据在高维空间中是稀疏的。这被称为**维度灾难**。当模型学到了训练数据中的噪声时，它会很难对未见过的数据进行泛化；这称为**过拟合**。通过限制模型使用的特征数量，特征选择有助于解决过拟合问题。'
- en: '**Feature extraction** is another way we can address the curse of dimensionality.
    During feature extraction, we reduce the dimensionality of our data by constructing
    combinations of features through a transformation. These new features can be used
    in place of the originals, thereby reducing the dimensionality of the problem.
    This process, called **dimensionality reduction**, also includes techniques where
    we find a certain number of components (less than the original) that explain most
    of the variance in the data. Feature extraction is often used in image recognition
    problems, since the dimensionality of the task is the total number of pixels in
    the image. For instance, square ads on websites are 350x350 pixels (this is one
    of the most common sizes), so an image recognition task using images that size
    has 122,500 dimensions.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征提取**是我们可以解决维度灾难的另一种方式。在特征提取过程中，我们通过变换构造特征的组合，从而减少数据的维度。这些新特征可以替代原始特征，从而减少问题的维度。这个过程称为**降维**，它还包括一些技术，利用少量的成分（比原始特征少）来解释数据中大部分的方差。特征提取通常用于图像识别问题，因为任务的维度是图像中的像素总数。例如，网站上的方形广告是350x350像素（这是最常见的尺寸之一），因此，使用该尺寸图像进行的图像识别任务具有122,500个维度。'
- en: Tip
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Thorough EDA and domain knowledge are a must for feature engineering.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 彻底的探索性数据分析（EDA）和领域知识是特征工程的必备条件。
- en: Feature engineering is the subject of entire books; however, as it is a more
    advanced topic, we will go over just a few techniques in this section. There is
    a good book on the subject in the *Further reading* section, which also touches
    upon using machine learning for feature learning.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是整本书的主题；然而，由于它是一个更为高级的话题，我们在本节中仅介绍几种技巧。在*进一步阅读*部分有一本关于该主题的好书，它还涉及使用机器学习进行特征学习。
- en: Interaction terms and polynomial features
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交互项和多项式特性
- en: We discussed the use of dummy variables back in the *Preprocessing data* section
    of *Chapter 9*, *Getting Started with Machine Learning in Python*; however, we
    merely considered the effect of that variable on its own. In our model that tries
    to predict red wine quality using chemical properties, we are considering each
    property separately. However, it is important to consider whether the interaction
    between these properties has an effect. Perhaps when the levels of citric acid
    and fixed acidity are both high or both low, the wine quality is different than
    if one is high and one is low. In order to capture the effect of this, we need
    to add an **interaction term**, which will be the product of the features.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第9章*的*数据预处理*部分讨论了虚拟变量的使用；然而，我们仅仅考虑了该变量单独的影响。在我们尝试使用化学特性预测红葡萄酒质量的模型中，我们分别考虑了每个特性。然而，考虑这些特性之间的相互作用是否会产生影响也很重要。也许当柠檬酸和固定酸度都较高或都较低时，葡萄酒的质量与其中一个较高、另一个较低时会有所不同。为了捕捉这种影响，我们需要添加一个**交互项**，即特性之间的乘积。
- en: We may also be interested in increasing the effect of a feature in the model
    through feature construction; we can achieve this by adding **polynomial features**
    made from this feature. This involves adding higher degrees of the original feature,
    so we could have *citric acid*, *citric acid*2, *citric acid*3, and so on in the
    model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可能希望通过特征构造来增加某个特征在模型中的作用；我们可以通过添加**多项式特征**来实现这一点，这些特征是由该特征构造的。这涉及到添加原始特征的更高阶次，所以我们可以在模型中拥有*柠檬酸*、*柠檬酸*²、*柠檬酸*³，依此类推。
- en: Tip
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: We can generalize linear models by using interaction terms and polynomial features
    because they allow us to model the linear relationship of non-linear terms. Since
    linear models tend to underperform in the presence of multiple or non-linear decision
    boundaries (the surface or hypersurface that separates the classes), this can
    improve performance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用交互项和多项式特征来推广线性模型，因为它们允许我们对非线性项的线性关系建模。由于线性模型在存在多个或非线性决策边界（分隔类别的表面或超表面）时往往表现不佳，这可以提高模型的性能。
- en: Scikit-learn provides the `PolynomialFeatures` class in the `preprocessing`
    module for easily creating interaction terms and polynomial features. This comes
    in handy when building models with categorical and continuous features. By specifying
    just the degree, we can get every combination of the features less than or equal
    to the degree. High degrees will increase model complexity greatly and may lead
    to overfitting.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn提供了`preprocessing`模块中的`PolynomialFeatures`类，用于轻松创建交互项和多项式特征。在构建包含类别特征和连续特征的模型时，这非常有用。只需指定度数，我们就可以得到所有小于或等于该度数的特征组合。较高的度数会大幅增加模型的复杂度，并可能导致过拟合。
- en: 'If we use `degree=2`, we can turn *citric acid* and *fixed acidity* into the
    following, where *1* is the bias term that can be used in a model as an intercept
    term:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`degree=2`，我们可以将*柠檬酸*和*固定酸度*转换为以下内容，其中*1*是可以作为模型截距项使用的偏置项：
- en: '![](img/Formula_10_001.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_10_001.jpg)'
- en: 'By calling the `fit_transform()` method on the `PolynomialFeatures` object,
    we can generate these features:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`PolynomialFeatures`对象上调用`fit_transform()`方法，我们可以生成这些特征：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s dissect the first row of our array in the previous code block (highlighted
    in bold) to understand how we got each of these values:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析前面代码块中数组的第一行（加粗部分），以了解我们是如何得到这些值的：
- en: '![Figure 10.3 – Examining the interaction terms and polynomial features created'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.3 – 检查所创建的交互项和多项式特征'
- en: '](img/Figure_10.3_B16834.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.3_B16834.jpg)'
- en: Figure 10.3 – Examining the interaction terms and polynomial features created
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 检查所创建的交互项和多项式特征
- en: 'If we are only interested in the interaction variables (*citric acid × fixed
    acidity*, here), we can specify `interaction_only=True`. In this case, we also
    don''t want the bias term, so we specify `include_bias=False` as well. This will
    give us the original variables along with their interaction term(s):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只对交互变量（此处为*柠檬酸 × 固定酸度*）感兴趣，可以指定`interaction_only=True`。在这种情况下，我们还不希望有偏置项，因此也指定`include_bias=False`。这将为我们提供原始变量及其交互项：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can add these polynomial features to our pipeline:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些多项式特征添加到我们的管道中：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note that this model is slightly better than before we added these additional
    terms, which was the model used in *Chapter 9*, *Getting Started with Machine
    Learning in Python*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个模型比我们在*第9章*《在Python中入门机器学习》使用的模型稍微更好，因为我们添加了这些额外的项：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Adding polynomial features and interaction terms increases the dimensionality
    of our data, which may not be desirable. Sometimes, rather than looking to create
    more features, we look for ways to consolidate them and reduce the dimensionality
    of our data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 添加多项式特征和交互项会增加我们数据的维度，这可能并不理想。有时，与其寻找创造更多特征的方法，我们更倾向于寻找合并特征并减少数据维度的方式。
- en: Dimensionality reduction
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 降维
- en: '**Dimensionality reduction** shrinks the number of features we train our model
    on. This is done to reduce the computational complexity of training the model
    without sacrificing much performance. We could just choose to train on a subset
    of the features (feature selection); however, if we think there is value in those
    features, albeit small, we may look for ways to extract the information we need
    from them.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**降维**是减少训练模型时所用特征数量的过程。这是为了在不大幅牺牲性能的情况下，降低训练模型的计算复杂度。我们可以选择仅在特征的子集上进行训练（特征选择）；然而，如果我们认为这些特征中存在一些有价值的信息，即便这些信息很小，我们也可以寻找提取所需信息的方法。'
- en: 'One common strategy for feature selection is to discard features with low variance.
    These features aren''t very informative since they are mostly the same value throughout
    the data. Scikit-learn provides the `VarianceThreshold` class for carrying out
    feature selection according to a minimum variance threshold. By default, it will
    discard any features that have zero variance; however, we can provide our own
    threshold. Let''s perform feature selection on our model that predicts whether
    a wine is red or white based on its chemical composition. Since we have no features
    with zero variance, we will choose to keep features whose variance is greater
    than 0.01:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的特征选择策略是丢弃方差较低的特征。这些特征不太具有信息性，因为它们在数据中大部分值相同。Scikit-learn提供了`VarianceThreshold`类，用于根据最小方差阈值进行特征选择。默认情况下，它会丢弃方差为零的特征；但是，我们可以提供自己的阈值。我们将在预测葡萄酒是红酒还是白酒的模型上进行特征选择，基于其化学成分。由于我们没有方差为零的特征，我们将选择保留方差大于0.01的特征：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This removed two features with low variance. We can get their names with the
    Boolean mask returned by the `VarianceThreshold` object''s `get_support()` method,
    which indicates the features that were kept:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这移除了两个方差较低的特征。我们可以通过`VarianceThreshold`对象的`get_support()`方法返回的布尔掩码来获取它们的名称，该掩码指示了保留下来的特征：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Using only 9 of the 11 features, our performance hasn''t been affected much:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用11个特征中的9个，我们的表现几乎没有受到影响：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Tip
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Check out the other feature selection options in the `feature_selection` module
    at [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`feature_selection`模块中其他特征选择的选项，地址为[https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)。
- en: If we believe there is value in all the features, we may decide to use feature
    extraction rather than discarding them entirely. **Principal component analysis**
    (**PCA**) performs feature extraction by projecting high-dimensional data into
    lower dimensions, thereby reducing the dimensionality. In return, we get the *n*
    components that maximize explained variance. This will be sensitive to the scale
    of the data, so we need to do some preprocessing beforehand.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们认为所有特征都有价值，我们可以选择使用特征提取，而不是完全丢弃它们。**主成分分析**（**PCA**）通过将高维数据投影到低维空间，从而进行特征提取，降低维度。作为回报，我们得到最大化解释方差的*n*个组件。由于这对数据的尺度非常敏感，因此我们需要事先进行一些预处理。
- en: 'Let''s take a look at the `pca_scatter()` function in the `ml_utils.pca` module,
    which will help us visualize our data when reduced to two dimensions:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`ml_utils.pca`模块中的`pca_scatter()`函数，它可以帮助我们在降至二维时可视化数据：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s visualize the wine data with two PCA components to see if there is a
    way to separate red from white:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用两个PCA组件来可视化葡萄酒数据，看看是否能找到将红酒与白酒分开的方法：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Most of the red wines are in the bright green mass of points at the top, and
    the white wines are in the blue point mass at the bottom. Visually, we can see
    how to separate them, but there is still some overlap:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数红酒位于顶部的亮绿色点块中，而白酒则位于底部的蓝色点块中。从视觉上来看，我们可以看到如何分开它们，但仍然有一些重叠：
- en: '![Figure 10.4 – Using two PCA components to separate wines by type'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.4 – 使用两个PCA组件按类型分离葡萄酒'
- en: '](img/Figure_10.4_B16834.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.4_B16834.jpg)'
- en: Figure 10.4 – Using two PCA components to separate wines by type
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – 使用两个PCA组件按类型分离葡萄酒
- en: Tip
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: PCA components will be linearly uncorrelated, since they were obtained through
    an orthogonal transformation (perpendicularity extended to higher dimensions).
    Linear regression assumes the regressors (input data) are not correlated, so this
    can help address multicollinearity.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: PCA组件将是线性无关的，因为它们是通过正交变换（垂直扩展到更高维度）得到的。线性回归假设回归变量（输入数据）之间没有相关性，因此这有助于解决多重共线性问题。
- en: 'Note the explained variances of each component from the previous plot''s legend—the
    components explain over 50% of the variance in the wine data. Let''s see if using
    three dimensions improves the separation. The `pca_scatter_3d()` function in the
    `ml_utils.pca` module uses `mpl_toolkits`, which comes with `matplotlib` for 3D
    visualizations:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意从之前图表的图例中得到的每个组件的解释方差——这些组件解释了葡萄酒数据中超过50%的方差。接下来，我们来看看使用三维是否能改善分离效果。`ml_utils.pca`模块中的`pca_scatter_3d()`函数使用了`mpl_toolkits`，这是`matplotlib`中用于3D可视化的工具包：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s use our 3D visualization function on the wine data again to see if white
    and red are easier to separate with three PCA components:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次使用 3D 可视化函数查看酒类数据，看看使用三个 PCA 组件时白酒和红酒是否更容易分开：
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'It seems like we could slice off the green (right) point mass from this angle,
    although we still have a few points in the wrong section:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度看，我们似乎可以从绿色（右侧）点集划分出一部分，尽管仍然有一些点在错误的区域：
- en: '![Figure 10.5 – Using three PCA components to separate wines by type'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.5 – 使用三个 PCA 组件按类型区分酒类'
- en: '](img/Figure_10.5_B16834.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.5_B16834.jpg)'
- en: Figure 10.5 – Using three PCA components to separate wines by type
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – 使用三个 PCA 组件按类型区分酒类
- en: Important note
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: PCA performs linear dimensionality reduction. Check out t-SNE and Isomap to
    perform manifold learning for non-linear dimensionality reduction.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 执行线性降维。可以查看 t-SNE 和 Isomap 进行流形学习以实现非线性降维。
- en: 'We can use the `pca_explained_variance_plot()` function from the `ml_utils.pca`
    module to visualize the cumulative explained variance as a function of the number
    of PCA components:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `ml_utils.pca` 模块中的 `pca_explained_variance_plot()` 函数来可视化随着 PCA 组件数量变化的累积解释方差：
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can pass the PCA part of our pipeline to this function in order to see the
    cumulative explained variance:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将管道中的 PCA 部分传递给此函数，以查看累积解释方差：
- en: '[PRE24]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The first four PCA components explain about 80% of the variance:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 前四个 PCA 组件解释了约 80% 的方差：
- en: '![Figure 10.6 – Explained variance for PCA components used'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.6 – PCA 组件的解释方差'
- en: '](img/Figure_10.6_B16834.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.6_B16834.jpg)'
- en: Figure 10.6 – Explained variance for PCA components used
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – PCA 组件的解释方差
- en: 'We can also use the elbow point method to find a good value for the number
    of PCA components to use, just as we did with k-means in *Chapter 9*, *Getting
    Started with Machine Learning in Python*. For this, we need to make a `ml_utils.pca`
    module has the `pca_scree_plot()` function for creating this visualization:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用肘部法则来找到适合的 PCA 组件数量，就像在 *第 9 章*《Python 机器学习入门》中使用 k-means 一样。为此，我们需要确保
    `ml_utils.pca` 模块包含 `pca_scree_plot()` 函数来创建这个可视化：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can pass the PCA part of our pipeline to this function in order to see the
    variance explained by each PCA component:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将管道中的 PCA 部分传递给此函数，以查看每个 PCA 组件解释的方差：
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The scree plot tells us we should try four PCA components because there are
    diminishing returns after that component:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 螺旋图告诉我们应该尝试四个 PCA 组件，因为之后的组件回报递减：
- en: '![Figure 10.7 – Diminishing returns for each additional PCA component after
    the fourth'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.7 – 每增加一个 PCA 组件后回报递减（第四个组件之后）'
- en: '](img/Figure_10.7_B16834.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.7_B16834.jpg)'
- en: Figure 10.7 – Diminishing returns for each additional PCA component after the
    fourth
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 每增加一个 PCA 组件后回报递减（第四个组件之后）
- en: 'We can build a model on top of these four PCA features in a process called
    **meta-learning**, where the last model in the pipeline is trained on the output
    from a different model, not the original data itself:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这些四个 PCA 特征上建立一个模型，这个过程叫做 **元学习**，即管道中的最后一个模型是基于其他模型的输出而非原始数据进行训练：
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Our new model performs nearly as well as the original logistic regression that
    used 11 features, with just 4 features made with PCA:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新模型表现几乎与使用 11 个特征的原始逻辑回归一样好，只用了通过 PCA 制作的 4 个特征：
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: After performing dimensionality reduction, we no longer have all of the features
    we started with—reducing the number of features was the point after all. However,
    it is possible that we will want to perform different feature engineering techniques
    on subsets of our features; in order to do so, we need to understand feature unions.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行降维处理后，我们不再拥有最初的所有特征——毕竟，减少特征数量就是目的。然而，我们可能希望对特征子集应用不同的特征工程技术；为了实现这一点，我们需要理解特征联合。
- en: Feature unions
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征联合
- en: We may want to build a model on features from a variety of sources, such as
    PCA, in addition to selecting a subset of the features. For these purposes, `scikit-learn`
    provides the `FeatureUnion` class in the `pipeline` module. This also allows us
    to perform multiple feature engineering techniques at once, such as feature extraction
    followed by feature transformation, when we combine this with a pipeline.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能希望在来自各种来源的特征上建立模型，例如 PCA，以及选择特征的子集。为此，`scikit-learn` 提供了 `pipeline` 模块中的
    `FeatureUnion` 类。这还允许我们一次执行多个特征工程技术，例如特征提取后跟特征变换，当我们将其与管道结合时。
- en: 'Creating a `FeatureUnion` object is just like creating a pipeline, but rather
    than passing the steps in order, we pass the transformations we want to make.
    These will be stacked side by side in the result. Let''s use a feature union of
    interaction terms and select the features with a variance above 0.01 to predict
    red wine quality:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `FeatureUnion` 对象就像创建一个管道一样，但我们传递的是要进行的变换，而不是按顺序传递步骤。这些变换将在结果中并排堆叠。让我们使用交互项的特征联合，并选择方差大于
    0.01 的特征来预测红酒质量：
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To illustrate the transformation that took place, let''s examine the first
    row from the training set for the red wine quality data after the `FeatureUnion`
    object transforms it. Since we saw that our variance threshold results in nine
    features, we know they are the first nine entries in the resulting NumPy array,
    and the rest are the interaction terms:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明发生的变换，让我们检查一下红酒质量数据集训练集的第一行，在 `FeatureUnion` 对象进行变换后的结果。由于我们看到方差阈值结果产生了九个特征，我们知道它们是结果
    NumPy 数组中的前九项，剩下的是交互项：
- en: '[PRE30]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can also look at the classification report to see that we got a marginal
    improvement in F1 score:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看分类报告，看到 F1 分数有了微小的提升：
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In this example, we selected our features such that they had variance greater
    than 0.01, making the assumption that if the feature doesn't take on many different
    values then it may not be that helpful. Rather than making this assumption, we
    can use a machine learning model to help determine which features are important.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们选择了具有大于 0.01 方差的特征，假设如果特征没有许多不同的值，它可能不会那么有用。我们可以使用机器学习模型来帮助确定哪些特征是重要的，而不是仅仅做出这个假设。
- en: Feature importances
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: '**Decision trees** recursively split the data, making decisions on which features
    to use for each split. They are **greedy learners**, meaning they look for the
    largest split they can make each time; this isn''t necessarily the optimal split
    when looking at the output of the tree. We can use a decision tree to gauge **feature
    importances**, which determine how the tree splits the data at the decision nodes.
    These feature importances can help inform feature selection. Note that feature
    importances will sum to one, and higher values are better. Let''s use a decision
    tree to see how red and white wine can be separated on a chemical level:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**递归地对数据进行划分，决定每次划分时使用哪些特征。它们是**贪婪学习者**，意味着它们每次都寻找可以进行的最大划分；这并不一定是查看树输出时的最佳划分。我们可以使用决策树来评估**特征重要性**，这决定了树在决策节点上如何划分数据。这些特征重要性可以帮助我们进行特征选择。请注意，特征重要性加起来会等于
    1，值越大越好。让我们使用决策树来看看如何在化学层面上将红酒和白酒分开：'
- en: '[PRE32]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This shows us that the most important chemical properties in distinguishing
    between red and white wine are total sulfur dioxide and chlorides:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，在区分红酒和白酒时，最重要的化学特性是总二氧化硫和氯化物：
- en: '![Figure 10.8 – Importance of each chemical property in predicting wine type'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.8 – 在预测酒的类型时，每种化学特性的重要性'
- en: '](img/Figure_10.8_B16834.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.8_B16834.jpg)'
- en: Figure 10.8 – Importance of each chemical property in predicting wine type
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – 在预测酒的类型时，每种化学特性的重要性
- en: Tip
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Using the top features, as indicated by the feature importances, we can try
    to build a simpler model (by using fewer features). If possible, we want to simplify
    our models without sacrificing much performance. See the `wine.ipynb` notebook
    for an example.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用由特征重要性指示的最重要特征，我们可以尝试构建一个更简单的模型（通过使用更少的特征）。如果可能，我们希望简化我们的模型，而不牺牲太多性能。有关示例，请参见
    `wine.ipynb` 笔记本。
- en: 'If we train another decision tree with a max depth of two, we can visualize
    the top of the tree (it is too large to visualize if we don''t limit the depth):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们训练另一棵最大深度为二的决策树，我们可以可视化树的顶部（如果不限制深度，树太大而无法可视化）：
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Important note
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Graphviz software will need to be installed (if it isn't already) in order to
    visualize the tree. It can be downloaded at [https://graphviz.gitlab.io/download/](https://graphviz.gitlab.io/download/),
    with the installation guide at [https://graphviz.readthedocs.io/en/stable/manual.html#installation](https://graphviz.readthedocs.io/en/stable/manual.html#installation).
    Note that the kernel will need to be restarted after installing. Otherwise, pass
    `out_file='tree.dot'` to the `export_graphviz()` `function` and then generate
    a PNG file by running `dot -T png tree.dot -o tree.png` from the command line.
    As an alternative, `scikit-learn` provides the `plot_tree()` function, which uses
    `matplotlib`; consult the notebook for an example.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 需要安装Graphviz软件（如果尚未安装）以可视化树结构。可以在[https://graphviz.gitlab.io/download/](https://graphviz.gitlab.io/download/)下载，安装指南在[https://graphviz.readthedocs.io/en/stable/manual.html#installation](https://graphviz.readthedocs.io/en/stable/manual.html#installation)。请注意，在安装后需要重新启动内核。否则，将`out_file='tree.dot'`传递给`export_graphviz()`函数，然后在命令行中运行`dot
    -T png tree.dot -o tree.png`生成PNG文件。作为替代，`scikit-learn`提供了`plot_tree()`函数，它使用`matplotlib`；请参考笔记本中的示例。
- en: 'This results in the following tree, which first splits on total sulfur dioxide
    (which has the highest feature importance), followed by chlorides on the second
    level. The information at each node tells us the criterion for the split (the
    top line), the value of the cost function (**gini**), the number of samples at
    that node (**samples**), and the number of samples in each class at that node
    (**values**):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下树形结构，首先在总二氧化硫上进行分割（具有最高的特征重要性），然后在第二级上进行氯化物分割。每个节点上的信息告诉我们分割的标准（顶部行），成本函数的值（**gini**），该节点处的样本数目（**samples**），以及每个类别中的样本数目（**values**）：
- en: '![Figure 10.9 – Decision tree for predicting wine type based on chemical properties'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.9 – 基于化学性质预测葡萄酒类型的决策树'
- en: '](img/Figure_10.9_B16834.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.9_B16834.jpg)'
- en: Figure 10.9 – Decision tree for predicting wine type based on chemical properties
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 – 基于化学性质预测葡萄酒类型的决策树
- en: 'We can also apply decision trees to regression problems. Let''s find the feature
    importances for the planets data using the `DecisionTreeRegressor` class:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将决策树应用于回归问题。让我们使用`DecisionTreeRegressor`类来查找行星数据的特征重要性：
- en: '[PRE34]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Basically, the semi-major axis is the main determinant in the period length,
    which we already knew, but if we visualize a tree, we can see why. The first four
    splits are all based on the semi-major axis:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，半长轴是周期长度的主要决定因素，这一点我们已经知道，但是如果我们可视化一棵树，我们就能看到为什么。前四个分割都基于半长轴：
- en: '![Figure 10.10 – Decision tree for predicting planet period'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.10 – 预测行星周期的决策树'
- en: '](img/Figure_10.10_B16834.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.10_B16834.jpg)'
- en: Figure 10.10 – Decision tree for predicting planet period
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 – 预测行星周期的决策树
- en: Decision trees can be `scikit-learn` documentation provides tips to address
    overfitting and other potential issues when using decision trees at [https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use](https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use).
    Keep this in mind as we discuss ensemble methods.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以使用`scikit-learn`文档提供的提示来解决决策树使用时的过拟合和其他潜在问题，参见[https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use](https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)。在讨论集成方法时，请记住这一点。
- en: Ensemble methods
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成方法
- en: '**Ensemble methods** combine many models (often weak ones) to create a stronger
    one that will either minimize the average error between observed and predicted
    values (the **bias**) or improve how well it generalizes to unseen data (minimize
    the **variance**). We have to strike a balance between complex models that may
    increase variance, as they tend to overfit, and simple models that may have high
    bias, as these tend to underfit. This is called the **bias-variance trade-off**,
    which is illustrated in the following subplots:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成方法**结合了许多模型（通常是弱模型），创建一个更强的模型，可以最小化观察和预测值之间的平均误差（**偏差**），或者改进其对未见数据的泛化能力（最小化**方差**）。我们必须在可能增加方差的复杂模型之间取得平衡，因为它们倾向于过拟合，并且可能具有高偏差的简单模型之间取得平衡，因为这些倾向于欠拟合。这被称为**偏差-方差权衡**，在以下子图中有所说明：'
- en: '![Figure 10.11 – The bias-variance trade-off'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.11 – 偏差-方差权衡'
- en: '](img/Figure_10.11_B16834.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.11_B16834.jpg)'
- en: Figure 10.11 – The bias-variance trade-off
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 – 偏差-方差权衡
- en: 'Ensemble methods can be broken down into three categories: **boosting**, **bagging**,
    and **stacking**. **Boosting** trains many weak learners, which learn from each
    other''s mistakes to reduce bias, making a stronger learner. **Bagging**, on the
    other hand, uses **bootstrap aggregation** to train many models on bootstrap samples
    of the data and aggregate the results together (using voting for classification,
    and the average for regression) to reduce variance. We can also combine many different
    model types together with voting. **Stacking** is an ensemble technique where
    we combine many different model types using the outputs of some as the inputs
    to others; this is done to improve predictions. We saw an example of stacking
    when we combined PCA and logistic regression in the *Dimensionality reduction*
    section earlier in this chapter.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法可以分为三类：**提升**、**装袋**和**堆叠**。**提升**训练许多弱学习器，它们从彼此的错误中学习，减少偏差，从而使学习器更强。另一方面，**装袋**使用**自助聚合**方法，在数据的自助样本上训练多个模型，并将结果聚合在一起（分类时使用投票，回归时使用平均值），从而减少方差。我们还可以通过投票将不同类型的模型组合在一起。**堆叠**是一种集成技术，其中我们将多种不同的模型类型结合在一起，使用某些模型的输出作为其他模型的输入；这样做是为了提高预测的准确性。我们在本章的*降维*部分中，结合了PCA和逻辑回归，这就是堆叠的一个例子。
- en: Random forest
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: Decision trees have a tendency to overfit, especially if we don't set limits
    on how far they can grow (with the `max_depth` and `min_samples_leaf` parameters).
    We can address this overfitting issue with a `oob_score` parameter.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树容易过拟合，尤其是当我们没有设置限制以控制树的生长深度（使用`max_depth`和`min_samples_leaf`参数）时。我们可以通过`oob_score`参数来解决这个过拟合问题。
- en: Important note
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The `min_samples_leaf` parameter requires a minimum number of samples to be
    on the final nodes in the tree (or leaves); this prevents the trees from being
    fit until they only have a single observation at each leaf.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`参数要求树的最终节点（或叶子）上有最少的样本数；这可以防止决策树过度拟合，直到每个叶子只有一个观测值。'
- en: Each of the trees also gets a subset of the features (random feature selection),
    which defaults to the square root of the number of features (the `max_features`
    parameter). This can help address the curse of dimensionality. As a consequence,
    however, the random forest can't be as easily interpreted as the decision trees
    that make it up. We can, however, extract feature importances from the random
    forest, just as we did with the decision tree.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 每棵树还会得到特征的子集（随机特征选择），其默认值为特征数量的平方根（即`max_features`参数）。这有助于解决维度灾难。然而，作为后果，随机森林不如构成它的决策树那样容易解释。然而，我们仍然可以像在决策树中一样，从随机森林中提取特征重要性。
- en: 'Let''s use the `RandomForestClassifier` class from the `ensemble` module to
    build a random forest (with `n_estimators` trees in it) for the classification
    of high-quality red wines:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`ensemble`模块中的`RandomForestClassifier`类来构建一个随机森林（其中包含`n_estimators`棵树），用于高质量红酒的分类：
- en: '[PRE35]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note that our precision with the random forest is already much better than the
    0.35 we got in *Chapter 9*, *Getting Started with Machine Learning in Python*.
    The random forest is robust to outliers and able to model non-linear decision
    boundaries to separate the classes, which may explain part of this dramatic improvement.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用随机森林的精度已经远远超过了*第9章*中我们得到的0.35精度，*Python中的机器学习入门*。随机森林对离群点具有鲁棒性，并且能够建模非线性决策边界以分隔类别，这可能解释了这种显著改进的部分原因。
- en: Gradient boosting
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Boosting looks to improve upon the mistakes of previous models. One way of doing
    this is to move in the direction of the steepest reduction in the loss function
    for the model. Since the **gradient** (the multi-variable generalization of the
    derivative) is the direction of steepest ascent, this can be done by calculating
    the negative gradient, which yields the direction of steepest descent, meaning
    the best improvement in the loss function from the current result. This technique
    is called **gradient descent**.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 提升方法旨在改进前一模型的错误。实现这一点的一种方式是沿着损失函数下降最快的方向移动。由于**梯度**（导数的多变量推广）是最陡的上升方向，因此可以通过计算负梯度来实现这一点，这样就得到了最陡的下降方向，也就是当前结果的损失函数中最佳的改进方向。这种技术称为**梯度下降**。
- en: Important note
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Although gradient descent sounds great, there are some potential issues with
    it. It's possible to end up in a local minimum (a minimum in a certain region
    of the cost function); the algorithm will stop, thinking that we have the optimal
    solution, when in fact we don't, because we would like the global minimum (the
    minimum over the whole region).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管梯度下降听起来不错，但它也存在一些潜在问题。可能会陷入局部最小值（在某一地区的损失函数最小值）；算法会停止，认为我们找到了最优解，实际上我们并没有找到，因为我们希望得到全局最小值（整个区域的最小值）。
- en: Scikit-learn's `ensemble` module provides the `GradientBoostingClassifier` and
    `GradientBoostingRegressor` classes for gradient boosting using decision trees.
    These trees will boost their performance through gradient descent. Note that gradient
    boosted trees are more sensitive to noisy training data than the random forest.
    In addition, we must consider the additional time required to build all the trees
    in series, unlike the parallel training we can benefit from with the random forest.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 的 `ensemble` 模块提供了 `GradientBoostingClassifier` 和 `GradientBoostingRegressor`
    类，用于通过决策树实现梯度提升。这些树将通过梯度下降来提高性能。请注意，梯度提升树对噪声训练数据比随机森林更敏感。此外，我们必须考虑到，构建所有树所需的额外时间是线性串行的，而不像随机森林那样可以并行训练。
- en: 'Let''s use grid search and gradient boosting to train another model for classifying
    the red wine quality data. In addition to searching for the best values for the
    `max_depth` and `min_samples_leaf` parameters, we will search for a good value
    for the `learning_rate` parameter, which determines the contribution each tree
    will make in the final estimator:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用网格搜索和梯度提升来训练另一个模型，用于对红葡萄酒质量数据进行分类。除了搜索 `max_depth` 和 `min_samples_leaf`
    参数的最佳值外，我们还将搜索 `learning_rate` 参数的最佳值，这个参数决定了每棵树在最终估计器中的贡献：
- en: '[PRE36]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The F1 macro score we achieve with gradient boosting is better than the 0.66
    we got with logistic regression in *Chapter 9*, *Getting Started with Machine
    Learning in Python*:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过梯度提升获得的 F1 宏观评分优于在 *第9章* *《Python机器学习入门》* 中使用逻辑回归得到的 0.66 分：
- en: '[PRE37]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Both bagging and boosting have given us better performance than the logistic
    regression model; however, we may find that the models don't always agree and
    that we could improve performance even more by having the models vote before making
    the final prediction.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是袋装方法还是提升方法，都给我们带来了比逻辑回归模型更好的表现；然而，我们可能会发现这些模型并不总是达成一致，且通过让模型投票再做最终预测，我们有可能进一步提升性能。
- en: Voting
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投票
- en: 'When trying out different models for classification, it may be interesting
    to measure their agreement using Cohen''s kappa score. We can use the `cohen_kappa_score()`
    function in the `sklearn.metrics` module to do so. The score ranges from complete
    disagreement (-1) to complete agreement (1). Our boosting and bagging predictions
    have a high level of agreement:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试不同的分类模型时，使用 Cohen's kappa 评分来衡量它们的一致性可能会很有趣。我们可以使用 `sklearn.metrics` 模块中的
    `cohen_kappa_score()` 函数来实现这一点。该评分从完全不一致（-1）到完全一致（1）。我们的提升和袋装预测有很高的一致性：
- en: '[PRE38]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Sometimes, we can't find a single model that works well for all of our data,
    so we may want to find a way to combine the opinions of various models to make
    the final decision. Scikit-learn provides the `VotingClassifier` class for aggregating
    model opinions on classification tasks. We have the option of specifying the voting
    type, where `hard` results in majority rules and `soft` will predict the class
    with the highest sum of probabilities across the models.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们找不到一个适用于所有数据的单一模型，因此我们可能需要找到一种方法，将各种模型的意见结合起来做出最终决策。Scikit-learn 提供了 `VotingClassifier`
    类，用于聚合分类任务中的模型意见。我们可以选择指定投票类型，其中 `hard` 代表多数规则，而 `soft` 将预测具有最高概率总和的类别。
- en: 'As an example, let''s create a classifier for each voting type using the three
    estimators (models) from this chapter—logistic regression, random forest, and
    gradient boosting. Since we will run `fit()`, we pass in the best estimator from
    each of our grid searches (`best_estimator_`). This avoids running each grid search
    again unnecessarily, which will also train our model faster:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，让我们为每种投票类型创建一个分类器，使用本章中的三种估计器（模型）——逻辑回归、随机森林和梯度提升。由于我们将运行 `fit()`，我们传入每次网格搜索中得到的最佳估计器（`best_estimator_`）。这样可以避免不必要地重新运行每次网格搜索，也能加速模型训练：
- en: '[PRE39]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Our `majority_rules` classifier requires two of the three models to agree (at
    a minimum), while the `max_probabilities` classifier has each model vote with
    its predicted probabilities. We can measure how well they perform on the test
    data with the `classification_report()` function, which tells us that `majority_rules`
    is a little better than `max_probabilities` in terms of precision. Both are better
    than the other models we have tried:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`majority_rules`分类器要求至少有三个模型中的两个达成一致，而`max_probabilities`分类器则让每个模型根据其预测的概率进行投票。我们可以使用`classification_report()`函数来衡量它们在测试数据上的表现，该函数告诉我们，`majority_rules`在精确度方面稍优于`max_probabilities`，而这两个分类器都比我们尝试的其他模型表现得更好：
- en: '[PRE40]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Another important option with the `VotingClassifier` class is the `weights`
    parameter, which lets us place more or less emphasis on certain estimators when
    voting. For example, if we pass `weights=[1, 2, 2]` to `majority_rules`, we are
    giving extra weight to the predictions made by the random forest and gradient
    boosting estimators. In order to determine which models (if any) should be given
    extra weight, we can look at individual performance and prediction confidence.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`VotingClassifier`类中的另一个重要选项是`weights`参数，它允许我们在投票时对某些估计器给予更多或更少的重视。例如，如果我们将`weights=[1,
    2, 2]`传递给`majority_rules`，我们就给随机森林和梯度提升估计器的预测赋予了更多的权重。为了确定哪些模型（如果有的话）应该给予更多的权重，我们可以查看单个模型的表现和预测信心水平。'
- en: Inspecting classification prediction confidence
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查分类预测的信心水平
- en: As we saw with ensemble methods, when we know the strengths and weaknesses of
    our model, we can employ strategies to attempt to improve performance. We may
    have two models to classify something, but they most likely won't agree on everything.
    However, say that we know that one does better on edge cases, while the other
    is better on the more common ones. In that case, we would likely want to investigate
    a voting classifier to improve our performance. How can we know how the models
    perform in different situations, though?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在集成方法中所看到的，当我们了解模型的优缺点时，可以采用策略来尝试提升性能。我们可能有两个模型来对某些事物进行分类，但它们很可能不会对所有事情达成一致。然而，假设我们知道其中一个在边缘情况上表现更好，而另一个在常见情况下更为准确。在这种情况下，我们很可能会希望研究一个投票分类器来提高我们的性能。那么，我们怎么知道模型在不同情况下的表现呢？
- en: 'By looking at the probabilities the model predicts of an observation belonging
    to a given class, we can gain insight into how confident our model is when it
    is correct and when it errs. We can use our `pandas` data wrangling skills to
    make quick work of this. Let''s see how confident our original `white_or_red`
    model from *Chapter 9*, *Getting Started with Machine Learning in Python*, was
    in its predictions:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看模型预测的观察值属于某一类别的概率，我们可以深入了解模型在正确预测和出错时的信心程度。我们可以运用`pandas`数据处理技能来快速完成这项任务。让我们来看一下我们在*第9章*，*Python机器学习入门*中，原始`white_or_red`模型在预测时的信心水平：
- en: '[PRE41]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Tip
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'We can tweak the probability threshold for our model''s predictions by using
    the `predict_proba()` method, instead of `predict()`. This will give us the probabilities
    that the observation belongs to each class. We can then compare that to our custom
    threshold. For example, we could use 75%: `white_or_red.predict_proba(w_X_test)[:,1]
    >= .75`.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`predict_proba()`方法，而不是`predict()`，来调整模型预测的概率阈值。这将为我们提供观察值属于每个类别的概率。然后我们可以将其与我们的自定义阈值进行比较。例如，我们可以使用75%的阈值：`white_or_red.predict_proba(w_X_test)[:,1]
    >= .75`。
- en: One way to identify this threshold is to determine the false positive rate we
    are comfortable with, and then use the data from the `roc_curve()` function in
    the `sklearn.metrics` module to find the threshold that results in that false
    positive rate. Another way is to find a satisfactory spot along the precision-recall
    curve, and then get the threshold from the `precision_recall_curve()` function.
    We will work through an example in [*Chapter 11*](B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237),
    *Machine Learning Anomaly Detection*.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 确定这一阈值的一种方法是确定我们可以接受的假阳性率，然后使用`sklearn.metrics`模块中的`roc_curve()`函数中的数据，找到导致该假阳性率的阈值。另一种方法是找到精确度-召回率曲线上的一个满意点，然后从`precision_recall_curve()`函数中得到该阈值。我们将在[*第11章*](B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237)，《机器学习异常检测》中详细讨论一个例子。
- en: 'Let''s use `seaborn` to make a plot showing the distribution of the prediction
    probabilities when the model was correct versus when it was incorrect. The `displot()`
    function makes it easy to plot the **kernel density estimate** (**KDE**) superimposed
    on a histogram. Here, we will also add a **rug plot**, which shows where each
    of our predictions ended up:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `seaborn` 绘制一个图，展示模型正确时与错误时预测概率的分布。`displot()` 函数使得绘制 **核密度估计** (**KDE**)
    并叠加在直方图上变得非常简单。在这里，我们还将添加一个 **地毯图**，它显示了每个预测的具体位置：
- en: '[PRE42]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The KDE for correct predictions is bimodal, with modes near 0 and near 1, meaning
    the model is very confident when it is correct, which, since it is correct most
    of the time, means it is very confident in general. The peak of the correct predictions
    KDE at 0 is much higher than the one at 1 because we have many more white wines
    than red wines in the data. Note that the KDE shows probabilities of less than
    zero and greater than one as possible. For this reason, we add the histogram to
    confirm that the shape we are seeing is meaningful. The histogram for correct
    predictions doesn''t have much in the middle of the distribution, so we include
    the rug plot to better see which probabilities were predicted. The incorrect predictions
    don''t have many data points, but it appears to be all over the place, because
    when the model got it wrong, it got fooled pretty badly:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 正确预测的 KDE 是双峰的，峰值分别接近 0 和接近 1，这意味着模型在正确时非常有信心，而由于它大多数时候是正确的，所以总体上它是非常自信的。正确预测的
    KDE 在 0 处的峰值远高于在 1 处的峰值，因为数据中白葡萄酒远多于红葡萄酒。请注意，KDE 显示的概率可能小于零或大于一。因此，我们添加了直方图来确认我们看到的形状是有意义的。正确预测的直方图在分布的中间部分没有多少数据，因此我们加入了**地毯图**，以更好地观察哪些概率被预测出来。错误的预测数据点不多，但似乎分布很广，因为当模型预测错误时，错得很离谱：
- en: '![Figure 10.12 – Prediction confidence when the model was correct versus incorrect'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.12 – 模型正确与错误时的预测置信度'
- en: '](img/Figure_10.12_B16834.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.12_B16834.jpg)'
- en: Figure 10.12 – Prediction confidence when the model was correct versus incorrect
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 – 模型正确与错误时的预测置信度
- en: This outcome tells us we may want to look into the chemical properties of the
    wines that were incorrectly classified. It's possible they were outliers and that
    is why they fooled the model. We can modify the box plots by wine type from the
    *Exploratory data analysis* section in *Chapter 9*, *Getting Started with Machine
    Learning in Python*, to see if anything stands out (*Figure 9.6*).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果告诉我们，可能需要研究那些被错误分类的酒的化学属性。它们可能是异常值，因此才会欺骗模型。我们可以修改第 9 章中 *Python机器学习入门*
    部分的 *探索性数据分析* 章节中的酒类箱线图，看看是否有什么特别之处（*图 9.6*）。
- en: 'First, we isolate the chemical properties for the incorrectly classified wines:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们隔离错误分类酒的化学属性：
- en: '[PRE43]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Then, we add some calls to `scatter()` on the `Axes` object to mark these wines
    on the box plots from before:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们在 `Axes` 对象上添加一些 `scatter()` 调用，将这些酒标记在之前的箱线图上：
- en: '[PRE44]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This results in each of the incorrectly classified wines being marked with
    a red **X**. In each subplot, the points on the left box plot are white wines
    and those on the right box plot are red wines. It appears that some of them may
    have been outliers for a few characteristics—such as red wines with high residual
    sugar or sulfur dioxide, and white wines with high volatile acidity:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致每个被错误分类的酒都被标记为红色 **X**。在每个子图中，左边箱线图上的点是白葡萄酒，右边箱线图上的点是红葡萄酒。似乎其中一些可能是因为某些特征的异常值—比如高残糖或二氧化硫的红葡萄酒，以及高挥发酸的白葡萄酒：
- en: '![Figure 10.13 – Checking whether incorrect predictions were outliers'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.13 – 检查错误预测是否为异常值'
- en: '](img/Figure_10.13_B16834.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.13_B16834.jpg)'
- en: Figure 10.13 – Checking whether incorrect predictions were outliers
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13 – 检查错误预测是否为异常值
- en: Despite having many more white wines than red wines in the data, our model is
    able to distinguish between them pretty well. This isn't always the case. Sometimes,
    in order to improve our performance, we need to address the class imbalance.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据中白葡萄酒远多于红葡萄酒，我们的模型仍然能够很好地区分它们。这并非总是如此。有时，为了提高性能，我们需要处理类别不平衡问题。
- en: Addressing class imbalance
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理类别不平衡
- en: 'When faced with a class imbalance in our data, we may want to try to balance
    the training data before we build a model around it. In order to do this, we can
    use one of the following imbalanced sampling techniques:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们面临数据中的类别不平衡问题时，可能希望在构建模型之前尝试平衡训练数据。为此，我们可以使用以下其中一种不平衡抽样技术：
- en: Over-sample the minority class.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对少数类进行上采样。
- en: Under-sample the majority class.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对多数类进行下采样。
- en: In the case of **over-sampling**, we pick a larger proportion from the minority
    class in order to get closer to the amount of the majority class; this may involve
    a technique such as bootstrapping or generating new data similar to the values
    in the existing data (using machine learning algorithms such as nearest neighbors).
    **Under-sampling**, on the other hand, will take less data overall by reducing
    the amount taken from the majority class. The decision to use over-sampling or
    under-sampling will depend on the amount of data we started with, and in some
    cases, computational costs. In practice, we wouldn't try either of these without
    first trying to build the model with the class imbalance. It's important not to
    try to optimize things prematurely; not to mention that by building the model
    first, we have a baseline to compare our imbalanced sampling attempts against.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在**上采样**的情况下，我们从少数类中挑选出更多的样本，以便接近多数类的样本数量；这可能涉及到自助法（bootstrapping）或生成与现有数据中值相似的新数据（使用机器学习算法，如最近邻）。另一方面，**下采样**则通过减少从多数类中采样的数量，来减少整体数据量。是否使用上采样或下采样将取决于我们最初的数据量，有时还需要考虑计算成本。实际上，在尝试使用任何这些方法之前，我们应该先尝试在类别不平衡的情况下构建模型。重要的是不要过早进行优化；更何况，通过先构建模型，我们可以将我们的不平衡抽样尝试与其作为基线进行比较。
- en: Important note
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Huge performance issues can arise if the minority class that we have in the
    data isn't truly representative of the full spectrum present in the population.
    For this reason, our method of collecting the data in the first place should be
    both known to us and carefully evaluated before proceeding to modeling. If we
    aren't careful, we could easily build a model that can't generalize to new data,
    regardless of how we handle the class imbalance.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们数据中的少数类不能真正代表整体人群的全貌，可能会出现巨大的性能问题。因此，最初收集数据的方法应该是我们熟知的，并在进行建模之前经过仔细评估。如果我们不小心，可能会轻易构建出无法对新数据进行泛化的模型，无论我们如何处理类别不平衡问题。
- en: 'Before we explore any imbalanced sampling techniques, let''s create a baseline
    model using **k-nearest neighbors** (**k-NN**) classification, which will classify
    observations according to the class of the k-nearest observations in the n-dimensional
    space of the data (our red wine quality data is 11-dimensional). For comparison
    purposes, we will use the same number of neighbors for all the models in this
    section; however, it is certainly possible that the sampling techniques will result
    in a different value performing better. We will use five neighbors:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索任何不平衡抽样技术之前，让我们使用**k-最近邻**（**k-NN**）分类器创建一个基线模型，它会根据数据在n维空间中k个最近邻的类别来对观察结果进行分类（我们的红酒质量数据是11维的）。为了便于比较，本节中的所有模型将使用相同数量的邻居；然而，采样技术可能会导致不同的值表现更好。我们将使用5个邻居：
- en: '[PRE45]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Our k-NN model is fast to train because it is a `%%timeit` magic to get an
    estimate of how long it takes on average to train. Note that this will train the
    model multiple times, so it might not be the best strategy to time a computationally
    intense model:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的k-NN模型训练速度很快，因为它是一个`%%timeit`魔法命令，用于估算训练所需的平均时间。请注意，这将多次训练模型，因此对于计算密集型模型来说，这可能不是最好的计时策略：
- en: '[PRE46]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s compare this result with training a **support vector machine** (**SVM**),
    which projects the data into a higher dimension to find the **hyperplane** that
    separates the classes. A hyperplane is the n-dimensional equivalent of a plane,
    just like a plane is the two-dimensional equivalent of a line. SVMs are typically
    robust to outliers and can model non-linear decision boundaries; however, SVMs
    get slow very quickly, so it will be a good comparison:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个结果与训练**支持向量机**（**SVM**）进行比较，SVM将数据投影到更高维度，以寻找能够分隔类别的**超平面**。超平面是n维空间中平面的等价物，就像平面是二维空间中直线的等价物一样。SVM通常对异常值具有鲁棒性，并且可以建模非线性决策边界；然而，SVM的训练速度很慢，因此这将是一个很好的比较：
- en: '[PRE47]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now that we have our baseline model and an idea of how it works, let''s see
    how the baseline k-NN model performs:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了基线模型，并且对它的工作方式有了了解，让我们看看基线k-NN模型的表现：
- en: '[PRE48]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: With this performance benchmark, we are ready to try out imbalanced sampling.
    We will be using the `imblearn` package, which is provided by the `scikit-learn`
    community. It provides implementations for over- and under-sampling using various
    strategies, and it is just as easy to use as `scikit-learn`, since they both follow
    the same API conventions. For reference, the documentation can be found at [https://imbalanced-learn.readthedocs.io/en/stable/api.html](https://imbalanced-learn.readthedocs.io/en/stable/api.html).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个性能基准，我们准备尝试不平衡采样。我们将使用由`scikit-learn`社区提供的`imblearn`包。它提供了使用多种策略进行过采样和欠采样的实现，并且与`scikit-learn`一样容易使用，因为它们遵循相同的API约定。相关文档可以在[https://imbalanced-learn.readthedocs.io/en/stable/api.html](https://imbalanced-learn.readthedocs.io/en/stable/api.html)找到。
- en: Under-sampling
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欠采样
- en: As we hinted at earlier, under-sampling will reduce the amount of data available
    to train our model on. This means we should only attempt this if we have enough
    data that we can accept eliminating some of it. Let's see what happens with the
    red wine quality data, since we don't have much data to begin with.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，欠采样将减少可用于训练模型的数据量。这意味着只有在我们拥有足够的数据，并且可以接受丢弃部分数据的情况下，才应尝试欠采样。既然我们本就没有太多数据，让我们看看红酒质量数据会发生什么。
- en: 'We will use the `RandomUnderSampler` class from `imblearn` to randomly under-sample
    the low-quality red wines in the training set:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`imblearn`中的`RandomUnderSampler`类对训练集中的低质量红酒进行随机欠采样：
- en: '[PRE49]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We went from almost 14% of the training data being high-quality red wine to
    50% of it; however, notice that this came at the price of 1,049 training samples
    (more than half of our training data):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练数据中约14%的高质量红酒提高到50%；然而，请注意，这一变化是以1,049个训练样本为代价的（这超过了我们训练数据的一半）：
- en: '[PRE50]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Fitting our model with the under-sampled data is no different from before:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 使用欠采样数据拟合模型与之前没有什么不同：
- en: '[PRE51]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Using the classification report, we see that under-sampling is definitely not
    an improvement—we hardly had any data for this model:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分类报告，我们可以看到欠采样绝对不是一种改进——我们几乎没有足够的数据用于这个模型：
- en: '[PRE52]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: In situations where we have limited data to start with, under-sampling is simply
    not feasible. Here, we lost over half of the already small amount of data we had.
    Models need a good amount of data to learn from, so let's try over-sampling the
    minority class now.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据本就有限的情况下，欠采样显然不可行。在这里，我们失去了超过一半原本已经很少的数据。模型需要足够的数据来进行学习，因此接下来我们尝试对少数类进行过采样。
- en: Over-sampling
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过采样
- en: It's clear that with smaller datasets, it won't be beneficial to under-sample.
    Instead, we can try over-sampling the minority class (the high-quality red wines,
    in this case). Rather than doing random over-sampling with the `RandomOverSampler`
    class, we are going to use the **Synthetic Minority Over-sampling Technique**
    (**SMOTE**) to create *new* (synthetic) red wines similar to the high-quality
    ones using the k-NN algorithm. By doing this, we are making a big assumption that
    the data we have collected about the chemical properties of the red wine does
    influence the quality rating of the wine.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，在数据集较小的情况下，欠采样不会带来好处。相反，我们可以尝试对少数类（在这种情况下是高质量的红酒）进行过采样。我们不打算使用`RandomOverSampler`类进行随机过采样，而是将使用**合成少数类过采样技术**（**SMOTE**）通过k-NN算法生成与高质量红酒相似的*新*（合成）红酒。这样做的前提是假设我们收集到的红酒化学性质数据确实会影响红酒的质量评分。
- en: Important note
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'The SMOTE implementation in `imblearn` comes from this paper:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`imblearn`中的SMOTE实现来源于这篇论文：'
- en: '*N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, SMOTE: synthetic
    minority over-sampling technique, Journal of Artificial Intelligence Research,
    321-357, 2002*, available at [https://arxiv.org/pdf/1106.1813.pdf](https://arxiv.org/pdf/1106.1813.pdf).'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '*N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, SMOTE: 合成少数类过采样技术，人工智能研究期刊，321-357，2002*，可在[https://arxiv.org/pdf/1106.1813.pdf](https://arxiv.org/pdf/1106.1813.pdf)找到。'
- en: 'Let''s use SMOTE with the five nearest neighbors to over-sample the high-quality
    red wines in our training data:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用SMOTE和五个最近邻来对训练数据中的高质量红酒进行过采样：
- en: '[PRE53]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Since we over-sampled, we will have more data than we did before, gaining an
    extra 1,049 high-quality red wine samples:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们进行了过采样，我们将获得比之前更多的数据，增加了1,049个高质量红酒样本：
- en: '[PRE54]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Once again, we will fit a k-NN model, using the over-sampled data this time:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们将使用过采样数据来拟合一个 k-NN 模型：
- en: '[PRE55]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Over-sampling performed much better than under-sampling, but unless we were
    looking to maximize recall, we are better off sticking with our original strategy
    for k-NN:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 过采样的表现明显优于欠采样，但除非我们希望最大化召回率，否则最好还是坚持使用原先的 k-NN 策略：
- en: '[PRE56]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Note that since SMOTE is creating synthetic data, we must carefully consider
    the side effects this may have on our model. If we can't make the assumption that
    all the values of a given class are representative of the full spectrum of the
    population and that this won't change over time, we cannot expect SMOTE to work
    well.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 SMOTE 生成的是合成数据，我们必须仔细考虑这可能对模型带来的副作用。如果我们不能假设某一类的所有值都能代表整个群体的全面特征，并且这种特征不会随时间改变，那么我们不能期望
    SMOTE 会有效。
- en: Regularization
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: When working with regressions, we may look to add a penalty term to our regression
    equation to reduce overfitting by punishing certain decisions for coefficients
    made by the model; this is called **regularization**. We are looking for the coefficients
    that will minimize this penalty term. The idea is to shrink the coefficients toward
    zero for features that don't contribute much to reducing the error of the model.
    Some common techniques are ridge regression, LASSO (short for *Least Absolute
    Shrinkage and Selection Operator*) regression, and elastic net regression, which
    combines the LASSO and ridge penalty terms. Note that since these techniques rely
    on the magnitude of the coefficients, the data should be scaled beforehand.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行回归分析时，我们可能会向回归方程中添加一个惩罚项，以减少过拟合，方法是惩罚模型在系数选择上做出的某些决策，这称为**正则化**。我们希望找到能够最小化这个惩罚项的系数。其核心思想是将那些对减少模型误差贡献不大的特征的系数收缩到零。常见的正则化技术有岭回归、LASSO（最小绝对收缩和选择算子）回归和弹性网络回归，后者结合了
    LASSO 和岭回归惩罚项。需要注意的是，由于这些技术依赖于系数的大小，因此数据应在使用这些方法前进行缩放。
- en: '**Ridge regression**, also called **L2 regularization**, punishes high coefficients
    (![](img/Formula_10_002.png)) by adding the sum of the squares of the coefficients
    to the cost function (which regression looks to minimize when fitting), as per
    the following penalty term:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**岭回归**，也称为 **L2 正则化**，通过将系数的平方和加到代价函数中（回归在拟合时要最小化的目标），来惩罚高系数（![](img/Formula_10_002.png)），具体表现为以下的惩罚项：'
- en: '![](img/Formula_10_003.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_10_003.jpg)'
- en: This penalty term is also weighted by λ (lambda), which indicates how large
    the penalty will be. When this is zero, we have ordinary least squares regression,
    as before.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这个惩罚项也会乘以 λ（lambda），它表示惩罚的大小。当 λ 为零时，我们得到的是普通最小二乘回归，如之前所述。
- en: Important note
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Remember the `C` parameter from the `LogisticRegression` class? By default,
    the `LogisticRegression` class will use the L2 penalty term, where `C` is 1/λ.
    However, it also supports L1, but only with certain solvers.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得 `LogisticRegression` 类中的 `C` 参数吗？默认情况下，`LogisticRegression` 类将使用 L2 惩罚项，其中
    `C` 是 1/λ。不过，它也支持 L1 惩罚，但仅在某些求解器中可用。
- en: '**LASSO regression**, also called **L1 regularization**, drives coefficients
    to zero by adding the sum of the absolute values of the coefficients to the cost
    function. This is more robust than L2 regularization because it is less sensitive
    to extreme values:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**LASSO 回归**，也称为 **L1 正则化**，通过将系数的绝对值之和加到代价函数中，将系数压缩为零。与 L2 正则化相比，这种方法更具鲁棒性，因为它对极端值的敏感性较低：'
- en: '![](img/Formula_10_004.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_10_004.jpg)'
- en: Since LASSO drives coefficients of certain features in the regression to zero
    (where they won't contribute to the model), it is said to perform feature selection.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LASSO 回归将某些特征的系数压缩为零（即这些特征不会对模型产生贡献），因此它被认为执行了特征选择。
- en: Important note
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Both the L1 and L2 penalties are also referred to as **L1 and L2 norms** (a
    mathematical transformation on a vector to be in the range [0, ∞)) and written
    as ![](img/Formula_10_005.png) and ![](img/Formula_10_006.png), respectively.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: L1 和 L2 惩罚也被称为**L1 和 L2 范数**（对一个向量的数学变换，使其处于区间[0, ∞)内），分别表示为 ![](img/Formula_10_005.png)
    和 ![](img/Formula_10_006.png)。
- en: '**Elastic net regression** combines both LASSO and ridge penalty terms into
    the following penalty term, where we can tune both the strength of the penalty
    (λ) and the percentage of the penalty that is L1 (and consequently, the percentage
    that is L2) with α (alpha):'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性网络回归**结合了 LASSO 和岭回归惩罚项，形成了以下惩罚项，我们可以通过 α（alpha）来调节惩罚的强度（λ）以及 L1 惩罚所占的比例（从而也决定了
    L2 的比例）：'
- en: '![](img/Formula_10_007.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_10_007.jpg)'
- en: 'Scikit-learn implements ridge, LASSO, and elastic net regressions with the
    `Ridge`, `Lasso`, and `ElasticNet` classes, respectively, which can be used in
    the same way as the `LinearRegression` class. There is also a `CV` version of
    each of these (`RidgeCV`, `LassoCV`, and `ElasticNetCV`), which features built-in
    cross-validation. Using all the defaults for these models, we find that LASSO
    performs the best at predicting the length of the year in Earth days with the
    planet data:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn实现了岭回归、LASSO回归和弹性网回归，分别使用`Ridge`、`Lasso`和`ElasticNet`类，它们的使用方式与`LinearRegression`类相同。每种方法都有一个`CV`版本（`RidgeCV`、`LassoCV`和`ElasticNetCV`），具备内置的交叉验证功能。在使用这些模型的所有默认设置时，我们发现LASSO在使用行星数据预测地球年长度时表现最佳：
- en: '[PRE57]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note that these `scikit-learn` classes have an `alpha` parameter, which lines
    up with λ in the previous equations (not α). For `ElasticNet`, α in the equations
    lines up with the `l1_ratio` parameter, which defaults to 50% LASSO. In practice,
    both of these hyperparameters are determined with cross-validation.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些`scikit-learn`类具有一个`alpha`参数，它与前面方程中的λ（而不是α）对应。对于`ElasticNet`，方程中的α与`l1_ratio`参数对应，默认值为50%的LASSO。在实际应用中，这两个超参数都是通过交叉验证确定的。
- en: Summary
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we reviewed various techniques we can employ to improve model
    performance. We learned how to use grid search to find the best hyperparameters
    in a search space, and how to tune our model using the scoring metric of our choosing
    with `GridSearchCV`. This means we don't have to accept the default in the `score()`
    method of our model and can customize it to our needs.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了可以用来提升模型性能的各种技术。我们学习了如何使用网格搜索在搜索空间中找到最佳超参数，以及如何使用`GridSearchCV`根据我们选择的评分指标来调整模型。这意味着我们不必接受模型`score()`方法中的默认设置，而可以根据我们的需求进行自定义。
- en: In our discussion of feature engineering, we learned how to reduce the dimensionality
    of our data using techniques such as PCA and feature selection. We saw how to
    use the `PolynomialFeatures` class to add interaction terms to models with categorical
    and numerical features. Then, we learned how to use the `FeatureUnion` class to
    augment our training data with transformed features. In addition, we saw how decision
    trees can help us understand which features in the data contribute most to the
    classification or regression task at hand, using feature importances. This helped
    us see the importance of sulfur dioxide and chlorides in distinguishing between
    red and white wine on a chemical level, as well as the importance of a planet's
    semi-major axis in determining its period.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征工程的讨论中，我们学习了如何使用PCA和特征选择等技术来降低数据的维度。我们看到了如何使用`PolynomialFeatures`类为具有分类和数值特征的模型添加交互项。然后，我们学习了如何使用`FeatureUnion`类将转化后的特征添加到我们的训练数据中。此外，我们还了解了决策树如何通过特征重要性帮助我们理解数据中哪些特征对分类或回归任务的贡献最大。这帮助我们看到了二氧化硫和氯化物在化学层面区分红葡萄酒和白葡萄酒中的重要性，以及行星半长轴在确定其周期中的重要性。
- en: Afterward, we took a look at the random forest, gradient boosting, and voting
    classifiers to discuss ensemble methods and how they seek to address the bias-variance
    trade-off through bagging, boosting, and voting strategies. We also saw how to
    measure agreement between classifiers with Cohen's kappa score. This led us to
    examine our `white_or_red` wine classifier's confidence in its correct and incorrect
    predictions. Once we know the ins and outs of our model's performance, we can
    try to improve upon it through the appropriate ensemble method to capitalize on
    its strengths and mitigate its weaknesses.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们研究了随机森林、梯度提升和投票分类器，讨论了集成方法以及它们如何通过装袋、提升和投票策略解决偏差-方差权衡问题。我们还了解了如何使用Cohen's
    kappa评分衡量分类器之间的协议一致性。这样我们就能审视我们`white_or_red`葡萄酒分类器在正确和错误预测中的信心。一旦我们了解了模型表现的细节，就可以尝试通过适当的集成方法来改进模型，以发挥其优势并减少其弱点。
- en: After that, we learned how to use the `imblearn` package to implement over-
    and under-sampling strategies when faced with a class imbalance. We tried to use
    this to improve our ability to predict red wine quality scores. In this example,
    we got some exposure to the k-NN algorithm and the issues with modeling small
    datasets. Finally, we learned how we can use regularization to penalize high coefficients
    and reduce overfitting with regression, using ridge (L2 norm), LASSO (L1 norm),
    and elastic net penalties; remember, LASSO is often used as a method of feature
    selection since it drives coefficients to zero.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们学习了如何使用`imblearn`包在面对类别不平衡时实现过采样和欠采样策略。我们尝试使用这种方法来提高预测红酒质量评分的能力。在这个例子中，我们接触了k-NN算法以及处理小数据集建模时的问题。最后，我们学习了如何使用正则化来惩罚高系数，并通过岭回归（L2范数）、LASSO（L1范数）和弹性网惩罚来减少回归中的过拟合；记住，LASSO通常作为特征选择的方法，因为它会将系数压缩为零。
- en: In the next chapter, we will revisit the simulated login attempt data and use
    machine learning to detect anomalies. We will also see how we can apply both unsupervised
    and supervised learning in practice.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重新审视模拟的登录尝试数据，并使用机器学习来检测异常。我们还将看到如何在实践中应用无监督学习和监督学习。
- en: Exercises
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Complete the following exercises to practice the skills covered in this chapter.
    Be sure to consult the *Machine learning workflow* section in the *Appendix* as
    a refresher on the process of building models:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 完成以下练习，练习本章中涵盖的技能。务必查阅*附录*中的*机器学习工作流*部分，以便回顾构建模型的过程：
- en: 'Predict star temperature with elastic net linear regression as follows:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用弹性网线性回归预测恒星温度，如下所示：
- en: a) Using the `data/stars.csv` file, build a pipeline to normalize the data with
    a `MinMaxScaler` object and then run elastic net linear regression using all the
    numeric columns to predict the temperature of the star.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 使用`data/stars.csv`文件，构建一个流水线，先使用`MinMaxScaler`对象对数据进行归一化，然后使用所有数字列进行弹性网线性回归，预测恒星的温度。
- en: b) Run grid search on the pipeline to find the best values for `alpha`, `l1_ratio`,
    and `fit_intercept` for the elastic net in the search space of your choice.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 在流水线中进行网格搜索，以找到`alpha`、`l1_ratio`和`fit_intercept`在你选择的搜索空间中的最佳值。
- en: c) Train the model on 75% of the initial data.
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 在75%的初始数据上训练模型。
- en: d) Calculate the R2 of your model.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 计算模型的R2值。
- en: e) Find the coefficients for each regressor and the intercept.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 查找每个回归器的系数和截距。
- en: f) Visualize the residuals using the `plot_residuals()` function from the `ml_utils.regression`
    module.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f) 使用`ml_utils.regression`模块中的`plot_residuals()`函数可视化残差。
- en: 'Perform multiclass classification of white wine quality using a support vector
    machine and feature union as follows:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用支持向量机和特征联合执行白葡萄酒质量的多类分类，如下所示：
- en: a) Using the `data/winequality-white.csv` file, build a pipeline to standardize
    data, then create a feature union between interaction terms and a feature selection
    method of your choice from the `sklearn.feature_selection` module, followed by
    an SVM (use the `SVC` class).
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 使用`data/winequality-white.csv`文件，构建一个流水线来标准化数据，然后在`sklearn.feature_selection`模块中选择一个你喜欢的特征选择方法，接着创建交互项和特征选择方法的特征联合，并使用支持向量机（`SVC`类）。
- en: b) Run grid search on your pipeline with 85% of the data to find the best values
    for the `include_bias` parameter (`PolynomialFeatures`) and the `C` parameter
    (`SVC`) in the search space of your choosing with `scoring='f1_macro'`.
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 在85%的数据上运行网格搜索，找到`include_bias`参数（`PolynomialFeatures`）和`C`参数（`SVC`）在你选择的搜索空间中的最佳值，`scoring='f1_macro'`。
- en: c) Look at the classification report for your model.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 查看你模型的分类报告。
- en: d) Create a confusion matrix using the `confusion_matrix_visual()` function
    from the `ml_utils.classification` module.
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 使用`ml_utils.classification`模块中的`confusion_matrix_visual()`函数创建混淆矩阵。
- en: e) Plot a precision-recall curve for multiclass data using the `plot_multiclass_pr_curve()`
    function from the `ml_utils.classification` module.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 使用`ml_utils.classification`模块中的`plot_multiclass_pr_curve()`函数绘制多类数据的精度-召回曲线。
- en: 'Perform multiclass classification of white wine quality using k-NN and over-sampling
    as follows:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用k-NN和过采样执行白葡萄酒质量的多类分类，如下所示：
- en: a) Using the `data/winequality-white.csv` file, create a test and training set
    with 85% of the data in the training set. Stratify on `quality`.
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 使用`data/winequality-white.csv`文件，创建一个包含85%数据的训练集和测试集。按照`quality`进行分层。
- en: b) With `imblearn`, use the `RandomOverSampler` class to over-sample the minority
    quality scores.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 使用`imblearn`中的`RandomOverSampler`类对少数类质量分数进行过采样。
- en: c) Build a pipeline to standardize data and run k-NN.
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 构建一个管道来标准化数据并运行k-NN。
- en: d) Run grid search on your pipeline with the over-sampled data on the search
    space of your choosing to find the best value for k-NN's `n_neighbors` parameter
    with `scoring='f1_macro'`.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 在你的管道上使用过采样的数据进行网格搜索，选择一个搜索空间，以找到k-NN的`n_neighbors`参数的最佳值，并使用`scoring='f1_macro'`。
- en: e) Look at the classification report for your model.
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 查看你的模型的分类报告。
- en: f) Create a confusion matrix using the `confusion_matrix_visual()` function
    from the `ml_utils.classification` module.
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f) 使用`ml_utils.classification`模块中的`confusion_matrix_visual()`函数创建混淆矩阵。
- en: g) Plot a precision-recall curve for multiclass data using the `plot_multiclass_pr_curve()`
    function from the `ml_utils.classification` module.
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: g) 使用`ml_utils.classification`模块中的`plot_multiclass_pr_curve()`函数为多类数据绘制精确度-召回率曲线。
- en: Can wine type (red or white) help determine the quality score?
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 葡萄酒类型（红葡萄酒或白葡萄酒）能否帮助确定质量分数？
- en: a) Using the `data/winequality-white.csv` and `data/winequality-red.csv` files,
    create a dataframe with the concatenated data and a column indicating which wine
    type the data belongs to (red or white).
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 使用`data/winequality-white.csv`和`data/winequality-red.csv`文件，创建一个包含连接数据的数据框，并添加一个列，指示数据属于哪种葡萄酒类型（红葡萄酒或白葡萄酒）。
- en: b) Create a test and training set with 75% of the data in the training set.
    Stratify on `quality`.
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 创建一个测试集和训练集，其中75%的数据用于训练集。按照`quality`进行分层抽样。
- en: c) Build a pipeline using a `ColumnTransformer` object to standardize the numeric
    data while one-hot encoding the wine type column (something like `is_red` and
    `is_white`, each with binary values), and then train a random forest.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 使用`ColumnTransformer`对象构建一个管道，对数值数据进行标准化，同时对葡萄酒类型列进行独热编码（类似于`is_red`和`is_white`，每个列包含二值），然后训练一个随机森林模型。
- en: d) Run grid search on your pipeline with the search space of your choosing to
    find the best value for the random forest's `max_depth` parameter with `scoring='f1_macro'`.
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 在你的管道上运行网格搜索，选择一个搜索空间，以找到随机森林的`max_depth`参数的最佳值，并使用`scoring='f1_macro'`。
- en: e) Take a look at the feature importances from the random forest.
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 查看随机森林的特征重要性。
- en: f) Look at the classification report for your model.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f) 查看你的模型的分类报告。
- en: g) Plot a ROC curve for multiclass data using the `plot_multiclass_roc()` function
    from the `ml_utils.classification` module.
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: g) 使用`ml_utils.classification`模块中的`plot_multiclass_roc()`函数为多类数据绘制ROC曲线。
- en: h) Create a confusion matrix using the `confusion_matrix_visual()` function
    from the `ml_utils.classification` module.
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: h) 使用`ml_utils.classification`模块中的`confusion_matrix_visual()`函数创建混淆矩阵。
- en: 'Make a multiclass classifier to predict wine quality with majority rules voting
    by performing the following steps:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个多类分类器，通过以下步骤使用多数规则投票来预测葡萄酒质量：
- en: a) Using the `data/winequality-white.csv` and `data/winequality-red.csv` files,
    create a dataframe with concatenated data and a column indicating which wine type
    the data belongs to (red or white).
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 使用`data/winequality-white.csv`和`data/winequality-red.csv`文件，创建一个包含连接数据的数据框，并添加一个列，指示数据属于哪种葡萄酒类型（红葡萄酒或白葡萄酒）。
- en: b) Create a test and training set with 75% of the data in the training set.
    Stratify on `quality`.
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 创建一个测试集和训练集，其中75%的数据用于训练集。按照`quality`进行分层抽样。
- en: 'c) Build a pipeline for each of the following models: random forest, gradient
    boosting, k-NN, logistic regression, and Naive Bayes (`GaussianNB`). The pipeline
    should use a `ColumnTransformer` object to standardize the numeric data while
    one-hot encoding the wine type column (something like `is_red` and `is_white`,
    each with binary values), and then build the model. Note that we will discuss
    Naive Bayes in [*Chapter 11*](B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237),
    *Machine Learning Anomaly Detection*.'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 为以下每个模型构建一个管道：随机森林、梯度提升、k-NN、逻辑回归和朴素贝叶斯（`GaussianNB`）。该管道应使用`ColumnTransformer`对象对数值数据进行标准化，同时对葡萄酒类型列进行独热编码（类似于`is_red`和`is_white`，每个列包含二值），然后构建模型。请注意，我们将在[*第11章*](B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237)中讨论朴素贝叶斯，*机器学习异常检测*。
- en: 'd) Run grid search on each pipeline except Naive Bayes (just run `fit()` on
    it) with `scoring=''f1_macro''` on the search space of your choosing to find the
    best values for the following:'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 在每个管道上运行网格搜索，除了朴素贝叶斯（对其运行`fit()`即可），在你选择的搜索空间上使用`scoring='f1_macro'`来查找以下参数的最佳值：
- en: i) `max_depth`
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: i) `max_depth`
- en: ii) `max_depth`
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ii) `max_depth`
- en: iii) `n_neighbors`
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: iii) `n_neighbors`
- en: iv) `C`
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: iv) `C`
- en: e) Find the level of agreement between each pair of two models using the `cohen_kappa_score()`
    function from the `metrics` module in `scikit-learn`. Note that you can get all
    the combinations of the two easily using the `combinations()` function from the
    `itertools` module in the Python standard library.
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`scikit-learn`中的`metrics`模块里的`cohen_kappa_score()`函数找出每对模型之间的一致性水平。请注意，您可以通过Python标准库中的`itertools`模块的`combinations()`函数轻松获取所有的组合。
- en: f) Build a voting classifier with the five models built using majority rules
    (`voting='hard'`) and weighting the Naive Bayes model half as much as the others.
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 构建一个投票分类器，使用五个模型并基于多数规则（`voting='hard'`）进行构建，同时将朴素贝叶斯模型的权重设置为其他模型的一半。
- en: g) Look at the classification report for your model.
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查看模型的分类报告。
- en: h) Create a confusion matrix using the `confusion_matrix_visual()` function
    from the `ml_utils.classification` module.
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用来自`ml_utils.classification`模块的`confusion_matrix_visual()`函数创建混淆矩阵。
- en: Further reading
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Check out the following resources for more information on the topics covered
    in this chapter:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 查阅以下资源以获取更多关于本章涉及主题的信息：
- en: '*A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning*:
    [https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*梯度提升算法在机器学习中的温和入门*：[https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)'
- en: '*A Kaggler''s Guide to Model Stacking in Practice*: [https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/](https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kaggler实践中的模型堆叠指南*：[https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/](https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)'
- en: '*Choosing the right estimator*: [https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*选择合适的估算器*：[https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)'
- en: '*Cross-validation: evaluating estimator performance*: [https://scikit-learn.org/stable/modules/cross_validation.html](https://scikit-learn.org/stable/modules/cross_validation.html)'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*交叉验证：评估估算器的性能*：[https://scikit-learn.org/stable/modules/cross_validation.html](https://scikit-learn.org/stable/modules/cross_validation.html)'
- en: '*Decision Trees in Machine Learning*: [https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052)'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*决策树在机器学习中的应用*：[https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052)'
- en: '*Ensemble Learning to Improve Machine Learning Results*: [https://blog.statsbot.co/ensemble-learning-d1dcd548e936](https://blog.statsbot.co/ensemble-learning-d1dcd548e936)'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集成学习提升机器学习结果*：[https://blog.statsbot.co/ensemble-learning-d1dcd548e936](https://blog.statsbot.co/ensemble-learning-d1dcd548e936)'
- en: '*Ensemble Methods*: [https://scikit-learn.org/stable/modules/ensemble.html](https://scikit-learn.org/stable/modules/ensemble.html)'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集成方法*：[https://scikit-learn.org/stable/modules/ensemble.html](https://scikit-learn.org/stable/modules/ensemble.html)'
- en: '*Feature Engineering Made Easy by Divya Susarla and Sinan Ozdemir*: [https://www.packtpub.com/big-data-and-business-intelligence/feature-engineering-made-easy](https://www.packtpub.com/big-data-and-business-intelligence/feature-engineering-made-easy)'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Divya Susarla 和 Sinan Ozdemir的特征工程简易教程*：[https://www.packtpub.com/big-data-and-business-intelligence/feature-engineering-made-easy](https://www.packtpub.com/big-data-and-business-intelligence/feature-engineering-made-easy)'
- en: '*Feature Selection*: [https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection)'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征选择*：[https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection)'
- en: '*Gradient Boosting vs Random Forest*: [https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80](mailto:https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80)'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*梯度提升与随机森林*：[https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80](mailto:https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80)'
- en: '*Hyperparameter Optimization in Machine Learning*: [https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models](https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models)'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习中的超参数优化*: [https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models](https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models)'
- en: '*L1 Norms versus L2 Norms*: [https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms](https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms)'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L1范数与L2范数*: [https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms](https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms)'
- en: '*Modern Machine Learning Algorithms: Strengths and Weaknesses*: [https://elitedatascience.com/machine-learning-algorithms](https://elitedatascience.com/machine-learning-algorithms)'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*现代机器学习算法：优势与劣势*: [https://elitedatascience.com/machine-learning-algorithms](https://elitedatascience.com/machine-learning-algorithms)'
- en: '*Principal component analysis*: [https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主成分分析*: [https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)'
- en: '*Regularization in Machine Learning*: [https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习中的正则化*: [https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)'
- en: '*The Elements of Statistical Learning by Jerome H. Friedman, Robert Tibshirani,
    and Trevor Hastie*: [https://web.stanford.edu/~hastie/ElemStatLearn/](https://web.stanford.edu/~hastie/ElemStatLearn/)'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*由Jerome H. Friedman、Robert Tibshirani和Trevor Hastie撰写的统计学习要素*: [https://web.stanford.edu/~hastie/ElemStatLearn/](https://web.stanford.edu/~hastie/ElemStatLearn/)'
