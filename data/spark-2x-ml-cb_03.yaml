- en: Spark&#x27;s Three Data Musketeers for Machine Learning - Perfect Together
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的三大数据武士-完美搭档
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章中，我们将涵盖以下内容： '
- en: Creating RDDs with Spark 2.0 using internal data sources
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内部数据源创建Spark 2.0的RDDs
- en: Creating RDDs with Spark 2.0 using external data sources
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外部数据源创建Spark 2.0的RDDs
- en: Transforming RDDs with Spark 2.0 using the filter() API
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark 2.0的filter() API转换RDDs
- en: Transforming RDDs with the super useful flatMap() API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用超级有用的flatMap() API转换RDD
- en: Transforming RDDs with set operation APIs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集合操作API转换RDDs
- en: RDD transformation/aggregation with groupBy() and reduceByKey()
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用groupBy()和reduceByKey()进行RDD转换/聚合
- en: Transforming RDDs with the zip() API
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用zip() API转换RDDs
- en: Join transformation with paired key-value RDDs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配对键值RDD进行连接转换
- en: Reduce and grouping transformation with paired key-value RDDs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配对键值RDD进行减少和分组转换
- en: Creating DataFrames from Scala data structures
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Scala数据结构创建数据框
- en: Operating on DataFrames programmatically without SQL
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在没有SQL的情况下以编程方式操作数据框
- en: Loading DataFrames and setup from an external source
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从外部源加载数据框和设置
- en: Using DataFrames with standard SQL language - SparkSQL
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标准SQL语言的数据框- SparkSQL
- en: Working with the Dataset API using a Scala sequence
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scala序列使用数据集API
- en: Creating and using Datasets from RDDs and back again
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从RDDs创建和使用数据集，然后再次转换
- en: Working with JSON using the Dataset API and SQL together
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据集API和SQL一起处理JSON
- en: Functional programming with the Dataset API using domain objects
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用领域对象使用数据集API进行函数式编程
- en: Introduction
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The three workhorses of Spark for efficient processing of data at scale are
    RDD, DataFrames, and the Dataset API. While each can stand on its own merit, the
    new paradigm shift favors Dataset as the unifying data API to meet all data wrangling
    needs in a single interface.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的三大工作马是RDD、数据框架和数据集API，用于高效处理规模化数据。虽然每个都有其自身的优点，但新的范式转变更青睐数据集作为统一的数据API，以满足单一接口中的所有数据整理需求。
- en: 'The new Spark 2.0 Dataset API is a type-safe collection of domain objects that
    can be operated on via transformation (similar to RDDs'' filter, `map`, `flatMap()`,
    and so on) in parallel using functional or relational operations. For backward
    compatibility, Dataset has a view called **DataFrame**, which is a collection
    of rows that are untyped. In this chapter, we demonstrate all three API sets.
    The figure ahead summarizes the pros and cons of the key components of Spark for
    data wrangling:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 新的Spark 2.0数据集API是一组类型安全的领域对象集合，可以通过转换（类似于RDD的filter、`map`、`flatMap()`等）并行使用功能或关系操作。为了向后兼容，数据集有一个名为**DataFrame**的视图，它是一组无类型的行。在本章中，我们演示了所有三个API集。前面的图总结了Spark数据整理的关键组件的优缺点：
- en: '![](img/00060.gif)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00060.gif)'
- en: An advanced developer in machine learning must understand and be able to use
    all three API sets without any issues, for algorithmic augmentation or legacy
    reasons. While we recommend that every developer should migrate toward the high-level
    Dataset API, you will still need to know RDDs for programming against the Spark
    core system. For example, it is very common for investment banking and hedge funds
    to read leading journals in machine learning, mathematical programming, finance,
    statistics, or artificial intelligence and then code the research in low-level
    APIs to gain competitive advantage.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的高级开发人员必须理解并能够无障碍地使用所有三个API集，用于算法增强或遗留原因。虽然我们建议每个开发人员都应该向高级数据集API迁移，但您仍然需要了解RDDs，以针对Spark核心系统进行编程。例如，投资银行和对冲基金经常阅读机器学习、数学规划、金融、统计或人工智能领域的领先期刊，然后使用低级API编写研究以获得竞争优势。
- en: RDDs - what started it all...
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDDs-一切的开始...
- en: The RDD API is a critical toolkit for Spark developers since it favors low-level
    control over the data within a functional programming paradigm. What makes RDDs
    powerful also makes it harder to work with for new programmers. While it may be
    easy to understand the RDD API and manual optimization techniques (for example,
    `filter()` before a `groupBy()` operation), writing advanced code would require
    consistent practice and fluency.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: RDD API是Spark开发人员的关键工具包，因为它偏向于在函数式编程范式中对数据进行低级控制。RDD强大的地方也使得新手程序员更难使用。虽然理解RDD
    API和手动优化技术（例如，在`groupBy()`操作之前的`filter()`）可能很容易，但编写高级代码需要持续的练习和流利。
- en: When data files, blocks, or data structures are converted to RDDs, the data
    is broken down into smaller units called **partitions** (similar to splits in
    Hadoop) and distributed among the nodes so they can be operated on in parallel
    at the same time. Spark provides this functionality right out of the box at scale
    without any additional coding. The framework will take care of all the details
    for you and you can concentrate on writing code without worrying about the data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据文件、块或数据结构转换为RDD时，数据被分解成称为**分区**的较小单元（类似于Hadoop中的拆分），并分布在节点之间，以便它们可以同时并行操作。Spark提供了这种功能，可以在规模上立即使用，无需额外编码。框架会为您处理所有细节，您可以专注于编写代码，而不必担心数据。
- en: 'To appreciate the genius and yet the elegance of the underlying RDDs, one must
    read the original paper on this subject, which was deemed as the best work on
    this subject. The paper can be accessed here:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要欣赏底层RDD的天才和优雅，必须阅读这个主题的原始论文，这被认为是这个主题上的最佳作品。可以在这里访问论文：
- en: '[https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)'
- en: There are many types of RDDs in Spark that can simplify programming. The following
    mind map depicts a partial taxonomy of RDDs. It is suggested that a programmer
    on Spark know the types of RDDs available out of the box at minimum, even the
    less-known ones such as **RandomRDD** ,**VertexRDD**, **HadoopRDD**, **JdbcRDD**,
    and **UnionRDD**, in order to avoid unnecessary coding.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中有许多类型的RDD可以简化编程。下面的思维导图描述了RDD的部分分类。建议Spark上的程序员至少了解可用的RDD类型，甚至是不太知名的类型，如**RandomRDD**、**VertexRDD**、**HadoopRDD**、**JdbcRDD**和**UnionRDD**，以避免不必要的编码。
- en: '![](img/00061.gif)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00061.gif)'
- en: DataFrame - a natural evolution to unite API and SQL via a high-level API
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame - 通过高级API将API和SQL统一的自然演变。
- en: The Spark developer community has always strived to provide an easy-to-use high-level
    API for the community starting from the AMPlab days at Berkley. The next evolution
    in the Data API materialized when Michael Armbrust gave the community the SparkSQL
    and Catalyst optimizer, which made data virtualization possible with Spark using
    a simple and well-understood SQL interface. The DataFrame API was a natural evolution
    to take advantage of SparkSQL by organizing data into named columns like relational
    tables.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Spark开发人员社区一直致力于为社区提供易于使用的高级API，从伯克利的AMPlab时代开始。数据API的下一个演变是当Michael Armbrust为社区提供了SparkSQL和Catalyst优化器，这使得使用简单且广为人知的SQL接口在Spark中实现了数据虚拟化。DataFrame
    API是利用SparkSQL的自然演变，通过将数据组织成命名列来利用SparkSQL。
- en: The DataFrame API made data wrangling via SQL available to a multitude of data
    scientists and developers familiar with DataFrames in R (data.frame) or Python/Pandas
    (pandas.DataFrame).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API使得通过SQL进行数据整理对于许多熟悉R（data.frame）或Python/Pandas（pandas.DataFrame）的数据科学家和开发人员变得可行。
- en: Dataset - a high-level unifying Data API
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集 - 一个高级统一的数据API
- en: A dataset is an immutable collection of objects which are modelled/mapped to
    a traditional relational schema. There are four attributes that distinguish it
    as the preferred method going forward. We particularly find the Dataset API appealing
    since we find it familiar to RDDs with the usual transformational operators (for
    example, `filter()`, `map()`, `flatMap()`, and so on). The Dataset will follow
    a lazy execution paradigm similar to RDD. The best way to try to reconcile DataFrames
    and Datasets is to think of a DataFrame as an alias that can be thought of as
    `Dataset[Row]`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是一个不可变的对象集合，它被建模/映射到传统的关系模式。有四个属性使其成为未来首选的方法。我们特别喜欢数据集API，因为我们发现它与RDD非常相似，具有通常的转换操作符（例如`filter()`、`map()`、`flatMap()`等）。数据集将遵循类似RDD的延迟执行范式。试图调和DataFrame和DataSet的最佳方法是将DataFrame视为可以被视为`Dataset[Row]`的别名。
- en: '**Strong type safety**: We now have both compile-time (syntax errors) and runtime
    safety in a unified Data API, which helps the ML developer not only during development,
    but can also help guard against mishaps during runtime. Developers hit by unexpected
    runtime errors using DataFrame or RDD Lambda either in Scala or Python (due to
    flaws in data) will better understand and appreciate this new contribution from
    the Spark community and Databricks ([https://databricks.com](https://databricks.com)).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强类型安全**：我们现在在统一的数据API中既有编译时（语法错误）又有运行时安全，这有助于机器学习开发人员不仅在开发过程中，还可以在运行时防范意外。由于数据中的缺陷而在Scala或Python中使用DataFrame或RDD
    Lambda遇到意外运行时错误的开发人员将更好地理解和欣赏来自Spark社区和Databricks（[https://databricks.com](https://databricks.com)）的这一新贡献。'
- en: '**Tungsten Memory Management enabled**: Tungsten brings Apache Spark closer
    to bare metal (that is, leveraging the `sun.misc.Unsafe interface`). The encoders
    facilitate mapping of JVM objects to tabular format (see the following figure).
    If you use the Dataset API, Spark will map the JVM objects to internal Tungsten
    off-heap binary format, which is more efficient. While the details of Tungsten
    internals are beyond the scope of a cookbook on machine learning, it is worth
    mentioning that the benchmarking shows significant improvement using off-head
    memory management versus JVM objects. It is noteworthy to mention that the concept
    of off-heap memory management has always been intrinsic in Apache Flink before
    it became available in Spark. Spark developers realized the importance of project
    Tungsten since Spark 1.4, 1.5, and 1.6 to its current state in Spark 2.0+. Again,
    we emphasize that even though DataFrame will be supported as of writing this,
    and has been covered in detail (most prod systems are still pre-Spark 2.0), we
    encourage you to start thinking in the Dataset paradigm. The following figure
    shows how RDD, DataFrame, and DataSet relate to the project Tungsten evolutionary
    roadmap:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**启用Tungsten内存管理**：Tungsten使Apache Spark更加接近裸金属（即利用`sun.misc.Unsafe`接口）。编码器便于将JVM对象映射到表格格式（见下图）。如果您使用数据集API，Spark将JVM对象映射到内部Tungsten堆外二进制格式，这更加高效。虽然Tungsten内部的细节超出了机器学习食谱的范围，但值得一提的是，基准测试显示使用堆外内存管理与JVM对象相比有显著改进。值得一提的是，在Spark中可用之前，堆外内存管理的概念在Apache
    Flink中一直是内在的。Spark开发人员意识到了Tungsten项目的重要性，自Spark 1.4、1.5和1.6以来，一直到Spark 2.0+的当前状态。再次强调，尽管在撰写本文时DataFrame将得到支持，并且已经详细介绍过（大多数生产系统仍然是Spark
    2.0之前的版本），我们鼓励您开始思考数据集范式。下图显示了RDD、DataFrame和DataSet与Tungsten项目的演进路线之间的关系：'
- en: '![](img/00062.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00062.jpeg)'
- en: '**Encoders**: Encoders are Spark''s serialization and deserialization (that
    is, SerDe) framework in Spark 2.0\. Encoders seamlessly handle the mapping of
    JVM objects to tabular format that you can get under the cover and modify if desired
    (expert level).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：编码器是Spark 2.0中的Spark序列化和反序列化（即SerDe）框架。编码器无缝处理将JVM对象映射到表格格式的操作，您可以在底层获取并根据需要进行修改（专家级别）。'
- en: Unlike standard Java serialization and other serialization schemes (for example,
    Kryo), the encoders do not use runtime reflection to discover object internals
    to serialize on the fly. Instead, encoder code is generated and compiled during
    compile time to bytecode for a given object, which will result in much faster
    operation (no reflection is used) to serialize and de-serialize the object. The
    reflection at runtime for object internals (for example, lookup of fields and
    their format) imposes extra overhead that is not present using Spark 2.0\. The
    ability to use Kryo, standard java serialization, or any other serialization technique
    still remains an option (edge cases and backward compatibility) if needed.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与标准Java序列化和其他序列化方案（例如Kryo）不同，编码器不使用运行时反射来动态发现对象内部以进行序列化。相反，编码器代码在编译时为给定对象生成并编译为字节码，这将导致更快的操作（不使用反射）来序列化和反序列化对象。运行时的反射对象内部（例如，查找字段及其格式）会带来额外的开销，在Spark
    2.0中不存在。如果需要，仍然可以使用Kryo、标准java序列化或任何其他序列化技术（边缘情况和向后兼容性）。
- en: The encoders for standard data types and objects (made of standard data types)
    are available in Tungsten out of the box. Using a quick informal program benchmark,
    serializing objects back and forth using Kryo serialization, which is popular
    with Hadoop MapReduce developers, versus encoders, revealed a significant 4x to
    8x improvement. When we looked at the source code and probed under the covers,
    we realized that the encoders actually use runtime code generation (at bytecode
    level!) to pack and unpack objects. For completeness, we mention that the objects
    also seemed to be smaller, but further details and the reasons as to why it is
    so, is beyond the scope of this book.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准数据类型和对象（由标准数据类型制成）的编码器在Tungsten中是开箱即用的。使用快速非正式的程序基准测试，使用Hadoop MapReduce开发人员广泛使用的Kryo序列化来回序列化对象，与编码器相比，发现了显着的4倍到8倍的改进。当我们查看源代码并深入了解后，我们意识到编码器实际上使用运行时代码生成（在字节码级别！）来打包和解包对象。为了完整起见，我们提到对象似乎也更小，但更多细节以及为什么会这样的原因超出了本书的范围。
- en: The Encoder[T] is an internal artifact made of the DataSet[T], which is just
    a schema of records. You can create your own custom encoders as needed in Scala
    using tuples of underlying data (for example, Long, Double, and Int). Before you
    embark on the custom encoder journey (for example, want to store custom objects
    in DataSet[T]), make sure you take a look at `[Encoders.scala](https://github.com/apache/spark/blob/v2.0.0/sql/catalyst/src/main/scala/org/apache/spark/sql/Encoders.scala#L270-L316)`
    and `[SQLImplicits.scala](https://github.com/apache/spark/blob/v2.0.0/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala#L77-L96)`
    in Spark's source directory. The plan and strategic direction for Spark is to
    provide a public API in future releases.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Encoder[T]是DataSet[T]的内部构件，它只是记录的模式。您可以根据需要在Scala中使用底层数据的元组（例如Long、Double和Int）创建自定义编码器。在开始自定义编码器之前（例如，想要在DataSet[T]中存储自定义对象），请确保查看Spark源目录中的`[Encoders.scala](https://github.com/apache/spark/blob/v2.0.0/sql/catalyst/src/main/scala/org/apache/spark/sql/Encoders.scala#L270-L316)`和`[SQLImplicits.scala](https://github.com/apache/spark/blob/v2.0.0/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala#L77-L96)`。Spark的计划和战略方向是在未来的版本中提供一个公共API。
- en: '**Catalyst optimizer friendly**: Using Catalyst, the API gestures are translated
    into logical query plans which use a catalog (user-defined functions) and ultimately
    translate the logical plan to a physical plan, which is often much more efficient
    than proposed by the original scheme (even if you try to put `groupBy()` before
    `filter()`, it is smart enough to arrange it the other way around). For better
    clarity, see the following figure:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Catalyst优化器友好**：使用Catalyst，API手势被转换为使用目录（用户定义的函数）的逻辑查询计划，并最终将逻辑计划转换为物理计划，这通常比原始方案提出的更有效（即使您尝试在`filter()`之前使用`groupBy()`，它也足够聪明地安排它的顺序）。为了更好地理解，请参见下图：'
- en: '![](img/00063.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00063.jpeg)'
- en: 'Noteworthy for pre-Spark 2.0 users:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Spark 2.0之前的用户值得注意：
- en: '`SparkSession` is now the single entry point into the system. SQLContext and
    HiveContext are replaced by SparkSession.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SparkSession`现在是系统的唯一入口点。SQLContext和HiveContext被SparkSession取代。'
- en: For Java users, be sure to replace DataFrame with `Dataset<Row>`
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Java用户，请确保将DataFrame替换为`Dataset<Row>`
- en: Use the new catalog interface via `SparkSession` to execute `cacheTable()`,
    `dropTempView()`, `createExternalTable()`, `ListTable()`, and so on.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`SparkSession`使用新的目录接口执行`cacheTable()`、`dropTempView()`、`createExternalTable()`、`ListTable()`等。
- en: 'DataFrame and DataSet API:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame和DataSet API：
- en: '`unionALL()` is deprecated; you should use now `union()`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unionALL()`已被弃用；现在应该使用`union()`'
- en: '`explode()` should be replaced by `functions.explode()` plus `select()` or
    `flatMap()`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`explode()`应该被`functions.explode()`加`select()`或`flatMap()`替换'
- en: '`registerTempTable` has been deprecated and replaced by `createOrReplaceTempView()`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`registerTempTable`已被弃用，替换为`createOrReplaceTempView()`'
- en: Creating RDDs with Spark 2.0 using internal data sources
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用内部数据源在Spark 2.0中创建RDD
- en: There are four ways to create RDDs in Spark. They range from the `parallelize()`
    method for simple testing and debugging within the client driver code to streaming
    RDDs for near-realtime responses. In this recipe, we provide you with several
    examples to demonstrate RDD creation using internal sources. The streaming case
    will be covered in the streaming Spark example in [Chapter 13](part0538.html#G12EK0-4d291c9fed174a6992fd24938c2f9c77),
    *Streaming Machine Learning System*, so we can address it in a meaningful way.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中有四种创建RDD的方法。它们从`parallelize()`方法用于在客户端驱动程序代码中进行简单测试和调试到流式RDD，用于近实时响应。在本教程中，我们提供了几个示例来演示使用内部源创建RDD。流式情况将在[第13章](part0538.html#G12EK0-4d291c9fed174a6992fd24938c2f9c77)中的流式Spark示例中进行介绍，因此我们可以以有意义的方式来解决它。
- en: How to do it...
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the necessary packages:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入设置`log4j`日志级别的包。这一步是可选的，但我们强烈建议这样做（随着开发周期的推移，适当更改级别）。
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置日志级别为警告和错误，以减少输出。有关包要求，请参阅上一步。
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Set up the Spark context and application parameter so Spark can run:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We declare two local data structures to hold the data prior to using any distributed
    RDDs. It should be noted that the data here will be held in the driver's heap
    space via local data structures. We make an explicit mention here, due to the
    multitude of problems programmers encounter when using large data sets for testing
    using the `parallelize()` technique. Ensure that you have enough space to hold
    the data locally in the driver if you use this technique.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们声明两个本地数据结构来保存数据，然后再使用任何分布式RDD。需要注意的是，这里的数据将通过本地数据结构保存在驱动程序的堆空间中。我们在这里明确提到，因为程序员在使用`parallelize()`技术进行大数据集测试时会遇到多种问题。如果使用这种技术，请确保驱动程序本地有足够的空间来保存数据。
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We use the `parallelize()` function to take the local data and distribute it
    across the cluster.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`parallelize()`函数将本地数据分发到集群中。
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s take a look at the difference between the two data structures as seen
    by Spark. This can be done by printing the two data structure handles: a local
    array and a cluster parallel collection (that is, RDD).'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看Spark如何看待这两种数据结构的区别。可以通过打印两个数据结构句柄来完成：一个本地数组和一个集群并行集合（即RDD）。
- en: 'The output will be as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Spark tries to set the number of partitions (that is, splits in Hadoop) itself
    based on the configuration of the cluster, but there are times when we need to
    set the number of partitions manually. The `parallelize()` function offers a second
    parameter that allows you to set the number of partitions manually.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark尝试根据集群的配置自动设置分区数（即Hadoop中的分区），但有时我们需要手动设置分区数。`parallelize()`函数提供了第二个参数，允许您手动设置分区数。
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output will be as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the first two lines, Spark has chosen two partitions by default, and, in
    the next two lines, we have set the number of partitions to 4 and 8, respectively.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两行中，Spark默认选择了两个分区，接下来两行中，我们分别将分区数设置为4和8。
- en: How it works...
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The data held in the client driver is parallelized and distributed across the
    cluster using the number of portioned RDDs (the second parameter) as the guideline.
    The resulting RDD is the magic of Spark that started it all (refer to Matei Zaharia's
    original white paper).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端驱动程序中保存的数据被并行化并分布到集群中，使用分区RDD的数量（第二个参数）作为指导。生成的RDD是Spark的魔力，它启动了一切（参考Matei
    Zaharia的原始白皮书）。
- en: The resulting RDDs are now fully distributed data structures with fault tolerance
    and lineage that can be operated on in parallel using Spark framework.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的RDD现在是完全分布式的数据结构，具有容错性和血统，可以使用Spark框架并行操作。
- en: We read a text file `A Tale of Two Cities by Charles Dickens` from [http://www.gutenberg.org/](http://www.gutenberg.org/) into
    Spark RDDs. We then proceed to split and tokenize the data and print the number
    of total words using Spark's operators (for example, `map`, `flatMap()`, and so
    on).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[http://www.gutenberg.org/](http://www.gutenberg.org/)读取了查尔斯·狄更斯的《双城记》文本文件到Spark
    RDD中。然后我们继续拆分和标记数据，并使用Spark的操作符（例如`map`，`flatMap()`等）打印总词数。
- en: Creating RDDs with Spark 2.0 using external data sources
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用外部数据源使用Spark 2.0创建RDD
- en: In this recipe, we provide you with several examples to demonstrate RDD creation
    using external sources.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们提供了几个示例来演示使用外部来源创建RDD。
- en: How to do it...
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Import the necessary packages:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入设置`log4j`日志级别的包。这一步是可选的，但我们强烈建议这样做（随着开发周期的推移，适当更改级别）。
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置日志级别为警告和错误，以减少输出。有关包要求，请参阅上一步。
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行。
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We obtain the data from the Gutenberg project. This is a great source for accessing
    actual text, ranging from the complete works of *Shakespeare* to *Charles Dickens*.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从古腾堡计划获取数据。这是一个获取实际文本的好来源，从*莎士比亚*的完整作品到*查尔斯·狄更斯*。
- en: 'Download the text from the following sources and store it in your local directory:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下来源下载文本并将其存储在本地目录中：
- en: 'Source: [http://www.gutenberg.org](http://www.gutenberg.org)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来源：[http://www.gutenberg.org](http://www.gutenberg.org)
- en: 'Selected book: *A Tale of Two Cities by Charles Dickens*'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择的书籍：*查尔斯·狄更斯的《双城记》*
- en: 'URL: [http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: URL：[http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt)
- en: Once again, we use `SparkContext`, available via `SparkSession`, and its function
    `textFile()` to read the external data source and parallelize it across the cluster.
    Remarkably, all the work is done for the developer behind the scenes by Spark
    using one single call to load a wide variety of formats (for example, text, S3,
    and HDFS), which parallelizes the data across the cluster using the `protocol:filepath`
    combination.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次使用`SparkContext`，通过`SparkSession`可用，并使用其`textFile()`函数来读取外部数据源，并在集群中并行化。值得注意的是，Spark在幕后使用一次单一调用来加载各种格式（例如文本、S3和HDFS），并使用`protocol:filepath`组合将数据并行化到集群中，为开发人员完成了所有工作。
- en: To demonstrate, we load the book, which is stored as ASCII, text using the `textFile()`
    method from `SparkContext` via `SparkSession`, which, in turn goes to work behind
    the scenes and creates portioned RDDs across the cluster.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了演示，我们使用`textFile()`方法从`SparkContext`通过`SparkSession`读取存储为ASCII文本的书籍，然后在幕后创建跨集群的分区RDDs。
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output will be as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Even though we have not covered the Spark transformation operator, we'll look
    at a small code snippet which will break the file into words using blanks as a
    separator. In a real-life situation, a regular expression will be needed to cover
    all the edge cases with all the whitespace variations (refer to the *Transforming
    RDDs with Spark using filter() APIs* recipe in this chapter).
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管我们还没有涵盖Spark转换运算符，但我们将看一个小的代码片段，它将使用空格将文件分割成单词。在现实生活中，需要使用正则表达式来处理所有的边缘情况和所有的空格变化（参考本章中的*使用filter()
    API转换RDDs*配方）。
- en: We use a lambda function to receive each line as it is read and split it into
    words using blanks as separator.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用一个lambda函数来接收每一行，并使用空格作为分隔符将其分割成单词。
- en: We use a flatMap to break the array of lists of words (that is, each group of
    words from a line corresponds to a distinct array/list for that line). In short,
    what we want is a list of words and not a list of a list of words for each line.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用flatMap来将单词列表的数组（即，每行的单词组对应于该行的一个不同的数组/列表）分解。简而言之，我们想要的是单词列表，而不是每行的单词列表。
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output will be as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We read a text file `A Tale of Two Cities by Charles Dickens` from [http://www.gutenberg.org/](http://www.gutenberg.org/) into
    an RDD and then proceed to tokenize the words by using whitespace as the separator
    in a lambda expression using `.split()` and `.flatmap()` of RDD itself. We then
    proceed to use the `.count()` method of RDDs to output the total number of words.
    While this is simple, you have to bear in mind that the operation takes place
    using the distributed parallel framework of Spark with only a couple of lines.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[http://www.gutenberg.org/](http://www.gutenberg.org/)读取了一本名为《双城记》的小说，并将其存储为RDD，然后通过使用空格作为分隔符的lambda表达式的`.split()`和`.flatmap()`来对单词进行标记。然后，我们使用RDD的`.count()`方法输出单词的总数。虽然这很简单，但您必须记住，这个操作是在Spark的分布式并行框架中进行的，只需要几行代码。
- en: There's more...
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Creating RDDs with external data sources, whether it is a text file, Hadoop
    HDFS, sequence file, Casandra, or Parquet file is remarkably simple. Once again,
    we use `SparkSession` (`SparkContext` prior to Spark 2.0) to get a handle to the
    cluster. Once the function (for example, textFile Protocol: file path) is executed,
    the data is broken into smaller pieces (partitions) and automatically flows to
    the cluster, which becomes available to the computations as fault-tolerant distributed
    collections that can be operated on in parallel.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '使用外部数据源创建RDDs，无论是文本文件、Hadoop HDFS、序列文件、Casandra还是Parquet文件，都非常简单。再次使用`SparkSession`（Spark
    2.0之前使用`SparkContext`）来获取对集群的控制。一旦执行函数（例如，textFile Protocol: file path），数据就会被分成更小的片段（分区），并自动流向集群，变成可供计算使用的容错分布式集合，可以并行操作。'
- en: There are a number of variations that one must consider when working with real-life
    situations. The best advice based on our own experience is to consult the documentation
    before writing your own functions or connectors. Spark either supports your data
    source right out of the box, or the vendor has a connector that can be downloaded
    to do the same.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在处理真实情况时，有许多变化需要考虑。根据我们自己的经验，最好的建议是在编写自己的函数或连接器之前查阅文档。Spark要么直接支持您的数据源，要么供应商有一个可以下载的连接器来完成相同的工作。
- en: Another situation that we often see is many small files that are generated (usually
    within `HDFS` directories) that need to be parallelized as RDDs for consumption.
    `SparkContext` has a method named `wholeTextFiles()` which lets you read a directory
    containing multiple files and returns each of them as (filename, content) key-value
    pairs. We found this to be very useful in multi-stage machine learning situations
    using lambda architecture, where the model parameters are calculated as a batch
    and then updated in Spark every day.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们经常看到的另一种情况是生成许多小文件（通常在`HDFS`目录中），需要将它们并行化为RDDs以供使用。`SparkContext`有一个名为`wholeTextFiles()`的方法，它允许您读取包含多个文件的目录，并将每个文件作为（文件名，内容）键值对返回。我们发现这在使用lambda架构进行多阶段机器学习时非常有用，其中模型参数作为批处理计算，然后每天在Spark中更新。
- en: In this example, we read multiple files and then print the first file for examination.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们读取多个文件，然后打印第一个文件进行检查。
- en: 'The `spark.sparkContext.wholeTextFiles()` function is used to read a large
    number of small files and present them as (K,V), or key-value:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.sparkContext.wholeTextFiles()`函数用于读取大量小文件，并将它们呈现为（K,V）或键值对：'
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: See also
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Spark documentation for the `textFile()` and `wholeTextFiles()` functions:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Spark文档中关于`textFile()`和`wholeTextFiles()`函数的说明：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)'
- en: The `textFile()` API is a single abstraction for interfacing to external data
    sources. The formulation of protocol/path is enough to invoke the right decoder.
    We'll demonstrate reading from an ASCII text file, Amazon AWS S3, and HDFS with
    code snippets that the user would leverage to build their own system.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`textFile()` API是与外部数据源交互的单一抽象。协议/路径的制定足以调用正确的解码器。我们将演示从ASCII文本文件、Amazon AWS
    S3和HDFS读取的代码片段，用户可以利用这些代码片段构建自己的系统。'
- en: The path can be expressed as a simple path (for example, local text file) to
    a complete URI with the required protocol (for example, s3n for AWS storage buckets)
    to complete resource path with server and port configuration (for example, to
    read HDFS file from a Hadoop cluster).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路径可以表示为简单路径（例如，本地文本文件）到具有所需协议的完整URI（例如，AWS存储桶的s3n）到具有服务器和端口配置的完整资源路径（例如，从Hadoop集群读取HDFS文件）。
- en: 'The `textFile()` method supports full directories, regex wildcards, and compressed
    formats as well. Take a look at this example code:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`textFile()`方法还支持完整目录、正则表达式通配符和压缩格式。看一下这个示例代码：'
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `textFile()` method has an optional parameter at the end that defines the
    minimum number of partitions required by RDDs.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`textFile()`方法在末尾有一个可选参数，定义了RDD所需的最小分区数。'
- en: 'For example, we explicitly direct Spark to break the file into 13 partitions:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们明确指示Spark将文件分成13个分区：
- en: '[PRE22]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You also have the option of specifying a URI to read and create RDDs from other
    sources such as HDFS, and S3 by specifying a complete URI (protocol:path). The
    following examples demonstrate the point:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以选择指定URI来从其他来源（如HDFS和S3）读取和创建RDD，通过指定完整的URI（协议：路径）。以下示例演示了这一点：
- en: 'Reading and creating files from Amazon S3 buckets. A word of caution is that
    the AWS inline credentials in the URI will break if the AWS secret key has a forward
    slash. See this sample file:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Amazon S3存储桶中读取和创建文件。需要注意的是，如果AWS秘钥中有斜杠，URI中的AWS内联凭据将会中断。请参阅此示例文件：
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Reading from HDFS is very similar. In this example, we are reading from a local
    Hadoop cluster, but, in a real-world situation, the port number will be different
    and set by administrator.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从HDFS读取非常相似。在这个例子中，我们从本地Hadoop集群读取，但在现实世界的情况下，端口号将是不同的，并由管理员设置。
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Transforming RDDs with Spark 2.0 using the filter() API
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0使用`filter()`API转换RDD
- en: In this recipe, we explore the `filter()` method of RDD which is used to select
    a subset of the base RDD and return a new filtered RDD. The format is similar
    to `map()`, but a lambda function selects which members are to be included in
    the resulting RDD.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探索了RDD的`filter()`方法，用于选择基本RDD的子集并返回新的过滤后的RDD。格式与`map()`类似，但是lambda函数选择要包含在结果RDD中的成员。
- en: How to do it...
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中开始一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的软件包位置：
- en: '[PRE25]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Import the necessary packages:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的软件包：
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入设置`log4j`日志级别的软件包。这一步是可选的，但我们强烈建议（随着开发周期的推移，适当更改级别）。
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误以减少输出。查看上一步的软件包要求。
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行。
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Add the following lines for the examples to compile. The `pow()` function will
    allow us to raise any number to any power (for example, square the number):'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下行以使示例编译通过。`pow()`函数将允许我们将任何数字提升到任意幂（例如，平方该数字）：
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We create some data and `parallelize()` it to get our base RDD. We also use
    `textFile()`to create the initial (for example, base RDD) from our text file that
    we downloaded earlier from the [http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt)
    link:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一些数据并`parallelize()`它以获得我们的基本RDD。我们还使用`textFile()`从我们之前从[http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt)链接下载的文本文件创建初始（例如，基本RDD）：
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We apply the `filter()` function to the RDDs to demonstrate the `filter()` function
    transformation. We use the `filter()` function to select the odd members from
    the original RDD.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应用`filter()`函数到RDD中，以演示`filter()`函数的转换。我们使用`filter()`函数从原始RDD中选择奇数成员。
- en: The `filter()` function iterates (in parallel) through members of the RDD and
    applies the mod function (%) and compares it to 1\. In short, if there is a reminder
    after dividing by 2, then it must be an odd number.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`filter()`函数并行迭代RDD的成员，并应用mod函数（%）并将其与1进行比较。简而言之，如果除以2后有余数，那么它必须是奇数。'
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This is a second variation of the previous line, but here we demonstrate the
    use of `_` (underscore), which acts as a wildcard. We use this notation in Scala
    to abbreviate the obvious:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这是上一行的第二种变体，但在这里我们演示了`_`（下划线）的使用，它充当通配符。我们在Scala中使用这种表示法来缩写明显的内容：
- en: '[PRE33]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行上述代码时，您将获得以下输出：
- en: '[PRE34]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Another example combines map and filter together. This code snippet first squares
    every number and then applies the `filter` function to select the odd numbers
    from the original RDD.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个例子将map和filter结合在一起。这段代码首先对每个数字进行平方，然后应用`filter`函数从原始RDD中选择奇数。
- en: '[PRE35]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output will be as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE36]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In this example, we use the `filter()` method to identify the lines that are
    fewer than 30 characters. The resulting RDD will only contain the short lines.
    A quick examination of counts and output verify the results. The RDD transformation
    functions can be chained together, as long as the format complies with the function
    syntax.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用`filter()`方法来识别少于30个字符的行。结果RDD将只包含短行。对计数和输出的快速检查验证了结果。只要格式符合函数语法，RDD转换函数就可以链接在一起。
- en: '[PRE37]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '![](img/00064.gif)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00064.gif)'
- en: In this example we use the `contain()` method to filter out sentences that contain
    the word `two` in any upper/lowercase combination. We use several methods chained
    together to find the desired sentences.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用`contain()`方法来过滤包含任何大小写组合的单词`two`的句子。我们使用多个方法链接在一起来找到所需的句子。
- en: '[PRE38]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: How it works...
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `filter()` API is demonstrated using several examples. In the first example
    we went through an RDD and output odd numbers by using a lambda expression `.filter
    ( i => (i%2) == 1)` which takes advantage of the mod (modulus) function.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用几个示例演示了`filter()` API。在第一个示例中，我们通过使用lambda表达式`.filter(i => (i%2) == 1)`遍历了一个RDD并输出了奇数，利用了模（modulus）函数。
- en: In the second example we made it a bit interesting by mapping the result to
    a square function using a lambda expression `num.map(pow(_,2)).filter(_ %2 ==
    1)`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个示例中，我们通过使用lambda表达式`num.map(pow(_,2)).filter(_ %2 == 1)`将结果映射到一个平方函数，使其变得有趣一些。
- en: In the third example, we went through the text and filtered out short lines
    (for example, lines under 30 character) using the lambda expression `.filter(_.length
    < 30).filter(_.length > 0)` to print short versus total number of lines (`.count()`
    ) as output.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个示例中，我们遍历文本并使用lambda表达式`.filter(_.length < 30).filter(_.length > 0)`过滤出短行（例如，少于30个字符的行），以打印短行与总行数（`.count()`）作为输出。
- en: There's more...
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The `filter()` API walks through the parallelized distributed collection (that
    is, RDDs) and applies the selection criteria supplied to `filter()` as a lambda
    in order to include or exclude the element from the resulting RDD. The combination
    uses `map()`, which transforms each element and `filter()`, which selects a subset
    is a powerful combination in Spark ML programming.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter()` API遍历并应用于`filter()`中提供的选择条件的并行分布式集合（即RDD），以便通过lambda包含或排除结果RDD中的元素。组合使用`map()`，它转换每个元素，和`filter()`，它选择一个子集，在Spark
    ML编程中是一个强大的组合。'
- en: We will see later with the `DataFrame` API how a similar `Filter()` API can
    be used to achieve the same effect using a higher-level framework used in R and
    Python (pandas).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后我们将看到，使用`DataFrame` API，可以使用类似的`Filter()` API来实现相同的效果，这是在R和Python（pandas）中使用的更高级框架。
- en: See also
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `.filter()`, which is a method call of RDD, is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.JavaRDD).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.filter()`的文档，这是RDD的一个方法调用，可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.JavaRDD)上找到。'
- en: Documentation for `BloomFilter()`--for the sake of completeness, be aware that
    there is also a bloom filter function already in existence and it is suggested
    that you avoid coding this yourselves. We will tackle this in [C](part0538.html#G12EK0-4d291c9fed174a6992fd24938c2f9c77)hapter
    13, *Spark Streaming and Machine Learning Library*, to match Spark's view and
    layout. The link for this same is [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.util.sketch.BloomFilter](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.util.sketch.BloomFilter).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BloomFilter()`的文档-为了完整起见，请注意已经存在一个布隆过滤器函数，并建议避免自行编写代码。我们将在第13章《Spark流处理和机器学习库》中解决这个问题，以匹配Spark的视图和布局。同样的链接是[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.util.sketch.BloomFilter](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.util.sketch.BloomFilter)。'
- en: Transforming RDDs with the super useful flatMap() API
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用非常有用的flatMap() API转换RDD
- en: In this recipe, we examine the `flatMap()` method which is often a source of
    confusion for beginners; however, on closer examination we demonstrate that it
    is a clear concept that applies the lambda function to each element just like
    map, and then flattens the resulting RDD as a single structure (rather than having
    a list of lists, we create a single list made of all sublist with sublist elements).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们研究了`flatMap()`方法，这经常让初学者感到困惑；然而，仔细研究后，我们证明它是一个清晰的概念，它像`map`一样将lambda函数应用于每个元素，然后将结果RDD展平为单个结构（而不是具有子列表的列表，我们创建一个由所有子列表元素组成的单个列表）。
- en: How to do it...
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的软件包位置
- en: '[PRE39]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Import the necessary packages
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的软件包
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于设置`log4j`日志级别的软件包。这一步是可选的，但我们强烈建议这样做（随着开发周期的推移，适当更改级别）。
- en: '[PRE41]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误以减少输出。有关软件包要求，请参阅上一步。
- en: '[PRE42]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行。
- en: '[PRE43]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We use `textFile()` function to create the initial (that is, base RDD) from
    our text file that we downloaded earlier from [http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt):'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`textFile()`函数从我们之前从[http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt)下载的文本文件创建初始（即基本RDD）：
- en: '[PRE44]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We apply the map function to the RDDs to demonstrate the `map()` function transformation.
    To start with, we are doing it the wrong way to make a point: we first attempt
    to separate all the words based on the regular expression *[\s\W]+]* using just
    `map()` to demonstrate that the resulting RDD is a list of lists in which each
    list corresponds to a line and the tokenized word within that line. This example
    demonstrates what could cause confusion for beginners when using `flatMap()`.'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将map函数应用于RDDs以演示`map()`函数的转换。首先，我们以错误的方式进行演示：我们首先尝试根据正则表达式*[\s\W]+]*使用`map()`来分隔所有单词，以演示生成的RDD是一个列表的列表，其中每个列表对应于一行和该行中的标记化单词。这个例子演示了初学者在使用`flatMap()`时可能引起混淆的地方。
- en: The following line trims each line and then splits the line into words. The
    resulting RDD (that is, wordRDD2) will be a list of lists of words rather than
    a single list of words for the whole file.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下行修剪每行，然后将行拆分为单词。结果RDD（即wordRDD2）将是一个单词列表的列表，而不是整个文件的单个单词列表。
- en: '[PRE45]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: On running the previous code, you will get the following output.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出。
- en: '[PRE46]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We use the `flatMap()` method to not only map, but also flatten the list of
    lists so we end up with an RDD which is made of words themselves. We trim and
    split the words (that is, tokenize) and then filter for words greater than zero
    and then map it to upper case.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`flatMap()`方法不仅进行映射，还将列表扁平化，因此最终得到的RDD由单词本身组成。我们修剪和拆分单词（即标记化），然后过滤长度大于零的单词，然后将其映射到大写形式。
- en: '[PRE47]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In this case, after flattening the list using `flatMap()`, we can get a list
    of the words back as expected.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，使用`flatMap()`扁平化列表后，我们可以按预期获得单词列表。
- en: '[PRE48]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE49]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: How it works...
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this short example, we read a text file and then split the words (that is,
    tokenize it) using the `flatMap(_.trim.split("""[\s\W]+""")` lambda expression
    to have a single RDD with the tokenized content. Additionally we use the `filter
    ()` API `filter(_.length > 0)` to exclude the empty lines and the lambda expression
    `.map(_.toUpperCase())` in a `.map()` API to map to uppercase before outputting
    the results.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简短的例子中，我们读取一个文本文件，然后使用`flatMap(_.trim.split("""[\s\W]+""")` lambda表达式来拆分单词（即对其进行标记），以便获得一个包含标记内容的单个RDD。此外，我们使用`filter()`API`filter(_.length
    > 0)`来排除空行，并在`.map()`API中使用lambda表达式`.map(_.toUpperCase())`将结果映射到大写形式。
- en: There are cases where we do not want to get a list back for every element of
    base RDD (for example, get a list for words corresponding to a line). We sometimes
    prefer to have a single flattened list that is flat and corresponds to every word
    in the document. In short, rather than a list of lists, we want a single list
    containing all the elements.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 有些情况下，我们不希望为基本RDD的每个元素都返回一个列表（例如，为与一行对应的单词获取一个列表）。我们有时更喜欢有一个单一的扁平化列表，它是平的并且对应于文档中的每个单词。简而言之，我们希望得到一个包含所有元素的单一列表，而不是一个列表的列表。
- en: There's more...
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The function `glom()` is a function that lets you model each partition in the
    RDD as an array rather than a row list. While it is possible to produce the results
    in most cases, `glom()` allows you to reduce the shuffling between partitions.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`glom()`是一个函数，它允许您将RDD中的每个分区建模为一个数组，而不是一个行列表。虽然在大多数情况下可能会产生结果，但`glom()`允许您减少分区之间的洗牌。
- en: While at the surface, both method 1 and 2 mentioned in the text below look similar
    for calculating the minimum numbers in an RDD, the `glom()` function will cause
    much less data shuffling across the network by first applying `min()` to all the
    partitions, and then sending over the resulting data. The best way to see the
    difference is to use this on 10M+ RDDs and watch the IO and CPU usage accordingly.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在下面的文本中，方法1和2看起来很相似，用于计算RDD中最小数的方法，但`glom()`函数将通过首先将`min()`应用于所有分区，然后发送结果数据来减少网络上的数据洗牌。查看差异的最佳方法是在10M+
    RDD上使用它，并相应地观察IO和CPU使用情况。
- en: 'The first method is to find the minimum value without using `glom()`:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种方法是在不使用`glom()`的情况下找到最小值：
- en: '[PRE50]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'On running the preceding code, you will get the following output:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE51]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The second method is to find the minimum value using `glom(`, which causes a
    local application of the min function to a partition and then sends the results
    across via a shuffle.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种方法是使用`glom()`找到最小值，这将导致将最小函数局部应用于一个分区，然后通过洗牌发送结果。
- en: '[PRE52]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'On running the preceding code, you will get the following output:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE53]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: See also
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `flatMap()`, `PairFlatMap()`, and other variations under RDD
    is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关`flatMap()`、`PairFlatMap()`和RDD下其他变体的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)上找到
- en: Documentation for the `FlatMap()` function under RDD is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD下`FlatMap()`函数的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction)上找到
- en: Documentation for the `PairFlatMap()` function - very handy variation for paired
    data elements is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.function.PairFlatMapFunction](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.function.PairFlatMapFunction)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PairFlatMap()`函数的文档-对于成对数据元素非常有用的变体可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.function.PairFlatMapFunction](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.function.PairFlatMapFunction)上找到'
- en: The `flatMap()` method applies the supplied function (lambda or named function
    via def) to every element, flattens the structure, and produces a new RDD.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flatMap()`方法将提供的函数（lambda或通过def命名的函数）应用于每个元素，展平结构，并生成新的RDD。'
- en: Transforming RDDs with set operation APIs
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集合操作API转换RDD
- en: In this recipe, we explore set operations on RDDs, such as `intersection()`,
    `union()`, `subtract(),` and `distinct()` and `Cartesian()`. Let's implement the
    usual set operations in a distributed manner.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探讨了RDD上的集合操作，比如`intersection()`、`union()`、`subtract()`和`distinct()`以及`Cartesian()`。让我们以分布式方式实现通常的集合操作。
- en: How to do it...
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中开始一个新项目。确保包含必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置
- en: '[PRE54]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Import the necessary packages
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包
- en: '[PRE55]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于设置`log4j`日志级别的包。这一步是可选的，但我们强烈建议这样做（随着开发周期的推移，适当更改级别）。
- en: '[PRE56]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误，以减少输出。有关包要求，请参阅上一步。
- en: '[PRE57]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行。
- en: '[PRE58]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Set up the data structures and RDD for the example:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置示例的数据结构和RDD：
- en: '[PRE59]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We apply the `intersection()` function to the RDDs to demonstrate the transformation:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`intersection()`函数应用于RDD，以演示转换：
- en: '[PRE60]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE61]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We apply the `union()` function to the RDDs to demonstrate the transformation:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`union()`函数应用于RDD，以演示转换：
- en: '[PRE62]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE63]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We apply the `subract()` function to the RDDs to demonstrate the transformation:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`subract()`函数应用于RDD，以演示转换：
- en: '[PRE64]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE65]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We apply the `distinct()` function to the RDDs to demonstrate the transformation:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`distinct()`函数应用于RDD，以演示转换：
- en: '[PRE66]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE67]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: We apply the `distinct()` function to the RDDs to demonstrate the transformation
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`distinct()`函数应用于RDD，以演示转换。
- en: '[PRE68]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE69]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: How it works...
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this example, we started with three sets of number Arrays (odd, even, and
    their combo) and then proceeded to pass them as parameters into the set operation
    API. We covered how to use `intersection()`, `union()`, `subtract()`, `distinct()`,
    and `cartesian()` RDD operators.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们从三组数字数组（奇数、偶数和它们的组合）开始，然后将它们作为参数传递到集合操作API中。我们介绍了如何使用`intersection()`、`union()`、`subtract()`、`distinct()`和`cartesian()`
    RDD操作符。
- en: See also
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: While the RDD set operators are easy to use, one must be careful with the data
    shuffling that Spark has to perform in the background to complete some of these
    operations (for example, intersection).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RDD集合操作符易于使用，但必须小心数据洗牌，Spark必须在后台执行一些操作（例如，交集）。
- en: It is worth nothing that the union operator does not remove duplicates from
    the resulting RDD set.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，union操作符不会从生成的RDD集合中删除重复项。
- en: RDD transformation/aggregation with groupBy() and reduceByKey()
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD转换/聚合与groupBy()和reduceByKey()
- en: In this recipe, we explore the `groupBy()` and `reduceBy()` methods, which allow
    us to group values corresponding to a key. It is an expensive operation due to
    internal shuffling. We first demonstrate `groupby()` in more detail and then cover
    `reduceBy()` to show the similarity in coding these while stressing the advantage
    of the `reduceBy()` operator.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探讨了`groupBy()`和`reduceBy()`方法，它们允许我们根据键对值进行分组。由于内部洗牌，这是一个昂贵的操作。我们首先更详细地演示了`groupby()`，然后涵盖了`reduceBy()`，以展示编码这些操作的相似性，同时强调了`reduceBy()`操作符的优势。
- en: How to do it...
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中开始一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE70]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Import the necessary packages:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE71]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Import the packages for setting up logging level for `log4j`. This step is
    optional, but we highly recommend it (change the level appropriately as you move
    through the development cycle):'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于设置`log4j`日志级别的包。这一步是可选的，但我们强烈建议这样做（随着开发周期的推移，适当更改级别）：
- en: '[PRE72]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误，以减少输出。有关包要求，请参阅上一步。
- en: '[PRE73]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Set up the Spark context and application parameter so Spark can run:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行：
- en: '[PRE74]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Set up the data structures and RDD for the example. In this example we create
    an RDD using range facilities and divide them into three partitions (that is,
    explicit parameter set). It simply creates numbers 1 through 12 and puts them
    into 3 partitions.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置示例的数据结构和RDD。在这个例子中，我们使用范围工具创建了一个RDD，并将其分成三个分区（即，显式参数集）。它只是创建了1到12的数字，并将它们放入3个分区。
- en: '[PRE75]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: We apply the `groupBy()` function to the RDDs to demonstrate the transformation.
    In the example, we take the partitioned RDD of ranges and label them as odd/even
    using the `mod` function.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`groupBy()`函数应用于RDD，以演示转换。在这个例子中，我们使用`mod`函数将分区RDD标记为奇数/偶数。
- en: '[PRE76]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '![](img/00065.jpeg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00065.jpeg)'
- en: Now that we have seen how to code `groupBy()`, we switch gears and demonstrate
    `reduceByKey()`.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何编写`groupBy()`，我们转而演示`reduceByKey()`。
- en: To see the difference in coding, while producing the same output more efficiently,
    we set up an array with two letters (that is, `a` and `b`) so we can show aggregation
    by summing them up.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了看到编码上的差异，同时更有效地产生相同的输出，我们设置了一个包含两个字母（即`a`和`b`）的数组，以便我们可以通过对它们进行求和来展示聚合。
- en: '[PRE77]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'In this step, we use a Spark context to produce a parallelized RDD:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们使用Spark上下文来生成并行化的RDD：
- en: '[PRE78]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We apply the `groupBy()` function first using the usual Scala syntax `(_+_)`
    to traverse the RDD and sum up, while aggregating by the type of alphabet (that
    is, considered key):'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先应用`groupBy()`函数，使用通常的Scala语法`(_+_)`来遍历RDD并在按字母类型（即被视为键）进行聚合时进行求和：
- en: '[PRE79]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: We apply the `reduceByKey()` function first using the usual Scala syntax `(_+_)`
    to traverse the RDD and sum up while aggregating by type of alphabet (that is,
    considered key)
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先应用`reduceByKey()`函数，使用通常的Scala语法`(_+_)`来遍历RDD并在按字母类型（即被视为键）进行聚合时进行求和。
- en: '[PRE80]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'We output the results:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们输出了结果：
- en: '[PRE81]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE82]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: How it works...
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this example, we created numbers one through twelve and placed them in three
    partitions. We then proceeded to break them into odd/even using a simple mod operation
    while. The `groupBy()` is used to aggregate them into two groups of odd/even.
    This is a typical aggregation problem that should look familiar to SQL users.
    Later in this chapter we revisit this operation using `DataFrame` which also takes
    advantage of the better optimization techniques provided by the SparkSQL engine.
    In the later part, we demonstrate the similarity of `groupBy()` and `reduceByKey()`.
    We set up an array of alphabets (that is, `a` and `b`) and then convert them into
    RDD. We then proceed to aggregate them based on key (that is, unique letters -
    only two in this case) and print the total in each group.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们创建了从一到十二的数字，并将它们放在三个分区中。然后我们使用简单的模运算将它们分成奇数/偶数。`groupBy()`用于将它们聚合成奇数/偶数两组。这是一个典型的聚合问题，对于SQL用户来说应该很熟悉。在本章的后面，我们将使用`DataFrame`重新讨论这个操作，它也利用了SparkSQL引擎提供的更好的优化技术。在后面的部分，我们演示了`groupBy()`和`reduceByKey()`的相似之处。我们设置了一个字母数组（即`a`和`b`），然后将它们转换为RDD。然后我们根据键（即唯一的字母
    - 在这种情况下只有两个）对它们进行聚合，并打印出每个组的总数。
- en: There's more...
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Given the direction for Spark which favors the Dataset/DataFrame paradigm over
    low level RDD coding, one must seriously consider the reasoning for doing `groupBy()`
    on an RDD. While there are legitimate situations for which the operation is needed,
    the readers are advised to reformulate their solution to take advantage of the
    SparkSQL subsystem and its optimizer called **Catalyst**.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于Spark更倾向于Dataset/DataFrame范式而不是低级别的RDD编码，我们必须认真考虑在RDD上执行`groupBy()`的原因。虽然有合法的情况需要这个操作，但读者们被建议重新构思他们的解决方案，以利用SparkSQL子系统及其名为**Catalyst**的优化器。
- en: The Catalyst optimizer takes into account Scala's powerful features such as
    **pattern matching** and **quasiquotes** while building an optimized query plan.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst优化器在构建优化的查询计划时考虑了Scala强大的特性，比如**模式匹配**和**准引用**。
- en: The documentation on Scala pattern matching is available at [http://docs.scala-lang.org/tutorials/tour/pattern-matching.html](http://docs.scala-lang.org/tutorials/tour/pattern-matching.html)
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关Scala模式匹配的文档可在[http://docs.scala-lang.org/tutorials/tour/pattern-matching.html](http://docs.scala-lang.org/tutorials/tour/pattern-matching.html)找到
- en: The documentation on Scala quasiquotes is available at [http://docs.scala-lang.org/overviews/quasiquotes/intro.html](http://docs.scala-lang.org/overviews/quasiquotes/intro.html)
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关Scala准引用的文档可在[http://docs.scala-lang.org/overviews/quasiquotes/intro.html](http://docs.scala-lang.org/overviews/quasiquotes/intro.html)找到
- en: 'Runtime efficiency consideration: The `groupBy()` function groups data by keys.
    The operation causes internal shuffling which can explode the execution time;
    one must always prefer to use the `reduceByKey()` family of operations to a straight
    `groupBy()` method call. The `groupBy()` method is an expensive operation due
    to shuffling. Each group is made of keys and items that belong to that key. The
    ordering of values corresponding to the key will not be guaranteed by Spark.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时效率考虑：`groupBy()`函数按键对数据进行分组。这个操作会导致内部的数据重分配，可能会导致执行时间的爆炸；必须始终优先使用`reduceByKey()`系列的操作，而不是直接使用`groupBy()`方法。`groupBy()`方法由于数据重分配而是一个昂贵的操作。每个组由键和属于该键的项目组成。Spark不保证与键对应的值的顺序。
- en: 'For an explanation of the two operations, see the Databricks knowledge base
    blog:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 有关这两个操作的解释，请参阅Databricks知识库博客：
- en: '[https://databricks.gitbooks.io/databricks-Spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://databricks.gitbooks.io/databricks-Spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html)'
- en: See also
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Documentation for `groupBy()` and `reduceByKey()` operations under RDD:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: RDD下`groupBy()`和`reduceByKey()`操作的文档：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)'
- en: Transforming RDDs with the zip() API
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用zip() API转换RDD
- en: In this recipe we explore the `zip()` function. For those of us working in Python
    or Scala, `zip()` is a familiar method that lets you pair items before applying
    an inline function. Using Spark, it can be used to facilitate RDD arithmetic between
    pairs. Conceptually, it combines the two RDDs in such a way that each member of
    one RDD is paired with the second RDD that occupies the same position (that is,
    it lines up the two RDDs and makes pairs out of the members).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探讨了`zip（）`函数。对于我们中的一些人来说，在Python或Scala中，`zip（）`是一个熟悉的方法，它允许您在应用内函数之前对项目进行配对。使用Spark，它可以用于便捷地在对之间进行RDD算术。从概念上讲，它以这样的方式组合两个RDD，使得一个RDD的每个成员与占据相同位置的第二个RDD配对（即，它将两个RDD对齐并将成员配对）。
- en: How to do it...
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置
- en: '[PRE83]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Import the necessary packages
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包
- en: '[PRE84]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入设置`log4j`日志级别的包。这一步是可选的，但我们强烈建议这样做（随着开发周期的推移，适当更改级别）。
- en: '[PRE85]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误，以减少输出。有关包要求，请参阅上一步。
- en: '[PRE86]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行。
- en: '[PRE87]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Set up the data structures and RDD for the example. In this example we create
    two RDDs from `Array[]` and let Spark decide on the number of partitions (that
    is, the second parameter in the `parallize()` method is not set).
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为示例设置数据结构和RDD。在这个例子中，我们从`Array[]`创建了两个RDD，并让Spark决定分区的数量（即`parallize（）`方法中的第二个参数未设置）。
- en: '[PRE88]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: We apply the `zip()` function to the RDDs to demonstrate the transformation.
    In the example, we take the partitioned RDD of ranges and label them as odd/even
    using the mod function. We use the `zip()` function to pair elements from the
    two RDDs (SignalNoiseRDD and SignalStrengthRDD) so we can apply a `map()` function
    and compute their ratio (noise to signal ratio). We can use this technique to
    perform almost all types of arithmetic or non-arithmetic operations involving
    individual members of two RDDs.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`zip（）`函数应用于RDD，以演示转换。在这个例子中，我们使用mod函数将分区RDD的范围标记为奇数/偶数。我们使用`zip（）`函数将两个RDD（SignalNoiseRDD和SignalStrengthRDD）中的元素配对，以便我们可以应用`map（）`函数并计算它们的比率（噪声与信号比）。我们可以使用这种技术执行几乎所有涉及两个RDD的个体成员的算术或非算术操作。
- en: The pairing of two RDD members act as a tuple or a row. The individual members
    of the pair created by `zip()` can be accessed by their position (for example,
    `._1` and `._2`)
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个RDD成员的配对作为元组或行。由`zip（）`创建的对的个体成员可以通过它们的位置（例如`._1`和`._2`）访问
- en: '[PRE89]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE90]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: How it works...
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In this example, we first set up two arrays representing signal noise and signal
    strength. They are simply a set of measured numbers that we could have received
    from the IoT platform. We then proceeded to pair the two separate arrays so each
    member looks like they have been input originally as a pair of (x, y). We then
    proceed to divide the pair and produce the noise to signal ratio using the following
    code snippet:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们首先设置了两个代表信号噪声和信号强度的数组。它们只是一组我们可以从IoT平台接收到的测量数字。然后，我们继续配对这两个单独的数组，使每个成员看起来就像它们最初被输入为一对（x，y）。然后，我们继续分割这对，并使用以下代码片段产生噪声到信号比：
- en: '[PRE91]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: The `zip()` method has many variations that involve partitions. The developers
    should familiarize themselves with variations of the `zip()` method with partition
    (for example, `zipPartitions`).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '`zip（）`方法有许多涉及分区的变体。开发人员应熟悉`zip（）`方法与分区的各种变体（例如`zipPartitions`）。'
- en: See also
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `zip()` and `zipPartitions()` operations under RDD is available
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD下`zip（）`和`zipPartitions（）`操作的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)找到
- en: Join transformation with paired key-value RDDs
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用配对键值RDD进行连接转换
- en: In this recipe, we introduce the `KeyValueRDD` pair RDD and the supporting join
    operations such as `join()`, `leftOuterJoin` and `rightOuterJoin()`, and `fullOuterJoin()`
    as an alternative to the more traditional and more expensive set operations available
    via the set operation API, such as `intersection()`, `union()`, `subtraction()`,
    `distinct()`, `cartesian()`, and so on.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们介绍了`KeyValueRDD`对RDD和支持的连接操作，如`join（）`，`leftOuterJoin`和`rightOuterJoin（）`，以及`fullOuterJoin（）`，作为传统和更昂贵的集合操作API的替代方法，例如`intersection（）`，`union（）`，`subtraction（）`，`distinct（）`，`cartesian（）`等。
- en: We'll demonstrate `join()`, `leftOuterJoin` and `rightOuterJoin()`, and `fullOuterJoin()`,
    to explain the power and flexibility of key-value pair RDDs.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将演示`join（）`，`leftOuterJoin`和`rightOuterJoin（）`，以及`fullOuterJoin（）`，以解释键值对RDD的强大和灵活性。
- en: '[PRE92]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: How to do it...
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Set up the data structures and RDD for the example:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为示例设置数据结构和RDD：
- en: '[PRE93]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Turn the List into RDDs:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将列表转换为RDDs：
- en: '[PRE94]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: We can access the `keys` and `values` inside a pair RDD.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以访问对RDD内部的`keys`和`values`。
- en: '[PRE95]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: We apply the `mapValues()` function to the pair RDDs to demonstrate the transformation.
    In this example we use the map function to lift up the value by adding 100 to
    every element. This is a popular technique to introduce noise to the data (that
    is, jittering).
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`mapValues（）`函数应用于对RDD，以演示转换。在这个例子中，我们使用map函数通过将每个元素加100来提升值。这是一种引入数据噪声（即抖动）的常用技术。
- en: '[PRE96]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE97]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: We apply the `join()` function to the RDDs to demonstrate the transformation.
    We use `join()` to join the two RDDs. We join the two RDDs based on keys (that
    is, north, south, and so on).
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`join()`函数应用于RDD，以演示转换。我们使用`join()`来连接两个RDD。我们基于键（即北、南等）连接两个RDD。
- en: '[PRE98]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE99]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: We apply the `leftOuterJoin()` function to the RDDs to demonstrate the transformation.
    The `leftOuterjoin` acts like a relational left outer join. Spark replaces the
    absence of a membership with `None` rather than `NULL`, which is common in relational
    systems.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`leftOuterJoin()`函数应用于RDD，以演示转换。`leftOuterjoin`类似于关系左外连接。Spark用`None`而不是`NULL`来替换成员缺失，这在关系系统中很常见。
- en: '[PRE100]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE101]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: We'll apply `rightOuterJoin()` to the RDDs to demonstrate the transformation.
    This is similar to a right outer join in relational systems.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将应用`rightOuterJoin()`到RDD，以演示转换。这类似于关系系统中的右外连接。
- en: '[PRE102]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE103]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: We then apply the `fullOuterJoin()` function to the RDDs to demonstrate the
    transformation. This is similar to full outer join in relational systems.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将`fullOuterJoin()`函数应用于RDD，以演示转换。这类似于关系系统中的全外连接。
- en: '[PRE104]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE105]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: How it works...
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we declared three lists representing typical data available
    in relational tables, which could be imported using a connector to Casandra or
    RedShift (not shown here to simplify the recipe). We used two of the three lists
    representing city names (that is, data tables) and joined them with the first
    list, which represents directions (for example, defining tables). The first step
    is to define three lists of paired values. We then parallelized them into key-value
    RDDs so we can perform join operations between the first RDD (that is, directions)
    and the other two RDDs representing city names. We applied the join function to
    the RDDs to demonstrate the transformation.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们声明了三个列表，表示关系表中可用的典型数据，可以使用连接器导入到Casandra或RedShift（这里没有显示以简化示例）。我们使用了三个列表中的两个，表示城市名称（即数据表），并将它们与表示方向（例如，定义表）的第一个列表进行了连接。第一步是定义三个成对值的列表。然后，我们将它们并行化为键值RDD，以便我们可以在第一个RDD（即方向）和另外两个表示城市名称的RDD之间执行连接操作。我们应用了连接函数到RDD，以演示转换。
- en: We demonstrated `join()`, `leftOuterJoin` and `rightOuterJoin()`, and `fullOuterJoin()`
    to show the power and flexibility when combined with key-value pair RDDs.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们演示了`join()`，`leftOuterJoin`和`rightOuterJoin()`，以及`fullOuterJoin()`，以展示与键值对RDD结合时的强大和灵活性。
- en: There's more...
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Documentation for `join()` and its variations under RDD is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '`join()`及其在RDD下的变体的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)上找到。'
- en: Reduce and grouping transformation with paired key-value RDDs
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用配对键值RDD进行减少和分组转换
- en: In this recipe, we explore reduce and group by key. The `reduceByKey()` and
    `groupbyKey()` operations are much more efficient and preferred to `reduce()`
    and `groupBy()` in most cases. The functions provide convenient facilities to
    aggregate values and combine them by key with less shuffling, which is problematic
    on large data sets.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探讨了减少和按键分组。`reduceByKey()`和`groupbyKey()`操作比`reduce()`和`groupBy()`更有效，通常更受欢迎。这些函数提供了方便的设施，以较少的洗牌来聚合值并按键组合，这在大型数据集上是有问题的。
- en: How to do it...
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的软件包位置
- en: '[PRE106]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: Import the necessary packages
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的软件包
- en: '[PRE107]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于设置`log4j`日志级别的软件包。这一步是可选的，但我们强烈建议这样做（随着开发周期的推移，适当更改级别）。
- en: '[PRE108]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirement:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误，以减少输出。请参阅上一步的软件包要求：
- en: '[PRE109]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行。
- en: '[PRE110]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Set up the data structures and RDD for the example:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置示例的数据结构和RDD：
- en: '[PRE111]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: We apply `groupByKey()` to demonstrate the transformation. In this example,
    we group all the buy and sell signals together while operating in a distributed
    setting.
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应用`groupByKey()`来演示转换。在这个例子中，我们在分布式环境中将所有买入和卖出信号分组在一起。
- en: '[PRE112]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE113]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: We apply the `reduceByKey()` function to the pair of RDDs to demonstrate the
    transformation. In this example the function is to sum up the total volume for
    the buy and sell signals. The Scala notation of `(_+_)` simply denotes adding
    two members at the time and producing a single result from it. Just like `reduce()`,
    we can apply any function (that is, inline for simple functions and named functions
    for more complex cases).
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`reduceByKey()`函数应用于RDD的一对，以演示转换。在这个例子中，函数是为买入和卖出信号的总成交量求和。`(_+_)`的Scala表示法简单地表示一次添加两个成员，并从中产生单个结果。就像`reduce()`一样，我们可以应用任何函数（即对于简单函数的内联和对于更复杂情况的命名函数）。
- en: '[PRE114]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE115]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: How it works...
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this example we declared a list of items as being sold or purchased and their
    corresponding price (that is, typical commercial transaction). We then proceeded
    to calculate the sum using Scala shorthand notation `(_+_)`. In the last step,
    we provided the total for each key group (that is, `Buy` or `Sell`). The key-value
    RDD is a powerful construct that can reduce coding while providing the functionality
    needed to group paired values into aggregated buckets. The `groupByKey()` and
    `reduceByKey()` functions mimic the same aggregation functionality, while `reduceByKey()`
    is more efficient due to less shuffling of the data while final results are being
    assembled.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们声明了一个项目列表，表示出售或购买的物品及其相应的价格（即典型的商业交易）。然后我们使用Scala的简写符号`(_+_)`来计算总和。在最后一步中，我们为每个键组（即`Buy`或`Sell`）提供了总和。键-值RDD是一个强大的构造，可以减少编码，同时提供将配对值分组到聚合桶中所需的功能。`groupByKey()`和`reduceByKey()`函数模拟了相同的聚合功能，而`reduceByKey()`由于在组装最终结果时数据洗牌较少，因此更有效。
- en: See also
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: Documentation for `groupByKey()` and `reduceByKey()` operations under RDD is
    available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD).
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 有关RDD下`groupByKey()`和`reduceByKey()`操作的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)找到。
- en: Creating DataFrames from Scala data structures
  id: totrans-408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Scala数据结构创建DataFrames
- en: In this recipe, we explore the `DataFrame` API, which provides a higher level
    of abstraction than RDDs for working with data. The API is similar to R and Python
    data frame facilities (pandas).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们探讨了`DataFrame` API，它提供了比RDD更高级的抽象级别，用于处理数据。该API类似于R和Python数据框架工具（pandas）。
- en: '`DataFrame` simplifies coding and lets you use standard SQL to retrieve and
    manipulate data. Spark keeps additional information about DataFrames, which helps
    the API to manipulate the frames with ease. Every `DataFrame` will have a schema
    (either inferred from data or explicitly defined) which allows us to view the
    frame like an SQL table. The secret sauce of SparkSQL and DataFrame is that the
    catalyst optimizer will work behind the scenes to optimize access by rearranging
    calls in the pipeline.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame`简化了编码，并允许您使用标准SQL来检索和操作数据。Spark保留了有关DataFrames的附加信息，这有助于API轻松地操作框架。每个`DataFrame`都将有一个模式（可以从数据中推断或显式定义），这使我们可以像查看SQL表一样查看框架。SparkSQL和DataFrame的秘密武器是，催化剂优化器将在幕后工作，通过重新排列管道中的调用来优化访问。'
- en: How to do it...
  id: totrans-411
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE116]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Set up the imports related to DataFrames and the required data structures and
    create the RDDs as needed for the example:'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置与DataFrames和所需数据结构相关的导入，并根据需要创建RDD：
- en: '[PRE117]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入设置`log4j`日志级别的包。这一步是可选的，但我们强烈建议这样做（随着开发周期的推移，适当更改级别）。
- en: '[PRE118]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirement.
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误，以减少输出。有关包要求，请参阅上一步。
- en: '[PRE119]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行。
- en: '[PRE120]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'We set up the Scala data structures as two `List()` objects and a sequence
    (that is, `Seq()`). We then proceed to turn the `List` structures into RDDs for
    conversion to `DataFrames` for the next steps:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了Scala数据结构作为两个`List()`对象和一个序列（即`Seq()`）。然后我们将`List`结构转换为RDD，以便进行下一步的DataFrame转换：
- en: '[PRE121]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: We take a list which is turned into an RDD using the `parallelize()` method
    and use the `toDF()` method of the RDD to turn it into a DataFrame. The `show()`
    method allows us to view the DataFrame, which is similar to a SQL table.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`parallelize()`方法将一个列表转换为RDD，并使用RDD的`toDF()`方法将其转换为DataFrame。`show()`方法允许我们查看DataFrame，这类似于SQL表。
- en: '[PRE122]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'On running the previous code, you will get the following output.:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行上述代码时，您将获得以下输出：
- en: '[PRE123]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: In the following code snippet, we take a generic Scala **Seq** (**Sequence**)
    data structure and use `createDataFrame()` explicitly to create a DataFrame while
    naming the columns at the same time.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们使用`createDataFrame()`显式地从一个通用的Scala **Seq**（**Sequence**）数据结构创建一个DataFrame，同时命名列。
- en: '[PRE124]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: In the next two steps, we use the `show()` method to see the contents and then
    proceed to use `printscheme()` to show the inferred scheme based on types. In
    this example, the DataFrame correctly identified the integer and double in the
    Seq as the valid type for the two columns of numbers.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的两个步骤中，我们使用`show()`方法查看内容，然后继续使用`printSchema()`来显示基于类型推断的模式。在这个例子中，DataFrame正确地识别了Seq中的整数和双精度作为两列数字的有效类型。
- en: '[PRE125]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行上述代码时，您将获得以下输出：
- en: '[PRE126]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: How it works...
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we took two lists and a Seq data structure and converted them
    to DataFrame and used `df1.show()` and `df1.printSchema()` to display contents
    and schema for the table.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们取了两个列表和一个Seq数据结构，并将它们转换为DataFrame，并使用`df1.show()`和`df1.printSchema()`来显示表的内容和模式。
- en: DataFrames can be created from both internal and external sources. Just like
    SQL tables, the DataFrames have schemas associated with them that can either be
    inferred or explicitly defined using Scala case classes or the `map()` function
    to explicitly convert while ingesting the data.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames可以从内部和外部源创建。就像SQL表一样，DataFrames有与之关联的模式，可以通过Scala case类或`map()`函数来推断或显式定义。
- en: There's more...
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'To ensure completeness, we include the `import` statement that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保完整性，我们包括了我们在Spark 2.0.0之前使用的`import`语句来运行代码（即Spark 1.5.2）：
- en: '[PRE127]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: See also
  id: totrans-441
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame的文档可在[https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)上找到。
- en: If you see any issues with implicit conversion, double check to make sure you
    have included the implicits import statement.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您发现隐式转换存在任何问题，请仔细检查是否已包含了隐式导入语句。
- en: 'Example code for Spark 2.0:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0的示例代码：
- en: '[PRE128]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: Operating on DataFrames programmatically without SQL
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在没有使用SQL的情况下以编程方式操作DataFrame
- en: In this recipe, we explore how to manipulate DataFrame with code and method
    calls only (without SQL). The DataFrames have their own methods that allow you
    to perform SQL-like operations using a programmatic approach. We demonstrate some
    of these commands such as `select()`, `show()`, and `explain()` to get the point
    across that the DataFrame itself is capable of wrangling and manipulating the
    data without using SQL.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们探讨如何仅通过代码和方法调用来操作DataFrame（而不使用SQL）。DataFrame有自己的方法，允许您使用编程方法执行类似SQL的操作。我们演示了一些这些命令，比如`select()`、`show()`和`explain()`，以表明DataFrame本身能够在不使用SQL的情况下处理和操作数据。
- en: How to do it...
  id: totrans-448
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置
- en: '[PRE129]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: Set up the imports related to DataFrames and the required data structures and
    create the RDDs as needed for the example
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置与DataFrame相关的导入和所需的数据结构，并根据需要创建RDDs
- en: '[PRE130]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入设置`log4j`日志级别的包。这一步是可选的，但我们强烈建议这样做（随着开发周期的推移，适当更改级别）。
- en: '[PRE131]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirement.
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置日志级别为警告和错误，以减少输出。有关包要求，请参阅上一步。
- en: '[PRE132]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行。
- en: '[PRE133]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'We are creating an RDD from an external source, which is a comma-separated
    text file:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从外部来源创建了一个RDD，这是一个逗号分隔的文本文件：
- en: '[PRE134]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: Here is a quick look at what the customer data file would look like
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是客户数据文件的快速查看
- en: '[PRE135]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: After creating the RDD for the corresponding customer data file, we proceed
    to explicitly parse and convert the data types using a `map()` function from the
    RDD. In this example, we want to make sure the last field (that is, age) is represented
    as an integer.
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在为相应的客户数据文件创建RDD之后，我们继续使用`map()`函数从RDD中显式解析和转换数据类型。在这个例子中，我们要确保最后一个字段（即年龄）表示为整数。
- en: '[PRE136]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: In the third step, we convert the RDD to a DataFrame using a `toDF()` call.
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第三步中，我们使用`toDF()`调用将RDD转换为DataFrame。
- en: '[PRE137]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: Once we have the DataFrame ready, we want to display the contents quickly for
    visual verification and also print and verify the schema.
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦DataFrame准备好，我们希望快速显示内容以进行视觉验证，并打印和验证模式。
- en: '[PRE138]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE139]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: Having the DataFrame ready and inspected, we proceed to demonstrate DataFrame
    access and manipulation via `show()`, `select()`, `sort()`, `groupBy()`, and `explain()`
    APIs.
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在DataFrame准备好并经过检查后，我们继续演示通过`show()`、`select()`、`sort()`、`groupBy()`和`explain()`API对DataFrame进行访问和操作。
- en: 'We use the `filter()` method to list customers that are more than 25 years
    old:'
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`filter()`方法来列出年龄超过25岁的客户：
- en: '[PRE140]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE141]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: We use the `select()` method to display the names of customers.
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`select()`方法来显示客户的姓名。
- en: '[PRE142]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: On running the previous code, you will get the following output.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出。
- en: '[PRE143]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'We use `select()` to list multiple columns:'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`select()`来列出多个列：
- en: '[PRE144]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE145]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'We use an alternative syntax to display and refer to fields within the DataFrame:'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用另一种语法来显示和引用DataFrame中的字段：
- en: '[PRE146]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE147]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: 'Using `select()` and a predicate, to list customers'' name and city where the
    age is less than 50:'
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`select()`和谓词，列出年龄小于50岁的客户的姓名和城市：
- en: '[PRE148]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '![](img/00066.gif)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00066.gif)'
- en: 'We use `sort()` and `groupBy()` to sort and group customers by their city of
    residence:'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`sort()`和`groupBy()`来按客户所在城市对客户进行排序和分组：
- en: '[PRE149]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: On running the previous code, you will get the following output.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出。
- en: '![](img/00067.gif)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00067.gif)'
- en: 'We can also ask for a plan of execution: this command will be more relevant
    with upcoming recipes in which we use SQL to access and manipulate the DataFrame.'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以要求执行计划：这个命令在我们使用SQL来访问和操作DataFrame的即将到来的配方中将更加相关。
- en: '[PRE150]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE151]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: How it works...
  id: totrans-501
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this example, we loaded data from a text file into an RDD and then converted
    it to a DataFrame structure using the `.toDF()` API. We then proceeded to mimic
    SQL queries using built-in methods such as `select()`, `filter()`, `show()`, and
    `explain()` that help us to programmatically explore the data (no SQL). The `explain()`
    command shows the query plan which can be awfully useful to remove the bottleneck.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们从文本文件中加载数据到RDD，然后使用`.toDF()`API将其转换为DataFrame结构。然后，我们使用内置方法如`select()`、`filter()`、`show()`和`explain()`来模拟SQL查询，这些方法帮助我们通过API程序化地探索数据（无需SQL）。`explain()`命令显示查询计划，这对于消除瓶颈非常有用。
- en: DataFrames provide multiple approaches to data wrangling.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame提供了多种数据整理方法。
- en: For those comfortable with the DataFrame API and packages from R ([https://cran.r-project.org](https://cran.r-project.org))
    like dplyr or an older version, we have a programmatic API with an extensive set
    of methods that lets you do all your data wrangling via the API.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些熟悉DataFrame API和来自R的包（[https://cran.r-project.org](https://cran.r-project.org)）如dplyr或旧版本的人，我们有一个编程API，其中包含一系列广泛的方法，让您可以通过API进行所有数据整理。
- en: For those more comfortable with SQL, you can simply use SQL to retrieve and
    manipulate data as if you were using Squirrel or Toad to query the database.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些更喜欢使用SQL的人，您可以简单地使用SQL来检索和操作数据，就像使用Squirrel或Toad查询数据库一样。
- en: There's more...
  id: totrans-506
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'To ensure completeness, we include the `import` statements that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保完整性，我们包括了我们在Spark 2.0.0之前使用的`import`语句来运行代码（即Spark 1.5.2）：
- en: '[PRE152]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: See also
  id: totrans-509
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame的文档可在[https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)找到。
- en: If you see any issues with implicit conversion, double check to make sure you
    have included the implicits `import` statement.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您发现隐式转换有任何问题，请仔细检查是否已包含了隐式`import`语句。
- en: 'Example `import` statement for Spark 2.0:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0的示例`import`语句：
- en: '[PRE153]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: Loading DataFrames and setup from an external source
  id: totrans-514
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从外部源加载DataFrame和设置
- en: In this recipe, we examine data manipulation using SQL. Spark's approach to
    provide both a pragmatic and SQL interface works very well in production settings
    in which we not only require machine learning, but also access to existing data
    sources using SQL to ensure compatibility and familiarity with existing SQL-based
    systems. DataFrame with SQL makes for an elegant process toward integration in
    real-life settings.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用SQL来进行数据操作。Spark提供的既实用又SQL接口的方法在生产环境中非常有效，我们不仅需要机器学习，还需要使用SQL访问现有数据源，以确保与现有基于SQL的系统的兼容性和熟悉度。DataFrame与SQL使得在现实生活中进行集成的过程更加优雅。
- en: How to do it...
  id: totrans-516
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE154]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: 'Set up the imports related to DataFrame and the required data structures and
    create the RDDs as needed for the example:'
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置与DataFrame相关的导入和所需的数据结构，并根据需要创建RDD：
- en: '[PRE155]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于设置`log4j`日志级别的包。这一步是可选的，但我们强烈建议这样做（随着开发周期的推移，适当更改级别）。
- en: '[PRE156]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: 'Set up the logging level to warning and `Error` to cut down on output. See
    the previous step for package requirement:'
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和`Error`以减少输出。有关包要求，请参阅上一步：
- en: '[PRE157]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行。
- en: '[PRE158]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: We create the DataFrame corresponding to the `customer` file. In this step,
    we first create an RDD and then proceed to use the `toDF()` to convert the RDD
    to DataFrame and name the columns.
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建与“customer”文件对应的DataFrame。在这一步中，我们首先创建一个RDD，然后使用`toDF()`将RDD转换为DataFrame并命名列。
- en: '[PRE159]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: 'Customer data contents for reference:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 客户数据内容供参考：
- en: '[PRE160]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: 'On running the preceding code, you will get the following output:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '![](img/00068.gif)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00068.gif)'
- en: We create the DataFrame corresponding to the `product` file. In this step, we
    first create an RDD and then proceed to use the `toDF()` to convert the RDD to
    DataFrame and name the columns.
  id: totrans-534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建与“product”文件对应的DataFrame。在这一步中，我们首先创建一个RDD，然后使用`toDF()`将RDD转换为DataFrame并命名列。
- en: '[PRE161]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: 'We convert `prodRDD` to DataFrame:'
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`prodRDD`转换为DataFrame：
- en: '[PRE162]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: Using SQL select, we display contents of the table.
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SQL select，我们显示表的内容。
- en: 'Product data contents:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 产品数据内容：
- en: '[PRE163]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '![](img/00069.gif)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00069.gif)'
- en: We create the DataFrame corresponding to the `sales` file. In this step we first
    create an RDD and then proceed to use `toDF()` to convert the RDD to DataFrame
    and name the columns.
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建与“sales”文件对应的DataFrame。在这一步中，我们首先创建一个RDD，然后使用`toDF()`将RDD转换为DataFrame并命名列。
- en: '[PRE164]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'We convert the `saleRDD` to DataFrame:'
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`saleRDD`转换为DataFrame：
- en: '[PRE165]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: We use SQL select to display the table.
  id: totrans-547
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用SQL select来显示表。
- en: 'Sales data contents:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 销售数据内容：
- en: '[PRE166]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '![](img/00070.gif)'
  id: totrans-551
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00070.gif)'
- en: 'We print schemas for the customer, product, and sales DataFrames to verify
    schema after column definition and type conversion:'
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印客户、产品和销售DataFrame的模式，以验证列定义和类型转换后的模式：
- en: '[PRE167]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE168]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: How it works...
  id: totrans-556
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this example, we first loaded data into an RDD and then converted it into
    a DataFrame using the `toDF()` method. The DataFrame is very good at inferring
    types, but there are occasions that require manual intervention. We used the `map()`
    function after creating the RDD (lazy initialization paradigm applies) to massage
    the data either by type conversion or calling on more complicated user defined
    functions (referenced in the `map()` method) to do the conversion or data wrangling.
    Finally, we proceeded to examine the schema for each of the three DataFrames using
    `show()` and `printSchema()`.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们首先将数据加载到RDD中，然后使用`toDF()`方法将其转换为DataFrame。DataFrame非常擅长推断类型，但有时需要手动干预。我们在创建RDD后（采用延迟初始化范式）使用`map()`函数来处理数据，可以进行类型转换，也可以调用更复杂的用户定义函数（在`map()`方法中引用）进行转换或数据整理。最后，我们使用`show()`和`printSchema()`来检查三个DataFrame的模式。
- en: There's more...
  id: totrans-558
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'To ensure completeness, we include the `import` statements that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保完整性，我们包括了在Spark 2.0.0之前使用的`import`语句来运行代码（即Spark 1.5.2）：
- en: '[PRE169]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: See also
  id: totrans-561
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame的文档可在[https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)找到。
- en: If you see any issues with implicit conversion, double check to make sure you
    have included the implicits `import` statement.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您发现隐式转换存在问题，请仔细检查是否已包含了隐式`import`语句。
- en: 'Example `import` statement for Spark 1.5.2:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 1.5.2的示例`import`语句：
- en: '[PRE170]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: Using DataFrames with standard SQL language - SparkSQL
  id: totrans-566
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用标准SQL语言的DataFrames - SparkSQL
- en: In this recipe, we demonstrate how to use DataFrame SQL capabilities to perform
    basic CRUD operations, but there is nothing limiting you from using the SQL interface
    provided by Spark to any level of sophistication (that is, DML) desired.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们演示了如何使用DataFrame的SQL功能执行基本的CRUD操作，但并没有限制您使用Spark提供的SQL接口进行任何所需的复杂操作（即DML）。
- en: How to do it...
  id: totrans-568
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-569
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动新项目。确保包含必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置
- en: '[PRE171]'
  id: totrans-571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: Set up the imports related to DataFrames and the required data structures and
    create the RDDs as needed for the example
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置与DataFrames和所需数据结构相关的导入，并根据需要创建RDDs
- en: '[PRE172]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入设置`log4j`日志级别的包。这一步是可选的，但我们强烈建议这样做（随着开发周期的推进，适当更改级别）。
- en: '[PRE173]'
  id: totrans-575
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: Set up the logging level to warning and `ERROR` to cut down on output. See the
    previous step for package requirement.
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置日志级别为警告和`ERROR`，以减少输出。有关包要求，请参见上一步。
- en: '[PRE174]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行。
- en: '[PRE175]'
  id: totrans-579
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: We will be using the DataFrames created in the previous recipe to demonstrate
    the SQL capabilities of DataFrames. You can refer to the previous steps for details.
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用上一个示例中创建的DataFrames来演示DataFrame的SQL功能。您可以参考上一步了解详情。
- en: '[PRE176]'
  id: totrans-581
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: Before we can use the DataFrame for queries via SQL, we have to register the
    DataFrame as a temp table so the SQL statements can refer to it without any Scala/Spark
    syntax. This step may cause confusion for many beginners as we are not creating
    any table (temp or permanent), but the call `registerTempTable()` (pre-Spark 2.0)
    and `createOrReplaceTempView()` (Spark 2.0+) creates a name in SQL land that the
    SQL statements can refer to without additional UDF or any domain-specific query
    language. In short, there is additional metadata that is kept by Spark in the
    background (`registerTempTable()` call), which facilitates querying in the execution
    phase.
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们可以通过SQL对DataFrame进行查询之前，我们必须将DataFrame注册为临时表，以便SQL语句可以在不使用任何Scala/Spark语法的情况下引用它。这一步可能会让许多初学者感到困惑，因为我们并没有创建任何表（临时或永久），但`registerTempTable()`（Spark
    2.0之前）和`createOrReplaceTempView()`（Spark 2.0+）的调用在SQL中创建了一个名称，SQL语句可以在其中引用，而无需额外的UDF或任何特定领域的查询语言。简而言之，Spark在后台保留了额外的元数据（`registerTempTable()`调用），这有助于在执行阶段进行查询。
- en: 'Create the `CustDf` DataFrame as a name which SQL statements recognize as `customers`:'
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建名为`customers`的`CustDf` DataFrame：
- en: '[PRE177]'
  id: totrans-584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: 'Create the `prodDf` DataFrame as a name which SQL statements recognize as `product`:'
  id: totrans-585
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建名为`product`的`prodDf` DataFrame：
- en: '[PRE178]'
  id: totrans-586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: 'Create the `saleDf` DataFrame as a name which SQL statements recognize as `sales`:'
  id: totrans-587
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建名为`sales`的`saleDf` DataFrame，以便SQL语句能够识别：
- en: '[PRE179]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: Now that everything is ready, let's demonstrate the power of DataFrames with
    standard SQL. For those who prefer not to work with SQL, the programmatic way
    is always at your fingertips.
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在一切准备就绪，让我们演示DataFrames与标准SQL的强大功能。对于那些不愿意使用SQL的人来说，编程方式始终是一个选择。
- en: In this example we see how to select a column from the customers table (it is
    not a SQL table underneath, but you can certainly abstract it as such).
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此示例中，我们演示了如何从customers表中选择列（实际上并不是SQL表，但您可以将其抽象为SQL表）。
- en: '[PRE180]'
  id: totrans-591
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: On running the previous code, you will get the following output.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出。
- en: '![](img/00071.gif)'
  id: totrans-593
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00071.gif)'
- en: 'Select multiple columns from the customer table:'
  id: totrans-594
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从customer表中选择多列：
- en: '[PRE181]'
  id: totrans-595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: On running the previous code, you will get the following output.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出。
- en: '![](img/00072.gif)'
  id: totrans-597
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00072.gif)'
- en: 'We print the schema for customer, product, and sales DataFrames to verify it
    after column definition and type conversion:'
  id: totrans-598
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印了customer、product和sales DataFrames的模式，以便在列定义和类型转换后进行验证：
- en: '[PRE182]'
  id: totrans-599
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: On running the previous code, you will get the following output.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出。
- en: '![](img/00073.gif)'
  id: totrans-601
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00073.gif)'
- en: In this example, we join the sales and product tables and list all the customers
    that have purchased a product at more than 20% discount. This SQL joins the sales
    and product tables and then uses a simple formula to find products that are sold
    at a deep discount. To reiterate, the key aspect of DataFrame is that we use standard
    SQL without any special syntax.
  id: totrans-602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个示例中，我们连接销售和产品表，并列出所有购买了打折超过20%的产品的客户。这个SQL连接了销售和产品表，然后使用一个简单的公式来找到以深度折扣出售的产品。重申一下，DataFrame的关键方面是我们使用标准SQL而没有任何特殊的语法。
- en: '[PRE183]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: On running the previous code, you will get the following output.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出。
- en: '![](img/00074.gif)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00074.gif)'
- en: We can always use the `explain()` method to examine the physical query plan
    that Spark SQL used to execute the query.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以始终使用`explain()`方法来检查Spark SQL用于执行查询的物理查询计划。
- en: '[PRE184]'
  id: totrans-607
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE185]'
  id: totrans-609
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: How it works...
  id: totrans-610
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The basic workflow for DataFrame using SQL is to first populate the DataFrame
    either through internal Scala data structures or via external data sources first,
    and then use the `createOrReplaceTempView()` call to register the DataFrame as
    a SQL-like artifact.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SQL处理DataFrame的基本工作流程是首先通过内部Scala数据结构或外部数据源填充DataFrame，然后使用`createOrReplaceTempView()`调用将DataFrame注册为类似SQL的工件。
- en: When you use DataFrames, you have the benefit of additional metadata that Spark
    stores (whether API or SQL approach) which can benefit you during the coding and
    execution.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用DataFrame时，您将获得Spark存储的额外元数据的好处（无论是API还是SQL方法），这在编码和执行过程中都会对您有所帮助。
- en: While RDDs are still the workhorses of core Spark, the trend is toward the DataFrame
    approach which has successfully shown its capabilities in languages such as Python/Pandas
    or R.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RDD仍然是核心Spark的主力军，但趋势是采用DataFrame方法，它已经在Python/Pandas或R等语言中成功展示了其能力。
- en: There's more...
  id: totrans-614
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'There has been a change for registration of a DataFrame as a table. Refer to
    this:'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame作为表进行注册已经发生了变化。请参考这个：
- en: 'For versions prior to Spark 2.0.0: `registerTempTable()`'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Spark 2.0.0之前的版本：`registerTempTable()`
- en: 'For Spark version 2.0.0 and previous: `createOrReplaceTempView()`'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Spark版本2.0.0及之前：`createOrReplaceTempView()`
- en: 'Pre-Spark 2.0.0 to register a DataFrame as a SQL table like artifact:'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0.0之前将DataFrame注册为SQL表类似的工件：
- en: Before we can use the DataFrame for queries via SQL, we have to register the
    DataFrame as a temp table so the SQL statements can refer to it without any Scala/Spark
    syntax. This step may cause confusion for many beginners as we are not creating
    any table (temp or permanent), but the call `registerTempTable()` creates a name
    in SQL land that the SQL statements can refer to without additional UDF or without
    any domain-specific query language.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以通过SQL查询使用DataFrame之前，我们必须将DataFrame注册为临时表，以便SQL语句可以引用它而不需要任何Scala/Spark语法。这一步可能会让许多初学者感到困惑，因为我们并没有创建任何表（临时或永久），但是`registerTempTable()`调用在SQL领域创建了一个名称，SQL语句可以引用它而不需要额外的UDF或任何特定于领域的查询语言。
- en: 'Register the `CustDf` DataFrame as a name which SQL statements recognize as
    `customers`:'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`CustDf` DataFrame注册为SQL语句识别为`customers`的名称：
- en: '[PRE186]'
  id: totrans-621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: 'Register the `prodDf` DataFrame as a name which SQL statements recognize as
    `product`:'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`prodDf` DataFrame注册为SQL语句识别为`product`的名称：
- en: '[PRE187]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: 'Register the `saleDf` DataFrame as a name which SQL statements recognize as
    `sales`:'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`saleDf` DataFrame注册为SQL语句识别为`sales`的名称：
- en: '[PRE188]'
  id: totrans-625
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: 'To ensure completeness, we include the `import` statements that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保完整性，我们包括了我们在Spark 2.0.0之前使用的`import`语句来运行代码（即Spark 1.5.2）：
- en: '[PRE189]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: See also
  id: totrans-628
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame的文档可在[https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)找到。
- en: If you see any issues with implicit conversion, please double check to make
    sure you have included implicits `import` statement.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遇到隐式转换的问题，请仔细检查是否已经包含了隐式`import`语句。
- en: Example `import` statement for Spark 1.5.2
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 1.5.2的示例`import`语句
- en: '[PRE190]'
  id: totrans-632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: DataFrame is an extensive subsystem and deserves an entire book on its own.
    It makes complex data manipulation at scale available to SQL programmers.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame是一个庞大的子系统，值得单独写一本书。它使得规模化的复杂数据操作对SQL程序员可用。
- en: Working with the Dataset API using a Scala Sequence
  id: totrans-634
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scala序列处理数据集API
- en: In this recipe, we examine the new Dataset and how it works with the *seq* Scala
    data structure. We often see a relationship between the LabelPoint data structure
    used with ML libraries and a Scala sequence (that is, seq data structure) that
    play nicely with dataset.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将研究新的Dataset以及它如何与*seq* Scala数据结构一起工作。我们经常看到ML库中使用的LabelPoint数据结构与Scala序列（即seq数据结构）之间存在关系，这些数据结构与数据集很好地配合。
- en: The Dataset is being positioned as a unifying API going forward. It is important
    to note that DataFrame is still available as an alias described as `Dataset[Row]`.
    We have covered the SQL examples extensively via DataFrame recipes, so we concentrate
    our efforts on other variations for dataset.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集被定位为未来的统一API。重要的是要注意，DataFrame仍然作为别名`Dataset[Row]`可用。我们已经通过DataFrame示例广泛涵盖了SQL示例，因此我们将集中精力研究数据集的其他变化。
- en: How to do it...
  id: totrans-637
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-638
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-639
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置
- en: '[PRE191]'
  id: totrans-640
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: Import the necessary packages for a Spark session to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  id: totrans-641
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以获取对集群的访问以及`Log4j.Logger`以减少Spark产生的输出量。
- en: '[PRE192]'
  id: totrans-642
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: Define a Scala `case class` to model data for processing, and the `Car` class
    will represent electric and hybrid cars.
  id: totrans-643
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala `case class`来对数据进行建模，`Car`类将代表电动和混合动力汽车。
- en: '[PRE193]'
  id: totrans-644
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: Let's create a Scala sequence and populate it with electric and hybrid cars.
  id: totrans-645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个Scala序列，并用电动车和混合动力车填充它。
- en: '[PRE194]'
  id: totrans-646
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: Configure output level to `ERROR` to reduce Spark's logging output.
  id: totrans-647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别配置为`ERROR`，以减少Spark的日志输出。
- en: '[PRE195]'
  id: totrans-648
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: Create a SparkSession yielding access to the Spark cluster, including the underlying
    session object attributes and functions.
  id: totrans-649
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个SparkSession，以便访问Spark集群，包括底层会话对象的属性和函数。
- en: '[PRE196]'
  id: totrans-650
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: Import Spark implicits, therefore adding in behavior with only an import.
  id: totrans-651
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark implicits，只需导入即可添加行为。
- en: '[PRE197]'
  id: totrans-652
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: Next, we will create a Dataset from the car data sequence utilizing the Spark
    session's `createDataset()` method.
  id: totrans-653
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将利用Spark会话的`createDataset()`方法从汽车数据序列创建一个数据集。
- en: '[PRE198]'
  id: totrans-654
  prefs: []
  type: TYPE_PRE
  zh: '[PRE198]'
- en: Let's print out the results as confirmation that our method invocation transformed
    the sequence into a Spark Dataset by invoking the show method.
  id: totrans-655
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印出结果，确认我们的方法调用将序列转换为Spark数据集。
- en: '[PRE199]'
  id: totrans-656
  prefs: []
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '![](img/00075.gif)'
  id: totrans-657
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00075.gif)'
- en: Print out the Dataset's implied column names. We can now use class attribute
    names as column names.
  id: totrans-658
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出数据集的隐含列名。现在我们可以使用类属性名称作为列名。
- en: '[PRE200]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE200]'
- en: Let's show the automatically generated schema, and validate inferred data types.
  id: totrans-660
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们展示自动生成的模式，并验证推断出的数据类型。
- en: '[PRE201]'
  id: totrans-661
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: Finally, we will filter the Dataset on price referring to the `Car` class attribute
    price as a column and show results.
  id: totrans-662
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将按价格过滤数据集，引用`Car`类属性价格作为列，并展示结果。
- en: '[PRE202]'
  id: totrans-663
  prefs: []
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '![](img/00076.gif)'
  id: totrans-664
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00076.gif)'
- en: We close the program by stopping the Spark session.
  id: totrans-665
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止Spark会话来关闭程序。
- en: '[PRE203]'
  id: totrans-666
  prefs: []
  type: TYPE_PRE
  zh: '[PRE203]'
- en: How it works...
  id: totrans-667
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we introduced Spark's Dataset feature which first appeared in
    Spark 1.6 and which was further refined in subsequent releases. First, we created
    an instance of a Dataset from a Scala sequence with the help of the `createDataset()`
    method belonging to the Spark session. The next step was to print out meta information
    about the generated Datatset to establish that the creation transpired as expected.
    Finally, snippets of Spark SQL were used to filter the Dataset by the price column
    for any price greater than $50, 000.00 and show the final results of execution.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们介绍了Spark的数据集功能，这个功能首次出现在Spark 1.6中，并在随后的版本中进一步完善。首先，我们使用Spark会话的`createDataset()`方法从Scala序列创建了一个数据集实例。接下来，我们打印出关于生成的数据集的元信息，以确保创建的过程符合预期。最后，我们使用Spark
    SQL的片段来按价格列过滤数据集，找出价格大于$50,000.00的数据，并展示执行的最终结果。
- en: There's more...
  id: totrans-669
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Dataset has a view called [DataFrame](https://spark.apache.org/docs/2.0.0/api/scala/org/apache/spark/sql/package.html#DataFrame=org.apache.spark.sql.Dataset%5Borg.apache.spark.sql.Row%5D),
    which is a Dataset of [row](https://spark.apache.org/docs/2.0.0/api/scala/org/apache/spark/sql/Row.html)s
    which is untyped. The Dataset still retains all the transformation abilities of
    RDD such as `filter()`, `map()`, `flatMap()`, and so on. This is one of the reasons
    we find Datasets easy to use if we have programmed in Spark using RDDs.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集有一个名为[DataFrame](https://spark.apache.org/docs/2.0.0/api/scala/org/apache/spark/sql/package.html#DataFrame=org.apache.spark.sql.Dataset%5Borg.apache.spark.sql.Row%5D)的视图，它是一个未命名的[行](https://spark.apache.org/docs/2.0.0/api/scala/org/apache/spark/sql/Row.html)数据集。数据集仍然保留了RDD的所有转换能力，比如`filter()`、`map()`、`flatMap()`等等。这是我们发现数据集易于使用的原因之一，如果我们已经使用RDD在Spark中编程。
- en: See also
  id: totrans-671
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for Dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的文档可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)找到。
- en: KeyValue grouped dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KeyValue分组的数据集可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)找到。
- en: Relational grouped dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系分组的数据集可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)找到。
- en: Creating and using Datasets from RDDs and back again
  id: totrans-675
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从RDD创建和使用数据集，然后再转回去
- en: In this recipe, we explore how to use RDD and interact with Dataset to build
    a multi-stage machine learning pipeline. Even though the Dataset (conceptually
    thought of as RDD with strong type-safety) is the way forward, you still have
    to be able to interact with other machine learning algorithms or codes that return/operate
    on RDD for either legacy or coding reasons. In this recipe, we also explore how
    to create and convert from Dataset to RDD and back.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探讨了如何使用RDD并与数据集交互，以构建多阶段的机器学习流水线。尽管数据集（在概念上被认为是具有强类型安全性的RDD）是未来的发展方向，但您仍然必须能够与其他机器学习算法或返回/操作RDD的代码进行交互，无论是出于传统还是编码的原因。在这个示例中，我们还探讨了如何创建和转换数据集到RDD，然后再转回去。
- en: How to do it...
  id: totrans-677
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-678
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-679
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE204]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE204]'
- en: Import the necessary packages for Spark session to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  id: totrans-681
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark会话所需的包，以便访问集群和`Log4j.Logger`，以减少Spark产生的输出量。
- en: '[PRE205]'
  id: totrans-682
  prefs: []
  type: TYPE_PRE
  zh: '[PRE205]'
- en: Define a Scala case class to model data for processing.
  id: totrans-683
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala case类来建模处理数据。
- en: '[PRE206]'
  id: totrans-684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE206]'
- en: Let's create a Scala sequence and populate it with electric and hybrid cars.
  id: totrans-685
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个Scala序列，并用电动车和混合动力车填充它。
- en: '[PRE207]'
  id: totrans-686
  prefs: []
  type: TYPE_PRE
  zh: '[PRE207]'
- en: Set output level to `ERROR` to reduce Spark's logging output.
  id: totrans-687
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`，以减少Spark的日志输出。
- en: '[PRE208]'
  id: totrans-688
  prefs: []
  type: TYPE_PRE
  zh: '[PRE208]'
- en: Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster.
  id: totrans-689
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器模式初始化Spark会话，从而为Spark集群提供入口点。
- en: '[PRE209]'
  id: totrans-690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE209]'
- en: Next, we retrieve a reference to the Spark context from the Spark session, because
    we will need it later to generate an RDD.
  id: totrans-691
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们从Spark会话中检索对Spark上下文的引用，因为我们稍后将需要它来生成RDD。
- en: '[PRE210]'
  id: totrans-692
  prefs: []
  type: TYPE_PRE
  zh: '[PRE210]'
- en: Import Spark implicits, therefore adding in behavior with only an import.
  id: totrans-693
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark implicits，因此只需导入即可添加行为。
- en: '[PRE211]'
  id: totrans-694
  prefs: []
  type: TYPE_PRE
  zh: '[PRE211]'
- en: Let's make an RDD from the car data sequence.
  id: totrans-695
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从汽车数据序列中创建一个RDD。
- en: '[PRE212]'
  id: totrans-696
  prefs: []
  type: TYPE_PRE
  zh: '[PRE212]'
- en: Next, we will create a Dataset from the RDD containing the car data by making
    use of Spark's session `createDataset()` method.
  id: totrans-697
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用Spark的会话`createDataset()`方法从包含汽车数据的RDD创建数据集。
- en: '[PRE213]'
  id: totrans-698
  prefs: []
  type: TYPE_PRE
  zh: '[PRE213]'
- en: Let's print out the Dataset to validate that creation happened as we would expect
    via the `show` method.
  id: totrans-699
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印出数据集，以验证创建是否按预期进行，通过`show`方法。
- en: '[PRE214]'
  id: totrans-700
  prefs: []
  type: TYPE_PRE
  zh: '[PRE214]'
- en: On running the previous code, you will get the following output.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出。
- en: '![](img/00077.gif)'
  id: totrans-702
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00077.gif)'
- en: Next, we will print out the implied column names.
  id: totrans-703
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将打印出暗示的列名。
- en: '[PRE215]'
  id: totrans-704
  prefs: []
  type: TYPE_PRE
  zh: '[PRE215]'
- en: Let's show the automatically generated schema, and validate that the inferred
    data types are correct.
  id: totrans-705
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们显示自动生成的模式，并验证推断的数据类型是否正确。
- en: '[PRE216]'
  id: totrans-706
  prefs: []
  type: TYPE_PRE
  zh: '[PRE216]'
- en: Now, let's group the Dataset by make, and count the number of makes in our dataset.
  id: totrans-707
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们按制造商对数据集进行分组，并计算数据集中制造商的数量。
- en: '[PRE217]'
  id: totrans-708
  prefs: []
  type: TYPE_PRE
  zh: '[PRE217]'
- en: On running the previous code, you will get the following output.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出。
- en: '![](img/00078.gif)'
  id: totrans-710
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00078.gif)'
- en: The next step will use Spark's SQL on the Dataset, filtering by make for the
    value of Tesla, and transforming the resulting Dataset back into an RDD.
  id: totrans-711
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步将使用Spark的SQL对数据集进行过滤，按制造商过滤特斯拉的值，并将结果数据集转换回RDD。
- en: '[PRE218]'
  id: totrans-712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE218]'
- en: Finally, display the contents of the RDD, taking advantage of the `foreach()`
    method.
  id: totrans-713
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，利用`foreach()`方法显示RDD的内容。
- en: '[PRE219]'
  id: totrans-714
  prefs: []
  type: TYPE_PRE
  zh: '[PRE219]'
- en: We close the program by stopping the Spark session.
  id: totrans-715
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止Spark会话来关闭程序。
- en: '[PRE220]'
  id: totrans-716
  prefs: []
  type: TYPE_PRE
  zh: '[PRE220]'
- en: How it works...
  id: totrans-717
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this section, we transformed an RDD into a Dataset and finally transformed
    it back to an RDD. We began with a Scala sequence which was changed into an RDD.
    After the creation of the RDD, invocation of Spark's session `createDataset()`
    method occurred, passing the RDD as an argument while receiving a Dataset as the
    result.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将RDD转换为数据集，最终将其转换回RDD。我们从一个Scala序列开始，然后将其更改为RDD。创建RDD后，调用了Spark的会话`createDataset()`方法，将RDD作为参数传递，同时接收数据集作为结果。
- en: Next, the Dataset was grouped by the make column, counting the existence of
    various makes of cars. The next step involved filtering the Dataset for makes
    of Tesla and transforming the results back to an RDD. Finally, we displayed the
    resulting RDD by way of the RDD `foreach()` method.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，数据集按制造商列进行分组，计算各种汽车制造商的存在。下一步涉及过滤制造商为特斯拉的数据集，并将结果转换回RDD。最后，我们通过RDD的`foreach()`方法显示了结果RDD。
- en: There's more...
  id: totrans-720
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The Dataset source file in Spark is only about 2500+ lines of Scala code. It
    is a very nice piece of code which can be leveraged for specialization under Apache
    license. We list the following URL and encourage you to at least scan the file
    and understand how buffering comes into play when using Dataset.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的数据集源文件只有大约2500多行的Scala代码。这是一段非常好的代码，可以在Apache许可下进行专门化利用。我们列出以下URL，并鼓励您至少扫描该文件并了解在使用数据集时缓冲如何发挥作用。
- en: Source code for Datasets hosted on GitHub is available at [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala).
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 托管在GitHub上的数据集源代码可在[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala)找到。
- en: See also
  id: totrans-723
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for Dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的文档可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)找到
- en: KeyValue grouped Dataset can be found at[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)找到分组的数据集
- en: Relational grouped Dataset can be found at[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)找到关系分组的数据集
- en: Working with JSON using the Dataset API and SQL together
  id: totrans-727
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据集API和SQL一起处理JSON
- en: In this recipe, we explore how to use JSON with Dataset. The JSON format has
    rapidly become the de-facto standard for data interoperability in the last 5 years.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将探讨如何使用数据集处理JSON。在过去5年中，JSON格式已迅速成为数据互操作性的事实标准。
- en: We explore how Dataset uses JSON and executes API commands like `select()`.
    We then progress by creating a view (that is, `createOrReplaceTempView()`) and
    then execute a SQL query to demonstrate how to query against a JSON file using
    API and SQL with ease.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了数据集如何使用JSON并执行API命令，如`select()`。然后，我们通过创建视图（即`createOrReplaceTempView()`）并执行SQL查询来展示如何轻松使用API和SQL来查询JSON文件。
- en: How to do it...
  id: totrans-730
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-731
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'We will use a JSON data file named `cars.json` which has been created for this
    example:'
  id: totrans-732
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用名为`cars.json`的JSON数据文件，该文件是为此示例创建的：
- en: '[PRE221]'
  id: totrans-733
  prefs: []
  type: TYPE_PRE
  zh: '[PRE221]'
- en: Set up the package location where the program will reside
  id: totrans-734
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置
- en: '[PRE222]'
  id: totrans-735
  prefs: []
  type: TYPE_PRE
  zh: '[PRE222]'
- en: Import the necessary packages for the Spark session to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  id: totrans-736
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark会话所需的包，以便访问集群和`Log4j.Logger`以减少Spark产生的输出量。
- en: '[PRE223]'
  id: totrans-737
  prefs: []
  type: TYPE_PRE
  zh: '[PRE223]'
- en: Define a Scala `case class` to model data for processing.
  id: totrans-738
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala `case class`来对数据进行建模。
- en: '[PRE224]'
  id: totrans-739
  prefs: []
  type: TYPE_PRE
  zh: '[PRE224]'
- en: Set output level to `ERROR` to reduce Spark's logging output.
  id: totrans-740
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出。
- en: '[PRE225]'
  id: totrans-741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE225]'
- en: Initialize a Spark session creating an entry point for access to the Spark cluster.
  id: totrans-742
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个Spark会话，为访问Spark集群创建一个入口点。
- en: '[PRE226]'
  id: totrans-743
  prefs: []
  type: TYPE_PRE
  zh: '[PRE226]'
- en: Import Spark implicits, therefore adding in behavior with only an import.
  id: totrans-744
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark implicits，只需导入即可添加行为。
- en: '[PRE227]'
  id: totrans-745
  prefs: []
  type: TYPE_PRE
  zh: '[PRE227]'
- en: Now, we will load the JSON data file into memory, specifying the class type
    as `Car`.
  id: totrans-746
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将JSON数据文件加载到内存中，指定类类型为`Car`。
- en: '[PRE228]'
  id: totrans-747
  prefs: []
  type: TYPE_PRE
  zh: '[PRE228]'
- en: Let's print out the data from our generated Dataset of type `Car`.
  id: totrans-748
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印出我们生成的`Car`类型数据集中的数据。
- en: '[PRE229]'
  id: totrans-749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE229]'
- en: '![](img/00079.gif)'
  id: totrans-750
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00079.gif)'
- en: Next, we will display column names of the Dataset to verify that the cars' JSON
    attribute names were processed correctly.
  id: totrans-751
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将显示数据集的列名，以验证汽车的JSON属性名称是否被正确处理。
- en: '[PRE230]'
  id: totrans-752
  prefs: []
  type: TYPE_PRE
  zh: '[PRE230]'
- en: Let's see the automatically generated schema and validate the inferred data
    types.
  id: totrans-753
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看自动生成的模式并验证推断出的数据类型。
- en: '[PRE231]'
  id: totrans-754
  prefs: []
  type: TYPE_PRE
  zh: '[PRE231]'
- en: In this step, we will select the Dataset's `make` column, removing duplicates
    by applying the `distinct` method and showing the results.
  id: totrans-755
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们将选择数据集的`make`列，通过应用`distinct`方法去除重复项，并显示结果。
- en: '[PRE232]'
  id: totrans-756
  prefs: []
  type: TYPE_PRE
  zh: '[PRE232]'
- en: '![](img/00080.gif)'
  id: totrans-757
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00080.gif)'
- en: Next, create a view on the cars Dataset so we can execute a literal Spark SQL
    query string against the dataset.
  id: totrans-758
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在汽车数据集上创建一个视图，以便我们可以针对数据集执行一个文字Spark SQL查询字符串。
- en: '[PRE233]'
  id: totrans-759
  prefs: []
  type: TYPE_PRE
  zh: '[PRE233]'
- en: Finally, we execute a Spark SQL query filtering the Dataset for electric cars,
    and returning only three of the defined columns.
  id: totrans-760
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们执行一个Spark SQL查询，过滤数据集以获取电动汽车，并仅返回三个已定义的列。
- en: '[PRE234]'
  id: totrans-761
  prefs: []
  type: TYPE_PRE
  zh: '[PRE234]'
- en: '![](img/00081.gif)'
  id: totrans-762
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00081.gif)'
- en: We close the program by stopping the Spark session.
  id: totrans-763
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止Spark会话来关闭程序。
- en: '[PRE235]'
  id: totrans-764
  prefs: []
  type: TYPE_PRE
  zh: '[PRE235]'
- en: How it works...
  id: totrans-765
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: It is extremely straightforward to read a **JavaScript Object Notation** (**JSON**)
    data file and to transform it into a Dataset with Spark. JSON has become a widely
    used data format over the past several years and Spark's support for the format
    is substantial.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 读取**JavaScript对象表示**（**JSON**）数据文件并将其转换为Spark数据集非常简单。在过去几年中，JSON已经成为广泛使用的数据格式，Spark对该格式的支持非常丰富。
- en: In the first part, we demonstrated loading JSON into a Dataset by means of built
    in JSON parsing functionality in Spark's session. You should take note of Spark's
    built-in functionality that transforms the JSON data into the car case class.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分中，我们演示了通过Spark会话中内置的JSON解析功能将JSON加载到数据集中。您应该注意Spark的内置功能，它将JSON数据转换为汽车case类。
- en: In the second part, we demonstrated Spark SQL being applied on the Dataset to
    wrangle the said data into a desirable state. We utilized the Dataset's select
    method to retrieve the `make` column and apply the `distinct` method for the removal
    of duplicates. Next, we set up a view on the cars Dataset, so we can apply SQL
    queries against it. Finally, we used the session's SQL method to execute a literal
    SQL query string against the Dataset, retrieving any items which are of kind electric.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分中，我们演示了在数据集上应用Spark SQL来整理所述数据以达到理想状态。我们利用数据集的select方法检索`make`列，并应用`distinct`方法来去除重复项。接下来，我们在汽车数据集上设置了一个视图，以便我们可以对其应用SQL查询。最后，我们使用会话的SQL方法来执行针对数据集的文字SQL查询字符串，检索任何属于电动汽车的项目。
- en: There's more...
  id: totrans-769
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: To fully understand and master the Dataset API, be sure to understand the concept
    of `Row` and `Encoder`.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全理解和掌握数据集API，请确保理解`Row`和`Encoder`的概念。
- en: Datasets follow the *lazy execution* paradigm, meaning that execution only occurs
    by invoking actions in Spark. When we execute an action, the Catalyst query optimizer
    produces a logical plan and generates a physical plan for optimized execution
    in a parallel distributed manner. See the figure in the introduction for all the
    detailed steps.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集遵循*延迟执行*范例，这意味着只有通过调用Spark中的操作才会执行。当我们执行操作时，Catalyst查询优化器会生成逻辑计划，并生成用于优化并行分布式执行的物理计划。有关所有详细步骤，请参见介绍中的图。
- en: Documentation for `Row` is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: '`Row`的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)找到。'
- en: Documentation for `Encoder` is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder)
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: '`Encoder`的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder)找到。'
- en: See also
  id: totrans-774
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for Dataset is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)找到。
- en: Documentation for KeyValue grouped Dataset is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KeyValue分组数据集的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)找到。
- en: Documentation for relational grouped Dataset [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系分组数据集的文档[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
- en: Again, be sure to download and explore the Dataset source file, which is about
    2500+ lines from GitHub. Exploring the Spark source code is the best way to learn
    advanced programming in Scala, Scala Annotations, and Spark 2.0 itself.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，请务必下载并探索数据集源文件，该文件来自GitHub，大约有2500多行。探索Spark源代码是学习Scala高级编程、Scala注解和Spark
    2.0本身的最佳途径。
- en: 'Noteworthy for Pre-Spark 2.0 users:'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Spark 2.0之前的用户值得注意的是：
- en: SparkSession is the single entry point into the system. SQLContext and HiveContext
    are replaced by SparkSession.
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkSession是系统的唯一入口点。SQLContext和HiveContext已被SparkSession取代。
- en: For Java users, be sure to replace DataFrame with `Dataset<Row>`.
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Java用户，请确保用`Dataset<Row>`替换DataFrame。
- en: Use the new catalog interface via SparkSession to execute `cacheTable()`, `dropTempView()`,
    `createExternalTable()`, and `ListTable()`, and so on.
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过SparkSession使用新的目录接口来执行`cacheTable()`、`dropTempView()`、`createExternalTable()`和`ListTable()`等操作。
- en: DataFrame and DataSet API
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame和DataSet API
- en: '`unionALL()` is deprecated and you should now use `union()` instead.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unionALL()`已被弃用，现在应该使用`union()`。'
- en: '`explode()` should be replaced by `functions.explode()` plus `select()` or
    `flatMap()`'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`explode()`应该被`functions.explode()`加上`select()`或`flatMap()`替换'
- en: '`registerTempTable` has been deprecated and replaced by `createOrReplaceTempView()`'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`registerTempTable`已被弃用，并被`createOrReplaceTempView()`取代'
- en: The `Dataset()` API source code (that is, `Dataset.scala` ) can be found via
    GitHub at [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala)
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dataset()` API源代码（即`Dataset.scala`）可以在GitHub上找到[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala)'
- en: Functional programming with the Dataset API using domain objects
  id: totrans-788
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用领域对象的DataSet API进行函数式编程
- en: In this recipe, we explore how functional programming works with Dataset. We
    use the Dataset and functional programming to separate the cars (domain object)
    by their models.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探索了如何使用DataSet进行函数式编程。我们使用DataSet和函数式编程来按照汽车的型号（领域对象）进行分离。
- en: How to do it...
  id: totrans-790
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-791
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: Use package instruction to provide the right path
  id: totrans-792
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用package指令提供正确的路径
- en: '[PRE236]'
  id: totrans-793
  prefs: []
  type: TYPE_PRE
  zh: '[PRE236]'
- en: Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  id: totrans-794
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的Spark上下文包以访问集群和`Log4j.Logger`以减少Spark产生的输出量。
- en: '[PRE237]'
  id: totrans-795
  prefs: []
  type: TYPE_PRE
  zh: '[PRE237]'
- en: Define a Scala case to contain our data for processing, and our car class will
    represent electric and hybrid cars.
  id: totrans-796
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala case来包含我们的数据进行处理，我们的汽车类将代表电动车和混合动力车。
- en: '[PRE238]'
  id: totrans-797
  prefs: []
  type: TYPE_PRE
  zh: '[PRE238]'
- en: Let's create a `Seq` populated with electric and hybrid cars.
  id: totrans-798
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个包含电动车和混合动力车的`Seq`。
- en: '[PRE239]'
  id: totrans-799
  prefs: []
  type: TYPE_PRE
  zh: '[PRE239]'
- en: Set output level to `ERROR` to reduce Spark's output.
  id: totrans-800
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的输出。
- en: '[PRE240]'
  id: totrans-801
  prefs: []
  type: TYPE_PRE
  zh: '[PRE240]'
- en: Create a SparkSession yielding access to the Spark cluster and underlying session
    object attributes such as the SparkContext and SparkSQLContext.
  id: totrans-802
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个SparkSession，以访问Spark集群和底层会话对象属性，如SparkContext和SparkSQLContext。
- en: '[PRE241]'
  id: totrans-803
  prefs: []
  type: TYPE_PRE
  zh: '[PRE241]'
- en: Import spark implicits, therefore adding in behavior with only an import.
  id: totrans-804
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入spark implicits，因此只需导入即可添加行为。
- en: '[PRE242]'
  id: totrans-805
  prefs: []
  type: TYPE_PRE
  zh: '[PRE242]'
- en: Now we will create a Dataset from the car data Seq utilizing the SparkSessions's
    `createDataset()` function.
  id: totrans-806
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用SparkSessions的`createDataset()`函数从汽车数据Seq创建一个DataSet。
- en: '[PRE243]'
  id: totrans-807
  prefs: []
  type: TYPE_PRE
  zh: '[PRE243]'
- en: Display the Dataset to understand how to transform data in subsequent steps.
  id: totrans-808
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示数据集以了解如何在后续步骤中转换数据。
- en: '[PRE244]'
  id: totrans-809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE244]'
- en: On running the previous code, you will get the following output.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出。
- en: '![](img/00082.gif)'
  id: totrans-811
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00082.gif)'
- en: Now we construct a functional sequence of steps to transform the original Dataset
    into data grouped by make with all various models attached.
  id: totrans-812
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们构建一个功能序列步骤，将原始数据集转换为按制造商分组的数据，并附上所有不同的型号。
- en: '[PRE245]'
  id: totrans-813
  prefs: []
  type: TYPE_PRE
  zh: '[PRE245]'
- en: Let's display results from our previous sequence of functional logic for validation.
  id: totrans-814
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们显示之前的函数逻辑序列的结果以进行验证。
- en: '[PRE246]'
  id: totrans-815
  prefs: []
  type: TYPE_PRE
  zh: '[PRE246]'
- en: On running the previous code, you will get the following output.
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出。
- en: '![](img/00083.gif)'
  id: totrans-817
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00083.gif)'
- en: We close the program by stopping the Spark session.
  id: totrans-818
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止Spark会话来关闭程序。
- en: '[PRE247]'
  id: totrans-819
  prefs: []
  type: TYPE_PRE
  zh: '[PRE247]'
- en: How it works...
  id: totrans-820
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this example, we use a Scala sequence data structure to hold the original
    data, which is a series of cars and their attributes. Using `createDataset()`*,*
    we create a DataSet and populate it. We then proceed to use the 'make' attribute
    with `groupBy` and `mapGroups()` to list cars by their models using a functional
    paradigm with DataSet. Using this form of functional programming with domain objects
    was not impossible before DataSet (for example, the case class with RDD or UDF
    with DataFrame), but the DataSet construct makes this easy and intrinsic.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用Scala序列数据结构来保存原始数据，即一系列汽车及其属性。使用`createDataset()`，我们创建一个DataSet并填充它。然后，我们使用`groupBy`和`mapGroups()`来使用函数范式与DataSet列出汽车的型号。在DataSet之前，使用这种形式的函数式编程与领域对象并不是不可能的（例如，使用RDD的case类或使用DataFrame的UDF），但是DataSet构造使这变得简单和内在化。
- en: There's more...
  id: totrans-822
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Be sure to include the `implicits` statement in all your DataSet coding:'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在所有的DataSet编码中包含`implicits`语句：
- en: '[PRE248]'
  id: totrans-824
  prefs: []
  type: TYPE_PRE
  zh: '[PRE248]'
- en: See also
  id: totrans-825
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The documentation for Datasets can be accessed at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的文档可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)中访问。
