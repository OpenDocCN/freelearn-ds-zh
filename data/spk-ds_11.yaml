- en: Chapter 11.  Building Data Science Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章：构建数据科学应用
- en: Data science applications are garnering a lot of excitement, mainly because
    of the promise they hold in harnessing data and extracting consumable results.
    There are already several successful data products that have had a transformative
    effect on our daily lives. The ubiquitous recommender systems, e-mail spam filters,
    and targeted advertisements and news content have become part and parcel of life.
    Music and movies have become data products streaming from providers such as iTunes
    and Netflix. Businesses, especially in the domains such as retail, are actively
    pursuing ways to gain a competitive advantage by studying the market and customer
    behavior using a data-driven approach.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学应用正在引起广泛关注，主要因为它们在利用数据并提取可消费的结果方面的巨大潜力。已经有几个成功的数据产品对我们的日常生活产生了深远影响。无处不在的推荐系统、电子邮件垃圾邮件过滤器、定向广告和新闻内容已经成为生活的一部分。音乐和电影也已成为数据产品，从iTunes和Netflix等平台流媒体播放。企业，尤其是在零售等领域，正在积极寻求通过数据驱动的方法研究市场和客户行为，从而获得竞争优势。
- en: We have discussed the data analytics workflow up to the model building phase
    so far in the previous chapters. But the real value of a model is when it is actually
    deployed in a production system. The end product, the fruit of a data science
    workflow, is an operationalized data product. In this chapter, we discuss this
    culminating stage of the data analytics workflow. We will not get into actual
    code snippets but take a step back to get the complete picture, including the
    non-technical aspects.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经讨论了数据分析工作流的模型构建阶段。但模型的真正价值体现在它实际部署到生产系统中的时候。最终产品，即数据科学工作流的成果，是一个已操作化的数据产品。在本章中，我们将讨论数据分析工作流的这一关键阶段。我们不会涉及具体的代码片段，而是退一步，全面了解整个过程，包括非技术性方面。
- en: The complete picture is not limited to the development process alone. It comprises
    the user application, developments in Spark itself, as well as rapid changes happening
    in the big data landscape. We'll start with the development process of the user
    application first and discuss various options at each stage. Then we'll delve
    into the features and enhancements in the latest Spark 2.0 release and future
    plans. Finally, we'll attempt to give a broad overview of the big data trends,
    especially the Hadoop ecosystem. References and useful links are included in individual
    sections in addition to the end of the chapter for further information about the
    specific context.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的图景不仅限于开发过程。它还包括用户应用程序、Spark本身的开发，以及大数据领域中快速变化的情况。我们将首先从用户应用程序的开发过程开始，并讨论每个阶段的各种选项。接着，我们将深入了解最新Spark
    2.0版本中的特性和改进以及未来计划。最后，我们将尝试全面概述大数据趋势，特别是Hadoop生态系统。此外，每个部分的末尾会提供相关参考资料和有用链接，供读者进一步了解特定的背景信息。
- en: Scope of development
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发范围
- en: 'Data analytics workflow can be roughly divided into two phases, the build phase
    and the operationalization phase. The first phase is usually a one-time exercise,
    with heavy human intervention. Once we''ve attained reasonable end results, we
    are ready to operationalize the product. The second phase starts with the models
    generated in the first phase and makes them available as a part of some production
    workflow. In this section, we''ll discuss the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析工作流大致可分为两个阶段：构建阶段和操作化阶段。第一阶段通常是一次性的工作，且需要大量人工干预。一旦我们获得了合理的最终结果，就可以准备将产品操作化。第二阶段从第一阶段生成的模型开始，并将其作为生产工作流的一部分进行部署。在本节中，我们将讨论以下内容：
- en: Expectations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期望
- en: Presentation options
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示选项
- en: Development and testing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发与测试
- en: Data quality management
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量管理
- en: Expectations
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 期望
- en: 'The primary goal of data science applications is to build "actionable" insights,
    actionable being the keyword. Many use cases such as fraud detection need the
    insights to be generated and made available in a consumable fashion in near real
    time, if you expect any action-ability at all. The end users of the data product
    vary with the use case. They may be customers of an e-commerce site or a decision
    maker of a major conglomerate. The end user need not always be a human being.
    It could be a risk assessment software tool in a financial institution. A one-size-fits-all
    approach does not fit in with many software products, and data products are no
    exception. However, there are some common expectations for data products, as listed
    here:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学应用的主要目标是构建“可操作”的洞察，"可操作"是关键字。许多使用案例，如欺诈检测，要求洞察必须生成并以可消费的方式接近实时地提供，才能期待有任何行动的可能。数据产品的最终用户根据使用案例而不同。它们可能是电子商务网站的客户，或者是某大型企业的决策者。最终用户不一定总是人类，可能是金融机构中的风险评估软件工具。单一的通用方法并不适用于许多软件产品，数据产品也不例外。然而，数据产品有一些共同的期望，如下所列：
- en: The first and foremost expectation is that the insight generation time frame
    based on real-world data should be within "actionable" timeframes. The actual
    time frame varies based on the use case.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首要的期望是，基于真实世界数据的洞察生成时间框架应处于“可操作”时间范围内。实际的时间框架会根据使用案例而有所不同。
- en: The data product should integrate into some (often already existing) production
    workflow.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据产品应能够融入某些（通常是已经存在的）生产工作流程中。
- en: The insights should be translated into something that people can use instead
    of obscure numbers or hard-to-interpret charts. The presentation should be unobtrusive.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 洞察结果应被转化为人们可以使用的东西，而不是晦涩难懂的数字或难以解释的图表。展示方式应该是简洁的。
- en: The data product should have the ability to fine-tune itself (self-adapting)
    based on the incoming data inputs.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据产品应该具备根据输入的数据自我调整（自适应）的能力。
- en: Ideally, there has to be some way to receive human feedback, which can be used
    as a source for self-tuning.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理想情况下，必须有某种方式接收人工反馈，并将其用作自我调节的来源。
- en: There should be a mechanism that quantitatively assesses its effectiveness periodically
    and automatically.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该有一个机制，定期且自动地定量评估其有效性。
- en: Presentation options
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演示选项
- en: The varied nature of data products calls for varied modes of presentation. Sometimes
    the end result of a data analytics exercise is to publish a research paper. Sometimes
    it could be a part of a dashboard, where this becomes one of several sources publishing
    results on a single web page. They may be overt and targeted for human consumption,
    or covert and feeding into some other software application. You may use a general-purpose
    engine such as Spark to build your solution, but the presentation must be highly
    aligned to the targeted user base.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据产品的多样性要求不同的展示方式。有时候，数据分析的最终结果是发布研究论文。有时候，它可能是仪表板的一部分，成为多个来源在同一网页上发布结果的其中之一。它们可能是显式的，目标是供人类使用，或者是隐式的，供其他软件应用使用。你可能会使用像Spark这样的通用引擎来构建你的解决方案，但展示方式必须高度对准目标用户群体。
- en: Sometimes all you need to do is write an e-mail with your findings or just export
    a CSV file of insights. Or you may have to develop a dedicated web application
    around your data product. Some other common options are discussed here, and you
    have to choose the right one that fits the problem on hand.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，你所需要做的只是写一封电子邮件，分享你的发现，或者仅仅导出一个CSV文件的洞察结果。或者，你可能需要围绕数据产品开发一个专门的Web应用程序。这里讨论了一些常见的选项，你必须选择适合当前问题的那一个。
- en: Interactive notebooks
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 互动笔记本
- en: Interactive notebooks are web applications that allow you to create and share
    documents that contain code chunks, results, equations, images, videos, and explanation
    text. They may be viewed as executable documents or REPL shells with visualization
    and equation support. These documents can be exported as PDFs, Markdown, or HTML.
    Notebooks contain several "kernels" or "computational engines" that execute code
    chunks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 互动笔记本是网络应用程序，允许你创建和分享包含代码块、结果、方程式、图像、视频和解释文本的文档。它们可以作为可执行文档或具有可视化和方程式支持的REPL
    Shell进行查看。这些文档可以导出为PDF、Markdown或HTML格式。笔记本包含多个“内核”或“计算引擎”，用于执行代码块。
- en: Interactive notebooks are the most suitable choice if the end goal of your data
    analytics workflow is to generate a written report. There are several notebooks
    and many of them have Spark support. These notebooks are useful tools during the
    exploration phase also. We have already introduced IPython and Zeppelin notebooks
    in previous chapters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 互动式笔记本是如果你的数据分析工作流的最终目标是生成书面报告时最合适的选择。市面上有几种笔记本，并且其中很多都支持 Spark。这些笔记本在探索阶段也非常有用。我们在前几章已经介绍过
    IPython 和 Zeppelin 笔记本。
- en: References
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'The IPython Notebook: A Comprehensive Tool for Data Science: [http://conferences.oreilly.com/strata/strata2013/public/schedule/detail/27233](http://conferences.oreilly.com/strata/strata2013/public/schedule/detail/27233)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IPython Notebook：数据科学的综合工具：[http://conferences.oreilly.com/strata/strata2013/public/schedule/detail/27233](http://conferences.oreilly.com/strata/strata2013/public/schedule/detail/27233)
- en: 'Sparkly Notebook: Interactive Analysis and Visualization with Spark: [http://www.slideshare.net/felixcss/sparkly-notebook-interactive-analysis-and-visualization-with-spark](http://www.slideshare.net/felixcss/sparkly-notebook-interactive-analysis-and-visualization-with-spark)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sparkly Notebook：与 Spark 进行交互式分析与可视化：[http://www.slideshare.net/felixcss/sparkly-notebook-interactive-analysis-and-visualization-with-spark](http://www.slideshare.net/felixcss/sparkly-notebook-interactive-analysis-and-visualization-with-spark)
- en: Web API
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Web API
- en: 'An **Application Programming Interface** (**API**) is a software-to-software
    interface; a specification that describes the available functionality, how it
    must be used, and what the inputs and outputs are. The software (service) provider
    exposes some of its functionality as an API. A developer may develop a software
    component that consumes this API. For example, Twitter offers APIs to get or post
    data onto Twitter or to query data programmatically. A Spark enthusiast may write
    a software component that automatically collects all tweets on #Spark, categorizes
    according to their requirements, and publishes that data on their personal website.
    Web APIs are a type of APIs where the interface is defined as a set of **Hypertext
    Transfer Protocol** (**HTTP**) request messages along with a definition of the
    structure of response messages. Nowadays REST-ful (Representational State Transfer)
    have become the de facto standard.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用程序编程接口**（**API**）是软件与软件之间的接口；它是一个描述可用功能、如何使用这些功能以及输入输出是什么的规范。软件（服务）提供方将其某些功能暴露为API。开发者可以开发一个软件组件来消费这个API。例如，Twitter
    提供 API 来获取或发布数据到 Twitter，或者通过编程方式查询数据。一位 Spark 爱好者可以编写一个软件组件，自动收集所有关于 #Spark 的推文，按需求进行分类，并将这些数据发布到他们的个人网站。Web
    API 是一种接口，其中接口被定义为一组**超文本传输协议**（**HTTP**）请求消息，并附带响应消息结构的定义。如今，RESTful（表现层状态转移）已成为事实上的标准。'
- en: You can implement your data product as an API, and perhaps this is the most
    powerful option. It can then be plugged into one or more applications, say the
    management dashboard as well as the marketing analytics workflow. You may develop
    a domain specific "insights-as-a-service" as a public Web API with a subscription
    model. The simplicity and ubiquity of Web APIs make them the most compelling choice
    for building data products.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将你的数据产品实现为一个 API，也许这是最强大的选择。它可以插入到一个或多个应用中，比如管理仪表板以及市场营销分析工作流。你可能会开发一个特定领域的“洞察即服务”，作为一个带订阅模式的公共
    Web API。Web API 的简洁性和普及性使其成为构建数据产品时最具吸引力的选择。
- en: References
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 参考文献
- en: Application programming interface: [https://en.wikipedia.org/wiki/Application_programming_interface](https://en.wikipedia.org/wiki/Application_programming_interface)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序编程接口：[https://en.wikipedia.org/wiki/Application_programming_interface](https://en.wikipedia.org/wiki/Application_programming_interface)
- en: Ready for APIs? Three steps to unlock the data economy's most promising channel:[http://www.forbes.com/sites/mckinsey/2014/01/07/ready-for-apis-three-steps-to-unlock-the-data-economys-most-promising-channel/#61e7103b89e5](http://www.forbes.com/sites/mckinsey/2014/01/07/ready-for-apis-three-steps-to-unlock-the-data-economys-most-promising-channel/#61e7103b89e5)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备好使用 API 了吗？三步解锁数据经济最有前景的渠道：[http://www.forbes.com/sites/mckinsey/2014/01/07/ready-for-apis-three-steps-to-unlock-the-data-economys-most-promising-channel/#61e7103b89e5](http://www.forbes.com/sites/mckinsey/2014/01/07/ready-for-apis-three-steps-to-unlock-the-data-economys-most-promising-channel/#61e7103b89e5)
- en: How Insights-as-a-service is growing based on big data: [http://www.kdnuggets.com/2015/12/insights-as-a-service-big-data.html](http://www.kdnuggets.com/2015/12/insights-as-a-service-big-data.html)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何基于大数据发展洞察即服务：[http://www.kdnuggets.com/2015/12/insights-as-a-service-big-data.html](http://www.kdnuggets.com/2015/12/insights-as-a-service-big-data.html)
- en: PMML and PFA
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PMML 和 PFA
- en: Sometimes you may have to expose your model in a way that other data mining
    tools can understand. The model and the complete pre- and post-processing steps
    should be converted into a standard format. PMML and PFA are two such standard
    formats in the data mining domain.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你可能需要以其他数据挖掘工具能理解的方式暴露你的模型。模型以及所有的预处理和后处理步骤应该转换为标准格式。PMML 和 PFA 就是数据挖掘领域的两种标准格式。
- en: '**Predictive Model Markup Language** (**PMML**) is an XML-based predictive
    model interchange format and Apache Spark API convert models into PMML out of
    the box. A PMML message may contain a myriad of data transformations as well as
    one or more predictive models. Different data mining tools can export or import
    PMML messages without the need for custom code.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测模型标记语言**（**PMML**）是一种基于 XML 的预测模型交换格式，Apache Spark API 可以直接将模型转换为 PMML。一个
    PMML 消息可以包含大量的数据转换，以及一个或多个预测模型。不同的数据挖掘工具可以在无需定制代码的情况下导入或导出 PMML 消息。'
- en: '**Portable Format for Analytics** (**PFA**) is the next generation of predictive
    model interchange format. It exchanges JSON documents and straightaway inherits
    all advantages of JSON documents as against XML documents. In addition, PFA is
    more flexible than PMML.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**分析的可移植格式**（**PFA**）是下一代预测模型交换格式。它交换 JSON 文档，并直接继承了 JSON 文档相比 XML 文档的所有优点。此外，PFA
    比 PMML 更具灵活性。'
- en: References
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'PMML FAQ: Predictive Model Markup Language: [http://www.kdnuggets.com/2013/01/pmml-faq-predictive-model-markup-language.html](http://www.kdnuggets.com/2013/01/pmml-faq-predictive-model-markup-language.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PMML 常见问题解答：预测模型标记语言：[http://www.kdnuggets.com/2013/01/pmml-faq-predictive-model-markup-language.html](http://www.kdnuggets.com/2013/01/pmml-faq-predictive-model-markup-language.html)
- en: 'Portable Format for Analytics: moving models to production: [http://www.kdnuggets.com/2016/01/portable-format-analytics-models-production.html](http://www.kdnuggets.com/2016/01/portable-format-analytics-models-production.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析的可移植格式：将模型移至生产环境：[http://www.kdnuggets.com/2016/01/portable-format-analytics-models-production.html](http://www.kdnuggets.com/2016/01/portable-format-analytics-models-production.html)
- en: What is PFA for?: [http://dmg.org/pfa/docs/motivation/](http://dmg.org/pfa/docs/motivation/)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PFA 是做什么的？：[http://dmg.org/pfa/docs/motivation/](http://dmg.org/pfa/docs/motivation/)
- en: Development and testing
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发与测试
- en: 'Apache Spark is a general-purpose cluster computing system that can run both
    by itself or over several existing cluster managers such as Apache Mesos, Hadoop,
    Yarn, and Amazon EC2\. In addition, several big data and enterprise software companies
    have already integrated Spark into their offerings: Microsoft Azure HDInsight,
    Cloudera, IBM Analytics for Apache Spark, SAP HANA, and the list goes on. Databricks,
    a company founded by the creators of Apache Spark, have their own product for
    data science workflow, from ingestion to production. Your responsibility is to
    understand your organizational requirements and existing talent pool and decide
    which option is the best for you.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个通用的集群计算系统，可以独立运行，也可以在多个现有集群管理器上运行，如 Apache Mesos、Hadoop、Yarn
    和 Amazon EC2。此外，许多大数据和企业软件公司已经将 Spark 集成到他们的产品中：Microsoft Azure HDInsight、Cloudera、IBM
    Analytics for Apache Spark、SAP HANA，等等。Databricks 是由 Apache Spark 创始人创办的公司，提供自己的数据科学工作流产品，涵盖从数据获取到生产的全过程。你的责任是了解组织的需求和现有的人才储备，并决定哪个选项最适合你。
- en: Regardless of the option chosen, follow the usual best practices in any software
    development life cycle, such as version control and peer reviews. Try to use high-level
    APIs wherever applicable. The data transformation pipelines used in production
    should be the same as the ones used in building the model. Document any questions
    that arise during the data analytics workflow. Often these may result in business
    process improvements.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选择哪种选项，都应遵循软件开发生命周期中的常规最佳实践，如版本控制和同行评审。在适用的情况下尽量使用高级 API。生产环境中使用的数据转换管道应该与构建模型时使用的相同。记录在数据分析工作流中出现的任何问题，这些问题往往可以促使业务流程的改进。
- en: 'As always, testing is extremely important for the success of your product.
    You have to maintain a set of automated scripts that give easy-to-understand results.
    The test cases should cover the following at the minimum:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，测试对产品的成功至关重要。你必须维护一套自动化脚本，提供易于理解的测试结果。最少的测试用例应该覆盖以下内容：
- en: Adherence to timeframe and resource consumption requirements
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遵守时间框架和资源消耗要求
- en: Resilience to bad data (for example, data type violations)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对不良数据（例如数据类型违规）的弹性
- en: New value in a categorical feature that was not encountered during the model
    building phase
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: New value in a categorical feature that was not encountered during the model
    building phase
- en: Very little data or too heavy data that is expected in the target production
    system
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Very little data or too heavy data that is expected in the target production
    system
- en: 'Monitor logs, resource utilization, and so on to uncover any performance bottlenecks.
    The Spark UI provides a wealth of information to monitor Spark applications. The
    following are some common tips that will help you improve performance:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'Monitor logs, resource utilization, and so on to uncover any performance bottlenecks.
    The Spark UI provides a wealth of information to monitor Spark applications. The
    following are some common tips that will help you improve performance:'
- en: Cache any input or intermediate data that might be used multiple times.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cache any input or intermediate data that might be used multiple times.
- en: Look at the Spark UI and identify jobs that are causing a lot of shuffle. Check
    the code and see whether you can reduce the shuffles.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Look at the Spark UI and identify jobs that are causing a lot of shuffle. Check
    the code and see whether you can reduce the shuffles.
- en: Actions may transfer the data from workers to the driver. See that you are not
    transferring any data that is not absolutely necessary.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Actions may transfer the data from workers to the driver. See that you are not
    transferring any data that is not absolutely necessary.
- en: "Stragglers; that run slower than others; \x94may increase the overall job completion\
    \ time. There may be several reasons for a straggler. If a job is running slow\
    \ due to a slow node, you may set `spark.speculation` to `true`. Then Spark automatically\
    \ relaunches such a task on a different node. Otherwise, you may have to revisit\
    \ the logic and see whether it can be improved."
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: "Stragglers; that run slower than others; \x94may increase the overall job completion\
    \ time. There may be several reasons for a straggler. If a job is running slow\
    \ due to a slow node, you may set `spark.speculation` to `true`. Then Spark automatically\
    \ relaunches such a task on a different node. Otherwise, you may have to revisit\
    \ the logic and see whether it can be improved."
- en: References
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: References
- en: Investigating Spark's performance: [http://radar.oreilly.com/2015/04/investigating-sparks-performance.html](http://radar.oreilly.com/2015/04/investigating-sparks-performance.html)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Investigating Spark's performance: [http://radar.oreilly.com/2015/04/investigating-sparks-performance.html](http://radar.oreilly.com/2015/04/investigating-sparks-performance.html)
- en: Tuning and Debugging in Apache Spark by Patrick Wendell: [https://sparkhub.databricks.com/video/tuning-and-debugging-apache-spark/](https://sparkhub.databricks.com/video/tuning-and-debugging-apache-spark/)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tuning and Debugging in Apache Spark by Patrick Wendell: [https://sparkhub.databricks.com/video/tuning-and-debugging-apache-spark/](https://sparkhub.databricks.com/video/tuning-and-debugging-apache-spark/)
- en: How to tune your Apache Spark jobs: http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/ and
    part 2
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: How to tune your Apache Spark jobs: [http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/) 和
    part 2
- en: Data quality management
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data quality management
- en: At the outset, let's not forget that we are trying to build fault-tolerant software
    data products from unreliable, often unstructured, and uncontrolled data sources.
    So data quality management gains even more importance in a data science workflow.
    Sometimes the data may solely come from controlled data sources, such as automated
    internal process workflows in an organization. But in all other cases, you need
    to carefully craft your data cleansing processes to protect the subsequent processing.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: At the outset, let's not forget that we are trying to build fault-tolerant software
    data products from unreliable, often unstructured, and uncontrolled data sources.
    So data quality management gains even more importance in a data science workflow.
    Sometimes the data may solely come from controlled data sources, such as automated
    internal process workflows in an organization. But in all other cases, you need
    to carefully craft your data cleansing processes to protect the subsequent processing.
- en: Metadata consists of the structure and meaning of data, and obviously the most
    critical repository to work with. It is the information about the structure of
    individual data sources and what each component in that structure means. You may
    not always be able to write some script and extract this data. A single data source
    may contain data with different structures or an individual component (column)
    may mean different things during different times. A label such as owner or high
    may mean different things in different data sources. Collecting and understanding
    all such nuances and documenting is a tedious, iterative task. Standardization
    of metadata is a prerequisite to data transformation development.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Metadata consists of the structure and meaning of data, and obviously the most
    critical repository to work with. It is the information about the structure of
    individual data sources and what each component in that structure means. You may
    not always be able to write some script and extract this data. A single data source
    may contain data with different structures or an individual component (column)
    may mean different things during different times. A label such as owner or high
    may mean different things in different data sources. Collecting and understanding
    all such nuances and documenting is a tedious, iterative task. Standardization
    of metadata is a prerequisite to data transformation development.
- en: 'Some broad guidelines that are applicable to most use cases are listed here:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'Some broad guidelines that are applicable to most use cases are listed here:'
- en: All data sources must be versioned and timestamped
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有数据源必须进行版本控制并加上时间戳
- en: Data quality management processes often require involvement of the highest authorities
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量管理过程通常需要最高层次的主管部门参与
- en: Mask or anonymize sensitive data
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 屏蔽或匿名化敏感数据
- en: One important step that is often missed out is to maintain traceability; a link
    between each data element (say a row) and its original source
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个常常被忽视的重要步骤是保持可追溯性；即每个数据元素（比如一行）与其原始来源之间的链接
- en: The Scala advantage
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scala 的优势
- en: Apache Spark allows you to write applications in Python, R, Java, or Scala.
    With this flexibility comes the responsibility of choosing the right language
    for your requirements. But regardless of your usual language of choice, you may
    want to consider Scala for your Spark-powered application. In this section, we
    will explain why.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 允许你用 Python、R、Java 或 Scala 编写应用程序。随着这种灵活性的出现，你也需要承担选择适合自己需求的编程语言的责任。不过，无论你通常选择哪种语言，你可能都希望考虑在
    Spark 驱动的应用程序中使用 Scala。在本节中，我们将解释为什么这么做。
- en: Let's digress to gain a high-level understanding of imperative and functional
    programming paradigms first. Languages such as C, Python, and Java belong to the
    imperative programming paradigm. In the imperative programming paradigm, a program
    is a sequence of instructions and it has a program state. The program state is
    usually represented as a set of variables and their values at any given point
    in time. Assignments and reassignments are fairly common. Variable values are
    expected to change over the period of execution by one or more functions. Variable
    value modification in a function is not limited to local variables. Global variables
    and public class variables are some examples of such variables.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微跑题，首先高层次地了解一下命令式和函数式编程范式。像 C、Python 和 Java 这样的语言属于命令式编程范式。在命令式编程范式中，程序是一系列指令，并且它有一个程序状态。程序状态通常表现为在任何给定时刻变量及其值的集合。赋值和重新赋值是比较常见的。变量值在执行过程中会随着一个或多个函数的执行而变化。函数中的变量值修改不仅限于局部变量。全局变量和公共类变量就是此类变量的例子。
- en: In contrast, programs written in functional programming languages such as Erlang
    can be viewed as stateless expression evaluators. Data is immutable. If a function
    is called with the same set of input arguments, then it is expected to produce
    the same result (that is, referential transparency). This is possible due to the
    absence of interference from a variable context in the form of global variables
    and the like. This implies that the sequence of function evaluation is of little
    importance. Functions can be passed as arguments to other functions. Recursive
    calls replace loops. The absence of state makes parallel programming much easier
    because it eliminates the need for locking and possible deadlocks. Coordination
    gets simplified when the execution order is less important. These factors make
    the functional programming paradigm a neat fit for parallel programming.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，用函数式编程语言如 Erlang 编写的程序可以看作是无状态的表达式求值器。数据是不可变的。如果函数以相同的输入参数被调用，那么它应该产生相同的结果（即参照透明性）。这是由于没有受到全局变量等变量上下文的干扰。这意味着函数评估的顺序不重要。函数可以作为参数传递给其他函数。递归调用取代了循环。无状态性使得并行编程变得更加容易，因为它消除了锁和潜在死锁的需求。当执行顺序不重要时，协调变得更为简化。这些因素使得函数式编程范式与并行编程非常契合。
- en: Pure functional programming languages are hard to work with because most of
    the programs require state changes. Most functional programming languages, including
    good old Lisp, do allow storing of data in variables (side-effects). Some languages
    such as Scala draw from multiple programming paradigms.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 纯函数式编程语言难以使用，因为大多数程序都需要状态的改变。包括老牌 Lisp 在内的大多数函数式编程语言都允许将数据存储在变量中（副作用）。一些语言，比如
    Scala，融合了多种编程范式。
- en: Returning to Scala, it is a JVM-based, statically typed multi-paradigm programming
    language. Its built-in-type inference mechanism allows programmers to omit some
    redundant type information. This gives a feel of the flexibility offered by dynamic
    languages while retaining the robustness of better compile time checks and fast
    runtime. Scala is an object-oriented language in the sense that every value is
    an object, including numerical values. Functions are first-class objects, which
    can be used as any data type, and they can be passed as arguments to other functions.
    Scala interoperates well with Java and its tools because Scala runs on JVM. Java
    and Scala classes can be freely mixed. That implies that Scala can easily interact
    with the Hadoop ecosystem.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 Scala，它是一种基于 JVM 的静态类型多范式编程语言。其内建的类型推断机制允许程序员省略一些冗余的类型信息。这使得 Scala 在保持良好编译时检查和快速运行时的同时，具备了动态语言的灵活性。Scala
    是面向对象的语言，意味着每个值都是一个对象，包括数值。函数是第一类对象，可以作为任何数据类型使用，并且可以作为参数传递给其他函数。由于 Scala 运行在
    JVM 上，它与 Java 及其工具有良好的互操作性，Java 和 Scala 类可以自由混合使用。这意味着 Scala 可以轻松地与 Hadoop 生态系统进行交互。
- en: All of these factors should be taken into account when you choose the right
    programming language for your application.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择适合您应用的编程语言时，应该考虑所有这些因素。
- en: Spark development status
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 开发状态
- en: Apache Spark has become the most currently active project in the Hadoop ecosystem
    in terms of the number of contributors by the end of 2015\. Having started as
    a research project at UC Berkeley AMPLAB in 2009, Spark is still relatively young
    when compared to projects such as Apache Hadoop and is still in active development.
    There were three releases in the year 2015, from 1.3 through 1.5, packed with
    features such as DataFrames API, SparkR, and Project Tungsten respectively. Version
    1.6 was released in early 2016 and included the new Dataset API and expansion
    of data science functionality. Spark 2.0 was released in July 2016, and this being
    a major release has a lot of new features and enhancements that deserve a section
    of their own.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 到 2015 年底，Apache Spark 已成为 Hadoop 生态系统中最活跃的项目之一，按贡献者数量来看。Spark 最初是 2009 年在 UC
    Berkeley AMPLAB 作为研究项目启动的，与 Apache Hadoop 等项目相比仍然相对年轻，且仍在积极开发中。2015 年有三次发布，从 1.3
    到 1.5，包含了如 DataFrames API、SparkR 和 Project Tungsten 等特性。1.6 版本于 2016 年初发布，包含了新的数据集
    API 和数据科学功能的扩展。Spark 2.0 于 2016 年 7 月发布，作为一个重要版本，包含了许多新特性和增强功能，值得单独拿出一节来介绍。
- en: Spark 2.0's features and enhancements
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 2.0 的特性和增强功能
- en: Apache Spark 2.0 included three major new features and several other performance
    improvements and under-the-hood changes. This section attempts to give a high-level
    overview yet step into the details to give a conceptual understanding wherever
    required.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 2.0 包含了三个主要的新特性以及其他一些性能改进和内部更改。本节尝试提供一个高层次的概述，并在需要时深入细节，帮助理解其概念。
- en: Unifying Datasets and DataFrames
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 统一数据集和数据框架
- en: DataFrames are high-level APIs that support a data abstraction conceptually
    equivalent to a table in a relational database or a DataFrame in R and Python
    (the pandas library). Datasets are an extension of the DataFrame API that provide
    a type-safe, object-oriented programming interface. Datasets add static types
    to DataFrames. Defining a structure on top of DataFrames provides information
    to the core that enables optimizations. It also helps in catching analysis errors
    early on, even before a distributed job starts.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框架（DataFrames）是支持数据抽象的高级 API，其概念上等同于关系型数据库中的表格或 R 和 Python 中的 DataFrame（如
    pandas 库）。数据集（Datasets）是数据框架 API 的扩展，提供类型安全的面向对象编程接口。数据集为数据框架增加了静态类型。在数据框架上定义结构为核心提供了优化信息，也有助于在分布式作业开始之前就能提前发现分析错误。
- en: RDDs, Datasets, and DataFrames are interchangeable. RDDs continue to be the
    low-level API. DataFrames, Datasets, and SQL share the same optimization and execution
    pipeline. Machine learning libraries take either DataFrames or Datasets. Both
    DataFrames and Datasets run on Tungsten, an initiative to improve runtime performance.
    They leverage Tungsten's fast in-memory encoding, which is responsible for converting
    between JVM objects and Spark's internal representation. The same APIs work on
    streams also, introducing the concept of continuous DataFrames.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: RDD、数据集（Datasets）和数据框（DataFrames）是可以互换的。RDD 仍然是低级 API。数据框、数据集和 SQL 共享相同的优化和执行管道。机器学习库使用的是数据框或数据集。数据框和数据集都在
    Tungsten 上运行，Tungsten 是一个旨在提升运行时性能的计划。它们利用了 Tungsten 的快速内存编码技术，负责在 JVM 对象和 Spark
    内部表示之间进行转换。相同的 API 也适用于流数据，引入了连续数据框的概念。
- en: Structured Streaming
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化流式计算
- en: Structure Streaming APIs are high-level APIs that are built on the Spark SQL
    engine and extend DataFrames and Datasets. Structured Streaming unifies streaming,
    interactive, and batch queries. In most use cases, streaming data needs to be
    combined with batch and interactive queries to form continuous applications. These
    APIs are designed to address that requirement. Spark takes care of running the
    query incrementally and continuously on streaming data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流式 API 是基于 Spark SQL 引擎构建的高级 API，扩展了数据框和数据集。结构化流式计算统一了流处理、交互式查询和批处理查询。在大多数使用场景中，流数据需要与批处理和交互式查询结合，形成持续的应用程序。这些
    API 旨在满足这一需求。Spark 负责增量和持续地执行流数据上的查询。
- en: The first release of structured streaming will be focusing on ETL workloads.
    Users will be able to specify the input, query, trigger, and type of output. An
    input stream is logically equivalent to an append-only table. Users define queries
    just the way they would on a traditional SQL table. The trigger is a timeframe,
    say one second. The output modes offered are complete output, deltas, or updates
    in place (for example, a DB table).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流式计算的首次发布将专注于 ETL 工作负载。用户将能够指定输入、查询、触发器和输出类型。输入流在逻辑上等同于一个仅追加的表。用户可以像在传统 SQL
    表上那样定义查询。触发器是一个时间框架，例如一秒。提供的输出模式包括完整输出、增量输出或就地更新（例如，数据库表）。
- en: 'Take this example: you can aggregate the data in a stream, serve it using the
    Spark SQL JDBC server, and pass it to a database such as MySQL for downstream
    applications. Or you could run ad hoc SQL queries that act on the latest data.
    You can also build and apply machine learning models.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以这个例子为例：你可以对流数据进行聚合，通过 Spark SQL JDBC 服务器提供服务，并将其传递给数据库（例如 MySQL）用于下游应用。或者，你可以运行临时
    SQL 查询，操作最新的数据。你还可以构建并应用机器学习模型。
- en: Project Tungsten phase 2
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 项目 Tungsten 第二阶段
- en: 'The central idea behind project Tungsten is to bring Spark''s performance closer
    to bare metal through native memory management and runtime code generation. It
    was first included in Spark 1.4 and enhancements were added in 1.5 and 1.6\. It
    focuses on substantially improving the efficiency of memory and CPU for Spark
    applications, primarily by the following ways:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 项目 Tungsten 的核心思想是通过本地内存管理和运行时代码生成，将 Spark 的性能推向接近硬件的极限。它首次包含在 Spark 1.4 中，并在
    1.5 和 1.6 中进行了增强。其重点是通过以下几种方式显著提升 Spark 应用程序的内存和 CPU 效率：
- en: Managing memory explicitly and eliminating the overhead of JVM object model
    and garbage collection. For example, a four-byte string would occupy around 48
    bytes in the JVM object model. Since Spark is not a general-purpose application
    and has more knowledge about the life cycle of memory blocks than the garbage
    collector, it can manage memory more efficiently than JVM.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确管理内存并消除 JVM 对象模型和垃圾回收的开销。例如，一个四字节的字符串在 JVM 对象模型中大约占用 48 字节。由于 Spark 不是一个通用应用程序，并且比垃圾回收器更了解内存块的生命周期，它能够比
    JVM 更高效地管理内存。
- en: Designing cache-friendly algorithms and data structures.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计适合缓存的数据结构和算法。
- en: Spark performs code generation to compile parts of queries to Java bytecode.
    This is being broadened to cover most built-in expressions.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 执行代码生成，将查询的部分编译为 Java 字节码。这一过程已扩展到覆盖大多数内置表达式。
- en: 'Spark 2.0 rolls out phase 2, which is an order of magnitude faster and includes:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0 推出了第二阶段，它的速度提升了一个数量级，并且包括：
- en: Whole stage code generation by removing expensive iterator calls and fusing
    across multiple operators so that the generated code looks like hand-optimized
    code
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过消除高开销的迭代器调用和跨多个操作符的融合，实现了整体阶段的代码生成，使生成的代码看起来像手工优化的代码
- en: Optimized input and output
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化的输入和输出
- en: What's in store?
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接下来有什么？
- en: 'Apache Spark 2.1 is expected to have the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 预计 Apache Spark 2.1 将具备以下特性：
- en: '**Continuous SQL** (**CSQL**)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续SQL** (**CSQL**)'
- en: BI application integration
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BI 应用程序集成
- en: Support for more streaming sources and sinks
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持更多的流式数据源和汇聚点
- en: Inclusion of additional operators and libraries for structured streaming
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括用于结构化流式处理的额外运算符和库
- en: Enhancements to a machine learning package
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习包的增强
- en: Columnar in-memory support in Tungsten
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tungsten 中的列存储内存支持
- en: The big data trends
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据趋势
- en: Big data processing has been an integral part of the IT industry, more so in
    the past decade. Apache Hadoop and other similar endeavors are focused on building
    the infrastructure to store and process massive amounts of data. After being around
    for over 10 years, the Hadoop platform is considered mature and almost synonymous
    with big data processing. Apache Spark, a general computing engine that works
    well with is and not limited to the Hadoop ecosystem, was quite successful in
    the year 2015.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据处理在过去的十年中成为IT行业的一个重要组成部分。Apache Hadoop 和其他类似的努力致力于构建存储和处理海量数据的基础设施。Hadoop
    平台已经运行超过10年，被认为成熟，几乎可以与大数据处理划上等号。Apache Spark 是一个通用的计算引擎，与Hadoop生态系统兼容，并且在2015年非常成功。
- en: Building data science applications requires knowledge of the big data landscape
    and what software products are available out of that box. We need to carefully
    map the right blocks that fit our requirements. There are several options with
    overlapping functionality, and picking the right tools is easier said than done.
    The success of the application very much depends on assembling the right mix of
    technologies and processes. The good news is that there are several open source
    options that drive down the cost of doing big data analytics; and at the same
    time, you have enterprise-quality end-to-end platforms backed by companies such
    as Databricks. In addition to the use case on hand, keeping track of the industry
    trends in general is equally important.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 构建数据科学应用程序需要了解大数据领域和可用软件产品。我们需要仔细地映射适合我们需求的正确组件。有几个功能重叠的选择，挑选合适的工具比说起来容易得多。应用程序的成功在很大程度上取决于组合适当的技术和流程。好消息是，有几个开源选项可以降低大数据分析的成本；与此同时，你还可以通过像Databricks这样的公司支持的企业级端到端平台。除了手头的用例外，追踪行业趋势也同样重要。
- en: The recent surge in NOSQL data stores with their own interfaces are adding SQL-based
    interfaces even though they are not relational data stores and may not adhere
    to ACID properties. This is a welcome trend because converging to a single, age-old
    interface across relational and non-relational data stores improves programmer
    productivity.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最近NOSQL数据存储的激增，带来了它们自己的接口，即使它们不是关系型数据存储，也可能不遵循ACID属性。这是一个受欢迎的趋势，因为在关系型和非关系型数据存储之间收敛到一个单一的古老接口，提高了程序员的生产力。
- en: The operational (OLTP) and analytical (OLAP) systems were being maintained as
    separate systems over the past couple of decades, but that's one more place where
    convergence is happening. This convergence brings us to near-real-time use cases
    such as fraud prevention. Apache Kylin is one open source distributed analytics
    engine in the Hadoop ecosystem that offers an extremely fast OLAP engine at scale.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年里，运营（OLTP）和分析（OLAP）系统一直被维护为独立的系统，但这正是收敛正在发生的地方之一。这种收敛将我们带到几乎实时用例，如欺诈预防。Apache
    Kylin 是Hadoop生态系统中的一个开源分布式分析引擎，提供了一个极其快速的OLAP引擎。
- en: The advent of the Internet of Things is accelerating real-time and streaming
    analytics, bringing in a whole lot of new use cases. The cloud frees up organizations
    from the operations and IT management overheads so that they can concentrate on
    their core competence, especially in big data processing. Cloud-based analytic
    engines, self-service data preparation tools, self-service BI, just-in-time data
    warehousing, advanced analytics, rich media analytics, and agile analytics are
    some of the commonly used buzzwords. The term big data itself is slowly evaporating
    or becoming implicit.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网的出现加速了实时和流式分析，引入了大量新的用例。云计算解放了组织的运营和IT管理开销，使它们可以集中精力于其核心竞争力，特别是在大数据处理方面。基于云的分析引擎，自助数据准备工具，自助BI，及时数据仓库，高级分析，丰富媒体分析和敏捷分析是一些常用的流行词。大数据这个术语本身正在慢慢消失或变得隐含。
- en: 'There are plenty of software products and libraries in the big data landscape
    with overlapping functionalities, as shown in this infographic (http://mattturck.com/wp-content/uploads/2016/02/matt_turck_big_data_landscape_v11.png).
    Choosing the right blocks for your application is a daunting but very important
    task. Here is a short list of projects to get you started. The list excludes popular
    names such as Cassandra and tries to include blocks with complementing functionality
    and mostly from Apache Software Foundation:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据领域，有大量功能重叠的软件产品和库，如下图所示（http://mattturck.com/wp-content/uploads/2016/02/matt_turck_big_data_landscape_v11.png）。为你的应用选择合适的模块是一个艰巨但非常重要的任务。以下是一个简短的项目列表，帮助你入门。该列表排除了像
    Cassandra 这样的流行名字，尽量包含具有互补功能的模块，并且大多数来自 Apache 软件基金会：
- en: '**Apache Arrow** ([https://arrow.apache.org/](https://arrow.apache.org/)) is
    an in-memory columnar layer used to accelerate analytical processing and interchange.
    It is a high-performance, cross-system, and in-memory data representation that
    is expected to bring in 100 times the performance improvements.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Arrow** ([https://arrow.apache.org/](https://arrow.apache.org/)) 是一个内存中的列式存储层，用于加速分析处理和数据交换。它是一个高性能、跨系统的内存数据表示，预计能带来
    100 倍的性能提升。'
- en: '**Apache Parquet** ([https://parquet.apache.org/](https://parquet.apache.org/))
    is a columnar storage format. Spark SQL provides support for both reading and
    writing parquet files while automatically capturing the structure of the data.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Parquet** ([https://parquet.apache.org/](https://parquet.apache.org/))
    是一种列式存储格式。Spark SQL 提供对读取和写入 parquet 文件的支持，同时自动捕获数据的结构。'
- en: '**Apache Kafka** ([http://kafka.apache.org/](http://kafka.apache.org/)) is
    a popular, high-throughput distributed messaging system. Spark streaming has a
    direct API to support streaming data ingestion from Kafka.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Kafka** ([http://kafka.apache.org/](http://kafka.apache.org/)) 是一个流行的高吞吐量分布式消息系统。Spark
    Streaming 提供直接的 API 来支持从 Kafka 进行流数据摄取。'
- en: '**Alluxio** ([http://alluxio.org/](http://alluxio.org/)), formerly called Tachyon,
    is a memory-centric, virtual distributed storage system that enables data sharing
    across clusters at memory speed. It aims to become the de facto storage unification
    layer for big data. Alluxio sits between computation frameworks such as Spark
    and storage systems such as Amazon S3, HDFS, and others.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Alluxio** ([http://alluxio.org/](http://alluxio.org/))，前身为 Tachyon，是一个以内存为中心的虚拟分布式存储系统，能够在集群之间以内存速度共享数据。它旨在成为大数据的事实上的存储统一层。Alluxio
    位于计算框架（如 Spark）和存储系统（如 Amazon S3、HDFS 等）之间。'
- en: '**GraphFrames** (https://databricks.com/blog/2016/03/03/introducing-graphframes.html)
    is a graph processing library for Apache spark that is built on top of DataFrames
    API.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphFrames** ([https://databricks.com/blog/2016/03/03/introducing-graphframes.html](https://databricks.com/blog/2016/03/03/introducing-graphframes.html))
    是一个基于 Apache Spark 的图处理库，建立在 DataFrames API 之上。'
- en: '**Apache Kylin** ([http://kylin.apache.org/](http://kylin.apache.org/)) is
    a distributed analytics engine designed to provide SQL interface and multidimensional
    analysis (OLAP) on Hadoop, supporting extremely large datasets.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Kylin** ([http://kylin.apache.org/](http://kylin.apache.org/)) 是一个分布式分析引擎，旨在提供
    SQL 接口和多维分析（OLAP），支持 Hadoop 上的超大规模数据集。'
- en: '**Apache Sentry** ([http://sentry.apache.org/](http://sentry.apache.org/))
    is a system for enforcing fine-grained role-based authorization to data and metadata
    stored on a Hadoop cluster. It is in the incubation stage at the time of writing
    this book.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Sentry** ([http://sentry.apache.org/](http://sentry.apache.org/))
    是一个系统，用于对存储在 Hadoop 集群中的数据和元数据执行细粒度的基于角色的授权。它在撰写本书时处于孵化阶段。'
- en: '**Apache Solr** ([http://lucene.apache.org/solr/](http://lucene.apache.org/solr/))
    is a blazing fast search platform. Check this [presentation](https://spark-summit.org/2015/events/integrating-spark-and-solr/)
    for integrating Solr and Spark.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Solr** ([http://lucene.apache.org/solr/](http://lucene.apache.org/solr/))
    是一个非常快速的搜索平台。查看这个 [演示](https://spark-summit.org/2015/events/integrating-spark-and-solr/)
    了解如何将 Solr 与 Spark 集成。'
- en: '**TensorFlow** ([https://www.tensorflow.org/](https://www.tensorflow.org/))
    is a machine learning library with extensive built-in support for deep learning.
    Check out this [blog](https://databricks.com/blog/2016/01/25/deep-learning-with-spark-and-tensorflow.html)
    to learn how it can be used with Spark.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow** ([https://www.tensorflow.org/](https://www.tensorflow.org/))
    是一个机器学习库，广泛支持深度学习。查看这个 [博客](https://databricks.com/blog/2016/01/25/deep-learning-with-spark-and-tensorflow.html)，了解如何与
    Spark 一起使用。'
- en: '**Zeppelin** ([http://zeppelin.incubator.apache.org/](http://zeppelin.incubator.apache.org/))
    is a web-based notebook that enables interactive data analytics. It is covered
    in the data visualization chapter.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Zeppelin** ([http://zeppelin.incubator.apache.org/](http://zeppelin.incubator.apache.org/))
    是一个基于 Web 的笔记本，支持交互式数据分析。它在数据可视化章节中有介绍。'
- en: Summary
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this final chapter, we discussed how to build real-world applications using
    Spark. We discussed the big picture consisting of technical and non-technical
    aspects of data analytics workflows.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们讨论了如何使用 Spark 构建现实世界的应用程序。我们讨论了包含技术性和非技术性方面的数据分析工作流的宏观视角。
- en: References
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: The Spark Summit site has a wealth of information on Apache Spark and related
    projects from completed events
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Summit 网站包含了关于 Apache Spark 和相关项目的大量信息，来自已完成的活动。
- en: Interview with *Matei Zaharia* by KDnuggets
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 *Matei Zaharia* 的访谈，由 KDnuggets 撰写。
- en: '*Why Spark Reached the Tipping Point* in 2015 from KDnuggets by *Matthew Mayo*'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 KDnuggets 的 *为什么 Spark 在 2015 年达到了临界点*，作者是 **Matthew Mayo**。
- en: 'Going Live: Preparing your first Spark production deployment is a very good
    starting point'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上线：准备你的第一个 Spark 生产部署是一个非常好的起点。
- en: '*What is Scala?* from the Scala home page'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*什么是 Scala？* 来自 Scala 官网。'
- en: '*Martin Odersky*, creator of Scala, explains the reasons why Scala fuses together
    imperative and functional programming'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**马丁·奥德斯基**（*Martin Odersky*），Scala 的创始人，解释了为什么 Scala 将命令式编程和函数式编程融合在一起。'
