- en: Apache Log Processing with Storm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Storm进行Apache日志处理
- en: In the previous chapter, we covered how we can integrate Storm with Redis, HBase,
    Esper and Elasticsearch.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了如何将Storm与Redis、HBase、Esper和Elasticsearch集成。
- en: In this chapter, we are covering the most popular use case of Storm, which is
    log processing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍Storm最流行的用例，即日志处理。
- en: 'This chapter covers the following major sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主要部分：
- en: Apache log processing elements
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache日志处理元素
- en: Installation of Logstash
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Logstash
- en: Configuring Logstash to produce the Apache log into Kafka
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置Logstash以将Apache日志生成到Kafka
- en: Splitting the Apache log file
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拆分Apache日志文件
- en: Calculating the country name, operating system type, and browser type
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算国家名称、操作系统类型和浏览器类型
- en: Identifying the search key words of your website
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别网站的搜索关键词
- en: Persisting the process data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久化处理数据
- en: Kafka spout and defining the topology
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka spout和定义拓扑
- en: Deploying the topology
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署拓扑
- en: Storing the data into Elasticsearch and reporting
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据存储到Elasticsearch并生成报告
- en: Apache log processing elements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache日志处理元素
- en: Log processing is becoming a necessity for every organization, as they need
    to collect the business information from log data. In this chapter, we are basically
    working on how we can process the Apache log data using Logstash, Kafka, Storm,
    and Elasticsearch to collect the business information.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 日志处理正在成为每个组织的必需品，因为他们需要从日志数据中收集业务信息。在本章中，我们基本上是在讨论如何使用Logstash、Kafka、Storm和Elasticsearch来处理Apache日志数据，以收集业务信息。
- en: 'The following diagram illustrates all the elements that we are developing in
    this chapter:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示了我们在本章中开发的所有元素：
- en: '![](img/00063.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00063.jpeg)'
- en: 'Figure 11.1: Log processing topology'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：日志处理拓扑
- en: Producing Apache log in Kafka using Logstash
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Logstash在Kafka中生成Apache日志
- en: As explained in [Chapter 8](part0137.html#42KT20-6149dd15e07b443593cc93f2eb31ee7b),
    *Integration of Storm and Kafka*, Kafka is a distributed messaging queue and can
    integrate very well with Storm. In this section, we will show you how we can use
    Logstash to read the Apache log file and publish it into the Kafka Cluster. We
    are assuming you already have the Kafka Cluster running. The installation steps
    of the Kafka Cluster are outlined in [Chapter 8](part0137.html#42KT20-6149dd15e07b443593cc93f2eb31ee7b),
    *Integration of Storm and Kafka*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第8章](part0137.html#42KT20-6149dd15e07b443593cc93f2eb31ee7b)中所解释的，*Storm和Kafka的集成*，Kafka是一个分布式消息队列，可以与Storm很好地集成。在本节中，我们将向您展示如何使用Logstash来读取Apache日志文件并将其发布到Kafka集群中。我们假设您已经运行了Kafka集群。Kafka集群的安装步骤在[第8章](part0137.html#42KT20-6149dd15e07b443593cc93f2eb31ee7b)中概述。
- en: Installation of Logstash
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Logstash
- en: 'Before moving on to the installation of Logstash, we are going to answer the
    questions: What is Logstash? Why are we using Logstash?'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续安装Logstash之前，我们将回答以下问题：什么是Logstash？为什么我们要使用Logstash？
- en: What is Logstash?
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Logstash？
- en: 'Logstash is a tool that is used to collect, filter/parse, and emit the data
    for future use. Collect, parse, and emit are divided into three sections, which
    are called input, filter, and output:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash是一个用于收集、过滤/解析和发送数据以供将来使用的工具。收集、解析和发送分为三个部分，称为输入、过滤器和输出：
- en: The **input** section is used to read the data from external sources. The common
    input sources are File, TCP port, Kafka, and so on.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input**部分用于从外部来源读取数据。常见的输入来源是文件、TCP端口、Kafka等。'
- en: The **filter** section is used to parse the data.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**filter**部分用于解析数据。'
- en: The **output** section is used to emit the data to some external source. The
    common external sources are Kafka, Elasticsearch, TCP, and so on.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**output**部分用于将数据发送到某些外部来源。常见的外部来源是Kafka、Elasticsearch、TCP等。'
- en: Why are we using Logstash?
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们要使用Logstash？
- en: We need to read the log data in real time and store it into Kafka before Storm
    starts the actual processing. We are using Logstash as it is very mature in reading
    the log files and pushing the logs data into Kafka.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在Storm开始实际处理之前，我们需要实时读取日志数据并将其存储到Kafka中。我们使用Logstash是因为它非常成熟地读取日志文件并将日志数据推送到Kafka中。
- en: Installation of Logstash
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Logstash
- en: 'We should have JDK 1.8 installed on the Linux box before installing Logstash,
    as we are going to use Logstash 5.4.1 and JDK 1.8 is the minimum requirement for
    this. The following are the steps to install Logstash:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装Logstash之前，我们应该在Linux服务器上安装JDK 1.8，因为我们将使用Logstash 5.4.1，而JDK 1.8是此版本的最低要求。以下是安装Logstash的步骤：
- en: Download Logstash 5.4.1 from [https://artifacts.elastic.co/downloads/logstash/logstash-5.4.1.zip](https://artifacts.elastic.co/downloads/logstash/logstash-5.4.1.zip).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://artifacts.elastic.co/downloads/logstash/logstash-5.4.1.zip](https://artifacts.elastic.co/downloads/logstash/logstash-5.4.1.zip)下载Logstash
    5.4.1。
- en: Copy the setup on all the machines whose Apache logs you want to publish into
    Kafka.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将设置复制到所有你想要发布到Kafka的Apache日志的机器上。
- en: 'Extract the setup by running the following command:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令提取设置：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Configuration of Logstash
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Logstash的配置
- en: Now, we are going to define the Logstash configuration to consume the Apache
    logs and store them into Kafka.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义Logstash配置来消耗Apache日志并将其存储到Kafka中。
- en: 'Create a `logstash.conf` file and add the following lines:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个`logstash.conf`文件并添加以下行：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We should change the following parameters in the preceding configuration:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该更改前述配置中的以下参数：
- en: '`TOPIC_NAME`: Replace with the Kafka topic you want to use for storing the
    Apache log'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TOPIC_NAME`：替换为您要用于存储Apache日志的Kafka主题'
- en: '`KAFKA_IP` and `KAFKA_PORT`: Specify the comma separated list of all the Kafka
    nodes'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KAFKA_IP`和`KAFKA_PORT`：指定所有Kafka节点的逗号分隔列表'
- en: '`PATH_TO_APACHE_LOG`: The location of Apache log file on the Logstash machine'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PATH_TO_APACHE_LOG`：Logstash机器上Apache日志文件的位置'
- en: 'Go to Logstash home directory and execute the following command to start the
    log reading and publishing into Kafka:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 转到Logstash主目录并执行以下命令以开始读取日志并发布到Kafka：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, the real-time log data is coming into the Kafka topic. In the next section,
    we are writing the Storm topology to consume the log data, process, and store
    the process data into the database.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，实时日志数据正在进入Kafka主题。在下一节中，我们将编写Storm拓扑来消费日志数据，处理并将处理数据存储到数据库中。
- en: Why are we using Kafka between Logstash and Storm?
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么在Logstash和Storm之间使用Kafka？
- en: As we all know, Storm provides guaranteed message processing, meaning that every
    message enters into the Storm topology and will be processed at least once. In
    Storm, data loss is possible only at the spout end, if the processing capacity
    of Storm spout is less than the producing capacity of Logstash. Hence, to avoid
    the data getting lost at the Storm spout end, we will generally publish the data
    into a messaging queue (Kafka) and Storm spout will use the messaging queue as
    the data source.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，Storm提供了可靠的消息处理，这意味着每条消息进入Storm拓扑都将至少被处理一次。在Storm中，数据丢失只可能发生在spout端，如果Storm
    spout的处理能力小于Logstash的生产能力。因此，为了避免数据在Storm spout端丢失，我们通常会将数据发布到消息队列（Kafka），Storm
    spout将使用消息队列作为数据源。
- en: Splitting the Apache log line
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割Apache日志行
- en: Now, we are creating a new topology, which will read the data from Kafka using
    the `KafkaSpout` spout. In this section, we are writing an `ApacheLogSplitter` bolt,
    that has a logic to fetch the IP, status code, referrer, bytes sent, and so on,
    information from the Apache log line. As this is a new topology, we must first
    create the new project.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们正在创建一个新的拓扑，它将使用`KafkaSpout` spout从Kafka中读取数据。在本节中，我们将编写一个`ApacheLogSplitter`
    bolt，它具有从Apache日志行中提取IP、状态码、引用来源、发送的字节数等信息的逻辑。由于这是一个新的拓扑，我们必须首先创建新项目。
- en: Create a new Maven project with `groupId` as `com.stormadvance` and `artifactId`
    as `logprocessing`.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Maven项目，`groupId`为`com.stormadvance`，`artifactId`为`logprocessing`。
- en: 'Add the following dependencies in the `pom.xml` file:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pom.xml`文件中添加以下依赖项：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We are creating an `ApacheLogSplitter` class in the `com.stormadvance.logprocessing`
    package. This class contains logic to fetch the different elements such as IP,
    referrer, user-agent, and so on, from the Apache log line.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在`com.stormadvance.logprocessing`包中创建`ApacheLogSplitter`类。这个类包含了从Apache日志行中提取不同元素（如IP、引用来源、用户代理等）的逻辑。
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The input for the `logSplitter(String apacheLog)` method is:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`logSplitter(String apacheLog)`方法的输入是：'
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of the `logSplitter(String apacheLog)` method is:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`logSplitter(String apacheLog)`方法的输出是：'
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we will create the `ApacheLogSplitterBolt` class in the `com.stormadvance.logprocessing`
    package. The `ApacheLogSplitterBolt` extends the `org.apache.storm.topology.base.BaseBasicBolt`
    class and passes the set of fields generated by `ApacheLogSplitter` class to the
    next bolt in the topology. The following is the source code of the `ApacheLogSplitterBolt`
    class:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将在`com.stormadvance.logprocessing`包中创建`ApacheLogSplitterBolt`类。`ApacheLogSplitterBolt`扩展了`org.apache.storm.topology.base.BaseBasicBolt`类，并将`ApacheLogSplitter`类生成的字段集传递给拓扑中的下一个bolt。以下是`ApacheLogSplitterBolt`类的源代码：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output of the `ApacheLogSplitterBolt` class contains seven fields. These
    fields are `ip`, `dateTime`, `request`, `response`, `bytesSent`, `referrer`, and
    `useragent`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`ApacheLogSplitterBolt`类的输出包含七个字段。这些字段是`ip`，`dateTime`，`request`，`response`，`bytesSent`，`referrer`和`useragent`。'
- en: Identifying country, operating system type, and browser type from the log file
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从日志文件中识别国家、操作系统类型和浏览器类型
- en: 'This section explains how we can calculate the user country name, operation
    system type, and browser type by analyzing the Apache log line. By identifying
    the country name, we can easily identify the location where our site is getting
    more attention and the location where we are getting less attention. Let''s perform
    the following steps to calculate the country name, operating system, and browser
    from the Apache log file:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了如何通过分析Apache日志行来计算用户国家名称、操作系统类型和浏览器类型。通过识别国家名称，我们可以轻松地确定我们网站受到更多关注的地点以及我们受到较少关注的地点。让我们执行以下步骤来计算Apache日志文件中的国家名称、操作系统和浏览器：
- en: 'We are using the open source `geoip` library to calculate the country name
    from the IP address. Add the following dependencies in the `pom.xml` file:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用开源的`geoip`库来从IP地址计算国家名称。在`pom.xml`文件中添加以下依赖项：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Add the following repository into the `pom`.`xml` file:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pom.xml`文件中添加以下存储库：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We are creating an `IpToCountryConverter` class in the `com.stormadvance.logprocessing`
    package. This class contains the parameterized constructor that is taking the
    location of the `GeoLiteCity.dat` file as input. You can find the `GeoLiteCity.dat`
    file in the resources folder of the `logprocessing` project. The location of the
    `GeoLiteCity.dat` file must be the same in all Storm nodes. The `GeoLiteCity.dat`
    file is a database which we are using to calculate the country name from the IP
    address. The following is the source code of the `IpToCountryConverter` class:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在`com.stormadvance.logprocessing`包中创建`IpToCountryConverter`类。这个类包含了带有`GeoLiteCity.dat`文件位置作为输入的参数化构造函数。你可以在`logprocessing`项目的资源文件夹中找到`GeoLiteCity.dat`文件。`GeoLiteCity.dat`文件的位置在所有Storm节点中必须相同。`GeoLiteCity.dat`文件是我们用来从IP地址计算国家名称的数据库。以下是`IpToCountryConverter`类的源代码：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now download the `UserAgentTools` class from [https://code.google.com/p/ndt/source/browse/branches/applet_91/Applet/src/main/java/edu/internet2/ndt/UserAgentTools.java?r=856](https://code.google.com/p/ndt/source/browse/branches/applet_91/Applet/src/main/java/edu/internet2/ndt/UserAgentTools.java?r=856).
    This class contains logic to calculate the operating system and browser type from
    the user agent. You can also find the `UserAgentTools` class in the `logprocessing`
    project.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在从[https://code.google.com/p/ndt/source/browse/branches/applet_91/Applet/src/main/java/edu/internet2/ndt/UserAgentTools.java?r=856](https://code.google.com/p/ndt/source/browse/branches/applet_91/Applet/src/main/java/edu/internet2/ndt/UserAgentTools.java?r=856)下载`UserAgentTools`类。这个类包含了从用户代理中计算操作系统和浏览器类型的逻辑。你也可以在`logprocessing`项目中找到`UserAgentTools`类。
- en: Let's write the `UserInformationGetterBolt` class in the `com.stormadvance.logprocessing`
    package. This bolt uses the `UserAgentTools` and `IpToCountryConverter` class
    to calculate the country name, operating system, and browser.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在`com.stormadvance.logprocessing`包中编写`UserInformationGetterBolt`类。这个bolt使用`UserAgentTools`和`IpToCountryConverter`类来计算国家名称、操作系统和浏览器。
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The output of the `UserInformationGetterBolt` class contains 10 fields. These
    fields are `ip`, `dateTime`, `request`, `response`, `bytesSent`, `referrer`, `useragent`,
    `country`, `browser`, and `os`.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`UserInformationGetterBolt`类的输出包含10个字段。这些字段是`ip`、`dateTime`、`request`、`response`、`bytesSent`、`referrer`、`useragent`、`country`、`browser`和`os`。'
- en: Calculate the search keyword
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算搜索关键词
- en: 'This section explains how we can calculate the search keyword from the referrer
    URL. Suppose a referrer URL is [https://www.google.co.in/#q=learning+storm](https://www.google.co.in/#q=learning+storm).
    We will pass this referrer URL to a class and the output of the class will be
    *learning storm*. By identifying the search keyword, we can easily identify the
    keywords users are searching to reach our site. Let''s perform the following steps
    to calculate the keywords from the referrer URL:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了如何从引荐URL计算搜索关键词。假设引荐URL是[https://www.google.co.in/#q=learning+storm](https://www.google.co.in/#q=learning+storm)。我们将把这个引荐URL传递给一个类，这个类的输出将是*learning
    storm*。通过识别搜索关键词，我们可以轻松地确定用户搜索关键词以到达我们的网站。让我们执行以下步骤来计算引荐URL中的关键词：
- en: 'We are creating a `KeywordGenerator` class in the `com.stormadvance.logprocessing`
    package. This class contains logic to generate the search keyword from the referrer
    URL. The following is the source code of the `KeywordGenerator` class:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在`com.stormadvance.logprocessing`包中创建一个`KeywordGenerator`类。这个类包含从引荐URL生成搜索关键词的逻辑。以下是`KeywordGenerator`类的源代码：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If the input for the `KeywordGenerator` class is: [https://in.search.yahoo.com/search;_ylt=AqH0NZe1hgPCzVap0PdKk7GuitIF?p=india+live+score&toggle=1&cop=mss&ei=UTF-8&fr=yfp-t-704](https://in.search.yahoo.com/search;_ylt=AqH0NZe1hgPCzVap0PdKk7GuitIF?p=india+live+score&toggle=1&cop=mss&ei=UTF-8&fr=yfp-t-704)
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`KeywordGenerator`类的输入是：[https://in.search.yahoo.com/search;_ylt=AqH0NZe1hgPCzVap0PdKk7GuitIF?p=india+live+score&toggle=1&cop=mss&ei=UTF-8&fr=yfp-t-704](https://in.search.yahoo.com/search;_ylt=AqH0NZe1hgPCzVap0PdKk7GuitIF?p=india+live+score&toggle=1&cop=mss&ei=UTF-8&fr=yfp-t-704)
- en: 'Then, the output of the `KeywordGenerator` class is:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，`KeywordGenerator`类的输出是：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We are creating a `KeyWordIdentifierBolt` class in the `com.stormadvance.logprocessing`
    package. This class calls the `KeywordGenerator` to generate the keyword from
    the referrer URL. The following is the source code of the `KeyWordIdentifierBolt`
    class:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在`com.stormadvance.logprocessing`包中创建一个`KeyWordIdentifierBolt`类。这个类调用`KeywordGenerator`来从引荐URL生成关键词。以下是`KeyWordIdentifierBolt`类的源代码：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The output of the `KeyWordIdentifierBolt` class contains 11 fields. These fields
    are `ip`, `dateTime`, `request`, `response`, `bytesSent`, `referrer`, `useragent`,
    `country`, `browser`, `os`, and `keyword`.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`KeyWordIdentifierBolt`类的输出包含11个字段。这些字段是`ip`、`dateTime`、`request`、`response`、`bytesSent`、`referrer`、`useragent`、`country`、`browser`、`os`和`keyword`。'
- en: Persisting the process data
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化处理数据
- en: 'This section will explain how we can persist the process data into a data store.
    We are using MySQL as a data store for the log processing use case. I am assuming
    you have MySQL installed on your centOS machine or you can follow the blog at
    [http://www.rackspace.com/knowledge_center/article/installing-mysql-server-on-centos](http://www.rackspace.com/knowledge_center/article/installing-mysql-server-on-centos)
    to install the MySQL on the centOS machine. Let''s perform the following steps
    to persist the record into MySQL:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将解释如何将处理数据持久化到数据存储中。我们在日志处理用例中使用MySQL作为数据存储。我假设您已经在您的centOS机器上安装了MySQL，或者您可以按照[http://www.rackspace.com/knowledge_center/article/installing-mysql-server-on-centos](http://www.rackspace.com/knowledge_center/article/installing-mysql-server-on-centos)上的博客来安装MySQL。让我们执行以下步骤将记录持久化到MySQL中：
- en: 'Add the following dependency to `pom.xml`:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下依赖项添加到`pom.xml`：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We are creating a `MySQLConnection` class in the `com.stormadvance.logprocessing`
    package. This class contains `getMySQLConnection(String ip, String database, String
    user, String password)` method, which returns the MySQL connection. The following
    is the source code of the `MySQLConnection` class:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在`com.stormadvance.logprocessing`包中创建一个`MySQLConnection`类。这个类包含`getMySQLConnection(String
    ip, String database, String user, String password)`方法，该方法返回MySQL连接。以下是`MySQLConnection`类的源代码：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we are creating a `MySQLDump` class in the `com.stormadvance.logprocessing`
    package. This class has a parameterized constructor that is taking MySQL `server
    ip, database name, user, and password` as arguments. This class calls the `getMySQLConnection(ip,database,user,password)`
    method of the MySQLConnection class to get the MySQL connection. The `MySQLDump`
    class contains the `persistRecord(Tuple tuple`) record method, and this method
    persists the input tuple into MySQL. The following is the source code of the `MySQLDump`
    class:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们在`com.stormadvance.logprocessing`包中创建一个`MySQLDump`类。这个类有一个带参数的构造函数，它以MySQL的`服务器ip、数据库名称、用户和密码`作为参数。这个类调用`MySQLConnection`类的`getMySQLConnection(ip,database,user,password)`方法来获取MySQL连接。`MySQLDump`类包含`persistRecord(Tuple
    tuple)`记录方法，这个方法将输入元组持久化到MySQL中。以下是`MySQLDump`类的源代码：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s create a `PersistenceBolt` class in the `com.stormadvance.logprocessing`
    package. This class implements `org.apache.storm.topology.IBasicBolt`. This class
    calls the `persistRecord(Tuple tuple)` method of the `MySQLDump` class to persist
    the records/events into MySQL. The following is the source code of the `PersistenceBolt`
    class:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在`com.stormadvance.logprocessing`包中创建一个`PersistenceBolt`类。这个类实现了`org.apache.storm.topology.IBasicBolt`。这个类调用`MySQLDump`类的`persistRecord(Tuple
    tuple)`方法来将记录/事件持久化到MySQL。以下是`PersistenceBolt`类的源代码：
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this section, we have covered how we can insert the input tuple into a data
    store.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经介绍了如何将输入元组插入数据存储中。
- en: Kafka spout and define topology
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka spout和定义拓扑
- en: 'This section will explain how we can read the Apache log from a Kafka topic.
    This section also defines the `LogProcessingTopology` that will chain together
    all the bolts created in the preceding sections. Let''s perform the following
    steps to consume the data from Kafka and define the topology:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将解释如何从Kafka主题中读取Apache日志。本节还定义了将在前面各节中创建的所有bolt链接在一起的`LogProcessingTopology`。让我们执行以下步骤来消费来自Kafka的数据并定义拓扑：
- en: 'Add the following dependency and repository for Kafka in the `pom.xml` file:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pom.xml`文件中添加以下Kafka的依赖和仓库：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Add the following `build` plugins in the `pom.xml` file. It will let us execute
    the `LogProcessingTopology` using Maven:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pom.xml`文件中添加以下`build`插件。这将让我们使用Maven执行`LogProcessingTopology`：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s create a `LogProcessingTopology` class in the `com.stormadvance.logprocessing`
    package. This class uses the `org.apache.storm.topology.TopologyBuilder` class
    to define the topology. The following is the source code of the `LogProcessingTopology`
    class with an explanation:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`com.stormadvance.logprocessing`包中创建一个`LogProcessingTopology`类。该类使用`org.apache.storm.topology.TopologyBuilder`类来定义拓扑。以下是`LogProcessingTopology`类的源代码及解释：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This section covered how we can chain the different types of bolts into a topology.
    Also, we have covered how we can consume the data from Kafka. In the next section,
    we will explain how we can deploy the topology.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了如何将不同类型的bolt链接成拓扑。我们还介绍了如何从Kafka消费数据。在下一节中，我们将解释如何部署拓扑。
- en: Deploy topology
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署拓扑
- en: 'This section will explain how we can deploy the `LogProcessingTopology`. Perform
    the following steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将解释如何部署`LogProcessingTopology`。执行以下步骤：
- en: 'Execute the following command on the MySQL console to define the database schema:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在MySQL控制台上执行以下命令定义数据库架构：
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: I am assuming you have already produced some data on the `apache_log` topic
    by using Logstash.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我假设您已经通过Logstash在`apache_log`主题上产生了一些数据。
- en: 'Go to the project home directory and run the following command to build the
    project:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入项目主目录并运行以下命令构建项目：
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Execute the following command to start the log processing topology in local
    mode:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令以在本地模式下启动日志处理拓扑：
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, go to MySQL console and check the rows in the `apachelog` table:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，进入MySQL控制台，检查`apachelog`表中的行：
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In this section, we have covered how we can deploy the log processing topology.
    The next section will explain how we can generate the statistics from data stored
    in MySQL.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了如何部署日志处理拓扑。下一节将解释如何从MySQL中存储的数据生成统计信息。
- en: MySQL queries
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MySQL查询
- en: 'This section will explain how we can analyze or query in store data to generate
    some statistics. We will cover the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将解释如何分析或查询存储数据以生成一些统计信息。我们将涵盖以下内容：
- en: Calculating the page hit from each country
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每个国家的页面点击量
- en: Calculating the count of each browser
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每个浏览器的数量
- en: Calculating the count of each operating system
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每个操作系统的数量
- en: Calculate the page hit from each country
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算每个国家的页面点击量
- en: 'Run the following command on the MySQL console to calculate the page hit from
    each country:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在MySQL控制台上运行以下命令，计算每个国家的页面点击量：
- en: '[PRE26]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Calculate the count for each browser
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算每个浏览器的数量
- en: 'Run the following command on the MySQL console to calculate the count for each
    browser:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在MySQL控制台上运行以下命令，计算每个浏览器的数量：
- en: '[PRE27]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Calculate the count for each operating system
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算每个操作系统的数量
- en: 'Run the following command on the MySQL console to calculate the count for each
    operating system:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在MySQL控制台上运行以下命令，计算每个操作系统的数量：
- en: '[PRE28]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Summary
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced you to how we can process the Apache log file,
    how we can identify the country name from the IP, how we can identify the user
    operating system and browser by analyzing the log file, and how we can identify
    the search keyword by analyzing the referrer field.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您介绍了如何处理Apache日志文件，如何通过分析日志文件识别IP的国家名称，如何通过分析日志文件识别用户操作系统和浏览器，以及如何通过分析引荐字段识别搜索关键字。
- en: In the next chapter, we will learn how we can solve machine learning problems
    through Storm.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何通过Storm解决机器学习问题。
