- en: Chapter 5. Collecting and Cleaning Data from the Web
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章：从网页收集和清理数据
- en: One of the most common and useful kitchen tools is a strainer, also called a
    sieve, a colander, or chinois, the purpose of which is to separate solids from
    liquids during cooking. In this chapter, we will be building strainers for the
    data we find on the Web. We will learn how to create several types of programs
    that can help us find and keep the data we want, while discarding the parts we
    do not want.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见和有用的厨房工具之一是滤网，也叫筛子、漏网或中式过滤器，其目的是在烹饪过程中将固体与液体分开。在本章中，我们将为我们在网页上找到的数据构建滤网。我们将学习如何创建几种类型的程序，帮助我们找到并保留我们想要的数据，同时丢弃不需要的部分。
- en: 'In this chapter, we will:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将：
- en: Understand two options to envision the structure of an HTML page, either (a)
    as a collection of lines that we can look for patterns in, or (b) as a tree structure
    containing nodes for which we can identify and collect values.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解两种想象HTML页面结构的方式，第一种是（a）作为我们可以查找模式的行集合，第二种是（b）作为包含节点的树形结构，我们可以识别并收集这些节点的值。
- en: Try out three methods to parse web pages, one that uses the line-by-line approach
    (regular expressions-based HTML parsing), and two that use the tree structure
    approach (Python's BeautifulSoup library and the Chrome browser tool called Scraper).
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试三种解析网页的方法，一种是使用逐行方法（基于正则表达式的HTML解析），另外两种使用树形结构方法（Python的BeautifulSoup库和名为Scraper的Chrome浏览器工具）。
- en: Implement all three of these techniques on some real-world data; we will practice
    scraping out the date and time from messages posted to a web forum.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一些现实世界的数据上实现这三种技术；我们将练习从网页论坛中抓取日期和时间。
- en: Understanding the HTML page structure
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解HTML页面结构
- en: 'A web page is just a text file that contains some special markup **elements**
    (sometimes called HTML **tags**) intended to indicate to a web browser how the
    page should look when displayed to the user, for example, if we want a particular
    word to be displayed in a way that indicates emphasis, we can surround it with
    `<em>` tags like this:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 网页只是一个包含一些特殊标记**元素**（有时称为HTML**标签**）的文本文件，目的是告诉网页浏览器页面应该如何在用户端显示。例如，如果我们想让某个特定的词语以强调的方式显示，我们可以将其用`<em>`标签包围，如下所示：
- en: It is `<em>very important</em>` that you follow these instructions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`<em>非常重要</em>`的是你必须遵循这些指令。'
- en: All web pages have these same features; they are made up of text and the text
    may include tags. There are two main mental models we can employ to extract data
    from web pages. Both models have their useful aspects. In this section, we will
    describe the two structural models, and then in the next section, we will use
    three different tools for extracting data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 所有网页都有这些相同的特征；它们由文本组成，文本中可能包含标签。我们可以使用两种主要的思维模型从网页中提取数据。两种模型各有其有用的方面。在这一节中，我们将描述这两种结构模型，然后在下一节中，我们将使用三种不同的工具来提取数据。
- en: The line-by-line delimiter model
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逐行分隔符模型
- en: In the simplest way of thinking about web pages, we concentrate on the fact
    that there are many dozens of HTML elements/tags that are used to organize and
    display pages on the Web. If we want to extract interesting data from web pages
    in this simple model, we can use the page text and the embedded HTML elements
    themselves as **delimiters**. For instance, in case of the preceding example,
    we may decide we want to collect everything inside the `<em>` tags, or maybe we
    want to collect everything before an `<em>` tag or after an `</em>` tag.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在思考网页的最简单方式时，我们集中于这样一个事实：有很多HTML元素/标签被用来组织和显示网页内容。如果我们想从网页中提取有趣的数据，在这个简单的模型中，我们可以使用页面文本和嵌入的HTML元素本身作为**分隔符**。例如，在前面的例子中，我们可能决定收集所有`<em>`标签中的内容，或者我们可能想收集在`<em>`标签之前或`</em>`标签之后的所有内容。
- en: In this model, we conceive the web page as a collection of largely unstructured
    text, and the HTML tags (or other features of the text, such as recurring words)
    help to provide structure which we can use to delimit the parts that we want.
    Once we have delimiters, we have the ability to strain out the interesting data
    from the junk.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，我们将网页视为一个大致无结构的文本集合，而HTML标签（或文本中的其他特征，如重复的词语）帮助提供结构，我们可以利用这些结构来界定我们想要的部分。一旦我们有了分隔符，就能从杂乱无章的内容中筛选出有趣的数据。
- en: 'For example, here is an excerpt from a real-world HTML page, a chat log from
    the Django IRC channel. Let''s consider how we can use its HTML elements as delimiters
    to extract the interesting data:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面是来自现实世界HTML页面的一段摘录，来自Django IRC频道的聊天日志。让我们考虑如何使用其HTML元素作为分隔符，提取有用的数据：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Given this example text, we could use the `<h2></h2>` tags as delimiters to
    extract the date of this particular chat log. We could use the `<li></li>` tags
    as delimiters for a line of text, and within that line, we can see that `rel=""`
    can be used to extract the username of the chatter. Finally, it appears that all
    the text extending from the end of `</span>` to the beginning of `</li>` is the
    actual line message sent to the chat channel by the user.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以这个示例文本为例，我们可以使用`<h2></h2>`标签作为分隔符，提取出这个聊天记录的日期。我们可以使用`<li></li>`标签作为分隔符提取文本行，在该行内，我们可以看到`rel=""`可用于提取聊天者的用户名。最后，似乎从`</span>`结束到`</li>`开始的所有文本就是用户发送到聊天频道的实际消息行。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: These chat logs are all available online at the Django IRC log website, [http://django-irc-logs.com](http://django-irc-logs.com).
    This website also provides a keyword search interface to the logs. The ellipses
    (`…`) in the preceding code represent text that has been removed for brevity in
    this example.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些聊天日志可以在线访问，网址是Django IRC日志网站，[http://django-irc-logs.com](http://django-irc-logs.com)。该网站还提供了一个关键词搜索接口，供用户查询日志。前述代码中的省略号（`…`）表示示例中已删除的部分文本。
- en: From this messy text, we are able to use the delimiter concept to extract three
    clean pieces of data (the date of log, user, and line message).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从这段杂乱的文本中，我们可以使用分隔符的概念提取出三部分干净的数据（日志日期、用户和消息行）。
- en: The tree structure model
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树形结构模型
- en: 'Another way to imagine the textual web page is as a tree structure made up
    of HTML elements/tags, each of which is related to the other tags on the page.
    Each tag is shown as a **node**, and a tree is made up of all the different nodes
    in a particular page. A tag that shows up within another tag in the HTML is considered
    a **child**, and the enclosing tag is the **parent**. In the previous example
    of IRC chat, the HTML excerpt can be shown in a tree diagram that looks like this:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种想象网页文本的方式是将其视为由HTML元素/标签组成的树形结构，每个元素与页面上的其他标签相互关联。每个标签都显示为一个**节点**，树形结构由页面中所有不同的节点组成。出现在另一个标签内的标签被视为**子节点**，而包围它的标签则是**父节点**。在之前的IRC聊天示例中，HTML代码可以通过树形图表示，如下所示：
- en: '![The tree structure model](img/image00273.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![树形结构模型](img/image00273.jpeg)'
- en: 'If we are able to envision our HTML text as a tree structure, we can use a
    programming language to build a tree for us. This allows us to pull out our desired
    text values based on their element name or position in the element list. For example:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够将HTML文本想象成树形结构，我们可以使用编程语言为我们构建树形结构。这使我们能够根据元素名称或元素在列表中的位置提取所需的文本值。例如：
- en: We may want the value of a tag by name (give me the text from the `<h2>` node)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能需要按名称获取标签的值（给我`<h2>`节点中的文本）
- en: We may want all the nodes of a particular type (give me all the `<li>` nodes
    which are inside `<ul>` which are inside `<div>`)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能需要某种特定类型的所有节点（给我所有在`<div>`中的`<ul>`里的`<li>`节点）
- en: We may want all the attributes of a given element (give me a list of `rel` attributes
    from the `<li>` elements)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能需要某个元素的所有属性（给我所有`<li>`元素中的`rel`属性列表）
- en: In the rest of the chapter, we will put both of these mental models—the line-by-line
    and the tree structure—into practice with some examples. We will walk through
    three different methods to extract and clean the data out of HTML pages.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将结合这两种思维模型——逐行处理和树形结构——并通过一些示例进行实践。我们将演示三种不同的方法，来提取和清理HTML页面中的数据。
- en: Method one – Python and regular expressions
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法一 – Python和正则表达式
- en: In this section, we will use a simple method to extract the data we want from
    an HTML page. This method is based on the concept of identifying delimiters in
    the page and using pattern matching via regular expressions to pull out the data
    we want.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一种简单的方法，从HTML页面中提取我们想要的数据。这种方法基于识别页面中的分隔符，并通过正则表达式进行模式匹配来提取数据。
- en: You may remember that we experimented a little bit with regular expressions
    (regex) in [Chapter 3](part0024.xhtml#aid-MSDG2 "Chapter 3. Workhorses of Clean
    Data – Spreadsheets and Text Editors"), *Workhorses of Clean Data – Spreadsheets
    and Text Editors*, when we were learning about text editors. In this chapter,
    some of the concepts will be similar, except we will write a Python program to
    find matching text and extract it instead of using a text editor for replacements
    like we did in that chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得我们在[第三章](part0024.xhtml#aid-MSDG2 "第三章：清理数据的得力助手——电子表格和文本编辑器")，*清理数据的得力助手——电子表格和文本编辑器*中，曾经尝试过一些正则表达式（regex），当时我们学习如何使用文本编辑器。在这一章中，某些概念将类似，不过这次我们将编写一个
    Python 程序来找到匹配的文本并提取出来，而不是像在那一章中那样使用文本编辑器进行替换。
- en: One final note before we start the example, although this regex method is fairly
    easy to understand, it does have some limitations which could be significant,
    depending on your particular project. We will describe the limitations of this
    method in detail at the end of the section.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始示例之前，最后需要注意的是，虽然这种正则表达式（regex）方法相对容易理解，但根据你特定的项目，它也有一些限制，可能会影响效果。我们将在本节的最后详细描述这种方法的局限性。
- en: Step one – find and save a Web file for experimenting
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步 —— 找到并保存一个用于实验的网页文件
- en: For this example, we are going to grab one of the IRC chat logs previously mentioned,
    from the Django project. These are publicly available files with a fairly regular
    structure, so they make a nice target for this project. Go to the Django IRC log
    archive at [http://django-irc-logs.com/](http://django-irc-logs.com/) and find
    a date that looks appealing to you. Navigate to the target page for one of the
    dates and save it to your working directory. You should have a single `.html`
    file when you are done.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将从之前提到的 Django 项目中获取一个 IRC 聊天记录。这些文件是公开的，具有相当规则的结构，因此非常适合用于这个项目。前往
    Django IRC 日志档案，[http://django-irc-logs.com/](http://django-irc-logs.com/)，找到一个你感兴趣的日期。进入目标日期的页面并将其保存到你的工作目录。当你完成时，应该会得到一个
    `.html` 文件。
- en: Step two – look into the file and decide what is worth extracting
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步 —— 查看文件并决定值得提取的内容
- en: Since we learned in [Chapter 2](part0020.xhtml#aid-J2B82 "Chapter 2. Fundamentals
    – Formats, Types, and Encodings"), *Fundamentals – Formats, Types, and Encodings*,
    that `.html` files are just text, and [Chapter 3](part0024.xhtml#aid-MSDG2 "Chapter 3. Workhorses
    of Clean Data – Spreadsheets and Text Editors"), *Workhorses of Clean Data – Spreadsheets
    and Text Editors*, made us very comfortable with viewing text files in a text
    editor, this step should be easy. Just open the HTML file in a text editor and
    look at it. What looks ripe for extracting?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在[第二章](part0020.xhtml#aid-J2B82 "第二章：基础知识——格式、类型和编码")，*基础知识——格式、类型和编码*中学到过，`.html`
    文件实际上就是文本，而[第三章](part0024.xhtml#aid-MSDG2 "第三章：清理数据的得力助手——电子表格和文本编辑器")，*清理数据的得力助手——电子表格和文本编辑器*，让我们非常熟悉如何在文本编辑器中查看文本文件，因此这一步应该很容易。只需在文本编辑器中打开
    HTML 文件，查看它。有什么内容看起来适合提取？
- en: When I look in my file, I see several things I want to extract. Right away I
    see that for each chat comment, there is a line number, a username, and the comment
    itself. Let's plan on extracting these three things from each chat line.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当我查看文件时，我看到有几个我想要提取的内容。立刻我就看到，对于每条聊天评论，有行号、用户名和评论本身。我们计划从每一行聊天记录中提取这三个项目。
- en: 'The following figure shows the HTML file open in my text editor. I have turned
    on soft wrapping since some of the lines are quite long (in TextWrangler this
    option is located in the menu under **View** | **Text Display** | **Soft Wrap
    Text**). Around line **29** we see the beginning of a list of chat lines, each
    of which has the three items we are interested in:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了我的文本编辑器中打开的 HTML 文件。由于某些行非常长，我已经启用了软换行（在 TextWrangler 中此选项位于菜单下的 **查看**
    | **文本显示** | **软换行文本**）。大约在第 **29** 行，我们看到了聊天记录列表的开始，每一行都包含我们感兴趣的三个项目：
- en: '![Step two – look into the file and decide what is worth extracting](img/image00274.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![第二步 —— 查看文件并决定值得提取的内容](img/image00274.jpeg)'
- en: 'Our job is therefore to find the features of each line that look the same so
    we can predictably pull out the same three items from each chat line. Looking
    at the text, here are some possible rules we can follow to extract each data item
    accurately and with minimal tweaking:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是找出每一行看起来相同的特征，以便我们可以预测性地从每一行聊天记录中提取出相同的三个项目。查看文本后，以下是我们可以遵循的一些可能规则，以准确地提取每个数据项并尽量减少调整：
- en: It appears that all three items we want are found within the `<li>` tags, which
    are themselves found inside the `<ul id="ll">` tag. Each `<li>` represents one
    chat message.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看起来我们要找的三个项目都位于`<li>`标签中，而这些`<li>`标签本身位于`<ul id="ll">`标签内。每个`<li>`表示一条聊天消息。
- en: 'Within that message, the line number is located in two places: it follows the
    string `<a href="#` and it is found within the quotation marks following the `name`
    attribute. In the example text shown, the first line number is `1574618`.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在该消息中，行号出现在两个位置：它跟在字符串`<a href="#`后面，并且出现在`name`属性后的引号内。在示例文本中，第一个行号是`1574618`。
- en: The `username` attribute is found in three places, the first of which is as
    the value of the `rel` attribute of the `li class="le"`. Within the `span` tag,
    the `username` attribute is found again as the value of the `rel` attribute, and
    it is also found between the `&lt`; and `&gt`; symbols. In the example text, the
    first `username` is `petisnnake`.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`username`属性出现在三个位置，首先是`li class="le"`的`rel`属性值。在`span`标签内，`username`属性再次作为`rel`属性的值出现，且它也出现在`&lt`;和`&gt`;符号之间。在示例文本中，第一个`username`是`petisnnake`。'
- en: The line message is found following the `</span>` tag and before the `</li>`
    tag. In the example shown, the first line message is `i didnt know that`.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行消息出现在`</span>`标签后和`</li>`标签前。在示例中，第一个行消息是`i didnt know that`。
- en: Now that we have the rules about where to find the data items, we can write
    our program.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了如何找到数据项的规则，可以开始编写我们的程序了。
- en: Step three – write a Python program to pull out the interesting pieces and save
    them to a CSV file
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步——编写一个Python程序，提取有用的信息并将其保存到CSV文件中
- en: 'Here is a short bit of code to open a given IRC log file in the format shown
    previously, parse out the three pieces we are interested in, and print them to
    a new CSV file:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个简短的代码，用于打开先前显示格式的IRC日志文件，解析出我们感兴趣的三个部分，并将它们打印到一个新的CSV文件中：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The trickiest bit of this code is the `pattern` line. This line builds the pattern
    match against which each line of the file will be compared.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中最棘手的部分是`pattern`这一行。该行构建了模式匹配，文件中的每一行都将与之进行比较。
- en: Tip
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Be vigilant. Any time the website developers change the HTML in the page, we
    run the risk that our carefully constructed regular expression pattern will no
    longer work. In fact, in the months spent writing this book, the HTML for this
    page changed at least once!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 保持警惕。每当网站开发者更改页面中的HTML时，我们就有可能面临构造的正则表达式模式失效的风险。事实上，在编写这本书的几个月里，页面的HTML至少更改过一次！
- en: Each matching target group looks like this:`.+?`. There are five of them. Three
    of these are the items we are interested in (`username`, `linenum`, and `message`),
    while the other two groups are just junk that we can discard. We will also discard
    the rest of the web page contents, since that did not match our pattern at all.
    Our program is like a sieve with exactly three functional holes in it. The good
    stuff will flow through the holes, leaving the junk behind.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 每个匹配的目标组看起来像这样：`.+?`。总共有五个匹配组。其中三个是我们感兴趣的项目（`username`、`linenum`和`message`），而其他两个组只是无关的内容，我们可以丢弃。我们还会丢弃网页中其他部分的内容，因为它们根本不符合我们的模式。我们的程序就像一个筛子，只有三个有效的孔。好东西会通过这些孔流出，而无关的内容则被留下。
- en: Step four – view the file and make sure it is clean
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四步——查看文件并确保其内容干净
- en: 'When we open the new CSV file in a text editor, we can see that the first few
    lines now look like this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在文本编辑器中打开新的CSV文件时，可以看到前几行现在像这样：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That looks like a solid result. One thing you may notice is that there is no
    encapsulation character surrounding the text in the third column. This could prove
    to be a problem since we have used commas as a delimiter. What if we have commas
    in our third column? If this worries you, you can either add quotation marks around
    the third column, or you can use tabs to delimit the columns. To do this, change
    the first `outfile.write()` line to have `\t` (tab) as the join character instead
    of comma. You can also add some whitespace trimming to the message via `ltrim()`
    to remove any stray characters.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来是一个很好的结果。你可能注意到，第三列的文本没有被包围在任何分隔符中。这可能会成为一个问题，因为我们已经使用逗号作为分隔符。如果第三列中有逗号怎么办？如果你担心这个问题，可以为第三列添加引号，或者使用制表符（tab）作为列的分隔符。为此，将第一行`outfile.write()`中的连接字符从逗号改为`\t`（制表符）。你也可以通过`ltrim()`函数修剪消息中的空格，去除任何多余的字符。
- en: The limitations of parsing HTML using regular expressions
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正则表达式解析HTML的局限性
- en: This regular expressions method seems pretty straightforward at first, but it
    has some limitations. First, for new data cleaners, regular expressions can be
    kind of a pain in the neck to design and perfect. You should definitely plan on
    spending a lot of time debugging and writing yourself copious documentation. To
    assist in the generation of regular expressions, I would definitely recommend
    using a regular expression tester, such as [http://Pythex.org](http://Pythex.org),
    or just use your favorite search engine to find one. Make sure you specify that
    you want a Python regex tester if that is the language you are using.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个正则表达式方法一开始看起来很简单，但它也有一些局限性。首先，对于新的数据清理者来说，设计和完善正则表达式可能会非常麻烦。你肯定需要计划花费大量时间进行调试，并且写下大量文档。为了帮助生成正则表达式，我强烈建议使用正则表达式测试工具，如
    [http://Pythex.org](http://Pythex.org)，或者直接使用你喜欢的搜索引擎找到一个。如果你使用的是 Python 语言，请确保你选择的是
    Python 正则表达式测试工具。
- en: Next, you should know in advance that regular expressions are completely dependent
    on the structure of the web page staying the same in the future. So, if you plan
    to collect data from a website on a schedule, the regular expressions you write
    today may not work tomorrow. They will only work if the layout of the page does
    not change. A single space added between two tags will cause the entire regex
    to fail and will be extremely difficult to troubleshoot. Keep in mind too that
    most of the time you have little or no control over website changes, since it
    is usually not your own website that you are collecting data from!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你应该提前知道，正则表达式完全依赖于网页结构在未来保持不变。因此，如果你计划定期从一个网站收集数据，你今天写的正则表达式可能明天就不管用了。只有在网页布局没有变化的情况下，它们才会有效。即使在两个标签之间添加一个空格，也会导致整个正则表达式失败，而且调试起来非常困难。还要记住，大多数时候你无法控制网站的变化，因为你通常不是自己的网站在收集数据！
- en: 'Finally, there are many, many cases where it is next-to-impossible to accurately
    write a regular expression to match a given HTML construct. Regex is powerful
    but not perfect or infallible. For an amusing take on this issue, I refer you
    to the famous Stack Overflow answer that has been upvoted over 4000 times: [http://stackoverflow.com/questions/1732348/](http://stackoverflow.com/questions/1732348/).
    In this answer, the author humorously expresses the frustration of so many programmers
    who try over and over to explain why regex is not a perfect solution to parsing
    irregular and ever-changing HTML found in web pages.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有许多情况几乎不可能准确地编写正则表达式来匹配给定的 HTML 结构。正则表达式很强大，但并不完美或无懈可击。对于这个问题的幽默解读，我推荐你去看那个在
    Stack Overflow 上被点赞超过 4000 次的著名回答：[http://stackoverflow.com/questions/1732348/](http://stackoverflow.com/questions/1732348/)。在这个回答中，作者幽默地表达了许多程序员的挫败感，他们一遍又一遍地尝试解释为什么正则表达式并不是解析不规则且不断变化的
    HTML 的完美解决方案。
- en: Method two – Python and BeautifulSoup
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法二 - Python 和 BeautifulSoup
- en: Since regular expressions have some limitations, we will definitely need more
    tools in our data cleaning toolkit. Here, we describe how to extract data from
    HTML pages using a parse tree-based Python library called **BeautifulSoup**.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于正则表达式存在一些局限性，我们在数据清理工具包中肯定还需要更多工具。这里，我们介绍如何使用基于解析树的 Python 库 **BeautifulSoup**
    从 HTML 页面中提取数据。
- en: Step one – find and save a file for experimenting
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步 - 找到并保存一个用于实验的文件
- en: 'For this step, we will use the same file as we did for Method 1: the file from
    the Django IRC channel. We will search for the same three items. Doing this will
    make it easy to compare the two methods to each other.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一步，我们将使用与方法 1 相同的文件：来自 Django IRC 频道的文件。我们将搜索相同的三个项目。这样做将使得这两种方法之间的比较更加容易。
- en: Step two – install BeautifulSoup
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步 - 安装 BeautifulSoup
- en: BeautifulSoup is currently in version 4\. This version will work for both Python
    2.7 and Python 3.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: BeautifulSoup 目前是 4 版本。这个版本可以在 Python 2.7 和 Python 3 中使用。
- en: Note
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you are using the Enthought Canopy Python environment, simply run `pip install
    beautifulsoup4` in the Canopy Terminal.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是 Enthought Canopy Python 环境，只需在 Canopy 终端中运行 `pip install beautifulsoup4`。
- en: Step three – write a Python program to extract the data
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步 - 编写一个 Python 程序来提取数据
- en: 'The three items we are interested in are found within the set of `li` tags,
    specifically those with `class="le"`. There are not any other `li` tags in this
    particular file, but let''s be specific just in case. Here are the items we want
    and where to find them in the parse tree:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的三个项位于一组 `li` 标签中，具体来说是那些 `class="le"` 的标签。在这个特定文件中没有其他 `li` 标签，但为了以防万一，我们还是要具体说明。以下是我们需要的项及其在解析树中的位置：
- en: We can extract the username from the `li` tag underneath the `rel` attribute.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以从 `li` 标签下的 `rel` 属性中提取用户名。
- en: We can get the `linenum` value from the `name` attribute in the `a` tag. The
    `a` tag is also the first item in the contents of the `li` tag.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以从 `a` 标签的 `name` 属性中获取 `linenum` 值。`a` 标签也是 `li` 标签内容中的第一个项。
- en: Note
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that arrays are zero-based so we need to ask for item 0.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，数组是从零开始的，所以我们需要请求项 0。
- en: In BeautifulSoup, the **contents** of a tag are the items underneath that tag
    in the parse tree. Some other packages will call them **child** items.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 BeautifulSoup 中，**标签的内容**是该标签在解析树中的下级项。一些其他包会将这些称为 **子项**。
- en: We can extract the message as the fourth content item in the `li` tag (referenced
    as array item [3]). We also notice that there is a leading space at the front
    of every message, so we want to strip that off before saving the data.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以从 `li` 标签的第四个内容项中提取消息（引用为数组项 [3]）。我们还注意到每条消息前面都有一个空格，因此我们需要在保存数据之前去除它。
- en: 'Here is the Python code that corresponds to the outline of what we want from
    the parse tree:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是与我们在解析树中想要的内容相对应的 Python 代码：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Step four – view the file and make sure it is clean
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四步 – 查看文件并确保其清洁
- en: 'When we open the new CSV file in a text editor, we can see that the first few
    lines now look identical to the ones from Method 1:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在文本编辑器中打开新的 CSV 文件时，可以看到前几行现在与方法 1 中的内容完全相同：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Just like with the regular expressions method, if you are worried about commas
    embedded within the last column, you can encapsulate the column text in quotes
    or just use tabs to delimit the columns instead.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 就像使用正则表达式方法一样，如果你担心最后一列中嵌入的逗号，你可以将该列的文本用引号括起来，或者直接使用制表符来分隔列。
- en: Method three – Chrome Scraper
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法三 – Chrome Scraper
- en: 'If you really do not want to write a program to parse out data, there are several
    browser-based tools that use a tree structure to allow you to identify and extract
    the data you are interested in. I think the easiest to use with minimum work is
    a Chrome extension called **Scraper**, created by a developer called **mnmldave**
    (real name: **Dave Heaton**).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的不想编写程序来解析数据，也有几个基于浏览器的工具使用树形结构来帮助你识别和提取感兴趣的数据。我认为使用起来最简单、工作量最少的工具是一个名为
    **Scraper** 的 Chrome 扩展，由名为 **mnmldave**（真名：**Dave Heaton**）的开发者创建。
- en: Step one – install the Scraper Chrome extension
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步 – 安装 Scraper Chrome 扩展
- en: Download the Chrome browser if you do not already have that running. Make sure
    that you get the correct Scraper extension; there are several extensions that
    have very similar names. I recommend using the developer's own GitHub site for
    this product, available at [http://mnmldave.github.io/scraper/](http://mnmldave.github.io/scraper/).
    This way you will be able to have the correct scraper tool, rather than trying
    to search using the Chrome store. From the [http://mmldave.github.io/scraper](http://mmldave.github.io/scraper)
    site, click the link to install the extension from the Google Store, and restart
    your browser.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有安装 Chrome 浏览器，请下载并安装。确保你获得正确的 Scraper 扩展；有几个扩展的名字非常相似。我建议使用开发者自己提供的 GitHub
    网站来下载该产品，网址是 [http://mnmldave.github.io/scraper/](http://mnmldave.github.io/scraper/)。这样你可以确保使用正确的抓取工具，而不是通过
    Chrome 商店搜索。从 [http://mmldave.github.io/scraper](http://mmldave.github.io/scraper)
    网站，点击链接从 Google 商店安装扩展并重启浏览器。
- en: Step two – collect data from the website
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步 – 从网站收集数据
- en: Point your browser to the same web URL we have been using to get the data for
    the other two web data extraction experiments, one of the Django IRC logs. I have
    been using the September 13, 2014 log for the examples and screenshots here, so
    I will go to [http://django-irc-logs.com/2014/sep/13/](http://django-irc-logs.com/2014/sep/13/).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 将浏览器指向我们之前用来获取其他两个网页数据提取实验数据的相同网页 URL，其中一个是 Django IRC 日志。我在这里使用的是 2014 年 9
    月 13 日的日志作为示例和截图，所以我将访问 [http://django-irc-logs.com/2014/sep/13/](http://django-irc-logs.com/2014/sep/13/)。
- en: 'In my browser, at the time of writing, this page looks like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我编写本文时，浏览器中的该页面显示如下：
- en: '![Step two – collect data from the website](img/image00275.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![第二步 – 从网站收集数据](img/image00275.jpeg)'
- en: 'We have three items from this IRC log that we are interested in:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这份IRC日志中的三个项目感兴趣：
- en: The line number (we know from our previous two experiments that this is part
    of the link underneath the **#** sign)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行号（我们从前两个实验中知道，这部分链接位于**#**符号下方）
- en: The username (located between the **<** and **>** symbols)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户名（位于**<**和**>**符号之间）
- en: The actual line message
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际的行信息
- en: 'Scraper allows us to highlight each of these three items in turn and export
    the values to a Google Spreadsheet, where we can then reassemble them into a single
    sheet and export as CSV (or do whatever else we want with them). Here is how to
    do it:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Scraper允许我们依次高亮显示这三项内容，并将其值导出到Google电子表格中，之后我们可以将它们重新组合成一个单一的表格，并导出为CSV格式（或根据需要进行其他操作）。以下是操作步骤：
- en: Use your mouse to highlight the item you want to scrape.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用鼠标高亮你想要抓取的项目。
- en: Right-click and choose **Scrape similar…** from the menu. In the following example,
    I have selected the username **petisnnake** as the one that I want Scraper to
    use:![Step two – collect data from the website](img/image00276.jpeg)
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 右键点击并从菜单中选择**抓取相似项…**。在以下示例中，我选择了用户名**petisnnake**作为我要让Scraper使用的目标：![步骤二 –
    从网站收集数据](img/image00276.jpeg)
- en: After selecting **Scrape similar**, the tool will show you a new window with
    a collection of all the similar items from the page. The following screenshot
    shows the entire list of usernames that Scraper found:![Step two – collect data
    from the website](img/image00277.jpeg)
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择**抓取相似项**后，工具会显示一个新窗口，列出页面上所有相似项目。以下截图显示了Scraper找到的所有用户名列表：![步骤二 – 从网站收集数据](img/image00277.jpeg)
- en: Scraper finds all the similar items based on one sample username.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Scraper根据一个示例用户名找到所有相似的项目。
- en: At the bottom of the window, there is a button labeled **Export to Google Docs…**.
    Note that depending on your settings, you may have to click to agree to allow
    access to Google Docs from within Scraper.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 窗口底部有一个标有**导出到Google文档…**的按钮。请注意，根据您的设置，您可能需要点击同意以允许Scraper访问Google文档。
- en: Step three – final cleaning on the data columns
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤三 – 数据列的最终清理
- en: 'Once we have all the data elements extracted from the page and housed in separate
    Google Docs, we will need to combine them into one file and do some final cleaning.
    Here is an example of what the line numbers look like once they have been extracted,
    but before we have cleaned them:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从页面上提取了所有数据元素并将它们存储在独立的Google文档中，我们将需要将它们合并为一个文件，并进行最后的清理。以下是提取后的行号示例，但在清理之前的样子：
- en: '![Step three – final cleaning on the data columns](img/image00278.jpeg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![步骤三 – 数据列的最终清理](img/image00278.jpeg)'
- en: We are not interested in column **A** at all, nor are we interested in the leading
    **#** symbol. The username and line message data is similar—we want most of it,
    but we would like to remove some symbols and combine everything into a single
    Google Spreadsheet.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对列**A**完全不感兴趣，也不关心前导的**#**符号。用户名和行信息数据类似——我们需要大部分内容，但我们想要去除一些符号，并将所有内容合并到一个Google电子表格中。
- en: 'Using our find-and-replace techniques from [Chapter 3](part0024.xhtml#aid-MSDG2
    "Chapter 3. Workhorses of Clean Data – Spreadsheets and Text Editors"), *Workhorses
    of Clean Data – Spreadsheets and Text Editors* (namely removing the **#**, **<**,
    and **>** symbols and pasting the rows into a single sheet), we end up with a
    single clean dataset that looks like this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们在[第3章](part0024.xhtml#aid-MSDG2 "第3章 清理数据的得力工具 – 电子表格和文本编辑器")中介绍的查找和替换技巧，*清理数据的得力工具
    – 电子表格和文本编辑器*（即去除**#**、**<**和**>**符号，并将行粘贴到单一表格中），我们最终得到一个干净的单一数据集，如下所示：
- en: '![Step three – final cleaning on the data columns](img/image00279.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![步骤三 – 数据列的最终清理](img/image00279.jpeg)'
- en: Scraper is a nice tool for extracting small amounts of data from web pages.
    It has a handy Google Spreadsheets interface, and can be a quick solution if you
    do not feel like writing a program to do this work. In the next section, we will
    tackle a larger project. It may be complicated enough that we will have to implement
    a few of the concepts from this chapter into a single solution.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Scraper是一个从网页中提取少量数据的好工具。它有一个方便的Google电子表格界面，如果你不想写程序来完成这个工作，它可以是一个快捷的解决方案。在下一部分，我们将处理一个更大的项目。这个项目可能足够复杂，需要我们将本章的一些概念融入到一个综合解决方案中。
- en: Example project – Extracting data from e-mail and web forums
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例项目 – 从电子邮件和网络论坛中提取数据
- en: 'The Django IRC logs project was pretty simple. It was designed to show you
    the differences between three solid techniques that are commonly used to extract
    clean data from within HTML pages. The data we extracted included the line number,
    the username, and the IRC chat message, all of which were easy to find and required
    almost no additional cleaning. In this new example project, we will consider a
    case that is conceptually similar, but that will require us to extend the idea
    of data extraction beyond HTML to two other types of semi-structured text found
    on the Web: e-mail messages hosted on the Web and web-based discussion forums.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Django IRC 日志项目相当简单。它的设计目的是向你展示三种常用于从 HTML 页面中提取干净数据的技术之间的差异。我们提取的数据包括行号、用户名和
    IRC 聊天消息，所有这些都很容易找到，几乎不需要额外的清理。在这个新的示例项目中，我们将考虑一个概念上类似的案例，但这将要求我们将数据提取的概念从 HTML
    扩展到 Web 上的另外两种半结构化文本：托管在 Web 上的电子邮件消息和基于 Web 的讨论论坛。
- en: The background of the project
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目的背景
- en: I was recently working on a research study about how social media can be used
    to provide software technical support. Specifically, I was trying to discover
    whether certain types of software development organizations that make APIs and
    frameworks should move their technical support for developers to Stack Overflow
    or whether they should continue to use older media, such as e-mail and web forums.
    To complete this study, I compared (among other things) how long it took developers
    to get an answer to their API question via Stack Overflow versus how long it took
    on older social media such as web forums and e-mail groups.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我在进行一项关于社交媒体如何用于提供软件技术支持的研究。具体来说，我在尝试发现，某些开发 API 和框架的软件开发组织是否应该将开发者的技术支持转移到
    Stack Overflow，还是应该继续使用旧的媒体，如电子邮件和网络论坛。为了完成这项研究，我比较了（其中之一）开发者通过 Stack Overflow
    获得 API 问题答案的时间与通过旧社交媒体（如网络论坛和电子邮件组）获得答案的时间。
- en: 'In this project, we will work on a small piece of this question. We will download
    two types of raw data representing the older social media: HTML files from a web
    forum and e-mail messages from Google Groups. We will write Python code to extract
    the dates and time of the messages sent to these two support forums. We will then
    figure out which messages were sent in reply to the others and calculate some
    basic summary statistics about how long it took each message to get a reply.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将处理这个问题的一个小部分。我们将下载两种表示旧社交媒体的原始数据：来自网络论坛的 HTML 文件和来自 Google Groups
    的电子邮件消息。我们将编写 Python 代码来提取这些两个支持论坛中发送消息的日期和时间。然后我们将找出哪些消息是对其他消息的回复，并计算一些关于每条消息收到回复所花时间的基本统计数据。
- en: Tip
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If you are wondering why we aren't extracting data for the Stack Overflow portion
    of the question in this example project, just wait until [Chapter 9](part0059.xhtml#aid-1O8H61
    "Chapter 9. Stack Overflow Project"), *Stack Overflow Project*. That entire chapter
    is devoted to creating and cleaning a Stack Overflow database.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道为什么我们在这个示例项目中没有提取 Stack Overflow 部分的数据，稍等一下，直到[第九章](part0059.xhtml#aid-1O8H61
    "第九章. Stack Overflow 项目")，*Stack Overflow 项目*。整章内容将致力于创建和清理一个 Stack Overflow 数据库。
- en: This project will be divided into two parts. In Part one, we will extract data
    from the e-mail archive from a project hosted on Google Groups, and in Part two,
    we will extract our data from the HTML files of a different project.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目将分为两部分。在第一部分，我们将从 Google Groups 上托管的项目的电子邮件存档中提取数据；在第二部分，我们将从另一个项目的 HTML
    文件中提取数据。
- en: Part one – cleaning data from Google Groups e-mail
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一部分 – 清理 Google Groups 电子邮件中的数据
- en: Many software companies have traditionally used e-mail mailing lists or hybrid
    e-mail-web forums to provide technical support for their products. Google Groups
    is a popular choice for this service. Users can either send e-mails to the group,
    or they can read and search the messages in a web browser. However, some companies
    have moved away from providing technical support to developers via Google Groups
    (including Google products themselves), and are instead using Stack Overflow.
    The database product Google BigQuery is one such group that now uses Stack Overflow.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 许多软件公司传统上使用电子邮件邮件列表或混合的电子邮件-网络论坛来为其产品提供技术支持。Google Groups 是这种服务的一个流行选择。用户可以通过电子邮件发送消息到小组，或者在
    Web 浏览器中阅读和搜索消息。然而，一些公司已经不再通过 Google Groups 为开发者提供技术支持（包括 Google 自己的产品），而是转而使用
    Stack Overflow。Google BigQuery 这样的数据库产品就是现在使用 Stack Overflow 的一个例子。
- en: Step one – collect the Google Groups messages
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一步 – 收集 Google Groups 消息
- en: 'To study the response times for questions on the BigQuery Google Group, I first
    created a list of the URLs for all the postings in that group. You can find my
    complete list of URLs on my GitHub site: [https://github.com/megansquire/stackpaper2015/blob/master/BigQueryGGurls.txt](https://github.com/megansquire/stackpaper2015/blob/master/BigQueryGGurls.txt).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究 BigQuery Google Group 中问题的响应时间，我首先创建了该组中所有帖子 URL 的列表。你可以在我的 GitHub 网站上找到我的完整
    URL 列表：[https://github.com/megansquire/stackpaper2015/blob/master/BigQueryGGurls.txt](https://github.com/megansquire/stackpaper2015/blob/master/BigQueryGGurls.txt)。
- en: 'Once we have a list of target URLs, we can write a Python program to download
    all the e-mails residing in those URLs, and save them to disk. In the following
    program, my list of URLs has been saved as the file called `GGurls.txt`. The `time`
    library is included, so we can take a short `sleep()` method in between requests
    to the Google Groups server:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了目标 URL 列表，就可以编写一个 Python 程序下载所有存储在这些 URL 中的电子邮件，并将它们保存到磁盘。在下面的程序中，我的 URL
    列表已保存为名为 `GGurls.txt` 的文件。`time` 库已经包含，所以我们可以在请求 Google Groups 服务器之间使用短暂的 `sleep()`
    方法：
- en: '[PRE5]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This program results in 667 files being written to disk.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序最终将 667 个文件写入磁盘。
- en: Step two – extract data from the Google Groups messages
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二步 - 从 Google Groups 消息中提取数据
- en: 'We now have 667 e-mail messages in separate files. Our task is to write a program
    to read these in one at a time and use one of the techniques from this chapter
    to extract the pieces of information we need. If we peek inside one of the e-mail
    messages, we see lots of **headers**, which store information about the e-mail,
    or its **metadata**. We can quickly see the three headers that identify the metadata
    elements that we need:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有 667 封电子邮件消息存储在不同的文件中。我们的任务是编写一个程序，一次读取这些文件并使用本章中的某种技术提取我们需要的信息。如果我们查看其中一封电子邮件消息，我们会看到许多
    **头部**，它们存储关于电子邮件的信息，或者说是其 **元数据**。我们可以迅速看到标识我们需要的元数据元素的三个头部：
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: All messages have `Message-ID` and `Date`, but the `In-Reply-To` header will
    only appear in a message that is a reply to another message. An `In-Reply-To`
    value must be the `Message-ID` value of another message.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 所有消息都有 `Message-ID` 和 `Date`，但是 `In-Reply-To` 头部只有在消息是对另一条消息的回复时才会出现。`In-Reply-To`
    的值必须是另一条消息的 `Message-ID` 值。
- en: 'The following code shows a regular expression-based solution to extract the
    `Date`, `Message-ID`, and `In-Reply-To` (if available) values and to create some
    lists of original and reply messages. Then, the code attempts to calculate the
    time differences between a message and its replies:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了基于正则表达式的解决方案，用于提取 `Date`、`Message-ID` 和 `In-Reply-To`（如果有）值，并创建一些原始消息和回复消息的列表。然后，代码尝试计算消息与其回复之间的时间差：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this code, the initial `for` loop zips through each message and extracts
    the three pieces of data we are interested in. (This program does not store these
    to a separate file or to disk, but you could add this functionality if you wish
    to.) This portion of the code also creates two important lists:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，初始的 `for` 循环遍历每一条消息并提取我们感兴趣的三项数据。（该程序不会将这些数据存储到单独的文件或磁盘上，但如果你需要，可以添加这个功能。）这部分代码还创建了两个重要的列表：
- en: '`originals[]` is a list of original messages. We make the assumption that these
    are primarily questions being asked of the list members.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`originals[]` 是原始消息的列表。我们假设这些主要是成员提问的问题。'
- en: '`replies[]` is a list of reply messages. We assume that these are primarily
    answers to questions asked in another message.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replies[]` 是回复消息的列表。我们假设这些主要是回答其他消息中问题的回复。'
- en: The second `for` loop processes each message in the list of original messages,
    doing the following, if there is a reply to the original message, try to figure
    out how long that reply took to be sent. We then keep a list of reply times.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个 `for` 循环处理原始消息列表中的每一条消息，执行以下操作：如果原始消息有回复，尝试计算该回复发送所花费的时间。然后我们会记录下回复时间的列表。
- en: Extraction code
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提取代码
- en: 'For this chapter, we are mostly interested in the cleaning and extraction portion
    of the code, so let''s look closely at those lines. Here, we process each line
    of the e-mail file looking for three e-mail headers: `In-Reply-To`, `Message-ID`,
    and `Date`. We use regex searching and grouping, just like we did in Method 1
    earlier in this chapter, to delimit the headers and easily extract the values
    that follow:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们主要关注代码中的清理和提取部分，因此让我们仔细看看这些代码行。在这里，我们处理每一行电子邮件文件，寻找三个电子邮件头部：`In-Reply-To`、`Message-ID`
    和 `Date`。我们使用正则表达式搜索和分组，就像我们在本章第一部分的方法中做的那样，来限定这些头部并轻松提取其后的值：
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Why did we decide to use regex here instead of a tree-based parser? There are
    two main reasons:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们决定在这里使用正则表达式而不是基于树的解析器？主要有两个原因：
- en: Because the e-mails we downloaded are not HTML, they cannot easily be described
    as a tree of nodes with parents and children. Therefore, a parse tree-based solution
    such as BeautifulSoup is not the best choice.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们下载的电子邮件不是 HTML 格式，因此它们不能轻松地描述为具有父节点和子节点的树状结构。因此，基于解析树的解决方案（如 BeautifulSoup）不是最好的选择。
- en: Because e-mail headers are structured and very predictable (especially the three
    headers we are looking for here), a regex solution is acceptable.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于电子邮件头部是结构化且非常可预测的（尤其是我们在这里寻找的三个头部），因此使用正则表达式解决方案是可以接受的。
- en: Program output
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 程序输出
- en: 'The output of this program is to print three numbers that estimate the mean,
    standard deviation, and median time in hours for replies to messages on this Google
    Group. When I run this code, my results are as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序的输出是打印出三个数字，估算该 Google Group 上消息的回复时间的均值、标准差和中位数（单位：小时）。当我运行此代码时，我得到的结果如下：
- en: '[PRE9]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This means that the median response time to a message posted to the BigQuery
    Google Group was about 18 hours. Now let''s consider how to extract similar data
    from a different source: web forums. Do you think responses to questions in a
    web forum will be faster, slower, or about the same as a Google Group?'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，发布到 BigQuery Google Group 上的消息的中位响应时间大约为 18 小时。现在，让我们考虑如何从另一种来源提取类似的数据：网络论坛。你认为在网络论坛上回答问题的速度会更快、更慢，还是与
    Google Group 相当？
- en: Part two – cleaning data from web forums
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二部分 – 清理来自网络论坛的数据
- en: The web forums we will study for this project are from a company called **DocuSign**.
    They also moved their developer support to Stack Overflow, but they have an archive
    of their older web-based developer forum still online. I poked around on their
    website until I found out how to download some of the messages from those old
    forums. The process shown here is more involved than the Google Groups example,
    but you will learn a lot about how to collect data automatically.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这个项目中研究的网络论坛来自一家名为**DocuSign**的公司。他们也将开发者支持转移到了 Stack Overflow，但他们仍然保留着一个旧版的基于网页的开发者论坛存档。我在他们的网站上四处寻找，直到发现了如何下载那些旧论坛中的一些消息。这里展示的过程比
    Google Groups 示例要复杂一些，但你将学到很多关于如何自动收集数据的知识。
- en: Step one – collect some RSS that points us to HTML files
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤一 – 收集一些指向 HTML 文件的 RSS
- en: The DocuSign developer forum has thousands of messages on it. We would like
    to have a list of the URLs for all those messages or discussion threads so that
    we can write some code to download them all automatically, and extract the reply
    times efficiently.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: DocuSign 开发者论坛上有成千上万的消息。我们希望能够获得所有这些消息或讨论帖子的 URL 列表，以便我们编写代码自动下载它们，并高效提取回复时间。
- en: To do this, first we will need a list of all the URLs for the discussions. I
    found that the archive of DocuSign's old Dev-Zone developer site is located at
    [https://community.docusign.com/t5/DevZone-Archives-Read-Only/ct-p/dev_zone](https://community.docusign.com/t5/DevZone-Archives-Read-Only/ct-p/dev_zone).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，首先我们需要获取所有讨论的 URL 列表。我发现 DocuSign 旧版 Dev-Zone 开发者网站的存档位于[https://community.docusign.com/t5/DevZone-Archives-Read-Only/ct-p/dev_zone](https://community.docusign.com/t5/DevZone-Archives-Read-Only/ct-p/dev_zone)。
- en: 'The site looks like this in the browser:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 该网站在浏览器中的显示如下：
- en: '![Step one – collect some RSS that points us to HTML files](img/image00280.jpeg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![步骤一 – 收集一些指向 HTML 文件的 RSS](img/image00280.jpeg)'
- en: We definitely do not want to click through each one of those forums and then
    click into each message and save it manually. That would take forever, and it
    would be extremely boring. Is there a better way?
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绝对不希望点击进入每一个论坛，再进入每条消息并手动保存。这会花费很长时间，而且极其无聊。有没有更好的方法？
- en: 'The DocuSign website''s **Help** pages indicate that it is possible to download
    a **Really Simple Syndication** (**RSS**) file showing the newest threads and
    messages in each forum. We can use the RSS files to automatically collect the
    URLs for many of the discussions on the site. The RSS files we are interested
    in are the ones relating to the developer support forums only ( not the announcements
    or sales forums). These RSS files are available from the following URLs:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**DocuSign**网站的**帮助**页面表明，可以下载一个**真正简单的聚合**（**RSS**）文件，显示每个论坛中最新的讨论和消息。我们可以使用这些
    RSS 文件自动收集网站上许多讨论的 URL。我们感兴趣的 RSS 文件仅与开发者支持论坛相关（而不是公告或销售论坛）。这些 RSS 文件可以通过以下网址获得：'
- en: '[https://community.docusign.com/docusign/rss/board?board.id=upcoming_releases](https://community.docusign.com/docusign/rss/board?board.id=upcoming_releases)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://community.docusign.com/docusign/rss/board?board.id=upcoming_releases](https://community.docusign.com/docusign/rss/board?board.id=upcoming_releases)'
- en: '[https://community.docusign.com/docusign/rss/board?board.id=DocuSign_Developer_Connection](https://community.docusign.com/docusign/rss/board?board.id=DocuSign_Developer_Connection)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://community.docusign.com/docusign/rss/board?board.id=DocuSign_Developer_Connection](https://community.docusign.com/docusign/rss/board?board.id=DocuSign_Developer_Connection)'
- en: '[https://community.docusign.com/docusign/rss/board?board.id=Electronic_Signature_API](https://community.docusign.com/docusign/rss/board?board.id=Electronic_Signature_API)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://community.docusign.com/docusign/rss/board?board.id=Electronic_Signature_API](https://community.docusign.com/docusign/rss/board?board.id=Electronic_Signature_API)'
- en: '[https://community.docusign.com/docusign/rss/board?board.id=Java](https://community.docusign.com/docusign/rss/board?board.id=Java)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://community.docusign.com/docusign/rss/board?board.id=Java](https://community.docusign.com/docusign/rss/board?board.id=Java)'
- en: '[https://community.docusign.com/docusign/rss/board?board.id=php_api](https://community.docusign.com/docusign/rss/board?board.id=php_api)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://community.docusign.com/docusign/rss/board?board.id=php_api](https://community.docusign.com/docusign/rss/board?board.id=php_api)'
- en: '[https://community.docusign.com/docusign/rss/board?board.id=dev_other](https://community.docusign.com/docusign/rss/board?board.id=dev_other)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://community.docusign.com/docusign/rss/board?board.id=dev_other](https://community.docusign.com/docusign/rss/board?board.id=dev_other)'
- en: '[https://community.docusign.com/docusign/rss/board?board.id=Ask_A_Development_Question_Board](https://community.docusign.com/docusign/rss/board?board.id=Ask_A_Development_Question_Board)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://community.docusign.com/docusign/rss/board?board.id=Ask_A_Development_Question_Board](https://community.docusign.com/docusign/rss/board?board.id=Ask_A_Development_Question_Board)'
- en: Visit each URL in that list in your web browser (or just one, if you are pressed
    for time). The file is RSS, which will look like semi-structured text with tags,
    similar to HTML. Save the RSS as a file on your local system and give each one
    a `.rss` file extension. At the end of this process, you should have at most seven
    RSS files, one for each preceding URL shown.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的浏览器中访问列表中的每个URL（如果时间紧迫，可以只访问一个）。该文件是RSS格式，类似于带标签的半结构化文本，类似HTML。将RSS保存为本地系统中的文件，并给每个文件加上`.rss`扩展名。完成此过程后，您最多应拥有七个RSS文件，每个文件对应前面展示的一个URL。
- en: 'Inside each of these RSS files is metadata describing all the discussion threads
    on the forum, including the one piece of data that we really want at this stage:
    the URL for each particular discussion thread. Open one of the RSS files in a
    text editor and you will be able to spot an example of a URL we are interested
    in. It looks like this, and inside the file, you will see that there is one of
    these for each discussion thread:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 每个RSS文件中都有描述论坛中所有讨论主题的元数据，其中包括我们在此阶段真正需要的数据：每个特定讨论主题的URL。用文本编辑器打开其中一个RSS文件，您将能够看到我们感兴趣的URL示例。它看起来像这样，并且在文件中，您会看到每个讨论主题都有一个这样的URL：
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, we can write a program to loop through each RSS file, look for these URLs,
    visit them, and then extract the reply times we are interested in. The next section
    breaks this down into a series of smaller steps, and then shows a program that
    does the entire job.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以编写一个程序，循环遍历每个RSS文件，查找这些URL，访问它们，然后提取我们感兴趣的回复时间。接下来的部分将这些步骤拆解成一系列更小的步骤，并展示一个完成整个任务的程序。
- en: Step two – Extract URLs from RSS; collect and parse HTML
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二步 – 从RSS中提取URL；收集并解析HTML
- en: 'In this step, we will write a program that will do the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们将编写一个程序，执行以下操作：
- en: Open each RSS file that we saved in Step 1.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开我们在第1步中保存的每个RSS文件。
- en: Every time we see a `<guid>` and `</guid>` tag pair, extract the URL inside
    and add it to a list.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次看到`<guid>`和`</guid>`标签对时，提取其中的URL并将其添加到列表中。
- en: For each URL in the list, download whatever HTML file is at that location.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对列表中的每个URL，下载该位置上的HTML文件。
- en: Read that HTML file and extract the original post time and the reply time from
    each message.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读该HTML文件，提取每条消息的原始发帖时间和回复时间。
- en: Calculate how long it took to send a reply with mean, median, and standard deviation,
    like we did in Part 1.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算发送回复所需的时间，并计算平均值、中位数和标准差，就像我们在第1部分做的那样。
- en: 'Here is some Python code to handle all these steps. We will go over the extraction
    parts in detail at the end of the code listing:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些Python代码，用于处理所有这些步骤。我们将在代码列出的末尾详细介绍提取部分：
- en: '[PRE11]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Program status
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 程序状态
- en: 'As the program works, it prints out status messages so we know what it is working
    on. The status messages look like this, and there is one of these for each URL
    that is found in the RSS feed(s):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 程序运行时，会打印出状态消息，以便我们知道它正在处理什么。每找到一个RSS源中的URL，就会有一个这样的状态消息，内容如下：
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this display, 74 represents the rounded number of hours between the posted
    time of the first message in the thread and the first reply in the thread (about
    three days, plus two hours).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在此展示中，74表示从线程中第一条消息的发布时间到第一条回复之间的小时数（大约三天，再加上两小时）。
- en: Program output
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 程序输出
- en: 'At its conclusion, this program prints out the mean, standard deviation, and
    median reply times in hours, just like the Part 1 program did for Google Groups:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在程序结束时，它会打印出平均值、标准差和中位数回复时间（以小时为单位），就像第一部分程序为Google Groups所做的那样：
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It looks like reply time in the DocuSign forum is a tiny bit slower than Google
    Groups. It is reporting 20 hours compared to Google Groups, which took 18 hours,
    but at least both numbers are in the same approximate range. Your results might
    change, since new messages are getting added all the time.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来DocuSign论坛的回复时间比Google Groups略慢。它报告了20小时，而Google Groups是18小时，但至少两个数字在同一大致范围内。你的结果可能会有所不同，因为新消息一直在添加。
- en: Extraction code
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提取代码
- en: 'Since we are mostly interested in data extraction, let''s look closely at the
    part of the code where that happens. Here is the most relevant line of code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们主要关注数据提取，让我们仔细看看代码中发生这一过程的部分。以下是最相关的一行代码：
- en: '[PRE14]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Just like with some of our previous examples, this code also relies on regular
    expressions to do its work. However, this regex is pretty messy. Maybe we should
    have written this with BeautifulSoup? Let''s take a look at the original HTML
    that we are trying to match so that we can understand more about what this code
    is trying to do and whether we should have done this a different way. What follows
    is a screenshot of how the page looks in the browser. The times we are interested
    in have been annotated on the screenshot:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前的一些示例一样，这段代码也依赖于正则表达式来完成工作。然而，这个正则表达式相当混乱。也许我们应该用BeautifulSoup来写？让我们看一下我们试图匹配的原始HTML，以便更好地理解这段代码的目的，以及是否应该采取不同的方式。以下是页面在浏览器中的截图，感兴趣的时间已在截图上做了标注：
- en: '![Extraction code](img/image00281.jpeg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![提取代码](img/image00281.jpeg)'
- en: 'What does the underlying HTML look like though? That is the part that our program
    will need to be able to parse. It turns out that the date of the original message
    is printed in several places on the HTML page, but the date and time combination
    is only printed once for the original and once for the reply. Here is the HTML
    showing how these look (the HTML has been condensed and newlines removed for easier
    viewing):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 那么底层HTML是什么样的呢？这正是我们程序需要能够解析的部分。原始消息的日期在HTML页面中打印了多个地方，但日期和时间组合只打印了一次，原始消息和回复各一次。以下是HTML的展示，显示了这些内容是如何呈现的（HTML已被压缩并去除换行，以便更易查看）：
- en: '[PRE15]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This turns out to be a pretty straightforward problem for regex to solve, since
    we can write a single regular expression and find all the instances of it for
    both types of messages. In the code, we state that the first instance we find
    becomes the original message, and the next one becomes the reply, as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是一个正则表达式可以轻松解决的问题，因为我们可以编写一个正则表达式，找到两种类型的消息中的所有实例。在代码中，我们声明第一个找到的实例是原始消息，下一个实例是回复，代码如下：
- en: '[PRE16]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We could have used a parse tree-based solution such as BeautifulSoup, but we
    would have to contend with the fact that the `span` class values are identical
    for both sets of dates, and even the parent element (the `<p>` tag) turns out
    to have the same class. So, this parse tree is substantially more complex than
    the one shown in Method 2 earlier in the chapter.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以使用基于解析树的解决方案，如BeautifulSoup，但我们需要处理这样一个事实：`span`类的值在两组日期中是相同的，甚至父元素（`<p>`标签）也有相同的类。因此，这个解析树比章节中第二种方法所展示的要复杂得多。
- en: If you really wanted to try to use BeautifulSoup for this extraction, my recommendation
    would be first to look at the structure of the page using your browser's Developer
    Tools, for example, in the Chrome browser, you can select the element you are
    interested in—the date and time in this case—and right-click it, and then choose
    **Inspect Element**. This will open a Developer Tools panel showing where this
    piece of data is found in the overall document tree. Little arrows to the left
    of each HTML element indicate if there are child nodes. You can then decide how
    to proceed through locating your target element in the parse tree programmatically,
    and you could make a plan to differentiate it from the other nodes. Since this
    task is well beyond the scope of this introductory book, I will leave that as
    an exercise for the reader.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的想尝试使用BeautifulSoup进行这种提取，我的建议是首先使用浏览器的开发者工具查看页面结构。例如，在Chrome浏览器中，你可以选择你感兴趣的元素——在这个案例中是日期和时间——右键点击它，然后选择**检查元素**。这将打开一个开发者工具面板，显示该数据在整个文档树中的位置。在每个HTML元素左侧的小箭头指示是否存在子节点。然后，你可以决定如何通过程序化地定位目标元素在解析树中的位置，并制定区分它与其他节点的计划。由于这个任务超出了本书的范围，我将留给读者作为练习。
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discovered a few tried-and-true techniques for separating
    interesting data from unwanted data. When we make broth in our chef's kitchen,
    we use a strainer to catch the bones and vegetable husks that we do not want,
    while allowing the delicious liquid that we *do* want to flow through the holes
    in the sieve into our container. The same idea applies when we are extracting
    data from web pages in our data science kitchen. We want to devise a cleaning
    plan that allows us to extract what we want, while leaving the rest of the HTML
    behind.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们发现了几种经过验证的技术，用于将有趣的数据与不需要的数据分离。当我们在厨师的厨房里做高汤时，我们使用筛子来过滤掉我们不需要的骨头和蔬菜残渣，同时让我们*想要*的美味液体通过筛网的孔流入容器中。当我们在数据科学的厨房里从网页中提取数据时，采用的也是同样的思路。我们需要设计一个清理计划，使我们能够提取所需的数据，同时把剩下的HTML丢弃。
- en: 'Along the way, we reviewed the two main mental models used in extracting data
    from HTML, namely a line-by-line delimiter approach and the parse tree/nodes model.
    We then looked into three solid, proven methods to parse HTML pages to extract
    the data we want: regular expressions, BeautifulSoup, and a Chrome-based point-and-click
    Scraper tool. Finally, we put together a project that collected and extracted
    useful data from real-world e-mail and HTML pages.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在过程中，我们回顾了提取HTML数据时使用的两种主要思维模型，即逐行分隔符方法和解析树/节点模型。接着，我们探讨了三种可靠且经过验证的HTML解析方法，用于提取我们想要的数据：正则表达式、BeautifulSoup和基于Chrome的点击式网页抓取工具。最后，我们完成了一个项目，收集并提取了来自现实世界电子邮件和HTML页面的有用数据。
- en: 'Text data such as e-mail and HTML turned out to not be very difficult to clean,
    but what about binary files? In the next chapter, we will explore how to extract
    clean data from a much more difficult target: PDF files.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如电子邮件和HTML之类的文本数据证明并不难清理，但那二进制文件呢？在下一章中，我们将探讨如何从一个更加复杂的目标——PDF文件中提取干净的数据。
