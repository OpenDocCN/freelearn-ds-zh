- en: Chapter 7. Learning Multilayer Perceptron Using Mahout
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章. 使用 Mahout 学习多层感知器
- en: 'To understand a **Multilayer Perceptron** (**MLP**), we will first explore
    one more popular machine learning technique: **neural network**. In this chapter,
    we will explore the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解**多层感知器**（**MLP**），我们首先将探索一种更流行的机器学习技术：**神经网络**。在本章中，我们将探讨以下主题：
- en: Neural network and neurons
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络和神经元
- en: MLP
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP
- en: Using Mahout for MLP implementation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Mahout 进行 MLP 实现
- en: Neural network and neurons
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络和神经元
- en: 'Neural network is an old algorithm, and it was developed with a goal in mind:
    to provide the computer with a brain. Neural network is inspired by the biological
    structure of the human brain where multiple neurons are connected and form columns
    and layers. A **neuron** is an electrically excitable cell that processes and
    transmits information through electrical and chemical signals. Perceptual input
    enters into the neural network through our sensory organs and is then further
    processed into higher levels. Let''s understand how neurons work in our brain.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一个古老的算法，它的开发目标是给计算机提供大脑。神经网络受到人类大脑生物结构的启发，其中多个神经元连接并形成列和层。**神经元**是一个电可兴奋的细胞，通过电和化学信号处理和传输信息。感知输入通过我们的感官器官进入神经网络，然后进一步处理到更高层次。让我们了解神经元在我们大脑中的工作方式。
- en: 'Neurons are computational units in the brain that collect the input from input
    nerves, which are called **dendrites**. They perform computation on these input
    messages and send the output using output nerves, which are called **axons**.
    See the following figure ([http://vv.carleton.ca/~neil/neural/neuron-a.html](http://vv.carleton.ca/~neil/neural/neuron-a.html)):'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是大脑中的计算单元，它们从输入神经（称为**树突**）收集输入。它们对这些输入信息进行计算，并通过输出神经（称为**轴突**）发送输出。请参见以下图示
    ([http://vv.carleton.ca/~neil/neural/neuron-a.html](http://vv.carleton.ca/~neil/neural/neuron-a.html))：
- en: '![Neural network and neurons](img/4959OS_07_01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络和神经元](img/4959OS_07_01.jpg)'
- en: 'On the same lines, we develop a neural network in computers. We can represent
    a neuron in our algorithm as shown in the following figure:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们在计算机中开发神经网络。我们可以在以下图中表示我们的算法中的神经元：
- en: '![Neural network and neurons](img/4959OS_07_02.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络和神经元](img/4959OS_07_02.jpg)'
- en: Here, **x1**, **x2**, and **x3** are the feature vectors, and they are assigned
    to a function **f**, which will do the computation and provide the output. This
    activation function is usually chosen from the family of sigmoidal functions (as
    defined in [Chapter 3](ch03.html "Chapter 3. Learning Logistic Regression / SGD
    Using Mahout"), *Learning Logistic Regression / SGD Using Mahout*). In the case
    of classification problems, softmax activation functions are used. In classification
    problems, we want the output as the probabilities of target classes. So, it is
    desirable for the output to lie between 0 and 1 and the sum close to 1\. Softmax
    function enforces these constraints. It is a generalization of the logistic function.
    More details on softmax function can be found at [http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**x1**、**x2**和**x3**是特征向量，它们被分配给一个函数**f**，该函数将进行计算并提供输出。这个激活函数通常从 S 形函数家族中选择（如第
    3 章中定义，[第 3 章](ch03.html "第 3 章. 使用 Mahout 学习逻辑回归 / SGD")，*使用 Mahout 学习逻辑回归 /
    SGD*）。在分类问题的情况下，使用 softmax 激活函数。在分类问题中，我们希望输出是目标类别的概率。因此，输出应介于 0 和 1 之间，总和接近 1。softmax
    函数强制执行这些约束。它是逻辑函数的推广。有关 softmax 函数的更多详细信息，请参阅 [http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html)。
- en: Multilayer Perceptron
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'A neural network or artificial neural network generally refers to an MLP network.
    We defined neuron as an implementation in computers in the previous section. An
    MLP network consists of multiple layers of these neuron units. Let''s understand
    a perceptron network of three layers, as shown in the next figure. The first layer
    of the MLP represents the input and has no other purpose than routing the input
    to every connected unit in a feed-forward fashion. The second layer is called
    hidden layers, and the last layer serves the special purpose of determining the
    output. The activation of neurons in the hidden layers can be defined as the sum
    of the weight of all the input. Neuron 1 in layer 2 is defined as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络或人工神经网络通常指的是MLP网络。我们在上一节中定义了神经元为计算机中的实现。MLP网络由多个层的这些神经元单元组成。让我们了解一个三层感知器网络，如图所示。MLP的第一层代表输入，除了将输入路由到每个连接单元的前馈方式外，没有其他目的。第二层被称为隐藏层，最后一层具有确定输出的特殊目的。隐藏层中神经元的激活可以定义为所有输入权重的总和。第2层的第1个神经元定义如下：
- en: Y12 = g(w110x0 +w111x1+w112x2+w113x3)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Y12 = g(w110x0 +w111x1+w112x2+w113x3)
- en: 'The first part where *x0 = 0* is called the bias and can be used as an offset,
    independent of the input. Neuron 2 in layer 2 is defined as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分，其中*x0 = 0*，被称为偏置，可以用作偏移，与输入无关。第2层的第2个神经元定义如下：
- en: Y22 = g(w120x0 +w121x1+w122x2+w123x3)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Y22 = g(w120x0 +w121x1+w122x2+w123x3)
- en: '![Multilayer Perceptron](img/4959OS_07_03.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知器](img/4959OS_07_03.jpg)'
- en: 'Neuron 3 in layer 2 is defined as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第2层的第3个神经元定义如下：
- en: Y32 = g (w130x0 +w131x1+w132x2+w133x3)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Y32 = g (w130x0 +w131x1+w132x2+w133x3)
- en: 'Here, g is a sigmoid function, as defined in [Chapter 3](ch03.html "Chapter 3. Learning
    Logistic Regression / SGD Using Mahout"), *Learning Logistic Regression / SGD
    Using Mahout*. The function is as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，g是sigmoid函数，如[第3章](ch03.html "第3章。使用Mahout学习逻辑回归/SGD")中定义的，*使用Mahout学习逻辑回归/SGD*。函数如下：
- en: g(z) = 1/1+e (-z)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: g(z) = 1/1+e (-z)
- en: In this MLP network output, from each input and hidden layers, neuron units
    are distributed to other nodes, and this is why this type of network is called
    a fully connected network. In this network, no values are fed back to the previous
    layer. (Feed forward is another strategy and is also known as back propagation.
    Details on this can be found at [http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html](http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html).)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个MLP网络输出中，从每个输入和隐藏层，神经元单元分布到其他节点，这就是为什么这种网络被称为全连接网络。在这个网络中，没有值被反馈到前一层。（前馈是另一种策略，也称为反向传播。有关详细信息，请参阅[http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html](http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html)。)
- en: An MLP network can have more than one hidden layer. To get the value of the
    weights so that we can get the predicted value as close as possible to the actual
    one is a training process of the MLP. To build an effective network, we consider
    a lot of items such as the number of hidden layers and neuron units in each layer,
    the cost function to minimize the error in predicted and actual values, and so
    on.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: MLP网络可以有多于一个的隐藏层。为了得到权重值，以便我们能够将预测值尽可能接近实际值，这是一个MLP的训练过程。为了构建一个有效的网络，我们考虑了许多项目，例如隐藏层的数量和每层的神经元单元数，最小化预测值和实际值之间误差的成本函数，等等。
- en: 'Now let''s discuss two more important and problematic questions that arise
    when creating an MLP network:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来讨论在创建MLP网络时出现的两个更重要且具有挑战性的问题：
- en: How many hidden layers should one use for the network?
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该使用多少个隐藏层来构建网络？
- en: How many numbers of hidden units (neuron units) should one use in a hidden layer?
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在隐藏层中应该使用多少个隐藏单元（神经元单元）？
- en: 'Zero hidden layers are required to resolve linearly separable data. Assuming
    your data does require separation by a non-linear technique, always start with
    one hidden layer. Almost certainly, that''s all you will need. If your data is
    separable using an MLP, then this MLP probably only needs a single hidden layer.
    In order to select the number of units in different layers, these are the guidelines:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性可分的数据，不需要隐藏层。假设你的数据确实需要通过非线性技术进行分离，始终从一个隐藏层开始。几乎可以肯定，这将是所有你需要的东西。如果你的数据可以使用MLP进行分离，那么这个MLP可能只需要一个隐藏层。为了选择不同层的单元数，以下是一些指导原则：
- en: '**Input layer**: This refers to the number of explanatory variables in the
    model plus one for the bias node.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：这指的是模型中的解释变量数量加上一个偏置节点。'
- en: '**Output layer**: In the case of classification, this refers to the number
    of target variables, and in the case of regression, this is obviously one.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：在分类的情况下，这指的是目标变量的数量，而在回归的情况下，这显然是一个。'
- en: '**Hidden layer**: Start your network with one hidden layer and use the number
    of neuron units equivalent to the units in the input layer. The best way is to
    train several neural networks with different numbers of hidden layers and hidden
    neurons and measure the performance of these networks using cross-validation.
    You can stick with the number that yields the best-performing network. Problems
    that require two hidden layers are rarely encountered. However, neural networks
    that have more than one hidden layer can represent functions with any kind of
    shape. There is currently no theory to justify the use of neural networks with
    more than two hidden layers. In fact, for many practical problems, there is no
    reason to use any more than one hidden layer. A network with no hidden layer is
    only capable of representing linearly separable functions. Networks with one layer
    can approximate any function that contains a continuous mapping from one finite
    space to another, and networks with two hidden layers can represent an arbitrary
    decision boundary to arbitrary accuracy with rational activation functions and
    can approximate any smooth mapping to any accuracy (Chapter 5 of the book *Introduction
    to Neural Networks for Java*).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：从单个隐藏层开始构建你的网络，并使用与输入层单元数量相当的神经元单元数量。最好的方法是训练几个具有不同隐藏层和隐藏神经元数量的神经网络，并使用交叉验证来衡量这些网络的性能。你可以坚持使用产生最佳性能网络的数字。需要两个隐藏层的问题很少遇到。然而，具有多个隐藏层的神经网络可以表示任何形状的函数。目前还没有理论来证明使用超过两个隐藏层的神经网络是合理的。实际上，对于许多实际问题，没有必要使用超过一个隐藏层。没有隐藏层的网络只能表示线性可分函数。单层网络可以逼近任何包含从一个有限空间到另一个连续映射的函数，而具有两个隐藏层的网络可以使用有理激活函数以任意精度表示任意决策边界，并且可以逼近任何平滑映射以任意精度（Java神经网络入门第五章）。'
- en: '**Number of neurons or hidden units**: Use the number of neuron units equivalent
    to the units in the input layer. The number of hidden units should be less than
    twice the number of units in the input layer. Another rule to calculate this is
    *(number of input units + number of output units)* 2/3*.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经元或隐藏单元的数量**：使用与输入层单元数量相当的神经元单元数量。隐藏单元的数量应小于输入层单元数量的两倍。另一种计算方法是 *(输入单元数量
    + 输出单元数量)* 2/3。'
- en: Do the testing for generalization errors, training errors, bias, and variance.
    When a generalization error dips, then just before it begins to increase again,
    the numbers of nodes are usually found to be perfect at this point.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 进行泛化误差、训练误差、偏差和方差的测试。当泛化误差下降时，通常在它开始再次增加之前，会发现节点数量在此点达到完美。
- en: Now let's move on to the next section and explore how we can use Mahout for
    an MLP.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续到下一节，探讨如何使用Mahout进行MLP。
- en: MLP implementation in Mahout
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mahout中的MLP实现
- en: The MLP implementation is based on a more general neural network class. It is
    implemented to run on a single machine using Stochastic Gradient Descent, where
    the weights are updated using one data point at a time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: MLP实现基于一个更通用的神经网络类。它被实现为在单个机器上使用随机梯度下降运行，其中权重是使用每个数据点一次进行更新的。
- en: The number of layers and units per layer can be specified manually and determines
    the whole topology with each unit being fully connected to the previous layer.
    A bias unit is automatically added to the input of every layer. A bias unit is
    helpful for shifting the activation function to the left or right. It is like
    adding a coefficient to the linear function.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 可以手动指定层数和每层的单元数，这决定了整个拓扑结构，每个单元都与前一层的每个单元完全连接。每个层的输入自动添加一个偏差单元。偏差单元有助于将激活函数向左或向右移动。它就像给线性函数添加一个系数一样。
- en: Currently, the logistic sigmoid is used as a squashing function in every hidden
    and output layer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，逻辑Sigmoid函数被用作每个隐藏层和输出层的压缩函数。
- en: The command-line version does not perform iterations that lead to bad results
    on small datasets. Another restriction is that the CLI version of the MLP only
    supports classification, since the labels have to be given explicitly when executing
    the implementation in the command line.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 命令行版本不会在小数据集上导致不良结果的迭代。另一个限制是，MLP的CLI版本仅支持分类，因为当在命令行中执行实现时，必须明确给出标签。
- en: A learned model can be stored and updated with new training instances using
    the `` `--update` `` flag. The output of the classification result is saved as
    a `.txt` file and only consists of the assigned labels. Apart from the command-line
    interface, it is possible to construct and compile more specialized neural networks
    using the API and interfaces in the `mrlegacy` package. (The core package is renamed
    as `mrlegacy`.)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `--update` 标志存储和更新学习模型，并使用新的训练实例。分类结果的输出保存为 `.txt` 文件，并且仅包含分配的标签。除了命令行界面外，还可以使用
    `mrlegacy` 包中的 API 和接口构建和编译更专业的神经网络。（核心包已重命名为 `mrlegacy`。）
- en: 'In the command line, we use `TrainMultilayerPerceptron` and `RunMultilayerPerceptron`
    classes that are available in the `mrlegacy` package with three other classes:
    Neural `network.java`, `NeuralNetworkFunctions.java`, and `MultilayerPerceptron.java`.
    For this particular implementation, users can freely control the topology of the
    MLP, including the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令行中，我们使用 `TrainMultilayerPerceptron` 和 `RunMultilayerPerceptron` 类，这些类在 `mrlegacy`
    包中可用，与另外三个类一起使用：`Neural network.java`、`NeuralNetworkFunctions.java` 和 `MultilayerPerceptron.java`。对于这个特定的实现，用户可以自由控制
    MLP 的拓扑结构，包括以下内容：
- en: The size of the input layer
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层的大小
- en: The number of hidden layers
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的数量
- en: The size of each hidden layer
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层的大小
- en: The size of the output layer
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层的大小
- en: The cost function
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本函数
- en: The squashing function
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挤压函数
- en: The model is trained in an online learning approach, where the weights of neurons
    in the MLP is updated and incremented using the backPropagation algorithm proposed
    by *Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986), Learning representations
    by back-propagating errors. Nature, 323, 533-536*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 模型以在线学习的方式进行训练，其中 MLP 中的神经元权重使用 Rumelhart, D. E.、Hinton, G. E. 和 Williams, R.
    J.（1986）提出的反向传播算法进行更新和增加。（*Rumelhart, D. E., Hinton, G. E., and Williams, R. J.
    (1986), Learning representations by back-propagating errors. Nature, 323, 533-536*。）
- en: Using Mahout for MLP
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Mahout 进行 MLP
- en: Mahout has implementation for an MLP network. The MLP implementation is currently
    located in the `Map-Reduce-Legacy` package. As with other classification algorithms,
    two separated classes are implemented to train and use this classifier. For training
    the classifier, the `org.apache.mahout.classifier.mlp.TrainMultilayerPerceptron`
    class, and for running the classifier, the `org.apache.mahout.classifier.mlp.RunMultilayerPerceptron`
    class is used. There are a number of parameters defined that are used with these
    classes, but we will discuss these parameters once we run our example on a dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Mahout 为 MLP 网络提供了实现。MLP 实现目前位于 `Map-Reduce-Legacy` 包中。与其他分类算法一样，实现了两个分离的类来训练和使用这个分类器。用于训练分类器的类是
    `org.apache.mahout.classifier.mlp.TrainMultilayerPerceptron`，用于运行分类器的类是 `org.apache.mahout.classifier.mlp.RunMultilayerPerceptron`。定义了多个参数，这些参数与这些类一起使用，但我们将在我们对数据集运行示例后讨论这些参数。
- en: '**Dataset**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集**'
- en: 'In this chapter, we will train an MLP to classify the iris dataset. The iris
    flower dataset contains data of three flower species, where each data point consists
    of four features. This dataset was introduced by Sir Ronald Fisher. It consists
    of 50 samples from each of three species of iris. These species are Iris setosa,
    Iris virginica, and Iris versicolor. Four features were measured from each sample:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将训练一个 MLP 来对鸢尾花数据集进行分类。鸢尾花数据集包含三种花卉物种的数据，每个数据点由四个特征组成。这个数据集是由罗纳德·费舍尔爵士引入的。它包括来自三种鸢尾花物种的
    50 个样本。这些物种是 Iris setosa、Iris virginica 和 Iris versicolor。从每个样本测量了四个特征：
- en: Sepal length
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片长度
- en: Sepal width
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣宽度
- en: Petal length
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣长度
- en: Petal width
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣宽度
- en: 'All measurements are in centimeters. You can download this dataset from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)
    and save it as a `.csv` file, as shown in the following screenshot:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所有测量值都以厘米为单位。您可以从 [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)
    下载此数据集，并将其保存为 `.csv` 文件，如下截图所示：
- en: '![Using Mahout for MLP](img/4959OS_07_04.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Mahout 进行 MLP](img/4959OS_07_04.jpg)'
- en: 'This dataset will look like the the following screenshot:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集看起来如下截图所示：
- en: '![Using Mahout for MLP](img/4959OS_07_05.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Mahout 进行 MLP](img/4959OS_07_05.jpg)'
- en: Steps to use the MLP algorithm in Mahout
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Mahout 中使用 MLP 算法的步骤
- en: 'The steps to use the MLP algorithm in Mahout are as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Mahout 中使用 MLP 算法的步骤如下：
- en: Create the MLP model.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 MLP 模型。
- en: 'To create the MLP model, we will use the `TrainMultilayerPerceptron` class.
    Use the following command to generate the model:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要创建 MLP 模型，我们将使用 `TrainMultilayerPerceptron` 类。使用以下命令生成模型：
- en: '[PRE0]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can also run using the core jar: Mahout core jar (`xyz` stands for the
    version). If you have directly installed Mahout, it can be found under the `/usr/lib/mahout`
    folder. Execute the following command:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您也可以使用核心jar运行：Mahout 核心jar（`xyz` 代表版本）。如果您直接安装了 Mahout，它可以在 `/usr/lib/mahout`
    文件夹下找到。执行以下命令：
- en: '[PRE1]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `TrainMultilayerPerceptron` class is used here and it takes different parameters.
    Also, `i` is the path for the input dataset. Here, we have put the dataset under
    the `/tmp` folder (local filesystem). Additionally, labels are defined in the
    dataset. Here we have the following labels:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里使用 `TrainMultilayerPerceptron` 类，它接受不同的参数。此外，`i` 是输入数据集的路径。在这里，我们将数据集放在了 `/tmp`
    文件夹下（本地文件系统）。此外，数据集中还定义了标签。以下是我们定义的标签：
- en: '`mo` is the output location for the created model.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mo` 是创建的模型的输出位置。'
- en: '`ls` is the number of units per layer, including input, hidden, and output
    layers. This parameter specifies the topology of the network. Here, we have `4`
    as the input feature, `8` for the hidden layer, and `3` for the output class number.'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ls` 是每层的单元数，包括输入层、隐藏层和输出层。此参数指定了网络的拓扑结构。在这里，我们使用 `4` 作为输入特征，`8` 作为隐藏层，`3`
    作为输出类别数。'
- en: '`l` is the learning rate that is used for weight updates. The default is 0.5\.
    To approximate gradient descent, neural networks are trained with algorithms.
    Learning is possible either by batch or online methods. In batch training, weight
    changes are accumulated over an entire presentation of the training data (an epoch)
    before being applied, while online training updates weighs after the presentation
    of each training example (instance). More details can be found at [http://axon.cs.byu.edu/papers/Wilson.nn03.batch.pdf](http://axon.cs.byu.edu/papers/Wilson.nn03.batch.pdf).'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l` 是用于权重更新的学习率。默认值为 0.5。为了近似梯度下降，神经网络通过算法进行训练。学习可以通过批量或在线方法进行。在批量训练中，权重变化在整个训练数据展示（一个epoch）之后才会应用，而在在线训练中，在每个训练示例（实例）展示后更新权重。更多详情请参阅
    [http://axon.cs.byu.edu/papers/Wilson.nn03.batch.pdf](http://axon.cs.byu.edu/papers/Wilson.nn03.batch.pdf)。'
- en: '`m` is the momentum weight that is used for gradient descent. This must be
    in the range between 0–1.0.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`m` 是用于梯度下降的动量权重。这个值必须在 0–1.0 的范围内。'
- en: '`r` is the regularization value for the weight vector. This must be in the
    range between 0–0.1\. It is used to prevent overfitting.'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r` 是权重向量的正则化值。这个值必须在 0–0.1 的范围内。它用于防止过拟合。'
- en: '![Steps to use the MLP algorithm in Mahout](img/4959OS_07_06.jpg)'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用 Mahout 中的 MLP 算法的步骤](img/4959OS_07_06.jpg)'
- en: 'To test/run the MLP classification of the trained model, we can use the following
    command:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要测试/运行训练模型的 MLP 分类，我们可以使用以下命令：
- en: '[PRE2]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can also run using the Mahout core jar (`xyz` stands for version). If you
    have directly installed Mahout, it can be found under the `/usr/lib/mahout` folder.
    Execute the following command:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您也可以使用 Mahout 核心jar运行（`xyz` 代表版本）。如果您直接安装了 Mahout，它可以在 `/usr/lib/mahout` 文件夹下找到。执行以下命令：
- en: '[PRE3]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `RunMultilayerPerceptron` class is employed here to use the model. This
    class also takes different parameters, which are as follows:'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里使用 `RunMultilayerPerceptron` 类来使用模型。此类也接受不同的参数，如下所示：
- en: '`i` indicates the input dataset location'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`i` 表示输入数据集的位置'
- en: '`cr` is the range of columns to use from the input file, starting with 0 (that
    is, `` `-cr 0 5` `` for including the first six columns only)'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cr` 是从输入文件中使用的列的范围，从 0 开始（即 `-cr 0 5` 仅包括前六列）'
- en: '`mo` is the location of the model built earlier'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mo` 是先前构建的模型的位置'
- en: '`o` is the path to store labeled results from running the model'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`o` 是存储模型运行结果的标签化结果的路径'
- en: '![Steps to use the MLP algorithm in Mahout](img/4959OS_07_07.jpg)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用 Mahout 中的 MLP 算法的步骤](img/4959OS_07_07.jpg)'
- en: Summary
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we discussed one of the newly implemented algorithms in Mahout:
    MLP. We started our discussion by understanding neural networks and neuron units
    and continued our discussion further to understand the MLP network algorithm.
    We discussed how to choose different layer units. We then moved to Mahout and
    used the iris dataset to test and run an MLP algorithm implemented in Mahout.
    With this, we have finished our discussion on classification algorithms available
    in Apache Mahout.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了 Mahout 中新实施的一个算法：MLP。我们通过理解神经网络和神经元单元开始了我们的讨论，并进一步讨论以理解 MLP 网络算法。我们讨论了如何选择不同的层单元。然后我们转向
    Mahout，并使用 iris 数据集来测试和运行 Mahout 中实现的 MLP 算法。至此，我们已经完成了对 Apache Mahout 中可用的分类算法的讨论。
- en: Now we move on to the next chapter of this book where we will discuss the new
    changes coming up in the new Mahout release.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续进入这本书的下一章节，我们将讨论新发布的 Mahout 版本中的新变化。
