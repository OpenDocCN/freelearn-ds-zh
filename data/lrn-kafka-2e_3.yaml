- en: Chapter 3. Kafka Design
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。Kafka设计
- en: Before we start getting our hands dirty by coding Kafka producers and consumers,
    let's quickly discuss the internal design of Kafka.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始编写Kafka生产者和消费者的代码之前，让我们快速讨论一下Kafka的内部设计。
- en: 'In this chapter, we shall be focusing on the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注以下主题：
- en: Kafka design fundamentals
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka设计基础
- en: Message compression in Kafka
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka中的消息压缩
- en: Replication in Kafka
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka中的复制
- en: Due to the overheads associated with JMS and its various implementations and
    limitations with the scaling architecture, LinkedIn ([www.linkedin.com](http://www.linkedin.com))
    decided to build Kafka to address its need for monitoring activity stream data
    and operational metrics such as CPU, I/O usage, and request timings.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 由于与JMS及其各种实现相关的开销以及缩放架构的限制，LinkedIn（[www.linkedin.com](http://www.linkedin.com)）决定构建Kafka来满足其对监控活动流数据和操作指标（如CPU、I/O使用情况和请求时间）的需求。
- en: 'While developing Kafka, the main focus was to provide the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发Kafka时，主要关注以下内容：
- en: An API for producers and consumers to support custom implementation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为生产者和消费者提供支持自定义实现的API
- en: Low overheads for network and storage with message persistence on disk
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络和存储的低开销，消息在磁盘上持久化
- en: A high throughput supporting millions of messages for both publishing and subscribing—for
    example, real-time log aggregation or data feeds
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持发布和订阅数百万条消息的高吞吐量，例如实时日志聚合或数据源
- en: Distributed and highly scalable architecture to handle low-latency delivery
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式和高度可扩展的架构，以处理低延迟传递
- en: Auto-balancing multiple consumers in the case of failure
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在故障情况下自动平衡多个消费者
- en: Guaranteed fault-tolerance in the case of server failures
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在服务器故障的情况下保证容错
- en: Kafka design fundamentals
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka设计基础
- en: 'Kafka is neither a queuing platform where messages are received by a single
    consumer out of the consumer pool, nor a publisher-subscriber platform where messages
    are published to all the consumers. In a very basic structure, a producer publishes
    messages to a Kafka topic (synonymous with "messaging queue"). A topic is also
    considered as a message category or feed name to which messages are published.
    Kafka topics are created on a Kafka broker acting as a Kafka server. Kafka brokers
    also store the messages if required. Consumers then subscribe to the Kafka topic
    (one or more) to get the messages. Here, brokers and consumers use Zookeeper to
    get the state information and to track message offsets, respectively. This is
    described in the following diagram:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka既不是一个消息队列平台，消息在其中由消费者池中的单个消费者接收，也不是一个发布-订阅平台，消息在其中发布给所有消费者。在一个非常基本的结构中，生产者将消息发布到Kafka主题（与“消息队列”同义）。主题也被视为消息类别或订阅名称，消息被发布到其中。Kafka主题在充当Kafka服务器的Kafka代理上创建。Kafka代理还在需要时存储消息。然后消费者订阅Kafka主题（一个或多个）以获取消息。在这里，代理和消费者使用Zookeeper获取状态信息和分别跟踪消息偏移量。这在下图中描述：
- en: '![Kafka design fundamentals](img/3090OS_03_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Kafka设计基础](img/3090OS_03_01.jpg)'
- en: 'In the preceding diagram, a single node—single broker architecture is shown
    with a topic having four partitions. In terms of the components, the preceding
    diagram shows all the five components of the Kafka cluster: Zookeeper, Broker,
    Topic, Producer, and Consumer.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，显示了单节点-单代理架构，其中一个主题有四个分区。在组件方面，上图显示了Kafka集群的所有五个组件：Zookeeper、代理、主题、生产者和消费者。
- en: In Kafka topics, every partition is mapped to a logical log file that is represented
    as a set of segment files of equal sizes. Every partition is an ordered, immutable
    sequence of messages; each time a message is published to a partition, the broker
    appends the message to the last segment file. These segment files are flushed
    to disk after configurable numbers of messages have been published or after a
    certain amount of time has elapsed. Once the segment file is flushed, messages
    are made available to the consumers for consumption.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kafka主题中，每个分区都映射到一个逻辑日志文件，表示为相等大小的一组段文件。每个分区都是有序的、不可变的消息序列；每次将消息发布到分区时，代理将消息附加到最后一个段文件。这些段文件在发布可配置数量的消息后或经过一定时间后被刷新到磁盘。刷新段文件后，消息就可以供消费者消费了。
- en: All the message partitions are assigned a unique sequential number called the
    *offset*, which is used to identify each message within the partition. Each partition
    is optionally replicated across a configurable number of servers for fault tolerance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所有消息分区都被分配一个称为*偏移量*的唯一顺序号，用于标识分区内的每条消息。每个分区可以选择在可配置数量的服务器上进行复制，以实现容错。
- en: Each partition available on either of the servers acts as the *leader* and has
    zero or more servers acting as *followers*. Here the leader is responsible for
    handling all read and write requests for the partition while the followers asynchronously
    replicate data from the leader. Kafka dynamically maintains a set of **in-sync
    replicas** (**ISR**) that are caught-up to the leader and always persist the latest
    ISR set to ZooKeeper. In if the leader fails, one of the followers (in-sync replicas)
    will automatically become the new leader. In a Kafka cluster, each server plays
    a dual role; it acts as a leader for some of its partitions and also a follower
    for other partitions. This ensures the load balance within the Kafka cluster.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务器上的每个分区都充当*领导者*，并且有零个或多个服务器充当*跟随者*。在这里，领导者负责处理分区的所有读写请求，而跟随者异步地从领导者复制数据。Kafka动态地维护一组**同步副本**（**ISR**），这些副本已经追赶上了领导者，并且始终将最新的ISR集合持久化到ZooKeeper。如果领导者失败，其中一个跟随者（同步副本）将自动成为新的领导者。在Kafka集群中，每个服务器都扮演双重角色；它既是一些分区的领导者，也是其他分区的跟随者。这确保了Kafka集群内的负载平衡。
- en: The Kafka platform is built based on what has been learned from both the traditional
    platforms and has the concept of consumer groups. Here, each consumer is represented
    as a process and these processes are organized within groups called **consumer
    groups**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka平台是基于从传统平台学到的东西构建的，并且具有消费者组的概念。在这里，每个消费者都表示为一个进程，并且这些进程组织在称为**消费者组**的组中。
- en: A message within a topic is consumed by a single process (consumer) within the
    consumer group and, if the requirement is such that a single message is to be
    consumed by multiple consumers, all these consumers need to be kept in different
    consumer groups. Consumers always consume messages from a particular partition
    sequentially and also acknowledge the message offset. This acknowledgement implies
    that the consumer has consumed all prior messages. Consumers issue an asynchronous
    pull request containing the offset of the message to be consumed to the broker
    and get the buffer of bytes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 主题中的消息由消费者组内的单个进程（消费者）消费，如果要求是单个消息由多个消费者消费，则所有这些消费者都需要保持在不同的消费者组中。消费者始终按顺序从特定分区消费消息，并确认消息偏移量。这种确认意味着消费者已经消费了所有先前的消息。消费者向代理发出包含要消费的消息偏移量的异步拉取请求，并获取字节缓冲区。
- en: In line with Kafka's design, brokers are stateless, which means the message
    state of any consumed message is maintained within the message consumer, and the
    Kafka broker does not maintain a record of what is consumed by whom. If this is
    poorly implemented, the consumer ends up in reading the same message multiple
    times. If the message is deleted from the broker (as the broker doesn't know whether
    the message is consumed or not), Kafka defines the time-based SLA (service level
    agreement) as a message retention policy. In line with this policy, a message
    will be automatically deleted if it has been retained in the broker longer than
    the defined SLA period. This message retention policy empowers consumers to deliberately
    rewind to an old offset and re-consume data although, as with traditional messaging
    systems, this is a violation of the queuing contract with consumers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与Kafka的设计一致，代理是无状态的，这意味着任何消费的消息的状态都在消息消费者内部维护，Kafka代理不会记录谁消费了什么。如果实现不好，消费者最终会多次读取相同的消息。如果消息从代理中被删除（因为代理不知道消息是否被消费），Kafka定义了基于时间的SLA（服务级别协议）作为消息保留策略。根据这个策略，如果消息在代理中保留的时间超过了定义的SLA期限，消息将被自动删除。这个消息保留策略使消费者能够有意地倒带到旧的偏移量并重新消费数据，尽管与传统的消息系统一样，这违反了与消费者的排队合同。
- en: 'Let''s discuss the message delivery semantic Kafka provides between producer
    and consumer. There are multiple possible ways to deliver messages, such as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论Kafka在生产者和消费者之间提供的消息传递语义。有多种可能的消息传递方式，例如：
- en: Messages are never redelivered but may be lost
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息永远不会被重新传递，但可能会丢失
- en: Messages may be redelivered but never lost
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息可能会被重新传递，但永远不会丢失
- en: Messages are delivered once and only once
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息只会被传递一次
- en: When publishing, a message is committed to the log. If a producer experiences
    a network error while publishing, it can never be sure if this error happened
    before or after the message was committed. Once committed, the message will not
    be lost as long as either of the brokers that replicate the partition to which
    this message was written remains available. For guaranteed message publishing,
    configurations such as getting acknowledgements and the waiting time for messages
    being committed are provided at the producer's end.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在发布时，消息被提交到日志中。如果生产者在发布时遇到网络错误，它永远无法确定此错误是在消息提交之前还是之后发生的。一旦提交，只要复制写入该消息的分区的任何代理仍然可用，消息就不会丢失。对于保证消息发布，生产者端提供了配置，如获取确认和等待消息提交的时间。
- en: From the consumer point-of-view, replicas have exactly the same log with the
    same offsets, and the consumer controls its position in this log. For consumers,
    Kafka guarantees that the message will be delivered at least once by reading the
    messages, processing the messages, and finally saving their position. If the consumer
    process crashes after processing messages but before saving their position, another
    consumer process takes over the topic partition and may receive the first few
    messages, which are already processed.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从消费者的角度来看，副本具有完全相同的日志和相同的偏移量，消费者控制其在此日志中的位置。对于消费者，Kafka保证消息将至少被读取一次，通过读取消息、处理消息，最后保存它们的位置。如果消费者进程在处理消息后崩溃，但在保存它们的位置之前崩溃，另一个消费者进程将接管主题分区，并可能接收已经处理的前几条消息。
- en: Log compaction
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志压缩
- en: Log compaction is a mechanism to achieve finer-grained, per-message retention,
    rather than coarser-grained, time-based retention. It ensures that the last known
    value for each message key within the log for a topic partition must be retained
    by removing the records where a more recent update with the same primary key is
    done. Log compaction also addresses system failure cases or system restarts, and
    so on.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 日志压缩是一种实现更精细的、基于每条消息的保留，而不是粗粒度的、基于时间的保留的机制。它确保了主题分区日志中每条消息键的最后已知值必须保留，方法是删除具有相同主键的更近更新的记录。日志压缩还解决了系统故障情况或系统重新启动等问题。
- en: 'In the Kafka cluster, the retention policy can be set on a per-topic basis
    such as time based, size-based, or log compaction-based. Log compaction ensures
    the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kafka集群中，保留策略可以根据每个主题进行设置，例如基于时间、基于大小或基于日志压缩。日志压缩确保以下内容：
- en: Ordering of messages is always maintained
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息的顺序始终保持不变
- en: The messages will have sequential offsets and the offset never changes
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息将具有顺序偏移量，偏移量永远不会改变
- en: Reads progressing from offset 0, or the consumer progressing from the start
    of the log, will see at least the final state of all records in the order they
    were written
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从偏移量0开始读取，或者从日志开头开始的消费者，将至少看到按写入顺序的所有记录的最终状态
- en: Log compaction is handled by a pool of background threads that recopy log segment
    files, removing records whose key appears in the head of the log.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 日志压缩由一组后台线程处理，它们重新复制日志段文件，删除出现在日志头部的键的记录。
- en: 'The following points summarize important Kafka design facts:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下要点总结了重要的Kafka设计事实：
- en: The fundamental backbone of Kafka is message caching and storing on the fiesystem.
    In Kafka, data is immediately written to the OS kernel page. Caching and flushing
    of data to the disk are configurable.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka的基本支柱是消息缓存和存储在文件系统中。在Kafka中，数据立即写入操作系统内核页面。数据缓存和刷新到磁盘是可配置的。
- en: Kafka provides longer retention of messages even after consumption, allowing
    consumers to re-consume, if required.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka提供了更长时间的消息保留，即使在消费后，也允许消费者重新消费。
- en: Kafka uses a message set to group messages to allow lesser network overhead.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka使用消息集来组合消息，以减少网络开销。
- en: 'Unlike most messaging systems, where metadata of the consumed messages are
    kept at the server level, in Kafka the state of the consumed messages is maintained
    at the consumer level. This also addresses issues such as:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与大多数消息系统不同，在Kafka中，消费消息的状态是在消费者级别维护的，而不是在服务器级别维护的。这也解决了诸如：
- en: Losing messages due to failure
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于故障而丢失消息
- en: Multiple deliveries of the same message
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同一消息的多次传递
- en: By default, consumers store the state in Zookeeper but Kafka also allows storing
    it within other storage systems used for **Online Transaction Processing** (**OLTP**)
    applications as well.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，消费者将状态存储在Zookeeper中，但Kafka也允许将其存储在用于在线事务处理（OLTP）应用程序的其他存储系统中。
- en: In Kafka, producers and consumers work on the traditional push-and-pull model,
    where producers push the message to a Kafka broker and consumers pull the message
    from the broker.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kafka中，生产者和消费者采用传统的推送和拉取模型工作，其中生产者将消息推送到Kafka代理，消费者从代理拉取消息。
- en: Kafka does not have any concept of a master and treats all the brokers as peers.
    This approach facilitates addition and removal of a Kafka broker at any point,
    as the metadata of brokers are maintained in Zookeeper and shared with consumers.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka没有任何主节点的概念，并将所有代理视为对等体。这种方法使得可以在任何时候添加和删除Kafka代理，因为代理的元数据在Zookeeper中进行维护并与消费者共享。
- en: Producers also have an option to choose between asynchronous or synchronous
    mode to send messages to a broker.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产者还可以选择异步或同步模式将消息发送到代理。
- en: Message compression in Kafka
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka中的消息压缩
- en: For the cases where network bandwidth is a bottleneck, Kafka provides a message
    group compression feature for efficient message delivery. Kafka supports efficient
    compression by allowing recursive message sets where the compressed message may
    have infinite depth relative to messages within itself. Efficient compression
    requires compressing multiple messages together rather than compressing each message
    individually. A batch of messages is compressed together and sent to the broker.
    There is a reduced network overhead for the compressed message set and decompression
    also attracts very little additional overhead.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网络带宽成为瓶颈的情况，Kafka提供了消息组压缩功能，以实现高效的消息传递。Kafka通过允许递归消息集来支持高效的压缩，其中压缩消息可能相对于其中的消息具有无限深度。高效的压缩需要将多个消息一起压缩并发送到代理。压缩消息集的网络开销减少，解压缩也吸引了非常少的额外开销。
- en: In an earlier version of Kafka, 0.7, compressed batches of messages remained
    compressed in the log files and were presented as a single message to the consumer
    who later decompressed it. Hence, the additional overhead of decompression was
    present only at the consumer's end.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kafka的早期版本0.7中，消息的压缩批次在日志文件中保持压缩状态，并且作为单个消息呈现给稍后对其进行解压缩的消费者。因此，解压缩的额外开销仅存在于消费者端。
- en: In Kafka 0.8, changes were made to the broker in the way it handles message
    offsets; this may also cause a degradation in broker performance in the case of
    compressed messages.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kafka 0.8中，对代理处理消息偏移量的方式进行了更改；这也可能导致在压缩消息的情况下降低代理性能。
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In Kafka 0.7, messages were addressable by physical byte offsets in the partition's
    log whereas in Kafka 0.8 each message is addressable by a non-comparable, increasingly
    logical offset that is unique per partition—that is, the first message has an
    offset of `1`, the tenth message has an offset of `10`, and so on. In Kafka 0.8,
    changes to offset management simplify the consumer capability to rewind the message
    offset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kafka 0.7中，消息是通过分区日志中的物理字节偏移量进行寻址的，而在Kafka 0.8中，每条消息都是通过一个不可比较的、逻辑偏移量进行寻址的，这个偏移量对每个分区是唯一的——也就是说，第一条消息的偏移量为`1`，第十条消息的偏移量为`10`，依此类推。在Kafka
    0.8中，对偏移量管理的更改简化了消费者重置消息偏移量的能力。
- en: In Kafka 0.8, the lead broker is responsible for serving the messages for a
    partition by assigning unique logical offsets to every message before it is appended
    to the logs. In the case of compressed data, the lead broker has to decompress
    the message set in order to assign offsets to the messages inside the compressed
    message set. Once offsets are assigned, the leader again compresses the data and
    then appends it to the disk. The lead broker follows this process for every compressed
    message sets it receives, which causes CPU load on a Kafka broker.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kafka 0.8中，领导代理负责通过为每条消息分配唯一的逻辑偏移量来为分区提供消息，然后将其附加到日志中。在压缩数据的情况下，领导代理必须解压消息集以便为压缩消息集中的消息分配偏移量。一旦偏移量被分配，领导者再次压缩数据，然后将其附加到磁盘上。领导代理对其接收到的每个压缩消息集都遵循此过程，这会导致Kafka代理的CPU负载。
- en: In Kafka, data is compressed by the message producer using either the **GZIP**
    or **Snappy** compression protocols. The following producer configurations need
    to be provided to use compression at the producer's end.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kafka中，数据由消息生产者使用GZIP或Snappy压缩协议进行压缩。需要提供以下生产者配置以在生产者端使用压缩。
- en: '| Property name | Description | Default value |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 属性名称 | 描述 | 默认值 |'
- en: '| --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `compression.codec` | This parameter specifies the compression codec for
    all data generated by this producer. Valid values are `none`, `gzip`, and `snappy`.
    | `none` |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `compression.codec` | 此参数指定此生产者生成的所有数据的压缩编解码器。有效值为`none`、`gzip`和`snappy`。
    | `none` |'
- en: '| `compressed.topics` | This parameter allows you to set whether compression
    should be turned on for particular topics. If the compression codec is anything
    other than `none`, enable compression only for specified topics, if any. If the
    list of compressed topics is empty, then enable the specified compression codec
    for all topics. If the compression codec is `none`, compression is disabled for
    all topics. | `null` |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `compressed.topics` | 此参数允许您设置是否应为特定主题打开压缩。如果压缩编解码器不是`none`，则仅为指定的主题启用压缩。如果压缩主题列表为空，则为所有主题启用指定的压缩编解码器。如果压缩编解码器是`none`，则为所有主题禁用压缩。
    | `null` |'
- en: The `ByteBufferMessageSet` class representing message sets may consist of both
    uncompressed as well as compressed data. To differentiate between compressed and
    uncompressed messages, a compression-attributes byte is introduced in the message
    header. Within this compression byte, the lowest two bits are used to represent
    the compression codec used for compression and the value 0 of these last two bits
    represents an uncompressed message.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 代表消息集的`ByteBufferMessageSet`类可能包含未压缩和压缩数据。为了区分压缩和未压缩的消息，在消息头中引入了一个压缩属性字节。在这个压缩字节中，最低的两位用于表示用于压缩的压缩编解码器，这两位的值为0表示未压缩的消息。
- en: Message compression techniques are very useful for mirroring data across datacenters
    using Kafka, where large amounts of data get transferred from active to passive
    datacenters in the compressed format.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kafka在数据中心之间镜像数据时，消息压缩技术非常有用，其中大量数据以压缩格式从活动数据中心传输到被动数据中心。
- en: Replication in Kafka
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka中的复制
- en: Before we talk about replication in Kafka, let's talk about message partitioning.
    In Kafka, a message partitioning strategy is used at the Kafka broker end. The
    decision about how the message is partitioned is taken by the producer, and the
    broker stores the messages in the same order as they arrive. The number of partitions
    can be configured for each topic within the Kafka broker.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论Kafka中的复制之前，让我们先谈谈消息分区。在Kafka中，消息分区策略是在Kafka代理端使用的。关于消息如何分区的决定由生产者做出，代理存储消息的顺序与它们到达的顺序相同。可以为Kafka代理中的每个主题配置分区数。
- en: 'Kafka replication is one of the very important features introduced in Kafka
    0.8\. Though Kafka is highly scalable, for better durability of messages and high
    availability of Kafka clusters, replication guarantees that the message will be
    published and consumed even in the case of broker failure, which may be caused
    by any reason. Both producers and consumers are replication-aware in Kafka. The
    following diagram explains replication in Kafka:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka复制是Kafka 0.8中引入的非常重要的功能之一。尽管Kafka具有高度可扩展性，但为了更好地保证消息的耐久性和Kafka集群的高可用性，复制保证了即使在经纪人故障的情况下（可能由任何原因引起），消息也将被发布和消费。Kafka中的生产者和消费者都具有复制意识。以下图解释了Kafka中的复制：
- en: '![Replication in Kafka](img/3090OS_03_02.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![Kafka中的复制](img/3090OS_03_02.jpg)'
- en: Let's discuss the preceding diagram in detail.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论前面的图表。
- en: In replication, each partition of a message has *n* replicas and can afford
    *n-1* failures to guarantee message delivery. Out of the *n* replicas, one replica
    acts as the lead replica for the rest of the replicas. Zookeeper keeps the information
    about the lead replica and the current follower **in-sync replicas** (**ISR**).
    The lead replica maintains the list of all in-sync follower replicas.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在复制中，消息的每个分区都有*n*个副本，并且可以承受*n-1*个故障以保证消息传递。在*n*个副本中，一个副本充当其余副本的领导副本。Zookeeper保存有关领导副本和当前跟随者**同步副本**（**ISR**）的信息。领导副本维护所有同步跟随者副本的列表。
- en: Each replica stores its part of the message in local logs and offsets, and is
    periodically synced to the disk. This process also ensures that either a message
    is written to all the replicas or to none of them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 每个副本将其消息的一部分存储在本地日志和偏移量中，并定期同步到磁盘。这个过程还确保消息要么被写入所有副本，要么一个也没有被写入。
- en: 'Kafka supports the following replication modes:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka支持以下复制模式：
- en: '**Synchronous replication**: In synchronous replication, a producer first identifies
    the lead replica from ZooKeeper and publishes the message. As soon as the message
    is published, it is written to the log of the lead replica and all the followers
    of the lead start pulling the message; by using a single channel, the order of
    messages is ensured. Each follower replica sends an acknowledgement to the lead
    replica once the message is written to its respective logs. Once replications
    are complete and all expected acknowledgements are received, the lead replica
    sends an acknowledgement to the producer. On the consumer''s side, all the pulling
    of messages is done from the lead replica.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步复制**：在同步复制中，生产者首先从ZooKeeper中识别领导副本并发布消息。一旦消息发布，它就会被写入领导副本的日志，并且所有领导的跟随者开始拉取消息；通过使用单个通道，确保消息的顺序。每个跟随者副本在将消息写入其各自的日志后向领导副本发送确认。一旦复制完成并收到所有预期的确认，领导副本会向生产者发送确认。在消费者方面，所有消息的拉取都是从领导副本进行的。'
- en: '**Asynchronous replication**: The only difference in this mode is that, as
    soon as a lead replica writes the message to its local log, it sends the acknowledgement
    to the message client and does not wait for acknowledgements from follower replicas.
    But, as a downside, this mode does not ensure message delivery in case of a broker
    failure.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步复制**：这种模式的唯一区别是，一旦领导副本将消息写入其本地日志，它会向消息客户端发送确认，而不会等待来自跟随者副本的确认。但是，作为缺点，这种模式在经纪人故障的情况下不能确保消息传递。'
- en: If any of the follower in-sync replicas fail, the leader drops the failed follower
    from its ISR list after the configured timeout period and writes will continue
    on the remaining replicas in ISRs. Whenever the failed follower comes back, it
    first truncates its log to the last checkpoint (the offset of the last committed
    message) and then starts to catch up with all messages from the leader, starting
    from the checkpoint. As soon as the follower becomes fully synced with the leader,
    the leader adds it back to the current ISR list.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何追随者的同步副本失败，领导者将在配置的超时期后从其ISR列表中删除失败的追随者，并且写操作将继续在剩余的ISR中进行。每当失败的追随者回来时，它首先将其日志截断到最后一个检查点（最后提交消息的偏移量），然后开始从领导者那里赶上所有消息，从检查点开始。一旦追随者与领导者完全同步，领导者将其重新添加到当前的ISR列表中。
- en: If the lead replica fails, either while writing the message partition to its
    local log or before sending the acknowledgement to the message producer, a message
    partition is resent by the producer to the new lead broker.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果领导复制品在将消息分区写入其本地日志之前或在向消息生产者发送确认之前失败，消息分区将由生产者重新发送到新的领导代理。
- en: The process of choosing the new lead replica involves all the followers' ISRs
    registering themselves with Zookeeper. The very first registered replica becomes
    the new lead replica and its **log end offset** (**LEO**) becomes the offset of
    the last committed message (also known as **high watermark** (**HW**)). The rest
    of the registered replicas become the followers of the newly elected leader. Each
    replica registers a listener in Zookeeper so that it will be informed of any leader
    change. Whenever the new leader is elected and the notified replica is not the
    leader, it truncates its log to the offset of the last committed message and then
    starts to catch up from the new leader. The new elected leader waits either until
    the time configured is passed or until all live replicas get in sync and then
    the leader writes the current ISR to Zookeeper and opens itself up for both message
    reads and writes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 选择新的领导复制品的过程涉及所有追随者的ISR向Zookeeper注册自己。第一个注册的复制品成为新的领导复制品，其**日志结束偏移量**（**LEO**）成为最后提交消息的偏移量（也称为**高水位标记**（**HW**））。其余注册的复制品成为新选举领导者的追随者。每个复制品在Zookeeper中注册一个监听器，以便在发生任何领导者更改时得到通知。每当选举出新的领导者并且被通知的复制品不是领导者时，它会将其日志截断到最后提交消息的偏移量，然后开始从新的领导者那里赶上。新选举的领导者等待直到经过配置的时间或直到所有活动的复制品同步，然后领导者将当前的ISR写入Zookeeper，并对消息的读写都开放。
- en: Replication in Kafka ensures stronger durability and higher availability. It
    guarantees that any successfully published message will not be lost and will be
    consumed, even in the case of broker failures.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka中的复制确保更强的耐用性和更高的可用性。它保证任何成功发布的消息都不会丢失，并且即使在经纪人故障的情况下也会被消费。
- en: Note
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more insight on Kafka replication implementation, visit [https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3](https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Kafka复制实现的更多见解，请访问[https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3](https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3)。
- en: Summary
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned the design concepts used to build a solid foundation
    for Kafka. You also learned how message compression and replication are done in
    Kafka.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了构建Kafka坚实基础所使用的设计概念。您还了解了Kafka中消息压缩和复制的实现方式。
- en: In the next chapter, we will be focusing on how to write Kafka producers using
    the API provided.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重点介绍如何使用提供的API编写Kafka生产者。
