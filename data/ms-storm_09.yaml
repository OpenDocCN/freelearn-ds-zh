- en: Storm and Hadoop Integration
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm和Hadoop集成
- en: So far, we have seen how Storm can be used for developing real-time stream processing
    applications. In general, these real-time applications are seldom used in isolation;
    they are more often than not used in combination with other batch processing operations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了Storm如何用于开发实时流处理应用程序。一般来说，这些实时应用程序很少单独使用；它们更常用于与其他批处理操作结合使用。
- en: The most common platform for developing batch jobs is Apache Hadoop. In this
    chapter, we will see how applications built with Apache Storm can be deployed
    over existing Hadoop clusters with the help of a Storm-YARN framework for optimized
    use and management of resources. We will also cover how we can write the process
    data into HDFS by creating an HDFS bolt in Storm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 开发批处理作业的最常见平台是Apache Hadoop。在本章中，我们将看到如何使用Apache Storm构建的应用程序可以借助Storm-YARN框架在现有的Hadoop集群上进行部署，以优化资源的使用和管理。我们还将介绍如何通过在Storm中创建一个HDFS
    bolt来将处理数据写入HDFS。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Overview of Apache Hadoop and its various components
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Hadoop及其各个组件概述
- en: Setting up a Hadoop cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置Hadoop集群
- en: Write Storm topology to persist data into HDFS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Storm拓扑写入HDFS以持久化数据
- en: Overview of Storm-YARN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm-YARN概述
- en: Deploying Storm-YARN on Hadoop
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Hadoop上部署Storm-YARN
- en: Running a storm application on Storm-YARN.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Storm-YARN上运行storm应用程序。
- en: Introduction to Hadoop
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop简介
- en: Apache Hadoop is an open source platform for developing and deploying big data
    applications. It was initially developed at Yahoo! based on the MapReduce and
    Google File System papers published by Google. Over the past few years, Hadoop
    has become the flagship big data platform.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop是一个用于开发和部署大数据应用程序的开源平台。最初是在Yahoo!上开发的，基于Google发布的MapReduce和Google文件系统论文。在过去几年里，Hadoop已成为旗舰大数据平台。
- en: In this section, we will discuss the key components of a Hadoop cluster.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论Hadoop集群的关键组件。
- en: Hadoop Common
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop通用
- en: This is the base library on which other Hadoop modules are based. It provides
    an abstraction for OS and filesystem operations so that Hadoop can be deployed
    on a variety of platforms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是其他Hadoop模块基于的基本库。它提供了一个操作系统和文件系统操作的抽象，使得Hadoop可以部署在各种平台上。
- en: Hadoop Distributed File System
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop分布式文件系统
- en: Commonly known as **HDFS**, the **Hadoop Distributed File System** is a scalable,
    distributed, fault-tolerant filesystem. HDFS acts as the storage layer of the
    Hadoop ecosystem. It allows the sharing and storage of data and application code
    among the various nodes in a Hadoop cluster.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常被称为**HDFS**，**Hadoop分布式文件系统**是一种可扩展的、分布式的、容错的文件系统。HDFS充当了Hadoop生态系统的存储层。它允许在Hadoop集群中的各个节点之间共享和存储数据和应用程序代码。
- en: 'The following are the key assumptions taken while designing HDFS:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计HDFS时，做出了以下关键假设：
- en: It should be deployable on a cluster of commodity hardware.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该可以部署在一组廉价硬件的集群上。
- en: Hardware failures are expected, and it should be tolerant to these.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件故障是预期的，它应该能够容忍这些故障。
- en: It should be scalable to thousands of nodes.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该可扩展到数千个节点。
- en: It should be optimized for high throughput, even at the cost of latency.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该针对高吞吐量进行优化，即使牺牲延迟。
- en: Most of the files will be large in size, so it should be optimized for big files.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数文件都会很大，因此应该针对大文件进行优化。
- en: Storage is cheap, so use replication for reliability.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储是廉价的，因此使用复制来保证可靠性。
- en: It should be locality aware so that the computations requested of the data can
    be performed on the physical node where it actually resides. This will result
    in less data movement, hence lower network congestion.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该具有位置感知能力，以便对数据请求的计算可以在实际数据所在的物理节点上执行。这将导致较少的数据移动，从而降低网络拥塞。
- en: An HDFS cluster has the following components.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个HDFS集群有以下组件。
- en: Namenode
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Namenode
- en: The namenode is the master node in an HDFS cluster. It is responsible for managing
    the filesystem metadata and operations. It does not store any user data--but only
    the filesystem tree of all files in the cluster. It also keeps track of the physical
    locations of the blocks that are part of the files.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Namenode是HDFS集群中的主节点。它负责管理文件系统的元数据和操作。它不存储任何用户数据，只存储集群中所有文件的文件系统树。它还跟踪文件的块的物理位置。
- en: Since, the namenode keeps all the data in RAM, it should be deployed on a machine
    with a large amount of RAM. Also, no other processes should be hosted on the machine
    that is hosting the namenode so that all the resources are dedicated to it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于namenode将所有数据保存在RAM中，因此应该部署在具有大量RAM的机器上。此外，不应该在托管namenode的机器上托管其他进程，以便所有资源都专门用于它。
- en: The namenode is the single point of failure in an HDFS cluster. If the namenode
    dies, no operations can take place on an HDFS cluster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Namenode是HDFS集群中的单点故障。如果namenode死机，HDFS集群上将无法进行任何操作。
- en: '![](img/00054.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00054.jpeg)'
- en: 'Figure 1: HDFS Cluster'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：HDFS集群
- en: Datanode
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Datanode
- en: The datanode is responsible for storing user data in HDFS clusters. There can
    be multiple datanodes in an HDFS cluster. A datanode stores data on the physical
    disks attached to the system hosting the datanode. It is not recommended to store
    datanode data on disks in a RAID configuration as HDFS achieves data protection
    by replicating data across datanodes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Datanode负责在HDFS集群中存储用户数据。在HDFS集群中可以有多个datanode。Datanode将数据存储在托管datanode的系统上的物理磁盘上。不建议将datanode数据存储在RAID配置的磁盘上，因为HDFS通过在datanode之间复制数据来实现数据保护。
- en: HDFS client
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS客户端
- en: An HDFS client is a client library that can be used to interact with HDFS clusters.
    It usually talks to the namenode to perform meta operations, such as creating
    new files and so on, while the datanodes serve the actual data read and write
    requests.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS客户端是一个客户端库，可用于与HDFS集群交互。它通常与namenode通信，执行元操作，如创建新文件等，而datanodes提供实际的数据读写请求。
- en: Secondary namenode
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 次要名称节点
- en: The secondary namenode is one of the poorly named components of HDFS. Despite
    its name, it is not a standby for the namenode. To understand its function, we
    need to delve deep into how the namenode works.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助namenode是HDFS中命名不当的组件之一。尽管它的名字是这样，但它并不是namenode的备用。要理解它的功能，我们需要深入了解namenode的工作原理。
- en: A namenode keeps the filesystem metadata in the main memory. For durability,
    it also writes this metadata to the local disk in the form of the image file.
    When a namenode starts, it reads this fs image snapshot file to recreate the in-memory
    data structure for holding the filesystem data. Any updates on the filesystem
    are applied to the in-memory data structure, but not to the image. These changes
    are written to disk in separate files called edit logs. When a namenode starts,
    it merges these edit logs into the image so that the next restart will be quick.
    In production, the edit logs can grow very large as the namenode is not restarted
    frequently. This could result in a very long boot time for the namenode whenever
    it is restarted.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Namenode将文件系统元数据保存在主内存中。为了持久性，它还将这些元数据以镜像文件的形式写入本地磁盘。当namenode启动时，它读取这个fs镜像快照文件，以重新创建内存数据结构来保存文件系统数据。文件系统的任何更新都会应用到内存数据结构，但不会应用到镜像中。这些更改会被写入称为编辑日志的单独文件中。当namenode启动时，它将这些编辑日志合并到镜像中，以便下次重新启动将会很快。在生产环境中，由于namenode不经常重新启动，编辑日志可能会变得非常大。这可能导致namenode在重新启动时启动时间非常长。
- en: The secondary namenode is responsible for merging the edit logs of the namenode
    with the image so that the namenode starts faster the next time. It takes the
    image snapshot and the edit logs from the namenode and merges them and then puts
    the updated image snapshot on the namenode machines. This reduces the amount of
    merging that is required from the namenode on the restarts, thus reducing the
    time to boot for the namenode.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助namenode负责将namenode的编辑日志与镜像合并，以便下次namenode启动更快。它从namenode获取镜像快照和编辑日志，然后将它们合并，然后将更新后的镜像快照放在namenode机器上。这减少了namenode在重新启动时需要进行的合并量，从而减少了namenode的启动时间。
- en: 'The following screenshot illustrates the working of the secondary namenode:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了辅助namenode的工作原理：
- en: '![](img/00055.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00055.jpeg)'
- en: 'Figure 2: Secondary Namenode functioning'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：辅助Namenode的功能
- en: So far, we have seen the storage side of Hadoop. Next we will look into the
    processing components.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了Hadoop的存储部分。接下来我们将看一下处理组件。
- en: YARN
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YARN
- en: YARN is a cluster resource management framework that enables users to submit
    a variety of jobs to a Hadoop cluster, and manages scalability, fault tolerance,
    scheduling of jobs, and so on. As HDFS provides a storage layer for large amounts
    of data, the YARN framework gives you the plumbing required for writing big data
    processing applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: YARN是一个集群资源管理框架，它使用户能够向Hadoop集群提交各种作业，并管理可伸缩性、容错性、作业调度等。由于HDFS提供了大量数据的存储层，YARN框架为编写大数据处理应用程序提供了所需的基础设施。
- en: The following are the major components of a YARN cluster.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是YARN集群的主要组件。
- en: ResourceManager (RM)
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ResourceManager（RM）
- en: The ResourceManager is the entry point for applications in the YARN cluster.
    It is the master process in the cluster that is responsible for managing all the
    resources in the cluster. It is also responsible for the scheduling of various
    jobs submitted to the cluster. This scheduling policy is pluggable, and can be
    customized by a user in case they want to support new kinds of application.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ResourceManager是YARN集群中应用程序的入口点。它是集群中负责管理所有资源的主进程。它还负责调度提交到集群的各种作业。这种调度策略是可插拔的，用户可以根据需要支持新类型的应用程序进行自定义。
- en: NodeManager (NM)
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NodeManager（NM）
- en: A NodeManager agent is deployed on each of the processing nodes in the cluster.
    It is the counterpart to the ResourceManager at the node level. It communicates
    with the ResourceManager to update the node state and receive any job requests
    from it. It is also responsible for the life cycle management and the reporting
    of various node metrics to the ResourceManager.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中的每个处理节点上部署了一个NodeManager代理。它是与节点级别的ResourceManager对应的。它与ResourceManager通信，更新节点状态并接收来自ResourceManager的任何作业请求。它还负责生命周期管理和向ResourceManager报告各种节点指标。
- en: ApplicationMaster (AM)
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ApplicationMaster（AM）
- en: Once a job is scheduled by the ResourceManager, it no longer keeps track of
    its status and progress. This results in the ResourceManager being able to support
    completely different kinds of application in the cluster without worrying about
    the internal communication and logic of the application.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦ResourceManager调度了作业，它就不再跟踪其状态和进度。这使得ResourceManager能够支持集群中完全不同类型的应用程序，而不必担心应用程序的内部通信和逻辑。
- en: Whenever an application is submitted, the ResourceManager creates a new ApplicationMaster
    for that application, which is then responsible for negotiating resources from
    ResourceManager and communicating with the NodeMangers for the resources. NodeManager
    provides resources in the form of resource containers, which are abstractions
    for resource allocation, where you can tell how much CPU, memory, and so on is
    required.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 每当提交一个应用程序时，ResourceManager都会为该应用程序创建一个新的ApplicationMaster，然后负责与ResourceManager协商资源，并与NodeMangers通信以获取资源。NodeManager以资源容器的形式提供资源，这是资源分配的抽象，您可以告诉需要多少CPU、内存等。
- en: Once the application starts running on various nodes in the cluster, the ApplicationMaster
    keeps track of the status of the various jobs and in case of failures, reruns
    those jobs. On completion of the job, it releases the resources to the ResourceManager.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦应用程序在集群中的各个节点上开始运行，ApplicationMaster就会跟踪各种作业的状态，并在失败时重新运行这些作业。作业完成后，它将释放资源给ResourceManager。
- en: 'The following screenshot illustrates the various components in a YARN cluster:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了YARN集群中的各种组件：
- en: '![](img/00056.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00056.jpeg)'
- en: 'Figure 3: YARN components'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：YARN组件
- en: Installation of Hadoop
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop安装
- en: Now that we have seen both the storage and processing parts of a Hadoop cluster,
    let's get started with the installation of Hadoop. We will be using Hadoop 2.2.0
    in this chapter. Please note that this version is not compatible with Hadoop 1.X
    versions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了Hadoop集群的存储和处理部分，让我们开始安装Hadoop。在本章中，我们将使用Hadoop 2.2.0。请注意，此版本与Hadoop
    1.X版本不兼容。
- en: 'We will be setting up a cluster on a single node. Before starting, please make
    sure that you have the following installed on your system:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在单节点上设置一个集群。在开始之前，请确保您的系统上已安装以下内容：
- en: JDK 1.7
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDK 1.7
- en: '`ssh-keygen`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ssh-keygen`'
- en: 'In case you don''t have `wget` or `ssh-keygen`, install it with the following
    command:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有`wget`或`ssh-keygen`，请使用以下命令进行安装：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we will need to set up a passwordless SSH on this machine as it is required
    for Hadoop.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要在此计算机上设置无密码SSH，因为这对于Hadoop是必需的。
- en: Setting passwordless SSH
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置无密码SSH
- en: 'The following are the steps for setting up a passwordless SSH:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是设置无密码SSH的步骤：
- en: 'Generate your SSH key pair by executing the following command:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令生成您的SSH密钥对：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next we need to copy the generated public key to the list of authorized keys
    in the current users. To do this, execute the following command:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将生成的公钥复制到当前用户的授权密钥列表中。要做到这一点，执行以下命令：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we can check whether the passwordless SSH is working by connecting to
    localhost with the SSH by the following command:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以通过以下命令连接到localhost检查无密码SSH是否正常工作：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Since we are able to use SSH into localhost without a password, our setup is
    working now and we will now proceed with the Hadoop setup.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们能够在本地主机上使用SSH而无需密码，我们的设置现在正在工作，我们现在将继续进行Hadoop设置。
- en: Getting the Hadoop bundle and setting up environment variables
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取Hadoop捆绑包并设置环境变量
- en: 'The following are the steps for setting up Hadoop:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是设置Hadoop的步骤：
- en: Download Hadoop 2.2.0 from the Apache site at [http://hadoop.apache.org/releases.html#Download](http://hadoop.apache.org/releases.html#Download).
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Apache网站下载Hadoop 2.2.0 [http://hadoop.apache.org/releases.html#Download](http://hadoop.apache.org/releases.html#Download)。
- en: 'Untar the archive at a location where we want to install Hadoop. We will call
    this location `$HADOOP_HOME`:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们想要安装Hadoop的位置解压存档。我们将称此位置为`$HADOOP_HOME`：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we need to set up the environment variables and the path for Hadoop,
    Add the following entries to your `~/.bashrc` file. Make sure that you are providing
    the paths for Java and Hadoop as per your system:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要设置环境变量和Hadoop的路径，将以下条目添加到您的`~/.bashrc`文件中。确保根据您的系统提供Java和Hadoop的路径：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Refresh your `~/.bashrc` file:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 刷新您的`~/.bashrc`文件：
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now let''s check whether the paths are properly configured with the following
    command:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们用以下命令检查路径是否正确配置：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding snippet, we can see that the paths are properly set. Now we
    will set up HDFS on our system.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的片段中，我们可以看到路径已正确设置。现在我们将在系统上设置HDFS。
- en: Setting up HDFS
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置HDFS
- en: 'Follow these steps for setting up HDFS:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤设置HDFS：
- en: 'Make directories for holding the namenode and datanode data:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建用于保存namenode和datanode数据的目录：
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Specify the namenode port in the `$HADOOP_CONF_DIR/core-site.xml` file by adding
    the following property inside the `<configuration>` tag:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在`$HADOOP_CONF_DIR/core-site.xml`文件的`<configuration>`标记中添加以下属性来指定namenode端口：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Specify the namenode and datanode directory in the `$HADOOP_CONF_DIR/hdfs-site.xml`
    file by adding the following property inside the `<configuration>` tag:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在`$HADOOP_CONF_DIR/hdfs-site.xml`文件的`<configuration>`标记中添加以下属性来指定namenode和datanode目录：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we will format the namenode. This is a one-time process, and it needs to
    be done only while setting up the HDFS:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将格式化namenode。这是一个一次性的过程，只需要在设置HDFS时执行：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we are done with the configuration, and we will start HDFS:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经完成了配置，我们将启动HDFS：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, execute the `jps` command to see if all the processes are running fine:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，执行`jps`命令查看所有进程是否正常运行：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we can see that all the expected processes are running.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到所有预期的进程都在运行。
- en: 'Now you can check the status of HDFS using the namenode UI by opening `http://localhost:50070`
    in your browser. You should see something similar to the following:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以通过在浏览器中打开`http://localhost:50070`来检查HDFS的状态。您应该看到类似以下的内容：
- en: '![](img/00057.jpeg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00057.jpeg)'
- en: 'Figure 4: Namenode web UI'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Namenode web UI
- en: You can interact with HDFS using the `hdfs dfs` command. Get all the options
    by running `hdfs dfs` on a console or refer to the documentation at [http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/FileSystemShell.html](http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/FileSystemShell.html).
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用`hdfs dfs`命令与HDFS进行交互。在控制台上运行`hdfs dfs`以获取所有选项，或者参考[http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/FileSystemShell.html](http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/FileSystemShell.html)上的文档。
- en: Now that HDFS is deployed, we will set up YARN next.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在HDFS已部署，我们将接下来设置YARN。
- en: Setting up YARN
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置YARN
- en: 'The following are the steps for setting up YARN:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是设置YARN的步骤：
- en: 'Create a `mapred-site.xml` file from the template `mapred-site.xml.template`:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模板`mapred-site.xml.template`创建`mapred-site.xml`文件：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Specify that we are using a YARN framework by adding the following property
    in the `$HADOOP_CONF_DIR/mapred-site.xml` file in the `<configuration>` tag:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在`$HADOOP_CONF_DIR/mapred-site.xml`文件的`<configuration>`标记中添加以下属性来指定我们正在使用YARN框架：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Configure the following properties in the `$HADOOP_CONF_DIR/yarn-site.xml`
    file:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`$HADOOP_CONF_DIR/yarn-site.xml`文件中配置以下属性：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Start the YARN processes with the following command:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令启动YARN进程：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, execute the `jps` command to see if all the processes are running fine:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，执行`jps`命令查看所有进程是否正常运行：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we can see that all the expected processes are running.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到所有预期的进程都在运行。
- en: 'Now you can check the status of YARN using the ResourceManager web UI by opening
    `http://localhost:8088/cluster` in your browser. You should see something similar
    to the following:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以通过在浏览器中打开`http://localhost:8088/cluster`来检查YARN的状态，使用ResourceManager web
    UI。您应该会看到类似以下内容的内容：
- en: '![](img/00058.jpeg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00058.jpeg)'
- en: 'Figure 5: ResourceManager web UI'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：ResourceManager web UI
- en: 'You can interact with YARN using the `yarn` command. Get all the options by
    running `yarn` on a console or refer to the documentation at [http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/YarnCommands.html](http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/YarnCommands.html).
    To get all the applications currently running on YARN, run the following command:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用`yarn`命令与YARN进行交互。在控制台上运行`yarn`或参考[http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/YarnCommands.html](http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/YarnCommands.html)获取所有选项。要获取当前在YARN上运行的所有应用程序，请运行以下命令：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: With this, we have completed the deployment of the Hadoop cluster on a single
    node. Next we will see how to run Storm topologies on this cluster.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们已经完成了在单节点上部署Hadoop集群。接下来我们将看到如何在此集群上运行Storm拓扑。
- en: Write Storm topology to persist data into HDFS
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Storm拓扑写入HDFS以持久化数据
- en: 'In this section, we are going to cover how we can write the HDFS bolt to persist
    data into HDFS. In this section, we are focusing on the following points:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何编写HDFS bolt以将数据持久化到HDFS中。在本节中，我们将重点介绍以下几点：
- en: Consuming data from Kafka
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Kafka消费数据
- en: The logic to store the data into HDFS
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据存储到HDFS的逻辑
- en: Rotating file into HDFS after a predefined time or size
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预定义的时间或大小后将文件旋转到HDFS
- en: 'Perform the following steps to create the topology to store the data into the
    HDFS:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来创建将数据存储到HDFS的拓扑：
- en: Create a new maven project with groupId `com.stormadvance` and artifactId `storm-hadoop`.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的maven项目，groupId为`com.stormadvance`，artifactId为`storm-hadoop`。
- en: 'Add the following dependencies in the `pom.xml` file. We are adding the Kafka
    Maven dependency in `pom.xml` to support Kafka Consumer. Please refer the previous
    chapter to produce data in Kafka as here we are going to consume data from Kafka
    and store in HDFS:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pom.xml`文件中添加以下依赖项。我们在`pom.xml`中添加Kafka Maven依赖项以支持Kafka消费者。请参考前一章节，在那里我们将从Kafka消费数据并存储在HDFS中：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Write a Storm Hadoop topology to consume data from HDFS and store it in HDFS.
    The following is a line-by-line description of the `com.stormadvance.storm_hadoop.topology.StormHDFSTopology`
    class:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个Storm Hadoop拓扑来消费HDFS中的数据并将其存储在HDFS中。以下是`com.stormadvance.storm_hadoop.topology.StormHDFSTopology`类的逐行描述：
- en: 'Use the following lines to consume the data from Kafka:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码行从Kafka消费数据：
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Use the following lines of code to define the HDFS Namenode details and the
    name of the HDFS data directory to store the data into HDFS, create a new file
    after every 5 MB chunk of data stored into HDFS, and sync the latest data into
    the file after every 1,000 records:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码行定义HDFS Namenode的详细信息和HDFS数据目录的名称，以将数据存储到HDFS中，在每存储5MB数据块后创建一个新文件，并在每存储1,000条记录后将最新数据同步到文件中：
- en: '[PRE22]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Use the following code to connect Spout with the HDFS bolt:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将Spout连接到HDFS bolt：
- en: '[PRE23]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Integration of Storm with Hadoop
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Storm与Hadoop集成
- en: The probability that the organizations developing and operating big data applications
    already have a Hadoop cluster deployed is very high. Also, there is a high possibility
    that they also have real-time stream processing applications deployed to go along
    with the batch applications running on Hadoop.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 开发和运行大数据应用程序的组织已经部署了Hadoop集群的可能性非常高。此外，他们也很可能已经部署了实时流处理应用程序，以配合在Hadoop上运行的批处理应用程序。
- en: It would be great if we can leverage the already deployed YARN cluster to also
    run the Storm topologies. This will reduce the operational cost of maintenance
    by giving you only one cluster to manage instead of two.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可以利用已部署的YARN集群来运行Storm拓扑，那将是很好的。这将通过只管理一个集群而不是两个来减少维护的操作成本。
- en: Storm-YARN is a project developed by Yahoo! that enables the deployment of Storm
    topologies over YARN clusters. It enables the deployment of Storm processes on
    nodes managed by YARN.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Storm-YARN是Yahoo!开发的一个项目，它可以在YARN集群上部署Storm拓扑。它可以在YARN管理的节点上部署Storm进程。
- en: 'The following diagram illustrates how the Storm processes are deployed on YARN:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了Storm进程如何部署在YARN上：
- en: '![](img/00059.gif)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00059.gif)'
- en: 'Figure 6: Storm processes on YARN'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：YARN上的Storm进程
- en: In the next section, we will see how to set up Storm-YARN.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将看到如何设置Storm-YARN。
- en: Setting up Storm-YARN
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Storm-YARN
- en: 'Since Storm-YARN is still in alpha, we will be proceeding with the base master
    branch of the `git` repository. Make sure you have `git` installed on your system.
    If not, then run the following command:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Storm-YARN仍处于alpha阶段，我们将继续使用`git`存储库的基础主分支。确保您的系统上已安装了`git`。如果没有，请运行以下命令：
- en: '[PRE24]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Also make sure that you have Apache Zookeeper and Apache Maven installed on
    your system. Refer to the previous chapters for their setup instructions.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 还要确保您的系统上已安装了Apache Zookeeper和Apache Maven。有关其设置说明，请参考前面的章节。
- en: 'The following are the steps for deploying Storm-YARN:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 部署Storm-YARN的步骤如下：
- en: 'Clone the `storm-yarn` repo with the following command:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令克隆`storm-yarn`存储库：
- en: '[PRE25]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Build `storm-yarn` by running the following `mvn` command:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下`mvn`命令构建`storm-yarn`：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Copy the `storm.zip` file from `storm-yarn/lib` to HDFS by using the following
    commands:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将`storm.zip`文件从`storm-yarn/lib`复制到HDFS：
- en: '[PRE27]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The exact version might be different from `1.0.2-wip21` in your case.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 确切的版本在您的情况下可能与`1.0.2-wip21`不同。
- en: 'Create a directory to hold our Storm configuration:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个目录来保存我们的Storm配置：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Add the following configuration in the `~/storm-data/storm-1.0.2-wip21/conf/storm.yaml`
    file:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`~/storm-data/storm-1.0.2-wip21/conf/storm.yaml`文件中添加以下配置：
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: If required, change the values as per your setup.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如有需要，根据您的设置更改值。
- en: 'Add a `storm-yarn/bin` folder to your path by adding the following to the `~/.bashrc`
    file:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将以下内容添加到`~/.bashrc`文件中，将`storm-yarn/bin`文件夹添加到您的路径中：
- en: '[PRE30]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Refresh `~/.bashrc`:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 刷新`~/.bashrc`：
- en: '[PRE31]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Make sure Zookeeper is running on your system. If not, then start ZooKeeper
    by running the following command:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保Zookeeper在您的系统上运行。如果没有，请运行以下命令启动ZooKeeper：
- en: '[PRE32]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Launch `storm-yarn` using the following command:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令启动`storm-yarn`：
- en: '[PRE33]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The Storm-YARN application has been submitted with the application ID `application_1397537047058_0001`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Storm-YARN应用程序已经提交，应用程序ID为`application_1397537047058_0001`。
- en: 'We can retrieve the status of our application by using the following `yarn`
    command:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下`yarn`命令检索应用程序的状态：
- en: '[PRE34]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can also see `storm-yarn` running on the ResourceManager web UI at `http://localhost:8088/cluster/`.
    You should be able to see something similar to the following:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以在ResourceManager web UI上看到`storm-yarn`运行在`http://localhost:8088/cluster/`。您应该能够看到类似以下内容：
- en: '![](img/00060.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00060.jpeg)'
- en: 'Figure 7: Storm-YARN on the ResourceManager web UI'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：ResourceManager web UI上的Storm-YARN
- en: You can explore the various metrics exposed by clicking through various links
    on the UI.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过单击UI上的各种链接来探索各种公开的指标。
- en: 'Nimbus should also be running now, and you should be able to see it through
    the Nimbus web UI at `http://localhost:7070/`:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nimbus现在也应该在运行中，您应该能够通过Nimbus web UI看到它，网址为`http://localhost:7070/`：
- en: '![](img/00061.jpeg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00061.jpeg)'
- en: 'Figure 8: Nimbus web UI running on YARN'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：Nimbus web UI在YARN上运行
- en: 'Now we need to get the Storm configuration that will be used when deploying
    topologies on this Storm cluster on YARN. To do so, execute the following command:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要获取将在YARN上的Storm集群上部署拓扑时使用的Storm配置。为此，请执行以下命令：
- en: '[PRE35]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Make sure that you are passing the correct application ID (as retrieved in step
    9) to the `-appId` parameter.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将正确的应用程序ID（在第9步中检索）传递给`-appId`参数。
- en: Now that we have successfully deployed Storm-YARN, we will see how to run our
    topologies on this storm cluster.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功部署了Storm-YARN，我们将看到如何在这个storm集群上运行我们的拓扑。
- en: Storm-Starter topologies on Storm-YARN
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Storm-YARN上运行Storm-Starter拓扑
- en: In this section, we will see how to deploy the Storm-Starter topologies on `storm-yarn`.
    Storm-Starter is a set of example topologies that comes with Storm.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何在`storm-yarn`上部署Storm-Starter拓扑。Storm-Starter是一组随Storm一起提供的示例拓扑。
- en: 'Follow these steps to run the topologies on Storm-YARN:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤在Storm-YARN上运行拓扑：
- en: 'Clone the `storm-starter` project:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆`storm-starter`项目：
- en: '[PRE36]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Package the topologies with the following `mvn` command:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下`mvn`命令打包拓扑：
- en: '[PRE37]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Deploy the topologies on `storm-yarn` with the following command:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在`storm-yarn`上部署拓扑：
- en: '[PRE38]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now we can see the deployed topology on the Nimbus web UI at `http://localhost:7070/`:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以在Nimbus web UI上看到部署的拓扑，网址为`http://localhost:7070/`：
- en: '![](img/00062.jpeg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00062.jpeg)'
- en: 'Figure 9: Nimbus web UI showing the word-count topology on YARN'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：Nimbus web UI显示了YARN上的单词计数拓扑
- en: 'To see how you can interact with the topologies running on `storm-yarn`, run
    the following command:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看如何与在`storm-yarn`上运行的拓扑进行交互，请运行以下命令：
- en: '[PRE39]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: It will list all the options interacting with the various Storm processes and
    starting new supervisors.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将列出与各种Storm进程交互和启动新监督者的所有选项。
- en: So in this section, we built a Storm-started topology and ran it on `storm-yarn`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本节中，我们构建了一个Storm-started拓扑，并在`storm-yarn`上运行它。
- en: Summary
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced Apache Hadoop and the various components, such
    as HDFS, YARN, and so on, that are part of a Hadoop cluster. We also saw the subcomponents
    of an HDFS and YARN cluster and how they interact with each other. Then we walked
    through how to set up a single node Hadoop cluster.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了Apache Hadoop以及HDFS、YARN等各种组件，这些组件是Hadoop集群的一部分。我们还看到了HDFS和YARN集群的子组件以及它们之间的交互。然后我们演示了如何设置单节点Hadoop集群。
- en: We also introduced Storm-YARN, which was the main point of this chapter. Storm-YARN
    enables you to run Storm topologies on a Hadoop cluster. This helps from a manageability
    and operations point of view. Finally, we saw how to deploy a topology on Storm
    running on YARN.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还介绍了Storm-YARN，这是本章的重点。Storm-YARN使您能够在Hadoop集群上运行Storm拓扑。从可管理性和运维角度来看，这对我们很有帮助。最后，我们看到了如何在YARN上运行的Storm上部署拓扑。
- en: In the next chapter, we will see how Storm can integrate with other big data
    technologies, such as HBase, Redis, and so on.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到Storm如何与其他大数据技术（如HBase、Redis等）集成。
