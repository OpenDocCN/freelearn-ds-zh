- en: Spark's Three Data Musketeers for Machine Learning - Perfect Together
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 的机器学习三剑客 - 完美结合
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下内容：
- en: Creating RDDs with Spark 2.0 using internal data sources
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Spark 2.0 的内部数据源创建 RDD
- en: Creating RDDs with Spark 2.0 using external data sources
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Spark 2.0 的外部数据源创建 RDD
- en: Transforming RDDs with Spark 2.0 using the filter() API
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Spark 2.0 的 filter() API 转换 RDD
- en: Transforming RDDs with the super useful flatMap() API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用非常有用的 flatMap() API 转换 RDD
- en: Transforming RDDs with set operation APIs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集合操作 API 转换 RDD
- en: RDD transformation/aggregation with groupBy() and reduceByKey()
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 groupBy() 和 reduceByKey() 进行 RDD 转换/聚合
- en: Transforming RDDs with the zip() API
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 zip() API 转换 RDD
- en: Join transformation with paired key-value RDDs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配对键值RDD进行连接转换
- en: Reduce and grouping transformation with paired key-value RDDs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配对键值 RDD 进行归约和分组转换
- en: Creating DataFrames from Scala data structures
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Scala 数据结构创建 DataFrame
- en: Operating on DataFrames programmatically without SQL
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以编程方式操作DataFrame而不使用SQL
- en: Loading DataFrames and setup from an external source ...
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从外部源加载 DataFrame 并进行设置...
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: The three workhorses of Spark for efficient processing of data at scale are
    RDD, DataFrames, and the Dataset API. While each can stand on its own merit, the
    new paradigm shift favors Dataset as the unifying data API to meet all data wrangling
    needs in a single interface.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 高效处理大规模数据的三驾马车是 RDD、DataFrames 和 Dataset API。虽然每个都有其独立的价值，但新的范式转变倾向于将
    Dataset 作为统一的数据 API，以满足单一接口中的所有数据处理需求。
- en: 'The new Spark 2.0 Dataset API is a type-safe collection of domain objects that
    can be operated on via transformation (similar to RDDs'' filter, `map`, `flatMap()`,
    and so on) in parallel using functional or relational operations. For backward
    compatibility, Dataset has a view called **DataFrame**, which is a collection
    of rows that are untyped. In this chapter, we demonstrate all three API sets.
    The figure ahead summarizes the pros and cons of the key components of Spark for
    data wrangling:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0 的新 Dataset API 是一种类型安全的领域对象集合，可以通过转换（类似于 RDD 的过滤、`map`、`flatMap()`
    等）并行使用函数或关系操作。为了向后兼容，Dataset 有一个名为 **DataFrame** 的视图，它是一个无类型的行集合。在本章中，我们展示了所有三种
    API 集。前面的图总结了 Spark 数据处理关键组件的优缺点：
- en: '![](img/b88f07c4-3036-4fdf-8d48-86a5b1c8c63f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b88f07c4-3036-4fdf-8d48-86a5b1c8c63f.png)'
- en: An advanced developer in machine learning must understand and be able to use
    all three API sets without any issues, for algorithmic augmentation or legacy
    reasons. While we recommend that every developer should migrate toward the high-level
    Dataset API, you will still need to know RDDs for programming against the Spark
    core system. For example, it is very common for investment banking and hedge funds
    to read leading journals in machine learning, mathematical programming, finance,
    statistics, or artificial intelligence and then code the research in low-level
    APIs to gain competitive advantage.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的高级开发者必须理解并能够无障碍地使用所有三种 API 集，无论是为了算法增强还是遗留原因。虽然我们建议每位开发者都应向高级 Dataset API
    迁移，但你仍需了解 RDD，以便针对 Spark 核心系统编程。例如，投资银行和对冲基金经常阅读机器学习、数学规划、金融、统计学或人工智能领域的领先期刊，然后使用低级
    API 编码研究以获得竞争优势。
- en: RDDs - what started it all...
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDDs - 一切的起点...
- en: The RDD API is a critical toolkit for Spark developers since it favors low-level
    control over the data within a functional programming paradigm. What makes RDDs
    powerful also makes it harder to work with for new programmers. While it may be
    easy to understand the RDD API and manual optimization techniques (for example,
    `filter()` before a `groupBy()` operation), writing advanced code would require
    consistent practice and fluency.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: RDD API 是 Spark 开发者的重要工具，因为它在函数式编程范式中提供了对数据底层控制的偏好。RDD 的强大之处同时也使得新程序员更难以使用。虽然理解
    RDD API 和手动优化技术（例如，在 `groupBy()` 操作之前使用 `filter()`）可能很容易，但编写高级代码需要持续的练习和熟练度。
- en: When data files, blocks, or data structures are converted to RDDs, the data
    is broken down into smaller units called **partitions** (similar to splits in
    Hadoop) and distributed among the nodes so they can be operated on in parallel
    at the same time. Spark provides this functionality right out ...
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据文件、块或数据结构转换为 RDD 时，数据被分解为称为 **分区**（类似于 Hadoop 中的拆分）的较小单元，并分布在节点之间，以便它们可以同时并行操作。Spark
    直接提供了这种功能...
- en: DataFrame - a natural evolution to unite API and SQL via a high-level API
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据帧——通过高级API统一API和SQL的自然演进
- en: The Spark developer community has always strived to provide an easy-to-use high-level
    API for the community starting from the AMPlab days at Berkley. The next evolution
    in the Data API materialized when Michael Armbrust gave the community the SparkSQL
    and Catalyst optimizer, which made data virtualization possible with Spark using
    a simple and well-understood SQL interface. The DataFrame API was a natural evolution
    to take advantage of SparkSQL by organizing data into named columns like relational
    tables.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark开发者社区始终致力于从伯克利的AMPlab时代开始为社区提供易于使用的高级API。数据API的下一个演进是在Michael Armbrust向社区提供SparkSQL和Catalyst优化器时实现的，这使得使用简单且易于理解的SQL接口进行数据虚拟化成为可能。数据帧API是利用SparkSQL的自然演进，通过将数据组织成关系表那样的命名列来实现。
- en: The DataFrame API made data wrangling via SQL available to a multitude of data
    scientists and developers familiar with DataFrames in R (data.frame) or Python/Pandas
    (pandas.DataFrame).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据帧API通过SQL使数据整理对众多熟悉R（data.frame）或Python/Pandas（pandas.DataFrame）中的数据帧的数据科学家和开发者可用。
- en: Dataset - a high-level unifying Data API
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集——一个高级的统一数据API
- en: A dataset is an immutable collection of objects which are modelled/mapped to
    a traditional relational schema. There are four attributes that distinguish it
    as the preferred method going forward. We particularly find the Dataset API appealing
    since we find it familiar to RDDs with the usual transformational operators (for
    example, `filter()`, `map()`, `flatMap()`, and so on). The Dataset will follow
    a lazy execution paradigm similar to RDD. The best way to try to reconcile DataFrames
    and Datasets is to think of a DataFrame as an alias that can be thought of as
    `Dataset[Row]`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是一个不可变的对象集合，这些对象被建模/映射到传统的关系模式。有四个属性使其成为未来首选的方法。我们特别发现数据集API具有吸引力，因为它与RDD相似，具有常规的转换操作符（例如，`filter()`、`map()`、`flatMap()`等）。数据集将遵循与RDD类似的惰性执行范式。尝试调和数据帧和数据集的最佳方式是将数据帧视为可以被认为是`Dataset[Row]`的别名。
- en: '**Strong type safety**: We now have both compile-time (syntax errors) and runtime
    safety in a unified Data API, which helps the ML developer ...'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强类型安全**：我们现在在统一的数据API中既有编译时（语法错误）也有运行时安全，这有助于ML开发者...'
- en: Creating RDDs with Spark 2.0 using internal data sources
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0通过内部数据源创建RDD
- en: There are four ways to create RDDs in Spark. They range from the `parallelize()`
    method for simple testing and debugging within the client driver code to streaming
    RDDs for near-realtime responses. In this recipe, we provide you with several
    examples to demonstrate RDD creation using internal sources.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中创建RDD有四种方式，从用于客户端驱动程序中简单测试和调试的`parallelize()`方法，到用于近实时响应的流式RDD。在本节中，我们将提供多个示例，展示如何使用内部数据源创建RDD。
- en: How to do it...
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the necessary packages:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于设置`log4j`日志级别的包。此步骤是可选的，但我们强烈建议这样做（根据开发周期适当更改级别）。
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误以减少输出。参见上一步骤了解包要求。
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How it works...
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The data held in the client driver is parallelized and distributed across the
    cluster using the number of portioned RDDs (the second parameter) as the guideline.
    The resulting RDD is the magic of Spark that started it all (refer to Matei Zaharia's
    original white paper).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端驱动程序中的数据通过分区RDD的数量（第二个参数）作为指导进行并行化和分布。生成的RDD是Spark的魔力，它开启了这一切（参阅Matei Zaharia的原始白皮书）。
- en: The resulting RDDs are now fully distributed data structures with fault tolerance
    and lineage that can be operated on in parallel using Spark framework.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的RDD现在是具有容错性和血统的完全分布式数据结构，可以使用Spark框架并行操作。
- en: We read a text file `A Tale of Two Cities by Charles Dickens` from [http://www.gutenberg.org/](http://www.gutenberg.org/) into
    Spark RDDs. We then proceed to split and tokenize the data and print the number
    of total words using Spark's operators (for example, `map`, `flatMap()`, and so
    on).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[http://www.gutenberg.org/](http://www.gutenberg.org/)读取文本文件`查尔斯·狄更斯的《双城记》`到Spark
    RDDs中。然后我们继续分割和标记化数据，并使用Spark的操作符（例如，`map`，`flatMap()`等）打印出总单词数。
- en: Creating RDDs with Spark 2.0 using external data sources
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用外部数据源创建Spark 2.0的RDDs
- en: In this recipe, we provide you with several examples to demonstrate RDD creation
    using external sources.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们为您提供了几个示例，以展示使用外部源创建RDD。
- en: How to do it...
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Import the necessary packages:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于设置`log4j`日志级别的包。这一步是可选的，但我们强烈建议这样做（根据开发周期适当更改级别）。
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误，以减少输出。请参阅上一步骤了解包要求。
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行。
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We obtain the data from the Gutenberg project. This is a great source for accessing
    actual text, ranging from the complete works of *Shakespeare* to *Charles Dickens*.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从古腾堡项目获取数据。这是一个获取实际文本的绝佳来源，涵盖了从*莎士比亚*全集到*查尔斯·狄更斯*的作品。
- en: 'Download the text from the following sources and store it in your local directory:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下来源下载文本并将其存储在本地目录中：
- en: 'Source: [http://www.gutenberg.org](http://www.gutenberg.org)'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来源：[http://www.gutenberg.org](http://www.gutenberg.org)
- en: 'Selected book: *A Tale of Two Cities by Charles Dickens*'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选定书籍：*查尔斯·狄更斯的《双城记》*
- en: 'URL: [http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt)'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'URL: [http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt)'
- en: Once again, we use `SparkContext`, available via `SparkSession`, and its function
    `textFile()` to read the external data source and parallelize it across the cluster.
    Remarkably, all the work is done for the developer behind the scenes by Spark
    using one single call to load a wide variety of formats (for example, text, S3,
    and HDFS), which parallelizes the data across the cluster using the `protocol:filepath`
    combination.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，我们使用`SparkContext`，通过`SparkSession`可用，并使用其`textFile()`函数读取外部数据源并在集群上并行化它。值得注意的是，所有工作都是由Spark在幕后为开发者完成的，只需一次调用即可加载多种格式（例如，文本、S3和HDFS），并使用`protocol:filepath`组合在集群上并行化数据。
- en: To demonstrate, we load the book, which is stored as ASCII, text using the `textFile()`
    method from `SparkContext` via `SparkSession`, which, in turn, goes to work behind
    the scenes and creates portioned RDDs across the cluster.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了演示，我们加载了这本书，它以ASCII文本形式存储，使用`SparkContext`通过`SparkSession`的`textFile()`方法，后者在幕后工作，并在集群上创建分区RDDs。
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output will be as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Even though we have not covered the Spark transformation operator, we'll look
    at a small code snippet which will break the file into words using blanks as a
    separator. In a real-life situation, a regular expression will be needed to cover
    all the edge cases with all the whitespace variations (refer to the *Transforming
    RDDs with Spark using filter() APIs* recipe in this chapter).
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管我们尚未涉及Spark转换操作符，我们将查看一小段代码，该代码使用空格作为分隔符将文件分解成单词。在实际情况下，需要一个正则表达式来处理所有边缘情况以及所有空白变化（请参考本章中的*使用filter()
    API的Spark中转换RDDs*配方）。
- en: We use a lambda function to receive each line as it is read and split it into
    words using blanks as separator.
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用lambda函数接收每行读取的内容，并使用空格作为分隔符将其分解成单词。
- en: We use a flatMap to break the array of lists of words (that is, each group of
    words from a line corresponds to a distinct array/list for that line). In short,
    what we want is a list of words and not a list of a list of words for each line.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用flatMap来分解单词列表的数组（即，每行的一组单词对应于该行的不同数组/列表）。简而言之，我们想要的是每行的单词列表，而不是单词列表的列表。
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output will be as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How it works...
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We read a text file `A Tale of Two Cities by Charles Dickens` from [http://www.gutenberg.org/](http://www.gutenberg.org/) into
    an RDD and then proceed to tokenize the words by using whitespace as the separator
    in a lambda expression using `.split()` and `.flatmap()` of RDD itself. We then
    proceed to use the `.count()` method of RDDs to output the total number of words.
    While this is simple, you have to bear in mind that the operation takes place
    using the distributed parallel framework of Spark with only a couple of lines.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[http://www.gutenberg.org/](http://www.gutenberg.org/)读取查尔斯·狄更斯的《双城记》文本文件到一个RDD中，然后通过使用空格作为分隔符在lambda表达式中使用`.split()`和`.flatmap()`方法对RDD本身进行单词分词。然后，我们使用RDD的`.count()`方法输出单词总数。虽然这很简单，但您必须记住，该操作是在Spark的分布式并行框架中进行的，仅用了几行代码。
- en: There's more...
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Creating RDDs with external data sources, whether it is a text file, Hadoop
    HDFS, sequence file, Casandra, or Parquet file is remarkably simple. Once again,
    we use `SparkSession` (`SparkContext` prior to Spark 2.0) to get a handle to the
    cluster. Once the function (for example, textFile Protocol: file path) is executed,
    the data is broken into smaller pieces (partitions) and automatically flows to
    the cluster, which becomes available to the computations as fault-tolerant distributed
    collections that can be operated on in parallel.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用外部数据源创建RDD，无论是文本文件、Hadoop HDFS、序列文件、Casandra还是Parquet文件，都异常简单。再次，我们使用`SparkSession`（Spark
    2.0之前的`SparkContext`）来获取集群的句柄。一旦执行了函数（例如，textFile 协议：文件路径），数据就会被分解成更小的部分（分区），并自动流向集群，这些数据作为可以在并行操作中使用的容错分布式集合变得可用。
- en: There are a number of variations that one must consider when working with real-life
    situations. The best advice based on our own experience is to consult the documentation
    before writing your own functions or connectors. Spark either supports your data
    source right out of the box, or the vendor has a connector that can be downloaded
    to do the same.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在处理实际场景时，必须考虑多种变体。根据我们的经验，最好的建议是在编写自己的函数或连接器之前查阅文档。Spark要么直接支持您的数据源，要么供应商有一个可下载的连接器来实现相同功能。
- en: Another situation that we often see is many small files that are generated (usually
    within `HDFS` directories) that need to be parallelized as RDDs for consumption.
    `SparkContext` has a method named `wholeTextFiles()` which lets you read a directory
    containing multiple files and returns each of them as (filename, content) key-value
    pairs. We found this to be very useful in multi-stage machine learning situations
    using lambda architecture, where the model parameters are calculated as a batch
    and then updated in Spark every day.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们经常遇到的另一种情况是，许多小文件（通常在`HDFS`目录中生成）需要并行化为RDD以供消费。`SparkContext`有一个名为`wholeTextFiles()`的方法，它允许您读取包含多个文件的目录，并将每个文件作为(文件名,
    内容)键值对返回。我们发现这在使用lambda架构的多阶段机器学习场景中非常有用，其中模型参数作为批处理计算，然后每天在Spark中更新。
- en: In this example, we read multiple files and then print the first file for examination.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们读取多个文件，然后打印第一个文件以供检查。
- en: 'The `spark.sparkContext.wholeTextFiles()` function is used to read a large
    number of small files and present them as (K,V), or key-value:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.sparkContext.wholeTextFiles()`函数用于读取大量小文件，并将它们呈现为(K,V)，即键值对：'
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码后，您将得到以下输出：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: See also
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'Spark documentation for the `textFile()` and `wholeTextFiles()` functions:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Spark文档中关于`textFile()`和`wholeTextFiles()`函数的说明：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)'
- en: The `textFile()` API is a single abstraction for interfacing to external data
    sources. The formulation of protocol/path is enough to invoke the right decoder.
    We'll demonstrate reading from an ASCII text file, Amazon AWS S3, and HDFS with
    code snippets that the user would leverage to build their own system.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`textFile()`API是与外部数据源接口的单一抽象。协议/路径的制定足以调用正确的解码器。我们将演示从ASCII文本文件、Amazon AWS
    S3和HDFS读取，用户可以利用这些代码片段来构建自己的系统。'
- en: The path can be expressed as a simple path (for example, local text file) to
    a complete URI with the required protocol (for example, s3n for AWS storage buckets)
    to complete resource path with server and port configuration (for example, to
    read HDFS file from a Hadoop cluster). ...
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路径可以表示为简单路径（例如，本地文本文件）到完整的URI，包含所需协议（例如，s3n用于AWS存储桶），直至具有服务器和端口配置的完整资源路径（例如，从Hadoop集群读取HDFS文件）。...
- en: Transforming RDDs with Spark 2.0 using the filter() API
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0的filter() API转换RDD
- en: In this recipe, we explore the `filter()` method of RDD which is used to select
    a subset of the base RDD and return a new filtered RDD. The format is similar
    to `map()`, but a lambda function selects which members are to be included in
    the resulting RDD.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们探讨了RDD的`filter()`方法，该方法用于选择基础RDD的子集并返回新的过滤RDD。格式类似于`map()`，但lambda函数决定哪些成员应包含在结果RDD中。
- en: How to do it...
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Import the necessary packages:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于设置`log4j`日志级别的包。此步骤可选，但我们强烈建议执行（根据开发周期调整级别）。
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误，以减少输出。请参阅上一步骤了解包要求。
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The `filter()` API is demonstrated using several examples. In the first example
    we went through an RDD and output odd numbers by using a lambda expression `.filter
    ( i => (i%2) == 1)` which takes advantage of the mod (modulus) function.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter()` API通过几个示例进行了演示。在第一个示例中，我们遍历了一个RDD，并通过使用lambda表达式`.filter(i => (i%2)
    == 1)`输出了奇数，该表达式利用了模（取模）函数。'
- en: In the second example we made it a bit interesting by mapping the result to
    a square function using a lambda expression `num.map(pow(_,2)).filter(_ %2 ==
    1)`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个示例中，我们通过使用lambda表达式`num.map(pow(_,2)).filter(_ %2 == 1)`将结果映射到平方函数，使其变得更有趣。
- en: In the third example, we went through the text and filtered out short lines
    (for example, lines under 30 character) using the lambda expression `.filter(_.length
    < 30).filter(_.length > 0)` to print short versus total number of lines (`.count()`
    ) as output.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个示例中，我们遍历文本并使用lambda表达式`.filter(_.length < 30).filter(_.length > 0)`过滤掉短行（例如，长度小于30个字符的行），以打印短行与总行数的对比（`.count()`）作为输出。
- en: There's more...
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The `filter()` API walks through the parallelized distributed collection (that
    is, RDDs) and applies the selection criteria supplied to `filter()` as a lambda
    in order to include or exclude the element from the resulting RDD. The combination
    uses `map()`, which transforms each element and `filter()`, which selects a subset
    is a powerful combination in Spark ML programming.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter()` API遍历并行分布式集合（即RDD），并应用作为lambda提供给`filter()`的选择标准，以便将元素包含或排除在结果RDD中。结合使用`map()`（转换每个元素）和`filter()`（选择子集），在Spark
    ML编程中形成强大组合。'
- en: We will see later with the `DataFrame` API how a similar `Filter()` API can
    be used to achieve the same effect using a higher-level framework used in R and
    Python (pandas).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后我们将通过`DataFrame` API看到，如何使用类似`Filter()` API在R和Python（pandas）中使用的高级框架实现相同效果。
- en: See also
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `.filter()`, which is a method call of RDD, is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.JavaRDD).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.filter()`方法的文档，作为RDD的方法调用，可访问[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.JavaRDD)。'
- en: Documentation for `BloomFilter()`--for the sake of completeness, be aware that
    there is also a bloom filter function already in existence and it is suggested
    that you avoid coding by yourselves. The link for this same is [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.util.sketch.BloomFilter](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.util.sketch.BloomFilter).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于`BloomFilter()`的文档——为了完整性，请注意已存在一个布隆过滤器函数，建议您避免自行编码。相关链接为[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.util.sketch.BloomFilter](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.util.sketch.BloomFilter)。
- en: Transforming RDDs with the super useful flatMap() API
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用极其有用的flatMap() API转换RDD
- en: In this recipe, we examine the `flatMap()` method which is often a source of
    confusion for beginners; however, on closer examination we demonstrate that it
    is a clear concept that applies the lambda function to each element just like
    map, and then flattens the resulting RDD as a single structure (rather than having
    a list of lists, we create a single list made of all sublist with sublist elements).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了常令初学者困惑的`flatMap()`方法；然而，通过深入分析，我们展示了它是一个清晰的概念，它像map一样将lambda函数应用于每个元素，然后将结果RDD扁平化为单一结构（不再是列表的列表，而是由所有子列表元素构成的单一列表）。
- en: How to do it...
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含了必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Import the necessary packages
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入设置`log4j`日志级别的包。此步骤可选，但我们强烈建议执行（根据开发周期调整级别）。
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误，以减少输出。请参阅上一步骤了解包需求。
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark能够运行。
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We use `textFile()` function to create the initial (that is, base RDD) from
    our text file that we downloaded earlier from [http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt):'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`textFile()`函数从之前下载的文本文件创建初始（即基础RDD）：[http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt)。
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We apply the map function to the RDDs to demonstrate the `map()` function transformation.
    To start with, we are doing it the wrong way to make a point: we first attempt
    to separate all the words based on the regular expression *[\s\W]+]* using just
    `map()` to demonstrate that the resulting RDD is a list of lists in which each
    list corresponds to a line and the tokenized word within that line. This example
    demonstrates what could cause confusion for beginners when using `flatMap()`.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对RDD应用map函数以展示`map()`函数的转换。首先，我们错误地尝试仅使用`map()`根据正则表达式*[\s\W]+]*分离所有单词，以说明结果RDD是列表的列表，其中每个列表对应一行及其内的分词单词。此例展示了初学者在使用`flatMap()`时可能遇到的困惑。
- en: The following line trims each line and then splits the line into words. The
    resulting RDD (that is, wordRDD2) will be a list of lists of words rather than
    a single list of words for the whole file.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码行修剪每行并将其分割成单词。结果RDD（即wordRDD2）将是单词列表的列表，而不是整个文件的单一单词列表。
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: On running the previous code, you will get the following output.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出。
- en: '[PRE26]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We use the `flatMap()` method to not only map, but also flatten the list of
    lists so we end up with an RDD which is made of words themselves. We trim and
    split the words (that is, tokenize) and then filter for words greater than zero
    and then map it to upper case.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`flatMap()`方法不仅进行映射，还扁平化列表的列表，最终得到由单词本身构成的RDD。我们修剪并分割单词（即分词），然后筛选出长度大于零的单词，并将其映射为大写。
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In this case, after flattening the list using `flatMap()`, we can get a list
    of the words back as expected.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在此情况下，使用`flatMap()`扁平化列表后，我们能如预期般取回单词列表。
- en: '[PRE28]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How it works...
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this short example, we read a text file and then split the words (that is,
    tokenize it) using the `flatMap(_.trim.split("""[\s\W]+""")` lambda expression
    to have a single RDD with the tokenized content. Additionally we use the `filter
    ()` API `filter(_.length > 0)` to exclude the empty lines and the lambda expression
    `.map(_.toUpperCase())` in a `.map()` API to map to uppercase before outputting
    the results.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简短的示例中，我们读取了一个文本文件，然后使用`flatMap(_.trim.split("""[\s\W]+""")` lambda表达式对单词进行分割（即，令牌化），以获得一个包含令牌化内容的单一RDD。此外，我们使用`filter()`
    API `filter(_.length > 0)`来排除空行，并在输出结果之前使用`.map()` API中的lambda表达式`.map(_.toUpperCase())`映射为大写。
- en: There are cases where we do not want to get a list back for every element of
    base RDD (for example, get a list for words corresponding to a line). We sometimes
    prefer to have a single flattened list that is flat and corresponds to every word
    in the document. In short, rather than a list of lists, we want a single list
    containing ...
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们不希望为基RDD的每个元素返回一个列表（例如，为对应于一行的单词获取一个列表）。有时我们更倾向于拥有一个单一的扁平列表，该列表对应于文档中的每个单词。简而言之，我们不想要一个列表的列表，而是想要一个包含...的单一列表。
- en: There's more...
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The function `glom()` is a function that lets you model each partition in the
    RDD as an array rather than a row list. While it is possible to produce the results
    in most cases, `glom()` allows you to reduce the shuffling between partitions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`glom()`函数允许你将RDD中的每个分区建模为数组，而不是行列表。虽然在大多数情况下可以产生结果，但`glom()`允许你减少分区之间的数据移动。'
- en: While at the surface, both method 1 and 2 mentioned in the text below look similar
    for calculating the minimum numbers in an RDD, the `glom()` function will cause
    much less data shuffling across the network by first applying `min()` to all the
    partitions, and then sending over the resulting data. The best way to see the
    difference is to use this on 10M+ RDDs and watch the IO and CPU usage accordingly.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在表面上，文本中提到的第一种和第二种方法在计算RDD中的最小数时看起来相似，但`glom()`函数将通过首先对所有分区应用`min()`，然后发送结果数据，从而在网络上引起更少的数据移动。要看到差异的最佳方式是在10M+
    RDD上使用此方法，并相应地观察IO和CPU使用情况。
- en: 'The first method is to find the minimum value without using `glom()`:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种方法是在不使用`glom()`的情况下找到最小值：
- en: '[PRE30]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'On running the preceding code, you will get the following output:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，你将得到以下输出：
- en: '[PRE31]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The second method is to find the minimum value using `glom(`, which causes a
    local application of the min function to a partition and then sends the results
    across via a shuffle.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种方法是通过使用`glom()`来找到最小值，这会导致对一个分区进行本地应用的最小函数，然后通过shuffle发送结果。
- en: '[PRE32]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'On running the preceding code, you will get the following output:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，你将得到以下输出：
- en: '[PRE33]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: See also
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: Documentation for `flatMap()`, `PairFlatMap()`, and other variations under RDD
    is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flatMap()`、`PairFlatMap()`及其他RDD下的变体的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)找到。'
- en: Documentation for the `FlatMap()` function under RDD is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD下`FlatMap()`函数的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction)找到。
- en: Documentation for the `PairFlatMap()` function - very handy variation for paired
    data elements is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.function.PairFlatMapFunction](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.function.PairFlatMapFunction)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PairFlatMap()`函数的文档——针对成对数据元素的非常便捷的变体，可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.function.PairFlatMapFunction](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.function.PairFlatMapFunction)找到。'
- en: The `flatMap()` method applies the supplied function (lambda or named function
    via def) to every element, flattens the structure, and produces a new RDD.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flatMap()`方法将提供的函数（lambda表达式或通过def定义的命名函数）应用于每个元素，展平结构，并生成一个新的RDD。'
- en: Transforming RDDs with set operation APIs
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集合操作API转换RDD
- en: In this recipe, we explore set operations on RDDs, such as `intersection()`,
    `union()`, `subtract(),` and `distinct()` and `Cartesian()`. Let's implement the
    usual set operations in a distributed manner.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们探索了RDD上的集合操作，如`intersection()`、`union()`、`subtract()`、`distinct()`和`Cartesian()`。让我们以分布式方式实现常规集合操作。
- en: How to do it...
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置
- en: '[PRE34]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Import the necessary packages
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包
- en: '[PRE35]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于设置`log4j`日志级别的包。此步骤是可选的，但我们强烈建议您（根据开发周期适当更改级别）。
- en: '[PRE36]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误，以减少输出。请参阅上一步骤了解包要求。
- en: '[PRE37]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: How it works...
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this example, we started with three sets of number Arrays (odd, even, and
    their combo) and then proceeded to pass them as parameters into the set operation
    API. We covered how to use `intersection()`, `union()`, `subtract()`, `distinct()`,
    and `cartesian()` RDD operators.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们以三组数字数组（奇数、偶数及其组合）开始，然后将它们作为参数传递给集合操作API。我们介绍了如何使用`intersection()`、`union()`、`subtract()`、`distinct()`和`cartesian()`
    RDD操作符。
- en: See also
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: While the RDD set operators are easy to use, one must be careful with the data
    shuffling that Spark has to perform in the background to complete some of these
    operations (for example, intersection).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RDD集合操作符易于使用，但必须注意Spark在后台为完成某些操作（例如，交集）而必须进行的数据洗牌。
- en: It is worth noting that the union operator does not remove duplicates from the
    resulting RDD set.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，union操作符不会从结果RDD集合中删除重复项。
- en: RDD transformation/aggregation with groupBy() and reduceByKey()
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD转换/聚合与`groupBy()`和`reduceByKey()`
- en: In this recipe, we explore the `groupBy()` and `reduceBy()` methods, which allow
    us to group values corresponding to a key. It is an expensive operation due to
    internal shuffling. We first demonstrate `groupby()` in more detail and then cover
    `reduceBy()` to show the similarity in coding these while stressing the advantage
    of the `reduceBy()` operator.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们探讨了`groupBy()`和`reduceBy()`方法，这些方法允许我们根据键对值进行分组。由于内部洗牌，这是一个昂贵的操作。我们首先更详细地演示`groupby()`，然后介绍`reduceBy()`，以展示编写这些代码时的相似性，同时强调`reduceBy()`操作符的优势。
- en: How to do it...
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE38]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Import the necessary packages:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE39]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle):'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于设置`log4j`日志级别的包。此步骤是可选的，但我们强烈建议您（根据开发周期适当更改级别）：
- en: '[PRE40]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误，以减少输出。请参阅上一步骤了解包要求。
- en: '[PRE41]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: How it works...
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this example, we created numbers one through twelve and placed them in three
    partitions. We then proceeded to break them into odd/even using a simple mod operation
    while. The `groupBy()` is used to aggregate them into two groups of odd/even.
    This is a typical aggregation problem that should look familiar to SQL users.
    Later in this chapter, we revisit this operation using `DataFrame` which also
    takes advantage of the better optimization techniques provided by the SparkSQL
    engine. In the later part, we demonstrate the similarity of `groupBy()` and `reduceByKey()`.
    We set up an array of alphabets (that is, `a` and `b`) and then convert them into
    RDD. We then proceed to aggregate them based on key (that is, unique letters -
    only two in this case) and print the total in each group.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们创建了数字一到十二，并将它们放置在三个分区中。然后，我们继续使用简单的模运算将它们分解为奇数/偶数。`groupBy()`用于将它们聚合为两个奇数/偶数组。这是一个典型的聚合问题，对于SQL用户来说应该很熟悉。在本章后面，我们将使用`DataFrame`重新审视此操作，`DataFrame`也利用了SparkSQL引擎提供的更好的优化技术。在后面的部分，我们展示了`groupBy()`和`reduceByKey()`的相似性。我们设置了一个字母数组（即，`a`和`b`），然后将它们转换为RDD。然后，我们根据键（即，唯一的字母
    - 在本例中只有两个）进行聚合，并打印每个组的总数。
- en: There's more...
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Given the direction for Spark which favors the Dataset/DataFrame paradigm over
    low-level RDD coding, one must seriously consider the reasoning for doing `groupBy()`
    on an RDD. While there are legitimate situations for which the operation is needed,
    the readers are advised to reformulate their solution to take advantage of the
    SparkSQL subsystem and its optimizer called **Catalyst**.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于Spark的发展方向，它更倾向于Dataset/DataFrame范式而不是低级RDD编码，因此必须认真考虑在RDD上执行`groupBy()`的原因。虽然有些情况下确实需要此操作，但建议读者重新制定解决方案，以利用SparkSQL子系统和称为**Catalyst**的优化器。
- en: The Catalyst optimizer takes into account Scala's powerful features such as
    **pattern matching** and **quasiquotes** while building an optimized query plan.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst优化器在构建优化查询计划时考虑了Scala的强大功能，如**模式匹配**和**准引用**。
- en: The documentation on Scala pattern matching is available at [http://docs.scala-lang.org/tutorials/tour/pattern-matching.html](http://docs.scala-lang.org/tutorials/tour/pattern-matching.html)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关Scala模式匹配的文档可在[http://docs.scala-lang.org/tutorials/tour/pattern-matching.html](http://docs.scala-lang.org/tutorials/tour/pattern-matching.html)找到
- en: The documentation on Scala quasiquotes is available at [http://docs.scala-lang.org/overviews/quasiquotes/intro.html
    ...](http://docs.scala-lang.org/overviews/quasiquotes/intro.html)
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关Scala准引用的文档可在[http://docs.scala-lang.org/overviews/quasiquotes/intro.html](http://docs.scala-lang.org/overviews/quasiquotes/intro.html)找到
- en: See also
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Documentation for `groupBy()` and `reduceByKey()` operations under RDD:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: RDD下的`groupBy()`和`reduceByKey()`操作文档：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)'
- en: Transforming RDDs with the zip() API
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用zip() API转换RDD
- en: In this recipe we explore the `zip()` function. For those of us working in Python
    or Scala, `zip()` is a familiar method that lets you pair items before applying
    an inline function. Using Spark, it can be used to facilitate RDD arithmetic between
    pairs. Conceptually, it combines the two RDDs in such a way that each member of
    one RDD is paired with the second RDD that occupies the same position (that is,
    it lines up the two RDDs and makes pairs out of the members).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们探讨了`zip()`函数。对于我们这些在Python或Scala中工作的人来说，`zip()`是一个熟悉的方法，它允许你在应用内联函数之前配对项目。使用Spark，它可以用来促进成对RDD之间的算术运算。从概念上讲，它以这样的方式组合两个RDD，即一个RDD的每个成员与第二个RDD中占据相同位置的成员配对（即，它对齐两个RDD并从成员中制作配对）。
- en: How to do it...
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置
- en: '[PRE42]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Import the necessary packages
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包
- en: '[PRE43]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于设置`log4j`日志级别的包。此步骤是可选的，但我们强烈建议这样做（根据开发周期适当更改级别）。
- en: '[PRE44]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误，以减少输出。请参阅上一步骤了解包要求。
- en: '[PRE45]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark能够运行。
- en: '[PRE46]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Set up the data structures and RDD for the example. In this example we create
    two RDDs from `Array[]` and let Spark decide on the number of partitions (that
    is, the second parameter in the `parallize()` method is not set).
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置示例的数据结构和RDD。在本例中，我们创建了两个从`Array[]`生成的RDD，并让Spark决定分区数量（即，`parallize()`方法中的第二个参数未设置）。
- en: '[PRE47]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We apply the `zip()` function to the RDDs to demonstrate the transformation.
    In the example, we take the partitioned RDD of ranges and label them as odd/even
    using the mod function. We use the `zip()` function to pair elements from the
    two RDDs (SignalNoiseRDD and SignalStrengthRDD) so we can apply a `map()` function
    and compute their ratio (noise to signal ratio). We can use this technique to
    perform almost all types of arithmetic or non-arithmetic operations involving
    individual members of two RDDs.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对RDD应用`zip()`函数以演示转换。在示例中，我们取分区RDD的范围，并使用模函数将其标记为奇数/偶数。我们使用`zip()`函数将来自两个RDD（SignalNoiseRDD和SignalStrengthRDD）的元素配对，以便我们可以应用`map()`函数并计算它们的比率（噪声与信号比率）。我们可以使用此技术执行几乎所有类型的算术或非算术操作，涉及两个RDD的单个成员。
- en: The pairing of two RDD members act as a tuple or a row. The individual members
    of the pair created by `zip()` can be accessed by their position (for example,
    `._1` and `._2`)
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个RDD成员的配对行为类似于元组或行。通过`zip()`创建的配对中的单个成员可以通过其位置访问（例如，`._1`和`._2`）
- en: '[PRE48]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码后，您将得到以下输出：
- en: '[PRE49]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: How it works...
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'In this example, we first set up two arrays representing signal noise and signal
    strength. They are simply a set of measured numbers that we could have received
    from the IoT platform. We then proceeded to pair the two separate arrays so each
    member looks like they have been input originally as a pair of (x, y). We then
    proceed to divide the pair and produce the noise to signal ratio using the following
    code snippet:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们首先设置两个数组，分别代表信号噪声和信号强度。它们只是一系列测量数字，我们可以从物联网平台接收这些数字。然后，我们将两个独立的数组配对，使得每个成员看起来像是原始输入的一对（x,
    y）。接着，我们通过以下代码片段将配对分割并计算噪声与信号的比率：
- en: '[PRE50]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The `zip()` method has many variations that involve partitions. The developers
    should familiarize themselves with variations of the `zip()` method with partition
    (for example, `zipPartitions`).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`zip()`方法有许多涉及分区的变体。开发者应熟悉带有分区的`zip()`方法的变体（例如，`zipPartitions`）。'
- en: See also
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `zip()` and `zipPartitions()` operations under RDD is available
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD下的`zip()`和`zipPartitions()`操作的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)找到。
- en: Join transformation with paired key-value RDDs
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用配对键值RDD的连接转换
- en: In this recipe, we introduce the `KeyValueRDD` pair RDD and the supporting join
    operations such as `join()`, `leftOuterJoin` and `rightOuterJoin()`, and `fullOuterJoin()`
    as an alternative to the more traditional and more expensive set operations available
    via the set operation API, such as `intersection()`, `union()`, `subtraction()`,
    `distinct()`, `cartesian()`, and so on.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们介绍了`KeyValueRDD`对RDD及其支持的连接操作，如`join()`、`leftOuterJoin`、`rightOuterJoin()`和`fullOuterJoin()`，作为通过集合操作API提供的更传统且更昂贵的集合操作（如`intersection()`、`union()`、`subtraction()`、`distinct()`、`cartesian()`等）的替代方案。
- en: We'll demonstrate `join()`, `leftOuterJoin` and `rightOuterJoin()`, and `fullOuterJoin()`,
    to explain the power and flexibility of key-value pair RDDs.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将演示`join()`、`leftOuterJoin`、`rightOuterJoin()`和`fullOuterJoin()`，以解释键值对RDD的强大功能和灵活性。
- en: '[PRE51]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: How to do it...
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Set up the data structures and RDD for the example:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置示例的数据结构和RDD：
- en: '[PRE52]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Turn the List into RDDs:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将列表转换为RDD：
- en: '[PRE53]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We can access the `keys` and `values` inside a pair RDD.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以访问配对RDD中的`键`和`值`。
- en: '[PRE54]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We apply the `mapValues()` function to the pair RDDs to demonstrate the transformation.
    In this example we use the map function to lift up the value by adding 100 to
    every element. This is a popular technique to introduce noise to the data (that
    is, jittering).
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对配对RDD应用`mapValues()`函数来演示这一转换。在此示例中，我们使用map函数将值提升，为每个元素增加100。这是一种向数据引入噪声（即抖动）的流行技术。
- en: '[PRE55]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE56]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We apply the `join()` function to the RDDs to demonstrate the transformation.
    We use `join()` to join the two RDDs. We join the two RDDs based on keys (that
    is, north, south, and so on).
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对RDD应用`join()`函数来演示这一转换。我们使用`join()`来连接两个RDD。我们基于键（即北、南等）连接两个RDD。
- en: '[PRE57]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE58]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: We apply the `leftOuterJoin()` function to the RDDs to demonstrate the transformation.
    The `leftOuterjoin` acts like a relational left outer join. Spark replaces the
    absence of a membership with `None` rather than `NULL`, which is common in relational
    systems.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对RDD应用`leftOuterJoin()`函数来演示这一转换。`leftOuterjoin`的作用类似于关系左外连接。Spark用`None`替换成员资格的缺失，而不是`NULL`，这在关系系统中很常见。
- en: '[PRE59]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE60]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We'll apply `rightOuterJoin()` to the RDDs to demonstrate the transformation.
    This is similar to a right outer join in relational systems.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将对RDD应用`rightOuterJoin()`来演示这一转换。这与关系系统中的右外连接类似。
- en: '[PRE61]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE62]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: We then apply the `fullOuterJoin()` function to the RDDs to demonstrate the
    transformation. This is similar to full outer join in relational systems.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们对RDD应用`fullOuterJoin()`函数来演示这一转换。这与关系系统中的全外连接类似。
- en: '[PRE63]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将得到以下输出：
- en: '[PRE64]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: How it works...
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we declared three lists representing typical data available
    in relational tables, which could be imported using a connector to Casandra or
    RedShift (not shown here to simplify the recipe). We used two of the three lists
    representing city names (that is, data tables) and joined them with the first
    list, which represents directions (for example, defining tables). The first step
    is to define three lists of paired values. We then parallelized them into key-value
    RDDs so we can perform join operations between the first RDD (that is, directions)
    and the other two RDDs representing city names. We applied the join function to
    the RDDs to demonstrate the transformation.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们声明了三个列表，代表关系表中可用的典型数据，这些数据可通过连接器导入Casandra或RedShift（为简化本食谱，此处未展示）。我们使用了三个列表中的两个来表示城市名称（即数据表），并将它们与第一个列表连接，该列表代表方向（例如，定义表）。第一步是定义三个配对值的列表。然后我们将它们并行化为键值RDD，以便我们可以在第一个RDD（即方向）和其他两个代表城市名称的RDD之间执行连接操作。我们对RDD应用了join函数来演示这一转换。
- en: We demonstrated `join()`, `leftOuterJoin` and `rightOuterJoin() ...`
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们演示了`join()`、`leftOuterJoin`和`rightOuterJoin()`...
- en: There's more...
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Documentation for `join()` and its variations under RDD is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: RDD下`join()`及其变体的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)找到。
- en: Reduce and grouping transformation with paired key-value RDDs
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配对键值RDD的reduce和分组转换
- en: In this recipe, we explore reduce and group by key. The `reduceByKey()` and
    `groupbyKey()` operations are much more efficient and preferred to `reduce()`
    and `groupBy()` in most cases. The functions provide convenient facilities to
    aggregate values and combine them by key with less shuffling, which is problematic
    on large data sets.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们探讨了reduce和按key分组。`reduceByKey()`和`groupbyKey()`操作在大多数情况下比`reduce()`和`groupBy()`更高效且更受青睐。这些函数提供了便捷的设施，通过减少洗牌来聚合值并按key组合它们，这在大型数据集上是一个问题。
- en: How to do it...
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含了必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置
- en: '[PRE65]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Import the necessary packages
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包
- en: '[PRE66]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于设置`log4j`日志级别的包。此步骤可选，但我们强烈建议执行（根据开发周期调整级别）。
- en: '[PRE67]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirement:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误以减少输出。请参阅前一步骤了解包要求：
- en: '[PRE68]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark能够运行。
- en: '[PRE69]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Set up the data structures and RDD for the example:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置示例所需的数据结构和RDD：
- en: '[PRE70]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: We apply `groupByKey()` to demonstrate the transformation. In this example,
    we group all the buy and sell signals together while operating in a distributed
    setting.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应用`groupByKey()`以演示转换。在此示例中，我们在分布式环境中将所有买卖信号分组在一起。
- en: '[PRE71]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码，您将得到以下输出：
- en: '[PRE72]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: We apply the `reduceByKey()` function to the pair of RDDs to demonstrate the
    transformation. In this example, the function is, to sum up the total volume for
    the buy and sell signals. The Scala notation of `(_+_)` simply denotes adding
    two members at the time and producing a single result from it. Just like `reduce()`,
    we can apply any function (that is, inline for simple functions and named functions
    for more complex cases).
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对RDD对应用`reduceByKey()`函数以演示转换。在此示例中，该函数用于计算买卖信号的总成交量。Scala符号`(_+_)`简单表示每次添加两个成员并从中产生单个结果。就像`reduce()`一样，我们可以应用任何函数（即简单函数的内联和更复杂情况下的命名函数）。
- en: '[PRE73]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码，您将得到以下输出：
- en: '[PRE74]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: How it works...
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this example we declared a list of items as being sold or purchased and their
    corresponding price (that is, typical commercial transaction). We then proceeded
    to calculate the sum using Scala shorthand notation `(_+_)`. In the last step,
    we provided the total for each key group (that is, `Buy` or `Sell`). The key-value
    RDD is a powerful construct that can reduce coding while providing the functionality
    needed to group paired values into aggregated buckets. The `groupByKey()` and
    `reduceByKey()` functions mimic the same aggregation functionality, while `reduceByKey()`
    is more efficient due to less shuffling of the data while final results are being
    assembled.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们声明了一个商品买卖清单及其对应价格（即典型的商业交易）。然后，我们使用Scala简写符号`(_+_)`计算总和。最后一步，我们为每个键组（即`Buy`或`Sell`）提供了总计。键值RDD是一个强大的结构，可以在减少代码量的同时提供所需的聚合功能，将配对值分组到聚合桶中。`groupByKey()`和`reduceByKey()`函数模拟了相同的聚合功能，而`reduceByKey()`由于在组装最终结果时数据移动较少，因此更高效。
- en: See also
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for `groupByKey()` and `reduceByKey()` operations under RDD is
    available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 有关RDD下的`groupByKey()`和`reduceByKey()`操作的文档，请访问[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)。
- en: Creating DataFrames from Scala data structures
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Scala数据结构创建DataFrames
- en: In this recipe, we explore the `DataFrame` API, which provides a higher level
    of abstraction than RDDs for working with data. The API is similar to R and Python
    data frame facilities (pandas).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了`DataFrame` API，它为处理数据提供了比RDD更高的抽象层次。该API类似于R和Python数据帧工具（pandas）。
- en: '`DataFrame` simplifies coding and lets you use standard SQL to retrieve and
    manipulate data. Spark keeps additional information about DataFrames, which helps
    the API to manipulate the frames with ease. Every `DataFrame` will have a schema
    (either inferred from data or explicitly defined) which allows us to view the
    frame like an SQL table. The secret sauce of SparkSQL and DataFrame is that the
    catalyst optimizer will work behind the scenes to optimize access by rearranging
    calls in the pipeline.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame`简化了编码，并允许您使用标准SQL检索和操作数据。Spark保留了关于DataFrames的额外信息，这有助于API轻松操作框架。每个`DataFrame`都将有一个模式（从数据推断或显式定义），允许我们像查看SQL表一样查看框架。SparkSQL和DataFrame的秘诀在于催化优化器将在幕后工作，通过重新排列管道中的调用来优化访问。'
- en: How to do it...
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在包的位置：
- en: '[PRE75]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Set up the imports related to DataFrames and the required data structures and
    create the RDDs as needed for the example:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置与DataFrames相关的导入以及所需的数据结构，并根据示例需要创建RDD：
- en: '[PRE76]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为`log4j`设置日志级别导入所需的包。此步骤可选，但我们强烈建议执行（根据开发周期调整级别）。
- en: '[PRE77]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirement.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误，以减少输出。有关包要求的详细信息，请参阅前一步骤。
- en: '[PRE78]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行。
- en: '[PRE79]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'We set up the Scala data structures as two `List()` objects and a sequence
    (that is, `Seq()`). We then proceed to turn the `List` structures into RDDs for
    conversion to `DataFrames` for the next steps:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了两个`List()`对象和一个序列（即`Seq()`）的Scala数据结构。然后，我们将`List`结构转换为RDD，以便转换为`DataFrames`进行后续步骤：
- en: '[PRE80]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: We take a list which is turned into an RDD using the `parallelize()` method
    and use the `toDF()` method of the RDD to turn it into a DataFrame. The `show()`
    method allows us to view the DataFrame, which is similar to a SQL table.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们取一个列表，使用`parallelize()`方法将其转换为RDD，并使用RDD的`toDF()`方法将其转换为DataFrame。`show()`方法允许我们查看类似于SQL表的DataFrame。
- en: '[PRE81]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'On running the previous code, you will get the following output.:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE82]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: In the following code snippet, we take a generic Scala **Seq** (**Sequence**)
    data structure and use `createDataFrame()` explicitly to create a DataFrame while
    naming the columns at the same time.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下代码片段中，我们取一个通用的Scala **Seq**（**序列**）数据结构，并使用`createDataFrame()`显式创建一个DataFrame，同时命名列。
- en: '[PRE83]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: In the next two steps, we use the `show()` method to see the contents and then
    proceed to use `printscheme()` to show the inferred scheme based on types. In
    this example, the DataFrame correctly identified the integer and double in the
    Seq as the valid type for the two columns of numbers.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的两个步骤中，我们使用`show()`方法查看内容，然后使用`printSchema()`方法显示基于类型的推断方案。在此示例中，DataFrame正确识别了Seq中的整数和双精度数作为两个数字列的有效类型。
- en: '[PRE84]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，您将获得以下输出：
- en: '[PRE85]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: How it works...
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we took two lists and a Seq data structure and converted them
    to DataFrame and used `df1.show()` and `df1.printSchema()` to display contents
    and schema for the table.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们取两个列表和一个Seq数据结构，将它们转换为DataFrame，并使用`df1.show()`和`df1.printSchema()`显示表的内容和模式。
- en: DataFrames can be created from both internal and external sources. Just like
    SQL tables, the DataFrames have schemas associated with them that can either be
    inferred or explicitly defined using Scala case classes or the `map()` function
    to explicitly convert while ingesting the data.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames可以从内部和外部源创建。与SQL表类似，DataFrames具有与之关联的模式，这些模式可以被推断或使用Scala case类或`map()`函数显式转换，同时摄取数据。
- en: There's more...
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'To ensure completeness, we include the `import` statement that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保完整性，我们包含了在Spark 2.0.0之前使用的`import`语句以运行代码（即，Spark 1.5.2）：
- en: '[PRE86]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: See also
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame文档可在此处找到：[https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)。
- en: If you see any issues with implicit conversion, double check to make sure you
    have included the implicit import statement.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到隐式转换问题，请确保已包含隐式导入语句。
- en: 'Example code for Spark 2.0:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 示例代码适用于Spark 2.0：
- en: '[PRE87]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Operating on DataFrames programmatically without SQL
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以编程方式操作DataFrames，无需SQL
- en: In this recipe, we explore how to manipulate DataFrame with code and method
    calls only (without SQL). The DataFrames have their own methods that allow you
    to perform SQL-like operations using a programmatic approach. We demonstrate some
    of these commands such as `select()`, `show()`, and `explain()` to get the point
    across that the DataFrame itself is capable of wrangling and manipulating the
    data without using SQL.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们探索如何仅通过代码和方法调用（不使用SQL）来操作数据框。数据框拥有自己的方法，允许您使用编程方式执行类似SQL的操作。我们展示了一些命令，如`select()`、`show()`和`explain()`，以说明数据框本身能够不使用SQL进行数据整理和操作。
- en: How to do it...
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置。
- en: '[PRE88]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Set up the imports related to DataFrames and the required data structures and
    create the RDDs as needed for the example
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置与数据框相关的导入以及所需的数据结构，并根据示例需要创建RDD。
- en: '[PRE89]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入设置`log4j`日志级别的包。此步骤可选，但我们强烈建议执行（根据开发周期调整级别）。
- en: '[PRE90]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirement.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和错误，以减少输出。请参阅上一步骤了解包要求。
- en: '[PRE91]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: How it works...
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this example, we loaded data from a text file into an RDD and then converted
    it to a DataFrame structure using the `.toDF()` API. We then proceeded to mimic
    SQL queries using built-in methods such as `select()`, `filter()`, `show()`, and
    `explain()` that help us to programmatically explore the data (no SQL). The `explain()`
    command shows the query plan which can be awfully useful to remove the bottleneck.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们从文本文件加载数据到RDD，然后使用`.toDF()`API将其转换为数据框结构。接着，我们使用内置方法如`select()`、`filter()`、`show()`和`explain()`来模拟SQL查询，以编程方式探索数据（无需SQL）。`explain()`命令显示查询计划，这对于消除瓶颈非常有用。
- en: DataFrames provide multiple approaches to data wrangling.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框提供了多种数据整理方法。
- en: For those comfortable with the DataFrame API and packages from R ([https://cran.r-project.org](https://cran.r-project.org))
    like dplyr or an older version, we have a programmatic API with an extensive set
    of methods that lets you do all your data wrangling via the API.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 对于熟悉数据框API和R语言包（如[https://cran.r-project.org](https://cran.r-project.org)的dplyr或旧版本）的用户，我们提供了一个具有丰富方法集的编程API，让您可以通过API进行所有数据整理。
- en: For those more comfortable with SQL, you can simply use SQL to retrieve and
    manipulate data as if you were using Squirrel or Toad to query the database.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更熟悉SQL的用户，您可以简单地使用SQL来检索和操作数据，就像使用Squirrel或Toad查询数据库一样。
- en: There's more...
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'To ensure completeness, we include the `import` statements that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保完整性，我们包含了在Spark 2.0.0之前运行代码（即Spark 1.5.2）所需的`import`语句。
- en: '[PRE92]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: See also
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框的文档可在[https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)获取。
- en: If you see any issues with implicit conversion, double check to make sure you
    have included the implicits `import` statement.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到隐式转换问题，请再次检查以确保您已包含隐式`import`语句。
- en: 'Example `import` statement for Spark 2.0:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0的示例`import`语句：
- en: '[PRE93]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Loading DataFrames and setup from an external source
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从外部源加载数据框并进行设置
- en: In this recipe, we examine data manipulation using SQL. Spark's approach to
    provide, both a pragmatic and SQL interface works very well in production settings
    in which we not only require machine learning, but also access to existing data
    sources using SQL to ensure compatibility and familiarity with existing SQL-based
    systems. DataFrame with SQL makes for an elegant process toward integration in
    real-life settings.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们探讨使用SQL进行数据操作。Spark提供实用且兼容SQL的接口，在生产环境中表现出色，我们不仅需要机器学习，还需要使用SQL访问现有数据源，以确保与现有SQL系统的兼容性和熟悉度。使用SQL的数据框在实际环境中实现集成是一个优雅的过程。
- en: How to do it...
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE94]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Set up the imports related to DataFrame and the required data structures and
    create the RDDs as needed for the example:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置与DataFrame相关的导入和所需的数据结构，并根据示例需要创建RDD：
- en: '[PRE95]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入设置`log4j`日志级别的包。此步骤是可选的，但我们强烈建议这样做（根据开发周期适当更改级别）。
- en: '[PRE96]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Set up the logging level to warning and `Error` to cut down on output. See
    the previous step for package requirement:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和`Error`以减少输出。请参阅前面的步骤了解包要求：
- en: '[PRE97]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Set up the Spark context and application parameter so Spark can run.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Spark上下文和应用程序参数，以便Spark可以运行。
- en: '[PRE98]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: We create the DataFrame corresponding to the `customer` file. In this step,
    we first create an RDD and then proceed to use the `toDF()` to convert the RDD
    to DataFrame and name the columns.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建对应于`customer`文件的DataFrame。在此步骤中，我们首先创建一个RDD，然后使用`toDF()`将RDD转换为DataFrame并命名列。
- en: '[PRE99]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Customer data contents for reference:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 客户数据内容参考：
- en: '[PRE100]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'On running the preceding code, you will get the following output:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码，您将得到以下输出：
- en: '![](img/af5a9571-9dc1-4585-aa7e-1bc87e6ccb83.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af5a9571-9dc1-4585-aa7e-1bc87e6ccb83.png)'
- en: We create the DataFrame corresponding to the `product` file. In this step, we
    first create an RDD and then proceed to use the `toDF()` to convert the RDD to
    DataFrame and name the columns.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建对应于`product`文件的DataFrame。在此步骤中，我们首先创建一个RDD，然后使用`toDF()`将RDD转换为DataFrame并命名列。
- en: '[PRE101]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'We convert `prodRDD` to DataFrame:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`prodRDD`转换为DataFrame：
- en: '[PRE102]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: Using SQL select, we display the contents of the table.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SQL select，我们显示表格内容。
- en: 'Product data contents:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 产品数据内容：
- en: '[PRE103]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码，您将得到以下输出：
- en: '![](img/d6567c3b-fd23-443a-a849-5029f7a1bb74.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6567c3b-fd23-443a-a849-5029f7a1bb74.png)'
- en: We create the DataFrame corresponding to the `sales` file. In this step we first
    create an RDD and then proceed to use `toDF()` to convert the RDD to DataFrame
    and name the columns.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建对应于`sales`文件的DataFrame。在此步骤中，我们首先创建一个RDD，然后使用`toDF()`将RDD转换为DataFrame并命名列。
- en: '[PRE104]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'We convert the `saleRDD` to DataFrame:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`saleRDD`转换为DataFrame：
- en: '[PRE105]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: We use SQL select to display the table.
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用SQL select来显示表格。
- en: 'Sales data contents:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 销售数据内容：
- en: '[PRE106]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码，您将得到以下输出：
- en: '![](img/e7cfde8b-1d1c-4182-a185-a32bde43e9f7.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7cfde8b-1d1c-4182-a185-a32bde43e9f7.png)'
- en: 'We print schemas for the customer, product, and sales DataFrames to verify
    schema after column definition and type conversion:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印客户、产品和销售DataFrame的架构，以验证列定义和类型转换后的架构：
- en: '[PRE107]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'On running the previous code, you will get the following output:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码，您将得到以下输出：
- en: '[PRE108]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: How it works...
  id: totrans-393
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this example, we first loaded data into an RDD and then converted it into
    a DataFrame using the `toDF()` method. The DataFrame is very good at inferring
    types, but there are occasions that require manual intervention. We used the `map()`
    function after creating the RDD (lazy initialization paradigm applies) to massage
    the data either by type conversion or calling on more complicated user-defined
    functions (referenced in the `map()` method) to do the conversion or data wrangling.
    Finally, we proceeded to examine the schema for each of the three DataFrames using
    `show()` and `printSchema()`.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们首先将数据加载到RDD中，然后使用`toDF()`方法将其转换为DataFrame。DataFrame非常擅长推断类型，但有时需要手动干预。我们在创建RDD后使用`map()`函数（应用惰性初始化范式）来处理数据，无论是通过类型转换还是调用更复杂的用户定义函数（在`map()`方法中引用）来进行转换或数据整理。最后，我们继续使用`show()`和`printSchema()`检查三个DataFrame的架构。
- en: There's more...
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'To ensure completeness, we include the `import` statements that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保完整性，我们包含了在Spark 2.0.0之前用于运行代码的`import`语句（即，Spark 1.5.2）：
- en: '[PRE109]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: See also
  id: totrans-398
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame的文档可在[https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)找到。
- en: If you see any issues with implicit conversion, double check to make sure you
    have included the implicits `import` statement.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到隐式转换问题，请再次检查以确保您已包含implicits `import`语句。
- en: 'Example `import` statement for Spark 1.5.2:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 1.5.2的示例`import`语句：
- en: '[PRE110]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Using DataFrames with standard SQL language - SparkSQL
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用标准SQL语言与DataFrames - SparkSQL
- en: In this recipe, we demonstrate how to use DataFrame SQL capabilities to perform
    basic CRUD operations, but there is nothing limiting you from using the SQL interface
    provided by Spark to any level of sophistication (that is, DML) desired.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们展示了如何使用DataFrame的SQL功能执行基本的CRUD操作，但没有任何限制您使用Spark提供的SQL接口达到所需的任何复杂程度（即DML）。
- en: How to do it...
  id: totrans-405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动新项目。确保包含必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置
- en: '[PRE111]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: Set up the imports related to DataFrames and the required data structures and
    create the RDDs as needed for the example
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置与DataFrames相关的导入以及所需的数据结构，并根据示例需要创建RDDs
- en: '[PRE112]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于设置`log4j`日志级别的包。此步骤是可选的，但我们强烈建议您根据开发周期的不同阶段适当调整级别。
- en: '[PRE113]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: Set up the logging level to warning and `ERROR` to cut down on output. See the
    previous step for package requirement.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为警告和`ERROR`以减少输出。请参阅上一步骤了解包要求。
- en: '[PRE114]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: How it works...
  id: totrans-415
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The basic workflow for DataFrame using SQL is to first populate the DataFrame
    either through internal Scala data structures or via external data sources first,
    and then use the `createOrReplaceTempView()` call to register the DataFrame as
    a SQL-like artifact.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SQL的基本DataFrame工作流程是首先通过内部Scala数据结构或外部数据源填充DataFrame，然后使用`createOrReplaceTempView()`调用将DataFrame注册为类似SQL的工件。
- en: When you use DataFrames, you have the benefit of additional metadata that Spark
    stores (whether API or SQL approach) which can benefit you during the coding and
    execution.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DataFrames时，您可以利用Spark存储的额外元数据（无论是API还是SQL方法），这可以在编码和执行期间为您带来好处。
- en: While RDDs are still the workhorses of core Spark, the trend is toward the DataFrame
    approach which has successfully shown its capabilities in languages such as Python/Pandas
    or R.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RDD仍然是核心Spark的主力，但趋势是向DataFrame方法发展，该方法已成功展示了其在Python/Pandas或R等语言中的能力。
- en: There's more...
  id: totrans-419
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'There has been a change for registration of a DataFrame as a table. Refer to
    this:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 将DataFrame注册为表的方式已发生变化。请参考此内容：
- en: 'For versions prior to Spark 2.0.0: `registerTempTable()`'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Spark 2.0.0之前的版本：`registerTempTable()`
- en: 'For Spark version 2.0.0 and previous: `createOrReplaceTempView()`'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Spark 2.0.0及更早版本：`createOrReplaceTempView()`
- en: 'Pre-Spark 2.0.0 to register a DataFrame as a SQL table like artifact:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0.0之前，将DataFrame注册为类似SQL表的工件：
- en: Before we can use the DataFrame for queries via SQL, we have to register the
    DataFrame as a temp table so the SQL statements can refer to it without any Scala/Spark
    syntax. This step may cause confusion for many beginners as we are not creating
    any table (temp or permanent), but the call `registerTempTable()` creates a name
    in SQL land that the SQL statements can refer to without additional UDF or without
    any domain-specific query language.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够使用DataFrame通过SQL进行查询之前，我们必须将DataFrame注册为临时表，以便SQL语句可以引用它而无需任何Scala/Spark语法。这一步骤可能会让许多初学者感到困惑，因为我们并没有创建任何表（临时或永久），但调用`registerTempTable()`在SQL领域创建了一个名称，SQL语句可以引用它而无需额外的UDF或无需任何特定领域的查询语言。
- en: Register the ...
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册...
- en: See also
  id: totrans-426
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框（DataFrame）的文档可在[此处](https://spark.apache.org/docs/latest/sql-programming-guide.html)获取。
- en: If you see any issues with implicit conversion, please double check to make
    sure you have included implicits `import` statement.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到隐式转换问题，请再次检查以确保您已包含implicits `import`语句。
- en: Example `import` statement for Spark 1.5.2
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 1.5.2的示例`import`语句
- en: '[PRE115]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: DataFrame is an extensive subsystem and deserves an entire book on its own.
    It makes complex data manipulation at scale available to SQL programmers.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame是一个广泛的子系统，值得用一整本书来介绍。它使SQL程序员能够大规模地进行复杂的数据操作。
- en: Working with the Dataset API using a Scala Sequence
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scala序列与数据集API协同工作
- en: In this recipe, we examine the new Dataset and how it works with the *seq* Scala
    data structure. We often see a relationship between the LabelPoint data structure
    used with ML libraries and a Scala sequence (that is, seq data structure) that
    play nicely with dataset.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们探讨了新的数据集以及它如何与Scala数据结构*seq*协同工作。我们经常看到LabelPoint数据结构与ML库一起使用，以及与数据集配合良好的Scala序列（即seq数据结构）之间的关系。
- en: The Dataset is being positioned as a unifying API going forward. It is important
    to note that DataFrame is still available as an alias described as `Dataset[Row]`.
    We have covered the SQL examples extensively via DataFrame recipes, so we concentrate
    our efforts on other variations for dataset.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集正被定位为未来统一的API。值得注意的是，DataFrame仍然可用，作为`Dataset[Row]`的别名。我们已经通过DataFrame的示例广泛地介绍了SQL示例，因此我们将重点放在数据集的其他变体上。
- en: How to do it...
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: Set up the package location where the program will reside
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置
- en: '[PRE116]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: Import the necessary packages for a Spark session to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以获取Spark会话访问集群，并导入`Log4j.Logger`以减少Spark产生的输出量。
- en: '[PRE117]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: Define a Scala `case class` to model data for processing, and the `Car` class
    will represent electric and hybrid cars.
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala `case class`来建模处理数据，`Car`类将代表电动和混合动力汽车。
- en: '[PRE118]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: Let's create a Scala sequence and populate it with electric and hybrid cars.
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个Scala序列，并用电动和混合动力汽车填充它。
- en: '[PRE119]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: Configure output level to `ERROR` to reduce Spark's logging output.
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别配置为`ERROR`以减少Spark的日志输出。
- en: '[PRE120]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Create a SparkSession yielding access to the Spark cluster, including the underlying
    session object attributes and functions.
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个SparkSession，以访问Spark集群，包括底层会话对象属性和功能。
- en: '[PRE121]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Import Spark implicits, therefore adding in behavior with only an import.
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark隐式，从而仅通过导入添加行为。
- en: '[PRE122]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: Next, we will create a Dataset from the car data sequence utilizing the Spark
    session's `createDataset()` method.
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将利用Spark会话的`createDataset()`方法从汽车数据序列创建一个数据集。
- en: '[PRE123]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: Let's print out the results as confirmation that our method invocation transformed
    the sequence into a Spark Dataset by invoking the show method.
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印出结果，以确认我们的方法调用通过调用show方法将序列转换为Spark数据集。
- en: '[PRE124]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '![](img/a252a4cf-d0d2-4e9b-ba36-ebe5a76de2f0.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a252a4cf-d0d2-4e9b-ba36-ebe5a76de2f0.png)'
- en: Print out the Dataset's implied column names. We can now use class attribute
    names as column names.
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出数据集的隐含列名。我们现在可以使用类属性名称作为列名。
- en: '[PRE125]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: Let's show the automatically generated schema, and validate inferred data types.
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们展示自动生成的模式，并验证推断的数据类型。
- en: '[PRE126]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: Finally, we will filter the Dataset on price referring to the `Car` class attribute
    price as a column and show results.
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将根据价格对数据集进行过滤，参考`Car`类属性价格作为列，并展示结果。
- en: '[PRE127]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '![](img/99ffee16-3167-451a-8586-308cf4d4321f.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99ffee16-3167-451a-8586-308cf4d4321f.png)'
- en: We close the program by stopping the Spark session.
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止Spark会话来关闭程序。
- en: '[PRE128]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: How it works...
  id: totrans-465
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we introduced Spark's Dataset feature which first appeared in
    Spark 1.6 and which was further refined in subsequent releases. First, we created
    an instance of a Dataset from a Scala sequence with the help of the `createDataset()`
    method belonging to the Spark session. The next step was to print out meta information
    about the generated Datatset to establish that the creation transpired as expected.
    Finally, snippets of Spark SQL were used to filter the Dataset by the price column
    for any price greater than $50, 000.00 and show the final results of execution.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们介绍了Spark的数据集功能，该功能首次出现在Spark 1.6中，并在后续版本中得到进一步完善。首先，我们借助Spark会话的`createDataset()`方法从Scala序列创建了一个数据集实例。接下来，我们打印出有关生成数据集的元信息，以确认创建过程如预期进行。最后，我们使用Spark
    SQL片段根据价格列过滤数据集，筛选出价格大于$50,000.00的记录，并展示最终执行结果。
- en: There's more...
  id: totrans-467
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Dataset has a view called [DataFrame](https://spark.apache.org/docs/2.0.0/api/scala/org/apache/spark/sql/package.html#DataFrame=org.apache.spark.sql.Dataset%5Borg.apache.spark.sql.Row%5D),
    which is a Dataset of [row](https://spark.apache.org/docs/2.0.0/api/scala/org/apache/spark/sql/Row.html)s
    which is untyped. The Dataset still retains all the transformation abilities of
    RDD such as `filter()`, `map()`, `flatMap()`, and so on. This is one of the reasons
    we find Datasets easy to use if we have programmed in Spark using RDDs.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集有一个名为[DataFrame](https://spark.apache.org/docs/2.0.0/api/scala/org/apache/spark/sql/package.html#DataFrame=org.apache.spark.sql.Dataset%5Borg.apache.spark.sql.Row%5D)的视图，它是[行](https://spark.apache.org/docs/2.0.0/api/scala/org/apache/spark/sql/Row.html)的未类型化数据集。数据集仍然保留了RDD的所有转换能力，如`filter()`、`map()`、`flatMap()`等。这就是为什么如果我们使用RDD编程Spark，我们会发现数据集易于使用的原因之一。
- en: See also
  id: totrans-469
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for Dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集文档可在[此处](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)找到。
- en: KeyValue grouped dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KeyValue分组数据集文档可在[此处](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)找到。
- en: Relational grouped dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系分组数据集文档可在[此处](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)找到。
- en: Creating and using Datasets from RDDs and back again
  id: totrans-473
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从RDD创建和使用数据集，以及反向操作
- en: In this recipe, we explore how to use RDD and interact with Dataset to build
    a multi-stage machine learning pipeline. Even though the Dataset (conceptually
    thought of as RDD with strong type-safety) is the way forward, you still have
    to be able to interact with other machine learning algorithms or codes that return/operate
    on RDD for either legacy or coding reasons. In this recipe, we also explore how
    to create and convert from Dataset to RDD and back.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们探讨了如何使用RDD与Dataset交互，以构建多阶段机器学习管道。尽管Dataset（概念上被认为是具有强类型安全的RDD）是未来的方向，但您仍然需要能够与其他机器学习算法或返回/操作RDD的代码进行交互，无论是出于遗留还是编码原因。在本食谱中，我们还探讨了如何创建和从Dataset转换为RDD以及反向操作。
- en: How to do it...
  id: totrans-475
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE129]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: Import the necessary packages for Spark session to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Spark会话导入必要的包以访问集群，并使用`Log4j.Logger`来减少Spark产生的输出量。
- en: '[PRE130]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: Define a Scala case class to model data for processing.
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala样例类来模拟处理数据。
- en: '[PRE131]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: Let's create a Scala sequence and populate it with electric and hybrid cars.
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个Scala序列，并用电动和混合动力汽车填充它。
- en: '[PRE132]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: How it works...
  id: totrans-485
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this section, we transformed an RDD into a Dataset and finally transformed
    it back to an RDD. We began with a Scala sequence which was changed into an RDD.
    After the creation of the RDD, invocation of Spark's session `createDataset()`
    method occurred, passing the RDD as an argument while receiving a Dataset as the
    result.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将RDD转换为Dataset，最终又转换回RDD。我们从一个Scala序列开始，将其转换为RDD。创建RDD后，调用Spark会话的`createDataset()`方法，将RDD作为参数传递，并接收作为结果的Dataset。
- en: Next, the Dataset was grouped by the make column, counting the existence of
    various makes of cars. The next step involved filtering the Dataset for makes
    of Tesla and transforming the results back to an RDD. Finally, we displayed the
    resulting RDD by way of the RDD `foreach()` method.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，数据集按制造商列分组，统计各种汽车制造商的存在情况。下一步涉及对特斯拉制造商的数据集进行过滤，并将结果转换回RDD。最后，我们通过RDD的`foreach()`方法显示了最终的RDD。
- en: There's more...
  id: totrans-488
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The Dataset source file in Spark is only about 2500+ lines of Scala code. It
    is a very nice piece of code which can be leveraged for specialization under Apache
    license. We list the following URL and encourage you to at least scan the file
    and understand how buffering comes into play when using Dataset.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的数据集源文件仅包含约2500+行Scala代码。这是一段非常优秀的代码，可以在Apache许可证下进行专业化利用。我们列出了以下URL，并鼓励您至少浏览该文件，了解在使用数据集时缓冲是如何发挥作用的。
- en: Source code for Datasets hosted on GitHub is available at [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala).
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的源代码托管在GitHub上，地址为[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala)。
- en: See also
  id: totrans-491
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: Documentation for Dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的文档可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)找到
- en: KeyValue grouped Dataset can be found at[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键值分组的数据集可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)找到
- en: Relational grouped Dataset can be found at[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系分组的数据集可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)找到
- en: Working with JSON using the Dataset API and SQL together
  id: totrans-495
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合使用数据集API和SQL处理JSON
- en: In this recipe, we explore how to use JSON with Dataset. The JSON format has
    rapidly become the de-facto standard for data interoperability in the last 5 years.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨如何使用JSON与数据集。在过去的5年中，JSON格式迅速成为数据互操作性的实际标准。
- en: We explore how Dataset uses JSON and executes API commands like `select()`.
    We then progress by creating a view (that is, `createOrReplaceTempView()`) and
    then execute a SQL query to demonstrate how to query against a JSON file using
    API and SQL with ease.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨数据集如何使用JSON并执行API命令，如`select()`。然后，我们通过创建一个视图（即`createOrReplaceTempView()`）并执行SQL查询来演示如何使用API和SQL轻松查询JSON文件。
- en: How to do it...
  id: totrans-498
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'We will use a JSON data file named `cars.json` which has been created for this
    example:'
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用一个名为`cars.json`的JSON数据文件，该文件是为这个示例创建的：
- en: '[PRE133]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: Set up the package location where the program will reside
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置
- en: '[PRE134]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: Import the necessary packages for the Spark session to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Spark会话导入必要的包以访问集群，并使用`Log4j.Logger`来减少Spark产生的输出量。
- en: '[PRE135]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: Define a Scala `case class` to model data for processing.
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala`case class`来建模处理数据。
- en: '[PRE136]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: Set output level to `ERROR` to reduce Spark's logging output.
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`，以减少Spark的日志输出。
- en: '[PRE137]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: Initialize a Spark session creating an entry point for access to the Spark cluster.
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个Spark会话，创建访问Spark集群的入口点。
- en: '[PRE138]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: Import Spark implicits, therefore adding in behavior with only an import.
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark隐式，从而仅通过导入添加行为。
- en: '[PRE139]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: Now, we will load the JSON data file into memory, specifying the class type
    as `Car`.
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将JSON数据文件加载到内存中，并指定类类型为`Car`。
- en: '[PRE140]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: Let's print out the data from our generated Dataset of type `Car`.
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印出我们生成的`Car`类型数据集中的数据。
- en: '[PRE141]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '![](img/1aa57574-0e2f-40e9-9892-35f18eaf2b8e.png)'
  id: totrans-518
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1aa57574-0e2f-40e9-9892-35f18eaf2b8e.png)'
- en: Next, we will display column names of the Dataset to verify that the cars' JSON
    attribute names were processed correctly.
  id: totrans-519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将显示数据集的列名，以验证汽车的JSON属性名称是否已正确处理。
- en: '[PRE142]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: Let's see the automatically generated schema and validate the inferred data
    types.
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们查看自动生成的模式并验证推断的数据类型。
- en: '[PRE143]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: In this step, we will select the Dataset's `make` column, removing duplicates
    by applying the `distinct` method and showing the results.
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们将选择数据集的`make`列，通过应用`distinct`方法去除重复项，并展示结果。
- en: '[PRE144]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '![](img/668b4ac1-6bf1-4b53-9db4-8770ce230f53.png)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![](img/668b4ac1-6bf1-4b53-9db4-8770ce230f53.png)'
- en: Next, create a view on the cars Dataset so we can execute a literal Spark SQL
    query string against the dataset.
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在cars数据集上创建一个视图，以便我们可以对数据集执行一个字面上的Spark SQL查询字符串。
- en: '[PRE145]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: Finally, we execute a Spark SQL query filtering the Dataset for electric cars,
    and returning only three of the defined columns.
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们执行一个Spark SQL查询，筛选数据集中的电动汽车，并仅返回定义的三个列。
- en: '[PRE146]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '![](img/ca582e1b-da30-46c7-8371-822fa08704f7.png)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca582e1b-da30-46c7-8371-822fa08704f7.png)'
- en: We close the program by stopping the Spark session.
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止Spark会话来结束程序。
- en: '[PRE147]'
  id: totrans-532
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: How it works...
  id: totrans-533
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: It is extremely straightforward to read a **JavaScript Object Notation** (**JSON**)
    data file and to transform it into a Dataset with Spark. JSON has become a widely
    used data format over the past several years and Spark's support for the format
    is substantial.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark读取**JavaScript对象表示法**(**JSON**)数据文件并将其转换为数据集非常简单。JSON在过去几年中已成为广泛使用的数据格式，Spark对这种格式的支持非常充分。
- en: In the first part, we demonstrated loading JSON into a Dataset by means of built-in
    JSON parsing functionality in Spark's session. You should take note of Spark's
    built-in functionality that transforms the JSON data into the car case class.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分中，我们展示了通过Spark会话内置的JSON解析功能将JSON加载到数据集的方法。您应该注意Spark的内置功能，它将JSON数据转换为car案例类。
- en: In the second part, we demonstrated Spark SQL being applied on the Dataset to
    wrangle the said data into a desirable state. We utilized the Dataset's select
    method to retrieve the `make` column and apply the `distinct` method for the removal
    ...
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分中，我们展示了如何将Spark SQL应用于数据集，以将所述数据整理成理想状态。我们利用数据集的select方法检索`make`列，并应用`distinct`方法去除...
- en: There's more...
  id: totrans-537
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: To fully understand and master the Dataset API, be sure to understand the concept
    of `Row` and `Encoder`.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 要全面理解和掌握数据集API，务必理解`Row`和`Encoder`的概念。
- en: Datasets follow the *lazy execution* paradigm, meaning that execution only occurs
    by invoking actions in Spark. When we execute an action, the Catalyst query optimizer
    produces a logical plan and generates a physical plan for optimized execution
    in a parallel distributed manner. See the figure in the introduction for all the
    detailed steps.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集遵循*惰性执行*范式，意味着执行仅在Spark中调用操作时发生。当我们执行一个操作时，Catalyst查询优化器生成一个逻辑计划，并为并行分布式环境中的优化执行生成物理计划。请参阅引言中的图表了解所有详细步骤。
- en: Documentation for `Row` is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '`Row`的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)找到。'
- en: Documentation for `Encoder` is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder)
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '`Encoder`的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder)找到。'
- en: See also
  id: totrans-542
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: Documentation for Dataset is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)找到。
- en: Documentation for KeyValue grouped Dataset is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KeyValue分组数据集的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)找到。
- en: Documentation for relational grouped Dataset [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系分组数据集的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)找到。
- en: Again, be sure to download and explore the Dataset source file, which is about
    2500+ lines from GitHub. Exploring the Spark source code is the best way to learn
    advanced programming in Scala, Scala Annotations, and Spark 2.0 itself.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 再次确保下载并探索来自GitHub的Dataset源文件，该文件约有2500+行。探索Spark源代码是学习Scala、Scala注解以及Spark 2.0本身高级编程的最佳方式。
- en: 'Noteworthy for Pre-Spark 2.0 users:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Spark 2.0之前的用户值得注意：
- en: SparkSession is the single entry ...
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkSession是单一入口...
- en: Functional programming with the Dataset API using domain objects
  id: totrans-549
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Dataset API进行领域对象的函数式编程
- en: In this recipe, we explore how functional programming works with Dataset. We
    use the Dataset and functional programming to separate the cars (domain object)
    by their models.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们探讨了如何使用Dataset进行函数式编程。我们利用Dataset和函数式编程将汽车（领域对象）按其车型进行分类。
- en: How to do it...
  id: totrans-551
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: Use package instruction to provide the right path
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用包指令提供正确的路径
- en: '[PRE148]'
  id: totrans-554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以获取Spark上下文对集群的访问权限，并使用`Log4j.Logger`减少Spark产生的输出量。
- en: '[PRE149]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: Define a Scala case to contain our data for processing, and our car class will
    represent electric and ...
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala案例类来包含我们处理的数据，我们的汽车类将代表电动和...
- en: How it works...
  id: totrans-558
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this example, we use a Scala sequence data structure to hold the original
    data, which is a series of cars and their attributes. Using `createDataset()`*,*
    we create a DataSet and populate it. We then proceed to use the 'make' attribute
    with `groupBy` and `mapGroups()` to list cars by their models using a functional
    paradigm with DataSet. Using this form of functional programming with domain objects
    was not impossible before DataSet (for example, the case class with RDD or UDF
    with DataFrame), but the DataSet construct makes this easy and intrinsic.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们使用Scala序列数据结构来存储原始数据，即一系列汽车及其属性。通过调用`createDataset()`，我们创建了一个DataSet并填充了它。接着，我们使用'make'属性配合`groupBy`和`mapGroups()`，以函数式范式列出按车型分类的汽车。在DataSet出现之前，使用领域对象进行这种形式的函数式编程并非不可能（例如，使用RDD的案例类或DataFrame的UDF），但DataSet结构使得这一过程变得简单且自然。
- en: There's more...
  id: totrans-560
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Be sure to include the `implicits` statement in all your DataSet coding:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在所有DataSet编码中包含`implicits`声明：
- en: '[PRE150]'
  id: totrans-562
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: See also
  id: totrans-563
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: The documentation for Datasets can be accessed at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: Dataset的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)访问。
