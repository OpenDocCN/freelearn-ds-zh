- en: Chapter 8. Creating Ensemble Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 创建集成模型
- en: A group of people have the ability to take better decisions than a single individual,
    especially when each group member comes in with their own biases. The ideology
    is also true for machine learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一组人往往比单个个体做出更好的决策，特别是当每个组员都有自己的偏见时。这一理念同样适用于机器学习。
- en: When single algorithms are not capable to generate the true prediction function,
    then ensemble machine-learning methods are used. When there is more focus on the
    performance of the model rather than the training time and the complexity of the
    model, then ensemble methods are preferred.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当单一算法无法生成真实的预测函数时，便会使用集成机器学习方法。当模型的性能比训练时间和模型复杂度更重要时，集成方法是首选。
- en: 'In this chapter, we will discuss:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论：
- en: What is ensemble learning?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是集成学习？
- en: Constructing ensembles.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建集成模型。
- en: Combination strategies.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合策略。
- en: Boosting, bagging, and injecting randomness.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升法、装袋法和引入随机性。
- en: Random forests.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林。
- en: What is ensemble learning?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是集成学习？
- en: 'Ensemble learning is a machine learning method where various models are prepared
    to work on the same problem. It is a process where multiple models are generated
    and the results obtained from them are combined to produce the final result. Moreover,
    ensemble models are inherently parallel; therefore, they are much more efficient
    at training and testing if we have access to multiple processors:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习是一种机器学习方法，其中多个模型被训练用于解决相同的问题。这是一个过程，其中生成多个模型，并将从它们得到的结果结合起来，产生最终结果。此外，集成模型本质上是并行的；因此，如果我们可以访问多个处理器，它们在训练和测试方面要高效得多：
- en: '**Ordinary methods of machine learning**: These use training data for learning
    a specific hypothesis.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**普通机器学习方法**：这些方法使用训练数据来学习特定的假设。'
- en: '**Ensemble learning**: This uses training data to build a set of hypothesis.
    These hypotheses are combined to build the final model.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成学习**：它使用训练数据构建一组假设。这些假设被组合以构建最终模型。'
- en: Therefore, it can be said that ensemble learning is the way toward preparing
    different individual learners for an objective function that utilizes different
    strategies, and in the long run, combines these learnings.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以说集成学习是一种为目标函数准备不同单独学习器的方法，采用不同的策略，并在长期内结合这些学习成果。
- en: '![What is ensemble learning?](img/B05321_08_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![什么是集成学习？](img/B05321_08_01.jpg)'
- en: Understanding ensemble learning
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解集成学习
- en: 'Ensemble learning, as we discussed, combines the learning of the various individual
    learners. It is the aggregation of multiple learned models that aim to improve
    accuracy:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所讨论的，集成学习将多个单独学习者的学习过程结合起来。它是多个已学习模型的聚合，旨在提高准确性：
- en: 'Base learners: Each individual learner is called a base learner. Base learners
    may be suited for a specific situation, but are not good in generalization.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基学习器：每个单独的学习器称为基学习器。基学习器可能适用于特定情境，但在泛化能力上较弱。
- en: Due to the weak generalization capability of the base learner, they are not
    suited for every scenario.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于基学习器的泛化能力较弱，它们并不适用于所有场景。
- en: Ensemble learning uses these base (weak) learners to construct a strong learner
    which results in a comparatively much more accurate model.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成学习使用这些基（弱）学习器来构建一个强学习器，从而得到一个相对更准确的模型。
- en: Generally, decision tree algorithms are used as the base learning algorithm.
    Using the same kind of learning algorithm results in homogeneous learners. However,
    different algorithms can also be used, which will result in heterogeneous learners.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，决策树算法作为基学习算法使用。使用相同类型的学习算法会产生同质学习器。然而，也可以使用不同的算法，这将导致异质学习器的产生。
- en: How to construct an ensemble
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何构建集成模型
- en: It is recommended that the base learners should be as diverse as possible. This
    enables the ensemble to handle and predict most of the situation with better accuracy.
    This diversity can be produced using different subsets of the dataset, manipulating
    or transforming the inputs, and using different techniques simultaneously for
    learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐基学习器应尽可能多样化。这使得集成能够以更高的准确性处理和预测大多数情况。可以通过使用数据集的不同子集、操控或转换输入，以及同时使用不同的学习技术来实现这种多样性。
- en: Also, when the individual base learners have a high accuracy then there is a
    good chance of having good accuracy with the ensemble.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当单个基学习器具有较高准确性时，集成学习通常也能得到较好的准确性。
- en: 'Typically, construction of an ensemble is a two-step process:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，集成的构建是一个两步过程：
- en: The first step is to create the base learners. They are generally constructed
    in parallel, but if base learners are influenced by the previously formed base
    learners then they are constructed in a sequential manner.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是创建基础学习器。它们通常是并行构建的，但如果基础学习器受到之前形成的基础学习器的影响，则会以顺序方式构建。
- en: The second step is to combine these base learners and create an ensemble which
    is best suited to the use case.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步是将这些基础学习器结合起来，创建一个最适合特定用例的集成模型。
- en: By using different types of base learners and combination techniques, we can
    altogether produce different ensemble learning models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用不同类型的基础学习器和组合技术，我们可以共同产生不同的集成学习模型。
- en: 'There are different ways to implement the ensemble model:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法可以实现集成模型：
- en: Sub-sample the training dataset
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对训练数据集进行子采样
- en: Manipulate the input features
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操控输入特征
- en: Manipulate the output features
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操控输出特征
- en: Inject randomness
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注入随机性
- en: Learning parameters of the classifier can be modified
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器的学习参数可以被修改
- en: Combination strategies
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 组合策略
- en: 'Combination strategies can be classified into two categories:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 组合策略可以分为两类：
- en: Static combiners
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态组合器
- en: Adaptive combiners
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应组合器
- en: '**Static combiners:** The combiner choice standard is autonomous of the component
    vector. Static methodologies can be comprehensively partitioned into trainable
    and non-trainable.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**静态组合器**：组合器的选择标准独立于组件向量。静态方法可以大致分为可训练和不可训练。'
- en: '**Trainable**: The combiner goes through a different training stage to enhance
    the performance of the ensemble. Here are two approaches that are widely used:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**可训练的**：组合器经历不同的训练阶段，以提高集成模型的表现。这里有两种广泛使用的方法：'
- en: '**Weighted averaging**: The yield of every classifier is weighted by its very
    own performance measure:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权平均法**：每个分类器的输出根据其自身的表现度量进行加权：'
- en: Measuring the accuracy of prediction on a different validation set
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同验证集上衡量预测的准确性
- en: '**Stacked generalization**: The yield of the ensemble is treated as the feature
    vector to a meta-classifier'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**堆叠泛化**：集成的输出被视为元分类器的特征向量'
- en: '**Non-trainable**: Performance of the individual classifier does not have an
    affect on the voting. Different combiners might be utilized. This depends upon
    the sort of yield delivered by the classifier:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**不可训练的**：个别分类器的表现不会影响投票。可能会使用不同的组合器，这取决于分类器所生成的输出类型：'
- en: '**Voting**: This is used when a single class label is generated by every classifier.
    Every classifier votes for a specific class. The class that receives the larger
    part vote on the ensemble wins.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投票法**：当每个分类器生成一个类标签时使用此方法。每个分类器为某一特定类别投票，获得最多票数的类别获胜。'
- en: '**Averaging**: When a confidence estimate is created by every classifier then
    averaging is utilized. The class that has the highest number of posterior in the
    ensemble wins.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均法**：当每个分类器都生成一个置信度估计时，使用平均法。集成中后验概率最大的类别获胜。'
- en: '**Borda counts**: This is used when a rank is produced by every classifier.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**博尔达计数法**：当每个分类器生成一个排名时使用此方法。'
- en: '**Adaptive combiners**: This is a type of combiner function that is dependent
    on the feature vector given as input:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**自适应组合器**：这是一种依赖于输入的特征向量的组合函数：'
- en: A function that is local to every region
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个区域局部的一个函数
- en: Divide and conquer methodology creates modular ensembles and simple classifiers
    specializing in different regions of I/O space
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分而治之的方法创建了模块化的集成和简单的分类器，它们专门处理不同区域的输入输出空间。
- en: The individual specialists are required to perform well in their region of ability
    and not for all inputs
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各个专家只需在其能力范围内表现良好，而不必对所有输入都有效
- en: Subsampling training dataset
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对训练数据集进行子采样
- en: 'The learner is considered to be unstable if the output classifier has to undergo
    radical changes when there are some small variations in the training data:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出分类器在训练数据出现小的变化时需要经历剧烈的变化，那么学习器被认为是不稳定的：
- en: '**Unstable learners**: Decision trees, neural networks, and so on'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不稳定学习器**：决策树、神经网络等'
- en: '**Stable learners**: Nearest neighbor, linear regression, and so on'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稳定学习器**：最近邻、线性回归等'
- en: This particular technique is more suited for the unstable learners.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特定的技术更适用于不稳定学习器。
- en: 'Two very common techniques used in subsampling are:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 子采样中常用的两种技术是：
- en: Bagging
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自助法
- en: Boosting
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升法
- en: Bagging
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自助法
- en: Bagging is also known as Bootstrap Aggregation. It generates the additional
    data that is used for training by using sub-sampling on the same dataset with
    replacement. It creates multiple combinations with repetitions to generate the
    training dataset of the same size.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法也称为自助聚合。它通过对同一数据集进行有放回的子抽样生成额外的训练数据。它通过重复组合生成与原始数据集大小相同的训练数据集。
- en: As sampling with replacement is done, on an average, each classifier is trained
    on 63.2% of the training example.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于采用了有放回的抽样，每个分类器平均训练63.2%的训练样本。
- en: After training on these multiple datasets, bagging combines the result by majority
    voting. The class that received the most number of votes wins. By using these
    multiple datasets, bagging aims to reduce the variance. Accuracy is improved if
    the induced classifiers are uncorrelated.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些多个数据集上训练后，袋装法通过多数投票来组合结果。获得最多票数的类别获胜。通过使用这些多个数据集，袋装法旨在减少方差。如果引入的分类器是无关的，则可以提高准确性。
- en: Random forest, a type of ensemble learning, uses bagging and is one of the most
    powerful methods.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种集成学习方法，采用袋装法，是最强大的方法之一。
- en: Let's go through the bagging algorithm.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看袋装算法。
- en: '**Training**:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练**：'
- en: 'For iteration, *t=1 to T*:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于迭代，*t=1 到 T*：
- en: Sample randomly from the training dataset with replacement *N* samples
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从训练数据集中随机采样 *N* 个样本，采用有放回抽样
- en: Base learner is trained (for example, decision tree or neural network) on this
    sample
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础学习器在该样本上训练（例如决策树或神经网络）
- en: '**Test**:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**测试**：'
- en: 'For test sample, *t=1 to T*:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于测试样本，*t=1 到 T*：
- en: Start all the models that were trained
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动所有已训练的模型
- en: 'Prediction is done on the basis of the following:'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测是基于以下内容进行的：
- en: '**Regression**: Averaging'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：平均'
- en: '**Classification**: Majority vote'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：多数投票'
- en: '![Bagging](img/B05321_08_02-1.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![袋装法](img/B05321_08_02-1.jpg)'
- en: When does bagging work?
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 什么情况下袋装法有效？
- en: 'Bagging works in scenarios where there would have been over-fitting if it wasn''t
    used. Let''s go through these scenarios:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法在如果不使用的话可能会发生过拟合的场景中有效。我们来看一下这些场景：
- en: '**Under-fitting**:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠拟合**：'
- en: '**High bias**: Models are not good enough and don''t fit the training data
    well'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高偏差**：模型不够好，未能很好地拟合训练数据'
- en: '**Small variance**: Whenever there is a change in the dataset, there is a very
    small change that needs to be done in the classifier'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小方差**：每当数据集发生变化时，分类器需要做出的调整非常小'
- en: '**Over-fitting**:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：'
- en: '**Small bias**: Models fit too well to the training data'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小偏差**：模型对训练数据拟合得过好'
- en: '**Large variance**: Whenever there is a small change in the dataset, there
    is a large or drastic change that needs to be done in the classifier'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大方差**：每当数据集发生小的变化时，分类器需要做出很大或剧烈的调整'
- en: Bagging aims to reduce the variance without affecting the bias. Therefore, the
    dependency of the model on the training dataset is reduced.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法旨在减少方差而不影响偏差。因此，模型对训练数据集的依赖减少。
- en: Boosting
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升
- en: Boosting is different to the bagging approach. It is based on the PAC framework,
    which is the Probably Approximately Correct framework.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 提升与袋装法不同。它基于 PAC 框架，即“可能近似正确”框架。
- en: '**PAC learning**: The probability of having larger confidence and smaller accuracy
    than the misclassification error:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**PAC 学习**：具有比误分类错误更大的置信度和更小的准确度的概率：'
- en: '**Accuracy**: This is the percentage of correctly classifying the samples in
    the test dataset'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确度**：这是测试数据集中正确分类样本的百分比'
- en: '**Confidence**: This is the probability of achieving the accuracy in the given
    experiment'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**置信度**：这是在给定实验中达到准确度的概率'
- en: Boosting approach
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提升方法
- en: 'The boosting approach is based on the concept of the "weak learner". When an
    algorithm performs somewhat better than 50% in binary classification tasks, then
    it is called a weak learner. The approach is to combine multiple weak learners
    together, and the aim is to:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 提升方法基于“弱学习器”的概念。当一个算法在二分类任务中的表现略好于 50% 时，它被称为弱学习器。该方法的目的是将多个弱学习器组合在一起，目标是：
- en: Improve confidence
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高置信度
- en: Improve accuracy
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高准确性
- en: This is done by training these different weak learners on different datasets.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过在不同数据集上训练不同的弱学习器来完成的。
- en: Boosting algorithm
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提升算法
- en: 'Training:'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练：
- en: Samples are taken randomly from the dataset.
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集中随机抽取样本。
- en: First, learner *h1* is trained on the sample.
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，学习器 *h1* 在样本上进行训练。
- en: Accuracy of this learner, *h1*, is evaluated on the dataset.
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估学习器 *h1* 在数据集上的准确度。
- en: A similar process is followed using different samples for multiple learners.
    They are divided such that they classify differently.
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的样本为多个学习者执行类似的过程。它们被划分为能够做出不同分类的子集。
- en: 'Test:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试：
- en: On the test dataset, the learning is applied using the majority vote of all
    the learners
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测试数据集上，学习通过所有学习者的多数投票来应用
- en: Confidence can also be boosted in a similar way, as boosting the accuracy with
    some trade-off.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 信心也可以通过类似方式增强，就像通过某些权衡来提升准确性一样。
- en: Boosting is less of an algorithm and rather it is a "framework". The main aim
    of this framework is to take a weak learning algorithm W and turn it into a strong
    learning algorithm. We will now discuss AdaBoost, which is short for "adaptive
    boosting algorithm". AdaBoost became famous because it was one of the first successful
    and practical boosting algorithms.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 提升方法更像是一个“框架”而非一个算法。该框架的主要目标是将一个弱学习算法 W 转化为强学习算法。我们现在将讨论 AdaBoost，简称为“自适应提升算法”。AdaBoost
    因为是最早成功且实用的提升算法之一而广为人知。
- en: It does not require a large number of hyper-parameters to be defined and executes
    in a polynomial time. The benefit of this algorithm is that it has the ability
    to automatically adapt to the data given to it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 它不需要定义大量的超参数，并且能够在多项式时间内执行。该算法的优势在于它能够自动适应给定的数据。
- en: AdaBoost – boosting by sampling
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AdaBoost – 通过采样进行提升
- en: 'After *n* iterations, a weak learner having distribution *D* over the training
    set is provided by the boosting:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *n* 次迭代之后，Boosting 提供了一个在训练集上有分布 *D* 的弱学习器：
- en: All examples have the equal probability of being selected as the first component
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有例子都有相同的概率被选为第一个组成部分
- en: Sub-sampling of the training set is done in accordance with distribution *Dn*
    and a model is trained by the weak learner
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集的子采样是根据分布 *Dn* 进行的，并通过弱学习器训练模型
- en: The weights of misclassified instances are adjusted in a way that subsequent
    classifiers work on comparatively difficult cases
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误分类实例的权重通过调整使得后续分类器能够处理相对较难的案例
- en: A distribution *D(n+1)* is generated with the probability of misclassified samples
    increasing and correctly classified samples decreasing
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成分布 *D(n+1)*，其中错误分类样本的概率增加，而正确分类样本的概率减少
- en: After *t* iterations, according to the performance of the models, the votes
    of individual hypotheses are weighted
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *t* 次迭代之后，根据模型的表现，对个别假设的投票进行加权
- en: The strength of AdaBoost derives from the adaptive resampling of examples, not
    from the final weighted combination.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 的强大之处在于其通过自适应重采样样本，而非最终加权组合
- en: What is boosting doing?
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提升方法在做什么？
- en: Every classifier has a specialization on the specific subset of dataset
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个分类器在特定数据子集上有其专长
- en: An algorithm concentrates on examples with increasing levels of difficulty
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种算法集中处理具有不断增加难度的例子
- en: Boosting is able to reduce variance (similar to bagging)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升方法能够减少方差（类似于 Bagging）
- en: It is also able to eliminate the consequences of high bias of the weak learner
    (not present in bagging)
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还能够消除弱学习器的高偏差所带来的影响（这在 Bagging 中不存在）
- en: 'Train versus test errors performance:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练与测试误差的表现：
- en: We can reduce the train errors to nearly 0
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将训练误差减少到接近 0
- en: Overfitting is not present and is evident by the test errors
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不会出现过拟合，测试误差体现了这一点
- en: The bias and variance decomposition
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 偏差与方差分解
- en: 'Let''s discuss how bagging and boosting affect the bias-variance decomposition
    of the classification error:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论 Bagging 和 Boosting 如何影响分类误差的偏差-方差分解：
- en: 'Features of errors that can be expected from a learning algorithm:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以从学习算法中预期的错误特征：
- en: A bias term is a measure of performance of the classifier with respect to the
    target function
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差项是衡量分类器相对于目标函数的表现的度量
- en: Variance terms measure the robustness of the classifier; if there is a change
    in the training dataset, then how is the model affected by it?
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方差项衡量分类器的鲁棒性；如果训练数据集发生变化，模型将如何受其影响？
- en: Bagging and boosting are capable to reduce the variance term, and therefore
    reducing the errors in the model
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging 和 Boosting 都能够减少方差项，从而减少模型中的误差
- en: It is also proved that boosting attempts to reduce the bias term since it focuses
    on misclassified samples
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还证明了提升方法试图减少偏差项，因为它专注于错误分类的样本
- en: Manipulating the input features
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作输入特征
- en: One other technique with which we can generate multiple classifiers is by manipulating
    the set of input features which we feed to the learning algorithm.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种生成多个分类器的技术是通过操控我们输入给学习算法的特征集。
- en: 'We can select different subsets of features and networks of different sizes.
    The input features subsets may be manually selected rather than in an automated
    fashion. This technique is widely used in image processing: one of the very famous
    examples is Principle Component Analysis.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择不同的特征子集和不同大小的网络。这些输入特征子集可以手动选择，而非自动生成。这种技术在图像处理领域广泛使用：一个非常著名的例子是主成分分析（PCA）。
- en: The ensemble classifier generated in many experiments was able to perform like
    an actual human.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实验中生成的集成分类器能够像真实的人类一样执行任务。
- en: It was also found that when we delete even a few of the features that we gave
    as the input, it affects the performance of the classifier. This affects the overall
    voting, and the ensemble thus generated is not able to perform up to the expectations.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 还发现，当我们删除输入的某些特征时，会影响分类器的性能。这会影响整体投票结果，因此生成的集成分类器无法达到预期效果。
- en: Injecting randomness
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注入随机性
- en: This is another universally useful technique for producing an ensemble of classifiers.
    In this method, we inject randomness into the learner algorithm. Neural networks
    with back-propagation are also created using the same technique for hidden weights.
    On the off chance that the calculation is connected to the same preparing illustrations,
    but with various starting weights, the subsequent classifier can be very diverse.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一种普遍有效的生成分类器集成的方法。在这种方法中，我们将随机性注入到学习算法中。反向传播神经网络也是使用相同的技术生成的，针对隐藏权重。如果计算应用于相同的训练样本，但使用不同的初始权重，则得到的分类器可能会有很大差异。
- en: One of the most computationally costly parts of outfits of decision trees involves
    preparing the decision tree. This is quick for decision stumps; however, for more
    profound trees, it can be restrictively costly.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树模型中最具计算成本的部分之一是准备决策树。对于决策树桩，这个过程是快速的；然而，对于更深的树结构，这个过程可能会非常昂贵。
- en: The costly part is picking the tree structure. Once the tree structure is picked,
    it is extremely cheap to fill in the leaves (which is the predictions of the trees)
    utilizing the training data. A shockingly productive and successful option is
    to utilize trees with altered structures and random features. Accumulations of
    trees are called forests, thus classifiers fabricated like this are called random
    forests.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 昂贵的部分在于选择树的结构。一旦选择了树结构，利用训练数据填充叶节点（即树的预测）是非常便宜的。一种令人惊讶的高效且成功的选择是使用具有不同结构和随机特征的树。由此生成的树的集合被称为森林，因此这样构建的分类器被称为随机森林。
- en: 'It takes three arguments:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 它需要三个参数：
- en: The training data
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据
- en: The depth of the decision trees
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的深度
- en: The number form
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字形式
- en: The calculation produces each of the K trees freely which makes it simple to
    parallelize. For every tree, it develops a full binary tree of a given depth.
    The elements utilized at the branches of this tree are chosen randomly, regularly
    with substitution, implying that the same element can seem numerous at times,
    even in one branch. Based on the training data, the leaves that will perform the
    actual predictions are filled. This last step is the main time when the training
    data is utilized. The subsequent classifier then only involves the voting of the
    K-numerous random trees.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法独立生成每一棵K棵树，这使得并行化变得简单。对于每一棵树，它都会构建一个给定深度的完整二叉树。树枝上使用的特征是随机选择的，通常是有替换地选择，这意味着同一个特征可以多次出现在同一分支上。根据训练数据，叶节点将进行实际预测的填充。这个最后的步骤是唯一使用训练数据的时刻。随后的分类器仅依赖于K棵随机树的投票。
- en: The most astonishing thing about this methodology is that it works strikingly
    well. It tends to work best when the greater part of the components are not significant,
    since the quantity of features chosen for any given tree is minimal. A portion
    of the trees will query the useless features.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法最令人惊讶的地方在于它表现得非常出色。它通常在大多数特征不重要时效果最佳，因为每棵树选择的特征数量很少。一些树将会查询那些无用的特征。
- en: These trees will basically make random forecasts. Be that as it may, a portion
    of the trees will query good features and will make good forecasts (on the grounds
    that the leaves are assessed in light of the training data). In the event that
    you have enough trees, the arbitrary ones will wash out as noise, and just the
    good trees will affect the final classification.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这些树基本上会做出随机预测。然而，其中一些树会查询好的特征并做出准确预测（因为叶子是基于训练数据进行评估的）。如果你有足够多的树，随机的预测会作为噪声被消除，最终只有好的树会影响最终分类。
- en: Random forests
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'Random forests were developed by Leo Breiman and Adele Cutler. Their strength
    in the field of machine learning has been shown nicely in a blog entry at Strata
    2012: "Ensembles of decision trees (often known as random forests) have been the
    most successful general-purpose algorithm in modern times", as they "automatically
    identify the structure, interactions, and relationships in the data".'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林由Leo Breiman和Adele Cutler开发。它们在机器学习领域的优势在2012年Strata博客中得到了很好的展示：“决策树集成（通常称为随机森林）是现代最成功的通用算法”，因为它们“能够自动识别数据中的结构、交互和关系”。
- en: Moreover, it has been noticed that "most Kaggle solutions have no less than
    one top entry that vigorously utilizes this methodology". Random forests additionally
    have been the preferred algorithm for recognizing the body part in Microsoft's
    Kinect, which is a movement detecting information gadgets for Xbox consoles and
    Windows PCs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，已注意到“许多Kaggle解决方案至少有一个顶级条目，强烈利用这一方法”。随机森林也成为了微软Kinect的首选算法，Kinect是一个用于Xbox主机和Windows电脑的运动检测信息设备。
- en: Random forests comprises of a group of decision trees. We will consequently
    begin to analyze decision trees.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林由一组决策树组成。因此，我们将开始分析决策树。
- en: A decision tree, as discussed previously, is a tree-like chart where on each
    node there is a choice, in view of one single feature. Given an arrangement of
    features, the tree is navigated from node to node, as indicated by these decisions,
    until you come to a leaf. The name of this leaf is the expectation for the given
    list of features. A straightforward decision tree could be utilized to choose
    what you have to bring with you when going out.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，决策树是一种类似树状的图表，每个节点都有一个基于单一特征的选择。给定一组特征，树会根据这些决策从节点到节点进行导航，直到到达叶子节点。这个叶子的名称即为给定特征列表的预测。一个简单的决策树可以用来决定你出门时需要带什么。
- en: '![Random forests](img/B05321_08_03.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/B05321_08_03.jpg)'
- en: 'Each tree is constructed in the following way:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 每棵树的构建方式如下：
- en: Randomly take an *n* sample case where n is the number of cases in the training
    set.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选取一个*n*样本案例，其中n是训练集中的案例数量。
- en: Replacement is used to select these n cases. This particular set of data is
    used to construct the tree.
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用替代法选择这n个案例。这一特定数据集用于构建树。
- en: The node is split by *x<X*, where *X* is the number of input variables. The
    *X* doesn't change with the growth of the forest.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点根据*x<X*进行拆分，其中*X*是输入变量的数量。*X*在森林生长过程中不会改变。
- en: Pruning is not done as trees are allowed to go the maximum depth.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于树木允许达到最大深度，因此没有进行修剪。
- en: 'The error rate in a random forest is dependent on the following (as given in
    the original paper):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的错误率依赖于以下因素（如原始论文所述）：
- en: When correlation among the trees increases, the error rate in the random forest
    increases
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当树之间的相关性增加时，随机森林的错误率也会增加。
- en: When individual trees are weak, the error rate increases and it decreases when
    the individual trees are strengthened
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当单个树较弱时，错误率会增加，而当单个树得到加强时，错误率会减少。
- en: It is found that *x*, which was mentioned previously, has an effect on correlation
    and strength. Increasing *x* increases both strength and correlation, and decreasing
    *x* decreases both. We try to find the particular value or the range where the
    *x* should lie to have the minimal error.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 发现之前提到的*x*对相关性和强度有影响。增加*x*会增加强度和相关性，而减少*x*则会减少两者。我们试图找到*x*应该处于的特定值或范围，以实现最小错误。
- en: We use the **oob** (**out of bag**) error to find the best value or the range
    of the value of *x*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用**oob**（**袋外**）误差来寻找最佳的*x*值或范围。
- en: This tree does not classify every one of the points effectively. We could transform
    this by expanding the tree depth. Thus, the tree can foresee the specimen data
    100% effectively, just by taking in the noise in the examples. In the most extreme
    case, the calculation is proportional to a word reference containing each specimen.
    This is known as over-fitting and prompts awful results when utilizing it for
    out of test forecasts. Keeping in mind the end goal to overcome over-fitting,
    we can prepare numerous decision trees by presenting weights for the examples
    and just considering an irregular subset of the features for every split. A definite
    conclusion of the random forest will be controlled by a lion's share vote on the
    trees' forecasts. This method is otherwise called bagging. It diminishes the variance
    (errors from noise in the training set) without expanding the bias (errors because
    of the insufficient adaptability of the model).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这棵树并不能有效地分类所有的点。我们可以通过增加树的深度来改进这一点。这样，树就能100%准确地预测样本数据，只需学习样本中的噪声。在极端情况下，这个算法相当于一个包含所有样本的词典。这就是过拟合，它会导致在进行测试外的预测时产生糟糕的结果。为了克服过拟合，我们可以通过为样本引入权重，并且每次分裂时仅考虑特征的一个随机子集来训练多个决策树。随机森林的最终预测将由大多数树的投票决定。这个方法也叫做集成（bagging）。它通过减少方差（训练集中的噪声错误）而不增加偏差（模型不足灵活所带来的错误）来提高预测的准确性。
- en: Features of random forests
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林的特征
- en: A random forest is highly accurate as compared to the existing algorithms.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与现有算法相比，随机森林具有很高的准确性。
- en: It can be used effectively and efficiently on big data. They are fast and don't
    require expensive hardware to run.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以在大数据上有效且高效地使用。它们速度快，并且不需要昂贵的硬件来运行。
- en: One of the key features of a random forest is its ability to deal several numbers
    of input variables.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林的一个关键特性是它能够处理多个输入变量。
- en: A random forest can show the generalization error estimate during the process
    of construction of the forest. It can also give the important variables for the
    classification.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林可以在构建森林的过程中展示泛化误差的估计。它还可以提供分类的重要变量。
- en: When the data is sparse, a random forest is an effective algorithm with good
    accuracy. It can also work on prediction of missing data.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据稀疏时，随机森林是一种有效且准确的算法。它还可以用于预测缺失数据。
- en: The models generated can be utilized later on data that we might receive in
    the future.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的模型可以在将来接收到的数据上使用。
- en: In unbalanced datasets, it provides the feature to balance the error present
    in the class population.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不平衡数据集中，它提供了平衡类人口中存在的错误的特性。
- en: Some of these features can be used for unsupervised clustering too and also
    as an effective method for outlier detection.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中一些特征也可以用于无监督聚类，并且也可以作为一种有效的异常检测方法。
- en: One other key feature of a random forest is that it doesn't overfit.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林的另一个关键特性是它不会发生过拟合。
- en: How do random forests work?
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林是如何工作的？
- en: To comprehend and utilize the different choices, additional data about how they
    are figured is valuable. The vast majority of the alternatives rely upon two information
    objects produced by random forests.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解和使用不同的选择，关于如何计算这些选择的额外数据是有帮助的。大多数选项依赖于随机森林生成的两个数据对象。
- en: At the point when the training set for the present tree is drawn by sampling
    with replacement, around 33% of the cases are left well enough alone for the example.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当当前树的训练集通过有放回的抽样绘制时，大约三分之一的样本被保留下来，用于示例。
- en: As more and more trees are added to the random forest, the information from
    the oob helps to generate the estimate of the classification error. After the
    construction of every tree, the greater part of the information keeps running
    down the tree, and vicinities are computed for every pair of cases.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的树被添加到随机森林中，oob数据有助于生成分类错误的估计。在每棵树构建完成后，大部分数据都会沿着树向下运行，并计算每对样本的接近度。
- en: On the other hand, if the same terminal node is shared by two cases then we
    increase their proximity by 1\. After the complete analysis, these proximities
    are normalized. Proximities are utilized as a part of supplanting missing information,
    finding outliers, and delivering in-depth perspectives of the information.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果两个样本共享相同的终端节点，则通过增加它们的接近度来增加1。经过完整分析后，这些接近度会被归一化。接近度被用作替代缺失数据、发现异常值以及提供数据的深入视角。
- en: The out-of-bag (oob) error estimate
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 袋外（oob）误差估计
- en: 'Random forests eliminate the requirement to cross-validate to achieve the unbiased
    estimate of the test set error. During the construction, it is estimated as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林消除了交叉验证的需求，以获得无偏的测试集误差估计。在构建过程中，误差估计如下：
- en: Every tree is constructed utilizing an alternate bootstrap sample from the original
    information. Around 33% of the cases are left alone for the bootstrap test and
    not utilized as a part of the development of the kth tree.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每棵树的构建都使用了来自原始数据的交替自助样本。大约33%的案例将留作自助测试，不参与第k棵树的构建。
- en: There may be some cases that were not considered during the construction of
    the trees. We put these cases down the kth to achieve a classification. It produces
    a classification test set in around 33% of the trees. Towards the end of the run,
    take *j* to be the class that received the vast majority of the votes each time
    the case *n* was oob. The number of times that *j* is not equivalent to the true
    class of *n* at the midpoint of all the cases is the oob error estimate. This
    follows being unbiased in the various tests.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能存在一些在构建树时没有考虑到的情况。我们将这些情况放到第k棵树下进行分类。大约33%的树会生成分类测试集。在运行的后期，每次当案例*n*为oob时，将*j*视为获得绝大多数投票的类别。*j*与*n*的真实类别在所有案例中点的中间位置不相等的次数就是oob误差估计。这符合在各种测试中保持无偏的原则。
- en: Gini importance
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基尼重要性
- en: The impurity measure is frequently used for decision trees. Misclassification
    is measured and is called gini impurity which applies in the context where there
    are multiple classifiers present.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 纯度度量通常用于决策树。误分类被度量，并称为基尼纯度，这适用于存在多个分类器的情境。
- en: There is also a gini coefficient. This is applicable to binary classification.
    It needs a classifier that is able to rank the sample according to the probability
    of being in the right class.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个基尼系数。这适用于二分类。它需要一个能够根据属于正确类别的概率对样本进行排名的分类器。
- en: Proximities
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 接近度
- en: As mentioned before, we don't prune the trees while constructing the random
    forest. Therefore, the terminal nodes don't have many numbers of instances.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在构建随机森林时我们不会修剪树。因此，终端节点不会有很多实例。
- en: To find the proximity measure, we run all the cases from the training set down
    the tree. Let's say case x and case y arrive at the same terminal node, we then
    increase the proximity by 1.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到接近度度量，我们将所有训练集中的案例运行到树中。假设案例x和案例y到达了同一个终端节点，那么我们就增加接近度1。
- en: After the run, we take twice the number of trees and divide the proximity of
    the case which was increased by 1 by this number.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 运行结束后，我们将树的数量乘以2，然后将增加了1的案例的接近度除以该数字。
- en: Implementation in Julia
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Julia中的实现
- en: 'Random forests are available in the Julia-registered packages from Kenta Sato:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林可以在Kenta Sato注册的Julia包中使用：
- en: '[PRE0]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is a CART-based random forest implementation in Julia. This package supports:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基于CART的随机森林在Julia中的实现。这个包支持：
- en: Classification models
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类模型
- en: Regression models
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归模型
- en: Out-of-bag (OOB) errors
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 袋外（OOB）误差
- en: Feature importances
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征重要性
- en: Various configurable parameters
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种可配置的参数
- en: 'There are two separate models available in this package:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 该包中有两个独立的模型：
- en: Classification
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Regression
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归
- en: 'Each model has its own constructor that is trained by applying the fit method.
    We can configure these constructors with some keyword arguments listed as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都有自己的构造函数，通过应用`fit`方法进行训练。我们可以使用以下列出的一些关键字参数来配置这些构造函数：
- en: '[PRE1]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This one is for the classification:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这个适用于分类：
- en: '[PRE2]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This one is for the regression:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这个适用于回归：
- en: '`n_estimators`: This is the number of weak estimators'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：这是弱估计器的数量'
- en: '`max_features`: This is the number of candidate features at each split'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`：这是每次分裂时候候选特征的数量'
- en: If Integer is given, the fixed number of features are used
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果给定的是整数（Integer），则使用固定数量的特征。
- en: If FloatingPoint is given, the proportions of the given value (0.0, 1.0] are
    used
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果给定的是浮动点（FloatingPoint），则使用给定值的比例（0.0, 1.0]。
- en: If Symbol is given, the number of candidate features is decided by a strategy
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果给定的是符号（Symbol），则候选特征的数量由策略决定。
- en: '`:sqrt: ifloor(sqrt(n_features))`'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`:sqrt: ifloor(sqrt(n_features))`'
- en: '`:third: div(n_features, 3)`'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`:third: div(n_features, 3)`'
- en: '`max_depth`: The maximum depth of each tree'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：每棵树的最大深度'
- en: The default argument nothing means there is no limitation of the maximum depth
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认参数`nothing`表示没有最大深度的限制。
- en: '`min_samples_split`: The minimum number of sub-samples to try to split a node'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`：分裂节点时尝试的最小子样本数量'
- en: '`criterion`: The criterion of the impurity measure (classification only)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`criterion`：分类的杂质度量准则（仅限分类）'
- en: '`:`gini: Gini index'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`:`gini：基尼指数'
- en: '`:entropy`: Cross entropy'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`:entropy`：交叉熵'
- en: '`RandomForestRegressor` always uses the mean squared error for its impurity
    measure. Currently, there are no configurable criteria for the regression model.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomForestRegressor` 总是使用均方误差作为其杂质度量。当前，回归模型没有可配置的准则。'
- en: Learning and prediction
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习与预测
- en: For our example, we will be using the amazing "DecisionTree" package provided
    by Ben Sadeghi.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将使用 Ben Sadeghi 提供的令人惊叹的 "DecisionTree" 包。
- en: 'The package supports the following available models:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 该包支持以下可用模型：
- en: '`DecisionTreeClassifier`'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DecisionTreeClassifier`'
- en: '`DecisionTreeRegressor`'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DecisionTreeRegressor`'
- en: '`RandomForestClassifier`'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomForestClassifier`'
- en: '`RandomForestRegressor`'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomForestRegressor`'
- en: '`AdaBoostStumpClassifier`'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AdaBoostStumpClassifier`'
- en: 'Installation is straightforward:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 安装很简单：
- en: '[PRE3]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let us start with the classification example:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从分类示例开始：
- en: '[PRE4]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We now take the famous iris dataset:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用著名的鸢尾花数据集：
- en: '[PRE5]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This generates a pruned tree classifier:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成一个修剪后的树分类器：
- en: '[PRE6]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Learning and prediction](img/B05321_08_04.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![学习与预测](img/B05321_08_04.jpg)'
- en: 'It generates such a tree given in the previous image. We now apply this learned
    model:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 它生成了前面图像中展示的树。现在我们应用这个学习到的模型：
- en: '[PRE7]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'It generates the following result:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 它生成了以下结果：
- en: '[PRE8]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now let''s train the random forest classifier:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练随机森林分类器：
- en: '[PRE9]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It generates the random forest classifier:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 它生成了随机森林分类器：
- en: '[PRE10]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we will apply this learned model and check the accuracy:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将应用这个学习到的模型并检查其准确性：
- en: '[PRE11]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The result is as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE12]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now let''s train a regression tree:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练一个回归树：
- en: '[PRE13]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It generates the following tree:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 它生成了以下树：
- en: '[PRE14]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now training a regression forest is made straightforward by the package:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，训练回归森林变得更加简单，通过这个包：
- en: '[PRE15]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It generates the following output:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 它生成了以下输出：
- en: '[PRE16]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Why is ensemble learning superior?
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么集成学习更优？
- en: To comprehend the generalization power of ensemble learning being superior to
    an individual learner, Dietterich provided three reasons.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解集成学习的泛化能力为何优于单一学习器，Dietterich 提出了三个理由。
- en: 'These three reasons help us understand the reason for the superiority of ensemble
    learning leading to a better hypothesis:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个原因帮助我们理解集成学习的优越性，从而得出更好的假设：
- en: The training information won't give adequate data to picking a single best learner.
    For instance, there might be numerous learners performing similarly well on the
    training information set. In this way, joining these learners might be a superior
    decision.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练信息不会提供足够的数据来选择单一的最佳学习器。例如，可能有多个学习器在训练数据集上表现得同样优秀。因此，将这些学习器结合起来可能是一个更好的选择。
- en: The second reason is that, the search procedures of the learning algorithms
    may be defective. For instance, regardless of the possibility that there exists
    a best hypothesis, the learning algorithms may not be able to achieve that due
    to various reasons including generation of an above average hypothesis. Ensemble
    learning can improve on that part by increasing the possibility to achieve the
    best hypothesis.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个原因是，学习算法的搜索过程可能存在缺陷。例如，即使存在一个最佳假设，学习算法可能因为各种原因（如生成了一个优于平均水平的假设）而无法达到该假设。集成学习通过增加达到最佳假设的可能性来改善这一点。
- en: The third reason is that one target function may not be present in the hypothesis
    space that we are searching in. This target function may lie in a combination
    of various hypothesis spaces, which is similar to combining various decision trees
    to generate the random forest.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个原因是，目标函数可能不存在于我们正在搜索的假设空间中。这个目标函数可能存在于不同假设空间的组合中，这类似于将多个决策树结合起来生成随机森林。
- en: There are numerous hypothetical studies on acclaimed ensemble techniques. For
    example, boosting and bagging are the methods to achieve these three points discussed.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 关于备受推崇的集成技术，已有许多假设研究。例如，提升（boosting）和袋装（bagging）是实现这些讨论的三点方法。
- en: It is also observed that boosting does not experience the ill effects of over-fitting
    even after countless, and now and then it is even ready to diminish the generalization
    error after the training error has achieved zero. Although numerous scientists
    have considered this marvel, hypothetical clarifications are still in belligerence.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 还观察到，提升方法即使在经过无数次训练后，也不会受到过拟合的影响，有时甚至能够在训练误差为零后进一步减少泛化误差。尽管许多科学家已经研究了这一现象，但理论上的解释仍然存在争议。
- en: The bias-variance decomposition is frequently utilized as a part of studying
    the execution of ensemble techniques. It is observed that bagging is able to nearly
    eliminate the variance, and by doing so becomes ideal to attach to learners that
    experience huge variance, such as unstable learners, decision trees, or neural
    networks.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差-方差分解常用于研究集成方法的表现。观察到Bagging几乎能够消除方差，从而使它特别适合与经历巨大方差的学习器结合，如不稳定的学习器、决策树或神经网络。
- en: Boosting is able to minimize the bias, notwithstanding diminishing the variance,
    and by doing so becomes more viable to weak learners such as decision trees.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 提升方法能够最小化偏差，尽管它会减少方差，通过这种方式，使得它更适合用于像决策树这样的弱学习器。
- en: Applications of ensemble learning
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成学习的应用
- en: 'Ensemble learning is used widely in applications, such as:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习广泛应用于以下场景：
- en: Optical character recognition
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 光学字符识别
- en: Text categorization
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Face recognition
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部识别
- en: Computer-aided medical diagnosis
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机辅助医学诊断
- en: Ensemble learning can be used in nearly all scenarios where machine learning
    techniques are used.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习几乎可以应用于所有使用机器学习技术的场景。
- en: Summary
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Ensemble learning is a method for generating highly accurate classifiers by
    combining weak or less accurate ones. In this chapter, we discussed some of the
    methods for constructing ensembles and went through the three fundamental reasons
    why ensemble methods are able to outperform any single classifier within the ensemble.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习是一种通过结合弱分类器或不太准确的分类器来生成高精度分类器的方法。在本章中，我们讨论了构建集成的一些方法，并讲解了集成方法能够超越集成中任何单一分类器的三个基本原因。
- en: We discussed bagging and boosting in detail. Bagging, also known as Bootstrap
    Aggregation, generates the additional data that is used for training by using
    sub-sampling on the same dataset with replacement. We also learned why AdaBoost
    performs so well and understood in detail about random forests. Random forests
    are highly accurate and efficient algorithms that don't overfit. We also studied
    how and why they are considered as one of the best ensemble models. We implemented
    a random forest model in Julia using the "DecisionTree" package.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们详细讨论了Bagging和Boosting。Bagging，亦称为Bootstrap聚合，通过对相同数据集进行有放回的子采样来生成用于训练的附加数据。我们还学习了为何AdaBoost表现如此优秀，并详细了解了随机森林。随机森林是高精度且高效的算法，不会发生过拟合。我们还研究了它们为何被认为是最佳的集成模型之一。我们使用“DecisionTree”包在Julia中实现了一个随机森林模型。
- en: References
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)'
- en: '[http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf](http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf](http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf)'
- en: '[http://www.machine-learning.martinsewell.com/ensembles/ensemble-learning.pdf](http://www.machine-learning.martinsewell.com/ensembles/ensemble-learning.pdf)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.machine-learning.martinsewell.com/ensembles/ensemble-learning.pdf](http://www.machine-learning.martinsewell.com/ensembles/ensemble-learning.pdf)'
- en: '[http://web.cs.wpi.edu/~xkong/publications/papers/ecml11.pdf](http://web.cs.wpi.edu/~xkong/publications/papers/ecml11.pdf)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://web.cs.wpi.edu/~xkong/publications/papers/ecml11.pdf](http://web.cs.wpi.edu/~xkong/publications/papers/ecml11.pdf)'
