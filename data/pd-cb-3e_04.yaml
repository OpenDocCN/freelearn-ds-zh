- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: The pandas I/O System
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pandas I/O 系统
- en: So far, we have been creating our `pd.Series` and `pd.DataFrame` objects *inline*
    with data. While this is helpful for establishing a theoretical foundation, very
    rarely would a user do this in production code. Instead, users would use the pandas
    I/O functions to read/write data from/to various formats.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在用数据创建`pd.Series`和`pd.DataFrame`对象，*内联*处理数据。虽然这样做有助于建立理论基础，但在生产代码中，很少有用户会这样做。相反，用户会使用
    pandas 的 I/O 函数来从各种格式读取/写入数据。
- en: I/O, which is short for **input/output**, generally refers to the process of
    reading from and writing to common data formats like CSV, Microsoft Excel, JSON,
    etc. There is, of course, not just one format for data storage, and many of these
    options represent trade-offs between performance, storage size, third-party integration,
    accessibility, and/or ubiquity. Some formats assume well-structured, stringently
    defined data (SQL being arguably the most extreme), whereas other formats can
    be used to represent semi-structured data that is not restricted to being two-dimensional
    (JSON being great example).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: I/O，指的是**输入/输出**，通常指从常见的数据格式（如 CSV、Microsoft Excel、JSON 等）中读取和写入数据的过程。当然，数据存储并不是只有一种格式，许多选项在性能、存储大小、第三方集成、可访问性和/或普及性之间进行权衡。有些格式假设数据是结构化且严格定义的（SQL
    可能是最极端的例子），而其他格式则可以用于表示半结构化数据，这些数据不局限于二维结构（JSON 就是一个很好的例子）。
- en: The fact that pandas can interact with so many of these data formats is one
    of its greatest strengths, allowing pandas to be the proverbial Swiss army knife
    of data analysis tools. Whether you are interacting with SQL databases, a set
    of Microsoft Excel files, HTML web pages, or a REST API endpoint that transmits
    data via JSON, pandas is up to the task of helping you build a cohesive view of
    all of your data. For this reason, pandas is considered a popular tool in the
    domain of ETL.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 能够与多种数据格式进行交互，这也是它的最大优势之一，使得 pandas 成为数据分析工具中的瑞士军刀。无论是与 SQL 数据库、Microsoft
    Excel 文件集、HTML 网页，还是通过 JSON 传输数据的 REST API 端点交互，pandas 都能够胜任帮助你构建数据的统一视图。因此，pandas
    被认为是 ETL 领域中的一个流行工具。
- en: 'We are going to cover the following recipes in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下操作方法：
- en: CSV – basic reading/writing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV – 基本的读写操作
- en: CSV – strategies for reading large files
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV – 读取大型文件的策略
- en: Microsoft Excel – basic reading/writing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft Excel – 基本的读写操作
- en: Microsoft Excel – finding tables in non-default locations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft Excel – 在非默认位置查找表格
- en: Microsoft Excel – hierarchical data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft Excel – 层次化数据
- en: SQL using SQLAlchemy
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SQLAlchemy 的 SQL
- en: SQL using ADBC
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 ADBC 的 SQL
- en: Apache Parquet
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Parquet
- en: JSON
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON
- en: HTML
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTML
- en: Pickle
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pickle
- en: Third party I/O libraries
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三方 I/O 库
- en: CSV – basic reading/writing
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CSV – 基本的读写操作
- en: CSV, which stands for *comma-separated values*, is one of the most common formats
    for data exchange. While there is no official standard that defines what a CSV
    file is, most developers and users would loosely consider it to be a plain text
    file, where each line in the file represents a row of data, and within each row,
    there are *delimiters* between each field to indicate when one record ends and
    the next begins. The most commonly used *delimiter* is a comma (hence the name
    *comma-separated values*), but this is not a hard requirement; it is not uncommon
    to see CSV files that use a pipe (`|`), tilde (`~`), or backtick (`` ` ``) character
    as the delimiter. If the delimiter character is expected to appear within a given
    record, usually some type of quoting surrounds the individual record (or all records)
    to allow a proper interpretation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CSV，代表*逗号分隔值*，是最常见的数据交换格式之一。虽然没有正式的标准来定义什么是 CSV 文件，但大多数开发者和用户通常认为它是一个纯文本文件，其中文件中的每一行表示一条数据记录，每条记录的字段之间有*分隔符*，用于表示一条记录的结束和下一条记录的开始。最常用的*分隔符*是逗号（因此叫做*逗号分隔值*），但这并不是硬性要求；有时我们也会看到使用管道符（`|`）、波浪符（`~`）或反引号（``
    ` ``）作为分隔符的 CSV 文件。如果期望分隔符字符出现在某条记录内，通常会对单个记录（或所有记录）加上引号，以确保正确解析。
- en: 'For example, let’s assume a CSV file uses a pipe separator with the following
    contents:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一个 CSV 文件使用管道分隔符，其内容如下：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first row would be read with only two columns of data, whereas the second
    row would contain three columns of data. Assuming we wanted the records `["a|b",
    "c"]` to appear in the second row, proper quoting would be required:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行将只读取两列数据，而第二行将包含三列数据。假设我们希望记录 `["a|b", "c"]` 出现在第二行，就需要进行适当的引号处理：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The above rules are relatively simple and make it easy to write CSV files, but
    that in turn makes reading CSV files much more difficult. The CSV format provides
    no metadata (i.e., what delimiter, quoting rule, etc.), nor does it provide any
    information about the type of data being provided (i.e., what type of data should
    be located in column X). This puts the onus on CSV readers to figure this all
    out on their own, which adds performance overhead and can easily lead to a misinterpretation
    of data. Being a text-based format, CSV is also an inefficient way of storing
    data compared to binary formats like Apache Parquet. Some of this can be offset
    by compressing CSV files (at the cost of read/write performance), but generally,
    CSV rates as one of the worst formats for CPU efficiency, memory usage, and losslessness.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 上述规则相对简单，可以轻松地写入 CSV 文件，但反过来这也使得读取 CSV 文件变得更加困难。CSV 格式没有提供元数据（例如，什么分隔符、引号规则等），也没有提供关于数据类型的任何信息（例如，哪些数据应位于
    X 列）。这使得 CSV 读取器必须自己搞清楚这些内容，从而增加了性能开销，并且很容易导致数据误解。作为一种基于文本的格式，与像 Apache Parquet
    这样的二进制格式相比，CSV 也是一种低效的数据存储方式。通过压缩 CSV 文件（以牺牲读/写性能为代价），可以在一定程度上弥补这些问题，但通常来说，CSV
    在 CPU 效率、内存使用和无损性方面是最差的格式之一。
- en: Despite these shortcomings and more, the CSV format has been around for a long
    time and won’t disappear any time soon, so it is beneficial to know how to read
    and write such files with pandas.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些缺点，CSV 格式已经存在很长时间，并且不会很快消失，因此了解如何使用 pandas 读取和写入此类文件是很有帮助的。
- en: How to do it
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: Let’s start with a simple `pd.DataFrame`. Building on our knowledge in *Chapter
    3,* *Data Types*, we know that the default types used by pandas are less than
    ideal, so we are going to use `pd.DataFrame.convert_dtypes` with the `dtype_backend="numpy_nullable"`
    argument to construct this and all of our `pd.DataFrame` objects going forward.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的 `pd.DataFrame` 开始。基于我们在*第3章*中学到的*数据类型*，我们知道 pandas 默认使用的数据类型并不理想，因此我们将使用
    `pd.DataFrame.convert_dtypes` 方法，并使用 `dtype_backend="numpy_nullable"` 参数来构建这个以及以后所有的
    `pd.DataFrame` 对象。
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To write this `pd.DataFrame` out to a CSV file, we can use the `pd.DataFrame.to_csv`
    method. Typically, the first argument you would provide is a filename, but in
    this example, we will use the `io.StringIO` object instead. An `io.StringIO` object
    acts like a file but does not save anything to your disk. Instead, it manages
    the file contents completely in memory, requiring no cleanup and leaving nothing
    behind on your filesystem:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这个 `pd.DataFrame` 写入到 CSV 文件中，我们可以使用 `pd.DataFrame.to_csv` 方法。通常，您提供的第一个参数是文件名，但在这个例子中，我们将使用
    `io.StringIO` 对象来代替。`io.StringIO` 对象类似于一个文件，但不会将任何内容保存到磁盘上。相反，它完全在内存中管理文件内容，无需清理，也不会在文件系统中留下任何东西：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have a “file” with CSV data, we can use the `pd.read_csv` function
    to read this data back in. However, by default, I/O functions in pandas will use
    the same default data types that a `pd.DataFrame` constructor would use. To avoid
    that, we can fortunately still use the `dtype_backend="numpy_nullable"` argument
    with I/O read functions:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个包含 CSV 数据的“文件”，我们可以使用 `pd.read_csv` 函数将这些数据读回来。然而，默认情况下，pandas 中的 I/O
    函数将使用与 `pd.DataFrame` 构造函数相同的默认数据类型。幸运的是，我们仍然可以使用 `dtype_backend="numpy_nullable"`
    参数与 I/O 读取函数一起使用，从而避免这个问题：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Interestingly, the `pd.read_csv` result does not exactly match the `pd.DataFrame`
    we started with, as it includes a newly added `Unnamed: 0` column. When you call
    `pd.DataFrame.to_csv`, it will write out both your row index and columns to the
    CSV file. The CSV format does not allow you to store any extra metadata to indicate
    which columns in the CSV file should map to the row index versus those that should
    represent a column in the `pd.DataFrame`, so `pd.read_csv` assumes everything
    to be a column.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '有趣的是，`pd.read_csv` 的结果并不完全与我们最初的 `pd.DataFrame` 匹配，因为它包含了一个新增的 `Unnamed: 0`
    列。当你调用 `pd.DataFrame.to_csv` 时，它会将行索引和列一起写入到 CSV 文件中。CSV 格式不允许你存储任何额外的元数据来指示哪些列应映射到行索引，哪些列应表示
    `pd.DataFrame` 中的列，因此 `pd.read_csv` 假设所有内容都是列。'
- en: 'You can rectify this situation by letting `pd.read_csv` know that the first
    column of data in the CSV file should form the row index with an `index_col=0`
    argument:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过让 `pd.read_csv` 知道 CSV 文件中的第一列数据应该形成行索引，并使用 `index_col=0` 参数来纠正这种情况：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Alternatively, you could avoid writing the index in the first place with the
    `index=False` argument of `pd.DataFrame.to_csv`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以通过 `pd.DataFrame.to_csv` 的 `index=False` 参数避免一开始就写入索引：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There’s more…
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'As mentioned back at the beginning of this section, CSV files use quoting to
    prevent any confusion between the appearance of the *delimiter* within a field
    and its intended use – to indicate the start of a new record. Fortunately, pandas
    handles this rather sanely by default, which we can see with some new sample data:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本节开头提到的，CSV 文件使用引号来防止字段中出现的*分隔符*与其预期用途（即指示新记录的开始）混淆。幸运的是，pandas 默认以一种相当理智的方式处理这一点，我们可以通过一些新示例数据来看到这一点：
- en: '[PRE12]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that we just have a `name` column that contains a comma, you can see that
    pandas quotes the field to indicate that the usage of a comma is part of the data
    itself and not a new record:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只有一个包含逗号的`name`列，可以看到 pandas 会对该字段加上引号，表示逗号的使用是数据的一部分，而不是新记录的开始：
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We could have alternatively decided upon the usage of a different *delimiter*,
    which can be toggled with the `sep=` argument:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以选择使用不同的*分隔符*，这可以通过 `sep=` 参数进行切换：
- en: '[PRE16]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We also mentioned that, while CSV files are naturally plain text, you can also
    compress them to save storage space. The easiest way to do this is to provide
    a filename argument with a common compression file extension, i.e., by saying
    `df.to_csv("data.csv.zip")`. For more explicit control, you can use the `compression=`
    argument.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提到，尽管 CSV 文件本质上是纯文本格式，您也可以通过压缩它们来节省存储空间。最简单的方法是提供带有常见压缩文件扩展名的文件名参数，例如通过 `df.to_csv("data.csv.zip")`。如果需要更明确的控制，您可以使用
    `compression=` 参数。
- en: 'To see this in action, let’s work with a larger `pd.DataFrame`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到这一点的实际效果，让我们使用一个更大的 `pd.DataFrame`：
- en: '[PRE18]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Take note of the number of bytes used to write this out as a plain text CSV
    file:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意将文件写出为纯文本 CSV 文件时所使用的字节数：
- en: '[PRE20]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Using `compression="gzip"`, we can produce a file with far less storage:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `compression="gzip"`，我们可以生成一个占用存储空间更少的文件：
- en: '[PRE22]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The trade-off here is that while compressed files require less disk storage,
    they require more work from the CPU to compress or decompress the file contents.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的权衡是，虽然压缩文件需要更少的磁盘存储空间，但它们需要更多的 CPU 工作来压缩或解压缩文件内容。
- en: CSV – strategies for reading large files
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CSV – 读取大文件的策略
- en: Handling large CSV files can be challenging, especially when they exhaust the
    memory of your computer. In many real-world data analysis scenarios, you might
    encounter datasets that are too large to be processed in a single-read operation.
    This can lead to performance bottlenecks and `MemoryError` exceptions, making
    it difficult to proceed with your analysis. However, fear not! There are quite
    a few levers you can pull to more efficiently try and process files.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大型 CSV 文件可能具有挑战性，尤其是在它们耗尽计算机内存时。在许多现实世界的数据分析场景中，您可能会遇到无法通过单次读取操作处理的数据集。这可能导致性能瓶颈和
    `MemoryError` 异常，使分析变得困难。不过，不必担心！有很多方法可以提高处理文件的效率。
- en: In this recipe, we will show you how you can use pandas to peek at parts of
    your CSV file to understand what data types are being inferred. With that understanding,
    we can instruct `pd.read_csv` to use more efficient data types, yielding far more
    efficient memory usage.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将展示如何使用 pandas 查看 CSV 文件的部分内容，以了解正在推断的数据类型。通过这个理解，我们可以指示 `pd.read_csv`
    使用更高效的数据类型，从而大大提高内存使用效率。
- en: How to do it
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: For this example, we will look at the *diamonds* dataset. This dataset is not
    actually all that big for modern computers, but let’s pretend that the file is
    a lot bigger than it is, or that the memory on our machine is limited to the point
    where a normal `read_csv` call would yield a `MemoryError`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将查看*钻石*数据集。这个数据集对于现代计算机来说并不算特别大，但让我们假设这个文件比实际要大得多，或者假设我们的机器内存有限，以至于正常的
    `read_csv` 调用会引发 `MemoryError`。
- en: To start, we will look at the first 1,000 rows from the dataset to get an idea
    of what is in the file via `nrows=1_000`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看数据集中的前 1,000 行，通过 `nrows=1_000` 来了解文件中的内容：
- en: '[PRE24]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `pd.DataFrame.info` method should give us an idea of how much memory this
    subset uses:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.DataFrame.info` 方法应该能让我们了解这个子集使用了多少内存：'
- en: '[PRE26]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The exact memory usage you see may depend on your version of pandas and operating
    system, but let’s assume that the `pd.DataFrame` we are using requires around
    85 KB of memory. If we had 1 billion rows instead of just 1,000, that would require
    85 GB of memory just to store this `pd.DataFrame`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您看到的具体内存使用量可能取决于您使用的 pandas 版本和操作系统，但假设我们使用的 `pd.DataFrame` 大约需要 85 KB 的内存。如果我们有
    10 亿行数据，而不是只有 1,000 行，那仅仅存储这个 `pd.DataFrame` 就需要 85 GB 的内存。
- en: 'So how can we fix this situation? For starters, it is worth looking more closely
    at the data types that have been inferred. The `price` column may be one that
    immediately catches our attention; this was inferred to be a `pd.Int64Dtype()`,
    but chances are that we don’t need 64 bits to store this information. Summary
    statistics will be explored in more detail in *Chapter 5*, *Algorithms and How
    to Apply Them* but for now, let’s just take a look at `pd.Series.describe` to
    see what pandas can tell us about this column:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何解决这个问题呢？首先，值得更仔细地查看已经推断出的数据类型。`price`列可能是一个立即引起我们注意的列；它被推断为`pd.Int64Dtype()`，但我们很可能不需要64位来存储这些信息。关于汇总统计的更多细节将在*第5章*中讨论，*算法及其应用*，但现在，让我们先看看`pd.Series.describe`，看看pandas可以为我们提供关于这个列的信息：
- en: '[PRE28]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The minimum value is 326 and the maximum is 2,898\. Those values can both safely
    fit into `pd.Int16Dtype()`, which would represent good memory savings compared
    to `pd.Int64Dtype()`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最小值为326，最大值为2,898。这些值可以安全地适配`pd.Int16Dtype()`，与`pd.Int64Dtype()`相比，这将节省大量内存。
- en: 'Let’s also take a look at some of the floating point types, starting with the
    *carat*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还来看看一些浮点类型，从*指数*开始：
- en: '[PRE30]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The values range from 0.2 to 1.27, and unless we expect to perform calculations
    with many decimal points, the 6 to 9 digits of decimal precision that a 32-bit
    floating point data type provides should be good enough to use here.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值的范围从0.2到1.27，除非我们预计要进行许多小数点计算，否则32位浮点数据类型提供的6到9位小数精度应该足够使用。
- en: 'For this recipe, we are going to assume that 32-bit floating point types can
    be used across all of the other floating point types as well. One way to tell
    `pd.read_csv` that we want to use smaller data types would be to use the `dtype=`
    parameter, with a dictionary mapping column names to the desired types. Since
    our `dtype=` parameter will cover all of the columns, we can also drop `dtype_backend="numpy_nullable"`,
    as it would be superfluous:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，我们假设32位浮动类型可以用于所有其他浮动类型。告诉`pd.read_csv`我们希望使用更小的数据类型的一个方法是使用`dtype=`参数，并通过字典将列名映射到所需的数据类型。由于我们的`dtype=`参数将覆盖所有列，因此我们也可以省略`dtype_backend="numpy_nullable"`，因为它是多余的：
- en: '[PRE32]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'These steps alone will probably yield a memory usage in the ballpark of 55
    KB, which is not a bad reduction from the 85 KB we started with! For added safety,
    we can use the `pd.DataFrame.describe()` method to get summary statistics and
    ensure that the two `pd.DataFrame` objects are similar. If the numbers are the
    same for both `pd.DataFrame` objects, it is a good sign that our conversions did
    not materially change our data:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅这些步骤可能就会将内存使用量减少到大约55KB，相较于最初的85KB，已经是一个不错的减少！为了更安全起见，我们可以使用`pd.DataFrame.describe()`方法获取汇总统计信息，并确保这两个`pd.DataFrame`对象相似。如果这两个`pd.DataFrame`对象的数字相同，那是一个很好的迹象，表明我们的转换没有实质性地改变数据：
- en: '[PRE34]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'So far, things are looking good, but we can still do better. For starters,
    it looks like the `cut` column has a relatively small amount of unique values:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切看起来不错，但我们仍然可以做得更好。首先，似乎`cut`列有相对较少的唯一值：
- en: '[PRE38]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The same can be said about the `color` column:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对`color`列也可以说同样的事情：
- en: '[PRE40]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'As well as the `clarity` column:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以及`clarity`列：
- en: '[PRE42]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Out of 1,000 rows sampled, there are only 5 distinct `cut` values, 7 distinct
    `color` values, and 8 distinct `clarity` values. We consider these columns to
    have a *low cardinality*, i.e., the number of distinct values is very low relative
    to the number of rows.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从1,000行抽样数据来看，`cut`列有5个不同的值，`color`列有7个不同的值，`clarity`列有8个不同的值。我们认为这些列具有*低基数*，即与行数相比，独特值的数量非常少。
- en: 'This makes these columns a perfect candidate for the use of categorical types.
    However, I would advise against using `pd.CategoricalDtype()` as an argument to
    `dtype=`, as by default it uses `np.nan` as a missing value indicator (for a refresher
    on this caveat, you may want to revisit the *Categorical types* recipe back in
    *Chapter 3*, *Data Types*). Instead, the best approach to convert your strings
    to categorical types is to first read in your columns as `pd.StringDtype()`, and
    then use `pd.DataFrame.astype` on the appropriate column(s):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得这些列非常适合使用分类类型。然而，我建议不要将`pd.CategoricalDtype()`作为`dtype=`的参数，因为默认情况下，它使用`np.nan`作为缺失值指示符（如果你需要回顾这个警告，可以重新查看在*第3章*中提到的*分类类型*配方）。相反，最佳的做法是首先将列读取为`pd.StringDtype()`，然后在适当的列上使用`pd.DataFrame.astype`：
- en: '[PRE44]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'To get even more savings, we may decide that there are columns in our CSV file
    that are just not worth reading at all. To allow pandas to skip this data and
    save even more memory, you can use the `usecols=` parameter:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步节省内存，我们可能会决定不读取CSV文件中的某些列。如果希望pandas跳过这些数据以节省更多内存，可以使用`usecols=`参数：
- en: '[PRE46]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'If the preceding steps are not sufficient to create a small enough `pd.DataFrame`,
    you might still be in luck. If you can process chunks of data at a time and do
    not need all of it in memory, you can use the `chunksize=` parameter to control
    the size of the chunks you would like to read from a file:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的步骤不足以创建足够小的`pd.DataFrame`，你可能还是有机会的。如果你可以一次处理一部分数据，而不需要将所有数据都加载到内存中，你可以使用`chunksize=`参数来控制从文件中读取数据块的大小：
- en: '[PRE48]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: There’s more...
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The `usecols` parameter introduced here can also accept a callable that, when
    evaluated against each column label encountered, should return `True` if the column
    should be read and `False` if it should be skipped. If we only wanted to read
    the `carat`, `cut`, `color`, and `clarity` columns, that might look something
    like:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里介绍的`usecols`参数也可以接受一个可调用对象，当它在每个遇到的列标签上求值时，如果该列应被读取，则返回`True`，如果应跳过，则返回`False`。如果我们只想读取`carat`、`cut`、`color`和`clarity`列，可能会像这样：
- en: '[PRE50]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Microsoft Excel – basic reading/writing
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Microsoft Excel – 基本的读写操作
- en: Microsoft Excel is an extremely popular tool for data analysis, given its ease
    of use and ubiquity. Microsoft Excel provides a rather powerful toolkit that can
    help to cleanse, transform, store, and visualize data, all without requiring any
    knowledge of programming languages. Many successful analysts may consider it to
    be the *only* tool they will ever need. Despite this, Microsoft Excel really struggles
    with performance and scalability and, when used as a storage medium, may even
    materially change your data in unexpected ways.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft Excel是一个极其流行的数据分析工具，因其易用性和普及性。Microsoft Excel提供了一个相当强大的工具包，帮助清洗、转换、存储和可视化数据，而无需了解编程语言。许多成功的分析师可能会认为它是他们永远需要的*唯一*工具。尽管如此，Microsoft
    Excel在性能和可扩展性上确实存在困难，作为存储介质时，它甚至可能在意想不到的方式上改变你的数据。
- en: If you have used Microsoft Excel before and are now picking up pandas, you will
    find that pandas works as a complementary tool. With pandas, you will give up
    the point-and-click usability of Microsoft Excel, but you will easily unlock performance
    that takes you far beyond the limits of Microsoft Excel.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以前使用过Microsoft Excel，现在开始学习pandas，你会发现pandas是一个互补工具。使用pandas时，你将放弃Microsoft
    Excel的点选操作便捷性，但你将轻松解锁性能，超越Microsoft Excel的限制。
- en: 'Before we jump into this recipe, it’s worth noting that Microsoft Excel support
    is not shipped as part of pandas, so you will need to install third-party package(s)
    for these recipes to work. While it is not the only choice, users are encouraged
    to opt for installing `openpyxl`, as it works very well to read and write all
    of the various Microsoft Excel formats. If you do not have it already, `openpyxl`
    can be installed via:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入这个方法之前，值得注意的是，Microsoft Excel支持并不是pandas的一部分，因此你需要安装第三方包来使这些方法生效。虽然这不是唯一的选择，但鼓励用户安装`openpyxl`，因为它非常适合读取和写入各种Microsoft
    Excel格式。如果你还没有安装`openpyxl`，可以通过以下命令进行安装：
- en: '[PRE52]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: How to do it
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'Let’s again start with a simple `pd.DataFrame`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次从一个简单的`pd.DataFrame`开始：
- en: '[PRE53]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'You can use the `pd.DataFrame.to_excel` method to write this to a file, with
    the first argument typically being a filename like `myfile.xlsx`, but here, we
    will again use `io.BytesIO`, which acts like a file but stores binary data in
    memory instead of on disk:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`pd.DataFrame.to_excel`方法将其写入文件，通常第一个参数是文件名，例如`myfile.xlsx`，但在这里，我们将再次使用`io.BytesIO`，它像文件一样工作，但将二进制数据存储在内存中，而不是磁盘上：
- en: '[PRE55]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'For reading, reach for the `pd.read_excel` function. We will continue to use
    `dtype_backend="numpy_nullable"` to prevent the default type inference that pandas
    performs:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于读取操作，使用`pd.read_excel`函数。我们将继续使用`dtype_backend="numpy_nullable"`，以防止pandas执行默认的类型推断：
- en: '[PRE56]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Many of the function parameters are shared with CSV. To get rid of the `Unnamed:
    0` column above, we can either specify the `index_col=` argument:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '许多函数参数与CSV共享。为了去除上面提到的`Unnamed: 0`列，我们可以指定`index_col=`参数：'
- en: '[PRE58]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Or choose to not write the index in the first place:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 或者选择根本不写索引：
- en: '[PRE60]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Data types can be controlled with the `dtype=` argument:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型可以通过`dtype=`参数进行控制：
- en: '[PRE62]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Microsoft Excel – finding tables in non-default locations
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Microsoft Excel – 在非默认位置查找表格
- en: In the previous recipe, Microsoft Excel – basic reading/writing, we used the
    Microsoft Excel I/O functions without thinking about *where* within the worksheet
    our data was. By default, pandas will read from / write to the first cell on the
    first sheet of data, but it is not uncommon to receive Microsoft Excel files where
    the data you want to read is located elsewhere within the document.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个教程中，Microsoft Excel – 基本读写，我们使用了 Microsoft Excel 的 I/O 函数，而没有考虑数据在工作表中的位置。默认情况下，pandas
    会从/写入数据的第一个工作表的第一个单元格开始读取，但通常会收到数据位于文档其他位置的 Microsoft Excel 文件。
- en: 'For this example, we have a Microsoft Excel workbook where the very first tab,
    **Sheet1**, is used as a cover sheet:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们有一个 Microsoft Excel 工作簿，其中第一个选项卡**Sheet1**用作封面页：
- en: '![A screenshot of a computer  Description automatically generated](img/B31091_04_01.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![计算机的屏幕截图  自动生成描述](img/B31091_04_01.png)'
- en: 'Figure 4.1: Workbook where Sheet1 contains no useful data'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：Sheet1 中不包含有用数据的工作簿
- en: 'The second sheet is where we have useful information:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选项卡是我们有用的信息所在：
- en: '![A screenshot of a computer](img/B31091_04_02.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![计算机的屏幕截图](img/B31091_04_02.png)'
- en: 'Figure 4.2: Workbook where another sheet has relevant data'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：另一个工作表包含相关数据的工作簿
- en: How to do it
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'To still be able to read this data, you can use a combination of the `sheet_name=`,
    `skiprows=`, and `usecols=` arguments to `pd.read_excel`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了仍然能够读取这些数据，您可以使用`pd.read_excel`的`sheet_name=`、`skiprows=`和`usecols=`参数的组合：
- en: '[PRE64]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: By passing `sheet_name="the_data"`, the `pd.read_excel` function is able to
    pinpoint the specific sheet within the Microsoft Excel file to start looking for
    data. Alternatively, we could have used `sheet_name=1` to search by tab position.
    After locating the correct sheet, pandas looks at the `skiprows=` argument and
    knows to ignore rows 1–4 on the worksheet. It then looks at the `usecols=` argument
    to select only columns C–E.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递`sheet_name="the_data"`，`pd.read_excel`函数能够准确定位 Microsoft Excel 文件中要开始查找数据的特定工作表。或者，我们也可以使用`sheet_name=1`按选项卡位置搜索。在找到正确的工作表后，pandas
    查看`skiprows=`参数，并知道要忽略工作表上的第 1-4 行。然后查看`usecols=`参数，仅选择 C 到 E 列。
- en: There’s more…
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Instead of `usecols="C:E"`, we could have also provided the labels we wanted:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提供我们想要的标签，而不是`usecols="C:E"`：
- en: '[PRE66]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Passing such an argument to `usecols=` was a requirement when working with the
    CSV format to select particular columns from a file. However, pandas provides
    special behavior when reading Microsoft Excel files to allow strings like `"C:E"`
    or `"C,D,E"` to refer to columns.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 将这样的参数传递给`usecols=`是在处理 CSV 格式时选择文件中特定列的要求。然而，pandas 在读取 Microsoft Excel 文件时提供了特殊行为，允许像`"C:E"`或`"C,D,E"`这样的字符串引用列。
- en: Microsoft Excel – hierarchical data
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微软 Excel – 分层数据
- en: One of the major tasks with data analysis is to take very detailed information
    and aggregate it into a summary that is easy to digest. Rather than having to
    sift through thousands of orders, most executives at a company just want to know,
    “What have my sales looked like in the last X quarters?”
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析的主要任务之一是将非常详细的信息汇总为易于消化的摘要。大多数公司的高管不想要翻阅成千上万的订单，他们只想知道，“过去 X 个季度我的销售情况如何？”
- en: 'With Microsoft Excel, users will commonly summarize this information in a view
    like the one shown in *Figure 4.3*, which represents a hierarchy of `Region`/`Sub-Region`
    along the rows and `Year`/`Quarter` along the columns:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Microsoft Excel，用户通常会在类似*图 4.3*所示的视图中总结这些信息，该视图代表了行上的`区域`/`子区域`层次结构和列上的`年份`/`季度`：
- en: '![A screenshot of a spreadsheet](img/B31091_04_03.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![电子表格的屏幕截图](img/B31091_04_03.png)'
- en: 'Figure 4.3: Workbook with hierarchical data – sales by Region and Quarter'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：具有分层数据的工作簿 – 按区域和季度销售
- en: While this summary does not seem too far-fetched, many analysis tools struggle
    to properly present this type of information. Taking a traditional SQL database
    as an example, there is no direct way to represent this `Year`/`Quarter` hierarchy
    in a table – your only option would be to concatenate all of the hierarchy fields
    together and produce columns like `2024`/`Q1`, `2024`/`Q2`, `2025`/`Q1` and `2025`/`Q2`.
    While that makes it easy to select any individual column, you lose the ability
    to easily select things like “all of 2024 sales” without additional effort.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个总结似乎并不太离谱，但许多分析工具很难正确呈现这种类型的信息。以传统的SQL数据库为例，没有直接的方法来在表中表示这种`Year`/`Quarter`层次结构
    - 您唯一的选择是将所有层次结构字段连接在一起，并生成像`2024`/`Q1`、`2024`/`Q2`、`2025`/`Q1`和`2025`/`Q2`这样的列。虽然这样可以轻松选择任何单独的列，但您失去了轻松选择诸如“所有2024年销售额”之类的内容而不需要额外努力的能力。
- en: Fortunately, pandas can handle this a lot more sanely than a SQL database can,
    directly supporting such hierarchical relationships in both the row and column
    index. If you recall *Chapter 2*, *Selection and Assignment*, we introduced the
    `pd.MultiIndex`; being able to maintain those relationships allows users to efficiently
    select from any and all levels of the hierarchies.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，pandas可以比SQL数据库更理智地处理这个问题，直接支持行和列索引中的这种层次关系。如果您回忆起*第2章*，*选择和赋值*，我们介绍了`pd.MultiIndex`；能够保持这些关系使用户能够高效地从任何层次的层次结构中进行选择。
- en: How to do it
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: Upon closer inspection of *Figure 4.3*, you will see that rows 1 and 2 contain
    the labels `Year` and `Quarter`, which can form the levels of the `pd.MultiIndex`
    that we want in the columns of our `pd.DataFrame`. Microsoft Excel uses 1-based
    numbering of each row, so rows `[1, 2]` translated to Python would actually be
    `[0, 1]`; we will use this as our `header=` argument to establish that we want
    the first two rows to form our column `pd.MultiIndex`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细检查*图4.3*，您会看到第1行和第2行包含标签`Year`和`Quarter`，这些标签可以形成我们想要在`pd.DataFrame`的列中形成的`pd.MultiIndex`的级别。Microsoft
    Excel使用每行基于1的编号，因此行`[1, 2]`转换为Python实际上是`[0, 1]`；我们将使用这个作为我们的`header=`参数，以确立我们希望前两行形成我们的列`pd.MultiIndex`。
- en: 'Switching our focus to columns A and B in Microsoft Excel, we can now see the
    labels `Region` and `Sub-Region`, which will help us shape the `pd.MultiIndex`
    in our rows. Back in the *CSV – basic reading/writing* section, we introduced
    the `index_col=` argument, which can be used to tell pandas which column(s) of
    data should actually be used to generate the row index. Columns A and B from the
    Microsoft Excel file represent the first and second columns, so we can once again
    use `[0, 1]` to let pandas know our intent:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的注意力转向Microsoft Excel中的A列和B列，我们现在可以看到标签`Region`和`Sub-Region`，这将帮助我们在行中塑造`pd.MultiIndex`。回到*CSV
    - 基本读取/写入*部分，我们介绍了`index_col=`参数，它可以告诉pandas实际上应该使用哪些列数据来生成行索引。Microsoft Excel文件中的A列和B列代表第一列和第二列，因此我们可以再次使用`[0,
    1]`来告诉pandas我们的意图：
- en: '[PRE68]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Voila! We have successfully read in the data and maintained the hierarchical
    nature of the rows and columns, which lets us use all of the native pandas functionality
    to select from this data, and even answer questions like, “What does the Q2 performance
    look like year over year for every East Sub-Region?”
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 大功告成！我们成功读取了数据并保持了行和列的层次结构，这使我们可以使用所有原生pandas功能从这些数据中进行选择，甚至回答诸如“每个东部子区域的Q2业绩在年度上看起来如何？”这样的问题。
- en: '[PRE70]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: SQL using SQLAlchemy
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SQLAlchemy的SQL
- en: The pandas library provides robust capabilities for interacting with SQL databases,
    allowing you to perform data analysis directly on data stored in relational databases.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: pandas库提供了与SQL数据库交互的强大功能，使您可以直接在关系数据库中存储的数据上进行数据分析。
- en: There are, of course, countless databases that exist (and more are coming!),
    each with its own features, authentication schemes, dialects, and quirks. To interact
    with them, pandas relies on another great Python library, SQLAlchemy, which at
    its core acts as a bridge between Python and the database world. In theory, pandas
    can work with any database that SQLAlchemy can connect to.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，存在着无数的数据库（而且还会有更多！），每个数据库都有自己的特点、认证方案、方言和怪癖。为了与它们交互，pandas依赖于另一个伟大的Python库SQLAlchemy，它在核心上充当Python和数据库世界之间的桥梁。理论上，pandas可以与SQLAlchemy可以连接的任何数据库一起使用。
- en: 'To get started, you should first install SQLAlchemy into your environment:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，您应该首先将SQLAlchemy安装到您的环境中：
- en: '[PRE72]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: SQLAlchemy supports all major databases, like MySQL, PostgreSQL, MS SQL Server,
    etc., but setting up and properly configuring those databases is an effort in
    its own right, which cannot be covered within the scope of this book. To make
    things as simple as possible, we will focus on using SQLite as our database, as
    it requires no setup and can operate entirely within memory on your computer.
    Once you feel comfortable experimenting with SQLite, you only need to change the
    credentials you use to point to your target database; otherwise, all of the functionality
    remains the same.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: SQLAlchemy支持所有主要的数据库，如MySQL、PostgreSQL、MS SQL Server等，但设置和正确配置这些数据库本身需要不少努力，这部分内容超出了本书的范围。为了尽可能简化，我们将专注于使用SQLite作为我们的数据库，因为它不需要任何设置，且可以完全在计算机内存中运行。一旦你熟悉了SQLite的使用，你只需更改连接的凭据来指向目标数据库；否则，所有功能保持不变。
- en: How to do it
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤
- en: 'The first thing we need to do is create a SQLAlchemy engine, using `sa.create_engine`.
    The argument to this function is a URL and will be dependent upon the database
    you are trying to connect to (see the SQLAlchemy docs for more info). For these
    examples, we are going to use SQLite in memory:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是创建一个SQLAlchemy引擎，使用`sa.create_engine`。这个函数的参数是一个URL，取决于你试图连接的数据库（更多信息请参见SQLAlchemy文档）。在这些示例中，我们将使用内存中的SQLite：
- en: '[PRE73]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'With the `pd.DataFrame.to_sql` method, you can take an existing `pd.DataFrame`
    and write it to a database table. The first argument is the name of the table
    you would like to create, with the second argument being an engine/connectable:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pd.DataFrame.to_sql`方法，你可以将一个现有的`pd.DataFrame`写入数据库表中。第一个参数是你想创建的表的名称，第二个参数是引擎/可连接对象：
- en: '[PRE74]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The `pd.read_sql` function can be used to go in the opposite direction and
    read from a database table:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.read_sql`函数可以用于反向操作，从数据库表中读取数据：'
- en: '[PRE76]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Alternatively, if you wanted something different than just a copy of the table,
    you could pass a SQL query to `pd.read_sql`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，如果你想要的不是仅仅复制表，而是某个不同的内容，你可以将SQL查询传递给`pd.read_sql`：
- en: '[PRE78]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'When a table already exists in a database, trying to write to the same table
    again will raise an error. You can pass `if_exists="replace"` to override this
    behavior and replace the table:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据库中已经存在表时，再次向同一表写入数据将会引发错误。你可以传递`if_exists="replace"`来覆盖此行为并替换表：
- en: '[PRE80]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'You can also use `if_exists="append"` to add data to a table:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用`if_exists="append"`将数据添加到表中：
- en: '[PRE82]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: The bulk of the heavy lifting is done behind the scenes by the SQLAlchemy engine,
    which is constructed using a URL of the form `dialect+driver://username:password@host:port/database`.
    Not all of the fields in that URL are required – the string will depend largely
    on the database you are using and how it is configured.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 绝大部分繁重的工作都由SQLAlchemy引擎在幕后完成，该引擎使用`dialect+driver://username:password@host:port/database`形式的URL构建。并非所有的字段都是必需的——该字符串会根据你使用的数据库及其配置有所不同。
- en: In our specific example, `sa.create_engine("sqlite:///:memory:")` creates and
    connects to a SQLite database in the memory space of our computer. This feature
    is specific to SQLite; instead of `:memory:`, we could have also passed a path
    to a file on our computer like `sa.create_engine("sqlite:///tmp/adatabase.sql")`.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的具体示例中，`sa.create_engine("sqlite:///:memory:")`在我们计算机的内存空间中创建并连接到一个SQLite数据库。这个特性是SQLite特有的；我们也可以传递一个文件路径，比如`sa.create_engine("sqlite:///tmp/adatabase.sql")`，而不是使用`:memory:`。
- en: For more information on SQLAlchemy URLs and to get an idea of drivers to pair
    with other databases, see the SQLAlchemy Backend-Specific URLs documentation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于SQLAlchemy URL的信息，以及如何搭配其他数据库使用驱动程序，请参考SQLAlchemy的后端特定URL文档。
- en: SQL using ADBC
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ADBC的SQL
- en: While using SQLAlchemy to connect to databases is a viable option and has helped
    users of pandas for many years, a new technology has emerged out of the Apache
    Arrow project that can help scale your SQL interactions even further. This new
    technology is called **Arrow Database Connectivity**, or **ADBC** for short. Starting
    in version 2.2, pandas added support for using ADBC drivers to interact with databases.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用SQLAlchemy连接数据库是一种可行的选择，并且多年来一直帮助着pandas的用户，但来自Apache Arrow项目的一项新技术已经出现，它能进一步扩展SQL交互。这项新技术被称为**Arrow数据库连接**，简称**ADBC**。从2.2版本开始，pandas增加了对使用ADBC驱动程序与数据库交互的支持。
- en: Using ADBC will offer better performance and type safety when interacting with
    SQL databases than the aforementioned SQLAlchemy-based approach can. The trade-off
    is that SQLAlchemy has support for far more databases, so depending on your database,
    it may be the only option. ADBC maintains a record of its Driver Implementation
    Status; I would advise looking there first for a stable driver implementation
    for the database you are using before falling back on SQLAlchemy.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ADBC 在与 SQL 数据库交互时，比上述基于 SQLAlchemy 的方法能提供更好的性能和类型安全。权衡之下，SQLAlchemy 支持更多的数据库，因此根据你的数据库，SQLAlchemy
    可能是唯一的选择。ADBC 会记录其驱动实现状态；我建议在回退到 SQLAlchemy 之前，首先查看该记录，以确保你所使用的数据库有一个稳定的驱动实现。
- en: 'Much like in the previous section, we will use SQLite for our database, given
    its ease of use to set up and configure. Make sure to install the appropriate
    ADBC Python package for SQLite:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 和上一节一样，我们将使用 SQLite 作为数据库，因为它易于设置和配置。请确保为 SQLite 安装适当的 ADBC Python 包：
- en: '[PRE84]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: How to do it
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'Let’s start by importing the `dbapi` object from our SQLite ADBC driver and
    creating some sample data:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从我们的 SQLite ADBC 驱动中导入 `dbapi` 对象，并创建一些示例数据：
- en: '[PRE85]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The term `dbapi` is taken from the Python Database API Specification defined
    in PEP-249, which standardizes how Python modules and libraries should be used
    to interact with databases. Calling the `.connect` method with credentials is
    the standardized way to open up a database connection in Python. Once again, we
    will use an in-memory SQLite application via `dbapi.connect("file::memory:")`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 `dbapi` 来自 Python 数据库 API 规范（PEP-249），该规范标准化了 Python 模块和库如何与数据库交互。调用 `.connect`
    方法并提供凭据是 Python 中打开数据库连接的标准化方式。我们将再次通过 `dbapi.connect("file::memory:")` 使用内存中的
    SQLite 应用程序。
- en: 'By using the `with ... as:` syntax to use a context manager in Python, we can
    connect to a database and assign it to a variable, letting Python automatically
    clean up the connection when the block is finished. While the connection is open
    within our block, we can use `pd.DataFrame.to_sql` / `pd.read_sql` to write to
    and read from the database, respectively:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 Python 中使用 `with ... as:` 语法来使用上下文管理器，我们可以连接到一个数据库，并将其赋值给一个变量，这样 Python
    就会在块执行完毕后自动清理连接。在块内连接处于打开状态时，我们可以使用 `pd.DataFrame.to_sql` / `pd.read_sql` 分别向数据库写入和从数据库读取数据：
- en: '[PRE87]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'For smaller datasets, you may not see much of a difference, but the performance
    gains of ADBC will be drastic with larger datasets. Let’s compare the time to
    write a 10,000 by 10 `pd.DataFrame` using SQLAlchemy:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的数据集，你可能看不出太大差别，但在较大的数据集上，ADBC 的性能提升会非常明显。让我们比较一下使用 SQLAlchemy 写入一个 10,000
    行、10 列的 `pd.DataFrame` 所需的时间：
- en: '[PRE89]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'To equivalent code using ADBC:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ADBC 的等效代码：
- en: '[PRE91]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Your results will vary, depending on your data and database, but generally,
    ADBC should perform much faster.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你的结果会有所不同，具体取决于你的数据和数据库，但通常情况下，ADBC 应该会表现得更快。
- en: There’s more…
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: To understand what ADBC does and why it matters, it is first worth a quick history
    lesson in database standards and how they have evolved. Back in the 1990s, the
    **Open Database Connectivity** (**ODBC**) and **Java Database Connectivity** (**JDBC**)
    standards were introduced, which helped standardize how different clients could
    talk to various databases. Before the introduction of these standards, if you
    developed an application that needed to work with two or more different databases,
    your application would have to speak the exact language that each database understood
    to interact with it.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 ADBC 的作用以及它为何重要，首先值得简单回顾一下数据库标准的历史以及它们如何发展。在 1990 年代，**开放数据库连接**（**ODBC**）和
    **Java 数据库连接**（**JDBC**）标准被引入，这帮助标准化了不同客户端如何与各种数据库进行通信。在这些标准引入之前，如果你开发了一个需要与两个或更多不同数据库交互的应用程序，那么你的应用程序就必须使用每个数据库能理解的语言来与其交互。
- en: Imagine then that this application wanted to just get a listing of tables available
    in each database. A PostgreSQL database stores this information in a table called
    `pg_catalog.pg_tables`, whereas a SQLite database stores this in a `sqlite_schema`
    table where `type='table'`. The application would need to be developed with this
    particular information, and then it would need to be re-released every time a
    database changed how it stored this information, or if an application wanted to
    support a new database.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这个应用程序只想获取每个数据库中可用表格的列表。PostgreSQL 数据库将这些信息存储在名为 `pg_catalog.pg_tables` 的表中，而
    SQLite 数据库则将其存储在一个 `sqlite_schema` 表中，条件是 `type='table'`。这个应用程序需要根据这些特定的信息来开发，并且每当数据库更改了存储这些信息的方式，或者当应用程序想要支持新数据库时，都需要重新发布。
- en: With a standard like ODBC, the application instead just needs to communicate
    with a driver, letting the driver know that it wants all of the tables in the
    system. This shifts the onus of properly interacting with a database from the
    application itself to the driver, giving the application a layer of abstraction.
    As new databases or versions are released, the application itself no longer needs
    to change; it simply works with a new ODBC/JDBC driver and continues to work.
    SQLAlchemy, in fact, is just like this theoretical application; it interacts with
    databases through either ODBC/JDBC drivers, rather than trying to manage the endless
    array of database interactions on its own.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像 ODBC 这样的标准时，应用程序只需要与驱动程序进行通信，告知驱动程序它需要系统中的所有表格。这将数据库交互的责任从应用程序转移到驱动程序，给应用程序提供了一个抽象层。随着新数据库或新版本的发布，应用程序本身不再需要改变；它只需与新的
    ODBC/JDBC 驱动程序配合工作，继续正常运作。事实上，SQLAlchemy 就像这个理论中的应用程序；它通过 ODBC/JDBC 驱动程序与数据库交互，而不是试图独立管理无尽的数据库交互。
- en: While these standards are fantastic for many purposes, it is worth noting that
    databases were very different in the 1990s than they are today. Many of the problems
    that these standards tried to solve were aimed at row-oriented databases, which
    were prevalent at the time. Column-oriented databases arrived more than a decade
    later, and they have since come to dominate the analytics landscape. Unfortunately,
    without a column-oriented standard for transferring data, many of these databases
    had to retrofit a design that made them ODBC/JDBC-compatible. This allowed them
    to work with the countless database client tools in existence today but required
    a trade-off in performance in efficiency.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些标准在许多用途上非常出色，但值得注意的是，1990 年代的数据库与今天的数据库差异很大。许多这些标准试图解决的问题是针对当时盛行的行式数据库的。列式数据库是在十多年后才出现的，并且它们已经主导了数据分析领域。不幸的是，在没有列式数据传输标准的情况下，许多数据库不得不重新设计，使其兼容
    ODBC/JDBC。这使得它们能够与今天存在的无数数据库客户端工具兼容，但也需要在性能和效率上做出一定的妥协。
- en: ADBC is the column-oriented specification that solves this problem. The pandas
    library, and many similar offerings in the space, are explicitly (or at least
    very close to) being column-oriented in their designs. When interacting with columnar
    databases like BigQuery, Redshift, or Snowflake, having a column-oriented driver
    to exchange information can lead to orders of magnitude better performance. Even
    if you aren’t interacting with a column-oriented database, the ADBC driver is
    so finely optimized toward analytics with Apache Arrow that it *still* would be
    an upgrade over any ODBC/JDBC driver that SQLAlchemy would use.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ADBC 是解决这个问题的列式规范。pandas 库以及许多类似的产品在设计上明确地（或至少非常接近）采用了列式设计。当与列式数据库（如 BigQuery、Redshift
    或 Snowflake）交互时，拥有一个列式驱动程序来交换信息可以带来数量级更好的性能。即使你没有与列式数据库交互，ADBC 驱动程序也经过精细优化，专为与
    Apache Arrow 配合的分析工作，因此它 *仍然* 比 SQLAlchemy 使用的任何 ODBC/JDBC 驱动程序都要好。
- en: For users wanting to know more about ADBC, I recommend viewing my talk from
    PyData NYC 2023, titled Faster SQL with pandas and Apache Arrow, on YouTube ([https://youtu.be/XhnfybpWOgA?si=RBrM7UUvpNFyct0L](https://youtu.be/XhnfybpWOgA?si=RBrM7UUvpNFyct0L)).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于想要了解更多 ADBC 的用户，我建议观看我在 PyData NYC 2023 上的演讲，标题为《使用 pandas 和 Apache Arrow
    加速 SQL》，可以在 YouTube 上观看 ([https://youtu.be/XhnfybpWOgA?si=RBrM7UUvpNFyct0L](https://youtu.be/XhnfybpWOgA?si=RBrM7UUvpNFyct0L))。
- en: Apache Parquet
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Parquet
- en: 'As far as a generic storage format for a `pd.DataFrame` goes, Apache Parquet
    is the best option. Apache Parquet allows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 就 `pd.DataFrame` 的通用存储格式而言，Apache Parquet 是最佳选择。Apache Parquet 允许：
- en: Metadata storage – this allows the format to track data types, among other features
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据存储 —— 这使得格式能够追踪数据类型等特性
- en: Partitioning – not everything needs to be in one file
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区 – 不是所有数据都需要在一个文件中
- en: Query support – Parquet files can be queried on disk, so you don’t have to bring
    all data into memory
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询支持 – Parquet 文件可以在磁盘上进行查询，因此你不必将所有数据都加载到内存中
- en: Parallelization – reading data can be parallelized for higher throughput
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行化 – 读取数据可以并行化以提高吞吐量
- en: Compactness – data is compressed and stored in a highly efficient manner
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紧凑性 – 数据被压缩并以高效的方式存储
- en: Unless you are working with legacy systems, the Apache Parquet format should
    replace the use of CSV files in your workflows, from persisting data locally and
    sharing with other team members to exchanging data across systems.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你在处理遗留系统，否则 Apache Parquet 格式应该取代在工作流中使用 CSV 文件的方式，从本地持久化数据、与其他团队成员共享，到跨系统交换数据。
- en: How to do it
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: The API to read/write Apache Parquet is consistent with all other pandas APIs
    we have seen so far; for reading, there is `pd.read_parquet`, and for writing,
    there is a `pd.DataFrame.to_parquet` method.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 读取/写入 Apache Parquet 的 API 与我们到目前为止看到的所有 pandas API 一致；读取使用 `pd.read_parquet`，写入使用
    `pd.DataFrame.to_parquet` 方法。
- en: 'Let’s start with some sample data and an `io.BytesIO` object:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些示例数据和 `io.BytesIO` 对象开始：
- en: '[PRE93]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'This is how you would write to a file handle:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何写入文件句柄的方式：
- en: '[PRE95]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'And here is how you would read from a file handle. Note that we are intentionally
    not providing `dtype_backend="numpy_nullable"`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何从文件句柄中读取数据。注意我们故意没有提供 `dtype_backend="numpy_nullable"`：
- en: '[PRE96]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Why don’t we need the `dtype_backend=` argument with `pd.read_parquet`? Unlike
    a format like CSV, which only stores data, the Apache Parquet format stores both
    data and metadata. Within the metadata, Apache Parquet is able to keep track of
    the data types in use, so whatever data type you write should be exactly what
    you get back.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们在 `pd.read_parquet` 中不需要 `dtype_backend=` 参数？与像 CSV 这样的格式只存储数据不同，Apache
    Parquet 格式同时存储数据和元数据。在元数据中，Apache Parquet 能够追踪正在使用的数据类型，所以你写入的数据类型应该和你读取的数据类型完全一致。
- en: 'You can test this by changing the data type of the `birth` column to a different
    type:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将 `birth` 列的数据类型更改为不同类型来进行测试：
- en: '[PRE98]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Roundtripping this through the Apache Parquet format will give you back the
    same data type you started with:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Apache Parquet 格式进行回环操作会返回你最初使用的相同数据类型：
- en: '[PRE100]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Of course, if you want to be extra-defensive, there is no harm in using `dtype_backend="numpy_nullable"`
    here as well. We intentionally left it out at the start to showcase the power
    of the Apache Parquet format, but if you are receiving files from other sources
    and developers that don’t use the type system we recommended in *Chapter 3*, *Data
    Types*, it may be helpful to make sure you work with the best types pandas has
    to offer:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果你想更加防守型，在这里使用 `dtype_backend="numpy_nullable"` 也没有坏处。我们一开始故意没有使用它，是为了展示
    Apache Parquet 格式的强大，但如果你从其他来源和开发者那里接收文件，他们没有使用我们在*第三章*《数据类型》中推荐的类型系统，那么确保使用 pandas
    提供的最佳类型可能会对你有帮助：
- en: '[PRE102]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Another great feature of the Apache Parquet format is that it supports *partitioning*,
    which loosens the requirement that all your data is located in a single file.
    By being able to split data across different directories and files, partitioning
    allows users an easy way to organize their content, while also making it easier
    for a program to optimize which files it may or may not have to read to solve
    an analytical query.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet 格式的另一个伟大特点是它支持 *分区*，这打破了所有数据必须位于一个文件中的要求。通过能够将数据分割到不同的目录和文件中，分区使得用户可以轻松地组织内容，同时也使程序能够更高效地优化可能需要或不需要读取的文件来解决分析查询。
- en: 'There are many ways to partition your data each with practical space/time trade-offs.
    For demonstration purposes, we are going to assume the use of *time-based partitioning*,
    whereby individual files are generated for different time periods. With that in
    mind, let’s work with the following data layout, where we create different directories
    for each year and, within each year, create individual files for every quarter
    of sales:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以对数据进行分区，每种方法都有实际的空间/时间权衡。为了演示，我们假设使用 *基于时间的分区*，即为不同的时间段生成单独的文件。考虑到这一点，让我们使用以下数据布局，其中我们为每个年份创建不同的目录，并在每个年份内，为每个销售季度创建单独的文件：
- en: '[PRE104]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Each of the sample Apache Parquet files distributed with this book has already
    been created with the pandas extension types we recommended in *Chapter 3*, *Data
    Types* so the `pd.read_parquet` calls we make intentionally do not include the
    `dtype_backend="numpy_nullable"` argument. Within any file, you will see that
    we store information about the `year`, `quarter`, `region`, and overall `sales`
    that were counted:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 本书附带的每个示例 Apache Parquet 文件都已经使用我们在*第 3 章*（数据类型）中推荐的 pandas 扩展类型创建，因此我们进行的 `pd.read_parquet`
    调用故意不包括 `dtype_backend="numpy_nullable"` 参数。在任何文件中，你都会看到我们存储了关于 `year`（年份）、`quarter`（季度）、`region`（地区）和总的
    `sales`（销售额）等信息：
- en: '[PRE105]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'If we wanted to see all of this data together, a brute-force approach would
    involve looping over each file and accumulating the results. However, with the
    Apache Parquet format, pandas can natively and effectively handle this. Instead
    of passing individual file names to `pd.read_parquet`, it simply passes the path
    to the directory:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想查看所有数据的汇总，最直接的方法是遍历每个文件并累积结果。然而，使用 Apache Parquet 格式，pandas 可以本地有效地处理这个问题。与其将单个文件名传递给
    `pd.read_parquet`，不如传递目录路径：
- en: '[PRE107]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: Because our sample data is so small, we have no problem reading all of the data
    into a `pd.DataFrame` first and then working with it from there. However, in production
    deployments, you may end up working with Apache Parquet files that measure in
    gigabytes or terabytes’ worth of storage. Attempting to read all of that data
    into a `pd.DataFrame` may throw a `MemoryError`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的示例数据非常小，我们没有问题先将所有数据读取到 `pd.DataFrame` 中，然后从那里进行操作。然而，在生产环境中，你可能会遇到存储量达到吉字节或太字节的
    Apache Parquet 文件。试图将所有数据读取到 `pd.DataFrame` 中可能会导致 `MemoryError` 错误。
- en: 'Fortunately, the Apache Parquet format gives you the capability to filter records
    on the fly as files are read. From pandas, you can enable this functionality with
    `pd.read_parquet` by passing a `filters=` argument. The argument should be a list,
    where each list element is a tuple, which itself contains three elements:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Apache Parquet 格式使你能够在读取文件时动态过滤记录。在 pandas 中，你可以通过传递 `filters=` 参数来启用此功能，方法是使用
    `pd.read_parquet`。该参数应该是一个列表，其中每个列表元素是一个包含三个元素的元组：
- en: Column Name
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列名
- en: Logical Operator
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑运算符
- en: Value
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值
- en: 'For example, if we only wanted to read in data where our `region` column is
    equal to the value `Europe`, we could write this as:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们只想读取 `region` 列值为 `Europe` 的数据，可以这样写：
- en: '[PRE109]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: JSON
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JSON
- en: '**JavaScript Object Notation** (**JSON**) is a common format used to transfer
    data over the internet. The JSON specification can be found at [https://www.json.org](https://www.json.org/json-en.html).
    Despite the name, it does not require JavaScript to read or create.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**JavaScript 对象表示法**（**JSON**）是一种常用于通过互联网传输数据的格式。JSON 规范可以在 [https://www.json.org](https://www.json.org/json-en.html)
    上找到。尽管名称中有 JavaScript，但它不需要 JavaScript 来读取或创建。'
- en: 'The Python standard library ships with the `json` library, which can serialize
    and deserialize Python objects to/from JSON:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Python 标准库附带了 `json` 库，它可以将 Python 对象序列化为 JSON，或者从 JSON 反序列化回 Python 对象：
- en: '[PRE111]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: However, the standard library does not know how to deal with pandas objects,
    so pandas provides its own set of I/O functions specifically for JSON.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，标准库并不知道如何处理 pandas 对象，因此 pandas 提供了自己的 I/O 函数，专门用于处理 JSON。
- en: How to do it
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'In the simplest form, `pd.read_json` can be used to read JSON data:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的形式下，`pd.read_json` 可以用于读取 JSON 数据：
- en: '[PRE113]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'And the `pd.DataFrame.to_json` method can be used for writing:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.DataFrame.to_json` 方法可以用于写入：'
- en: '[PRE115]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: However, in practice, there are endless ways to represent tabular data in JSON.
    Some users may want to see each row of the `pd.DataFrame` represented as a JSON
    array, whereas other users may want to see each column shown as an array. Others
    may want to see the row index, column index, and data listed as separate JSON
    objects, whereas others may not care about seeing the row or column labels at
    all.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际应用中，存在无数种将表格数据表示为 JSON 的方式。有些用户可能希望看到 `pd.DataFrame` 中的每一行作为 JSON 数组，而另一些用户可能希望看到每一列作为数组。还有一些用户可能希望查看行索引、列索引和数据作为独立的
    JSON 对象列出，而其他用户可能根本不关心是否看到行或列标签。
- en: 'For these use cases and more, pandas allows you to pass an argument to `orient=`,
    whose value dictates the layout of the JSON to be read or written:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些用例以及更多用例，pandas 允许你传递一个参数给 `orient=`，其值决定了要读取或写入的 JSON 布局：
- en: '`columns` (default): Produces JSON objects, where the key is a column label
    and the value is another object that maps the row label to a data point.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`columns`（默认值）：生成 JSON 对象，其中键是列标签，值是另一个对象，该对象将行标签映射到数据点。'
- en: '`records`: Each row of the `pd.DataFrame` is represented as a JSON array, containing
    objects that map column names to a data point.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`records`：`pd.DataFrame` 的每一行都表示为一个 JSON 数组，其中包含将列名映射到数据点的对象。'
- en: '`split`: Maps to `{"columns": […], "index": […], "data": […]}`. Columns/index
    values are arrays of labels, and data contains arrays of arrays.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split`：映射到 `{"columns": […], "index": […], "data": […]}`。列/索引值是标签的数组，数据包含数组的数组。'
- en: '`index`: Similar to columns, except that the usage of row and column labels
    as keys is reversed.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index`：与列类似，不同之处在于行和列标签作为键的使用被反转。'
- en: '`values`: Maps the data of a `pd.DataFrame` to an array of arrays. Row/column
    labels are dropped.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`values`：将 `pd.DataFrame` 的数据映射到数组的数组中。行/列标签被丢弃。'
- en: '`table`: Adheres to the JSON Table Schema.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`table`：遵循 JSON 表格模式。'
- en: JSON is a *lossy* format for exchanging data, so each of the orients above is
    a trade-off between loss, verbosity, and end user requirements. `orient="table"`
    would be the least lossy and produce the largest payload, whereas `orient="values"`
    falls completely on the other end of that spectrum.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 是一种*有损*的数据交换格式，因此上述每种 `orient` 都是在损失、冗长性和最终用户需求之间的权衡。`orient="table"` 会最少损失数据，但会产生最大负载，而
    `orient="values"` 完全位于该范围的另一端。
- en: 'To highlight the differences in each of these orients, let’s begin with a rather
    simple `pd.DataFrame`:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出每种 `orient` 之间的差异，让我们从一个相当简单的 `pd.DataFrame` 开始：
- en: '[PRE117]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Passing `orient="columns"` will produce data using the pattern of `{"column":{"row":
    value, "row": value, ...}, ...}`:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '传递 `orient="columns"` 会生成使用以下模式的数据：`{"column":{"row": value, "row": value,
    ...}, ...}`：'
- en: '[PRE119]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'This is a rather verbose way of storing the data, as it will repeat the row
    index labels for every column. On the plus side, pandas can do a reasonably good
    job of reconstructing the proper `pd.DataFrame` from this `orient`:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种相当冗长的存储数据方式，因为它会为每一列重复行索引标签。优点是，pandas 可以相对较好地从这种 `orient` 中重建正确的 `pd.DataFrame`：
- en: '[PRE121]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'With `orient="records"`, you end up with each row of the `pd.DataFrame` being
    represented without its row index label, yielding a pattern of `[{"col": value,
    "col": value, ...}, ...]`:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 `orient="records"` 时，你会得到每一行 `pd.DataFrame` 的表示，不带行索引标签，形成一个模式 `[{"col":
    value, "col": value, ...}, ...]`：'
- en: '[PRE123]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'While this representation is more compact than `orient="columns"`, it does
    not store any row labels, so on reconstruction, you will get back a `pd.DataFrame`
    with a newly generated `pd.RangeIndex`:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种表示方式比 `orient="columns"` 更紧凑，但它不存储任何行标签，因此在重建时，你将得到一个带有新生成 `pd.RangeIndex`
    的 `pd.DataFrame`：
- en: '[PRE125]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'With `orient="split"`, the row index labels, column index labels, and data
    are all stored separately:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `orient="split"` 时，行索引标签、列索引标签和数据会分别存储：
- en: '[PRE127]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'This format uses a relatively lesser amount of characters than `orient="columns"`,
    and you can still recreate a `pd.DataFrame` reasonably well, since it mirrors
    how you would build a `pd.DataFrame` using the constructor (with arguments like
    `pd.DataFrame(data, index=index, columns=columns)`):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这种格式使用的字符比 `orient="columns"` 少，而且你仍然可以相对较好地重建一个 `pd.DataFrame`，因为它与你使用构造函数（如
    `pd.DataFrame(data, index=index, columns=columns)`）构建 `pd.DataFrame` 的方式相似：
- en: '[PRE129]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: While this is a good format when roundtripping a `pd.DataFrame`, the odds of
    coming across this JSON format “in the wild” are much lower as compared to other
    formats.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个很好的格式，用于双向转换 `pd.DataFrame`，但与其他格式相比，在“野外”遇到这种 JSON 格式的可能性要低得多。
- en: '`orient="index"` is very similar to `orient="columns"`, but it reverses the
    roles of the row and column labels:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`orient="index"` 与 `orient="columns"` 非常相似，但它反转了行和列标签的角色：'
- en: '[PRE131]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'Once again, you can recreate your `pd.DataFrame` reasonably well:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，你可以合理地重建你的 `pd.DataFrame`：
- en: '[PRE133]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: Generally, `orient="index"` will take up more space than `orient="columns"`,
    since most `pd.DataFrame` objects use column labels that are more verbose than
    index labels. I would only advise using this format in the possibly rare instances
    where your column labels are less verbose, or if you have strict formatting requirements
    imposed by another system.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，`orient="index"` 会比 `orient="columns"` 占用更多空间，因为大多数 `pd.DataFrame` 对象使用的列标签比索引标签更冗长。我建议仅在列标签不那么冗长，或者有其他系统强制要求特定格式的情况下使用此格式。
- en: 'For the most minimalistic representation, you can opt for `orient="values"`.
    With this `orient`, neither row nor column labels are preserved:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最简洁的表示方式，你可以选择 `orient="values"`。使用此 `orient`，既不保存行标签，也不保存列标签：
- en: '[PRE135]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'Of course, since they are not represented in the JSON data, you will not maintain
    row/column labels when reading with `orient="values"`:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，由于它们没有在 JSON 数据中表示，当使用 `orient="values"` 读取时，你将无法保留行/列标签：
- en: '[PRE137]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'Finally, we have `orient="table"`. This will be the most verbose out of all
    of the outputs, but it is the only one backed by an actual standard, which is
    called the JSON Table Schema:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有了 `orient="table"`。这是所有输出中最冗长的一种，但它是唯一一个有实际标准支持的输出，标准叫做 JSON 表格模式：
- en: '[PRE139]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'The Table Schema is more verbose because it stores metadata about the data
    being serialized, similar to what we saw with the Apache Parquet format (although
    with fewer features than Apache Parquet). With all of the other `orient=` arguments,
    pandas would have to infer the type of data as it is being read, but the JSON
    Table Format preserves that information for you. As such, you don’t even need
    the `dtype_backend="numpy_nullable"` argument, assuming you used the pandas extension
    types to begin with:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 表格模式更冗长，因为它存储了关于数据序列化的元数据，类似于我们在 Apache Parquet 格式中看到的内容（尽管功能不如 Apache Parquet）。对于所有其他的
    `orient=` 参数，pandas 必须在读取时推断数据的类型，但 JSON 表格格式会为你保留这些信息。因此，假设你一开始就使用了 pandas 扩展类型，你甚至不需要
    `dtype_backend="numpy_nullable"` 参数：
- en: '[PRE141]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: There’s more...
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: When attempting to read JSON, you may find that none of the above formats can
    still sufficiently express what you are trying to accomplish. Fortunately, there
    is still `pd.json_normalize`, which can act as a workhorse function to convert
    your JSON data into a tabular format.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 当尝试读取 JSON 时，你可能会发现以上格式仍然无法充分表达你想要实现的目标。幸运的是，仍然有 `pd.json_normalize`，它可以作为一个功能强大的函数，将你的
    JSON 数据转换为表格格式。
- en: 'Imagine working with the following JSON data from a theoretical REST API with
    pagination:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，正在处理来自一个理论上的带有分页的 REST API 的以下 JSON 数据：
- en: '[PRE143]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'While the `"pagination"` key is useful for navigating the API, it is of little
    reporting value to us and can trip up the JSON serializers. What we actually care
    about is the array associated with the `"records"` key. You can direct `pd.json_normalize`
    to look at this data exclusively, using the `record_path=` argument. Please note
    that `pd.json_normalize` is not a true I/O function, since it deals with Python
    objects and not file handles, so it has no `dtype_backend=` argument; instead,
    we will chain in a `pd.DataFrame.convert_dtypes` call to get the desired pandas
    extension types:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `"pagination"` 键对于导航 API 很有用，但对于我们来说报告价值不大，而且它可能会导致 JSON 序列化器出现问题。我们真正关心的是与
    `"records"` 键相关的数组。你可以指示 `pd.json_normalize` 只查看这些数据，使用 `record_path=` 参数。请注意，`pd.json_normalize`
    不是一个真正的 I/O 函数，因为它处理的是 Python 对象而不是文件句柄，因此没有 `dtype_backend=` 参数；相反，我们将链接一个 `pd.DataFrame.convert_dtypes`
    调用，以获得所需的 pandas 扩展类型：
- en: '[PRE144]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'By providing the `record_path=` argument, we were able to ignore the undesired
    `"pagination"` key, but unfortunately, we now have the side effect of dropping
    the `"type"` key, which contained valuable metadata about each record. To preserve
    this information, you can use the `meta=` argument:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供 `record_path=` 参数，我们能够忽略不需要的 `"pagination"` 键，但不幸的是，我们现在有了一个副作用，就是丢失了包含每条记录重要元数据的
    `"type"` 键。为了保留这些信息，你可以使用 `meta=` 参数：
- en: '[PRE146]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: HTML
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HTML
- en: You can use pandas to read HTML tables from websites. This makes it easy to
    ingest tables such as those found on Wikipedia.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 pandas 从网站读取 HTML 表格。这使得获取像 Wikipedia 上的表格变得容易。
- en: 'In this recipe, we will scrape tables from the Wikipedia entry for *The Beatles
    Discography* ([https://en.wikipedia.org/wiki/The_Beatles_discography](https://en.wikipedia.org/wiki/The_Beatles_discography)).
    In particular, we want to scrape the table in the image that was on Wikipedia
    in 2024:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将从 Wikipedia 上关于 *The Beatles Discography* 的页面抓取表格 ([https://en.wikipedia.org/wiki/The_Beatles_discography](https://en.wikipedia.org/wiki/The_Beatles_discography))。特别是，我们想要抓取
    2024 年 Wikipedia 上图片中展示的表格：
- en: '![A screenshot of a chart](img/B31091_04_04.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![一张图表的截图](img/B31091_04_04.png)'
- en: 'Figure 4.4: Wikipedia page for The Beatles Discography'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：The Beatles Discography 的 Wikipedia 页面
- en: 'Before attempting to read HTML, users will need to install a third-party library.
    For the examples in this section, we will use `lxml`:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试读取 HTML 之前，用户需要安装一个第三方库。在本节的示例中，我们将使用 `lxml`：
- en: '[PRE148]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: How to do it
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: '`pd.read_html` allows you to read a table from a website:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.read_html` 允许你从网站读取表格：'
- en: '[PRE149]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'Contrary to the other I/O methods we have seen so far, `pd.read_html` doesn’t
    return a `pd.DataFrame` but, instead, returns a list of `pd.DataFrame` objects.
    Let’s see what the first list element looks like:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们到目前为止看到的其他I/O方法不同，`pd.read_html`不会返回一个`pd.DataFrame`，而是返回一个`pd.DataFrame`对象的列表。让我们看看第一个列表元素是什么样子的：
- en: '[PRE151]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: The preceding table is a summary of the count of studio albums, live albums,
    compilation albums, and so on. This is not the table we wanted. We could loop
    through each of the tables that `pd.read_html` created, or we could give it a
    hint to find a specific table.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 上述表格是工作室专辑、现场专辑、合辑专辑等的统计摘要。这不是我们想要的表格。我们可以循环遍历`pd.read_html`创建的每个表格，或者我们可以给它一个提示，以找到特定的表格。
- en: 'One way of getting the table we want would be to leverage the `attrs=` argument
    of `pd.read_html`. This parameter accepts a dictionary mapping HTML attributes
    to values. Because an `id` attribute in HTML is supposed to be unique within a
    document, trying to find a table with `attrs={"id": ...}` is usually a safe approach.
    Let’s see if we can get that to work here.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '获取我们想要的表格的一种方法是利用`pd.read_html`的`attrs=`参数。此参数接受一个将HTML属性映射到值的字典。因为HTML中的`id`属性应该在文档中是唯一的，尝试使用`attrs={"id":
    ...}`来查找表格通常是一个安全的方法。让我们看看我们能否在这里做到这一点。'
- en: Use your web browser to inspect the HTML of the web page (if you are unsure
    how to do this, search online for terms like *Firefox inspector*, *Safari Web
    Inspector*, or *Google Chrome DevTools*; the terminology is unfortunately not
    standardized). Look for any `id` fields, unique strings, or attributes of the
    table element that help us identify the table we are after.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你的网络浏览器检查网页的HTML（如果你不知道如何做到这一点，请在网上搜索诸如*Firefox inspector*、*Safari Web Inspector*或*Google
    Chrome DevTools*之类的术语；不幸的是，术语并不标准化）。寻找任何`id`字段、唯一字符串或帮助我们识别所需表格的表格元素属性。
- en: 'Here is a portion of the raw HTML:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这是原始HTML的一部分：
- en: '[PRE153]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: Unfortunately, the table we are looking for does *not* have an `id` attribute.
    We could try using either the `class` or `style` attributes we see in the HTML
    snippet above, but chances are those won’t be unique.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们正在寻找的表格没有`id`属性。我们可以尝试使用上述HTML片段中看到的`class`或`style`属性，但这些可能不会是唯一的。
- en: 'Another parameter we can try is `match=`, which can be a string or a regular
    expression and matches against the table contents. In the `<caption>` tag of the
    above HTML, you will see the text `"List of studio albums"`; let’s try that as
    an argument. To help readability, we are just going to look at each Album and
    its performance in the UK, AUS, and CAN:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试的另一个参数是`match=`，它可以是一个字符串或正则表达式，用来匹配表格内容。在上述HTML的`<caption>`标签中，你会看到文本`"List
    of studio albums"`；让我们将其作为一个参数试一试。为了提高可读性，我们只需查看每张专辑及其在英国、澳大利亚和加拿大的表现：
- en: '[PRE154]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: 'While we are able to now find the table, the column names are less than ideal.
    If you look closely at the Wikipedia table, you will notice that it partially
    creates a hierarchy between the `Peak chart positions` text and the name of countries
    below it, which pandas turns into a `pd.MultiIndex`. To make our table easier
    to read, we can pass `header=1` to ignore the very first level of the generated
    `pd.MultiIndex`:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们现在能够找到表格，但列名不太理想。如果你仔细观察维基百科的表格，你会注意到它在`Peak chart positions`文本和下面国家名称之间部分创建了一个层次结构，这些内容会被pandas转换为`pd.MultiIndex`。为了使我们的表格更易读，我们可以传递`header=1`来忽略生成的`pd.MultiIndex`的第一个级别：
- en: '[PRE156]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: 'As we look closer at the data, we can see that Wikipedia uses `—` to represent
    missing values. If we pass this as an argument to the `na_values=` parameter of
    `pd.read_html`, we will see the `=—=` values converted to missing values:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们更仔细地查看数据时，我们可以看到维基百科使用`—`来表示缺失值。如果我们将其作为参数传递给`pd.read_html`的`na_values=`参数，我们将看到`=—=`值被转换为缺失值：
- en: '[PRE158]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: Pickle
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pickle
- en: The pickle format is Python’s built-in serialization format. Pickle files typically
    end with a `.pkl` extension.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: Pickle格式是Python的内置序列化格式。Pickle文件通常以`.pkl`扩展名结尾。
- en: Unlike other formats encountered so far, the pickle format should not be used
    to transfer data across machines. The main use case is for saving pandas objects
    *that themselves contain Python objects* to your own machine, returning to them
    at a later point in time. If you are unsure if you should be using this format
    or not, I would advise trying the Apache Parquet format first, which covers a
    wider array of use cases.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前遇到的其他格式不同，pickle 格式不应该用于在机器间传输数据。它的主要使用场景是将*包含 Python 对象*的 pandas 对象保存在本地机器上，并在以后重新加载。如果你不确定是否应该使用这个格式，我建议你首先尝试
    Apache Parquet 格式，它涵盖了更广泛的使用场景。
- en: '**Do not load pickle files from untrusted sources**. I would generally only
    advise using pickle for your own analyses; do not share data or expect to receive
    data from others in the pickle format.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '**不要从不可信来源加载 pickle 文件**。我通常只建议将 pickle 用于自己的分析；不要分享数据，也不要指望从别人那里收到 pickle
    格式的数据。'
- en: How to do it
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'To highlight that the pickle format should really only be used when your pandas
    objects contain Python objects, let’s imagine we decided to store our Beatles
    data as a `pd.Series` of `namedtuple` types. It is a fair question as to *why*
    you would do this in the first place, as it would be better represented as a `pd.DataFrame`…
    but questions aside, it is valid to do so:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强调 pickle 格式应该仅在 pandas 对象包含 Python 对象时使用，假设我们决定将 Beatles 数据存储为一个包含 `namedtuple`
    类型的 `pd.Series`。人们可能会质疑*为什么*要这么做，因为它更适合表示为 `pd.DataFrame`……但不论如何，这样做是有效的：
- en: '[PRE160]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: 'None of the other I/O methods we have discussed in this chapter would be able
    to faithfully represent a `namedtuple`, which is purely a Python construct. `pd.Series.to_pickle`,
    however, has no problem writing this out:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章讨论的其他 I/O 方法都无法准确表示`namedtuple`，因为它是纯 Python 构造。然而，`pd.Series.to_pickle`却能够顺利写出这个内容：
- en: '[PRE162]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: 'When you call `pd.read_pickle`, you will get the exact representation you started
    with returned:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用 `pd.read_pickle` 时，你将获得你开始时所使用的确切表示：
- en: '[PRE163]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'You can further validate this by inspecting an individual element:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过检查单个元素进一步验证这一点：
- en: '[PRE165]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: Once again, it is worth stressing that the Apache Parquet format should be preferred
    to pickle, only using this as a last resort when Python-specific objects within
    your `pd.Series` or `pd.DataFrame` need to be roundtripped. Be sure to **never
    load pickle files from untrusted sources**; unless you created the pickle file
    yourself, I would highly advise against trying to process it.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，Apache Parquet 格式应优先于 pickle，只有在 `pd.Series` 或 `pd.DataFrame` 中包含 Python
    特定对象且需要回传时，才应作为最后的选择使用。务必**不要从不可信来源加载 pickle 文件**；除非你自己创建了该 pickle 文件，否则强烈建议不要尝试处理它。
- en: Third-party I/O libraries
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三方 I/O 库
- en: While pandas covers an impressive amount of formats it cannot hope to cover
    *every* important format out there. Third-party libraries exist to cover that
    gap.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 pandas 支持大量格式，但它无法涵盖所有*重要*格式。为此，第三方库应运而生，填补了这个空白。
- en: 'Here are a few you may be interested in – the details of how they work are
    outside the scope of this book, but they all generally follow the pattern of having
    read functions that return `pd.DataFrame` objects and write methods that accept
    a `pd.DataFrame` argument:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些你可能感兴趣的库——它们的工作原理超出了本书的范围，但它们通常遵循的模式是：具有读取函数返回`pd.DataFrame`对象，并且写入方法接受`pd.DataFrame`参数：
- en: pandas-gbq allows you to exchange data with Google BigQuery
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas-gbq 让你与 Google BigQuery 交换数据
- en: AWS SDK for pandas works with Redshift and the AWS ecosystem at large
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS SDK for pandas 可以与 Redshift 以及更广泛的 AWS 生态系统一起使用
- en: Snowflake Connector for Python helps exchange with Snowflake databases
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snowflake Connector for Python 帮助与 Snowflake 数据库交换数据
- en: 'pantab lets you move `pd.DataFrame` objects in and out of Tableau’s Hyper database
    format (note: I am also the author of pantab)'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pantab 让你将 `pd.DataFrame` 对象在 Tableau 的 Hyper 数据库格式中进出（注意：我也是 pantab 的作者）
- en: Join our community on Discord
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 讨论区，与作者和其他读者互动：
- en: '[https://packt.link/pandas](https://packt.link/pandas)'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/pandas](https://packt.link/pandas)'
- en: '![](img/QR_Code5040900042138312.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5040900042138312.png)'
