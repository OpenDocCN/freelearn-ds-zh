- en: Getting Your Big Data into the Spark Environment Using RDDs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RDD将大数据导入Spark环境
- en: Primarily, this chapter will provide a brief overview of how to get your big
    data into the Spark environment using **resilient distributed datasets** (**RDDs**).
    We will be using a wide array of tools to interact with and modify this data so
    that useful insights can be extracted. We will first load the data on Spark RDDs
    and then carry out parallelization with Spark RDDs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 主要是，本章将简要介绍如何使用**弹性分布式数据集**（**RDDs**）将大数据导入Spark环境。我们将使用各种工具来与和修改这些数据，以便提取有用的见解。我们将首先将数据加载到Spark
    RDD中，然后使用Spark RDD进行并行化。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Loading data onto Spark RDDs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据加载到Spark RDD中
- en: Parallelization with Spark RDDs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark RDD进行并行化
- en: Basics of RDD operation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD操作的基础知识
- en: Loading data on to Spark RDDs
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据加载到Spark RDD中
- en: 'In this section, we are going to look at loading data on to Spark RDDs, and
    will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看看如何将数据加载到Spark RDD中，并将涵盖以下主题：
- en: The UCI machine learning data repository
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UCI机器学习数据库
- en: Getting data from the repository to Python
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从存储库将数据导入Python
- en: Getting data into Spark
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据导入Spark
- en: Let's start with an overview of the UCI machine learning data repository.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先概述一下UCI机器学习数据库。
- en: The UCI machine learning repository
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UCI机器学习库
- en: We can access the UCI machine learning repository by navigating to [https://archive.ics.uci.edu/ml/](https://archive.ics.uci.edu/ml/). So,
    what is the UCI machine learning repository? UCI stands for the University of
    California Irvine machine learning repository, and it is a very useful resource
    for getting open source and free datasets for machine learning. Although PySpark's
    main issue or solution doesn't concern machine learning, we can use this as a
    chance to get big datasets that help us test out the functions of PySpark.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过导航到[https://archive.ics.uci.edu/ml/](https://archive.ics.uci.edu/ml/)来访问UCI机器学习库。那么，UCI机器学习库是什么？UCI代表加州大学尔湾分校机器学习库，它是一个非常有用的资源，可以获取用于机器学习的开源和免费数据集。尽管PySpark的主要问题或解决方案与机器学习无关，但我们可以利用这个机会获取帮助我们测试PySpark功能的大型数据集。
- en: Let's take a look at the KDD Cup 1999 dataset, which we will download, and then
    we will load the whole dataset into PySpark.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下KDD Cup 1999数据集，我们将下载，然后将整个数据集加载到PySpark中。
- en: Getting the data from the repository to Spark
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据从存储库加载到Spark
- en: 'We can follow these steps to download the dataset and load it in PySpark:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照以下步骤下载数据集并将其加载到PySpark中：
- en: Click on Data Folder.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击数据文件夹。
- en: 'You will be redirected to a folder that has various files as follows:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将被重定向到一个包含各种文件的文件夹，如下所示：
- en: '![](img/e7fb27a2-161d-4d0a-bddf-1955206aab1d.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7fb27a2-161d-4d0a-bddf-1955206aab1d.png)'
- en: You can see that there's kddcup.data.gz, and there is also 10% of that data
    available in kddcup.data_10_percent.gz. We will be working with food datasets.
    To work with the food datasets, right-click on kddcup.data.gz, select Copy link
    address, and then go back to the PySpark console and import the data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到有kddcup.data.gz，还有kddcup.data_10_percent.gz中的10%数据。我们将使用食品数据集。要使用食品数据集，右键单击kddcup.data.gz，选择复制链接地址，然后返回到PySpark控制台并导入数据。
- en: 'Let''s take a look at how this works using the following steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用以下步骤：
- en: 'After launching PySpark, the first thing we need to do is import `urllib`,
    which is a library that allows us to interact with resources on the internet,
    as follows:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动PySpark后，我们需要做的第一件事是导入`urllib`，这是一个允许我们与互联网上的资源进行交互的库，如下所示：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The next thing to do is use this `request` library to pull some resources from
    the internet, as shown in the following code:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来要做的是使用这个`request`库从互联网上拉取一些资源，如下面的代码所示：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This command will take some time to process. Once the file has been downloaded,
    we can see that Python has returned and the console is active.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将需要一些时间来处理。一旦文件被下载，我们可以看到Python已经返回，控制台是活动的。
- en: 'Next, load this using `SparkContext`. So, `SparkContext` is materialized or
    objectified in Python as the `sc` variable, as follows:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`SparkContext`加载这个。所以，在Python中，`SparkContext`被实例化或对象化为`sc`变量，如下所示：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This output is as demonstrated in the following code snippet:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出如下面的代码片段所示：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Getting data into Spark
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据导入Spark
- en: 'Next, load the KDD cup data into PySpark using `sc`, as shown in the following
    command:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`sc`将KDD cup数据加载到PySpark中，如下面的命令所示：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the following command, we can see that the raw data is now in the `raw_data`
    variable:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下面的命令中，我们可以看到原始数据现在在`raw_data`变量中：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This output is as demonstrated in the following code snippet:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出如下面的代码片段所示：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If we enter the `raw_data` variable, it gives us details regarding `kddcup.data.gz`,
    where raw data underlying the data file is located, and tells us about `MapPartitionsRDD.`
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们输入`raw_data`变量，它会给我们关于`kddcup.data.gz`的详细信息，其中包含数据文件的原始数据，并告诉我们关于`MapPartitionsRDD`。
- en: Now that we know how to load the data into Spark, let's learn about parallelization
    with Spark RDDs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何将数据加载到Spark中，让我们学习一下如何使用Spark RDD进行并行化。
- en: Parallelization with Spark RDDs
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark RDD进行并行化
- en: Now that we know how to create RDDs within the text file that we received from
    the internet, we can look at a different way to create this RDD. Let's discuss
    parallelization with our Spark RDDs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何在从互联网接收的文本文件中创建RDD，我们可以看一种不同的创建这个RDD的方法。让我们讨论一下如何使用我们的Spark RDD进行并行化。
- en: 'In this section, we will cover the following topics:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将涵盖以下主题：
- en: What is parallelization?
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是并行化？
- en: How do we parallelize Spark RDDs?
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何将Spark RDD并行化？
- en: Let's start with parallelization.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从并行化开始。
- en: What is parallelization?
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是并行化？
- en: The best way to understand Spark, or any language, is to look at the documentation.
    If we look at Spark's documentation, it clearly states that, for the `textFile`
    function that we used last time, it reads the text file from HDFS.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 了解Spark或任何语言的最佳方法是查看文档。如果我们查看Spark的文档，它清楚地说明，对于我们上次使用的`textFile`函数，它从HDFS读取文本文件。
- en: On the other hand, if we look at the definition of `parallelize`, we can see
    that this is creating an RDD by distributing a local Scala collection.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们看一下`parallelize`的定义，我们可以看到这是通过分发本地Scala集合来创建RDD。
- en: So, the main difference between using `parallelize` to create an RDD and using
    the `textFile` to create an RDD is where the data is sourced from.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`parallelize`创建RDD和使用`textFile`创建RDD之间的主要区别在于数据的来源。
- en: Let's look at how this works practically. Let's go to the PySpark installation
    screen, from where we left off previously. So, we imported `urllib`, we used `urllib.request`
    to retrieve some data from the internet, and we used `SparkContext` and `textFile`
    to load this data into Spark. The other way to do this is to use `parallelize`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是如何实际工作的。让我们回到之前离开的PySpark安装屏幕。因此，我们导入了`urllib`，我们使用`urllib.request`从互联网检索一些数据，然后我们使用`SparkContext`和`textFile`将这些数据加载到Spark中。另一种方法是使用`parallelize`。
- en: 'Let''s look at how we can do this. Let''s first assume that our data is already
    in Python, and so, for demonstration purposes, we are going to create a Python
    list of a hundred numbers as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们可以如何做到这一点。让我们首先假设我们的数据已经在Python中，因此，为了演示目的，我们将创建一个包含一百个数字的Python列表如下：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This gives us the following output:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For example, if we look at `a`, it is simply a list of 100 numbers. If we convert
    this into a `list`, it will show us the list of 100 numbers:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们看一下`a`，它只是一个包含100个数字的列表。如果我们将其转换为`list`，它将显示我们的100个数字的列表：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This gives us the following output:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following command shows us how to turn this into an RDD:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令向我们展示了如何将其转换为RDD：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If we look at what `list_rdd` contains, we can see that it is `PythonRDD.scala:52`,
    so, this tells us that the Scala-backed PySpark instance has recognized this as
    a Python-created RDD, as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下`list_rdd`包含什么，我们可以看到它是`PythonRDD.scala:52`，因此，这告诉我们Scala支持的PySpark实例已经识别出这是一个由Python创建的RDD，如下所示：
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This gives us the following output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, let''s look at what we can do with this list. The first thing we can do
    is count how many elements are present in `list_rdd` by using the following command:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们可以用这个列表做什么。我们可以做的第一件事是通过以下命令计算`list_rdd`中有多少元素：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This gives us the following output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can see that `list_rdd` is counted at 100\. If we run it again without cutting
    through into the results, we can actually see that, since Scala is running in
    a real time when going through the RDD, it is slower than just running the length
    of `a`, which is instant.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`list_rdd`计数为100。如果我们再次运行它而不切入结果，我们实际上可以看到，由于Scala在遍历RDD时是实时运行的，它比只运行`a`的长度要慢，后者是瞬时的。
- en: However, RDD takes some time, because it needs time to go through the parallelized
    version of the list. So, at small scales, where there are only a hundred numbers,
    it might not be very helpful to have this trade-off, but with larger amounts of
    data and larger individual sizes of the elements of the data, it will make a lot
    more sense.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，RDD需要一些时间，因为它需要时间来遍历列表的并行化版本。因此，在小规模的情况下，只有一百个数字，可能没有这种权衡非常有帮助，但是对于更大量的数据和数据元素的更大个体大小，这将更有意义。
- en: 'We can also take an arbitrary amount of elements from the list, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从列表中取任意数量的元素，如下所示：
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This gives us the following output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: When we run the preceding command, we can see that PySpark has performed some
    calculations before returning the first ten elements of the list. Notice that
    all of this is now backed by PySpark, and we are using Spark's power to manipulate
    this list of 100 items.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行上述命令时，我们可以看到PySpark在返回列表的前十个元素之前进行了一些计算。请注意，所有这些现在都由PySpark支持，并且我们正在使用Spark的功能来操作这个包含100个项目的列表。
- en: 'Let''s now use the reduce function in `list_rdd`, or in RDDs in general, to
    demonstrate what we can do with PySpark''s RDDs. We will apply two parameter functions
    as an anonymous `lambda` function to the `reduce` call as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在`list_rdd`中使用`reduce`函数，或者在RDDs中一般使用，来演示我们可以用PySpark的RDDs做什么。我们将两个参数函数应用为匿名的`lambda`函数到`reduce`调用如下：
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, `lambda` takes two parameters, `a` and `b`. It simply adds these two numbers
    together, hence `a+b`, and returns the output. With the RDD `reduce` call, we
    can sequentially add the first two numbers of RDD lists together, return the results,
    and then add the third number to the results, and so on. So, eventually, you add
    all 100 numbers to the same results by using `reduce`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`lambda`接受两个参数`a`和`b`。它简单地将这两个数字相加，因此`a+b`，并返回输出。通过`RDD`的`reduce`调用，我们可以依次将RDD列表的前两个数字相加，返回结果，然后将第三个数字添加到结果中，依此类推。因此，最终，通过使用`reduce`，您可以将所有100个数字添加到相同的结果中。
- en: Now, after some work through the distributed database, we can now see that adding
    numbers from `0` to `99` gives us `4950`, and it is all done using PySpark's RDD
    methodology. You might recognize this function from the term MapReduce, and, indeed,
    it's the same thing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在通过分布式数据库进行一些工作之后，我们现在可以看到，从`0`到`99`的数字相加得到`4950`，并且所有这些都是使用PySpark的RDD方法完成的。您可能会从MapReduce这个术语中认出这个函数，确实，它就是这样。
- en: We have just learned what parallelization is in PySpark, and how we can parallelize
    Spark RDDs. This effectively amounts to another way for us to create RDDs, and
    that's very useful for us. Now, let's look at some basics of RDD operation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了在PySpark中并行化是什么，以及我们如何可以并行化Spark RDDs。这实际上相当于我们创建RDDs的另一种方式，对我们非常有用。现在，让我们来看一些RDD操作的基础知识。
- en: Basics of RDD operation
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD操作的基础知识
- en: Let's now go through some RDD operational basics. The best way to understand
    what something does is to look at the documentation so that we can get a rigorous
    understanding of what a function performs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一些RDD操作的基础知识。了解某个功能的最佳方法是查看文档，以便我们可以严格理解函数的执行方式。
- en: The reason why this is very important is that the documentation is the golden
    source of how a function is defined and what it is designed to be used as. By
    reading the documentation, we make sure that we are as close to the source as
    possible in our understanding. The link to the relevant documentation is [https://spark.apache.org/docs/latest/rdd-programming-guide.html](https://spark.apache.org/docs/latest/rdd-programming-guide.html).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是非常重要的原因是文档是函数定义和设计用途的黄金来源。通过阅读文档，我们确保我们在理解上尽可能接近源头。相关文档的链接是[https://spark.apache.org/docs/latest/rdd-programming-guide.html](https://spark.apache.org/docs/latest/rdd-programming-guide.html)。
- en: So, let's start with the `map` function. The `map` function returns an RDD by
    applying the `f` function to each element of this RDD. In other words, it works
    the same as the `map` function we see in Python. On the other hand, the `filter` function
    returns a new RDD containing only the elements that satisfy a predicate, and that
    predicate, which is a Boolean, is often returned by an `f` function fed into the `filter` function.
    Again, this works very similarly to the `filter` function in Python. Lastly, the `collect()` function
    returns a list that contains all the elements in this RDD. And this is where I
    think reading the documentation really shines, when we see notes like this. This
    would never come up in Stack Overflow or a blog post if you were simply googling
    what this is.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`map`函数开始。`map`函数通过将`f`函数应用于此RDD的每个元素来返回一个RDD。换句话说，它的工作方式与我们在Python中看到的`map`函数相同。另一方面，`filter`函数返回一个仅包含满足谓词的元素的新RDD，该谓词是一个布尔值，通常由输入`filter`函数的`f`函数返回。同样，这与Python中的`filter`函数非常相似。最后，`collect()`函数返回一个包含此RDD中所有元素的列表。这就是我认为阅读文档真正发光的地方，当我们看到这样的说明时。如果你只是在谷歌搜索这个，这种情况永远不会出现在Stack
    Overflow或博客文章中。
- en: So, we're saying that `collect()` should only be used if the resulting array
    is expected to be small, as all the data is loaded in a driver's memory. What
    that means is, if we think back on [Chapter 01](fee531a9-9789-4860-9117-00b55fc77462.xhtml), *Installing
    PySpark and Setting Up Your Development Environment*, Spark is superb because
    it can collect and parallelize data across many different unique machines, and
    have it transparently operatable from one Terminal. What collects notes is saying
    is that, if we call `collect()`, the resulting RDD would be completely loaded
    into the driver's memory, in which case we lose the benefits of distributing the
    data around a cluster of Spark instances.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们说`collect()`只有在预期结果数组很小的情况下才应该使用，因为所有数据都加载在驱动程序的内存中。这意味着，如果我们回想一下[第01章](fee531a9-9789-4860-9117-00b55fc77462.xhtml)，*安装PySpark并设置开发环境*，Spark非常出色，因为它可以在许多不同的独立机器上收集和并行化数据，并且可以从一个终端透明地操作。`collect()`的说明是，如果我们调用`collect()`，则生成的RDD将完全加载到驱动程序的内存中，在这种情况下，我们将失去在Spark实例集群中分发数据的好处。
- en: Now that we know all of this, let's see how we actually apply these three functions
    to our data. So, go back to the PySpark Terminal; we have already loaded our raw
    data as a text file, as we have seen in previous chapters.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了所有这些，让我们看看如何实际将这三个函数应用于我们的数据。因此，返回到PySpark终端；我们已经将原始数据作为文本文件加载，就像我们在之前的章节中看到的那样。
- en: 'We will write a `filter` function to find all the lines to indicate RDD data,
    where each line `contains` the word `normal`, as seen in the following screenshot:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编写一个`filter`函数来查找所有包含单词`normal`的行，指示RDD数据，如下面的屏幕截图所示：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s analyze what this means. Firstly, we are calling the `filter` function
    for the RDD raw data, and we''re feeding it an anonymous `lambda` function that
    takes one `line` parameter and returns the predicates, as we have read in the
    documentation, on whether or not the word `normal` exists in the line. At this
    moment, as we have discussed in the previous chapters, we haven''t actually computed
    this `filter` operation. What we need to do is call a function that actually consolidates
    the data and forces Spark to calculate something. In this case, we can count on `contains_normal`,
    as demonstrated in the following screenshot:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下这意味着什么。首先，我们正在为RDD原始数据调用`filter`函数，并且我们正在向其提供一个匿名的`lambda`函数，该函数接受一个`line`参数并返回谓词，正如我们在文档中所读到的，关于单词`normal`是否存在于该行中。此刻，正如我们在之前的章节中讨论的那样，我们实际上还没有计算这个`filter`操作。我们需要做的是调用一个实际整合数据并迫使Spark计算某些内容的函数。在这种情况下，我们可以依赖`contains_normal`，就像下面的屏幕截图中所示的那样：
- en: '![](img/67a5b615-fcae-4356-99da-0bf6b0e22232.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67a5b615-fcae-4356-99da-0bf6b0e22232.png)'
- en: You can see that it has counted just over 970,000 lines in the raw data that
    contain the word `normal`. To use the `filter` function, we provide it with the `lambda` function
    and use a consolidating function, such as `counts`, that forces Spark to calculate
    and compute the data in the underlying DataFrame.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，在原始数据中，包含单词`normal`的行数超过了970,000行。要使用`filter`函数，我们提供了一个`lambda`函数，并使用一个整合函数，比如`counts`，来强制Spark计算和计算底层DataFrame中的数据。
- en: 'For the second example, we will use the map. Since we downloaded the KDD cup
    data, we know that it is a comma-separated value file, and so, one of the very
    easy things for us to do is to split each line by two commas, as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个例子，我们将使用map。由于我们下载了KDD杯数据，我们知道它是一个逗号分隔的值文件，因此，我们很容易做的一件事是通过两个逗号拆分每一行，如下所示：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let's analyze what is happening. We call the `map` function on `raw_data`. We
    feed it an anonymous `lambda` function called `line`, where we are splitting the `line` function by
    using `,`. The result is a split file. Now, here the power of Spark really comes
    into play. Recall that, in the `contains_normal.` filter, when we called a function
    that forced Spark to calculate `count`, it took us a few minutes to come up with
    the correct results. If we perform the `map` function, it is going to have the
    same effect, because there are going to be millions of lines of data that we need
    to map through. And so, one of the ways to quickly preview whether our mapping
    function runs correctly is if we can materialize a few lines instead of the whole
    file.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下发生了什么。我们在`raw_data`上调用`map`函数。我们向它提供了一个名为`line`的匿名`lambda`函数，在这个函数中，我们使用`,`来分割`line`函数。结果是一个分割文件。现在，这里真正发挥了Spark的力量。回想一下，在`contains_normal.`过滤器中，当我们调用一个强制Spark计算`count`的函数时，需要几分钟才能得出正确的结果。如果我们执行`map`函数，它会产生相同的效果，因为我们需要对数百万行数据进行映射。因此，快速预览我们的映射函数是否正确运行的一种方法是，我们可以将几行材料化而不是整个文件。
- en: 'To do this, we can use the `take` function that we have used before, as demonstrated in
    the following screenshot:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们可以使用之前使用过的`take`函数，如下面的截图所示：
- en: '![](img/e99a6e8a-4409-4b4e-b1e1-858e278abc2d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e99a6e8a-4409-4b4e-b1e1-858e278abc2d.png)'
- en: 'This might take a few seconds because we are only taking five lines, which
    is our splits and is actually quite manageable. If we look at this sample output,
    we can understand that our `map` function has been created successfully. The last
    thing we can do is to call `collect()` on raw data as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要几秒钟，因为我们只取了五行，这是我们的分割，实际上相当容易管理。如果我们查看这个样本输出，我们可以理解我们的`map`函数已经成功创建。我们可以做的最后一件事是在原始数据上调用`collect()`，如下所示：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This is designed to move all of the raw data from Spark's RDD data structure
    into the memory.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这旨在将Spark的RDD数据结构中的所有原始数据移动到内存中。
- en: Summary
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to load data on Spark RDDs and also covered
    parallelization with Spark RDDs. We had a brief overview of the UCI machine learning
    repository before loading the data. We had an overview of the basic RDD operations,
    and also checked the functions from the official documentation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何在Spark RDD上加载数据，还介绍了Spark RDD的并行化。在加载数据之前，我们简要概述了UCI机器学习存储库。我们概述了基本的RDD操作，并检查了官方文档中的函数。
- en: In the next chapter, we will cover big data cleaning and data wrangling.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍大数据清洗和数据整理。
