- en: Chapter 9. Deploying Storm on Hadoop for Advertising Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。在Hadoop上部署风暴进行广告分析
- en: In the previous two chapters, we saw how we might integrate Storm with a real-time
    analytics system. We then extended that implementation, supporting the real-time
    system with batch processing. In this chapter, we will explore the reverse.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们看到了如何将Storm与实时分析系统集成。然后我们扩展了该实现，支持批处理的实时系统。在本章中，我们将探讨相反的情况。
- en: We will examine a batch processing system that computes the effectiveness of
    an advertising campaign. We will take the system that was built on Hadoop and
    convert it into a real-time processing system.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究一个批处理系统，计算广告活动的有效性。我们将把建立在Hadoop上的系统转换成实时处理系统。
- en: To do this, we will leverage the Storm-YARN project out of Yahoo! The Storm-YARN
    project allows users to leverage YARN to deploy and run Storm clusters. The running
    of Storm on Hadoop allows enterprises to consolidate operations and utilize the
    same infrastructure for both real time and batch processing.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将利用雅虎的Storm-YARN项目。Storm-YARN项目允许用户利用YARN来部署和运行Storm集群。在Hadoop上运行Storm允许企业
    consoli操作并利用相同的基础设施进行实时和批处理。
- en: 'This chapter covers the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: An introduction to Pig
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pig简介
- en: YARN (resource management with Hadoop v2)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YARN（Hadoop v2的资源管理）
- en: Deploying Storm using Storm-YARN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Storm-YARN部署Storm
- en: Examining the use case
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 审查用例
- en: In our use case, we will process the logs of an advertising campaign to determine
    the most effective campaigns. The batch processing mechanism will process a single
    large flat file using a Pig script. Pig is a high-level language that allows users
    to perform data transformation and analysis. Pig is similar to SQL and compiles
    down into map/reduce jobs that typically deploy and run on Hadoop infrastructure.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的用例中，我们将处理广告活动的日志，以确定最有效的广告活动。批处理机制将使用Pig脚本处理单个大型平面文件。Pig是一种高级语言，允许用户执行数据转换和分析。Pig类似于SQL，并编译成通常部署和运行在Hadoop基础设施上的map/reduce作业。
- en: In this chapter, we will convert the Pig script into a topology and deploy that
    topology using Storm-YARN. This allows us to transition from a batch processing
    approach to one that is capable of ingesting and reacting to real-time events
    (for example, clicks on a banner advertisement).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将把Pig脚本转换成拓扑，并使用Storm-YARN部署该拓扑。这使我们能够从批处理方法过渡到能够摄取和响应实时事件的方法（例如，点击横幅广告）。
- en: In advertising, an impression is an advertising event that represents an advertisement
    displayed in front of a user, regardless of whether or not it was clicked. For
    our analysis, we will track each impression and use a field to indicate whether
    the user clicked on the advertisement.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在广告中，印象是代表广告显示在用户面前的广告事件，无论是否被点击。对于我们的分析，我们将跟踪每个印象，并使用一个字段来指示用户是否点击了广告。
- en: 'Each row in the flat file contains four fields that are described as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行的平面文件包含四个字段，描述如下：
- en: '| Field | Description |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '|字段|描述|'
- en: '| --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| cookie | This is a unique identifier from the browser. We will use this to
    represent users in the system. |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| cookie |这是来自浏览器的唯一标识符。我们将使用它来表示系统中的用户。|'
- en: '| campaign | This is a unique identifier that represents a specific set of
    advertising content. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| campaign |这是代表特定广告内容集的唯一标识符。|'
- en: '| product | This is the name of the product being advertised. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|产品|这是正在广告的产品的名称。|'
- en: '| click-thru | This is the Boolean field that represents whether or not the
    user clicked on the advertisement: true if the user clicked on the ad; otherwise,
    false. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|点击|这是布尔字段，表示用户是否点击了广告：如果用户点击了广告，则为true；否则为false。|'
- en: Typically, advertisers will run campaigns for products. A campaign may have
    a specific set of content associated with it. We want to calculate the most effective
    campaign per product.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，广告商会为产品运行广告活动。一个广告活动可能有一组特定的内容与之相关联。我们想计算每个产品的最有效广告活动。
- en: 'In this context, we will calculate the effectiveness of a campaign by counting
    distinct click-thrus as a percentage of the overall impressions. We will deliver
    a report in the following format:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将通过计算不同点击次数占总体印象的百分比来计算广告活动的有效性。我们将以以下格式提供报告：
- en: '| Product | Campaign | Distinct click-thrus | Impressions |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|产品|广告活动|不同点击次数|印象|'
- en: '| --- | --- | --- | --- |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| X | Y | 107 | 252 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| X | Y | 107 | 252 |'
- en: The number of impressions is simply the total count of impressions for the product
    and campaign. We do not distinct the impressions because we may have shown the
    same advertisement to the same user multiple times to attain a single click-thru.
    Since we are most likely paying per impression, we want to use the total number
    of impressions as a means of calculating the cost required to drive interest.
    Interest is represented as a click-thru.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 印象的数量只是产品和广告活动的印象总数。我们不区分印象，因为我们可能多次向同一用户展示相同的广告以获得单次点击。由于我们很可能按印象付费，我们希望使用印象的总数来计算驱动兴趣所需的成本。兴趣表示为点击。
- en: Establishing the architecture
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立架构
- en: We touched on Hadoop in the previous chapter, but we focused mainly on the map/reduce
    mechanism within Hadoop. In this chapter, we will do the opposite and focus on
    the **Hadoop File System** (**HDFS**) and **Yet Another Resource Negotiator**
    (**YARN**). We will leverage HDFS to stage the data, and leverage YARN to deploy
    the Storm framework that will host the topology.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章中提到了Hadoop，但主要关注了Hadoop中的map/reduce机制。在本章中，我们将做相反的事情，关注**Hadoop文件系统**（**HDFS**）和**Yet
    Another Resource Negotiator**（**YARN**）。我们将利用HDFS来分阶段数据，并利用YARN来部署将托管拓扑的Storm框架。
- en: 'The recent componentization within Hadoop allows any distributed system to
    use it for resource management. In Hadoop 1.0, resource management was embedded
    into the MapReduce framework as shown in the following diagram:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop内的最新组件化允许任何分布式系统使用它进行资源管理。在Hadoop 1.0中，资源管理嵌入到MapReduce框架中，如下图所示：
- en: '![Establishing the architecture](img/8294OS_09_01.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![建立架构](img/8294OS_09_01.jpg)'
- en: 'Hadoop 2.0 separates out resource management into YARN, allowing other distributed
    processing frameworks to run on the resources managed under the Hadoop umbrella.
    In our case, this allows us to run Storm on YARN as shown in the following diagram:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2.0将资源管理分离为YARN，允许其他分布式处理框架在Hadoop伞下管理的资源上运行。在我们的情况下，这允许我们在YARN上运行Storm，如下图所示：
- en: '![Establishing the architecture](img/8294OS_09_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![建立架构](img/8294OS_09_02.jpg)'
- en: As shown in the preceding diagram, Storm fulfills the same function as MapReduce.
    It provides a framework for the distributed computation. In this specific use
    case, we use Pig scripts to articulate the ETL/analysis that we want to perform
    on the data. We will convert that script into a Storm topology that performs the
    same function, and then we will examine some of the intricacies involved in doing
    that transformation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，Storm实现了与MapReduce相同的功能。它提供了分布式计算的框架。在这个特定的用例中，我们使用Pig脚本来表达我们想要对数据执行的ETL/分析。我们将将该脚本转换为执行相同功能的Storm拓扑，然后我们将检查执行该转换涉及的一些复杂性。
- en: 'To understand this better, it is worth examining the nodes in a Hadoop cluster
    and the purpose of the processes running on those nodes. Assume that we have a
    cluster as depicted in the following diagram:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，值得检查Hadoop集群中的节点以及在这些节点上运行的进程的目的。假设我们有一个如下图所示的集群：
- en: '![Establishing the architecture](img/8294OS_09_03.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![建立架构](img/8294OS_09_03.jpg)'
- en: There are two different components/subsystems shown in the diagram. The first
    is YARN, which is the new resource management layer introduced in Hadoop 2.0\.
    The second is HDFS. Let's first delve into HDFS since that has not changed much
    since Hadoop 1.0.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图中显示了两个不同的组件/子系统。第一个是YARN，这是Hadoop 2.0引入的新资源管理层。第二个是HDFS。让我们首先深入研究HDFS，因为自Hadoop
    1.0以来它并没有发生太大变化。
- en: Examining HDFS
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查HDFS
- en: HDFS is a distributed filesystem. It distributes blocks of data across a set
    of slave nodes. The NameNode is the catalog. It maintains the directory structure
    and the metadata indicating which nodes have what information. The NameNode does
    not store any data itself, it only coordinates **create, read, update, and delete**
    (**CRUD**) operations across the distributed filesystem. Storage takes place on
    each of the slave nodes that run DataNode processes. The DataNode processes are
    the workhorses in the system. They communicate with each other to rebalance, replicate,
    move, and copy data. They react and respond to the CRUD operations of clients.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS是一个分布式文件系统。它在一组从节点上分发数据块。NameNode是目录。它维护目录结构和指示哪些节点具有什么信息的元数据。NameNode本身不存储任何数据，它只协调分布式文件系统上的
    **创建、读取、更新和删除**（CRUD）操作。存储发生在运行DataNode进程的每个从节点上。DataNode进程是系统中的工作马。它们彼此通信以重新平衡、复制、移动和复制数据。它们对客户端的CRUD操作做出反应和响应。
- en: Examining YARN
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查YARN
- en: YARN is the resource management system. It monitors the load on each of the
    nodes and coordinates the distribution of new jobs to the slaves in the cluster.
    The **ResourceManager** collects status information from the **NodeManagers**.
    The ResourceManager also services job submissions from clients.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: YARN是资源管理系统。它监视每个节点的负载，并协调将新作业分配给集群中的从节点。 **ResourceManager** 收集来自 **NodeManagers**
    的状态信息。ResourceManager 还为客户端的作业提交提供服务。
- en: One additional abstraction within YARN is the concept of an **ApplicationMaster**.
    An ApplicationMaster manages resource and container allocation for a specific
    application. The ApplicationMaster negotiates with the ResourceManager for the
    assignment of resources. Once the resources are assigned, the ApplicationMaster
    coordinates with the NodeManagers to instantiate **containers**. The containers
    are logical holders for the processes that actually perform the work.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: YARN中的另一个抽象概念是 **ApplicationMaster**。ApplicationMaster管理特定应用程序的资源和容器分配。ApplicationMaster与ResourceManager协商分配资源。一旦分配了资源，ApplicationMaster就会与NodeManagers协调实例化
    **容器**。容器是实际执行工作的进程的逻辑持有者。
- en: The ApplicationMaster is a processing-framework-specific library. Storm-YARN
    provides the ApplicationMaster for running Storm processes on YARN. HDFS distributes
    the ApplicationMaster as well as the Storm framework itself. Presently, Storm-YARN
    expects an external ZooKeeper. Nimbus starts up and connects to the ZooKeeper
    when the application is deployed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ApplicationMaster是一个特定于处理框架的库。Storm-YARN提供了在YARN上运行Storm进程的ApplicationMaster。HDFS分发ApplicationMaster以及Storm框架本身。目前，Storm-YARN需要外部ZooKeeper。当应用程序部署时，Nimbus启动并连接到ZooKeeper。
- en: 'The following diagram depicts the Hadoop infrastructure running Storm via Storm-YARN:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了通过Storm-YARN在Hadoop基础设施上运行Storm：
- en: '![Examining YARN](img/8294OS_09_04.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![检查YARN](img/8294OS_09_04.jpg)'
- en: As shown in the preceding diagram, YARN is used to deploy the Storm application
    framework. At launch, Storm Application Master is started within a YARN container.
    That, in turn, creates an instance of Storm Nimbus and the Storm UI.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，YARN用于部署Storm应用程序框架。在启动时，Storm Application Master在YARN容器内启动。然后，它创建了一个Storm
    Nimbus和Storm UI的实例。
- en: After that, Storm-YARN launches supervisors in separate YARN containers. Each
    of these supervisor processes can spawn workers within its container.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，Storm-YARN在单独的YARN容器中启动监督员。这些监督员进程中的每一个都可以在其容器内生成工作人员。
- en: Both Application Master and the Storm framework are distributed via HDFS. Storm-YARN
    provides command-line utilities to start the Storm cluster, launch supervisors,
    and configure Storm for topology deployment. We will see these facilities later
    in this chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序主节点和Storm框架都是通过HDFS分发的。Storm-YARN提供了命令行实用程序来启动Storm集群，启动监督者，并配置Storm以进行拓扑部署。我们将在本章后面看到这些设施。
- en: 'To complete the architectural picture, we need to layer in the batch and real-time
    processing mechanisms: Pig and Storm topologies, respectively. We also need to
    depict the actual data.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成建筑图，我们需要分层处理批处理和实时处理机制：分别是Pig和Storm拓扑。我们还需要描述实际数据。
- en: 'Often a queuing mechanism such as Kafka is used to queue work for a Storm cluster.
    To simplify things, we will use data stored in HDFS. The following depicts our
    use of Pig, Storm, YARN, and HDFS for our use case, omitting elements of the infrastructure
    for clarity. To fully realize the value of converting from Pig to Storm, we would
    convert the topology to consume from Kafka instead of HDFS as shown in the following
    diagram:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会使用诸如Kafka之类的排队机制来为Storm集群排队工作。为了简化，我们将使用存储在HDFS中的数据。以下描述了我们在使用案例中使用Pig、Storm、YARN和HDFS，为了清晰起见省略了基础设施的元素。为了充分实现从Pig转换到Storm的价值，我们将转换拓扑以从Kafka而不是HDFS中获取数据，如下图所示：
- en: '![Examining YARN](img/8294OS_09_05.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![检查YARN](img/8294OS_09_05.jpg)'
- en: 'As the preceding diagram depicts, our data will be stored in HDFS. The dashed
    lines depict the batch process for analysis, while the solid lines depict the
    real-time system. In each of the systems, the following steps take place:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，我们的数据将存储在HDFS中。虚线表示用于分析的批处理过程，实线表示实时系统。在每个系统中，以下步骤都会发生：
- en: '| Step | Purpose | Pig Equivalent | Storm-Yarn Equivalent |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 | 目的 | Pig等效 | Storm-Yarn等效 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | The processing frameworks are deployed | The MapReduce Application Master
    is deployed and started | Storm-YARN launches Application Master and distributes
    Storm framework |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 处理框架已部署 | MapReduce应用程序主节点已部署并启动 | Storm-YARN启动应用程序主节点并分发Storm框架 |'
- en: '| 2 | The specific analytics are launched | The Pig script is compiled to MapReduce
    jobs and submitted as a job | Topologies are deployed to the cluster |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 特定的分析已启动 | Pig脚本被编译为MapReduce作业并提交为一个作业 | 拓扑被部署到集群 |'
- en: '| 3 | The resources are reserved | Map and reduce tasks are created in YARN
    containers | Supervisors are instantiated with workers |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 资源已保留 | 在YARN容器中创建Map和reduce任务 | 监督者与工作人员一起实例化 |'
- en: '| 4 | The analyses reads the data from storage and performs the analyses |
    Pig reads the data out of HDFS | Storm reads the work, typically from Kafka; but
    in this case, the topology reads it from a flat file |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 分析从存储中读取数据并执行分析 | Pig从HDFS中读取数据 | Storm通常从Kafka中读取工作，但在这种情况下，拓扑从一个平面文件中读取它
    |'
- en: Another analogy can be drawn between Pig and Trident. Pig scripts compile down
    into MapReduce jobs, while Trident topologies compile down into Storm topologies.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Pig和Trident之间也可以进行类比。Pig脚本编译成MapReduce作业，而Trident拓扑编译成Storm拓扑。
- en: 'For more information on the Storm-YARN project, visit the following URL:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Storm-YARN项目的更多信息，请访问以下网址：
- en: '[https://github.com/yahoo/storm-yarn](https://github.com/yahoo/storm-yarn)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/yahoo/storm-yarn](https://github.com/yahoo/storm-yarn)'
- en: Configuring the infrastructure
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置基础设施
- en: First, we need to configure the infrastructure. Since Storm will run on the
    YARN infrastructure, we will first configure YARN and then show how to configure
    Storm-YARN for deployment on that cluster.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要配置基础设施。由于Storm将在YARN基础设施上运行，我们将首先配置YARN，然后展示如何配置Storm-YARN以部署在该集群上。
- en: The Hadoop infrastructure
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop基础设施
- en: To configure a set of machines, you will need a copy of Hadoop residing on them
    or a copy that is accessible to each of them. First, download the latest copy
    of Hadoop and unpack the archive. For this example, we will use Version 2.1.0-beta.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置一组机器，您需要在它们中的每一台上都有一个Hadoop的副本或者可以访问到的副本。首先，下载最新的Hadoop副本并解压缩存档。在本例中，我们将使用版本2.1.0-beta。
- en: 'Assuming that you have uncompressed the archive into `/home/user/hadoop`, add
    the following environment variables on each of the nodes in the cluster:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已将存档解压缩到`/home/user/hadoop`，在集群中的每个节点上添加以下环境变量：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Add YARN to your execute path as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将YARN添加到执行路径中，如下所示：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'All the Hadoop configuration files are located in `$HADOOP_CONF_DIR`. The three
    key configuration files for this example are: `core-site.xml`, `yarn-site.xml`,
    and `hdfs-site.xml`.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Hadoop配置文件都位于`$HADOOP_CONF_DIR`中。本例中的三个关键配置文件是：`core-site.xml`、`yarn-site.xml`和`hdfs-site.xml`。
- en: In this example, we will assume that we have a Master node named `master` and
    four slave-nodes named `slave01-04`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们假设有一个名为`master`的主节点和四个名为`slave01-04`的从节点。
- en: 'Test the YARN configuration by executing the following command line:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下命令行来测试YARN配置：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Configuring HDFS
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置HDFS
- en: As per the architecture diagram, to configure HDFS you need to start the NameNode
    and then connect one or more DataNode.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据架构图，要配置HDFS，您需要启动NameNode，然后连接一个或多个DataNode。
- en: Configuring the NameNode
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置NameNode
- en: 'To start the NameNode, you need to specify a host and port. Configure the host
    and port in the `core-site.xml` file by using the following elements:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动NameNode，您需要指定主机和端口。通过使用以下元素在`core-site.xml`文件中配置主机和端口：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Additionally, configure where the NameNode stores its metadata. This configuration
    is stored in the `hdfs-site.xml` file, in the `dfs.name.dir` variable.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，配置NameNode存储其元数据的位置。此配置存储在`hdfs-site.xml`文件中的`dfs.name.dir`变量中。
- en: 'To keep the example simple, we will also disable security on the distributed
    filesystem. To do this, we set `dfs.permissions` to `False`. After these edits,
    the HDFS configuration file looks like the following code snippet:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使示例简单，我们还将在分布式文件系统上禁用安全性。为此，我们将`dfs.permissions`设置为`False`。在进行这些编辑之后，HDFS配置文件看起来像以下代码片段：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The final step before starting the NameNode is the formatting of the distributed
    filesystem. Do this with the following command:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动NameNode之前的最后一步是格式化分布式文件系统。使用以下命令进行此操作：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we are ready to start the NameNode. Do so with the following command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备启动NameNode。使用以下命令：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The last line of the startup will indicate where the logs are located:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 启动的最后一行将指示日志的位置：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tip
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Despite the message, the logs will actually be located in another file with
    the same name but with the suffix `log` instead of `out`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管消息如此，但日志实际上将位于另一个具有相同名称但后缀为`log`而不是`out`的文件中。
- en: 'Also, ensure that the name directory you declared in the configuration exists;
    otherwise, you will receive the following error in the logfile:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 还要确保您在配置中声明的名称目录存在；否则，您将在日志文件中收到以下错误：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Verify that the NameNode has started with the following code snippet:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码片段验证NameNode是否已启动：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Additionally, you should be able to navigate to the UI in a web browser. By
    default, the server starts on port 50070\. Navigate to `http://master:50070` in
    a browser. You should see the following screenshot:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您应该能够在Web浏览器中导航到UI。默认情况下，服务器在端口50070上启动。在浏览器中导航到`http://master:50070`。您应该看到以下截图：
- en: '![Configuring the NameNode](img/8294OS_09_06.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![配置NameNode](img/8294OS_09_06.jpg)'
- en: 'Clicking on the **Live Nodes** link will show the nodes available and the space
    allocation per node, as shown in the following screenshot:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**Live Nodes**链接将显示可用的节点以及每个节点的空间分配，如下截图所示：
- en: '![Configuring the NameNode](img/8294OS_09_07.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![配置NameNode](img/8294OS_09_07.jpg)'
- en: Finally, from the main page, you can also browse the filesystem by clicking
    on **Browse the filesystem**.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，从主页，您还可以通过点击**浏览文件系统**来浏览文件系统。
- en: Configuring the DataNode
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置DataNode
- en: In general, it is easiest to share the core configuration file between nodes
    in the cluster. The data nodes will use the host and port defined in the `core-site.xml`
    file to locate the NameNode and connect to it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，最容易在集群中的节点之间共享核心配置文件。数据节点将使用`core-site.xml`文件中定义的主机和端口来定位NameNode并连接到它。
- en: 'Additionally, each DataNode needs to configure the location for local storage.
    This is defined in the following element within the `hdfs-site.xml` file:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个DataNode需要配置本地存储的位置。这在`hdfs-site.xml`文件中的以下元素中定义：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If this location is consistent across slave machines, then this configuration
    file can be shared as well. With this set, you can start the DataNode with the
    following command:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个位置在从节点上是一致的，那么这个配置文件也可以共享。设置好后，您可以使用以下命令启动DataNode：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once again, verify that the DataNode is running using `jps` and monitor the
    logs for any errors. In a few moments, the DataNode should appear in the **Live
    Nodes** screen of the NameNode as previously shown.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用`jps`验证DataNode是否正在运行，并监视任何错误日志。在几分钟内，DataNode应该会出现在NameNode的**Live Nodes**屏幕上，就像之前显示的那样。
- en: Configuring YARN
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置YARN
- en: With HDFS up and running, it is now time to turn our attention to YARN. Similar
    to what we did with HDFS, we will first get the ResourceManager running and then
    we will attach slave nodes by running NodeManager.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS已经运行起来了，现在是时候把注意力转向YARN了。与我们在HDFS中所做的类似，我们将首先运行ResourceManager，然后通过运行NodeManager来连接从节点。
- en: Configuring the ResourceManager
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 配置ResourceManager
- en: The ResourceManager has various subcomponents, each of which acts as a server
    that requires a host and port on which to run. All of the servers are configured
    within the `yarn-site.xml` file.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ResourceManager有各种子组件，每个子组件都充当需要在其上运行的主机和端口的服务器。所有服务器都在`yarn-site.xml`文件中配置。
- en: 'For this example, we will use the following YARN configuration:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将使用以下YARN配置：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The first four variables in the preceding configuration file assign host and
    ports for the subcomponents. Setting the `yarn.acl.enable` variable to `False`
    disables security on the YARN cluster. The `yarn.nodemanager.local-dirs` variable
    specifies the place on the local filesystem where YARN will place the data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配置文件中，前四个变量分配了子组件的主机和端口。将`yarn.acl.enable`变量设置为`False`会禁用YARN集群上的安全性。`yarn.nodemanager.local-dirs`变量指定了YARN将数据放置在本地文件系统的位置。
- en: Finally, the `yarn.nodemanager.aux-services` variable starts an auxiliary service
    within the NodeManager's runtime to support MapReduce jobs. Since our Pig scripts
    compile down into MapReduce jobs, they depend on this variable.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`yarn.nodemanager.aux-services`变量在NodeManager的运行时内启动一个辅助服务，以支持MapReduce作业。由于我们的Pig脚本编译成MapReduce作业，它们依赖于这个变量。
- en: 'Like the NameNode, start the ResourceManager with the following command line:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 像NameNode一样，使用以下命令启动ResourceManager：
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Again, check for the existence of the process with `jps`, monitor the logs for
    exceptions, and then you should be able to navigate to the UI which by default
    runs on port 8088.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用`jps`检查进程是否存在，监视异常日志，然后您应该能够导航到默认运行在端口8088上的UI。
- en: 'The UI is shown in the following screenshot:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: UI显示在以下截图中：
- en: '![Configuring the ResourceManager](img/8294OS_09_08.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![配置ResourceManager](img/8294OS_09_08.jpg)'
- en: Configuring the NodeManager
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置NodeManager
- en: The NodeManager uses the same configuration file (`yarn-site.xml`) to locate
    the respective servers. Thus, it is safe to copy or share that file between the
    nodes in the cluster.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: NodeManager使用相同的配置文件（`yarn-site.xml`）来定位相应的服务器。因此，在集群中的节点之间可以安全地复制或共享该文件。
- en: 'Start the NodeManager with the following command:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令启动NodeManager：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After all NodeManagers register with the ResourceManager, you will be able
    to see them in the ResourceManager UI after clicking on **Nodes**, as shown in
    the following screenshot:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有NodeManagers向ResourceManager注册之后，您将能够在ResourceManager UI中点击**Nodes**后看到它们，如下截图所示：
- en: '![Configuring the NodeManager](img/8294OS_09_09.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![配置NodeManager](img/8294OS_09_09.jpg)'
- en: Deploying the analytics
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署分析
- en: With Hadoop in place, we can now focus on the distributed processing frameworks
    that we will use for analysis.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有了Hadoop，我们现在可以专注于我们将用于分析的分布式处理框架。
- en: Performing a batch analysis with the Pig infrastructure
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Pig基础设施执行批量分析
- en: The first of the distributed processing frameworks that we will examine is Pig.
    Pig is a framework for data analysis. It allows the user to articulate analysis
    in a simple high-level language. These scripts then compile down to MapReduce
    jobs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要检查的第一个分布式处理框架是Pig。Pig是一个用于数据分析的框架。它允许用户用简单的高级语言表达分析。然后这些脚本编译成MapReduce作业。
- en: Although Pig can read data from a few different systems (for example, S3), we
    will use HDFS as our data storage mechanism in this example. Thus, the first step
    in our analysis is to copy the data into HDFS.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Pig可以从几个不同的系统（例如S3）中读取数据，但在本例中，我们将使用HDFS作为我们的数据存储机制。因此，我们分析的第一步是将数据复制到HDFS中。
- en: 'To do this, we issue the following Hadoop commands:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们发出以下Hadoop命令：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The preceding commands create a directory for the data file and copy the click-thru
    data file into that directory.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令创建了一个数据文件目录，并将点击数据文件复制到该目录中。
- en: To execute a Pig script against that data, we will need to install Pig. For
    this, we simply download Pig and expand the archive on that machine configured
    with Hadoop. For this example, we will use Version 0.11.1.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行Pig脚本对该数据，我们需要安装Pig。为此，我们只需下载Pig并在配置了Hadoop的机器上展开存档。在这个例子中，我们将使用版本0.11.1。
- en: 'Just like we did with Hadoop, we will add the following environment variables
    to our environment:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在Hadoop中所做的那样，我们将向我们的环境添加以下环境变量：
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `PIG_CLASSPATH` variable tells Pig where to find Hadoop.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`PIG_CLASSPATH`变量告诉Pig在哪里找到Hadoop。'
- en: 'Once you have those variables in your environment, you should be able to test
    your Pig installation with the following commands:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的环境中有了这些变量之后，您应该能够使用以下命令测试您的Pig安装：
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: By default, Pig will read the Hadoop configuration and connect to the distributed
    filesystem. You can see that in the previous output. It is connected to our distributed
    filesystem at `hdfs://master:8020`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Pig将读取Hadoop配置并连接到分布式文件系统。您可以在先前的输出中看到。它连接到我们的分布式文件系统`hdfs://master:8020`。
- en: 'Via Pig, you can interact with HDFS in the same way as you would with a regular
    filesystem. For example, `ls` and `cat` both work as shown in the following code
    snippet:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Pig，您可以与HDFS进行交互，方式与常规文件系统相同。例如，`ls`和`cat`都可以像以下代码片段中所示那样工作：
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Performing a real-time analysis with the Storm-YARN infrastructure
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Storm-YARN基础设施执行实时分析
- en: Now that we have infrastructure working for batch processing, let's leverage
    the exact same infrastructure for real-time processing. Storm-YARN makes it easy
    to reuse the Hadoop infrastructure for Storm.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为批处理工作建立了基础设施，让我们利用完全相同的基础设施进行实时处理。Storm-YARN使得重用Hadoop基础设施进行Storm变得容易。
- en: 'Since Storm-YARN is a new project, it is best to build from source and create
    the distribution using the instructions in the `README` file found at the following
    URL:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Storm-YARN是一个新项目，最好是根据源代码构建并使用`README`文件中的说明创建分发，该文件位于以下URL：
- en: '[https://github.com/yahoo/storm-yarn](https://github.com/yahoo/storm-yarn)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/yahoo/storm-yarn](https://github.com/yahoo/storm-yarn)'
- en: After building the distribution, you need to copy the Storm framework into HDFS.
    This allows Storm-YARN to deploy the framework to each of the nodes in the cluster.
    By default, Storm-YARN will look for the Storm library as a ZIP file in the launching
    user's directory on HDFS. Storm-YARN provides a copy of a compatible Storm in
    the `lib` directory of its distribution.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 构建分发后，您需要将Storm框架复制到HDFS。这允许Storm-YARN将框架部署到集群中的每个节点。默认情况下，Storm-YARN将在HDFS上启动用户目录中的Storm库作为ZIP文件。Storm-YARN在其分发的`lib`目录中提供了一个兼容的Storm的副本。
- en: 'Assuming that you are in the Storm-YARN directory, you can copy the ZIP file
    into the correct HDFS directory with the following commands:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您在Storm-YARN目录中，您可以使用以下命令将ZIP文件复制到正确的HDFS目录中：
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can then verify that the Storm framework is HDFS by browsing the filesystem
    through the Hadoop administration interface. You should see the following screenshot:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以通过Hadoop管理界面浏览文件系统来验证Storm框架是否在HDFS中。您应该看到以下截图：
- en: '![Performing a real-time analysis with the Storm-YARN infrastructure](img/8294OS_09_10.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![使用Storm-YARN基础设施执行实时分析](img/8294OS_09_10.jpg)'
- en: With the Storm framework staged on HDFS, the next step is to configure the local
    YAML file for Storm-YARN. The YAML file used with Storm-YAML is the configuration
    for both Storm-YAML and Storm. The Storm-specific parameters in the YAML file
    get passed along to Storm.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在HDFS上暂存了Storm框架后，下一步是为Storm-YARN配置本地YAML文件。与Storm-YAML一起使用的YAML文件是Storm-YAML和Storm的配置。YAML文件中的Storm特定参数将传递给Storm。
- en: 'An example of the YAML file is shown in the following code snippet:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段显示了YAML文件的示例：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Many of the parameters are self-descriptive. However, take note of the last
    variable in particular. This is the location of the ZooKeeper host. Although it
    might not be the case always, for now Storm-YARN assumes you have a pre-existing
    ZooKeeper.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 许多参数都是自描述的。但特别注意最后一个变量。这是ZooKeeper主机的位置。尽管现在可能并非总是如此，但目前Storm-YARN假设您有一个预先存在的ZooKeeper。
- en: Tip
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'To monitor whether Storm-YARN will continue to require a pre-existing ZooKeeper
    instance, go through the information available at the following link:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要监视Storm-YARN是否仍然需要预先存在的ZooKeeper实例，请查看以下链接中提供的信息：
- en: '[https://github.com/yahoo/storm-yarn/issues/22](https://github.com/yahoo/storm-yarn/issues/22)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/yahoo/storm-yarn/issues/22](https://github.com/yahoo/storm-yarn/issues/22)'
- en: 'With the the Storm framework in HDFS and the YAML file configured, the command
    line to launch Storm on YARN is the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用HDFS中的Storm框架和配置的YAML文件，启动YARN上的Storm的命令行如下：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You specify the location of the YAML file, the queue for YARN, a name for the
    application, and the location of the ZIP file, which is relative to the user directory
    unless a full path is specified.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 您指定YAML文件的位置，YARN队列，应用程序的名称以及ZIP文件的位置，相对于用户目录，除非指定了完整路径。
- en: Tip
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Queues in YARN are beyond the scope of this discussion, but by default YARN
    is configured with a default queue that is used in the preceding command line.
    If you are running Storm on a pre-existing cluster, examine `capacity-scheduler.xml`
    in the YARN configuration to locate potential queue names.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: YARN中的队列超出了本讨论的范围，但默认情况下，YARN配置了一个默认队列，该队列在上述命令行中使用。如果您在现有集群上运行Storm，请检查YARN配置中的`capacity-scheduler.xml`以查找潜在的队列名称。
- en: 'After executing the preceding command line, you should see the application
    deployed in the YARN administration screen, as shown in the following screenshot:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述命令行后，您应该会在YARN管理屏幕上看到应用程序部署，如下截图所示：
- en: '![Performing a real-time analysis with the Storm-YARN infrastructure](img/8294OS_09_11.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![使用Storm-YARN基础设施进行实时分析](img/8294OS_09_11.jpg)'
- en: 'Clicking on the application shows where the application master is deployed.
    Examine the node value for the Application Master. This is where you will find
    the Storm UI as shown in the following screenshot:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 单击应用程序显示应用程序主管部署的位置。检查Application Master的节点值。这就是您将找到Storm UI的地方，如下截图所示：
- en: '![Performing a real-time analysis with the Storm-YARN infrastructure](img/8294OS_09_12.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![使用Storm-YARN基础设施进行实时分析](img/8294OS_09_12.jpg)'
- en: 'Drilling down one more level, you will be able to see the logfiles for Storm,
    as shown in the following screenshot:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 再深入一级，您将能够看到Storm的日志文件，如下截图所示：
- en: '![Performing a real-time analysis with the Storm-YARN infrastructure](img/8294OS_09_13.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![使用Storm-YARN基础设施进行实时分析](img/8294OS_09_13.jpg)'
- en: 'With any luck, the logs will show a successful startup of Nimbus and the UI.
    Examining the standard output stream, you will see Storm-YARN launching the supervisors:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的话，日志将显示Nimbus和UI成功启动。检查标准输出流，您将看到Storm-YARN启动监督者：
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The key lines in the preceding output are highlighted. If you navigate to those
    URLs, you will see the supervisor logs for the respective instances. Looking back
    at the YAML file we used to launch Storm-YARN, notice that we specified the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出中的关键行已经突出显示。如果导航到这些URL，您将看到各自实例的监督者日志。回顾我们用于启动Storm-YARN的YAML文件，注意我们指定了以下内容：
- en: '[PRE23]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Navigate to the UI using the node that hosts the ApplicationMaster, and then
    navigate to the UI port specified in the YAML file used for launch (`ui.port:
    7070`).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '使用托管ApplicationMaster的节点导航到UI，然后导航到用于启动的YAML文件中指定的UI端口（`ui.port: 7070`）。'
- en: 'In a browser, open `http://node:7070/`, where node is the host for the Application
    Master. You should see the familiar Storm UI as shown in the following screenshot:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中打开`http://node:7070/`，其中node是Application Master的主机。您应该会看到熟悉的Storm UI，如下截图所示：
- en: '![Performing a real-time analysis with the Storm-YARN infrastructure](img/8294OS_09_14.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![使用Storm-YARN基础设施进行实时分析](img/8294OS_09_14.jpg)'
- en: 'The infrastructure is now ready for use. To kill the Storm deployment on YARN,
    you can use the following command:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施现在已经准备就绪。要在YARN上终止Storm部署，可以使用以下命令：
- en: '[PRE24]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding statement, the `appId` parameter corresponds to the `appId`
    parameter assigned to Storm-YARN, and it is visible in the Hadoop administration
    screen.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述语句中，`appId`参数对应于分配给Storm-YARN的`appId`参数，并且在Hadoop管理屏幕上可见。
- en: Tip
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Storm-YARN will use the local Hadoop configuration to locate the master Hadoop
    node. If you are launching from a machine that is not a part of the Hadoop cluster,
    you will need to configure that machine with the Hadoop environment variables
    and configuration files. Specifically, it launches through the ResourceManager.
    Thus, you will need the following variables configured in `yarn-site.xml`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Storm-YARN将使用本地Hadoop配置来定位主Hadoop节点。如果您是从不属于Hadoop集群的机器启动的，您将需要使用Hadoop环境变量和配置文件配置该机器。具体来说，它通过ResourceManager启动。因此，您需要在`yarn-site.xml`中配置以下变量：
- en: '`yarn.resourcemanager.address`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`yarn.resourcemanager.address`'
- en: Performing the analytics
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行分析
- en: With both the batch and real-time infrastructure in place, we can focus on the
    analytics. First, we will take a look at the processing in Pig, and then we will
    translate the Pig script into a Storm topology.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 有了批处理和实时基础设施，我们可以专注于分析。首先，我们将看一下Pig中的处理，然后将Pig脚本转换为Storm拓扑。
- en: Executing the batch analysis
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行批量分析
- en: For the batch analysis, we use Pig. The Pig script calculates the effectiveness
    of a campaign by computing the ratio between the distinct numbers of customers
    that have clicked-thru and the total number of impressions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批量分析，我们使用Pig。Pig脚本通过计算点击次数和总曝光次数之间的不同客户数量的比率来计算活动的有效性。
- en: 'The Pig script is shown in the following code snippet:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Pig脚本如下所示：
- en: '[PRE25]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Let's take a closer look at the preceding code.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看一下上述代码。
- en: The first `LOAD` statement specifies the location of the data and a schema with
    which to load the data. Typically, Pig loads denormalized data. The location for
    the data is a URL. When operating in local mode, as previously shown, this is
    a relative path. When running in MapReduce mode, the URL will most likely be a
    location in HDFS. When running a Pig script against **Amazon Web Services** (**AWS**),
    this will most likely be an S3 URL.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`LOAD`语句指定了数据的位置和用于加载数据的模式。通常，Pig加载非规范化数据。数据的位置是一个URL。在本地模式下操作时，如前所示，这是一个相对路径。在MapReduce模式下运行时，URL很可能是HDFS中的位置。在针对**亚马逊网络服务**（**AWS**）运行Pig脚本时，这很可能是一个S3
    URL。
- en: In the subsequent lines after the `Load` statement, the script calculates all
    the distinct click-thru. In the first line, it filters the dataset for only the
    rows that have `True` in the column, which indicates that the impression resulted
    in a click-thru. After filtering, the rows are filtered for only distinct entries.
    The rows are then grouped by campaign and each distinct click-thru is counted
    by campaign. The results of this analysis are stored in the alias `count_of_click_thrus_by_campaign`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Load`语句之后的后续行中，脚本计算了所有不同的点击次数。在第一行中，它过滤了仅在该列中为`True`的行的数据集，这表示印象导致了点击次数。过滤后，行被过滤为仅包含不同条目。然后，按广告系列对行进行分组，并计算每个广告系列的不同点击次数。这项分析的结果存储在别名`count_of_click_thrus_by_campaign`中。
- en: The second dimension of the problem is then computed in the subsequent lines.
    No filter is necessary since we simply want a count of the impressions by campaign.
    The results of this are stored in the alias `count_of_impressions_by_campaign`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在后续行中计算了问题的第二个维度。不需要过滤，因为我们只想要按广告系列计算印象的计数。这些结果存储在别名`count_of_impressions_by_campaign`中。
- en: 'Executing the Pig script yields the following output:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 执行Pig脚本会产生以下输出：
- en: '[PRE26]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The first element in the output is the campaign identifier. The number of all
    the distinct click-thru and the total number of impressions follow that. The last
    element is the effectiveness, which is the ratio of all the distinct click-thru
    to total number of impressions.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中的第一个元素是广告系列标识符。接着是所有不同的点击次数和总印象次数。最后一个元素是效果，即所有不同的点击次数与总印象次数的比率。
- en: Executing real-time analysis
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行实时分析
- en: 'Now, let''s translate the batch analysis into real-time analysis. A strict
    interpretation of the Pig script might result in the following topology:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将批处理分析转化为实时分析。对Pig脚本的严格解释可能会导致以下拓扑：
- en: '[PRE27]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the preceding topology, we fork the stream into two separate streams: `click_thru_stream`
    and `impressions_stream`. The `click_thru_stream` contains the count of distinct
    impressions. The `impressions_stream` contains the total count of impressions.
    Those two streams are then joined using the `topology.join` method.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述拓扑中，我们将流分成两个独立的流：`click_thru_stream`和`impressions_stream`。`click_thru_stream`包含不同印象的计数。`impressions_stream`包含印象的总计数。然后使用`topology.join`方法将这两个流连接起来。
- en: The issue with the preceding topology is the join. In Pig, since the sets are
    static they can easily be joined. Joins within Storm are done on a per batch basis.
    This would not necessarily be a problem. However, the join is also an inner join,
    which means records are only emitted if there are corresponding tuples between
    the streams. In this case, we are filtering records from the `click_thru_stream`
    because we only want distinct records. Thus, the cardinality of that stream is
    smaller than that of the `impressions_stream`, which means that tuples are lost
    in the join process.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 前述拓扑的问题在于连接。在Pig中，由于集合是静态的，它们可以很容易地连接。Storm中的连接是基于每个批次进行的。这不一定是个问题。然而，连接也是内连接，这意味着只有在流之间存在对应元组时才会发出记录。在这种情况下，我们正在从`click_thru_stream`中过滤记录，因为我们只想要不同的记录。因此，该流的基数小于`impressions_stream`的基数，这意味着在连接过程中会丢失元组。
- en: Tip
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Operations such as join are well defined for discrete sets, but it is unclear
    how to translate their definitions into a real-time world of infinite event streams.
    For more on this, visit the following URLs:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离散集合，诸如连接之类的操作是明确定义的，但不清楚如何将它们的定义转化为无限事件流的实时世界。有关更多信息，请访问以下URL：
- en: '[https://cwiki.apache.org/confluence/display/PIG/Pig+on+Storm+Proposal](https://cwiki.apache.org/confluence/display/PIG/Pig+on+Storm+Proposal)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://cwiki.apache.org/confluence/display/PIG/Pig+on+Storm+Proposal](https://cwiki.apache.org/confluence/display/PIG/Pig+on+Storm+Proposal)'
- en: '[https://issues.apache.org/jira/browse/PIG-3453](https://issues.apache.org/jira/browse/PIG-3453)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://issues.apache.org/jira/browse/PIG-3453](https://issues.apache.org/jira/browse/PIG-3453)'
- en: Instead, we will use Trident's state construct to share the counts between the
    streams.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们将使用Trident的状态构造来在流之间共享计数。
- en: 'This is shown in the corrected topology in the following diagram:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下图表中显示了更正后的拓扑：
- en: '![Executing real-time analysis](img/8294OS_09_15.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![执行实时分析](img/8294OS_09_15.jpg)'
- en: 'The code for this topology is as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 此拓扑的代码如下：
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s first take a look at the spout. It simply reads the file, parses the
    rows, and emits the tuples, as shown in the following code snippet:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看spout。它简单地读取文件，解析行，并发出元组，如下面的代码片段所示：
- en: '[PRE29]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In a real system, the preceding spout would most likely read from a Kafka queue.
    Alternatively, a spout could read directly from HDFS if we sought to recreate
    exactly what the batch processing mechanism was doing.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实系统中，前述spout很可能会从Kafka队列中读取。或者，如果我们想要重新创建批处理机制正在执行的操作，spout可以直接从HDFS中读取。
- en: Tip
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'There is some preliminary work on a spout that can read from HDFS; check out
    the following URL for more information:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些关于可以从HDFS读取的spout的初步工作；请查看以下URL以获取更多信息：
- en: '[https://github.com/jerrylam/storm-hdfs](https://github.com/jerrylam/storm-hdfs)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jerrylam/storm-hdfs](https://github.com/jerrylam/storm-hdfs)'
- en: To compute the distinct count of all the click-thru, the topology first filters
    the stream for only those impressions that resulted in a click-thru.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算所有点击次数的不同计数，拓扑首先过滤流，仅保留导致点击次数的印象。
- en: 'The code for this filter is as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 此过滤器的代码如下：
- en: '[PRE30]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Then, the stream filters for only distinct click-thrus. In this example, it
    uses an in-memory cache to filter for distinct tuples. In reality, this should
    use distributed state and/or a grouping operation to direct like tuples to the
    same host. Without persistent storage, the example would eventually run out of
    memory in the JVM.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，流仅过滤出不同的点击次数。在这个例子中，它使用内存缓存来过滤不同的元组。实际上，这应该使用分布式状态和/或分组操作来将相似的元组定向到同一主机。没有持久存储，该示例最终会在JVM中耗尽内存。
- en: Tip
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'There is active work on algorithms to approximate distinct sets against data
    streams. For more information on **Streaming Quotient Filter** (**SQF**), check
    out the following URL:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 正在积极研究算法来近似数据流中的不同集合。有关**Streaming Quotient Filter**（**SQF**）的更多信息，请查看以下网址：
- en: '[http://www.vldb.org/pvldb/vol6/p589-dutta.pdf](http://www.vldb.org/pvldb/vol6/p589-dutta.pdf)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.vldb.org/pvldb/vol6/p589-dutta.pdf](http://www.vldb.org/pvldb/vol6/p589-dutta.pdf)'
- en: 'For our example, the `Distinct` function is shown in the following code snippet:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，`Distinct`函数显示在以下代码片段中：
- en: '[PRE31]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Once it has all the distinct click-thru, Storm persists that information into
    Trident state using a call to `persistAggregate`. This collapses the stream by
    using the `Count` operator. In the example, we use a MemoryMap. However, in a
    real system we would most likely apply a distributed storage mechanism such as
    Memcache or Cassandra.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦获得所有不同的点击量，Storm会使用`persistAggregate`调用将该信息持久化到Trident状态中。这通过使用`Count`运算符来折叠流。在示例中，我们使用了MemoryMap。然而，在实际系统中，我们很可能会应用分布式存储机制，如Memcache或Cassandra。
- en: 'The result of processing the initial stream is a `TridentState` object that
    contains the count of all the distinct click-thru grouped by the campaign identifier.
    The critical line that *joins* the two streams is shown as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 处理初始流的结果是一个包含按广告系列标识符分组的所有不同点击量的`TridentState`对象。*连接*两个流的关键行如下所示：
- en: '[PRE32]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This incorporates the state developed in the initial stream into the analysis
    developed by the second stream. Effectively, the second stream queries the state
    mechanism for the distinct count of all the click-thru for that campaign and adds
    it as a field to the tuples processed in this stream. That field can then be leveraged
    in the effectiveness computation, which is encapsulated in the following class:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这将将初始流中开发的状态合并到第二个流中开发的分析中。实际上，第二个流查询状态机制以获取该广告系列的所有不同点击量，并将其作为字段添加到在此流程中处理的元组中。然后可以利用该字段进行效果计算，该计算封装在以下类中：
- en: '[PRE33]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As shown in the preceding code, this class computes effectiveness by computing
    the ratio between the field that contains the total count and the field introduced
    by the state query.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，该类通过计算包含总计数的字段与状态查询引入的字段之间的比率来计算效果。
- en: Deploying the topology
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署拓扑
- en: 'To deploy the preceding topology, we must first retrieve the Storm-YAML configuration
    using the following command:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署前面的拓扑，必须首先使用以下命令检索Storm-YAML配置：
- en: '[PRE34]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding command interacts with the specified instance of the Storm-YARN
    application to retrieve a `storm.yaml` file that can be used to deploy topologies
    by using the standard mechanisms. Simply copy the `output.yaml` file into the
    appropriate location (typically, `~/.storm/storm.yaml`) and deploy using the standard
    `storm jar` command as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令与指定的Storm-YARN应用程序实例交互，以检索可以使用标准机制部署拓扑的`storm.yaml`文件。只需将`output.yaml`文件复制到适当的位置（通常为`~/.storm/storm.yaml`），然后使用标准的`storm
    jar`命令进行部署，如下所示：
- en: '[PRE35]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Executing the topology
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行拓扑
- en: 'Executing the preceding topology results in the following output:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的拓扑将产生以下输出：
- en: '[PRE36]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Notice that the values are the same as those emitted by Pig. If we let the
    topology run, we eventually see decreasing effectiveness scores as shown in the
    following output:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些值与Pig发出的值相同。如果让拓扑运行，最终会看到效果得分逐渐降低，如下面的输出所示：
- en: '[PRE37]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This stands to reason because we now have a real-time system, which is continually
    consuming the same impression events. Since we are only counting all the distinct
    click-thru and the entire set of click-thru has already been accounted for in
    the calculation, the effectiveness will continue to drop.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有道理的，因为我们现在有了一个实时系统，它不断地消耗相同的印象事件。由于我们只计算所有不同的点击量，并且整个点击量集已经在计算中被考虑，效果将继续下降。
- en: Summary
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw a few different things. First, we saw the blueprint
    for converting a batch processing mechanism that leverages Pig into a real-time
    system that is implemented in Storm. We saw how a direct translation of that script
    would not work due to the limitations of joins in a real-time system, because
    traditional join operations require finite set of data. To overcome this problem,
    we used a shared state pattern with the forked streams.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了一些不同的东西。首先，我们看到了将利用Pig的批处理机制转换为在Storm中实现的实时系统的蓝图。我们看到了直接翻译该脚本将不起作用的原因，因为实时系统中联接的限制，传统的联接操作需要有限的数据集。为了解决这个问题，我们使用了带有分叉流的共享状态模式。
- en: Secondly, and perhaps most importantly, we examined Storm-YARN; it allows a
    user to reuse the Hadoop infrastructure to deploy Storm. Not only does this provide
    a means for existing Hadoop users to quickly transition to Storm, it also allows
    a user to capitalize on cloud mechanisms for Hadoop such as Amazon's **Elastic
    Map Reduce** (**EMR**). Using EMR, Storm can be deployed quickly to cloud infrastructure
    and scaled to meet demand.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，也许最重要的是，我们研究了Storm-YARN；它允许用户重用Hadoop基础设施来部署Storm。这不仅为现有的Hadoop用户提供了快速过渡到Storm的途径，还允许用户利用Hadoop的云机制，如亚马逊的**弹性Map
    Reduce**（**EMR**）。使用EMR，Storm可以快速部署到云基础设施，并根据需求进行扩展。
- en: Finally, as future work, the community is exploring methods to run Pig scripts
    directly on Storm. This would allow users to directly port their existing analytics
    over to Storm.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，作为未来的工作，社区正在探索直接在Storm上运行Pig脚本的方法。这将允许用户直接将其现有的分析移植到Storm上。
- en: To monitor this work, visit [https://cwiki.apache.org/confluence/display/PIG/Pig+on+Storm+Proposal.](https://cwiki.apache.org/confluence/display/PIG/Pig+on+Storm+Proposal.)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 要监视这项工作，请访问[https://cwiki.apache.org/confluence/display/PIG/Pig+on+Storm+Proposal.](https://cwiki.apache.org/confluence/display/PIG/Pig+on+Storm+Proposal.)
- en: In the next chapter, we will explore automated Storm deployment to the cloud
    using Apache Whirr. Although not specifically addressed, the techniques in the
    next chapter can be used in cloud deployments.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨使用Apache Whirr在云中自动部署Storm。虽然没有明确提到，但下一章中的技术可以用于云部署。
