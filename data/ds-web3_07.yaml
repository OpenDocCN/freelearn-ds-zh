- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: A Primer on Machine Learning and Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习与深度学习入门
- en: Before applying any machine learning algorithm, having a comprehensive understanding
    of the dataset and its key features is essential. This understanding is typically
    derived through **exploratory data analysis** (**EDA**). Once acquainted with
    the data, we must invest time in feature engineering, which involves selecting,
    transforming, and creating new features (if necessary) to enable the use of the
    chosen model or enhance its performance. Feature engineering may include tasks
    such as converting classes into numerical values, scaling or normalizing features,
    creating new features from existing ones, and more. This process is tailored for
    each specific model and dataset under analysis. Once this process is completed,
    we can proceed to modeling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用任何机器学习算法之前，全面理解数据集及其关键特征至关重要。这种理解通常通过**探索性数据分析**（**EDA**）得出。一旦熟悉了数据，我们必须投入时间进行特征工程，这包括选择、转换和创建新的特征（如果需要），以使所选模型能够使用或提升其性能。特征工程可能包括将类别转换为数值、对特征进行缩放或标准化、从现有特征中创建新特征等任务。这个过程针对每个特定的模型和数据集进行定制。一旦这个过程完成，我们就可以开始建模。
- en: The goal of this chapter is to review introductory concepts of machine learning
    and deep learning, laying the foundation for *Part 2* of this book. In *Part 2*,
    we will delve into various use cases where artificial intelligence is applied
    to Web3 data. While not covering every possible model in detail, we will provide
    brief descriptions of project motivations, the models themselves, and the tools
    used, and include useful references for further reading.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是回顾机器学习和深度学习的基础概念，为本书的*第二部分*奠定基础。在*第二部分*中，我们将深入探讨人工智能在Web3数据中的各种应用案例。尽管不会详细介绍每一种可能的模型，但我们将简要描述项目的动机、模型本身以及使用的工具，并提供有价值的参考资料以供进一步阅读。
- en: We will explore the main concepts of machine learning and deep learning, discussing
    two typical machine learning pipelines – one using scikit-learn and the other
    using Keras. Additionally, we have compiled an extensive *Further reading* section
    for each theme covered in this chapter to encourage continued learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨机器学习和深度学习的主要概念，讨论两个典型的机器学习管道——一个使用scikit-learn，另一个使用Keras。此外，我们为本章涵盖的每个主题编写了广泛的*进一步阅读*部分，以鼓励持续学习。
- en: 'Specifically, the following topics will be addressed:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，以下主题将被讨论：
- en: Basic concepts of machine learning and deep learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习和深度学习的基本概念
- en: Machine learning pipeline with scikit-learn and Keras
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn和Keras的机器学习管道
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We will be using **scikit-learn**, a popular Python library specially designed
    for machine learning tasks. It offers algorithms and tools for data preprocessing,
    feature selection, model selection, and model evaluation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**scikit-learn**，这是一个专为机器学习任务设计的流行Python库。它提供了数据预处理、特征选择、模型选择和模型评估的算法和工具。
- en: 'If you have not worked with scikit-learn before, it can be installed by using
    the following code snippet:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前没有使用过scikit-learn，可以通过以下代码片段进行安装：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The documentation for scikit-learn can be found at https://scikit-learn.org/stable/.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的文档可以在https://scikit-learn.org/stable/找到。
- en: For deep learning, we have the option to use **TensorFlow** or **Keras**. TensorFlow
    is a powerful open source library for numerical computation that provides solutions
    to train, test, and deploy a variety of deep learning neural networks. It serves
    as the infrastructure layer, which enables low-level tensor operations on the
    CPU, TPU, and GPU. On the other hand, Keras is a high-level Python API built on
    top of TensorFlow. It is specially prepared to enable fast experimentation and
    provides informative feedback when an error is discovered. According to the 2022
    survey *State of Data Science and Machine Learning*, by Kaggle, Keras reached
    a 61% adoption rate among machine learning developers and data scientists.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习，我们可以选择使用**TensorFlow**或**Keras**。TensorFlow是一个强大的开源数值计算库，提供训练、测试和部署各种深度学习神经网络的解决方案。它作为基础设施层，使得在CPU、TPU和GPU上进行低级张量操作成为可能。另一方面，Keras是一个建立在TensorFlow之上的高级Python
    API。它专为快速实验而设计，并在发现错误时提供有用的反馈。根据Kaggle的2022年*数据科学与机器学习现状*调查，Keras在机器学习开发者和数据科学家中达到了61%的采用率。
- en: 'If you have not worked with TensorFlow or Keras before, they can be installed
    with the following code snippet:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前没有使用过TensorFlow或Keras，可以使用以下代码片段进行安装：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For deep learning, a large amount of computational power is required; our normal
    CPU may not be fully prepared for the task, resulting in slow training and inference.
    The alternative is to run a GPU locally or in the cloud – hosted using Kaggle
    Kernel or Google Colab. They have a similar UI that resembles the structure of
    a Jupyter notebook, making it easy to run the code from the repository on any
    of these platforms.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习来说，需要大量的计算能力；我们的普通CPU可能无法完全胜任任务，导致训练和推理速度较慢。另一种选择是本地或云端运行GPU——可以通过Kaggle
    Kernel或Google Colab进行托管。它们有类似的用户界面，结构上类似于Jupyter notebook，使得在这些平台上运行仓库中的代码变得更加容易。
- en: You can find all the data and code files for this chapter in this book’s GitHub
    repository at [https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter07](https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter07).
    We recommend that you read through the code files in the `Chapter07` folder to
    follow along.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的GitHub仓库中找到本章的所有数据和代码文件，地址为[https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter07](https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter07)。我们建议你阅读`Chapter07`文件夹中的代码文件，跟随学习。
- en: Introducing machine learning
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入机器学习
- en: The definition of machine learning, as provided by Computer Science Wiki, is
    “*a field of inquiry devoted to understanding and building methods that “learn”
    – that is, methods that leverage data to improve performance on some set of tasks.
    It is seen as a part of artificial intelligence. Machine learning algorithms build
    a model based on sample data, known as training data, in order to make predictions
    or decisions without being explicitly programmed to* *do so*.”
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学维基提供的机器学习定义是“*一个致力于理解和构建能够‘学习’的方法的研究领域——也就是利用数据来提高某些任务表现的方法。它被视为人工智能的一部分。机器学习算法基于样本数据（称为训练数据）构建模型，从而在没有明确编程指令的情况下进行预测或决策*。”
- en: '(Source: [https://computersciencewiki.org/index.php/Machine_learning](https://computersciencewiki.org/index.php/Machine_learning))'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：[https://computersciencewiki.org/index.php/Machine_learning](https://computersciencewiki.org/index.php/Machine_learning)）
- en: Professor Jason Brownlee defines deep learning as “*a subfield of machine learning
    concerned with algorithms inspired by the structure and function of the brain
    called artificial neural networks*.” Deep learning is distinguishable from other
    machine learning methods because it uses artificial neural networks as a basis
    for its methods.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 杰森·布朗利教授将深度学习定义为“*一种受大脑结构和功能启发的机器学习子领域，称为人工神经网络*。”深度学习与其他机器学习方法的区别在于，它以人工神经网络为基础进行方法设计。
- en: 'The relationship between these two fields is generally represented as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个领域之间的关系通常表示如下：
- en: '![Figure 7.1 – Venn diagram of artificial intelligence](img/B19446_07_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 人工智能的维恩图](img/B19446_07_01.jpg)'
- en: Figure 7.1 – Venn diagram of artificial intelligence
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 人工智能的维恩图
- en: 'Let’s analyze the definition of machine learning further:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步分析机器学习的定义：
- en: Machine learning models create their own rules based on the data we provide,
    as stated by the phrases “*understanding and building methods that learn*” and
    “*make predictions or decisions without being explicitly programmed to do so*.”
    Previously, we used filters in our queries or *if statements* in our programs.
    With machine learning, particularly supervised learning, we feed data and let
    the model infer the rules. In the book *Python Data Science Handbook*, the author
    challenges the idea that the model learns by itself, instead suggesting that it
    tunes the parameters we provide by adapting to the observed data. Once it fits
    those parameters to the seen data, it can infer results as needed from unseen
    data.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型根据我们提供的数据创建自己的规则，正如“*理解并构建能够学习的方法*”以及“*在没有明确编程指令的情况下进行预测或决策*”这两句话所述。之前，我们在查询中使用了过滤器，或在程序中使用了*if语句*。使用机器学习，特别是监督学习时，我们输入数据，让模型推断出规则。在《*Python数据科学手册*》中，作者挑战了模型自动学习的观点，而是建议模型通过适应观察到的数据来调节我们提供的参数。一旦它将这些参数与已知数据进行拟合，就能根据未见过的数据推断出结果。
- en: '“*Machine learning algorithms build a model based on sample data, known as
    training data*.” Data passed to machine learning algorithms needs to be split
    at least into two: training and test data. The training dataset is used to build
    the model. The test dataset is used to evaluate the model’s capacity to make predictions
    with data it has not seen before. The model’s predictions are then compared to
    the ground-truth data and the evaluation metrics are calculated.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “*机器学习算法基于样本数据构建模型，这些数据称为训练数据*。” 传递给机器学习算法的数据需要至少分为两部分：训练数据和测试数据。训练数据集用于构建模型，测试数据集用于评估模型在未见过的数据上进行预测的能力。然后将模型的预测与真实数据进行比较，并计算评估指标。
- en: 'Machine learning techniques can be classified as supervised learning, unsupervised
    learning, and reinforcement learning. Common tasks that are solved by machine
    learning techniques are shown in *Figure 7**.2*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习技术可以分为监督学习、无监督学习和强化学习。通过机器学习技术解决的常见任务如*图 7.2*所示：
- en: '![Figure 7.2 – Machine learning applications](img/B19446_07_02.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 机器学习应用](img/B19446_07_02.jpg)'
- en: Figure 7.2 – Machine learning applications
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 机器学习应用
- en: '**Supervised learning** consists of creating a function that can map inputs
    to outputs, allowing the model to infer outputs from unseen or similar inputs.
    In this process, we use features to describe the characteristics of a variable
    and labels or tags to identify the predicted variable. Through this, our model
    can learn the relationship between the features and the labels or tags.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**的目标是创建一个可以将输入映射到输出的函数，从而使模型能够根据未见过或相似的输入推断输出。在这个过程中，我们使用特征来描述变量的特性，标签或标记来识别预测的变量。通过这一过程，我们的模型能够学习特征与标签或标记之间的关系。'
- en: In Web3 analysis, **tagging** plays a crucial role as it allows us to attribute
    an identity to addresses that are a combination of numbers and letters and have
    no direct connection to the outside world. However, creating a library of tagged
    addresses can be a challenging task and just recently, it has become the business
    of a company named Arkham that incentivizes the “*de-anonymizing of the blockchain*”
    with public data. Tagged addresses are one of the main leverages for companies
    such as Nansen, which have made significant progress in tagging hundreds of addresses
    on Ethereum and other chains, enabling machine learning techniques and data analysis
    reports.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Web3 分析中，**标签**起着至关重要的作用，因为它使我们能够为那些由数字和字母组成、与外部世界没有直接联系的地址赋予身份。然而，创建一个标记地址库可能是一个挑战，最近，这已经成为一家名为
    Arkham 的公司的业务，该公司通过公开数据激励“*去匿名化区块链*”。标记地址是像 Nansen 这样的公司主要的杠杆之一，它们在以太坊及其他链上标记了数百个地址，推动了机器学习技术和数据分析报告的进展。
- en: Tagging can also be found in Etherscan, where important projects tag their addresses
    to enable public audits. Also, Dune and Flipside have tables with labels where
    their research teams add relevant information that can help with queries. If you
    want to learn more about identity attribution, Nick Fourneaux, in the book *Investigating
    Cryptocurrencies*, teaches how to extract addresses from websites such as forums
    or software-sharing sites, download HTML as raw text, and execute a regex analysis.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 标签也可以在 Etherscan 中找到，重要的项目会标记其地址以便进行公开审计。此外，Dune 和 Flipside 也有带标签的表格，研究团队在其中添加了相关信息，以帮助查询。如果你想了解更多关于身份归属的知识，可以参考
    Nick Fourneaux 在《*调查加密货币*》一书中的方法，教你如何从论坛或软件下载网站提取地址，下载 HTML 原始文本，并执行正则表达式分析。
- en: Supervised learning can be further divided into regression and classification
    techniques. In classification techniques, we have a discrete set of categories
    as labels (such as fraudulent transactions or non-fraudulent transactions). In
    regression, we have quantitative labels, such as the price of NFT art or tokens.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习可以进一步分为回归和分类技术。在分类技术中，我们有一组离散的类别作为标签（例如欺诈交易或非欺诈交易）。在回归中，我们有定量标签，例如 NFT 艺术品或代币的价格。
- en: '**Unsupervised learning** consists of trying to identify the structure or patterns
    of a dataset that may not be explicit. The tasks that fall under unsupervised
    learning typically include the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习**的目标是尝试识别数据集中可能不显式的结构或模式。无监督学习下的任务通常包括以下几种：'
- en: Clustering – that is, identifying groups within a given dataset
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类 – 即在给定的数据集中识别出不同的群体
- en: Dimensionality reduction – that is, attempting to represent the dataset with
    a smaller amount of features
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维 —— 即，尝试用更少的特征表示数据集
- en: Novelty detection – that is, trying to identify when a change has occurred in
    the data
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新颖性检测 —— 即，尝试识别数据中何时发生变化
- en: '**Reinforcement learning** teaches a model to find the optimal solution for
    a problem by leveraging what the model already knows and what it can learn via
    a cumulative reward after interacting with its environment. The model receives
    feedback from the environment in the form of rewards or penalties, and its goal
    is to maximize its total reward. The idea behind reinforcement learning is to
    mimic the way humans learn by trial and error:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**通过利用模型已知的信息以及通过与环境互动后获得的累计奖励来教导模型寻找问题的最佳解决方案。模型通过环境反馈（奖励或惩罚）来进行调整，目标是最大化总奖励。强化学习的核心思想是模仿人类通过试错学习的方式：'
- en: '![Figure 7.3 – Agent-environment loop (adapted from https://gymnasium.farama.org/content/basic_usage/)](img/B19446_07_03.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 智能体-环境循环（改编自 https://gymnasium.farama.org/content/basic_usage/）](img/B19446_07_03.jpg)'
- en: Figure 7.3 – Agent-environment loop (adapted from [https://gymnasium.farama.org/content/basic_usage/](https://gymnasium.farama.org/content/basic_usage/))
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 智能体-环境循环（改编自 [https://gymnasium.farama.org/content/basic_usage/](https://gymnasium.farama.org/content/basic_usage/)）
- en: 'To make a project come to life, there are some initial business/data steps
    that must be undertaken:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使一个项目变得生动起来，必须执行一些初步的业务/数据步骤：
- en: '*Defining a Web3 data science problem* means stating what we want to solve
    with the data we have with precision. In such a definition, we have to be able
    to describe the problem we want to solve, why we want to solve it, and what assumptions
    are considered.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*定义一个 Web3 数据科学问题*意味着精确地阐明我们希望用现有数据解决的问题。在这样的定义中，我们必须能够描述我们要解决的问题，为什么要解决它，以及我们所考虑的假设。'
- en: '*Getting the data* means getting our hands on the dataset we will work with.
    It is possible that the dataset has already been built with all the rows and columns
    of interest, or that we have to build it by combining multiple sources of data.
    An initial list of data sources is listed in *Chapters 2* and *3*. More data sources
    may be needed, depending on the problem we will tackle.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*获取数据*意味着获取我们将要处理的数据集。数据集可能已经包含了所有相关的行和列，或者我们需要通过结合多个数据源来构建它。初步的数据源列表列在*第2章*和*第3章*中。根据我们要解决的问题，可能需要更多的数据源。'
- en: '*EDA* is used to make sense of the dataset using summary statistics and data
    visualization techniques. *Data preparation* is a preprocessing step where we
    transform the dataset to improve its quality or make it digestible to the model.
    On-chain data may need a lot of transformations. We analyzed some of those methods
    in [*Chapter 6*](B19446_06.xhtml#_idTextAnchor210).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*EDA* 用于通过总结统计和数据可视化技术来理解数据集。*数据准备*是一个预处理步骤，在这个步骤中，我们转换数据集以提高其质量或使其更容易为模型消化。链上数据可能需要大量的转换。我们在[*第6章*](B19446_06.xhtml#_idTextAnchor210)中分析了一些这些方法。'
- en: Now, let’s analyze the steps to select, train, and evaluate a model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们分析选择、训练和评估模型的步骤。
- en: Building a machine learning pipeline
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建机器学习流程
- en: 'After cleaning the data and selecting the most important features, the machine
    learning flow can be summarized into steps, as shown in *Figure 7**.4*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在清理数据并选择最重要的特征后，机器学习流程可以概括为以下几个步骤，如*图 7.4*所示：
- en: '![Figure 7.4 – Machine learning pipeline](img/B19446_07_04.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – 机器学习流程](img/B19446_07_04.jpg)'
- en: Figure 7.4 – Machine learning pipeline
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 机器学习流程
- en: 'To carry out this process, we must do the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这个过程，我们必须进行以下操作：
- en: Select a model and its initial parameters based on the problem and available
    data.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据问题和可用数据选择一个模型及其初始参数。
- en: 'Train: First, we must split the data into a training set and a test set. The
    process of training consists of making the model learn from the data. Each model’s
    training process can vary in time and computational consumption. To improve the
    model’s performance, we must employ hyperparameter tuning through techniques such
    as grid search or random grid search.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练：首先，我们必须将数据分割成训练集和测试集。训练过程包括让模型从数据中学习。每个模型的训练过程在时间和计算消耗上可能有所不同。为了提高模型的性能，我们必须通过超参数调整技术（如网格搜索或随机网格搜索）来优化模型。
- en: 'Predict and evaluate: The trained model is then used to predict over the test
    set, which contains rows of data that have not been seen by the algorithm. If
    we evaluate the model with the data that we used to train it, the model will always
    predict well, and we will not be able to improve it. Model performance is assessed
    using task-specific evaluation metrics.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测和评估：然后，使用训练好的模型对测试集进行预测，测试集包含算法未曾见过的数据行。如果我们使用训练数据评估模型，模型总是能够预测得很好，那么我们就无法进一步改进它。模型的表现通过特定任务的评估指标来评估。
- en: When we achieve a good model, we must save it so that we can use it when we
    receive unseen data. We can use tools such as *Pickle* and *Keras Tokenizer* to
    accomplish this. Pickle serializes the trained model and converts it into a file,
    allowing it to be used in another environment. To produce a result, we must pass
    data with the same structure that it is ready to receive so that the model can
    make predictions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们得到一个良好的模型时，必须保存它，以便在接收到未见数据时使用它。我们可以使用*Pickle*和*Keras Tokenizer*等工具来实现这一点。Pickle将训练好的模型序列化并转换为文件，使其能够在另一个环境中使用。为了产生结果，我们必须传递结构相同的数据，使模型能够进行预测。
- en: Let’s apply this pipeline with a hands-on example. In `Chapter07/ML_warmup`,
    we aim to identify fraudulent transactions on the Ethereum network using a Kaggle
    dataset named *Ethereum Fraud Detection Dataset*, where only 17% of its rows are
    fraudulent. This is a typical supervised classification task.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个实际示例来应用这个流程。在`Chapter07/ML_warmup`中，我们的目标是使用一个名为*以太坊欺诈检测数据集*的Kaggle数据集，识别以太坊网络上的欺诈交易，其中只有17%的行是欺诈的。这是一个典型的监督分类任务。
- en: Model
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: 'Based on the problem at hand, we must select a model or a couple of models
    to test which one performs better on our data. If we are unsure about the model
    to select, we can examine similar structured problems solved on Kaggle. In the
    Jupyter notebook, we selected a random forest classifier with the following code
    snippet:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据手头的问题，我们必须选择一个或多个模型进行测试，以判断哪个模型在我们的数据上表现更好。如果我们不确定该选择哪个模型，可以查看Kaggle上已解决的相似结构问题。在Jupyter
    notebook中，我们选择了一个随机森林分类器，代码片段如下：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Many algorithms are available for training and the choice can be difficult.
    One way to choose among many models is to reduce the reducible error. Literature
    usually refers to this matter as the bias-variance trade-off. Before addressing
    that trade-off, we need to understand the different types of errors that exist.
    The prediction error for any machine learning algorithm can be classified as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多算法可以用于训练，选择可能会很困难。选择多种模型的一种方法是减少可减少的误差。文献通常将这个问题称为偏差-方差权衡。在解决这个权衡之前，我们需要理解存在的不同类型的误差。任何机器学习算法的预测误差可以分为以下几类：
- en: '**Noise** or **irreducible error**: This type of error cannot be deleted, no
    matter how well we implement the model.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声**或**不可减少的误差**：这种类型的误差无论我们如何实现模型，都无法消除。'
- en: '**Bias error**: This can be reduced. Wikipedia defines it as “*an error from
    erroneous assumptions in the learning algorithm*.” A model with high bias oversimplifies
    reality and leads to a high error between the prediction and the ground-truth
    value. High-bias models oversimplify, which means that they do not have enough
    parameters to capture the complexity of the data they learn from, resulting in
    underfitting. More on this concept will be covered in the next section.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差误差**：这是可以减少的。维基百科将其定义为“*由于学习算法中的错误假设而产生的误差*”。高偏差的模型过于简化现实，导致预测值和真实值之间存在较大的误差。高偏差模型过度简化，意味着它们没有足够的参数来捕捉它们学习数据的复杂性，导致欠拟合。下一节将详细讨论这一概念。'
- en: '**Variance error**: This can also be reduced. Wikipedia defines it as an error
    derived from “*sensitivity to small fluctuations in the training set*.” This means
    that the model is learning the particularities of the training dataset so well
    that it will not generalize enough to predict on unseen data. These models are
    highly dependent on the exact training data and are unable to generalize. We encounter
    this error when the model performs well on training data but poorly on test/validation
    data, indicating an overfitting problem.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差误差**：这也是可以减少的。维基百科将其定义为“*对训练集中的微小波动的敏感性导致的误差*”。这意味着模型过于深入地学习了训练数据集的特殊性，以至于无法对未见数据进行有效的泛化。这些模型高度依赖于精确的训练数据，无法做到泛化。当模型在训练数据上表现良好，但在测试/验证数据上表现不佳时，我们就会遇到这个误差，表明存在过拟合问题。'
- en: Low variance with high bias algorithms train less complex models with a rather
    simple or rigid underlying structure – for example, linear regression. On the
    other hand, high variance with low bias algorithms train complex, flexible models
    that can be exact on training data but inconsistent in prediction – for example,
    KNN.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 低方差高偏差的算法训练复杂度较低的模型，具有简单或刚性的基础结构 – 例如线性回归。另一方面，高方差低偏差的算法训练复杂、灵活的模型，可以在训练数据上准确，但在预测中不一致
    – 例如KNN。
- en: 'If we understand bias and variance and recognize that both are derived from
    the choice of the model we make, to make an optimal decision we will have to choose
    the model that reduces the total error with a trade-off between both:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们理解了偏差和方差，并认识到两者都源于我们所做的模型选择，为了做出最优决策，我们将选择在两者之间进行总误差的权衡的模型：
- en: '![Figure 7.5 – The bias-variance trade-off](img/B19446_07_05.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – 偏差-方差权衡](img/B19446_07_05.jpg)'
- en: Figure 7.5 – The bias-variance trade-off
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 偏差-方差权衡
- en: Another criterion for selecting a model is its performance, which is measured
    by the evaluation metric of choice. We can run multiple models and evaluate them
    all with the same metric, and the model that performs better is the one we continue
    tuning. We will discuss evaluation metrics in subsequent sections.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 选择模型的另一个标准是其性能，这由所选的评估指标来衡量。我们可以运行多个模型，并使用相同的指标评估它们，性能更好的模型将被继续调优。我们将在后续章节讨论评估指标。
- en: In `Chapter07/ML_warmup`, we selected a random forest classifier. This algorithm
    looks to reduce the variance of the model without compromising bias and performs
    well in the evaluation metric known as recall. More about the random forest algorithm
    can be found in the *Further* *reading* section.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Chapter07/ML_warmup`中，我们选择了随机森林分类器。这种算法旨在减少模型的方差而不损害偏差，并在被称为召回率的评估指标上表现良好。有关随机森林算法的更多信息，请参阅*进一步阅读*部分。
- en: Training
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: 'To begin the training process, we split the data into a training dataset and
    a test dataset. This allows us to keep part of the data unseen by the model during
    training, and we can evaluate its performance afterward:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 开始训练过程时，我们将数据分为训练数据集和测试数据集。这样做可以使模型在训练过程中看不到部分数据，并且我们可以在训练后评估其性能：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The training process consists of passing the features and labels to the algorithm
    so that it learns from them. The learning algorithm will try to find patterns
    in the training data that map the attributes of the input data to the target.
    The trained model captures these patterns.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程包括将特征和标签传递给算法，使其从中学习。学习算法将尝试在训练数据中找到模式，将输入数据的属性映射到目标上。训练后的模型捕获这些模式。
- en: 'In `Chapter07/ML_warmup`, we instruct the model to learn with the following
    code snippet:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Chapter07/ML_warmup`中，我们使用以下代码片段指导模型学习：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Underfitting and overfitting
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欠拟合和过拟合
- en: Let’s consider three scenarios, where the model is represented by a black line.
    Which scenario performs classification better?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑三种情景，模型由一条黑线表示。哪种情景能更好地进行分类？
- en: '![Figure 7.6 – Three classification scenarios](img/B19446_07_06.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 三种分类情景](img/B19446_07_06.jpg)'
- en: Figure 7.6 – Three classification scenarios
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 三种分类情景
- en: 'Let’s understand the scenarios:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解这些情景：
- en: '**Scenario A**: The model is very simple and unable to capture the boundary
    between the two classes. This is called **underfitting**.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情景 A**：模型非常简单，无法捕捉两个类别之间的边界。这被称为**欠拟合**。'
- en: '**Scenario B**: The model was able to find an acceptable boundary between both
    classes, although it may misclassify some of the border samples. In general, it
    captures the complexity of the dataset.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情景 B**：模型能够找到两类之间可接受的边界，尽管可能会误分类一些边界样本。总体上，它捕捉了数据集的复杂性。'
- en: '**Scenario C**: The model adapted too much to the training dataset and learned
    all the details, not just the relevant characteristics that differentiate one
    class from another. It was unable to generalize. This is called **overfitting**.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情景 C**：模型过度适应了训练数据集，并学到了所有细节，而不仅仅是区分一个类别与另一个类别的相关特征。它无法泛化。这被称为**过拟合**。'
- en: '![Figure 7.7 – What a model does when overfitting (source: https://twitter.com/MaartenvSmeden/status/1522230905468862464)](img/B19446_07_07.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 过拟合时模型的行为（来源：https://twitter.com/MaartenvSmeden/status/1522230905468862464）](img/B19446_07_07.jpg)'
- en: 'Figure 7.7 – What a model does when overfitting (source: https://twitter.com/MaartenvSmeden/status/1522230905468862464)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 模型过拟合时的表现（来源：https://twitter.com/MaartenvSmeden/status/1522230905468862464）
- en: We aim for scenario B, where the model is complex enough to capture the important
    features but does not adapt too much to the training data so that it performs
    well on unseen samples.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是场景B，在这个场景中，模型足够复杂，能够捕捉重要特征，但不会过度适应训练数据，因此在未见过的样本上表现良好。
- en: Prediction and evaluation
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测与评估
- en: Here, we pass unseen data to our trained model and evaluate how accurate its
    predictions are compared to the ground truth. If the result is acceptable, we
    keep the model; otherwise, we tune hyperparameters and train again. *A hyperparameter
    is a variable that is set before the training process and cannot be changed during
    learning. Parameters are those that are fine-tuned* *during training.*
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将未见过的数据传入训练好的模型，并评估它的预测与实际情况的准确性。如果结果可接受，我们会保留模型；否则，我们将调整超参数并重新训练。*超参数是在训练过程前设置的变量，在学习过程中不能更改。参数是那些在训练过程中调整的值。*
- en: 'In the Jupyter notebook, we use the following code snippet to predict and evaluate
    the model:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter notebook中，我们使用以下代码片段进行预测和评估：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To evaluate whether the result is acceptable and we can keep the model, we
    can use the confusion matrix for a binary classification task. The resulting confusion
    matrix for the dataset we analyzed in the Jupyter notebook is shown in *Figure
    7**.8*:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估结果是否可接受并决定是否保留模型，我们可以使用二分类任务的混淆矩阵。在Jupyter notebook中，我们分析的数据集的混淆矩阵如*图7.8*所示：
- en: '![Figure 7.8 – Confusion matrix](img/B19446_07_08.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – 混淆矩阵](img/B19446_07_08.jpg)'
- en: Figure 7.8 – Confusion matrix
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 混淆矩阵
- en: 'Let’s understand the components of the confusion matrix:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来理解混淆矩阵的组成部分：
- en: '**True negative** (**TN**): The model predicted negative, and it is true. These
    transactions are not fraudulent.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真反例**（**TN**）：模型预测为负，并且预测正确。这些交易不是欺诈性的。'
- en: '**True positive** (**TP**): The model predicted positive, and it is true. These
    transactions are fraudulent.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正例**（**TP**）：模型预测为正，并且预测正确。这些交易是欺诈性的。'
- en: '**False negative** (**FN**): The model failed to predict and they were fraudulent.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假反例**（**FN**）：模型未能预测，并且它们是欺诈性的。'
- en: '**False positive** (**FP**): The model flagged these transactions as fraudulent,
    but they were not.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假正例**（**FP**）：模型将这些交易标记为欺诈性交易，但它们并非如此。'
- en: Based on these numbers, we can calculate precision and recall. **Precision**
    answers the question, of all the classes we predicted as positive, how many were
    actually positive?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些数据，我们可以计算精确度和召回率。**精确度**回答的问题是：我们预测为正的所有类别中，实际为正的有多少？
- en: TP _ TP + FP
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: TP _ TP + FP
- en: Our precision is `0.91`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的精确度是`0.91`。
- en: '**Recall** answers the question, of all the fraudulent classes, how many did
    our model predict correctly? The formula for this is as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**召回率**回答的问题是：所有欺诈性类别中，我们的模型正确预测了多少？其公式如下：'
- en: TP _ TP + FN
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: TP _ TP + FN
- en: Our recall is `0.98`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的召回率是`0.98`。
- en: The evaluation of the results depends on the problem at hand. Selecting the
    metrics correctly is very important as it will impact the subsequent decisions
    we make. In `Chapter07/ML_warmup`, we are working to find fraudulent transactions,
    so we value models that result in higher recall than precision. We prefer recall
    because the cost of missing a fraudulent transaction is much higher than flagging
    a potentially harmless transaction. However, the number of FP flags cannot be
    enormous because of the cost of the transaction and its impact on the client.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的评估取决于具体问题。正确选择评估指标非常重要，因为它将影响我们后续的决策。在`Chapter07/ML_warmup`中，我们的目标是发现欺诈交易，因此我们更看重召回率高于精确度。我们更偏向召回率，因为漏掉一个欺诈交易的成本远高于错误标记一个可能无害的交易。然而，假正例（FP）的数量不能过大，因为这会带来交易成本并影响客户。
- en: Real-world datasets are mostly imbalanced, which means that the classes are
    not equally represented. It is our job to apply techniques that enable the model
    to learn about the existence and characteristics of both classes, particularly
    when the less frequent class is the one that we are trying to detect.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的数据集大多数是不平衡的，这意味着不同类别的样本数量不均衡。我们的任务是应用技术，让模型学习两种类别的存在和特征，特别是当我们试图检测的类别是较少出现的类别时。
- en: A note on balanced and imbalanced datasets
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 关于平衡和不平衡数据集的说明
- en: '**Accuracy**, as the percentage of correct predictions, is another commonly
    used evaluation metric. However, it will not yield good results if the dataset
    is not balanced. If we take accuracy as an evaluation metric in an imbalanced
    dataset, the model only needs to identify the majority class to return a good
    result, and that does not guarantee that this is a good model.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**准确度**，即正确预测的百分比，是另一个常用的评估指标。然而，如果数据集不平衡，准确度将无法得出好的结果。如果在不平衡数据集中将准确度作为评估指标，模型只需要识别出多数类，就能返回一个好的结果，这并不能保证这是一个好的模型。'
- en: In our EDA, we will examine the proportion of each class and determine whether
    we are dealing with a balanced or imbalanced dataset. For example, in `Chapter07/ML_warmup`,
    we know that the proportion of fraudulent stances is 17%.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的EDA（探索性数据分析）中，我们将检查每个类别的比例，并确定我们处理的是平衡数据集还是不平衡数据集。例如，在`Chapter07/ML_warmup`中，我们知道欺诈性样本的比例是17%。
- en: We can solve this by using oversampling or undersampling techniques in the feature
    engineering preprocessing step. This must be done with caution as it may alter
    the underlying relationships in our data or remove some critical information.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在特征工程预处理步骤中使用过采样或欠采样技术来解决这个问题。必须谨慎进行，因为这可能会改变数据中的潜在关系，或者删除一些关键信息。
- en: We can also use algorithms that have already been optimized for imbalanced datasets
    and allow the user to add that information to the training process – for example,
    by using the `class_weight` parameter in the random forest algorithm.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用已经为不平衡数据集优化的算法，并允许用户在训练过程中添加这些信息——例如，使用随机森林算法中的`class_weight`参数。
- en: Additionally, we can optimize the split by considering the unequal representation
    of the classes by using `stratify` in `train_test_split`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以通过在`train_test_split`中使用`stratify`来考虑类别的不平衡表示，从而优化拆分过程。
- en: Introducing deep learning
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入深度学习
- en: In *Part 2* of this book, we will also use deep learning methodologies when
    solving the use cases. Deep learning models employ multiple layers of interconnected
    nodes called neurons, which process input data and produce outputs based on learned
    weights and activation functions. The connections between neurons facilitate information
    flow, and the architecture of the network determines how information is processed
    and transformed.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的*第二部分*中，我们在解决用例时也将使用深度学习方法。深度学习模型采用多个互联的节点（称为神经元），这些神经元处理输入数据，并基于学习到的权重和激活函数产生输出。神经元之间的连接促进了信息流动，网络的架构决定了信息如何被处理和转化。
- en: We will study three types of neural network architectures in detail in their
    corresponding chapters. For now, let’s introduce the framework and terminology
    that we will use in them.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在各自的章节中详细研究三种神经网络架构。现在，让我们先介绍一下我们将在这些章节中使用的框架和术语。
- en: 'The neuron serves as the fundamental building block of the system and can be
    defined as a node with one or more input values, weights, and output values:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元作为系统的基本构建单元，可以定义为一个节点，具有一个或多个输入值、权重和输出值：
- en: '![Figure 7.9 – A neuron’s structure](img/B19446_07_09.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 神经元的结构](img/B19446_07_09.jpg)'
- en: Figure 7.9 – A neuron’s structure
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 神经元的结构
- en: 'When we stack multiple layers with this structure, it becomes a neural network.
    This architecture typically consists of an input layer, hidden layers, and an
    output layer:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将多个具有这种结构的层叠加在一起时，它就形成了一个神经网络。这种架构通常由输入层、隐藏层和输出层组成：
- en: '![Figure 7.10 – Neural network structure](img/B19446_07_10.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – 神经网络结构](img/B19446_07_10.jpg)'
- en: Figure 7.10 – Neural network structure
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 神经网络结构
- en: The input layer initiates the network and passes data to the hidden layers,
    which perform calculations on the features and patterns. The more hidden layers
    there are, the more complex calculations are executed.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层启动网络并将数据传递给隐藏层，隐藏层对特征和模式进行计算。隐藏层越多，执行的计算就越复杂。
- en: The output layer receives the processed information from the hidden layers and
    provides a result or output summarizing the information that’s been processed
    within the network.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层接收来自隐藏层的处理信息，并提供一个总结网络内已处理信息的结果或输出。
- en: The connections between nodes contain weights that carry information on how
    to solve a specific problem. During model training, we calibrate these weights
    to adapt the model to our dataset. The weights represent the learnable parameters
    of the model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 节点之间的连接包含权重，这些权重携带有关如何解决特定问题的信息。在模型训练过程中，我们调整这些权重以使模型适应我们的数据集。这些权重代表了模型的可学习参数。
- en: 'This flexible structure allows users to tune numerous hyperparameters to enhance
    the model’s performance. The fundamentals are as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个灵活的结构允许用户调节多个超参数，以增强模型的表现。基本原理如下：
- en: '**Learning rate**: This hyperparameter controls how much a model changes in
    response to weight updates. Finding the correct value is crucial as a very small
    learning rate may result in a lengthy training process, while a higher one can
    lead to sub-optimal weight sets and altered results. The learning rate is closely
    related to the optimizers.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：这个超参数控制模型在响应权重更新时的变化程度。找到正确的学习率非常重要，因为过小的学习率可能会导致训练过程过长，而较高的学习率可能会导致次优的权重集合和结果变化。学习率与优化器密切相关。'
- en: '**Activation functions**: These functions determine whether a neuron should
    be activated or not, meaning they decide whether the neuron’s input to the network
    is important for the prediction process using simple mathematical operations.
    The activation function derives output from a set of input values fed into each
    layer. A list of activation functions in Keras can be found at [https://keras.io/api/layers/activations/](https://keras.io/api/layers/activations/).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：这些函数决定神经元是否应该被激活，即它们决定神经元对网络的输入是否对预测过程重要，通常通过简单的数学运算来实现。激活函数从每层输入的值中得出输出。Keras中的激活函数列表可以在[https://keras.io/api/layers/activations/](https://keras.io/api/layers/activations/)找到。'
- en: '**Cost functions**: These functions quantify the error between the predicted
    and expected values, summarizing the model’s performance in a single value to
    be minimized during training. The choice of the cost function depends on the problem
    being solved, with common examples being mean squared error for regression tasks
    and categorical cross-entropy for classification tasks. Keras lists the various
    losses at [https://keras.io/api/losses/](https://keras.io/api/losses/).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**：这些函数量化了预测值与期望值之间的误差，将模型的表现总结为一个需要在训练过程中最小化的单一值。损失函数的选择取决于所解决的问题，常见的例子包括回归任务中的均方误差和分类任务中的交叉熵。Keras列出了各种损失函数，详情请见[https://keras.io/api/losses/](https://keras.io/api/losses/)。'
- en: '**Optimizers**: These algorithms help improve model performance by adjusting
    the attributes of the neural network. Its responsibility in the architecture is
    to change the learning rate and weights of the neurons to reach the minimum of
    the loss function. The supported optimizers in Keras are listed here: [https://keras.io/api/optimizers/](https://keras.io/api/optimizers/).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**：这些算法通过调整神经网络的属性来帮助改善模型的表现。它在架构中的职责是调整学习率和神经元的权重，以达到损失函数的最小值。Keras支持的优化器列在此：[https://keras.io/api/optimizers/](https://keras.io/api/optimizers/)。'
- en: '**Epochs**: This denotes the number of times the algorithm runs through the
    entire dataset.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练轮数**：这表示算法遍历整个数据集的次数。'
- en: '**Batch size**: This refers to the number of samples considered to update the
    model parameters. A batch size of *N* means that *N* samples from the training
    dataset will be used to update the model parameters. Keep in mind that these samples
    are held in memory, so a higher batch size requires more memory.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**：指的是用于更新模型参数的样本数量。批量大小为*N*意味着将使用训练数据集中的*N*个样本来更新模型参数。请记住，这些样本会保存在内存中，因此更高的批量大小需要更多的内存。'
- en: All the models we use, will be analyzed within the Keras framework, which has
    excellent documentation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的所有模型都将在Keras框架下进行分析，Keras具有出色的文档。
- en: Model preparation
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型准备
- en: In `Chapter07/DL_warmup`, we will work with the same dataset as in the previous
    section – *Ethereum Fraud Detection Dataset*. This time, we will select fewer
    columns and standardize the numbers using `RobustScaler()` from sklearn.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Chapter07/DL_warmup`中，我们将使用与上一节相同的数据集——*以太坊欺诈检测数据集*。这一次，我们将选择更少的列，并使用`RobustScaler()`来自sklearn对数字进行标准化。
- en: As in all prediction problems, we want to separate the test and training datasets
    with `train test` `split ()`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有预测问题一样，我们希望通过`train test` `split ()`将测试集和训练集分开。
- en: Model building
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型构建
- en: We’ll create a sequential model with Keras. The structure of sequential models
    consists of a stack of the same or different layers, where the output of one layer
    goes into the other.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Keras创建一个顺序模型。顺序模型的结构由相同或不同的层堆叠而成，其中一层的输出进入另一层。
- en: 'The following code snippet sets the input layer to expect rows of data with
    the number of columns of the dataset. In this case, we are only working with 11
    columns:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段设置输入层，期望输入具有与数据集列数相同的行数据。在这种情况下，我们只处理11列数据：
- en: '[PRE6]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We add three hidden layers, each with a decreasing number of nodes. All of
    them use `relu` as the activation function. The `Dense` layer is a fully connected
    layer and is one of the many types of layers, such as `Convolutional` or `LSTM`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了三层隐藏层，每一层的节点数逐渐减少。所有这些层都使用`relu`作为激活函数。`Dense`层是一个全连接层，是多种类型层之一，例如`卷积层`或`LSTM`层：
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Since this is a binary classification task, in the last layer, we will use
    the `sigmoid` activation function and `1` in the output layer:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个二分类任务，在最后一层，我们将使用`sigmoid`激活函数和输出层中的`1`：
- en: '[PRE8]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Before training the model, it needs to be compiled with the optimizer, the
    loss functions, and the metrics. The compiler configures the learning process.
    It’s worth mentioning that as this is an imbalanced dataset, we are interested
    in precision and recall, so we must build the metric by leveraging the `keras`
    library, as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型之前，需要通过优化器、损失函数和评估指标来编译模型。编译器配置学习过程。值得一提的是，由于这是一个不平衡的数据集，我们关心的是精确度和召回率，因此我们必须利用`keras`库来构建评估指标，具体如下：
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we must add it to the compiler:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须将其添加到编译器中：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Training and evaluating a model
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和评估模型
- en: Once the model has been built, we have to feed it our dataset, which means we
    have to train it. This is done with `fit(),` and in this case, we decided to do
    it for 90 epochs.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型构建完成，我们需要将数据集输入模型进行训练。这是通过`fit()`来完成的，在本例中，我们决定训练90个epoch。
- en: Once training has been performed, it is necessary to evaluate the model by predicting
    data that has not been part of the training. We can do this with `X_test` and
    `y_test`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成训练，就需要通过预测未参与训练的数据来评估模型。我们可以使用`X_test`和`y_test`来实现这一点。
- en: The classification report shows that recall for the minority class is 95%, which
    is very good. With more data preprocessing and by applying techniques for imbalanced
    datasets and hyperparameter tuning, we could further improve the results.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 分类报告显示，少数类的召回率为95%，这一结果非常好。通过更多的数据预处理，应用针对不平衡数据集的技术以及超参数调优，我们可以进一步提高结果。
- en: In this particular exercise, one of the Zen of Python principles applies perfectly.
    *Simple is better than complex* – a simpler machine learning model performed better
    than a complex neural network.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的练习中，**Python Zen**的原则之一得到了完美应用。*简单优于复杂* —— 一个更简单的机器学习模型表现优于复杂的神经网络。
- en: 'Now that we have explored both methodologies, we will highlight additional
    characteristics of each field:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了这两种方法，我们将重点介绍每个领域的附加特征：
- en: '| **Machine learning** | **Deep learning** |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| **机器学习** | **深度学习** |'
- en: '| Can train and make inferences from smaller datasets | Requires large amounts
    of data |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 可以从较小的数据集进行训练和推断 | 需要大量数据 |'
- en: '| Shorter training and can be done with a CPU | Longer training and needs a
    GPU to train effectively |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 较短的训练时间，可以使用CPU完成 | 较长的训练时间，需要GPU才能有效训练 |'
- en: '| Makes simple correlations | Makes non-linear complex correlations |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 进行简单的相关性分析 | 进行非线性复杂相关性分析 |'
- en: '| Mostly explainable | Opaque model, complex to explain |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 大多是可解释的 | 不透明的模型，难以解释 |'
- en: Table 7.1 – Differences between machine learning and deep learning
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.1 – 机器学习与深度学习的区别
- en: A note on the ethical and social impact of artificial intelligence
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 关于人工智能的伦理和社会影响的提示
- en: 'Discussions regarding ethics and social impact may appear distant from our
    daily work, but given that our projects typically unfold within a business environment,
    it is advisable to consider their broader implications. The ethical and social
    ramifications of machine learning and deep learning encompass diverse dimensions,
    including the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 关于伦理和社会影响的讨论可能离我们的日常工作较远，但考虑到我们的项目通常在商业环境中展开，因此建议考虑其更广泛的影响。机器学习和深度学习的伦理与社会影响涵盖了多个方面，包括以下内容：
- en: '**Bias**: Similar to the bias error, which oversimplifies its learning outcome,
    machine learning models can inherit biases present in training data, potentially
    leading to discriminatory outcomes. Bias can be introduced at various stages of
    the machine learning life cycle, from data collection to model deployment. It
    is important to obtain unbiased data to train our models and regularly audit them
    to detect and rectify bias.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差**：与偏差误差类似，机器学习模型可能继承训练数据中的偏差，进而导致歧视性结果。偏差可以在机器学习生命周期的各个阶段引入，从数据收集到模型部署。为了训练我们的模型，获取无偏的数据非常重要，并且应定期审计模型以检测并修正偏差。'
- en: '**Transparency**: The opacity of complex machine learning models poses challenges
    for regulators and may undermine user trust. Many DeFi ventures are actively seeking
    regulatory approval to facilitate the flow of funds from traditional banking systems
    into the DeFi world. Given the highly regulated nature of the finance sector,
    data scientists working in this domain must make efforts to enhance model interpretability
    and provide explanations for their decisions to regulatory authorities.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**透明性**：复杂机器学习模型的不透明性给监管机构带来了挑战，可能削弱用户的信任。许多DeFi项目正在积极寻求监管批准，以促进资金从传统银行体系流向DeFi世界。鉴于金融领域的高度监管性质，在该领域工作的数据科学家必须努力提升模型的可解释性，并向监管机构提供决策的解释。'
- en: Addressing these ethical considerations necessitates a multidisciplinary approach
    that involves technology developers, policymakers, ethicists, and more. As professionals
    working with models, we need to keep these challenges in mind, especially when
    selecting datasets, preprocessing them, or evaluating their results in the real
    world.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些伦理问题需要一个多学科的方式，涉及技术开发者、政策制定者、伦理学家等。作为从事模型工作专业人士，我们需要牢记这些挑战，尤其是在选择数据集、进行数据预处理或在现实世界中评估模型结果时。
- en: Summary
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we delved into the fundamental concepts of artificial intelligence,
    which will serve as the foundation for our journey in *Part 2* of this book. We
    explored various types of tasks, including supervised learning, unsupervised learning,
    and reinforcement learning. Through a hands-on example, we gained insights into
    the typical machine learning process, which encompasses model selection, training,
    and evaluation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了人工智能的基本概念，这将为我们在本书*第二部分*中的旅程奠定基础。我们探讨了包括监督学习、无监督学习和强化学习在内的各种任务类型。通过一个实践示例，我们深入了解了典型的机器学习过程，包括模型选择、训练和评估。
- en: Throughout this chapter, we acquired essential knowledge related to common challenges
    in machine learning, such as striking the right balance between underfitting and
    overfitting models, the existence of imbalanced datasets, and which metrics are
    relevant to evaluate models that are trained with them. Understanding these concepts
    is vital for any successful machine learning project.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们获得了与机器学习中常见挑战相关的基本知识，如在模型欠拟合和过拟合之间找到正确的平衡，存在不平衡数据集，以及哪些评估指标适用于训练过这些数据集的模型。理解这些概念对于任何成功的机器学习项目至关重要。
- en: Moreover, we progressed into the basics of deep learning, where we explored
    the key components of a neural network using Keras. Additionally, we implemented
    a pipeline to tackle a supervised problem to see all the concepts duly applied.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们深入学习了深度学习的基础知识，使用Keras探索了神经网络的关键组件。此外，我们实现了一个流程来处理一个监督学习问题，亲自体验了所有概念的应用。
- en: In the next chapter, we will discuss an important topic, sentiment analysis.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论一个重要的话题——情感分析。
- en: Further reading
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 想要了解更多本章涵盖的主题，请参考以下资源：
- en: 'Definitions:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义：
- en: 'Igual, L. and Seguí, S. (2017). *Introduction to data science*: *A python approach
    to concepts, techniques and* *applications*. Springer.'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Igual, L. 和 Seguí, S. (2017). *数据科学导论*：*基于Python的概念、技术和应用方法*。Springer。
- en: Ertel, W. (2018). *Introduction to artificial* *intelligence*. Springer.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ertel, W. (2018). *人工智能导论*。Springer。
- en: 'Skansi, S. (2018). *Introduction to deep learning: From logical calculus to
    artificial* *intelligence*. Springer.'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skansi, S. (2018). *深度学习导论：从逻辑演算到人工智能*。Springer。
- en: Ian Goodfellow, Yoshua Bengio, and Aaron Courville. (2016). *Deep Learning*.
    Available at [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/).
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ian Goodfellow, Yoshua Bengio, 和 Aaron Courville. (2016). *深度学习*。可在 [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)
    查阅。
- en: Chollet, F. (2017). *Deep Learning with Python*. Manning Publications.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet, F. (2017). *使用Python进行深度学习*。Manning出版公司。
- en: 'Müller, A. C. and Guido, S. (2016). *Introduction to Machine Learning with
    Python: A guide for data scientists*. O’Reilly Media.'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Müller, A. C. 和 Guido, S. (2016). *使用Python进行机器学习导论：数据科学家指南*。O’Reilly Media出版社。
- en: VanderPlas, J. (n.d.). *What Is Machine Learning?* Pythonic Perambulations.
    Available at [https://jakevdp.github.io/PythonDataScienceHandbook/05.01-what-is-machine-learning.xhtml](https://jakevdp.github.io/PythonDataScienceHandbook/05.01-what-is-machine-learning.xhtml).
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: VanderPlas, J. （无日期）。*什么是机器学习？* Pythonic Perambulations。可在 [https://jakevdp.github.io/PythonDataScienceHandbook/05.01-what-is-machine-learning.xhtml](https://jakevdp.github.io/PythonDataScienceHandbook/05.01-what-is-machine-learning.xhtml)
    查阅。
- en: '*What is Deep* *Learning?*: [https://machinelearningmastery.com/what-is-deep-learning/](https://machinelearningmastery.com/what-is-deep-learning/).'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*什么是深度学习？*：[https://machinelearningmastery.com/what-is-deep-learning/](https://machinelearningmastery.com/what-is-deep-learning/)。'
- en: 'Mining addresses from websites: Furneaux, Nick. *Investigating Cryptocurrencies,*
    [*Chapter 9*](B19446_09.xhtml#_idTextAnchor269). Understanding, Extracting, and
    Analyzing Blockchain Evidence, Wiley, 2018\. Page 125.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网站挖掘地址：Furneaux, Nick. *研究加密货币，* [*第9章*](B19446_09.xhtml#_idTextAnchor269)。理解、提取和分析区块链证据，Wiley，2018年。第125页。
- en: 'James, G., Witten, D., Hastie, T., and Tibshirani, R. (2022). *An Introduction
    to Statistical Learning: With Applications*. In R. Springer.'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: James, G., Witten, D., Hastie, T., 和 Tibshirani, R. (2022). *统计学习导论：应用篇*。R.
    Springer出版社。
- en: 'Gymnasium documentation: [https://gymnasium.farama.org/](https://gymnasium.farama.org/).'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gymnasium文档：[https://gymnasium.farama.org/](https://gymnasium.farama.org/)。
- en: '*Introduction – Spinning up documentation*. (n.d.). Welcome to Spinning Up
    in Deep RL! Spinning Up documentation. Available at [https://spinningup.openai.com/en/latest/user/introduction.xhtml#what-this-is](https://spinningup.openai.com/en/latest/user/introduction.xhtml#what-this-is).'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Introduction – Spinning up文档*。（无日期）。欢迎来到深度强化学习中的Spinning Up！Spinning Up文档。可在
    [https://spinningup.openai.com/en/latest/user/introduction.xhtml#what-this-is](https://spinningup.openai.com/en/latest/user/introduction.xhtml#what-this-is)
    查阅。'
- en: '*Nansen Wallet Labels and Emojis: What Do They Mean?* (2023, March 14). Nansen
    – Crypto, DeFi and NFT Analytics. Available at [https://www.nansen.ai/guides/wallet-labels-emojis-what-do-they-mean#alpha-labels](https://www.nansen.ai/guides/wallet-labels-emojis-what-do-they-mean#alpha-labels).'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Nansen钱包标签与表情符号：它们是什么意思？* (2023年3月14日)。Nansen – 加密货币、DeFi 和 NFT 分析。可在 [https://www.nansen.ai/guides/wallet-labels-emojis-what-do-they-mean#alpha-labels](https://www.nansen.ai/guides/wallet-labels-emojis-what-do-they-mean#alpha-labels)
    查阅。'
- en: 'Pipelines:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流水线：
- en: EliteDataScience. (2022, July 8). *WTF is the Bias-Variance Tradeoff?* (Infographic).
    Available at [https://elitedatascience.com/bias-variance-tradeoff](https://elitedatascience.com/bias-variance-tradeoff).
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: EliteDataScience. (2022年7月8日). *什么是偏差-方差权衡？*（信息图表）。可在 [https://elitedatascience.com/bias-variance-tradeoff](https://elitedatascience.com/bias-variance-tradeoff)
    查阅。
- en: '*Sklearn.ensemble.RandomForestClassifier*. (n.d.). scikit-learn. Retrieved
    March 14, 2023, from [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml).'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sklearn.ensemble.RandomForestClassifier*。（无日期）。scikit-learn。检索于2023年3月14日，网址
    [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml)。'
- en: '*SMOTE oversampling*. (n.d.). Machine Learning Mastery. Available at [https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SMOTE过采样*。（无日期）。机器学习精通。可在 [https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)
    查阅。'
- en: Nyuytiymbiy, K. (2022, March 28). *Parameters and Hyperparameters in Machine
    Learning and Deep Learning*. Medium. Available at [https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac](https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac).
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nyuytiymbiy, K. (2022年3月28日). *机器学习与深度学习中的参数与超参数*。Medium。可在 [https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac](https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac)
    查阅。
- en: 'Heatmap on `Chapter07/ML_warmup`: T, D. (2019, July 25). *Confusion matrix
    visualization*. Medium. Available at [https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea](mailto:https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea).'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Chapter07/ML_warmup`中的热力图：T, D. (2019年7月25日). *混淆矩阵可视化*。Medium。可访问 [https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea](mailto:https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea)。'
- en: 'Tutorial to use a Kaggle dataset on Colaboratory. Useful to follow along with
    `Chapter07/ML_warmup`: Gupta, K. (2022, August 24). *How to Load Kaggle Datasets
    into Google Colab?* Analytics Vidhya. Available at [https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/](https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/).'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Colaboratory中使用Kaggle数据集的教程。跟随`Chapter07/ML_warmup`的内容非常有用：Gupta, K. (2022年8月24日).
    *如何将Kaggle数据集加载到Google Colab中？* Analytics Vidhya。可访问 [https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/](https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/)。
- en: Pramoditha, R. (2022, January 26). *How to Choose the Right Activation Function
    for Neural Networks*. Medium. Available at [https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c](https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c).
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pramoditha, R. (2022年1月26日). *如何选择神经网络的激活函数*。Medium。可访问 [https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c](https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c)。
- en: Gupta, A. (2022, May 24). *A Comprehensive Guide on Deep Learning Optimizers*.
    Analytics Vidhya. Available at [https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/](https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/).
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta, A. (2022年5月24日). *深度学习优化器的全面指南*。Analytics Vidhya。可访问 [https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/](https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/)。
- en: '*PEP 20 – The Zen of Python*. (2022, March 15). PEP 0 – Index of Python Enhancement
    Proposals (PEPs) | peps.python.org. Available at [https://peps.python.org/pep-0020/](https://peps.python.org/pep-0020/)'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*PEP 20 – Python之禅*。(2022年3月15日). PEP 0 – Python增强提案（PEPs）索引 | peps.python.org。可访问
    [https://peps.python.org/pep-0020/](https://peps.python.org/pep-0020/)。'
- en: 'Keras Team. (2020, April 17). *Keras documentation: Imbalanced classification:
    credit card fraud detection*. Keras: Deep Learning for Humans. Available at [https://keras.io/examples/structured_data/imbalanced_classification/](https://keras.io/examples/structured_data/imbalanced_classification/).'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras团队。 (2020年4月17日). *Keras文档：不平衡分类：信用卡欺诈检测*。Keras：深度学习工具。可访问 [https://keras.io/examples/structured_data/imbalanced_classification/](https://keras.io/examples/structured_data/imbalanced_classification/)。
- en: Ramchandani, P. (2021, April 10). *Random Forests and the Bias-Variance Tradeoff*.
    Medium. Available at [https://towardsdatascience.com/random-forests-and-the-bias-variance-tradeoff-3b77fee339b4](https://towardsdatascience.com/random-forests-and-the-bias-variance-tradeoff-3b77fee339b4).
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramchandani, P. (2021年4月10日). *随机森林与偏差-方差权衡*。Medium。可访问 [https://towardsdatascience.com/random-forests-and-the-bias-variance-tradeoff-3b77fee339b4](https://towardsdatascience.com/random-forests-and-the-bias-variance-tradeoff-3b77fee339b4)。
- en: 'Tuning with Bayesian optimization: Rendyk. (2023, August 17). *Tuning the Hyperparameters
    and layers of neural network deep learning*. Analytics Vidhya. Available at [https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/](https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/).'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用贝叶斯优化调优：Rendyk. (2023年8月17日). *调整神经网络深度学习的超参数和层次结构*。Analytics Vidhya。可访问 [https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/](https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/)。
- en: '*How to Grid Search Hyperparameters for Deep Learning Models in Python with
    Keras*. (2022, August). Machine Learning Mastery. Available at [https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/).'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何在Python中使用Keras进行深度学习模型的网格搜索超参数*。(2022年8月)。机器学习精通。可访问 [https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)。'
