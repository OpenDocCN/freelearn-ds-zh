- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Introducing Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍自然语言处理
- en: 'Why in the world would a **network analysis** book start with **Natural Language
    Processing** (**NLP**)?! I expect you to be asking yourself that question, and
    it’s a very good question. Here is why: we humans use language and text to describe
    the world around us. We write about the people we know, the things we do, the
    places we visit, and so on. Text can be used to reveal relationships that exist.
    The relationship between things can be shown via network visualization. It can
    be studied with network analysis.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么一本**网络分析**书籍会从**自然语言处理**（**NLP**）开始呢？我猜你一定会问这个问题，这个问题问得非常好。原因如下：我们人类用语言和文本来描述周围的世界。我们写关于我们认识的人、我们做的事情、我们去的地方等等。文本可以用来揭示存在的关系。事物之间的关系可以通过网络可视化来展示。我们可以通过网络分析来研究这些关系。
- en: In short, text can be used to extract interesting relationships, and networks
    can be used to study those relationships much further. We will use text and NLP
    to identify relationships and network analysis and visualization to learn more.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，文本可以用来提取有趣的关系，网络可以用来进一步研究这些关系。我们将使用文本和NLP来识别关系，使用网络分析和可视化来深入了解。
- en: NLP is very useful for creating network data, and we can use that network data
    to learn network analysis. This book is an opportunity to learn a bit about NLP
    and network analysis, and how they can be used together.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: NLP对于创建网络数据非常有用，我们可以利用这些网络数据来学习网络分析。本书是一个学习NLP和网络分析的机会，了解它们如何一起使用。
- en: 'In explaining NLP at a very high level, we will be discussing the following
    topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在以非常高的层次解释NLP时，我们将讨论以下主题：
- en: What is NLP?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是NLP？
- en: Why NLP in a network analysis book?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么在一本网络分析书中讨论NLP？
- en: A very brief history of NLP
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP的简短历史
- en: How has NLP helped me?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP是如何帮助我的？
- en: Common uses for NLP
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP的常见用途
- en: Advanced uses of NLP
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP的高级应用
- en: How can a beginner get started with NLP?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初学者如何开始学习NLP？
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Although there are a few places in this chapter where I show some code, I do
    not expect you to write the code yet. These examples are only for a demonstration
    to give a preview of what can be done. The rest of this book will be very hands-on,
    so take a look and read to understand what I am doing. Don’t worry about writing
    code yet. First, learn the concepts.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章有几个地方展示了代码示例，但我并不期望你现在就写代码。这些示例仅用于展示，给你一个预览，看看能做什么。本书的其余部分将非常实践，所以先看一看并理解我在做什么。现在不用担心写代码。首先，学习概念。
- en: What is NLP?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是NLP？
- en: NLP is a set of techniques that helps computers work with human language. However,
    it can be used for more than dealing with words and sentences. It can also work
    with application log files, source code, or anything else where human text is
    used, and on imaginary languages as well, so long as the text is consistent in
    following a language’s rules. *Natural language* is a language that humans speak
    or write. *Processing* is the act of a computer using data. So, *NLP* is the act
    of a computer using spoken or written human language. It’s that simple.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: NLP是一组帮助计算机处理人类语言的技术。然而，它不仅仅可以用于处理单词和句子。它还可以处理应用日志文件、源代码或任何其他使用人类文本的地方，甚至是虚构语言，只要文本遵循语言的规则。*自然语言*是人类说或写的语言。*处理*是计算机使用数据的行为。所以，*NLP*就是计算机使用口语或书面的人类语言。这么简单。
- en: Many of us software developers have been doing NLP for years, maybe even without
    realizing it. I will give my own example. I started my career as a web developer.
    I was entirely self-educated in web development. Early in my career, I built a
    website that became very popular and had a nice community, so I took inspiration
    from Yahoo Chats (popular at the time), reverse-engineered it, and built my own
    internet message board. It grew rapidly, providing years of entertainment and
    making me some close friends. However, with any good social application, trolls,
    bots, and generally nasty people eventually became a problem, so I needed a way
    to flag and quarantine abusive content automatically.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为软件开发人员中的许多人，可能多年来我们都在做NLP，甚至没有意识到。我就拿我自己的例子来说。我从事网页开发时完全是自学的。在我职业生涯的初期，我建立了一个非常受欢迎的网站，拥有一个不错的社区，于是我从当时流行的Yahoo
    Chats中获得灵感，逆向工程它，并建立了我自己的互联网留言板。它迅速成长，提供了多年的娱乐，并让我结交了几个亲密的朋友。然而，像所有优秀的社交应用一样，恶搞者、机器人以及各种恶劣的用户最终成为了问题，所以我需要一种方式来自动标记和隔离恶意内容。
- en: Back then, I created lists of examples of abusive words and strings that could
    help catch abuse. I was not interested in stopping all obscenities, as I do not
    believe in completely controlling how people post text online; however, I was
    looking to identify toxic behavior, violence, and other nasty things. Anyone with
    a comment section on their website is very likely doing something similar in order
    to moderate their website, or they should be. The point is that I have been doing
    NLP since the beginning of my career without even noticing, but it was rule-based.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当时，我创建了包含侮辱性词汇和字符串的列表，这些词汇可以帮助识别虐待行为。我并不想停止所有的脏话，因为我不相信完全控制人们在线发布的文本；然而，我希望识别有毒行为、暴力和其他不良行为。任何拥有评论区的网站都很可能做类似的事情来管理网站，或者应该这样做。关键是，我从职业生涯开始就一直在做
    NLP，而我甚至没有意识到，但那时还是基于规则的。
- en: These days, machine learning dominates the NLP landscape, as we are able to
    train models to detect abuse, violence, or pretty much anything we can imagine,
    which is one thing that I love the most about NLP. I feel that I am limited only
    by the extent of my own creativity. As such, I have created classifiers to detect
    discussions that contained or were about extreme political sentiment, violence,
    music, art, data science, natural sciences, and disinformation, and at any given
    moment, I typically have several NLP models in mind that I want to build but haven’t
    found time. I have even used NLP to detect malware. But, again, NLP doesn’t have
    to be against written or spoken words, as my malware classifier has shown. If
    you keep that in mind, then your potential uses for NLP massively expand. My rule
    of thumb *is that if there are sequences in data that can be extracted as words
    – even if they are not words – they can potentially be used with* *NLP techniques*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，机器学习主导了 NLP 领域，因为我们能够训练模型来检测虐待、暴力，或几乎任何我们能想象的东西，这也是我最喜欢 NLP 的一个原因。我感觉自己的创造力才是唯一的限制。因此，我已经创建了分类器来检测包含极端政治情绪、暴力、音乐、艺术、数据科学、自然科学和虚假信息的讨论，在任何时刻，我通常都有几个
    NLP 模型在脑海中，想要构建却还没找到时间。我甚至使用 NLP 来检测恶意软件。但是，NLP 不仅仅是针对书面或口语的，如我的恶意软件分类器所示。如果你记住这一点，你会发现
    NLP 的潜在用途会大大扩展。我的经验法则是*如果数据中存在可以提取为词语的序列——即使它们本身不是词语——也可以使用 NLP 技术*。
- en: In the past, and probably still now, analysts would drop columns containing
    text or do very basic transformations or computations, such as one-hot encoding,
    counts, or determining the presence/absence (true/false). However, there is so
    much more that you can do, and I hope this chapter and book will ignite some inspiration
    and curiosity in you from reading this.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，可能现在也一样，分析师会丢弃包含文本的列，或者进行一些非常基础的转换或计算，例如一热编码、计数，或确定某物是否存在（真/假）。然而，你可以做得更多，我希望这一章和这本书能激发你们的灵感和好奇心，带给你们启发。
- en: Why NLP in a network analysis book?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么在一本网络分析书中讲 NLP？
- en: 'Most of you probably bought this book in order to learn applied social network
    analysis using Python. So, why am I explaining NLP? Here’s why: if you know your
    way around NLP and are comfortable extracting data from text, that can be extremely
    powerful for creating network data and investigating the relationship between
    things that are mentioned in text. Here is an example from the book *Alice’s Adventures
    in Wonderland* by Lewis Carroll, my favorite book.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你们中的大多数人可能是为了学习如何使用 Python 进行应用社会网络分析才购买了这本书。那么，为什么我要讲解 NLP 呢？原因是：如果你熟悉 NLP
    并且能够从文本中提取数据，那么这对于创建网络数据并调查文本中提到的事物之间的关系来说，可以非常强大。下面是我最喜欢的书《爱丽丝梦游仙境》中的一个例子，作者是路易斯·卡罗尔。
- en: “Once upon a time there were three little sisters” the Dormouse began in a great
    hurry; “and their names were Elsie, Lacie, and Tillie; and they lived at the bottom
    of a well.”
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: “从前有三个小妹妹，”睡鼠急匆匆地开始讲，“她们的名字叫 Elsie、Lacie 和 Tillie，她们住在一个井底。”
- en: 'What can we observe from these words? What characters or places are mentioned?
    We can see that the Dormouse is telling a story about three sisters named *Elsie*,
    *Lacie*, and *Tillie* and that they lived at the bottom of a well. If you allow
    yourself to think in terms of relationships, you will see that these relationships
    exist:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些词语中我们能观察到什么？提到了哪些角色或地方？我们可以看到，睡鼠正在讲述一个关于三姐妹的故事，她们的名字分别是*Elsie*、*Lacie* 和
    *Tillie*，并且她们住在一个井底。如果你能以关系的角度来思考，你会发现这些关系是存在的：
- en: Three sisters -> Dormouse (he either knows them or knows a story about them)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三姐妹 -> 睡鼠（他要么认识她们，要么知道一个关于她们的故事）
- en: Dormouse -> Elsie
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dormouse -> Elsie
- en: Dormouse -> Lacie
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dormouse -> Lacie
- en: Dormouse -> Tillie
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dormouse -> Tillie
- en: Elsie -> bottom of a well
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsie -> 井底
- en: Lacie -> bottom of a well
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lacie -> 井底
- en: Tillie -> bottom of a well
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tillie -> 井底
- en: 'It’s also very likely that the three sisters all know each other, so additional
    relationships emerge:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 也很可能这三姐妹彼此都认识，因此出现了额外的关系：
- en: Elsie -> Lacie
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsie -> Lacie
- en: Elsie -> Tillie
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsie -> Tillie
- en: Lacie -> Elsie
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lacie -> Elsie
- en: Lacie -> Tillie
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lacie -> Tillie
- en: Tillie -> Elsie
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tillie -> Elsie
- en: Tillie -> Lacie
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tillie -> Lacie
- en: Our minds build these relationship maps so effectively that we don’t even realize
    that we are doing it. The moment I read that the three were sisters, I drew a
    mental image that the three knew each other.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑如此高效地构建这些关系图，以至于我们甚至没有意识到自己在这么做。当我读到这三人是姐妹时，我脑海中立刻浮现出这三人彼此认识的画面。
- en: 'Let’s try another example from a current news story: *Ocasio-Cortez doubles
    down on Manchin criticism* (CNN, June 2021: [https://edition.cnn.com/videos/politics/2021/06/13/alexandria-ocasio-cortez-joe-manchin-criticism-sot-sotu-vpx.cnn](https://edition.cnn.com/videos/politics/2021/06/13/alexandria-ocasio-cortez-joe-manchin-criticism-sot-sotu-vpx.cnn)).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一个来自当前新闻故事的例子：*奥卡西奥-科尔特斯加大对曼钦批评力度*（CNN，2021年6月：[https://edition.cnn.com/videos/politics/2021/06/13/alexandria-ocasio-cortez-joe-manchin-criticism-sot-sotu-vpx.cnn](https://edition.cnn.com/videos/politics/2021/06/13/alexandria-ocasio-cortez-joe-manchin-criticism-sot-sotu-vpx.cnn)）。
- en: Rep. Alexandria Ocasio-Cortez (D-NY) says that Sen. Joe Manchin (D-WV) not supporting
    a house voting rights bill is being influenced by the legislation’s sweeping reforms
    to limit the role of lobbyists and the influence of “dark money” political donations.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 代表亚历山大·奥卡西奥-科尔特斯（纽约州D）表示，乔·曼钦参议员（西弗吉尼亚州D）不支持一项房屋投票权法案，受到了该法案广泛改革的影响，旨在限制游说者的角色和“黑暗资金”政治捐赠的影响。
- en: Who is mentioned, and what is their relationship? What can we learn from this
    short text?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 谁被提到，他们之间是什么关系？我们从这段简短的文字中能学到什么？
- en: Rep. Alexandria Ocasio-Cortez is talking about Sen. Joe Manchin
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代表亚历山大·奥卡西奥-科尔特斯在谈论参议员乔·曼钦
- en: Both are Democrats
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两者都是民主党成员
- en: Sen. Joe Manchin does not support a house voting rights bill
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参议员乔·曼钦不支持一项房屋投票权法案
- en: Rep. Alexandria Ocasio-Cortez claims that Sen. Joe Manchin is being influenced
    by the legislation’s reforms
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代表亚历山大·奥卡西奥-科尔特斯声称参议员乔·曼钦受到该法案改革的影响
- en: Rep. Alexandria Ocasio-Cortez claims that Sen. Joe Manchin is being influenced
    by “dark money” political donations
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代表亚历山大·奥卡西奥-科尔特斯声称，参议员乔·曼钦受到了“黑暗资金”政治捐赠的影响
- en: There may be a relationship between Sen. Joe Manchin and “dark money” political
    donors
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能存在参议员乔·曼钦与“黑暗资金”政治捐赠者之间的关系
- en: We can see that even a small amount of text has a lot of information embedded.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，即使是少量的文本，也蕴含了大量的信息。
- en: 'If you are stuck trying to figure out relationships when dealing with text,
    I learned in college creative writing classes to consider the “*W*” questions
    (and *How*) in order to explain things in a story:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在处理文本时无法确定关系，我在大学创意写作课上学到，要考虑“*W*”问题（以及*如何*），以便在故事中解释事物：
- en: 'Who: Who is involved? Who is telling the story?'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁：谁参与其中？谁在讲述故事？
- en: 'What: What is being talked about? What is happening?'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么：在谈论什么？发生了什么？
- en: 'When: When does this take place? What time of the day is it?'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么时候：这发生在什么时候？是哪一天的什么时间？
- en: 'Where: Where is this taking place? What location is being described?'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪里：发生在何处？描述的是哪个地点？
- en: 'Why: Why is this important?'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么：这为什么重要？
- en: 'How: How is the thing being done?'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何：这件事是怎么做的？
- en: If you ask these questions, you will notice relationships between things and
    other things, which is foundational for building and analyzing networks. If you
    can do this, you can identify relationships in text. If you can identify relationships
    in text, you can use that knowledge to build social networks. If you can build
    social networks, you can analyze relationships, detect importance, detect weaknesses,
    and use this knowledge to gain a really profound understanding of whatever it
    is that you are analyzing. You can also use this knowledge to attack dark networks
    (crime, terrorism, and so on) or protect people, places, and infrastructure. This
    isn’t just insights. *These are actionable insights*—the best kind.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你提出这些问题，你会注意到事物之间的关系，这对构建和分析网络是基础。如果你能做到这一点，你就能在文本中识别关系。如果你能识别文本中的关系，你就能利用这些知识构建社交网络。如果你能构建社交网络，你就能分析关系，检测重要性，发现弱点，并利用这些知识深入理解你所分析的任何事物。你还可以利用这些知识攻击黑暗网络（犯罪、恐怖主义等）或保护人、地方和基础设施。这不仅仅是洞察力，*这些是可操作的洞察力*——最好的那种。
- en: That is the point of this book. Marrying NLP with social network analysis and
    data science is extremely powerful for acquiring a new perspective. If you can
    scrape or get the data you need, you can really gain deep knowledge of how things
    relate and why.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本书的要点。将NLP与社交网络分析和数据科学相结合，对于获得新的视角来说极为强大。如果你能抓取或获得所需的数据，你将真正深入了解事物之间的关系及其原因。
- en: That is why this chapter aims to explain very simply what NLP is, how to use
    it, and what it can be used for. But before that, let’s get into the history for
    a bit, as that is often left out of NLP books.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么本章旨在非常简单地解释什么是NLP，如何使用它，以及它能做什么。但在此之前，让我们稍微了解一下NLP的历史，因为这通常被NLP书籍所忽略。
- en: A very brief history of NLP
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一段简短的NLP历史
- en: If you research the history of NLP, you will not find one conclusive answer
    as to its origins. As I was planning the outline for this chapter, I realized
    that I knew quite a bit about the uses and implementation of NLP but that I had
    a blind spot regarding its history and origins. I knew that it was tied to computational
    linguistics, but I did not know the history of that field, either. The earliest
    conceptualization of **Machine Translation** (**MT**) supposedly took place in
    the seventeenth century; however, I am deeply skeptical that this was the origin
    of the idea of MT or NLP, as I bet people have been puzzling over the relationships
    between words and characters for as long as language has existed. I would assume
    that to be unavoidable, as people thousands of years ago were not simpletons.
    They were every bit as clever and inquisitive as we are, if not more. However,
    let me give some interesting information I have dug up on the origins of NLP.
    Please understand that this is not the complete history. An entire book could
    be written about the origins and history of NLP. So that I quickly move on, I
    am going to keep this brief. I am going to just list some of the highlights that
    I found. If you want to know more, this is a rich topic for research.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你研究NLP的历史，你不会找到一个关于其起源的确凿答案。在我为本章制定大纲时，我意识到自己对NLP的应用和实现了解颇多，但对其历史和起源却有盲点。我知道它与计算语言学息息相关，但我也不了解计算语言学的历史。关于**机器翻译**（**MT**）的最早概念据说出现在十七世纪；然而，我对这个说法持深深怀疑态度，因为我敢打赌，人类在语言诞生的同时，就已经在为词汇和字符之间的关系困惑不解。我认为这是不可避免的，因为几千年前的人并非愚笨。他们和我们一样聪明、好奇，甚至可能更为聪明。然而，让我给出一些我挖掘到的关于NLP起源的有趣信息。请理解，这并不是完整的历史。关于NLP的起源和历史可以写成一本书。所以，为了快速推进，我将简要地列出我发现的一些亮点。如果你想了解更多，这个话题是一个值得深入研究的领域。
- en: 'One thing that puzzles me is that I rarely see cryptology (cryptography and
    cryptanalysis) mentioned as being part of the origins of NLP or even MT when cryptography
    is the act of translating a message into gibberish, and cryptanalysis is the act
    of reversing secret gibberish into a useful message. So, to me, any automation,
    even hundreds or thousands of years ago, that could assist in carrying out cryptography
    or cryptanalysis should be part of the conversation. It might not be MT in the
    same way that modern translation is, but it is a form of translation, nonetheless.
    So, I would suggest that MT goes back even to the Caesar cipher invented by Julius
    Caesar, and probably much earlier than that. The Caesar cipher translated a message
    into code by shifting the text by a certain number. As an example, let’s take
    the sentence:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有件事让我困惑，那就是我很少看到密码学（密码学和密码分析）被提到作为NLP或甚至MT的起源之一，然而密码学本身就是将信息转换为乱码，而密码分析则是将密文恢复为有用的信息。因此，对我来说，任何能够辅助进行密码学或密码分析的自动化技术，哪怕是几百年前或几千年前的，也应该是讨论的一部分。虽然它可能不像现代的翻译那样是机器翻译（MT），但它仍然是一种翻译的形式。所以，我认为机器翻译甚至可以追溯到由尤利乌斯·凯撒发明的凯撒密码，甚至更早。凯撒密码通过将文本按一定的数字偏移来将信息转化为代码。举个例子，我们来看这个句子：
- en: '*I really* *love NLP.*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*我真的很* *喜欢NLP。*'
- en: 'First, we should probably remove the spaces and casing so that any eavesdropper
    can’t get hints on word boundaries. The string is now as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该去掉空格和大小写，以免任何窃听者能够猜测出单词的边界。现在字符串如下：
- en: '*ireallylovenlp*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*ireallylovenlp*'
- en: 'If we do a `shift-1`, we shift each letter by one character to the right, so
    we get:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们进行`shift-1`，每个字母都向右偏移一个字符，那么我们就得到：
- en: '*jsfbmmzmpwfomq*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*jsfbmmzmpwfomq*'
- en: The number that we shift is arbitrary. We could also use a reverse shift. Wooden
    sticks were used for converting text into code, so I would consider that as a
    translation tool.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们移动的数字是任意的。我们也可以使用反向移动。木棍曾被用来将文本转换为密码，因此我认为它也可以作为一种翻译工具。
- en: After the Caesar cipher, many, many other techniques were invented for encrypting
    human text, some of which were quite sophisticated. There is an outstanding book
    called *The Code Book* by Simon Singh that goes into the several thousand-year-long
    history of cryptology. With that said, let’s move on to what people typically
    think of with regard to NLP and MT.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在凯撒密码之后，还发明了许多其他复杂的加密技术。一部名为*《密码书》*的杰出作品，由西蒙·辛格（Simon Singh）撰写，深入探讨了几千年来的密码学历史。话虽如此，我们接着讨论人们通常认为与自然语言处理（NLP）和机器翻译（MT）相关的内容。
- en: In the seventeenth century, philosophers began to submit proposals for codes
    that could be used to relate words between languages. This was all theoretical,
    and none of them were used in the development of an actual machine, but ideas
    such as MT came about first by considering future possibilities, and then implementation
    was considered. A few hundred years later, in the early 1900s, Ferdinand de Saussure,
    a Swiss linguistics professor, developed an approach for describing language as
    a system. He passed away in the early 1900s and almost deprived the world of the
    concept of *language as a science*, but realizing the importance of his ideas,
    two of his colleagues wrote the *Cours de linguistique generale* in 1916\. This
    book laid the foundation for the structuralist approach that started with linguistics
    but eventually expanded to other fields, including computers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在十七世纪，哲学家们开始提交关于如何在语言之间建立联系的编码提案。这些提案完全是理论性的，且没有被用于实际机器的开发，但像机器翻译这样的思想最初是通过考虑未来的可能性提出的，随后才考虑其实现。几百年后，在20世纪初，瑞士语言学教授费迪南·德·索绪尔（Ferdinand
    de Saussure）提出了一种将语言作为一个系统来描述的方法。他在20世纪初去世，几乎让“*语言作为科学*”的概念失传，但意识到他的思想重要性后，他的两位同事于1916年撰写了《普通语言学教程》（*Cours
    de linguistique generale*）。这本书为结构主义方法奠定了基础，该方法起初应用于语言学，后来扩展到了包括计算机在内的其他领域。
- en: Finally, in the 1930s, the first patents for MT were applied for.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在1930年代，第一批机器翻译的专利开始申请。
- en: Later, World War II began, and this is what caused me to consider the Caesar
    cipher and cryptology as early forms of MT. During World War II, Germany used
    a machine called the Enigma machine to encrypt German messages. The sophistication
    of the technique made the codes nearly unbreakable, with devastating effects.
    In 1939, along with other British cryptanalysts, Alan Turing designed the bombe
    after the Polish bomba that had been decrypting Enigma messages the seven years
    prior. Eventually, the bombe was able to reverse German codes, taking away the
    advantage of secrecy that German U-boats were enjoying and saving many lives.
    This is a fascinating story in itself, and I encourage readers to learn more about
    the effort to decrypt messages that were encrypted by the Enigma machines.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，第二次世界大战爆发，这让我开始考虑凯撒密码和密码学作为早期的机器翻译形式。在二战期间，德国使用一种名为恩尼格玛的机器来加密德语信息。该技术的复杂性使得这些密码几乎无法破解，造成了极其严重的后果。1939年，艾伦·图灵与其他英国密码分析师一道，设计了“炸弹机”（bombe），该机灵感来源于波兰的bomba，后者在七年前曾用于破解恩尼格玛信息。最终，炸弹机能够破解德国语言密码，剥夺了德国潜艇利用密码保护的秘密优势，拯救了许多生命。这个故事本身非常引人入胜，我鼓励读者了解更多关于破解恩尼格玛密码的努力。
- en: After the war, research into MT and NLP really took off. In 1950, Alan Turing
    published *Computing Machinery and Intelligence*, which proposed the Turing Test
    as a way of assessing intelligence. To this day, the Turing Test is frequently
    mentioned as a criterion of intelligence for **Artificial Intelligence** (**AI**)
    to be judged by.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 战后，机器翻译（MT）和自然语言处理（NLP）的研究真正起飞。1950年，艾伦·图灵发布了*《计算机与智能》*，提出了图灵测试作为评估智能的方式。至今，图灵测试常被提及作为衡量**人工智能**（**AI**）智能水平的标准。
- en: In 1954, the Georgetown experiment fully automated translations of more than
    sixty Russian sentences into English. In 1957, Noam Chomsky’s *Syntactic Structures*
    revolutionized linguistics with a rule-based system of syntactic structures known
    as **Universal** **Grammar** (**UG**).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 1954年，乔治敦实验将超过60个俄语句子自动翻译成英语。1957年，诺姆·乔姆斯基的*《句法结构》*通过基于规则的句法结构系统革新了语言学，被称为**普遍语法**（**UG**）。
- en: To evaluate the progress of MT and NLP research, the US **National Research
    Council** (**NRC**) created the **Automatic Language Processing Advisory Committee**
    (**ALPAC**) in 1964\. At the same time, at MIT, Joseph Weizenbaum had created
    *ELIZA*, the world’s first chatbot. Based on reflection techniques and simple
    grammar rules, ELIZA was able to rephrase any sentence into another sentence as
    a response to users.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估机器翻译（MT）和NLP研究的进展，美国**国家研究委员会**（**NRC**）在1964年创建了**自动语言处理咨询委员会**（**ALPAC**）。与此同时，在麻省理工学院，Joseph
    Weizenbaum创造了世界上第一个聊天机器人*ELIZA*。基于反射技巧和简单的语法规则，ELIZA能够将任何句子重述为另一句作为对用户的回应。
- en: Then winter struck. In 1966, due to a report by ALPAC, an NLP stoppage occurred,
    and funding for NLP and MT was discontinued. As a result, AI and NLP research
    were seen as a dead end by many people, but not all. This freeze lasted until
    the late 1980s, when a new revolution in NLP would begin, driven by a steady increase
    in computational power and the shift to **Machine Learning** (**ML**) algorithms
    rather than hard-coded rules.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后冬天来临了。1966年，由于ALPAC报告的影响，NLP研究遭遇了停滞，NLP和机器翻译（MT）的资金被撤销。因此，AI和NLP研究在许多人眼中变成了死胡同，但并非所有人都这么认为。这一停滞期持续到1980年代末期，当时一场新的NLP革命开始了，推动力来自计算能力的稳步增长和转向**机器学习**（**ML**）算法，而非硬编码规则。
- en: In the 1990s, the popularity of statistical models for NLP arose. Then, in 1997,
    **Long Short-Term Memory** (**LSTM**) and **Recurrent Neural Network** (**RNN**)
    models were introduced, and they found their niche for voice and text processing
    in 2007\. In 2001, Yoshua Bengio and his team provided the first feed-forward
    neural language model. In 2011, Apple’s Siri became known as one of the world’s
    first successful AI and NLP assistants to be used by general consumers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 1990年代，统计模型在NLP领域的流行开始崛起。随后，在1997年，**长短期记忆网络**（**LSTM**）和**递归神经网络**（**RNN**）模型被引入，它们在2007年找到了用于语音和文本处理的应用场景。2001年，Yoshua
    Bengio及其团队提出了第一个前馈神经语言模型。2011年，苹果的Siri成为全球第一个被普通消费者广泛使用的成功AI和NLP助手之一。
- en: Since 2011, NLP research and development has exploded, so this is as far as
    I will go into history. I am positive that there are many gaps in the history
    of NLP and MT, so I encourage you to do your own research and really dig into
    the parts that fascinate you. I have spent much of my career working in cyber
    security, so I am fascinated by almost anything having to do with the history
    of cryptology, especially old techniques for cryptography.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 自2011年以来，NLP的研究与发展爆炸性增长，所以我在这里讲到的历史仅到此为止。我相信NLP和MT的历史中还有很多空白，因此我鼓励你自己做一些研究，深入挖掘那些令你着迷的部分。我大部分职业生涯都在从事网络安全工作，所以我对几乎任何与密码学历史相关的事物都感兴趣，特别是古老的密码学技术。
- en: How has NLP helped me?
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP如何帮助了我？
- en: I want to do more than show you how to do something. I want to show you how
    it can help you. The easiest way for me to explain how this may be useful to you
    is to explain how it has been useful to me. There are a few things that were really
    appealing to me about NLP.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我想做的不仅仅是教你如何做某件事，我还想展示它如何帮助你。最简单的解释这种方法如何对你有用的方式，就是解释它如何对我有用。有几件事让我对自然语言处理（NLP）感到非常吸引。
- en: Simple text analysis
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单文本分析
- en: I am really into reading and grew up loving literature, so when I first learned
    that NLP techniques could be used for text analysis, I was immediately intrigued.
    Even something as simple as counting the number of times a specific word is mentioned
    in a book can be interesting and spark curiosity. For example, how many times
    is Eve, the first woman mentioned in the Bible, mentioned in the book of Genesis?
    How many times is she mentioned in the entire Bible? How many times is Adam mentioned
    in Genesis? How many times is Adam mentioned in the entire Bible? For this example,
    I’m using the King James Version.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常喜欢阅读，从小就热爱文学，所以当我第一次了解到NLP技术可以用于文本分析时，我立刻被吸引住了。即便是像统计书中某个特定单词出现次数这么简单的事，也能引发兴趣并激发好奇心。例如，夏娃，圣经中的第一位女性，在创世纪中提到过多少次？她在整个圣经中被提到多少次？亚当在创世纪中被提到多少次？亚当在整个圣经中被提到多少次？这个例子中，我使用的是《钦定版圣经》。
- en: 'Let’s compare:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做个对比：
- en: '| **Name** | **Genesis Count** | **Bible Count** |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **创世纪计数** | **圣经计数** |'
- en: '| Eve | 2 | 4 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 夏娃 | 2 | 4 |'
- en: '| Adam | 17 | 29 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 亚当 | 17 | 29 |'
- en: Figure 1.1 – Table of biblical mentions of Adam and Eve
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 亚当与夏娃在圣经中的提及表
- en: These are interesting results. Even if we do not take the Genesis story as literal
    truth, it’s still an interesting story, and we often hear about Adam and Eve.
    Hence, it is easy to assume they would be mentioned as frequently, but Adam is
    actually mentioned over eight times as often as Eve in Genesis and over seven
    times as often in the entire Bible. Part of understanding literature is building
    a mental map of what is happening in the text. To me, it’s a little odd that Eve
    is mentioned so rarely, and it makes me want to investigate the amount of male
    versus female mentions or maybe investigate which books of the Bible have the
    largest number of female characters and then what those stories are about. If
    nothing else, it sparks curiosity, which should lead to deeper analysis and understanding.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果很有趣。即使我们不将《创世纪》故事视为字面上的真相，它仍然是一个有趣的故事，我们常常听到亚当和夏娃的故事。因此，容易假设他们会被同样频繁地提到，但在《创世纪》中，亚当的出现频率是夏娃的八倍多，而在整本圣经中，亚当的出现频率是夏娃的七倍多。理解文学的一部分是建立一个关于文本内容的心智图。对我来说，夏娃出现得如此少有点奇怪，这让我想要调查男性与女性的提及频率，或者研究哪几本圣经书籍中女性角色最多，以及这些故事的内容。如果没有其他的话，这至少激发了我的好奇心，应该会促使我进行更深入的分析和理解。
- en: NLP gives me tools to extract quantifiable data from raw text. It empowers me
    to use that quantifiable data in research that would have been impossible otherwise.
    Imagine how long it would have taken to do this manually, reading every single
    page without missing a detail, to get to these small counts. Now, consider that
    this took me about a second once the code was written. That is powerful, and I
    can use this functionality to research any person in any text.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）为我提供了从原始文本中提取可量化数据的工具。它使我能够在研究中使用这些可量化的数据，而这些研究在没有这些工具的情况下是不可能完成的。试想一下，如果要手动完成这个过程，逐页阅读每一页而不遗漏任何细节，来得到这些小的计数，可能需要多长时间。而现在，考虑到代码编写完成后，这只花费了我大约一秒钟。这是非常强大的，我可以利用这个功能在任何文本中研究任何人。
- en: Community sentiment analysis
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社区情感分析
- en: Second, and related to the point I just made, NLP provides ways to investigate
    themes and sentiments carried by groups of people. During the Covid-19 pandemic,
    a group of people has been vocally anti-mask, spreading fear and misinformation.
    If I capture text from those people, I can use sentiment analysis techniques to
    determine and measure sentiment shared by that group of people across various
    topics. I did exactly that. I was able to scrape thousands of tweets and understand
    what they really felt about various topics such as Covid-19, the flag, the Second
    Amendment, foreigners, science, and many more. I did this exact analysis for one
    of my projects, *#100daysofnlp*, on LinkedIn ([https://www.linkedin.com/feed/hashtag/100daysofnlp/](https://www.linkedin.com/feed/hashtag/100daysofnlp/)),
    and the results were illuminating.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，和我刚才提到的观点相关，NLP提供了调查和分析群体情感与主题的方法。在 Covid-19 大流行期间，一些人群公开表示反对佩戴口罩，传播恐惧和误信息。如果我从这些人群中抓取文本，我可以利用情感分析技术来确定并衡量这些人群在不同话题上的情感共识。我就做了这一点。我能够抓取成千上万条推文，并了解他们对不同话题的真实感受，比如
    Covid-19、国旗、第二修正案、外国人、科学等。我为我的一个项目 *#100daysofnlp* 在 LinkedIn 上做了这个分析 ([https://www.linkedin.com/feed/hashtag/100daysofnlp/](https://www.linkedin.com/feed/hashtag/100daysofnlp/))，结果非常有启发性。
- en: 'NLP allows us to objectively investigate and analyze group sentiment about
    anything, so long as we are able to acquire text or audio. Much of Twitter data
    is posted openly and is consumable by the public. Just one caveat: if you are
    going to scrape, please use your abilities for good, not evil. Use this to understand
    what people are thinking about and what they feel. Use it for research, not surveillance.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）让我们能够客观地调查和分析人群对任何事物的情感，只要我们能够获取文本或音频。许多推特数据是公开发布的，公众可以自由使用。唯一的警告是：如果你打算进行数据抓取，请将你的能力用于正当用途，而非恶意用途。利用这些数据来了解人们在思考什么、感受什么。将其用于研究，而不是监视。
- en: Answer previously unanswerable questions
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解答以前无法回答的问题
- en: Really, what ties these two together is that NLP helps me answer questions that
    were previously unanswerable. In the past, we could have conversations discussing
    what people felt and why or describing literature we had read but only at a surface
    level. With what I am going to show you how to do in this book, you will no longer
    be limited to the surface. You will be able to map out complex relationships that
    supposedly took place thousands of years ago very quickly, and you will be able
    to closely analyze any relationship and even the evolution of relationships. You
    will be able to apply these same techniques to any kind of text, including transcribed
    audio, books, news articles, and social media posts. NLP opens up a universe of
    untapped knowledge.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，将这两者联系在一起的是，NLP帮助我回答那些以前无法回答的问题。在过去，我们可以讨论人们的感受以及为什么这样，或描述我们阅读过的文学作品，但只能停留在表面层次。通过本书中我要向你展示的内容，你将不再局限于表面。你将能够快速绘制出几千年前发生的复杂关系，并能够深入分析任何关系，甚至是关系的演变。你将能够将这些相同的技术应用于任何类型的文本，包括转录音频、书籍、新闻文章和社交媒体帖子。自然语言处理打开了一个未被开发的知识宇宙。
- en: Safety and security
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全与保障
- en: In 2020, the Covid-19 pandemic hit the entire world. When it hit, I was worried
    that many people would lose their jobs and their homes, and I feared that the
    world would spin out of control into total anarchy. It’s gotten bad but we do
    not have armed gangs raiding towns and communities around the world. Tension is
    up, but I wanted a way to keep an eye on violence in my area in real time. So,
    I scraped police tweets from several police accounts in my area, as they report
    all kinds of crime in near real time, including violence. I created a dataset
    of violent versus non-violent tweets, where violent tweets contained words such
    as shooting, stabbing, and other violence-related words. I then trained an ML
    classifier to detect tweets having to do with violence. Using the results of this
    classifier, I can keep an eye on violence in my area. I can keep an eye on anything
    that I want, so long as I can get text but knowing how my area is doing in terms
    of street violence could serve as a warning or give me comfort. Again, replacing
    what was limited to feeling and emotion with quantifiable data is powerful.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年，Covid-19疫情席卷全球。当疫情爆发时，我担心许多人会失去工作和住所，我害怕世界会失控，陷入彻底的无政府状态。虽然情况变得严峻，但我们并没有看到武装帮派在全球各地袭击城镇和社区。紧张局势加剧，但我希望找到一种方法，可以实时监控我所在地区的暴力事件。因此，我从我所在地区的多个警察账户上抓取了推文，因为他们几乎实时报告各种犯罪事件，包括暴力。我创建了一个包含暴力与非暴力推文的数据集，其中暴力推文包含诸如枪击、刺伤等暴力相关的词汇。然后，我训练了一个机器学习分类器，用来检测与暴力相关的推文。通过这个分类器的结果，我可以随时了解我所在地区的暴力情况。我可以关注任何我想关注的内容，只要我能获取文本，但了解我所在地区的街头暴力状况可以为我提供警示或安慰。再次强调，将原本仅限于感觉和情感的内容转化为可量化的数据是非常强大的。
- en: Common uses for NLP
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理的常见用途
- en: One thing that I like the most about NLP is that you are primarily limited by
    your imagination and what you can do with it. If you are a creative person, you
    will be able to come up with many ideas that I have not explained.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我最喜欢自然语言处理（NLP）的一点是，你的主要限制就是你的想象力和你能用它做的事情。如果你是一个富有创造力的人，你将能够提出许多我没有解释的想法。
- en: I will explain some of the common uses of NLP that I have found. Some of this
    may not typically appear in NLP books, but as a lifelong programmer, when I think
    of NLP, I automatically think about any programmatic work with string, with a
    string being a sequence of characters. `ABCDEFG` is a *string*, for instance.
    `A` is a *character*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我将解释一些我发现的自然语言处理常见用途。虽然其中一些可能通常不会出现在NLP书籍中，但作为一个终身程序员，每当我想到NLP时，我自然会想到任何与字符串相关的编程工作，而字符串就是字符的序列。例如，`ABCDEFG`
    是一个*字符串*，`A` 是一个*字符*。
- en: Note
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项
- en: Please don’t bother writing the code for now unless you just want to experiment
    with some of your own data. The code in this chapter is just to show what is possible
    and what the code may look like. We will go much deeper into actual code throughout
    this book.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请现在不要担心编写代码，除非你只是想用一些自己的数据进行实验。本章中的代码仅用于展示可能实现的功能以及代码可能的样子。我们将在本书的后续章节深入探讨实际代码。
- en: True/False – Presence/Absence
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 真/假 – 存在/不存在
- en: This may not fit strictly into NLP, but it is very often a part of any text
    operation, and this also happens in ML used in NLP, where one-hot encoding is
    used. Here, we are looking strictly for the presence or absence of something.
    For instance, as we saw earlier in the chapter, I wanted to count the number of
    times that Adam and Eve appeared in the Bible. I could have similarly written
    some simple code to determine whether Adam and Eve were in the Bible at all or
    whether they were in the book of Exodus.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能不完全符合NLP的严格定义，但它经常出现在任何文本操作中，这在NLP中的机器学习中也会发生，那里使用了独热编码（one-hot encoding）。在这里，我们严格关注某个事物的存在与否。例如，如我们在本章之前看到的，我想计算亚当和夏娃在《圣经》中出现的次数。我也可以写一些简单的代码，来确定亚当和夏娃是否出现在《圣经》中，或者是否出现在《出埃及记》中。
- en: 'For this example, let’s use this DataFrame that I have set up:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，让我们使用我已经设置好的这个DataFrame：
- en: '![Figure 1.2 – pandas DataFrame containing the entire King James Version text
    of the Bible](img/B17105_01_002.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2 – 包含《圣经》全书金句版文本的pandas DataFrame](img/B17105_01_002.jpg)'
- en: Figure 1.2 – pandas DataFrame containing the entire King James Version text
    of the Bible
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – 包含《圣经》全书的金句版文本的pandas DataFrame
- en: 'I specifically want to see whether Eve exists as one of the entities in `df[''entities'']`.
    I want to keep the data in a DataFrame, as I have uses for it, so I will just
    do some pattern matching on the `entities` field:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我特别想查看夏娃是否作为`df['entities']`中的一个实体存在。我希望将数据保存在DataFrame中，因为我会用到它，所以我会在`entities`字段上进行一些模式匹配：
- en: '[PRE0]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, I am using what is called a `^` symbol means that the `E` in Eve sits
    at the very beginning of the string, and `$` means that the `e` sits at the very
    end of the string. This ensures that there is an entity that is exactly named
    Eve, with nothing before and after. With regex, you have a lot more flexibility
    than this, but this is a simple example.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我使用了所谓的`^`符号，表示“E”在夏娃中的位置位于字符串的最开始，`$`表示“e”在字符串的末尾。这确保了存在一个名为Eve的实体，且前后没有其他字符。使用正则表达式，你可以获得比这更大的灵活性，但这是一个简单的例子。
- en: 'In Python, if you have a series of `True` and `False` values, `.min()` will
    give you `False`, and `.max()` will give you `True`, and that makes sense as another
    way of looking at `True` and `False` is a `1` and `0`, and `1` is greater than
    `0`. There are other ways to do this, but I am going to do it this way. So, to
    see whether Eve is mentioned even once in the whole Bible, I can do the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，如果你有一个`True`和`False`值的系列，`.min()`会给你`False`，`.max()`会给你`True`，这也有道理，因为从另一个角度来看，`True`和`False`分别是`1`和`0`，而`1`大于`0`。虽然有其他方法可以做到这一点，但我打算用这种方式。所以，为了查看《圣经》是否至少提到一次夏娃，我可以做如下操作：
- en: '[PRE1]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If I want to see if Adam is in the Bible, I can replace `Eve` with `Adam`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我想查看亚当是否在《圣经》中，可以将`Eve`替换为`Adam`：
- en: '[PRE2]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Detecting the presence or absence of something in a piece of text can be useful.
    For instance, if we want to very quickly get a list of Bible verses that are about
    **Eve**, we can do the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 检测文本中某个事物的存在或不存在是很有用的。例如，如果我们想快速获取一份关于**夏娃**的圣经经文列表，可以这样做：
- en: '[PRE3]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will give us a DataFrame of Bible verses mentioning Eve:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个包含提及夏娃的圣经经文的DataFrame：
- en: '![Figure 1.3 – Bible verses containing strict mentions of Eve](img/B17105_01_003.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图1.3 – 包含严格提及夏娃的圣经经文](img/B17105_01_003.jpg)'
- en: Figure 1.3 – Bible verses containing strict mentions of Eve
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 包含严格提及夏娃的圣经经文
- en: 'If we want to get a list of verses that are about Noah, we can do this:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要得到关于诺亚的经文列表，可以这样做：
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will give us a DataFrame of Bible verses mentioning **Noah**:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个包含提及**诺亚**的圣经经文的DataFrame：
- en: '![Figure 1.4 – Bible verses containing strict mentions of Noah](img/B17105_01_004.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图1.4 – 包含严格提及诺亚的圣经经文](img/B17105_01_004.jpg)'
- en: Figure 1.4 – Bible verses containing strict mentions of Noah
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 – 包含严格提及诺亚的圣经经文
- en: I have added .`head(10)` to only see the first ten rows. With text, I often
    find myself wanting to see more than the default five rows.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经添加了.`head(10)`，只查看前十行。对于文本，我经常发现自己想要查看更多内容，而不是默认的五行。
- en: And if we didn’t want to use the **entities** field, we could look in the text
    instead.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不想使用**entities**字段，我们也可以改为在文本中查找。
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will give us a DataFrame of Bible verses where the text of the verse included
    a mention of **Eve**.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个包含提到**夏娃**的圣经经文的DataFrame。
- en: '![Figure 1.5 – Bible verses containing mentions of Eve](img/B17105_01_005.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 – 包含提及夏娃的圣经经文](img/B17105_01_005.jpg)'
- en: Figure 1.5 – Bible verses containing mentions of Eve
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – 包含提及夏娃的圣经经文
- en: That is where this gets a bit messy. I have already done some of the hard work,
    extracting entities for this dataset, which I will show how to do in a later chapter.
    When you are dealing with raw text, regex and pattern matching can be a headache,
    as shown in the preceding figure. I only wanted the verses that contained Eve,
    but instead, I got matches for words such as **even** and **every**. That’s not
    what I want.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是问题变得有些复杂的地方。我已经做了一些繁重的工作，提取了该数据集中的实体，稍后我会展示如何做到这一点。当你处理原始文本时，正则表达式（regex）和模式匹配可能会很麻烦，正如前面图示所示。我只想要包含“Eve”的诗句，但结果却匹配了**even**和**every**等词。这不是我想要的。
- en: Anyone who works with text data is going to want to learn the basics of regex.
    Take heart, though. I have been using regex for over twenty years, and I still
    very frequently have to Google search to get mine working correctly. I’ll revisit
    regex, but I hope that you can see that it is pretty simple to determine if a
    word exists in a string. For something more practical, if you had 400,000 scraped
    tweets and you were only interested in the ones that were about a specific thing,
    you could easily use the preceding techniques or regex to look for an exact or
    close match.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 任何处理文本数据的人都会想要学习正则表达式的基础知识。不过，别担心。我已经使用正则表达式超过二十年了，但我仍然经常需要在谷歌上搜索来确保正则表达式正确工作。我会再次讲解正则表达式，但我希望你能明白，判断一个词是否存在于字符串中其实是相当简单的。举个更实际的例子，如果你有40万个抓取的推文，而你只对那些关于特定主题的推文感兴趣，你可以轻松地使用前面提到的技巧或正则表达式来查找精确匹配或相近匹配。
- en: Regular expressions (regex)
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则表达式（regex）
- en: 'I briefly explained regex in the previous section, but there is much more that
    you can use it for than to simply determine the presence or absence of something.
    For instance, you can also use regex to extract data from text to enrich your
    datasets. Let’s look at a data science feed that I scrape:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我在前一部分简要解释了正则表达式，但它的用途远不止于此，远比仅仅用来判断某些东西的存在或不存在。例如，你还可以使用正则表达式从文本中提取数据，以丰富你的数据集。我们来看一个我抓取的数据科学动态：
- en: '![Figure 1.6 – Scraped data science Twitter feed](img/B17105_01_006.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图1.6 – 抓取的数据科学Twitter动态](img/B17105_01_006.jpg)'
- en: Figure 1.6 – Scraped data science Twitter feed
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 – 抓取的数据科学Twitter动态
- en: 'There’s a lot of value in that text field, but it is difficult to work with
    in its current form. What if I only want a list of links that are posted every
    day? What if I want to see the hashtags that are used by the data science community?
    What if I want to take these tweets and build a social network to analyze who
    interacts? The first thing we should do is enrich the dataset by extracting things
    that we want. So, if I wanted to create three new fields that contained lists
    of hashtags, mentions, and URLs, I could do the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文本字段中包含很多有价值的信息，但以当前形式处理起来很困难。如果我只想要每天发布的链接列表怎么办？如果我想看到数据科学社区使用的标签怎么办？如果我想把这些推文拿去构建一个社交网络来分析谁在互动呢？我们应该做的第一件事是通过提取我们需要的内容来丰富数据集。所以，如果我想创建包含标签、提及和网址列表的三个新字段，我可以这样做：
- en: '[PRE6]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the first three lines, I am adding a space behind each mention, hashtag,
    and URL just to give a little breathing room for splitting. In the next three
    lines, I am splitting each tweet by space and then applying rules to identify
    mentions, hashtags, and URLs. In this case, I don’t use fancy logic. Mentions
    start with `@`, hashtags start with `#`, and URLs start with HTTP (to include
    HTTPS). The result of this code is that I end up with three additional columns,
    containing lists of users, tags, and URLs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面三行中，我在每个提及、标签和网址后面加了一个空格，以便为分割提供一些空间。在接下来的三行中，我通过空格分割每条推文，然后应用规则来识别提及、标签和网址。在这种情况下，我没有使用复杂的逻辑。提及以`@`开头，标签以`#`开头，网址以HTTP（包括HTTPS）开头。这个代码的结果是，我最终得到了三列额外的数据，分别包含用户、标签和网址的列表。
- en: 'If I then use `explode()` on the users, tags, and URLs, I will get a DataFrame
    where each individual user, tag, and URL has its own row. This is what the DataFrame
    looks like after `explode()`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我对用户、标签和网址使用`explode()`，我将得到一个每个用户、标签和网址都有自己一行的DataFrame。以下是`explode()`后的DataFrame样式：
- en: '![Figure 1.7 – Scraped data science Twitter feed, enriched with users, tags,
    and URLs](img/B17105_01_007.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图1.7 – 抓取的数据科学Twitter动态，已增加用户、标签和网址](img/B17105_01_007.jpg)'
- en: Figure 1.7 – Scraped data science Twitter feed, enriched with users, tags, and
    URLs
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 – 抓取的数据科学Twitter动态，已增加用户、标签和网址
- en: 'I can then use these new columns to get a list of unique hashtags used:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我可以使用这些新列来获取使用过的独特标签列表：
- en: '[PRE7]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Clearly, the regex used in my data enrichment is not perfect, as punctuation
    should not be included in hashtags. That’s something to fix. Be warned, working
    with human language is very messy and difficult to get perfect. We just have to
    be persistent to get exactly what we want.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，我在数据丰富过程中使用的正则表达式并不完美，因为标点符号不应该包含在标签中。这是需要修正的地方。请注意，处理人类语言是非常混乱的，很难做到完美。我们只需要坚持不懈，才能准确地得到我们想要的结果。
- en: 'Let’s see what the unique mentions look like. By unique mentions, I mean the
    deduplicated individual accounts mentioned in tweets:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看独特的提及是怎样的。所谓独特的提及，指的是推文中去重后的单独账户：
- en: '[PRE8]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That looks a lot better, though `@` should not exist alone, the fourth one looks
    suspicious, and a few of these look like they were mistakenly used as mentions
    when they should have been used as hashtags. That’s a problem with the tweet text,
    not the regular expression, most likely, but worth investigating.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来好多了，不过`@`符号不应该单独出现，第四个看起来有些可疑，其中一些看起来像是被错误地作为提及，而本应作为标签使用。这可能是推文文本的问题，而不是正则表达式的问题，但值得进一步调查。
- en: I like to lowercase mentions and hashtags so that it is easier to find unique
    tags. This is often done as *preprocessing* *for NLP*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢将提及和标签转换为小写字母，这样更容易找到独特的标签。这通常作为*预处理* *用于自然语言处理（NLP）*。
- en: 'Finally, let’s get a list of unique URLs mentioned (which can then be used
    for further scraping):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们获取提到的独特网址列表（这些可以进一步用于抓取）：
- en: '[PRE9]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This looks very clean. How many URLs was I able to extract?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很干净。我能够提取多少个网址？
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That’s a lot of links. As this is Twitter data, a lot of URLs are often photos,
    selfies, YouTube links, and other things that may not be too useful to a researcher,
    but this is my scraped data science feed, which pulls information from dozens
    of data science related accounts, so many of these URLs likely include exciting
    news and research.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这有很多链接。由于这是Twitter数据，很多网址通常是照片、自拍、YouTube链接和其他一些对研究者可能不太有用的内容，但这是我的抓取数据科学信息流，它从数十个与数据科学相关的账户中获取信息，所以这些网址很可能包含一些令人兴奋的新闻和研究成果。
- en: Regex allows you to extract additional data from your data and use it to enrich
    your datasets to do easier or further analysis, and if you extract URLs, you can
    use that as input for additional scraping.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式（Regex）允许你从数据中提取额外的数据，并使用它来丰富数据集，以便进行更简单或进一步的分析。如果你提取了网址，可以将其作为额外抓取的输入。
- en: I’m not going to give a long lesson into regex. There are a whole lot of books
    dedicated to the topic. It is likely that, eventually, you will need to learn
    how to use regex. For what we are doing in this book, the preceding regex is probably
    all you will need, as we are using these tools to build social networks that we
    can analyze. This book isn’t primarily about NLP. We just use some NLP techniques
    to create or enrich our data, and then we will use network analysis for everything
    else.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会对正则表达式做一个长篇的讲解。关于这个话题有很多专门的书籍。最终，你很可能需要学习如何使用正则表达式。对于本书中的内容，前面提到的正则表达式大概是你所需要的全部，因为我们使用这些工具来构建可以分析的社交网络。本书的核心并不是关于NLP的，我们只是在创建或丰富数据时使用一些NLP技术，其他部分则主要依赖网络分析。
- en: Word counts
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词频统计
- en: Word counts are also useful, especially when we want to compare things against
    each other. For instance, we already compared the number of times that Adam and
    Eve were mentioned in the Bible, but what if we want to see the number of times
    that all entities are mentioned in the Bible? We can do this the simple way, and
    we can do this the NLP way. I prefer to do things the simple way, where possible,
    but frequently, the NLP or graph way ends up being the simpler way, so learn everything
    that you can and decide for yourself.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 词频统计也很有用，特别是当我们想要进行对比时。例如，我们已经比较了《圣经》中亚当和夏娃被提及的次数，但如果我们想要看到所有实体在《圣经》中被提及的次数该怎么办呢？我们可以用简单的方法来做，也可以用NLP方法来做。我个人偏好尽可能使用简单的方法，但很多时候，NLP或图形方法反而变成了更简单的方式，所以要学会所有的方法，再根据情况做选择。
- en: We will do this the simple way by counting the number of times entities were
    mentioned. Let’s use the dataset again and just do some aggregation to see who
    the most mentioned people are in the Bible. Keep in mind we can do this for any
    feed that we scrape, so long as we have enriched the dataset to contain a list
    of mentions. But for this demonstration, I’ll use the Bible.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用简单的方法来做，计算实体被提及的次数。我们再次使用数据集，并进行一些聚合操作，看看在《圣经》中最常被提及的人物是谁。记住，我们可以对任何我们抓取的内容执行这个操作，只要我们已将数据集丰富到包含提及列表。但在这个示范中，我将使用《圣经》。
- en: 'On the third line, I am keeping entities with a name longer than two characters,
    effectively dropping some junk entities that ended up in the data. I am using
    this as a filter:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三行，我保留了名字长度超过两个字符的实体，有效地去除了那些最终出现在数据中的无关实体。我使用的是这个过滤器：
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This is shown in the following DataFrame:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在下面的DataFrame中显示：
- en: '![Figure 1.8 – Entity counts across the entire Bible](img/B17105_01_008.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.8 – 整个圣经中实体计数](img/B17105_01_008.jpg)'
- en: Figure 1.8 – Entity counts across the entire Bible
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 – 整个圣经中实体计数
- en: This looks pretty good. Entities are people, places, and things, and the only
    oddball in this bunch is the word **thou**. The reason that snuck in is that in
    the Bible, often the word *thou* is capitalized as *Thou*, which gets tagged as
    an **NNP** (**Proper Noun**) when doing entity recognition and extraction in NLP.
    However, *thou* is in reference to *You*, and so it makes sense. For example,
    *Thou shalt not kill, thou shalt* *not steal*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很不错。实体是指人、地点和事物，唯一不同的地方是词汇**thou**。之所以会出现它，是因为在圣经中，*thou*这个词通常会被大写为*Thou*，而在进行实体识别和提取时，它会被标记为**NNP**（**专有名词**）。然而，*thou*指的是*You*，因此也能理解。例如，*Thou
    shalt not kill, thou shalt* *not steal*。
- en: 'If we have the data like this, we can also very easily visualize it for perspective:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有这样的数据，我们也可以非常容易地进行可视化，帮助我们从不同的角度理解：
- en: '[PRE12]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will give us a horizontal bar chart of entity counts:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个水平条形图，显示实体计数：
- en: '![Figure 1.9 – Visualized entity counts across the entire Bible](img/B17105_01_009.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.9 – 整个圣经中实体计数的可视化](img/B17105_01_009.jpg)'
- en: Figure 1.9 – Visualized entity counts across the entire Bible
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 – 整个圣经中实体计数的可视化
- en: This is obviously not limited to use on the Bible. If you have any text at all
    that you are interested in, you can use these techniques to build a deeper understanding.
    If you want to use these techniques to pursue art, you can. If you want to use
    these techniques to help fight crime, you can.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这不仅限于圣经中的应用。如果你有任何你感兴趣的文本，你可以使用这些技术来构建更深的理解。如果你想将这些技术用于艺术创作，你可以。如果你想使用这些技术来帮助打击犯罪，你也可以。
- en: Sentiment analysis
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 情感分析
- en: 'This is my favorite technique in all of NLP. I want to know what people are
    talking about and how they feel about it. This is an often underexplained area
    of NLP, and if you pay attention to how most people use it, you will see many
    demonstrations on how to build classifiers that can determine positive or negative
    sentiment. However, we humans are complicated. We are not just happy or sad. Sometimes,
    we are neutral. Sometimes we are mostly neutral but more positive than negative.
    Our feelings are nuanced. One book that I have used a lot for my own education
    and research into sentiment analysis mentions a study that mapped out human emotions
    as having primary, secondary, and tertiary emotions (Liu, *Sentiment Analysis*,
    2015, p. 35). Here are a few examples:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我在所有NLP技术中最喜欢的一项。我想知道人们在谈论什么，他们对这些事物的感受如何。这是NLP中一个经常被低估的领域，如果你关注大多数人如何使用它，你会看到很多关于如何构建分类器的展示，这些分类器可以确定积极或消极的情感。然而，我们人类很复杂。我们不仅仅是开心或悲伤。有时，我们是中立的。有时，我们大部分是中立的，但更多的是积极而非消极。我们的感情是微妙的。我用过的一本书为我自己的情感分析教育和研究提供了很多帮助，书中提到了一项研究，讲述了人类情感被划分为主要、次要和三级情感（Liu,
    *Sentiment Analysis*, 2015，第35页）。这里有几个例子：
- en: '| **Primary Emotion** | **Secondary Emotion** | **Tertiary Emotion** |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| **主要情感** | **次要情感** | **三级情感** |'
- en: '| --- | --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Anger | Disgust | Contempt |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 愤怒 | 反感 | 蔑视 |'
- en: '| Anger | Envy | Jealousy |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 愤怒 | 嫉妒 | 妒忌 |'
- en: '| Fear | Horror | Alarm |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 恐惧 | 惊悚 | 警觉 |'
- en: '| Fear | Nervousness | Anxiety |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 恐惧 | 紧张 | 焦虑 |'
- en: '| Love | Affection | Adoration |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 爱 | 喜爱 | 崇拜 |'
- en: '| Love | Lust | Desire |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 爱 | 欲望 | 渴望 |'
- en: Figure 1.10 – A table of primary, secondary, and tertiary emotions
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10 – 主要、次要和三级情感的表格
- en: There are a few primary emotions, there are more secondary emotions, and there
    are many, many more tertiary emotions. Sentiment analysis can be used to try to
    classify yes/no for any emotion as long as you have training data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些主要情感，更多的是次要情感，而更多的是三级情感。情感分析可以用来尝试分类任何情感的“是”或“否”，只要你有训练数据。
- en: Sentiment analysis doesn’t have to only be used for detecting emotions. The
    techniques can also be used for classification, so I don’t feel that sentiment
    analysis is quite the complete wording, and maybe that is why there are so many
    demonstrations of people simply detecting positive/negative sentiment from Yelp
    and Amazon reviews.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析并不仅仅用于检测情感。这些技术也可以用于分类，因此我觉得情感分析这个术语并不完全准确，也许这就是为什么有这么多人只是简单地从 Yelp 和 Amazon
    评论中检测积极/消极情感的原因。
- en: I have more interesting uses for sentiment classification. Right now, I use
    these techniques to detect toxic speech (really abusive language), positive sentiment,
    negative sentiment, violence, good news, bad news, questions, disinformation research,
    and network science research. You can use this as intelligent pattern matching,
    which learns the nuances of how text about a topic is often written about. For
    instance, if we wanted to catch tweets related to disinformation, we could train
    a model on text having to do with misinformation, disinformation, and fake news.
    The model would learn other related terms during training, and it would do a much
    better and much faster job of catching them than any human could.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一些更有趣的情感分类应用。目前，我使用这些技术来检测有毒言论（真正的辱骂性语言）、积极情感、消极情感、暴力、好消息、坏消息、问题、虚假信息研究和网络科学研究。你可以将其视为一种智能模式匹配，它能够学习关于某个话题的文本通常是如何被写作的。例如，如果我们想捕捉关于虚假信息的推文，我们可以训练一个关于虚假信息、误导信息和假新闻的文本模型。在训练过程中，模型会学习到其他相关术语，能够比任何人都更快速、更精准地捕捉到它们。
- en: Sentiment analysis and text classification advice
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 情感分析和文本分类建议
- en: 'Here is some advice before I move on to the next section: for sentiment analysis
    and text classification, in many cases, you do not need a neural network for something
    this simple. If you are building a classifier to detect hate speech, a “bag of
    words” approach will work for preprocessing, and a simple model will work for
    classification. Always start simple. A neural network may give you a couple of
    percents better accuracy if you work at it, but it’ll take more time and be less
    explainable. A `linearsvc` model can be trained in a split second and often do
    as well, sometimes even better, and some other simple models and techniques should
    be attempted as well.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在我进入下一部分之前，这里有一些建议：对于情感分析和文本分类，在很多情况下，你不需要神经网络来处理这么简单的任务。如果你正在构建一个检测仇恨言论的分类器，使用“词袋”方法进行预处理，再加上一个简单的模型进行分类就足够了。总是从简单的开始。如果你努力训练，神经网络可能会提高几个百分点的准确率，但它需要更多时间，且可解释性较差。一个
    `linearsvc` 模型可以在瞬间训练完成，并且通常能达到同样的效果，有时甚至更好，你也应该尝试一些其他简单的模型和技术。
- en: 'Another piece of advice: experiment with stopword removal, but don’t just remove
    stopwords because that’s what you have been told. Sometimes it helps, and sometimes
    it hurts your model. The majority of the time, it might help, but it’s simple
    enough to experiment.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个建议是：试验停用词去除，但不要仅仅因为别人告诉你要去除停用词就去除它们。有时候它有帮助，有时候却会对模型产生不利影响。大多数时候，它可能有帮助，但这足够简单，可以进行实验。
- en: 'Also, when building your datasets, you can often get the best results if you
    do sentiment analysis against sentences rather than large chunks of text. Imagine
    that we have the following text:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在构建数据集时，如果你对句子进行情感分析，而不是对大块文本进行分析，通常能得到最佳效果。假设我们有以下文本：
- en: '*Today, I woke up early, had some coffee, and then I went outside to check
    on the flowers. The sky was blue, and it was a nice, warm June morning. However,
    when I got back into the house, I found that a pipe had sprung a leak and flooded
    the entire kitchen. The rest of the day was garbage. I am so angry* *right now.*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*今天我早早醒来，喝了些咖啡，然后出去查看花朵。天空湛蓝，是一个温暖的六月早晨。然而，当我回到屋子里时，我发现水管漏水，整个厨房被淹了。接下来的一整天都很糟糕。我现在真是太生气了。*'
- en: Do you think that the emotions of the first sentence are identical to the emotions
    of the last sentence? This imaginary story is all over the place, starting very
    cheerful and positive and ending in disaster and anger. If you classify at the
    sentence level, you are able to be more precise. However, even this is not perfect.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为第一句话的情感和最后一句话的情感是完全一样的吗？这个虚构故事的情感变化很大，从一开始的愉快和积极到最后的灾难和愤怒。如果你在句子层面进行分类，你能做到更精确。然而，即便如此，这也并不完美。
- en: '*Today started out perfectly, but everything went to hell and I am so angry*
    *right now.*'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*今天一开始一切都很完美，但最后一切都崩溃了，我现在真是太生气了。*'
- en: What is the sentiment of that sentence? Is it positive or negative? It’s both.
    And so, ideally, if you could capture that a sentence has multiple emotions on
    display, that would be powerful.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 那个句子的情感是什么？是积极的还是消极的？它其实是两者都有。因此，理想情况下，如果你能够捕捉到一个句子同时表现出多种情感，那将会非常强大。
- en: Finally, when you build your models, you always have the choice of whether you
    want to build binary or multiclass language models. For my own uses, and according
    to research that has resonated with me, it is often easiest to build small models
    that simply look for the presence of something. So, rather than building a neural
    network to determine whether the text is positive, negative, or neutral, you could
    build a model that looks for positive versus other and another one that looks
    for negative versus other.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在构建模型时，你总是可以选择是否构建二分类或多分类的语言模型。根据我自己的使用经验以及一些与我产生共鸣的研究，通常最简单的是构建一些小模型，只需要检查某些内容是否存在。因此，与其构建一个神经网络来判断文本是积极、消极还是中立的，不如构建一个模型检查“积极”与其他情感的对比，另一个模型检查“消极”与其他情感的对比。
- en: 'This may seem like more work, but I find that it goes much faster, can be done
    with very simple models, and the models can be chained together to look for an
    array of different things. For instance, if I wanted to classify political extremism,
    I could use three models: toxic language, politics, and violence. If a piece of
    text was classified as positive for toxic language, was political, and was advocating
    violence, then it is likely that the poster may be showing some dangerous traits.
    If only toxic language and political sentiment were being displayed, well, that’s
    common and not usually politically extreme or dangerous. Political discussion
    is often hostile.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来可能会更费力，但我发现这做起来要快得多，可以使用非常简单的模型，而且这些模型可以组合在一起，寻找各种不同的信息。例如，如果我想分类政治极端主义，我可以使用三个模型：有毒语言、政治和暴力。如果一段文本被分类为有毒语言，属于政治话题，并且倡导暴力，那么这篇内容的发布者可能表现出一些危险的特征。如果只显示有毒语言和政治情感，那很常见，通常不具备极端政治或危险性。政治讨论往往带有敌意。
- en: Information extraction
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息提取
- en: We have already done some information extraction in previous examples, so I
    will keep this brief. In the previous section, we extracted user mentions, hashtags,
    and URLs. This was done to enrich the dataset, making further analysis much easier.
    I added the steps that extract this into my scrapers directly so that I have the
    lists of users, mentions, and URLs immediately when I download fresh data. This
    allows me to immediately jump into network analysis or investigate the latest
    URLs. Basically, if there is information you are looking for, and you come up
    with a way to repeatedly extract it from text, and you find yourself repeating
    the steps over and over on different datasets, you should consider adding that
    functionality to your scrapers.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在之前的示例中做了一些信息提取，所以我会简要说明。在前一部分中，我们提取了用户提及、话题标签和网址。这些操作是为了丰富数据集，使进一步的分析变得更加容易。我将这些提取步骤直接加到我的抓取程序中，这样我在下载新数据时就能立即获得用户、提及和网址的列表。这让我可以立即开始网络分析或调查最新的URL。基本上，如果你正在寻找某些信息，并且你找到了一种可以反复从文本中提取信息的方法，而你发现自己在不同数据集上重复做这些步骤，那么你应该考虑将这些功能加入到你的抓取程序中。
- en: 'The most powerful data that is enriching my Twitter datasets are two fields:
    *publisher*, and *users*. **Publisher** is the account that posted the tweet.
    **Users** are the accounts mentioned by the publisher. Each of my feeds has dozens
    of publishers. With publishers and users, I can build social networks from raw
    text, which will be explained in this book. It is one of the most useful things
    I have figured out how to do, and you can use the results to find other interesting
    accounts to scrape.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最丰富的、提升我Twitter数据集的数据来自两个字段：*发布者*和*用户*。**发布者**是发布推文的账号。**用户**是发布者提到的账号。我的每个信息流中都有几十个发布者。有了发布者和用户，我可以从原始文本中构建社交网络，本书中将对此进行详细解释。这是我发现的一项最有用的技巧，你也可以利用这些结果去寻找其他有趣的账号进行抓取。
- en: Community detection
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社区检测
- en: '**Community detection** is not typically mentioned with regard to NLP, but
    I do think that it should be, especially when using social media text. For instance,
    if we know that certain hashtags are used by certain groups of people, we can
    detect other people that may be affiliated with or are supporters of those groups
    by the hashtags that they use. It is very easy to use this to your advantage when
    researching groups of people. Just scrape a bunch of them, see what hashtags they
    use, then search those hashtags. Mentions can give you hints of other accounts
    to scrape as well.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**社区检测**通常不会在自然语言处理（NLP）中提及，但我认为它应该被提到，尤其是在使用社交媒体文本时。例如，如果我们知道某些标签（hashtag）被特定群体使用，我们可以通过他们使用的标签检测到其他可能与这些群体有联系或支持这些群体的人。利用这一点进行人群研究非常简单。只需抓取一堆数据，查看他们使用的标签，然后搜索这些标签。提及也可以给你提供其他账户的信息，供你进一步抓取。'
- en: Community detection is commonly mentioned in social network analysis, but it
    can also very easily be done with NLP, and I have used topic modeling and the
    preceding approach as ways of doing so.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 社区检测通常在社交网络分析中提到，但它也可以非常容易地通过NLP实现，我也曾使用话题建模和上述方法来进行社区检测。
- en: Clustering
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: '**Clustering** is a technique commonly found in unsupervised learning but also
    done in network analysis. In clustering, we are looking for things that are similar
    to other things. There are different approaches for doing this, and even NLP topic
    modeling can be used as clustering. In unsupervised ML, you can use algorithms
    such as k-means to find tweets, sentences, or books that are similar to other
    tweets, sentences, or books. You could do similar with topic modeling, using TruncatedSVD.
    Or, if you have an actual sociogram (map of a social network), you could look
    at the connected components to see which nodes are connected or apply k-means
    against certain network metrics (we will go into this later) to see nodes with
    similar characteristics.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**是无监督学习中常见的一种技术，也常用于网络分析。在聚类中，我们是在寻找与其他事物相似的事物。做这项工作的方式有很多种，甚至NLP的话题建模也可以作为一种聚类方式。在无监督机器学习中，你可以使用像k-means这样的算法，找到与其他推文、句子或书籍相似的推文、句子或书籍。你也可以使用话题建模，利用TruncatedSVD来做类似的事情。或者，如果你有一个实际的社交图（社交网络图），你可以查看连接的组件，看看哪些节点是连接的，或者应用k-means算法来分析某些网络度量（我们稍后会深入讨论），看看哪些节点具有相似的特征。'
- en: Advanced uses of NLP
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP的高级应用
- en: Most of the NLP that you will do on a day-to-day basis will probably fall into
    one of the simpler uses, but let’s also discuss a few advanced uses. In some cases,
    what I am describing as advanced uses is really a combination of simpler uses
    to deliver a more complicated solution. So, let’s discuss some of the more advanced
    uses, such as chatbots and conversational agents, language modeling, information
    retrieval, text summarization, topic discovery and modeling, text-to-speech and
    speech-to-text conversion, MT, and personal assistants.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 你日常进行的大部分NLP工作可能都会属于较为简单的应用，但我们也来讨论一些高级应用。在某些情况下，我所描述的高级应用实际上是将多个简单应用结合起来，提供更复杂的解决方案。所以，让我们讨论一些更高级的应用，比如聊天机器人和对话代理、语言建模、信息检索、文本摘要、话题发现和建模、文本转语音和语音转文本、机器翻译（MT）以及个人助手。
- en: Chatbots and conversational agents
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聊天机器人和对话代理
- en: A **chatbot** is a program that can hold a conversation with its user. These
    have existed for years, with the earliest chatbots being created in the 1960s,
    but they have been steadily improving and are now an effective means of forwarding
    a user to a more specific form of customer support, for instance. If you go to
    a website’s support section, you may be presented with a small chatbox popup that’ll
    say something like, “*What can we help you with today?*”. You may then type, “*I
    want to pay off my credit card balance.*” When the application receives your answer,
    it’ll be able to use it to determine the correct form of support that you need.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天机器人**是能够与用户进行对话的程序。这类程序已经存在多年，最早的聊天机器人出现在20世纪60年代，但它们一直在不断改进，现在已成为一种有效的工具，用于将用户引导到更具体的客户支持形式。例如，如果你进入一个网站的支持部分，可能会看到一个小的聊天框弹出，里面写着类似“*今天我们能为您提供什么帮助？*”的话。你可能会输入“*我想偿还我的信用卡余额*。”当应用程序收到你的答案时，它就能用这个信息来判断你需要哪种支持形式。'
- en: While a chatbot is built to handle human text, a **conversational agent** can
    handle voice audio. Siri and Alexa are examples of conversational agents. You
    can talk to them and ask them questions.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然聊天机器人是为处理人类文本而构建的，但**对话代理**可以处理语音音频。Siri 和 Alexa 就是对话代理的例子。你可以与它们对话并提问。
- en: Chatbots and conversational agents are not limited to text, though; we often
    run into similar switchboards when we call a company with our phones. We will
    get the same set of questions, either looking for a word answer or numeric input.
    So, behind the scenes, where voice is involved, there will be a voice-to-text
    conversion element in place. Applications also need to determine whether someone
    is asking a question or making a statement, so there is likely text classification
    involved as well.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，聊天机器人和对话代理不仅限于文本；当我们用电话拨打公司电话时，也经常遇到类似的电话交换机。我们会接到一系列相同的问题，可能需要回答一个单词或输入数字。因此，在后台，如果涉及语音，就会有一个语音转文本的转换元素。同时，应用程序还需要确定用户是在提问还是在陈述，因此很可能还会涉及文本分类。
- en: Finally, to provide an answer, text summarization could convert search results
    into a concise statement to return as text or speech to the user to complete the
    interaction.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了提供答案，文本摘要可以将搜索结果转化为简洁的陈述，以文本或语音的形式返回给用户，完成交互。
- en: Chatbots are not just rudimentary question-and-answer systems, though. I believe
    that they will be an effective way for us to interact with text. For instance,
    you could build a chatbot around the book *Alice’s Adventures in Wonderland* (or
    the Bible) to give answers to questions specifically about the book. You could
    build a chatbot from your own private messages and talk to yourself. There’s a
    lot of room for creativity here.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，聊天机器人不仅仅是简单的问答系统。我认为它们将成为我们与文本互动的有效方式。例如，你可以围绕《爱丽丝梦游仙境》这本书（或者圣经）构建一个聊天机器人，以回答有关这本书的具体问题。你还可以根据自己的私人信息构建一个聊天机器人，与自己对话。在这里，有很多创造空间。
- en: Language modeling
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言建模
- en: '**Language modeling** is concerned with predicting the next word given a sequence
    of words. For instance, what would come after this: “*The cow jumped over the
    ______.*” Or this: “*Green eggs and _____.*” If you go to Google and start typing
    in the search bar, take note that the next predicted word will show in a drop-down
    list to speed up your search.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言建模**关注的是在给定一系列单词的情况下，预测下一个单词。例如，接下来会是什么：“*The cow jumped over the ______.*”
    或者：“*Green eggs and _____.*”如果你去Google并开始在搜索栏中输入，你会注意到下一个预测的单词会显示在下拉列表中，以加速你的搜索。'
- en: We can see that Google has predicted the next word to be **ham** but that it
    is also finding queries that are related to what you have already typed. This
    looks like a combination of language modeling as well as clustering or topic modeling.
    They are looking to predict the next word before you even type it, and they are
    even going a step further to find other queries that are similar to the text you
    have already typed.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，Google预测下一个单词是**ham**，但它也在查找与已输入文本相关的查询。这看起来像是语言建模与聚类或主题建模的结合。它们在你输入之前就预测了下一个单词，甚至还进一步寻找与你已输入文本相关的其他查询。
- en: 'Data scientists are also able to use language modeling as part of the creation
    of generative models. In 2020, I trained a model on thousands of lines of many
    Christmas songs and trained it to write Christmas poetry. The results were rough
    and humorous, as I only spent a couple of days on it, but it was able to take
    seed text and use it to create entire poems. For instance, seed text could be,
    “Jingle bells,” and then the model would continuously take previous text and use
    it to create a poem until it reached the end limits for words and lines. Here
    is my favorite of the whole batch:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家还可以将语言建模作为生成模型创建的一部分。在2020年，我用数千行圣诞歌曲的歌词训练了一个模型，并训练它写圣诞诗。结果虽然粗糙且幽默，因为我只花了几天时间，但它能够以种子文本为基础，利用这些文本生成整首诗。例如，种子文本可以是“铃儿响叮当”，然后模型会不断根据之前的文本生成诗句，直到达到单词和行数的限制。以下是我最喜欢的那首：
- en: '[PRE13]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: I built the generative process to take a random first word from any of the lines
    of the training data songs. From there, it would create lines of 6 words for a
    total of 25 lines. I only trained it for 24 hours, as I wanted to quickly get
    this done in time for Christmas. There are several books on creating generative
    models, so if you would like to use AI to augment your own creativity, then I
    highly suggest looking into them. It feels like a collaboration with a model rather
    than replacing ourselves with a model.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我构建了一个生成过程，从训练数据中的任意一行随机选取一个单词作为开头。接着，它会生成由6个单词组成的句子，直到完成25行。我只训练了24小时，因为我希望能在圣诞节前迅速完成这项工作。有几本关于创建生成模型的书籍，如果你想利用人工智能来增强你的创造力，我强烈建议你了解一下它们。这更像是与模型的合作，而不是用模型来取代我们自己。
- en: These days, generative text models are becoming quite impressive. ChatGPT –
    released in November 2022 – has grabbed the attention of so many people with its
    ability to answer most questions and give realistic-looking answers. The answers
    are not always correct, so generative models still have a long way to go, but
    there is now a lot of hype around generative models, and people are considering
    how they can use them in their own work and lives and what they mean for our future.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现如今，生成式文本模型变得相当令人印象深刻。ChatGPT——2022年11月发布——凭借其回答大多数问题并提供看似现实的答案的能力，吸引了众多人的关注。这些答案并不总是正确的，因此生成模型仍有很长的路要走，但如今关于生成模型的讨论热度很高，人们也在考虑如何将它们应用到自己的工作和生活中，以及它们对我们未来的意义。
- en: Text summarization
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本摘要
- en: '**Text summarization** is pretty much self-explanatory. The goal is to take
    text as input and return a summary as output. This can be very powerful when you
    are managing thousands or millions of documents and want to provide a concise
    sentence about what is in each. It is essentially returning similar to an “abstract”
    section you would find in an academic article. Many of the details are removed,
    and only the essence is returned.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本摘要**几乎不言自明。其目标是将文本作为输入，返回一个摘要作为输出。当你需要管理成千上万或数百万篇文档，并希望提供关于每篇文档的简明概述时，这项技术非常有力。它本质上类似于你在学术文章中找到的“摘要”部分。许多细节会被去除，最终只保留核心内容。'
- en: However, this is not a perfect art, so be aware that if you use this, the algorithm
    may throw away important concepts from the text while keeping those that are of
    less importance. ML is not perfect, so keep an eye on the results.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是一种完美的艺术，因此请注意，如果使用此方法，算法可能会舍弃文本中的重要概念，而保留那些较不重要的部分。机器学习并不完美，因此请时刻关注结果。
- en: However, this is not for search so much as it is for returning a short summary.
    You can use topic modeling and classification to determine document similarity
    and then use this to summarize the final set of documents.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个方法更适用于返回简短摘要，而非搜索。你可以使用主题建模和分类来判断文档的相似性，再利用这些信息来总结最终的文档集。
- en: If you were to take the content of this entire book and feed it to a text summarization
    algorithm, I would hope that it would capture that the marriage of NLP and network
    analysis is powerful and approachable by everyone. You do not need to be a genius
    to work with ML, NLP, or social network analysis. I hope this book will spark
    your creativity and make you more effective at problem solving and thinking critically
    about things. There are a lot of important details in this text, but that is the
    essence.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将整本书的内容输入到文本摘要算法中，我希望它能够捕捉到自然语言处理（NLP）与网络分析的结合是强大且人人可及的。你不需要成为天才才能使用机器学习、自然语言处理或社交网络分析。我希望这本书能激发你的创造力，使你在解决问题和批判性思考方面更加高效。文中有许多重要的细节，但这就是其本质。
- en: Topic discovery and modeling
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题发现与建模
- en: '**Topic discovery and modeling** is very similar to clustering. This is used
    in **Latent Semantic Indexing** (**LSI**), which can be powerful for identifying
    themes (topics) that exist in text, and it can also be an effective preprocessing
    step for text classification, allowing models to be trained on context rather
    than words alone. I mentioned previously, in the *Clustering* and *Community detection*
    subsections, how this could be used to identify subtle groups within communities
    based on the words and hashtags they place into their account descriptions.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题发现与建模**非常类似于聚类。这在**潜在语义索引**（**LSI**）中得到应用，它对于识别文本中存在的主题（topic）非常有效，并且还可以作为文本分类的一个有效预处理步骤，使得模型能够根据上下文而非单纯的词语进行训练。此前我在*聚类*和*社区检测*小节中提到过，这一方法如何根据用户在其账户描述中使用的词汇和标签来识别社区内的细微群体。'
- en: For instance, topic modeling would find similar strings in topics. If you were
    to do topic modeling on political news and social media posts, you will notice
    that in topics, like attracts like. Words will be found with other similar words.
    For instance, *2A* may be spelled out as the *Second Amendment*, *USA* may be
    written in its expanded form (*United States of America*), and so on.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，主题建模会在主题中找到相似的字符串。如果你对政治新闻和社交媒体帖子进行主题建模，你会注意到，在这些主题中，类似的事物往往会聚集在一起。词语会和其他相似的词汇出现在一起。例如，*2A*
    可能会写作 *第二修正案*，*USA* 可能会写作其扩展形式（*美利坚合众国*）等等。
- en: Text-to-speech and speech-to-text conversion
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语音合成与语音识别转换
- en: This type of NLP model aims to convert text into speech audio or audio into
    text transcripts. This is then used as input into classification or conversational
    agents (chatbots, personal assistants).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的自然语言处理（NLP）模型旨在将文本转换为语音音频，或将语音转换为文本记录。然后，这些文本可以作为输入用于分类或对话代理（聊天机器人、个人助理）。
- en: What I mean by that is that you can’t just feed audio to a text classifier.
    Also, it’s difficult to capture context from audio alone without any language
    analysis components, as people speak in different dialects, in different tones,
    and so on.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是，你不能仅仅将音频输入到文本分类器中。此外，如果没有任何语言分析组件，仅凭音频捕捉上下文也是困难的，因为人们说话时会使用不同的方言、语气等等。
- en: The first step is often transcribing the audio into text and then analyzing
    the text itself.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步通常是将音频转录为文本，然后分析文本本身。
- en: MT
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器翻译（MT）
- en: Judging by the history of NLP, I think it is safe to say that translating from
    language A to language B has probably been on the minds of humans for as long
    as we have had to interact with other humans who use a different language. For
    instance, there is even a story in the Bible about the Tower of Babel and we lost
    the ability to understand each other’s words when it was destroyed. MT has so
    many useful implications, for collaboration, security, and even creativity.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 从自然语言处理的历史来看，我认为可以安全地说，从语言A翻译到语言B可能在人类开始与使用不同语言的其他人互动时，就已经成为人类的心思。例如，圣经中甚至有关于巴别塔的故事，说在塔被摧毁时，我们失去了彼此理解对方语言的能力。机器翻译有着许多有用的应用，不仅在合作、保密性上，在创造力方面也有重要意义。
- en: For instance, for collaboration, you need to be able to share knowledge, even
    if team members do not share the same language. In fact, this is useful anywhere
    that sharing knowledge is desirable, so you will often find a **see translation**
    link in social media posts and comments. Today, MT seems almost perfect, though
    there are occasionally entertaining mistakes.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，对于合作来说，你需要能够共享知识，即使团队成员之间不共享相同的语言。事实上，这在任何需要共享知识的地方都非常有用，所以你经常会在社交媒体帖子和评论中看到
    **查看翻译** 的链接。今天，机器翻译（MT）几乎已经完美了，尽管偶尔会有一些有趣的错误。
- en: For security, you want to know what your enemies are planning. Spying is probably
    not very useful if you can’t actually understand what your enemies are saying
    or planning on doing. Translation is a specialized skill and is a long and manual
    process when humans are involved. MT can greatly speed up analysis, as the other
    language can be translated into your own.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在安全领域，你需要知道敌人正在计划什么。如果你根本无法理解敌人正在说什么或打算做什么，那么间谍活动可能就毫无意义。翻译是一项专业技能，当涉及人工翻译时，这通常是一个漫长且手动的过程。机器翻译可以大大加快分析速度，因为另一种语言可以被迅速翻译成你自己的语言。
- en: And for creativity, how fun would it be to convert text from one language into
    your own created language? This is completely doable.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对于创造力来说，将文本从一种语言转化为自己创造的语言是多么有趣啊？这是完全可行的。
- en: Due to the importance of MT and text generation, massive neural networks have
    been trained to handle text generation and MT.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器翻译和文本生成的重要性，庞大的神经网络已经被训练来处理文本生成和机器翻译。
- en: Personal assistants
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 个人助理
- en: Most of us are probably aware of personal assistants such as Alexa and Siri,
    as they have become an integral part of our lives. I suspect we are going to become
    even more dependent on them, and we will eventually talk to our cars like on the
    old TV show *Knight Rider* (broadcast on TV from 1982 to 1986). “*Hey car, drive
    me to the grocery store*” will probably be as common as “*Hey Alexa, what’s the
    weather going to be* *like tomorrow?*”
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大多数人可能已经知道像 Alexa 和 Siri 这样的个人助理，它们已经成为我们生活中的重要组成部分。我猜我们将变得更加依赖这些助手，最终，我们会像在老电视节目
    *霹雳游侠* 中一样与我们的汽车对话（该节目于1982年至1986年间播出）。“*嘿，车子，带我去超市*”可能会像“*嘿 Alexa，明天的天气怎么样？*”一样常见。
- en: Personal assistants use a combination of several NLP techniques previously mentioned.
    They may use classification to determine whether your query is a question or a
    statement. They may then search the internet to find web content most related
    to the question that you asked. It can then capture the raw text from one or more
    of the results and then use summarization to build a concise answer. Finally,
    it will convert text to audio and speak the answer back to the user.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 个人助手结合了前面提到的几种NLP技术。它们可能使用分类技术来判断你的查询是一个问题还是一个陈述。然后，它们可能会在互联网上搜索与你的问题最相关的网页内容。接着，它可以从一个或多个结果中提取原始文本，再使用摘要技术来构建简洁的答案。最后，它会将文本转化为语音，并将答案反馈给用户。
- en: 'Personal assistants use a combination of several NLP techniques mentioned previously:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 个人助手结合了前面提到的几种NLP技术：
- en: They may use classification to determine whether your query is a question or
    a statement.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们可能使用分类技术来判断你的查询是一个问题还是一个陈述。
- en: They may then search the internet to find web content that is most related to
    the question that you asked.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它们可能会在互联网上搜索与你的问题最相关的网页内容。
- en: They can then capture the raw text from one or more of the results and then
    use summarization to build a concise answer.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们可以从一个或多个结果中提取原始文本，再使用摘要技术来构建简洁的答案。
- en: Finally, they will convert text to audio and speak the answer back to the user.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，它们会将文本转化为语音，并将答案反馈给用户。
- en: I am very excited about the future of personal assistants. I would love to have
    a robot and car that I can talk to. I think that creativity is probably our only
    limitation for the different types of personal assistants that we can create or
    for the modules that they use.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我对个人助手的未来感到非常兴奋。我很想拥有一个可以与之对话的机器人和汽车。我认为，创造力可能是我们在创建不同类型的个人助手或它们所使用的模块时唯一的限制。
- en: How can a beginner get started with NLP?
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初学者如何入门自然语言处理（NLP）？
- en: This book will be of little use if we do not eventually jump into how to use
    these tools and technologies. The common and advanced uses that I described here
    are just some of the uses. As you become comfortable with NLP, I want you to constantly
    consider other uses for NLP that are possibly not being met. For instance, in
    text classification alone, you can go very deep. You could use text classification
    to attempt to classify even more difficult concepts, such as sarcasm or empathy,
    for instance, but let’s not get ahead of ourselves. This is what I want you to
    do.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们最终不深入探讨如何使用这些工具和技术，这本书将几乎没有什么用处。我在这里描述的常见和高级应用只是其中的一部分。当你对NLP感到熟悉时，我希望你不断考虑其他可能尚未得到满足的NLP应用。例如，仅仅在文本分类方面，你就可以深入探讨。你可以使用文本分类技术尝试分类更复杂的概念，比如讽刺或共情，但我们暂时不提前讨论这些。这是我希望你做的事。
- en: Start with a simple idea
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从一个简单的想法开始
- en: Think simply, and only add complexity as needed. Think of something that interests
    you that you would like to know more about, and then find people who talk about
    it. If you are interested in photography, find a few Twitter accounts that talk
    about it. If you are looking to analyze political extremism, find a few Twitter
    accounts that proudly show their unifying hashtags. If you are interested in peanut
    allergy research, find a few Twitter accounts of researchers that post their results
    and articles in their quest to save lives. I mention Twitter over and over because
    it is a goldmine for investigating how groups of people talk about issues, and
    people often post links, which can lead to even more scraping. But you could use
    any social media platform, as long as you can scrape it.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地思考，只有在需要时才增加复杂性。想想有什么事情是你感兴趣的，并且想要了解更多的，然后找到讨论这个话题的人。如果你对摄影感兴趣，找几个讨论摄影的Twitter账号。如果你想分析政治极端主义，找一些在Twitter上公开展示其统一标签的账号。如果你对花生过敏研究感兴趣，找一些研究人员的Twitter账号，他们发布自己的研究成果和文章，努力挽救生命。我之所以反复提到Twitter，是因为它是一个研究人们如何讨论问题的宝贵资源，且人们常常会发布链接，这可能引导你进一步抓取更多内容。但你也可以使用任何社交媒体平台，只要它能被抓取。
- en: However, start with a very simple idea. What would you like to know about a
    piece of text (or a lot of Tweets)? What would you like to know about a community
    of people? Brainstorm. Get a notebook and start writing down every question that
    comes to mind. Prioritize them. Then you will have a list of questions to seek
    answers for.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从一个非常简单的想法开始。你想了解一段文本（或大量的推文）中的什么内容？你想了解一个社区的人们的什么情况？头脑风暴一下。拿出一个笔记本，开始写下你脑海中浮现的每一个问题。给它们排个优先级。然后，你就会有一份问题清单，去寻找答案。
- en: For instance, my research question could be, “*What are people saying about
    Black Lives Matter protests?*” Or, we could research something less serious and
    ask, “*What are people saying about the latest Marvel movie?*” Personally, I prefer
    to at least attempt to use data science for good, to make the world a bit safer,
    so I am not very interested in movie reviews, but others are. We all have our
    preferences. Study what interests you.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我的研究问题可能是：“*人们在说关于黑人的命也是命抗议的事情吗？*”或者，我们也可以研究一些不那么严肃的话题，问：“*人们在说关于最新的漫威电影的事吗？*”
    个人来说，我更倾向于至少尝试用数据科学为善，去让世界变得稍微安全一些，所以我对电影评论不太感兴趣，但别人可能会。每个人都有自己的偏好。研究你感兴趣的内容。
- en: 'For this demonstration, I will use my scraped data science feed. I have a few
    starter questions:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个演示，我将使用我抓取的数据科学信息流。我有一些初步的问题：
- en: Which accounts post the most frequently every week?
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些账户每周发布最多？
- en: Which accounts are mentioned the most?
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些账户被提及最多？
- en: Which are the primary hashtags used by this community of people?
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个群体的人们主要使用哪些标签？
- en: What follow-up questions can we think of after answering these questions?
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在回答完这些问题后，我们可以想到哪些后续问题？
- en: We will only use NLP and simple string operations to answer these questions,
    as I have not yet begun to explain social network analysis. I am also going to
    assume that you know your way around Python programming and are familiar with
    the pandas library. I will cover pandas in more detail in a later chapter, but
    I will not be giving in-depth training. There are a few great books that cover
    pandas in depth.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只使用自然语言处理（NLP）和简单的字符串操作来回答这些问题，因为我还没有开始讲解社交网络分析。我还假设你熟悉 Python 编程，并且熟悉 pandas
    库。我将在后面的章节中更详细地讲解 pandas，但不会进行深入的培训。有一些很棒的书籍可以深入讲解 pandas。
- en: 'Here is what the raw data for my scraped data science feed looks like:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我抓取的数据科学信息流的原始数据样式：
- en: '![Figure 1.11 – Scraped and enriched data science Twitter feed](img/B17105_01_011.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.11 – 抓取并丰富的数据科学 Twitter 信息流](img/B17105_01_011.jpg)'
- en: Figure 1.11 – Scraped and enriched data science Twitter feed
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11 – 抓取并丰富的数据科学 Twitter 信息流
- en: To save time, I have set up the regex steps in the scraper to create columns
    for users, tags, and URLs. All of this is scraped or generated as a step during
    automated scraping. This will make it much easier and faster to answer the four
    questions I posed. So, let’s get to it.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省时间，我已经在抓取程序中设置了正则表达式步骤，用来创建用户、标签和 URL 的列。所有这些都是在自动化抓取的过程中被抓取或生成的。这将使得回答我提出的四个问题变得更加容易和迅速。那么，我们开始吧。
- en: Accounts that post most frequently
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发布频率最高的账户
- en: 'The first thing I want to do is see which accounts post the most in total.
    I will also take a glimpse at which accounts post the least to see whether any
    of the accounts have dried up since adding them to my scrapers. For this, I will
    simply take the columns for `publisher` (the account that posted the tweet) and
    `tweet`, do a `groupby` operation on the publisher, and then take the count:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我首先想做的是看看哪些账户总共发布了最多的内容。我还会看看哪些账户发布最少，以检查是否有账户自从被加入到我的抓取程序中后已经停止更新。为此，我将简单地选取`publisher`（发布推文的账户）和`tweet`这两列，对`publisher`进行`groupby`操作，然后进行计数：
- en: '[PRE14]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will display a DataFrame of publishers by tweet count, showing us the
    most active publishers:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示一个按推文数量排序的发布者数据框，向我们展示最活跃的发布者：
- en: '![Figure 1.12 – User tweet counts from the data science Twitter feed](img/B17105_01_012.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.12 – 数据科学 Twitter 信息流中的用户推文数量](img/B17105_01_012.jpg)'
- en: Figure 1.12 – User tweet counts from the data science Twitter feed
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12 – 数据科学 Twitter 信息流中的用户推文数量
- en: That’s awesome. So, if you want to break into data science and you use Twitter,
    then you should probably follow these accounts.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了。如果你想进入数据科学领域，而且你使用 Twitter，那么你应该关注这些账户。
- en: 'However, to me, this is of limited use. I really want to see each account’s
    posting behavior. For this, I will use a pivot table. I will use `publisher` as
    the index, `created_week` as the columns, and run a count aggregation. Here is
    what the top ten looks like, sorted by the current week:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对我来说，这些问题的实用性有限。我真正想看到的是每个账户的发布行为。为此，我将使用数据透视表。我将使用`publisher`作为索引，`created_week`作为列，并进行计数聚合。以下是按当前周排序的前十名：
- en: '[PRE15]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This creates the following DataFrame:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下数据框：
- en: '![Figure 1.13 – Pivot table of user tweet counts by week](img/B17105_01_013.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.13 – 按周的用户推文数量数据透视表](img/B17105_01_013.jpg)'
- en: Figure 1.13 – Pivot table of user tweet counts by week
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13 – 按周的用户推文数量数据透视表
- en: 'This looks much more useful, and it is sensitive to the week. This should also
    be interesting to see as a visualization, to get a feel for the scale:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来更有用，而且对周次敏感。作为可视化，它也应该很有趣，可以感受一下规模：
- en: '[PRE16]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We get the following plot:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下图表：
- en: '![Figure 1.14 – Bar chart of user tweet counts by week](img/B17105_01_014.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.14 – 用户推文按周数的条形图](img/B17105_01_014.jpg)'
- en: Figure 1.14 – Bar chart of user tweet counts by week
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14 – 用户推文按周数的条形图
- en: 'It’s a bit difficult to see individual weeks when visualized like this. With
    any visualization, you will want to think about how you can most easily tell the
    story that you want to tell. As I am mostly interested in visualizing which accounts
    post the most in total, I will use the results from the first aggregation instead.
    This is interesting and cool to look at, but it’s not very useful:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方式可视化时，看到单独的周次有点困难。对于任何可视化，你都需要考虑如何最容易地讲述你想要表达的故事。由于我主要对展示哪些账户的总推文数最多感兴趣，因此我将使用第一次聚合的结果。这看起来很有趣，也很酷，但并不特别实用：
- en: '[PRE17]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This code gives us the following graph:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码给我们以下图表：
- en: '![Figure 1.15 – A bar chart of user tweet counts in total](img/B17105_01_015.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.15 – 用户推文总数的条形图](img/B17105_01_015.jpg)'
- en: Figure 1.15 – A bar chart of user tweet counts in total
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.15 – 用户推文总数的条形图
- en: That is much easier to understand.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这更容易理解。
- en: Accounts mentioned most frequently
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最常被提及的账户
- en: 'Now, I want to see which accounts are mentioned by publishers (the account
    making the tweet) the most often. This can show people who collaborate, and it
    can also show other interesting accounts that are worth scraping. For this, I’m
    just going to use `value_counts` of the top 20 accounts. I want a fast answer:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我想看看哪些账户被发布者（发推文的账户）提及得最频繁。这可以显示合作伙伴，也可以显示其他值得抓取的有趣账户。为此，我将使用`value_counts`来查看前
    20 个账户。我想要一个快速的答案：
- en: '[PRE18]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This looks great. I bet there are some interesting data scientists in this bunch
    of accounts. I should look into that and consider scraping them and adding them
    to my data science feed.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很棒。我敢打赌这些账户中有一些有趣的数据科学家。我应该去看看，并考虑抓取这些账户并将它们添加到我的数据科学信息流中。
- en: Top 10 data science hashtags
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前 10 个数据科学标签
- en: 'Next, I want to see which hashtags are used the most often. The code is going
    to be very similar, other than I need to run `explode()` against the tags field
    in order to create one row for every element of each tweet’s list of hashtags.
    Let’s do that first. For this, we can simply create the DataFrame, drop nulls,
    lowercase the tags for uniformity, and then use `value_counts()` to get what we
    want:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我想看看哪些标签使用得最频繁。代码会非常相似，唯一不同的是，我需要对标签字段运行`explode()`，以便为每个推文的标签列表中的每个元素创建一行。我们先做这个。为此，我们可以简单地创建
    DataFrame，去除空值，将标签小写化以保持一致，然后使用`value_counts()`来得到我们想要的结果：
- en: '[PRE19]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This looks great. I’m going to visualize the top ten results. However, `value_counts()`
    was somehow causing the hashtags to get butchered a bit, so I did a `groupby`
    operation against the DataFrame instead:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很棒。我打算可视化前十名结果。然而，`value_counts()` somehow 使得标签有些损坏，所以我改用了 DataFrame 的 `groupby`
    操作：
- en: '![Figure 1.16 – Hashtag counts from the data science Twitter feed](img/B17105_01_016.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.16 – 数据科学 Twitter 信息流中的标签计数](img/B17105_01_016.jpg)'
- en: Figure 1.16 – Hashtag counts from the data science Twitter feed
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.16 – 数据科学 Twitter 信息流中的标签计数
- en: Let’s finish up this section with a few more related ideas.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一些相关的想法结束这一部分。
- en: Additional questions or action items from simple analysis
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单分析得出的额外问题或行动项
- en: In total, this analysis would have taken me about 10 minutes to do if I was
    not writing a book. The code might seem strange, as you can chain commands together
    in Python. I prefer to have significant operations on their own line so that the
    next person who will have to manage my code will not miss something important
    that was tacked on to the end of a line. However, notebooks are pretty personal,
    and notebook code is not typically written with perfectly clean code. When investigating
    data or doing rough visualizations, focus on what you are trying to do. You do
    not need to write perfect code until you are ready to write the production version.
    That said, do not throw notebook quality code into production.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，如果我不在写书，这个分析大约需要我花费10分钟时间。代码看起来可能有些奇怪，因为你可以在 Python 中将命令链式连接。我更喜欢将重要操作单独放在一行，这样下一个需要管理我代码的人就不会漏掉任何附加在一行末尾的重要内容。然而，笔记本是相当个人化的，笔记本中的代码通常不是写得非常干净。当你在研究数据或进行粗略的可视化时，重点应该放在你要做什么上。直到你准备好编写生产版本的代码之前，你不需要写出完美的代码。话虽如此，不要把笔记本质量的代码直接投入生产环境。
- en: 'Now that we have done the quick analysis, I have some follow-up questions that
    I should look into answering:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了快速分析，我有一些后续问题，应该去回答：
- en: How many of these accounts are actually data science related and that I am not
    already scraping?
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些账号中有多少实际上与数据科学相关，而我没有已经在抓取的？
- en: Do any of these accounts give me ideas for new feeds? For instance, I have feeds
    for data science, disinformation research, art, natural sciences, news, political
    news, politicians, and more. Maybe I should have a photography feed, for instance.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些账号中有哪一些给了我新的推送灵感吗？例如，我有关于数据科学、虚假信息研究、艺术、自然科学、新闻、政治新闻、政治人物等方面的推送。也许我应该增加一个摄影方面的推送。
- en: Would it be worth scraping by keyword for any of the top keywords to harvest
    more interesting content and accounts?
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否值得通过关键词抓取任何一个热门关键词，来收集更多有趣的内容和账号？
- en: Have any of the accounts dried up (no new posts ever)? Which ones? When did
    they dry up? Why did they dry up?
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有任何账号已经停止更新（从未发布新帖子）？是哪些账号？它们何时停止更新的？为什么停止更新？
- en: You try. Do you have any questions you can think of, given this dataset?
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 你试试看。你能从这个数据集中想到任何问题吗？
- en: 'Next, let’s try something similar but slightly different, using NLP tools against
    the book *Alice’s Adventures in Wonderland*. Specifically, I want to see whether
    I can take the `tf-idf` vectors and plot out character appearance by chapter.
    If you are unfamiliar with it, **term frequency-inverse of document frequency**
    (**TF-IDF**) is an appropriate name because that is exactly the math. I won’t
    go into the code, but this is what the results look like:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们尝试一个相似但稍微不同的方法，使用自然语言处理工具分析《爱丽丝梦游仙境》这本书。具体来说，我想看看是否能将 `tf-idf` 向量化并绘制出每章中角色的出现情况。如果你不熟悉的话，**词频-逆文档频率**（**TF-IDF**）这个名称非常合适，因为这正是数学原理。我不会讲解代码，但这就是结果的展示：
- en: '![Figure 1.17 – TF-IDF character visualization of Alice’s Adventures in Wonderland
    by book chapter](img/B17105_01_017.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.17 – 基于书籍章节的《爱丽丝梦游仙境》TF-IDF人物可视化](img/B17105_01_017.jpg)'
- en: Figure 1.17 – TF-IDF character visualization of Alice’s Adventures in Wonderland
    by book chapter
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.17 – 基于书籍章节的《爱丽丝梦游仙境》TF-IDF人物可视化
- en: By using a stacked bar chart, I can see which characters appear together in
    the same chapters, as well as their relative importance based on the frequency
    with that they were named. This is completely automated, and I think it would
    allow for some very interesting applications, such as a more interactive way of
    researching various books. In the next chapter, I will introduce social network
    analysis, and if you were to add that in as well, you could even build the social
    network of Alice in Wonderland, or any other piece of literature, allowing you
    to see which characters interact.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用堆叠条形图，我可以看到哪些角色在同一章节中一起出现，以及它们的相对重要性，基于它们被提到的频率。这完全是自动化的，我认为这将带来一些非常有趣的应用，比如一种更互动的方式来研究各种书籍。在下一章中，我将介绍社交网络分析，如果你也加入这一部分，你甚至可以构建《爱丽丝梦游仙境》或任何其他文学作品中的社交网络，从而看到哪些角色之间的互动。
- en: 'In order to perform a `tf-idf` vectorization, you need to split sentences apart
    into tokens. Tokenization is NLP 101 stuff, with a token being a word. So, for
    instance, if we were to tokenize this sentence:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行 `tf-idf` 向量化，你需要将句子分割成词汇单元。分词是自然语言处理（NLP）的基础，词汇单元就是一个词。所以，比如说，如果我们要分词这个句子：
- en: Today was a wonderful day.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 今天是美好的一天。
- en: 'I would end up with a list of the following tokens:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我最终会得到以下词元的列表：
- en: '`[''Today'', ''was'', ''a'', ''wonderful'', ''``day'', ''.'']`'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`[''今天'', ''是'', ''一个'', ''美好'', ''的'', ''``一天'', ''.'']`'
- en: If you have a collection of several sentences, you can then feed it to `tf-idf`
    to return the relative importance of each token in a corpus of text. This is often
    very useful for text classification using simpler models, and it can also be used
    as input for topic modeling or clustering. However, I have never seen anyone else
    use it to determine character importance by book chapters, so that’s a creative
    approach.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有几个句子，你可以将它们输入到 `tf-idf` 中，以返回文本语料库中每个词元的相对重要性。这通常对使用简单模型进行文本分类非常有用，也可以作为主题建模或聚类的输入。然而，我从未见过其他人使用它来按书籍章节确定角色的重要性，所以这是一个创造性的做法。
- en: This example only scratches the surface of what we can do with NLP and investigates
    only a few of the questions we could come up with. As you do your own research,
    I encourage you to keep a paper notebook handy, so that you can write down questions
    to investigate whenever they come to mind.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子只是 NLP 所能做的一小部分，它只探讨了我们可能提出的几个问题。当你进行自己的研究时，我鼓励你随时保持一个纸质笔记本，这样当有问题冒出来时，你可以随时记录下来进行调查。
- en: Summary
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered what NLP is, how it has helped me, some common and
    advanced uses of NLP, and how a beginner can get started.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了什么是 NLP，它是如何帮助我的，NLP 的一些常见和高级应用，以及初学者如何入门。
- en: I hope this chapter gave you a rough idea of what NLP is, what it can be used
    for, what text analysis looks like, and some resources for more learning. This
    chapter is in no way the complete picture of NLP. It was difficult to even write
    the history, as there is just so much to the story and so much that has been forgotten
    over time.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这一章能给你一个大致的概念，了解什么是 NLP，它可以用于什么，文本分析是什么样的，以及一些可以进一步学习的资源。这一章绝不是 NLP 的完整图景。即使是写历史部分也很困难，因为其中有太多内容，而且很多已经随着时间的推移被遗忘了。
- en: Thank you for reading. This is my first time writing a book, and this is my
    first chapter ever written for a book, so this really means a lot to me! I hope
    you are enjoying this so far, and I hope I gave you an adequate first look into
    NLP.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。这是我第一次写书，也是我第一次为书写的章节，所以这对我来说意义重大！希望到目前为止你喜欢这本书，并且我希望我能给你提供一个充分的初步了解 NLP
    的机会。
- en: 'Next up: network science and social network analysis!'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来：网络科学和社交网络分析！
