- en: Chapter 11.  Building Data Science Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。构建数据科学应用
- en: Data science applications are garnering a lot of excitement, mainly because
    of the promise they hold in harnessing data and extracting consumable results.
    There are already several successful data products that have had a transformative
    effect on our daily lives. The ubiquitous recommender systems, e-mail spam filters,
    and targeted advertisements and news content have become part and parcel of life.
    Music and movies have become data products streaming from providers such as iTunes
    and Netflix. Businesses, especially in the domains such as retail, are actively
    pursuing ways to gain a competitive advantage by studying the market and customer
    behavior using a data-driven approach.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学应用引起了很多兴奋，主要是因为它们在利用数据和提取可消费的结果方面的承诺。已经有几个成功的数据产品对我们的日常生活产生了变革性影响。无处不在的推荐系统、电子邮件垃圾邮件过滤器、定向广告和新闻内容已经成为生活的一部分。音乐和电影已经成为来自iTunes和Netflix等提供商的数据产品。企业，特别是在零售等领域，正在积极寻求通过使用数据驱动的方法来研究市场和客户行为以获得竞争优势的方式。
- en: We have discussed the data analytics workflow up to the model building phase
    so far in the previous chapters. But the real value of a model is when it is actually
    deployed in a production system. The end product, the fruit of a data science
    workflow, is an operationalized data product. In this chapter, we discuss this
    culminating stage of the data analytics workflow. We will not get into actual
    code snippets but take a step back to get the complete picture, including the
    non-technical aspects.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在前几章中，我们已经讨论了数据分析工作流程直到模型构建阶段。但是模型真正的价值在于它实际部署到生产系统中。数据科学工作流的最终产品是一个操作化的数据产品。在本章中，我们将讨论数据分析工作流的这个最终阶段。我们不会涉及实际的代码片段，而是退一步，获取完整的画面，包括非技术方面。
- en: The complete picture is not limited to the development process alone. It comprises
    the user application, developments in Spark itself, as well as rapid changes happening
    in the big data landscape. We'll start with the development process of the user
    application first and discuss various options at each stage. Then we'll delve
    into the features and enhancements in the latest Spark 2.0 release and future
    plans. Finally, we'll attempt to give a broad overview of the big data trends,
    especially the Hadoop ecosystem. References and useful links are included in individual
    sections in addition to the end of the chapter for further information about the
    specific context.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的画面不仅仅局限于开发过程本身。它包括用户应用程序、Spark本身的发展，以及大数据领域正在发生的快速变化。我们将首先从用户应用程序的开发过程开始，讨论每个阶段的各种选项。然后我们将深入探讨最新的Spark
    2.0版本和未来计划中的功能和增强功能。最后，我们将尝试对大数据趋势，特别是Hadoop生态系统，进行广泛的概述。参考资料和有用的链接将包括在各个部分中，以及章节末尾，以获取有关特定上下文的更多信息。
- en: Scope of development
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发范围
- en: 'Data analytics workflow can be roughly divided into two phases, the build phase
    and the operationalization phase. The first phase is usually a one-time exercise,
    with heavy human intervention. Once we''ve attained reasonable end results, we
    are ready to operationalize the product. The second phase starts with the models
    generated in the first phase and makes them available as a part of some production
    workflow. In this section, we''ll discuss the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析工作流程大致可以分为两个阶段，即构建阶段和操作化阶段。第一阶段通常是一次性的练习，需要大量人为干预。一旦我们获得了合理的最终结果，我们就准备好将产品操作化。第二阶段从第一阶段生成的模型开始，并将其作为某个生产工作流程的一部分提供。在本节中，我们将讨论以下内容：
- en: Expectations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期望
- en: Presentation options
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示选项
- en: Development and testing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发和测试
- en: Data quality management
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量管理
- en: Expectations
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 期望
- en: 'The primary goal of data science applications is to build "actionable" insights,
    actionable being the keyword. Many use cases such as fraud detection need the
    insights to be generated and made available in a consumable fashion in near real
    time, if you expect any action-ability at all. The end users of the data product
    vary with the use case. They may be customers of an e-commerce site or a decision
    maker of a major conglomerate. The end user need not always be a human being.
    It could be a risk assessment software tool in a financial institution. A one-size-fits-all
    approach does not fit in with many software products, and data products are no
    exception. However, there are some common expectations for data products, as listed
    here:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学应用的主要目标是构建“可操作”的洞察力，可操作是关键词。许多用例，如欺诈检测，需要以可消费的方式生成并在几乎实时中提供洞察力，如果您期望有任何可操作性。数据产品的最终用户因用例而异。他们可能是电子商务网站的客户，也可能是大型企业的决策者。最终用户不一定总是人类。它可能是金融机构中的风险评估软件工具。一刀切的方法并不适用于许多软件产品，数据产品也不例外。然而，对于数据产品，有一些共同的期望，如下所列：
- en: The first and foremost expectation is that the insight generation time frame
    based on real-world data should be within "actionable" timeframes. The actual
    time frame varies based on the use case.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首要和最重要的期望是基于现实世界数据的洞察力生成时间应该在“可操作”的时间范围内。实际时间范围根据用例而异。
- en: The data product should integrate into some (often already existing) production
    workflow.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据产品应该整合到一些（通常已经存在的）生产工作流程中。
- en: The insights should be translated into something that people can use instead
    of obscure numbers or hard-to-interpret charts. The presentation should be unobtrusive.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 洞察力应该被转化为人们可以使用的东西，而不是晦涩的数字或难以解释的图表。演示应该是不显眼的。
- en: The data product should have the ability to fine-tune itself (self-adapting)
    based on the incoming data inputs.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据产品应该能够根据输入的数据自我调整（自适应）。
- en: Ideally, there has to be some way to receive human feedback, which can be used
    as a source for self-tuning.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理想情况下，应该有一种方式来接收人类反馈，这可以作为自我调整的来源。
- en: There should be a mechanism that quantitatively assesses its effectiveness periodically
    and automatically.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该有一个定期和自动地定量评估其有效性的机制。
- en: Presentation options
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演示选项
- en: The varied nature of data products calls for varied modes of presentation. Sometimes
    the end result of a data analytics exercise is to publish a research paper. Sometimes
    it could be a part of a dashboard, where this becomes one of several sources publishing
    results on a single web page. They may be overt and targeted for human consumption,
    or covert and feeding into some other software application. You may use a general-purpose
    engine such as Spark to build your solution, but the presentation must be highly
    aligned to the targeted user base.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据产品的多样性要求多样的展示方式。有时，数据分析练习的最终结果是发表研究论文。有时它可能是仪表板的一部分，其中它成为单个网页上发布结果的几个来源之一。它们可能是公开的，针对人类消费，也可能是隐蔽的，供其他软件应用程序使用。您可以使用通用引擎，如Spark来构建解决方案，但展示必须与目标用户群高度对齐。
- en: Sometimes all you need to do is write an e-mail with your findings or just export
    a CSV file of insights. Or you may have to develop a dedicated web application
    around your data product. Some other common options are discussed here, and you
    have to choose the right one that fits the problem on hand.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您只需要写一封电子邮件，附上您的发现，或者只是导出一个洞察力的CSV文件。或者您可能需要围绕您的数据产品开发一个专用的Web应用程序。这里讨论了一些其他常见的选项，您必须选择适合手头问题的正确选项。
- en: Interactive notebooks
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交互式笔记本
- en: Interactive notebooks are web applications that allow you to create and share
    documents that contain code chunks, results, equations, images, videos, and explanation
    text. They may be viewed as executable documents or REPL shells with visualization
    and equation support. These documents can be exported as PDFs, Markdown, or HTML.
    Notebooks contain several "kernels" or "computational engines" that execute code
    chunks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式笔记本是允许您创建和共享包含代码块、结果、方程、图像、视频和解释文本的文档的Web应用程序。它们可以被视为可执行文档或带有可视化和方程支持的REPL
    shell。这些文档可以导出为PDF、Markdown或HTML。笔记本包含多个“内核”或“计算引擎”来执行代码块。
- en: Interactive notebooks are the most suitable choice if the end goal of your data
    analytics workflow is to generate a written report. There are several notebooks
    and many of them have Spark support. These notebooks are useful tools during the
    exploration phase also. We have already introduced IPython and Zeppelin notebooks
    in previous chapters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据分析工作流的最终目标是生成书面报告，交互式笔记本是最合适的选择。有几种笔记本，其中许多都支持Spark。这些笔记本在探索阶段也是有用的工具。我们在之前的章节中已经介绍了IPython和Zeppelin笔记本。
- en: References
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'The IPython Notebook: A Comprehensive Tool for Data Science: [http://conferences.oreilly.com/strata/strata2013/public/schedule/detail/27233](http://conferences.oreilly.com/strata/strata2013/public/schedule/detail/27233)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IPython Notebook：数据科学的综合工具：[http://conferences.oreilly.com/strata/strata2013/public/schedule/detail/27233](http://conferences.oreilly.com/strata/strata2013/public/schedule/detail/27233)
- en: 'Sparkly Notebook: Interactive Analysis and Visualization with Spark: [http://www.slideshare.net/felixcss/sparkly-notebook-interactive-analysis-and-visualization-with-spark](http://www.slideshare.net/felixcss/sparkly-notebook-interactive-analysis-and-visualization-with-spark)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sparkly Notebook：使用Spark进行交互式分析和可视化：[http://www.slideshare.net/felixcss/sparkly-notebook-interactive-analysis-and-visualization-with-spark](http://www.slideshare.net/felixcss/sparkly-notebook-interactive-analysis-and-visualization-with-spark)
- en: Web API
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Web API
- en: 'An **Application Programming Interface** (**API**) is a software-to-software
    interface; a specification that describes the available functionality, how it
    must be used, and what the inputs and outputs are. The software (service) provider
    exposes some of its functionality as an API. A developer may develop a software
    component that consumes this API. For example, Twitter offers APIs to get or post
    data onto Twitter or to query data programmatically. A Spark enthusiast may write
    a software component that automatically collects all tweets on #Spark, categorizes
    according to their requirements, and publishes that data on their personal website.
    Web APIs are a type of APIs where the interface is defined as a set of **Hypertext
    Transfer Protocol** (**HTTP**) request messages along with a definition of the
    structure of response messages. Nowadays REST-ful (Representational State Transfer)
    have become the de facto standard.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用程序编程接口**（**API**）是软件到软件的接口；描述可用功能、如何使用以及输入和输出的规范。软件（服务）提供者将其部分功能公开为API。开发人员可以开发一个消耗此API的软件组件。例如，Twitter提供API以在Twitter上获取或发布数据，或以编程方式查询数据。Spark爱好者可以编写一个软件组件，自动收集所有关于#Spark的推文，根据他们的要求进行分类，并在其个人网站上发布这些数据。Web
    API是API的一种类型，其中接口被定义为一组**超文本传输协议**（**HTTP**）请求消息以及响应消息结构的定义。如今，REST-ful（表述性状态转移）已成为事实上的标准。'
- en: You can implement your data product as an API, and perhaps this is the most
    powerful option. It can then be plugged into one or more applications, say the
    management dashboard as well as the marketing analytics workflow. You may develop
    a domain specific "insights-as-a-service" as a public Web API with a subscription
    model. The simplicity and ubiquity of Web APIs make them the most compelling choice
    for building data products.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将数据产品实现为API，也许这是最强大的选择。然后可以将其插入一个或多个应用程序，比如管理仪表板以及营销分析工作流。您可以开发一个特定领域的“见解即服务”作为公共Web
    API，并采用订阅模式。Web API的简单性和普遍性使其成为构建数据产品的最具吸引力的选择。
- en: References
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 参考资料
- en: Application programming interface: [https://en.wikipedia.org/wiki/Application_programming_interface](https://en.wikipedia.org/wiki/Application_programming_interface)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序编程接口：[https://en.wikipedia.org/wiki/Application_programming_interface](https://en.wikipedia.org/wiki/Application_programming_interface)
- en: Ready for APIs? Three steps to unlock the data economy's most promising channel:[http://www.forbes.com/sites/mckinsey/2014/01/07/ready-for-apis-three-steps-to-unlock-the-data-economys-most-promising-channel/#61e7103b89e5](http://www.forbes.com/sites/mckinsey/2014/01/07/ready-for-apis-three-steps-to-unlock-the-data-economys-most-promising-channel/#61e7103b89e5)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备好使用API了吗？解锁数据经济最有前途的渠道的三个步骤：[http://www.forbes.com/sites/mckinsey/2014/01/07/ready-for-apis-three-steps-to-unlock-the-data-economys-most-promising-channel/#61e7103b89e5](http://www.forbes.com/sites/mckinsey/2014/01/07/ready-for-apis-three-steps-to-unlock-the-data-economys-most-promising-channel/#61e7103b89e5)
- en: How Insights-as-a-service is growing based on big data: [http://www.kdnuggets.com/2015/12/insights-as-a-service-big-data.html](http://www.kdnuggets.com/2015/12/insights-as-a-service-big-data.html)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于大数据增长的洞察作为服务：[http://www.kdnuggets.com/2015/12/insights-as-a-service-big-data.html](http://www.kdnuggets.com/2015/12/insights-as-a-service-big-data.html)
- en: PMML and PFA
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PMML和PFA
- en: Sometimes you may have to expose your model in a way that other data mining
    tools can understand. The model and the complete pre- and post-processing steps
    should be converted into a standard format. PMML and PFA are two such standard
    formats in the data mining domain.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您可能需要以其他数据挖掘工具能够理解的方式公开您的模型。模型和完整的预处理和后处理步骤应转换为标准格式。PMML和PFA是数据挖掘领域的两种标准格式。
- en: '**Predictive Model Markup Language** (**PMML**) is an XML-based predictive
    model interchange format and Apache Spark API convert models into PMML out of
    the box. A PMML message may contain a myriad of data transformations as well as
    one or more predictive models. Different data mining tools can export or import
    PMML messages without the need for custom code.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测模型标记语言**（**PMML**）是基于XML的预测模型交换格式，Apache Spark API可以将模型转换为PMML。 PMML消息可能包含多种数据转换以及一个或多个预测模型。不同的数据挖掘工具可以导出或导入PMML消息，无需自定义代码。'
- en: '**Portable Format for Analytics** (**PFA**) is the next generation of predictive
    model interchange format. It exchanges JSON documents and straightaway inherits
    all advantages of JSON documents as against XML documents. In addition, PFA is
    more flexible than PMML.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**Analytics的便携格式**（**PFA**）是下一代预测模型交换格式。它交换JSON文档，并立即继承JSON文档相对于XML文档的所有优势。此外，PFA比PMML更灵活。'
- en: References
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'PMML FAQ: Predictive Model Markup Language: [http://www.kdnuggets.com/2013/01/pmml-faq-predictive-model-markup-language.html](http://www.kdnuggets.com/2013/01/pmml-faq-predictive-model-markup-language.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PMML FAQ：预测模型标记语言：[http://www.kdnuggets.com/2013/01/pmml-faq-predictive-model-markup-language.html](http://www.kdnuggets.com/2013/01/pmml-faq-predictive-model-markup-language.html)
- en: 'Portable Format for Analytics: moving models to production: [http://www.kdnuggets.com/2016/01/portable-format-analytics-models-production.html](http://www.kdnuggets.com/2016/01/portable-format-analytics-models-production.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Analytics的便携格式：将模型移至生产：[http://www.kdnuggets.com/2016/01/portable-format-analytics-models-production.html](http://www.kdnuggets.com/2016/01/portable-format-analytics-models-production.html)
- en: What is PFA for?: [http://dmg.org/pfa/docs/motivation/](http://dmg.org/pfa/docs/motivation/)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PFA的用途是什么？：[http://dmg.org/pfa/docs/motivation/](http://dmg.org/pfa/docs/motivation/)
- en: Development and testing
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发和测试
- en: 'Apache Spark is a general-purpose cluster computing system that can run both
    by itself or over several existing cluster managers such as Apache Mesos, Hadoop,
    Yarn, and Amazon EC2\. In addition, several big data and enterprise software companies
    have already integrated Spark into their offerings: Microsoft Azure HDInsight,
    Cloudera, IBM Analytics for Apache Spark, SAP HANA, and the list goes on. Databricks,
    a company founded by the creators of Apache Spark, have their own product for
    data science workflow, from ingestion to production. Your responsibility is to
    understand your organizational requirements and existing talent pool and decide
    which option is the best for you.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一种通用的集群计算系统，可以独立运行，也可以在几个现有的集群管理器上运行，如Apache Mesos、Hadoop、Yarn和Amazon
    EC2。此外，一些大数据和企业软件公司已经将Spark集成到其产品中：Microsoft Azure HDInsight、Cloudera、IBM Analytics
    for Apache Spark、SAP HANA等等。由Apache Spark的创始人创立的Databricks公司拥有自己的产品，用于数据科学工作流程，从数据摄取到生产。您的责任是了解组织的要求和现有的人才储备，并决定哪个选项对您最好。
- en: Regardless of the option chosen, follow the usual best practices in any software
    development life cycle, such as version control and peer reviews. Try to use high-level
    APIs wherever applicable. The data transformation pipelines used in production
    should be the same as the ones used in building the model. Document any questions
    that arise during the data analytics workflow. Often these may result in business
    process improvements.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选择哪个选项，都要遵循任何软件开发生命周期中的通常最佳实践，例如版本控制和同行审查。尽量在适用的地方使用高级API。生产中使用的数据转换流水线应与构建模型时使用的流水线相同。记录在数据分析工作流程中出现的任何问题。通常这些问题可能导致业务流程改进。
- en: 'As always, testing is extremely important for the success of your product.
    You have to maintain a set of automated scripts that give easy-to-understand results.
    The test cases should cover the following at the minimum:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，测试对于产品的成功非常重要。您必须维护一组自动化脚本，以提供易于理解的结果。测试用例应至少涵盖以下内容：
- en: Adherence to timeframe and resource consumption requirements
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遵守时间框架和资源消耗要求
- en: Resilience to bad data (for example, data type violations)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对不良数据的弹性（例如，数据类型违规）
- en: New value in a categorical feature that was not encountered during the model
    building phase
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型构建阶段未遇到的分类特征中的新值
- en: Very little data or too heavy data that is expected in the target production
    system
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在目标生产系统中预期的非常少的数据或过重的数据
- en: 'Monitor logs, resource utilization, and so on to uncover any performance bottlenecks.
    The Spark UI provides a wealth of information to monitor Spark applications. The
    following are some common tips that will help you improve performance:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 监视日志、资源利用率等，以发现任何性能瓶颈。Spark UI提供了大量信息来监视Spark应用程序。以下是一些常见的提示，将帮助您提高性能：
- en: Cache any input or intermediate data that might be used multiple times.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存可能多次使用的任何输入或中间数据。
- en: Look at the Spark UI and identify jobs that are causing a lot of shuffle. Check
    the code and see whether you can reduce the shuffles.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看Spark UI并识别导致大量洗牌的作业。检查代码，看看是否可以减少洗牌。
- en: Actions may transfer the data from workers to the driver. See that you are not
    transferring any data that is not absolutely necessary.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作可能会将数据从工作节点传输到驱动程序。请注意，您不要传输任何绝对不必要的数据。
- en: "Stragglers; that run slower than others; \x94may increase the overall job completion\
    \ time. There may be several reasons for a straggler. If a job is running slow\
    \ due to a slow node, you may set `spark.speculation` to `true`. Then Spark automatically\
    \ relaunches such a task on a different node. Otherwise, you may have to revisit\
    \ the logic and see whether it can be improved."
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stragglers；比其他任务运行速度慢；可能会增加整体作业完成时间。出现任务运行缓慢可能有几个原因。如果作业因为一个慢节点而运行缓慢，您可以将`spark.speculation`设置为`true`。然后Spark会自动在不同节点上重新启动这样的任务。否则，您可能需要重新审视逻辑，看看是否可以改进。
- en: References
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考
- en: Investigating Spark's performance: [http://radar.oreilly.com/2015/04/investigating-sparks-performance.html](http://radar.oreilly.com/2015/04/investigating-sparks-performance.html)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调查Spark的性能：[http://radar.oreilly.com/2015/04/investigating-sparks-performance.html](http://radar.oreilly.com/2015/04/investigating-sparks-performance.html)
- en: Tuning and Debugging in Apache Spark by Patrick Wendell: [https://sparkhub.databricks.com/video/tuning-and-debugging-apache-spark/](https://sparkhub.databricks.com/video/tuning-and-debugging-apache-spark/)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patrick Wendell的《Apache Spark的调整和调试》：[https://sparkhub.databricks.com/video/tuning-and-debugging-apache-spark/](https://sparkhub.databricks.com/video/tuning-and-debugging-apache-spark/)
- en: How to tune your Apache Spark jobs: http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/ and
    part 2
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何调整Apache Spark作业：http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/和part
    2
- en: Data quality management
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量管理
- en: At the outset, let's not forget that we are trying to build fault-tolerant software
    data products from unreliable, often unstructured, and uncontrolled data sources.
    So data quality management gains even more importance in a data science workflow.
    Sometimes the data may solely come from controlled data sources, such as automated
    internal process workflows in an organization. But in all other cases, you need
    to carefully craft your data cleansing processes to protect the subsequent processing.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们不要忘记，我们正在尝试从不可靠、通常是非结构化和不受控的数据源构建容错的软件数据产品。因此，在数据科学工作流程中，数据质量管理变得更加重要。有时数据可能仅来自受控数据源，例如组织中的自动化内部流程工作流。但在所有其他情况下，您需要仔细制定数据清洗流程，以保护后续处理。
- en: Metadata consists of the structure and meaning of data, and obviously the most
    critical repository to work with. It is the information about the structure of
    individual data sources and what each component in that structure means. You may
    not always be able to write some script and extract this data. A single data source
    may contain data with different structures or an individual component (column)
    may mean different things during different times. A label such as owner or high
    may mean different things in different data sources. Collecting and understanding
    all such nuances and documenting is a tedious, iterative task. Standardization
    of metadata is a prerequisite to data transformation development.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据包括数据的结构和含义，显然是要处理的最关键的存储库。它是关于单个数据源结构和该结构中每个组件含义的信息。您可能并不总是能够编写一些脚本并提取这些数据。单个数据源可能包含具有不同结构的数据，或者单个组件（列）在不同时间可能意味着不同的事情。例如，标签如所有者或高在不同的数据源中可能意味着不同的事情。收集和理解所有这样的细微差别并记录是一项繁琐的迭代任务。元数据的标准化是数据转换开发的先决条件。
- en: 'Some broad guidelines that are applicable to most use cases are listed here:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于大多数用例的一些广泛指导原则列在这里：
- en: All data sources must be versioned and timestamped
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有数据源必须进行版本控制和时间戳标记
- en: Data quality management processes often require involvement of the highest authorities
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量管理过程通常需要最高管理机构的参与
- en: Mask or anonymize sensitive data
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 屏蔽或匿名化敏感数据
- en: One important step that is often missed out is to maintain traceability; a link
    between each data element (say a row) and its original source
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经常被忽视的一个重要步骤是保持可追溯性；每个数据元素（比如一行）与其原始来源之间的链接
- en: The Scala advantage
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scala的优势
- en: Apache Spark allows you to write applications in Python, R, Java, or Scala.
    With this flexibility comes the responsibility of choosing the right language
    for your requirements. But regardless of your usual language of choice, you may
    want to consider Scala for your Spark-powered application. In this section, we
    will explain why.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark允许您使用Python、R、Java或Scala编写应用程序。这种灵活性带来了选择适合您需求的正确语言的责任。但无论您通常选择的语言是什么，您可能会考虑为您的Spark应用程序选择Scala。在本节中，我们将解释为什么。
- en: Let's digress to gain a high-level understanding of imperative and functional
    programming paradigms first. Languages such as C, Python, and Java belong to the
    imperative programming paradigm. In the imperative programming paradigm, a program
    is a sequence of instructions and it has a program state. The program state is
    usually represented as a set of variables and their values at any given point
    in time. Assignments and reassignments are fairly common. Variable values are
    expected to change over the period of execution by one or more functions. Variable
    value modification in a function is not limited to local variables. Global variables
    and public class variables are some examples of such variables.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们离题一下，以便对命令式和函数式编程范式有一个高层次的理解。像C、Python和Java这样的语言属于命令式编程范式。在命令式编程范式中，程序是一系列指令，它有一个程序状态。程序状态通常表示为一组变量及其在任何给定时间点的值。赋值和重新赋值是相当常见的。变量的值预计会在执行期间由一个或多个函数改变。函数中的变量值修改不仅限于局部变量。全局变量和公共类变量是这些变量的一些例子。
- en: In contrast, programs written in functional programming languages such as Erlang
    can be viewed as stateless expression evaluators. Data is immutable. If a function
    is called with the same set of input arguments, then it is expected to produce
    the same result (that is, referential transparency). This is possible due to the
    absence of interference from a variable context in the form of global variables
    and the like. This implies that the sequence of function evaluation is of little
    importance. Functions can be passed as arguments to other functions. Recursive
    calls replace loops. The absence of state makes parallel programming much easier
    because it eliminates the need for locking and possible deadlocks. Coordination
    gets simplified when the execution order is less important. These factors make
    the functional programming paradigm a neat fit for parallel programming.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，使用函数式编程语言编写的程序可以被视为无状态的表达式求值器。数据是不可变的。如果一个函数使用相同的输入参数集合调用，那么预期会产生相同的结果（即引用透明）。这是由于全局变量等变量上下文的干扰的缺失。这意味着函数求值的顺序并不重要。函数可以作为参数传递给其他函数。递归调用取代了循环。无状态使得并行编程更容易，因为它消除了锁定和可能的死锁的需要。当执行顺序不那么重要时，协调变得更加简单。这些因素使得函数式编程范式非常适合并行编程。
- en: Pure functional programming languages are hard to work with because most of
    the programs require state changes. Most functional programming languages, including
    good old Lisp, do allow storing of data in variables (side-effects). Some languages
    such as Scala draw from multiple programming paradigms.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 纯函数式编程语言很难使用，因为大多数程序需要状态改变。大多数函数式编程语言，包括老式的Lisp，都允许在变量中存储数据（副作用）。一些语言，如Scala，汲取了多种编程范式。
- en: Returning to Scala, it is a JVM-based, statically typed multi-paradigm programming
    language. Its built-in-type inference mechanism allows programmers to omit some
    redundant type information. This gives a feel of the flexibility offered by dynamic
    languages while retaining the robustness of better compile time checks and fast
    runtime. Scala is an object-oriented language in the sense that every value is
    an object, including numerical values. Functions are first-class objects, which
    can be used as any data type, and they can be passed as arguments to other functions.
    Scala interoperates well with Java and its tools because Scala runs on JVM. Java
    and Scala classes can be freely mixed. That implies that Scala can easily interact
    with the Hadoop ecosystem.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 回到Scala，它是一种基于JVM的、静态类型的多范式编程语言。它的内置类型推断机制允许程序员省略一些冗余的类型信息。这给人一种动态语言所提供的灵活性的感觉，同时保留了更好的编译时检查和快速运行时的健壮性。Scala是一种面向对象的语言，因为每个值都是一个对象，包括数值值。函数是一级对象，可以用作任何数据类型，并且可以作为参数传递给其他函数。Scala与Java及其工具很好地互操作，因为Scala在JVM上运行。Java和Scala类可以自由混合。这意味着Scala可以轻松地与Hadoop生态系统进行交互。
- en: All of these factors should be taken into account when you choose the right
    programming language for your application.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适合你的应用程序的编程语言时，所有这些因素都应该被考虑进去。
- en: Spark development status
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的开发状态
- en: Apache Spark has become the most currently active project in the Hadoop ecosystem
    in terms of the number of contributors by the end of 2015\. Having started as
    a research project at UC Berkeley AMPLAB in 2009, Spark is still relatively young
    when compared to projects such as Apache Hadoop and is still in active development.
    There were three releases in the year 2015, from 1.3 through 1.5, packed with
    features such as DataFrames API, SparkR, and Project Tungsten respectively. Version
    1.6 was released in early 2016 and included the new Dataset API and expansion
    of data science functionality. Spark 2.0 was released in July 2016, and this being
    a major release has a lot of new features and enhancements that deserve a section
    of their own.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark已成为截至2015年底Hadoop生态系统中活跃度最高的项目，以贡献者数量计算。Spark始于2009年的加州大学伯克利分校AMPLAB的一个研究项目，与Apache
    Hadoop等项目相比，Spark仍然相对年轻，并且仍在积极开发中。2015年有三个版本发布，从1.3到1.5，分别包含了DataFrames API、SparkR和Project
    Tungsten等功能。1.6版本于2016年初发布，包括新的Dataset API和数据科学功能的扩展。Spark 2.0于2016年7月发布，作为一个重大版本，具有许多新功能和增强功能，值得单独介绍。
- en: Spark 2.0's features and enhancements
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 2.0的功能和增强
- en: Apache Spark 2.0 included three major new features and several other performance
    improvements and under-the-hood changes. This section attempts to give a high-level
    overview yet step into the details to give a conceptual understanding wherever
    required.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 2.0包括三个主要的新功能和其他性能改进和底层更改。本节试图给出一个高层次的概述，同时在必要时深入细节，以便理解概念。
- en: Unifying Datasets and DataFrames
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 统一Datasets和DataFrames
- en: DataFrames are high-level APIs that support a data abstraction conceptually
    equivalent to a table in a relational database or a DataFrame in R and Python
    (the pandas library). Datasets are an extension of the DataFrame API that provide
    a type-safe, object-oriented programming interface. Datasets add static types
    to DataFrames. Defining a structure on top of DataFrames provides information
    to the core that enables optimizations. It also helps in catching analysis errors
    early on, even before a distributed job starts.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames是支持数据抽象概念上等同于关系数据库中的表或R和Python中的DataFrame（pandas库）的高级API。Datasets是DataFrame
    API的扩展，提供了一个类型安全的、面向对象的编程接口。Datasets为DataFrames添加了静态类型。在DataFrames之上定义结构提供了信息给核心，从而实现了优化。它还有助于在分布式作业开始之前及早捕捉分析错误。
- en: RDDs, Datasets, and DataFrames are interchangeable. RDDs continue to be the
    low-level API. DataFrames, Datasets, and SQL share the same optimization and execution
    pipeline. Machine learning libraries take either DataFrames or Datasets. Both
    DataFrames and Datasets run on Tungsten, an initiative to improve runtime performance.
    They leverage Tungsten's fast in-memory encoding, which is responsible for converting
    between JVM objects and Spark's internal representation. The same APIs work on
    streams also, introducing the concept of continuous DataFrames.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: RDD、Dataset和DataFrame是可互换的。RDD仍然是低级API。DataFrame、Dataset和SQL共享相同的优化和执行管道。机器学习库可以使用DataFrame或Dataset。DataFrame和Dataset都在钨上运行，这是一个改进运行时性能的倡议。它们利用钨的快速内存编码，负责在JVM对象和Spark内部表示之间进行转换。相同的API也适用于流，引入了连续DataFrame的概念。
- en: Structured Streaming
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化流
- en: Structure Streaming APIs are high-level APIs that are built on the Spark SQL
    engine and extend DataFrames and Datasets. Structured Streaming unifies streaming,
    interactive, and batch queries. In most use cases, streaming data needs to be
    combined with batch and interactive queries to form continuous applications. These
    APIs are designed to address that requirement. Spark takes care of running the
    query incrementally and continuously on streaming data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流API是构建在Spark SQL引擎上并扩展了DataFrame和Dataset的高级API。结构化流统一了流式、交互式和批处理查询。在大多数情况下，流数据需要与批处理和交互式查询相结合，形成连续的应用程序。这些API旨在满足这一要求。Spark负责在流数据上增量和连续地运行查询。
- en: The first release of structured streaming will be focusing on ETL workloads.
    Users will be able to specify the input, query, trigger, and type of output. An
    input stream is logically equivalent to an append-only table. Users define queries
    just the way they would on a traditional SQL table. The trigger is a timeframe,
    say one second. The output modes offered are complete output, deltas, or updates
    in place (for example, a DB table).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流的第一个版本将专注于ETL工作负载。用户将能够指定输入、查询、触发器和输出类型。输入流在逻辑上等同于只追加的表。用户定义查询的方式与传统SQL表相同。触发器是一个时间段，比如一秒。提供的输出模式包括完整输出、增量或原地更新（例如，DB表）。
- en: 'Take this example: you can aggregate the data in a stream, serve it using the
    Spark SQL JDBC server, and pass it to a database such as MySQL for downstream
    applications. Or you could run ad hoc SQL queries that act on the latest data.
    You can also build and apply machine learning models.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子：您可以在流中聚合数据，使用Spark SQL JDBC服务器提供数据，并将其传递给MySQL等数据库进行下游应用。或者您可以运行针对最新数据的临时SQL查询。您还可以构建和应用机器学习模型。
- en: Project Tungsten phase 2
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 项目钨2期
- en: 'The central idea behind project Tungsten is to bring Spark''s performance closer
    to bare metal through native memory management and runtime code generation. It
    was first included in Spark 1.4 and enhancements were added in 1.5 and 1.6\. It
    focuses on substantially improving the efficiency of memory and CPU for Spark
    applications, primarily by the following ways:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 项目钨的核心思想是通过本机内存管理和运行时代码生成将Spark的性能更接近于裸金属。它首次包含在Spark 1.4中，并在1.5和1.6中添加了增强功能。它主要通过以下方式显着提高Spark应用程序的内存和CPU效率：
- en: Managing memory explicitly and eliminating the overhead of JVM object model
    and garbage collection. For example, a four-byte string would occupy around 48
    bytes in the JVM object model. Since Spark is not a general-purpose application
    and has more knowledge about the life cycle of memory blocks than the garbage
    collector, it can manage memory more efficiently than JVM.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显式管理内存并消除JVM对象模型和垃圾收集的开销。例如，一个四字节的字符串在JVM对象模型中占用大约48字节。由于Spark不是通用应用程序，并且对内存块的生命周期比垃圾收集器更了解，因此它可以比JVM更有效地管理内存。
- en: Designing cache-friendly algorithms and data structures.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计友好缓存的算法和数据结构。
- en: Spark performs code generation to compile parts of queries to Java bytecode.
    This is being broadened to cover most built-in expressions.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark执行代码生成以将查询的部分编译为Java字节码。这将被扩展以覆盖大多数内置表达式。
- en: 'Spark 2.0 rolls out phase 2, which is an order of magnitude faster and includes:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0推出了第2阶段，速度提高了一个数量级，并包括：
- en: Whole stage code generation by removing expensive iterator calls and fusing
    across multiple operators so that the generated code looks like hand-optimized
    code
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过消除昂贵的迭代器调用和在多个运算符之间融合来进行整体代码生成，从而使生成的代码看起来像手动优化的代码
- en: Optimized input and output
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化的输入和输出
- en: What's in store?
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未来会有什么？
- en: 'Apache Spark 2.1 is expected to have the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 预计Apache Spark 2.1将具有以下功能：
- en: '**Continuous SQL** (**CSQL**)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续SQL**（**CSQL**）'
- en: BI application integration
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BI应用程序集成
- en: Support for more streaming sources and sinks
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多流式数据源和接收器的支持
- en: Inclusion of additional operators and libraries for structured streaming
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括用于结构化流的额外运算符和库
- en: Enhancements to a machine learning package
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习包的增强功能
- en: Columnar in-memory support in Tungsten
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钨中的列式内存支持
- en: The big data trends
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据趋势
- en: Big data processing has been an integral part of the IT industry, more so in
    the past decade. Apache Hadoop and other similar endeavors are focused on building
    the infrastructure to store and process massive amounts of data. After being around
    for over 10 years, the Hadoop platform is considered mature and almost synonymous
    with big data processing. Apache Spark, a general computing engine that works
    well with is and not limited to the Hadoop ecosystem, was quite successful in
    the year 2015.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据处理一直是IT行业的一个重要组成部分，尤其是在过去的十年中。Apache Hadoop和其他类似的努力致力于构建存储和处理海量数据的基础设施。在经过10多年的发展后，Hadoop平台被认为是成熟的，几乎可以与大数据处理等同起来。Apache
    Spark是一个通用计算引擎，与Hadoop生态系统兼容，并且在2015年取得了相当大的成功。
- en: Building data science applications requires knowledge of the big data landscape
    and what software products are available out of that box. We need to carefully
    map the right blocks that fit our requirements. There are several options with
    overlapping functionality, and picking the right tools is easier said than done.
    The success of the application very much depends on assembling the right mix of
    technologies and processes. The good news is that there are several open source
    options that drive down the cost of doing big data analytics; and at the same
    time, you have enterprise-quality end-to-end platforms backed by companies such
    as Databricks. In addition to the use case on hand, keeping track of the industry
    trends in general is equally important.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 构建数据科学应用程序需要了解大数据领域和可用软件产品。我们需要仔细地映射符合我们要求的正确模块。有几个具有重叠功能的选项，选择合适的工具说起来容易做起来难。应用程序的成功很大程度上取决于组装正确的技术和流程。好消息是，有几个开源选项可以降低大数据分析的成本；同时，你也可以选择由Databricks等公司支持的企业级端到端平台。除了手头的用例，跟踪行业趋势同样重要。
- en: The recent surge in NOSQL data stores with their own interfaces are adding SQL-based
    interfaces even though they are not relational data stores and may not adhere
    to ACID properties. This is a welcome trend because converging to a single, age-old
    interface across relational and non-relational data stores improves programmer
    productivity.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最近NOSQL数据存储的激增及其自己的接口正在添加基于SQL的接口，尽管它们不是关系数据存储，也可能不遵守ACID属性。这是一个受欢迎的趋势，因为在关系和非关系数据存储之间趋同于一个古老的接口可以提高程序员的生产力。
- en: The operational (OLTP) and analytical (OLAP) systems were being maintained as
    separate systems over the past couple of decades, but that's one more place where
    convergence is happening. This convergence brings us to near-real-time use cases
    such as fraud prevention. Apache Kylin is one open source distributed analytics
    engine in the Hadoop ecosystem that offers an extremely fast OLAP engine at scale.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几十年里，操作（OLTP）和分析（OLAP）系统一直被维护为独立的系统，但这是又一个融合正在发生的地方。这种融合使我们接近实时的用例，比如欺诈预防。Apache
    Kylin是Hadoop生态系统中的一个开源分布式分析引擎，提供了一个极快的大规模OLAP引擎。
- en: The advent of the Internet of Things is accelerating real-time and streaming
    analytics, bringing in a whole lot of new use cases. The cloud frees up organizations
    from the operations and IT management overheads so that they can concentrate on
    their core competence, especially in big data processing. Cloud-based analytic
    engines, self-service data preparation tools, self-service BI, just-in-time data
    warehousing, advanced analytics, rich media analytics, and agile analytics are
    some of the commonly used buzzwords. The term big data itself is slowly evaporating
    or becoming implicit.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网的出现正在加速实时和流式分析，带来了许多新的用例。云释放了组织的运营和IT管理开销，使他们可以集中精力在他们的核心竞争力上，特别是在大数据处理方面。基于云的分析引擎、自助数据准备工具、自助BI、及时数据仓库、高级分析、丰富的媒体分析和敏捷分析是一些常用的词汇。大数据这个词本身正在慢慢消失或变得隐含。
- en: 'There are plenty of software products and libraries in the big data landscape
    with overlapping functionalities, as shown in this infographic (http://mattturck.com/wp-content/uploads/2016/02/matt_turck_big_data_landscape_v11.png).
    Choosing the right blocks for your application is a daunting but very important
    task. Here is a short list of projects to get you started. The list excludes popular
    names such as Cassandra and tries to include blocks with complementing functionality
    and mostly from Apache Software Foundation:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据领域有许多具有重叠功能的软件产品和库，如此信息图所示（http://mattturck.com/wp-content/uploads/2016/02/matt_turck_big_data_landscape_v11.png）。选择适合你的应用程序的正确模块是一项艰巨但非常重要的任务。以下是一些让你开始的项目的简短列表。该列表不包括像Cassandra这样的知名名称，而是试图包括具有互补功能的模块，大多来自Apache软件基金会：
- en: '**Apache Arrow** ([https://arrow.apache.org/](https://arrow.apache.org/)) is
    an in-memory columnar layer used to accelerate analytical processing and interchange.
    It is a high-performance, cross-system, and in-memory data representation that
    is expected to bring in 100 times the performance improvements.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Arrow**（[https://arrow.apache.org/](https://arrow.apache.org/)）是用于加速分析处理和交换的内存中的列式层。这是一个高性能、跨系统和内存数据表示，预计将带来100倍的性能改进。'
- en: '**Apache Parquet** ([https://parquet.apache.org/](https://parquet.apache.org/))
    is a columnar storage format. Spark SQL provides support for both reading and
    writing parquet files while automatically capturing the structure of the data.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Parquet**（[https://parquet.apache.org/](https://parquet.apache.org/)）是一种列式存储格式。Spark
    SQL提供对读写parquet文件的支持，同时自动捕获数据的结构。'
- en: '**Apache Kafka** ([http://kafka.apache.org/](http://kafka.apache.org/)) is
    a popular, high-throughput distributed messaging system. Spark streaming has a
    direct API to support streaming data ingestion from Kafka.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Kafka**（[http://kafka.apache.org/](http://kafka.apache.org/)）是一种流行的、高吞吐量的分布式消息系统。Spark
    streaming具有直接的API，支持从Kafka进行流式数据摄入。'
- en: '**Alluxio** ([http://alluxio.org/](http://alluxio.org/)), formerly called Tachyon,
    is a memory-centric, virtual distributed storage system that enables data sharing
    across clusters at memory speed. It aims to become the de facto storage unification
    layer for big data. Alluxio sits between computation frameworks such as Spark
    and storage systems such as Amazon S3, HDFS, and others.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Alluxio**（[http://alluxio.org/](http://alluxio.org/)），以前称为Tachyon，是一个以内存为中心的虚拟分布式存储系统，可以在内存速度下跨集群共享数据。它旨在成为大数据的事实存储统一层。Alluxio位于计算框架（如Spark）和存储系统（如Amazon
    S3、HDFS等）之间。'
- en: '**GraphFrames** (https://databricks.com/blog/2016/03/03/introducing-graphframes.html)
    is a graph processing library for Apache spark that is built on top of DataFrames
    API.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphFrames**（https://databricks.com/blog/2016/03/03/introducing-graphframes.html）是建立在DataFrames
    API之上的Apache Spark的图处理库。'
- en: '**Apache Kylin** ([http://kylin.apache.org/](http://kylin.apache.org/)) is
    a distributed analytics engine designed to provide SQL interface and multidimensional
    analysis (OLAP) on Hadoop, supporting extremely large datasets.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Kylin（[http://kylin.apache.org/](http://kylin.apache.org/)）是一个分布式分析引擎，旨在提供SQL接口和Hadoop上的多维分析（OLAP）支持，支持极大的数据集。
- en: '**Apache Sentry** ([http://sentry.apache.org/](http://sentry.apache.org/))
    is a system for enforcing fine-grained role-based authorization to data and metadata
    stored on a Hadoop cluster. It is in the incubation stage at the time of writing
    this book.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Sentry（[http://sentry.apache.org/](http://sentry.apache.org/)）是一个用于强制执行基于角色的细粒度授权的系统，用于存储在Hadoop集群上的数据和元数据。在撰写本书时，它处于孵化阶段。
- en: '**Apache Solr** ([http://lucene.apache.org/solr/](http://lucene.apache.org/solr/))
    is a blazing fast search platform. Check this [presentation](https://spark-summit.org/2015/events/integrating-spark-and-solr/)
    for integrating Solr and Spark.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Solr（[http://lucene.apache.org/solr/](http://lucene.apache.org/solr/)）是一个极快的搜索平台。查看这个[演示](https://spark-summit.org/2015/events/integrating-spark-and-solr/)，了解如何集成Solr和Spark。
- en: '**TensorFlow** ([https://www.tensorflow.org/](https://www.tensorflow.org/))
    is a machine learning library with extensive built-in support for deep learning.
    Check out this [blog](https://databricks.com/blog/2016/01/25/deep-learning-with-spark-and-tensorflow.html)
    to learn how it can be used with Spark.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow（[https://www.tensorflow.org/](https://www.tensorflow.org/)）是一个具有广泛内置深度学习支持的机器学习库。查看这篇[博客](https://databricks.com/blog/2016/01/25/deep-learning-with-spark-and-tensorflow.html)了解它如何与Spark一起使用。
- en: '**Zeppelin** ([http://zeppelin.incubator.apache.org/](http://zeppelin.incubator.apache.org/))
    is a web-based notebook that enables interactive data analytics. It is covered
    in the data visualization chapter.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeppelin（[http://zeppelin.incubator.apache.org/](http://zeppelin.incubator.apache.org/)）是一个基于Web的笔记本，可以进行交互式数据分析。它在数据可视化章节中有所涉及。
- en: Summary
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this final chapter, we discussed how to build real-world applications using
    Spark. We discussed the big picture consisting of technical and non-technical
    aspects of data analytics workflows.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何使用Spark构建真实世界的应用程序。我们讨论了由技术和非技术方面组成的数据分析工作流的大局。
- en: References
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: The Spark Summit site has a wealth of information on Apache Spark and related
    projects from completed events
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Summit网站上有大量关于Apache Spark和相关项目的信息，来自已完成的活动
- en: Interview with *Matei Zaharia* by KDnuggets
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KDnuggets对Matei Zaharia的采访
- en: '*Why Spark Reached the Tipping Point* in 2015 from KDnuggets by *Matthew Mayo*'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2015年，为什么Spark达到了临界点，来自KDnuggets的Matthew Mayo
- en: 'Going Live: Preparing your first Spark production deployment is a very good
    starting point'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上线：准备您的第一个Spark生产部署是一个非常好的起点
- en: '*What is Scala?* from the Scala home page'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自Scala官网的“什么是Scala？”
- en: '*Martin Odersky*, creator of Scala, explains the reasons why Scala fuses together
    imperative and functional programming'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala的创始人Martin Odersky解释了为什么Scala将命令式和函数式编程融合在一起的原因
