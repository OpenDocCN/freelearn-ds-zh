- en: Advanced Text Processing with Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行高级文本处理
- en: In [Chapter 4](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml), *Obtaining, Processing,
    and Preparing Data with Spark*, we covered various topics related to feature extraction
    and data processing, including the basics of extracting features from text data.
    In this chapter, we will introduce more advanced text processing techniques available
    in Spark ML to work with large-scale text datasets.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml)中，*使用Spark获取、处理和准备数据*，我们涵盖了与特征提取和数据处理相关的各种主题，包括从文本数据中提取特征的基础知识。在本章中，我们将介绍Spark
    ML中可用的更高级的文本处理技术，以处理大规模文本数据集。
- en: 'In this chapter, we will:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将：
- en: Work through detailed examples that illustrate data processing, feature extraction,
    and the modeling pipeline, as they relate to text data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过详细示例，说明数据处理、特征提取和建模流程，以及它们与文本数据的关系
- en: Evaluate the similarity between two documents based on the words in the documents
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于文档中的单词评估两个文档之间的相似性
- en: Use the extracted text features as inputs for a classification model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用提取的文本特征作为分类模型的输入
- en: Cover a recent development in natural language processing to model words themselves
    as vectors and illustrate the use of Spark's Word2Vec model to evaluate the similarity
    between two words, based on their meaning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍自然语言处理的最新发展，将单词本身建模为向量，并演示使用Spark的Word2Vec模型评估两个单词之间的相似性，基于它们的含义
- en: We will look at how to use Spark's MLlib as well as Spark ML for text processing
    examples as well clustering of documents.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究如何使用Spark的MLlib以及Spark ML进行文本处理示例，以及文档的聚类。
- en: What's so special about text data?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本数据有何特殊之处？
- en: Text data can be complex to work with for two main reasons. First, text and
    language have an inherent structure that is not easily captured using the raw
    words as is (for example, meaning, context, different types of words, sentence
    structure, and different languages, to highlight a few). Therefore, naive feature
    extraction is usually relatively ineffective.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 处理文本数据可能会很复杂，主要有两个原因。首先，文本和语言具有固有的结构，不容易使用原始单词来捕捉（例如，含义、上下文、不同类型的单词、句子结构和不同语言等）。因此，天真的特征提取通常相对无效。
- en: Second, the effective dimensionality of text data is extremely large and potentially
    limitless. Think about the number of words in the English language alone and add
    all kinds of special words, characters, slang, and so on to this. Then, throw
    in other languages and all the types of text one might find across the Internet.
    The dimension of text data can easily exceed tens or even hundreds of millions
    of words, even in relatively small datasets. For example, the Common Crawl dataset
    of billions of websites contains over 840 billion individual words.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，文本数据的有效维度非常大，潜在无限。想想仅英语单词的数量，再加上各种特殊单词、字符、俚语等等。然后，再加入其他语言以及互联网上可能找到的各种文本类型。文本数据的维度很容易超过数千万甚至数亿个单词，即使是相对较小的数据集。例如，数十亿个网站的Common
    Crawl数据集包含超过8400亿个单词。
- en: To deal with these issues, we need ways of extracting more structured features
    and methods to handle the huge dimensionality of text data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们需要提取更结构化的特征的方法，以及处理文本数据的巨大维度的方法。
- en: Extracting the right features from your data
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据中提取正确的特征
- en: 'The field of **Natural Language Processing** (**NLP**) covers a wide range
    of techniques to work with text, from text processing and feature extraction through
    to modeling and machine learning. In this chapter, we will focus on two feature
    extraction techniques available within Spark MLlib and Spark ML: the **term frequency-inverse
    document frequency** (**tf-idf**) term weighting scheme and feature hashing.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）领域涵盖了处理文本的各种技术，从文本处理和特征提取到建模和机器学习。在本章中，我们将重点关注Spark MLlib和Spark
    ML中可用的两种特征提取技术：**词频-逆文档频率**（**tf-idf**）术语加权方案和特征哈希。'
- en: Working through an example of tf-idf, we will also explore the ways in which
    processing, tokenization, and filtering during feature extraction can help reduce
    the dimensionality of our input data as well as improve the information content
    and usefulness of the features we extract.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通过tf-idf的示例，我们还将探讨在特征提取过程中的处理、标记化和过滤如何帮助减少输入数据的维度，以及改善我们提取的特征的信息内容和有用性。
- en: Term weighting schemes
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语加权方案
- en: In [Chapter 4](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml), *Obtaining, Processing,
    and Preparing Data with Spark*, we looked at vector representation, where text
    features are mapped to a simple binary vector called the **bag-of-words** model.
    Another representation used commonly in practice is called Term Frequency-Inverse
    Document Frequency.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml)中，*使用Spark获取、处理和准备数据*，我们研究了向量表示，其中文本特征被映射到一个简单的二进制向量，称为**词袋**模型。实践中常用的另一种表示称为词频-逆文档频率。
- en: 'tf-idf weights each term in a piece of text (referred to as a **document**)
    based on its frequency in the document (the **term frequency**). A global normalization,
    called the **inverse document frequency**, is then applied based on the frequency
    of this term among all documents (the set of documents in a dataset is commonly
    referred to as a **corpus**). The standard definition of tf-idf is shown here:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: tf-idf根据文本（称为**文档**）中术语的频率对每个术语进行加权。然后，基于该术语在所有文档中的频率（数据集中的文档集通常称为**语料库**），应用全局归一化，称为**逆文档频率**。tf-idf的标准定义如下：
- en: '*tf-idf(t,d) = tf(t,d) x idf(t)*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*tf-idf(t,d) = tf(t,d) x idf(t)*'
- en: 'Here, *tf(t,d)* is the frequency (number of occurrences) of term *t* in document
    *d* and *idf(t)* is the inverse document frequency of term *t* in the corpus;
    this is defined as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*tf(t,d)*是文档*d*中术语*t*的频率（出现次数），*idf(t)*是语料库中术语*t*的逆文档频率；定义如下：
- en: '*idf(t) = log(N / d)*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*idf(t) = log(N / d)*'
- en: Here, *N* is the total number of documents, and *d* is the number of documents
    in which the term *t* occurs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*N*是文档的总数，*d*是术语*t*出现的文档数。
- en: The tf-idf formulation means that terms occurring many times in a document receive
    a higher weighting in the vector representation relative to those that occur few
    times in the document. However, the IDF normalization has the effect of reducing
    the weight of terms that are very common across all documents. The end result
    is that truly rare or important terms should be assigned higher weighting, while
    more common terms (which are assumed to have less importance) should have less
    impact in terms of weighting.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: tf-idf公式意味着在文档中多次出现的术语在向量表示中会获得更高的权重，相对于在文档中出现少次的术语。然而，IDF归一化会减少在所有文档中非常常见的术语的权重。最终结果是真正罕见或重要的术语应该被分配更高的权重，而更常见的术语（假定具有较低重要性）在权重方面应该影响较小。
- en: A good resource to learn more about the bag-of-words model (or vector space
    model) is the book Introduction to Information Retrieval, Christopher D. Manning,
    Prabhakar Raghavan and Hinrich Schütze, Cambridge University Press (available
    in HTML form at [http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html](http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 关于词袋模型（或向量空间模型）的更多学习资源是《信息检索导论》，作者是克里斯托弗·D·曼宁、普拉巴卡尔·拉加万和亨里希·舒兹，剑桥大学出版社（在[http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html](http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html)以HTML形式提供）。
- en: It contains sections on text processing techniques, including tokenization,
    stop word removal, stemming, and the vector space model, as well as weighting
    schemes such as tf-idf.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含有关文本处理技术的部分，包括标记化、停用词去除、词干提取和向量空间模型，以及诸如tf-idf之类的加权方案。
- en: An overview can also be found at [http://en.wikipedia.org/wiki/Tf%E2%80%93idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以在[http://en.wikipedia.org/wiki/Tf%E2%80%93idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf)找到概述。
- en: Feature hashing
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征哈希
- en: '**Feature hashing** is a technique to deal with high-dimensional data and is
    often used with text and categorical datasets where the features can take on many
    unique values (often many millions of values). In the previous chapters, we often
    used the *1-of-K* encoding approach for categorical features, including text.
    While this approach is simple and effective, it can break down in the face of
    extremely high-dimensional data.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征哈希**是一种处理高维数据的技术，通常与文本和分类数据集一起使用，其中特征可以具有许多唯一值（通常有数百万个值）。在先前的章节中，我们经常对分类特征（包括文本）使用*1-of-K*编码方法。虽然这种方法简单而有效，但在面对极高维数据时可能会失效。'
- en: Building and using *1-of-K* feature encoding requires us to keep a mapping of
    each possible feature value to an index in a vector. Furthermore, the process
    of creating the mapping itself requires at least one additional pass through the
    dataset and can be tricky to do in parallel scenarios. Up until now, we have often
    used a simple approach of collecting the distinct feature values and zipping this
    collection with a set of indices to create a map of feature value to index. This
    mapping is then broadcast (either explicitly in our code or implicitly by Spark)
    to each worker.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 构建和使用*1-of-K*特征编码需要我们保留每个可能特征值到向量中的索引的映射。此外，创建映射本身的过程至少需要对数据集进行一次额外的遍历，并且在并行场景中可能会很棘手。到目前为止，我们经常使用简单的方法来收集不同的特征值，并将此集合与一组索引进行压缩，以创建特征值到索引的映射。然后将此映射广播（无论是在我们的代码中明确地还是由Spark隐式地）到每个工作节点。
- en: However, when dealing with huge feature dimensions in the tens of millions or
    more that are common when working with text, this approach can be slow and can
    require significant memory and network resources, both on the Spark master (to
    collect the unique values) and workers (to broadcast the resulting mapping to
    each worker, which keeps it in memory to allow it to apply the feature encoding
    to its local piece of the input data).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在处理文本时常见的数千万维甚至更高维的特征时，这种方法可能会很慢，并且可能需要大量的内存和网络资源，无论是在Spark主节点（收集唯一值）还是工作节点（广播生成的映射到每个工作节点，以便它可以将特征编码应用于其本地的输入数据）。
- en: Feature hashing works by assigning the vector index for a feature based on the
    value obtained by hashing this feature to a number (usually, an integer value)
    using a hash function. For example, let's say the hash value of a categorical
    feature for the geolocation of `United States` is `342`. We will use the hashed
    value as the vector index, and the value at this index will be `1.0` to indicate
    the presence of the `United States` feature. The hash function used must be consistent
    (that is, for a given input, it returns the same output each time).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 特征哈希通过使用哈希函数将特征的值哈希为一个数字（通常是整数值），并基于此值为特征分配向量索引。例如，假设“美国”地理位置的分类特征的哈希值为“342”。我们将使用哈希值作为向量索引，该索引处的值将为“1.0”，以表示“美国”特征的存在。所使用的哈希函数必须是一致的（即对于给定的输入，每次返回相同的输出）。
- en: This encoding works the same way as mapping-based encoding, except that we choose
    a size for our feature vector upfront. As the most common hash functions return
    values in the entire range of integers, we will use a *modulo* operation to restrict
    the index values to the size of our vector, which is typically much smaller (a
    few tens of thousands to a few million, depending on our requirements).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种编码方式与基于映射的编码方式相同，只是我们需要提前为我们的特征向量选择一个大小。由于大多数常见的哈希函数返回整数范围内的值，我们将使用*模*运算将索引值限制为我们向量的大小，这通常要小得多（根据我们的要求，通常是几万到几百万）。
- en: Feature hashing has the advantage that we do not need to build a mapping and
    keep it in memory. It is also easy to implement, very fast, and can be done online
    and in real time, thus not requiring a pass through our dataset first. Finally,
    because we selected a feature vector dimension that is significantly smaller than
    the raw dimensionality of our dataset, we bound the memory usage of our model
    both in training and production; hence, memory usage does not scale with the size
    and dimensionality of our data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 特征哈希的优点在于我们不需要构建映射并将其保存在内存中。它也很容易实现，非常快速，可以在线和实时完成，因此不需要先通过我们的数据集。最后，因为我们选择了一个明显小于数据集原始维度的特征向量维度，我们限制了模型在训练和生产中的内存使用；因此，内存使用量不随数据的大小和维度而扩展。
- en: 'However, there are two important drawbacks, which are as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，存在两个重要的缺点，如下所示：
- en: As we don't create a mapping of features to index values, we also cannot do
    the reverse mapping of feature index to value. This makes it harder to, for example,
    determine which features are most informative in our models.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们不创建特征到索引值的映射，因此也无法进行特征索引到值的反向映射。例如，这使得在我们的模型中确定哪些特征最具信息量变得更加困难。
- en: 'As we are restricting the size of our feature vectors, we might experience
    **hash collisions**. This happens when two different features are hashed into
    the same index in our feature vector. Surprisingly, this doesn''t seem to have
    a severe impact on model performance as long as we choose a reasonable feature
    vector dimension relative to the dimension of the input data. If the Hashed vector
    is large the affect of collision is minimal but the gain is still significant.
    Please refer to this paper for more details : [http://www.cs.jhu.edu/~mdredze/publications/mobile_nlp_feature_mixing.pdf](http://www.cs.jhu.edu/~mdredze/publications/mobile_nlp_feature_mixing.pdf)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们限制了特征向量的大小，我们可能会遇到**哈希冲突**。当两个不同的特征被哈希到特征向量中的相同索引时，就会发生这种情况。令人惊讶的是，只要我们选择一个相对于输入数据维度合理的特征向量维度，这似乎并不会严重影响模型性能。如果哈希向量很大，冲突的影响就很小，但收益仍然很大。有关更多细节，请参阅此论文：[http://www.cs.jhu.edu/~mdredze/publications/mobile_nlp_feature_mixing.pdf](http://www.cs.jhu.edu/~mdredze/publications/mobile_nlp_feature_mixing.pdf)
- en: Further information on hashing can be found at [http://en.wikipedia.org/wiki/Hash_function](http://en.wikipedia.org/wiki/Hash_function).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有关哈希的更多信息可以在[http://en.wikipedia.org/wiki/Hash_function](http://en.wikipedia.org/wiki/Hash_function)找到。
- en: 'A key paper that introduced the use of hashing for feature extraction and machine
    learning is:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 引入哈希用于特征提取和机器学习的关键论文是：
- en: '*Kilian Weinberger*, *Anirban Dasgupta*, *John Langford*, *Alex Smola*, and
    *Josh Attenberg*. *Feature Hashing for Large Scale Multitask Learning*. *Proc.
    ICML 2009*, which is available at [http://alex.smola.org/papers/2009/Weinbergeretal09.pdf](http://alex.smola.org/papers/2009/Weinbergeretal09.pdf).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kilian Weinberger*，*Anirban Dasgupta*，*John Langford*，*Alex Smola*和*Josh Attenberg*。*大规模多任务学习的特征哈希*。*ICML
    2009年会议论文*，网址为[http://alex.smola.org/papers/2009/Weinbergeretal09.pdf](http://alex.smola.org/papers/2009/Weinbergeretal09.pdf)。'
- en: Extracting the tf-idf features from the 20 Newsgroups dataset
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从20个新闻组数据集中提取tf-idf特征
- en: To illustrate the concepts in this chapter, we will use a well-known text dataset
    called **20 Newsgroups**; this dataset is commonly used for text-classification
    tasks. This is a collection of newsgroup messages posted across 20 different topics.
    There are various forms of data available. For our purposes, we will use the `bydate`
    version of the dataset, which is available at [http://qwone.com/~jason/20Newsgroups](http://qwone.com/~jason/20Newsgroups).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明本章中的概念，我们将使用一个名为**20 Newsgroups**的著名文本数据集；这个数据集通常用于文本分类任务。这是一个包含20个不同主题的新闻组消息的集合。有各种形式的可用数据。为了我们的目的，我们将使用数据集的`bydate`版本，该版本可在[http://qwone.com/~jason/20Newsgroups](http://qwone.com/~jason/20Newsgroups)上找到。
- en: This dataset splits up the available data into training and test sets that comprise
    60 percent and 40 percent of the original data, respectively. Here, the messages
    in the test set occur after those in the training set. This dataset also excludes
    some of the message headers that identify the actual newsgroup; hence, it is an
    appropriate dataset to test the real-world performance of classification models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集将可用数据分为训练集和测试集，分别占原始数据的60%和40%。在这里，测试集中的消息出现在训练集中的消息之后。该数据集还排除了一些标识实际新闻组的消息头；因此，这是一个适合测试分类模型实际性能的数据集。
- en: Further information on the original dataset can be found in the *UCI Machine
    Learning Repository* page at [http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有关原始数据集的更多信息可以在*UCI机器学习库*页面上找到，网址为[http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html)。
- en: 'To get started, download the data and unzip the file using the following command:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，请下载数据并使用以下命令解压文件：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will create two folders: one called `20news-bydate-train` and another
    one called `20news-bydate-test`. Let''s take a look at the directory structure
    under the training dataset folder:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建两个文件夹：一个名为`20news-bydate-train`，另一个名为`20news-bydate-test`。让我们来看看训练数据集文件夹下的目录结构：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You will see that it contains a number of subfolders, one for each newsgroup:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到它包含许多子文件夹，每个子文件夹对应一个新闻组：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There are a number of files under each newsgroup folder; each file contains
    an individual message posting:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个新闻组文件夹下有许多文件；每个文件包含一个单独的消息帖子：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can take a look at a part of one of these messages to see the format:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看其中一条消息的一部分以查看格式：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we can see, each message contains some header fields that contain the sender,
    subject, and other metadata, followed by the raw content of the message.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，每条消息都包含一些包含发件人、主题和其他元数据的标题字段，然后是消息的原始内容。
- en: Exploring the 20 Newsgroups data
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索20个新闻组数据
- en: We will use a Spark Program to load and analyze the dataset.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个Spark程序来加载和分析数据集。
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Looking at the directory structure, you might recognize that once again, we
    have data contained in individual text files (one text file per message). Therefore,
    we will again use Spark's `wholeTextFiles` method to read the content of each
    file into a record in our RDD.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 查看目录结构时，您可能会认出，我们再次有数据包含在单独的文本文件中（每个消息一个文本文件）。因此，我们将再次使用Spark的`wholeTextFiles`方法将每个文件的内容读入RDD中的记录。
- en: 'In the code that follows, `PATH` refers to the directory in which you extracted
    the `20news-bydate` ZIP file:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码中，`PATH`指的是您提取`20news-bydate` ZIP文件的目录：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If you put a breakpoint, you will see the following line displayed, indicating
    the total number of files that Spark has detected:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您设置断点，您将看到以下行显示，指示Spark检测到的文件总数：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After the command has finished running, you will see the total record count,
    which should be the same as the preceding `Total input paths to process` screen
    output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 命令运行完毕后，您将看到总记录数，应该与前面的`要处理的总输入路径`屏幕输出相同：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let us now print first element of the `rdd` into which data has been loaded:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打印`rdd`的第一个元素，其中已加载数据：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we will take a look at the newsgroup topics available:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看可用的新闻组主题：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will display the following result:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下结果：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can see that the number of messages is roughly even between the topics.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到消息数量在主题之间大致相等。
- en: Applying basic tokenization
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用基本标记化
- en: 'The first step in our text processing pipeline is to split up the raw text
    content in each document into a collection of terms (also referred to as **tokens**).
    This is known as **tokenization**. We will start by applying a simple **whitespace**
    tokenization, together with converting each token to lowercase for each document:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们文本处理管道的第一步是将每个文档中的原始文本内容拆分为一组术语（也称为**标记**）。这就是**标记化**。我们将首先应用简单的**空格**标记化，同时将每个文档的每个标记转换为小写：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding code, we used the `flatMap` function instead of `map`, as for
    now, we want to inspect all the tokens together for exploratory analysis. Later
    in this chapter, we will apply our tokenization scheme on a per-document basis,
    so we will use the `map` function.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用了`flatMap`函数而不是`map`，因为现在我们想要一起检查所有标记以进行探索性分析。在本章的后面，我们将在每个文档的基础上应用我们的标记方案，因此我们将使用`map`函数。
- en: 'After running this code snippet, you will see the total number of unique tokens
    after applying our tokenization:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码片段后，您将看到应用我们的标记化后的唯一标记总数：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, for even a relatively small amount of text, the number of raw
    tokens (and, therefore, the dimensionality of our feature vectors) can be very
    high.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，即使对于相对较少的文本，原始标记的数量（因此，我们的特征向量的维度）也可能非常高。
- en: 'Let''s take a look at a randomly selected document. We will use the sample
    method of RDD:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下随机选择的文档。我们将使用RDD的sample方法：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that we set the third parameter to the `sample` function, which is the
    random seed. We set this function to `42` so that we get the same results from
    each call to `sample` so that your results match those in this chapter.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将`sample`函数的第三个参数设置为随机种子。我们将此函数设置为`42`，以便每次调用`sample`时都获得相同的结果，以便您的结果与本章中的结果相匹配。
- en: 'This will display the following result:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下结果：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Improving our tokenization
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进我们的标记化
- en: 'The preceding simple approach results in a lot of tokens and does not filter
    out many nonword characters (such as punctuation). Most tokenization schemes will
    remove these characters. We can do this by splitting each raw document on nonword
    characters using a regular expression pattern:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的简单方法会产生大量的标记，并且不会过滤掉许多非单词字符（如标点符号）。大多数标记方案都会去除这些字符。我们可以通过使用正则表达式模式在非单词字符上拆分每个原始文档来实现这一点：
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This reduces the number of unique tokens significantly:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这显著减少了唯一标记的数量：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If we inspect the first few tokens, we will see that we have eliminated most
    of the less useful characters in the text:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查前几个标记，我们会发现我们已经消除了文本中大部分不太有用的字符：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You will see the following result displayed:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下结果显示：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: While our nonword pattern to split text works fairly well, we are still left
    with numbers and tokens that contain numeric characters. In some cases, numbers
    can be an important part of a corpus. For our purposes, the next step in our pipeline
    will be to filter out numbers and tokens that are words mixed with numbers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们用于拆分文本的非单词模式效果相当不错，但我们仍然留下了数字和包含数字字符的标记。在某些情况下，数字可能是语料库的重要部分。对于我们的目的，管道中的下一步将是过滤掉数字和包含数字的标记。
- en: We can do this by applying another regular expression pattern and use this to
    filter out tokens that do not match the pattern, `val regex = """[^0-9]*""".r`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过应用另一个正则表达式模式来实现这一点，并使用它来过滤不匹配模式的标记，`val regex = """[^0-9]*""".r`。
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This further reduces the size of the token set:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这进一步减少了标记集的大小：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let's take a look at another random sample of the filtered tokens.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一下过滤后的标记的另一个随机样本。
- en: 'You will see output like the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出：
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can see that we have removed all the numeric characters. This still leaves
    us with a few strange *words*, but we will not worry about these too much here.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们已经删除了所有数字字符。这仍然给我们留下了一些奇怪的*单词*，但我们在这里不会太担心这些。
- en: Removing stop words
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 删除停用词
- en: '**Stop words** refer to common words that occur many times across almost all
    documents in a corpus (and across most corpuses). Examples of typical English
    stop words include and, but, the, of, and so on. It is a standard practice in
    text feature extraction to exclude stop words from the extracted tokens.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是指在语料库中（以及大多数语料库中）几乎所有文档中都出现多次的常见词。典型的英语停用词包括and、but、the、of等。在文本特征提取中，通常会排除停用词。
- en: When using tf-idf weighting, the weighting scheme actually takes care of this
    for us. As stop words have a very low idf score, they will tend to have very low
    tf-idf weightings and thus less importance. In some cases, for information retrieval
    and search tasks, it might be desirable to include stop words. However, it can
    still be beneficial to exclude stop words during feature extraction, as it reduces
    the dimensionality of the final feature vectors as well as the size of the training
    data.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用tf-idf加权时，加权方案实际上会为我们处理这个问题。由于停用词的idf得分非常低，它们往往具有非常低的tf-idf权重，因此重要性较低。然而，在某些情况下，对于信息检索和搜索任务，可能希望包括停用词。然而，在特征提取过程中排除停用词仍然是有益的，因为它减少了最终特征向量的维度以及训练数据的大小。
- en: 'We can take a look at some of the tokens in our corpus that have the highest
    occurrence across all documents to get an idea about some other stop words to
    exclude:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看我们语料库中出现次数最多的一些标记，以了解其他需要排除的停用词：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the preceding code, we took the tokens after filtering out numeric characters
    and generated a count of the occurrence of each token across the corpus. We can
    now use Spark's top function to retrieve the top 20 tokens by count. Notice that
    we need to provide the top function with an ordering that tells Spark how to order
    the elements of our RDD. In this case, we want to order by the count, so we will
    specify the second element of our key-value pair.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们在过滤掉数字字符后获取了标记，并生成了每个标记在整个语料库中出现次数的计数。现在我们可以使用Spark的top函数来检索按计数排名的前20个标记。请注意，我们需要为top函数提供一个排序方式，告诉Spark如何对我们的RDD元素进行排序。在这种情况下，我们希望按计数排序，因此我们将指定键值对的第二个元素。
- en: 'Running the preceding code snippet will result in the following top tokens:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码片段将产生以下前几个标记：
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As we might expect, there are a lot of common words in this list that we could
    potentially label as stop words. Let's create a set of stop words with some of
    these
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所预期的，这个列表中有很多常见词，我们可能会将其标记为停用词。让我们创建一个包含其中一些常见词的停用词集
- en: 'as well as other common words. We will then look at the tokens after filtering
    out these stop words:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以及其他常见词。然后我们将在过滤掉这些停用词后查看标记：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You will see the following output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出：
- en: '[PRE26]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You might notice that there are still quite a few common words in this top list.
    In practice, we might have a much larger set of stop words. However, we will keep
    a few (partly to illustrate the impact of common words when using tf-idf weighting
    a little later).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到在这个前面的列表中仍然有相当多的常见词。在实践中，我们可能会有一个更大的停用词集。然而，我们将保留一些（部分是为了稍后使用tf-idf加权时常见词的影响）。
- en: 'You can find list of common stop words here : [http://xpo6.com/list-of-english-stop-words/](http://xpo6.com/list-of-english-stop-words/)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到常见停用词列表：[http://xpo6.com/list-of-english-stop-words/](http://xpo6.com/list-of-english-stop-words/)
- en: 'One other filtering step that we will use is removing any tokens that are only
    one character in length. The reasoning behind this is similar to removing stop
    words-these single-character tokens are unlikely to be informative in our text
    model and can further reduce the feature dimension and model size. We will do
    this with another filtering step:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的另一个过滤步骤是删除长度为一个字符的任何标记。这背后的原因类似于删除停用词-这些单字符标记不太可能在我们的文本模型中提供信息，并且可以进一步减少特征维度和模型大小。我们将通过另一个过滤步骤来实现这一点：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Again, we will examine the tokens remaining after this filtering step:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将在此过滤步骤之后检查剩下的标记：
- en: '[PRE28]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Apart from some of the common words that we have not excluded, we see that a
    few, potentially more informative words are starting to appear.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们没有排除的一些常见词之外，我们看到一些潜在更具信息量的词开始出现。
- en: Excluding terms based on frequency
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 根据频率排除术语
- en: 'It is also a common practice to exclude terms during tokenization when their
    overall occurrence in the corpus is very low. For example, let''s examine the
    least occurring terms in the corpus (notice the different ordering we use here
    to return the results sorted in ascending order):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记化过程中，通常会排除语料库中整体出现非常少的术语。例如，让我们来检查语料库中出现次数最少的术语（注意我们在这里使用不同的排序方式来返回按升序排序的结果）：
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You will get the following results:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下结果：
- en: '[PRE30]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'As we can see, there are many terms that only occur once in the entire corpus.
    Since typically, we want to use our extracted features for other tasks such as
    document similarity or machine learning models, tokens that only occur once are
    not useful to learn from, as we will not have enough training data relative to
    these tokens. We can apply another filter to exclude these rare tokens:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，有许多术语在整个语料库中只出现一次。通常情况下，我们希望将我们提取的特征用于其他任务，如文档相似性或机器学习模型，只出现一次的标记对于学习来说是没有用的，因为相对于这些标记，我们将没有足够的训练数据。我们可以应用另一个过滤器来排除这些罕见的标记：
- en: '[PRE31]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can see that we are left with tokens that occur at least twice in the corpus:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们剩下的标记至少在语料库中出现了两次：
- en: '[PRE32]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, let''s count the number of unique tokens:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们统计一下唯一标记的数量：
- en: '[PRE33]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You will see the following output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出：
- en: '[PRE34]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As we can see, by applying all the filtering steps in our tokenization pipeline,
    we have reduced the feature dimension from `402,978` to `51,801`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，通过在我们的标记化流程中应用所有过滤步骤，我们已将特征维度从`402,978`减少到`51,801`。
- en: 'We can now combine all our filtering logic into one function, which we can
    apply to each document in our RDD:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将所有过滤逻辑组合成一个函数，然后将其应用到我们RDD中的每个文档：
- en: '[PRE35]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can check whether this function gives us the same result with the following
    code snippet:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查这个函数是否给我们相同的结果，使用以下代码片段：
- en: '[PRE36]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This will output `51801`, giving us the same unique token count as our step-by-step
    pipeline.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出`51801`，给我们与逐步流程相同的唯一标记计数。
- en: 'We can tokenize each document in our RDD as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按如下方式对RDD中的每个文档进行标记化：
- en: '[PRE37]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You will see output similar to the following, showing the first part of the
    tokenized version of our first document:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到类似以下的输出，显示我们第一个文档的标记化版本的前部分：
- en: '[PRE38]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: A note about stemming
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于词干的一点说明
- en: A common step in text processing and tokenization is **stemming**. This is the
    conversion of whole words to a **base form** (called a **word stem**). For example,
    plurals might be converted to singular (*dogs* becomes *dog*), and forms such
    as *walking* and *walker* might become walk. Stemming can become quite complex
    and is typically handled with specialized NLP or search engine software (such
    as NLTK, OpenNLP, and Lucene, for example). We will ignore stemming for the purpose
    of our example here.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 文本处理和标记化中的一个常见步骤是**词干提取**。这是将整个单词转换为**基本形式**（称为**词干**）的过程。例如，复数可能会转换为单数（*dogs*变成*dog*），而*walking*和*walker*这样的形式可能会变成*walk*。词干提取可能会变得非常复杂，通常需要专门的NLP或搜索引擎软件（例如NLTK、OpenNLP和Lucene等）来处理。在这个例子中，我们将忽略词干提取。
- en: A full treatment of stemming is beyond the scope of this book. You can find
    more details at [http://en.wikipedia.org/wiki/Stemming](http://en.wikipedia.org/wiki/Stemming).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对词干提取的全面处理超出了本书的范围。您可以在[http://en.wikipedia.org/wiki/Stemming](http://en.wikipedia.org/wiki/Stemming)找到更多细节。
- en: Feature Hashing
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征哈希
- en: First we explain what is feature hashing so that it becomes easier to understand
    the tf-idf model in the next section.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们解释什么是特征哈希，以便更容易理解下一节中的tf-idf模型。
- en: Feature hashing converts a String or a word into a fixed length vector which
    makes it easy to process text.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 特征哈希将字符串或单词转换为固定长度的向量，这样可以更容易地处理文本。
- en: Spark currently uses Austin Appleby's MurmurHash 3 algorithm (MurmurHash3_x86_32)
    for hashing text into numbers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Spark目前使用Austin Appleby的MurmurHash 3算法（MurmurHash3_x86_32）将文本哈希为数字。
- en: You can find the implementation here
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到实现
- en: '[PRE39]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Please note functions hashInt, hasnLong etc are called from Util.scala
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，函数`hashInt`、`hasLong`等是从`Util.scala`中调用的
- en: Building a tf-idf model
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建tf-idf模型
- en: We will now use Spark ML to transform each document, in the form of processed
    tokens, into a vector representation. The first step will be to use the `HashingTF`
    implementation, which makes use of feature hashing to map each token in the input
    text to an index in the vector of term frequencies. Then, we will compute the
    global IDF and use it to transform the term frequency vectors into tf-idf vectors.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用Spark ML将每个文档（以处理后的标记形式）转换为向量表示。第一步将是使用`HashingTF`实现，它利用特征哈希将输入文本中的每个标记映射到术语频率向量中的索引。然后，我们将计算全局IDF，并使用它将术语频率向量转换为tf-idf向量。
- en: For each token, the index will thus be the hash of the token (mapped in turn
    onto the dimension of the feature vector). The value for each token will be the
    tf-idf weighting for that token (that is, the term frequency multiplied by the
    inverse document frequency).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个标记，索引将是标记的哈希值（依次映射到特征向量的维度）。每个标记的值将是该标记的tf-idf加权值（即，术语频率乘以逆文档频率）。
- en: 'First, we will import the classes we need and create our `HashingTF` instance,
    passing in a `dim` dimension parameter. While the default feature dimension is
    2^(20) (or around 1 million), we will choose 2^(18) (or around 260,000), since
    with about 50,000 tokens, we should not experience a significant number of hash
    collisions, and a smaller dimension will be more memory and processing friendly
    for illustrative purposes:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入我们需要的类并创建我们的`HashingTF`实例，传入一个`dim`维度参数。虽然默认的特征维度是2^(20)（大约100万），我们将选择2^(18)（大约26万），因为大约有5万个标记，我们不应该遇到显著数量的哈希碰撞，而较小的维度对于说明目的来说更加节省内存和处理资源：
- en: '[PRE40]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note that we imported MLlib's `SparseVector` using an alias of `SV`. This is
    because later, we will use Breeze's `linalg` module, which itself also imports
    `SparseVector`. This way, we will avoid namespace collisions.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用`SV`的别名导入了MLlib的`SparseVector`。这是因为稍后，我们将使用Breeze的`linalg`模块，它本身也导入`SparseVector`。这样，我们将避免命名空间冲突。
- en: The `transform` function of `HashingTF` maps each input document (that is, a
    sequence of tokens) to an MLlib `Vector`. We will also call `cache` to pin the
    data in memory to speed up subsequent operations.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`HashingTF`的`transform`函数将每个输入文档（即标记序列）映射到MLlib的`Vector`。我们还将调用`cache`将数据固定在内存中，以加速后续操作。'
- en: 'Let''s inspect the first element of our transformed dataset:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查转换后数据集的第一个元素：
- en: Note that `HashingTF.transform` returns an `RDD[Vector]`, so we will cast the
    result returned to an instance of an MLlib `SparseVector`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`HashingTF.transform`返回一个`RDD[Vector]`，因此我们将返回的结果转换为MLlib`SparseVector`的实例。
- en: The `transform` method can also work on an individual document by taking an
    `Iterable` argument (for example, a document as a `Seq[String]`). This returns
    a single vector.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`transform`方法也可以通过接受一个`Iterable`参数（例如，作为`Seq[String]`的文档）来处理单个文档。这将返回一个单一的向量。'
- en: '[PRE41]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You will see the following output displayed:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出显示：
- en: '[PRE42]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We can see that the dimension of each sparse vector of term frequencies is 262,144
    (or 2^(18) as we specified). However, the number on non-zero entries in the vector
    is only 706\. The last two lines of the output show the frequency counts and vector
    indexes for the first few entries in the vector.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每个稀疏向量的特征频率的维度为262,144（或者我们指定的2^(18)）。然而，向量中非零条目的数量只有706。输出的最后两行显示了向量中前几个条目的频率计数和索引。
- en: 'We will now compute the inverse document frequency for each term in the corpus
    by creating a new `IDF` instance and calling `fit` with our RDD of term frequency
    vectors as the input. We will then transform our term frequency vectors to tf-idf
    vectors through the `transform` function of `IDF`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过创建一个新的`IDF`实例并调用`fit`方法来计算语料库中每个术语的逆文档频率。然后，我们将通过`IDF`的`transform`函数将我们的术语频率向量转换为tf-idf向量：
- en: '[PRE43]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'When you examine the first element in the RDD of tf-idf transformed vectors,
    you will see output similar to the one shown here:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当您检查tf-idf转换后向量的RDD中的第一个元素时，您将看到类似于这里显示的输出：
- en: '[PRE44]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We can see that the number of non-zero entries hasn't changed (at `706`), nor
    have the vector indices for the terms. What has changed are the values for each
    term. Earlier, these represented the frequency of each term in the document, but
    now, the new values represent the frequencies weighted by the IDF.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到非零条目的数量没有改变（为`706`），术语的向量索引也没有改变。改变的是每个术语的值。早些时候，这些值代表了文档中每个术语的频率，但现在，新值代表了由IDF加权的频率。
- en: IDF weightage came into picture when we executed the following two lines
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行以下两行时，IDF加权就出现了
- en: '[PRE45]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Analyzing the tf-idf weightings
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析tf-idf加权
- en: Next, let's investigate the tf-idf weighting for a few terms to illustrate the
    impact of the commonality or rarity of a term.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们调查一些术语的tf-idf加权，以说明术语的普遍性或稀有性的影响。
- en: 'First, we can compute the minimum and maximum tf-idf weights across the entire
    corpus:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以计算整个语料库中的最小和最大tf-idf权重：
- en: '[PRE46]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'As we can see, the minimum tf-idf is zero, while the maximum is significantly
    larger:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，最小的tf-idf是零，而最大的tf-idf显着更大：
- en: '[PRE47]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We will now explore the tf-idf weight attached to various terms. In the previous
    section on stop words, we filtered out many common terms that occur frequently.
    Recall that we did not remove all such potential stop words. Instead, we kept
    a few in the corpus so that we could illustrate the impact of applying the tf-idf
    weighting scheme on these terms.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨附加到各种术语的tf-idf权重。在停用词的上一节中，我们过滤掉了许多经常出现的常见术语。请记住，我们没有删除所有这些潜在的停用词。相反，我们在语料库中保留了一些术语，以便我们可以说明应用tf-idf加权方案对这些术语的影响。
- en: 'Tf-idf weighting will tend to assign a lower weighting to common terms. To
    see this, we can compute the tf-idf representation for a few of the terms that
    appear in the list of top occurrences that we previously computed, such as `you`,
    `do`, and `we`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Tf-idf加权将倾向于为常见术语分配较低的权重。为了证明这一点，我们可以计算我们先前计算的顶部出现列表中的一些术语的tf-idf表示，例如`you`，`do`和`we`：
- en: '[PRE48]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'If we form a tf-idf vector representation of this document, we would see the
    following values assigned to each term. Note that because of feature hashing,
    we are not sure exactly which term represents what. However, the values illustrate
    that the weighting applied to these terms is relatively low:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们形成这篇文章的tf-idf向量表示，我们会看到每个术语分配的以下值。请注意，由于特征散列，我们不确定哪个术语代表什么。但是，这些值说明了对这些术语应用的加权相对较低：
- en: '[PRE49]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, let''s apply the same transformation to a few less common terms that we
    might intuitively associate with being more linked to specific topics or concepts:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将相同的转换应用于一些我们可能直观地认为与特定主题或概念更相关的不太常见的术语：
- en: '[PRE50]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We can see from the following results that the tf-idf weightings are indeed
    significantly higher than for the more common terms:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下结果中我们可以看到，tf-idf加权确实比更常见的术语要高得多：
- en: '[PRE51]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Using a tf-idf model
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用tf-idf模型
- en: While we often refer to training a tf-idf model, it is actually a feature extraction
    process or transformation rather than a machine learning model. Tf-idf weighting
    is often used as a preprocessing step for other models, such as dimensionality
    reduction, classification, or regression.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们经常提到训练tf-idf模型，但实际上它是一个特征提取过程或转换，而不是一个机器学习模型。Tf-idf加权通常用作其他模型的预处理步骤，例如降维、分类或回归。
- en: To illustrate the potential uses of tf-idf weighting, we will explore two examples.
    The first is using the tf-idf vectors to compute document similarity, while the
    second involves training a multilabel classification model with the tf-idf vectors
    as input features.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明tf-idf加权的潜在用途，我们将探讨两个例子。第一个是使用tf-idf向量计算文档相似性，而第二个涉及使用tf-idf向量作为输入特征训练多标签分类模型。
- en: Document similarity with the 20 Newsgroups dataset and tf-idf features
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20个新闻组数据集和tf-idf特征的文档相似性
- en: You might recall from [Chapter 5](d3bf76a8-26be-4db7-8310-b936d220407e.xhtml),
    *Building a Recommendation Engine with Spark*, that the similarity between two
    vectors can be computed using a distance metric. The closer two vectors are (that
    is, the lower the distance metric), the more similar they are. One such metric
    that we used to compute similarity between movies is cosine similarity.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得[第5章](d3bf76a8-26be-4db7-8310-b936d220407e.xhtml)中的*使用Spark构建推荐引擎*，两个向量之间的相似度可以使用距离度量来计算。两个向量越接近（即距离度量越小），它们就越相似。我们用于计算电影之间相似度的一种度量是余弦相似度。
- en: Just like we did for movies, we can also compute the similarity between two
    documents. Using tf-idf, we have transformed each document into a vector representation.
    Hence, we can use the same techniques as we used for movie vectors to compare
    two documents.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们为电影所做的那样，我们也可以计算两个文档之间的相似性。使用tf-idf，我们已将每个文档转换为向量表示。因此，我们可以使用与我们用于比较两个文档的电影向量相同的技术。
- en: Intuitively, we might expect two documents to be more similar to each other
    if they share many terms. Conversely, we might expect two documents to be less
    similar if they each contain many terms that are different from each other. As
    we compute cosine similarity by computing a dot product of the two vectors and
    each vector is made up of the terms in each document, we can see that documents
    with a high overlap of terms will tend to have a higher cosine similarity.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉上，如果两个文档共享许多术语，我们可能期望这两个文档彼此更相似。相反，如果它们各自包含许多彼此不同的术语，我们可能期望这两个文档更不相似。由于我们通过计算两个向量的点积来计算余弦相似度，而每个向量由每个文档中的术语组成，我们可以看到具有高重叠术语的文档将倾向于具有更高的余弦相似度。
- en: Now, we can see tf-idf at work. We might reasonably expect that even very different
    documents might contain many overlapping terms that are relatively common (for
    example, our stop words). However, due to a low tf-idf weighting, these terms
    will not have a significant impact on the dot product and, therefore, will not
    have much impact on the similarity computed.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到tf-idf在起作用。我们可能合理地期望，即使非常不同的文档也可能包含许多重叠的相对常见的术语（例如，我们的停用词）。然而，由于tf-idf加权较低，这些术语对点积的影响不大，因此对计算的相似度也没有太大影响。
- en: 'For example, we might expect two randomly chosen messages from the `hockey`
    newsgroup to be relatively similar to each other. Let''s see if this is the case:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能期望从`冰球`新闻组中随机选择的两条消息之间相对相似。让我们看看是否是这种情况：
- en: '[PRE52]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: In the preceding code, we first filtered our raw input RDD to keep only the
    messages within the hockey topic. We then applied our tokenization and term frequency
    transformation functions. Note that the `transform` method used is the version
    that works on a single document (in the form of a `Seq[String]`) rather than the
    version that works on an RDD of documents.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先过滤了原始输入RDD，只保留了冰球主题内的消息。然后应用了我们的标记化和词项频率转换函数。请注意，使用的`transform`方法是适用于单个文档（以`Seq[String]`形式）的版本，而不是适用于RDD文档的版本。
- en: Finally, we applied the `IDF` transform (note that we use the same IDF that
    we have already computed on the whole corpus).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应用了`IDF`转换（请注意，我们使用的是已经在整个语料库上计算过的相同IDF）。
- en: 'Once we have our `hockey` document vectors, we can select two of these vectors
    at random and compute the cosine similarity between them (as we did earlier, we
    will use Breeze for the linear algebra functionality, in particular converting
    our MLlib vectors to Breeze `SparseVector` instances first):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了我们的`冰球`文档向量，我们可以随机选择其中的两个向量，并计算它们之间的余弦相似度（就像之前一样，我们将使用Breeze进行线性代数功能，特别是首先将我们的MLlib向量转换为Breeze`SparseVector`实例）：
- en: '[PRE53]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can see that the cosine similarity between the documents is around 0.06:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到文档之间的余弦相似度大约为0.06：
- en: '[PRE54]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: While this might seem quite low, recall that the effective dimensionality of
    our features is high due to the large number of unique terms that is typical when
    dealing with text data. Hence, we can expect that any two documents might have
    a relatively low overlap of terms even if they are about the same topic, and therefore
    would have a lower absolute similarity score.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能看起来相当低，但要记住，由于处理文本数据时通常会出现大量唯一术语，因此我们特征的有效维度很高。因此，我们可以期望，即使两个文档是关于相同主题的，它们之间的术语重叠也可能相对较低，因此绝对相似度得分也会较低。
- en: 'By contrast, we can compare this similarity score to the one computed between
    one of our `hockey` documents and another document chosen randomly from the `comp.graphics`
    newsgroup, using the same methodology:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们可以将此相似度得分与使用相同方法在`计算机图形`新闻组中随机选择的另一个文档与我们的`冰球`文档之间计算的相似度进行比较：
- en: '[PRE55]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The cosine similarity is significantly lower at `0.0047`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度显著较低，为`0.0047`：
- en: '[PRE56]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Finally, it is likely that a document from another sports-related topic might
    be more similar to our `hockey` document than one from a computer-related topic.
    However, we would probably expect a `baseball` document to not be as similar as
    our `hockey` document. Let''s see whether this is the case by computing the similarity
    between a random message from the `baseball` newsgroup and our `hockey` document:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，很可能来自另一个与体育相关的主题的文档与我们的`冰球`文档更相似，而不像来自与计算机相关的主题的文档。但是，我们可能预期`棒球`文档与我们的`冰球`文档不太相似。让我们通过计算`棒球`新闻组中的随机消息与我们的`冰球`文档之间的相似度来看看是否如此：
- en: '[PRE57]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Indeed, as we expected, we found that the `baseball` and `hockey` documents
    have a cosine similarity of `0.05`, which is significantly higher than the `comp.graphics`
    document, but also somewhat lower than the other `hockey` document:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，正如我们预期的那样，我们发现`棒球`和`冰球`文档的余弦相似度为`0.05`，这显著高于`计算机图形`文档，但也略低于另一个`冰球`文档：
- en: '[PRE58]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Source Code:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码：
- en: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_10/scala-2.0.x/src/main/scala/TFIDFExtraction.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_10/scala-2.0.x/src/main/scala/TFIDFExtraction.scala)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_10/scala-2.0.x/src/main/scala/TFIDFExtraction.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_10/scala-2.0.x/src/main/scala/TFIDFExtraction.scala)'
- en: Training a text classifier on the 20 Newsgroups dataset using tf-idf
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用tf-idf在20个新闻组数据集上训练文本分类器
- en: When using tf-idf vectors, we expected that the cosine similarity measure would
    capture the similarity between documents, based on the overlap of terms between
    them. In a similar way, we would expect that a machine learning model, such as
    a classifier, would be able to learn weightings for individual terms; this would
    allow it to distinguish between documents from different classes. That is, it
    should be possible to learn a mapping between the presence (and weighting) of
    certain terms and a specific topic.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用tf-idf向量时，我们预期余弦相似度度量将捕捉文档之间的相似性，基于它们之间的术语重叠。类似地，我们预期机器学习模型，如分类器，将能够学习每个术语的加权；这将使其能够区分不同类别的文档。也就是说，应该可以学习到存在（和加权）某些术语与特定主题之间的映射。
- en: In the 20 Newsgroups example, each newsgroup topic is a class, and we can train
    a classifier using our tf-idf transformed vectors as input.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在20个新闻组的例子中，每个新闻组主题都是一个类别，我们可以使用我们的tf-idf转换后的向量来训练分类器。
- en: 'Since we are dealing with a multiclass classification problem, we will use
    the naive Bayes model in MLlib, which supports multiple classes. As the first
    step, we will import the Spark classes that we will be using:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在处理一个多类分类问题，我们将在MLlib中使用朴素贝叶斯模型，该模型支持多个类别。作为第一步，我们将导入我们将使用的Spark类：
- en: '[PRE59]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We will keep our clustering code in an object called `Document clustering`
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保留我们的聚类代码在一个名为`文档聚类`的对象中
- en: '[PRE60]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Next, we will need to extract the 20 topics and convert them to class mappings.
    We can do this in exactly the same way as we might for *1-of-K* feature encoding,
    by assigning a numeric index to each class:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要提取20个主题并将它们转换为类映射。我们可以像对*1-of-K*特征编码一样做，为每个类分配一个数字索引：
- en: '[PRE61]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: In the preceding code snippet, we took the `newsgroups` RDD, where each element
    is the topic, and used the `zip` function to combine it with each element in our
    `tfidf` RDD of tf-idf vectors. We then mapped over each key-value element in our
    new zipped RDD and created a `LabeledPoint` instance, where `label` is the class
    index and `features` is the tf-idf vector.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们取了`newsgroups` RDD，其中每个元素都是主题，并使用`zip`函数将其与我们的tf-idf向量RDD中的每个元素组合在一起。然后，我们在我们的新压缩RDD中的每个键值元素上进行映射，并创建一个`LabeledPoint`实例，其中`label`是类索引，`features`是tf-idf向量。
- en: Note that the `zip` operator assumes that each RDD has the same number of partitions
    as well as the same number of elements in each partition. It will fail if this
    is not the case. We can make this assumption here because we have effectively
    created both our `tfidf` RDD and `newsgroups` RDD from a series of `map` transformations
    on the same original RDD that preserved the partitioning structure.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`zip`操作符假定每个RDD具有相同数量的分区以及每个分区中相同数量的元素。如果不是这种情况，它将失败。我们可以做出这种假设，因为我们实际上已经通过对相同原始RDD进行一系列`map`转换来创建了`tfidf`
    RDD和`newsgroups` RDD，并保留了分区结构。
- en: 'Now that we have an input RDD in the correct form, we can simply pass it to
    the naive Bayes `train` function:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了正确形式的输入RDD，我们可以简单地将其传递给朴素贝叶斯的`train`函数：
- en: '[PRE62]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let''s evaluate the performance of the model on the test dataset. We will load
    the raw test data from the `20news-bydate-test` directory, again using `wholeTextFiles`
    to read each message into an RDD element. We will then extract the class labels
    from the file paths in the same way as we did for the `newsgroups` RDD:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们评估模型在测试数据集上的性能。我们将从“20news-bydate-test”目录加载原始测试数据，再次使用`wholeTextFiles`将每条消息读入RDD元素。然后，我们将从文件路径中提取类标签，方式与我们对`newsgroups`
    RDD所做的方式相同。
- en: '[PRE63]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Transforming the text in the test dataset follows the same procedure as for
    the training data-we will apply our `tokenize` function followed by the term frequency
    transformation, and we will again use the same IDF computed from the training
    data to transform the TF vectors into tf-idf vectors. Finally, we will zip the
    test class labels with the tf-idf vectors and create our test `RDD[LabeledPoint]`:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对测试数据集中的文本进行转换的过程与训练数据相同-我们将应用我们的`tokenize`函数，然后进行词项频率转换，然后再次使用从训练数据中计算的相同IDF来将TF向量转换为tf-idf向量。最后，我们将测试类标签与tf-idf向量进行压缩，并创建我们的测试`RDD[LabeledPoint]`：
- en: '[PRE64]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Note that it is important that we use the training set IDF to transform the
    test data, as this creates a more realistic estimation of model performance on
    new data, which might potentially contain terms that the model has not yet been
    trained on. It would be "cheating" to recompute the IDF vector based on the test
    dataset and, more importantly, would potentially lead to incorrect estimates of
    optimal model parameters selected through cross-validation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，重要的是我们使用训练集的IDF来转换测试数据，因为这样可以更真实地估计模型在新数据上的性能，新数据可能包含模型尚未训练过的术语。如果基于测试数据集重新计算IDF向量，这将是“作弊”，更重要的是，可能会导致通过交叉验证选择的最佳模型参数的不正确估计。
- en: 'Now, we''re ready to compute the predictions and true class labels for our
    model. We will use this RDD to compute accuracy and the multiclass weighted F-measure
    for our model:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备计算模型的预测和真实类标签。我们将使用此RDD来计算模型的准确性和多类加权F-度量：
- en: '[PRE65]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The weighted F-measure is an overall measure of precision and recall performance
    (where, like area under an ROC curve, values closer to 1.0 indicate better performance),
    which is then combined through a weighted averaged across the classes.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 加权F-度量是精确度和召回率性能的综合度量（类似于ROC曲线下面积，值越接近1.0表示性能越好），然后通过在类别之间进行加权平均来组合。
- en: 'We can see that our simple multiclass naive Bayes model has achieved close
    to 80 percent for both accuracy and F-measure:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们简单的多类朴素贝叶斯模型的准确性和F-度量都接近80％：
- en: '[PRE66]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Evaluating the impact of text processing
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估文本处理的影响
- en: Text processing and tf-idf weighting are examples of feature extraction techniques
    designed to both reduce the dimensionality of, and extract some structure from,
    raw text data. We can see the impact of applying these processing techniques by
    comparing the performance of a model trained on raw text data with one trained
    on processed and tf-idf weighted text data.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 文本处理和tf-idf加权是旨在减少原始文本数据的维度并提取一些结构的特征提取技术的例子。通过比较在原始文本数据上训练的模型与在处理和tf-idf加权文本数据上训练的模型的性能，我们可以看到应用这些处理技术的影响。
- en: Comparing raw features with processed tf-idf features on the 20 Newsgroups dataset
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较20个新闻组数据集上的原始特征和处理后的tf-idf特征
- en: 'In this example, we will simply apply the hashing term frequency transformation
    to the raw text tokens obtained using a simple whitespace splitting of the document
    text. We will train a model on this data and evaluate the performance on the test
    set as we did for the model trained with tf-idf features:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将简单的哈希词项频率转换应用于使用文档文本的简单空格拆分获得的原始文本标记。我们将在这些数据上训练一个模型，并评估在测试集上的性能，就像我们对使用tf-idf特征训练的模型一样：
- en: '[PRE67]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Perhaps surprisingly, the raw model does quite well, although both accuracy
    and F-measure are a few percentage points lower than those of the tf-idf model.
    This is also partly a reflection of the fact that the naive Bayes model is well
    suited to data in the form of raw frequency counts:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 也许令人惊讶的是，原始模型表现得相当不错，尽管准确性和F-度量都比tf-idf模型低几个百分点。这在一定程度上也反映了朴素贝叶斯模型适合以原始频率计数形式的数据。
- en: '[PRE68]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Text classification with Spark 2.0
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行文本分类
- en: In this section, we will use the libsvm version of *20newsgroup* data to use
    the Spark DataFrame-based APIs to classify the text documents. In the current
    version of Spark libsvm version 3.22 is supported ([https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/))
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用libsvm版本的*20newsgroup*数据，使用Spark DataFrame-based API对文本文档进行分类。在当前版本的Spark中，支持libsvm版本3.22
    ([https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/))
- en: Download the libsvm formatted data from the following link and copy output folder
    under Spark-2.0.x.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下链接下载libsvm格式的数据并将输出文件夹复制到Spark-2.0.x下。
- en: 'Visit the following link for the *20newsgroup libsvm* data: [https://1drv.ms/f/s!Av6fk5nQi2j-iF84quUlDnJc6G6D](https://1drv.ms/f/s!Av6fk5nQi2j-iF84quUlDnJc6G6D)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 访问以下链接以获取*20newsgroup libsvm*数据：[https://1drv.ms/f/s!Av6fk5nQi2j-iF84quUlDnJc6G6D](https://1drv.ms/f/s!Av6fk5nQi2j-iF84quUlDnJc6G6D)
- en: 'Import the appropriate packages from `org.apache.spark.ml` and create Wrapper
    Scala:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 从`org.apache.spark.ml`中导入适当的包并创建Wrapper Scala：
- en: '[PRE69]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next, we will load the `libsvm` data into a Spark DataFrame:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将将`libsvm`数据加载到Spark DataFrame中：
- en: '[PRE70]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Instantiate the `NaiveBayes` model from the `org.apache.spark.ml.classification.NaiveBayes`
    class and train the model:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 从`org.apache.spark.ml.classification.NaiveBayes`类中实例化`NaiveBayes`模型并训练模型：
- en: '[PRE71]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The following table is the output of the predictions DataFrame `.show()` command:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格是预测DataFrame的输出`.show()`命令：
- en: '[PRE72]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Test the accuracy of the model:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 测试模型的准确性：
- en: '[PRE73]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Accuracy of this model is above `0.8` as shown in the following output:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如下输出所示，该模型的准确性高于`0.8`：
- en: '[PRE74]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Word2Vec models
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vec模型
- en: Until now, we have used a bag-of-words vector, optionally with some weighting
    scheme such as tf-idf to represent the text in a document. Another recent class
    of models that has become popular is related to representing individual words
    as vectors.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用了词袋向量，可选地使用一些加权方案，如tf-idf来表示文档中的文本。另一个最近流行的模型类别与将单个单词表示为向量有关。
- en: These are generally based in some way on the co-occurrence statistics between
    the words in a corpus. Once the vector representation is computed, we can use
    these vectors in ways similar to how we might use tf-idf vectors (such as using
    them as features for other machine learning models). One such common use case
    is computing the similarity between two words with respect to their meanings,
    based on their vector representations.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型通常在某种程度上基于语料库中单词之间的共现统计。一旦计算出向量表示，我们可以以类似于使用tf-idf向量的方式使用这些向量（例如，将它们用作其他机器学习模型的特征）。这样一个常见的用例是根据它们的向量表示计算两个单词之间的相似性。
- en: Word2Vec refers to a specific implementation of one of these models, often referred
    to as **distributed vector representations**. The MLlib model uses a **skip-gram**
    model, which seeks to learn vector representations that take into account the
    contexts in which words occur.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec是指这些模型中的一个特定实现，通常被称为**分布式向量表示**。MLlib模型使用**skip-gram**模型，该模型旨在学习考虑单词出现上下文的向量表示。
- en: While a detailed treatment of Word2Vec is beyond the scope of this book, Spark's
    documentation at [http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec](http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec)
    contains some further details on the algorithm as well as links to the reference
    implementation.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对Word2Vec的详细处理超出了本书的范围，但Spark的文档[http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec](http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec)中包含有关算法的更多详细信息以及参考实现的链接。
- en: One of the main academic papers underlying Word2Vec is *Tomas Mikolov*, *Kai
    Chen*, *Greg Corrado*, and *Jeffrey Dean*. *Efficient Estimation of Word Representations
    in Vector Space*. *In Proceedings of Workshop at ICLR*, *2013*.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec的主要学术论文之一是*Tomas Mikolov*，*Kai Chen*，*Greg Corrado*和*Jeffrey Dean*。*Efficient
    Estimation of Word Representations in Vector Space*。*在2013年ICLR研讨会论文集中*。
- en: It is available at [http://arxiv.org/pdf/1301.3781.pdf](http://arxiv.org/pdf/1301.3781.pdf).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以在[http://arxiv.org/pdf/1301.3781.pdf](http://arxiv.org/pdf/1301.3781.pdf)上找到。
- en: Another recent model in the area of word vector representations is GloVe at
    [http://www-nlp.stanford.edu/projects/glove/](http://www-nlp.stanford.edu/projects/glove/).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在词向量表示领域的另一个最近的模型是GloVe，网址为[http://www-nlp.stanford.edu/projects/glove/](http://www-nlp.stanford.edu/projects/glove/)。
- en: You can also leverage third party libraries to do Parts of Speech tagging. For
    example Stanford NLP library could be hooked into scala code. Please refer to
    this discussion thread ([http://stackoverflow.com/questions/18416561/pos-tagging-in-scala](http://stackoverflow.com/questions/18416561/pos-tagging-in-scala))
    for more details on how to do it.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以利用第三方库进行词性标注。例如，Stanford NLP库可以连接到scala代码中。有关如何执行此操作的更多详细信息，请参阅此讨论线程([http://stackoverflow.com/questions/18416561/pos-tagging-in-scala](http://stackoverflow.com/questions/18416561/pos-tagging-in-scala))。
- en: Word2Vec with Spark MLlib on the 20 Newsgroups dataset
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在20 Newsgroups数据集上使用Spark MLlib的Word2Vec
- en: Training a Word2Vec model in Spark is relatively simple. We will pass in an
    RDD where each element is a sequence of terms. We can use the RDD of tokenized
    documents we have already created as input to the model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中训练Word2Vec模型相对简单。我们将传入一个RDD，其中每个元素都是一个术语序列。我们可以使用我们已经创建的标记化文档的RDD作为模型的输入。
- en: '[PRE75]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Our code is in the Scala object `Word2VecMllib`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码在Scala对象`Word2VecMllib`中：
- en: '[PRE76]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Let us start by loading the text file:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载文本文件开始：
- en: '[PRE77]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'We use the tokens created by tf-idf as the starting point for Word2Vec. Let
    us first initialize the object and set a seed:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用tf-idf创建的标记作为Word2Vec的起点。让我们首先初始化对象并设置一个种子：
- en: '[PRE78]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Now, let's create the model by calling `word2vec.fit()` on the tf-idf tokens:.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过在tf-idf标记上调用`word2vec.fit()`来创建模型：
- en: '[PRE79]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: You will see some output while the model is being trained.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，您将看到一些输出。
- en: 'Once trained, we can easily find the top 20 synonyms for a given term (that
    is, the most similar term to the input term, computed by cosine similarity between
    the word vectors). For example, to find the 20 most similar terms to `philosopher`,
    use the following lines of code:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以轻松地找到给定术语的前20个同义词（即，与输入术语最相似的术语，由单词向量之间的余弦相似性计算得出）。例如，要找到与`philosopher`最相似的20个术语，请使用以下代码行：
- en: '[PRE80]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'As we can see from the following output, most of the terms relate to hockey
    or others:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出中可以看出，大多数术语与曲棍球或其他相关：
- en: '[PRE81]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Word2Vec with Spark ML on the 20 Newsgroups dataset
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在20个新闻组数据集上使用Spark ML的Word2Vec
- en: In this section, we look at how to use the Spark ML DataFrame and newer implementations
    from Spark 2.0.X to create a Word2Vector model.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看看如何使用Spark ML DataFrame和Spark 2.0.X中的新实现来创建Word2Vector模型。
- en: 'We will create a DataFrame from the dataSet:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从数据集创建一个DataFrame：
- en: '[PRE82]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'This will be followed by creating the `Word2Vec` class and training the model
    on the DataFrame `textDF` created above:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来将创建`Word2Vec`类，并在上面创建的DataFrame `textDF`上训练模型：
- en: '[PRE83]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now let us try to find some synonyms for `hockey`:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试找一些`hockey`的同义词：
- en: The following
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 以下
- en: '[PRE84]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Following output will be generated:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成以下输出：
- en: '[PRE85]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: As you can see, the results are quite different from the results we got using
    RDD. This is because the two implementations differ for Word2Vector conversion
    in Spark 1.6 and Spark 2.0/2.1.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，结果与我们使用RDD得到的结果非常不同。这是因为Spark 1.6和Spark 2.0/2.1中的Word2Vector转换两种实现不同。
- en: Summary
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we took a deeper look into more complex text processing and
    explored MLlib's text feature extraction capabilities, in particular the tf-idf
    term weighting schemes. We covered examples of using the resulting tf-idf feature
    vectors to compute document similarity and train a newsgroup topic classification
    model. Finally, you learned how to use MLlib's cutting-edge Word2Vec model to
    compute a vector representation of words in a corpus of text and use the trained
    model to find words with contextual meaning that is similar to a given word. We
    also looked at using Word2Vec with Spark ML
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入研究了更复杂的文本处理，并探索了MLlib的文本特征提取能力，特别是tf-idf术语加权方案。我们介绍了使用生成的tf-idf特征向量来计算文档相似性和训练新闻组主题分类模型的示例。最后，您学会了如何使用MLlib的尖端Word2Vec模型来计算文本语料库中单词的向量表示，并使用训练好的模型找到具有类似给定单词的上下文含义的单词。我们还研究了如何在Spark
    ML中使用Word2Vec
- en: In the next chapter, we will take a look at online learning, and you will learn
    how Spark Streaming relates to online learning models.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看一看在线学习，您将学习Spark Streaming与在线学习模型的关系。
