- en: 5 Persisting Time Series Data to Databases
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 将时间序列数据持久化到数据库
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们在Discord上的书籍社区
- en: '![](img/file0.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/file0.png)'
- en: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
- en: 'It is common that, after completing a **data analysis** task, in which data
    is extracted from a source system, processed, transformed, and possibly modeled,
    the output is stored in a database for persistence. You can always store the data
    in a flat file or export it to a CSV, but when dealing with a large amount of
    corporate data (including proprietary data), you will need a more robust and secure
    way to store it. **Databases** offer several advantages: security (encryption
    at rest), concurrency (allowing many users to query the database without impacting
    performance), fault tolerance, **ACID** compliance, optimized read-write mechanisms,
    distributed computing, and distributed storage.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成一个**数据分析**任务后，通常会从源系统提取数据，进行处理、转换并可能建模，最后将结果存储到数据库中以实现持久化。你总是可以将数据存储在平面文件中或导出为CSV，但在处理大量企业数据（包括专有数据）时，你需要一种更强大且安全的存储方式。**数据库**提供了多个优势：安全性（静态加密）、并发性（允许多个用户查询数据库而不影响性能）、容错性、**ACID**合规性、优化的读写机制、分布式计算和分布式存储。
- en: In a corporate context, once data is stored in a database, it can be shared
    across different departments; for example, finance, marketing, sales, and product
    development can now access the data stored for their own needs. Furthermore, the
    data can now be democratized and applied to numerous use cases by different organizational
    roles, such as business analysts, data scientists, data engineers, marketing analysts,
    and business intelligence developers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业环境中，一旦数据被存储在数据库中，它可以跨不同部门共享；例如，财务、市场营销、销售和产品开发部门现在可以根据自己的需求访问存储的数据。此外，数据现在可以实现民主化，供不同角色的组织人员应用于各种用例，如业务分析师、数据科学家、数据工程师、市场分析师和商业智能开发人员。
- en: In this chapter, you will write your time series data to a database system for
    persistence. You will explore different types of databases (relational and non-relational)
    and use **Python** to push your data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将把时间序列数据写入数据库系统以实现持久化。你将探索不同类型的数据库（关系型和非关系型），并使用**Python**推送你的数据。
- en: More specifically, you will be using the **pandas** library since you will be
    doing much of your analysis using pandas **DataFrames**. You will learn how to
    use the pandas library to persist your time series DataFrame to a database storage
    system. Many databases offer Python APIs and connectors, and recently, many of
    them support pandas DataFrames (for reading and writing) given their popularity
    and mainstream adoption. In this chapter, you will work with a relational database,
    a document database, a cloud data warehouse, and a specialized time series database.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，你将使用**pandas**库，因为你会通过使用pandas的**DataFrame**进行大部分的分析。你将学习如何使用pandas库将你的时间序列DataFrame持久化到数据库存储系统中。许多数据库提供Python
    API和连接器，最近，许多数据库已经支持pandas DataFrame（用于读取和写入），因为它们的流行和主流应用。在本章中，你将使用关系型数据库、文档数据库、云数据仓库和专门的时间序列数据库。
- en: This chapter aims to give you first-hand experience working with different methods
    to connect to these database systems to persist your time series DataFrame.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是让你通过与不同方法连接到这些数据库系统，亲身体验如何持久化时间序列DataFrame。
- en: 'Here is the list of the recipes that we will cover in this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章将涵盖的内容列表：
- en: Writing time series data to a relational database
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将时间序列数据写入关系型数据库
- en: Writing time series data to MongoDB
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将时间序列数据写入MongoDB
- en: Writing time series data to InfluxDB
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将时间序列数据写入InfluxDB
- en: Writing time series data to Snowflake
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将时间序列数据写入Snowflake
- en: WRITING TO A DATABASE AND PERMISSIONS
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 写入数据库和权限
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Remember that when you install your database instance or use your cloud service,
    writing your data is straightforward since you are in the owner/admin role.
  id: totrans-16
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 记住，当你安装数据库实例或使用云服务时，写入数据是直接的，因为你是所有者/管理员角色。
- en: ''
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This will not be the case in any corporation when it's their database system.
    You must align and work with the database owners, maintainers, and possibly IT,
    database admins, or cloud admins. In most cases, they can permit you to write
    your data in a sandbox or a development environment. Then, once you are done,
    possibly the same or another team (such as a DevOps team) may want to inspect
    the code and evaluate performance before they migrate the code to a Quality Assurance
    (QA) / User Acceptance Testing (UAT) environment. Once there, the business may
    get involved to test and validate the data for approval. Finally, it may be promoted
    to the production environment so that everyone can start using the data.
  id: totrans-18
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在任何公司中，这种情况在他们的数据库系统中并不适用。你必须与数据库的所有者、维护者以及可能的IT人员、数据库管理员或云管理员对接。在大多数情况下，他们可以允许你将数据写入沙盒或开发环境。然后，一旦完成，可能是同一个团队或另一个团队（如DevOps团队）会检查代码并评估性能，之后才会将代码迁移到质量保证（QA）/用户验收测试（UAT）环境。一旦进入该环境，业务部门可能会参与测试并验证数据，以便获得批准。最终，它可能会被推广到生产环境，以便所有人都可以开始使用数据。
- en: Technical requirements
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will extensively use pandas 2.2.2 (released April 10, 2024).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将广泛使用 pandas 2.2.2（于2024年4月10日发布）。
- en: Throughout our journey, you will install several Python libraries to work with
    pandas. These are highlighted in the Getting ready section for each recipe. You
    can also download the Jupyter notebooks from the GitHub repository at [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook)
    to follow along.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的旅程中，你将安装多个 Python 库来与 pandas 一起使用。这些库在每个配方的准备部分中都有说明。你还可以从 GitHub 仓库下载 Jupyter
    notebooks，网址为 [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook)
    来跟着一起练习。
- en: You should refer to the *Technical Requirements* section in *Chapter 3*, *Reading
    Time Series Data from Databases*. This includes creating a **configuration file**
    such as the `database.cfg`.
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你应该参考 *第三章* 中的 *技术要求* 部分，*从数据库中读取时间序列数据*。这包括创建一个 **配置文件**，如 `database.cfg`。
- en: You will be using the same dataset throughout the recipes in this chapter. The
    dataset is based on Amazon's stock data from January 2019 to December 2023 pulled
    using the `yfnance` library and written as a pandas DataFrame.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有的配方将使用相同的数据集。该数据集基于2019年1月至2023年12月的亚马逊股票数据，通过 `yfinance` 库获取，并以 pandas
    DataFrame 的形式存储。
- en: 'Start by installing the `yfinance` library, which you can install using **conda**
    with:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先安装 `yfinance` 库，你可以通过 **conda** 安装，方法如下：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can also install using **pip** with:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过 **pip** 安装，方法如下：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To understand how the library works, you will start by pulling Amazon stock
    data using `yfinance`
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解这个库的工作原理，你将从使用 `yfinance` 拉取亚马逊股票数据开始。
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The resulting DataFrame has seven (7) columns and **1258** rows. There is a
    `DatetimeIndex` with the following format `2019-01-02 00:00:00-05:00`. Let’s focus
    on a handful of columns (Open, High, Low, Close, and Volume), and change the `DatetimeIndex`
    datetime format to be `YYYY-MM-DD` as well:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的 DataFrame 有七（7）列和 **1258** 行。它包含一个 `DatetimeIndex`，格式为 `2019-01-02 00:00:00-05:00`。我们将重点关注几列（Open、High、Low、Close
    和 Volume），并将 `DatetimeIndex` 的日期时间格式更改为 `YYYY-MM-DD`：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Based on the preceding example, we can generalize the approach by creating
    a function that we can call throughout this chapter:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的示例，我们可以通过创建一个可以在本章中调用的函数来概括这种方法：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `get_stock_data` function will return a pandas DataFrame with select columns
    and formatted DatetimeIndex. It requires three inputs: a `ticker` symbol, a `start`
    date, and an `end` date. If you can want to get stock data from January 1, 2024,
    up to today, just pass `None` to the `end` parameter. Here is an example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_stock_data` 函数将返回一个包含选定列和格式化 DatetimeIndex 的 pandas DataFrame。它需要三个输入：一个
    `ticker` 符号，一个 `start` 日期和一个 `end` 日期。如果你想获取从2024年1月1日到今天的股票数据，只需将 `end` 参数传递为
    `None`。下面是一个示例：'
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will give you stock data from January 1, 2024, up to the latest available
    data as of when the request is made.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供从2024年1月1日到请求时的最新数据的股票数据。
- en: Writing time series data to a relational database (PostgreSQL and MySQL)
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将时间序列数据写入关系型数据库（PostgreSQL 和 MySQL）
- en: In this recipe, you will write your DataFrame to a relational database such
    as **PostgreSQL**. The approach is the same for any relational database system
    supported by the `SQLAlchemy` Python library. You will experience how SQLAlchemy
    makes switching the backend database (called `dialect`) simple without altering
    the code. The abstraction layer provided by the SQLAlchemy library makes it feasible
    to switch to any supported database, such as from PostgreSQL to Amazon Redshift,
    using the same code.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，你将把 DataFrame 写入 PostgreSQL 等关系型数据库。对于 `SQLAlchemy` Python 库支持的任何关系型数据库系统，这种方法都是一样的。你将体验到
    SQLAlchemy 如何使得切换后端数据库（称为 `dialect`）变得简单，而无需更改代码。SQLAlchemy 提供的抽象层使得你能够使用相同的代码在任何支持的数据库之间切换，例如从
    PostgreSQL 切换到 Amazon Redshift。
- en: 'The sample list of supported relational databases (dialects) in SQLAlchemy
    includes the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: SQLAlchemy 支持的关系型数据库（方言）示例包括以下内容：
- en: Microsoft SQL Server
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft SQL Server
- en: MySQL/MariaDB
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MySQL/MariaDB
- en: PostgreSQL
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PostgreSQL
- en: Oracle
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oracle
- en: SQLite
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQLite
- en: 'Additionally, external dialects are available to install and use with SQLAlchemy
    to support other databases (dialects), such as `Snowflake`, `Microsoft SQL Server`,
    and `Google BigQuery`. Please visit the official page of SQLAlchemy for a list
    of available dialects: [https://docs.sqlalchemy.org/en/14/dialects/](https://docs.sqlalchemy.org/en/14/dialects/).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还可以安装并使用外部方言与 SQLAlchemy 配合使用，以支持其他数据库（方言），如 `Snowflake`、`Microsoft SQL Server`
    和 `Google BigQuery`。请访问 SQLAlchemy 的官方网站，查看可用的方言列表：[https://docs.sqlalchemy.org/en/14/dialects/](https://docs.sqlalchemy.org/en/14/dialects/)。
- en: Getting ready
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: You should refer to the recipe “*Reading data from a relational database*” in
    *Chapter 3*, *Reading Time Series Data from Databases,* as a refresher on the
    different ways to connect to PostgreSQL.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你应参考第 *3 章* 中的配方“*从关系型数据库中读取数据*”，以回顾连接 PostgreSQL 的不同方式。
- en: In this recipe, you will use the `yfinance` Python library to pull stock data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配方中，你将使用 `yfinance` Python 库来拉取股票数据。
- en: 'To install the libraries using **conda**, run the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 **conda** 安装这些库，请运行以下命令：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To install the libraries using `pip`, run the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `pip` 安装这些库，请运行以下命令：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The file is provided in the GitHub repository for this book, which you can
    find here: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的 GitHub 仓库中提供了文件，你可以在这里找到：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook)。
- en: How to do it…
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'In this recipe, you will be pulling Amazon''s stock data from January, 2019
    to December, 2023 using the `yfnance` library into a pandas DataFrame, and then
    writing the DataFrame to a table in a PostgreSQL database:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配方中，你将使用 `yfnance` 库从 2019 年 1 月到 2023 年 12 月获取亚马逊的股票数据，并将其存入一个 pandas DataFrame
    中，然后将该 DataFrame 写入 PostgreSQL 数据库中的表：
- en: Start by calling the `get_stock_data` function created in the *Technical Requirements*
    section.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从调用 *技术要求* 部分中创建的 `get_stock_data` 函数开始。
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You will need to create a SQLAlchemy **engine** object. The engine informs SQLAlchemy
    and pandas which dialect (backend database) we plan to interact with and connection
    details for the running database instance. Utilize `URL.create()` method to create
    a properly formatted URL object by providing the necessary parameters (`drivername`,
    `username`, `password`, `host`, `port`, and `database`). These parameters are
    stored in a `database.cfg` file.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要创建一个 SQLAlchemy **engine** 对象。该引擎告诉 SQLAlchemy 和 pandas 我们计划与之交互的方言（后端数据库）以及运行中数据库实例的连接详情。利用
    `URL.create()` 方法，通过提供必要的参数（`drivername`、`username`、`password`、`host`、`port` 和
    `database`）来创建一个格式正确的 URL 对象。这些参数存储在 `database.cfg` 文件中。
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can now pass the `url` object to `create_engine`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将 `url` 对象传递给 `create_engine`：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s write the `amz_hist` DataFrame in a new `amzn` table in our PostgreSQL
    database instance. This is achieved using `the DataFrame.to_sql()` writer function,
    which leverages SQLAlchemy''s capabilities to convert the DataFrame into the appropriate
    table schema and translate the data into the appropriate SQL statements such as
    `CREATE TABLE` and `INSERT INTO` specific to the dialect (backend database). If
    the table does not exist, a new table is created before loading the data, otherwise,
    if the table exists, then you will need to provide instructions on how it should
    be handled. This is done through the `if_exists` parameter, which accepts one
    of these options: `''fail''`, `''replace''`, or `''append''`.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将`amz_hist` DataFrame写入PostgreSQL数据库实例中的新`amzn`表。这是通过使用`DataFrame.to_sql()`写入函数来实现的，该函数利用SQLAlchemy的功能将DataFrame转换为合适的表模式，并将数据转换为适当的SQL语句（如`CREATE
    TABLE`和`INSERT INTO`），这些语句特定于方言（后端数据库）。如果表不存在，在加载数据之前会创建一个新表；如果表已存在，你需要提供如何处理表的指令。这是通过`if_exists`参数来完成的，参数可以接受以下选项之一：`'fail'`、`'replace'`或`'append'`。
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: An alternative method to accomplish the same tasks as the preceding code is
    by utilizing the `with` clause, this way you do not have to manage the connection.
    This would be a preferred approach in general.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 完成与前面的代码相同任务的另一种方法是利用`with`语句，这样你就无需管理连接。这通常是一个更优的做法。
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once the preceding code is executed, a new `amzn` table is created under the
    public schema in the default `postgres` database (default).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行上述代码，一个新的`amzn`表会在默认的`postgres`数据库的public模式下创建（默认）。
- en: 'You can validate this by running the following against the database:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式验证数据库中的内容：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Notice the use of `text()` function that wraps our query. The `text()` constructs
    a new `TextClause` to represent a textual SQL string.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意使用`text()`函数包裹我们的查询。`text()`构造了一个新的`TextClause`，用来表示文本SQL字符串。
- en: 'Confirm the data was written to the database by querying the `amzn` table and
    counting the number of records:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过查询`amzn`表并统计记录数来确认数据已写入数据库：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, request additional Amazon stock prices using `get_stock_data`, this time
    the 2024 data (for example, January 1, 2024, to September 23, 2024), and append
    it to the existing `amzn` table. Here, you will take advantage of the `if_exists`
    parameter in the `to_sql()` writer function.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`get_stock_data`请求额外的亚马逊股价数据，这次是2024年的数据（例如，2024年1月1日到2024年9月23日），并将其附加到现有的`amzn`表中。在这里，你将利用`to_sql()`写入函数中的`if_exists`参数。
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Make sure to pass `append` to the `if_exists` parameter, as shown in the following
    code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将`append`传递给`if_exists`参数，如以下代码所示：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Count the total number of records to ensure we have appended all 182 to the
    original 1258 records. You will run the same query that was executed earlier,
    as shown in the following code:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 统计记录的总数，以确保我们已经将182条记录附加到原来的1258条记录中。你将运行与之前相同的查询，如以下代码所示：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Indeed, you can observe that all of the 1440 records were written to the `amzn`
    table.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，你可以观察到所有1440条记录都已写入`amzn`表。
- en: How it works…
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Using the `DataFrame.to_sql()` writer function, SQLAlchemy handles many details
    under the hood, such as creating the table schema, inserting our records, and
    committing to the database.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`DataFrame.to_sql()`写入函数，SQLAlchemy在后台处理许多细节，比如创建表的模式、插入记录并提交到数据库。
- en: Working with pandas and SQLAlchemy to write and read to a relational database
    is very similar. We discussed using SQLAlchemy for reading data in the *Reading
    data from a relational database* recipe in *Chapter 3*, *Reading Time Series Data
    from Databases*. Many of the concepts discussed apply here as well.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pandas和SQLAlchemy向关系型数据库写入和读取数据非常相似。我们在*第3章*的*从关系型数据库读取数据*章节中讨论了使用SQLAlchemy读取数据。许多讨论的概念同样适用于这里。
- en: We always start with `create_engine` and specify the **dialect** (backend database).
    The `to_sql()` function will map the DataFrame data types to the appropriate PostgreSQL
    data types. The advantage of using an **Object Relational Mapper (ORM)** such
    as SQLAlchemy is that it gives you an abstraction layer, so you do not have to
    worry about *converting* the DataFrame schema into a specific database schema.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是从`create_engine`开始，并指定**方言**（后端数据库）。`to_sql()`函数会将DataFrame的数据类型映射到适当的PostgreSQL数据类型。使用**对象关系映射器（ORM）**如SQLAlchemy的优势在于，它提供了一个抽象层，使你无需担心将DataFrame模式转换为特定数据库模式。
- en: 'In the preceding example, you used the `if_exists` parameter in the `DataFrame.to_sql()`
    function with two different arguments:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，你在 `DataFrame.to_sql()` 函数中使用了 `if_exists` 参数，并传递了两个不同的参数：
- en: Initially, you set the value to `replace`, which would overwrite the table if
    it existed. If we translate this overwrite operation into SQL commands, it will
    execute a `DROP TABLE` followed by `CREATE TABLE`. This can be dangerous if you
    already have a table with data and you intend to append to it. Because of this
    concern, the default value is set to `fail` if you do not pass any argument. This
    default behavior would throw an error if the table existed.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初，你将值设置为 `replace`，这会在表存在时覆盖该表。如果我们将此覆盖操作转换为 SQL 命令，它会执行 `DROP TABLE`，然后是 `CREATE
    TABLE`。如果你已经有一个包含数据的表并打算向其中添加记录，这可能会很危险。因此，如果不传递任何参数，默认值会设置为 `fail`。这种默认行为会在表已存在时抛出错误。
- en: In the second portion of the recipe, the plan was to insert additional records
    into the existing table, and you updated the argument from `replace` to `append`.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在食谱的第二部分，计划是将额外的记录插入到现有表中，并且你将参数从 `replace` 更新为 `append`。
- en: When you pulled the stock data using `yfinance`, it automatically assigned the
    `Date` field as `DatetimeIndex`. In other words, the `Date` was not a column but
    an index. The default behavior in `to_sql()` is to write the DataFrame index as
    a column in the database, which is controlled by the `index` parameter. This is
    a Boolean parameter, and the default is set to `True`, which writes the DataFrame
    index as a column.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用 `yfinance` 拉取股票数据时，它会自动将 `Date` 字段指定为 `DatetimeIndex`。换句话说，`Date` 不是一列，而是一个索引。在
    `to_sql()` 中，默认行为是将 DataFrame 的索引作为数据库中的一列写入，这由 `index` 参数控制。这个参数是布尔类型，默认值为 `True`，表示将
    DataFrame 索引作为列写入。
- en: Another interesting parameter that can be extremely useful is `chunksize`. The
    default value is `None`, which writes all the rows in the DataFrame at once. If
    your dataset is extremely large, you can use the `chunksize` parameter to write
    to the database in batches; for example, a `chunksize` of 500 would write to the
    database in batches of 500 rows at a time.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常有用的参数是 `chunksize`。默认值为 `None`，表示一次性将 DataFrame 中的所有行写入数据库。如果你的数据集非常庞大，可以使用
    `chunksize` 参数批量写入数据库；例如，设置 `chunksize` 为 500 会一次性批量写入 500 行数据。
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: There's more…
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多…
- en: When using the pandas `read_sql`, `read_sql_table`, `read_sql_query`, and `to_sql`
    I/O functions, they expect a SQLAlchemy connection object (SQLAlchemy engine).
    To use SQLAlchemy to connect to a database of choice, you need to install the
    appropriate Python DBAPI (driver) for that specific database (for example, Amazon
    Redshift, Google BigQuery, MySQL, MariaDB, PostgreSQL, or Oracle). This gives
    you the advantage of writing your script once and still having it work with other
    dialects (backend databases) supported by SQLAlchemy. To demonstrate this, let's
    extend the last example.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 的 `read_sql`、`read_sql_table`、`read_sql_query` 和 `to_sql` I/O 函数时，它们需要一个
    SQLAlchemy 连接对象（SQLAlchemy 引擎）。要使用 SQLAlchemy 连接到目标数据库，你需要为特定的数据库（例如 Amazon Redshift、Google
    BigQuery、MySQL、MariaDB、PostgreSQL 或 Oracle）安装相应的 Python DBAPI（驱动程序）。这样你可以一次编写脚本，并且仍然能够与
    SQLAlchemy 支持的其他方言（后端数据库）一起使用。为了演示这一点，我们将扩展最后一个例子。
- en: '**Amazon Redshift**, a popular cloud data warehouse database, is based on PostgreSQL
    at its core, and it has several enhancements, including columnar storage for fast
    analytical queries. You will explore the simplicity of SQLAlchmey, as well as
    other options for writing a pandas DataFrame to Amazon Redshift.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon Redshift** 是一款流行的云数据仓库数据库，基于 PostgreSQL 架构，并在其基础上做了多项增强，包括列存储以加速分析查询。你将探索
    SQLAlchemy 的简便性，以及其他将 pandas DataFrame 写入 Amazon Redshift 的选项。'
- en: Writing to Amazon Redshift with SQLAclhemy
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 SQLAlchemy 向 Amazon Redshift 写入数据
- en: You will use the same code but write to an Amazon Redshift database this time.
    The only requirement, aside from a running MySQL instance, is installing a Python
    DBAPI (driver) for
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这次你将使用相同的代码，但写入 Amazon Redshift 数据库。除了运行 MySQL 实例之外，唯一的要求是安装适用于的 Python DBAPI（驱动程序）。
- en: Amazon Redshift. Note, that `sqlalchemy-redshift` requires `psycopgy2`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift。注意，`sqlalchemy-redshift` 需要 `psycopg2`。
- en: 'To install using **conda**, run the following command:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 **conda** 安装，请运行以下命令：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To install using **pip**, run the following command:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 **pip** 安装，请运行以下命令：
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You will use the same code for PostgreSQL; the only difference is the SQLAlchemy
    engine, which uses Amazon Redshift DBAPI. Start by loading the connection parameters
    from your configuration file. In this example, the configurations are stored in
    a database.cfg file
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用相同的代码来操作 PostgreSQL；唯一的不同是 SQLAlchemy 引擎，它使用的是 Amazon Redshift 的 DBAPI。首先从配置文件中加载连接参数。在这个示例中，配置存储在
    `database.cfg` 文件中。
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Using `ConfigParser` and `URL` to extract the parameters and construct the
    URL:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `ConfigParser` 和 `URL` 提取参数并构建 URL：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You can now create the engine using:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以使用以下代码创建引擎：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Using the `yfinance` library create a new `amzn_hist` DataFrame based on the
    past **5-years** of stock data:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `yfinance` 库创建一个基于过去 **5年** 股票数据的新 `amzn_hist` DataFrame：
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Before writing our DataFrame, we need to reset the index. This will give us
    back our Date column. We do this because Amazon Redshift does not support traditional
    indexes since it is a columnar database (instead, you can define a **sort key**).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在写入 DataFrame 之前，我们需要重置索引。这将使我们恢复 Date 列。我们这样做是因为 Amazon Redshift 不支持传统的索引，因为它是列式数据库（相反，你可以定义一个
    **排序键**）。
- en: '[PRE25]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Notice the index=False in the preceding code. This is done because `to_sql`
    will write the index object in a DataFrame since the default is `index=True`.
    When you reset the DataFrame index, it moves the DatetimeIndex to a Date column
    and replaces the index with a `RangeIndex` (ranging from 0 to 1257). Using `index=False`
    ensures we do not attempt to write the `RangeIndex` in Amazon Redshift.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意前面的代码中的 `index=False`。这是因为 `to_sql` 会写入 DataFrame 中的索引对象，默认情况下 `index=True`。当你重置
    DataFrame 索引时，它会将 DatetimeIndex 移动到 Date 列，并用 `RangeIndex`（从 0 到 1257）替换索引。使用
    `index=False` 确保我们不会尝试将 `RangeIndex` 写入 Amazon Redshift。
- en: 'Finally, you can validate the total number of records written:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以验证写入的记录总数：
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Writing to Amazon Redshift using the redshift_connector
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 redshift_connector 写入 Amazon Redshift
- en: In this example, you will utilize a different library, the `redshift_connector`.
    You will first need to install the library.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，你将使用一个不同的库，即 `redshift_connector`。你首先需要安装该库。
- en: 'You can install it with **conda** using:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 **conda** 安装：
- en: '[PRE27]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You can also install it with **pip** using:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用 **pip** 安装它：
- en: '[PRE28]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Note, the `redshift_connector` expets a `user` parameter, unlike SQLAclhemy
    which expects a `username` parameter. For this, you can create a new section in
    your configuration file. An example is shown below:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`redshift_connector` 需要一个 `user` 参数，这与 SQLAlchemy 需要 `username` 参数不同。为此，你可以在配置文件中创建一个新部分。下面是一个示例：
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The following code reads the parameters from a `database.cfg` file, and passes
    these parameters to `redshift_connector.connect()` to create a connection object.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码从 `database.cfg` 文件中读取参数，并将这些参数传递给 `redshift_connector.connect()` 来创建连接对象。
- en: '[PRE30]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: You will create a cursor object which gives access to the `write_dataframe`
    method.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你将创建一个游标对象，它提供对 `write_dataframe` 方法的访问。
- en: '[PRE31]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Finally, you will commit the transaction.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将提交事务。
- en: '[PRE32]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Do note that the `write_dataframe` does not provide arguments for specifying
    append, replace/overwrite, or fail behaviors, as you have seen with SQLAclehmy.
    The `write_dataframe` method expects an existing table in Amazon Redshift to append
    to.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`write_dataframe` 方法没有提供指定追加、替换/覆盖或失败行为的参数，正如你在 SQLAlchemy 中所看到的那样。`write_dataframe`
    方法期望在 Amazon Redshift 中已存在的表进行追加。
- en: Writing to Amazon Redshift with AWS SDK for pandas
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 AWS SDK for pandas 写入 Amazon Redshift
- en: The **awswrangler** library or AWS SDK for pandas make integrating with several
    AWS services such as Athena, Glue, Redshift, Neptune, DynamoDB, EMR, S3, and others
    easy.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**awswrangler** 库或 AWS SDK for pandas 可以轻松地与多个 AWS 服务（如 Athena、Glue、Redshift、Neptune、DynamoDB、EMR、S3
    等）集成。'
- en: 'You can install the library using **conda**:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 **conda** 安装该库：
- en: '[PRE33]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You can also install it using **pip**:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用 **pip** 安装：
- en: '[PRE34]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You can utilize the `conn` object created in the previous section, *Writing
    to Amazon Redshift using the redshift_connector*
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以利用在上一节中创建的 `conn` 对象，*使用 redshift_connector 写入 Amazon Redshift*。
- en: '[PRE35]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Note the mode parameter supports three (3) different options: `overwrite`,
    `append`, or `upsert`.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，mode 参数支持三种（3）不同的选项：`overwrite`、`append` 或 `upsert`。
- en: See also
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Here are some additional resources:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些额外的资源：
- en: To learn more about the `DataFrame.to_sql()` function, you can visit [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html).
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 若要了解更多关于 `DataFrame.to_sql()` 函数的信息，可以访问 [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html)。
- en: 'To learn more about **SQLAlchemy** features, you can start by reading their
    features page: [https://www.sqlalchemy.org/features.html](https://www.sqlalchemy.org/features.html).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于**SQLAlchemy**的功能，你可以先阅读它们的功能页面：[https://www.sqlalchemy.org/features.html](https://www.sqlalchemy.org/features.html)。
- en: To learn about `awswrangler` you can visit their GitHub repo here [https://github.com/aws/aws-sdk-pandas](https://github.com/aws/aws-sdk-pandas)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解`awswrangler`，你可以访问他们的GitHub仓库：[https://github.com/aws/aws-sdk-pandas](https://github.com/aws/aws-sdk-pandas)
- en: Writing time series data to MongoDB
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将时间序列数据写入MongoDB
- en: '**MongoDB** is a document database system that stores data in **BSON** format.
    When you query data from MongoDB, the data will be represented in JSON format.
    BSON is similar to JSON; it is the binary encoding of JSON. Unlike JSON, though,
    it is not in a human-readable format. JSON is great for transmitting data and
    is system-agnostic. BSON is designed to store data and is associated with MongoDB.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**MongoDB**是一个文档数据库系统，它以**BSON**格式存储数据。当你从MongoDB查询数据时，数据将以JSON格式呈现。BSON与JSON类似，它是JSON的二进制编码格式。不过，BSON不像JSON那样是人类可读的格式。JSON非常适合传输数据，且与系统无关，而BSON则专为存储数据并与MongoDB相关联。'
- en: In this recipe, you will explore writing a pandas DataFrame to MongoDB.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，你将学习如何将一个pandas DataFrame写入MongoDB。
- en: Getting ready
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: You should refer to the recipe “*Reading data from a document database*” in
    *Chapter 3*, *Reading Time Series Data from Databases* as a refresher on the different
    ways to connect to MongoDB.
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你应该参考*第3章*中“*从文档数据库读取数据*”这一实例，以便复习连接MongoDB的不同方法。
- en: In the *Reading data from a document database recipe* in *Chapter 3*, *Reading
    Time Series Data from Databases*, we installed `pymongo`. For this recipe, you
    will be using that same library again.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3章*中“*从文档数据库读取数据*”这一实例中，我们安装了`pymongo`。在本实例中，你将再次使用该库。
- en: 'To install using **conda**, run the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过**conda**安装，请运行以下命令：
- en: '[PRE36]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To install using **pip**, run the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过**pip**安装，请运行以下命令：
- en: '[PRE37]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The file is provided in the GitHub repository for this book, which you can
    find here: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的GitHub仓库中提供了该文件，你可以在此找到：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook)。
- en: How to do it…
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作…
- en: To store data in MongoDB, you will create a **database** and a **collection**.
    A database contains one or more collections, which are like tables in relational
    databases. Once a collection is created, you will write your data as documents.
    A collection contains documents, which are equivalent to rows in relational databases.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据存储到MongoDB中，你需要创建一个**数据库**和一个**集合**。一个数据库包含一个或多个集合，这些集合类似于关系数据库中的表。一旦创建集合，你将以文档形式写入数据。集合包含文档，文档相当于关系数据库中的行。
- en: 'Start by importing the necessary libraries:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入必要的库：
- en: '[PRE38]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Create a `MongoClient` instance to establish a connection to the database:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`MongoClient`实例以建立与数据库的连接：
- en: '[PRE39]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Create a new database named `stock_data` and a **time series collection** named
    `daily_stock`.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`stock_data`的新数据库，并创建一个名为`daily_stock`的**时间序列集合**。
- en: 'First, we create a **regular collection** in MongoDB:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在MongoDB中创建一个**常规集合**：
- en: '[PRE40]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This will create a new database called `stock_data` and a collection named amazon.
    If `stock_data` already exists, it will add the `amazon` collection to the existing
    database.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为`stock_data`的新数据库，并创建一个名为amazon的集合。如果`stock_data`已存在，它将把`amazon`集合添加到现有数据库中。
- en: However, since we are working with time series data, we can use a more efficient
    way to store and query our data by creating a **time series collection.** Starting
    with MongoDB version 5.0, time series collections are optimized for time-stamped
    data. We can modify the previous code to create a `daily_stock` time series collection.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们处理的是时间序列数据，我们可以通过创建一个**时间序列集合**来更高效地存储和查询数据。从MongoDB 5.0版本开始，时间序列集合已针对带时间戳的数据进行了优化。我们可以修改之前的代码来创建一个`daily_stock`时间序列集合。
- en: '[PRE41]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: With this update, we are now using a time series collection, which improved
    storage efficiency and query performance for time-based data like our stock prices.
    Moving forward, we’ll use the `ts` reference to interact with the time series
    collection.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过此次更新，我们现在使用时间序列集合，这提高了时间数据（如股票价格）的存储效率和查询性能。以后，我们将使用`ts`引用来与时间序列集合进行交互。
- en: 'You will utilize the `get_stock_data` function created in the Technical Requirements
    section to pull Amazon stock data from January 1, 2019, to August 31, 2024:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将利用在技术要求部分创建的`get_stock_data`函数，拉取从2019年1月1日到2024年8月31日的亚马逊股票数据：
- en: '[PRE42]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In pandas, we work with data in a tabular format, where each column represents
    a variable, and each row represents a data point. MongoDB, however, stores data
    as documents in a JSON-like format (BSON), where each document is an independent
    record that can include timestamp, metadata, and other key-value pairs.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 pandas 中，我们以表格格式处理数据，其中每一列代表一个变量，每一行代表一个数据点。而 MongoDB 则将数据存储为类似 JSON 的格式（BSON），其中每个文档是一个独立的记录，可以包含时间戳、元数据和其他键值对。
- en: 'Before inserting data into MongoDB, you need to transform the DataFrame into
    a list of dictionaries where each dictionary (or document) represents a stock
    data point. Each dictionary will include a timestamp (`Date`), stock information
    (e.g. `High`, `Low`), and metadata (e.g. `"ticker": "AMZN"`)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '在将数据插入 MongoDB 之前，你需要将 DataFrame 转换为一个字典列表，每个字典（或文档）表示一个股票数据点。每个字典将包含时间戳（`Date`）、股票信息（例如`High`、`Low`）和元数据（例如`"ticker":
    "AMZN"`）。'
- en: 'You will explore two options: the fist option utilizing the `to_dict()` method,
    and a second option were you iterate over the DataFrame.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你将探索两个选项：第一个选项使用`to_dict()`方法，第二个选项是遍历 DataFrame。
- en: 'Let’s explore the first option:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来探索第一个选项：
- en: '[PRE43]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here, we assumed that all the data have the same metadata information (e.g.
    `"ticker": "AMZN`).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们假设所有数据都有相同的元数据信息（例如`"ticker": "AMZN"`）。'
- en: The default value for the `orient` parameter in the `to_dict()` method is `dict`
    which produces a **dictionary** that follows the `{column -> {index -> value}}`
    pattern.
  id: totrans-173
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`to_dict()`方法中`orient`参数的默认值是`dict`，它会生成一个**字典**，其格式为`{column -> {index ->
    value}}`。'
- en: ''
  id: totrans-174
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: On the other hand, using `records` as the value, produces a **list** that follows
    the `[{column -> value}, … , {column -> value}]` pattern.
  id: totrans-175
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 另一方面，使用`records`作为值，会生成一个**列表**，它遵循`[{column -> value}, … , {column -> value}]`的模式。
- en: 'Now, let’s explore the second option which provides more flexibility for adding
    specific fields or applying transformation to individual records, for example,
    a different ticker value based:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来探索第二个选项，它提供了更多的灵活性，可以为特定字段添加数据或对单个记录进行转换，例如，基于不同的股票代码值：
- en: '[PRE44]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You now have a Python list of length `1426` (a dictionary for each record):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你有了一个长度为`1426`的 Python 列表（每个记录是一个字典）：
- en: '[PRE45]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now, you are ready to write to the time series `daily_stock` collection using
    the `insert_many()` method:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你已经准备好使用`insert_many()`方法将数据写入时间序列`daily_stock`集合中：
- en: '[PRE46]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You can validate that the database and collection are created with the following
    code:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用以下代码验证数据库和集合是否已创建：
- en: '[PRE47]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Next, pull Microsoft stock data (MSFT) and add it to the same `daily_stock`
    time series collection. Later, you will explore how the metadata can be used to
    distinguish between different stock symbols (AMZN vs MSFT) when querying the data.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，拉取微软的股票数据（MSFT）并将其添加到同一个`daily_stock`时间序列集合中。稍后，你将探索如何利用元数据在查询数据时区分不同的股票代码（如
    AMZN 与 MSFT）。
- en: '[PRE48]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You can check the total number of documents written by querying the database,
    as shown in the following code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过查询数据库来检查写入的文档总数，如下面的代码所示：
- en: '[PRE49]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Now, the collection contains data for two stock symbols. You can use metadata
    to filter for each symbol in your queries. You will start by querying the `daily_stock`
    collection to retrieve only the Microsoft (MSFT) stock data. This is where the
    `metadata` field becomes useful, allowing you to filter by the stock symbol. Let's
    first define a date range and then query for MSFT data only.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，集合中包含了两个股票代码的数据。你可以使用元数据在查询中为每个代码进行筛选。你将首先查询`daily_stock`集合，检索仅包含微软（MSFT）股票数据的记录。这时，`metadata`字段变得非常有用，允许你按股票代码进行筛选。让我们首先定义一个日期范围，然后仅查询
    MSFT 的数据。
- en: '[PRE50]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You can also perform an aggregation to calculate the average `Close` price
    per ticker (symbol):'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以进行聚合计算每个股票代码的平均`Close`价格：
- en: '[PRE51]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: How it works…
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: '`PyMongo` provides two insert functions to write our records as documents into
    a collection. These functions are as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyMongo`提供了两个插入函数，用于将我们的记录作为文档写入集合。这些函数如下：'
- en: '`insert_one()` inserts one document into a collection.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`insert_one()`会将一个文档插入到集合中。'
- en: '`insert_many()` inserts multiple documents into a collection.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`insert_many()`会将多个文档插入到集合中。'
- en: In the preceding example, you used `insert_many()` and passed the data to be
    written as documents simultaneously. However, before doing so, it was essential
    to convert the DataFrame to a list of dictionaries format that follows the `[{column
    -> value}, … , {column -> value}]` pattern. This was accomplished with `orient='records'`
    in the `to_dict()` DataFrame method.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，你使用了`insert_many()`并同时传入了要写入的数据作为文档。然而，在执行此操作之前，将DataFrame转换为符合`[{column
    -> value}, … , {column -> value}]`模式的字典列表格式是至关重要的。这是通过在`to_dict()` DataFrame方法中使用`orient='records'`来完成的。
- en: 'When documents are inserted into the database, they are assigned a unique `_id`
    value. MongoDB will automatically generate one during the insert operation if
    the document does not already have an `_id`. You can capture the generated `_id`
    because the insert functions return a result object—either an `InsertOneResult`
    for single inserts or an `InsertManyResultfor` bulk inserts. The following code
    demonstrates how this works using `insert_one` and the `InsertOneResult` class:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当文档插入数据库时，它们会被分配一个唯一的`_id`值。如果文档尚未拥有`_id`，MongoDB会在插入操作中自动生成一个。你可以捕获生成的`_id`，因为插入函数会返回一个结果对象——对于单次插入是`InsertOneResult`，对于批量插入是`InsertManyResult`。以下代码演示了如何使用`insert_one`和`InsertOneResult`类来实现这一点：
- en: '[PRE52]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The returned object is an instance of `InsertOneResult`; to see the actual
    value, you can use the `insert_id` property:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的对象是`InsertOneResult`的一个实例；要查看实际的值，你可以使用`insert_id`属性：
- en: '[PRE53]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: If you have ticker data in minutes, you can take advantage of the `granularity`
    attribute, which can be `seconds`, `minutes`, or `hours`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有分钟级的股票数据，你可以利用`granularity`属性，它可以是`seconds`、`minutes`或`hours`。
- en: There's more…
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'In the previous example, if you run the following code to query the database
    to list the collections available, you will see three collections:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，如果你运行以下代码查询数据库以列出可用的集合，你将看到三个集合：
- en: '[PRE54]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: You created the `daily_stock` collection, so what are the other two collections?
    Let’s first explore the **Bucket Pattern** in MongoDB.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你创建了`daily_stock`集合，那么另外两个集合是什么呢？让我们首先探讨一下MongoDB中的**桶模式**。
- en: The bucket patterns is a data modeling technique to optimize how data is stored
    in the database. By default, when you convert your DataFrame into a list of dictionaries,
    you are essentially inserting each DataFrame record (data point) as a separate
    MongoDB document. This creates a 1-on-1 mapping between records and documents.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 桶模式是一种数据建模技术，用来优化数据在数据库中的存储方式。默认情况下，当你将DataFrame转换为字典列表时，实际上是将每个DataFrame记录（数据点）作为一个独立的MongoDB文档插入。这会在记录和文档之间创建一对一的映射。
- en: However, the **bucket strategy** allows you to group related data points into
    a single document. For example, if you have hourly data, you can group it into
    a bucket, such as 24-hour period, and store all the data for that range in one
    document. Similarly, if we have sensor data from multiple devices, you can use
    the bucket pattern to group the data (for example, by device ID and by a time
    range) and insert them as a single document. This will reduce the number of documents
    in the database, improve overall performance, and simplify querying.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**桶策略**允许你将相关的数据点分组到一个文档中。例如，如果你有每小时的数据，你可以将它们分组到一个桶中，比如24小时周期，并将该时间范围内的所有数据存储在一个文档中。同样，如果我们有来自多个设备的传感器数据，你可以使用桶模式将数据（例如按设备ID和时间范围）分组，并将它们作为一个文档插入。这将减少数据库中文档的数量，提高整体性能，并简化查询。
- en: 'When you create a time series collection, MongoDB automatically applies the
    bucket pattern to store the data in an efficient format. Let’s break it down:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建一个时间序列集合时，MongoDB会自动应用桶模式，以高效的格式存储数据。让我们分解一下：
- en: '`daily_stock`: This is the main time series collection you created. It acts
    as a view that allows you to interact with your time series data using standard
    MongoDB operations.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`daily_stock`：这是你创建的主要时间序列集合。它充当视图，允许你使用标准的MongoDB操作与时间序列数据进行交互。'
- en: '`system.buckets.daily_stock`: This is an internal collection where MongoDB
    stores the actual time series data using the bucket pattern. MongoDB automatically
    implements this strategy for time series collections to improve storage and query
    performance. Here''s how it works:'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`system.buckets.daily_stock`：这是一个内部集合，MongoDB使用桶模式存储实际的时间序列数据。MongoDB会自动为时间序列集合实现这一策略，以提高存储和查询性能。它是如何工作的呢：'
- en: Documents are grouped into "buckets" based on timestamps and metadata fields
    (e.g., the stock symbol).
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档会根据时间戳和元数据字段（例如，股票符号）被分组到“桶”中。
- en: Each bucket contains data points close together in time and share the same metadata
    values.
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个桶包含时间上接近的数据点，并共享相同的元数据值。
- en: This bucketing strategy significantly reduces the number of documents stored,
    improving query efficiency and reducing disk usage.
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种分桶策略显著减少了存储的文档数量，提高了查询效率并减少了磁盘使用。
- en: '`system.views`: This is a system collection that MongoDB uses to store information
    about all views in the database, including the view for your time series collection.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`system.views`：这是一个系统集合，MongoDB 用来存储数据库中所有视图的信息，包括你的时间序列集合的视图。'
- en: 'To better understand how the bucket pattern is applied, let’s explore the technique
    by creating a new collection (a regular collection, not a time series collection)
    and bucket the daily stock data by year and month:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解桶模式的应用，我们将通过创建一个新的集合（常规集合，而非时间序列集合），并按年和月将每日股票数据分桶：
- en: '[PRE55]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, let’s create a new DataFrame and add two additional columns: **month**
    and **year**:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个新的 DataFrame，并添加两个额外的列：**month** 和 **year**：
- en: '[PRE56]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'In the preceding code, you added a `month` and `year` columns to the DataFrame
    and initiated a new collection as `stocks_bucket`. In the next code segment, you
    will loop through the data and write your groups (by year and month) as a single
    document:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，你向 DataFrame 添加了 `month` 和 `year` 列，并创建了一个名为 `stocks_bucket` 的新集合。在接下来的代码段中，你将循环遍历数据，并将按年和月分组的数据作为一个单一文档写入：
- en: '[PRE57]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: In the code, you looped through the unique year and month combinations, then
    for each combination you create a record dictionary containing the month, year,
    symbol, and a list of Close prices. The record is then inserted into the `stock_bucket`
    collection, effectively bucketing the data by month and year.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，你遍历了唯一的年和月组合，然后为每个组合创建一个包含月、年、符号和收盘价列表的记录字典。然后，该记录被插入到 `stock_bucket` 集合中，有效地按月和年对数据进行了分桶。
- en: 'To illustrate the difference in number of documents, run the following code:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明文档数量的差异，请运行以下代码：
- en: '[PRE58]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Notice the `stock_bucket` collection contains 72 documents, representing the
    data grouped by year and month.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`stock_bucket` 集合包含 72 个文档，代表按年和月分组的数据。
- en: 'To query the database for the year 2024 and the month of June and see how the
    document is represented, use the following code example:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要查询 2024 年和 6 月的数据，并查看文档如何表示，请使用以下代码示例：
- en: '[PRE59]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'You can also run the same query using MongoDB Compass, and you should get similar
    results as shown in the figure:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用 MongoDB Compass 运行相同的查询，结果应与图示中显示的类似：
- en: '![Figure – Using MongoDB Compass to query the stock_bucket collection](img/file41.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图示 – 使用 MongoDB Compass 查询 stock_bucket 集合](img/file41.png)'
- en: Figure – Using MongoDB Compass to query the stock_bucket collection
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图示 – 使用 MongoDB Compass 查询 stock_bucket 集合
- en: See also
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: 'For more information on storing time series data and bucketing in MongoDB,
    you can refer to this MongoDB blog post: [https://www.mongodb.com/blog/post/time-series-data-and-mongodb-part-2-schema-design-best-practices](https://www.mongodb.com/blog/post/time-series-data-and-mongodb-part-2-schema-design-best-practices)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欲了解更多关于在 MongoDB 中存储时间序列数据和分桶的信息，你可以参考这篇 MongoDB 博客文章：[https://www.mongodb.com/blog/post/time-series-data-and-mongodb-part-2-schema-design-best-practices](https://www.mongodb.com/blog/post/time-series-data-and-mongodb-part-2-schema-design-best-practices)
- en: Check out the MongoDB manual to learn more about **time series collections**
    [https://www.mongodb.com/docs/manual/core/timeseries-collections/](https://www.mongodb.com/docs/manual/core/timeseries-collections/)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看 MongoDB 手册，了解更多关于**时间序列集合**的信息 [https://www.mongodb.com/docs/manual/core/timeseries-collections/](https://www.mongodb.com/docs/manual/core/timeseries-collections/)
- en: Writing time series data to InfluxDB
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写入时间序列数据到 InfluxDB
- en: When working with large time series data, such as a sensor or **Internet of
    Things** (**IoT**) data, you will need a more efficient way to store and query
    such data for further analytics. This is where **time series databases** shine,
    as they are built exclusively to work with complex and very large time series
    datasets.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大规模时间序列数据时，如传感器或**物联网**（**IoT**）数据，你需要一种更高效的方式来存储和查询这些数据，以便进行进一步的分析。这就是**时间序列数据库**的优势所在，因为它们专门为处理复杂且非常大的时间序列数据集而构建。
- en: In this recipe, we will work with **InfluxDB** as an example of how to write
    to a time series database.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将以**InfluxDB**为例，演示如何写入时间序列数据库。
- en: Getting ready
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: You should refer to the recipe “*Reading data from a time series database*”
    in Chapter 3 “*Reading Time Series Data from Databases*” as a refresher on the
    different ways to connect to InfluxDB.
  id: totrans-237
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你应该参考第三章“*从数据库中读取时间序列数据*”中的教程“*从时间序列数据库中读取数据*”，以便复习连接InfluxDB的不同方式。
- en: 'You will be using the `ExtraSensory` dataset, a mobile sensory dataset made
    available by the University of California, San Diego: *Vaizman, Y., Ellis, K.,
    and Lanckriet, G. "Recognizing Detailed Human Context In-the-Wild from Smartphones
    and Smartwatches". IEEE Pervasive Computing, vol. 16, no. 4, October-December
    2017, pp. 62-74\. doi:10.1109/MPRV.2017.3971131*'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用`ExtraSensory`数据集，这是由加利福尼亚大学圣地亚哥分校提供的一个移动感应数据集：*Vaizman, Y., Ellis, K.,
    和 Lanckriet, G. “从智能手机和智能手表识别复杂的人类背景”。IEEE Pervasive Computing, vol. 16, no. 4,
    2017年10月至12月, pp. 62-74\. doi:10.1109/MPRV.2017.3971131*
- en: 'You can download the dataset here: [http://extrasensory.ucsd.edu/#download](http://extrasensory.ucsd.edu/#download)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里下载数据集：[http://extrasensory.ucsd.edu/#download](http://extrasensory.ucsd.edu/#download)
- en: 'The dataset consists of 60 files, each representing a participant, each identified
    by a unique identifier (UUID). Each file contains a total of 278 columns: 225
    (features), 51 (labels), and 2 (timestamp and label_source).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含60个文件，每个文件代表一个参与者，并通过唯一标识符（UUID）来识别。每个文件包含278列：225列（特征）、51列（标签）和2列（时间戳和标签来源）。
- en: 'This recipe aims to demonstrate how to write a time series DataFrame to InfluxDB.
    In this recipe, two columns are selected: the timestamp (date ranges from `2015-07-23`
    to `2016-06-02`, covering 152 days) and the watch accelerometer reading (measured
    in milli G-forces or milli-G).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的目标是演示如何将时间序列DataFrame写入InfluxDB。在这个教程中，选择了两列：时间戳（日期范围从`2015-07-23`到`2016-06-02`，共覆盖152天）和手表加速度计读数（以毫G为单位测量）。
- en: Before you can interact with InfluxDB in Python, you will need to install the
    InfluxDB Python library.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在你可以在Python中与InfluxDB交互之前，你需要安装InfluxDB Python库。
- en: 'You can install the library with **pip** by running the following command:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行以下命令，使用**pip**安装该库：
- en: '[PRE60]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'To install using **conda** use the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用**conda**安装，请使用以下命令：
- en: '[PRE61]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: How to do it…
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'You will start this recipe by reading a file from the **ExtraSensory** dataset
    (for a specific UUID) focusing on one feature column - the watch accelerometer.
    You will be performing some data transformations to prepare the data before writing
    the time series DataFrame to InfluxDB:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你将通过读取**ExtraSensory**数据集中的一个文件（针对特定的UUID）来开始这个教程，重点关注一个特征列——手表加速度计。你将进行一些数据转换，为将时间序列DataFrame写入InfluxDB做准备：
- en: 'Start by loading the required libraries:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先加载所需的库：
- en: '[PRE62]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The data consists of 60 compressed CSV files `(csv.gz`), which you can read
    using `pandas.read_csv()`. The default `compression` parameter in `read_csv` is
    set to `infer`. This means that pandas will infer based on the file extension
    which compression or decompression protocol to use. The files have a (`gz`) extension,
    which will be used to infer which decompression protocol to use. Alternatively,
    you can indicate which compression protocol to use with `compression='gzip'`.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集由60个压缩CSV文件（`csv.gz`）组成，你可以使用`pandas.read_csv()`读取这些文件。`read_csv`的默认`compression`参数设置为`infer`，这意味着pandas会根据文件扩展名推断使用哪种压缩或解压协议。文件扩展名为（`gz`），pandas会使用这个扩展名来推断需要使用的解压协议。或者，你也可以通过`compression='gzip'`明确指定使用哪种压缩协议。
- en: 'In the following code, you will read one of these files, select both `timestamp`
    and `watch_acceleration:magnitude_stats:mean` columns, rename the columns, and,
    finally, perform a **backfill** operation for all **na** (missing) values:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，你将读取这些文件中的一个，选择`timestamp`和`watch_acceleration:magnitude_stats:mean`两列，重命名这些列，最后，针对所有**na**（缺失）值执行**回填**操作：
- en: '[PRE63]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: From the preceding output, you have `3960` sensor readings from that one file.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，你有来自那个文件的`3960`个传感器读数。
- en: To write the data to InfluxDB, you need at least one `measurement` column and
    a `timestamp` column. Currently, the timestamp is a Unix timestamp (**epoch**)
    captured in seconds, which is an acceptable format for writing out data to InfluxDB.
    For example`, 2015-12-08 7:06:37 PM` is stored as `1449601597` in the dataset.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将数据写入InfluxDB，你需要至少一个`measurement`列和一个`timestamp`列。目前，时间戳是一个Unix时间戳（**epoch**），以秒为单位捕获，这是一个可接受的写入InfluxDB的数据格式。例如，`2015-12-08
    7:06:37 PM`在数据集中以`1449601597`的形式存储。
- en: '**InfluxDB** stores timestamps in epoch nanoseconds on disk, but when querying
    data, InfluxDB will display the data in **RFC3339 UTC format** to make it more
    human-readable. So, `1449601597` in **RFC3339** would be represented as `2015-12-08T19:06:37+00:00.000Z`.
    Note the precision in InfluxDB is in *nanoseconds*.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**InfluxDB** 在磁盘上以纪元纳秒存储时间戳，但在查询数据时，InfluxDB 将数据显示为**RFC3339 UTC格式**，以使其更易读。因此，在**RFC3339**中`1449601597`将表示为`2015-12-08T19:06:37+00:00.000Z`。请注意InfluxDB中的精度为*纳秒*。'
- en: 'In the following step, you will convert the Unix timestamp to a format that
    is more human readable for your analysis in **pandas**, which is also an acceptable
    format with InfluxDB:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，您将把Unix时间戳转换为在**pandas**中更易读的格式，这也是InfluxDB中可接受的格式：
- en: '[PRE64]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: In the preceding code, the `unit` parameter is set to `'s'` for **seconds**.
    This instructs pandas to calculate the number of seconds based on the origin.
    The `origin` parameter is set to `unix` by default, so the conversion will calculate
    the number of seconds to the Unix epoch start provided. The `utc` parameter is
    set to `True`, which will return a **UTC** `DatetimeIndex` type. The `dtype` of
    our DataFrame index is now `datetime64[ns, UTC]`.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`unit`参数设置为`'s'`用于**秒**。这指示pandas基于起点计算秒数。`origin`参数默认设置为`unix`，因此转换将计算到Unix纪元开始的秒数。`utc`参数设置为`True`，这将返回一个**UTC**的`DatetimeIndex`类型。我们的DataFrame索引的`dtype`现在是`datetime64[ns,
    UTC]`。
- en: You can learn more about Unix epoch timestamps in the recipe, *Working with
    Unix epoch timestamps*, from *Chapter 6*, *Working with Date and Time in Python*
  id: totrans-260
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您可以在*第6章*中的*Chapter 6*中的*Working with Unix epoch timestamps*中的食谱中了解有关Unix纪元时间戳的更多信息
- en: 'Next, you will need to establish a connection with the InfluxDB database instance
    running. All you need is to pass your API read/write token. When writing to the
    database, you will need to specify the bucket and organization name as well:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您需要建立与运行的InfluxDB数据库实例的连接。您只需传递您的API读/写令牌即可。在写入数据库时，您需要指定bucket和组织名称：
- en: '[PRE65]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Initialize the `write_api` and configure `WriterOptions`. This includes specifying
    `writer_type` as `SYNCHRONOUS`, `batch_size`, and `max_retries` before it fails:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`write_api`并配置`WriterOptions`。包括指定`writer_type`为`SYNCHRONOUS`，`batch_size`和`max_retries`，在失败之前：
- en: '[PRE66]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'To verify that the data is written properly you can query the database using
    the `query_data_frame` method as shown in the following code:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要验证数据是否正确写入，您可以使用`query_data_frame`方法查询数据库，如以下代码所示：
- en: '[PRE67]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Inspect the returned DataFrame:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 检查返回的DataFrame：
- en: '[PRE68]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Notice that the DataFrame has two `datetime64[ns, UTC]` type columns.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意 DataFrame 有两个`datetime64[ns, UTC]`类型的列。
- en: 'Now that you are done, you can close your writer object and shut down the client
    as shown:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在您完成了，可以关闭您的写入对象并关闭客户端，如下所示：
- en: '[PRE69]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: How it works…
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作原理…
- en: 'Before writing a pandas DataFrame to InfluxDB using the `write_api` you will
    need to define few things required in InfluxDB. This includes the following:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`write_api`将pandas DataFrame写入InfluxDB之前，您需要在InfluxDB中定义几个必需的事项。包括以下内容：
- en: '**Measurement**: These are the values you are tracking. InfluxDB accepts one
    measurement per data point.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测量**：这些是您要跟踪的值。InfluxDB 每个数据点接受一个测量。'
- en: '**Field**: We do not need to specify fields per se, since any columns not in
    the tag definition will be marked as fields. Fields are metadata objects stored
    as key-value pairs. Fields are not indexed, unlike tags.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字段**：我们不需要明确指定字段，因为未在标签定义中的任何列将被标记为字段。字段作为键值对存储的元数据对象。与标签不同，字段不被索引。'
- en: '**Tag** (optional): A metadata object in which you specify the columns that
    would get indexed for improved query performance. This is stored as a key-value
    pair as well.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签**（可选）：一个元数据对象，您可以指定要索引以提高查询性能的列。这也存储为键值对。'
- en: The **WriteAPI** supports *synchronous* and *asynchronous* writes. Additionally,
    the **WriteAPI** also provides several options when writing to InfluxDB (such
    as line protocol strings, line protocol bytes, data point structure, dictionary
    style, as well as support for pandas DataFrames). In the *Reading data from time
    series database* recipe in Chapter 3, *Reading Time Series Data from Databases*,
    you used the `query_data_frame()` method to specify that the results of the query
    should be returned as a **pandas DataFrame**.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**WriteAPI** 支持 *同步* 和 *异步* 写入。此外，**WriteAPI** 在写入 InfluxDB 时还提供了多个选项（例如，行协议字符串、行协议字节、数据点结构、字典样式，以及对
    `pandas` DataFrame 的支持）。在第3章的 *从时间序列数据库读取数据* 示例中，你使用了 `query_data_frame()` 方法，指定查询结果应该作为
    **pandas DataFrame** 返回。'
- en: 'Similarly, `write_api` provides additional parameters when writing a pandas
    DataFrames to InfluxDB:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，`write_api` 在将 `pandas` DataFrame 写入 InfluxDB 时提供了额外的参数：
- en: '`data_frame_measurement_name`: The name of the measurement for writing pandas
    DataFrames'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_frame_measurement_name`：用于写入 `pandas` DataFrame 的测量名称'
- en: '`data_frame_tag_columns`: The list of DataFrame columns that are tags; the
    rest of the columns will be fields'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_frame_tag_columns`：作为标签的 DataFrame 列表；其余列将作为字段'
- en: There's more…
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'In the previous example, we had to manually flush the data using `writer.close()`
    and terminate the connection using `client.close()`. For better resource management
    (for example, automatically closing the connection) and exception handling, you
    can benefit from using the `with` statement. The following example shows how you
    can rewrite the same code in a cleaner and more efficient format:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们需要手动使用 `writer.close()` 刷新数据，并使用 `client.close()` 终止连接。为了更好的资源管理（例如，自动关闭连接）和异常处理，你可以使用
    `with` 语句，享受它带来的便利。以下示例展示了如何以更清晰、更高效的格式重写相同的代码：
- en: '[PRE70]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: See also
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: 'To learn more about the InfluxDB line protocol, please refer to their documentation
    here: [https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/).'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于 InfluxDB 行协议的信息，请参阅他们的文档：[https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/)。
- en: 'To learn more about the Python API for InfluxDB 2.x, please refer to the official
    documentation here: [https://docs.influxdata.com/influxdb/cloud/tools/client-libraries/python/](https://docs.influxdata.com/influxdb/cloud/tools/client-libraries/python/).'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于 InfluxDB 2.x Python API 的信息，请参阅官方文档：[https://docs.influxdata.com/influxdb/cloud/tools/client-libraries/python/](https://docs.influxdata.com/influxdb/cloud/tools/client-libraries/python/)。
- en: Writing time series data to Snowflake
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将时间序列数据写入 Snowflake
- en: '**Snowflake** has become a very popular cloud database option for building
    big data analytics, due to its scalability, performance, and being SQL-oriented
    (a columnar-stored relational database).'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**Snowflake** 已成为构建大数据分析的热门云数据库选项，因其可扩展性、性能以及 SQL 导向（列存储关系数据库）。'
- en: Snowflake's connector for Python simplifies the interaction with the database
    whether it's for reading or writing data, or, more specifically, the built-in
    support for `pandas` DataFrames. In this recipe, you will use the sensor IoT dataset
    prepared in the *Writing time series data to InfluxDB* recipe. The technique applies
    to any `pandas` DataFrame that you plan to write to Snowflake.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake 的 Python 连接器简化了与数据库的交互，无论是读取数据还是写入数据，特别是对 `pandas` DataFrame 的内建支持。在这个示例中，你将使用在
    *将时间序列数据写入 InfluxDB* 示例中准备的传感器物联网数据集。这项技术适用于任何你打算写入 Snowflake 的 `pandas` DataFrame。
- en: Getting ready
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: You should refer to the recipe *Reading data from a Snowflake* in *Chapter 3*,
    *Reading Time Series Data from Databases* as a refresher on the different ways
    to connect to Snowflake.
  id: totrans-291
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以参考 *第3章* 中的 *从 Snowflake 读取数据* 示例，作为回顾，了解连接到 Snowflake 的不同方式。
- en: 'The recommended approach for the `snowflake-connector-python` library is to
    install it using **pip** allowing you to install *extras* such as `pandas` as
    shown:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `snowflake-connector-python` 库，推荐的安装方法是使用 **pip**，这允许你安装诸如 `pandas` 等 *额外*
    组件，如下所示：
- en: '[PRE71]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: You can also install with **conda**, but if you want to use `snowflake-connector-python`
    with pandas you will need to use the pip install.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过 **conda** 安装，但如果你想在使用 `snowflake-connector-python` 与 pandas 时，必须使用 pip
    安装。
- en: '[PRE72]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Create a configuration, for example `database.cfg` to store your Snowflake
    connection information as shown:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个配置文件，例如 `database.cfg`，用于存储你的 Snowflake 连接信息，如下所示：
- en: '[PRE73]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Using `ConfigParser`, extract the content under the `[SNOWFLAKE]` section to
    avoid exposing or hardcoding your credentials. Read parameters under the `[SNOWFLAKE]`
    section and convert to a Python dictionary as shown:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `ConfigParser`，提取 `[SNOWFLAKE]` 部分的内容，以避免暴露或硬编码你的凭证。读取 `[SNOWFLAKE]` 部分下的参数，并将其转换为
    Python 字典，如下所示：
- en: '[PRE74]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'You will utilize the `get_stock_data` function created in the *Technical Requirements*
    section to pull **Amazon** stock data from January 1, 2019 to August 31, 2024:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 你将利用在*技术要求*部分创建的 `get_stock_data` 函数，拉取 **Amazon** 从 2019 年 1 月 1 日到 2024 年
    8 月 31 日的股票数据：
- en: '[PRE75]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The `amzn_hist` DataFrame does not have a **Date** column, instead it has a
    `DatetimeIndex` . You will need to convert the index into a column since the API
    do not support writing index objects.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '`amzn_hist` DataFrame 没有 **Date** 列，而是有一个 `DatetimeIndex`。由于 API 不支持写入索引对象，你需要将索引转换为一列。'
- en: '[PRE76]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: You will be referencing the `amzn_hist` DataFrame and the object `params` throughout
    this recipe,
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，你将引用 `amzn_hist` DataFrame 和对象 `params`。
- en: How to do it…
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作…
- en: We will explore three (3) methods and libraries to connect to the Snowflake
    database. You will start by using the **Snowflake Python connector**, then explore
    the **Snowflake** **SQLAlchemy**, and finally you will explore the **Snowpark
    Python API**. Let’s get started.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索三种方法和库来连接到 Snowflake 数据库。你将首先使用 **Snowflake Python 连接器**，然后探索 **Snowflake**
    **SQLAlchemy**，最后探索 **Snowpark Python API**。让我们开始吧。
- en: Using snowflake-connector-python (write_pandas)
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 snowflake-connector-python（write_pandas）
- en: The recipe in this section will utilize the snowflake-connector-python library
    for connecting and writing data to a Snowflake database.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的食谱将利用 snowflake-connector-python 库来连接并将数据写入 Snowflake 数据库。
- en: 'Import the required libraries for this recipe:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入本食谱所需的库：
- en: '[PRE77]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The `pands_tools` module provides several functions for working with pandas
    DataFrames, this includes two writer methods (`write_pandas` and `pd_writer`).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`pands_tools` 模块提供了几个用于处理 pandas DataFrame 的函数，其中包括两个写入方法（`write_pandas` 和
    `pd_writer`）。'
- en: The `write_pandas` is a method for writing pandas DataFrame to a Snowflake database.
    Behind the scenes, the function will store the data to **Parquet** files, uploads
    the files to a **temporary stage**, and finally inserts the data from the files
    to the specified table via `COPY INTO` command.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_pandas` 是一个将 pandas DataFrame 写入 Snowflake 数据库的方法。背后，该函数会将数据存储为 **Parquet**
    文件，将文件上传到 **临时阶段**，然后通过 `COPY INTO` 命令将数据从文件插入到指定的表中。'
- en: On the other hand, the `pd_writer` method, is an insertion method for inserting
    data into a Snowflake database with the `DataFrame.to_sql()` method and passing
    a SQLAlchemy engine. You will explore `pd_writer` in the *Using SQLAlchemy* section
    following this recipe.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`pd_writer` 方法是一个插入方法，用于通过 `DataFrame.to_sql()` 方法将数据插入到 Snowflake 数据库，并传递一个
    SQLAlchemy 引擎。在本食谱后面的*使用 SQLAlchemy* 部分，你将探索 `pd_writer`。
- en: 'Establish a connection to your Snowflake database instance and create a cursor
    object:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立与 Snowflake 数据库实例的连接，并创建一个游标对象：
- en: '[PRE78]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: The `cursor` object will be used to execute a SQL query to verify the dataset
    has been properly written to the Snowflake database.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`cursor` 对象将用于执行 SQL 查询，以验证数据集是否已正确写入 Snowflake 数据库。'
- en: Write the amzn_hist DataFrame to Snowflake using the writer_pandas method. The
    method takes the connection object con, the DataFrame, destination table name,
    and other optional arguments such as auto_create_table, and table_type to name
    a few.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 writer_pandas 方法将 amzn_hist DataFrame 写入 Snowflake。该方法接受连接对象 con、DataFrame、目标表名及其他可选参数，如
    auto_create_table 和 table_type 等。
- en: '[PRE79]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'When using write_pandas it returns a tuple. In the previous code we unpacked
    the tuple into: success, nchunks, nrows, and copy_into. Let’s inspect the values
    inside these objects:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 write_pandas 时，它返回一个元组。在之前的代码中，我们将元组解包为：success、nchunks、nrows 和 copy_into。让我们查看这些对象内部的值：
- en: '[PRE80]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: The `success` objects is a boolean (True or False) to indicate whether the function
    successfully write the data to the specified table. The `nchunks` represents the
    number of chunks during the write process, which in this case, the entire data
    was written as one chunk. The `nrows` represents the number of rows inserted by
    the function. Lastly, the `copy_into` object contains the output of the `COPY
    INTO` command.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '`success` 对象是一个布尔值（True 或 False），用于指示函数是否成功将数据写入指定的表中。`nchunks` 表示写入过程中的块数，在本例中，整个数据作为一个块写入。`nrows`
    表示函数插入的行数。最后，`copy_into` 对象包含 `COPY INTO` 命令的输出。'
- en: Notice the use of `auto_create_table=True`, if this is not set to True and the
    table AMAZON did not already exist in Snowflake, `write_pandas` will through an
    error. When it is set to `True` we are explicitly asking `write_pandas` to create
    the table. Additionally, if the table exists, you can specify if you want to overwrite
    the existing table with the parameter `overwrite=True`.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `auto_create_table=True` 的使用，如果没有设置为 True 并且表 AMAZON 在 Snowflake 中不存在，`write_pandas`
    会抛出一个错误。当设置为 `True` 时，我们明确要求 `write_pandas` 创建该表。此外，如果表已经存在，你可以使用 `overwrite=True`
    参数指定是否希望覆盖现有的表。
- en: 'The `table_type` supports **permanent**, **temporary**, and **transient** table
    types in Snowflake. The parameter can take on of these values: `''temp''`, `''temporary''`,
    and `''transient''`. If an empty string is passed `table_type='''',` then it will
    create a **permanent** table (the default behavior).'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '`table_type` 支持 Snowflake 中的 **永久**、**临时** 和 **瞬态** 表类型。该参数可以采用以下值：`''temp''`、`''temporary''`
    和 `''transient''`。如果传递空字符串 `table_type=''''`，则会创建一个 **永久** 表（默认行为）。'
- en: 'You can further validate that all 1426 are written in the temporary table:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以进一步验证所有 1426 条记录是否已写入临时表：
- en: '[PRE81]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Indeed, you have all 1426 records written into the `AMAZON` table in Snowflake.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你已将所有 1426 条记录写入 Snowflake 中的 `AMAZON` 表。
- en: Using SQLAlchmey
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 SQLAlchemy
- en: The recipe in this section will utilize snowflake-sqlalchemy and the snowflake-connector-python
    libraries for connecting and writing data to a Snowflake database.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的示例将使用 snowflake-sqlalchemy 和 snowflake-connector-python 库来连接和将数据写入 Snowflake
    数据库。
- en: 'Import the required libraries for this recipe:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入本示例所需的库：
- en: '[PRE82]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'You will use the `URL` function from the Snowflake SQLAlchemy library to construct
    the connection string and create the SQLAlchemy **engine** to establish a connection
    to your Snowflake instance:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将使用 Snowflake SQLAlchemy 库中的 `URL` 函数来构建连接字符串并创建 SQLAlchemy **引擎**，以便与 Snowflake
    实例建立连接：
- en: '[PRE83]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Write the DataFrame to the Snowflake database using the `to_sql()` writer function.
    You will need to pass an **insertion method**; in this case, you will pass `pd_writer`
    :'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `to_sql()` 写入函数将数据框架写入 Snowflake 数据库。你需要传递一个 **插入方法**；在此情况下，你将传递 `pd_writer`：
- en: '[PRE84]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The preceding code uses standard `SQL INSERT` clauses one per row. The Snowflake
    connector API provides an insertion method, `pd_writer`, that you can pass to
    the method parameter in the `to_sql` method as shown:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用标准的 `SQL INSERT` 子句，每一行一个。Snowflake 连接器 API 提供了一个插入方法 `pd_writer`，你可以将其传递给
    `to_sql` 方法中的方法参数，如下所示：
- en: '[PRE85]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Behind the scenes, the `pd_writer` function will use the `write_pandas` function
    to write the DataFrame to the Snowflake database instead.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，`pd_writer` 函数将使用 `write_pandas` 函数将数据框架写入 Snowflake 数据库。
- en: 'To read and verify that the data was written, you can use `pandas.read_sql()`
    to query the table:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要读取并验证数据是否已写入，你可以使用 `pandas.read_sql()` 来查询表：
- en: '[PRE86]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The new DataFrame contains all 1426 records and exact number of columns expected.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 新的数据框架包含所有 1426 条记录以及预期的确切列数。
- en: Using snowflake-snowpark-python
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 snowflake-snowpark-python
- en: The recipe in this section will utilize the Snowpark API for writing a pandas
    DataFrame.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的示例将使用 Snowpark API 来写入 pandas 数据框架。
- en: 'Import the required libraries for this recipe:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入本示例所需的库：
- en: '[PRE87]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Create a session by establishing a connection with the Snowflake database
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过与 Snowflake 数据库建立连接来创建一个会话。
- en: '[PRE88]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Before writing the DataFrame, you must convert the pandas DataFrame to a Snowpark
    DataFrame.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在写入数据框架之前，你必须将 pandas 数据框架转换为 Snowpark 数据框架。
- en: '[PRE89]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Snowpark DataFrames uses lazy evaluation and provides many advantages over Panda
    DataFrames.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark 数据框架采用延迟计算，并提供了许多相对于 Panda 数据框架的优势。
- en: 'To write the Snowpark DataFrame, you can use the `write` and `save_as_table`
    methods:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要写入 Snowpark 数据框架，你可以使用 `write` 和 `save_as_table` 方法：
- en: '[PRE90]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'To read and verify that the data was written, you can use `session.table` to
    query the table:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要读取并验证数据是否已写入，你可以使用 `session.table` 来查询表：
- en: '[PRE91]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'If you are more comfortable with pandas, then you can convert the Snowpark
    DataFrame to a pandas DataFrame using the `to_pandas` method as shown:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更习惯使用 pandas，你可以使用 `to_pandas` 方法将 Snowpark 数据框架转换为 pandas 数据框架，如下所示：
- en: '[PRE92]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: The DataFrame contains all 1426 records and all six (6) columns as expected.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框架包含所有 1426 条记录和预期的六（6）列。
- en: How it works...
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The Snowflake Python API provides two mechanisms for writing pandas DataFrames
    to Snowflake, which are provided to you in the `pandas_tools` module:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake Python API 提供了两种将 pandas 数据框架写入 Snowflake 的机制，这些机制包含在 `pandas_tools`
    模块中：
- en: '[PRE93]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'In the recipe, you used `pd_writer` and passed it as an **insertion method**
    to the `DataFrame.to_sql()` writer function. When using `pd_writer` within `to_sql()`,
    you can change the insertion behavior through the `if_exists` parameter, which
    takes three arguments:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，你使用了 `pd_writer` 并将其作为**插入方法**传递给 `DataFrame.to_sql()` 写入函数。当在 `to_sql()`
    中使用 `pd_writer` 时，你可以通过 `if_exists` 参数来改变插入行为，该参数有三个选项：
- en: '`fail`, which raises `ValueError` if the table exists'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fail`，如果表格已存在，则引发`ValueError`错误'
- en: '`replace`, which drops the table before inserting new values'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replace`，在插入新值之前会删除表格'
- en: '`append`, which inserts the data into the existing table'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`append`，将数据插入到现有表格中'
- en: If the table doesn't exist, SQLAlchemy takes care of creating the table for
    you and maps the data types from pandas DataFrames to the appropriate data types
    in the Snowflake database. This is also true when reading the data from Snowflake
    using the SQLAlchemy engine through `pandas.read_sql()`.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 如果表格不存在，SQLAlchemy 会为你创建表格，并将 pandas DataFrame 中的数据类型映射到 Snowflake 数据库中的相应数据类型。通过
    `pandas.read_sql()` 使用 SQLAlchemy 引擎从 Snowflake 读取数据时也适用此规则。
- en: Note that `pd_writer` uses the `write_pandas` function behind the scenes. They
    both work by dumping the DataFrame into Parquet files, uploading them to a temporary
    stage, and, finally, copying the data into the table via `COPY INTO`.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`pd_writer` 在后台使用了 `write_pandas` 函数。它们的工作方式都是将 DataFrame 转储为 Parquet 文件，上传到临时阶段，最后通过
    `COPY INTO` 将数据复制到表格中。
- en: 'You used the `write.mode()` method when using the Snowpark API to write the
    DataFrame. The `mode()` method accepts different write mode options:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用 Snowpark API 写入 DataFrame 时，你使用了 `write.mode()` 方法。`mode()` 方法接受不同的写入模式选项：
- en: '`append`: Append data of the DataFrame to the existing table. If the table
    does not exist, it will be created.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`append`：将 DataFrame 的数据追加到现有表格。如果表格不存在，它将被创建。'
- en: '`overwrite`: Overwrite the existing table by **dropping** the old table.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overwrite`：通过**删除**旧表来覆盖现有表格。'
- en: '`truncate`: Overwrite the existing table by **truncating** the old table.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncate`：通过**截断**旧表来覆盖现有表格。'
- en: '`errorifexists`: Throw an exception **error** if the table already exists.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errorifexists`：如果表格已存在，则抛出异常**错误**。'
- en: '`ignore`: Ignore the operation if the table already exists.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore`：如果表格已存在，则忽略该操作。'
- en: Keep in mind the default value is `errorifexists`.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，默认值是 `errorifexists`。
- en: There's more...
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'There is a useful method in Snowpark to write a pandas DataFrame directly without
    the need to convert it into a Snowpark DataFrame:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Snowpark 中有一个有用的方法，可以直接写入 pandas DataFrame，而无需将其转换为 Snowpark DataFrame：
- en: '[PRE94]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: The `write_pandas` function writes the pandas DataFrame to Snowflake and returns
    a Snowpark DataFrame object.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_pandas` 函数将 pandas DataFrame 写入 Snowflake，并返回一个 Snowpark DataFrame 对象。'
- en: See also
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: 'Visit the Snowflake documentation to learn more about `write_pandas` and `pd_write`
    methods: [https://docs.snowflake.com/en/user-guide/python-connector-api.html#write_pandas](https://docs.snowflake.com/en/user-guide/python-connector-api.html#write_pandas).'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问 Snowflake 文档，了解更多关于 `write_pandas` 和 `pd_write` 方法的信息：[https://docs.snowflake.com/en/user-guide/python-connector-api.html#write_pandas](https://docs.snowflake.com/en/user-guide/python-connector-api.html#write_pandas)。
- en: 'You can learn more about the `pandas DataFrame.to_sql()` function here: [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html).'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在这里了解更多关于 `pandas DataFrame.to_sql()` 函数的信息：[https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html)。
- en: To learn more about the write_pandas method from the Snowpark API refer to the
    official documentation here [https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/1.22.1/snowpark/api/snowflake.snowpark.Session.write_pandas](ch006.xhtml)
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于 Snowpark API 中 `write_pandas` 方法的信息，请参阅官方文档：[https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/1.22.1/snowpark/api/snowflake.snowpark.Session.write_pandas](ch006.xhtml)
