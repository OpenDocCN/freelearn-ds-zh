- en: Chapter 4.  Unified Data Access
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：统一数据访问
- en: Data integration from disparate data sources had always been a daunting feat.
    The three V's of big data and ever-shrinking processing time frames have made
    the task even more challenging. Delivering a clear view of well-curated data in
    near real time is extremely important for business. However, real-time curated
    data along with the ability to perform different operations such as ETL, ad hoc
    querying, and machine learning in a unified fashion is what is emerging as a key
    business differentiator.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 来自不同数据源的数据集成一直是一个艰巨的任务。大数据的三大特征（量、速度、种类）和不断缩短的处理时间框架使得这一任务更加具有挑战性。以接近实时的方式提供清晰且精心整理的数据视图对于企业至关重要。然而，实时整理的数据以及以统一方式执行不同操作（如
    ETL、临时查询和机器学习）的能力，正在成为企业的关键差异化因素。
- en: Apache Spark was created to offer a single general-purpose engine that can process
    data from a variety of data sources and support large-scale data processing for
    various different operations. Spark enables developers to combine SQL, Streaming,
    graphs, and machine learning algorithms in a single workflow!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 的创建旨在提供一个通用引擎，能够处理来自各种数据源的数据，并支持大规模的数据处理，适用于各种不同的操作。Spark 使得开发人员能够将
    SQL、流处理、图计算和机器学习算法结合到一个工作流中！
- en: 'In the previous chapters, we discussed **Resilient Distributed Datasets** (**RDDs**)
    as well as DataFrames. In [Chapter 3](ch03.xhtml "Chapter 3.  Introduction to
    DataFrames"), *Introduction to DataFrames*, we introduced Spark SQL and the Catalyst
    optimizer. This chapter builds on this foundation and delves deeper into these
    topics to help you realize the real essence of unified data access. We''ll introduce
    new constructs such as Datasets and Structured Streaming. Specifically, we''ll
    discuss the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了**弹性分布式数据集**（**RDD**）以及数据框（DataFrames）。在[第三章](ch03.xhtml "第三章：数据框简介")，*数据框简介*中，我们介绍了
    Spark SQL 和 Catalyst 优化器。本章在此基础上进行扩展，深入探讨这些主题，帮助你理解统一数据访问的真正本质。我们将介绍新概念，如数据集（Datasets）和结构化流处理（Structured
    Streaming）。具体来说，我们将讨论以下内容：
- en: Data abstractions in Apache Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 中的数据抽象
- en: Datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集
- en: Working with Datasets
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据集
- en: Dataset API limitations
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集 API 限制
- en: Spark SQL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL
- en: SQL operations
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL 操作
- en: Under the hood
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底层实现
- en: Structured Streaming
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流处理
- en: Spark streaming programming model
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 流处理编程模型
- en: Under the hood
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底层实现
- en: Comparison with other streaming engines
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他流处理引擎的比较
- en: Continuous applications
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续应用
- en: Summary
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: Data abstractions in Apache Spark
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 中的数据抽象
- en: The MapReduce framework and its popular open source implementation Hadoop enjoyed
    widespread adoption in the past decade. However, iterative algorithms and interactive
    ad-hoc querying are not well supported. Any data sharing between jobs or stages
    within an algorithm is always through disk writes and reads as against in-memory
    data sharing. So, the logical next step would be to have a mechanism that facilitates
    reuse of intermediate results across multiple jobs. RDD is a general-purpose data
    abstraction that was developed to address this requirement.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 框架及其流行的开源实现 Hadoop 在过去十年中得到了广泛的应用。然而，迭代算法和交互式临时查询并不被很好地支持。在算法中的作业或阶段之间，任何数据共享总是通过磁盘读写实现，而不是内存数据共享。因此，逻辑上的下一步应该是拥有一种机制，能够在多个作业之间复用中间结果。RDD
    是一种通用数据抽象，旨在解决这一需求。
- en: RDD is the core abstraction in Apache Spark. It is an immutable, fault-tolerant
    distributed collection of statically typed objects that are usually stored in-memory.
    RDD API offer simple operations such as map, reduce, and filter that can be composed
    in arbitrary ways.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 是 Apache Spark 中的核心抽象。它是一个不可变的、容错的、分布式的静态类型对象集合，通常存储在内存中。RDD API 提供了简单的操作，如
    map、reduce 和 filter，这些操作可以按任意方式组合。
- en: DataFrame abstraction is built on top of RDD and it adds "named" columns. So,
    a Spark DataFrame has rows of named columns similar to relational database tables
    and DataFrames in R and Python (pandas). This familiar higher level abstraction
    makes the development effort much easier because it lets you perceive data like
    an SQL table or an Excel file. Moreover, the Catalyst optimizer, under the hood,
    compiles the operations and generates JVM bytecode for efficient execution. However,
    the named columns approach gives rise to a new problem. Static type information
    is no longer available to the compiler, and hence we lose the advantage of compile-time
    type safety.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 抽象是在 RDD 之上构建的，并增加了“命名”列。因此，Spark DataFrame 具有类似于关系型数据库表格和 R、Python（pandas）中的
    DataFrame 的命名列行。这种熟悉的高级抽象大大简化了开发工作，因为它让你可以像对待 SQL 表或 Excel 文件一样处理数据。此外，Catalyst
    优化器在背后会编译操作并生成 JVM 字节码，以实现高效执行。然而，命名列的方法带来了一个新问题。静态类型信息不再对编译器可用，因此我们失去了编译时类型安全的优势。
- en: Dataset API was introduced to combine the best traits from both RDDs and DataFrames
    plus some more features of its own. Datasets provide row and column data abstraction
    similar to the DataFrames, but with a structure defined on top of them. This structure
    may be defined by a case class in Scala or a class in Java. They provide type
    safety and lambda functions like RDDs. So, they support both typed methods such
    as `map` and `groupByKey` as well as untyped methods such as `select` and `groupBy`.
    In addition to the Catalyst optimizer, Datasets leverage in-memory encoding provided
    by the Tungsten execution engine, which improves performance even further.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Dataset API 的引入结合了 RDD 和 DataFrame 的优点，并增加了一些独特的功能。Datasets 提供了类似于 DataFrame
    的行列数据抽象，但在其之上定义了结构。这个结构可以通过 Scala 中的 case class 或 Java 中的类来定义。它们提供了类型安全和类似 RDD
    的 Lambda 函数。因此，它们支持类型化方法，如`map`和`groupByKey`，也支持非类型化方法，如`select`和`groupBy`。除了
    Catalyst 优化器外，Datasets 还利用了 Tungsten 执行引擎提供的内存编码，这进一步提升了性能。
- en: The data abstractions introduced so far form the core abstractions. There are
    some more specialized data abstractions that work on top of these abstractions.
    Streaming APIs are introduced to process real-time streaming data from various
    sources such as Flume and Kafka. These APIs work together to provide data engineers
    a unified, continuous DataFrame abstraction that can be used for interactive and
    batch queries. Another example of specialized data abstraction is a GraphFrame.
    This enables developers to analyze social networks and any other graphs alongside
    Excel-like two-dimensional data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，介绍的数据抽象构成了核心抽象。在这些抽象之上，还有一些更为专门化的数据抽象。Streaming API 被引入用于处理来自 Flume 和
    Kafka 等各种来源的实时流数据。这些 API 协同工作，为数据工程师提供了一个统一的、连续的 DataFrame 抽象，可以用于交互式和批量查询。另一种专门化的数据抽象是
    GraphFrame，它使开发者能够分析社交网络和其他图形数据，同时处理类似 Excel 的二维数据。
- en: 'Now with the basics of the available data abstractions in mind, let''s understand
    what we exactly mean by a unified data access platform:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，了解了可用数据抽象的基本概念后，让我们来理解“统一数据访问平台”到底意味着什么：
- en: '![Data abstractions in Apache Spark](img/image_04_001.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark 中的数据抽象](img/image_04_001.jpg)'
- en: The intention behind this unified platform is that it not only lets you combine
    the static and streaming data together, but also allows various different kinds
    of operations on the data in a unified way! From the developer's perspective,
    a Dataset is the core abstraction to work with, and Spark SQL is the main interface
    to the Spark functionality. A two-dimensional data structure coupled with a SQL
    declarative programming interface had been a familiar way of dealing with data,
    thereby shortening the learning curve for the data engineers. So, understanding
    the unified platform translates to understanding Datasets and Spark SQL.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个统一平台的目的是不仅可以将静态数据和流式数据结合在一起，还可以以统一的方式对数据进行各种不同类型的操作！从开发者的角度来看，Dataset 是核心抽象，Spark
    SQL 是与 Spark 功能交互的主要接口。结合 SQL 声明式编程接口的二维数据结构已成为处理数据的常见方式，从而缩短了数据工程师的学习曲线。因此，理解统一平台就意味着理解
    Datasets 和 Spark SQL。
- en: Datasets
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Datasets
- en: Apache Spark **Datasets** are an extension of the DataFrame API that provide
    a type-safe object-oriented programming interface. This API was first introduced
    in the 1.6 release. Spark 2.0 version brought out unification of DataFrame and
    Dataset APIs. DataFrame becomes a generic, untyped Dataset; or a Dataset is a
    DataFrame with an added structure. The term "structure" in this context refers
    to a pattern or an organization of underlying data, more like a table schema in
    RDBMS parlance. The structure imposes a limit on what can be expressed or contained
    in the underlying data. This in turn enables better optimizations in memory organization
    as well as physical execution. Compile-time type checking leads to catching errors
    earlier than during runtime. For example, a type mismatch in a SQL comparison
    does not get caught until runtime, whereas it would be caught during compile time
    itself if it were expressed as a sequence of operations on Datasets. However,
    the inherent dynamic nature of Python and R implies that there is no compile-time
    type safety, and hence the concept Datasets does not apply to those languages.
    The unification of Datasets and DataFrames applies to Scala and Java API only.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark **数据集**是 DataFrame API 的扩展，提供了一种类型安全的面向对象编程接口。这个 API 最早在 1.6 版本中引入。Spark
    2.0 版本实现了 DataFrame 和 Dataset API 的统一。DataFrame 变成了一个通用的、无类型的数据集；或者说，数据集是一个添加了结构的
    DataFrame。这里的“结构”一词指的是底层数据的模式或组织，类似于关系型数据库中的表模式。结构对底层数据中可以表达或包含的内容施加了限制。这反过来能够在内存组织和物理执行上实现更好的优化。编译时的类型检查可以比运行时更早地捕获错误。例如，SQL
    比较中的类型不匹配直到运行时才会被发现，而如果它是作为数据集操作序列表达的，则会在编译时被捕获。然而，Python 和 R 的固有动态特性意味着这些语言没有编译时类型安全，因此数据集的概念不适用于这些语言。数据集和
    DataFrame 的统一仅适用于 Scala 和 Java API。
- en: At the core of Dataset abstraction are the **encoders**. These encoders translate
    between JVM objects and Spark's internal Tungsten binary format. This internal
    representation bypasses JVM's memory management and garbage collection. Spark
    has its own C-style memory access that is specifically written to address the
    kind of workflows it supports. The resultant internal representations take less
    memory and have efficient memory management. Compact memory representation leads
    to reduced network load during shuffle operations. The encoders generate compact
    byte code that directly operates on serialized objects without de-serializing,
    thereby enhancing performance. Knowing the schema early on results in a more optimal
    layout in memory when caching Datasets.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集抽象的核心是 **编码器**。这些编码器用于在 JVM 对象和 Spark 内部的 Tungsten 二进制格式之间进行转换。此内部表示绕过了 JVM
    的内存管理和垃圾回收。Spark 有自己专门为其支持的工作流编写的 C 风格内存访问方式。最终的内部表示占用更少的内存，并且具有高效的内存管理。紧凑的内存表示在
    Shuffle 操作中减少了网络负载。编码器生成的紧凑字节码直接在序列化对象上操作，而无需反序列化，从而提高了性能。提前了解模式能够在缓存数据集时实现更优化的内存布局。
- en: Working with Datasets
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用数据集
- en: In this section, we will create Datasets and perform transformations and actions,
    much like DataFrames and RDDs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建数据集，并执行转换和操作，类似于 DataFrame 和 RDD。
- en: 'Example 1-creating a Dataset from a simple collection:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 1 - 从简单集合创建数据集：
- en: '**Scala:**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala：**'
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As shown in the last example in the preceding code, `case class` adds structure
    information. Spark uses this structure to create the best data layout and encoding.
    The following code shows us the structure and the plan for execution:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面代码中的最后一个示例所示，`case class` 添加了结构信息。Spark 使用此结构来创建最佳的数据布局和编码。以下代码展示了我们要查看的结构和执行计划：
- en: '**Scala:**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala：**'
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding example shows the structure and the implementation physical plan
    as anticipated. If you want to get a more detailed execution plan, you have to
    pass explain (true), which prints extended information, including the logical
    plan as well.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例展示了预期的结构和物理执行计划。如果你想查看更详细的执行计划，必须传入 explain（true），这会打印扩展信息，包括逻辑计划。
- en: We have examined Dataset creation from simple collections and RDDs. We have
    already discussed that DataFrames are just untyped Datasets. The following examples
    show conversion between Datasets and DataFrames.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经检查了如何从简单集合和 RDD 创建数据集。我们已经讨论过，DataFrame 只是无类型的数据集。以下示例展示了数据集和 DataFrame
    之间的转换。
- en: Example 2-converting the Dataset to a DataFrame
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2 - 将数据集转换为 DataFrame
- en: '**Scala:**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala：**'
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This example looks very much like the examples we have seen in [Chapter 3](ch03.xhtml
    "Chapter 3.  Introduction to DataFrames"), *Introduction to DataFrames*. These
    conversions become very handy in the real world. Consider adding a structure (aka
    case class) to imperfect data. You may first read that data into a DataFrame,
    perform cleansing, and then convert it to a Dataset. Another use case could be
    that you want to expose only a subset (rows and columns) of the data based on
    some runtime information, such as `user_id`. You could read the data into a DataFrame,
    register it as a temporary table, apply conditions, and expose the subset as a
    Dataset. The following example creates a `DataFrame` first and then converts it
    into `Dataset`. Note that the DataFrame column names must match the case class.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例与我们在[第 3 章](ch03.xhtml "第 3 章 数据框介绍")，*数据框介绍*中看到的示例非常相似。这些转换在现实世界中非常实用。考虑为不完美的数据添加一个结构（也叫案例类）。您可以先将这些数据读取到
    DataFrame 中，进行清洗，然后转换成 Dataset。另一个应用场景是，您可能希望根据某些运行时信息（如 `user_id`）仅暴露数据的子集（行和列）。您可以将数据读取到
    DataFrame 中，将其注册为临时表，应用条件，然后将子集暴露为 Dataset。以下示例首先创建一个 `DataFrame`，然后将其转换为 `Dataset`。请注意，DataFrame
    的列名必须与案例类匹配。
- en: Example 3-convert a DataFrame to a Dataset
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 3 - 将 DataFrame 转换为 Dataset
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The explain command response shows `WholeStageCodegen`, which fuses multiple
    operations into a single Java function call. This enhances performance due to
    reduction in multiple virtual function calls. Code generation had been around
    in Spark engine since 1.1, but at that time it was limited to expression evaluation
    and a small number of operations such as filter. In contrast, whole stage code
    generation from Tungsten generates code for the entire query plan.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`explain` 命令的响应显示 `WholeStageCodegen`，它将多个操作融合成一个 Java 函数调用。由于减少了多个虚拟函数调用，这增强了性能。代码生成自
    Spark 1.1 以来就存在，但当时仅限于表达式评估和少量操作，如过滤。而与此不同，Tungsten 的整个阶段代码生成会为整个查询计划生成代码。'
- en: Creating Datasets from JSON
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 JSON 创建 Datasets
- en: Datasets can be created from JSON files, similar to DataFrames. Note that a
    JSON file may contain several records, but each record has to be on one line.
    If your source JSON has newlines, you have to programmatically remove them. The
    JSON records may have arrays and may be nested. They need not have uniform schema.
    The following example file has JSON records with one record having an additional
    tag and an array of data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以通过 JSON 文件创建，类似于 DataFrame。请注意，一个 JSON 文件可以包含多个记录，但每条记录必须在一行内。如果源 JSON
    文件中有换行符，您需要通过编程手段去除它们。JSON 记录可能包含数组，并且可能是嵌套的。它们不需要具有统一的模式。以下示例文件包含 JSON 记录，其中一条记录具有额外的标签和数据数组。
- en: Example 4-creating a Dataset from JSON
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 4 - 从 JSON 创建 Dataset
- en: '**Scala:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Datasets API's limitations
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Datasets API 的限制
- en: 'Even though the Datasets API is created using the best of both RDDs and DataFrames,
    it still has some limitations as of its current stage of development:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 Datasets API 已经结合了 RDD 和 DataFrame 的优势，但它仍然存在一些局限性，特别是在当前的开发阶段：
- en: While querying the dataset, the selected fields should be given specific data
    types as in the case class, or else the output will become a DataFrame. An example
    is `auth.select(col("first_name").as[String])`.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在查询数据集时，选定的字段应赋予与案例类相同的特定数据类型，否则输出将变成 DataFrame。例如，`auth.select(col("first_name").as[String])`。
- en: Python and R are inherently dynamic in nature, and hence typed Datasets do not
    fit in.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 和 R 本质上是动态的，因此类型化的 Datasets 并不适用。
- en: Spark SQL
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: '**Spark SQL** is a Spark module for structured data processing that was introduced
    in Spark 1.0\. This module is a tightly integrated relational engine that inert-operates
    with the core Spark API. It enables data engineers to write applications that
    load structured data from disparate sources and join them as a unified, and possibly
    continuous, Excel-like data frames; and then they can implement complex ETL workflows
    and advanced analytics.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark SQL** 是 Spark 1.0 引入的一个用于结构化数据处理的 Spark 模块。这个模块是一个紧密集成的关系引擎，与核心 Spark
    API 协同工作。它使得数据工程师可以编写应用程序，从不同的来源加载结构化数据并将它们连接成统一的，可能是连续的，类似 Excel 的数据框；然后他们可以实施复杂的
    ETL 工作流和高级分析。'
- en: The Spark 2.0 release brought in significant unification of APIs and expanded
    the SQL capabilities, including support for subqueries. The Dataset API and DataFrames
    API are now unified, with DataFrames being a "kind" of Datasets. The unified APIs
    build the foundation for Spark's future, spanning across all libraries. Developers
    can impose "structure" onto their data and can work with high-level declarative
    APIs, thereby improving performance as well as their productivity. The performance
    gains come as a result of the underlying optimization layer. DataFrames, Datasets,
    and SQL share the same optimization and execution pipeline.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0 版本带来了 API 的重要统一，并扩展了 SQL 功能，包括支持子查询。现在，数据集 API 和数据框架 API 已经统一，数据框架是数据集的一种“形式”。这些统一的
    API 为 Spark 的未来奠定了基础，涵盖了所有库。开发者可以为他们的数据施加“结构”，并可以使用高级声明式 API，从而提高性能和生产力。性能的提升来源于底层的优化层。数据框架、数据集和
    SQL 共享相同的优化和执行管道。
- en: SQL operations
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SQL 操作
- en: SQL operations are most widely used constructs for data manipulation. Some of
    most used operations are, selecting all or some of the columns, filtering based
    on one or more conditions, sorting and grouping operations, and computing summary
    functions such as `average` on GroupedData. The  `JOIN` operations on multiple
    data sources and `set` operations such as `union`, `intersect` and `minus` are
    some other operations that are widely performed. Furthermore, data frames are
    registered as temporary tables and passed traditional SQL statements to perform
    the aforementioned operations. **User-Defined Functions** (**UDF**) are defined
    and used with and without registration. We'll be focusing on window operations,
    which have been just introduced in Spark 2.0\. They address sliding window operations.
    For example, if you want to report the average peak temperature every day in the
    past seven days, then you are operating on a sliding window of seven days until
    today. Here is an example that computes average sales per month for the past three
    months. The data file contains 24 observations showing monthly sales for two products,
    P1 and P2.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 操作是最广泛使用的数据处理构造。常见的操作包括选择所有或部分列、根据一个或多个条件进行过滤、排序和分组操作，以及计算诸如`average`等汇总函数在分组数据上的应用。`JOIN`操作用于多个数据源之间的操作，`set`操作如`union`、`intersect`和`minus`也是常见的操作。此外，数据框架作为临时表注册，并通过传统的
    SQL 语句执行上述操作。**用户定义函数**（**UDF**）可以在注册与未注册的情况下定义和使用。我们将重点关注窗口操作，这些操作在 Spark 2.0
    中刚刚引入，主要用于滑动窗口操作。例如，如果你想报告过去七天内每天的平均峰值温度，那么你就是在一个滑动的七天窗口上操作，直到今天为止。以下是一个示例，计算过去三个月的每月平均销售额。数据文件包含
    24 个观测值，显示了 P1 和 P2 两个产品的每月销售数据。
- en: Example 5-window example with moving average computation
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 5-窗口示例与移动平均计算
- en: '**Scala:**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala：**'
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Python:**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python：**'
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Under the hood
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 底层原理
- en: When a developer is writing programs using RDD API, efficient execution for
    the workload on hand is his/her responsibility. The data types and computations
    are not available for Spark. In contrast, when a developer is using DataFrames
    and Spark SQL, the underlying engine has information about the schema and operations.
    In this case, the developer can write less code while the optimizer does all the
    hard work.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当开发者使用 RDD API 编写程序时，工作负载的高效执行是开发者的责任。Spark 并不提供数据类型和计算的支持。而当开发者使用数据框架和 Spark
    SQL 时，底层引擎已经了解模式和操作信息。在这种情况下，开发者可以写更少的代码，同时优化器负责处理所有复杂工作。
- en: The Catalyst optimizer contains libraries for representing trees and applying
    rules to transform the trees. These tree transformations are applied to create
    the most optimized logical and physical execution plans. In the final phase, it
    generates Java bytecode using a special feature of the Scala language called **quasiquotes**.
    The optimizer also enables external developers to extend the optimizer by adding
    data-source-specific rules that result in pushing operations to external systems,
    or support for new data types.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst 优化器包含用于表示树并应用规则进行转换的库。这些树的转换应用于创建最优化的逻辑和物理执行计划。在最终阶段，它使用 Scala 语言的特殊特性
    **quasiquotes** 生成 Java 字节码。优化器还允许外部开发者通过添加特定于数据源的规则来扩展优化器，这些规则会将操作推送到外部系统，或者支持新的数据类型。
- en: 'The Catalyst optimizer arrives at the most optimized plan to execute the operations
    on hand. The actual execution and related improvements are provided by the Tungsten
    engine. The goal of Tungsten is to improve the memory and CPU efficiency of Spark
    backend execution. The following are some salient features of this engine:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst 优化器会生成最优化的计划来执行当前操作。实际执行和相关改进由 Tungsten 引擎提供。Tungsten 的目标是提高 Spark
    后端执行的内存和 CPU 效率。以下是该引擎的一些显著特点：
- en: Reducing the memory footprint and eliminating garbage collection overheads by
    bypassing (off-heap) Java memory management.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过绕过（堆外）Java 内存管理来减少内存占用并消除垃圾回收的开销。
- en: Code generation fuses across multiple operators and too many virtual function
    calls are avoided. The generated code looks like hand-optimized code.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码生成跨多个操作符融合，并避免了过多的虚拟函数调用。生成的代码看起来像手动优化过的代码。
- en: Memory layout is in columnar, in-memory parquet format because that enables
    vectorized processing and is also closer to usual data access operations.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存布局采用列式存储的内存中 Parquet 格式，因为这能够支持矢量化处理，且更贴近常见的数据访问操作。
- en: In-memory encoding using encoders. Encoders use runtime code generation to build
    custom byte code for faster and compact serialization and deserialization. Many
    operations can be performed in-place without deserialization because they are
    already in Tungsten binary format.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编码器进行内存中的编码。编码器通过运行时代码生成构建自定义字节码，实现更快速且紧凑的序列化与反序列化。许多操作可以在内存中就地执行，无需反序列化，因为它们已经是
    Tungsten 二进制格式。
- en: Structured Streaming
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流处理
- en: Streaming is a seemingly broad topic! If you take a closer look at the real-world
    problems, businesses do not just want a streaming engine to make decisions in
    real time. There has always been a need to integrate both batch stack and streaming
    stack, and integrate with external storage systems and applications. Also, the
    solution should be such that it should adapt to dynamic changes in business logic
    to address new and changing business requirements.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理是一个看似广泛的话题！如果深入观察实际问题，企业不仅希望有一个流处理引擎来实时做出决策。一直以来，都有需求将批处理和流处理栈结合，并与外部存储系统和应用程序集成。此外，解决方案应能适应业务逻辑的动态变化，以应对新的和不断变化的业务需求。
- en: Apache Spark 2.0 has the first version of the higher level stream processing
    API called the **Structured Streaming** engine. This scalable and fault-tolerant
    engine leans on the Spark SQL API to simplify the development of real-time, continuous
    big data applications. It is probably the first successful attempt in unifying
    the batch and streaming computation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 2.0 引入了首个高层次的流处理 API，称为 **结构化流处理（Structured Streaming）** 引擎。这个可扩展且容错的引擎依赖于
    Spark SQL API，简化了实时、连续大数据应用的开发。这可能是统一批处理和流处理计算的首次成功尝试。
- en: 'At a technical level, Structured Streaming leans on the Spark SQL API, which
    extends DataFrames/Datasets, which we already discussed in the previous sections.
    Spark 2.0 lets you perform radically different activities in a unified way, such
    as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度讲，结构化流处理依赖于 Spark SQL API，该 API 扩展了我们之前讨论的 DataFrames/Datasets。Spark 2.0
    让你以统一的方式执行完全不同的活动，例如：
- en: Building ML models and applying them on streaming data
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建机器学习模型并将其应用于流式数据
- en: Combining streaming data with other static data
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将流式数据与其他静态数据结合
- en: Performing ad hoc, interactive, and batch queries
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行临时查询、交互式查询和批处理查询
- en: Changing queries at runtime
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行时改变查询
- en: Aggregating data streams and serving using Spark SQL JDBC
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合数据流并通过 Spark SQL JDBC 提供服务
- en: Unlike other streaming engines, Spark lets you combine real-time **Streaming
    Data** with **Static data** and lets you perform the preceding operations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他流处理引擎不同，Spark 允许你将实时 **流式数据（Streaming Data）** 与 **静态数据（Static data）** 结合，并执行前述操作。
- en: '![Structured Streaming](img/image_04_002.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![结构化流处理](img/image_04_002.jpg)'
- en: Fundamentally, Structured Streaming is empowered by Spark SQL's Catalyst optimizer.
    So, it frees up the developers from worrying about the underlying plumbing of
    making queries more efficient while dealing with static or real-time streams of
    data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，结构化流处理（Structured Streaming）得益于 Spark SQL 的 Catalyst 优化器。因此，它让开发者无需担心底层的查询优化问题，能够更高效地处理静态或实时数据流。
- en: As of this writing, Structured Streaming of Spark 2.0 is focused on ETL, and
    later versions will have more operators and libraries.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文撰写时，Spark 2.0 的结构化流处理专注于 ETL，后续版本将引入更多操作符和库。
- en: Let us look at a simple example. The following example listens to **System Activity
    Report** (**sar**) on Linux on a local machine and computes the average free memory.
    System Activity Report gives system activity statistics and the current example
    collects memory usage, reported 20 times at a 2-second interval. The Spark stream
    reads this streaming output and computes average memory. We use a handy networking
    utility **netcat** (**nc**) to redirect the `sar` output onto a given port. The
    options `l` and `k` specify that `nc` should listen for an incoming connection
    and it has to keep listening for another connection even after its current connection
    is completed.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子。以下示例在本地机器上监听 Linux 上的 **系统活动报告** (**sar**) 并计算平均空闲内存。系统活动报告提供系统活动统计信息，当前示例收集内存使用情况，每隔
    2 秒报告 20 次。Spark 流读取这个流式输出并计算平均内存。我们使用一个方便的网络工具 **netcat** (**nc**) 将 `sar` 输出重定向到指定的端口。选项
    `l` 和 `k` 指定 `nc` 应该监听传入连接，并且即使当前连接完成后，它也必须继续监听另一个连接。
- en: '**Scala:**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: Example 6-Streaming example
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 6 - 流式示例
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Python:**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The preceding example defined a continuous data frame (also known as stream)
    to listen to a particular port, perform some transformations, and aggregations
    and show continuous output.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例定义了一个连续数据框（也称为流），用于监听特定端口，执行一些转换和聚合操作，并显示连续输出。
- en: The Spark streaming programming model
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 流式编程模型
- en: 'As demonstrated earlier in this chapter, there is just a single API to take
    care of both static and streaming data. The idea is to treat the real-time data
    stream as a table that is continuously being appended, as shown in the following
    figure:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章前面所展示的，只有一个 API 可以同时处理静态数据和流式数据。其思想是将实时数据流视为一个不断追加的表，如下图所示：
- en: '![The Spark streaming programming model](img/image_04_003.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 流式编程模型](img/image_04_003.jpg)'
- en: 'So whether for static or streaming data, you just fire up the batch-like queries
    as you would do on static data tables, and Spark runs it as an incremental query
    on the unbounded input table, as shown in the following figure:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无论是静态数据还是流式数据，你都可以像对待静态数据表一样启动批处理查询，Spark 会将其作为增量查询在无界输入表上执行，如下图所示：
- en: '![The Spark streaming programming model](img/image_04_004.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 流式编程模型](img/image_04_004.jpg)'
- en: 'So, the developers define a query on the input table, in the same way for both
    static-bounded as well as dynamic-unbounded table. Let us understand the various
    technical jargons for this whole process to understand how it works:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，开发人员以相同的方式在输入表上定义查询，适用于静态有界表和动态无界表。为了理解它是如何工作的，我们来了解一下这个过程中的各种技术术语：
- en: '**Input:** Data from sources as an append-only table'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入：** 作为追加式表格的数据源'
- en: '**Trigger:** When to check the input for new data'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**触发器：** 何时检查输入数据以获取新数据'
- en: '**Query:** What operation to perform on the data, such as filter, group, and
    so on'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询：** 对数据执行的操作，如过滤、分组等'
- en: '**Result:** The resultant table at every trigger interval'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果：** 每次触发间隔后的结果表'
- en: '**Output:** Choose what part of the result to write to the data sink after
    every trigger'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出：** 每次触发后，选择要写入数据接收器的结果部分'
- en: 'Let''s now look at how the Spark SQL planner treats the whole process:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看 Spark SQL 计划器是如何处理整个过程的：
- en: '![The Spark streaming programming model](img/image_04_005.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 流式编程模型](img/image_04_005.jpg)'
- en: 'Courtesy: Databricks'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢：Databricks
- en: The preceding screenshot is very simply explained in the structured programming
    guide at the official Apache Spark site, as indicated in the *References* section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图在官方 Apache Spark 网站的结构化编程指南中有非常简单的解释，如 *参考文献* 部分所示。
- en: '![The Spark streaming programming model](img/image_04_006.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 流式编程模型](img/image_04_006.jpg)'
- en: 'At this point, we need to know about the supported output models. Every time
    the result table is updated, the changes need to be written to an external system,
    such as HDFS, S3, or any other database. We usually prefer to write output incrementally.
    For this purpose, Structured Streaming provides three output modes:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们需要了解支持的输出模型。每次结果表更新时，必须将更改写入外部系统，如 HDFS、S3 或其他数据库。我们通常更倾向于增量写入输出。为此，结构化流提供了三种输出模式：
- en: '**Append:** In the external storage, only the new rows appended to the result
    table since the last trigger will be written. This is applicable only on queries
    where existing rows in the result table cannot change (for example, a map on an
    input stream).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**追加模式：** 在外部存储中，只有自上次触发以来追加到结果表的新行会被写入。此模式仅适用于查询中结果表中的现有行不能更改的情况（例如，对输入流的映射）。'
- en: '**Complete:** In the external storage, the entire updated result table will
    be written as is.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整模式：** 在外部存储中，整个更新后的结果表将被完整写入。'
- en: '**Update:** In the external storage, only the rows that were updated in the
    result table since the last trigger will be changed. This mode works for output
    sinks that can be updated in place, such as a MySQL table.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新模式：** 在外部存储中，只有自上次触发以来更新过的行会被更改。此模式适用于可以就地更新的输出接收器，例如 MySQL 表。'
- en: In our example, we used complete mode, which was straightaway writing to the
    console. You may want to write into some external file such as Parquet to get
    a better understanding.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用了完整模式，这直接将结果写入控制台。你可能希望将数据写入某些外部文件（例如 Parquet），以便更好地理解。
- en: Under the hood
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幕后机制
- en: 'If you look at the "behind the screen" execution mechanism of the operations
    performed on **DataFrames/Datasets**, it would appear as the following figure
    suggests:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看在 **DataFrames/Datasets** 上执行操作的“幕后”执行机制，它将呈现如下图所示：
- en: '![Under the hood](img/image_04_007.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![幕后](img/image_04_007.jpg)'
- en: 'Please note here that the **Planner** knows apriori how to convert a streaming
    **Logical Plan** to a continuous series of **Incremental Execution Plans**. This
    can be represented by the following figure:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，**Planner** 已知如何将流处理的**逻辑计划**转换为一系列连续的**增量执行计划**。这可以通过以下图示表示：
- en: '![Under the hood](img/image_04_008.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![幕后](img/image_04_008.jpg)'
- en: The **Planner** can poll the data sources for new data to be able to plan the
    execution in an optimized way.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**Planner** 可以轮询数据源中的新数据，以便能够以优化的方式规划执行。'
- en: Comparison with other streaming engines
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与其他流处理引擎的比较
- en: 'We have discussed many unique features of Structured Streaming. Let us now
    have a comparative view with other available streaming engines:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了结构化流处理的许多独特功能。现在让我们与其他流处理引擎做一个比较：
- en: '![Comparison with other streaming engines](img/image_04_009.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![与其他流引擎的比较](img/image_04_009.jpg)'
- en: 'Courtesy: Databricks'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由此提供：Databricks
- en: Continuous applications
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续应用程序
- en: We discussed how unified data access is empowered by Spark. It lets you process
    data in a myriad of ways to build end-to-end continuous applications by enabling
    various analytic workloads, such as ETL processing, ad hoc queries, online machine
    learning modeling, or to generate necessary reports... all of this in a unified
    way by letting you work on both static as well as streaming data using a high-level,
    SQL-like API. In this way, Structured Streaming has substantially simplified the
    development and maintenance of real-time, continuous applications.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了 Spark 如何使统一数据访问成为可能。它让你可以以多种方式处理数据，构建端到端的连续应用程序，通过启用各种分析工作负载，如 ETL 处理、临时查询、在线机器学习建模，或生成必要的报告……这一切都通过高层次的类似
    SQL 的 API 实现统一方式，让你同时处理静态和流式数据。通过这种方式，结构化流处理大大简化了实时连续应用程序的开发和维护。
- en: '![Continuous applications](img/image_04_010.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![连续应用](img/image_04_010.jpg)'
- en: 'Courtesy: Databricks'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由此提供：Databricks
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed what is really meant by unified data access and
    how Spark serves this purpose. We took a closer look at the Datasets API and how
    real-time streaming is empowered through it. We learned the advantages of Datasets
    and also their limitations. We also looked at the fundamentals behind continuous
    applications.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了统一数据访问的真正含义以及 Spark 如何实现这一目标。我们详细介绍了 Datasets API 以及如何通过它支持实时流处理。我们学习了
    Datasets 的优点，也了解了它们的局限性。我们还研究了连续应用程序背后的基本原理。
- en: In the following chapter, we will look at the various ways in which we can leverage
    the Spark platform for data analysis operations at scale.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何利用 Spark 平台进行大规模数据分析操作。
- en: References
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf](http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf)
    : Spark SQL: Relational Data Processing in Spark'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf](http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf)
    : Spark SQL：Spark 中的关系数据处理'
- en: '[https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)
    : A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets - When to
    use them and why'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)
    : 三种 Apache Spark API 的故事：RDDs、DataFrames 和 Datasets - 何时以及为何使用它们'
- en: '[https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html](https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html)
    : Introducing Apache Spark Datasets'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html](https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html)
    : 介绍 Apache Spark Datasets'
- en: '[https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)
    : Deep Dive into Spark SQL''s Catalyst Optimizer'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)
    : 深入探讨 Spark SQL 的 Catalyst 优化器'
- en: '[https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)
    : Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)
    : Apache Spark 作为编译器：在笔记本电脑上每秒连接十亿行'
- en: '[https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)
    : Bringing Spark closer to baremetal'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)
    : 将 Spark 接近裸机的项目钨'
- en: '[https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html)
    : Structured Streaming API details'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html)
    : Apache Spark 中的结构化流处理 API 详解'
- en: '[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
    : Spark Structured Streaming Programming Guide'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
    : Spark 结构化流处理编程指南'
- en: '[https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/](https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/):
    Structuring Apache Spark SQL, DataFrames, Datasets, and Streaming by Michael Armbrust'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/](https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/)
    : Michael Armbrust 主讲的结构化 Apache Spark SQL、DataFrames、Datasets 和流处理'
- en: '[https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html](https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html):
    Apache Spark Key terms explained'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html](https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html)
    : Apache Spark 关键术语解释'
