- en: Detecting Dark Matter - The Higgs-Boson Particle
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探测暗物质 - 弥散子粒子
- en: True or false? Positive or negative? Pass or no pass? User clicks on the ad
    versus not clicking the ad? If you've ever asked/encountered these questions before
    then you are already familiar with the concept of *binary classification.*
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 真或假？积极或消极？通过还是不通过？用户点击广告与不点击广告？如果你以前曾经问过/遇到过这些问题，那么你已经熟悉*二元分类*的概念。
- en: 'At it''s core, binary classification - also referred to as *binomial classification*
    - attempts to categorize a set of elements into two distinct groups using a classification
    rule, which in our case, can be a machine learning algorithm. This chapter shows
    how to deal with it in the context of Spark and big data. We are going to explain
    and demonstrate:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，二元分类 - 也称为*二项分类* - 试图使用分类规则将一组元素分类为两个不同的组，而在我们的情况下，可以是一个机器学习算法。本章将展示如何在Spark和大数据的背景下处理这个问题。我们将解释和演示：
- en: Spark MLlib models for binary classification including decision trees, random
    forest, and the gradient boosted machine
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark MLlib二元分类模型包括决策树、随机森林和梯度提升机
- en: Binary classification support in H2O
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: H2O中的二元分类支持
- en: Searching for the best model in a hyperspace of parameters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在参数的超空间中寻找最佳模型
- en: Evaluation metrics for binomial models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二项模型的评估指标
- en: Type I versus type II error
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Type I与Type II错误
- en: Binary classifiers have intuitive interpretation since they are trying to separate
    data points into two groups. This sounds simple, but we need to have some notion
    of measuring the quality of this separation. Furthermore, one important characteristic
    of a binary classification problem is that, often, the proportion of one group
    of labels versus the other can be disproportionate. That means the dataset may
    be imbalanced with respect to one label which necessitates careful interpretation
    by the data scientist.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类器具有直观的解释，因为它们试图将数据点分成两组。这听起来很简单，但我们需要一些衡量这种分离质量的概念。此外，二元分类问题的一个重要特征是，通常一个标签组的比例与另一个标签组的比例可能不成比例。这意味着数据集可能在一个标签方面不平衡，这需要数据科学家仔细解释。
- en: Suppose, for example, we are trying to detect the presence of a particular rare
    disease in a population of 15 million people and we discover that - using a large
    subset of the population - only 10,000 or 10 million individuals actually carry
    the disease. Without taking this huge disproportion into consideration, the most
    naive algorithm would guess "no presence of disease" on the remaining five million
    people simply because 0.1% of the subset carried the disease. Suppose that of
    the remaining five million people, the same proportion, 0.1%, carried the disease,
    then these 5,000 people would not be correctly diagnosed because the naive algorithm
    would simply guess no one carries the disease. Is this acceptable? In this situation,
    the *cost* of the errors posed by binary classification is an important factor
    to consider, which is relative to the question being asked.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们试图在1500万人口中检测特定罕见疾病的存在，并且我们发现 - 使用人口的大子集 - 只有10,000或1千万人实际上携带疾病。如果不考虑这种巨大的不成比例，最天真的算法会简单地猜测剩下的500万人中“没有疾病存在”，仅仅因为子集中有0.1%的人携带疾病。假设在剩下的500万人中，同样的比例，0.1%，携带疾病，那么这5000人将无法被正确诊断，因为天真的算法会简单地猜测没有人携带疾病。这种情况下，二元分类所带来的错误的*成本*是需要考虑的一个重要因素，这与所提出的问题有关。
- en: 'Given that we are only dealing with two outcomes for this type of problem,
    we can create a 2-D representation of the different types of errors that are possible.
    Keeping our preceding example of the people carrying / not carrying the disease,
    we can think about the outcome of our classification rule as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们只处理这种类型问题的两种结果，我们可以创建一个二维表示可能的不同类型错误的表示。保持我们之前的例子，即携带/不携带疾病的人，我们可以将我们的分类规则的结果考虑如下：
- en: '![](img/00014.jpeg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00014.jpeg)'
- en: Figure 1 - Relation between predicted and actual values
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 - 预测和实际值之间的关系
- en: 'From the preceding table, the green area represents where we are *correctly *predicting
    the presence / absence of disease in the individual whereas the white areas represent
    where our prediction was incorrect. These false predictions fall into two categories
    known as **Type I** and **Type II** errors:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从上表中可以看出，绿色区域代表我们在个体中*正确*预测疾病的存在/不存在，而白色区域代表我们的预测是错误的。这些错误的预测分为两类，称为**Type I**和**Type
    II**错误：
- en: '**Type I error**: When we reject the null hypothesis (that is, a person not
    carrying the disease) when in fact, it is true in actuality'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Type I错误**：当我们拒绝零假设（即一个人没有携带疾病），而实际上，实际上是真的'
- en: '**Type II error**: Where we predict the presence of the disease when the individual
    does *not* carry the disease'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Type II错误**：当我们预测个体携带疾病时，实际上个体并没有携带疾病'
- en: Clearly, both errors are not good but often, in practice, some errors are more
    acceptable than others.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这两种错误都不好，但在实践中，有些错误比其他错误更可接受。
- en: Consider the situation where our model makes significantly more Type II errors
    than Type I errors; in this case, our model would be predicting more people are
    carrying the disease than actually are - a conservative approach may be *more
    acceptable* than a Type II error where we are failing to identify the presence
    of the disease. Determining the *cost* of each type of error is a function of
    the question being asked and is something the data scientist must consider. We
    will revisit this topic of errors and some other metrics of model quality after
    we build our first binary classification model which tries to predict the presence
    / non-presence of the Higgs-Boson particle.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这样一种情况，即我们的模型产生的II型错误明显多于I型错误；在这种情况下，我们的模型会预测患病的人数比实际上更多 - 保守的方法可能比我们未能识别疾病存在的II型错误更为*可接受*。确定每种错误的*成本*是所提出的问题的函数，这是数据科学家必须考虑的事情。在我们建立第一个尝试预测希格斯玻色子粒子存在/不存在的二元分类模型之后，我们将重新讨论错误和模型质量的一些其他指标。
- en: Finding the Higgs-Boson particle
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找希格斯玻色子粒子
- en: 'On July 4, 2012, scientists from Europe''s CERN lab in Geneva, Switzerland,
    presented strong evidence of a particle they believe is the Higgs-Boson, sometimes
    referred to as the *God-particle*. Why is this discovery so meaningful and important?
    As popular physicist and author Michio Kaku wrote:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年7月4日，来自瑞士日内瓦的欧洲CERN实验室的科学家们提出了强有力的证据，证明了他们认为是希格斯玻色子的粒子，有时被称为*上帝粒子*。为什么这一发现如此有意义和重要？正如知名物理学家和作家迈克·卡库所写：
- en: '"In quantum physics, it was a Higgs-like particle that sparked the cosmic explosion
    (that is, the big bang). In other words, everything we see around us, including
    galaxies, stars, planets, and us, owes its existence to the Higgs-Boson."'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '"在量子物理学中，是一种类似希格斯的粒子引发了宇宙大爆炸（即大爆炸）。换句话说，我们周围看到的一切，包括星系、恒星、行星和我们自己，都归功于希格斯玻色子。"'
- en: In layman's terms, the Higgs-Boson is the particle that gives mass to matter
    and offers a possible explanation for how the Earth was originally created and
    hence, its huge popularity in mainstream media channels.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 用通俗的话来说，希格斯玻色子是赋予物质质量的粒子，并为地球最初的形成提供了可能的解释，因此在主流媒体渠道中备受欢迎。
- en: The LHC and data creation
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LHC和数据生成
- en: To test for the presence of the Higgs-Boson, scientists constructed the largest
    human-made machine called the **Large Hadron Collider** (**LHC**) near Geneva,
    close to the Franco-Swiss border. The LHC is a ring-shaped tunnel that runs 27
    kilometers long (equivalent to the Circle Line from London's Underground) and
    lies 100 meters underground.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测希格斯玻色子的存在，科学家们建造了人造最大的机器，称为日内瓦附近的**大型强子对撞机**（**LHC**）。LHC是一个环形隧道，长27公里（相当于伦敦地铁的环线），位于地下100米。
- en: Through this tunnel, subatomic particles are fired in opposite directions with
    the help of the aforementioned magnets with speeds approaching the speed of light.
    Once a critical speed is reached, the particles are put on a collision course
    where detectors monitor and record the collisions. There are literally millions
    upon millions of collisions and sub-collisions! - and the resultant *particle
    debris* give in hope of detecting the Higgs-Boson.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这条隧道，亚原子粒子在磁铁的帮助下以接近光速的速度相反方向发射。一旦达到临界速度，粒子就被放在碰撞轨道上，探测器监视和记录碰撞。有数以百万计的碰撞和亚碰撞！
    - 而由此产生的*粒子碎片*有望检测到希格斯玻色子的存在。
- en: The theory behind the Higgs-Boson
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 希格斯玻色子的理论
- en: For quite some time, physicists have known that some fundamental particles have
    mass which contradicts the mathematics underlying the Standard Model which states
    these particles should be mass-less. In the 1960s, Peter Higgs and his colleagues
    challenged this mass conundrum by studying the universe after the big bang. At
    the time, it was largely believed that particles should be thought of as ripples
    in a quantum jelly as opposed to tiny billiard balls bouncing off one another.
    Higgs believed that during this early period, all particle jellies were runny
    with a consistency like water; but as the universe began to *cool down*, one particle
    jelly, known first as the *Higgs field*, began to condense and become thick. Consequently,
    other particle jellies, when interacting with the Higgs field, are drawn towards
    it thanks to inertia; and, according to Sir Isaac Newton, any particle with inertia
    should contain mass. This mechanism offers an explanation to how particles that
    makeup the Standard Model - born massless at first - may have acquired mass. It
    follows then that the amount of mass acquired by each particle is proportional
    to the strength with which it feels the effects of the Higgs field.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 相当长一段时间以来，物理学家已经知道一些基本粒子具有质量，这与标准模型的数学相矛盾，该模型规定这些粒子应该是无质量的。在20世纪60年代，彼得·希格斯和他的同事们通过研究大爆炸后的宇宙挑战了这个质量难题。当时，人们普遍认为粒子应该被视为量子果冻中的涟漪，而不是彼此弹来弹去的小台球。希格斯认为，在这个早期时期，所有的粒子果冻都像水一样稀薄；但随着宇宙开始*冷却*，一个粒子果冻，最初被称为*希格斯场*，开始凝结变厚。因此，其他粒子果冻在与希格斯场相互作用时，由于惯性而被吸引；根据艾萨克·牛顿爵士的说法，任何具有惯性的粒子都应该含有质量。这种机制解释了标准模型中的粒子如何获得质量
    - 起初是无质量的。因此，每个粒子获得的质量量与其感受到希格斯场影响的强度成正比。
- en: The article [https://plus.maths.org/content/particle-hunting-lhc-higgs-boson](https://plus.maths.org/content/particle-hunting-lhc-higgs-boson)
    is a great source of information for curious readers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 文章[https://plus.maths.org/content/particle-hunting-lhc-higgs-boson](https://plus.maths.org/content/particle-hunting-lhc-higgs-boson)是对好奇读者的一个很好的信息来源。
- en: Measuring for the Higgs-Boson
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量希格斯玻色子
- en: 'Testing this theory goes back to the original notion of particle jelly ripples
    and in particular, the Higgs jelly which a) can ripple and b) would resemble a
    particle in an experiment: the infamous Higgs-Boson. So how do scientists detect
    this ripple using the LHC?'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 测试这个理论回到了粒子果冻波纹的最初概念，特别是希格斯果冻，它a）可以波动，b）在实验中会类似于一个粒子：臭名昭著的希格斯玻色子。那么科学家们如何利用LHC检测这种波纹呢？
- en: To monitor the collisions and the resulting post-collisions, scientists set
    up detectors which act like three-dimensional digital cameras which measure the
    tracks of particles coming from the collisions. Properties from these tracks -
    that is, how much they curve in magnetic fields - are used to infer various properties
    of the particles that generated them; one extremely common property that can be
    measured is an electric charge where it is believed the Higgs exists somewhere
    between 120 and 125 giga-electronvolts. Meaning, if the detectors find an event
    with an electric charge that exists between these two ranges, this would indicate
    a new particle which may be indicative of the Higgs-Boson.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了监测碰撞和碰撞后的结果，科学家们设置了探测器，它们就像三维数字摄像机，测量来自碰撞的粒子轨迹。这些轨迹的属性 - 即它们在磁场中的弯曲程度 - 被用来推断生成它们的粒子的各种属性；一个非常常见的可以测量的属性是电荷，据信希格斯玻色子存在于120到125吉电子伏特之间。也就是说，如果探测器发现一个电荷存在于这两个范围之间的事件，这将表明可能存在一个新的粒子，这可能是希格斯玻色子的迹象。
- en: The dataset
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: Upon releasing their findings to the scientific community in 2012, researchers
    later made the data public from the LHC experiments where they observed - and
    identified - a signal which is indicative of the Higgs-Boson particle. However,
    amidst the positive findings is a lot of background noise which causes an imbalance
    within the dataset. Our task as data scientist is to build a machine learning
    model which can accurately identify the Higgs-Boson particle from background noise.
    Already, you should be thinking about how this question is phrased which would
    be indicative of binary classification (that is, is this example the Higgs-Boson
    versus background noise?).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年，研究人员向科学界发布了他们的研究结果，随后公开了LHC实验的数据，他们观察到并确定了一种信号，这种信号表明存在希格斯玻色子粒子。然而，在积极的发现中存在大量的背景噪音，这导致数据集内部不平衡。我们作为数据科学家的任务是构建一个机器学习模型，能够准确地从背景噪音中识别出希格斯玻色子粒子。你现在应该考虑这个问题的表述方式，这可能表明这是一个二元分类问题（即，这个例子是希格斯玻色子还是背景噪音？）。
- en: You can download the dataset from [https://archive.ics.uci.edu/ml/datasets/HIGGS](https://archive.ics.uci.edu/ml/datasets/HIGGS)
    or use the script `getdata.sh` located in the  `bin` folder of this chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[https://archive.ics.uci.edu/ml/datasets/HIGGS](https://archive.ics.uci.edu/ml/datasets/HIGGS)下载数据集，或者使用本章的`bin`文件夹中的`getdata.sh`脚本。
- en: 'This file is 2.6 gigs (uncompressed) and contains 11 million examples that
    have been labeled as 0 - background noise and 1 - Higgs-Boson. First, you will
    need to uncompress this file and then we will begin loading the data into Spark
    for processing and analysis. There are 29 total fields which make up the dataset:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件有2.6吉字节（未压缩），包含了1100万个被标记为0 - 背景噪音和1 - 希格斯玻色子的例子。首先，您需要解压缩这个文件，然后我们将开始将数据加载到Spark中进行处理和分析。数据集总共有29个字段：
- en: 'Field 1: Class label (1 = signal for Higgs-Boson, 2 = background noise)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字段1：类别标签（1 = 希格斯玻色子信号，2 = 背景噪音）
- en: 'Fields 2-22: 21 "low-level" features that come from the collision detectors'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字段2-22：来自碰撞探测器的21个“低级”特征
- en: 'Fields 23-29: seven "high-level" features that have been hand-derived by particle
    physicists to help classify the particle into its appropriate class (Higgs or
    background noise)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字段23-29：由粒子物理学家手工提取的七个“高级”特征，用于帮助将粒子分类到适当的类别（希格斯或背景噪音）
- en: Later in this chapter, we cover a **Deep Neural Network** (**DNN**) example
    that will attempt to *learn* these hand-derived features through layers of non-linear
    transformations to the input data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面，我们将介绍一个**深度神经网络**（**DNN**）的例子，它将尝试通过非线性转换层来*学习*这些手工提取的特征。
- en: Note that for the purposes of this chapter, we will work with a subset of the
    data, the first 100,000 rows, but all the code we show would also work on the
    original dataset.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了本章的目的，我们将使用数据的一个子集，即前100,000行，但我们展示的所有代码也适用于原始数据集。
- en: Spark start and data load
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark启动和数据加载
- en: 'Now it''s time to fire up a Spark cluster which will give us all the functionality
    of Spark while simultaneously allowing us to use H2O algorithms and visualize
    our data. As always, we must download Spark 2.1 distribution from [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)
    and declare the execution environment beforehand. For example, if you download
    `spark-2.1.1-bin-hadoop2.6.tgz` from the Spark download page, you can prepare
    the environment in the following way:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候启动一个Spark集群了，这将为我们提供Spark的所有功能，同时还允许我们使用H2O算法和可视化我们的数据。和往常一样，我们必须从[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)下载Spark
    2.1分发版，并在执行之前声明执行环境。例如，如果您从Spark下载页面下载了`spark-2.1.1-bin-hadoop2.6.tgz`，您可以按照以下方式准备环境：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When the environment is ready, we can start the interactive Spark shell with
    Sparkling Water packages and this book package:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当环境准备好后，我们可以使用Sparkling Water包和本书包启动交互式Spark shell：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: H2O.ai is constantly keeping up with the latest releases of the Spark project
    to match the version of Sparkling Water. The book is using Spark 2.1.1 distribution
    and Sparkling Water 2.1.12\. You can find the latest version of Sparkling Water
    for your version of Spark at [http://h2o.ai/download/](http://h2o.ai/download/)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: H2O.ai一直在与Spark项目的最新版本保持同步，以匹配Sparkling Water的版本。本书使用Spark 2.1.1分发版和Sparkling
    Water 2.1.12。您可以在[http://h2o.ai/download/](http://h2o.ai/download/)找到适用于您版本Spark的最新版本Sparkling
    Water。
- en: This case is using the provided Spark shell which downloads and uses Spark packages
    of Sparkling Water version 2.1.12\. The packages are identified by Maven coordinates
    - in this case `ai.h2o` represents organization ID, `sparkling-water-core` identifies
    Sparkling Water implementation (for Scala 2.11, since Scala versions are not binary
    compatible), and, finally, `2.1.12` is a version of the package. Furthermore,
    we are using this book -specific package which provides handful utilities.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例使用提供的Spark shell，该shell下载并使用Sparkling Water版本2.1.12的Spark软件包。这些软件包由Maven坐标标识
    - 在本例中，`ai.h2o`代表组织ID，`sparkling-water-core`标识Sparkling Water实现（对于Scala 2.11，因为Scala版本不兼容），最后，`2.1.12`是软件包的版本。此外，我们正在使用本书特定的软件包，该软件包提供了一些实用工具。
- en: 'The list of all published Sparkling Water versions is also available on Maven
    central: [http://search.maven.org](http://search.maven.org)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所有已发布的Sparkling Water版本列表也可以在Maven中央仓库上找到：[http://search.maven.org](http://search.maven.org)
- en: 'The command starts Spark in a local mode - that is, the Spark cluster has a
    single node running on your computer. Assuming you did all this successfully,
    you should see the standard Spark shell output like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令在本地模式下启动Spark - 也就是说，Spark集群在您的计算机上运行一个单节点。假设您成功完成了所有这些操作，您应该会看到标准的Spark
    shell输出，就像这样：
- en: '![](img/00015.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00015.jpeg)'
- en: Figure 2 - Notice how the shell starts up showing you the version of Spark you
    are using.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图2 - 注意shell启动时显示的Spark版本。
- en: The provided book source code provides for each chapter the command starting
    the Spark environment; for this chapter, you can find it in the `chapter2/bin`
    folder.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的书籍源代码为每一章提供了启动Spark环境的命令；对于本章，您可以在`chapter2/bin`文件夹中找到它。
- en: The Spark shell is a Scala - based console application that accepts Scala code
    and executes it in an interactive way. The next step is to prepare the computation
    environment by importing packages which we are going to use during our example.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Spark shell是一个基于Scala的控制台应用程序，它接受Scala代码并以交互方式执行。下一步是通过导入我们将在示例中使用的软件包来准备计算环境。
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s first ingest the `.csv` file that you should have downloaded and do
    a quick count to see how much data is in our subset. Here, please notice, that
    the code expects the data folder "data" relative to the current process working
    directory or location specified:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先摄取您应该已经下载的`.csv`文件，并快速计算一下我们的子集中有多少数据。在这里，请注意，代码期望数据文件夹"data"相对于当前进程的工作目录或指定的位置：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00016.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00016.jpeg)'
- en: You can observe that execution of the command `sc.textFile(...)` took no time
    and returned instantly, while executing `rawData.count` took the majority amount
    of time. This exactly demonstrates the difference between Spark **transformations**
    and **actions**. By design, Spark adopts **lazy evaluation** - it means that if
    a transformation is invoked, Spark just records it directly into its so-called
    **execution graph/plan**. That perfectly fits into the big data world, since users
    can pile up transformations without waiting. On the other hand, an action evaluates
    the execution graph - Spark instantiates each recorded transformation and applies
    it onto the output of previous transformations. This concept also helps Spark
    to analyze and optimize an execution graph before its execution - for example,
    Spark can reorganize the order of transformations or can decide to run transformations
    in parallel if they are independent.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以观察到执行命令`sc.textFile(...)`几乎没有花费时间并立即返回，而执行`rawData.count`花费了大部分时间。这正好展示了Spark
    **转换**和**操作**之间的区别。按设计，Spark采用**惰性评估** - 这意味着如果调用了一个转换，Spark会直接记录它到所谓的**执行图/计划**中。这非常适合大数据世界，因为用户可以堆叠转换而无需等待。另一方面，操作会评估执行图
    - Spark会实例化每个记录的转换，并将其应用到先前转换的输出上。这个概念还帮助Spark在执行之前分析和优化执行图 - 例如，Spark可以重新组织转换的顺序，或者决定如果它们是独立的话并行运行转换。
- en: 'Right now, we defined a transformation which loads data into the Spark data
    structure `RDD[String]` which contains all the lines of input data file. So, let''s
    look at the first two rows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义了一个转换，它将数据加载到Spark数据结构`RDD[String]`中，其中包含输入数据文件的所有行。因此，让我们看一下前两行：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](img/00017.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00017.jpeg)'
- en: 'The first two lines contain raw data as loaded from the file. You can see that
    a row is composed of a response column having the value 0,1 (the first value of
    the row) and other columns having real values. However, the lines are still represented
    as strings and require parsing and transformation into regular rows. Hence, based
    on the knowledge of the input data format, we can define a simple parser which
    splits an input line into numbers based on a comma:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 前两行包含从文件加载的原始数据。您可以看到一行由一个响应列组成，其值为0,1（行的第一个值），其他列具有实际值。但是，这些行仍然表示为字符串，并且需要解析和转换为常规行。因此，基于对输入数据格式的了解，我们可以定义一个简单的解析器，根据逗号将输入行拆分为数字：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we can extract a response column (the first column in the dataset) and
    the rest of data representing the input features:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以提取响应列（数据集中的第一列）和表示输入特征的其余数据：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After this transformation, we have two RDDs:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 进行这个转换之后，我们有两个RDD：
- en: One representing the response column
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个代表响应列
- en: Another which contains dense vectors of numbers holding individual input features
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个包含持有单个输入特征的数字的密集向量
- en: 'Next, let''s look in more detail at the input features and perform some very
    rudimentary data analysis:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更详细地查看输入特征并进行一些非常基本的数据分析：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We converted this vector into a distributed *RowMatrix*. This gives us the ability
    to perform simple summary statistics (for example, compute mean, variance, and
    so on:)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个向量转换为分布式*RowMatrix*。这使我们能够执行简单的摘要统计（例如，计算均值、方差等）。
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00018.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00018.jpeg)'
- en: 'Take a look at following code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下代码：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00019.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00019.jpeg)'
- en: 'In the next step, let''s explore columns in more details. We can get directly
    the number of non-zeros in each column to figure out if the data is dense or sparse.
    Dense data contains mostly non-zeros, sparse data the opposite. The ratio between
    the number of non-zeros in the data and the number of all values represents the sparsity
    of data. The sparsity can drive our selection of the computation method, since
    for sparse data it is more efficient to iterate over non-zeros only:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更详细地探索列。我们可以直接获取每列中非零值的数量，以确定数据是密集还是稀疏。密集数据主要包含非零值，稀疏数据则相反。数据中非零值的数量与所有值的数量之间的比率代表了数据的稀疏度。稀疏度可以驱动我们选择计算方法，因为对于稀疏数据，仅迭代非零值更有效：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00020.jpeg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00020.jpeg)'
- en: 'However, the call just gives us the number of non-zeros for all column, which
    is not so interesting. We are more curious about columns that contain some zero
    values:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，该调用只是给出了所有列的非零值数量，这并不那么有趣。我们更感兴趣的是包含一些零值的列：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In this case, we augmented the original vector of non-zeros by the index of
    each value and then filter out all the values which are equal to the number of
    rows in the original matrix. And we get:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们通过每个值的索引增加了原始的非零向量，然后过滤掉原始矩阵中等于行数的所有值。然后我们得到：
- en: '![](img/00021.jpeg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00021.jpeg)'
- en: 'We can see that columns 8, 12, 16, and 20 contain some zero numbers, but still
    not enough to consider the matrix as sparse. To confirm our observation, we can
    compute the overall sparsity of the matrix (remainder: the matrix does not include
    the response column):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到列8、12、16和20包含一些零数，但仍然不足以将矩阵视为稀疏。为了确认我们的观察，我们可以计算矩阵的整体稀疏度（剩余部分：矩阵不包括响应列）：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00022.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00022.jpeg)'
- en: And the computed number confirms our former observation - the input matrix is
    dense.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 计算出的数字证实了我们之前的观察 - 输入矩阵是密集的。
- en: 'Now it is time to explore the response column in more detail. As the first
    step, we verify that the response contains only the values `0` and `1` by computing
    the unique values inside the response vector:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候更详细地探索响应列了。作为第一步，我们通过计算响应向量中的唯一值来验证响应是否只包含值`0`和`1`：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/00023.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00023.jpeg)'
- en: 'The next step is to explore the distribution of labels in the response vector.
    We can compute the rate directly via Spark:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是探索响应向量中标签的分布。我们可以直接通过Spark计算速率：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00024.jpeg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00024.jpeg)'
- en: In this step, we simply transform each row into a tuple representing the row
    value and `1` expressing that the value occurs once in the row. Having RDDs of
    pairs, the Spark method `countByKey` aggregates pairs by a key and gives us a
    summary of the keys count. It shows that the data surprisingly contains slightly
    more cases which do represent Higgs-Boson but we can still consider the response
    nicely balanced.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们简单地将每行转换为表示行值的元组，以及表示该值在行中出现一次的`1`。拥有成对RDDs后，Spark方法`countByKey`通过键聚合成对，并给我们提供了键计数的摘要。它显示数据意外地包含了略微更多代表希格斯玻色子的情况，但我们仍然可以认为响应是平衡的。
- en: 'We can also explore labels distribution visually with help of the H2O library.
    For that we need to start H2O services represented by `H2OContext`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以利用H2O库以可视化的方式探索标签分布。为此，我们需要启动由`H2OContext`表示的H2O服务：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The code initializes the H2O library and starts H2O services on each node of
    the Spark clusters. It also exposes an interactive environment called Flow, which
    is useful for data exploration and model building. In the console, `h2oContext`
    prints the location of the exposed UI:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码初始化了H2O库，并在Spark集群的每个节点上启动了H2O服务。它还提供了一个名为Flow的交互式环境，用于数据探索和模型构建。在控制台中，`h2oContext`打印出了暴露的UI的位置：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we can directly open the Flow UI address and start exploring the data.
    However, before doing that, we need to publish the Spark data as an H2O frame
    called `response`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以直接打开Flow UI地址并开始探索数据。但是，在这样做之前，我们需要将Spark数据发布为名为`response`的H2O框架：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If you import implicit conversions exposed by `H2OContext`, you will be able
    to invoke transformation transparently based on the defined type on the left-side
    of assignment:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您导入了`H2OContext`公开的隐式转换，您将能够根据赋值左侧的定义类型透明地调用转换：
- en: 'For example:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now it is time to open the Flow UI. You can open it directly by accessing the
    URL reported by `H2OContext` or by typing `h2oContext.openFlow` in the Spark shell.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候打开Flow UI了。您可以通过访问`H2OContext`报告的URL直接打开它，或者在Spark shell中键入`h2oContext.openFlow`来打开它。
- en: '![](img/00025.jpeg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00025.jpeg)'
- en: Figure 3 - Interactive Flow UI
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图3 - 交互式Flow UI
- en: 'The Flow UI allows for interactive work with the stored data. Let,s look at
    which data is exposed for the Flow by typing `getFrames` into the highlighted
    cell:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Flow UI允许与存储的数据进行交互式工作。让我们通过在突出显示的单元格中键入`getFrames`来查看Flow暴露的数据：
- en: '![](img/00026.jpeg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00026.jpeg)'
- en: Figure 4 - Get list of available H2O frames
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图4 - 获取可用的H2O框架列表
- en: 'By clicking on the response field or typing `getColumnSummary "response", "values"`,
    we can get visual confirmation about the distribution of values in the response
    column and see that the problem is slightly imbalanced:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击响应字段或键入`getColumnSummary "response", "values"`，我们可以直观地确认响应列中值的分布，并看到问题略微不平衡：
- en: '![](img/00027.jpeg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00027.jpeg)'
- en: Figure 5 - Statistical properties of column named "response".
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图5 - 名为“response”的列的统计属性。
- en: Labeled point vector
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记点向量
- en: Prior to running any supervised machine learning algorithm using Spark MLlib,
    we must convert our dataset into a labeled point vector which maps features to
    a given label/response; labels are stored as doubles which facilitates their use
    for both classification and regression tasks. For all binary classification problems,
    labels should be stored as either `0` or `1`, which we confirmed from the preceding
    summary statistics holds true for our example.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Spark MLlib运行任何监督机器学习算法之前，我们必须将数据集转换为标记点向量，将特征映射到给定的标签/响应；标签存储为双精度，这有助于它们用于分类和回归任务。对于所有二元分类问题，标签应存储为`0`或`1`，我们从前面的摘要统计中确认了这一点对我们的例子成立。
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'An example of a labeled point vector follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 标记点向量的示例如下：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the preceding example, all doubles inside the bracket are the features and
    the single number outside the bracket is our label. Note that we are yet to tell
    Spark that we are performing a classification task and not a regression task which
    will happen later.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，括号内的所有双精度数都是特征，括号外的单个数字是我们的标签。请注意，我们尚未告诉Spark我们正在执行分类任务而不是回归任务，这将在稍后发生。
- en: In this example, all input features contain only numeric values, but in many
    situations data that contains categorical values or string data. All this non-numeric
    representation needs to be converted into numbers, which we will show later in
    this book.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，所有输入特征只包含数值，但在许多情况下，数据包含分类值或字符串数据。所有这些非数值表示都需要转换为数字，我们将在本书的后面展示。
- en: Data caching
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据缓存
- en: Many machine learning algorithms are iterative in nature and thus require multiple
    passes over the data. However, all data stored in Spark RDD are by default transient,
    since RDD just stores the transformation to be executed and not the actual data.
    That means each action would recompute data again and again by executing the transformation
    stored in RDD.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法具有迭代性质，因此需要对数据进行多次遍历。然而，默认情况下，存储在Spark RDD中的所有数据都是瞬时的，因为RDD只存储要执行的转换，而不是实际数据。这意味着每个操作都会通过执行RDD中存储的转换重新计算数据。
- en: 'Hence, Spark provides a way to persist the data in case we need to iterate
    over it. Spark also publishes several `StorageLevels` to allow storing data with
    various options:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Spark提供了一种持久化数据的方式，以便我们需要对其进行迭代。Spark还发布了几个`StorageLevels`，以允许使用各种选项存储数据：
- en: '`NONE`: No caching at all'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NONE`：根本不缓存'
- en: '`MEMORY_ONLY`: Caches RDD data only in memory'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_ONLY`：仅在内存中缓存RDD数据'
- en: '`DISK_ONLY`: Write cached RDD data to a disk and releases from memory'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DISK_ONLY`：将缓存的RDD数据写入磁盘并释放内存'
- en: '`MEMORY_AND_DISK`: Caches RDD in memory, if it''s not possible to offload data
    to a disk'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_AND_DISK`：如果无法将数据卸载到磁盘，则在内存中缓存RDD'
- en: '`OFF_HEAP`: Use external memory storage which is not part of JVM heap'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OFF_HEAP`：使用不属于JVM堆的外部内存存储'
- en: 'Furthermore, Spark gives users the ability to cache data in two flavors: *raw*
    (for example, `MEMORY_ONLY`) and *serialized* (for example, `MEMORY_ONLY_SER`).
    The later uses large memory buffers to store serialized content of RDD directly.
    Which one to use is very task and resource dependent. A good rule of thumb is
    if the dataset you are working with is less than 10 gigs then raw caching is preferred
    to serialized caching. However, once you cross over the 10 gigs soft-threshold,
    raw caching imposes a greater memory footprint than serialized caching.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark为用户提供了以两种方式缓存数据的能力：*原始*（例如`MEMORY_ONLY`）和*序列化*（例如`MEMORY_ONLY_SER`）。后者使用大型内存缓冲区直接存储RDD的序列化内容。使用哪种取决于任务和资源。一个很好的经验法则是，如果你正在处理的数据集小于10吉字节，那么原始缓存优于序列化缓存。然而，一旦超过10吉字节的软阈值，原始缓存比序列化缓存占用更大的内存空间。
- en: Spark can be forced to cache by calling the `cache()` method on RDD or directly
    via calling the method persist with the desired persistent target - `persist(StorageLevels.MEMORY_ONLY_SER)`.
    It is useful to know that RDD allows us to set up the storage level only once.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以通过在RDD上调用`cache()`方法或直接通过调用带有所需持久目标的persist方法（例如`persist(StorageLevels.MEMORY_ONLY_SER)`）来强制缓存。有用的是RDD只允许我们设置存储级别一次。
- en: 'The decision on what to cache and how to cache is part of the Spark magic;
    however, the golden rule is to use caching when we need to access RDD data several
    times and choose a destination based on the application preference respecting
    speed and storage. A great blogpost which goes into far more detail than what
    is given here is available at:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 决定缓存什么以及如何缓存是Spark魔术的一部分；然而，黄金法则是在需要多次访问RDD数据并根据应用程序偏好选择目标时使用缓存，尊重速度和存储。一个很棒的博客文章比这里提供的更详细，可以在以下链接找到：
- en: '[http://sujee.net/2015/01/22/understanding-spark-caching/#.VpU1nJMrLdc](http://sujee.net/2015/01/22/understanding-spark-caching/#.VpU1nJMrLdc)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://sujee.net/2015/01/22/understanding-spark-caching/#.VpU1nJMrLdc](http://sujee.net/2015/01/22/understanding-spark-caching/#.VpU1nJMrLdc)'
- en: 'Cached RDDs can be accessed as well from the H2O Flow UI by evaluating the
    cell with `getRDDs`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存的RDD也可以通过在H2O Flow UI中评估带有`getRDDs`的单元格来访问：
- en: '![](img/00028.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00028.jpeg)'
- en: Creating a training and testing set
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建训练和测试集
- en: 'As with most supervised learning tasks, we will create a split in our dataset
    so that we *teach* a model on one subset and then test its ability to generalize
    on new data against the holdout set. For the purposes of this example, we split
    the data 80/20 but there is no hard rule on what the ratio for a split should
    be - or for that matter - how many splits there should be in the first place:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数监督学习任务一样，我们将创建数据集的拆分，以便在一个子集上*教*模型，然后测试其对新数据的泛化能力，以便与留出集进行比较。在本例中，我们将数据拆分为80/20，但是拆分比例没有硬性规定，或者说
    - 首先应该有多少拆分：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: By creating our 80/20 split on the dataset, we are taking a random sample of
    8.8 million examples as our training set and the remaining 2.2 million as our
    testing set. We could just as easily take another random 80/20 split and generate
    a new training set with the same number of examples (8.8 million) but with different
    data. Doing this type of *hard* splitting of our original dataset introduces a
    sampling bias, which basically means that our model will learn to fit the training
    data but the training data may not be representative of "reality". Given that
    we are working with 11 million examples already, this bias is not as prominent
    versus if our original dataset is 100 rows, for example. This is often referred
    to as the **holdout method** for model validation.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在数据集上创建80/20的拆分，我们随机抽取了880万个示例作为训练集，剩下的220万个作为测试集。我们也可以随机抽取另一个80/20的拆分，并生成一个具有相同数量示例（880万个）但具有不同数据的新训练集。这种*硬*拆分我们原始数据集的方法引入了抽样偏差，这基本上意味着我们的模型将学会拟合训练数据，但训练数据可能不代表“现实”。鉴于我们已经使用了1100万个示例，这种偏差并不像我们的原始数据集只有100行的情况那样显著。这通常被称为模型验证的**留出法**。
- en: 'You can also use the H2O Flow to split the data:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用H2O Flow来拆分数据：
- en: 'Publish the Higgs data as H2OFrame:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将希格斯数据发布为H2OFrame：
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Split data in the Flow UI using the command `splitFrame` (see *Figure 07*).
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Flow UI中使用`splitFrame`命令拆分数据（见*图07*）。
- en: And then publish the results back to RDD.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将结果发布回RDD。
- en: '![](img/00029.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00029.jpeg)'
- en: Figure 7 - Splitting Higgs dataset into two H2O frames representing 80 and 20
    percent of data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图7 - 将希格斯数据集拆分为代表80%和20%数据的两个H2O框架。
- en: In contrast to Spark lazy evaluation, the H2O computation model is eager. That
    means the `splitFrame` invocation processes the data right away and creates two
    new frames, which can be directly accessed.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 与Spark的惰性评估相比，H2O计算模型是急切的。这意味着`splitFrame`调用会立即处理数据并创建两个新框架，可以直接访问。
- en: What about cross-validation?
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证呢？
- en: Often, in the case of smaller datasets, data scientists employ a technique known
    as cross-validation, which is also available to you in Spark. The `CrossValidator`
    class starts by splitting the dataset into N-folds (user declared) - each fold
    is used N-1 times as part of the training set and once for model validation. For
    example, if we declare that we wish to use a **5-fold cross-validation**, the
    `CrossValidator` class will create five pairs (training and testing) of datasets
    using four-fifths  of the dataset to create the training set with the final fifth as
    the test set, as shown in the following figure.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在较小的数据集的情况下，数据科学家会使用一种称为交叉验证的技术，这种技术在Spark中也可用。`CrossValidator`类首先将数据集分成N折（用户声明），每个折叠被用于训练集N-1次，并用于模型验证1次。例如，如果我们声明希望使用**5折交叉验证**，`CrossValidator`类将创建五对（训练和测试）数据集，使用四分之四的数据集创建训练集，最后四分之一作为测试集，如下图所示。
- en: The idea is that we would see the performance of our algorithm across different,
    randomly sampled datasets to account for the inherent sampling bias when we create
    our training/testing split on 80% of the data. An example of a model that does
    not generalize well would be one where the accuracy - as measured by overall error,
    for example - would be all over the map with wildly different error rates, which
    would suggest we need to rethink our model.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的想法是，我们将看到我们的算法在不同的随机抽样数据集上的性能，以考虑我们在80%的数据上创建训练/测试拆分时固有的抽样偏差。一个不太好泛化的模型的例子是，准确性（例如整体错误）会在不同的错误率上大幅度变化，这表明我们需要重新考虑我们的模型。
- en: '![](img/00030.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00030.jpeg)'
- en: Figure 8 - Conceptual schema of 5-fold cross-validation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图8 - 5折交叉验证的概念模式。
- en: There is no set rule on how many folds you should perform, as these questions
    are highly individual with respect to the type of data being used, the number
    of examples, and so on. In some cases, it makes sense to have extreme cross-validation
    where N is equal to the number of data points in the input dataset. In this case,
    the **Test** set contains only one row. This method is called as **Leave-One-Out**
    (**LOO**) validation and is more computationally expensive.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 关于应该执行多少次交叉验证并没有固定的规则，因为这些问题在很大程度上取决于所使用的数据类型、示例数量等。在某些情况下，进行极端的交叉验证是有意义的，其中N等于输入数据集中的数据点数。在这种情况下，**测试**集只包含一行。这种方法称为**留一法**（**LOO**）验证，计算成本更高。
- en: In general, it is recommended that you perform some cross-validation (often
    5-folds, or 10-folds cross-validation is recommended) during the model construction
    to validate the quality of a model - especially when the dataset is small.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，建议在模型构建过程中进行一些交叉验证（通常建议使用5折或10折交叉验证），以验证模型的质量 - 尤其是当数据集很小的时候。
- en: Our first model – decision tree
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的第一个模型 - 决策树
- en: 'Our first attempt at trying to classify the Higgs-Boson from background noise
    will use a decision tree algorithm. We purposely eschew from explaining the intuition
    behind this algorithm as this has already been well documented with plenty of
    supporting literature for the reader to consume ([http://www.saedsayad.com/decision_tree.htm](http://www.saedsayad.com/decision_tree.htm),
    http://spark.apache.org/docs/latest/mllib-decision-tree.html). Instead, we will
    focus on the hyper-parameters and how to interpret the model''s efficacy with
    respect to certain criteria / error measures. Let''s start with the basic parameters:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试使用决策树算法来对希格斯玻色子和背景噪音进行分类。我们故意不解释这个算法背后的直觉，因为这已经有大量支持文献供读者消化（[http://www.saedsayad.com/decision_tree.htm](http://www.saedsayad.com/decision_tree.htm),
    http://spark.apache.org/docs/latest/mllib-decision-tree.html）。相反，我们将专注于超参数以及如何根据特定标准/错误度量来解释模型的有效性。让我们从基本参数开始：
- en: '[PRE23]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we are explicitly telling Spark that we wish to build a decision tree classifier
    that looks to distinguish between two classes. Let''s take a closer look at some
    of the hyper-parameters for our decision tree and see what they mean:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们明确告诉Spark，我们希望构建一个决策树分类器，用于区分两类。让我们更仔细地看看我们决策树的一些超参数，看看它们的含义：
- en: '`numClasses`: How many classes are we trying to classify? In this example,
    we wish to distinguish between the Higgs-Boson particle and background noise and
    thus there are four classes:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`numClasses`：我们要分类多少类？在这个例子中，我们希望区分希格斯玻色子粒子和背景噪音，因此有四类：'
- en: '`categoricalFeaturesInfo`: A specification whereby we declare what features
    are categorical features and should not be treated as numbers (for example, ZIP
    code is a popular example). There are no categorical features in this dataset
    that we need to worry about.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`categoricalFeaturesInfo`：一种规范，声明哪些特征是分类特征，不应被视为数字（例如，邮政编码是一个常见的例子）。在这个数据集中，我们不需要担心有分类特征。'
- en: '`impurity`: A measure of the homogeneity of the labels at the node. Currently
    in Spark, there are two measures of impurity with respect to classification: Gini
    and Entropy and one impurity for regression: variance.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`杂质`：节点标签同质性的度量。目前在Spark中，关于分类有两种杂质度量：基尼和熵，回归有一个杂质度量：方差。'
- en: '`maxDepth`: A stopping criterion which limits the depth of constructed trees.
    Generally, deeper trees lead to more accurate results but run the risk of overfitting.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxDepth`：限制构建树的深度的停止准则。通常，更深的树会导致更准确的结果，但也会有过拟合的风险。'
- en: '`maxBins`: Number of bins (think "values") for the tree to consider when making
    splits. Generally, increasing the number of bins allows the tree to consider more
    values but also increases computation time.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxBins`：树在进行分裂时考虑的箱数（考虑“值”）。通常，增加箱数允许树考虑更多的值，但也会增加计算时间。'
- en: Gini versus Entropy
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基尼与熵
- en: In order to determine which one of the impurity measures to use, it's important
    that we cover some foundational knowledge beginning with the concept of **information
    gain**.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定使用哪种杂质度量，重要的是我们先了解一些基础知识，从**信息增益**的概念开始。
- en: 'At it''s core, information gain is as it sounds: the gain in information from
    moving between two states. More accurately, the information gain of a certain
    event is the difference between the amount of information known before and after
    the event takes place. One common measure of this information is looking at the
    **Entropy** which can be defined as:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上，信息增益就是它听起来的样子：在两种状态之间移动时的信息增益。更准确地说，某个事件的信息增益是事件发生前后已知信息量的差异。衡量这种信息的一种常见方法是查看**熵**，可以定义为：
- en: '![](img/00031.jpeg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00031.jpeg)'
- en: Where *p[j]* is the frequency of label *j* at a node.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*p[j]*是节点上标签*j*的频率。
- en: Now that you are familiar with the concept of information gain and Entropy,
    we can move on to what is meant by the **Gini Index** (there is no correlation
    whatsoever to the Gini coefficient).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了信息增益和熵的概念，我们可以继续了解**基尼指数**的含义（与基尼系数完全没有关联）。
- en: 'The **Gini Index**: is a measure of how often a randomly chosen element would
    be misclassified if it were randomly given a label according to the distribution
    of labels at a given node.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**基尼指数**：是一个度量，表示如果随机选择一个元素，根据给定节点的标签分布随机分配标签，它会被错误分类的频率。'
- en: '![](img/00032.jpeg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00032.jpeg)'
- en: Compared with the equation for Entropy, the Gini Index should be computed slightly
    faster due to the absence of a log computation which may be why it is the **default** option
    for many other machine learning libraries including MLlib.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与熵的方程相比，由于没有对数计算，基尼指数的计算速度应该稍快一些，这可能是为什么它是许多其他机器学习库（包括MLlib）的**默认**选项。
- en: 'But does this make it a **better** measure for which to make splits for our
    decision tree? It turns out that the choice of impurity measure has little effect
    on performance with respect to single decision tree algorithms. The reason for
    this, according to Tan et. al, in the book *Introduction to Data Mining*, is that:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是否使它成为我们决策树分裂的**更好**度量？事实证明，杂质度量的选择对于单个决策树算法的性能几乎没有影响。根据谭等人在《数据挖掘导论》一书中的说法，原因是：
- en: '"...This is because impurity measures are quite consistent with each other
    [...]. Indeed, the strategy used to prune the tree has a greater impact on the
    final tree than the choice of impurity measure."'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: “...这是因为杂质度量在很大程度上是一致的 [...]. 实际上，用于修剪树的策略对最终树的影响大于杂质度量的选择。”
- en: 'Now it''s time we train our decision tree classifier on the training data:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候在训练数据上训练我们的决策树分类器了：
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This should yield a final output which looks like this (note that your results
    will be slightly different due to the random split of the data):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该产生一个最终输出，看起来像这样（请注意，由于数据的随机分割，您的结果可能会略有不同）：
- en: '![](img/00033.jpeg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00033.jpeg)'
- en: 'The output shows that decision tree has depth `5` and `63` nodes organized
    in an hierarchical decision predicate. Let''s go ahead and interpret it looking
    at the first five *decisions*. The way it reads is: *"If feature 25''s value is
    less than or equal to 1.0559 AND is less than or equal to 0.61558 AND feature
    27''s value is less than or equal to 0.87310 AND feature 5''s value is less than
    or equal to 0.89683 AND finally, feature 22''s value is less than or equal to
    0.76688, then the prediction is 1.0 (the Higgs-Boson). BUT, these five conditions
    must be met in order for the prediction to hold."* Notice that if the last condition
    is not held (feature 22''s value is `> 0.76688`) but the previous four held conditions
    remain true, then the prediction changes from 1 to 0, indicating background noise.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示决策树的深度为`5`，有`63`个节点按层次化的决策谓词组织。让我们继续解释一下，看看前五个*决策*。它的读法是：“如果特征25的值小于或等于1.0559并且小于或等于0.61558并且特征27的值小于或等于0.87310并且特征5的值小于或等于0.89683并且最后，特征22的值小于或等于0.76688，那么预测值为1.0（希格斯玻色子）。但是，这五个条件必须满足才能成立。”请注意，如果最后一个条件不成立（特征22的值大于0.76688），但前四个条件仍然成立，那么预测将从1变为0，表示背景噪音。
- en: 'Now, let''s score the model on our test dataset and print the prediction error:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对我们的测试数据集对模型进行评分并打印预测错误：
- en: '[PRE25]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00034.jpeg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00034.jpeg)'
- en: 'After some period of time, the model will score all of the test set data and
    then compute an error rate which we defined in the preceding code. Again, your
    error rate will be slightly different than ours but as we show, our simple decision
    tree model has an error rate of ~33%. However, as you know, there are different
    kinds of errors that we can possibly make and so it''s worth exploring what those
    types of error are by constructing a confusion matrix:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间后，模型将对所有测试集数据进行评分，然后计算一个我们在前面的代码中定义的错误率。同样，你的错误率可能会略有不同，但正如我们所展示的，我们的简单决策树模型的错误率约为33%。然而，正如你所知，我们可能会犯不同类型的错误，因此值得探索一下通过构建混淆矩阵来了解这些错误类型是什么：
- en: '[PRE26]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding code is using advanced the Spark method `combineByKey` which
    allows us to map each (K,V)-pair to a value, which is going to represent the output
    of the group by the key operation. In this case, the (K,V)-pair represents the
    actual value K and prediction V. We map each prediction to a tuple by creating
    a combiner (parameter `createCombiner`) - if the predicted values is `0`, then
    we map to `(1,0)`; otherwise, we map to `(0,1)`. Then we need to define how combiners
    accept a new value and how combiners are merged together. At the end, the method
    produces:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码使用了高级的Spark方法`combineByKey`，它允许我们将每个(K,V)对映射到一个值，这个值将代表按键操作的输出。在这种情况下，(K,V)对表示实际值K和预测值V。我们通过创建一个组合器（参数`createCombiner`）将每个预测映射到一个元组
    - 如果预测值为`0`，则映射为`(1,0)`；否则，映射为`(0,1)`。然后我们需要定义组合器如何接受新值以及如何合并组合器。最后，该方法产生：
- en: '[PRE27]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The resulting array contains two tuples - one for the actual value `0` and another
    for the actual value `1`. Each tuple contains the number of predictions `0` and
    `1`. Hence, it is easy to extract all necessary to present a nice confusion matrix.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的数组包含两个元组 - 一个用于实际值`0`，另一个用于实际值`1`。每个元组包含预测`0`和`1`的数量。因此，很容易提取所有必要的内容来呈现一个漂亮的混淆矩阵。
- en: '[PRE28]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The code extracts all true negatives and positives predictions and also missed
    predictions and outputs of the confusion matrix based on the template shown on
    *Figure 9*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码提取了所有真负和真正的预测，还有错过的预测和基于*图9*模板的混淆矩阵的输出：
- en: '![](img/00035.jpeg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00035.jpeg)'
- en: 'In the preceding code, we are using a powerful Scala feature, which is called
    *string interpolation*: `println(f"...")`. It allows for the easy construction
    of the desired output by combining a string output and actual Scala variables.
    Scala supports different string "interporlators", but the most used are *s* and
    *f*. The *s* interpolator allows for referencing any Scala variable or even code:
    `s"True negative: ${tn}"`. While, the *f* interpolator is type-safe - that means
    the user is required to specify the type of variable to show: `f"True negative:
    ${tn}%5d"` - and references the variable `tn` as decimal type and asks for printing
    on five decimal spaces.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '在前面的代码中，我们使用了一个强大的Scala特性，称为*字符串插值*：`println(f"...")`。它允许通过组合字符串输出和实际的Scala变量来轻松构造所需的输出。Scala支持不同的字符串“插值器”，但最常用的是*s*和*f*。*s*插值器允许引用任何Scala变量甚至代码：`s"True
    negative: ${tn}"`。而*f*插值器是类型安全的 - 这意味着用户需要指定要显示的变量类型：`f"True negative: ${tn}%5d"`
    - 并引用变量`tn`作为十进制类型，并要求在五个十进制空间上打印。'
- en: Going back to our first example in this chapter, we can see that our model is
    making most of the errors in detecting the actual Boson particle. In this case,
    all data points representing detection of Boson are wrongly missclassified as
    non-Boson. However, the overall error rate is pretty low! This is a nice example
    of how the overall error rate can be misleading for a dataset with an imbalanced
    response.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 回到本章的第一个例子，我们可以看到我们的模型在检测实际的玻色子粒子时出现了大部分错误。在这种情况下，代表玻色子检测的所有数据点都被错误地分类为非玻色子。然而，总体错误率非常低！这是一个很好的例子，说明总体错误率可能会对具有不平衡响应的数据集产生误导。
- en: '![](img/00036.jpeg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00036.jpeg)'
- en: Figure 9 - Confusion matrix schema.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图9 - 混淆矩阵模式。
- en: 'Next, we will consider another modeling metric used to judge classification
    models, called the **Area Under the** (Receiver Operating Characteristic) **Curve**
    (**AUC**) (see the following figure for an example). The **Receiver Operating
    Characteristic** (**ROC**) curve is a graphical representation of the **True Positive
    Rate** versus the **False Positive Rate**:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将考虑另一个用于评判分类模型的建模指标，称为**曲线下面积**（受试者工作特征）**AUC**（请参见下图示例）。**受试者工作特征**（**ROC**）曲线是**真正率**与**假正率**的图形表示：
- en: '**True Positive Rate**: The total number of true positives divided by the sum
    of true positives and false negatives. Expressed differently, it is the ratio
    of the true signals for the Higgs-Boson particle (where the actual label was 1)
    to all the predicted signals for the Higgs-Boson (where our model predicted label
    is 1). The value is shown on the *y*-axis.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真正阳性率：真正阳性的总数除以真正阳性和假阴性的总和。换句话说，它是希格斯玻色子粒子的真实信号（实际标签为1）与希格斯玻色子的所有预测信号（我们的模型预测标签为1）的比率。该值显示在*y*轴上。
- en: '**False Positive Rate**: The total number of false positives divided by the
    sum of false positives and true negatives, which is plotted on the *x*-axis.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假正率：假阳性的总数除以假阳性和真阴性的总和，这在*x*轴上绘制。
- en: For more metrics, please see the figure for "Metrics derived from confusion
    matrix".
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多指标，请参见“从混淆矩阵派生的指标”图。
- en: '![](img/00037.jpeg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00037.jpeg)'
- en: Figure 10 - Sample AUC Curve with an AUC value of 0.94
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图10 - 具有AUC值0.94的样本AUC曲线
- en: 'It follows that the ROC curve portrays our model''s tradeoff of TPR against
    FPR for a given decision threshold (the decision threshold is the cutoff point
    whereby we say it is label 0 or label 1). Therefore, the area under the ROC curve
    can be thought of as an *average model accuracy* whereby a value of 1.0 would
    represent perfect classification, 0.5 would be a coin-flip (meaning our model
    is doing a 50-50 job at guessing 1 or 0), and anything less than 0.5 would mean
    flipping a coin is more accurate than our model! This is an incredibly useful
    metric which we will see can be used to compare against different hyper-parameter
    tweaks and different models altogether! Let''s go ahead and create a function
    which will allow us to calculate the AUC for our decision tree model which we
    will use to compare against other models:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 由此可见，ROC曲线描绘了我们的模型在给定决策阈值下TPR与FPR的权衡。因此，ROC曲线下的面积可以被视为*平均模型准确度*，其中1.0代表完美分类，0.5代表抛硬币（意味着我们的模型在猜测1或0时做了一半的工作），小于0.5的任何值都意味着抛硬币比我们的模型更准确！这是一个非常有用的指标，我们将看到它可以用来与不同的超参数调整和不同的模型进行比较！让我们继续创建一个函数，用于计算我们的决策树模型的AUC，以便与其他模型进行比较：
- en: '[PRE29]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00038.jpeg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00038.jpeg)'
- en: Spark MLlib models do not share a common definition of interfaces; hence in
    the preceding example, we have to define the type `Predictor` exposing the method
    predict and use Scala structural typing in the definition of the method `computeMetrics`.
    Later in the book, we will show the Spark ML package, which is based on a unified
    pipeline-based API.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib模型没有共同的接口定义；因此，在前面的例子中，我们必须定义类型`Predictor`，公开方法`predict`并在方法`computeMetrics`的定义中使用Scala结构化类型。本书的后面部分将展示基于统一管道API的Spark
    ML包。
- en: '![](img/00039.jpeg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00039.jpeg)'
- en: Figure 11 - Metrics derived from confusion matrix.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图11 - 从混淆矩阵派生的指标。
- en: Interested in a great read on this subject? There is no holy bible that is the
    be-all-end-all. The book, *The Elements of Statistical Learning,* by Trevor Hastie
    - renowned statistics professor from Stanford University - is a great source of
    information. This book offers useful nuggets for both beginners and advanced practitioners
    of machine learning and is highly recommended.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个主题感兴趣吗？没有一本圣经是万能的。斯坦福大学著名统计学教授Trevor Hastie的书《统计学习的要素》是一个很好的信息来源。这本书为机器学习的初学者和高级实践者提供了有用的信息，强烈推荐。
- en: It is important to keep in mind that results between runs can be slightly different,
    since the Spark decision tree implementation is using internally the `RandomForest`
    algorithm, which is non-deterministic if a seed for a random generator is not
    specified. The problem is that the MLLib API for Spark `DecisionTree` does not
    allow to pass a seed as a parameter.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是，由于Spark决策树实现在内部使用`RandomForest`算法，如果未指定随机生成器的种子，运行之间的结果可能会略有不同。问题在于Spark的MLLib
    API`DecisionTree`不允许将种子作为参数传递。
- en: Next model – tree ensembles
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一个模型 - 树集成
- en: 'Algorithms such as **Random Forest** (**RF**) or **Gradient Boosted Machine**
    (**GBM)** (also referred to as Gradient Boosted Trees) are two examples of ensemble
    tree-based models which are currently available in MLlib; you can think of an
    ensemble as an *uber-model* which represents a collection of base models. The
    best way to think about what an ensemble is doing behind the scenes is to consider
    a simple analogy:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林（RF）或梯度提升机（GBM）（也称为梯度提升树）等算法是目前在MLlib中可用的集成基于树的模型的两个例子；您可以将集成视为代表基本模型集合的*超级模型*。想要了解集成在幕后的工作原理，最好的方法是考虑一个简单的类比：
- en: '*"Suppose that you are the head coach of a famous soccer club and you have
    heard rumors of an incredible athlete from Brazil and it may be advantageous to
    sign this young athlete to your club before the other teams do; but your schedule
    is incredibly busy and instead, you send 10 of your assistant coaches to assess
    the player. Each one of your assistant coaches grades the player based on his/her
    coaching philosophy - maybe one coach wants to measure how fast the player can
    run 40 yards while another coach thinks height and arm-reach are important. Regardless
    of how each coach defines "athlete potential" you, as head coach, just want to
    know if you should sign the player to a contract now or wait. And so it goes that
    your coaches fly down to Brazil and each coach makes an assessment; upon arrival
    you go up to each of your coaches and ask "should we draft this player now or
    wait?" and, based on a simple rule like majority vote, you can make your decision.
    This an example of what an ensemble is doing behind the scenes with respect to
    a classification task."*'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: “假设你是一家著名足球俱乐部的主教练，你听说了一位来自巴西的不可思议的运动员的传闻，签下这位年轻运动员可能对你的俱乐部有利，但你的日程安排非常繁忙，所以你派了10名助理教练去评估这位球员。你的每一位助理教练都根据他/她的教练理念对球员进行评分——也许有一位教练想要测量球员跑40码的速度，而另一位教练认为身高和臂展很重要。无论每位教练如何定义“运动员潜力”，你作为主教练，只想知道你是否应该立即签下这位球员或者等待。于是你的教练们飞到巴西，每位教练都做出了评估；到达后，你走到每位教练面前问：“我们现在应该选这位球员还是等一等？”根据多数投票的简单规则，你可以做出决定。这是一个关于集成在分类任务中背后所做的事情的例子。”
- en: You can think of each coach as a decision tree and, therefore, you will have
    an ensemble of 10 trees (for 10 coaches). How each coach assesses the player is
    highly specific and this holds true for our trees as well; for each of the 10
    trees created features are selected randomly at each node (hence the random, in
    RF. Forest because there are many trees!). The reason for introducing this randomness
    and other base models is to prevent over-fitting the data. While RF and GBM are
    both tree-based ensembles, the manner in which they go about training is slightly
    different and deserves mention.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将每个教练看作是一棵决策树，因此您将拥有10棵树的集合（对应10个教练）。每个教练如何评估球员都是非常具体的，我们的树也是如此；对于创建的10棵树，每个节点都会随机选择特征（因此RF中有随机性，因为有很多树！）。引入这种随机性和其他基本模型的原因是防止过度拟合数据。虽然RF和GBM都是基于树的集合，但它们训练的方式略有不同，值得一提。
- en: GBMs must be trained one tree at a time in order to minimize a `loss` function
    (for example, `log-loss`, squared error, and so on) and usually take longer to
    train than an RF which can generate multiple trees in parallel.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: GBM必须一次训练一棵树，以最小化`loss`函数（例如`log-loss`，平方误差等），通常比RF需要更长的时间来训练，因为RF可以并行生成多棵树。
- en: However, when training a GBM, it is recommended to make shallow trees which
    in turn lends itself to faster training.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在训练GBM时，建议制作浅树，这反过来有助于更快的训练。
- en: RFs generally do not overfit the data compared to a GBM; that is, we can add
    more trees to our forest and be less prone to over-fitting than if we added more
    trees to our GBM.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RFs通常不像GBM那样过度拟合数据；也就是说，我们可以向我们的森林中添加更多的树，而不容易过度拟合，而如果我们向我们的GBM中添加更多的树，就更容易过度拟合。
- en: Hyper-parameter tuning for an RF is much simpler than GBM. In his paper, *Influence
    of Hyperparameters on Random Forest Accuracy*, Bernard et al. show via experimentation
    that the number of K random features to select at each node is a key influencer
    with respect to model accuracy ([https://hal.archives-ouvertes.fr/hal-00436358/document](https://hal.archives-ouvertes.fr/hal-00436358/document))
    Conversely, a GBM has much more hyper-parameters that must be considered, such
    as `loss` function, learning rate, number of iterations, and so on.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RF的超参数调整比GBM简单得多。在他的论文《超参数对随机森林准确性的影响》中，Bernard等人通过实验证明，在每个节点选择的K个随机特征数是模型准确性的关键影响因素。相反，GBM有更多必须考虑的超参数，如`loss`函数、学习率、迭代次数等。
- en: As with most *which is better* questions in data science, choosing between an
    RF and a GBM is open-ended and very task and dataset dependent.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数数据科学中的“哪个更好”问题一样，选择RF和GBM是开放式的，非常依赖任务和数据集。
- en: Random forest model
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林模型
- en: Now, let's try building a random forest using 10 decision trees.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用10棵决策树构建一个随机森林。
- en: '[PRE30]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Just like our single decision tree model, we start by declaring the hyper-parameters,
    many of which should be familiar to you already from the decision tree example.
    In the preceding code, we will start by creating a random forest of 10 trees,
    solving a two-class problem. One key feature that is different is the feature
    subset strategy described as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们的单棵决策树模型一样，我们首先声明超参数，其中许多参数您可能已经从决策树示例中熟悉。在前面的代码中，我们将创建一个由10棵树解决两类问题的随机森林。一个不同的关键特性是特征子集策略，描述如下：
- en: The `featureSubsetStrategy` object gives the number of features to use as candidates
    for making splits at each node. Can either be a fraction (for example, 0.5) or
    a function based on the number of features in your dataset. The setting `auto` allows
    the algorithm to choose this number for you but a common soft-rule states to use
    the square-root of the number of features you have.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`featureSubsetStrategy`对象给出了要在每个节点进行分割的候选特征数。可以是一个分数（例如0.5），也可以是基于数据集中特征数的函数。设置`auto`允许算法为您选择这个数字，但一个常见的软规则是使用您拥有的特征数的平方根。'
- en: 'Now that we have trained our model, let''s score it against our hold-out set
    and compute the total error:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练好了我们的模型，让我们对我们的留出集进行评分并计算总误差：
- en: '[PRE31]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00040.jpeg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00040.jpeg)'
- en: 'And also compute AUC by using the already defined method `computeMetrics`:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用已定义的`computeMetrics`方法计算AUC：
- en: '[PRE32]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](img/00041.jpeg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00041.jpeg)'
- en: Our RF - where we hardcode the hyper-parameters - performs much better than
    our single decision tree with respect to the overall model error and AUC. In the
    next section, we will introduce the concept of a grid search and how we can try
    varying hyper-parameter values / combinations and measure the impact on the model
    performance.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的RF - 在其中硬编码超参数 - 相对于整体模型错误和AUC表现得比我们的单棵决策树要好得多。在下一节中，我们将介绍网格搜索的概念以及我们如何尝试变化超参数值/组合并衡量对模型性能的影响。
- en: Again, results can slightly differ between runs. However, in contrast to the
    decision tree, it is possible to make a run deterministic by passing a seed as
    a parameter of the method `RandomForest.trainClassifier`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，结果在运行之间可能略有不同。但是，与决策树相比，可以通过将种子作为`RandomForest.trainClassifier`方法的参数传递来使运行确定性。
- en: Grid search
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网格搜索
- en: As with most algorithms in MLlib and H2O, there are many hyper-parameters to
    choose from which can have a significant effect on the performance of the model.
    Given the endless amount of combinations that are possible, is there an intelligent
    way we can begin looking at what combinations look more promising than others?
    Thankfully, the answer is an emphatic "YES!" and the solution is known as a grid
    search, which is ML-speak for running many models that use different combinations
    of hyper-parameters.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLlib和H2O中，与大多数算法一样，有许多可以选择的超参数，这些超参数对模型的性能有显著影响。鉴于可能存在无限数量的组合，我们是否可以以智能的方式开始查看哪些组合比其他组合更有前途？幸运的是，答案是“YES！”解决方案被称为网格搜索，这是运行使用不同超参数组合的许多模型的ML术语。
- en: 'Let''s try running a simple grid search using the RF algorithm. In this case,
    the RF model builder is invoked for each combination of parameters from a defined
    hyper-space of parameters:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用RF算法运行一个简单的网格搜索。在这种情况下，RF模型构建器被调用，用于从定义的超参数空间中的每个参数组合：
- en: '[PRE33]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'What we have just written is a `for`-loop that is going to try a number of
    different combinations with respect to the number of trees, impurity type, depth
    of the trees, and the bins (that is, values to try); And then, for each model
    created based on these hyper-parameter permutations, we are going to score the
    trained model against our hold-out set while computing the AUC metric and the
    overall error rate. In total we get *2*2*2*2=16* models. Again, your models will
    be slightly different than the ones we show here but your output should resemble
    something like this:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚写的是一个`for`循环，它将尝试不同组合的数量，涉及树的数量、不纯度类型、树的深度和bin值（即要尝试的值）；然后，对于基于这些超参数排列组合创建的每个模型，我们将对训练模型进行评分，同时计算AUC指标和整体错误率。总共我们得到*2*2*2*2=16*个模型。再次强调，您的模型可能与我们在此处展示的模型略有不同，但您的输出应该类似于这样：
- en: '![](img/00042.jpeg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00042.jpeg)'
- en: 'Look at the first entry of our output:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 查看我们输出的第一个条目：
- en: '[PRE34]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can interpret this as follows: for the combination of 15 decision trees,
    using Entropy as our impurity measure, along with a tree depth of 20 (for each
    tree) and a bin value of 20, our AUC is `0.695`. Note that the results are shown
    in the order you wrote them initially. For our grid search using the RF algorithm,
    we can easily get a combination of hyper-parameters producing the highest AUC:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样解释：对于15棵决策树的组合，使用熵作为我们的不纯度度量，以及树深度为20（对于每棵树）和bin值为20，我们的AUC为`0.695`。请注意，结果按照您最初编写它们的顺序显示。对于我们使用RF算法的网格搜索，我们可以轻松地获得产生最高AUC的超参数组合：
- en: '[PRE35]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00043.jpeg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00043.jpeg)'
- en: Gradient boosting machine
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升机
- en: So far, the best AUC we are able to muster is a 15-decision tree RF that has
    an AUC value of `0.698`. Now, let's go through the same process of running a single
    gradient boosted machine with hardcoded hyper-parameters and then doing a grid
    search over these parameters to see if we can get a higher AUC using this algorithm.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们能够达到的最佳AUC是一个15棵决策树的RF，其AUC值为`0.698`。现在，让我们通过相同的过程来运行一个使用硬编码超参数的单个梯度提升机，然后对这些参数进行网格搜索，以查看是否可以使用该算法获得更高的AUC。
- en: 'Recall that a GBM is slightly different than an RF due to its iterative nature
    of trying to reduce an overall `loss` function that we declare beforehand. Within
    MLlib there are three different loss functions to choose from as of 1.6.0:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，由于其迭代性质试图减少我们事先声明的总体`loss`函数，GBM与RF略有不同。在MLlib中，截至1.6.0，有三种不同的损失函数可供选择：
- en: '**Log-loss**: Use this `loss` function for classification tasks (note that
    GBM only supports binary classification for Spark. If you wish to use a GBM for
    multi-class classification, please use H2O''s implementation, which we will show
    in the next chapter.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对数损失**：对于分类任务使用这个`loss`函数（请注意，对于Spark，GBM仅支持二元分类。如果您希望对多类分类使用GBM，请使用H2O的实现，我们将在下一章中展示）。'
- en: '**Squared-error**: Use this `loss` function for regression tasks it is is the
    current default `loss` function for this type of problem.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平方误差**：对于回归任务使用这个`loss`函数，它是这种类型问题的当前默认`loss`函数。'
- en: '**Absolute-error**: Another `loss` function that is available to use for regression
    tasks. Given that this function takes the absolute difference between the predicted
    and actual value, it controls for outliers much better than the squared error.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**绝对误差**：另一个可用于回归任务的`loss`函数。鉴于该函数取预测值和实际值之间的绝对差异，它比平方误差更好地控制异常值。'
- en: 'Given our task of binary classification, we will employ the `log-loss` function
    and begin building a 10 tree GBM model:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们的二元分类任务，我们将使用`log-loss`函数并开始构建一个10棵树的GBM模型：
- en: '[PRE36]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Notice that we must declare a boosting strategy before we can build our model.
    The reason is that MLlib does not know what type of problem we are tackling beforehand:
    classification or regression? So this strategy is letting Spark know that this
    is a binary classification problem and to use the declared hyper-parameters to
    build our model.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们必须在构建模型之前声明一个提升策略。原因是MLlib不知道我们要解决什么类型的问题：分类还是回归？因此，这个策略让Spark知道这是一个二元分类问题，并使用声明的超参数来构建我们的模型。
- en: 'Following are some hyper-parameters to keep in mind when training GBMs:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些训练GBM时要记住的超参数：
- en: '`numIterations`: By definition, a GBM builds trees one at a time in order to
    minimize a `loss` function we declare. This hyper-parameter controls the number
    of trees to build; be careful to not build too many trees as performance at test-time
    may not be ideal.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numIterations`：根据定义，GBM一次构建一棵树，以最小化我们声明的`loss`函数。这个超参数控制要构建的树的数量；要小心不要构建太多的树，因为测试时的性能可能不理想。'
- en: '`loss`: Where you declare which `loss` function to use depends on the question
    being asked and the dataset.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`：您声明使用哪个`loss`函数取决于所提出的问题和数据集。'
- en: '`learningRate`: Optimizes speed of learning. Lower values (< 0.1) means slower
    learning, and improved generalization. However, it also needs a higher number
    of iterations and hence a longer computation time.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learningRate`：优化学习速度。较低的值（<0.1）意味着学习速度较慢，泛化效果更好。然而，它也需要更多的迭代次数，因此计算时间更长。'
- en: 'Let''s score this model against the hold-out set and compute our AUC:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对保留集对这个模型进行评分，并计算我们的AUC：
- en: '[PRE37]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00044.jpeg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00044.jpeg)'
- en: 'As a final step, we will perform a grid-search over a few hyper-parameters
    and, similar to our previous RF grid-search example, output the combinations and
    their respective errors and AUC calculations:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将对一些超参数进行网格搜索，并且与我们之前的RF网格搜索示例类似，输出组合及其相应的错误和AUC计算：
- en: '[PRE38]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can print the first 10 lines of the result sorted by AUC:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以打印前10行结果，按AUC排序：
- en: '[PRE39]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00045.jpeg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00045.jpeg)'
- en: 'And we can easily get the model producing maximal AUC:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 而且我们可以很容易地得到产生最大AUC的模型：
- en: '[PRE40]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00046.jpeg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00046.jpeg)'
- en: Last model - H2O deep learning
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后一个模型-H2O深度学习
- en: So far we used the Spark MLlib for building different models; however, we can
    use also H2O algorithms as well. So let's try them!
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用Spark MLlib构建了不同的模型；然而，我们也可以使用H2O算法。所以让我们试试吧！
- en: At first, we are going to transfer our training and testing datasets over to
    H2O and create a DNN for our binary classification problem. To reiterate, this
    is made possible because Spark and H2O are sharing the same JVM which facilitates
    passing Spark RDDs over to H2O hex frames and vice versa.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将我们的训练和测试数据集传输到H2O，并为我们的二元分类问题创建一个DNN。重申一遍，这是可能的，因为Spark和H2O共享相同的JVM，这有助于将Spark
    RDD传递到H2O六角框架，反之亦然。
- en: 'All the models that we have run up to now have been in MLlib but now we are
    going to use H2O to build a DNN using the same training and testing sets that
    we used, which means we need to send this data over to our H2O cloud as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们运行的所有模型都是在MLlib中，但现在我们将使用H2O来使用相同的训练和测试集构建一个DNN，这意味着我们需要将这些数据发送到我们的H2O云中，如下所示：
- en: '[PRE41]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: To verify that we have successfully transferred our training and testing RDDs
    (which we converted to DataFrames), we can execute this command in our Flow notebook
    (all commands are executed with *Shift+Enter*). Notice that we have two H2O frames
    now called `trainingRDD` and `testRDD` which you can see in our H2O notebook by
    running the command `getFrames.`
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们已成功转移我们的训练和测试RDD（我们转换为数据框），我们可以在我们的Flow笔记本中执行这个命令（所有命令都是用*Shift+Enter*执行的）。请注意，我们现在有两个名为`trainingRDD`和`testRDD`的H2O框架，您可以通过运行命令`getFrames`在我们的H2O笔记本中看到。
- en: '![](img/00047.jpeg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00047.jpeg)'
- en: Figure 12 - List of available H2O frames is available by typing "getFrames"
    into Flow UI.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图12 - 通过在Flow UI中输入“getFrames”可以查看可用的H2O框架列表。
- en: We can easily explore frames to see their structure by typing `getFrameSummary
    "trainingHF"` into the Flow cell or just by clicking on the frame name (see *Figure
    13*).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地探索框架，查看它们的结构，只需在Flow单元格中键入`getFrameSummary "trainingHF"`，或者只需点击框架名称（参见*图13*）。
- en: '![](img/00048.jpeg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00048.jpeg)'
- en: Figure 13 - Structure of training frame.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图13 - 训练框架的结构。
- en: The preceding figure shows structure of the training frame - it has 80,491 rows
    and 29 columns; there are numeric columns named *features0*, *features1*, ...
    with real values and the first column label containing integer values.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了训练框架的结构-它有80,491行和29列；有名为*features0*、*features1*的数值列，具有实际值，第一列标签包含整数值。
- en: 'Since we would like to perform a binary classification, we need to transform
    the "label" column from the integer to categorical type. You can do that easily
    by clicking on the action *Convert to enum* in the Flow UI or in the Spark console
    by executing the following commands:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想进行二元分类，我们需要将“label”列从整数转换为分类类型。您可以通过在Flow UI中点击*Convert to enum*操作或在Spark控制台中执行以下命令来轻松实现：
- en: '[PRE42]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The code replaces the first vector by a transformed vector and removes the original
    vector from memory. Furthermore, the call `update` propagates changes into the
    shared distributed store, so they become visible by all the nodes in the cluster.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码将第一个向量替换为转换后的向量，并从内存中删除原始向量。此外，调用`update`将更改传播到共享的分布式存储中，因此它们对集群中的所有节点都是可见的。
- en: Build a 3-layer DNN
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个3层DNN
- en: 'H2O exposes slightly different way of building models; however, it is unified
    among all H2O models. There are three basic building blocks:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: H2O暴露了略有不同的构建模型的方式；然而，它在所有H2O模型中是统一的。有三个基本构建模块：
- en: '**Model parameters**: Defines inputs and algorithm specific parameters'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型参数**：定义输入和特定算法参数'
- en: '**Model builder**: Accepts model parameters and produces a model'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型构建器**：接受模型参数并生成模型'
- en: '**Model**: Contains model definition but also technical information about model
    building such as score times or error rates for each iteration'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：包含模型定义，但也包括有关模型构建的技术信息，如每次迭代的得分时间或错误率。'
- en: 'Prior to building our model, we need to construct parameters for the DeepLearning
    algorithm:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建我们的模型之前，我们需要为深度学习算法构建参数：
- en: '[PRE43]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let''s walk through the parameters and figure out the model we just initialized:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们浏览一下参数，并找出我们刚刚初始化的模型：
- en: '`train` and `valid`: Specifying the training and testing set that we created.
    Note that these RDDs are in fact, H2O frames.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train`和`valid`：指定我们创建的训练和测试集。请注意，这些RDD实际上是H2O框架。'
- en: '`response_column`: Specifying the label that we use which we declared beforehand
    was the first element (indexes from 0) in each frame.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`response_column`：指定我们使用的标签，我们之前声明的是每个框架中的第一个元素（从0开始索引）。'
- en: '`epochs`: An extremely important parameter which specifies how many times the
    network should pass over the training data; generally, models that are trained
    with higher `epochs` allow the network to *learn* new features and produce better
    model results. The caveat to this, however, is that these networks that have been
    trained for a long time suffer from overfitting and may not generalize well on
    new data.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`：这是一个非常重要的参数，它指定网络应该在训练数据上传递多少次；通常，使用更高`epochs`训练的模型允许网络*学习*新特征并产生更好的模型结果。然而，这种训练时间较长的网络容易出现过拟合，并且可能在新数据上泛化效果不佳。'
- en: '`activation`: These are the various non-linear functions that will be applied
    to the input data. In H2O there are three primary activations from which to choose
    from:'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`激活`：这些是将应用于输入数据的各种非线性函数。在H2O中，有三种主要的激活函数可供选择：'
- en: '`Rectifier`: Sometimes referred to as **rectified linear unit** (**ReLU**),
    this is a function that has a lower limit of **0** but goes to positive infinity
    in a linear fashion. In terms of biology, these units are shown to be closer to
    actual neuron activations. Currently, this is the default activation function
    in H2O given its results for tasks such as image recognition and speed.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Rectifier`：有时被称为**整流线性单元**（**ReLU**），这是一个函数，其下限为**0**，但以线性方式达到正无穷大。从生物学的角度来看，这些单元被证明更接近实际的神经元激活。目前，这是H2O中默认的激活函数，因为它在图像识别和速度等任务中的结果。'
- en: '![](img/00049.jpeg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00049.jpeg)'
- en: Figure 14 - Rectifier activation function
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图14 - 整流器激活函数
- en: '`Tanh`: A modified logistic function that is bound between **-1** and **1**
    but goes through the origin at (0,0). Due to its symmetry around **0**, convergence
    is usually faster.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tanh`：一个修改后的逻辑函数，其范围在**-1**和**1**之间，但在(0,0)处通过原点。由于其在**0**周围的对称性，收敛通常更快。'
- en: '![](img/00050.jpeg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00050.jpeg)'
- en: Figure 15 - Tanh activation function and Logistic function - note difference
    between Tanh.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图15 - 双曲正切激活函数和逻辑函数 - 注意双曲正切之间的差异。
- en: '`Maxout`: A function whereby each neuron picks the largest value coming from
    k separate channels:'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Maxout`：一种函数，其中每个神经元选择来自k个单独通道的最大值：'
- en: '**hidden**: Another extremely important hyper-parameter, this is where we specify
    two things:'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**hidden**：另一个非常重要的超参数，这是我们指定两件事的地方：'
- en: The number of layers (which you can create with additional commas). Note that
    in the GUI, the default parameter is a two-layers hidden network with 200 hidden
    neurons per layer.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层的数量（您可以使用额外的逗号创建）。请注意，在GUI中，默认参数是一个具有每层200个隐藏神经元的两层隐藏网络。
- en: 'The number of neurons per layer. As with most things regarding machine learning,
    there is no set rule on what this number should be and experimentation is usually
    best. However, there are some additional tuning parameters we will cover in the
    next chapter that will help you think about this, namely: L1 and L2 regularization
    and dropout.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每层的神经元数量。与大多数关于机器学习的事情一样，关于这个数字应该是多少并没有固定的规则，通常最好进行实验。然而，在下一章中，我们将介绍一些额外的调整参数，这将帮助您考虑这一点，即：L1和L2正则化和丢失。
- en: Adding more layers
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加更多层
- en: The reason for adding more layers to the network comes from our understanding
    of how the visual cortex works for humans. This is a dedicated area in the rear
    part of your brain that is used for recognizing objects/patterns/numbers, and
    so on, and is composed of complex layers of neurons that work to encode visual
    information and classify them accordingly based on prior knowledge.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 增加网络层的原因来自于我们对人类视觉皮层工作原理的理解。这是大脑后部的一个专门区域，用于识别物体/图案/数字等，并由复杂的神经元层组成，用于编码视觉信息并根据先前的知识进行分类。
- en: Not surprisingly, there is no set rule on how many layers a network needs in
    order to produce good results and experimentation is highly recommended!
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，网络需要多少层才能产生良好的结果并没有固定的规则，强烈建议进行实验！
- en: Building models and inspecting results
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型和检查结果
- en: 'So now that you understand a little about the parameters and the model that
    we want to run, it''s time to go ahead and train and inspect our network:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了一些关于参数和我们想要运行的模型的信息，是时候继续训练和检查我们的网络了：
- en: '[PRE44]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The code created the `DeepLearning` model builder and launched it. By default,
    the launch of `trainModel` is asynchronous (that is, it never blocks, but returns
    a job), but it is possible to wait until the end of computation by calling the
    method `get`. You can also explore the job progress in UI or even explore the
    unfinished model by typing `getJobs` into the Flow UI (see *Figure 18*).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 代码创建了`DeepLearning`模型构建器并启动了它。默认情况下，`trainModel`的启动是异步的（即它不会阻塞，但会返回一个作业），但可以通过调用`get`方法等待计算结束。您还可以在UI中探索作业进度，甚至可以通过在Flow
    UI中键入`getJobs`来探索未完成的模型（参见*图18*）。
- en: '![](img/00051.jpeg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00051.jpeg)'
- en: Figure 18 - The command getJobs provides a list of executed jobs with their
    status.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图18 - 命令getJobs提供了一个已执行作业的列表及其状态。
- en: 'The result of the computation is a DeepLearning model - we can directly explore
    the model and its details from the Spark shell:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 计算的结果是一个深度学习模型 - 我们可以直接从Spark shell探索模型及其细节：
- en: '[PRE45]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We can also obtain a frame of predictions for the test data directly by calling
    the `score` method of the model:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过调用模型的`score`方法直接获得测试数据的预测框架：
- en: '[PRE46]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The table contains three columns:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 表格包含三列：
- en: '`predict`: Predicted value based on default threshold'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict`：基于默认阈值的预测值'
- en: '`p0`: Probability of selecting class 0'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p0`：选择类0的概率'
- en: '`p1`: Probability of selecting class 1'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p1`：选择类1的概率'
- en: 'We can also get model metrics for the test data:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以获得测试数据的模型指标：
- en: '[PRE47]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![](img/00052.jpeg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00052.jpeg)'
- en: The output directly shows the AUC and accuracy (respective error rate). Please
    note that the model is really good at predicting Higgs-Boson; on the other hand,
    it has a high False Positive rate!
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 输出直接显示了AUC和准确率（相应的错误率）。请注意，该模型在预测希格斯玻色子方面确实很好；另一方面，它的假阳性率很高！
- en: 'Finally, let''s see how we can build a similar model using the GUI, only this
    time, we are going to exclude the physicist-hand-derived features from our model
    and use more neurons for inner layers:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看如何使用GUI构建类似的模型，只是这一次，我们将从模型中排除物理学家手工提取的特征，并在内部层使用更多的神经元：
- en: Select the model to use for TrainingHF.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择用于TrainingHF的模型。
- en: As you can see, H2O and MLlib share many of the same algorithms with differing
    levels of functionality. Here we are going to select *Deep Learning* and then
    de-select the last eight hand-derived features.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，H2O和MLlib共享许多相同的算法，但功能级别不同。在这里，我们将选择*深度学习*，然后取消选择最后八个手工提取的特征。
- en: '![](img/00053.jpeg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00053.jpeg)'
- en: Figure 19- Selecting model algorithm
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图19- 选择模型算法
- en: Build DNN and exclude hand-derived features.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建DNN并排除手工提取的特征。
- en: Here we are manually choosing to ignore features 21-27, which represent physicist-derived
    features in the hope that our network will learn them. Note also the ability to
    perform k-folds cross - validation should you choose to go this route as well.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们手动选择忽略特征21-27，这些特征代表物理学家提取的特征，希望我们的网络能够学习它们。还要注意，如果选择这条路线，还可以执行k折交叉验证。
- en: '![](img/00054.jpeg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00054.jpeg)'
- en: Figure 20 - Selecting input features.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图20 - 选择输入特征。
- en: Specify the network topology.
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定网络拓扑。
- en: As you can see, we are going to build a three-layer DNN using the rectifier
    activation function, where each layer will have 1,024 hidden neurons and this
    will run for 100 `epochs`.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们将使用整流器激活函数构建一个三层DNN，其中每一层将有1,024个隐藏神经元，并且将运行100个`epochs`。
- en: '![](img/00055.jpeg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00055.jpeg)'
- en: Figure 21 - Configuring network topology with 3 layers, 1024 neurons per layer.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图21 - 配置具有3层，每层1024个神经元的网络拓扑。
- en: Explore the model results.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索模型结果。
- en: 'After running this model, which takes some time, we can click on the View button
    to inspect the AUC for both the training and testing set:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此模型后，需要一些时间，我们可以单击“查看”按钮来检查训练集和测试集的AUC：
- en: '![](img/00056.jpeg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00056.jpeg)'
- en: Figure 22 - AUC curve for validation data.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图22 - 验证数据的AUC曲线。
- en: If you click your mouse and drag-and-drop on a section of the AUC curve, you
    can actually zoom in on that particular part of the curve and H2O gives summary
    statistics about the accuracy and precision at the various thresholds of the selected
    area.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您点击鼠标并在AUC曲线的某个部分上拖放，实际上可以放大该曲线的特定部分，并且H2O会提供有关所选区域的阈值的准确性和精度的摘要统计信息。
- en: '![](img/00057.jpeg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00057.jpeg)'
- en: Figure 23 - ROC curve can be easily explored to find optimal threshold.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图23 - ROC曲线可以轻松探索以找到最佳阈值。
- en: Also, there is a little button labeled Preview **Plain Old Java Object** (**POJO**),
    which we will explore in the latter chapters, which is how you will deploy your
    model into a production setting.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个标有预览**普通的Java对象**（**POJO**）的小按钮，我们将在后面的章节中探讨，这是您将模型部署到生产环境中的方式。
- en: OK so we've built a few dozen models; it's time now to begin inspecting our
    results and figuring which one gives us the best results given the overall error
    and AUC metric. Interestingly, when we host the many meetups at our office and
    talk with top kagglers, these types of tables showing results are frequently constructed
    and it is a good way to keep track of a) what works and what doesn't and b) look
    back at what you have tried as a form of documentation.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们已经建立了几十个模型；现在是时候开始检查我们的结果，并找出哪一个在整体错误和AUC指标下给我们最好的结果。有趣的是，当我们在办公室举办许多聚会并与顶级kagglers交谈时，这些显示结果的表格经常被构建，这是一种跟踪a）什么有效和什么无效的好方法，b）回顾您尝试过的东西作为一种文档形式。
- en: '| Model | Error | AUC |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 错误 | AUC |'
- en: '| Decision Tree | 0.332 | 0.665 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.332 | 0.665 |'
- en: '| Grid-Search: Random Forest | 0.294 | 0.704 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: 网格搜索：随机森林 | 0.294 | 0.704
- en: '| **Grid-Search: GBM** | **0.287** | **0.712** |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| **网格搜索：GBM** | **0.287** | **0.712** |'
- en: '| Deep Learning - all features | 0.376 | 0.705 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 - 所有特征 | 0.376 | 0.705 |'
- en: '| Deep Learning - subset feat. | 0.301 | 0.716 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 - 子集特征 | 0.301 | 0.716 |'
- en: So, which one do we go with? In this case, we like the the GBM model since it
    provides the second highest AUC value with the lowest accuracy. But always this
    decision is driven by the modeling goal - in this example, we were strictly motivated
    by accuracy of the model in finding Higgs-Bosons; however, in other cases, selection
    of the right model or models can be influenced by various aspects - for example,
    the time to find and build the best model.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们选择哪一个？在这种情况下，我们喜欢GBM模型，因为它提供了第二高的AUC值和最低的准确率。但是这个决定总是由建模目标驱动 - 在这个例子中，我们严格受到模型在发现希格斯玻色子方面的准确性的影响；然而，在其他情况下，选择正确的模型或模型可能会受到各种方面的影响
    - 例如，找到并构建最佳模型的时间。
- en: Summary
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter was all about the binary classification problem: true or false
    and, for our example, the signal indicative of the Higgs-Boson or background noise?
    We have explored four different algorithms: **single decision tree**, **random
    forest**, **gradient boosted machine**, and DNN. For this exact problem, DNNs
    are the current world-beaters as the models can continue to train for longer (that
    is, increase the number of `epochs`) and more layers can be added ([http://papers.nips.cc/paper/5351-searching-for-higgs-boson-decay-modes-with-deep-learning.pdf](http://papers.nips.cc/paper/5351-searching-for-higgs-boson-decay-modes-with-deep-learning.pdf))'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要讨论了二元分类问题：真或假，对于我们的示例来说，信号是否表明希格斯玻色子或背景噪音？我们已经探索了四种不同的算法：单决策树、随机森林、梯度提升机和DNN。对于这个确切的问题，DNN是当前的世界冠军，因为这些模型可以继续训练更长时间（即增加“epochs”的数量），并且可以添加更多的层。
- en: In addition to exploring four algorithms and how to perform a grid-search against
    many hyper-parameters, we also looked at some important model metrics to help
    you better differentiate between models and understand ways to define how *good* is
    good. Our goal for this chapter was to expose you to a variety of different algorithms
    and tweaks within Spark and H2O to solve binary classification problems. In the
    next chapter, we will explore multi-class classification and how to create ensembles
    of models (sometimes called super-learners) to arrive at a good solution for our
    real-world example.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 除了探索四种算法以及如何对许多超参数执行网格搜索之外，我们还研究了一些重要的模型指标，以帮助您更好地区分模型并了解如何定义“好”的方式。我们本章的目标是让您接触到不同算法和Spark和H2O中的调整，以解决二元分类问题。在下一章中，我们将探讨多类分类以及如何创建模型集成（有时称为超学习者）来找到我们真实示例的良好解决方案。
