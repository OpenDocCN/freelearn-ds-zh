- en: Chapter 6.  Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章 机器学习
- en: We are the consumers of machine learning every day, whether we notice or not.
    E-mail providers such as Google automatically push some incoming mails into the `Spam`
    folder and online shopping sites such as Amazon or social networking sites such
    as Facebook jump in with unsolicited recommendations that are surprisingly useful.
    So, what enables these software products to reconnect long lost friends? These
    are just a few examples of machine learning in action.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们每天都在使用机器学习，无论我们是否注意到。像Google这样的电子邮件服务提供商会自动将一些来信推送到`垃圾邮件`文件夹，像Amazon这样的在线购物网站或Facebook这样的社交网络网站会推荐一些意外有用的商品或信息。那么，是什么让这些软件产品能够重新连接久未联系的朋友呢？这些只是机器学习应用的几个例子。
- en: Formally, machine learning is a part of **Artificial Intelligence** (**AI**)
    which deals with a class of algorithms that can learn from data and make predictions.
    The techniques and underlying concepts are drawn from the field of statistics.
    Machine learning exists at the intersection of computer science and statistics
    and is considered one of the most important components of data science. It has
    been around for quite some time now, but its complexity has only increased with
    increase in data and scalability requirements. Machine learning algorithms tend
    to be resource intensive and iterative in nature, which render them a poor fit
    for MapReduce paradigm. MapReduce works very well for single pass algorithms but
    does not cater so well for multi-pass counterparts. The Spark research program
    was started precisely to address this challenge. Apache Spark is equipped with
    efficient algorithms in its MLlib library that are designed to perform well even
    in iterative computational requirements.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上来说，机器学习是**人工智能**（**AI**）的一部分，它涉及一类能够从数据中学习并进行预测的算法。其技术和基本概念来自统计学领域。机器学习位于计算机科学和统计学的交叉点，被认为是数据科学最重要的组成部分之一。虽然它已经存在一段时间，但随着数据量和可扩展性要求的增加，其复杂性也不断提升。机器学习算法往往是资源密集型和迭代的，这使得它们不太适合MapReduce范式。MapReduce非常适合单次运行的算法，但对于多次运行的算法并不太适用。正是为了应对这一挑战，Spark研究项目应运而生。Apache
    Spark在其MLlib库中配备了高效的算法，旨在即使在迭代计算要求下也能表现良好。
- en: The previous chapter outlined the data analytics' life cycle and its various
    components such as data cleaning, data transformation, sampling techniques, and
    graphical techniques to visualize the data, along with concepts covering descriptive
    statistics and inferential statistics. We also looked at some of the statistical
    testing that could be performed on the Spark platform. Further to the basics we
    built up in the previous chapter, we are going to cover in this chapter most of
    the machine learning algorithms and how to use them to build models on Spark.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章概述了数据分析的生命周期及其各种组成部分，如数据清洗、数据转换、采样技术和用于可视化数据的图形技术，以及描述性统计和推断统计的概念。我们还看了一些可以在Spark平台上执行的统计测试。在上一章所建立的基础上，本章将介绍大多数机器学习算法及如何在Spark上使用它们构建模型。
- en: 'As a prerequisite for this chapter, basic understanding of machine learning
    algorithms and computer science fundamentals are nice to have. However, we have
    covered some theoretical basics of the algorithms with right set of practical
    examples to make those more comprehendible and easy to implement. The topics covered
    in this chapter are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的前提是对机器学习算法和计算机科学基础知识有一定的了解。尽管如此，我们已经通过一些理论基础和适当的实际案例讲解了这些算法，使其更加易于理解和实现。本章涵盖的主题包括：
- en: Introduction to machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: The evolution
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演化
- en: Supervised learning
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有监督学习
- en: Unsupervised learning
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: MLlib and the Pipeline API
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLlib和管道API
- en: MLlib
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLlib
- en: ML pipeline
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习管道
- en: Introduction to machine learning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: Parametric methods
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数化方法
- en: Non-parametric methods
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非参数化方法
- en: Regression methods
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归方法
- en: Linear regression
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Regularization on regression
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归的正则化
- en: Classification methods
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类方法
- en: Logistic regression
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Linear Support Vector Machines (SVMs)
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性支持向量机（SVM）
- en: Decision trees
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Impurity measures
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不纯度度量
- en: Stopping rule
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停止规则
- en: Split canditate
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割候选
- en: Advantages of decision tress
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的优点
- en: Example
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例
- en: Ensembles
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法
- en: Random forests
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Gradient boosted trees
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: Multilayer perceptron classifier
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层感知机分类器
- en: Clustering techniques
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类技术
- en: K-means clustering
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值聚类
- en: Summary
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: Introduction
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Machine learning is all about learning by example data; examples that produce
    a particular output for a given input. There are various business use cases for
    machine learning. Let us look at a few examples to get an idea of what exactly
    it is:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习完全是通过示例数据来学习；这些示例为特定输入产生特定输出。机器学习在商业中有多种应用案例。让我们通过几个例子来了解它究竟是什么：
- en: A recommendation engine that recommends users what they might be interested
    in buying
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个推荐引擎，向用户推荐他们可能感兴趣的购买项目
- en: Customer segmentation (grouping customers who share similar characteristics)
    for marketing campaigns
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户细分（将具有相似特征的客户分组）用于市场营销活动
- en: Disease classification for cancer - malignant/benign
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 癌症的疾病分类——恶性/良性
- en: Predictive modeling, for example, sales forecasting, weather forecasting
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测建模，例如，销售预测，天气预测
- en: Drawing business inferences, for example, understanding what effect will change
    the price of a product have on sales
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制商业推断，例如，理解改变产品价格对销售的影响
- en: The evolution
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演变
- en: The concept of statistical learning was existent even before the first computer
    system was introduced. In the nineteenth century, the least squares technique
    (now called linear regression) had already been developed. For classification
    problems, Fisher came up with **Linear Discriminant Analysis** (**LDA**). Around
    the 1940s, an alternative to LDA, known as **logistic regression**, was proposed
    and all these approaches not only improved with time, but also inspired the development
    of other new algorithms.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学习的概念在第一个计算机系统被引入之前就已存在。在十九世纪，最小二乘法（现在称为线性回归）已经被开发出来。对于分类问题，费舍尔提出了**线性判别分析**（**LDA**）。大约在1940年代，一种LDA的替代方法——**逻辑回归**被提出，所有这些方法不仅随着时间的推移得到了改进，还激发了其他新算法的发展。
- en: During those times, computation was a big problem as it was done using pen and
    paper. So fitting non-linear equations was not quite feasible as it required a
    lot of computations. After the 1980s, with improvements in technology and the
    introduction of computer systems, classification/regression trees were introduced.
    Slowly, with further advancements in technology and computing systems, statistical
    learning in a way converged with what is now known as machine learning.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在那个时期，计算是一个大问题，因为它是通过笔和纸来完成的。因此，拟合非线性方程并不十分可行，因为它需要大量的计算。1980年代以后，随着技术的进步和计算机系统的引入，分类/回归树被提出。随着技术和计算系统的进一步发展，统计学习在某种程度上与现在所称的机器学习融合在一起。
- en: Supervised learning
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'As discussed in the previous section, machine learning is all about learning
    by example data. Based on how the algorithms understand data and get trained on
    it, they are broadly divided into two categories: **supervised learning** and
    **unsupervised learning**.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上一节所讨论的，机器学习完全是通过示例数据来学习。根据算法如何理解数据并在其上进行训练，机器学习大致可以分为两类：**监督学习**和**无监督学习**。
- en: 'Supervised statistical learning involves building a model based on one or more
    inputs for a particular output. This means that the output that we get can supervise
    our analysis based on the inputs we supply. In other words, for each observation of
    the predictor variables (for example, age, education, and expense variables),
    there is an associated response measurement of the outcome variable (for example,
    salary). Refer to the following table to get an idea of the example dataset where
    we are trying to predict the **Salary** based on the **Age**, **Education,** and
    **Expense** variables:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 监督统计学习涉及构建基于一个或多个输入的模型，以产生特定的输出。这意味着我们得到的输出可以根据我们提供的输入来监督我们的分析。换句话说，对于每个预测变量的观察（例如，年龄、教育和支出变量），都有一个与之相关的响应变量的测量（例如，薪水）。请参考下表了解我们尝试基于**年龄**、**教育**和**支出**变量预测**薪水**的示例数据集：
- en: '![Supervised learning](img/image_06_001.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习](img/image_06_001.jpg)'
- en: Supervised algorithms can be used for predicting, estimating, classifying, and
    other similar requirements which we will cover in the following sections.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 监督算法可以用于预测、估算、分类以及其他类似的需求，我们将在接下来的部分中进行介绍。
- en: Unsupervised learning
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Unsupervised statistical learning involves building a model based on one or
    more inputs but with no intention to produce a specific output. This means that
    there is no response/output variable to predict explicitly; but the output is
    usually the groups of data points that share some similar characteristics. Unlike
    supervised learning, you are not aware of the groups/labels to classify the data
    points into, per say, and you leave it to the algorithm to decide by itself.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督统计学习是通过一个或多个输入构建模型，但没有预期产生特定的输出。这意味着没有明确需要预测的响应/输出变量；但输出通常是具有一些相似特征的数据点分组。与监督学习不同，你并不知道将数据点分类到哪些组/标签，而是将这一决策交给算法自己去决定。
- en: Here, there is no concept of a `training` dataset that is used to `relate` the
    outcome variable with the `predictor` variables by building a model and then validate
    the model using the `test` dataset. The output of unsupervised algorithm cannot
    supervise your analysis based on the inputs you supply. Such algorithms can learn
    relationships and structure from data. *Clustering* and *Association rule learning*
    are examples of unsupervised learning techniques.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，并没有一个 `训练` 数据集来通过构建模型将结果变量与 `预测` 变量关联起来，并随后使用 `测试` 数据集验证模型。无监督算法的输出无法根据你提供的输入来监督你的分析。这类算法可以从数据中学习关系和结构。*聚类*
    和 *关联规则学习* 是无监督学习技术的例子。
- en: 'The following image depicts how clustering is used to group the data items
    that share some similar characteristics:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片展示了如何使用聚类将具有相似特征的数据项分组：
- en: '![Unsupervised learning](img/image_06_002.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习](img/image_06_002.jpg)'
- en: MLlib and the Pipeline API
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLlib 和 Pipeline API
- en: Let us first learn some Spark fundamentals to be able to perform the machine
    learning operations on it. We will discuss the MLlib and the pipeline API in this
    section.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解一些 Spark 的基础知识，以便能够在其上执行机器学习操作。本节将讨论 MLlib 和 Pipeline API。
- en: MLlib
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLlib
- en: MLlib is the machine learning library built on top of Apache Spark which homes
    most of the algorithms that can be implemented at scale. The seamless integration
    of MLlib with other components such as GraphX, SQL, and Streaming provides developers
    with an opportunity to assemble complex, scalable, and efficient workflows relatively
    easily. The MLlib library consists of common learning algorithms and utilities
    including classification, regression, clustering, collaborative filtering, and
    dimensionality reduction.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 是建立在 Apache Spark 之上的机器学习库，包含了大部分可以大规模实现的算法。MLlib 与 GraphX、SQL 和 Streaming
    等其他组件的无缝集成为开发者提供了相对容易组装复杂、可扩展和高效工作流的机会。MLlib 库包含常用的学习算法和工具，包括分类、回归、聚类、协同过滤和降维等。
- en: MLlib works in conjunction with the `spark.ml` package which provides a high
    level Pipeline API. The fundamental difference between these two packages is that
    MLlib (`spark.mllib`) works on top of RDDs whereas the ML (`spark.ml`) package
    works on top of DataFrames and supports ML Pipeline. Currently, both packages
    are supported by Spark but it is recommended to use the `spark.ml` package.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 与 `spark.ml` 包协同工作，后者提供了一个高级的 Pipeline API。这两个包之间的根本区别在于，MLlib（`spark.mllib`）在
    RDD 之上工作，而 ML（`spark.ml`）包在 DataFrame 之上工作，并支持 ML Pipeline。目前，Spark 支持这两个包，但建议使用
    `spark.ml` 包。
- en: 'Fundamental data types in this library are vectors and matrices. Vectors are
    local, and may be dense or sparse. Dense vectors are stored as an array of values.
    Sparse vectors are stored as two arrays; the first array stores the non-zero value
    indices and the second array stores the actual values. All element values are
    stored as doubles and indices are stored as integers starting from zero. Understanding
    the fundamental structures goes a long way in effective use of the libraries and
    it should help code up any new algorithm from scratch. Let us see some example
    code for a better understanding of these two vector representations:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该库中的基本数据类型是向量和矩阵。向量是局部的，可以是密集的或稀疏的。密集向量以值数组的形式存储。稀疏向量则存储为两个数组；第一个数组存储非零值的索引，第二个数组存储实际的值。所有元素值都以双精度浮点数形式存储，索引以从零开始的整数形式存储。理解这些基本结构有助于高效使用库，并帮助从零开始编写任何新的算法。让我们看一些示例代码，帮助更好地理解这两种向量表示方式：
- en: '**Scala**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Python:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'Python:'
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Matrices may be local or distributed, dense or sparse. A local matrix is stored
    on a single machine as a single dimensional array. A dense local matrix is stored
    in column major order (column members are contiguous) whereas a sparse matrix
    values are stored in **Compressed Sparse Column** (**CSC**) format in column major
    order. In this format, the matrix is stored in the form of three arrays. The first
    array contains row indices of non-zero values, the second array has the beginning
    value index for each column, and the third one is an array of all the non-zero
    values. Indices are of type integer starting from zero. The first array contains
    values from zero to the number of rows minus one. The third array has elements
    of type double. The second array requires some explanation. Every entry in this
    array corresponds to the index of the first non-zero element in each column. For
    example, assume that there is only one non-zero element in each column in a 3
    by 3 matrix. Then the second array would contain 0,1,2 as its elements. The first
    array contains row positions and the third array contains three values. If none
    of the elements in a column are non-zero, you will note the same index repeating
    in the second array. Let us examine some example code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵可以是局部的或分布式的，可以是稠密的或稀疏的。局部矩阵存储在单台机器上，作为一维数组。稠密的局部矩阵按照列主序存储（列成员是连续的），而稀疏矩阵的值则以**压缩稀疏列（**CSC**）**格式以列主序存储。在这种格式中，矩阵以三个数组的形式存储。第一个数组包含非零值的行索引，第二个数组包含每列第一个非零值的起始位置索引，第三个数组包含所有非零值。索引的类型是整数，从零开始。第一个数组包含从零到行数减一的值。第三个数组的元素类型为双精度。第二个数组需要一些解释。该数组中的每一项对应每一列第一个非零元素的索引。例如，假设在一个3×3的矩阵中，每列只有一个非零元素。那么第二个数组将包含0、1、2作为其元素。第一个数组包含行位置，第三个数组包含三个值。如果某一列中没有非零元素，你会注意到第二个数组中的相同索引会重复。让我们看一些示例代码：
- en: '**Scala:**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Python:**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Distributed matrices are the most sophisticated ones and choosing the right
    type of distributed matrix is very important. A distributed matrix is backed by
    one or more RDDs. The row and column indices are of the type `long` to support
    very large matrices. The basic type of distributed matrix is a `RowMatrix`, which
    is simply backed by an RDD of its rows.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式矩阵是最复杂的矩阵，选择合适的分布式矩阵类型非常重要。分布式矩阵由一个或多个RDDs支持。行和列的索引是`long`类型，以支持非常大的矩阵。分布式矩阵的基本类型是`RowMatrix`，它仅由其行的RDD支持。
- en: 'Each row in turn is a local vector. This is suitable when the number of columns
    is very low. Remember, we need to pass RDDs to create distributed matrices, unlike
    the local ones. Let us look at an example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行依次是一个局部向量。当列数非常低时，这种方式很适用。记住，我们需要传递RDDs来创建分布式矩阵，而不像局部矩阵那样。让我们来看一个例子：
- en: '**Scala:**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Python:**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: An `IndexedRowMatrix` stores a row index prefixed to the row entry. This is
    useful in executing joins. You need to pass `IndexedRow` objects to create an
    `IndexedRowMatrix`. An `IndexedRow` object is a wrapper with a long `Index` and
    a `Vector` of row elements.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`IndexedRowMatrix`将行索引前缀添加到行条目。这在执行连接操作时很有用。你需要传递`IndexedRow`对象来创建一个`IndexedRowMatrix`。`IndexedRow`对象是一个包含`long`类型`Index`和行元素的`Vector`的封装器。'
- en: A `CoordinatedMatrix` stores data as tuples of row, column indexes, and element
    value. A `BlockMatrix` represents a distributed matrix in blocks of local matrices.
    Methods to convert matrices from one type to another are provided but these are
    expensive operations and should be used with caution.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`CoordinatedMatrix`将数据存储为行列索引和元素值的元组。`BlockMatrix`将分布式矩阵表示为局部矩阵的块。提供了将矩阵从一种类型转换为另一种类型的方法，但这些操作非常昂贵，使用时应谨慎。'
- en: ML pipeline
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML 流水线
- en: A real life machine learning workflow is an iterative cycle of data extraction,
    data cleansing, pre-processing, exploration, feature extraction, model fitting,
    and evaluation. ML Pipeline on Spark is a simple API for users to set up complex
    ML workflows. It was designed to address some of the pain areas such as parameter
    tuning, or training many models based on different splits of data (cross-validation),
    or different sets of parameters. Writing scripts to automate this whole thing
    is no more a requirement and can be taken care of within the Pipeline API itself.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现实中的机器学习工作流程是一个迭代循环，包含数据提取、数据清洗、预处理、探索、特征提取、模型拟合和评估。Spark 上的 ML 流水线是一个简单的 API，供用户设置复杂的机器学习工作流程。它的设计旨在解决一些常见问题，如参数调整、基于不同数据划分（交叉验证）或不同参数集训练多个模型等。编写脚本来自动化整个过程不再是必需的，所有这些都可以在
    Pipeline API 中处理。
- en: The Pipeline API consists of a series of pipeline stages (implemented as abstractions
    such as *transformers* and *estimators*) to get executed in a desired order.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Pipeline API 由一系列流水线阶段组成（作为 *转换器* 和 *估算器* 等抽象的实现），这些阶段将按预定顺序执行。
- en: In the ML Pipeline, you can invoke the data cleaning/transformation functions
    as discussed in the previous chapter and call the machine learning algorithms
    that are available in the MLlib. This can be done in an iterative fashion till
    you get the desired performance of your model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ML 流水线中，您可以调用上一章讨论过的数据清洗/转换函数，并调用 MLlib 中可用的机器学习算法。这可以通过迭代的方式进行，直到您获得模型的理想性能。
- en: '![ML pipeline](img/image_06_003.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![ML pipeline](img/image_06_003.jpg)'
- en: Transformer
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换器
- en: A transformer is an abstraction which implements the `transform()` method to
    convert one DataFrame into another. If the method is a feature transformer, the
    resulting DataFrame might contain some additional transformed columns based on
    the operation you performed. However, if the method is a learning model, then
    the resulting DataFrame would contain an extra column with predicted outcomes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器是一个抽象，实现了 `transform()` 方法，将一个 DataFrame 转换成另一个 DataFrame。如果该方法是一个特征转换器，结果
    DataFrame 可能包含一些基于您执行的操作的额外转换列。然而，如果该方法是一个学习模型，那么结果 DataFrame 将包含一个包含预测结果的额外列。
- en: Estimator
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 估算器
- en: An Estimator is an abstraction that can be any learning algorithm which implements
    the `fit()` method to get trained on a DataFrame to produce a model. Technically,
    this model is a transformer for the given DataFrame.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 估算器是一个抽象，它可以是任何实现了 `fit()` 方法的学习算法，用来在 DataFrame 上训练以生成模型。从技术上讲，这个模型是给定 DataFrame
    的转换器。
- en: 'Example: Logistic regression is a learning algorithm, hence an estimator. Calling
    `fit()` trains a logistic regression model, which is a resultant model, and hence
    a transformer which can produce a DataFrame containing a predicted column.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：逻辑回归是一种学习算法，因此是一个估算器。调用 `fit()` 会训练一个逻辑回归模型，生成的模型是一个转换器，可以生成一个包含预测列的 DataFrame。
- en: The following example demonstrates a simple, single stage pipeline.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了一个简单的单阶段流水线。
- en: '**Scala:**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Python:**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: "The above example showed pipeline creation and execution although with a single\
    \ stage, a Tokenizer in this context. Spark provides several \"\x80\x9Cfeature\
    \ transformers\x80\" out of the box. These feature transformers are quite handy\
    \ during data cleaning and data preparation phases."
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例展示了流水线的创建和执行，尽管这里只有一个阶段，在此上下文中是一个 Tokenizer。Spark 提供了若干个“特征转换器”，这些特征转换器在数据清洗和数据准备阶段非常有用。
- en: The following example shows a real world example of converting raw text into
     feature vectors. If you are not familiar with TF-IDF, read this short tutorial
    from [http://www.tfidf.com](http://www.tfidf.com).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了将原始文本转换为特征向量的实际案例。如果您不熟悉 TF-IDF，可以阅读这个来自 [http://www.tfidf.com](http://www.tfidf.com)
    的简短教程。
- en: '**Scala:**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Python:**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This example has created and executed a multi-stage pipeline that has converted
    text to a feature vector that can be processed by machine learning algorithms.
    Let us see a few more features before we move on.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例创建并执行了一个多阶段流水线，将文本转换为可以被机器学习算法处理的特征向量。在继续之前，我们再看几个其他特性。
- en: '**Scala:**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Python:**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Introduction to machine learning
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: 'In the previous sections of the book, we learnt how the response/outcome variable
    is related to the predictor variables, typically in a supervised learning context.
    There are various different names for both of those types of variables that people
    use these days. Let us see some of the synonymous terms for them and we will use
    them interchangeably in the book:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几节中，我们学习了响应/结果变量如何与预测变量相关，通常是在监督学习的背景下。如今，人们通常用不同的名称来表示这两类变量。让我们看看它们的一些同义词，并在本书中交替使用：
- en: '**Input variables (X)**: Features, predictors, explanatory variables, independent
    variables'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入变量 (X)**: 特征，预测变量，解释变量，自变量'
- en: '**Output variables (Y)**: Response variable, dependent variable'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出变量 (Y)**: 响应变量，因变量'
- en: 'If there is a relation between *Y* and *X* where *X=X[1], X[2], X[3],..., X[n]*
    (n different predictors) then it can be written as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在一个 *Y* 与 *X* 之间的关系，其中 *X=X[1], X[2], X[3],..., X[n]* （n个不同的预测变量），那么可以写成如下形式：
- en: '![Introduction to machine learning](img/image_06_004.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_004.jpg)'
- en: Here ![Introduction to machine learning](img/image_06_005.jpg)is a function
    that represents how *X* describes *Y* and is unknown! This is what we figure out
    using the observed data points at hand. The term
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![机器学习简介](img/image_06_005.jpg) 是一个函数，表示 *X* 如何描述 *Y*，并且是未知的！这就是我们利用手头的观测数据点来找出的内容。这个术语
- en: '![Introduction to machine learning](img/image_06_006.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_006.jpg)'
- en: is a random error term with mean zero and is independent of *X*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个均值为零并且与 *X* 独立的随机误差项。
- en: There are basically two types of errors associated with such an equation - reducible
    errors and irreducible errors. As the name suggests, a reducible error is associated
    with the function and can be minimized by improving the accuracy of
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这样的方程式会涉及两种类型的误差——可约误差和不可约误差。顾名思义，可约误差与函数相关，并且可以通过提高精度来最小化。
- en: '![Introduction to machine learning](img/image_06_007.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_007.jpg)'
- en: by using a better learning algorithm or by tuning the same algorithm. Since
    *Y* is also a function of
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用更好的学习算法或调整相同的算法来提高。
- en: '![Introduction to machine learning](img/image_06_008.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_008.jpg)'
- en: ', which is independent of *X*, there would still be some error associated that
    cannot be addressed. This is called an irreducible error ('
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ', 如果与 *X* 无关，仍然会存在一些无法解决的误差。这被称为不可约误差 ('
- en: '![Introduction to machine learning](img/image_06_009.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_009.jpg)'
- en: ). There are always some factors which influence the outcome variable but are
    not considered in building the model (as they are unknown most of the time), and
    contribute to the irreducible error term. So, our approaches discussed throughout
    this book will only be focused on minimizing the reducible error.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: )。总是存在一些影响结果变量的因素，但在建立模型时未考虑这些因素（因为它们大多数时候是未知的），并且这些因素会贡献到不可约误差项。因此，本书中讨论的方法将专注于最小化可约误差。
- en: Most of the machine learning models that we build can be used for either prediction
    or for inference, or a combination of both. For some of the algorithms, the function
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建的大多数机器学习模型可以用于预测、推断或两者的组合。对于某些算法，函数
- en: '![Introduction to machine learning](img/image_06_010.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习简介](img/image_06_010.jpg)'
- en: can be represented as an equation which tells us how the dependent variable
    *Y* is related to the independent variables (*X1*, *X2*,..., *Xn*). In such cases,
    we can do both inference and prediction. However, some of the algorithms are black
    box, where we can only predict and no inference is possible, because how *Y* is
    related to *X* is unknown.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 可以表示为一个方程，告诉我们因变量 *Y* 如何与自变量 (*X1*, *X2*,..., *Xn*) 相关联。在这种情况下，我们可以进行推断和预测。然而，某些算法是“黑盒”算法，我们只能进行预测，无法进行推断，因为
    *Y* 如何与 *X* 相关是未知的。
- en: Note that the linear machine learning models can be more apt for an inference
    setting because they are more interpretable to business users. However, on a prediction
    setting, there can be better algorithms providing more accurate predictions but
    they are less interpretable. When inference is the target, we should prefer the
    restrictive models such as linear regression for better interpretability, and
    when only prediction is the goal, we may choose to use highly flexible models
    such as **Support Vector Machines** (**SVM**) that are less interpretable and
    more accurate (this may not hold true in all cases, however). You need to be careful
    in choosing an algorithm based on the business requirement, by accounting for
    the trade-off between interpretability and accuracy. Let us dive deeper into understanding
    the fundamentals behind these concepts.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，线性机器学习模型可能更适用于推断场景，因为它们对于业务用户来说更具可解释性。然而，在预测场景中，可能会有更好的算法提供更准确的预测，但它们的可解释性较差。当推断是目标时，我们应优先选择具有更好可解释性的限制性模型，如线性回归；而当只有预测是目标时，我们可以选择使用高度灵活的模型，如**支持向量机**（**SVM**），这些模型可解释性较差但准确性更高（然而，这在所有情况下并不成立）。您需要根据业务需求仔细选择算法，权衡可解释性和准确性之间的利弊。让我们更深入地理解这些概念背后的基本原理。
- en: Basically, we need a set of data points (training data) to build a model to
    estimate
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们需要一组数据点（训练数据）来构建模型以估算
- en: '![Introduction to machine learning](img/image_06_011.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习介绍](img/image_06_011.jpg)'
- en: '*(X)* so that *Y =*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*(X)*，从而*Y =*'
- en: '![Introduction to machine learning](img/image_06_012.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习介绍](img/image_06_012.jpg)'
- en: '*(X)*. Broadly, such learning methods can be either parametric or non-parametric.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*(X)*。一般来说，这些学习方法可以是参数化的，也可以是非参数化的。'
- en: Parametric methods
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数方法
- en: Parametric methods follow a two-step process. In the first step, you assume
    the shape of
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 参数方法遵循一个两步过程。在第一步中，您假设*X*的形状
- en: '![Parametric methods](img/image_06_013.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参数方法](img/image_06_013.jpg)'
- en: '*()*. For example, *X* is linearly related to *Y*, so the function of *X,*
    which is'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*()*。例如，*X*与*Y*呈线性关系，因此*X*的函数是'
- en: '![Parametric methods](img/image_06_014.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参数方法](img/image_06_014.jpg)'
- en: '*(X),* can be represented with a linear equation as shown next:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*(X)，*可以用以下线性方程表示：'
- en: '![Parametric methods](img/Beta1.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参数方法](img/Beta1.jpg)'
- en: 'After the model is selected, the second step is to estimate the parameters
    *Î²0*, *Î²1*,..., *Î²n* by using the data points at hand to train the model, so
    that:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 选择模型后，第二步是使用手头的数据点来训练模型，估算参数*Î²0*、*Î²1*、...、*Î²n*，从而：
- en: '![Parametric methods](img/Beta-2.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参数方法](img/Beta-2.jpg)'
- en: The one disadvantage to this parametric approach is that our assumption of linearity
    for ![Parametric methods](img/image_06_016.jpg) *()* might not hold true in real
    life situations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这种参数化方法的一个缺点是我们假设的线性关系对于![参数方法](img/image_06_016.jpg)*()*可能在实际生活中并不成立。
- en: Non-parametric methods
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非参数方法
- en: We do not make any assumptions about the linear relation between *Y* and *X*
    as well as data distributions of variables, and hence the form of
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不对*Y*和*X*之间的线性关系以及变量的数据分布做任何假设，因此*X*的形式是
- en: '![Non-parametric methods](img/image_06_017.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![非参数方法](img/image_06_017.jpg)'
- en: '*()* in non-parametric. Since it does not assume any form of'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*()*在非参数方法中。由于它不假设任何形式的'
- en: '![Non-parametric methods](img/image_06_018.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![非参数方法](img/image_06_018.jpg)'
- en: '*()*, it can produce better results by fitting well with data points, which
    could be an advantage.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*()*，它可以通过与数据点良好拟合来产生更好的结果，这可能是一个优势。'
- en: So, the non-parametric methods require more data points compared to parametric
    methods to estimate
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，非参数方法需要比参数方法更多的数据点来估算
- en: '![Non-parametric methods](img/image_06_019.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![非参数方法](img/image_06_019.jpg)'
- en: '*()* accurately. Note however, it can lead to overfitting problems if not handled
    properly. We will discuss more on this as we move further.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*()*准确。不过请注意，如果没有得到妥善处理，它可能会导致过拟合问题。我们将在进一步讨论中详细探讨这个问题。'
- en: Regression methods
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归方法
- en: Regression methods are a type of supervised learning. If the response variable
    is quantitative/continuous (takes on numeric values such as age, salary, height,
    and so on), then the problem can be called a regression problem regardless of
    the explanatory variables' type. There are various kinds of modeling techniques
    to address the regression problems. In this section, our focus will be on linear
    regression techniques and some different variations of it.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 回归方法是一种监督学习方法。如果响应变量是定量/连续的（如年龄、薪水、身高等数值），那么这个问题可以被称为回归问题，而不管解释变量的类型。针对回归问题，有多种建模技术。本节的重点将是线性回归技术及其一些不同的变种。
- en: 'Regression methods can be used to predict any real valued outcomes. Following
    are a few examples:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 回归方法可以用来预测任何实际数值的结果。以下是一些例子：
- en: Predict the salary of an employee based on his educational level, location,
    type of job, and so on
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据员工的教育水平、位置、工作类型等预测薪资
- en: Predict stock prices
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测股价
- en: Predict buying potential of a customer
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测客户的购买潜力
- en: Predict the time a machine would take before failing
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测机器故障前的运行时间
- en: Linear regression
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归
- en: Further to what we discussed in the previous section *Parametric methods*, after
    the assumption of linearity is made for
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们前一节*参数方法*讨论的基础上，在线性假设成立后，
- en: '![Linear regression](img/image_06_020.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_020.jpg)'
- en: '*(X)*, we need the training data to fit a model that would describe the relation
    between explanatory variables (denoted as *X*) and the response variable (denoted
    as *Y*). When there is only one explanatory variable present, it is called simple
    linear regression and when there are multiple explanatory variables present, it
    is called multiple linear regression. The simple linear regression is all about
    fitting a straight line in a 2-D setting, and when there are say two predictor
    variables, it would fit a plane in a 3-D setting, and so on for higher dimensional
    settings when there are more than two variables.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*(X)*，我们需要训练数据来拟合一个模型，该模型描述解释变量（记作 *X*）和响应变量（记作 *Y*）之间的关系。当只有一个解释变量时，称为简单线性回归；当有多个解释变量时，称为多元线性回归。简单线性回归是将一条直线拟合到二维空间中，当有两个预测变量时，它将拟合一个三维空间中的平面，依此类推，在维度更高的设置中，当变量超过两个时，也是如此。'
- en: 'The usual form of a linear regression equation can be represented as:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归方程的常见形式可以表示为：
- en: Y' =
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Y' =
- en: '![Linear regression](img/image_06_021.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_021.jpg)'
- en: (X) +
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: (X) +
- en: '![Linear regression](img/image_06_022.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_022.jpg)'
- en: Here *Y'* represents the predicted outcome variable.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *Y'* 代表预测的结果变量。
- en: 'A linear regression equation with only one predictor variable can be given
    as:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个预测变量的线性回归方程可以表示为：
- en: '![Linear regression](img/Beta11.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/Beta11.jpg)'
- en: 'A linear regression equation with multiple predictor variables can be given
    as:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 具有多个预测变量的线性回归方程可以表示为：
- en: '![Linear regression](img/Beta22.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/Beta22.jpg)'
- en: Here ![Linear regression](img/image_06_025.jpg) is the irreducible error term
    independent of *X* and has a mean of zero. We do not have any control over it,
    but we can work towards optimizing
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![线性回归](img/image_06_025.jpg) 是与 *X* 无关的无法简化的误差项，且其均值为零。我们无法控制它，但可以朝着优化的方向努力。
- en: '![Linear regression](img/image_06_026.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_026.jpg)'
- en: '*(X)*. Since none of the models can achieve a 100 percent accuracy, there would
    always be some error associated with it because of the irreducible error component
    ('
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*(X)*。由于没有任何模型能够达到100%的准确率，因此总会有一些与之相关的误差，这些误差源自无法简化的误差成分（'
- en: '![Linear regression](img/image_06_027.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_027.jpg)'
- en: ).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ）.
- en: 'The most common approach of fitting a linear regression is called **least squares**,
    also known as, the **Ordinary Least Squares** (**OLS**) approach. This method
    finds the regression line that best fits the observed data points by minimizing
    the sum of squares of the vertical deviations from each data point to the regression
    line. To get a better understanding on how the linear regression works, let us
    look at a simple linear regression of the following form for now:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的线性回归拟合方法叫做**最小二乘法**，也称为**普通最小二乘法**（**OLS**）方法。该方法通过最小化每个数据点到回归线的垂直偏差的平方和，找到最适合观察数据点的回归线。为了更好地理解线性回归的工作原理，让我们现在看一个简单线性回归的例子：
- en: '![Linear regression](img/Beta33.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/Beta33.jpg)'
- en: 'Where, *Î²0* is the Y-intercept of the regression line and *Î²1* defines the
    slope of the line. What it means is that *Î²1* is the average change in *Y* for
    every one unit change in *X*. Let us take an example with *X* and *Y*:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*Î²0* 是回归线的 Y 截距，*Î²1* 定义了回归线的斜率。意思是，*Î²1* 是 *X* 变化一个单位时 *Y* 的平均变化。我们以 *X*
    和 *Y* 为例：
- en: '| **X** | **Y** |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| **X** | **Y** |'
- en: '| **1** | 12 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 12 |'
- en: '| **2** | 20 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 20 |'
- en: '| **3** | 13 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 13 |'
- en: '| **4** | 38 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 38 |'
- en: '| **5** | 27 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 27 |'
- en: 'If we fit a linear regression line through the data points as shown in the
    preceding table, then it would appear as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过数据点拟合一条线性回归线，如上表所示，那么它将呈现如下：
- en: '![Linear regression](img/image_06_028.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_028.jpg)'
- en: 'The red vertical lines in the preceding figure indicate the error of prediction
    which can be defined as the difference between the actual *Y* value and the predicted
    *Y''* value. If you square these differences and sum them up, it is called the
    **Sum of Squared Error** (**SSE**), which is the most common measure that is used
    to find the best fitting line. The following table shows how to calculate the
    SSE:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，红色的垂直线表示预测误差，可以定义为实际的 *Y* 值和预测的 *Y'* 值之间的差异。如果你将这些差异平方并求和，就得到了 **平方误差和**
    (**SSE**)，这是用来找到最优拟合线的最常见度量。下表显示了如何计算SSE：
- en: '| **X** | **Y** | **Y''** | **Y-Y''** | **(Y-Y'') 2** |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| **X** | **Y** | **Y''** | **Y-Y''** | **(Y-Y'') ²** |'
- en: '| **1** | 12 | 12.4 | 0.4 | 0.16 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 12 | 12.4 | 0.4 | 0.16 |'
- en: '| **2** | 20 | 17.2 | 2.8 | 7.84 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 20 | 17.2 | 2.8 | 7.84 |'
- en: '| **3** | 13 | 22 | -9 | 81 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 13 | 22 | -9 | 81 |'
- en: '| **4** | 38 | 26.8 | 11.2 | 125.44 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 38 | 26.8 | 11.2 | 125.44 |'
- en: '| **5** | 27 | 31.6 | -4.6 | 21.16 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 27 | 31.6 | -4.6 | 21.16 |'
- en: '|  |  |  | SUM | 235.6 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 总和 | 235.6 |'
- en: 'In the above table, the term **(Y-Y'')** is called the residual. The **Residual
    Sum of Squares** (**RSS**) can be represented as:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述表格中，**(Y-Y')** 被称为残差。**残差平方和** (**RSS**) 可以表示为：
- en: '*RSS = residual[1]² + residual[2]² + residual[3]² + ......+ residual[n]²*'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*RSS = residual[1]² + residual[2]² + residual[3]² + ......+ residual[n]²*'
- en: Note that regression is highly susceptible to outliers and can introduce huge
    RSS error if not handled prior to applying regression.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，回归对异常值非常敏感，如果不在应用回归之前处理异常值，可能会引入巨大的RSS误差。
- en: After a regression line is fit into the observed data points, you should examine
    the residuals by plotting them on the Y-Axis against explanatory the variable
    on the X-Axis. If the plot is nearly a straight line, then your assumption about
    linear relationship is valid, or else it may indicate the presence of some kind
    of non-linear relationship. In case of the presence of nonlinear relationships,
    you may have to account for the non-linearity. One of the techniques is by adding
    higher order polynomials to the equation.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归线拟合到观察数据点之后，你应该通过将残差绘制在 Y 轴上，对应于 X 轴上的解释变量来检查残差。如果图像接近直线，那么你关于线性关系的假设是有效的，否则可能表明存在某种非线性关系。如果存在非线性关系，你可能需要考虑非线性。一个技术是通过向方程中加入高阶多项式来解决。
- en: We saw that RSS was an important characteristic in fitting the regression line
    (while building the model). Now, to assess how good your regression fit is (once
    the model is built), you need two other statistics - **Residual Standard Error**
    (**RSE**) and **R²** statistic.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，RSS 是拟合回归线时的重要特征（在建立模型时）。现在，为了评估回归拟合的好坏（在模型建立后），你需要另外两个统计量——**残差标准误差**
    (**RSE**) 和 **R²** 统计量。
- en: 'We discussed the irreducible error component *Îµ*, because of which there would
    always be some level of error with your regression (even if your equation exactly
    fits your data points and you have estimated the coefficients properly). RSE is
    an estimate of standard deviation of *Îµ* which can be defined as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了不可减少的误差成分 *Îµ*，由于这个原因，即使你的方程完全拟合数据点并且正确估计了系数，也总会存在某种程度的误差。RSE 是 *Îµ* 标准差的估计，可以定义如下：
- en: '![Linear regression](img/image_06_029.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_029.jpg)'
- en: This means that the actual values would deviate from the true regression line
    by a factor of RSE on an average.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着实际值会平均偏离真实回归线一个RSE因子。
- en: Since RSE is actually measured in the units of *Y* (refer to how we calculated
    RSS in the previous section), it is difficult to say that it is the only best
    statistic for the model accuracy.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RSE实际上是以 *Y* 的单位来衡量的（参考我们在上一节如何计算RSS），所以很难说它是模型精度的唯一最佳统计量。
- en: 'So, an alternative approach was introduced, called the R² statistic (also known
    as the coefficient of determination). The formula to calculate R² is as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，引入了一种替代方法，称为R²统计量（也叫做决定系数）。计算R²的公式如下：
- en: '![Linear regression](img/image_06_030.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_030.jpg)'
- en: 'The **Total Sum of Squares** (**TSS**) can be calculated as:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**总平方和** (**TSS**) 可以通过以下方式计算：'
- en: '![Linear regression](img/image_06_031.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image_06_031.jpg)'
- en: Note here that TSS measures the total variance inherent in *Y* even before performing
    the regression to predict *Y*. Observe that there is no *Y'* in it. On the contrary,
    RSS represents the variability in *Y* that is unexplained after regression. This
    means that (*TSS - RSS*) is able to explain the variability in response after
    regression is performed.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的是，TSS衡量的是*Y*中固有的总变异性，即使在进行回归预测*Y*之前也包含在内。可以观察到其中没有*Y'*。相反，RSS表示回归后*Y*中未解释的变异性。这意味着(*TSS
    - RSS*)能够解释回归后响应变量中的变异性。
- en: The *R²* statistic usually ranges from 0 to 1, but can be negative if the fit
    is worse than fitting just a horizontal line, but that is rarely the case. A value
    close to 1 indicates that the regression equation could explain a large proportion
    of the variability in the response variable and is a good fit. On the contrary,
    a value close to 0 indicates that the regression did not explain much of the variance
    in the response variable and is not a good fit. As an example, an *R²* of 0.25
    means that 25 percent of the variance in *Y* is explained by *X* and is indicating
    to tune the model for improvement.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*R²*统计量通常介于0到1之间，但如果拟合结果比拟合一个水平线还要差，它可能为负值，但这种情况很少发生。接近1的值表示回归方程可以解释响应变量中大部分的变异性，并且拟合效果良好。相反，接近0的值表示回归方程几乎没有解释响应变量中的变异性，拟合效果不好。例如，*R²*
    为0.25意味着*Y*的25%变异性由*X*解释，表示需要调优模型以改善效果。'
- en: Let us now discuss how to address the non-linearity in the dataset through regression.
    As discussed earlier, when you find nonlinear relations, it needs to be handled
    properly. To model a non-linear equation using the same linear regression technique,
    you have to create the higher order features, which will be treated as just another
    variable by the regression technique. For example, if *salary* is a feature/variable
    that is predicting the *buying potential*, and we find that there is a non-linear
    relationship between them, then we might create a feature called (*salary3*) depending
    on how much of the non-linearity needs to be addressed. Note that while you create
    such higher order features, you also have to keep the base features. In this example,
    you have to use both (*salary*) and (*salary3*) in the regression equation.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论如何通过回归来处理数据集中的非线性。正如前面所讨论的，当你发现非线性关系时，需要适当处理。为了使用相同的线性回归方法建模非线性方程，必须创建高阶特征，回归方法会将其视为另一个变量。例如，如果*薪水*是一个预测*购买潜力*的特征/变量，而我们发现它们之间存在非线性关系，那么我们可能会创建一个特征叫做(*薪水³*)，具体取决于需要解决多少非线性问题。请注意，在创建这种高阶特征时，你还需要保留基础特征。在这个例子中，你必须在回归方程中同时使用(*薪水*)和(*薪水³*)。
- en: So far, we have kind of assumed that all the predictor variables are continuous.
    What if there are categorical predictors? In such cases, we have to dummy-code
    those variables (say 1 for male and 0 for female) so that the regression technique
    generates two equations, one for gender = male (the equation will have the gender
    variable) and the other for gender = female (the equation will not have the gender
    variable as it will be dropped as coded 0). At times, with very few categorical
    variables, it may be a good idea to divide the dataset based on the levels of
    categorical variables and build separate models for them.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设所有预测变量都是连续的。如果存在类别型预测变量怎么办？在这种情况下，我们必须将这些变量进行虚拟编码（例如，将男性编码为1，女性编码为0），这样回归方法就会生成两个方程，一个用于性别=男性（该方程包含性别变量），另一个用于性别=女性（该方程不包含性别变量，因为它会被作为0值丢弃）。有时，当类别变量较少时，可以考虑根据类别变量的水平将数据集划分，并为每个部分构建单独的模型。
- en: One major advantage of the least squares linear regression is that it explains
    how the outcome variable is related to the predictor variables. This makes it
    very interpretable and can be used to draw inferences as well as to do predictions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘线性回归的一个主要优点是它能解释结果变量如何与预测变量相关。这使得它非常易于解释，并且可以用来做推断和预测。
- en: Loss function
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: Many machine learning problems can be formulated as a convex optimization problem.
    The objective of this problem is to find the values of the coefficients for which
    the squared loss is minimum. This objective function has basically two components
    - regularizer and the loss function. The regularizer is there to control the complexity
    of the model (so it does not overfit) and the loss function is there to estimate
    the coefficients of the regression function for which squared loss (RSS) is minimum.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习问题可以被表述为凸优化问题。这个问题的目标是找到使平方损失最小的系数值。这个目标函数基本上包含两个部分——正则化项和损失函数。正则化项用于控制模型的复杂度（防止过拟合），而损失函数用于估计回归函数的系数，使得平方损失（RSS）最小。
- en: 'The loss function used for least squares is called **squared loss**, as shown
    next:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 用于最小二乘法的损失函数称为**平方损失**，如下所示：
- en: '![Loss function](img/image_06_032.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![损失函数](img/image_06_032.jpg)'
- en: Here *Y* is the response variable (real valued), *W* is the weight vector (value
    of the coefficients), and *X* is the feature vector. So
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*Y*是响应变量（实值），*W*是权重向量（系数值），*X*是特征向量。所以
- en: '![Loss function](img/Capture-1.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![损失函数](img/Capture-1.jpg)'
- en: gives the predicted values which we equate with the actual values *Y* to find
    the squared loss that needs to be minimized.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 给出预测值，我们将其与实际值*Y*进行比较，以求得需要最小化的平方损失。
- en: The algorithm used to estimate the coefficients is called **gradient descent**.
    There are different types of loss functions and optimization algorithms for different
    kinds of machine learning algorithms which we will cover as and when needed.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 用于估计系数的算法称为**梯度下降**。不同类型的损失函数和优化算法适用于不同种类的机器学习算法，我们将在需要时介绍。
- en: Optimization
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化
- en: Ultimately, the linear methods have to optimize the loss function. Under the
    hood, linear methods use convex optimization methods to optimize the objective
    functions. MLlib has **Stochastic Gradient Descent** (**SGD**) and **Limited Memory
    - Broyden-Fletcher-Goldfarb-Shanno** (**L-BFGS**) supported out of the box. Currently,
    most algorithm APIs support SGD and a few support L-BFGS.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，线性方法必须优化损失函数。在底层，线性方法使用凸优化方法来优化目标函数。MLlib 支持**随机梯度下降**（**SGD**）和**有限内存 -
    Broyden-Fletcher-Goldfarb-Shanno**（**L-BFGS**）算法。目前，大多数算法 API 支持SGD，少数支持L-BFGS。
- en: SGD is a first-order optimization technique that works best for large scale
    data and distributed computing environment. Optimization problems whose objective
    function (loss function) is written as a sum are best suited to be solved using
    SGD.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 是一种一阶优化技术，最适合大规模数据和分布式计算环境。目标函数（损失函数）可以写作求和形式的优化问题最适合使用SGD来解决。
- en: L-BFGS is an optimization algorithm in the family of quasi-Newton methods to
    solve the optimization problems. L-BFGS often achieves a rapider convergence compared
    with other first-order optimization techniques such as SGD.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: L-BFGS 是一种优化算法，属于拟牛顿法家族，用于解决优化问题。与其他一阶优化技术如SGD相比，L-BFGS通常能实现更快的收敛。
- en: Some of the linear methods available in MLlib support both SGD and L-BFGS. You
    should choose one over the other depending on the objective function under consideration.
    In general, L-BFGS is recommended over SGD as it converges faster but you need
    to evaluate carefully based on the requirement.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 中的一些线性方法同时支持SGD和L-BFGS。你应该根据考虑的目标函数选择其中之一。一般来说，L-BFGS 相对于 SGD 更推荐，因为它收敛更快，但你需要根据需求仔细评估。
- en: Regularizations on regression
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归的正则化
- en: With large weights (coefficient values), it is easier to overfit the model.
    Regularization is a technique used mainly to eliminate the overfitting problem
    by controlling the complexity of the model. This is usually done when you see
    a difference between the model performance on training data and test data. If
    the training performance is more than that of the test data, it could be a case
    of overfitting (high variance case).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在权重（系数值）较大的情况下，更容易导致模型过拟合。正则化是一种主要用于通过控制模型复杂度来消除过拟合问题的技术。通常当你发现训练数据和测试数据上的模型表现存在差异时，便可采用正则化。如果训练性能高于测试数据的性能，可能是过拟合的情况（高方差问题）。
- en: To address this, a regularization technique was introduced that would penalize
    the loss function. It is always recommended to use any of the regularizations
    techniques, especially when the training data has a small number of observations.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，提出了一种正则化技术，通过对损失函数进行惩罚来改进模型。特别是在训练数据样本较少时，建议使用任何一种正则化技术。
- en: Before we discuss further on the regularization techniques, we have to understand
    what *bias* and *variance* mean in a supervised learning setting and why there
    is always a trade-off associated. While both are related to errors, a *biased*
    model means that it is biased towards some erroneous assumption and may miss the
    relation between the predictor variables and the response variable to some extent.
    This is a case of underfitting! On the other hand, a *high variance* model means
    that it tries to touch every data point and ends up modelling the random noise
    present in the dataset. It represents the case of overfitting.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步讨论正则化技术之前，我们需要理解在有监督学习环境下，*偏差*和*方差*的含义，以及为什么它们总是存在某种权衡。虽然两者都与误差有关，*有偏*的模型意味着它倾向于某种错误的假设，并且可能在某种程度上忽视了预测变量与响应变量之间的关系。这是欠拟合的情况！另一方面，*高方差*模型意味着它试图拟合每一个数据点，最终却在建模数据集中的随机噪声。这就是过拟合的情况。
- en: Linear regression with the L2 penalty (L2 regularization) is called **ridge
    regression** and with the L1 penalty (L1 regularization) is called **lasso regression**.
    When both L1 and L2 penalties are used together, it is called **elastic net regression**.
    We will discuss them one by one in the following section.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 带L2惩罚的线性回归（L2正则化）称为**岭回归**，带L1惩罚的线性回归（L1正则化）称为**Lasso回归**。当同时使用L1和L2惩罚时，称为**弹性网回归**。我们将在接下来的部分中逐一讨论它们。
- en: L2 regularized problems are usually easy to solve compared to L1 regularized
    problems due to smoothness, but the L1 regularized problems can cause sparsity
    in weights leading to smaller and more interpretable models. Because of this,
    lasso is at times used for feature selection.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于L1正则化问题，L2正则化问题通常更容易解决，因为L2正则化具有平滑性，但L1正则化问题会导致权重的稀疏性，从而产生更小且更具可解释性的模型。因此，Lasso有时被用来进行特征选择。
- en: Ridge regression
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 岭回归
- en: 'When we add the L2 penalty (also known as the **shrinkage penalty**) to the
    loss function of least squares, it becomes the ridge regression, as shown next:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将L2惩罚（也称为**收缩惩罚**）添加到最小二乘法的损失函数中时，它变成了岭回归，如下所示：
- en: '![Ridge regression](img/image_06_034.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![岭回归](img/image_06_034.jpg)'
- en: Here *λ* (greater than 0) is a tuning parameter which is determined separately.
    The second term in the preceding equation is called the shrinkage penalty and
    can be small only if the coefficients (*Î²0*, *Î²1*...and so on) are small and
    close to 0\. When *λ = 0*, the ridge regression becomes least squares. As lambda
    approaches infinity, the regression coefficients approach zero (but are never
    zero).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*λ*（大于0）是一个调节参数，需要单独确定。前面公式中的第二项被称为收缩惩罚，只有当系数（*Î²0*、*Î²1*...等）较小并接近0时，收缩惩罚才能变得较小。当*λ
    = 0*时，岭回归就变成了最小二乘法。随着lambda趋向无穷大，回归系数趋近于零（但永远不会是零）。
- en: The ridge regression generates different sets of coefficient values for each
    value of *λ*. So, the lambda value needs to be carefully selected using cross-validation.
    As we increase the lambda value, the flexibility of the regression line decreases,
    thereby decreasing variance and increasing bias.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归为每个*λ*值生成不同的系数值集合。因此，需要通过交叉验证仔细选择lambda值。随着lambda值的增加，回归线的灵活性减少，从而降低方差并增加偏差。
- en: Note that the shrinkage penalty is applied to all the explanatory variables
    except the intercept term *Î²0*.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，收缩惩罚应用于所有解释变量，除截距项*Î²0*外。
- en: The ridge regression works really well when the training data is less or even
    in the case where the number of predictors or features are more than the number
    of observations. Also, the computation needed for ridge is almost the same as
    that of least squares.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练数据较少时，或者当预测变量或特征的数量大于观察值的数量时，岭回归表现得非常好。此外，岭回归所需的计算与最小二乘法几乎相同。
- en: Since ridge does not reduce any coefficient value to zero, all the variables
    will be present in the model which can make it less interpretable if the number
    of variables is high.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 由于岭回归不会将任何系数值缩减为零，所有变量都会出现在模型中，这可能使得模型在变量数目较多时变得不易解释。
- en: Lasso regression
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Lasso回归
- en: 'Lasso was introduced after ridge. When we add the L1 penalty to the loss function
    of least squares, it becomes lasso regression, as shown next:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso是在岭回归之后引入的。当我们将L1惩罚添加到最小二乘的损失函数中时，它变成了Lasso回归，如下所示：
- en: '![Lasso regression](img/image_06_035.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![Lasso回归](img/image_06_035.jpg)'
- en: The difference here is that instead of taking the squared coefficients, it takes
    the mod of the coefficient. Unlike ridge, it can force some of its coefficients
    to be exactly zero which can result in elimination of some of the variables. So,
    lasso can be used for variable selection as well!
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的区别在于，它不是取平方系数，而是取系数的模。与岭回归不同，Lasso回归可以将一些系数强制设为零，这可能导致一些变量被消除。因此，Lasso回归也可以用于变量选择！
- en: Lasso generates different sets of coefficient values for each value of lambda.
    So lambda value needs to be carefully selected using cross-validation. Like ridge,
    as you increase lambda, variance decreases and bias increases.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso为每个lambda值生成不同的系数值。因此，需要通过交叉验证仔细选择lambda值。像岭回归一样，随着lambda的增加，方差减小，偏差增大。
- en: Lasso produces better interpretable models compared to ridge because it usually
    has a subset of the total number of variables. When there are many categorical
    variables, it is advisable to choose lasso over ridge.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 与岭回归相比，Lasso产生的模型更容易解释，因为它通常只有部分变量。若有许多类别型变量，建议选择Lasso而不是岭回归。
- en: In reality, neither ridge nor lasso is always better over the other. Lasso usually
    performs well with a small number of predictor variables that have substantial
    coefficients and the rest have very small coefficients. Ridge usually performs
    better when there are many predictors and almost all have substantial yet similar
    coefficient sizes.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，岭回归和Lasso回归并不是总是互相优劣。Lasso通常在少量预测变量且它们的系数较大，而其余系数非常小的情况下表现较好。岭回归通常在有大量预测变量且几乎所有的变量系数都很大且相似时表现较好。
- en: Ridge is good for grouped selection and can also address multicollinearity problems.
    Lasso, on the other hand, cannot do grouped selection and tends to pick only one
    of the predictors. Also, if a group of predictors are highly correlated amongst
    themselves, Lasso tends to pick only one of them and shrink the others to zero.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归适用于分组选择，并且能解决多重共线性问题。而Lasso则无法进行分组选择，通常只会选择一个预测变量。如果一组预测变量之间高度相关，Lasso倾向于只选择其中一个，并将其他变量的系数缩小为零。
- en: Elastic net regression
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 弹性网回归
- en: 'When we add both L1 and L2 penalties to the loss function of least squares,
    it becomes elastic net regression, as shown next:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将L1和L2惩罚都添加到最小二乘损失函数中时，它就变成了弹性网回归，如下所示：
- en: '![Elastic net regression](img/image_06_036.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![弹性网回归](img/image_06_036.jpg)'
- en: 'Following are the advantages of elastic net regression:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是弹性网回归的优点：
- en: Enforces sparsity and helps remove least effective variables
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强制稀疏性并有助于移除最不有效的变量
- en: Encourages grouping effect
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鼓励分组效应
- en: Combines the strengths of both ridge and lasso
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合了岭回归和Lasso回归的优点
- en: 'The Naive version of elastic net regression incurs a double shrinkage problem
    which leads to increased bias and poorer prediction accuracy. To address this,
    one approach could be rescaling the estimated coefficients by multiplying (*1+
    λ2*) with them:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性网回归的朴素版本会产生双重收缩问题，这会导致偏差增大和预测精度下降。为了解决这个问题，一种方法是通过将（*1 + λ2*）与估计的系数相乘来重新缩放它们：
- en: '**Scala**'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE12]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Python**:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE13]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Classification methods
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类方法
- en: If the response variable is qualitative/categorical (takes on categorical values
    such as gender, loan default, marital status, and such), then the problem can
    be called a classification problem regardless of the explanatory variables' type.
    There are various types of classification methods, but we will focus on logistic
    regression and Support Vector Machines in this section.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果响应变量是定性/类别型的（例如性别、贷款违约、婚姻状况等），那么无论解释变量的类型如何，这个问题都可以称为分类问题。分类方法有很多种，但在本节中我们将重点讨论逻辑回归和支持向量机。
- en: 'Following are a few examples of some implications of classification methods:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些分类方法的应用实例：
- en: A customer buys a product or does not buy it
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个顾客购买某个产品或不购买
- en: A person is diabetic or not diabetic
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个人是否患有糖尿病
- en: An individual applying for a loan would default or not
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个申请贷款的个人会违约或不会违约
- en: An e-mail receiver would read the e-mail or not
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个电子邮件接收者会读邮件或不读
- en: Logistic regression
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression measures the relation between the explanatory variables
    and the categorical response variable. We do not use linear regression for the
    categorical response variable because the response variable is not on a continuous
    scale and hence the error terms are not normally distributed.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归衡量解释变量与分类响应变量之间的关系。我们不会对分类响应变量使用线性回归，因为响应变量不是连续的，因此误差项不是正态分布的。
- en: 'So logistic regression is a classification algorithm. Instead of modelling
    the response variable *Y* directly, logistic regression models the probability
    distribution of *P(Y*|*X)* that *Y* belongs to a particular category. The conditional
    distribution of (*Y*|*X*) is a Bernoulli distribution rather than a Gaussian distribution.
    The logistic regression equation can be represented as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 所以逻辑回归是一个分类算法。逻辑回归不是直接建模响应变量*Y*，而是建模*P(Y*|*X)*的概率分布，即*Y*属于某一特定类别的概率。条件分布（*Y*|*X*）是伯努利分布，而不是高斯分布。逻辑回归方程可以表示为：
- en: '![Logistic regression](img/image_06_037.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/image_06_037.jpg)'
- en: 'For a two class classification, the output of the model should be restricted
    to only one of the two classes (say either 0 or 1). Since logistic regression
    predicts probabilities and not classes directly, we use a logistic function (also
    known as the, *sigmoid function*) to restrict the output to a single class:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类问题，模型的输出应该仅限于两个类别之一（例如0或1）。由于逻辑回归预测的是概率而不是直接预测类别，我们使用一个逻辑函数（也称为*sigmoid函数*）来将输出限制为一个单一的类别：
- en: '![Logistic regression](img/image_06_038.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/image_06_038.jpg)'
- en: 'Solving for the preceding equation gives us the following:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 解前面的方程会得到以下结果：
- en: '![Logistic regression](img/Capture-2.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/Capture-2.jpg)'
- en: 'It can be further simplified as:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以进一步简化为：
- en: '![Logistic regression](img/image_06_040.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/image_06_040.jpg)'
- en: The quantity on the left *P(X)/1-P(X)* is called the *odds*. The value of odds
    ranges from 0 to infinity. The values close to 0 indicate very less probability
    and the ones bigger in numbers indicate high probability. At times odds are used
    directly instead of probabilities, depending on the situation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 左边的量 *P(X)/1-P(X)* 被称为*赔率*。赔率的值范围从0到无穷大。接近0的值表示概率很小，而数值较大的则表示概率很高。在某些情况下，赔率会直接代替概率使用，这取决于具体情况。
- en: 'If we take the log of the odds, it becomes log-odd or logit and can be shown
    as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们取赔率的对数，它就变成了对数赔率或logit，可以表示如下：
- en: '![Logistic regression](img/image_06_041.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/image_06_041.jpg)'
- en: You can see from the previous equation that logit is linearly related to *X*.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的方程中可以看出，logit 与 *X* 是线性相关的。
- en: In the situation where there are two classes, 1 and 0, then we predict *Y =
    1* if *p >= 0.5* and *Y = 0* when *p < 0.5*. So logistic regression is actually
    a linear classifier with decision boundary at *p = 0.5*. There could be business
    cases where *p* is just not set to 0.5 by default and you may have to figure out
    the right value using some mathematical techniques.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在有两个类别1和0的情况下，我们当*p >= 0.5*时预测*Y = 1*，当*p < 0.5*时预测*Y = 0*。因此，逻辑回归实际上是一个线性分类器，决策边界为*p
    = 0.5*。在某些商业场景中，*p*可能默认并不设置为0.5，您可能需要使用一些数学技巧来确定合适的值。
- en: A method known as maximum likelihood is used to fit the model by computing the
    regression coefficients, and the algorithm can be a gradient descent like in a
    linear regression setting.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一种称为最大似然法的方法被用来通过计算回归系数来拟合模型，该算法可以像线性回归一样使用梯度下降。
- en: 'In logistic regression, the loss function should address the misclassification
    rate. So, the loss function used for logistic regression is called *logistic loss*,
    as shown next:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，损失函数应关注错误分类率。因此，逻辑回归使用的损失函数称为*逻辑损失*，如下所示：
- en: '![Logistic regression](img/image_06_042.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/image_06_042.jpg)'
- en: Note
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that logistic regression is also prone to overfitting when you use higher
    order polynomial to better fit a model. To solve this, you can use regularization
    terms like you did in linear regression. As of this writing, Spark does not support
    regularized logistic regression so we will skip this part for now.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当您使用更高阶的多项式来更好地拟合模型时，逻辑回归也容易过拟合。为了解决这个问题，您可以像在线性回归中一样使用正则化项。到目前为止，Spark
    不支持正则化逻辑回归，因此我们暂时跳过这一部分。
- en: Linear Support Vector Machines (SVM)
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性支持向量机（SVM）
- en: '**Support Vector Machines** (**SVM**) is a type of supervised machine learning
    algorithm and can be used for both classification and regression. However, it
    is more popular in addressing the classification problems, and since Spark offers
    it as an SVM classifier, we will limit our discussion to the classification setting
    only. When used as a classifier, unlike logistic regression, it is a non-probabilistic
    classifier.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）是一种监督学习算法，可以用于分类和回归。然而，它在解决分类问题时更为流行，且由于Spark将其作为SVM分类器提供，我们将仅限于讨论分类设置。当作为分类器使用时，与逻辑回归不同，它是一个非概率性分类器。'
- en: The SVM has evolved from a simple classifier called the **maximal margin classifier**.
    Since the maximal margin classifier required that the classes be separable by
    a linear boundary, it could not be applied to many datasets. So it was extended
    to an improved version called the **support vector classifier** that could address
    the cases where the classes overlapped and there were no clear separation between
    the classes. The support vector classifier was further extended to what we call
    an SVM to accommodate the non-linear class boundaries. Let us discuss the evolution
    of the SVM step by step so we get a clear understanding of how it works.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: SVM（支持向量机）起源于一种简单的分类器，称为**最大间隔分类器**。由于最大间隔分类器要求类别通过线性边界可分，因此它无法应用于许多数据集。因此，它被扩展为一种改进版本，称为**支持向量分类器**，能够处理类别重叠且类别之间没有明显分隔的情况。支持向量分类器进一步扩展为我们所称的SVM，以适应非线性类别边界。让我们一步步讨论SVM的演变，帮助我们更清楚地理解它的工作原理。
- en: 'If there are *p* dimensions (features) in a dataset, then we fit a hyperplane
    in that p-dimensional space whose equation can be defined as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集有*p*维度（特征），那么我们将在这个p维空间中拟合一个超平面，其方程可以定义如下：
- en: '![Linear Support Vector Machines (SVM)](img/image_06_043.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![线性支持向量机（SVM）](img/image_06_043.jpg)'
- en: 'This hyperplane is called the separating hyperplane that forms the decision
    boundary. The result will be classified based on the result; if greater than 0,
    then on one side and if less than 0, then on the other side, as shown in the following
    figure:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这个超平面被称为分隔超平面，形成决策边界。结果将根据结果进行分类；如果大于0，则位于一侧；如果小于0，则位于另一侧，如下图所示：
- en: '![Linear Support Vector Machines (SVM)](img/image_06_044.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![线性支持向量机（SVM）](img/image_06_044.jpg)'
- en: Observe in the preceding figure that there can be multiple hyperplanes (they
    can be infinite). There should be a reasonable way to choose the best hyperplane.
    This is where we select the maximal margin hyperplane. If you compute the perpendicular
    distance of all data points to the separating hyperplane, then the smallest distance
    would be called as the margin. So, for the maximal margin classifier, the hyperplane
    should have the highest margin.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中观察到，可以有多个超平面（它们可以是无限的）。应该有一种合理的方法来选择最优的超平面。这就是我们选择最大间隔超平面的地方。如果你计算所有数据点到分隔超平面的垂直距离，那么最小的距离被称为间隔。因此，对于最大间隔分类器，超平面应该具有最大的间隔。
- en: The training observations that are close yet equidistant from the separating
    hyperplane are known as support vectors. For any slight change in the support
    vectors, the hyperplane would also get reoriented. These support vectors actually
    define the margin. Now, what if the two classes under consideration are not separable?
    We would probably want a classifier that does not perfectly separate the two classes
    and has a softer boundary that allows some level of misclassification as well.
    This requirement led to the introduction of the support vector classifier (also
    known as the soft margin classifier).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 与分隔超平面距离近且等距的训练观测值被称为支持向量。对于支持向量的任何轻微变化，超平面也会重新定向。这些支持向量实际上定义了间隔。那么，如果考虑的两个类别不可分怎么办？我们可能希望有一个分类器，它不完美地将两个类别分开，并且有一个较软的边界，允许一定程度的误分类。这一需求促使了支持向量分类器（也称为软间隔分类器）的引入。
- en: "Mathematically, it is the slack variable in the equation that allows for misclassification.\
    \ Also, there is a tuning parameter in the support vector classifier which should\
    \ be selected using cross-\x80\x93validation. This tuning parameter is the one\
    \ that trades off between bias and variance and should be handled with care. When\
    \ it is large, the margin is wider and includes many support vectors, and has\
    \ low variance and high bias. If it is small, then the margin will have fewer\
    \ support vectors and the classifier will have low bias but high variance."
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，它是方程中的松弛变量，允许出现误分类。此外，支持向量分类器中还有一个调节参数，应该通过交叉验证来选择。这个调节参数是在偏差和方差之间进行权衡的，需要小心处理。当它较大时，边界较宽，包含许多支持向量，具有较低的方差和较高的偏差。如果它较小，边界中的支持向量较少，分类器将具有较低的偏差但较高的方差。
- en: 'The loss function for the SVM can be represented as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 的损失函数可以表示如下：
- en: '![Linear Support Vector Machines (SVM)](img/image_06_045.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![线性支持向量机 (SVM)](img/image_06_045.jpg)'
- en: As of this writing, Spark supports only linear SVMs. By default, linear SVMs
    are trained with an L2 regularization. Spark also supports alternative L1 regularization.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文写作时，Spark 只支持线性 SVM。默认情况下，线性 SVM 会使用 L2 正则化进行训练。Spark 还支持替代的 L1 正则化。
- en: 'So far so good! But how would the support vector classifier work when there
    is a non-linear boundary between the classes, as shown in the following image:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止都很好！但是，当类别之间存在非线性边界时，支持向量分类器如何工作呢？如下图所示：
- en: '![Linear Support Vector Machines (SVM)](img/image_06_046.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![线性支持向量机 (SVM)](img/image_06_046.jpg)'
- en: Any linear classifier, such as a support vector classifier, would perform very
    poorly in the preceding situation. If it draws a straight line through the data
    points, then the classes would not be separated properly. This is a case of non-linear
    class boundaries. A solution to this problem is the SVM. In other words, when
    a support vector classifier is fused with a non-linear kernel, it becomes an SVM.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 任何线性分类器，例如支持向量分类器，在前述情况下都会表现得非常差。如果它通过数据点画一条直线，那么类别将无法正确分离。这就是非线性类别边界的一个例子。解决这个问题的方法是使用
    SVM。换句话说，当支持向量分类器与非线性核函数结合时，它就变成了 SVM。
- en: Similar to the way we introduced higher order polynomial terms in the regression
    equation to account for the non-linearity, something can also be done in the SVM
    context. The SVM uses something called kernels to take care of different kinds
    of non-linearity in the dataset; different kernels for different kinds of non-linearity.
    Kernel methods map the data into higher dimensional space as the data might get
    well separated if it does so. Also, it makes distinguishing different classes
    easier. Let us discuss a few of the important kernels so as to be able to select
    the right one.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在回归方程中引入高阶多项式项以处理非线性一样，SVM 中也可以做类似的处理。SVM 使用被称为核函数的东西来处理数据集中的不同类型的非线性；不同的核函数适用于不同类型的非线性。核方法将数据映射到更高维的空间中，因为如果这样做，数据可能会被很好地分离开来。此外，它还使区分不同类别变得更加容易。我们来讨论几个重要的核函数，以便能够选择正确的核函数。
- en: Linear kernel
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性核函数
- en: This is one of the most basic type of kernels that allows us to pick out only
    lines or hyperplanes. It is equivalent to a support vector classifier. It cannot
    address the non-linearity if present in the dataset.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最基本的一种核函数类型，它只允许我们挑选出直线或超平面。它相当于一个支持向量分类器。如果数据集中存在非线性，它无法处理。
- en: Polynomial kernel
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多项式核函数
- en: This allows us to address some level of non-linearity to the extent of the order
    of polynomials. This works well when the training data is normalized. This kernel
    usually has more hyperparameters and therefore increases the complexity of the
    model.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够在多项式阶数的范围内处理一定程度的非线性。当训练数据已经规范化时，这种方法表现得很好。这个核函数通常有更多的超参数，因此会增加模型的复杂度。
- en: Radial Basis Function kernel
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 径向基函数核
- en: When you are not really sure of which kernel to use, **Radial Basis Function**
    (**RBF**) can be a good default choice. It allows you to pick out even circles
    or hyperspheres. Though this usually performs better than linear or polynomial
    kernel, it does not perform well when the number of features is huge.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 当你不确定使用哪个核函数时，**径向基函数** (**RBF**) 是一个很好的默认选择。它可以让你挑选出圆形或超球体。虽然它通常比线性核函数或多项式核函数表现得更好，但当特征数非常大时，它的表现可能不佳。
- en: Sigmoid kernel
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sigmoid 核函数
- en: The sigmoid kernel has its roots in neural networks. So, an SVM with a sigmoid
    kernel is equivalent to a neural network with a two layered perceptron.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 核函数源自神经网络。因此，具有 Sigmoid 核的 SVM 等同于具有双层感知机的神经网络。
- en: Training an SVM
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个 SVM
- en: 'While training an SVM, the modeler has to take a number of decisions:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 SVM 时，建模者需要做出一些决策：
- en: How to pre-process the data (transformation and scaling). The categorical variables
    should be converted to numeric ones by dummifying them. Also, scaling the numeric
    values is needed (either 0 to 1 or -1 to +1).
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何预处理数据（转换与缩放）。分类变量应通过虚拟化转换为数值型变量。同时，需要对数值进行缩放（将其归一化到 0 到 1 或 -1 到 +1）。
- en: "Which kernel to use (check using cross-\x80\x93validation if you are unable\
    \ to visualize the data and/ or conclude on it)."
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择哪个核函数（如果你无法可视化数据或得出结论，可以通过交叉验证检查）。
- en: "What parameters to set for the SVM: penalty parameter and the kernel parameter\
    \ (find using cross-\x80\x93validation or grid search)"
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM 的参数设置：惩罚参数和核函数参数（通过交叉验证或网格搜索找到）。
- en: If needed, you can use an entropy based feature selection to include only the
    important features in your model.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如有需要，您可以使用基于熵的特征选择方法，只包含模型中的重要特征。
- en: '**Scala**:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE14]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '`mllib` has already entered maintenance mode and SVM is still not available
    under ml so only Scala code is provided for illustration.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '`mllib` 已经进入维护模式，SVM 仍未在 ml 模块下提供，因此这里只提供了 Scala 代码示例。'
- en: Decision trees
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: A decision tree is a non-parametric supervised learning algorithm which can
    be used for both classification and regression. Decision trees are like inverted
    trees with the root node at the top and leaf nodes forming downwards. There are
    different algorithms to split the dataset into branch-like segments. Each leaf
    node is assigned to a class that represents the most appropriate target values.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种非参数化的监督学习算法，既可以用于分类问题，也可以用于回归问题。决策树像倒立的树，根节点位于顶部，叶节点向下延展。存在不同的算法用于将数据集划分为分支状的段。每个叶节点被分配到一个类别，表示最合适的目标值。
- en: Decision trees do not require any scaling or transformations of the dataset
    and work as the data is. They can handle both categorical and continuous features,
    and also address non-linearity in the dataset. At its core, a decision tree is
    a greedy algorithm (it considers the best split at the moment and does not take
    into consideration the future scenarios) that performs a recursive binary partitioning
    of the feature space. Splitting is done based on information gain at each node
    because information gain measures how well a given attribute separates the training
    examples as per the target class or value. The first split happens for the feature
    that generates maximum information gain and becomes the root node.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树不需要对数据集进行任何缩放或转换，直接处理原始数据。它们既能处理分类特征，又能处理连续特征，并且能够解决数据集中的非线性问题。决策树本质上是一种贪心算法（它只考虑当前最佳分裂，而不考虑未来的情况），通过递归二元划分特征空间进行操作。划分是基于每个节点的信息增益，因为信息增益衡量了给定特征在目标类别或值上的区分效果。第一次分裂发生在产生最大信息增益的特征上，成为根节点。
- en: The information gain at a node is the difference between the parent node impurity
    and the weighted sum of two child node impurities. To estimate information gain,
    Spark currently has two impurity measures for classification problems and one
    impurity measure for regression, as explained next.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的信息增益是父节点的不纯度与两个子节点的不纯度加权和之间的差值。为了估算信息增益，Spark 目前针对分类问题提供了两种 impurity 度量方法，针对回归问题提供了一种度量方法，具体如下。
- en: Impurity measures
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不纯度度量
- en: 'Impurity is a measure of homogeneity and the best criteria for recursive partitioning.
    By calculating the impurity, the best split candidate is decided. Most of the
    impurity measures are probability based:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 不纯度是衡量同质性的一种方法，也是递归划分的最佳标准。通过计算不纯度，可以决定最佳的划分候选。大多数不纯度度量方法是基于概率的：
- en: '*Probability of a class = number of observations of that class / total number
    of observations*'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '*某一类别的概率 = 该类别的观察次数 / 总观察次数*'
- en: Let us spend some time on different types of important impurity measures that
    are supported by Spark.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间来探讨 Spark 支持的几种重要的 impurity（不纯度）度量方法。
- en: Gini Index
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基尼指数
- en: 'The Gini Index is mainly intended for the continuous attributes or features
    in a dataset. If not, it would assume that all the attributes and features are
    continuous. The split makes the child nodes more *purer* than the parent node.
    Gini tends to find the largest class - the class of response variable that has
    got the maximum observations. It can be defined as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼指数主要用于数据集中连续的属性或特征。如果不是这样，它将假设所有属性和特征都是连续的。该分裂使得子节点比父节点更加*纯净*。基尼倾向于找到最大类别——即响应变量中观察数最多的类别。它可以定义如下：
- en: '![Gini Index](img/image_06_047.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![基尼指数](img/image_06_047.jpg)'
- en: If all observations of a response belong to a single class, then probability
    *P* of that class *j*, that is (*Pj*), will be 1 as there is only one class, and
    *(Pj)2* would also be 1\. This makes the Gini Index to be zero.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有响应的观察值都属于同一类别，那么该类别*P*的概率*j*，即(*Pj*)，将为1，因为只有一个类别，而*(Pj)2*也将为1。这使得基尼指数为零。
- en: Entropy
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 熵
- en: 'Entropy is mainly intended for the categorical attributes or features in a
    dataset. It can be defined as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 熵主要用于数据集中的分类属性或特征。它可以定义如下：
- en: '![Entropy](img/image_06_048.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![熵](img/image_06_048.jpg)'
- en: If all observations of a response belong to a single class, then the probability
    of that class (*Pj*) will be 1, and *log(P)* would be zero. This makes the entropy
    to be zero.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有响应的观察值都属于同一类别，那么该类别的概率(*Pj*)将为1，*log(P)*将为零。这样熵将为零。
- en: 'The following graph depicts the probability of a fair coin toss:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了公平掷硬币的概率：
- en: '![Entropy](img/Capture-3.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![熵](img/Capture-3.jpg)'
- en: Just to explain the preceding graph, if you toss a fair coin, the probability
    of a head or a tail would be 0.5, so there will be maximum observations at a probability
    of 0.5.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明前面的图，如果你掷一枚公平的硬币，正面或反面的概率为0.5，因此在概率为0.5时将有最多的观察值。
- en: If the data sample is completely homogeneous then the entropy will be zero,
    and if the sample can be equally divided into two, then the entropy will be one.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据样本完全同质，那么熵为零；如果样本可以平分为两个部分，那么熵为一。
- en: It is a little slower to compute than Gini because it has to compute the log
    as well.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 它的计算速度比基尼指数稍慢，因为它还需要计算对数。
- en: Variance
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方差
- en: 'Unlike the Gini Index and entropy, variance is used for calculating information
    gain for regression problems. Variance can be defined as:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 与基尼指数和熵不同，方差用于计算回归问题的信息增益。方差可以定义为：
- en: '![Variance](img/image_06_050.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![方差](img/image_06_050.jpg)'
- en: Stopping rule
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停止规则
- en: 'The recursive tree construction is stopped at a node when one of the following
    conditions is met:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 当满足以下条件之一时，递归树构建将在某个节点停止：
- en: The node depth is equal to the `maxDepth` training parameter
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点深度等于`maxDepth`训练参数
- en: No split candidate leads to an information gain greater than `minInfoGain`
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有分裂候选导致的信息增益大于`minInfoGain`
- en: No split candidate produces child nodes, each of which have at least a `minInstancesPerNode`
    training instances
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有分裂候选产生的子节点，每个子节点至少有`minInstancesPerNode`个训练实例
- en: Split candidates
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分裂候选
- en: A dataset typically has a mixture of categorical and continuous features. How
    the features get split further into split candidates is something we should understand
    because we at times need some level of control over them to build a better model.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集通常包含分类特征和连续特征的混合。我们需要了解特征如何进一步分裂成分裂候选，因为有时我们需要对它们进行一定程度的控制，以建立更好的模型。
- en: Categorical features
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类特征
- en: "For a categorical feature with *M* possible values (categories), one could\
    \ come up with *2(M-\x88\x921)-\x88\x921* split candidates. Whether for binary\
    \ classification or regression, the number of split candidates can be reduced\
    \ to *M-\x88\x921* by ordering the categorical feature values by the average label."
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: "对于具有*M*个可能值（类别）的分类特征，可以提出*2(M-\x88\x921)-\x88\x921*个分裂候选。无论是二分类还是回归，分裂候选的数量可以通过按平均标签对分类特征值排序减少到*M-\x88\
    \x921*。"
- en: For example, consider a binary classification (0/1) problem with one categorical
    feature that has three categories A, B, and C, and their corresponding proportions
    of label-1 response variables are 0.2, 0.6, and 0.4 respectively. In this case,
    the categorical features can be ordered as A, C, B. So, the two split candidates
    (*M-1* = *3-1* = *2*) can be *A | (C, B)* and *A, (C | B)* where '*|'* denotes
    the split.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个二元分类（0/1）问题，其中有一个具有三个类别 A、B 和 C 的分类特征，它们对应的标签-1 响应变量的比例分别为 0.2、0.6 和
    0.4。在这种情况下，分类特征可以按 A、C、B 排列。所以，两个分裂候选项（*M-1* = *3-1* = *2*）可以是 *A | (C, B)* 和
    *A, (C | B)*，其中 '*|*' 表示分裂。
- en: Continuous features
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连续特征
- en: For a continuous feature variable, there can be a chance that no two values
    are the same (at least we can assume so). If there are *n* observations, then
    *n* split candidates might not be a good idea, especially in a big data setting.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个连续特征变量，可能没有两个值是相同的（至少我们可以假设如此）。如果有 *n* 个观测值，那么 *n* 个分裂候选项可能不是一个好主意，尤其是在大数据环境下。
- en: In Spark, it is done by performing a quantile calculation on a sample of data,
    and binning the data accordingly. You can still have control over the maximum
    bins that you would like to allow, using the `maxBins` parameter. The maximum
    default value for `maxBins` is `32`.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，通过对数据样本进行分位数计算，并相应地对数据进行分箱来完成此操作。你仍然可以通过 `maxBins` 参数控制最大分箱数。`maxBins`
    的最大默认值是 `32`。
- en: Advantages of decision trees
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树的优点
- en: They are simple to understand and interpret, so easy to explain to business
    users
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们容易理解和解释，因此也很容易向业务用户解释
- en: They works for both classification and regression
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们适用于分类和回归
- en: Both qualitative and quantitative data can be accommodated in constructing the
    decision trees
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构建决策树时，定性和定量数据都可以得到处理
- en: Information gains in decision trees are biased in favor of the attributes with
    more levels.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树中的信息增益偏向于具有更多层次的属性。
- en: Disadvantages of decision trees
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树的缺点
- en: They do not work that greatly for effectively continuous outcome variables
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们对于连续结果变量的效果不是特别好
- en: Performance is poor when there are many classes and the dataset is small
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当类别很多且数据集很小时，性能较差
- en: Axis parallel split reduces the accuracy
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轴平行切分会降低精度
- en: They suffer from high variance as they try to fit almost all data points
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们容易受到高方差的影响，因为它们尝试拟合几乎所有的数据点
- en: Example
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: "Implementation -\x80\x93 wise there are no major differences between classification\
    \ and regression trees. Let us have a look at the practical implementation of\
    \ it on Spark."
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现方面，分类树和回归树之间没有太大区别。让我们看看在 Spark 上的实际实现。
- en: '**Scala:**'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE15]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Ensembles
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成方法
- en: As the name suggests, ensemble methods use multiple learning algorithms to obtain
    a more accurate model in terms of prediction accuracy. Usually these techniques
    require more computing power and make the model more complex, which makes it difficult
    to interpret. Let us discuss the various types of ensemble techniques available
    on Spark.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，集成方法通过使用多个学习算法来获得在预测准确性方面更精确的模型。通常，这些技术需要更多的计算能力，并使得模型更加复杂，进而增加了解释的难度。我们来讨论
    Spark 上可用的各种集成技术。
- en: Random forests
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: A random forest is an ensemble technique for the decision trees. Before we get
    to random forests, let us see how it has evolved. We know that decision trees
    usually have high variance issues and tend to overfit the model. To address this,
    a concept called *bagging* (also known as bootstrap aggregating) was introduced.
    For the decision trees, the idea was to take multiple training sets (bootstrapped
    training sets) from the dataset and create separate decision trees out of those,
    and then average them out for regression trees. For the classification trees,
    we can take the majority vote or the most commonly occurring class from all the
    trees. These trees grew deep and were not pruned at all. This definitely reduced
    the variance though the individual trees might have high variance.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是决策树的一种集成技术。在介绍随机森林之前，我们先来看看它是如何发展的。我们知道，决策树通常存在较高的方差问题，容易导致过拟合。为了解决这个问题，引入了一个叫做
    *袋装*（也叫做自助聚合）的概念。对于决策树来说，方法是从数据集中获取多个训练集（自助训练集），用这些训练集分别构建决策树，然后对回归树进行平均。对于分类树，我们可以取所有树的多数投票或最常见的类别。这些树生长得很深，且没有任何修剪。虽然每棵树可能会有较高的方差，但这显著降低了方差。
- en: One problem with the plain bagging approach was that for most of the bootstrapped
    training sets, the strong predictors took their positions at the top split which
    almost made the bagged trees look similar. This meant that the prediction also
    looked similar and if you averaged them out, then it did not reduce the variance
    to the extent expected. To address this, a technique was needed which would take
    a similar approach as that of bagged trees but eliminate the correlation amongst
    the trees, hence the *random forest*.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 使用传统的袋装方法时，存在一个问题，即对于大多数自助法训练集，强预测变量位于顶部分裂位置，几乎使得袋装树看起来相似。这意味着预测结果也相似，如果你对它们进行平均，方差并没有达到预期的减少效果。为了解决这个问题，需要一种技术，它采用与袋装树类似的方法，但消除了树之间的相关性，从而形成了*随机森林*。
- en: In this approach, you build bootstrapped training samples to create decision
    trees, but the only difference is that every time a split happens, a random sample
    of P predictors are chosen from a total of say K predictors. This is how a random
    forest injects randomness to this approach. As a thumb rule, we can take P as
    the square root of Q.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，你构建自助法训练样本来创建决策树，但唯一的区别是每次进行分裂时，会从总共的K个预测变量中随机选择P个预测变量。这就是随机森林向这种方法注入随机性的方式。作为经验法则，我们可以将P设置为Q的平方根。
- en: 'Like in the case of bagging, in this approach you also average the predictions
    if your goal is regression and take the majority vote if the goal is classification.
    Spark provides some tuning parameters to tune this model, which are as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 和袋装方法一样，在这种方法中，如果目标是回归，则对预测结果进行平均，如果目标是分类，则进行多数投票。Spark提供了一些调优参数来调整该模型，具体如下：
- en: '`numTrees`: You can specify the number of trees to consider in the random forest.
    If the numbers are high then the variance in prediction would be less, but the
    time required would be more.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numTrees`：你可以指定在随机森林中考虑的树的数量。如果树的数量较多，则预测的方差较小，但所需的时间会更长。'
- en: '`maxDepth`: You can specify the maximum depth of each tree. An increased depth
    makes the trees more powerful in terms of prediction accuracy. Though they tend
    to overfit the individual trees, the overall output is still good because we average
    the results anyway, which reduces the variance.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxDepth`：你可以指定每棵树的最大深度。增加深度可以提高树的预测准确性。尽管它们容易过拟合单棵树，但由于我们最终会对结果进行平均，因此整体输出仍然不错，从而减少了方差。'
- en: '`subsamplingRate`: This parameter is mainly used to speed up training. It is
    used to set the bootstrapped training sample size. A value less than 1.0 speeds
    up the performance.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsamplingRate`：此参数主要用于加速训练。它用于设置自助法训练样本的大小。值小于1.0会加速性能。'
- en: '`featureSubsetStrategy`: This parameter can also help speed up the execution.
    It is used to set the number of features to use as split candidates for every
    node. It should be set carefully as too low or too high a value can impact the
    accuracy of the model.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`featureSubsetStrategy`：此参数也有助于加速执行。它用于设置每个节点用于分裂的特征数。需要谨慎设置此值，因为过低或过高的值可能会影响模型的准确性。'
- en: Advantages of random forests
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林的优点
- en: They run faster as the execution happens in parallel
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的运行速度较快，因为执行过程是并行的。
- en: They are less prone to overfitting
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不易过拟合。
- en: They are easy to tune
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们易于调优。
- en: Prediction accuracy is more compared to trees or bagged trees
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与树或袋装树相比，预测准确性更高。
- en: They work well even when the predictor variables are a mixture of categorical
    and continuous features, and do not require scaling
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使预测变量是类别特征和连续特征的混合，它们也能很好地工作，并且不需要缩放。
- en: Gradient-Boosted Trees
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: Like random forests, **Gradient-Boosted Trees** (**GBTs**) are also an ensemble
    of trees. They can be applied to both classification and regression problems.
    Unlike bagged trees or random forests, where trees are built in parallel on independent
    datasets and are independent of each other, GBTs are built sequentially. Each
    tree is grown using the result of the previously grown tree. Note that GBTs do
    not work on bootstrapped samples.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林类似，**梯度提升树** (**GBTs**)也是一种树的集成方法。它们既可以应用于分类问题，也可以应用于回归问题。与袋装树或随机森林不同，后者是基于独立数据集并行构建的树，彼此独立，GBT是按顺序构建的。每棵树都是基于之前已构建的树的结果来生成的。需要注意的是，GBT不适用于自助法样本。
- en: On each iteration, GBTs use the current ensemble at hand to predict the labels
    for the training instances and compares them with true labels and estimates the
    error. The training instances with poor prediction accuracy get relabeled so that
    the decision trees get corrected in the next iteration based on the error rate
    for the previous mistakes.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，GBT会使用当前的集成模型来预测训练实例的标签，并将其与真实标签进行比较，估算误差。预测精度较差的训练实例会被重新标记，以便决策树在下一次迭代中根据上一次的误差率进行修正。
- en: 'The mechanism behind finding the error rate and relabeling the instances is
    based on the loss function. GBTs are designed to reduce this loss function for
    every iteration. The following types of loss functions are supported by Spark:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 找到误差率并重新标记实例的机制是基于损失函数的。GBT的设计旨在在每次迭代中减少这个损失函数。Spark 支持以下类型的损失函数：
- en: '**Log loss**: This is used for classification problems.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对数损失**：这用于分类问题。'
- en: '**Squared error (L2 loss)**: This is used for regression problems and is set
    by default. It is the summation of the squared differences between the actual
    and predicted output for all the observations. Outliers should be treated well
    for this loss function to perform well.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平方误差（L2损失）**：这用于回归问题，并且是默认设置。它是所有观察值的实际输出与预测输出之间平方差的总和。对于这种损失函数，应该对异常值进行良好的处理。'
- en: '**Absolute error (L1 loss)**: This is also used for regression problems. It
    is the summation of the absolute differences between the actual and predicted
    output for all the observations. It is more robust to outliers compared to squared
    error.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**绝对误差（L1损失）**：这也用于回归问题。它是所有观察值的实际输出与预测输出之间绝对差的总和。与平方误差相比，它对异常值更为稳健。'
- en: 'Spark provides some tuning parameters to tune this model, which are as follows:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了一些调参参数来调整此模型，具体如下：
- en: '`loss`: You can pass a loss function as discussed in the previous section,
    depending on the dataset you are dealing with and whether you intend to do classification
    or regression.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`：你可以传递一个损失函数，如前节所讨论的，具体取决于你处理的数据集以及你是要进行分类还是回归。'
- en: '`numIterations`: Each iteration produces only one tree! If you set this very
    high, then the time needed for execution will also be high as the operation would
    be sequential and can also lead to overfitting. It should be carefully set for
    better performance and accuracy.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numIterations`：每次迭代只产生一棵树！如果你设置得很高，那么执行所需的时间也会很长，因为操作将是顺序进行的，这也可能导致过拟合。为了更好的性能和准确性，应该谨慎设置。'
- en: '`learningRate`: This is not really a tuning parameter. If the algorithm''s
    behavior is unstable then reducing this can help stabilize the model.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learningRate`：这其实并不是一个调参参数。如果算法的行为不稳定，降低学习率可以帮助稳定模型。'
- en: '`algo`: *Classification* or *regression* is set based on what you want.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`algo`：*分类*或*回归*，根据你需要的类型来设置。'
- en: GBTs can overfit the models with a greater number of trees, so Spark provides
    the `runWithValidation` method to prevent overfitting.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: GBT可能会因为树的数量过多而导致过拟合，因此Spark提供了`runWithValidation`方法来防止过拟合。
- en: Tip
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: As of this writing, GBTs on Spark do not yet support multiclass classification.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，Spark 上的GBT尚不支持多类分类。
- en: "Let us look at an example to illustrate GBTs in action. The example dataset\
    \ contains average marks and attendance of twenty students. The data also contains\
    \ result as Pass or Fail, which follow a set of criteria. However, a couple of\
    \ students (ids 1009 and 1020) were \"\x80\x9Cgranted\" Pass status event though\
    \ they did not really qualify. Now our task is to check if the models pick up\
    \ these two students are not."
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来说明GBT的实际应用。这个示例数据集包含了二十名学生的平均分数和出勤情况。数据中还包含了通过或未通过的结果，这些结果遵循一组标准。然而，几个学生（ID为1009和1020）尽管未符合标准，但却被“授予”了通过状态。现在我们的任务是检查模型是否会把这两名学生排除在外。
- en: 'The Pass criteria are as follows:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 通过标准如下：
- en: "Marks should be at least 40 and Attendance should be at least \"\x80\x9CEnough\""
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分数应该至少为40，出勤率应该至少为“足够”。
- en: "If Marks are between 40 and 60, then attendance should be \"\x80\x9CFull\"\
    \ to pass"
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分数在40到60之间，那么出勤率应该是“完整”才能通过。
- en: The following example also emphasizes on reuse of pipeline stages across multiple
    models. So, we build a DecisionTree classifier first and then a GBT. We build
    two different pipelines that share stages.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例还强调了在多个模型中重用管道阶段。因此，我们首先构建一个决策树分类器，然后构建GBT。我们构建了两个共享阶段的不同管道。
- en: '**Input**:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：'
- en: '[PRE16]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Scala:**'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE17]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Python**:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE18]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Multilayer perceptron classifier
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知机分类器
- en: A **multilayer perceptron classifier** (**MLPC**) is a feedforward artificial
    neural network with multiple layers of nodes connected to each other in a directed
    fashion. It uses a supervised learning technique called *backpropagation* for
    training the network.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知器分类器**（**MLPC**）是一个前馈人工神经网络，具有多个层次的节点，节点之间以有向方式相互连接。它使用一种名为*反向传播*的有监督学习技术来训练网络。'
- en: Nodes in the intermediary layer use the sigmoid function to restrict the output
    between 0 and 1, and the nodes in the output layer use the `softmax` function,
    which is a generalized version of the sigmoid function.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层的节点使用sigmoid函数将输出限制在0和1之间，输出层的节点使用`softmax`函数，它是sigmoid函数的广义版本。
- en: '**Scala**:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE19]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Clustering techniques
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类技术
- en: Clustering is an unsupervised learning technique where there is no response
    variable to supervise the model. The idea is to cluster the data points that have
    some level of similarity. Apart from exploratory data analysis, it is also used
    as a part of a supervised pipeline where classifiers or regressors can be built
    on the distinct clusters. There are a bunch of clustering techniques available.
    Let us look into a few important ones that are supported by Spark.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督学习技术，其中没有响应变量来监督模型。其思想是将具有一定相似度的数据点进行聚类。除了探索性数据分析外，它还作为有监督管道的一部分，分类器或回归器可以在不同的聚类上构建。聚类技术有很多种可用的。让我们来看看一些Spark支持的重要方法。
- en: K-means clustering
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means聚类
- en: K-means is one of the most common clustering techniques. The k-means problem
    is to find cluster centers that minimize the intra-class variance, that is, the
    sum of squared distances from each data point being clustered to its cluster center
    (the center that is closest to it). You have to specify in advance the number
    of clusters you want in the dataset.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: K-means是最常见的聚类技术之一。k-means问题是找到聚类中心，以最小化类内方差，即每个数据点与其聚类中心（与其最近的中心）之间的平方距离之和。你必须提前指定数据集中所需的聚类数目。
- en: 'Since it uses the Euclidian distance measure to find the differences between
    the data points, the features need to be scaled to a comparable unit prior to
    using k-means. The Euclidian distance can be better explained in a graphical way
    as follows:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它使用欧几里得距离度量来找到数据点之间的差异，因此在使用k-means之前，需要将特征缩放到一个可比单位。欧几里得距离可以通过图形的方式更好地解释如下：
- en: '![K-means clustering](img/image_06_051.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![K-means聚类](img/image_06_051.jpg)'
- en: 'Given a set of data points (*x1*, *x2*, ..., *xn*) with as many dimensions
    as the number of variables, k-means clustering aims to partition the n observations
    into k (less than *n*) sets where *S = {S1, S2, ..., Sk}*, so as to minimize the
    **within-cluster sum of squares** (**WCSS**). In other words, its objective is
    to find:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组数据点（*x1*，*x2*，...，*xn*），这些数据点的维度与变量的数量相同，k-means聚类的目标是将这n个观测值划分为k个（小于*n*）集合，记作*S
    = {S1，S2，...，Sk}*，以最小化**类内平方和**（**WCSS**）。换句话说，它的目标是找到：
- en: '![K-means clustering](img/image_06_052.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![K-means聚类](img/image_06_052.jpg)'
- en: 'Spark requires the following parameters to be passed to this algorithm:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: Spark要求传递以下参数给这个算法：
- en: '`k`: This is the number of desired clusters.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k`：这是所需的聚类数目。'
- en: '`maxIterations`: This is the maximum number of iterations to run.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxIterations`：这是要执行的最大迭代次数。'
- en: '`initializationMode`: This specifies either random initialization or initialization
    via k-means||.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializationMode`：该参数指定随机初始化或通过k-means初始化||。'
- en: '`runs`: This is the number of times to run the k-means algorithm (k-means is
    not guaranteed to find a globally optimal solution, and when run multiple times
    on a given dataset, the algorithm returns the best clustering result).'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`runs`：这是运行k-means算法的次数（k-means不保证找到全局最优解，当对给定数据集运行多次时，算法会返回最好的聚类结果）。'
- en: '`initializationSteps`: This determines the number of steps in the k-means||
    algorithm.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializationSteps`：这个参数确定k-means||算法的步骤数。'
- en: '`epsilon`: This determines the distance threshold within which we consider
    k-means to have converged.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon`：这个参数确定了我们认为k-means已收敛的距离阈值。'
- en: '`initialModel`: This is an optional set of cluster centers used for initialization.
    If this parameter is supplied, only one run is performed.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initialModel`：这是一个可选的聚类中心集合，用于初始化。如果提供了此参数，则只执行一次运行。'
- en: Disadvantages of k-means
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-means的缺点
- en: It works only on the numeric features
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它仅适用于数值特征
- en: It requires scaling before implementing the algorithm
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实现算法之前，需要进行缩放
- en: It is susceptible to local optima (the solution to this is k-means++)
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它容易受到局部最优解的影响（解决方法是k-means++）
- en: Example
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: Let us run k-means clustering on the same students data.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在相同的学生数据上运行k-means聚类。
- en: '[PRE20]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Python**:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE21]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Summary
  id: totrans-443
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explained various machine learning algorithms, how they
    are implemented in the MLlib library and how they can be used with the pipeline API
    for a streamlined execution. The concepts were covered with Python and Scala code
    examples for a ready reference.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们解释了各种机器学习算法，如何在MLlib库中实现它们，以及如何使用pipeline API进行简化的执行。概念通过Python和Scala代码示例进行了讲解，方便参考。
- en: In the next chapter, we will discuss how Spark supports R programming language
    focusing on some of the algorithms and their executions similar to what we covered
    in this chapter.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论Spark如何支持R编程语言，重点介绍一些算法及其执行方式，类似于我们在本章中讲解的内容。
- en: References
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Supported algorithms in MLlib:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib中支持的算法：
- en: '[http://spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html)'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html)'
- en: '[http://spark.apache.org/docs/latest/mllib-decision-tree.html](http://spark.apache.org/docs/latest/mllib-decision-tree.html)'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/mllib-decision-tree.html](http://spark.apache.org/docs/latest/mllib-decision-tree.html)'
- en: 'Spark ML Programming Guide:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML编程指南：
- en: '[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)'
- en: 'Advanced datascience on spark.pdf from June 2015 summit slides:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 来自2015年6月峰会幻灯片的高级数据科学文档：
- en: '[https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html](https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html)'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html](https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html)'
- en: '[https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html](https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html)'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html](https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html)'
- en: '[https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html](https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html)'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html](https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html)'
