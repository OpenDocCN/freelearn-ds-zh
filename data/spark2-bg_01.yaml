- en: Chapter 1. Spark Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 Spark基础
- en: Data is one of the most important assets of any organization. The scale at which
    data is being collected and used in organizations is growing beyond imagination.
    The speed at which data is being ingested, the variety of the data types in use,
    and the amount of data that is being processed and stored are breaking all-time
    records every moment. It is very common these days, even in small-scale organizations,
    that data is growing from gigabytes to terabytes to petabytes. For the same reason,
    the processing needs are also growing that ask for capability to process data
    at rest as well as data on the move.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是任何组织最重要的资产之一。组织中收集和使用的数据规模正以超出想象的速度增长。数据摄取的速度、使用的数据类型多样性以及处理和存储的数据量每时每刻都在打破历史记录。如今，即使在小型组织中，数据从千兆字节增长到太字节再到拍字节也变得非常普遍。因此，处理需求也在增长，要求能够处理静态数据以及移动中的数据。
- en: Take any organization; its success depends on the decisions made by its leaders
    and for making sound decisions, you need the backing of good data and the information
    generated by processing the data. This poses a big challenge on how to process
    the data in a timely and cost-effective manner so that right decisions can be
    made. Data processing techniques have evolved since the early days of computers.
    Countless data processing products and frameworks came into the market and disappeared
    over these years. Most of these data processing products and frameworks were not
    general purpose in nature. Most of the organizations relied on their own bespoke
    applications for their data processing needs, in a silo way, or in conjunction
    with specific products.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 以任何组织为例，其成功取决于领导者所做的决策，而为了做出明智的决策，你需要依赖于处理数据产生的良好数据和信息。这给如何及时且成本效益高地处理数据提出了巨大挑战，以便做出正确决策。自计算机早期以来，数据处理技术已经发展。无数的数据处理产品和框架进入市场，又随着时间的推移而消失。这些数据处理产品和框架大多数并非通用性质。大多数组织依赖于定制的应用程序来满足其数据处理需求，以孤岛方式或与特定产品结合使用。
- en: Large-scale Internet applications, popularly known as **Internet of Things**
    (**IoT**) applications, heralded the common need to have open frameworks to process
    huge amounts of data ingested at great speed dealing with various types of data.
    Large-scale web sites, media streaming applications, and the huge batch processing
    needs of organizations made the need even more relevant. The open source community
    is also growing considerably along with the growth of the Internet, delivering
    production quality software supported by reputed software companies. A huge number
    of companies started using open source software and started deploying them in
    their production environments.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模互联网应用，俗称**物联网**（**IoT**）应用，预示着对开放框架的共同需求，以高速处理各种类型的大量数据。大型网站、媒体流应用以及组织的大规模批处理需求使得这一需求更加迫切。开源社区也随着互联网的发展而显著壮大，提供由知名软件公司支持的生产级软件。众多公司开始采用开源软件，并将其部署到生产环境中。
- en: In a technological perspective, the data processing needs were facing huge challenges.
    The amount of data started overflowing from single machines to clusters of huge
    numbers of machines. The processing power of the single CPU plateaued and modern
    computers started combining them together to get more processing power, known
    as multi-core computers. The applications were not designed and developed to make
    use of all the processors in a multi-core computer and wasted lots of the processing
    power available in a typical modern computer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度看，数据处理需求正面临巨大挑战。数据量从单机溢出到大量机器集群。单个CPU的处理能力达到瓶颈，现代计算机开始将它们组合起来以获取更多处理能力，即所谓的多核计算机。应用程序并未设计成充分利用多核计算机中的所有处理器，导致现代计算机中大量处理能力被浪费。
- en: Note
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Throughout this book, the terms *node*, *host*, and *machine* refer to a computer
    that is running in a standalone mode or in a cluster.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，*节点*、*主机*和*机器*这些术语指的是在独立模式或集群中运行的计算机。
- en: In this context, what are the qualities an ideal data processing framework should
    possess?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在此背景下，理想的数据处理框架应具备哪些特质？
- en: It should be capable of processing the blocks of data distributed across a cluster
    of computers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应能处理分布在计算机集群中的数据块
- en: It should be able to process the data in a parallel fashion so that a huge data
    processing job can be divided into multiple tasks processed in parallel so that
    the processing time can be reduced considerably
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该能够以并行方式处理数据，以便将大型数据处理任务分解为多个并行处理的子任务，从而显著减少处理时间
- en: It should be capable of using the processing power of all the cores or processors
    in a computer
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该能够利用计算机中所有核心或处理器的处理能力
- en: It should be capable of using all the available computers in a cluster
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该能够利用集群中所有可用的计算机
- en: It should be capable of running on commodity hardware
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该能够在商品硬件上运行
- en: There are two open source data processing frameworks that are worth mentioning
    that satisfy all these requirements. The first is being Apache Hadoop and the
    second one is Apache Spark.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个开源数据处理框架值得提及，它们满足所有这些要求。第一个是Apache Hadoop，第二个是Apache Spark。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Apache Hadoop
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Hadoop
- en: Apache Spark
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark
- en: Spark 2.0 installation
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Spark 2.0
- en: An overview of Apache Hadoop
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Hadoop概览
- en: 'Apache Hadoop is an open source software framework designed from ground-up
    to do distributed data storage on a cluster of computers and to do distributed
    data processing of the data that is spread across the cluster of computers. This
    framework comes with a distributed filesystem for the data storage, namely, **Hadoop
    Distributed File System** (**HDFS**), and a data processing framework, namely,
    MapReduce. The creation of HDFS is inspired from the Google research paper, *The
    Google File System* and MapReduce is based on the Google research paper, *MapReduce:
    Simplified Data Processing on Large Clusters*.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 'Apache Hadoop是一个开源软件框架，从零开始设计用于在计算机集群上进行分布式数据存储，并对分布在集群计算机上的数据进行分布式数据处理。该框架配备了一个分布式文件系统用于数据存储，即**Hadoop分布式文件系统**（**HDFS**），以及一个数据处理框架，即MapReduce。HDFS的创建灵感来自Google的研究论文《The
    Google File System》，而MapReduce则基于Google的研究论文《MapReduce: Simplified Data Processing
    on Large Clusters》。'
- en: Hadoop was adopted by organizations in a really big way by implementing huge
    Hadoop clusters for data processing. It saw tremendous growth from Hadoop MapReduce
    version 1 (MRv1) to Hadoop MapReduce version 2 (MRv2). From a pure data processing
    perspective, MRv1 consisted of HDFS and MapReduce as the core components. Many
    applications, generally called SQL-on-Hadoop applications, such as Hive and Pig,
    were stacked on top of the MapReduce framework. It is very common to see that
    even though these types of applications are separate Apache projects, as a suite,
    many such projects provide great value.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop被组织大规模采用，通过实施庞大的Hadoop集群进行数据处理。从Hadoop MapReduce版本1（MRv1）到Hadoop MapReduce版本2（MRv2），它经历了巨大的增长。从纯粹的数据处理角度来看，MRv1由HDFS和MapReduce作为核心组件组成。许多应用程序，通常称为SQL-on-Hadoop应用程序，如Hive和Pig，都建立在MapReduce框架之上。尽管这些类型的应用程序是独立的Apache项目，但作为一套，许多此类项目提供了巨大的价值，这种情况非常常见。
- en: The **Yet Another Resource Negotiator** (**YARN**) project  came to the fore
    with computing frameworks other than MapReduce type to run on the Hadoop ecosystem.
    With the introduction of YARN sitting on top of HDFS, and below MapReduce in a
    component architecture layering perspective, the users could write their own applications
    that can run on YARN and HDFS to make use of the distributed data storage and
    data processing capabilities of the Hadoop ecosystem. In other words, the newly
    overhauled MapReduce version 2 (MRv2) became one of the application frameworks
    sitting on top of HDFS and YARN.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**Yet Another Resource Negotiator**（**YARN**）项目随着非MapReduce类型的计算框架在Hadoop生态系统中运行而崭露头角。随着YARN的引入，位于HDFS之上，从组件架构分层的角度看，位于MapReduce之下，用户可以编写自己的应用程序，这些应用程序可以在YARN和HDFS上运行，以利用Hadoop生态系统的分布式数据存储和数据处理能力。换句话说，经过全面改造的MapReduce版本2（MRv2）成为了位于HDFS和YARN之上的应用程序框架之一。'
- en: '*Figure 1* gives a brief idea about these components and how they are stacked
    together:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1*简要介绍了这些组件以及它们如何堆叠在一起：'
- en: '![An overview of Apache Hadoop](img/image_01_002.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Hadoop概览](img/image_01_002.jpg)'
- en: Figure 1
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: MapReduce is a generic data processing model. The data processing goes through
    two steps, namely, *map* step and *reduce* step. In the first step, the input
    data is divided into a number of smaller parts so that each one of them can be
    processed independently. Once the *map* step is completed, its output is consolidated
    and the final result is generated in the *reduce* step. In a typical word count
    example, the creation of key-value pairs with each word as the key and the value
    1 is the *map* step. The sorting of these pairs on the key, summing the values
    of the pairs with the same key falls into an intermediate *combine* step. Producing
    the pairs containing unique words and their occurrence count is the *reduce* step.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce是一种通用数据处理模型。数据处理经过两个步骤，即*映射*步骤和*归约*步骤。在第一步中，输入数据被分割成许多较小的部分，以便每个部分可以独立处理。一旦*映射*步骤完成，其输出被整合，最终结果在*归约*步骤中生成。在典型的词频统计示例中，以每个单词为键，值为1创建键值对是*映射*步骤。基于键对这些对进行排序，对具有相同键的对的值进行求和属于中间*合并*步骤。生成包含唯一单词及其出现次数的对是*归约*步骤。
- en: 'From an application programming perspective, the basic ingredients for an over-simplified
    MapReduce application are as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从应用程序编程的角度来看，一个过度简化的MapReduce应用程序的基本要素如下：
- en: Input location
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入位置
- en: Output location
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出位置
- en: Map function implemented for the data processing need from the appropriate interfaces
    and classes from the `MapReduce` library
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapReduce库中适当的接口和类实现了数据处理所需的`Map`函数。
- en: Reduce function implemented for the data processing need from the appropriate
    interfaces and classes from the `MapReduce` library
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapReduce库中适当的接口和类实现了数据处理所需的`Reduce`函数。
- en: The MapReduce job is submitted for running in Hadoop and once the job is completed,
    the output can be taken from the output location specified.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 将MapReduce作业提交给Hadoop运行，一旦作业完成，可以从指定的输出位置获取输出。
- en: This two-step process of dividing a `MapReduce` data processing job to *map*
    and *reduce* tasks was highly effective and turned out to be a perfect fit for
    many batch data processing use cases. There is a lot of Input/Output (I/O) operations
    with the disk happening under the hood during the whole process. Even in the intermediate
    steps of the MapReduce job, if the internal data structures are filled with data
    or when the tasks are completed beyond a certain percentage, writing to the disk
    happens. Because of this, the subsequent steps in the MapReduce jobs have to read
    from the disk.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 将`MapReduce`数据处理作业分为*映射*和*归约*任务的两个步骤过程非常有效，并且证明是许多批量数据处理用例的完美匹配。在整个过程中，有许多与磁盘的输入/输出（I/O）操作在幕后发生。即使在MapReduce作业的中间步骤中，如果内部数据结构充满数据或当任务完成超过一定百分比时，写入磁盘也会发生。因此，MapReduce作业的后续步骤必须从磁盘读取。
- en: Then the other biggest challenge comes when there are multiple MapReduce jobs
    to be completed in a chained fashion. In other words, if a big data processing
    work is to be accomplished by two MapReduce jobs in such a way that the output
    of the first MapReduce job is the input of the second MapReduce job. In this situation,
    whatever may be the size of the output of the first MapReduce job, it has to be
    written to the disk before the second MapReduce could use it as its input. So
    in this simple case, there is a definite and *unnecessary* write operation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当有多个MapReduce作业需要以链式方式完成时，另一个最大的挑战出现了。换句话说，如果一项大数据处理工作是通过两个MapReduce作业完成的，使得第一个MapReduce作业的输出成为第二个MapReduce作业的输入。在这种情况下，无论第一个MapReduce作业的输出大小如何，它都必须写入磁盘，然后第二个MapReduce作业才能将其用作输入。因此，在这种情况下，存在一个明确且*不必要的*写操作。
- en: In many of the batch data processing use cases, these I/O operations are not
    a big issue. If the results are highly reliable, for many batch data processing
    use cases, latency is tolerated. But the biggest challenge comes when doing real-time
    data processing. The huge amount of I/O operations involved in MapReduce jobs
    makes it unsuitable for real-time data processing with the lowest possible latency.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多批量数据处理的用例中，这些I/O操作并不是大问题。如果结果高度可靠，对于许多批量数据处理用例来说，延迟是可以容忍的。但最大的挑战出现在进行实时数据处理时。MapReduce作业中涉及的大量I/O操作使其不适合以最低可能延迟进行实时数据处理。
- en: Understanding Apache Spark
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Apache Spark
- en: 'Spark is a **Java Virtual Machine** (**JVM**) based distributed data processing
    engine that scales, and it is fast compared to many other data processing frameworks.
    Spark was originated at the *University of California Berkeley* and later became
    one of the top projects in Apache. The research paper, *Mesos: A Platform for
    Fine-Grained Resource Sharing in the Data Center*, talks about the philosophy
    behind the design of Spark. The research paper states:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个基于**Java虚拟机**（**JVM**）的分布式数据处理引擎，具有可扩展性，且速度远超许多其他数据处理框架。Spark起源于*加州大学伯克利分校*，后来成为Apache的顶级项目之一。研究论文《Mesos：数据中心细粒度资源共享平台》阐述了Spark设计背后的理念。论文指出：
- en: '*"To test the hypothesis that simple specialized frameworks provide value,
    we identified one class of jobs that were found to perform poorly on Hadoop by
    machine learning researchers at our lab: iterative jobs, where a dataset is reused
    across a number of iterations. We built a specialized framework called Spark optimized
    for these workloads."*'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"为了验证简单专用框架的价值，我们识别出实验室机器学习研究人员发现运行不佳的一类作业：迭代作业，其中数据集在多次迭代中被重复使用。我们构建了一个专为这些工作负载优化的框架，名为Spark。"*'
- en: The biggest claim from Spark regarding speed is that it is able to *"Run programs
    up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk"*. Spark
    could make this claim because it does the processing in the main memory of the
    worker nodes and prevents the *unnecessary* I/O operations with the disks. The
    other advantage Spark offers is the ability to chain the tasks even at an application
    programming level without writing onto the disks at all or minimizing the number
    of writes to the disks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spark关于速度的最大宣称是，它能在内存中*“运行程序比Hadoop MapReduce快100倍，或在磁盘上快10倍”*。Spark之所以能做出这一宣称，是因为它在工作者节点的主内存中进行处理，避免了*不必要*的磁盘I/O操作。Spark的另一优势是，即使在应用程序编程级别，也能链式执行任务，完全不写入磁盘或最小化磁盘写入次数。
- en: 'How did Spark become so efficient in data processing as compared to MapReduce?
    It comes with a very advanced **Directed Acyclic Graph** (**DAG**) data processing
    engine. What it means is that for every Spark job, a DAG of tasks is created to
    be executed by the engine. The DAG in mathematical parlance consists of a set
    of vertices and directed edges connecting them. The tasks are executed as per
    the DAG layout. In the MapReduce case, the DAG consists of only two vertices,
    with one vertex for the *map* task and the other one for the *reduce* task. The
    edge is directed from the *map* vertex to the *reduce* vertex. The in-memory data
    processing combined with its DAG-based data processing engine makes Spark very
    efficient. In Spark''s case, the DAG of tasks can be as complicated as it can.
    Thankfully, Spark comes with utilities that can give excellent visualization of
    the DAG of any Spark job that is running. In a word count example, Spark''s Scala
    code will look something like the following code snippet . The details of this
    programming aspects will be covered in the coming chapters:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Spark相较于MapReduce，为何在数据处理上如此高效？这得益于其先进的**有向无环图**（**DAG**）数据处理引擎。这意味着每个Spark作业都会创建一个任务DAG供引擎执行。在数学术语中，DAG由一组顶点和连接它们的定向边组成。任务按照DAG布局执行。而在MapReduce中，DAG仅包含两个顶点，一个用于*映射*任务，另一个用于*归约*任务，边从*映射*顶点指向*归约*顶点。内存数据处理与基于DAG的数据处理引擎相结合，使得Spark极为高效。在Spark中，任务的DAG可以非常复杂。幸运的是，Spark提供了实用工具，能够出色地可视化任何运行中的Spark作业的DAG。以词频统计为例，Spark的Scala代码将类似于以下代码片段。这些编程细节将在后续章节中详细介绍：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The web application that comes with Spark is capable of monitoring the workers
    and applications. The DAG of the preceding Spark job generated on the fly will
    look like *Figure 2*, as shown here:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 随Spark提供的Web应用程序能够监控工作者和应用程序。前述Spark作业实时生成的DAG将呈现为*图2*，如图所示：
- en: '![Understanding Apache Spark](img/image_01_003.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![理解Apache Spark](img/image_01_003.jpg)'
- en: Figure 2
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图2
- en: The Spark programming paradigm is very powerful and exposes a uniform programming
    model supporting the application development in multiple programming languages.
    Spark supports programming in Scala, Java, Python, and R even though there is
    no functional parity across all the programming languages supported. Apart from
    writing Spark applications in these programming languages, Spark has an interactive
    shell with **Read, Evaluate, Print, and Loop** (**REPL**) capabilities for the
    programming languages Scala, Python, and R. At this moment, there is no REPL support
    for Java in Spark. The Spark REPL is a very versatile tool that can be used to
    try and test Spark application code in an interactive fashion. The Spark REPL
    enables easy prototyping, debugging, and much more.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Spark编程范式非常强大，提供了一个统一的编程模型，支持使用多种编程语言进行应用程序开发。尽管在所有支持的编程语言之间没有功能对等性，但Spark支持Scala、Java、Python和R的编程。除了使用这些编程语言编写Spark应用程序外，Spark还为Scala、Python和R提供了具有**读取、评估、打印和循环**（**REPL**）功能的交互式Shell。目前，Spark中没有为Java提供REPL支持。Spark
    REPL是一个非常多功能的工具，可用于以交互方式尝试和测试Spark应用程序代码。Spark REPL便于原型设计、调试等。
- en: 'In addition to the core data processing engine, Spark comes with a powerful
    stack of domain specific libraries that use the core Spark libraries and provide
    various functionalities useful for various big data processing needs. The following
    table lists the supported libraries:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '-   除了核心数据处理引擎外，Spark还配备了一个强大的特定领域库栈，这些库使用核心Spark库并提供各种功能，以满足各种大数据处理需求。下表列出了支持的库：'
- en: '| **Library** | **Use** | **Supported Languages** |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **库** | **用途** | **支持的语言** |'
- en: '| Spark SQL | Enables the use of SQL statements or DataFrame API inside Spark
    applications | Scala, Java, Python, and R |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Spark SQL | 使在Spark应用程序中使用SQL语句或DataFrame API成为可能 | Scala, Java, Python,
    和 R |'
- en: '| Spark Streaming | Enables processing of live data streams | Scala, Java,
    and Python |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Spark Streaming | 使处理实时数据流成为可能 | Scala, Java, 和 Python |'
- en: '| Spark MLlib | Enables development of machine learning applications | Scala,
    Java, Python, and R |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Spark MLlib | 使机器学习应用程序的开发成为可能 | Scala, Java, Python, 和 R |'
- en: '| Spark GraphX | Enables graph processing and supports a growing library of
    graph algorithms | Scala |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Spark GraphX | 启用图形处理并支持不断增长的图形算法库 | Scala |'
- en: Spark can be deployed on a variety of platforms. Spark runs on the **operating
    systems** (**OS**) Windows and UNIX (such as Linux and Mac OS). Spark can be deployed
    in a standalone mode on a single node having a supported OS. Spark can also be
    deployed in cluster node on Hadoop YARN as well as Apache Mesos. Spark can be
    deployed in the Amazon EC2 cloud as well. Spark can access data from a wide variety
    of data stores, and some of the most popular ones include HDFS, Apache Cassandra,
    Hbase, Hive, and so on. Apart from the previously listed data stores, if there
    is a driver or connector program available, Spark can access data from pretty
    much any data source.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以在各种平台上部署。Spark运行在**操作系统**（**OS**）Windows和UNIX（如Linux和Mac OS）上。Spark可以在具有支持OS的单个节点上以独立模式部署。Spark也可以在Hadoop
    YARN和Apache Mesos的集群节点上部署。Spark还可以在Amazon EC2云上部署。Spark可以从各种数据存储中访问数据，其中一些最受欢迎的包括HDFS、Apache
    Cassandra、Hbase、Hive等。除了前面列出的数据存储外，如果有驱动程序或连接器程序可用，Spark几乎可以从任何数据源访问数据。
- en: Tip
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Tip
- en: All the examples used in this book are developed, tested, and run on a Mac OS
    X Version 10.9.5 computer. The same instructions are applicable for all the other
    platforms except Windows. In Windows, corresponding to all the UNIX commands,
    there is a file with a `.cmd` extension and it has to be used. For example, for
    `spark-shell` in UNIX, there is a `spark-shell.cmd` in Windows. The program behavior
    and results should be the same across all the supported OS.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '-   本书中使用的所有示例均在Mac OS X Version 10.9.5计算机上开发、测试和运行。除Windows外，相同的指令适用于所有其他平台。在Windows上，对应于所有UNIX命令，都有一个带有`.cmd`扩展名的文件，必须使用该文件。例如，对于UNIX中的`spark-shell`，Windows中有`spark-shell.cmd`。程序行为和结果应在所有支持的操作系统上保持一致。'
- en: 'In any distributed application, it is common to have a driver program that
    controls the execution and there will be one or more worker nodes. The driver
    program allocates the tasks to the appropriate workers. This is the same even
    if Spark is running in standalone mode. In the case of a Spark application, its
    **SparkContext** object is the driver program and it communicates with the appropriate
    cluster manager to run the tasks. The Spark master, which is part of the Spark
    core library, the Mesos master, and the Hadoop YARN Resource Manager, are some
    of the cluster managers that Spark supports. In the case of a Hadoop YARN deployment
    of Spark, the Spark driver program runs inside the Hadoop YARN application master
    process or the Spark driver program runs as a client to the Hadoop YARN. *Figure
    3* describes the standalone deployment of Spark:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何分布式应用中，通常都有一个控制执行的主程序和多个工作节点。主程序将任务分配给相应的工作节点。即使在Spark独立模式下也是如此。对于Spark应用，其**SparkContext**对象即为主程序，它与相应的集群管理器通信以运行任务。Spark核心库中的Spark主节点、Mesos主节点和Hadoop
    YARN资源管理器都是Spark支持的一些集群管理器。在Hadoop YARN部署的Spark情况下，Spark驱动程序在Hadoop YARN应用主进程内运行，或者作为Hadoop
    YARN的客户端运行。*图3*描述了Spark的独立部署：
- en: '![Understanding Apache Spark](img/image_01_006.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![理解Apache Spark](img/image_01_006.jpg)'
- en: Figure 3
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3
- en: 'In the Mesos deployment mode of Spark, the cluster manager will be the **Mesos
    Master**. *Figure 4* describes the Mesos deployment of Spark:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark的Mesos部署模式下，集群管理器将是**Mesos主节点**。*图4*描述了Spark的Mesos部署：
- en: '![Understanding Apache Spark](img/image_01_008.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![理解Apache Spark](img/image_01_008.jpg)'
- en: Figure 4
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图4
- en: 'In the Hadoop YARN deployment mode of Spark, the cluster manager will be the
    Hadoop Resource Manager and its address will be picked up from the Hadoop configuration.
    In other words, when submitting the Spark jobs, there is no need to give an explicit
    master URL and it will pick up the details of the cluster manager from the Hadoop
    configuration. *Figure 5* describes the Hadoop YARN deployment of Spark:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark的Hadoop YARN部署模式下，集群管理器将是Hadoop资源管理器，其地址将从Hadoop配置中获取。换句话说，在提交Spark作业时，无需给出明确的master
    URL，它将从Hadoop配置中获取集群管理器的详细信息。*图5*描述了Spark的Hadoop YARN部署：
- en: '![Understanding Apache Spark](img/image_01_010.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![理解Apache Spark](img/image_01_010.jpg)'
- en: Figure 5
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图5
- en: Spark runs in the cloud too. In the case of the deployment of Spark on Amazon
    EC2, apart from accessing the data from the regular supported data sources, Spark
    can also access data from Amazon S3, which is the online data storage service
    from Amazon.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Spark也运行在云端。在Spark部署在Amazon EC2的情况下，除了从常规支持的数据源访问数据外，Spark还可以从Amazon S3访问数据，这是亚马逊提供的在线数据存储服务。
- en: Installing Spark on your machines
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在您的机器上安装Spark
- en: Spark supports application development in Scala, Java, Python, and R. In this
    book, Scala, Python, and R, are used. Here is the reason behind the choice of
    the languages for the examples in this book. The Spark interactive shell, or REPL,
    allows the user to execute programs on the fly just like entering OS commands
    on a terminal prompt and it is available only for the languages Scala, Python
    and R. REPL is the best way to try out Spark code before putting them together
    in a file and running them as applications. REPL helps even the experienced programmer
    to try and test the code and thus facilitates fast prototyping. So, especially
    for beginners, using REPL is the best way to get started with Spark.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持使用Scala、Java、Python和R进行应用开发。本书中使用了Scala、Python和R。以下是本书示例选择这些语言的原因。Spark交互式shell，或REPL，允许用户像在终端提示符下输入操作系统命令一样即时执行程序，并且仅适用于Scala、Python和R语言。REPL是在将代码组合到文件中并作为应用程序运行之前尝试Spark代码的最佳方式。REPL甚至可以帮助经验丰富的程序员尝试和测试代码，从而促进快速原型设计。因此，特别是对于初学者，使用REPL是开始使用Spark的最佳方式。
- en: As a pre-requisite to Spark installation and to do Spark programming in Python
    and R, both Python and R are to be installed prior to the installation of Spark.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 作为安装Spark和使用Python和R进行Spark编程的前提条件，必须在安装Spark之前安装Python和R。
- en: Python installation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Python
- en: 'Visit [https://www.python.org](https://www.python.org/) for downloading and
    installing Python for your computer. Once the installation is complete, make sure
    that the required binaries are in the OS search path and the Python interactive
    shell is coming up properly. The shell should display some content similar to
    the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[https://www.python.org](https://www.python.org/)以下载并安装适用于您计算机的Python。安装完成后，确保所需的二进制文件位于操作系统搜索路径中，且Python交互式shell能正常启动。shell应显示类似以下内容：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For charting and plotting, the `matplotlib` library is being used.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图表和绘图使用的是`matplotlib`库。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Python version 3.5.0 is used as a version of choice for Python. Even though
    Spark supports programming in Python version 2.7, as a forward looking practice,
    the latest and most stable version of Python available is used. Moreover, most
    of the important libraries are getting ported to Python version 3.x as well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本3.5.0被选为Python的版本。尽管Spark支持Python 2.7版本进行编程，但为了面向未来，我们采用了最新且最稳定的Python版本。此外，大多数重要库也正在迁移至Python
    3.x版本。
- en: 'Visit [http://matplotlib.org](http://matplotlib.org/) for downloading and installing
    the library. To make sure that the library is installed properly and that charts
    and plots are getting displayed properly, visit the [http://matplotlib.org/examples/index.html](http://matplotlib.org/examples/index.html)
    page to pick up some example code and see that your computer has all the required
    resources and components for charting and plotting. While trying to run some of
    these charting and plotting samples, in the context of the import of the libraries
    in Python code, there is a possibility that it may complain about the missing
    locale. In that case, set the following environment variables in the appropriate
    user profile to get rid of the error messages:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[http://matplotlib.org](http://matplotlib.org/)以下载并安装该库。为确保库已正确安装且图表和图形能正常显示，请访问[http://matplotlib.org/examples/index.html](http://matplotlib.org/examples/index.html)页面，获取一些示例代码，并确认您的计算机具备图表和绘图所需的所有资源和组件。在尝试运行这些图表和绘图示例时，如果Python代码中引入了库，可能会出现缺少locale的错误。此时，请在相应的用户配置文件中设置以下环境变量以消除错误信息：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: R installation
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: R安装
- en: 'Visit [https://www.r-project.org](https://www.r-project.org/) for downloading
    and installing R for your computer. Once the installation is complete, make sure
    that the required binaries are in the OS search path and the R interactive shell
    is coming up properly. The shell should display some content similar to the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[https://www.r-project.org](https://www.r-project.org/)以下载并安装适用于您计算机的R。安装完成后，确保所需的二进制文件位于操作系统搜索路径中，且R交互式shell能正常启动。shell应显示类似以下内容：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: R version 3.2.2 is the choice for R.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: R版本3.2.2是R的选择。
- en: Spark installation
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark安装
- en: 'Spark installation can be done in many different ways. The most important pre-requisite
    for Spark installation is that the Java 1.8 JDK is installed in the system and
    the `JAVA_HOME` environment variable is set to point to the Java 1.8 JDK installation
    directory. Visit [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)
    for understanding, choosing, and downloading the right type of installation for
    your computer. Spark version 2.0.0 is the version of choice for following the
    examples given in this book. Anyone who is interested in building and using Spark
    from the source code should visit: [http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html)
    for the instructions. By default, when you build Spark from the source code, it
    will not build the R libraries for Spark. For that, the SparkR libraries have
    to be built and the appropriate profile has to be included while building Spark
    from source code. The following command shows how to include the profile required
    to build the SparkR libraries:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Spark安装有多种方式。Spark安装最重要的前提是系统中已安装Java 1.8 JDK，并且`JAVA_HOME`环境变量指向Java 1.8 JDK的安装目录。访问[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)以了解、选择并下载适合您计算机的安装类型。Spark版本2.0.0是本书示例所选用的版本。对于有兴趣从源代码构建和使用Spark的用户，应访问：[http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html)以获取指导。默认情况下，从源代码构建Spark时不会构建Spark的R库。为此，需要构建SparkR库，并在从源代码构建Spark时包含适当的配置文件。以下命令展示了如何包含构建SparkR库所需的配置文件：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once the Spark installation is complete, define the following environment variables
    in the appropriate user profile:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Spark安装完成，在适当的用户配置文件中定义以下环境变量：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If there are multiple versions of Python executables in the system, then it
    is better to explicitly specify the Python executable to be used by Spark in the
    following environment variable setting:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统中有多个版本的Python可执行文件，那么最好在以下环境变量设置中明确指定Spark要使用的Python可执行文件：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the `$SPARK_HOME/bin/pyspark` script, there is a block of code that determines
    the Python executable to be used by Spark:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在`$SPARK_HOME/bin/pyspark`脚本中，有一段代码用于确定Spark要使用的Python可执行文件：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So, it is always better to explicitly set the Python executable for Spark, even
    if there is only one version of Python available in the system. This is a safeguard
    to prevent unexpected behavior when an additional version of Python is installed
    in the future.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，即使系统中只有一个版本的Python，也最好明确设置Spark的Python可执行文件。这是为了防止将来安装其他版本的Python时出现意外行为的安全措施。
- en: 'Once all the preceding steps are completed successfully, make sure that all
    the Spark shells for the languages Scala, Python, and R are working properly.
    Run the following commands on the OS terminal prompt and make sure that there
    are no errors and that content similar to the following is getting displayed.
    The following set of commands is used to bring up the Scala REPL of Spark:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成所有前面的步骤并成功，确保所有语言（Scala、Python和R）的Spark shell都能正常工作。在操作系统终端提示符下运行以下命令，并确保没有错误，且显示内容与以下类似。以下命令集用于启动Spark的Scala
    REPL：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding display, verify that the JDK version, Scala version, and Spark
    version are correct as per the settings in the computer in which Spark is installed.
    The most important point to verify is that no error messages are displayed.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述显示中，验证JDK版本、Scala版本和Spark版本是否与安装Spark的计算机中的设置相符。最重要的是验证没有错误消息显示。
- en: 'The following set of commands is used to bring up the Python REPL of Spark:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令集用于启动Spark的Python REPL：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding display, verify that the Python version, and Spark version
    are correct as per the settings in the computer in which Spark is installed. The
    most important point to verify is that no error messages are displayed.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述显示中，验证Python版本和Spark版本是否与安装Spark的计算机中的设置相符。最重要的是验证没有错误消息显示。
- en: 'The following set of commands are used to bring up the R REPL of Spark:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令集用于启动Spark的R REPL：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding display, verify that the R version and Spark version are correct
    as per the settings in the computer in which Spark is installed. The most important
    point to verify is that no error messages are displayed.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述显示中，验证R版本和Spark版本是否与安装Spark的计算机中的设置相符。最重要的是验证没有错误消息显示。
- en: 'If all the REPL for Scala, Python, and R are working fine, it is almost certain
    that the Spark installation is good. As a final test, run some of the example
    programs that came with Spark and make sure that they are giving proper results
    close to the results shown below the commands and not throwing any error messages
    in the console. When these example programs are run, apart from the output shown
    below the commands, there will be lot of other messages displayed in the console.
    They are omitted to focus on the results:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Scala、Python和R的所有REPL都运行良好，那么几乎可以肯定Spark安装是良好的。作为最终测试，运行一些随Spark附带的示例程序，并确保它们给出的结果与命令下方所示结果相近，且不在控制台抛出任何错误消息。当运行这些示例程序时，除了命令下方显示的输出外，控制台还会显示许多其他消息。为了专注于结果，这些消息被省略了：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Development tool installation
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发工具安装
- en: Most of the code that is going to be discussed in this book can be tried and
    tested in the appropriate REPL. But the proper Spark application development is
    not possible without some basic build tools. As a bare minimum requirement, for
    developing and building Spark applications in Scala, the **Scala build tool**
    (**sbt**) is a must. Visit [http://www.scala-sbt.org](http://www.scala-sbt.org/)
    for downloading and installing sbt.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中将要讨论的大部分代码都可以在相应的REPL中尝试和测试。但没有一些基本的构建工具，就无法进行适当的Spark应用程序开发。作为最低要求，对于使用Scala开发和构建Spark应用程序，**Scala构建工具**（**sbt**）是必需的。访问[http://www.scala-sbt.org](http://www.scala-sbt.org/)以下载和安装sbt。
- en: Maven is the preferred build tool for building Java applications. This book
    is not talking about Spark application development in Java, but it is good to
    have Maven also installed in the system. Maven will come in handy if Spark is
    to be built from source. Visit [https://maven.apache.org](https://maven.apache.org/)
    for downloading and installing Maven.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Maven是构建Java应用程序的首选构建工具。本书虽不涉及Java中的Spark应用程序开发，但系统中安装Maven也是有益的。如果需要从源代码构建Spark，Maven将派上用场。访问[https://maven.apache.org](https://maven.apache.org/)以下载并安装Maven。
- en: There are many **Integrated Development Environments** (**IDEs**) available
    for Scala as well as Java. It is a personal choice, and the developer can choose
    the tool of his/her choice for the language in which he/she is developing Spark
    applications.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多**集成开发环境**（**IDEs**）适用于Scala和Java。这是个人选择，开发者可以根据自己开发Spark应用程序所用的语言选择工具。
- en: Optional software installation
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可选软件安装
- en: Spark REPL for Scala is a good start to get into the prototyping and testing
    of some small snippets of code. But when there is a need to develop, build, and
    package Spark applications in Scala, it is good to have sbt-based Scala projects
    and develop them using a supported IDE, including but not limited to Eclipse or
    IntelliJ IDEA. Visit the appropriate website for downloading and installing the
    preferred IDE for Scala.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Spark REPL for Scala是开始进行代码片段原型设计和测试的好方法。但当需要开发、构建和打包Scala中的Spark应用程序时，拥有基于sbt的Scala项目并在支持的IDE（包括但不限于Eclipse或IntelliJ
    IDEA）中开发它们是明智的。访问相应的网站以下载并安装首选的Scala IDE。
- en: Notebook style application development tools are very common these days among
    data analysts and researchers. This is akin to a lab notebook. In a typical lab
    notebook, there will be instructions, detailed descriptions, and steps to follow
    to conduct an experiment. Then the experiments are conducted. Once the experiments
    are completed, there will be results captured in the notebook. If all these constructs
    are combined together and fit into the context of a software program and modeled
    in a lab notebook format, there will be documentation, code, input, and the output
    generated by running the code. This will give a very good effect, especially if
    the programs generate a lot of charts and plots.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本式应用程序开发工具在数据分析师和研究人员中非常普遍。这类似于实验室笔记本。在典型的实验室笔记本中，会有指导、详细描述和步骤，以进行实验。然后进行实验。一旦实验完成，结果将被记录在笔记本中。如果将所有这些构造结合起来，并将其置于软件程序的上下文中，以实验室笔记本格式建模，将会有文档、代码、输入和运行代码产生的输出。这将产生非常好的效果，特别是如果程序生成大量图表和绘图。
- en: Tip
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'For those who are not familiar with notebook style application development
    IDEs, there is a very nice article entitled *Interactive Notebooks: Sharing the
    Code* that can be read from [http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261](http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261).
    As an optional software development IDE for Python, the IPython notebook is described
    in the following section. After the installation, get yourself familiar with the
    tool before getting into serious development with it.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不熟悉笔记本式应用程序开发IDE的人，有一篇很好的文章名为*交互式笔记本：共享代码*，可从[http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261](http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261)阅读。作为Python的可选软件开发IDE，IPython笔记本将在下一节中描述。安装后，请先熟悉该工具，再进行严肃的开发。
- en: IPython
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IPython
- en: In the case of Spark application development in Python, IPython provides an
    excellent notebook-style development tool, which is a Python language kernel for
    Jupyter. Spark can be integrated with IPython, so that when the Spark REPL for
    Python is invoked, it will start the IPython notebook. Then, create a notebook
    and start writing code in the notebook just like the way commands are given in
    the Spark REPL for Python. Visit [http://ipython.org](http://ipython.org/) to
    download and install the IPython notebook. Once the installation is complete,
    invoke the IPython notebook interface and make sure that some example Python code
    is running fine. Invoke commands from the directory from where the notebooks are
    stored or where the notebooks are to be stored. Here, the IPython notebook is
    started from a temporary directory. When the following commands are invoked, it
    will open up the web interface and from there create a new notebook by clicking
    the New drop-down box and picking up the appropriate Python version.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中开发Spark应用程序时，IPython提供了一个出色的笔记本式开发工具，它是Jupyter的Python语言内核。Spark可以与IPython集成，以便当调用Python的Spark
    REPL时，它将启动IPython笔记本。然后，创建一个笔记本并在其中编写代码，就像在Python的Spark REPL中给出命令一样。访问[http://ipython.org](http://ipython.org/)下载并安装IPython笔记本。安装完成后，调用IPython笔记本界面，并确保一些示例Python代码运行正常。从存储笔记本的目录或将要存储笔记本的目录调用命令。这里，IPython笔记本是从临时目录启动的。当调用以下命令时，它将打开Web界面，从中通过点击“新建”下拉框并选择适当的Python版本来创建新笔记本。
- en: 'The following screenshot shows how to combine a markdown style documentation,
    a Python program, and the generated output together in an IPython notebook:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了如何在IPython笔记本中将Markdown风格的文档、Python程序以及生成的输出结合起来：
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![IPython](img/image_01_011.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![IPython](img/image_01_011.jpg)'
- en: Figure 6
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6*'
- en: '*Figure 6* shows how the IPython notebook can be used to write simple Python
    programs. The IPython notebook can be configured as a shell of choice for Spark,
    and when the Spark REPL for Python is invoked, it will start up the IPython notebook
    and Spark application development can be done using IPython notebook. To achieve
    that, define the following environment variables in the appropriate user profile:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6*展示了如何使用IPython笔记本编写简单的Python程序。IPython笔记本可以配置为Spark的首选Shell，当调用Python的Spark
    REPL时，它将启动IPython笔记本，从而可以使用IPython笔记本进行Spark应用程序开发。为此，需要在适当的用户配置文件中定义以下环境变量：'
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, instead of invoking the IPython notebook from the command prompt, invoke
    the Spark REPL for Python. Just like what has been done before, create a new IPython
    notebook and start writing Spark code in Python:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，不是从命令提示符调用IPython笔记本，而是调用Python的Spark REPL。就像之前所做的那样，创建一个新的IPython笔记本并在其中编写Spark代码：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Take a look at the following screenshot:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请看下面的截图：
- en: '![IPython](img/image_01_012.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![IPython](img/image_01_012.jpg)'
- en: Figure 7
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7*'
- en: Tip
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In the standard Spark REPL for any language, it is possible to refer the files
    located in the local filesystem with their relative path. When the IPython notebook
    is being used, local files are to be referred with their full path.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何语言的标准Spark REPL中，都可以通过相对路径引用本地文件系统中的文件。当使用IPython笔记本时，需要通过完整路径引用本地文件。
- en: RStudio
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RStudio
- en: 'Among the R user community, the preferred IDE for R is the RStudio. RStudio
    can be used to develop Spark applications in R as well. Visit [https://www.rstudio.com](https://www.rstudio.com/)
    to download and install RStudio. Once the installation is complete, before running
    any Spark R code, it is mandatory to include the `SparkR` libraries and set some
    variables to make sure that the Spark R programs are running smoothly from RStudio.
    The following code snippet does that:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在R用户社区中，首选的IDE是RStudio。RStudio也可用于开发R中的Spark应用程序。访问[https://www.rstudio.com](https://www.rstudio.com/)下载并安装RStudio。安装完成后，在运行任何Spark
    R代码之前，必须包含`SparkR`库并设置一些变量，以确保从RStudio顺利运行Spark R程序。以下代码片段实现了这一点：
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding R code, change the `SPARK_HOME_DIR` variable definition to
    point to the directory where Spark is installed. *Figure 8* shows a sample run
    of the Spark R code from RStudio:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述R代码中，将`SPARK_HOME_DIR`变量定义更改为指向Spark安装目录。*图8*展示了从RStudio运行Spark R代码的示例：
- en: '![RStudio](img/image_01_013.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![RStudio](img/image_01_013.jpg)'
- en: Figure 8
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8*'
- en: Once all the required software is installed, configured, and working as per
    the details given previously, the stage is set for Spark application development
    in Scala, Python, and R.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有必需的软件安装、配置并按先前所述正常运行，便可以开始在Scala、Python和R中进行Spark应用程序开发。
- en: Tip
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The Jupyter notebook supports multiple languages through the custom kernel implementation
    strategy for various languages. There is a native R kernel, namely IRkernel, for
    Jupyter which can be installed as an R package.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本通过为各种语言定制内核实现策略支持多种语言。Jupyter有一个原生的R内核，即IRkernel，可以作为R包安装。
- en: Apache Zeppelin
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache Zeppelin
- en: Apache Zeppelin is another promising project that is getting incubated right
    now. It is a web-based notebook similar to Jupyter but supporting multiple languages,
    shells, and technologies through its interpreter strategy enabling Spark application
    development inherently. Right now it is in its infant stage, but it has a lot
    of potential to become one of the best notebook-based application development
    platforms. Zeppelin has very powerful built-in charting and plotting capabilities
    using the data generated by the programs written in the notebook.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Zeppelin是另一个目前正处于孵化阶段的有前景的项目。它是一个基于Web的笔记本，类似于Jupyter，但通过其解释器策略支持多种语言、Shell和技术，从而内在地支持Spark应用程序开发。目前它还处于初期阶段，但有很大的潜力成为最佳的基于笔记本的应用程序开发平台之一。Zeppelin利用笔记本中编写的程序生成的数据，具备非常强大的内置图表和绘图功能。
- en: Zeppelin is built with high extensibility having the ability to plug in many
    types of interpreters using its Interpreter Framework. End users, just like any
    other notebook-based system, enter various commands in the notebook interface.
    These commands are to be processed by some interpreter to generate the output.
    Unlike many other notebook-style systems, Zeppelin supports a good number of interpreters
    or backends out of the box such as Spark, Spark SQL, Shell, Markdown, and many
    more. In terms of the frontend, again it is a pluggable architecture, namely,
    the **Helium Framework**. The data generated by the backend is displayed by the
    frontend components such as Angular JS. There are various options to display the
    data in tabular format, raw format as generated by the interpreters, charts, and
    plots. Because of the architectural separation of concerns such as the backend,
    the frontend, and the ability to plug in various components, it is a great way
    to choose heterogeneous components for the right job. At the same time, it integrates
    very well to provide a harmonious end-user-friendly data processing ecosystem.
    Even though there is pluggable architecture capability for various components
    in Zeppelin, the visualizations are limited. In other words, there are only a
    few charting and plotting options available out of the box in Zeppelin. Once the
    notebooks are working fine and producing the expected results, typically, the
    notebooks are shared with other people and for that, the notebooks are to be persisted.
    Zeppelin is different again here and it has a highly versatile notebook storage
    system. The notebooks can be persisted to the filesystem, Amazon S3, or Git, and
    other storage targets can be added if required.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Zeppelin具有高度的可扩展性，能够通过其解释器框架插入多种类型的解释器。终端用户，就像使用任何其他基于笔记本的系统一样，在笔记本界面中输入各种命令。这些命令需要由某个解释器处理以生成输出。与许多其他笔记本风格的系统不同，Zeppelin开箱即支持大量解释器或后端，如Spark、Spark
    SQL、Shell、Markdown等。在前端方面，它同样采用可插拔架构，即**Helium框架**。后端生成的数据由前端组件（如Angular JS）显示。有多种选项可以显示数据，包括表格格式、解释器生成的原始格式、图表和绘图。由于后端、前端以及能够插入各种组件的架构分离关注点，它是一种选择异构组件以适应不同任务的绝佳方式。同时，它能够很好地集成，提供一个和谐的用户友好型数据处理生态系统。尽管Zeppelin为各种组件提供了可插拔架构能力，但其可视化选项有限。换句话说，Zeppelin开箱即用提供的图表和绘图选项并不多。一旦笔记本正常工作并产生预期结果，通常会将笔记本共享给其他人，为此，笔记本需要被持久化。Zeppelin在这方面再次与众不同，它拥有一个高度灵活的笔记本存储系统。笔记本可以持久化到文件系统、Amazon
    S3或Git，并且如有需要，还可以添加其他存储目标。
- en: '**Platform as a Service** (**PaaS**) has been evolving over the last couple
    of years since the massive innovations happening around Cloud as an application
    development and deployment platform. For software developers, there are many PaaS
    platforms available delivered through Cloud, which obviates the need for them
    to have their own application development stack. Databricks has introduced a Cloud-based
    big data platform in which users can have access to a notebook-based Spark application
    development interface in conjunction with micro-cluster infrastructure to which
    the Spark applications can be submitted. There is a community edition as well,
    catering to the needs of a wider development community. The biggest advantage
    of this PaaS platform is that it is a browser-based interface and users can run
    their code against multiple versions of Spark and on different types of clusters.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**平台即服务**（**PaaS**）自云计算作为应用开发和部署平台以来，在过去几年中经历了巨大的创新和发展。对于软件开发者而言，通过云提供的众多PaaS平台消除了他们拥有自己的应用开发栈的需求。Databricks推出了一款基于云的大数据平台，用户可以访问基于笔记本的Spark应用开发界面，并与微集群基础设施相结合，以便提交Spark应用。此外，还有一个社区版，服务于更广泛的开发社区。该PaaS平台最大的优势在于它是一个基于浏览器的界面，用户可以在多个版本的Spark和不同类型的集群上运行代码。'
- en: References
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'For more information please refer to the following links:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息请参考以下链接：
- en: '[http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)'
- en: '[http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)'
- en: '[https://www.cs.berkeley.edu/~alig/papers/mesos.pdf](https://www.cs.berkeley.edu/~alig/papers/mesos.pdf)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.cs.berkeley.edu/~alig/papers/mesos.pdf](https://www.cs.berkeley.edu/~alig/papers/mesos.pdf)'
- en: '[http://spark.apache.org/](http://spark.apache.org/)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/](http://spark.apache.org/)'
- en: '[https://jupyter.org/](https://jupyter.org/)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://jupyter.org/](https://jupyter.org/)'
- en: '[https://github.com/IRkernel/IRkernel](https://github.com/IRkernel/IRkernel)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/IRkernel/IRkernel](https://github.com/IRkernel/IRkernel)'
- en: '[https://zeppelin.incubator.apache.org/](https://zeppelin.incubator.apache.org/)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://zeppelin.incubator.apache.org/](https://zeppelin.incubator.apache.org/)'
- en: '[https://community.cloud.databricks.com/](https://community.cloud.databricks.com/)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://community.cloud.databricks.com/](https://community.cloud.databricks.com/)'
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Spark is a very powerful data processing platform supporting a uniform programming
    model. It supports application development in Scala, Java, Python, and R, providing
    a stack of highly interoperable libraries used for various types of data processing
    needs, and a plethora of third-party libraries that make use of the Spark ecosystem
    covering various other data processing use cases. This chapter gave a brief introduction
    to Spark and setting up the development environment for the Spark application
    development that is going to be covered in forthcoming chapters of the book.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个功能强大的数据处理平台，支持统一的编程模型。它支持Scala、Java、Python和R中的应用程序开发，提供了一系列高度互操作的库，用于满足各种数据处理需求，以及大量利用Spark生态系统的第三方库，涵盖了其他各种数据处理用例。本章简要介绍了Spark，并为本书后续章节将要介绍的Spark应用程序开发设置了开发环境。
- en: The next chapter is going to discuss the Spark programming model, the basic
    abstractions and terminologies, Spark transformations, and Spark actions, in conjunction
    with real-world use cases.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论Spark编程模型、基本抽象和术语、Spark转换和Spark操作，结合实际用例进行阐述。
