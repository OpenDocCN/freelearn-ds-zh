- en: Chapter 6. Graph-based Storage
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。基于图形的存储
- en: 'Processing with Apache Spark and especially GraphX provides the ability to
    use in memory cluster-based, real-time processing for graphs. However, Apache
    Spark does not provide storage; the graph-based data must come from somewhere
    and after processing, probably there will be a need for storage. In this chapter,
    I will examine graph-based storage using the Titan graph database as an example.
    This chapter will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Apache Spark和特别是GraphX进行处理提供了使用基于内存的集群的实时图形处理的能力。然而，Apache Spark并不提供存储；基于图形的数据必须来自某个地方，并且在处理之后，可能会需要存储。在本章中，我将以Titan图形数据库为例，研究基于图形的存储。本章将涵盖以下主题：
- en: An overview of Titan
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Titan概述
- en: An overview of TinkerPop
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TinkerPop概述
- en: Installing Titan
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Titan
- en: Using Titan with HBase
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用HBase与Titan
- en: Using Titan with Cassandra
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Cassandra的Titan
- en: Using Titan with Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark与Titan
- en: The young age of this field of processing means that the storage integration
    between Apache Spark, and the graph-based storage system Titan is not yet mature.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个处理领域的年轻意味着Apache Spark和基于图形的存储系统Titan之间的存储集成还不够成熟。
- en: In the previous chapter, the Neo4j Mazerunner architecture was examined, which
    showed how the Spark-based transactions could be replicated to Neo4j. This chapter
    deals with Titan not because of the functionality that it shows today, but due
    to the future promise that it offers for the field of the graph-based storage
    when used with Apache Spark.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们研究了Neo4j Mazerunner架构，展示了基于Spark的事务如何被复制到Neo4j。本章讨论Titan并不是因为它今天展示的功能，而是因为它与Apache
    Spark一起在图形存储领域所提供的未来前景。
- en: Titan
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Titan
- en: Titan is a graph database that was developed by Aurelius ([http://thinkaurelius.com/](http://thinkaurelius.com/)).
    The application source and binaries can be downloaded from GitHub ([http://thinkaurelius.github.io/titan/](http://thinkaurelius.github.io/titan/)),
    and this location also contains the Titan documentation. Titan has been released
    as an open source application under an Apache 2 license. At the time of writing
    this book, Aurelius has been acquired by DataStax, although Titan releases should
    go ahead.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Titan是由Aurelius（[http://thinkaurelius.com/](http://thinkaurelius.com/)）开发的图形数据库。应用程序源代码和二进制文件可以从GitHub（[http://thinkaurelius.github.io/titan/](http://thinkaurelius.github.io/titan/)）下载，该位置还包含Titan文档。Titan已经作为Apache
    2许可证的开源应用程序发布。在撰写本书时，Aurelius已被DataStax收购，尽管Titan的发布应该会继续。
- en: Titan offers a number of storage options, but I will concentrate only on two,
    HBase—the Hadoop NoSQL database, and Cassandra—the non-Hadoop NoSQL database.
    Using these underlying storage mechanisms, Titan is able to provide a graph-based
    storage in the big data range.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Titan提供了许多存储选项，但我只会集中在两个上面，即HBase——Hadoop NoSQL数据库和Cassandra——非Hadoop NoSQL数据库。使用这些底层存储机制，Titan能够在大数据范围内提供基于图形的存储。
- en: The TinkerPop3-based Titan release 0.9.0-M2 was released in June 2015, which
    will enable greater integration with Apache Spark (TinkerPop will be explained
    in the next section). It is this release that I will use in this chapter. It is
    TinkerPop that the Titan database now uses for graph manipulation. This Titan
    release is an experimental development release but hopefully, future releases
    should consolidate Titan functionality.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基于TinkerPop3的Titan 0.9.0-M2版本于2015年6月发布，这将使其与Apache Spark更好地集成（TinkerPop将在下一节中解释）。我将在本章中使用这个版本。Titan现在使用TinkerPop进行图形操作。这个Titan版本是一个实验性的开发版本，但希望未来的版本能够巩固Titan的功能。
- en: This chapter concentrates on the Titan database rather than an alternative graph
    database, such as Neo4j, because Titan can use Hadoop-based storage. Also, Titan
    offers the future promise of integration with Apache Spark for a big data scale,
    in memory graph-based processing. The following diagram shows the architecture
    being discussed in this chapter. The dotted line shows direct Spark database access,
    whereas the solid lines represent Spark access to the data through Titan classes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章集中讨论Titan数据库而不是其他图形数据库，比如Neo4j，因为Titan可以使用基于Hadoop的存储。此外，Titan在与Apache Spark集成方面提供了未来的前景，用于大数据规模的内存图形处理。下图显示了本章讨论的架构。虚线表示直接Spark数据库访问，而实线表示Spark通过Titan类访问数据。
- en: '![Titan](img/B01989_06_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![Titan](img/B01989_06_01.jpg)'
- en: The Spark interface doesn't officially exist yet (it is only available in the
    M2 development release), but it is just added for reference. Although Titan offers
    the option of using Oracle for storage, it will not be covered in this chapter.
    I will initially examine the Titan to the HBase and Cassandra architectures, and
    consider the Apache Spark integration later. When considering (distributed) HBase,
    ZooKeeper is required as well for integration. Given that I am using an existing
    CDH5 cluster, HBase and ZooKeeper are already installed.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Spark接口目前还没有正式存在（只在M2开发版本中可用），但这只是为了参考而添加的。尽管Titan提供了使用Oracle进行存储的选项，但本章不会涉及。我将首先研究Titan与HBase和Cassandra的架构，然后考虑Apache
    Spark的集成。在考虑（分布式）HBase时，还需要ZooKeeper进行集成。鉴于我正在使用现有的CDH5集群，HBase和ZooKeeper已经安装好。
- en: TinkerPop
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TinkerPop
- en: TinkerPop, currently at version 3 as of July 2015, is an Apache incubator project,
    and can be found at [http://tinkerpop.incubator.apache.org/](http://tinkerpop.incubator.apache.org/).
    It enables both graph databases ( like Titan ) and graph analytic systems ( like
    Giraph ) to use it as a sub system for graph processing rather than creating their
    own graph processing modules.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: TinkerPop，截至2015年7月目前版本为3，是一个Apache孵化器项目，可以在[http://tinkerpop.incubator.apache.org/](http://tinkerpop.incubator.apache.org/)找到。它使得图形数据库（如Titan）和图形分析系统（如Giraph）可以将其作为图形处理的子系统使用，而不是创建自己的图形处理模块。
- en: '![TinkerPop](img/B01989_06_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![TinkerPop](img/B01989_06_02.jpg)'
- en: 'The previous figure (borrowed from the TinkerPop website) shows the TinkerPop
    architecture. The blue layer shows the Core TinkerPop API, which offers the graph
    processing API for graph, vertex, and edge processing. The **Vendor API** boxes
    show the APIs that the vendors will implement to integrate their systems. The
    diagram shows that there are two possible APIs: one for the **OLTP** database
    systems, and another for the **OLAP** analytics systems.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表（从TinkerPop网站借来）显示了TinkerPop架构。蓝色层显示了核心TinkerPop API，为图、顶点和边处理提供了图处理API。**供应商API**框显示了供应商将实现以整合其系统的API。图表显示有两种可能的API：一种用于**OLTP**数据库系统，另一种用于**OLAP**分析系统。
- en: The diagram also shows that the **Gremlin** language is used to create and manage
    graphs for TinkerPop, and so for Titan. Finally, the Gremlin server sits at the
    top of the architecture, and allows integration to monitoring systems like Ganglia.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图表还显示，**Gremlin**语言用于为TinkerPop和Titan创建和管理图。最后，Gremlin服务器位于架构的顶部，并允许集成到像Ganglia这样的监控系统。
- en: Installing Titan
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Titan
- en: 'As Titan is required throughout this chapter, I will install it now, and show
    how it can be acquired, installed, and configured. I have downloaded the latest
    prebuilt version (0.9.0-M2) of Titan at: [s3.thinkaurelius.com/downloads/titan/titan-0.9.0-M2-hadoop1.zip](http://s3.thinkaurelius.com/downloads/titan/titan-0.9.0-M2-hadoop1.zip).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章需要使用Titan，我现在将安装它，并展示如何获取、安装和配置它。我已经下载了最新的预构建版本（0.9.0-M2）的Titan：[s3.thinkaurelius.com/downloads/titan/titan-0.9.0-M2-hadoop1.zip](http://s3.thinkaurelius.com/downloads/titan/titan-0.9.0-M2-hadoop1.zip)。
- en: 'I have downloaded the zipped release to a temporary directory, as shown next.
    Carry out the following steps to ensure that Titan is installed on each node in
    the cluster:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我已将压缩版本下载到临时目录，如下所示。执行以下步骤，确保Titan在集群中的每个节点上都安装了：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using the Linux unzip command, unpack the zipped Titan release file:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Linux的解压命令，解压缩压缩的Titan发行文件：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, use the Linux `su` (switch user) command to change to the `root` account,
    and move the install to the `/usr/local/` location. Change the file and group
    membership of the install to the `hadoop` user, and create a symbolic link called
    `titan` so that the current Titan release can be referred to as the simplified
    path called `/usr/local/titan`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用Linux的`su`（切换用户）命令切换到`root`账户，并将安装移到`/usr/local/`位置。更改安装文件和组成员身份为`hadoop`用户，并创建一个名为`titan`的符号链接，以便将当前的Titan版本简化为路径`/usr/local/titan`：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Using a Titan Gremlin shell that will be demonstrated later, Titan is now available
    for use. This version of Titan needs Java 8; make sure that you have it installed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用稍后将演示的Titan Gremlin shell，现在可以使用Titan。这个版本的Titan需要Java 8；确保您已经安装了它。
- en: Titan with HBase
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有HBase的Titan
- en: As the previous diagram shows, HBase depends upon ZooKeeper. Given that I have
    a working ZooKeeper quorum on my CDH5 cluster (running on the `hc2r1m2`, `hc2r1m3`,
    and `hc2r1m4` nodes), I only need to ensure that HBase is installed and working
    on my Hadoop cluster.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，HBase依赖于ZooKeeper。鉴于我在CDH5集群上有一个正常运行的ZooKeeper仲裁（运行在`hc2r1m2`、`hc2r1m3`和`hc2r1m4`节点上），我只需要确保HBase在我的Hadoop集群上安装并正常运行。
- en: The HBase cluster
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HBase集群
- en: I will install a distributed version of HBase using the Cloudera CDH cluster
    manager. Using the manager console, it is a simple task to install HBase. The
    only decision required is where to locate the HBase servers on the cluster. The
    following figure shows the **View By Host** form from the CDH HBase installation.
    The HBase components are shown to the right as **Added Roles**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用Cloudera CDH集群管理器安装分布式版本的HBase。使用管理器控制台，安装HBase是一项简单的任务。唯一需要决定的是在集群上放置HBase服务器的位置。下图显示了CDH
    HBase安装的**按主机查看**表单。HBase组件显示在右侧作为**已添加角色**。
- en: I have chosen to add the HBase region servers (RS) to the `hc2r1m2`, `hc2r1m3`,
    and `hc2r1m4` nodes. I have installed the HBase master (M), the HBase REST server
    (HBREST), and HBase Thrift server (HBTS) on the `hc2r1m1` host.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择将HBase区域服务器（RS）添加到`hc2r1m2`、`hc2r1m3`和`hc2r1m4`节点上。我在`hc2r1m1`主机上安装了HBase主服务器（M）、HBase
    REST服务器（HBREST）和HBase Thrift服务器（HBTS）。
- en: '![The HBase cluster](img/B01989_06_03.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![HBase集群](img/B01989_06_03.jpg)'
- en: I have manually installed and configured many Hadoop-based components in the
    past, and I find that this simple manager-based installation and configuration
    of components is both quick and reliable. It saves me time so that I can concentrate
    on other systems, such as Titan.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我过去曾手动安装和配置过许多基于Hadoop的组件，我发现这种简单的基于管理器的安装和配置组件的方法既快速又可靠。这节省了我时间，让我可以集中精力处理其他系统，比如Titan。
- en: 'Once HBase is installed, and has been started from the CDH manager console,
    it needs to be checked to ensure that it is working. I will do this using the
    HBase shell command shown here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了HBase，并且已经从CDH管理器控制台启动，需要检查以确保它正常工作。我将使用下面显示的HBase shell命令来执行此操作：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As you can see from the previous commands, I run the HBase shell as the Linux
    user `hadoop`. The HBase version 0.98.6 has been installed; this version number
    will become important later when we start using Titan:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的命令所示，我以Linux用户`hadoop`身份运行HBase shell。已安装HBase版本0.98.6；在开始使用Titan时，这个版本号将变得重要：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'I have created a simple table called `table2` with a column family of `cf1`.
    I have then added two rows with two different values. This table has been created
    from the `hc2r1m2` node, and will now be checked from an alternate node called
    `hc2r1m4` in the HBase cluster:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经创建了一个名为`table2`的简单表，列族为`cf1`。然后我添加了两行，每行有两个不同的值。这个表是从`hc2r1m2`节点创建的，现在将从HBase集群中的另一个名为`hc2r1m4`的节点进行检查：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, the two data rows are visible in `table2` from a different host,
    so HBase is installed and working. It is now time to try and create a graph in
    Titan using HBase and the Titan Gremlin shell.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，从不同的主机可以看到`table2`中的两行数据，因此HBase已安装并正常工作。现在是时候尝试使用HBase和Titan Gremlin shell在Titan中创建图了。
- en: The Gremlin HBase script
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gremlin HBase脚本
- en: 'I have checked my Java version to make sure that I am on version 8, otherwise
    Titan 0.9.0-M2 will not work:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经检查了我的Java版本，以确保我使用的是8版本，否则Titan 0.9.0-M2将无法工作：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If you do not set your Java version correctly, you will get errors like this,
    which don''t seem to be meaningful until you Google them:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有正确设置Java版本，您将会遇到这样的错误，直到您谷歌它们，它们似乎没有意义：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The interactive Titan Gremlin shell can be found within the bin directory of
    the Titan install, as shown here. Once started, it offers a Gremlin prompt:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式Titan Gremlin shell可以在Titan安装的bin目录中找到，如下所示。一旦启动，它会提供一个Gremlin提示：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following script will be entered using the Gremlin shell. The first section
    of the script defines the configuration in terms of the storage (HBase), the ZooKeeper
    servers used, the ZooKeeper port number, and the HBase table name that is to be
    used:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本将使用Gremlin shell输入。脚本的第一部分定义了存储（HBase）的配置，使用的ZooKeeper服务器，ZooKeeper端口号以及要使用的HBase表名：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The next section defines the generic vertex properties'' name and age for the
    graph to be created using the Management System. It then commits the management
    system changes:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分定义了要使用管理系统创建的图的通用顶点属性的名称和年龄。然后提交管理系统的更改：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, six vertices are added to the graph. Each one is given a numeric label
    to represent its identity. Each vertex is given an age and name value:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将六个顶点添加到图中。每个顶点都被赋予一个数字标签来表示其身份。每个顶点都被赋予年龄和姓名值：
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, the graph edges are added to join the vertices together. Each edge
    has a relationship value. Once created, the changes are committed to store them
    to Titan, and therefore HBase:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，图的边被添加以将顶点连接在一起。每条边都有一个关系值。一旦创建，更改就会被提交以将它们存储到Titan，因此也存储到HBase：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This results in a simple person-based graph, shown in the following figure,
    which was also used in the previous chapter:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一个简单的基于人的图，如下图所示，这也是在上一章中使用的：
- en: '![The Gremlin HBase script](img/B01989_06_04.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![The Gremlin HBase script](img/B01989_06_04.jpg)'
- en: 'This graph can then be tested in Titan via the Gremlin shell using a similar
    script to the previous one. Just enter the following script at the `gremlin>`
    prompt, as was shown previously. It uses the same initial six lines to create
    the `titanGraph` configuration, but it then creates a graph traversal variable
    `g`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以在Titan中使用Gremlin shell测试这个图，使用与之前类似的脚本。只需在`gremlin>`提示符下输入以下脚本，就像之前展示的那样。它使用相同的六行来创建`titanGraph`配置，但然后创建了一个图遍历变量`g`：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, the graph traversal variable can be used to check the graph contents.
    Using the `ValueMap` option, it is possible to search for the graph nodes called
    `Mike` and `Flo`. They have been successfully found here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，图遍历变量可以用来检查图的内容。使用`ValueMap`选项，可以搜索名为`Mike`和`Flo`的图节点。它们已经成功找到了：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'So, the graph has been created and checked in Titan using the Gremlin shell,
    but we can also check the storage in HBase using the HBase shell, and check the
    contents of the Titan table. The following scan shows that the table exists, and
    contains `72` rows of the data for this small graph:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用Gremlin shell在Titan中创建并检查了图，但我们也可以使用HBase shell检查HBase中的存储，并检查Titan表的内容。以下扫描显示表存在，并包含此小图的`72`行数据：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now that the graph has been created, and I am confident that it has been stored
    in HBase, I will attempt to access the data using apache Spark. I have already
    started Apache Spark on all the nodes as shown in the previous chapter. This will
    be a direct access from Apache Spark 1.3 to the HBase storage. I won't at this
    stage be attempting to use Titan to interpret the HBase stored graph.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在图已经创建，并且我确信它已经存储在HBase中，我将尝试使用apache Spark访问数据。我已经在所有节点上启动了Apache Spark，如前一章所示。这将是从Apache
    Spark 1.3直接访问HBase存储。我目前不打算使用Titan来解释存储在HBase中的图。
- en: Spark on HBase
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark on HBase
- en: In order to access HBase from Spark, I will be using Cloudera's `SparkOnHBase`
    module, which can be downloaded from [https://github.com/cloudera-labs/SparkOnHBase](https://github.com/cloudera-labs/SparkOnHBase).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从Spark访问HBase，我将使用Cloudera的`SparkOnHBase`模块，可以从[https://github.com/cloudera-labs/SparkOnHBase](https://github.com/cloudera-labs/SparkOnHBase)下载。
- en: 'The downloaded file is in a zipped format, and needs to be unzipped. I have
    done this using the Linux unzip command in a temporary directory:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下载的文件是以压缩格式的，需要解压。我使用Linux unzip命令在临时目录中完成了这个操作：
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'I have then moved into the unpacked module, and used the Maven command `mvn`
    to build the JAR file:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我进入解压后的模块，并使用Maven命令`mvn`来构建JAR文件：
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, I moved the built component to my development area to keep things
    tidy, so that I could use this module in my Spark HBase code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我将构建的组件移动到我的开发区域，以保持整洁，这样我就可以在我的Spark HBase代码中使用这个模块：
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Accessing HBase with Spark
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark访问HBase
- en: 'As in previous chapters, I will be using SBT and Scala to compile my Spark-based
    scripts into applications. Then, I will use spark-submit to run these applications
    on the Spark cluster. My SBT configuration file looks like this. It contains the
    Hadoop, Spark, and HBase libraries:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前的章节一样，我将使用SBT和Scala将基于Spark的脚本编译成应用程序。然后，我将使用spark-submit在Spark集群上运行这些应用程序。我的SBT配置文件如下所示。它包含了Hadoop、Spark和HBase库：
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Notice that I am running this application on the `hc2r1m2` server, using the
    Linux `hadoop` account, under the directory `/home/hadoop/spark/titan_hbase`.
    I have created a Bash shell script called `run_titan.bash.hbase`, which allows
    me to run any application that is created and compiled under the `src/main/scala`
    subdirectory:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我正在`hc2r1m2`服务器上运行此应用程序，使用Linux`hadoop`帐户，在`/home/hadoop/spark/titan_hbase`目录下。我创建了一个名为`run_titan.bash.hbase`的Bash
    shell脚本，允许我运行在`src/main/scala`子目录下创建和编译的任何应用程序：
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The Bash script is held within the same `titan_hbase` directory, and takes
    a single parameter of the application class name. The parameters to the `spark-submit`
    call are the same as the previous examples. In this case, there is only a single
    script under `src/main/scala`, called `spark3_hbase2.scala`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Bash脚本保存在相同的`titan_hbase`目录中，并接受应用程序类名的单个参数。`spark-submit`调用的参数与先前的示例相同。在这种情况下，在`src/main/scala`下只有一个脚本，名为`spark3_hbase2.scala`：
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The Scala script starts by defining the package name to which the application
    class will belong. It then imports the Spark, Hadoop, and HBase classes:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Scala脚本首先定义了应用程序类所属的包名称。然后导入了Spark、Hadoop和HBase类：
- en: '[PRE22]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The application class name is defined as well as the main method. A configuration
    object is then created in terms of the application name, and the Spark URL. Finally,
    a Spark context is created from the configuration:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序类名也被定义，以及主方法。然后根据应用程序名称和Spark URL创建一个配置对象。最后，从配置创建一个Spark上下文：
- en: '[PRE23]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, an HBase configuration object is created, and a Cloudera CDH `hbase-site.xml`
    file-based resource is added:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建一个HBase配置对象，并添加一个基于Cloudera CDH `hbase-site.xml`文件的资源：
- en: '[PRE24]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'An HBase context object is created using the Spark context and the HBase configuration
    object. The scan and cache configurations are also defined:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark上下文和HBase配置对象创建一个HBase上下文对象。还定义了扫描和缓存配置：
- en: '[PRE25]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, the data from the HBase `Titan` table is retrieved using the `hbaseRDD`
    HBase context method, and the scan object. The RDD count is printed, and then
    the script closes:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用`hbaseRDD` HBase上下文方法和扫描对象检索了HBase `Titan`表中的数据。打印了RDD计数，然后关闭了脚本：
- en: '[PRE26]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: I am only printing the count of the data retrieved because Titan compresses
    the data in GZ format. So, it would make little sense in trying to manipulate
    it directly.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我只打印了检索到的数据计数，因为Titan以GZ格式压缩数据。因此，直接尝试操纵它将没有多大意义。
- en: 'Using the `run_titan.bash.hbase` script, the Spark application called `spark3_hbase2`
    is run. It outputs an RDD row count of `72`, matching the Titan table row count
    that was previously found. This proves that Apache Spark has been able to access
    the raw Titan HBase stored graph data, but Spark has not yet used the Titan libraries
    to access the Titan data as a graph. This will be discussed later. And here is
    the code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`run_titan.bash.hbase`脚本运行名为`spark3_hbase2`的Spark应用程序。它输出了一个RDD行计数为`72`，与先前找到的Titan表行计数相匹配。这证明Apache
    Spark已能够访问原始的Titan HBase存储的图形数据，但Spark尚未使用Titan库来访问Titan数据作为图形。这将在稍后讨论。以下是代码：
- en: '[PRE27]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Titan with Cassandra
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Cassandra的Titan
- en: In this section, the Cassandra NoSQL database will be used as a storage mechanism
    for Titan. Although it does not use Hadoop, it is a large-scale, cluster-based
    database in its own right, and can scale to very large cluster sizes. This section
    will follow the same process. As for HBase, a graph will be created, and stored
    in Cassandra using the Titan Gremlin shell. It will then be checked using Gremlin,
    and the stored data will be checked in Cassandra. The raw Titan Cassandra graph-based
    data will then be accessed from Spark. The first step then will be to install
    Cassandra on each node in the cluster.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，Cassandra NoSQL数据库将作为Titan的存储机制。尽管它不使用Hadoop，但它本身是一个大规模的基于集群的数据库，并且可以扩展到非常大的集群规模。本节将遵循相同的流程。与HBase一样，将创建一个图，并使用Titan
    Gremlin shell将其存储在Cassandra中。然后将使用Gremlin进行检查，并在Cassandra中检查存储的数据。然后将从Spark中访问原始的Titan
    Cassandra图形数据。因此，第一步是在集群中的每个节点上安装Cassandra。
- en: Installing Cassandra
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Cassandra
- en: 'Create a repo file that will allow the community version of DataStax Cassandra
    to be installed using the Linux `yum` command. Root access will be required for
    this, so the `su` command has been used to switch the user to the root. Install
    Cassandra on all the nodes:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个允许使用Linux的`yum`命令安装DataStax Cassandra社区版本的repo文件。这将需要root访问权限，因此使用`su`命令切换用户到root。在所有节点上安装Cassandra：
- en: '[PRE28]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, install Cassandra on each node in the cluster using the Linux `yum` command:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在集群中的每个节点上使用Linux的`yum`命令安装Cassandra：
- en: '[PRE29]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Set up the Cassandra configuration under `/etc/cassandra/conf` by altering
    the `cassandra.yaml` file:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过修改`cassandra.yaml`文件，在`/etc/cassandra/conf`下设置Cassandra配置：
- en: '[PRE30]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'I have made the following changes to specify my cluster name, the server seed
    IP addresses, the RPC address, and the snitch value. Seed nodes are the nodes
    that the other nodes will try to connect to first. In this case, the NameNode
    (`103`), and node2 (`108`) have been used as `seeds`. The snitch method manages
    network topology and routing:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经做了以下更改，以指定我的集群名称、服务器种子IP地址、RPC地址和snitch值。种子节点是其他节点首先尝试连接的节点。在这种情况下，NameNode（`103`）和node2（`108`）被用作`seeds`。snitch方法管理网络拓扑和路由：
- en: '[PRE31]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Cassandra can now be started on each node as root using the service command:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以作为root在每个节点上启动Cassandra，使用service命令：
- en: '[PRE32]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Log files can be found under `/var/log/cassandra`, and the data is stored under
    `/var/lib/cassandra`. The `nodetool` command can be used on any Cassandra node
    to check the status of the Cassandra cluster:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 日志文件可以在`/var/log/cassandra`下找到，数据存储在`/var/lib/cassandra`下。`nodetool`命令可以在任何Cassandra节点上使用，以检查Cassandra集群的状态：
- en: '[PRE33]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The Cassandra CQL shell command called `cqlsh` can be used to access the cluster,
    and create objects. The shell is invoked next, and it shows that Cassandra version
    2.0.13 is installed:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra CQL shell命令称为`cqlsh`，可用于访问集群并创建对象。接下来调用shell，它显示Cassandra版本2.0.13已安装：
- en: '[PRE34]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The Cassandra query language next shows a key space called `keyspace1` that
    is being created and used via the CQL shell:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra查询语言接下来显示了一个名为`keyspace1`的键空间，通过CQL shell创建和使用：
- en: '[PRE35]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Since Cassandra is installed and working, it is now time to create a Titan graph
    using Cassandra for storage. This will be tackled in the next section using the
    Titan Gremlin shell. It will follow the same format as the HBase section previously.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Cassandra已安装并运行，现在是时候使用Cassandra创建Titan图形存储。这将在下一节中使用Titan Gremlin shell来解决。它将遵循之前HBase部分的相同格式。
- en: The Gremlin Cassandra script
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gremlin Cassandra脚本
- en: 'As with the previous Gremlin script, this Cassandra version creates the same
    simple graph. The difference with this script is in the configuration. The backend
    storage type is defined as Cassandra, and the hostnames are defined to be the
    Cassandra seed nodes. The key space and the port number are specified and finally,
    the graph is created:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的Gremlin脚本一样，这个Cassandra版本创建了相同的简单图。这个脚本的不同之处在于配置。后端存储类型被定义为Cassandra，主机名被定义为Cassandra种子节点。指定了key
    space和端口号，最后创建了图：
- en: '[PRE36]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'From this point, the script is the same as the previous HBase example, so I
    will not repeat it. This script will be available in the download package as `cassandra_create.bash`.
    The same checks, using the previous configuration, can be carried out in the Gremlin
    shell to check the data. This returns the same results as the previous checks,
    and so proves that the graph has been stored:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一点开始，脚本与之前的HBase示例相同，所以我不会重复它。这个脚本将作为`cassandra_create.bash`在下载包中提供。可以在Gremlin
    shell中使用之前的配置进行相同的检查以检查数据。这返回与之前检查相同的结果，证明图已经被存储：
- en: '[PRE37]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Using the Cassandra CQL shell, and the Titan `keyspace`, it can be seen that
    a number of Titan tables have been created in Cassandra:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用Cassandra CQL shell和Titan `keyspace`，可以看到在Cassandra中已经创建了许多Titan表：
- en: '[PRE38]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'It can also be seen that the data exists in the `edgestore` table within Cassandra:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以看到数据存在于Cassandra的`edgestore`表中：
- en: '[PRE39]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This assures me that a Titan graph has been created in the Gremlin shell, and
    is stored in Cassandra. Now, I will try to access the data from Spark.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这向我保证了在Gremlin shell中已经创建了Titan图，并且存储在Cassandra中。现在，我将尝试从Spark中访问数据。
- en: The Spark Cassandra connector
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark Cassandra连接器
- en: In order to access Cassandra from Spark, I will download the DataStax Spark
    Cassandra connector and driver libraries. Information and version matching on
    this can be found at [http://mvnrepository.com/artifact/com.datastax.spark/](http://mvnrepository.com/artifact/com.datastax.spark/).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从Spark访问Cassandra，我将下载DataStax Spark Cassandra连接器和驱动程序库。关于这方面的信息和版本匹配可以在[http://mvnrepository.com/artifact/com.datastax.spark/](http://mvnrepository.com/artifact/com.datastax.spark/)找到。
- en: The version compatibility section of this URL shows the Cassandra connector
    version that should be used with each Cassandra and Spark version. The version
    table shows that the connector version should match the Spark version that is
    being used. The next URL allows the libraries to be sourced at [http://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector_2.10](http://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector_2.10).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个URL的版本兼容性部分显示了应该与每个Cassandra和Spark版本一起使用的Cassandra连接器版本。版本表显示连接器版本应该与正在使用的Spark版本匹配。下一个URL允许在[http://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector_2.10](http://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector_2.10)找到这些库。
- en: 'By following the previous URL, and selecting a library version, you will see
    a compile dependencies table associated with the library, which indicates all
    of the other dependent libraries, and their versions that you will need. The following
    libraries are those that are needed for use with Spark 1.3.1\. If you use the
    previous URLs, you will see which version of the Cassandra connector library to
    use with each version of Spark. You will also see the libraries that the Cassandra
    connector depends upon. Be careful to choose just (and all of) those library versions
    that are required:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上面的URL，并选择一个库版本，你将看到与该库相关的编译依赖关系表，其中指示了你需要的所有其他依赖库及其版本。以下库是与Spark 1.3.1一起使用所需的。如果你使用前面的URL，你将看到每个Spark版本应该使用哪个版本的Cassandra连接器库。你还将看到Cassandra连接器依赖的库。请小心选择所需的库版本：
- en: '[PRE40]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Accessing Cassandra with Spark
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark访问Cassandra
- en: 'Now that I have the Cassandra connector library and all of it''s dependencies
    in place, I can begin to think about the Scala code, required to connect to Cassandra.
    The first thing to do, given that I am using SBT as a development tool, is to
    set up the SBT build configuration file. Mine looks like this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我已经准备好了Cassandra连接器库和所有的依赖关系，我可以开始考虑连接到Cassandra所需的Scala代码。首先要做的事情是设置SBT构建配置文件，因为我使用SBT作为开发工具。我的配置文件看起来像这样：
- en: '[PRE41]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The Scala script for the Cassandra connector example, called `spark3_cass.scala`,
    now looks like the following code. First, the package name is defined. Then, the
    classes are imported for Spark, and the Cassandra connector. Next, the object
    application class `spark3_cass` ID is defined, and so is the main method:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra连接器示例的Scala脚本，名为`spark3_cass.scala`，现在看起来像以下代码。首先，定义了包名。然后，为Spark和Cassandra连接器导入了类。接下来，定义了对象应用类`spark3_cass`
    ID，以及主方法：
- en: '[PRE42]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'A Spark configuration object is created using a Spark URL and application name.
    The Cassandra connection host is added to the configuration. Then, the Spark context
    is created using the configuration object:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark URL和应用程序名称创建了一个Spark配置对象。将Cassandra连接主机添加到配置中。然后，使用配置对象创建了Spark上下文：
- en: '[PRE43]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The Cassandra `keyspace`, and table names that are to be checked are defined.
    Then, the Spark context method called `cassandraTable` is used to connect to Cassandra,
    and obtain the contents of the `edgestore` table as an RDD. The size of this RDD
    is then printed, and the script exits. We won''t look at this data at this time,
    because all that was needed was to prove that a connection to Cassandra could
    be made:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查的Cassandra `keyspace`和表名已经定义。然后，使用名为`cassandraTable`的Spark上下文方法连接到Cassandra，并获取`edgestore`表的内容作为RDD。然后打印出这个RDD的大小，脚本退出。我们暂时不会查看这些数据，因为只需要证明可以连接到Cassandra：
- en: '[PRE44]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'As in the previous examples, the Spark `submit` command has been placed in
    a Bash script called `run_titan.bash.cass`. This script, shown next, looks similar
    to many others used already. The point to note here is that there is a JARs option,
    which lists all of the JAR files used so that they are available at run time.
    The order of JAR files in this option has been determined to avoid the class exception
    errors:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的示例一样，Spark的`submit`命令已放置在一个名为`run_titan.bash.cass`的Bash脚本中。下面显示的脚本看起来与已经使用的许多其他脚本类似。这里需要注意的是有一个JARs选项，列出了所有在运行时可用的JAR文件。这个选项中JAR文件的顺序已经确定，以避免类异常错误：
- en: '[PRE45]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This application is invoked using the previous Bash script. It connects to Cassandra,
    selects the data, and returns a Cassandra table data-based count of `218` rows.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 此应用程序是通过之前的Bash脚本调用的。它连接到Cassandra，选择数据，并返回基于Cassandra表数据的计数为`218`行。
- en: '[PRE46]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This proves that the raw Cassandra-based Titan table data can be accessed from
    Apache Spark. However, as in the HBase example, this is raw table-based Titan
    data, and not the data in Titan graph form. The next step will be to use Apache
    Spark as a processing engine for the Titan database. This will be examined in
    the next section.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了可以从Apache Spark访问基于原始Cassandra的Titan表数据。然而，与HBase示例一样，这是基于原始表的Titan数据，而不是Titan图形中的数据。下一步将是使用Apache
    Spark作为Titan数据库的处理引擎。这将在下一节中进行讨论。
- en: Accessing Titan with Spark
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark访问Titan
- en: So far in this chapter, Titan 0.9.0-M2 has been installed, and the graphs have
    successfully been created using both HBase and Cassandra as backend storage options.
    These graphs have been created using Gremlin-based scripts. In this section, a
    properties file will be used via a Gremlin script to process a Titan-based graph
    using Apache Spark. The same two backend storage options, HBase and Cassandra,
    will be used with Titan.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，已经安装了Titan 0.9.0-M2，并成功使用HBase和Cassandra作为后端存储选项创建了图形。这些图形是使用基于Gremlin的脚本创建的。在本节中，将使用属性文件通过Gremlin脚本来处理基于Titan的图形，使用相同的两个后端存储选项HBase和Cassandra。
- en: The following figure, based on the TinkerPop3 diagram earlier in this chapter,
    shows the architecture used in this section. I have simplified the diagram, but
    it is basically the same as the previous TinkerPop version. I have just added
    the link to Apache Spark via the Graph Computer API. I have also added both HBase
    and Cassandra storage via the Titan vendor API. Of course, a distributed installation
    of HBase uses both Zookeeper for configuration, and HDFS for storage.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表基于本章前面的TinkerPop3图表，展示了本节中使用的架构。我简化了图表，但基本上与之前的TinkerPop版本相同。我只是通过图形计算机API添加了到Apache
    Spark的链接。我还通过Titan供应商API添加了HBase和Cassandra存储。当然，HBase的分布式安装同时使用Zookeeper进行配置，使用HDFS进行存储。
- en: 'Titan uses TinkerPop''s Hadoop-Gremlin package for graph processing OLAP processes.
    The link to the documentation section can be found at: [http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html](http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Titan使用TinkerPop的Hadoop-Gremlin包进行图处理OLAP过程。文档部分的链接可以在这里找到：[http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html](http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html)。
- en: This section will show how the Bash shell, Groovy, and properties files can
    be used to configure, and run a Titan Spark-based job. It will show different
    methods for configuring the job, and it will also show methods for managing logging
    to enable error tracking. Also, different configurations of the property file
    will be described to give access to HBase, Cassandra, and the Linux file system.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将展示如何使用Bash shell、Groovy和属性文件来配置和运行基于Titan Spark的作业。它将展示配置作业的不同方法，并展示管理日志以启用错误跟踪的方法。还将描述属性文件的不同配置，以便访问HBase、Cassandra和Linux文件系统。
- en: Remember that the Titan release 0.9.0-M2, that this chapter is based on, is
    a development release. It is a prototype release, and is not yet ready for production.
    I assume that as the future Titan releases become available, the link between
    Titan and Spark will be more developed and stable. Currently, the work in this
    section is for demonstration purposes only, given the nature of the Titan release.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，本章基于的Titan 0.9.0-M2版本是一个开发版本。这是一个原型版本，尚未准备好投入生产。我假设随着未来Titan版本的推出，Titan与Spark之间的链接将更加完善和稳定。目前，本节中的工作仅用于演示目的，考虑到Titan版本的性质。
- en: '![Accessing Titan with Spark](img/B01989_06_05.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark访问Titan](img/B01989_06_05.jpg)'
- en: In the next section, I will explain the use of Gremlin, and Groovy scripts before
    moving onto connecting Titan to Spark using Cassandra and HBase as storage options.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我将解释使用Gremlin和Groovy脚本，然后转向使用Cassandra和HBase作为存储选项将Titan连接到Spark。
- en: Gremlin and Groovy
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gremlin和Groovy
- en: 'The Gremlin shell, which is used to execute Groovy commands against Titan,
    can be used in a number of ways. The first method of use just involves starting
    a Gremlin shell for use as an interactive session. Just execute the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 用于执行Groovy命令的Gremlin shell可以以多种方式使用。第一种使用方法只涉及启动Gremlin shell以用作交互式会话。只需执行以下命令：
- en: '[PRE47]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This starts the session, and automatically sets up required plug-ins such as
    TinkerPop and Titan (see next). Obviously, the previous `TITAN_HOME` variable
    is used to indicate that the bin directory in question is located within your
    Titan install (`TITAN_HOME`) directory:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动会话，并自动设置所需的插件，如TinkerPop和Titan（见下文）。显然，之前的`TITAN_HOME`变量用于指示所讨论的bin目录位于您的Titan安装（`TITAN_HOME`）目录中：
- en: '[PRE48]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: It then provides you with a Gremlin shell prompt where you can interactively
    execute your shell commands against your Titan database. This shell is useful
    for testing scripts and running ad hoc commands against your Titan database.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它会提供一个Gremlin shell提示符，您可以在其中交互式地执行对Titan数据库的shell命令。此shell对于测试脚本和针对Titan数据库运行临时命令非常有用。
- en: '[PRE49]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'A second method is to embed your Groovy commands inline in a script when you
    call the `gremlin.sh` command. In this example, the Groovy commands between the
    EOF markers are piped into the Gremlin shell. When the last Groovy command has
    executed, the Gremlin shell will terminate. This is useful when you still want
    to use the automated environment setup of the Gremlin shell, but you still want
    to be able to quickly re-execute a script. This code snippet has been executed
    from a Bash shell script, as can be seen in the next example. The following script
    uses the `titan.sh` script to manage the Gremlin server:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是在调用`gremlin.sh`命令时将您的Groovy命令嵌入到脚本中。在这个例子中，EOF标记之间的Groovy命令被传送到Gremlin
    shell中。当最后一个Groovy命令执行完毕时，Gremlin shell将终止。当您仍希望使用Gremlin shell的自动化环境设置，但仍希望能够快速重新执行脚本时，这是很有用的。这段代码片段是从Bash
    shell脚本中执行的，如下一个例子所示。以下脚本使用`titan.sh`脚本来管理Gremlin服务器：
- en: '[PRE50]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'A third method involves moving the Groovy commands into a separate Groovy file,
    and using the `–e` option with the Gremlin shell to execute the file. This method
    offers extra logging options for error tracking, but means that extra steps need
    to be taken when setting up the Gremlin environment for a Groovy script:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法涉及将Groovy命令移动到一个单独的Groovy文件中，并使用Gremlin shell的`-e`选项来执行该文件。这种方法为错误跟踪提供了额外的日志选项，但意味着在为Groovy脚本设置Gremlin环境时需要采取额外的步骤：
- en: '[PRE51]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: So, this script defines a Gremlin log level, which can be set to different logging
    levels to obtain extra information about a problem, that is, INFO, WARN, and DEBUG.
    It also redirects the script output to a log file (`GREMLIN_LOG_FILE`), and redirects
    errors to the same log file (`2>&1`). This has the benefit of allowing the log
    file to be continuously monitored, and provides a permanent record of the session.
    The Groovy script name that is to be executed is then passed to the encasing Bash
    shell script as a parameter (`$1`).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个脚本定义了Gremlin日志级别，可以设置为不同的日志级别以获取有关问题的额外信息，即INFO、WARN和DEBUG。它还将脚本输出重定向到日志文件（`GREMLIN_LOG_FILE`），并将错误重定向到同一个日志文件（`2>&1`）。这样做的好处是可以持续监视日志文件，并提供会话的永久记录。要执行的Groovy脚本名称然后作为参数（`$1`）传递给封装的Bash
    shell脚本。
- en: 'As I already mentioned, the Groovy scripts invoked in this way need extra environment
    configuration to set up the Gremlin session when compared to the previous Gremlin
    session options. For instance, it is necessary to import the necessary TinkerPop
    and Aurelius classes that will be used:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，以这种方式调用的Groovy脚本需要额外的环境配置，以设置Gremlin会话，与之前的Gremlin会话选项相比。例如，需要导入将要使用的必要的TinkerPop和Aurelius类：
- en: '[PRE52]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Having described the script and configuration options necessary to start a Gremlin
    shell session, and run a Groovy script, from this point onwards I will concentrate
    on Groovy scripts, and the property files necessary to configure the Gremlin session.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述了启动Gremlin shell会话和运行Groovy脚本所需的脚本和配置选项之后，从现在开始我将集中讨论Groovy脚本和配置Gremlin会话所需的属性文件。
- en: TinkerPop's Hadoop Gremlin
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TinkerPop的Hadoop Gremlin
- en: As already mentioned previously in this section, it is the TinkerPop Hadoop
    Gremlin package within Titan that will be used to call Apache Spark as a processing
    engine (Hadoop Giraph can be used for processing as well). The link available
    at [http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html](http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html)
    provides documentation for Hadoop Gremlin; remember that this TinkerPop package
    is still being developed and is subject to change.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面在本节中已经提到的，Titan中的TinkerPop Hadoop Gremlin包将用于调用Apache Spark作为处理引擎（Hadoop
    Giraph也可以用于处理）。链接[http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html](http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html)提供了Hadoop
    Gremlin的文档；请记住，这个TinkerPop包仍在开发中，可能会有所改变。
- en: 'At this point, I will examine a properties file that can be used to connect
    to Cassandra as a storage backend for Titan. It contains sections for Cassandra,
    Apache Spark, and the Hadoop Gremlin configuration. My Cassandra properties file
    is called `cassandra.properties`, and it looks like this (lines beginning with
    a hash character (`#`) are comments):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我将检查一个属性文件，该文件可用于将Cassandra作为Titan的存储后端进行连接。它包含了用于Cassandra、Apache Spark和Hadoop
    Gremlin配置的部分。我的Cassandra属性文件名为`cassandra.properties`，内容如下（以井号（`#`）开头的行是注释）：
- en: '[PRE53]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The previous Cassandra-based properties describe the Cassandra host and port.
    This is why the storage backend type is Cassandra, the Cassandra `keyspace` that
    is to be used is called `dead` (short for grateful dead—the data that will be
    used in this example). Remember that the Cassandra tables are grouped within keyspaces.
    The previous `partitioner` class defines the Cassandra class that will be used
    to partition the Cassandra data. The Apache Spark configuration section contains
    the master URL, executor memory, and the data `serializer` class that is to be
    used:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 前面基于Cassandra的属性描述了Cassandra主机和端口。这就是存储后端类型为Cassandra的原因，要使用的Cassandra `keyspace`称为`dead`（代表感激的死者——在本例中将使用的数据）。请记住，Cassandra表是在keyspaces中分组的。前面的`partitioner`类定义了将用于对Cassandra数据进行分区的Cassandra类。Apache
    Spark配置部分包含主URL、执行器内存和要使用的数据`serializer`类：
- en: '[PRE54]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Finally, the Hadoop Gremlin section of the properties file, which defines the
    classes to be used for graph and non-graph input and output is shown here. It
    also defines the data input and output locations, as well as the flags for caching
    JAR files, and deriving memory:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这里显示了属性文件中Hadoop Gremlin部分，该部分定义了用于图形和非图形输入和输出的类。它还定义了数据输入和输出位置，以及用于缓存JAR文件和推导内存的标志。
- en: '[PRE55]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Blueprints is the TinkerPop property graph model interface. Titan releases
    it''s own implementation of blueprints, so instead of seeing `blueprints.graph`
    in the preceding properties, you see `gremlin.graph`. This defines the class,
    used to define the graph that is supposed to be used. If this option were omitted,
    then the graph type would default to the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝图是TinkerPop属性图模型接口。Titan发布了自己的蓝图实现，所以在前面的属性中，你会看到`gremlin.graph`而不是`blueprints.graph`。这定义了用于定义应该使用的图的类。如果省略了这个选项，那么图类型将默认为以下内容：
- en: '[PRE56]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The `CassandraInputFormat` class defines that the data is being retrieved from
    the Cassandra database. The graph output serialization class is defined to be
    `GryoOutputFormat`. The memory output format class is defined to use the Hadoop
    Map Reduce class `SequenceFileOutputFormat`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`CassandraInputFormat`类定义了数据是从Cassandra数据库中检索出来的。图输出序列化类被定义为`GryoOutputFormat`。内存输出格式类被定义为使用Hadoop
    Map Reduce类`SequenceFileOutputFormat`。'
- en: The `jarsInDistributedCache` value has been defined to be true so that the JAR
    files are copied to the memory, enabling Apache Spark to source them. Given more
    time, I would investigate ways to make the Titan classes visible to Spark, on
    the class path, to avoid excessive memory usage.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`jarsInDistributedCache`值已被定义为true，以便将JAR文件复制到内存中，使Apache Spark能够使用它们。如果有更多时间，我会研究使Titan类对Spark可见的方法，以避免过多的内存使用。'
- en: Given that the TinkerPop Hadoop Gremlin module is only available as a development
    prototype release, currently the documentation is minimal. There are very limited
    coding examples, and there does not seem to be documentation available describing
    each of the previous properties.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于TinkerPop Hadoop Gremlin模块目前仅作为开发原型版本发布，目前文档很少。编码示例非常有限，似乎也没有文档描述之前的每个属性。
- en: Before I delve into the examples of Groovy scripts, I thought that I would show
    you an alternative method for configuring your Groovy jobs using a configuration
    object.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在我深入探讨Groovy脚本示例之前，我想向您展示一种使用配置对象配置Groovy作业的替代方法。
- en: Alternative Groovy configuration
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替代Groovy配置
- en: 'A configuration object can be created using the `BaseConfiguration` method.
    In this example, I have created a Cassandra configuration called `cassConf`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`BaseConfiguration`方法创建配置对象。在这个例子中，我创建了一个名为`cassConf`的Cassandra配置：
- en: '[PRE57]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The `setProperty` method is then used to define Cassandra connection properties,
    such as backend type, host, port, and `keyspace`. Finally, a Titan graph is created
    called `titanGraph` using the open method. As will be shown later, a Titan graph
    can be created using a configuration object or a path to a properties file. The
    properties that have been set match those that were defined in the Cassandra properties
    file described previously.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用`setProperty`方法来定义Cassandra连接属性，如后端类型、主机、端口和`keyspace`。最后，使用open方法创建了一个名为`titanGraph`的Titan图。稍后将会展示，Titan图可以使用配置对象或属性文件路径来创建。已设置的属性与之前描述的Cassandra属性文件中定义的属性相匹配。
- en: The next few sections will show how graphs can be created, and traversed. They
    will show how Cassandra, HBase, and the file system can be used for storage. Given
    that I have gone to such lengths to describe the Bash scripts, and the properties
    files, I will just describe those properties that need to be changed in each instance.
    I will also provide simple Groovy script snippets in each instance.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几节将展示如何创建和遍历图。它们将展示如何使用Cassandra、HBase和文件系统进行存储。鉴于我已经花了很多篇幅描述Bash脚本和属性文件，我只会描述每个实例中需要更改的属性。我还将在每个实例中提供简单的Groovy脚本片段。
- en: Using Cassandra
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Cassandra
- en: 'The Cassandra-based properties file called `cassandra.properties` has already
    been described, so I will not repeat the details here. This example Groovy script
    creates a sample graph, and stores it in Cassandra. It has been executed using
    the **end of file markers** (**EOF**) to pipe the script to the Gremlin shell:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 名为`cassandra.properties`的基于Cassandra的属性文件已经在前面描述过，所以我不会在这里重复细节。这个示例Groovy脚本创建了一个示例图，并将其存储在Cassandra中。它已经使用**EOF**来将脚本传输到Gremlin
    shell执行：
- en: '[PRE58]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'A Titan graph has been created using the `TitanFactory.open` method and the
    Cassandra properties file. It is called `t1`. The graph of the Gods, an example
    graph provided with Titan, has been loaded into the graph `t1` using the method
    `GraphOfTheGodsFactory.load`. A count of vertices (`V()`) has then been generated
    along with a `ValueMap` to display the contents of the graph. The output looks
    like this:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`TitanFactory.open`方法和Cassandra属性文件创建了一个Titan图。它被称为`t1`。上帝之图，一个提供给Titan的示例图，已经被加载到图`t1`中，使用了`GraphOfTheGodsFactory.load`方法。然后生成了顶点的计数(`V()`)以及`ValueMap`来显示图的内容。输出如下：
- en: '[PRE59]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'So, there are 12 vertices in the graph, each has a name and age element shown
    in the previous data. Having successfully created a graph, it is now possible
    to configure the previous graph traversal Gremlin command to use Apache Spark
    for processing. This is simply achieved by specifying the `SparkGraphComputer`
    in the traversal command. See the full *TinkerPop* diagram at the top of this
    chapter for architectural details. When this command is executed, you will see
    the task appear on the Spark cluster user interface:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，图中有12个顶点，每个顶点都有一个在前面数据中显示的名称和年龄元素。成功创建了一个图后，现在可以配置之前的图遍历Gremlin命令以使用Apache
    Spark进行处理。只需在遍历命令中指定`SparkGraphComputer`即可实现。有关架构细节，请参见本章顶部的完整*TinkerPop*图。执行此命令时，您将在Spark集群用户界面上看到任务出现：
- en: '[PRE60]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Using HBase
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用HBase
- en: 'When using HBase, the properties file needs to change. The following values
    have been taken from my `hbase.properties` file:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用HBase时，需要更改属性文件。以下数值取自我的`hbase.properties`文件：
- en: '[PRE61]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Remember that HBase uses Zookeeper for configuration purposes. So, the port
    number, and server for connection now becomes a `zookeeper` server, and `zookeeper`
    master port 2181\. The `znode` parent value in Zookeeper is also defined as the
    top level node `/hbase`. Of course, the backend type is now defined to be `hbase`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，HBase使用Zookeeper进行配置。因此，连接的端口号和服务器现在变成了`zookeeper`服务器和`zookeeper`主端口2181。在Zookeeper中，`znode`父值也被定义为顶级节点`/hbase`。当然，后端类型现在被定义为`hbase`。
- en: Also, the `GraphInputFormat` class has been changed to `HBaseInputFormat` to
    describe HBase as an input source. A Titan graph can now be created using this
    properties file, as shown in the last section. I won't repeat the graph creation
    here, as it will be the same as the last section. Next, I will move on to filesystem
    storage.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`GraphInputFormat`类已更改为`HBaseInputFormat`，以描述HBase作为输入源。现在可以使用此属性文件创建Titan图，就像上一节所示的那样。我不会在这里重复图的创建，因为它与上一节相同。接下来，我将转向文件系统存储。
- en: Using the filesystem
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用文件系统
- en: 'In order to run this example, I used a basic Gremlin shell (`bin/gremlin.sh`).
    Within the data directory of the Titan release, there are many example data file
    formats that can be loaded to create graphs. In this example, I will use the file
    called `grateful-dead.kryo`. So this time, the data will be loaded straight from
    the file to a graph without specifying a storage backend, such as Cassandra. The
    properties file that I will use only contains the following entries:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行这个例子，我使用了一个基本的Gremlin shell（`bin/gremlin.sh`）。在Titan发布的数据目录中，有许多可以加载以创建图形的示例数据文件格式。在这个例子中，我将使用名为`grateful-dead.kryo`的文件。因此，这次数据将直接从文件加载到图形中，而不需要指定存储后端，比如Cassandra。我将使用的属性文件只包含以下条目：
- en: '[PRE62]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Again, it uses the Hadoop Gremlin package but this time the graph input and
    output formats are defined as `GryoInputFormat` and `GryoOutputFormat`. The input
    location is specified to be the actual `kyro`-based file. So, the source for input
    and output is the file. So now, the Groovy script looks like this. First, the
    graph is created using the properties file. Then, a graph traversal is created,
    so that we can count vertices and see the structure:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，它使用了Hadoop Gremlin包，但这次图形输入和输出格式被定义为`GryoInputFormat`和`GryoOutputFormat`。输入位置被指定为实际的基于`kyro`的文件。因此，输入和输出的源是文件。现在，Groovy脚本看起来像这样。首先，使用属性文件创建图形。然后创建图形遍历，以便我们可以计算顶点并查看结构：
- en: '[PRE63]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Next, a vertex count is executed, which shows that there are over 800 vertices;
    and finally, a value map shows the structure of the data, which I have obviously
    clipped to save the space. But you can see the song name, type, and the performance
    details:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，执行了一个顶点计数，显示有800多个顶点；最后，一个值映射显示了数据的结构，我显然剪辑了一些以节省空间。但你可以看到歌曲名称、类型和表演细节：
- en: '[PRE64]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'This gives you a basic idea of the available functionality. I am sure that
    if you search the web, you will find more complex ways of using Spark with Titan.
    Take this for example:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这给您一个关于可用功能的基本概念。我相信，如果您搜索网络，您会发现更复杂的使用Spark与Titan的方法。以这个为例：
- en: '[PRE65]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The previous example specifies the use of the `SparkGraphComputer` class using
    the compute method. It also shows how the page rank vertex program, supplied with
    Titan, can be executed using the program method. This would modify your graph
    by adding page ranks to each vertex. I provide this as an example, as I am not
    convinced that it will work with Spark at this time.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子指定了使用`SparkGraphComputer`类的compute方法。它还展示了如何使用Titan提供的页面排名顶点程序来执行程序方法。这将通过为每个顶点添加页面排名来修改您的图形。我提供这个作为一个例子，因为我不确定它现在是否适用于Spark。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter has introduced the Titan graph database from Aurelius. It has shown
    how it can be installed and configured on a Linux cluster. Using a Titan Gremlin
    shell example, the graphs have been created, and stored in both HBase and Cassandra
    NoSQL databases. The choice of Titan storage option required will depend upon
    your project requirements; HBase HDFS based storage or Cassandra non HDFS based
    storage. This chapter has also shown that you can use the Gremlin shell both interactively
    to develop the graph scripts, and with Bash shell scripts so that you can run
    scheduled jobs with associated logging.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了Aurelius的Titan图形数据库。它展示了如何在Linux集群上安装和配置它。使用Titan Gremlin shell示例，图形已经被创建，并存储在HBase和Cassandra
    NoSQL数据库中。所需的Titan存储选项将取决于您的项目需求；HBase基于HDFS的存储或Cassandra非HDFS的存储。本章还表明，您可以交互地使用Gremlin
    shell开发图形脚本，并使用Bash shell脚本运行带有关联日志的定期作业。
- en: Simple Spark Scala code has been provided, which shows that Apache Spark can
    access the underlying tables that Titan creates on both HBase and Cassandra. This
    has been achieved by using the database connector modules provided by Cloudera
    (for HBase), and DataStax (for Cassandra). All example code and build scripts
    have been described along with the example output. I have included this Scala-based
    section to show you that the graph-based data can be accessed in Scala. The previous
    section processed data from the Gremlin shell, and used Spark as a processing
    backend. This section uses Spark as the main processing engine, and accesses Titan
    data from Spark. If the Gremlin shell was not suitable for your requirements,
    you might consider this approach. As Titan matures, so will the ways in which
    you can integrate Titan with Spark via Scala.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了简单的Spark Scala代码，显示了Apache Spark可以访问Titan在HBase和Cassandra上创建的基础表。这是通过使用Cloudera提供的数据库连接器模块（用于HBase）和DataStax（用于Cassandra）实现的。所有示例代码和构建脚本都已经描述，并附有示例输出。我包含了这个基于Scala的部分，以向您展示可以在Scala中访问基于图形的数据。前面的部分从Gremlin
    shell处理数据，并使用Spark作为处理后端。这一部分将Spark作为主要处理引擎，并从Spark访问Titan数据。如果Gremlin shell不适合您的需求，您可以考虑这种方法。随着Titan的成熟，您可以通过Scala以不同的方式将Titan与Spark集成。
- en: Finally, Titan's Gremlin shell has been used along with Apache Spark to demonstrate
    simple methods for creating, and accessing Titan-based graphs. Data has been stored
    on the file system, Cassandra, and HBase to do this.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Titan的Gremlin shell已经与Apache Spark一起使用，演示了创建和访问基于Titan的图形的简单方法。为此，数据已存储在文件系统、Cassandra和HBase上。
- en: Google groups are available for Aurelius and Gremlin users via the URLs at [https://groups.google.com/forum/#!forum/aureliusgraphs](https://groups.google.com/forum/#!forum/aureliusgraphs)
    and [https://groups.google.com/forum/#!forum/gremlin-users](https://groups.google.com/forum/#!forum/gremlin-users).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下网址，Aurelius和Gremlin用户可以使用Google群组：[https://groups.google.com/forum/#!forum/aureliusgraphs](https://groups.google.com/forum/#!forum/aureliusgraphs)
    和 [https://groups.google.com/forum/#!forum/gremlin-users](https://groups.google.com/forum/#!forum/gremlin-users)。
- en: Although the community seems smaller than other Apache projects, posting volume
    can be somewhat light, and it can be difficult to get a response to posts.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Titan社区似乎比其他Apache项目要小，帖子数量可能有些少，很难得到回复。
- en: DataStax, the people who created Cassandra, acquired Aurelius, the creators
    of Titan this year. The creators of Titan are now involved in the development
    of DataStax's DSE graph database, which may have a knock-on effect on Titan's
    development. Having said that, the 0.9.x Titan release has been created, and a
    1.0 release is expected.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 今年，创建了Cassandra的DataStax收购了创建Titan的Aurelius。Titan的创建者现在参与开发DataStax的DSE图数据库，这可能会对Titan的发展产生影响。话虽如此，0.9.x
    Titan版本已经发布，预计会有1.0版本的发布。
- en: So, having shown some of the Titan functionality with the help of an example
    with both Scala and Gremlin, I will close the chapter here. I wanted to show the
    pairing of Spark-based graph processing, and a graph storage system. I like open
    source systems for their speed of development and accessibility. I am not saying
    that Titan is the database for you, but it is a good example. If its future can
    be assured, and its community grows, then as it matures, it could offer a valuable
    resource.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过一个使用Scala和Gremlin的示例展示了Titan功能的一部分后，我将在此结束本章。我想展示基于Spark的图处理和图存储系统的配对。我喜欢开源系统的开发速度和可访问性。我并不是说Titan就是适合你的数据库，但它是一个很好的例子。如果它的未来能够得到保证，并且其社区不断壮大，那么随着其成熟，它可能会成为一个有价值的资源。
- en: 'Note that two versions of Spark have been used in this chapter: 1.3 and 1.2.1\.
    The earlier version was required, because it was apparently the only version that
    would work with Titan''s `SparkGraphComputer`, and so avoids Kyro serialization
    errors.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本章中使用了两个版本的Spark：1.3和1.2.1。较早的版本是必需的，因为显然它是唯一与Titan的`SparkGraphComputer`兼容的版本，因此避免了Kyro序列化错误。
- en: In the next chapter, extensions to the Apache Spark MLlib machine learning library
    will be examined in terms of the [http://h2o.ai/](http://h2o.ai/) H2O product.
    A neural-based deep learning example will be developed in Scala to demonstrate
    its potential functionality.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，将从[http://h2o.ai/](http://h2o.ai/) H2O产品的角度，研究对Apache Spark MLlib机器学习库的扩展。将使用Scala开发一个基于神经网络的深度学习示例，以展示其潜在功能。
