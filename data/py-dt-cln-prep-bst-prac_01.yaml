- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Data Ingestion Techniques
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据摄取技术
- en: '**Data ingestion** is a critical component of the data life cycle and sets
    the foundation for subsequent data transformation and cleaning. It involves the
    process of collecting and importing data from various sources into a storage system
    where it can be accessed and analyzed. Effective data ingestion is crucial for
    ensuring data quality, integrity, and availability, which directly impacts the
    efficiency and accuracy of data transformation and cleaning processes. In this
    chapter, we will dive deep into the different types of data sources, explore various
    data ingestion methods, and discuss their respective advantages, disadvantages,
    and real-world applications.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据摄取**是数据生命周期的一个关键组成部分，它为随后的数据转换和清理奠定了基础。它涉及从各种来源收集和导入数据到存储系统的过程，数据可以在此系统中访问和分析。有效的数据摄取对于确保数据的质量、完整性和可用性至关重要，这直接影响数据转换和清理过程的效率和准确性。在本章中，我们将深入探讨不同类型的数据源，探索各种数据摄取方法，并讨论它们各自的优缺点以及实际应用。'
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Ingesting data in batch mode
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量模式的数据摄取
- en: Ingesting data in streaming mode
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式模式的数据摄取
- en: Real-time versus semi-real-time ingestion
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时摄取与半实时摄取
- en: Data sources technologies
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源技术
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find all the code for the chapter in the following GitHub repository:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下GitHub仓库中找到本章的所有代码：
- en: '[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter01](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter01)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter01](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter01)'
- en: You can use your favorite IDE (VS Code, PyCharm, Google Colab, etc.) to write
    and execute your code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用你喜欢的IDE（VS Code、PyCharm、Google Colab等）来编写和执行代码。
- en: Ingesting data in batch mode
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量模式的数据摄取
- en: '**Batch ingestion** is a data processing technique whereby large volumes of
    data are collected, processed, and loaded into a system at scheduled intervals,
    rather than in real-time. This approach allows organizations to handle substantial
    amounts of data efficiently by grouping data into batches, which are then processed
    collectively. For example, a company might collect customer transaction data throughout
    the day and then process it in a single batch during off-peak hours. This method
    is particularly useful for organizations that need to process high volumes of
    data but do not require immediate analysis.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量摄取**是一种数据处理技术，其中大量数据在预定的时间间隔内收集、处理并加载到系统中，而不是实时进行。通过将数据分成批次处理，组织可以高效地处理大量数据。例如，一家公司可能会在一天内收集客户交易数据，然后在非高峰时段将其作为一个批次处理。这种方法对于需要处理大量数据但不要求即时分析的组织特别有用。'
- en: Batch ingestion is beneficial because it optimizes system resources by spreading
    the processing load across scheduled times, often when the system is underutilized.
    This reduces the strain on computational resources and can lower costs, especially
    in cloud-based environments where computing power is metered. Additionally, batch
    processing simplifies data management, as it allows for the easy application of
    consistent transformations and validations across large datasets. For organizations
    with regular, predictable data flows, batch ingestion provides a reliable, scalable,
    and cost-effective solution for data processing and analytics.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 批量摄取有益于优化系统资源，通过将处理负载分配到计划好的时间，通常是在系统空闲时进行。这减少了计算资源的压力，并且可以降低成本，尤其是在基于云的环境中，计算能力是按使用量计费的。此外，批处理简化了数据管理，因为它允许对大规模数据集应用一致的转换和验证。对于数据流规律可预测的组织，批量摄取提供了一种可靠、可扩展且具有成本效益的数据处理和分析解决方案。
- en: Let’s explore batch ingestion in more detail, starting with its advantages and
    disadvantages.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探讨批量摄取，从其优点和缺点开始。
- en: Advantages and disadvantages
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势与劣势
- en: 'Batch ingestion offers several notable advantages that make it an attractive
    choice for many data processing needs:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 批量摄取提供了几个显著的优势，使其成为许多数据处理需求的有吸引力的选择：
- en: '**Efficiency** is a key benefit, as batch processing allows for the handling
    of large volumes of data in a single operation, optimizing resource usage and
    minimizing overhead'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率**是一个关键优势，因为批处理允许在一次操作中处理大量数据，优化了资源使用并最小化了开销。'
- en: '**Cost-effectiveness** is another benefit, reducing the need for continuous
    processing resources and lowering operational expenses.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**是另一个好处，减少了对连续处理资源的需求，降低了运营成本。'
- en: '**Simplicity** makes it easier to manage and implement periodic data processing
    tasks compared to real-time ingestion, which often requires more complex infrastructure
    and management'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单性**使得相对于实时摄取，更容易管理和实现周期性数据处理任务，后者通常需要更复杂的基础设施和管理。'
- en: '**Robustness**, as batch processing is well-suited for performing complex data
    transformations and comprehensive data validation, ensuring high-quality, reliable
    data'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鲁棒性**，因为批处理非常适合进行复杂的数据转换和全面的数据验证，确保高质量、可靠的数据。'
- en: 'However, batch ingestion also comes with certain drawbacks:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，批处理摄取也带来一些缺点：
- en: There is an inherent delay between the generation of data and its availability
    for analysis, which can be a critical issue for applications requiring real-time
    insights.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据生成和可用性之间存在固有的延迟，对于需要实时洞察力的应用程序来说，这可能是一个关键问题。
- en: Resource spikes can occur during batch processing windows, leading to high resource
    usage and potential performance bottlenecks
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理窗口期间可能会出现资源峰值，导致高资源使用率和潜在的性能瓶颈。
- en: Scalability can also be a concern, as handling very large datasets may require
    significant infrastructure investment and management
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可伸缩性也可能是一个问题，处理非常大的数据集可能需要大量的基础设施投资和管理。
- en: Lastly, maintenance is a crucial aspect of batch ingestion; it demands careful
    scheduling and ongoing maintenance to ensure the timely and reliable execution
    of batch jobs
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，维护是批处理摄取的一个关键方面；它需要仔细的调度和持续的维护，以确保批处理作业的及时可靠执行。
- en: Let’s look at some common use cases for ingesting data in batch mode.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看批处理模式中数据摄取的一些常见用例。
- en: Common use cases for batch ingestion
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理摄取的常见用例
- en: Any data analytics platform such as data warehouses or data lakes requires regularly
    updated data for **Business Intelligence** (**BI**) and reporting. Batch ingestion
    is integral as it ensures that data is continually updated with the latest information,
    enabling businesses to perform comprehensive and up-to-date analyses. By processing
    data in batches, organizations can efficiently handle vast amounts of transactional
    and operational data, transforming it into a structured format suitable for querying
    and reporting. This supports BI initiatives, allowing analysts and decision-makers
    to generate insightful reports, track **Key Performance Indicators** (**KPIs**),
    and make data-driven decisions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 任何数据分析平台，如数据仓库或数据湖，都需要定期更新的数据用于**商业智能**（**BI**）和报告。批处理摄取至关重要，因为它确保数据始终使用最新信息进行更新，使企业能够进行全面和最新的分析。通过批处理处理数据，组织可以高效处理大量的交易和运营数据，将其转换为适合查询和报告的结构化格式。这支持BI倡议，允许分析师和决策者生成深刻见解的报告，跟踪**关键绩效指标**（**KPIs**），并做出数据驱动的决策。
- en: '**Extract, Transform, and Load** (**ETL**) processes are a cornerstone of data
    integration projects, and batch ingestion plays a crucial role in these workflows.
    In ETL processes, data is extracted from various sources, transformed to fit the
    operational needs of the target system, and loaded into a database or data warehouse.
    Batch processing allows for efficient handling of these steps, particularly when
    dealing with large datasets that require significant transformation and cleansing.
    This method is ideal for periodic data consolidation, where data from disparate
    systems is integrated to provide a unified view, supporting activities such as
    data migration, system integration, and master data management.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**提取、转换和加载**（**ETL**）过程是数据集成项目的基石，批量摄取在这些工作流程中起着关键作用。在ETL过程中，数据从各种来源提取，经过转换以适应目标系统的操作需求，然后加载到数据库或数据仓库中。批处理允许有效处理这些步骤，特别是在处理需要大量转换和清洗的大型数据集时。这种方法非常适合周期性数据整合，将来自不同系统的数据集成为统一视图，支持数据迁移、系统集成和主数据管理等活动。'
- en: Batch ingestion is also widely used for backups and archiving, which are critical
    processes for data preservation and disaster recovery. Periodic batch processing
    allows for the scheduled backup of databases, ensuring that all data is captured
    and securely stored at regular intervals. This approach minimizes the risk of
    data loss and provides a reliable restore point in case of system failures or
    data corruption. Additionally, batch processing is used for data archiving, where
    historical data is periodically moved from active systems to long-term storage
    solutions. This not only helps in managing storage costs but also ensures that
    important data is retained and can be retrieved for compliance, auditing, or historical
    analysis purposes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 批量数据导入在备份和归档中也得到了广泛应用，这些过程对于数据的保存和灾难恢复至关重要。定期的批处理允许按计划备份数据库，确保所有数据都能在定期间隔内被捕获并安全存储。这种方法最大限度地减少了数据丢失的风险，并在系统故障或数据损坏时提供可靠的恢复点。此外，批处理还用于数据归档，将历史数据定期从活动系统转移到长期存储解决方案中。这不仅有助于管理存储成本，还能确保重要数据被保留并可供合规、审计或历史分析使用。
- en: Batch ingestion use cases
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量数据导入的使用案例
- en: 'Batch ingestion is a methodical process involving several key steps: data extraction,
    data transformation, data loading, scheduling, and automation. To illustrate these
    steps, let’s explore a use case involving an investment bank that needs to process
    and analyze trading data for regulatory compliance and performance reporting.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 批量数据导入是一个系统化的过程，涉及多个关键步骤：数据提取、数据转换、数据加载、调度和自动化。为了说明这些步骤，让我们以一个投资银行为例，探讨它如何处理和分析交易数据以确保合规性和生成绩效报告。
- en: Batch ingestion in an investment bank
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 投资银行的批量数据导入
- en: An investment bank needs to collect, transform, and load trading data from various
    financial markets into a central data warehouse. This data will be used for generating
    daily compliance reports, evaluating trading strategies, and making informed investment
    decisions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 投资银行需要从多个金融市场收集、转换并加载交易数据到一个中央数据仓库。这些数据将用于生成每日合规报告、评估交易策略以及做出明智的投资决策。
- en: Data extraction
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据提取
- en: The first step is identifying the sources from which data will be extracted.
    For the investment bank, this includes trading systems, market data providers,
    and internal risk management systems. These sources contain critical data such
    as trade execution details, market prices, and risk assessments. Once the sources
    are identified, data is collected using connectors or scripts. This involves setting
    up data pipelines that extract data from trading systems, import real-time market
    data feeds, and pull risk metrics from internal systems. The extracted data is
    then temporarily stored in staging areas before processing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是识别数据提取的来源。对于投资银行来说，这包括交易系统、市场数据提供商和内部风险管理系统。这些来源包含关键数据，如交易执行细节、市场价格和风险评估。一旦确定了来源，数据就会通过连接器或脚本进行收集。这涉及建立数据管道，从交易系统提取数据，导入实时市场数据流，并从内部系统提取风险指标。提取的数据将暂时存储在暂存区，待处理。
- en: Data transformation
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据转换
- en: The extracted data often contains inconsistencies, duplicates, and missing values.
    Data cleaning is performed to remove duplicates, fill in missing information,
    and correct errors. For the investment bank, this ensures that trade records are
    accurate and complete, providing a reliable foundation for compliance reporting
    and performance analysis. After cleaning, the data undergoes transformations such
    as aggregations, joins, and calculations. For example, the investment bank might
    aggregate trade data to calculate daily trading volumes, join trade records with
    market data to analyze price movements, and calculate key metrics such as **Profit
    and Loss** (**P&L**) and risk exposure. The transformed data must be mapped to
    the schema of the target system. This involves aligning the data fields with the
    structure of the data warehouse. For instance, trade data might be mapped to tables
    representing transactions, market data, and risk metrics, ensuring seamless integration
    with the existing data model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的数据通常包含不一致、重复和缺失值。数据清洗会移除重复项、填补缺失信息并纠正错误。对于投资银行来说，这确保了交易记录的准确性和完整性，为合规报告和绩效分析提供了可靠的基础。清洗后，数据会进行如聚合、连接和计算等转换。例如，投资银行可能会聚合交易数据以计算每日交易量，连接交易记录与市场数据以分析价格波动，并计算关键指标如**盈亏**（**P&L**）和风险暴露。转换后的数据必须映射到目标系统的模式中。这涉及将数据字段与数据仓库的结构对齐。例如，交易数据可能会映射到代表交易、市场数据和风险指标的表中，确保与现有数据模型的无缝集成。
- en: Data loading
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据加载
- en: The transformed data is processed in batches, which allows the investment bank
    to handle large volumes of data efficiently, performing complex transformations
    and aggregations in a single run. Once processed, the data is loaded into the
    target storage system, such as a data warehouse or data lake. For the investment
    bank, this means loading the cleaned and transformed trading data into their data
    warehouse, where it can be accessed for compliance reporting and performance analysis.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的数据以批处理方式进行处理，这使得投资银行能够高效地处理大量数据，在一次运行中执行复杂的转换和聚合。一旦处理完成，数据会被加载到目标存储系统中，如数据仓库或数据湖。对于投资银行来说，这意味着将清洗和转换后的交易数据加载到他们的数据仓库中，从而可以用于合规报告和绩效分析。
- en: Scheduling and automation
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调度与自动化
- en: To ensure that the batch ingestion process runs smoothly and consistently, scheduling
    tools such as Apache Airflow or Cron jobs are used. These tools automate the data
    ingestion workflows, scheduling them to run at regular intervals, such as every
    night or every day. This allows the investment bank to have up-to-date data available
    for analysis without manual intervention. Implementing monitoring is crucial to
    track the success and performance of batch jobs. Monitoring tools provide insights
    into job execution, identifying any failures or performance bottlenecks. For the
    investment bank, this ensures that any issues in the data ingestion process are
    promptly detected and resolved, maintaining the integrity and reliability of the
    data pipeline.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保批量摄取过程顺利且一致地运行，通常会使用调度工具，如 Apache Airflow 或 Cron 作业。这些工具自动化数据摄取工作流，并安排在固定的时间间隔运行，例如每晚或每天一次。这使得投资银行能够在没有人工干预的情况下获得最新的数据以供分析。实施监控至关重要，用于跟踪批处理作业的成功与性能。监控工具提供作业执行的洞察，帮助识别任何失败或性能瓶颈。对于投资银行来说，这确保了数据摄取过程中出现的任何问题都能及时被发现并解决，从而维护数据管道的完整性和可靠性。
- en: Batch ingestion with an example
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带示例的批量摄取
- en: Let’s have a look at a simple example of a batch processing ingestion system
    written in Python. This example will simulate the ETL process. We’ll generate
    some mock data, process it in batches, and load it into a simulated database.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个用 Python 编写的简单批处理摄取系统示例。这个示例将模拟 ETL 过程。我们将生成一些模拟数据，按批处理方式处理它，并将其加载到模拟数据库中。
- en: 'You can find the code for this part in the GitHub repository at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/1.batch.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/1.batch.py).
    To run this example, we don’t need any bespoke library installation. We just need
    to ensure that we are running it in a standard Python environment (Python 3.x):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub仓库的[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/1.batch.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/1.batch.py)中找到这部分代码。要运行这个示例，我们不需要安装任何特别的库，只需要确保在标准的Python环境（Python
    3.x）中运行即可。
- en: 'We create a `generate_mock_data` function that generates a list of mock data
    records:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个`generate_mock_data`函数，用于生成模拟数据记录的列表：
- en: '[PRE0]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we create a batch processing function:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个批处理函数：
- en: '[PRE1]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This function takes the data, which is a list of data records to process, and
    `batch_size`, which represents the number of records per batch, as parameters.
    The function uses a `for` loop to iterate over the data in steps of `batch_size`.
    The `yield` keyword is used to generate batches of data, each of the `batch_size`
    size. A generator that yields batches of data is returned.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数将数据（数据记录的列表）和`batch_size`（每个批次的记录数）作为参数。该函数使用`for`循环以`batch_size`为步长遍历数据。使用`yield`关键字生成每个大小为`batch_size`的数据批次，返回一个生成器来产生数据批次。
- en: 'We create a `transform_data` function that transforms each record in the batch:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个`transform_data`函数，用于转换批次中的每一条记录：
- en: '[PRE2]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This function takes as an argument the batch, which is a list of data records
    to be transformed. The transformation logic is simple: a new `transformed_value`
    field is added to each record, which is the original value multiplied by 1.1\.
    At the end, we have a list of transformed records. Let’s have a look at some of
    our transformed records:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数的参数是批次，它是一个需要转换的数据记录列表。转换逻辑很简单：每个记录中添加一个新的`transformed_value`字段，它是原始值乘以1.1。最后，我们得到了一个转换后的记录列表。让我们看看一些转换后的记录：
- en: '[PRE3]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we create a `load_data` function to load the data. This function simulates
    loading each transformed record into a database:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个`load_data`函数来加载数据。这个函数模拟将每个转换后的记录加载到数据库中的过程：
- en: '[PRE4]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This function takes the batch as a parameter, which is a list of transformed
    data records that is ready to be loaded. Each record is printed to the console
    to simulate loading it into a database.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数以批次作为参数，批次是一个准备加载的转换后的数据记录列表。每条记录都会打印到控制台，以模拟将其加载到数据库中。
- en: 'Finally, we create a `main` function. This function calls all the aforementioned
    functions:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们创建一个`main`函数。这个函数调用了所有前面提到的函数：
- en: '[PRE5]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This function calls `generate_mock_data` to create the mock data and uses `process_in_batches`
    to divide the data into batches. For each batch, the function does the following:'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个函数调用`generate_mock_data`来生成模拟数据，并使用`process_in_batches`将数据分成批次。对于每个批次，函数执行以下操作：
- en: Transforms the batch using `transform_data`
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`transform_data`转换批次
- en: Prints the batch to show its contents before loading
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打印批处理，以显示加载前的内容
- en: Simulates loading the batch using `load_data`
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`load_data`模拟加载批次
- en: Now, let’s transition from batch processing to a streaming paradigm. In streaming,
    data is processed as it arrives, rather than in predefined batches.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从批处理转向流式处理模式。在流式处理中，数据在到达时被处理，而不是在预定义的批次中处理。
- en: Ingesting data in streaming mode
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以流式模式摄取数据
- en: '**Streaming ingestion** is a data processing technique whereby data is collected,
    processed, and loaded into a system in real-time, as it is generated. Unlike batch
    ingestion, which accumulates data for processing at scheduled intervals, streaming
    ingestion handles data continuously, allowing organizations to analyze and act
    on information immediately. For instance, a company might process customer transaction
    data the moment it occurs, enabling real-time insights and decision-making. This
    method is particularly useful for organizations that require up-to-the-minute
    data analysis, such as in financial trading, fraud detection, or sensor data monitoring.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**流式摄取**是一种数据处理技术，通过该技术，数据在生成时实时收集、处理并加载到系统中。与批处理摄取不同，批处理摄取是在预定的时间间隔内积累数据进行处理，流式摄取则是持续处理数据，允许组织立即分析和采取行动。例如，一家公司可以在客户交易发生时就处理交易数据，从而实现实时的洞察和决策。这种方法对于那些需要实时数据分析的组织尤其有用，比如金融交易、欺诈检测或传感器数据监控等领域。'
- en: Streaming ingestion is advantageous because it enables immediate processing
    of data, reducing latency and allowing organizations to react quickly to changing
    conditions. This is particularly beneficial in scenarios where timely responses
    are critical, such as detecting anomalies, personalizing user experiences, or
    responding to real-time events. Additionally, streaming can lead to more efficient
    resource utilization by distributing the processing load evenly over time, rather
    than concentrating it into specific batch windows. In cloud-based environments,
    this can also translate into cost savings, as resources can be scaled dynamically
    to match the real-time data flow. For organizations with irregular or unpredictable
    data flows, streaming ingestion offers a flexible, responsive, and scalable approach
    to data processing and analytics. Let’s look at some of its advantages and disadvantages.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据摄取具有优势，因为它可以实现数据的即时处理，减少延迟并使组织能够快速响应变化的环境条件。这在及时响应至关重要的场景中尤其有益，比如检测异常、个性化用户体验或应对实时事件。此外，流式处理可以通过将处理负载均匀地分布在时间上，而不是集中在特定的批处理窗口内，从而提高资源利用效率。在基于云的环境中，这也可以转化为成本节约，因为可以动态扩展资源以匹配实时数据流。对于数据流不规则或不可预测的组织，流数据摄取提供了一种灵活、响应迅速且可扩展的数据处理和分析方法。让我们来看一下它的一些优点和缺点。
- en: Advantages and disadvantages
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势与劣势
- en: 'Streaming ingestion offers several distinct advantages, making it an essential
    choice for specific data processing needs:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据摄取具有几个明显的优势，使其成为某些数据处理需求的必要选择：
- en: One of the primary benefits is the ability to obtain real-time insights from
    data. This immediacy is crucial for applications such as fraud detection, real-time
    analytics, and dynamic pricing, where timely data is vital.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中一个主要的好处是能够从数据中获取实时洞察。这种即时性对于诸如欺诈检测、实时分析和动态定价等应用至关重要，其中及时的数据是至关重要的。
- en: Streaming ingestion supports continuous data processing, allowing systems to
    handle data as it arrives, thereby reducing latency and improving responsiveness.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流数据摄取支持连续的数据处理，使系统能够在数据到达时进行处理，从而减少延迟并提高响应能力。
- en: This method is highly scalable, as well as capable of managing high-velocity
    data streams from multiple sources without significant delays.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法具有高度可扩展性，能够有效管理来自多个来源的高速数据流而不会产生显著延迟。
- en: 'However, streaming ingestion also presents some challenges:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，流数据摄取也面临一些挑战：
- en: Implementing a streaming ingestion system can be *complex*, requiring sophisticated
    infrastructure and specialized tools to manage data streams effectively.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施流数据摄取系统可能是*复杂的*，需要精密的基础设施和专门的工具来有效管理数据流。
- en: Continuous processing demands constant computational resources, which can be
    costly and resource-intensive.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续处理需要不断的计算资源，这可能是昂贵且资源密集型的。
- en: Ensuring data consistency and accuracy in a streaming environment can be difficult
    due to the constant influx of data and the potential for out-of-order or duplicate
    records
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在流式环境中确保数据一致性和准确性可能很困难，因为数据不断涌入，并且可能会出现乱序或重复记录。
- en: Let’s look at common use cases for ingesting data in batch mode.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下批量模式下数据摄取的常见应用场景。
- en: Common use cases for streaming ingestion
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流数据摄取的常见应用场景
- en: While batch processing is well-suited for periodic, large-scale data updates
    and transformations, streaming data ingestion is crucial for real-time data analytics
    and applications that require immediate insights. Here are some common use cases
    for streaming data ingestion.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然批量处理非常适合周期性的大规模数据更新和转换，但流数据摄取对实时数据分析和需要即时洞察的应用至关重要。以下是流数据摄取的常见应用场景。
- en: Real-time fraud detection and security monitoring
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实时欺诈检测和安全监控
- en: Financial institutions use streaming data to detect fraudulent activities by
    analyzing transaction data in real-time. Immediate anomaly detection helps prevent
    fraud before it can cause significant damage. Streaming data is used in cybersecurity
    to detect and respond to threats immediately. Continuous monitoring of network
    traffic, user behavior, and system logs helps identify and mitigate security breaches
    as they occur.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 金融机构使用流数据来实时检测欺诈活动，分析交易数据。即时异常检测帮助在欺诈造成重大损失之前予以阻止。流数据还用于网络安全领域，以便立即检测和应对威胁。对网络流量、用户行为和系统日志的持续监控有助于在发生安全漏洞时进行识别和缓解。
- en: IoT and sensor data
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 物联网和传感器数据
- en: In manufacturing, streaming data from sensors on machinery allows for predictive
    maintenance. By continuously monitoring equipment health, companies can prevent
    breakdowns and optimize maintenance schedules.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在制造业中，来自机器设备传感器的流式数据可以实现预测性维护。通过持续监控设备健康状况，公司能够防止设备故障并优化维护计划。
- en: Another interesting application in the IoT and sensors space is **smart cities**.
    Streaming data from various sensors across a city (traffic, weather, pollution,
    etc.) helps in managing city operations in real-time, improving services such
    as traffic management and emergency response.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网和传感器领域的另一个有趣应用是**智慧城市**。来自城市中各种传感器（如交通、天气、污染等）的流式数据有助于实时管理城市运营，改善交通管理、应急响应等服务。
- en: Online recommendations and personalization
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在线推荐与个性化
- en: Streaming data enables e-commerce platforms to provide real-time recommendations
    to users based on their current browsing and purchasing behavior. This enhances
    user experience and increases sales. Platforms such as Netflix and Spotify use
    streaming data to update recommendations as users interact with the service, providing
    personalized content suggestions in real-time.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 流式数据使电子商务平台能够根据用户当前的浏览和购买行为，向用户提供实时推荐。这提升了用户体验，并增加了销售额。像Netflix和Spotify这样的平台利用流式数据在用户与服务互动时更新推荐，实时提供个性化内容建议。
- en: Financial market data
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 金融市场数据
- en: Stock traders rely on streaming data for up-to-the-second information on stock
    prices and market conditions to make informed trading decisions. Automated trading
    systems use streaming data to execute trades based on predefined criteria, requiring
    real-time data processing for optimal performance.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 股票交易员依赖流式数据获取关于股价和市场状况的最新信息，从而做出明智的交易决策。自动化交易系统利用流式数据根据预设标准执行交易，这需要实时数据处理以确保最佳性能。
- en: Telecommunications
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 电信
- en: Telecommunication companies use streaming data to monitor network performance
    and usage in real-time, ensuring optimal service quality and quick resolution
    of issues. Streaming data also helps in tracking customer interactions and service
    usage in real-time, enabling personalized customer support and improving the overall
    experience.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 电信公司使用流式数据实时监控网络性能和使用情况，确保服务质量最佳并能迅速解决问题。流式数据还帮助跟踪客户互动和服务使用情况，实现个性化客户支持，提升整体体验。
- en: Real-time logistics and supply chain management
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实时物流与供应链管理
- en: Streaming data from GPS devices allows logistics companies to track vehicle
    locations and optimize routes in real-time, improving delivery efficiency. Real-time
    inventory tracking helps businesses maintain optimal stock levels, reducing overstock
    and stockouts while ensuring timely replenishment.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 来自GPS设备的流式数据使物流公司能够实时追踪车辆位置并优化路线，提高交付效率。实时库存跟踪帮助企业维持最佳库存水平，减少过度库存和缺货现象，同时确保及时补货。
- en: Streaming ingestion in an e-commerce platform
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 电子商务平台中的流式数据摄取
- en: 'Streaming ingestion is a methodical process involving several key steps: data
    extraction, data transformation, data loading, and monitoring and alerting. To
    illustrate these steps, let’s explore a use case involving an e-commerce platform
    that needs to process and analyze user activity data in real-time for personalized
    recommendations and dynamic inventory management.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 流式数据摄取是一个有序的过程，涉及多个关键步骤：数据提取、数据转换、数据加载以及监控与告警。为了说明这些步骤，让我们探讨一个电子商务平台的用例，该平台需要实时处理和分析用户活动数据，以实现个性化推荐和动态库存管理。
- en: An e-commerce platform needs to collect, transform, and load user activity data
    from various sources such as website clicks, search queries, and purchase transactions
    into a central system. This data will be used for generating real-time personalized
    recommendations, monitoring user behavior, and managing inventory dynamically.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个电子商务平台需要收集、转换并加载来自各种来源的用户活动数据，如网站点击、搜索查询和购买交易，将其导入到中央系统中。这些数据将用于生成实时个性化推荐、监控用户行为，并动态管理库存。
- en: Data extraction
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据提取
- en: This is the first step is identifying the sources from which data will be extracted.
    For the e-commerce platform, this includes web servers, mobile apps, and third-party
    analytics services. These sources contain critical data such as user clicks, search
    queries, and transaction details. Once the sources are identified, data is collected
    using streaming connectors or APIs. This involves setting up data pipelines that
    extract data from web servers, mobile apps, and analytics services in real-time.
    The extracted data is then streamed to processing systems such as Apache Kafka
    or AWS Kinesis.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是识别数据将从哪些来源提取。对于电子商务平台，这些来源包括 Web 服务器、移动应用程序和第三方分析服务。这些来源包含关键信息，如用户点击、搜索查询和交易详情。一旦确定了数据来源，就可以使用流式连接器或
    API 进行数据采集。这涉及到设置数据管道，从 Web 服务器、移动应用程序和分析服务中实时提取数据。提取的数据随后会流式传输到如 Apache Kafka
    或 AWS Kinesis 等处理系统。
- en: Data transformation
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据转换
- en: The extracted data often contains inconsistencies and noise. Real-time data
    cleaning is performed to filter out irrelevant information, handle missing values,
    and correct errors. For the e-commerce platform, this ensures that user activity
    records are accurate and relevant for analysis. After cleaning, the data undergoes
    transformations such as parsing, enrichment, and aggregation. For example, the
    e-commerce platform might parse user clickstream data to identify browsing patterns,
    enrich transaction data with product details, and aggregate search queries to
    identify trending products. The transformed data must be mapped to the schema
    of the target system. This involves aligning the data fields with the structure
    of the real-time analytics system. For instance, user activity data might be mapped
    to tables representing sessions, products, and user profiles, ensuring seamless
    integration with the existing data model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的数据通常包含不一致和噪音。实时数据清洗会过滤掉不相关的信息，处理缺失值并纠正错误。对于电子商务平台来说，这确保了用户活动记录的准确性，并且对分析有意义。清洗之后，数据会进行诸如解析、丰富和聚合等转换。例如，电子商务平台可能会解析用户点击流数据，以识别浏览模式，用产品详情丰富交易数据，并聚合搜索查询以识别流行的产品。转换后的数据必须映射到目标系统的架构中。这涉及到将数据字段与实时分析系统的结构对齐。例如，用户活动数据可能会映射到表示会话、产品和用户档案的表格中，确保与现有数据模型的无缝集成。
- en: Data loading
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据加载
- en: The transformed data is processed continuously using tools such as Apache Flink
    or Apache Spark Streaming. Continuous processing allows the e-commerce platform
    to handle high-velocity data streams efficiently, performing transformations and
    aggregations in real-time. Once processed, the data is loaded into the target
    storage system, such as a real-time database or analytics engine, where it can
    be accessed for personalized recommendations and dynamic inventory management.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的数据通过 Apache Flink 或 Apache Spark Streaming 等工具持续处理。持续处理使电子商务平台能够高效地处理高速数据流，实时执行转换和聚合。一旦处理完成，数据会被加载到目标存储系统中，如实时数据库或分析引擎，在那里可以进行个性化推荐和动态库存管理。
- en: Monitoring and alerting
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控与告警
- en: To ensure that the streaming ingestion process runs smoothly and consistently,
    monitoring tools such as Prometheus or Grafana are used. These tools provide real-time
    insights into the performance and health of the data ingestion pipelines, identifying
    any failures or performance bottlenecks. Implementing alerting mechanisms is crucial
    to promptly detect and resolve any issues in the streaming ingestion process.
    For the e-commerce platform, this ensures that any disruptions in data flow are
    quickly addressed, maintaining the integrity and reliability of the data pipeline.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保流式数据摄取过程顺利且一致地运行，使用 Prometheus 或 Grafana 等监控工具。这些工具提供有关数据摄取管道性能和健康状况的实时洞察，能够识别任何故障或性能瓶颈。实现告警机制至关重要，以便及时发现并解决流式数据摄取过程中的问题。对于电子商务平台来说，这确保了数据流中断能迅速得到解决，从而保持数据管道的完整性和可靠性。
- en: Streaming ingestion with an example
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式数据摄取示例
- en: 'As we said, in streaming, data is processed as it arrives rather than in predefined
    batches. Let’s modify the batch example to transition to a streaming paradigm.
    For simplicity, we will generate data continuously, process it immediately upon
    arrival, transform it, and then load it:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所说，在流式处理中，数据是随着到达而被处理，而不是按照预定义的批次进行处理。让我们修改批处理示例，转向流式处理范式。为了简化起见，我们将持续生成数据，数据一到达就立即处理，进行转换，然后加载：
- en: 'The `generate_mock_data` function generates records *continuously* using a
    generator and simulates a delay between each record:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`generate_mock_data` 函数使用生成器*持续*生成记录，并模拟每条记录之间的延迟：'
- en: '[PRE6]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `process_stream` function processes each record as it arrives from the
    data generator, without waiting for a batch to be filled:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`process_stream` 函数处理来自数据生成器的每条记录，而无需等待批次填充：'
- en: '[PRE7]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `transform_data` function transforms each record individually as it arrives:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`transform_data` 函数在每条记录到达时分别进行转换：'
- en: '[PRE8]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `load_data` function simulates loading data by processing each record as
    it arrives, instead of processing each record within a batch as before:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`load_data` 函数模拟通过处理每一条记录来加载数据，而不是像以前那样在批次中处理每一条记录：'
- en: '[PRE9]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let’s move from real-time to semi-real-time processing, which you can think
    it as batch processing over short intervals. It is usually called micro-batch
    processing.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从实时处理转向半实时处理，可以将其视为在短时间间隔内的批处理。这通常称为微批处理。
- en: Real-time versus semi-real-time ingestion
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时与半实时数据摄取
- en: Real-time ingestion refers to the process of collecting, processing, and loading
    data almost instantaneously as it is generated, as we have discussed. This approach
    is critical for applications that require immediate insights and actions, such
    as fraud detection, stock trading, and live monitoring systems. Real-time ingestion
    provides *the lowest latency*, enabling businesses to react to events as they
    occur. *However, it demands robust infrastructure and continuous resource allocation,
    making it complex and potentially expensive* *to maintain*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 实时数据摄取是指几乎即时地收集、处理和加载数据的过程，正如我们所讨论的那样。这种方法对于需要立即洞察和行动的应用程序至关重要，例如欺诈检测、股票交易和实时监控系统。实时数据摄取提供了*最低延迟*，使企业能够在事件发生时立即做出反应。*然而，它需要强大的基础设施和持续的资源分配，这使得其维护复杂且可能昂贵*。
- en: Semi-real-time ingestion, on the other hand, also known as near real-time ingestion,
    involves processing data with *minimal delay*, typically in seconds or minutes,
    rather than instantly. This approach *strikes a balance between real-time and
    batch processing*, providing timely insights while reducing the resource intensity
    and complexity associated with true real-time systems. Semi-real-time ingestion
    is suitable for applications such as social media monitoring, customer feedback
    analysis, and operational dashboards, where near-immediate data processing is
    beneficial but not critically time-sensitive.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 半实时数据摄取，另一方面，也称为接近实时数据摄取，涉及在*最小延迟*下处理数据，通常是几秒钟或几分钟，而不是即时处理。这种方法*在实时和批处理之间达成平衡*，提供及时的洞察，同时减少了与真正的实时系统相关的资源强度和复杂性。半实时数据摄取适用于社交媒体监控、客户反馈分析和运营仪表板等应用，其中接近即时的数据处理是有益的，但并非至关重要的时间敏感。
- en: Common use cases for near-real-time ingestion
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接近实时摄取的常见用例
- en: Let’s look at some of the common use cases wherein we can use near-real-time
    ingestion.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些常见的用例，其中可以使用接近实时的数据摄取。
- en: Real-time analytics
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实时分析
- en: Streaming enables organizations to continuously monitor data as it flows in,
    allowing for real-time dashboards and visualizations. This is critical in industries
    such as finance, where stock prices, market trends, and trading activities need
    to be tracked live. It also allows for instant report generation, facilitating
    timely decision-making and reducing the latency between data generation and analysis.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 流式处理使组织能够持续监控数据流动，允许实时仪表板和可视化。这在金融行业尤为重要，因为股票价格、市场趋势和交易活动需要实时跟踪。它还允许即时生成报告，促进及时决策，并减少数据生成与分析之间的延迟。
- en: Social media and sentiment analysis
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 社交媒体和情感分析
- en: Companies track mentions and sentiments on social media in real-time to manage
    brand reputation and respond to customer feedback promptly. Streaming data allows
    for the continuous analysis of public sentiment towards brands, products, or events,
    providing immediate insights that can influence marketing and PR strategies.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 企业实时跟踪社交媒体上的提及和情感，以管理品牌声誉并迅速回应客户反馈。流式数据使得可以持续分析公众对品牌、产品或事件的情感，提供即时洞察，这些洞察可能影响营销和公关策略。
- en: Customer experience enhancement
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户体验提升
- en: Near-real-time processing allows support teams to access up-to-date information
    on customer issues and behavior, enabling quicker and more accurate responses
    to customer inquiries. Businesses can also use near-real-time data to update customer
    profiles and trigger personalized marketing messages, such as emails or notifications,
    shortly after a customer interacts with their website or app.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 近实时处理使支持团队能够访问客户问题和行为的最新信息，从而更快更准确地回应客户询问。企业还可以利用近实时数据更新客户档案，并在客户与网站或应用程序互动后不久触发个性化的营销信息，如电子邮件或通知。
- en: Semi-real-time mode with an example
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有示例的半实时模式
- en: 'Transitioning from real-time to semi-real-time data processing involves adjusting
    the example to introduce a more structured approach to handling data updates,
    rather than processing each record immediately upon arrival. This can be achieved
    by batching data updates over short intervals, which allows for more efficient
    processing while still maintaining a responsive data processing pipeline. Let’s
    have a look at the example and as always, you can find the code in the GitHub
    repository [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/3.semi_real_time.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/3.semi_real_time.py):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从实时数据处理过渡到半实时数据处理涉及调整示例，引入更结构化的方式来处理数据更新，而不是在每条记录到达时立即处理。可以通过在短时间间隔内批量处理数据更新来实现，这样可以更高效地处理数据，同时保持响应式的数据处理管道。让我们看看这个示例，和往常一样，您可以在
    GitHub 仓库中找到代码 [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/3.semi_real_time.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/3.semi_real_time.py)：
- en: For generating mock data continuously, there are no changes from the previous
    example. This continuously generates mock data records with a slight delay (`time.sleep(0.1)`).
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连续生成模拟数据，与之前的示例没有变化。这个过程会持续生成模拟数据记录，并有轻微的延迟（`time.sleep(0.1)`）。
- en: 'For processing in semi-real-time, we can use a deque to buffer incoming records.
    This function processes records when either the specified time interval has elapsed,
    or the buffer reaches a specified size (`batch_size`). Then, it converts the deque
    to a list (`list(buffer)`) before passing it to `transform_data`, ensuring the
    data is processed in a batch:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于半实时处理，我们可以使用一个双端队列（deque）来缓存传入的记录。这个功能会在指定的时间间隔过去，或者缓冲区达到指定大小（`batch_size`）时处理记录。然后，它将双端队列转换为列表（`list(buffer)`），再传递给
    `transform_data`，确保数据是以批处理的方式进行处理：
- en: '[PRE10]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Check whether the interval has elapsed, or the buffer size has been reached:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查间隔是否已过，或缓冲区大小是否已达到：
- en: '[PRE11]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Process and clear the buffer:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理并清理缓冲区：
- en: '[PRE12]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Then, we transform each record in the batch. There are no changes from the previous
    example and we load the data.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们转换批次中的每一条记录。与之前的示例没有变化，我们加载数据。
- en: When you run this code, it continuously generates mock data records. Records
    are buffered until either the specified time interval (`interval`) has elapsed,
    or the buffer reaches the specified size (`batch_size`). Once the conditions are
    met, the buffered records are processed as a batch, transformed, and then “loaded”
    (printed) into the simulated database.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行此代码时，它会持续生成模拟数据记录。记录会被缓冲，直到指定的时间间隔（`interval`）过去，或者缓冲区达到指定的大小（`batch_size`）。一旦条件满足，缓冲的记录将作为一个批次进行处理、转换，然后“加载”（打印）到模拟数据库中。
- en: When discussing the different types of data sources that are suitable for batch,
    streaming, or semi-real-time streaming processing, it’s essential to consider
    the diversity and characteristics of these sources. Data can originate from various
    sources, such as databases, logs, IoT devices, social media, or sensors, as we
    will see in the next section.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论适合批处理、流式处理或半实时流处理的不同类型数据源时，必须考虑这些数据源的多样性和特性。数据可以来自多种来源，例如数据库、日志、物联网设备、社交媒体或传感器，正如我们将在下一部分看到的那样。
- en: Data source solutions
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据源解决方案
- en: In the world of modern data analytics and processing, the diversity of data
    sources available for ingestion spans a wide spectrum. From traditional file formats
    such as CSV, JSON, and XML to robust database systems encompassing both SQL and
    NoSQL variants, the landscape expands further to include dynamic APIs such as
    REST, facilitating real-time data retrieval. Message queues such as Kafka offer
    scalable solutions for handling event-driven data while streaming services such
    as Kinesis and pub/sub enable continuous data flows crucial for applications demanding
    immediate insights. Understanding and effectively harnessing these diverse data
    ingestion sources is fundamental to building robust data pipelines that support
    a broad array of analytical and operational needs.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代数据分析和处理的世界中，可供摄取的数据源种类繁多，涵盖了从传统文件格式（如 CSV、JSON 和 XML）到强大的数据库系统（包括 SQL 和 NoSQL
    系统）的广泛范围。此外，还包括动态 API（如 REST），以便实时获取数据。像 Kafka 这样的消息队列提供了可扩展的解决方案来处理事件驱动的数据，而
    Kinesis 和 pub/sub 等流媒体服务则支持连续的数据流，这对于要求即时洞察的应用至关重要。理解并有效利用这些多样化的数据摄取来源，是构建支持广泛分析和操作需求的强大数据管道的基础。
- en: Let’s start with event processing.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从事件处理开始。
- en: Event data processing solution
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事件数据处理解决方案
- en: In a real-time processing system, data is ingested, processed, and responded
    to almost instantaneously, as we’ve discussed. Real-time processing systems often
    use message queues to handle incoming data streams and ensure that data is processed
    in the order it is received, without delays.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在实时处理系统中，数据几乎是即时摄取、处理并响应的，正如我们之前讨论过的。实时处理系统通常使用消息队列来处理传入的数据流，并确保数据按接收顺序处理，不会产生延迟。
- en: 'The following Python code demonstrates a basic example of using a message queue
    for processing messages, which is a foundational concept in both real-time and
    semi-real-time data processing systems. The `Queue` class from Python’s `queue`
    module is used to create a queue—a data structure that follows the `message 0`,
    `message 1`, etc.) are added to a queue. This mimics a scenario wherein events
    or tasks are generated and need to be processed in the order they arrive. Let’s
    have a look at each part of the code. You can find the code file at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/4.work_with_queue.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/4.work_with_queue.py):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 Python 代码展示了使用消息队列处理消息的基本示例，这是实时和半实时数据处理系统中的基础概念。Python `queue` 模块中的 `Queue`
    类用于创建队列——一个遵循先进先出（FIFO）原则的数据结构。在队列中，消息（例如`message 0`、`message 1` 等）会按照顺序被添加。这模拟了事件或任务的生成，需要按到达的顺序进行处理。我们来看看代码的每个部分。你可以在
    [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/4.work_with_queue.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/4.work_with_queue.py)
    找到完整代码文件：
- en: 'The `read_message_queue()` function initializes a queue object `q` using the
    `Queue` class from the `queue` module:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`read_message_queue()` 函数使用来自 `queue` 模块的 `Queue` 类初始化一个队列对象 `q`：'
- en: '[PRE13]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This loop adds 10 messages to the queue. Each message is a string in the format
    message `i`, where `i` ranges from 0 to 9:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个循环将10条消息添加到队列中。每条消息是一个格式为 `message i` 的字符串，其中 `i` 从0到9不等：
- en: '[PRE14]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This loop continuously retrieves and processes messages from the queue until
    it is empty. `q.get()` retrieves a message from the queue, and `q.task_done()`
    signals that the retrieved message has been processed:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个循环会持续从队列中获取并处理消息，直到队列为空。`q.get()` 从队列中获取消息，`q.task_done()` 表示已处理完获取的消息：
- en: '[PRE15]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following function takes a message as input and prints it to the console,
    simulating the processing of the message:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下函数接收一个消息作为输入，并将其打印到控制台，模拟消息的处理：
- en: '[PRE16]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Call the `read_message_queue` function:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `read_message_queue` 函数：
- en: '[PRE17]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, the `read_message_queue` function reads messages from the queue and processes
    them one by one using the `process_message` function. This demonstrates how event-based
    systems handle tasks—by placing them in a queue and processing each task as it
    becomes available.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`read_message_queue` 函数从队列中读取消息，并使用 `process_message` 函数逐一处理这些消息。这演示了基于事件的系统如何处理任务——通过将它们放入队列并在任务变得可用时进行处理。
- en: The `while not q.empty()` loop ensures that each message is processed in the
    exact order it was added to the queue. This is crucial in many real-world applications
    where the order of processing matters, such as in handling user requests or processing
    logs.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`while not q.empty()` 循环确保每条消息都按照加入队列的顺序被处理。这在许多现实世界的应用中至关重要，例如处理用户请求或日志时，处理顺序非常重要。'
- en: The `q.task_done()` method signals that a message has been processed. This is
    important in real-world systems where tracking the completion of tasks is necessary
    for ensuring reliability and correctness, especially in systems with multiple
    workers or threads.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`q.task_done()` 方法表示消息已被处理。在现实世界的系统中，这一点非常重要，因为跟踪任务完成情况对于确保系统的可靠性和正确性至关重要，尤其是在有多个工作线程或线程的系统中。'
- en: In real-world applications, message queues are often integrated into more sophisticated
    data streaming platforms to ensure scalability, fault tolerance, and high availability.
    For instance, in real-time data processing, platforms such as Kafka and AWS Kinesis
    come into play.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的应用中，消息队列通常会与更复杂的数据流平台集成，以确保可扩展性、容错性和高可用性。例如，在实时数据处理中，像 Kafka 和 AWS Kinesis
    这样的平台就发挥了作用。
- en: Ingesting event data with Apache Kafka
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Apache Kafka 摄取事件数据
- en: There are different technologies to ingest and handle event data. One of the
    technologies we will discuss is Apache Kafka. Kafka is an open source distributed
    event streaming platform first developed by LinkedIn and later donated to the
    Apache Software Foundation. It is designed to handle large amounts of data in
    real-time and provides a scalable and fault-tolerant system for processing and
    storing streams.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种技术可用于摄取和处理事件数据。我们将讨论的一种技术是 Apache Kafka。Kafka 是一个开源的分布式事件流平台，最初由 LinkedIn
    开发，后来捐赠给了 Apache 软件基金会。它被设计用来实时处理大量数据，并提供一个可扩展、容错的系统，用于处理和存储数据流。
- en: '![Figure 1.1 – Components of Apache Kafka](img/B19801_01_1.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – Apache Kafka 的组件](img/B19801_01_1.jpg)'
- en: Figure 1.1 – Components of Apache Kafka
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – Apache Kafka 的组件
- en: 'Let’s see the different components of Apache Kafka:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 Apache Kafka 的不同组件：
- en: '**Ingestion***:* Data streams can be ingested into Kafka using Kafka producers.
    Producers are applications that write data to Kafka topics, which are logical
    channels that can hold and organize data streams.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摄取**:*：可以使用 Kafka 生产者将数据流摄取到 Kafka 中。生产者是将数据写入 Kafka 主题的应用程序，Kafka 主题是可以存储和组织数据流的逻辑通道。'
- en: '**Processing**: Kafka can process streams of data using Kafka Streams, a client
    library for building real-time stream processing applications. Kafka Streams allows
    developers to build custom stream-processing applications that can perform transformations,
    aggregations, and other operations on data streams.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理**：Kafka 可以使用 Kafka Streams 处理数据流，Kafka Streams 是一个用于构建实时流处理应用程序的客户端库。Kafka
    Streams 允许开发人员构建自定义的流处理应用程序，可以对数据流进行转换、聚合和其他操作。'
- en: '**Storage***:* Kafka stores data streams in distributed, fault-tolerant clusters
    called Kafka brokers. Brokers store the data streams in partitions, which are
    replicated across numerous brokers for fault tolerance.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储**:*：Kafka 将数据流存储在分布式的、容错的集群中，这些集群被称为 Kafka 经纪人（brokers）。经纪人将数据流存储在分区中，这些分区在多个经纪人之间进行复制，以确保容错性。'
- en: '**Consumption***:* Data streams can be consumed from Kafka using Kafka consumers.
    Consumers are applications that read data from Kafka topics and process it as
    needed.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费**:*：可以使用 Kafka 消费者从 Kafka 获取数据流。消费者是从 Kafka 主题读取数据并根据需要处理数据的应用程序。'
- en: Several libraries can be used to interact with Apache Kafka in Python; we will
    explore the most popular ones in the next section.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用多个库与 Apache Kafka 进行交互；我们将在下一节中探讨其中最流行的几个。
- en: Which library should you use for your use case?
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应该使用哪个库来处理你的使用案例？
- en: '`Kafka-Python` is a pure Python implementation of Kafka’s protocol, offering
    a more Pythonic interface for interacting with Kafka. It is designed to be simple
    and easy to use, making it particularly appealing for beginners. One of its primary
    advantages is its simplicity, making it easier to install and use compared to
    other Kafka libraries. Kafka-Python is flexible and well-suited for small to medium-sized
    applications, providing the essential features needed for basic Kafka operations
    without the complexity of additional dependencies. Its pure Python nature means
    that it does not rely on any external libraries beyond Python itself, streamlining
    the installation and setup process.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`Kafka-Python` 是 Kafka 协议的纯 Python 实现，提供了一个更加 Pythonic 的接口，用于与 Kafka 交互。它设计简洁、易于使用，尤其适合初学者。它的主要优点之一就是简便性，相比其他
    Kafka 库，它更易于安装和使用。Kafka-Python 灵活且非常适合小型到中型应用，提供了进行基本 Kafka 操作所需的必要功能，而不涉及额外的复杂依赖。由于它是纯
    Python 库，因此不依赖于任何超出 Python 本身的外部库，从而简化了安装和设置过程。'
- en: '`Confluent-kafka-python` is a library developed and maintained by Confluent,
    the original creator of Kafka. It stands out for its high-performance and low-latency
    capabilities, leveraging the `librdkafka` C library for efficient operations.
    The library offers extensive configuration options akin to the Java Kafka client
    and closely aligns with Kafka’s feature set, often pioneering support for new
    Kafka features. It is particularly well-suited for production environments where
    both performance and stability are crucial, making it an ideal choice for handling
    high-throughput data streams and ensuring reliable message processing in critical
    applications.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`Confluent-kafka-python` 是由 Confluent（Kafka 的原创开发者）开发并维护的一个库。它以高性能和低延迟能力而著称，利用
    `librdkafka` C 库进行高效操作。该库提供了类似 Java Kafka 客户端的广泛配置选项，并与 Kafka 的功能集高度契合，常常率先支持
    Kafka 的新特性。它特别适用于生产环境，在性能和稳定性至关重要的情况下，是处理高吞吐量数据流和确保关键应用中可靠消息处理的理想选择。'
- en: Transitioning from event data processing to databases involves shifting focus
    from real-time data streams to persistent data storage and retrieval. While event
    data processing emphasizes handling continuous streams of data in near real-time
    for immediate insights or actions, databases are structured repositories designed
    for storing and managing data over the long term.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 从事件数据处理转向数据库涉及将重点从实时数据流转移到持久数据存储和检索。事件数据处理强调处理连续的数据流，以便快速获得洞察或采取行动，而数据库则是结构化的存储库，旨在长期存储和管理数据。
- en: Ingesting data from databases
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从数据库中摄取数据
- en: Databases, whether relational or non-relational, serve as foundational components
    in data management systems. Classic databases and NoSQL databases are two different
    types of database management systems that differ in architecture and characteristics.
    A classic database, also known as a relational database, stores data in tables
    with a fixed schema. Classic databases are ideal for applications that require
    complex querying and transactional consistency, such as financial systems or enterprise
    applications.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库，无论是关系型还是非关系型，都是数据管理系统的基础组成部分。经典数据库和NoSQL数据库是两种不同类型的数据库管理系统，它们在架构和特性上有所不同。经典数据库，也称为关系型数据库，将数据存储在具有固定模式的表格中。经典数据库非常适合需要复杂查询和事务一致性的应用，例如金融系统或企业应用。
- en: On the other hand, NoSQL databases do not store data in tables with a fixed
    schema. They use a document-based approach to store data in a flexible schema
    format. They are designed to be scalable and handle large amounts of data, with
    a focus on high-performance data retrieval. NoSQL databases are well-suited for
    applications that require high performance and scalability, such as real-time
    analytics, content management systems, and e-commerce platforms.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，NoSQL 数据库不会将数据存储在具有固定模式的表格中。它们采用基于文档的方式，以灵活的模式格式存储数据。它们设计为可扩展并能处理大量数据，重点是高性能的数据检索。NoSQL
    数据库非常适合需要高性能和可扩展性的应用，例如实时分析、内容管理系统和电子商务平台。
- en: Let’s start with relational databases.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从关系型数据库开始。
- en: Performing data ingestion from a relational database
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从关系型数据库执行数据摄取
- en: 'Relational databases are useful for batch ETL processes where structured data
    from various sources needs consolidation, transformation, and loading into a data
    warehouse or analytical system. SQL-based operations are efficient for joining
    and aggregating data before processing. Let’s try to understand how SQL databases
    represent data in tables with rows and columns using a code example. We’ll simulate
    a basic SQL database interaction using Python dictionaries to represent tables
    and rows. You can see the full code example at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/5.sql_databases.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/5.sql_databases.py):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 关系数据库在批量ETL过程中非常有用，其中需要将来自各种源的结构化数据进行整合、转换并加载到数据仓库或分析系统中。在处理之前，基于SQL的操作可以有效地进行数据连接和聚合。让我们尝试了解SQL数据库如何使用表格、行和列来表示数据，使用代码示例。我们将使用Python字典来模拟基本的SQL数据库交互，以表示表格和行。您可以在[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/5.sql_databases.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/5.sql_databases.py)看到完整的代码示例：
- en: 'We create a `read_sql` function that simulates reading rows from a SQL table,
    represented here as a list of dictionaries where each dictionary corresponds to
    a row in the table:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个`read_sql`函数，模拟从SQL表格中读取行，这里表示为一个字典列表，其中每个字典对应表中的一行：
- en: '[PRE18]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `process_row` function takes a row (dictionary) as input and prints its
    contents, simulating the processing of a row from a SQL table:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`process_row`函数将一个行（字典）作为输入，并打印其内容，模拟从SQL表格中处理行的过程：'
- en: '[PRE19]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let’s print our SQL table in the proper format:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们以正确的格式打印我们的SQL表格：
- en: '[PRE20]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will print the following output:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印如下输出：
- en: '[PRE21]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The key to learning from the previous example is understanding how SQL databases
    structure and manage data through tables composed of rows and columns, and how
    to efficiently retrieve and process these rows programmatically. This knowledge
    is crucial because it lays the foundation for effective database management and
    data manipulation in any application.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的例子中学习的关键是理解SQL数据库通过由行和列组成的表来结构化和管理数据，以及如何通过编程方式高效检索和处理这些行。这些知识是至关重要的，因为它为任何应用程序中的有效数据库管理和数据操作奠定了基础。
- en: In real-world applications, this interaction is often facilitated by libraries
    and drivers such as **Java Database Connectivity** (**JDBC**) or **Open Database
    Connectivity** (**ODBC**), which provide standardized methods for connecting to
    and querying databases. These libraries are typically wrapped by higher-level
    frameworks or libraries in Python, making it easier for developers to ingest data
    from various SQL databases without worrying about the underlying connectivity
    details. Several libraries can be used to interact with SQL databases using Python;
    we will explore the most popular ones in the following section.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用程序中，这种交互通常由像**Java数据库连接**（**JDBC**）或**开放数据库连接**（**ODBC**）这样的库和驱动程序实现，它们提供了连接和查询数据库的标准方法。这些库通常被Python中的更高级别框架或库包装，使开发人员能够从各种SQL数据库中导入数据，而不必担心底层连接细节。可以使用多个库与Python交互SQL数据库；我们将在下一节探讨最流行的几个。
- en: Which library should you use for your use case?
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对于您的用例应该使用哪个库？
- en: 'Let’s explore the different libraries available for interacting with SQL databases
    in Python, and understand when to use each one:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索Python中用于与SQL数据库交互的不同库，并理解何时使用每个库：
- en: '**SQLite** (sqlite3) is ideal for small to medium-sized applications, local
    storage, and prototyping. Its zero-configuration, serverless architecture makes
    it perfect for lightweight, embedded database needs and quick development cycles.
    It is especially useful in scenarios where the overhead of a full-fledged database
    server is unnecessary. Avoid using sqlite3 for applications requiring high concurrency
    or extensive write operations, or where multiple users need to access the database
    simultaneously. It is not suitable for large-scale applications or those needing
    robust security features and advanced database functionalities.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SQLite** (sqlite3)非常适合小到中型应用程序、本地存储和原型设计。它的零配置、无服务器架构使其非常适合轻量级、嵌入式数据库需求和快速开发周期。在不需要完整数据库服务器开销的场景中特别有用。避免在需要高并发或大量写操作的应用程序中使用sqlite3，或者多用户需要同时访问数据库的场景。它不适合大规模应用或需要强大安全功能和高级数据库功能的场景。'
- en: '**SQLAlchemy** is suitable for applications requiring a high level of abstraction
    over raw SQL, support for multiple database engines, and complex queries and data
    models. It is ideal for large-scale production environments that need flexibility,
    scalability, and the ability to switch between different databases with minimal
    code changes. Avoid using SQLAlchemy for small, lightweight applications where
    the overhead of its comprehensive ORM capabilities is unnecessary. If you need
    direct, low-level access to a specific database’s features and are comfortable
    writing raw SQL queries, a simpler database adapter such as sqlite3, Psycopg2,
    or MySQL Connector/Python might be more appropriate.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SQLAlchemy** 适用于需要对原始 SQL 进行高度抽象、支持多种数据库引擎以及复杂查询和数据模型的应用程序。它非常适合需要灵活性、可扩展性，并且能够在不同数据库之间切换而无需做大量代码修改的大规模生产环境。避免在小型轻量级应用程序中使用
    SQLAlchemy，因为它的全面 ORM 功能会带来不必要的开销。如果您需要直接、低级地访问特定数据库的功能，并且能够编写原始 SQL 查询，那么像 sqlite3、Psycopg2
    或 MySQL Connector/Python 这样的简单数据库适配器可能会更合适。'
- en: '**Psycopg2** is the go-to choice for interacting with PostgreSQL databases,
    making it suitable for applications that leverage PostgreSQL’s advanced features,
    such as ACID compliance, complex queries, and extensive data types. It is ideal
    for production environments requiring reliability and efficiency in handling PostgreSQL
    databases. Avoid using Psycopg2 if your application does not interact with PostgreSQL.
    If you need compatibility with multiple database systems or a higher-level abstraction,
    consider using SQLAlchemy instead. Also, it might not be the best choice for lightweight
    applications where the overhead of a full PostgreSQL setup is unnecessary.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Psycopg2** 是与 PostgreSQL 数据库交互的首选工具，适用于需要利用 PostgreSQL 高级功能（如 ACID 合规性、复杂查询和丰富数据类型）的应用程序。它非常适合需要可靠性和效率来处理
    PostgreSQL 数据库的生产环境。如果您的应用程序不与 PostgreSQL 交互，请避免使用 Psycopg2。如果您需要与多个数据库系统兼容或需要更高层次的抽象，请考虑使用
    SQLAlchemy。另外，对于不需要完整 PostgreSQL 设置的轻量级应用程序，Psycopg2 可能不是最佳选择。'
- en: '`mysql-connector-python`) is great for applications that need to interact directly
    with MySQL databases. It is suitable for environments where compatibility and
    official support from Oracle are critical, as well as for applications leveraging
    MySQL’s features such as transaction management and connection pooling. Do not
    use MySQL Connector/Python if your application requires compatibility with multiple
    database systems or a higher-level abstraction. For simpler applications where
    the overhead of a full MySQL setup is unnecessary, or where MySQL’s features are
    not specifically needed, consider other lightweight alternatives.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mysql-connector-python`）非常适合需要与 MySQL 数据库直接交互的应用程序。它适用于需要兼容性并获得 Oracle 官方支持的环境，也适用于利用
    MySQL 功能（如事务管理和连接池）的应用程序。如果您的应用程序需要与多个数据库系统兼容或需要更高层次的抽象，请不要使用 MySQL Connector/Python。对于那些不需要完整
    MySQL 设置的简单应用程序，或者不特别需要 MySQL 功能的应用程序，可以考虑其他轻量级的替代方案。'
- en: After understanding the various libraries and their use cases for interacting
    with SQL databases, it’s equally important to explore alternatives for scenarios
    where the traditional relational model of SQL databases may not be the best fit.
    This brings us to NoSQL databases, which offer flexibility, scalability, and performance
    for handling unstructured or semi-structured data. Let’s delve into the key Python
    libraries for working with popular NoSQL databases and examine when and how to
    use them effectively.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了与 SQL 数据库交互的各种库及其使用场景后，探索在传统关系型 SQL 数据库可能不太适用的情况下的替代方案同样重要。这引出了 NoSQL 数据库，它们提供了处理非结构化或半结构化数据的灵活性、可扩展性和性能。让我们深入研究与流行的
    NoSQL 数据库进行交互的关键 Python 库，并探讨何时以及如何有效地使用它们。
- en: Performing data ingestion from the NoSQL database
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 NoSQL 数据库进行数据摄取
- en: Non-relational databases can be used for storing and processing large volumes
    of semi-structured or unstructured data in batch operations. They are particularly
    effective when the schema can evolve or when handling diverse data types in a
    consolidated manner. NoSQL databases excel in streaming and semi-real-time workloads
    due to their ability to handle high throughput and low-latency data ingestion.
    They are commonly used for capturing and processing real-time data from IoT devices,
    logs, social media feeds, and other sources that generate continuous streams of
    data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 非关系型数据库可以用于存储和处理大量半结构化或非结构化数据的批量操作。它们在模式可以演变或需要以统一方式处理多种数据类型时尤其有效。NoSQL 数据库在流式处理和半实时工作负载中表现出色，因为它们能够处理高吞吐量和低延迟的数据摄取。它们通常用于捕获和处理来自
    IoT 设备、日志、社交媒体动态以及其他生成连续数据流的来源的实时数据。
- en: 'The provided Python code mocks a NoSQL database with a dictionary and processes
    each key-value pair. Let’s have a look at each part of the code:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的 Python 代码模拟了一个使用字典的 NoSQL 数据库，并处理每个键值对。让我们来看看代码的每个部分：
- en: 'The `process_entry` function takes a key and its associated value from the
    data store and prints a formatted message showing the processing of that key-value
    pair. It provides a simple way to view or handle individual entries, highlighting
    how data is accessed and manipulated based on its key:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`process_entry`函数从数据存储中获取一个键及其关联的值，并打印出一条格式化的消息，展示该键值对的处理过程。它提供了一种简单的方法来查看或处理单个条目，突出显示如何基于键访问和操作数据：'
- en: '[PRE22]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following function prints the entire `data_store` dictionary in a tabular
    format:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下函数以表格格式打印整个`data_store`字典：
- en: '[PRE23]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'It starts by printing column headers for `Key`, `Name`, and `Age`, followed
    by a separator line for clarity. It then iterates over all key-value pairs in
    the `data_store` dictionary, printing each entry’s key, name, and age. This function
    helps visualize the current state of the data store. The initial state of the
    data is as follows:'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它首先打印`Key`、`Name`和`Age`的列标题，然后跟随一条分隔线以增加清晰度。接着，它遍历`data_store`字典中的所有键值对，打印每个条目的键、名称和年龄。这个函数有助于可视化当前数据存储的状态。数据的初始状态如下：
- en: '[PRE24]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'def create_entry(data_store, key, value):'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def create_entry(data_store, key, value):'
- en: data_store[key] = value
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: data_store[key] = value
- en: return data_store
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return data_store
- en: '[PRE25]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'def update_entry(data_store, key, new_value):'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def update_entry(data_store, key, new_value):'
- en: 'if key in data_store:'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'if key in data_store:'
- en: data_store[key] = new_value
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: data_store[key] = new_value
- en: return data_store
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return data_store
- en: '[PRE26]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following function removes an entry from the `data_store` dictionary:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下函数从`data_store`字典中移除一个条目：
- en: '[PRE27]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It takes a key, and if the key is found in the `data_store` dictionary, it deletes
    the corresponding entry. The updated `data_store` dictionary is then returned.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它接受一个键，如果该键在`data_store`字典中找到，则删除相应的条目。更新后的`data_store`字典会被返回。
- en: 'The following function wraps all the process together:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下函数将所有过程整合在一起：
- en: '[PRE28]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This code illustrates the core principles of NoSQL databases, including schema
    flexibility, key-value pair storage, and basic CRUD operations. It begins with
    the `read_nosql()` function, which simulates a NoSQL database using a dictionary,
    `data_store`, where each key-value pair represents a unique identifier and associated
    user information. Initially, the `print_data_store()` function displays the data
    in a tabular format, highlighting the schema flexibility inherent in NoSQL systems.
    The code then demonstrates CRUD operations. It starts by adding a new entry with
    the `create_entry()` function, showcasing how new data is inserted into the store.
    Following this, the `process_entry()` function retrieves and prints the details
    of the newly added entry, illustrating the read operation. Next, the `update_entry()`
    function modifies an existing entry, demonstrating the update capability of NoSQL
    databases. The `delete_entry()` function is used to remove an entry, showing how
    data can be deleted from the store. Finally, the updated state of the `data_store`
    dictionary is printed again, providing a clear view of how the data evolves through
    these operations.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码演示了 NoSQL 数据库的核心原则，包括架构灵活性、键值对存储和基本的 CRUD 操作。它从 `read_nosql()` 函数开始，该函数使用字典
    `data_store` 模拟了一个 NoSQL 数据库，其中每个键值对代表一个唯一的标识符和相关的用户信息。最初，`print_data_store()`
    函数以表格格式显示数据，突出展示了 NoSQL 系统固有的架构灵活性。接着，代码演示了 CRUD 操作。首先，`create_entry()` 函数添加了一个新条目，展示了如何将新数据插入存储中。随后，`process_entry()`
    函数检索并打印新添加条目的详细信息，展示了读取操作。接下来，`update_entry()` 函数修改了一个现有条目，展示了 NoSQL 数据库的更新能力。`delete_entry()`
    函数则用于删除一个条目，展示了如何从存储中删除数据。最后，再次打印更新后的 `data_store` 字典，清晰地展示了数据在这些操作中的变化过程。
- en: 'Let’s execute the whole process:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们执行整个过程：
- en: '[PRE29]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This returns the final datastore:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将返回最终的数据存储：
- en: '[PRE30]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding example, we demonstrated an interaction with a *mocked* NoSQL
    system using Python so that we can showcase the core principles of NoSQL databases
    such as schema flexibility, key-value pair storage, and basic CRUD operations.
    We can now better grasp how NoSQL databases differ from traditional SQL databases
    in terms of data modeling and handling unstructured or semi-structured data efficiently.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们演示了如何使用 Python 与一个*模拟*的 NoSQL 系统进行交互，以便展示 NoSQL 数据库的核心原则，例如架构灵活性、键值对存储和基本的
    CRUD 操作。现在我们可以更好地理解 NoSQL 数据库在数据建模和高效处理非结构化或半结构化数据方面与传统 SQL 数据库的区别。
- en: There are several libraries that can be used to interact with NoSQL databases.
    In the next section, we will explore the most popular ones.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个库可以用来与 NoSQL 数据库进行交互。在接下来的章节中，我们将探索其中最受欢迎的几个。
- en: Which library should you use for your use case?
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你应该根据自己的使用案例选择哪一个库？
- en: 'Let’s explore the different libraries available for interacting with NoSQL
    databases in Python, and understand when to use each one:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索在 Python 中与 NoSQL 数据库交互的不同库，并了解何时使用每一个：
- en: '`pymongo` is the official Python driver for MongoDB, a popular NoSQL database
    known for its flexibility and scalability. `pymongo` allows Python applications
    to interact seamlessly with MongoDB, offering a straightforward API to perform
    CRUD operations, manage indexes, and execute complex queries. `pymongo` is particularly
    favored for its ease of use and compatibility with Python’s data structures, making
    it suitable for a wide range of applications from simple prototypes to large-scale
    production systems.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pymongo` 是 MongoDB 的官方 Python 驱动，MongoDB 是一种因其灵活性和可扩展性而广受欢迎的 NoSQL 数据库。`pymongo`
    允许 Python 应用程序与 MongoDB 无缝互动，提供了一个简洁的 API 来执行 CRUD 操作、管理索引以及执行复杂查询。`pymongo` 特别受到欢迎，因为它易于使用且与
    Python 数据结构兼容，使其适用于从简单原型到大规模生产系统的各种应用。'
- en: '`cassandra-driver` (Cassandra): The `cassandra-driver` library provides Python
    applications with direct access to Apache Cassandra, a highly scalable NoSQL database
    designed for handling large amounts of data across distributed commodity servers.
    Cassandra’s architecture is optimized for write-heavy workloads and offers tunable
    consistency levels, making it suitable for real-time analytics, IoT data, and
    other applications requiring high availability and fault tolerance.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cassandra-driver`（Cassandra）：`cassandra-driver` 库为 Python 应用程序提供了直接访问 Apache
    Cassandra 的能力，Cassandra 是一个高度可扩展的 NoSQL 数据库，设计用于处理分布式普通服务器上的大量数据。Cassandra 的架构针对写密集型工作负载进行了优化，并提供了可调的一致性级别，适用于实时分析、物联网数据及其他需要高可用性和容错性的应用。'
- en: Transitioning from databases to file systems involves shifting the focus from
    structured data storage and retrieval mechanisms to more flexible and versatile
    storage solutions.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据库过渡到文件系统涉及将重点从结构化的数据存储和检索机制转向更灵活和多功能的存储解决方案。
- en: Performing data ingestion from cloud-based file systems
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从基于云的文件系统执行数据摄取
- en: Cloud storage is a service model that allows data to be remotely maintained,
    managed, and backed up over the internet. It involves storing data on remote servers
    accessed from anywhere via the internet, *rather than on local devices*. Cloud
    storage has revolutionized the way we store and access data. It provides a flexible
    and scalable solution for individuals and organizations, enabling them to store
    large amounts of data *without investing in physical hardware*. This is particularly
    useful for ensuring that data is always accessible and can be shared easily.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 云存储是一种服务模型，允许通过互联网远程维护、管理和备份数据。它涉及将数据存储在远程服务器上，并通过互联网从任何地方访问，*而不是存储在本地设备上*。云存储彻底改变了我们存储和访问数据的方式。它为个人和组织提供了一种灵活且可扩展的解决方案，使他们能够存储大量数据，*无需投资物理硬件*。这对于确保数据始终可访问并且可以轻松共享尤其有用。
- en: Amazon S3, Microsoft Azure Blob Storage, and Google Cloud Storage are all cloud-based
    object storage services that allow you to store and retrieve files in the cloud.
    Cloud-based file systems are becoming increasingly popular for several reasons.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon S3、Microsoft Azure Blob Storage 和 Google Cloud Storage 都是基于云的对象存储服务，允许你在云中存储和检索文件。基于云的文件系统因多个原因变得越来越流行。
- en: Firstly, they provide a flexible and scalable storage solution that can easily
    adapt to the changing needs of an organization. This means that as the amount
    of data grows, additional storage capacity can be added without the need for significant
    capital investment or physical infrastructure changes. Thus, it can help reduce
    capital expenditures and operational costs associated with maintaining and upgrading
    on-premises storage infrastructure.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它们提供了一种灵活且可扩展的存储解决方案，可以轻松适应组织不断变化的需求。这意味着，随着数据量的增长，可以在不需要大量资本投资或物理基础设施更改的情况下增加额外的存储容量。因此，它可以帮助减少与维护和升级本地存储基础设施相关的资本支出和运营成本。
- en: Secondly, cloud-based file systems offer high levels of accessibility and availability.
    With data stored in the cloud, users can access it from anywhere with an internet
    connection, making it easier to collaborate and share information across different
    teams, departments, or locations. Additionally, cloud-based file systems are designed
    with redundancy and failover mechanisms, ensuring that data is always available
    even in the event of a hardware failure or outage. Finally, they provide enhanced
    security features to protect data from unauthorized access, breaches, or data
    loss. Cloud service providers typically have advanced security protocols, encryption,
    and monitoring tools to safeguard data and ensure compliance with data privacy
    regulations.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，基于云的文件系统提供高水平的可访问性和可用性。数据存储在云中，用户可以从任何有互联网连接的地方访问它，这使得跨不同团队、部门或地点的协作和信息共享变得更加容易。此外，基于云的文件系统设计了冗余和故障转移机制，确保数据在硬件故障或停机事件发生时仍然可用。最后，它们提供增强的安全功能，以防止未经授权的访问、数据泄露或数据丢失。云服务提供商通常具备先进的安全协议、加密和监控工具来保护数据，并确保遵守数据隐私法规。
- en: Files in cloud-based storage systems are essentially the same as those on local
    devices, but they are stored on remote servers and accessed over the internet.
    However, how are these files organized in these cloud storage systems? Let’s discuss
    that next.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 云存储系统中的文件本质上与本地设备上的文件相同，但它们存储在远程服务器上并通过互联网访问。然而，这些文件在云存储系统中是如何组织的呢？接下来我们来讨论这个问题。
- en: Organizing files in cloud storage systems
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在云存储系统中组织文件
- en: 'One of the primary methods of organizing files in cloud storage is by using
    folder structures, similar to local file systems. Users can create folders and
    subfolders to categorize and store files systematically. Let’s have a look at
    some best practices:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在云存储中组织文件的主要方法之一是使用文件夹结构，这类似于本地文件系统。用户可以创建文件夹和子文件夹，以系统地分类和存储文件。我们来看一下最佳实践：
- en: Creating a logical and intuitive hierarchy that reflects how you work or how
    your projects are structured is essential. This involves designing a folder structure
    that mimics your workflow, making it easier to locate and manage files. For instance,
    you might create main folders for different departments, projects, or clients,
    with subfolders for specific tasks or document types. This hierarchical organization
    not only saves time by reducing the effort needed to find files but also enhances
    collaboration by providing a clear and consistent framework that team members
    can easily navigate.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个符合逻辑且直观的层级结构，反映出你的工作方式或项目结构是至关重要的。这涉及设计一个与工作流程相匹配的文件夹结构，使文件的定位和管理更加简便。例如，你可以为不同的部门、项目或客户创建主文件夹，并为特定任务或文档类型创建子文件夹。这种层级组织不仅通过减少寻找文件所需的努力节省时间，还通过提供一个清晰且一致的框架，提升了团队成员之间的协作，便于大家快速导航。
- en: Using *consistent naming conventions* for folders and files is crucial for ensuring
    easy retrieval and maintaining order within your cloud storage. A standardized
    naming scheme helps avoid confusion, reduces errors, and speeds up the process
    of locating specific documents. For example, adopting a format such as `YYYY-MM-DD_ProjectName_DocumentType`
    can provide immediate context and make sorting and searching more straightforward.
    Consistent naming also facilitates automation and integration with other tools,
    as predictable file names can be more easily processed by scripts and applications.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*一致的命名规范*来命名文件夹和文件对于确保云存储中的文件便捷检索和保持有序至关重要。标准化的命名方案有助于避免混淆，减少错误，并加快查找特定文档的过程。例如，采用如`YYYY-MM-DD_项目名称_文档类型`这样的格式，可以提供即时的上下文信息，并使排序和搜索变得更加直接。一致的命名还促进了自动化和与其他工具的集成，因为可预测的文件名更易于被脚本和应用程序处理。
- en: Grouping files by project or client is an effective way to keep related documents
    together and streamline project management. This method involves creating dedicated
    folders for each project or client, where all relevant files, such as contracts,
    communications, and deliverables, are stored.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按项目或客户对文件进行分组是保持相关文档集中和简化项目管理的有效方法。这种方法涉及为每个项目或客户创建专门的文件夹，所有相关文件，如合同、通信和交付物，都存储在这些文件夹中。
- en: Many cloud storage systems allow tagging files with keywords or metadata, which
    significantly enhances file categorization and searchability. Tags are essentially
    labels that you can attach to files, making it easier to group and find documents
    based on specific criteria. Metadata includes detailed information, such as the
    author, date, project name, and file type, which provides additional context and
    aids in more precise searches. By using relevant tags and comprehensive metadata,
    you can quickly filter and locate files, regardless of their location within the
    folder hierarchy. This practice is particularly useful in large storage systems
    where traditional folder structures might become cumbersome.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多云存储系统允许通过关键字或元数据为文件加标签，这大大增强了文件分类和搜索功能。标签本质上是你可以附加到文件上的标签，使得根据特定标准对文档进行分组和查找变得更加容易。元数据包括详细信息，如作者、日期、项目名称和文件类型，这些信息提供了额外的上下文，有助于更精确的搜索。通过使用相关标签和全面的元数据，你可以快速筛选并定位文件，无论它们位于文件夹层级的哪个位置。这种做法在大型存储系统中尤为有用，因为在这些系统中，传统的文件夹结构可能会变得笨重。
- en: From discussing cloud storage systems, the focus now shifts to exploring the
    capabilities and integration opportunities offered by APIs.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 从讨论云存储系统开始，焦点现在转向探索API所提供的功能和集成机会。
- en: APIs
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: APIs
- en: APIs have become increasingly popular in recent years due to their ability to
    enable seamless communication and integration between different systems and services.
    APIs provide developers with a standardized and flexible way to access data and
    functionality from other systems, allowing them to easily build new applications
    and services that leverage existing resources. APIs have become a fundamental
    building block for modern software development and are widely used across a wide
    range of industries and applications.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，API变得越来越受欢迎，因为它们能够实现不同系统和服务之间的无缝通信和集成。API为开发者提供了一种标准化且灵活的方式，能够访问其他系统的数据和功能，使他们能够轻松构建利用现有资源的新应用和服务。API已成为现代软件开发的基础构件，广泛应用于各个行业和领域。
- en: Now that we understand what APIs represent, let’s move on to the `requests`
    Python library with which developers can programmatically access and manipulate
    data from remote servers.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了API的含义，让我们继续了解`requests` Python库，它使开发人员能够以编程方式访问和操作远程服务器上的数据。
- en: The requests library
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`requests`库'
- en: When it comes to working with APIs in Python, the `requests` library is the
    go-to Python library for making HTTP requests to APIs and other web services.
    It makes it easy to send HTTP/1.1 requests using Python, and it provides many
    convenient features for working with HTTP responses.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Python与API进行交互时，`requests`库是用于向API和其他网络服务发送HTTP请求的首选Python库。它使得用Python发送HTTP/1.1请求变得简单，并提供了许多方便的功能来处理HTTP响应。
- en: 'Run the following command to install the `requests` library:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令来安装`requests`库：
- en: '[PRE31]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let’s have a quick look at how we can use this library:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看看如何使用这个库：
- en: 'Import the `requests` library:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`requests`库：
- en: '[PRE32]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Specify the API endpoint URL:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定API端点的URL：
- en: '[PRE33]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Make a `GET` request to the API endpoint:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对API端点发起`GET`请求：
- en: '[PRE34]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Get the response content:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取响应内容：
- en: '[PRE35]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here, we’re making a `GET` request to the API endpoint at [https://jsonplaceholder.typicode.com/posts](https://jsonplaceholder.typicode.com/posts)
    and storing the `response` object in the `response` variable. We can then print
    the response content using the `content` attribute of the `response` object. The
    `requests` library provides many other methods and features for making HTTP requests,
    including support for `POST`, `PUT`, `DELETE`, and other HTTP methods, handling
    headers and cookies, and handling redirects and authentication.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们向[https://jsonplaceholder.typicode.com/posts](https://jsonplaceholder.typicode.com/posts)的API端点发起了`GET`请求，并将`response`对象存储在`response`变量中。然后，我们可以使用`response`对象的`content`属性打印响应内容。`requests`库提供了许多其他方法和功能来发起HTTP请求，包括对`POST`、`PUT`、`DELETE`等HTTP方法的支持、处理头信息和Cookies，以及处理重定向和身份验证。
- en: Now that we’ve explained the `requests` library, let’s move on to a specific
    example of retrieving margarita cocktail data from the `Cocktail DB` API, which
    can illustrate how practical web requests can be in accessing and integrating
    real-time data sources into applications.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解释了`requests`库，让我们来看看一个具体的例子，如何从`Cocktail DB` API中获取玛格丽塔鸡尾酒的数据，这可以说明如何将实际的网页请求应用到访问和整合实时数据源到应用程序中。
- en: Learn how to make a margarita!
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习如何制作玛格丽塔鸡尾酒！
- en: 'The use case demonstrates retrieving cocktail data from the `Cocktail DB` API
    using Python. If you want to improve your bartending skills and impress your friends,
    you can use an open API to get real-time information on the ingredients required
    for any cocktail. For this, we will use the `Cocktail DB` API and the `request`
    library to see which ingredients we need for a margarita:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 本用例演示了如何使用Python从`Cocktail DB` API中检索鸡尾酒数据。如果你想提高你的调酒技能并给朋友留下深刻印象，你可以使用开放API获取任何鸡尾酒所需成分的实时信息。为此，我们将使用`Cocktail
    DB` API和`requests`库，看看我们需要哪些成分来制作玛格丽塔：
- en: 'Define the API endpoint URL. We are making a request to the Cocktail DB API
    endpoint to search for cocktails with the `margarita` name:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义API端点URL。我们正在向Cocktail DB API端点发出请求，搜索名称为`margarita`的鸡尾酒：
- en: '[PRE36]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Make the API request. We define the API endpoint URL as a string and pass it
    to the `requests.get()` function to make the `GET` request:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发起API请求。我们将API端点URL定义为字符串，并将其传递给`requests.get()`函数来发起`GET`请求：
- en: '[PRE37]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Check whether the request was successful (status code `200`) and get the data.
    The API response is returned as a JSON string, which we can extract by calling
    the `response.json()` method. We then assign this JSON data to a variable called
    `data`:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查请求是否成功（状态码`200`），并获取数据。API响应以JSON字符串形式返回，我们可以通过调用`response.json()`方法来提取它。然后，我们将这个JSON数据赋值给一个名为`data`的变量：
- en: '[PRE38]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'If the request was not successful, print this error message:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果请求不成功，请打印此错误信息：
- en: '[PRE39]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: You can replace the `margarita` search parameter with any other cocktail name
    or ingredient to get data for different drinks.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将`margarita`搜索参数替换为其他鸡尾酒名称或成分，以获取不同饮品的数据。
- en: With this, we come to the end of our first chapter. Let’s summarize what we
    have learned so far.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 到这里，我们已经结束了第一章。让我们总结一下到目前为止学到的内容。
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Throughout this chapter, we covered essential technologies in modern computing
    and data management. We began by discussing batch ingestion, a method whereby
    large volumes of data are collected and processed at scheduled intervals, offering
    efficiency and cost-effectiveness for organizations with predictable data flows.
    In contrast, we explored streaming ingestion, which allows data to be processed
    in real-time, enabling immediate analysis and rapid response to changing conditions.
    We followed with streaming services such as Kafka for real-time data processing.
    We moved to SQL and NoSQL databases—such as PostgreSQL, MySQL, MongoDB, and Cassandra—highlighting
    their strengths in structured and flexible data storage, respectively. We explored
    APIs such as REST for seamless system integration. Also, we delved into file systems,
    file types, and attributes, alongside cloud storage solutions such as Amazon S3
    and Google Cloud Storage, emphasizing scalability and data management strategies.
    These technologies collectively enable robust, scalable, and efficient applications
    in today’s digital ecosystem.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了现代计算和数据管理中的重要技术。我们首先讨论了批量摄取，这是一种在预定时间间隔内收集和处理大量数据的方法，为数据流可预测的组织提供了高效且具成本效益的解决方案。与之相对，我们探讨了流式摄取，它允许数据实时处理，从而能够即时分析并迅速应对变化的条件。接着我们介绍了如Kafka等流式服务，用于实时数据处理。随后我们讲解了SQL和NoSQL数据库——如PostgreSQL、MySQL、MongoDB和Cassandra——并突出了它们在结构化和灵活数据存储方面的优势。我们还探索了像REST这样的API，以实现系统的无缝集成。此外，我们深入讨论了文件系统、文件类型及其属性，并介绍了如Amazon
    S3和Google Cloud Storage等云存储解决方案，强调了可扩展性和数据管理策略。这些技术共同推动了当今数字生态系统中强大、可扩展且高效的应用程序。
- en: In the upcoming chapter, we will dive deep into the critical aspects of data
    quality and its significance in building reliable data products. We’ll explore
    why ensuring high data quality is paramount for making informed business decisions,
    enhancing customer experiences, and maintaining operational efficiency.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在即将到来的章节中，我们将深入探讨数据质量的关键方面及其在构建可靠数据产品中的重要性。我们将探索为什么确保高数据质量对于做出明智的商业决策、提升客户体验和维持运营效率至关重要。
