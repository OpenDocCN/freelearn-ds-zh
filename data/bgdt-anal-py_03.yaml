- en: '*Chapter 3*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第3章*'
- en: Working with Big Data Frameworks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用大数据框架
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Explain the HDFS and YARN Hadoop components
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释HDFS和YARN Hadoop组件
- en: Perform file operations with HDFS
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行HDFS文件操作
- en: Compare a pandas DataFrame with a Spark DataFrame
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较pandas DataFrame和Spark DataFrame
- en: Read files from a local filesystem and HDFS using Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark从本地文件系统和HDFS中读取文件
- en: Write files in Parquet format using Spark
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark以Parquet格式写入文件
- en: Write partitioned files in Parquet for fast analysis
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写分区文件以便快速分析
- en: Manipulate non-structured data with Spark
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark操作非结构化数据
- en: In this chapter, we will explore big data tools such as Hadoop and Spark.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨大数据工具，如Hadoop和Spark。
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'We saw in the previous chapters how to work with data using pandas and Matplotlib
    for visualization and the other tools in the Python data science stack. So far,
    the datasets that we have used have been relatively small and with a relatively
    simple structure. Real-life datasets can be orders of magnitude larger than can
    fit into the memory of a single machine, the time to process these datasets can
    be long, and the usual software tools may not be up to the task. This is the usual
    definition of what big data is: an amount of data that does not fit into memory
    or cannot be processed or analyzed in a reasonable amount of time by common software
    methods. What is big data for some may not be big data for others, and this definition
    can vary depending on who you ask.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们看到如何使用pandas和Matplotlib进行数据操作与可视化，以及Python数据科学栈中的其他工具。到目前为止，我们使用的数据集相对较小，且结构相对简单。现实中的数据集可能比单台机器的内存容量要大几个数量级，处理这些数据集所需的时间也可能很长，常用的软件工具可能无法胜任。这就是通常定义的大数据：一种无法装入内存，或者无法在合理的时间内通过常规软件方法进行处理或分析的数据量。对某些人来说是大数据的东西，对于其他人来说可能不是大数据，这一定义可能因提问对象的不同而有所变化。
- en: 'Big Data is also associated with the 3 V’s (later extended to 4 V’s):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据还与3V（后扩展为4V）相关联：
- en: '**Volume**: Big data, as the name suggests, is usually associated with very
    large volumes of data. What is large depends on the context: for one system, gigabytes
    can be large, while for another, we have to go to petabytes of data.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容量**：顾名思义，大数据通常与非常大的数据量相关联。什么是“大”取决于上下文：对于一个系统，千兆字节就可以算作大数据，而对于另一个系统，则可能需要达到PB级数据。'
- en: '**Variety**: Usually, big data is associated with different data formats and
    types, such as text, video, and audio. Data can be structured, like relational
    tables, or unstructured, like text and video.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**：通常，大数据与不同的数据格式和类型相关联，如文本、视频和音频。数据可以是结构化的，如关系表，或是非结构化的，如文本和视频。'
- en: '**Velocity**: The speed at which data is generated and stored is faster than
    other systems and produced more continuously. Streaming data can be generated
    by platforms such as telecommunications operators or online stores, or even Twitter.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：数据生成和存储的速度比其他系统更快，并且生产速度更加连续。流数据可以通过像电信运营商、在线商店甚至Twitter这样的平台生成。'
- en: '**Veracity**: This was added later and tries to show that knowing the data
    that is being used and its meaning is important in any analysis work. We need
    to check whether the data corresponds with what we expect the data to be, that
    the transformation process didn’t change the data, and whether it reflects what
    was collected.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真实性**：这一点是后来加入的，旨在说明在任何分析工作中，了解正在使用的数据及其含义非常重要。我们需要检查数据是否与我们预期的相符，转换过程是否改变了数据，数据是否反映了所收集的内容。'
- en: 'But one aspect that makes big data compelling is the analysis component: big
    data platforms are created to allow analysis and information extraction over these
    large datasets. This is where this chapter starts: we will learn how to manipulate,
    store, and analyze large datasets using two of the most common and versatile frameworks
    for big data: Hadoop and Spark.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但让大数据具有吸引力的一个方面是分析组件：大数据平台旨在支持对这些庞大数据集进行分析和信息提取。这一章从这里开始：我们将学习如何使用两个最常见、最灵活的大数据框架——Hadoop和Spark——来操作、存储和分析大数据集。
- en: Hadoop
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop
- en: Apache Hadoop is a set of software components created for the parallel storage
    and computation of large volumes of data. The main idea at the time of its inception
    was to use commonly available computers in a distributed fashion, with high resiliency
    against failure and distributed computation. With its success, more high-end computers
    started to be used on Hadoop clusters, although commodity hardware is still a
    common use case.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop是一个为大规模数据的并行存储和计算而创建的软件组件集合。它在最初的设计理念是使用普通计算机进行分布式处理，具备高容错性和分布式计算能力。随着Hadoop的成功，越来越多的高端计算机开始被用于Hadoop集群，尽管普通硬件仍然是常见的使用案例。
- en: By parallel storage, we mean any system that stores and retrieves stored data
    in a parallel fashion, using several nodes interconnected by a network.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 并行存储是指使用多个通过网络互联的节点，以并行方式存储和检索数据的任何系统。
- en: 'Hadoop is composed of the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop由以下组件组成：
- en: '**Hadoop Common**: the basic common Hadoop items'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop Common**：基础的Hadoop通用项目'
- en: '**Hadoop YARN**: a resource and job manager'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop YARN**：资源和作业管理器'
- en: '**Hadoop MapReduce**: a large-scale parallel processing engine'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop MapReduce**：一个大规模并行处理引擎'
- en: '**Hadoop Distributed File System** (**HDFS**): as the name suggests, HDFS is
    a file system that can be distributed over several machines, using local disks,
    to create a large storage pool:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop分布式文件系统**（**HDFS**）：顾名思义，HDFS是一个可以分布在多台计算机上的文件系统，通过使用本地磁盘，创建一个大规模的存储池：'
- en: '![Figure 3.1: Architecture of HDFS](img/C12913_03_01.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1：HDFS架构](img/C12913_03_01.jpg)'
- en: 'Figure 3.1: Architecture of HDFS'
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.1：HDFS架构
- en: Another important component is **YARN** (**Yet Another Resource Negotiator**),
    a resource manager and job scheduler for Hadoop. It is responsible for managing
    jobs submitted to the Hadoop cluster, allocating memory and CPU based on the required
    and available resources.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要组件是**YARN**（**Yet Another Resource Negotiator**），是Hadoop的资源管理器和作业调度器。它负责管理提交到Hadoop集群的作业，根据所需和可用资源分配内存和CPU。
- en: Hadoop popularized a parallel computation model called MapReduce, a distributed
    computation paradigm first developed by Google. It’s possible to run a program
    using MapReduce in Hadoop directly. But since Hadoop’s creation, other parallel
    computation paradigms and frameworks have been developed (such as Spark), so MapReduce
    is not commonly used for data analysis. Before diving into Spark, let’s see how
    we can manipulate files on the HDFS.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop普及了一种并行计算模型，叫做MapReduce，这是Google最初开发的一种分布式计算范式。在Hadoop中直接运行使用MapReduce的程序是可能的。但自从Hadoop诞生以来，已经开发出了其他并行计算范式和框架（如Spark），因此MapReduce在数据分析中并不常用。在深入了解Spark之前，让我们先看看如何在HDFS上操作文件。
- en: Manipulating Data with the HDFS
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用HDFS操作数据
- en: 'The HDFS is a distributed file system with an important distinction: it was
    designed to run on thousands of computers that were not built specially for it—so-called
    **commodity hardware**. It doesn’t require any special networking gear or special
    disks, it can run on common hardware. Another idea that permeates HDFS is that
    it is resilient: hardware will always fail, so instead of trying to prevent failure,
    the HDFS works around this by being extremely fault-tolerant. It assumes that
    failures will occur considering its scale, so the HDFS implements fault detection
    for fast and automatic recovery. It is also portable, running in diverse platforms,
    and can hold single files with terabytes of data.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS是一个分布式文件系统，其一个重要特点是：它被设计用来运行在成千上万台并非专门为此设计的计算机上——也就是所谓的**商品硬件**。它不需要特殊的网络设备或特殊硬盘，可以运行在普通硬件上。HDFS的另一个理念是容错性：硬件总会发生故障，因此HDFS通过高度容错的方式绕过故障，而不是试图避免它。它假定在大规模环境下会发生故障，因此HDFS实现了故障检测机制，以实现快速和自动恢复。它也具有可移植性，能在不同平台上运行，并且可以容纳单个文件，数据容量达到TB级。
- en: 'One of the big advantages from a user perspective is that the HDFS supports
    traditional hierarchical file structure organization (folders and files in a tree
    structure), so users can create folders inside folders and files inside folders
    on each level, simplifying its use and operation. Files and folders can be moved
    around, deleted, and renamed, so users do not need to know about data replication
    or **NameNode**/**DataNode** architecture to use the HDFS; it would look similar
    to a Linux filesystem. Before demonstrating how to access files, we need to explain
    a bit about the addresses used to access Hadoop data. For example, the URI for
    accessing files in HDFS has the following format:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从用户的角度来看，HDFS的一个大优点是它支持传统的层次化文件结构组织（文件夹和文件的树形结构），因此用户可以在每一层创建文件夹和文件，简化了使用和操作。文件和文件夹可以移动、删除和重命名，因此用户无需了解数据复制或**NameNode**/**DataNode**架构即可使用HDFS；它看起来与Linux文件系统类似。在演示如何访问文件之前，我们需要先解释一下访问Hadoop数据所使用的地址。例如，访问HDFS中文件的URI格式如下：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Where `namenode.domainname` is the address configured in Hadoop. Hadoop user
    guide ([https://exitcondition.com/install-hadoop-windows/](https://exitcondition.com/install-hadoop-windows/))
    details a bit more on how to access different parts of the Hadoop system. Let’s
    look at a few examples to better understand how all this works.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，`namenode.domainname`是Hadoop中配置的地址。Hadoop用户指南（[https://exitcondition.com/install-hadoop-windows/](https://exitcondition.com/install-hadoop-windows/)）详细介绍了如何访问Hadoop系统的不同部分。让我们通过几个例子更好地理解这一切是如何工作的。
- en: 'Exercise 16: Manipulating Files in the HDFS'
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习16：在HDFS中操作文件
- en: 'An analyst just received a large dataset to analyze and it’s stored on an HDFS
    system. How would this analyst list, copy, rename, and move these files? Let’s
    assume that the analyst received a file with raw data, named `new_data.csv`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个分析师刚刚收到了一个要分析的大型数据集，并且它存储在HDFS系统中。这个分析师如何列出、复制、重命名和移动这些文件？假设分析师收到了一个名为`new_data.csv`的原始数据文件：
- en: 'Let’s start checking the current directories and files using the following
    command on the terminal if you are on Linux based system or command prompt if
    you are on Windows system:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您使用的是Linux系统，请在终端中使用以下命令，或者如果使用Windows系统，请在命令提示符下执行此命令，以开始检查当前的目录和文件：
- en: '[PRE1]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We have a local file on disk, called `new_data.csv`, which we want to copy
    to the HDFS data folder:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在本地磁盘上有一个名为`new_data.csv`的文件，我们希望将其复制到HDFS数据文件夹中：
- en: '[PRE2]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Notice that the last part of the command is the path inside HDFS. Now, create
    a folder in HDFS using the command `mkdir`:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，命令的最后部分是HDFS中的路径。现在，使用`mkdir`命令在HDFS中创建一个文件夹：
- en: '[PRE3]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'And move the file to a data folder in HDFS:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将文件移动到HDFS中的数据文件夹：
- en: '[PRE4]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Change the name of the CSV file:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改CSV文件的名称：
- en: '[PRE5]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Use the following command to check whether the file is present in the current
    location or not:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令检查文件是否存在于当前路径：
- en: '[PRE6]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE7]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Commands after the HDFS part have the same name as commands in the Linux shell.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: HDFS部分之后的命令与Linux Shell中的命令相同。
- en: Knowing how to manipulate files and directories with the HDFS is an important
    part of big data analysis, but usually, direct manipulation is done only on ingestion.
    To analyze data, HDFS is not directly used, and tools such as Spark are more powerful.
    Let’s see how to use Spark in sequence.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 了解如何在HDFS中操作文件和目录是大数据分析的重要部分，但通常，直接操作仅限于数据摄取时进行。要分析数据，HDFS通常不会直接使用，像Spark这样的工具更为强大。让我们按顺序看看如何使用Spark。
- en: Spark
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark
- en: '**Spark** ([https://spark.apache.org](https://spark.apache.org)) is a unified
    analytics engine for large-scale data processing. Spark started as a project by
    the University of California, Berkeley, in 2009, and moved to the Apache Software
    Foundation in 2013.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark**（[https://spark.apache.org](https://spark.apache.org)）是一个用于大规模数据处理的统一分析引擎。Spark最初是由加利福尼亚大学伯克利分校于2009年发起的项目，并于2013年移交给Apache软件基金会。'
- en: 'Spark was designed to tackle some problems with the Hadoop architecture when
    used for analysis, such as data streaming, SQL over files stored on HDFS and machine
    learning. It can distribute data over all computing nodes in a cluster in a way
    that decreases the latency of each computing step. Another Spark difference is
    its flexibility: there are interfaces for Java, Scala, SQL, R and Python, and
    libraries for different problems, such as MLlib for machine learning, GraphX for
    graph computation, and Spark Streaming, for streaming workloads.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的设计旨在解决使用Hadoop架构进行分析时的一些问题，如数据流、SQL操作存储在HDFS上的文件和机器学习。它可以将数据分布到集群中的所有计算节点，以减少每个计算步骤的延迟。Spark的另一个特点是它的灵活性：它有适用于Java、Scala、SQL、R和Python的接口，以及适用于不同问题的库，例如用于机器学习的MLlib、用于图计算的GraphX和用于流式工作负载的Spark
    Streaming。
- en: Spark uses the worker abstraction, having a driver process that receives user
    input to start parallel executions, and worker processes that reside on the cluster
    nodes, executing tasks. It has a built-in cluster management tool and supports
    other tools, such as Hadoop YARN and Apache Mesos (and even Kubernetes), integrating
    into different environments and resource distribution scenarios.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Spark使用工作节点抽象，具有一个接收用户输入以启动并行执行的驱动进程，以及在集群节点上执行任务的工作进程。它具有内置的集群管理工具，并支持其他工具，如Hadoop
    YARN、Apache Mesos（甚至Kubernetes），可以集成到不同的环境和资源分配场景中。
- en: 'Spark can also be very fast, because it first tries to distribute data over
    all nodes and keep it in memory instead of relying only on data on disk. It can
    handle datasets larger than the total available memory, shifting data between
    memory and disk, but making the process slower than if the entire dataset fitted
    in the total available memory of all nodes:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Spark也可以非常快速，因为它首先尝试将数据分布到所有节点，并将其保留在内存中，而不是仅仅依赖磁盘上的数据。它可以处理比所有可用内存总和还大的数据集，通过在内存和磁盘之间切换数据，但这个过程会比将整个数据集完全适配到所有节点的内存中时慢。
- en: '![Figure 3.2: Working mechanism of Spark](img/C12913_03_02.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2：Spark工作机制](img/C12913_03_02.jpg)'
- en: 'Figure 3.2: Working mechanism of Spark'
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.2：Spark工作机制
- en: Other great advantages are that Spark has interfaces for a large variety of
    local and distributed storage systems, such as HDFS, Amazon S3, Cassandra, and
    others; can connect to RDBMS such as PostgreSQL and MySQL via JDBC or ODBC connectors;
    and can use the **Hive Metastore** to run SQL directly over a HDFS file. File
    formats such as CSV, Parquet, and ORC can also be read directly by Spark.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其他显著优势是，Spark具有适用于多种本地和分布式存储系统的接口，如HDFS、Amazon S3、Cassandra等；可以通过JDBC或ODBC连接器连接到RDBMS，如PostgreSQL和MySQL；还可以使用**Hive
    Metastore**直接对HDFS文件运行SQL。CSV、Parquet和ORC等文件格式也可以被Spark直接读取。
- en: This flexibility can be a great help when working with big data sources, which
    can have varying formats.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这种灵活性在处理大数据源时非常有帮助，因为大数据源可能具有不同的格式。
- en: Spark can be used either as an interactive shell with Scala, Python, and R,
    or as a job submission platform with the spark-submit command. The submit method
    is used to dispatch jobs to a Spark cluster coded in a script. The Spark shell
    interface for Python is called PySpark. It can be accessed directly from the terminal,
    where the Python version that is the default will be used; it can be accessed
    using the IPython shell or even inside a Jupyter notebook.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以作为交互式Shell使用，支持Scala、Python和R，也可以作为作业提交平台使用，使用` spark-submit`命令将作业分发到Spark集群。submit方法用于将作业调度到Spark集群，该作业是通过脚本编码的。Spark的Python接口被称为PySpark，可以直接从终端访问，使用默认的Python版本；也可以通过IPython
    shell或在Jupyter笔记本内访问。
- en: Spark SQL and Pandas DataFrames
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark SQL和Pandas DataFrames
- en: The **RDD**, or **Resilient Distributed Dataset**, is the base abstraction that
    Spark uses to work with data. Starting on Spark version 2.0, the recommended API
    to manipulate data is the DataFrame API. The DataFrame API is built on top of
    the RDD API, although the RDD API can still be accessed.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**RDD**（弹性分布式数据集）是Spark用来处理数据的基础抽象。从Spark 2.0版本开始，推荐使用的API是DataFrame API。DataFrame
    API是在RDD API之上构建的，尽管仍然可以访问RDD API。'
- en: Working with RDDs is considered low-level and all operations are available in
    the DataFrame API, but it doesn’t hurt learning a bit more about the RDD API.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RDD被认为是低级的，所有操作都可以在DataFrame API中实现，但学习一些关于RDD API的内容也无妨。
- en: The SQL module enables users to query the data in Spark using SQL queries, similar
    to common relational databases. The DataFrame API is part of the SQL module, which
    works with structured data. This interface for data helps to create extra optimizations,
    with the same execution engine being used, independently of the API or language
    used to express such computations.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 模块使用户能够使用 SQL 查询在 Spark 中查询数据，类似于常见的关系型数据库。DataFrame API 是 SQL 模块的一部分，处理结构化数据。这个数据接口有助于创建额外的优化，使用相同的执行引擎，无论使用什么
    API 或语言来表达这些计算。
- en: The DataFrame API is similar to the **Pandas DataFrame**. In Spark, a DataFrame
    is a distributed collection of data, organized into columns, with each column
    having a name. With Spark 2.0, the DataFrame is a part of the more general Dataset
    API, but as this API is only available for the Java and Scala languages, we will
    discuss only the DataFrame API (called **Untyped Dataset Operations** in the documentation).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API 类似于 **Pandas DataFrame**。在 Spark 中，DataFrame 是一个分布式的数据集合，按列组织，每一列都有一个名称。随着
    Spark 2.0 的发布，DataFrame 成为更通用的 Dataset API 的一部分，但由于该 API 仅适用于 Java 和 Scala 语言，我们将只讨论
    DataFrame API（在文档中称为 **Untyped Dataset Operations**）。
- en: 'The interface for Spark DataFrames is similar to the pandas interface, but
    there are important differences:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrame 的接口类似于 pandas 接口，但也有一些重要的区别：
- en: 'The first difference is that Spark DataFrames are **immutable**: after being
    created, they cannot be altered.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个区别是 Spark DataFrame 是 **不可变的**：创建后无法修改。
- en: 'The second difference is that Spark has two different kinds of operations:
    **transformations** and **actions**.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个区别是 Spark 有两种不同的操作：**转换**和**行动**。
- en: '**Transformations** are operations that are applied over the elements of a
    DataFrame and are queued to be executed later, not fetching data yet.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**转换** 是应用于 DataFrame 元素的操作，并且是排队待执行的，数据尚未被提取。'
- en: Only when an **action** is called is data fetched and all queued transformations
    are executed. This is called lazy evaluation.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有在调用 **行动** 时，数据才会被提取，并且所有排队的转换操作都会执行。这叫做延迟计算。
- en: 'Exercise 17: Performing DataFrame Operations in Spark'
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 17：在 Spark 中执行 DataFrame 操作
- en: 'Let’s start using Spark to perform input/output and simple aggregation operations.
    The Spark interface, as we said before, was inspired by the pandas interface.
    What we learned in *Chapter 2*, *Statistical Visualizations Using Matplotlib and
    Seaborn*, can be applied here, making it easier to carry out more complex analysis
    faster, including aggregations, statistics, computation, and visualization on
    aggregated data later on. We want to read a CSV file, as we did before, to perform
    some analysis on it:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始使用 Spark 执行输入/输出和简单的聚合操作。正如我们之前所说，Spark 的接口灵感来自 pandas 接口。在 *第二章*，*使用 Matplotlib
    和 Seaborn 的统计可视化* 中学到的内容可以应用到这里，从而加速执行更复杂的分析，包括聚合、统计、计算和对聚合数据的可视化。我们希望读取一个 CSV
    文件，就像我们之前做的那样，对其进行一些分析：
- en: 'First, let’s use the following command on Jupyter notebook to create a Spark
    session:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在 Jupyter notebook 中使用以下命令创建一个 Spark 会话：
- en: '[PRE8]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let’s use the following command to read the data from the `mydata.csv`
    file:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下命令从 `mydata.csv` 文件中读取数据：
- en: '[PRE9]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As we said before, Spark evaluation is lazy, so if we want to show what values
    are inside the DataFrame, we need to call the action, as illustrated here:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，Spark 的计算是延迟的，因此如果我们想显示 DataFrame 中的值，我们需要调用行动，如这里所示：
- en: '[PRE10]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'This is not necessary with pandas: printing the DataFrame would work directly.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这在 pandas 中不是必需的：直接打印 DataFrame 就能显示内容。
- en: 'Exercise 18: Accessing Data with Spark'
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 18：使用 Spark 访问数据
- en: 'After reading our DataFrame and showing its contents, we want to start manipulating
    the data so that we can do an analysis. We can access data using the same NumPy
    selection syntax, providing the column name as `Column`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取 DataFrame 并显示其内容后，我们希望开始操作数据，以便进行分析。我们可以使用相同的 NumPy 选择语法来访问数据，提供列名作为 `Column`：
- en: 'Let’s select one column from the DataFrame that we ingested in the previous
    exercise:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们选择上一练习中导入的 DataFrame 中的某一列：
- en: '[PRE11]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is different from what we’ve seen with pandas. The method that selects
    values from columns in a Spark DataFrame is `select`. So, let’s see what happens
    when we use this method.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这与我们在 pandas 中看到的有所不同。选择 Spark DataFrame 中列的值的方法是 `select`。那么，让我们看看当我们使用这个方法时会发生什么。
- en: 'Using the same DataFrame again, use the `select` method to select the name
    column:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次使用相同的 DataFrame，使用 `select` 方法选择名称列：
- en: '[PRE12]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, it changed from `Column` to `DataFrame`. Because of that, we can use the
    methods for DataFrames. Use the `show` method for showing the results from the
    `select` method for `age`:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，它从 `Column` 变成了 `DataFrame`。因此，我们可以使用 DataFrame 的方法。使用 `show` 方法显示 `select`
    方法对 `age` 列的结果：
- en: '[PRE13]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s select more than one column. We can use the names of the columns to do
    this:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们选择多个列。我们可以使用列的名称来做到这一点：
- en: '[PRE14]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This is extensible for other columns, selecting by name, with the same syntax.
    We will look at more complex operations, such as **aggregations with GroupBy**,
    in the next chapter.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于其他列是可扩展的，通过名称选择，语法相同。我们将在下一章中讨论更复杂的操作，例如 **带有 GroupBy 的聚合**。
- en: 'Exercise 19: Reading Data from the Local Filesystem and the HDFS'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 19：从本地文件系统和 HDFS 读取数据
- en: 'As we saw before, to read files from the local disk, just give Spark the path
    to it. We can also read several other file formats, located in different storage
    systems. Spark can read files in the following formats:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，要从本地磁盘读取文件，只需将路径提供给 Spark。我们还可以读取位于不同存储系统中的其他文件格式。Spark 可以读取以下格式的文件：
- en: CSV
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV
- en: JSON
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON
- en: ORC
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ORC
- en: Parquet
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet
- en: Text
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本
- en: 'And can read from the following storage systems:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 并且可以从以下存储系统中读取：
- en: JDBC
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDBC
- en: ODBC
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ODBC
- en: Hive
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive
- en: S3
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S3
- en: HDFS
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HDFS
- en: 'Based on a URL scheme, as an exercise, let’s read data from different places
    and formats:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 URL 方案，作为练习，我们来读取来自不同位置和格式的数据：
- en: 'Import the necessary libraries on the Jupyter notebook:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebook 中导入必要的库：
- en: '[PRE15]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s assume that we have to get some data from a JSON file, which is common
    for data collected from APIs on the web. To read a file directly from HDFS, use
    the following URL:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们需要从一个 JSON 文件中获取一些数据，这对于从网络 API 收集的数据很常见。要直接从 HDFS 读取文件，请使用以下 URL：
- en: '[PRE16]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that, with this kind of URL, we have to provide the full address of the
    HDFS endpoint. We could also use only the simplified path, assuming that Spark
    was configured with the right options.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，使用这种 URL 时，我们必须提供 HDFS 端点的完整地址。我们也可以只使用简化路径，前提是 Spark 已经配置了正确的选项。
- en: 'Now, read the data into the Spark object using the following command:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令将数据读取到 Spark 对象中：
- en: '[PRE17]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'So, we choose the format on the `read` method and the storage system on the
    access URL. The same method is used to access JDBC connections, but usually, we
    have to provide a user and a password to connect. Let’s see how to connect to
    a PostgreSQL database:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们在 `read` 方法中选择格式，并在访问 URL 中选择存储系统。对于 JDBC 连接也使用相同的方法，但通常我们需要提供用户名和密码来连接。让我们看看如何连接到
    PostgreSQL 数据库：
- en: '[PRE18]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Exercise 20: Writing Data Back to the HDFS and PostgreSQL'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 20：将数据写回 HDFS 和 PostgreSQL
- en: 'As we saw with pandas, after performing some operations and transformations,
    let’s say that we want to write the results back to the local file system. This
    can be very useful when we finish an analysis and want to share the results with
    other teams, or we want to show our data and results using other tools:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在使用 pandas 时看到的那样，执行一些操作和转换后，假设我们想将结果写回本地文件系统。当我们完成分析并希望与其他团队共享结果时，或者我们希望使用其他工具展示数据和结果时，这非常有用：
- en: 'We can use the `write` method directly on the HDFS from the DataFrame:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以直接在 HDFS 上使用 `write` 方法从 DataFrame 写入：
- en: '[PRE19]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For the relational database, use the same URL and properties dictionary as
    illustrated here:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于关系型数据库，使用与这里演示的相同 URL 和属性字典：
- en: '[PRE20]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This gives Spark great flexibility in manipulating large datasets and combining
    them for analysis.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这使得 Spark 在处理大型数据集并将其组合进行分析时具有极大的灵活性。
- en: Note
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Spark can be used as an intermediate tool to transform data, including aggregations
    or fixing data issues, and saving in a different format for other applications.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Spark 可以作为一个中间工具来转换数据，包括聚合或修复数据问题，并以不同的格式保存供其他应用使用。
- en: Writing Parquet Files
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写入 Parquet 文件
- en: The Parquet data format ([https://parquet.apache.org/](https://parquet.apache.org/))
    is binary, columnar storage that can be used by different tools, including Hadoop
    and Spark. It was built to support compression, to enable higher performance and
    storage use. Its column-oriented design helps with data selection for performance,
    as only the data in the required columns are retrieved, instead of searching for
    the data and discarding values in rows that are not required, reducing the retrieval
    time for big data scenarios, where the data is distributed and on disk. Parquet
    files can also be read and written by external applications, with a C++ library,
    and even directly from pandas.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 数据格式（[https://parquet.apache.org/](https://parquet.apache.org/)）是一种二进制的列式存储格式，可以被不同的工具使用，包括
    Hadoop 和 Spark。它被构建以支持压缩，从而实现更高的性能和存储利用率。它的列式设计有助于在性能上进行数据选择，因为只会检索所需列的数据，而不是在不需要的行中查找并丢弃值，从而减少了大数据场景下的检索时间，在这些场景中数据是分布式并存储在磁盘上。Parquet
    文件也可以通过外部应用程序读取和写入，使用 C++ 库，甚至可以直接从 pandas 中操作。
- en: The Parquet library is currently being developed with the **Arrow project**
    ([https://arrow.apache.org/](https://arrow.apache.org/)).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 库目前正在与 **Arrow 项目**（[https://arrow.apache.org/](https://arrow.apache.org/)）一起开发。
- en: When considering more complex queries in Spark, storing the data in Parquet
    format can increase performance, especially when the queries need to search a
    massive dataset. Compression helps to decrease the data volume that needs to be
    communicated when an operation is being done in Spark, decreasing the network
    I/O. It also supports schemas and nested schemas, similar to JSON, and Spark can
    read the schema directly from the file.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中考虑更复杂的查询时，将数据存储为 Parquet 格式可以提高性能，特别是当查询需要搜索大规模数据集时。压缩有助于减少在 Spark
    执行操作时需要传输的数据量，从而降低网络 I/O。它还支持模式和嵌套模式，类似于 JSON，Spark 可以直接从文件中读取模式。
- en: 'The Parquet writer in Spark has several options, such as mode (append, overwrite,
    ignore or error, the default option) and compression, a parameter to choose the
    compression algorithm. The available algorithms are as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中的 Parquet 写入器有几个选项，例如模式（追加、覆盖、忽略或错误，默认为错误）和压缩，选择压缩算法的参数。可用的算法如下：
- en: '`gzip`'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gzip`'
- en: '`lzo`'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lzo`'
- en: '`brottli`'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`brottli`'
- en: '`lz4`'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lz4`'
- en: Snappy
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速压缩
- en: Uncompressed
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未压缩
- en: The default algorithm is **snappy**.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 默认算法是 **snappy**。
- en: 'Exercise 21: Writing Parquet Files'
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 21：写入 Parquet 文件
- en: 'Let’s say that we received lots of CSV files and we need to do some analysis
    on them. We also need to reduce the data volume size. We can do that using Spark
    and Parquet. Before starting our analysis, let’s convert the CSV files to Parquet:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们收到大量的 CSV 文件，需要对其进行一些分析，并且还需要减小数据体积。我们可以使用 Spark 和 Parquet 来实现。在开始分析之前，让我们将
    CSV 文件转换为 Parquet 格式：
- en: 'First, read the CSV files from the HDFS:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从 HDFS 读取 CSV 文件：
- en: '[PRE21]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Write the CSV files in the DataFrame back to the HDFS, but now in Parquet format:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 DataFrame 中的 CSV 文件以 Parquet 格式写回到 HDFS：
- en: '[PRE22]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now read the Parquet file to a new DataFrame:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将 Parquet 文件读取到一个新的 DataFrame 中：
- en: '[PRE23]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The `write.parquet` method creates a folder named `data_file` with a file with
    a long name such as `part-00000-1932c1b2-e776-48c8-9c96-2875bf76769b-c000.snappy.parquet`.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`write.parquet` 方法会创建一个名为 `data_file` 的文件夹，并生成一个长文件名的文件，例如 `part-00000-1932c1b2-e776-48c8-9c96-2875bf76769b-c000.snappy.parquet`。'
- en: Increasing Analysis Performance with Parquet and Partitions
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Parquet 和分区提高分析性能
- en: An important concept that Parquet supports and that can also increase the performance
    of queries is partitioning. The idea behind partitioning is that data is split
    into divisions that can be accessed faster. The partition key is a column with
    the values used to split the dataset. Partitioning is useful when there are divisions
    in your data that are meaningful to work on separately. For example, if your data
    is based on time intervals, a partition column could be the year value. That way,
    when a query uses a filter value based on the year, only the data in the partition
    that matches the requested year is read, instead of the entire dataset.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 支持并且也能提高查询性能的一个重要概念是分区。分区的想法是将数据拆分成可以更快速访问的部分。分区键是一个列，其值用于拆分数据集。分区在数据中存在有意义的划分时非常有用，这些划分可以单独处理。例如，如果你的数据是基于时间间隔的，则分区列可以是年份值。这样，当查询使用基于年份的筛选值时，只会读取与请求年份匹配的分区中的数据，而不是整个数据集。
- en: 'Partitions can also be nested and are represented by a directory structure
    in Parquet. So, let’s say that we also want to partition by the column month.
    The folder structure of the Parquet dataset would be similar to the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 分区也可以是嵌套的，并通过 Parquet 中的目录结构表示。所以，假设我们还想按月份列进行分区，那么 Parquet 数据集的文件夹结构将类似于以下形式：
- en: '[PRE24]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Partitioning allows better performance when partitions are filtered, as only
    the data in the chosen partition will be read, increasing performance. To save
    partitioned files, the `partitionBy` option should be used, either in the `parquet`
    command or as the previous command chained to the write operation:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当对分区进行过滤时，分区可以提高性能，因为只会读取所选分区中的数据，从而提高性能。要保存分区文件，应该使用 `partitionBy` 选项，可以在 `parquet`
    命令中使用，或者将前一个命令与写入操作链式调用：
- en: '[PRE25]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The alternative method is:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是：
- en: '[PRE26]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The latter format can be used with the previous operations. When reading partitioned
    data, Spark can infer the partition structure from the directory structure.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 后者格式可以与前面的操作一起使用。读取分区数据时，Spark 可以根据目录结构推断分区结构。
- en: An analyst can considerably improve the performance of their queries if partitioning
    is used correctly. But partitioning can hinder performance if partition columns
    are not chosen correctly. For example, if there is only one year in the dataset,
    partitioning per year will not provide any benefits. If there is a column with
    too many distinct values, partitioning using this column could also create problems,
    creating too many partitions that would not improve speed and may even slow things
    down.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果正确使用分区，分析人员可以显著提高查询性能。但如果没有正确选择分区列，分区反而可能会影响性能。例如，如果数据集中只有一年的数据，按年份分区就没有任何好处。如果某列有太多不同的值，按该列进行分区也可能会产生问题，导致创建过多的分区，无法提升速度，甚至可能降低性能。
- en: 'Exercise 22: Creating a Partitioned Dataset'
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 22：创建分区数据集
- en: 'We discovered in our preliminary analysis that the data has date columns, one
    for the year, one for the month, and one for the day. We will be aggregating this
    data to get the minimum, mean, and maximum values per year, per month, and per
    day. Let’s create a partitioned dataset saved in Parquet from our database:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的初步分析中，我们发现数据中包含日期列，其中一个表示年份，一个表示月份，另一个表示日期。我们将对这些数据进行汇总，以获取每年、每月和每天的最小值、平均值和最大值。让我们从数据库中创建一个保存为
    Parquet 格式的分区数据集：
- en: 'Define a PostgreSQL connection:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个 PostgreSQL 连接：
- en: '[PRE27]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Read the data from PostgreSQL to a DataFrame, using the JDBC connector:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 JDBC 连接器从 PostgreSQL 读取数据到 DataFrame：
- en: '[PRE28]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'And let’s convert this into partitioned Parquet:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将把这些数据转换为分区 Parquet 格式：
- en: '[PRE29]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The use of Spark as an intermediary for different data sources, and considering
    its data processing and transformation capabilities, makes it an excellent tool
    for combining and analyzing data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark 作为不同数据源的中介，并考虑其数据处理和转换能力，使其成为结合和分析数据的优秀工具。
- en: Handling Unstructured Data
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理非结构化数据
- en: Unstructured data usually refers to data that doesn’t have a fixed format. CSV
    files are structured, for example, and JSON files can also be considered structured,
    although not tabular. Computer logs, on the other hand, don’t have the same structure,
    as different programs and daemons will output messages without a common pattern.
    Images are also another example of unstructured data, like free text.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据通常指没有固定格式的数据。例如，CSV 文件是结构化的，而 JSON 文件也可以被认为是结构化的，尽管它不是表格形式。另一方面，计算机日志没有固定结构，不同的程序和守护进程会输出没有共同模式的消息。图像也是另一种非结构化数据的例子，类似于自由文本。
- en: We can leverage Spark’s flexibility for reading data to parse unstructured formats
    and extract the required information into a more structured format, allowing analysis.
    This step is usually called **pre-processing** or **data wrangling**.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用 Spark 在读取数据时的灵活性，解析非结构化格式并将所需信息提取到更结构化的格式中，便于分析。这一步通常称为 **预处理** 或 **数据清洗**。
- en: 'Exercise 23: Parsing Text and Cleaning'
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 23：解析文本并清洗数据
- en: 'In this exercise, we will read a text file, split it into lines and remove
    the words `the` and `a` from the string given string:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将读取一个文本文件，将其拆分成行，并从给定的字符串中移除 `the` 和 `a` 这两个词：
- en: 'Read the text file `shake.txt` ([https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson03/data/shake.txt](https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson03/data/shake.txt))
    into the Spark object using the `text` method:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`text`方法将文本文件`shake.txt`（[https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson03/data/shake.txt](https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson03/data/shake.txt)）读入Spark对象：
- en: '[PRE30]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Extract the lines from the text using the following command:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从文本中提取行：
- en: '[PRE31]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This splits each line in the file as an entry in the list. To check the result,
    you can use the `collect` method, which gathers all data back to the driver process:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将把文件中的每一行拆分为列表中的一个条目。要检查结果，可以使用`collect`方法，它会将所有数据收集到驱动程序进程中：
- en: '[PRE32]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, let’s count the number of lines, using the `count` method:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用`count`方法计算行数：
- en: '[PRE33]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Be careful when using the `collect` method! If the DataFrame or RDD being collected
    is larger than the memory of the local driver, Spark will throw an error.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`collect`方法时要小心！如果收集的DataFrame或RDD的大小超过了本地驱动程序的内存，Spark将抛出错误。
- en: 'Now, let’s first split each line into words, breaking it by the space around
    it, and combining all elements, removing words in uppercase:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们首先将每一行拆分为单词，通过周围的空格进行分割，并合并所有元素，移除大写字母的单词：
- en: '[PRE34]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let’s also remove the words `the` and `a`, and punctuations like ‘`.`’, ‘`,`’
    from the given string:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们还移除字符串中的`the`和`a`，以及像‘`.`’，‘`,`’这样的标点符号：
- en: '[PRE35]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Use the following command to remove the stop words from our token list:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从我们的标记列表中移除停用词：
- en: '[PRE36]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We can now process our token list and count the unique words. The idea is to
    generate a list of tuples, where the first element is the token and the second
    element is the count of that particular token.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们现在可以处理我们的标记列表并统计唯一的单词。其思想是生成一个元组列表，其中第一个元素是标记，第二个元素是该标记的计数。
- en: 'Let’s map our token to a list:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将我们的标记映射到列表中：
- en: '[PRE37]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Use the `reduceByKey` operation, which will apply the operation to each of
    the lists:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`reduceByKey`操作，它会对每个列表应用此操作：
- en: '[PRE38]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.3: Parsing Text and Cleaning](img/C12913_03_03.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3：解析文本并清理](img/C12913_03_03.jpg)'
- en: 'Figure 3.3: Parsing Text and Cleaning'
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.3：解析文本并清理
- en: Note
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Remember, `collect()`collects all data back to the driver node! Always check
    whether there is enough memory by using tools such as `top` and `htop`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`collect()`会将所有数据收集到驱动节点！使用`top`和`htop`等工具检查是否有足够的内存。
- en: 'Activity 8: Removing Stop Words from Text'
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动8：从文本中移除停用词
- en: 'In this activity, we will read a text file, split it into lines and remove
    the `stopwords` from the text:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将读取一个文本文件，将其拆分为行，并从文本中移除`停用词`：
- en: Read the text file `shake.txt` as used in Exercise 8\.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取在练习8中使用的文本文件`shake.txt`。
- en: Extract the lines from the text and create a list with each line.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从文本中提取行并创建一个包含每行的列表。
- en: Split each line into words, breaking it by the space around it and remove words
    in uppercase.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每一行拆分为单词，按空格进行分割，并移除大写字母的单词。
- en: 'Remove the stop words: ‘of’, ‘a’, ‘and’, ‘to’ from our token list.'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的标记列表中移除停用词：‘of’，‘a’，‘and’，‘to’。
- en: Process the token list and count the unique words, generating list of tuples
    made up of the token and its count.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理标记列表并统计唯一单词，生成由标记及其计数组成的元组列表。
- en: Map our tokens to a list using the `reduceByKey` operation.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`reduceByKey`操作将我们的标记映射到列表中。
- en: 'The output is as follows:'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.4: Removing Stop Words from Text](img/C12913_03_04.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4：从文本中移除停用词](img/C12913_03_04.jpg)'
- en: 'Figure 3.4: Removing Stop Words from Text'
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.4：从文本中移除停用词
- en: Note
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 213.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第213页找到。
- en: 'We get the list of tuples, where each tuple is a token and the count of the
    number of times that word appeared in the text. Notice that, before the final
    collect on count (an action), the operations that were transformations did not
    start running right away: we needed the action operation count to Spark start
    executing all the steps.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到一个元组列表，每个元组包含一个标记和该单词在文本中出现的次数。请注意，在最终对计数进行`collect`操作（一个动作）之前，作为转换的操作并没有立即执行：我们需要通过`count`这个动作操作来启动Spark执行所有步骤。
- en: Other kinds of unstructured data can be parsed using the preceding example,
    and either operated on directly, such as in the preceding activity or transformed
    into a DataFrame later.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的非结构化数据可以使用前面的示例进行解析，并可以直接操作，如前面的活动所示，或者稍后转换为DataFrame。
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: After a review of what big data is, we learned about some tools that were designed
    for the storage and processing of very large volumes of data. Hadoop is an entire
    ecosystem of frameworks and tools, such as HDFS, designed to store data in a distributed
    fashion in a huge number of commodity-computing nodes, and YARN, a resource and
    job manager. We saw how to manipulate data directly on the HDFS using the HDFS
    fs commands.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了大数据的定义之后，我们学习了一些专为存储和处理大数据量而设计的工具。Hadoop 是一个完整的生态系统，包括 HDFS 等工具和框架，旨在在大量廉价计算节点上以分布式方式存储数据，以及资源和作业管理器
    YARN。我们看到了如何使用 HDFS fs 命令直接操作 HDFS 上的数据。
- en: We also learned about Spark, a very powerful and flexible parallel processing
    framework that integrates well with Hadoop. Spark has different APIs, such as
    SQL, GraphX, and Streaming. We learned how Spark represents data in the DataFrame
    API and that its computation is similar to pandas’ methods. We also saw how to
    store data in an efficient manner using the Parquet file format, and how to improve
    performance when analyzing data using partitioning. To finish up, we saw how to
    handle unstructured data files, such as text.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了关于 Spark 的知识，这是一个非常强大和灵活的并行处理框架，与 Hadoop 集成良好。Spark 拥有不同的 API，如 SQL、GraphX
    和 Streaming。我们学习了 Spark 如何使用 DataFrame API 表示数据，以及其计算方式类似于 pandas 的方法。我们还看到了如何使用
    Parquet 文件格式高效存储数据，并在分析数据时通过分区来提高性能。最后，我们学习了如何处理诸如文本之类的非结构化数据文件。
- en: In the next chapter, we will go more deeply into how to create a meaningful
    statistical analysis using more advanced techniques with Spark and how to use
    Jupyter notebooks with Spark.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨如何使用更高级的技术和 Spark 进行有意义的统计分析，并学习如何在 Spark 中使用 Jupyter 笔记本。
