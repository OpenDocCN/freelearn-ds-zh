- en: Chapter 8.  Adapting Your Machine Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。调整您的机器学习模型
- en: 'This chapter covers advanced **machine learning** (**ML**) techniques in order
    to be able to make algorithms adaptable to new data. Readers will also see how
    machine learning algorithms learn incrementally over the data, that is to say
    that the models are updated each time they see a new training instance. Learning
    in dynamic environments by conceding different constraints will also be discussed.
    In summary, the following topics will be covered in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了高级**机器学习**（**ML**）技术，以便能够使算法适应新数据。读者还将看到机器学习算法如何逐渐学习数据，也就是说，每次看到新的训练实例时，模型都会更新。还将讨论在动态环境中通过让步不同的约束来学习。总之，本章将涵盖以下主题：
- en: Adapting machine learning models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适应机器学习模型
- en: Generalization of ML models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML模型的泛化
- en: Adapting through incremental algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过增量算法进行适应
- en: Adapting through reusing ML models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过重用ML模型进行适应
- en: Machine learning in dynamic environments
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态环境中的机器学习
- en: Adapting machine learning models
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的适应
- en: As we discussed earlier, as a part of the ML training process, a model is trained
    using a set of data (that is, training, test, and validation set). Machine learning
    models that can adapt to their environments and learn from their experience have
    attracted consumers and researchers from diverse areas, including computer science,
    engineering, mathematics, physics, neuroscience, and cognitive science. In this
    section, we will provide a technical overview of how to adopt machine learning
    models for the new data and requirements.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，作为ML训练过程的一部分，模型是使用一组数据进行训练的（即训练、测试和验证集）。能够适应其环境并从经验中学习的机器学习模型吸引了来自不同领域的消费者和研究人员，包括计算机科学、工程、数学、物理学、神经科学和认知科学。在本节中，我们将提供如何为新数据和需求采用机器学习模型的技术概述。
- en: Technical overview
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术概述
- en: Technically, the same models might need to be retrained at a later stage if
    required for the betterment. This is really dependent on several factors, for
    example, when the new data becomes available, or when the consumer of the API
    has their own data to train the model or when the data needs to be filtered and
    the model trained with the subset of data. In these scenarios, ML algorithms should
    provide enough APIs to provide a convenient way to allow its consumers to produce
    a client that can be used on a one-time or regular basis, so that they can retrain
    the model using their own data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，如果需要，同样的模型可能需要在以后的阶段进行重新训练以改进。这实际上取决于几个因素，例如新数据何时可用，或者API的使用者是否有自己的数据来训练模型，或者数据是否需要被过滤并且模型需要用数据子集进行训练。在这些情景中，ML算法应该提供足够的API，以便为其消费者提供方便的方式，使他们能够一次或定期地使用客户端，以便他们可以使用自己的数据重新训练模型。
- en: 'As a result, the client will be able to evaluate the results of retraining
    and updating the web service API accordingly. Alternatively, they will be able
    to use the newly trained model. In this regard, there are several contexts of
    domain adaptation. However, they differ in the information considered for application
    type and requirements:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，客户将能够评估重新训练和更新Web服务API的结果。或者，他们将能够使用新训练的模型。在这方面，域自适应有几种不同的情境。但是，它们在考虑应用类型和需求方面有所不同：
- en: '**Unsupervised domain adaptation**: The learning sample contains a set of labeled
    source examples, a set of unlabeled source examples, and an unlabeled set of target
    examples'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督域自适应**：学习样本包含一组标记的源示例、一组未标记的源示例和一组未标记的目标示例'
- en: '**Semi-supervised domain adaptation**: In this situation, we also consider
    a small set of labeled target examples'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半监督域自适应**：在这种情况下，我们还考虑了一小部分标记的目标示例'
- en: '**Supervised domain adaptation**: All the examples considered are supposed
    to be labeled:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督域自适应**：所有考虑的示例都应该被标记：'
- en: '![Technical overview](img/00017.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![技术概述](img/00017.jpeg)'
- en: 'Figure 1: The retraining process overview (dashed line presents the retrain
    steps)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：重新训练过程概述（虚线表示重新训练步骤）
- en: 'Technically, there should be three alternatives for making the ML models adaptable:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，制作ML模型适应的方法应该有三种选择：
- en: The most widely used machine learning techniques and algorithms include decision
    trees, decision rules, neural networks, statistical classifiers, and probabilistic
    graphical models, and they all need to be developed so that they can be adaptable
    for the new requirements
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最广泛使用的机器学习技术和算法包括决策树、决策规则、神经网络、统计分类器和概率图模型，它们都需要进行开发，以便能够适应新的需求
- en: Secondly, previous mentioned algorithms or techniques should be generalized
    so that they can be used with minimum effort
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，先前提到的算法或技术应该被泛化，以便能够以最小的努力使用
- en: Moreover, more robust theoretical frameworks and algorithms such as Bayesian
    learning theory, classical statistical theory, minimum description length theory,
    and statistical mechanics approaches need to be developed in order to understand
    computational learning theory
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，还需要开发更加健壮的理论框架和算法，如贝叶斯学习理论、经典统计理论、最小描述长度理论和统计力学方法，以便理解计算学习理论
- en: The benefits from these three adaptation properties and techniques will provide
    insight into experimental results that will also guide machine learning communities
    to contribute towards the different learning algorithms.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种适应性属性和技术的好处将为实验结果提供见解，也将指导机器学习社区为不同的学习算法做出贡献。
- en: The generalization of ML models
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML模型的泛化
- en: In [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Example*, we discussed how and why to generalize the
    learning algorithm to fit semi-supervised learning, active learning, structured
    prediction, and reinforcement learning. In this section, we will discuss how to
    generalize the linear regression algorithm on the **Optical Character Recognition**
    (**OCR**) dataset to show an example of generalization of the linear regression
    model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d "第5章。通过示例进行监督和无监督学习")中，*通过示例进行监督和无监督学习*，我们讨论了如何以及为什么将学习算法推广到适应半监督学习、主动学习、结构化预测和强化学习。在本节中，我们将讨论如何将线性回归算法推广到光学字符识别（OCR）数据集上，以展示线性回归模型的推广示例。
- en: Generalized linear regression
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广义线性回归
- en: As discussed in [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Example*, the linear regression and logistic regression
    techniques assume that the output follows a Gaussian distribution. The **generalized
    linear models** (**GLMs**) on the other hand are specifications of the linear
    models where the response variable, that is, Yi, follows the linear distribution
    from an exponential family of distribution.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[第5章](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d "第5章。通过示例进行监督和无监督学习")中讨论的，*通过示例进行监督和无监督学习*，线性回归和逻辑回归技术假设输出遵循高斯分布。另一方面，广义线性模型（GLMs）是线性模型的规范，其中响应变量Yi遵循来自分布的指数家族的线性分布。
- en: Spark's `GeneralizedLinearRegression` API allows us a flexible specification
    of the GLMs. The current implementation of the generalized linear regression in
    Spark can be used for numerous types of prediction problems, for example, linear
    regression, Poisson regression, logistic regression, and others.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`GeneralizedLinearRegression` API允许我们灵活地指定GLMs。Spark中广义线性回归的当前实现可用于多种类型的预测问题，例如线性回归、泊松回归、逻辑回归等。
- en: However, the limitation is that only a subset of the exponential family distributions
    is supported in the current implementation of the Spark-based GLM algorithm. In
    addition, there is another scalability issue that only 4096 features are supported
    through its `GeneralizedLinearRegression` API. Consequently, if the number of
    features are beyond 4096, the algorithm will throw an exception.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当前实现的Spark GLM算法仅支持指数家族分布的子集。此外，还存在另一个可伸缩性问题，即其`GeneralizedLinearRegression`
    API仅支持4096个特征。因此，如果特征数量超过4096，算法将抛出异常。
- en: Fortunately, your models with an increased number of features can be trained
    using the LinearRegression and LogisticRegression estimators, as shown in several
    examples in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，可以使用LinearRegression和LogisticRegression估计器训练具有增加特征数量的模型，正如在[第6章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "第6章。构建可扩展的机器学习管道")中的几个示例中所示，*构建可扩展的机器学习管道*。
- en: Generalized linear regression with Spark
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark进行广义线性回归
- en: In this sub-section, we discuss a step-by-step example that shows how to apply
    the generalized linear regression on the `libsvm` version of the **Optical Character
    Recognition** (**OCR**) data that we discussed in [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*.
    Since the same dataset will be re-used here, we've decided not to describe them
    further.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将讨论一个逐步示例，展示如何在我们在[第7章](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "第7章。调整机器学习模型")中讨论的光学字符识别（OCR）数据的`libsvm`版本上应用广义线性回归。由于这里将重复使用相同的数据集，我们决定不再详细描述它们。
- en: '**Step 1: Load the necessary API and packages**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：加载必要的API和软件包**'
- en: 'Here is the code to load the necessary API and packages:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载必要API和软件包的代码：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2: Create the Spark session**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：创建Spark会话**'
- en: 'The following code shows how to create the Spark session:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了如何创建Spark会话：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step 3: Load and create the Dataset**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：加载和创建数据集**'
- en: 'Load and create the dataset from the OCR dataset. Here we have specified the
    dataset format as `libsvm`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从OCR数据集加载和创建数据集。在这里，我们指定了数据集格式为`libsvm`。
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Step 4: Prepare the training and test sets**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：准备训练和测试集**'
- en: 'The following code illustrates how to prepare the training and test sets:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码说明了如何准备训练和测试集：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step 5: Create a generalized linear regression estimator**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：创建广义线性回归估计器**'
- en: 'Create the generalized linear regression estimator by specifying the family,
    link, and max iteration and regression parameters. Here we have selected the family
    as `"gaussian"` and link as `"identity"`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定家族、链接和最大迭代和回归参数来创建广义线性回归估计器。在这里，我们选择了家族为“高斯”和链接为“身份”：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Note that according to the API documentation at [http://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression](http://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression),
    the following options are supported with this algorithm implementation with Spark:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，根据Spark的API文档[http://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression](http://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression)，此算法实现与Spark支持以下选项：
- en: '![Generalized linear regression with Spark](img/00153.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark进行广义线性回归](img/00153.jpeg)'
- en: 'Figure 2: Available supported families with the current implementation of Generalized
    Linear Regression'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：当前实现的广义线性回归支持的家族
- en: '**Step 6: Fit the model**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：拟合模型**'
- en: 'Here is the code to fit the model:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是拟合模型的代码：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Step 7: Check the coefficients and intercept**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：检查系数和截距**'
- en: 'Print the coefficients and intercept for the linear regression model that we
    created in Step 6:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 打印我们在第6步创建的线性回归模型的系数和截距：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Output for these two parameters will be similar to the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个参数的输出将类似于以下内容：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It is to be noted that the `System.out.println` method will not work in cluster
    mode. This will work only in standalone or Pseudo mode. It is only for the verification
    of the result.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是`System.out.println`方法在集群模式下不起作用。这只在独立模式或伪模式下起作用。这仅用于验证结果。
- en: '**Step 8: Summarize the model**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8：总结模型**'
- en: 'Summarize the model over the training set and print out some metrics:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总结训练集上的模型并打印出一些指标：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Step 9: Verify some generalized metrics**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9：验证一些广义指标**'
- en: 'Let''s print some generalized metrics such as **Coefficient Standard Errors**
    (**CSE**), T values, P values, Dispersions, Null deviance, the Residual degree
    of freedom null, AIC and Deviance residuals. Owing to the page limitations, we
    have not shown how these values are significant or the calculations procedure:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印一些广义指标，如**系数标准误差**（**CSE**）、T值、P值、离散度、零偏差、零残差自由度、AIC和偏差残差。由于页面限制，我们没有展示这些值的重要性或计算过程：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s see the values for the training set we created previously:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们之前创建的训练集的值：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Step 10: Show the deviance residuals**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤10：显示偏差残差**'
- en: 'The following code is used to show the deviance residuals:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于显示偏差残差：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Generalized linear regression with Spark](img/00002.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark进行广义线性回归](img/00002.jpeg)'
- en: 'Figure 3: Summary of the deviance residuals for the OCR dataset'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：OCR数据集的偏差残差总结
- en: Tip
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Interested readers should refer to the following web page to get more information
    on and insight into this algorithm and its implementation details: [http://spark.apache.org/docs/latest/ml-classification-regression.html](http://spark.apache.org/docs/latest/ml-classification-regression.html)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣的读者应参考以下网页，以获取有关该算法及其实现细节的更多信息和见解：[http://spark.apache.org/docs/latest/ml-classification-regression.html](http://spark.apache.org/docs/latest/ml-classification-regression.html)
- en: Adapting through incremental algorithms
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过增量算法进行适应
- en: 'According to Robi Polikar et al., (*Learn++: An Incremental Learning Algorithm
    for Supervised Neural Networks, IEEE Transactions on Systems, Man, And Cybernetics,
    V-21, No-4, November 2001*), various algorithms have been suggested for incremental
    learning. The incremental learning is therefore implied for solving different
    problems. In some literature, the term incremental learning has been used to refer
    to either the growing of or pruning of a classifier. Alternatively, it may refer
    to the selection of most informative training samples for solving a problem in
    an incremental way.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '根据Robi Polikar等人的说法（*Learn++: An Incremental Learning Algorithm for Supervised
    Neural Networks, IEEE Transactions on Systems, Man, And Cybernetics, V-21, No-4,
    November 2001*），已经提出了各种算法用于增量学习。因此，增量学习被暗示用于解决不同的问题。在一些文献中，增量学习一词被用来指代分类器的生长或修剪。或者，它可能指的是以增量方式选择最具信息量的训练样本来解决问题。'
- en: In other cases, making a regular ML algorithm incremental means performing some
    form of controlled modification of weights in the classifier, by retraining with
    misclassified signals. Some algorithms are capable of learning new information;
    however, they do not synchronously satisfy all of the previously mentioned criteria.
    Moreover, they either require access to the old data or need to forget the prior
    knowledge along the way, and as they are unable to accommodate new classes they
    are not adaptable for new datasets.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，使常规的ML算法增量意味着通过对分类器中的权重进行一定形式的受控修改，通过对错误分类的信号进行重新训练。一些算法能够学习新信息；然而，它们并不同步满足先前提到的所有标准。此外，它们要么需要访问旧数据，要么需要在途中忘记先前的知识，由于它们无法适应新类别，因此对新数据集不具有适应性。
- en: Considering the previously mentioned issues, in this section, we will discuss
    how to adopt ML models using an incremental version of the original algorithms.
    Incremental SVM, Bayesian Network, and Neural networks will be discussed in brief.
    Moreover, when applicable, we will provide regular Spark implementation of these
    algorithms.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到先前提到的问题，在本节中，我们将讨论如何使用原始算法的增量版本来采用ML模型。我们将简要讨论增量SVM、贝叶斯网络和神经网络。此外，如果适用，我们将提供这些算法的常规Spark实现。
- en: Incremental support vector machine
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增量支持向量机
- en: It's pretty difficult to make a regular ML algorithm incremental. In short,
    it's possible but not altogether easy. If you want to do it you have to change
    the underlying source codes in the Spark library you are using or implement the
    training algorithm yourself.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使常规ML算法增量是相当困难的。简而言之，这是可能的，但并不十分容易。如果您想要这样做，您必须更改您正在使用的Spark库的底层源代码，或者自己实现训练算法。
- en: Unfortunately, Spark does not have an incremental version of SVM implemented.
    However, before making the linear SVM incremental, you need to first understand
    the linear SVM itself. Therefore, we provide some concepts of linear SVMs in the
    next sub-section using Spark for the new dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Spark没有实现增量版本的SVM。然而，在使线性SVM增量之前，您需要先了解线性SVM本身。因此，我们将在下一个子节中使用Spark为新数据集提供一些线性SVM的概念。
- en: Tip
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'According to our knowledge, we have found only two possible solutions called
    SVMHeavy ([http://people.eng.unimelb.edu.au/shiltona/svm/](http://people.eng.unimelb.edu.au/shiltona/svm/))
    and LaSVM ([http://leon.bottou.org/projects/lasvm](http://leon.bottou.org/projects/lasvm)),
    which support incremental training. But we haven''t used either. Interested readers
    should follow these two papers on incremental SVMs to get some insight. These
    two papers are straightforward and show good research if you''re just getting
    started:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，我们发现了只有两种可能的解决方案，称为SVMHeavy ([http://people.eng.unimelb.edu.au/shiltona/svm/](http://people.eng.unimelb.edu.au/shiltona/svm/))
    和 LaSVM ([http://leon.bottou.org/projects/lasvm](http://leon.bottou.org/projects/lasvm))，它们支持增量训练。但我们没有使用任何一种。有兴趣的读者应该阅读这两篇关于增量SVM的论文以获取一些见解。这两篇论文都很简单，展示了很好的研究，如果您刚开始学习的话：
- en: '[http://cbcl.mit.edu/cbcl/publications/ps/cauwenberghs-nips00.pdf](http://cbcl.mit.edu/cbcl/publications/ps/cauwenberghs-nips00.pdf).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://cbcl.mit.edu/cbcl/publications/ps/cauwenberghs-nips00.pdf](http://cbcl.mit.edu/cbcl/publications/ps/cauwenberghs-nips00.pdf)。'
- en: '[http://www.jmlr.org/papers/volume7/laskov06a/laskov06a.pdf](http://www.jmlr.org/papers/volume7/laskov06a/laskov06a.pdf).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.jmlr.org/papers/volume7/laskov06a/laskov06a.pdf](http://www.jmlr.org/papers/volume7/laskov06a/laskov06a.pdf)。'
- en: Adapting SVMs for new data with Spark
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spark适应新数据的SVM
- en: In this section, we will first discuss how to perform binary classification
    using linear SVMs of Spark implementation. Then we will show how to adopt the
    same algorithm for the new data type.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先讨论如何使用Spark实现的线性SVM进行二元分类。然后我们将展示如何将相同的算法应用于新数据类型。
- en: '**Step 1: Data collection and exploration**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤1：数据收集和探索
- en: 'We have collected a colon cancer dataset from [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html).
    Originally, the dataset was labeled as -1.0 and 1.0 as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html)收集了一个结肠癌数据集。最初，数据集的标签为-1.0和1.0如下：
- en: '![Adapting SVMs for new data with Spark](img/00096.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark适应新数据的SVM](img/00096.jpeg)'
- en: 'Figure 4: Original colon cancer data snapshot'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：原始结肠癌数据快照
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'The dataset was used in the following publication: *U. Alon, N. Barkai, D.
    A. Notterman, K. Gish, S.Ybarra, D.Mack, and A. J. Levine. Broad patterns of gene
    expression revealed by clustering analysis of tumour and normal colon tissues
    probed by oligonucleotide arrays*. *Cell Biology, 96:6745-6750, 1999*. Interested
    readers should refer to the publication to get more insights into the dataset.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集在以下出版物中使用：*U. Alon, N. Barkai, D. A. Notterman, K. Gish, S.Ybarra, D.Mack,
    and A. J. Levine. Broad patterns of gene expression revealed by clustering analysis
    of tumour and normal colon tissues probed by oligonucleotide arrays*. *Cell Biology,
    96:6745-6750, 1999*。感兴趣的读者应参考该出版物以获取有关数据集的更多见解。
- en: 'After that, instance-wise normalization is carried out to mean zero and variance
    one. Then feature wise normalization is carried out to get zero and variance one
    as a pre-processing step. However, for simplicity, we have considered -1.0 as
    0.1, since SVM does not recognize symbols (that is, + or -). Therefore, the dataset
    now contains two labels 1 and 0 (that is, to say it''s a binary classification
    problem). After pre-processing and scaling, there are two classes and 2000 features.
    Here is a sample of the dataset in *Figure 5*:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，进行实例归一化以使均值为零，方差为一。然后进行特征归一化以获得零均值和方差为一作为预处理步骤。然而，为简单起见，我们将-1.0视为0.1，因为SVM不识别符号（即+或-）。因此，数据集现在包含两个标签1和0（即，这是一个二元分类问题）。经过预处理和缩放后，有两个类别和2000个特征。以下是数据集的示例*图5*：
- en: '![Adapting SVMs for new data with Spark](img/00051.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark适应新数据的SVM](img/00051.jpeg)'
- en: 'Figure 5: Pre-processed colon cancer data'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：预处理的结肠癌数据
- en: '**Step 2: Load the necessary packages and APIs**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2：加载必要的软件包和API
- en: 'Here is the code to load the necessary packages:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载必要软件包的代码：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Step 3: Configure the Spark session**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤3：配置Spark会话
- en: 'The following code helps us to create the Spark session:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码帮助我们创建Spark会话：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Step 4: Create a Dataset out of the data**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤4：从数据中创建数据集
- en: 'Here is the code to create a Dataset:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建数据集的代码：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Step 5: Prepare the training and test sets**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤5：准备训练和测试集
- en: 'Here is the code to prepare the training and test sets:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是准备训练和测试集的代码：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Step 6: Build and train the SVM model**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤6：构建和训练SVM模型
- en: 'The following code illustrates how to build and train the SVM model:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码说明了如何构建和训练SVM模型：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Step 7: Compute the raw prediction score on the test set**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤7：在测试集上计算原始预测分数
- en: 'Here is the code to compute the raw prediction:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是计算原始预测的代码：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Step 8: Evaluate the model**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤8：评估模型
- en: 'Here is the code to evaluate the model:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是评估模型的代码：
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: However, the value of ROC is between 0.5 and 1.0\. Where the value is more than
    0.8 this indicates a good classifier and if the value of ROC is less than 0.8,
    this signals a bad classifier. The `SVMWithSGD.train()` method by default performs
    Level Two (L2) regularization with the regularization parameter set to 1.0.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，ROC的值在0.5和1.0之间。当值大于0.8时，这表明是一个好的分类器，如果ROC的值小于0.8，则表示是一个糟糕的分类器。`SVMWithSGD.train()`方法默认执行二级（L2）正则化，正则化参数设置为1.0。
- en: If you want to configure this algorithm, you should customize the `SVMWithSGD`
    further by creating a new object directly. After that, you can further the setter
    methods to set the value of the object.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要配置此算法，应通过直接创建新对象来进一步定制`SVMWithSGD`。之后，可以使用setter方法来设置对象的值。
- en: Interestingly, all the other Spark MLlib algorithms can be customized this way.
    However, after the customization has been completed, you need to build the source
    code to make changes up to API level. Interested readers can add themselves to
    the Apache Spark mailing list if they want to contribute to the open source.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，所有其他Spark MLlib算法都可以以这种方式定制。然而，在定制完成后，您需要构建源代码以进行API级别的更改。有兴趣的读者可以加入Apache
    Spark邮件列表，如果他们想为开源项目做出贡献。
- en: Note the source code of Spark is available on GitHub at the URL [https://github.com/apache/spark](https://github.com/apache/spark)
    as an open source and it sends pull requests to enrich Spark. More technical discussion
    can be found at the Spark website at [http://spark.apache.org/](http://spark.apache.org/).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Spark的源代码可以在GitHub上找到，网址为[https://github.com/apache/spark](https://github.com/apache/spark)，作为开源项目，它发送拉取请求来丰富Spark。更多技术讨论可以在Spark网站上找到，网址为[http://spark.apache.org/](http://spark.apache.org/)。
- en: 'For example, the following code produces a level one (`L1`) regularized variant
    of SVMs with the regularization parameter set to 0.1, and runs the training algorithm
    for 500 iterations as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下代码生成了一个正则化变体（`L1`）的SVM，正则化参数设置为0.1，并且运行训练算法500次，如下所示：
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Your model is now trained. Now if you perform *step 7* and *step 8*, the following
    metrics will be generated:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型现在已经训练好了。现在，如果您执行*步骤7*和*步骤8*，将生成以下指标：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If you compare this result with the result produced in *step 8*, it's much better
    now, isn't it? However, depending on the data preparation, you might experience
    different results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将这个结果与*步骤8*中产生的结果进行比较，现在它会好得多，不是吗？然而，根据数据准备的不同，您可能会得到不同的结果。
- en: It indicates a better classification (please see also at [https://www.researchgate.net/post/What_is_the_value_of_the_area_under_the_roc_curve_AUC_to_conclude_that_a_classifier_is_excellent](https://www.researchgate.net/post/What_is_the_value_of_the_area_under_the_roc_curve_AUC_to_conclude_that_a_classifier_is_excellent)).
    In this way the SVM can be optimized or adaptive for the new data type.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 它指示了更好的分类（请参见[https://www.researchgate.net/post/What_is_the_value_of_the_area_under_the_roc_curve_AUC_to_conclude_that_a_classifier_is_excellent](https://www.researchgate.net/post/What_is_the_value_of_the_area_under_the_roc_curve_AUC_to_conclude_that_a_classifier_is_excellent)）。通过这种方式，支持向量机可以针对新的数据类型进行优化或自适应。
- en: However, the parameters (that is, number of iterations, regression params, and
    updater) should be set accordingly.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，参数（即迭代次数、回归参数和更新器）应该相应地设置。
- en: Incremental neural networks
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增量神经网络
- en: The incremental version of the neural network in R or Mat lab provides adaptability
    using the adapt function. Does this update instead of overwriting iteratively?
    To verify this statement, readers can try using the R or Mat lab version of the
    incremental neural network-based classifier that may need to select a subset of
    your first data chunk as the second chunk in training. If it is overwriting, when
    you use the trained net with the subset to test your first data chunk, it will
    likely poorly predict the data that does not belong to the subset.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: R或Matlab中的神经网络的增量版本使用adapt函数提供了适应性。这种更新是否是迭代地而不是覆盖地进行？为了验证这个说法，读者可以尝试使用R或Matlab版本的增量神经网络分类器，可能需要选择第一个数据块的子集作为训练中的第二个块。如果是覆盖的，当您使用经过训练的网络与子集一起测试第一个数据块时，它可能会对不属于子集的数据进行糟糕的预测。
- en: Multilayer perceptron classification with Spark
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spark进行多层感知器分类
- en: To date, there is no implementation of the incremental version of the neural
    network in Spark yet. According to the API documentation provided at [https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier](https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier),
    Spark's **Multilayer Perceptron Classifier** (**MLPC**) is a classifier based
    on the **Feedforward Artificial Neural Network** (**FANN**). The MLPC consists
    of multiple layers of nodes including hidden layers. Each layer is fully connected
    to the next layer and so on in a network. A node in the input layer represents
    the input data. All other nodes map inputs to outputs by a linear combination
    of the inputs with the node's weights *w* and bias *b* and by applying the activation
    function. The number of nodes *N* in the output layer corresponds to the number
    of classes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，Spark尚未实现神经网络的增量版本。根据提供在[https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier](https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier)的API文档，Spark的**多层感知器分类器**（**MLPC**）是基于**前馈人工神经网络**（**FANN**）的分类器。MLPC包括多层节点，包括隐藏层。每一层都与下一层等等完全连接在一起。输入层中的节点表示输入数据。所有其他节点通过输入的线性组合与节点的权重*w*和偏差*b*以及应用激活函数将输入映射到输出。输出层中的节点数*N*对应于类的数量。
- en: 'MLPC also performs backpropagation for learning the model. Spark uses the logistic
    loss function for optimization and **Limited-memory Broyden-Fletcher-Goldfarb-Shanno**
    (**L-BFGS**) as an optimization routine. Note that the L-BFGS is an optimization
    algorithm in the family of **Quasi-Newton Method** (**QNM**) that approximates
    the Broyden-Fletcher-Goldfarb-Shanno algorithm using a limited main memory. To
    train the multilayer perceptron classifier, the following parameters need to be
    set:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: MLPC还执行反向传播来学习模型。Spark使用逻辑损失函数进行优化，**有限内存的Broyden-Fletcher-Goldfarb-Shanno**（**L-BFGS**）作为优化例程。请注意，L-BFGS是**拟牛顿法**（**QNM**）家族中的一种优化算法，它使用有限的主内存来近似Broyden-Fletcher-Goldfarb-Shanno算法。为了训练多层感知器分类器，需要设置以下参数：
- en: Layer
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层
- en: Tolerance of iteration
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代的容限
- en: The block size of the learning
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习的块大小
- en: Seed size
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 种子大小
- en: Max iteration number
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大迭代次数
- en: Note the layers consist of the input, hidden, and output layers. Moreover, a
    smaller value of convergence tolerance will lead to higher accuracy with the cost
    of more iterations. The default block size parameter is 128 and the maximum number
    of iteration is set to be 100 as a default value. We suggest you set these values
    accordingly and carefully.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，层包括输入层、隐藏层和输出层。此外，收敛容限的较小值将导致更高的准确性，但需要更多的迭代。默认的块大小参数为128，最大迭代次数默认设置为100。我们建议您相应地和谨慎地设置这些值。
- en: In this sub-section, we will show how Spark has implemented the neural network
    learning algorithms through the multilayer perception classifier on the Iris dataset.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小节中，我们将展示Spark如何通过Iris数据集实现了神经网络学习算法的多层感知器分类器。
- en: '**Step 1: Dataset collection, processing, and exploration**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：数据集收集、处理和探索**'
- en: 'The original Iris plant dataset was collected from the UCI machine learning
    repositories ([http://www.ics.uci.edu/~mlearn/MLRepository.html](http://www.ics.uci.edu/~mlearn/MLRepository.html))
    and then pre-processed, scaled to libsvm format by Chang et al., and placed as
    the libsvm a comprehensive library for support vector machine at ([https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html))
    for the binary, multi-class, and multi-label classification task. The Iris dataset
    contains three classes and four features, where the sepal and petal lengths are
    scaled according to the libsvm format. More specifically, here is the attribute
    information:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的鸢尾植物数据集是从UCI机器学习仓库（[http://www.ics.uci.edu/~mlearn/MLRepository.html](http://www.ics.uci.edu/~mlearn/MLRepository.html)）收集的，然后由Chang等人进行了预处理，缩放到libsvm格式，并放置在libsvm支持向量机的综合库中，网址为（[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html)）用于二进制、多类别和多标签分类任务。鸢尾花数据集包含三个类别和四个特征，其中萼片和花瓣的长度根据libsvm格式进行了缩放。更具体地说，这是属性信息：
- en: 'Class: Iris Setosa, Iris Versicolour, Iris Virginica (column 1)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别：鸢尾花Setosa、鸢尾花Versicolour、鸢尾花Virginica（第1列）
- en: Sepal length in cm (column 2)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 厘米的萼片长度（第2列）
- en: Sepal width in cm (column 3)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 厘米的萼片宽度（第3列）
- en: Petal length in cm (column 4)
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 厘米的花瓣长度（第4列）
- en: Petal width in cm (column 5)
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 厘米的花瓣宽度（第5列）
- en: 'A snapshot of the dataset is shown in *Figure 6*:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的快照显示在*图6*中：
- en: '![Multilayer perceptron classification with Spark](img/00044.jpeg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark进行多层感知器分类](img/00044.jpeg)'
- en: 'Figure 6: Irish dataset snapshot'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：鸢尾花数据集快照
- en: '**Step 2: Load the required packages and APIs**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：加载所需的包和API**'
- en: 'Here is the code to load the required packages and APIs:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载所需包和API的代码：
- en: '[PRE21]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Step 3: Create a Spark session**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：创建一个Spark会话**'
- en: 'The following code helps us to create the Spark session:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码帮助我们创建Spark会话：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Note, the `mySession()` method that creates and returns a Spark session object
    is as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，创建并返回Spark会话对象的`mySession()`方法如下：
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Step 4: Parse and prepare the dataset**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：解析和准备数据集**'
- en: 'Load the input data as `libsvm` format:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入数据加载为`libsvm`格式：
- en: '[PRE24]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Step 5: Prepare the training and test set**'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：准备训练和测试集**'
- en: 'Prepare the training and test set: training = 70%, test = 30%, and seed = 12345L:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 准备训练和测试集：训练= 70%，测试= 30%，种子= 12345L：
- en: '[PRE25]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Step 6: Specify the layers for the neural network**'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：为神经网络指定层**'
- en: 'Specify the layers for the neural network. Here, input layer size 4 (features),
    two intermediate layers (that is, hidden layers) of size 4 and 3, and output size
    3 (classes):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为神经网络指定层。这里，输入层大小为4（特征），两个中间层（即隐藏层）的大小分别为4和3，输出大小为3（类别）：
- en: '[PRE26]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Step 7: Create the multilayer perceptron estimator**'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：创建多层感知器估计器**'
- en: Create the `MultilayerPerceptronClassifier` trainer and set its parameters.
    Here, set the value of `param [[layers]]` using the `setLayers()` method from
    *Step 6*. Set the convergence tolerance of iterations using the `setTol()` method,
    since, a smaller value will lead to higher accuracy with the cost of more iterations.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`MultilayerPerceptronClassifier`训练器并设置其参数。在这里，使用*步骤6*中的`setLayers()`方法设置`param
    [[layers]]`的值。使用`setTol()`方法设置迭代的收敛容限，因为较小的值会导致更高的准确性，但需要更多的迭代。
- en: 'Note the default is `1E-4`. Set the value of Param `[[blockSize]]` using the
    `setBlockSize()` method, where the default is 128KB. Set the seed for weight initialization
    if the weights using the `setInitialWeights()` are not set. Finally, set the maximum
    number of iterations using the `setMaxIter()` method, where the default is 100:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意默认值为`1E-4`。使用`setBlockSize()`方法设置Param `[[blockSize]]`的值，默认为128KB。如果未设置权重，可以使用`setInitialWeights()`设置权重初始化的种子。最后，使用`setMaxIter()`方法设置最大迭代次数，默认为100：
- en: '[PRE27]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**Step 8: Train the model**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8：训练模型**'
- en: 'Train the `MultilayerPerceptronClassificationModel` using the preceding estimator
    from *step 7*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*步骤7*中的先前估计器训练`MultilayerPerceptronClassificationModel`：
- en: '[PRE28]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Step 9: Compute the accuracy on the test set**'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9：在测试集上计算准确率**'
- en: 'Here is the code to compute the accuracy on the test set:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在测试集上计算准确率的代码：
- en: '[PRE29]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Step 10: Evaluate the model**'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤10：评估模型**'
- en: 'Evaluate the model, calculate the metrics`, and print the accuracy, weighted
    precision and weighted recall:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型，计算指标，并打印准确率、加权精度和加权召回率：
- en: '[PRE30]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output should appear as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE31]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Step 11: Stop the Spark session**'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤11：停止Spark会话**'
- en: 'The following code is used to stop the Spark session:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于停止Spark会话：
- en: '[PRE32]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: From the preceding prediction metrics, it is clear that the classification task
    is quite impressive. Now it's your turn to make your model adaptable. Now try
    training and testing with the new dataset and make your ML model adaptable.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 从先前的预测指标可以看出，分类任务相当令人印象深刻。现在轮到你使你的模型适应了。现在尝试使用新数据集进行训练和测试，并使你的ML模型适应。
- en: Incremental Bayesian networks
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增量贝叶斯网络
- en: As we discussed earlier, Naive Bayes is a simple multiclass classification algorithm
    with the assumption of independence between each pair of features. The Naive Bayes
    based model can be trained very efficiently. The model can compute the conditional
    probability distribution of each feature, given the label, since a pass to the
    training data. After that, it applies the Bayes theorem to compute the conditional
    probability distribution of the labels for making the prediction.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，朴素贝叶斯是一种简单的多类别分类算法，假设每对特征之间都是独立的。基于朴素贝叶斯的模型可以被训练得非常高效。该模型可以计算每个特征在给定标签的条件概率分布，因为通过对训练数据的传递。之后，它应用贝叶斯定理来计算用于进行预测的标签的条件概率分布。
- en: However, there is still no implementation of the incremental version of the
    Bayesian network into Spark yet. According to the API documentation provided at
    [http://spark.apache.org/docs/latest/mllib-naive-bayes.html](http://spark.apache.org/docs/latest/mllib-naive-bayes.html),
    each observation is a document and each feature represents a term. The value of
    an observation is the frequency of the term or a zero or one. This value indicates
    if the term has been found in the document for the multinomial Naive Bayes and
    Bernoulli Naive Bayes respectively for the document classification.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前还没有将增量版本的贝叶斯网络实现到Spark中。根据提供的API文档[http://spark.apache.org/docs/latest/mllib-naive-bayes.html](http://spark.apache.org/docs/latest/mllib-naive-bayes.html)，每个观察结果是一个文档，每个特征代表一个术语。观察结果的值是术语的频率或零或一。这个值表示了术语是否在多项式朴素贝叶斯和伯努利朴素贝叶斯的文档分类中是否被发现。
- en: Note that as with linear SVM-based learning, here the feature values must be
    non-negative too. The type of the model is selected with an optional parameter,
    multinomial or Bernoulli. The default model type is multinomial. Furthermore,
    additive smoothing (that is, lambda) can be used by setting the parameter Î».
    Note the default of lambda is 1.0.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与基于线性SVM的学习一样，这里的特征值也必须是非负的。模型的类型是通过可选参数选择的，多项式或伯努利。默认模型类型是多项式。此外，可以通过设置参数Î»来使用加法平滑（即lambda）。请注意，默认的lambda值为1.0。
- en: 'More technical details on the big data approach of Bayesian network based learning
    can be found in the paper: *A Scalable Data Science Workflow Approach for Big
    Data Bayesian Network Learning b* *y Jianwu W., et al.*, ([http://users.sdsc.edu/~jianwu/JianwuWang_files/A_Scalable_Data_Science_Workflow_Approach_for_Big_Data_Bayesian_Network_Learning.pdf](http://users.sdsc.edu/~jianwu/JianwuWang_files/A_Scalable_Data_Science_Workflow_Approach_for_Big_Data_Bayesian_Network_Learning.pdf)).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 有关基于大数据方法的贝叶斯网络学习的更多技术细节，请参阅论文：*Jianwu W.等人的大数据贝叶斯网络学习的可扩展数据科学工作流方法* ([http://users.sdsc.edu/~jianwu/JianwuWang_files/A_Scalable_Data_Science_Workflow_Approach_for_Big_Data_Bayesian_Network_Learning.pdf](http://users.sdsc.edu/~jianwu/JianwuWang_files/A_Scalable_Data_Science_Workflow_Approach_for_Big_Data_Bayesian_Network_Learning.pdf))。
- en: 'Interested readers also should refer to the following publications for more
    insight into the incremental Bayesian networks:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于增量贝叶斯网络的更多见解，感兴趣的读者还应参考以下出版物：
- en: '[http://www.jmlr.org/papers/volume11/henderson10a/henderson10a.pdf](http://www.jmlr.org/papers/volume11/henderson10a/henderson10a.pdf)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.jmlr.org/papers/volume11/henderson10a/henderson10a.pdf](http://www.jmlr.org/papers/volume11/henderson10a/henderson10a.pdf)'
- en: '[http://www.machinelearning.org/proceedings/icml2007/papers/351.pdf](http://www.machinelearning.org/proceedings/icml2007/papers/351.pdf)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.machinelearning.org/proceedings/icml2007/papers/351.pdf](http://www.machinelearning.org/proceedings/icml2007/papers/351.pdf)'
- en: '[https://tel.archives-ouvertes.fr/tel-01284332/document](https://tel.archives-ouvertes.fr/tel-01284332/document)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://tel.archives-ouvertes.fr/tel-01284332/document](https://tel.archives-ouvertes.fr/tel-01284332/document)'
- en: Classification using Naive Bayes with Spark
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spark进行朴素贝叶斯分类
- en: The current implementation in Spark MLlib supports both the multinomial Naive
    Bayes and Bernoulli Naive Bayes. However, the incremental version has not been
    implemented yet. Therefore, in this section, we will show you how to perform the
    classification using the Spark MLlib version of NaÃ¯ve Bayes on the Vehicle Scale
    dataset to provide you with some concepts of the NaÃ¯ve Bayes based learning.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib中的当前实现支持多项式朴素贝叶斯和伯努利朴素贝叶斯。然而，增量版本尚未实现。因此，在本节中，我们将向您展示如何使用Spark MLlib版本的朴素贝叶斯对车辆规模数据集进行分类，以便为您提供基于朴素贝叶斯的学习的一些概念。
- en: Note, due to the low accuracy and precision using Spark ML, we did not provide
    the Pipeline version but implemented the same using only Spark MLlib. Moreover,
    if you have suitable and better data, you can try to implement the Spark ML version
    with ease.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于使用Spark ML的低准确性和精度，我们没有提供Pipeline版本，而是仅使用Spark MLlib实现了相同的功能。此外，如果您有合适且更好的数据，可以轻松尝试实现Spark
    ML版本。
- en: '**Step 1: Data collection, pre-processing, and exploration**'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：数据收集、预处理和探索**'
- en: 'The dataset was downloaded from [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#aloi](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#aloi)
    and provided by David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. RCV1: A
    new benchmark collection for text categorization research. *Journal of Machine
    Learning Research*, 5:361-397, 2004.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是从[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#aloi](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#aloi)下载的，并由David
    D. Lewis，Yiming Yang，Tony G. Rose和Fan Li提供。RCV1：文本分类研究的新基准集。*机器学习研究杂志*，5：361-397，2004年。
- en: '**Pre-processing:** For the pre-processing, two steps were considered as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**预处理：** 对于预处理，考虑了两个步骤如下：'
- en: The label hierarchy is reorganized by mapping the data set to the second level
    of RCV1 (that is, revision) topic hierarchy. The documents, having the third or
    fourth level, are mapped to their parent category of the second level only. Consequently,
    documents having the first level are not considered for creating the mapping.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签层次结构通过将数据集映射到RCV1的第二级（即修订）主题层次结构进行重新组织。具有第三或第四级的文档仅映射到其第二级的父类别。因此，不考虑具有第一级的文档来创建映射。
- en: Multi-labeled instances were removed since the current implementation of multi-level
    classifier in Spark is not robust enough.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于Spark中多级分类器的当前实现不够健壮，已删除了多标签实例。
- en: After performing these two steps, there are finally 53 classes and 47,236 features
    collected. Here is a snapshot of the dataset shown in *Figure 7:*
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这两个步骤后，最终收集到53个类别和47,236个特征。以下是数据集的快照，显示在*图7*中：
- en: '![Classification using Naive Bayes with Spark](img/00035.jpeg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark进行朴素贝叶斯分类](img/00035.jpeg)'
- en: 'Figure 7: RCV1 topic hierarchy dataset'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：RCV1主题层次数据集
- en: '**Step 2: Load the required library and packages**'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：加载所需的库和包**'
- en: 'Here is the code to load the library and packages:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载库和包的代码：
- en: '[PRE33]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**Step 3: Initiate a Spark session**'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：初始化Spark会话**'
- en: 'The following code helps us to create the Spark session:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码帮助我们创建Spark会话：
- en: '[PRE34]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Step 4: Prepare LabeledPoint RDDs**'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步：准备LabeledPoint RDDs
- en: 'Parse the dataset in the libsvm format and prepare `LabeledPoint` RDDs:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以libsvm格式解析数据集并准备`LabeledPoint` RDDs：
- en: '[PRE35]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: For document classification, the input feature vectors are usually sparse, and
    sparse vectors should be supplied as input to take advantage of sparsity. Since
    the training data is only used once, it is not necessary to cache it.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文档分类，输入特征向量通常是稀疏的，应该提供稀疏向量作为输入以利用稀疏性。由于训练数据只使用一次，因此不需要将其缓存。
- en: '**Step 5: Prepare the training and test set**'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步：准备训练和测试集
- en: 'Here is the code to prepare the training and test set:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是准备训练和测试集的代码：
- en: '[PRE36]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '**Step 6: Train the Naive Bayes model**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 第6步：训练朴素贝叶斯模型
- en: 'Train a Naive Bayes model by specifying the model type as multinomial and lambda
    = 1.0, which is the default and suitable for the multiclass classification of
    any features. However, note that Bernoulli naive Bayes requires 0 or 1 feature
    values:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定模型类型为多项式和lambda = 1.0来训练朴素贝叶斯模型，这是默认值，适用于任何特征的多类分类。但是，请注意，伯努利朴素贝叶斯要求特征值为0或1：
- en: '[PRE37]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '**Step 7: Calculate the prediction on the test dataset**'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 第7步：计算测试数据集上的预测
- en: 'Here is the code to calculate the prediction:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是计算预测的代码：
- en: '[PRE38]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '**Step 8: Calculate the prediction accuracy**'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 第8步：计算预测准确度
- en: 'Here is the code to calculate the prediction accuracy:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是计算预测准确度的代码：
- en: '[PRE39]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '**Step 9: Print the accuracy**'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 第9步：打印准确度
- en: 'Here is the code to print the accuracy:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是打印准确度的代码：
- en: '[PRE40]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This provides the following output:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了以下输出：
- en: '[PRE41]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This is pretty low, right? This is as we discussed when we tuned the ML models
    in [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*.
    There are further opportunities to improve the prediction accuracy by selecting
    appropriate algorithms (that is, classifier or regressor) via cross-validation
    and train split.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这很低，对吧？这正如我们在[第7章](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "第7章。调整机器学习模型")中调整ML模型时所讨论的。通过选择适当的算法（即分类器或回归器）进行交叉验证和训练分割，可以进一步提高预测准确度。
- en: Adapting through reusing ML models
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过重用ML模型进行适应
- en: In this section, we will describe how to make a machine learning model adaptable
    for new datasets. An example will be shown for the prediction of heart disease.
    At first we will describe the problem statement, and then we will explore the
    heart diseases dataset. Following the dataset exploration, we will train and save
    the model to local storage. After that the model will be evaluated to see how
    it performs. Finally, we will reuse/reload the same model trained to work for
    the new data type.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述如何使机器学习模型适应新数据集。将展示一个用于预测心脏病的示例。首先我们将描述问题陈述，然后我们将探索心脏疾病数据集。在数据集探索之后，我们将训练并保存模型到本地存储。之后将评估模型的表现。最后，我们将重用/重新加载相同的模型，以适用于新的数据类型。
- en: More specifically, we will show how to predict the possibility of future heart
    disease by using the Spark machine learning APIs including Spark MLlib, Spark
    ML, and Spark SQL.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们将展示如何使用Spark机器学习API，包括Spark MLlib、Spark ML和Spark SQL来预测未来心脏病的可能性。
- en: Problem statements and objectives
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述和目标
- en: Machine learning and big data together are a radical combination that has created
    some great impacts in the field of research to academia and the industry as well
    as in the biomedical sector. In the area of biomedical data analytics, this carries
    a better impact on a real dataset for diagnosis and prognosis for better healthcare.
    Moreover, life science research is also entering into big data since datasets
    are being generated and produced in an unprecedented way. This imposes great challenges
    to machine learning and bioinformatics tools and algorithms to find the VALUE
    from big data criteria such as volume, velocity, variety, veracity, visibility,
    and value.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和大数据的结合是一个激进的组合，在研究、学术界以及生物医学领域都产生了巨大的影响。在生物医学数据分析领域，这对诊断和预后的真实数据集产生了更好的影响，以实现更好的医疗保健。此外，生命科学研究也正在进入大数据领域，因为数据集以前所未有的方式被生成和产生。这给机器学习和生物信息学工具和算法带来了巨大的挑战，以从大数据标准（如容量、速度、多样性、真实性、可见性和价值）中找到价值。
- en: Data exploration
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据探索
- en: In recent times, biomedical research has advanced enormously and more and more
    life sciences datasets are being generated making many of them open source. However,
    for simplicity and ease, we have decided to use the Cleveland database. To date,
    most of the researchers who have applied the machine learning technique to biomedical
    data analytics have used this dataset. According to the dataset description at
    [https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names),
    this heart disease dataset is one of the most used and well-studied datasets by
    researchers from biomedical data analytics and machine learning fields, respectively.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，生物医学研究取得了巨大进展，越来越多的生命科学数据集正在生成，其中许多是开放源代码的。然而，为了简单和便利起见，我们决定使用克利夫兰数据库。迄今为止，大多数将机器学习技术应用于生物医学数据分析的研究人员都使用了这个数据集。根据数据集描述，这个心脏病数据集是生物医学数据分析和机器学习领域的研究人员最常使用和研究的数据集之一。
- en: The dataset is freely available at the UCI machine learning dataset repository
    at [https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/).
    This data contains a total of 76 attributes, however, most of the published research
    papers refer to using a subset of only 14 features in the field. The "goal" field
    is used to refer to if the heart diseases are present or absent. It has five possible
    values ranging from 0 to 4\. The value 0 signifies no presence of heart diseases.
    The values 1 and 2 signify that the disease is present but in the primary stage.
    The values 3 and 4, on the other hand, indicate the strong possibility of heart
    disease. Biomedical laboratory experiments with the Cleveland dataset have simply
    attempted to distinguish presence (values 1, 2, 3, 4) from absence (value 0).
    In short, the higher the value the more the disease is possible and the more evidence
    of the presence there is. Another thing is that privacy is an important concern
    in the area of biomedical data analytics as well as all kinds of diagnosis and
    prognosis. Therefore, the names and social security numbers of the patients were
    recently removed from the dataset to avoid the privacy issue. Consequently, those
    values have been replaced with dummy values instead.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '该数据集可以在UCI机器学习数据集存储库[https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/)免费获取。该数据包含共76个属性，但大多数已发表的研究论文只使用了该领域的14个特征的子集。"goal"字段用于指代心脏疾病是否存在。它有5个可能的值，范围从0到4。值0表示没有心脏疾病。而值1和2表示疾病存在，但处于初期阶段。另一方面，值3和4表示心脏疾病的强烈可能性。克利夫兰数据集的生物医学实验仅仅试图区分存在（值1、2、3、4）和不存在（值0）。简而言之，数值越高，疾病可能性越大，存在的证据也越多。另一件事是，隐私在生物医学数据分析领域以及所有类型的诊断和预后中都是一个重要关注点。因此，最近已从数据集中删除了患者的姓名和社会安全号码，以避免隐私问题。因此，这些值已被替换为虚拟值。 '
- en: 'It is to be noted that three files have been processed, containing the Cleveland,
    Hungarian, and Switzerland datasets altogether. All four unprocessed files also
    exist in this directory. To demonstrate the example, we will use the Cleveland
    dataset for training and evaluating the models. However, the Hungarian dataset
    will be used to re-use the saved model. As we have said already, although the
    number of attributes is 76 (including the predicted attribute), like other ML/Biomedical
    researchers, we will also use only 14 attributes with the following attribute
    information:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，已经处理了三个文件，其中包含了克利夫兰、匈牙利和瑞士的数据集。所有四个未处理的文件也存在于此目录中。为了演示示例，我们将使用克利夫兰数据集来训练和评估模型。然而，匈牙利数据集将用于重新使用保存的模型。正如我们已经说过的，尽管属性数量为76（包括预测属性），但像其他ML/生物医学研究人员一样，我们也将只使用14个属性，具体属性信息如下：
- en: '| **No.** | **Attribute name** | **Explanation** |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| **编号** | **属性名称** | **解释** |'
- en: '| 1 | age | Age in years |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 1 | age | 年龄（以年为单位） |'
- en: '| 2 | sex | Either male or female: sex (1 = male; 0 = female) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 2 | sex | 男性或女性：性别（1 = 男性；0 = 女性） |'
- en: '| 3 | cp | Chest pain type:— Value 1: typical angina— Value 2: atypical angina—
    Value 3: non-angina pain— Value 4: asymptomatic |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 3 | cp | 胸痛类型：— 值1：典型心绞痛— 值2：非典型心绞痛— 值3：非心绞痛— 值4：无症状 |'
- en: '| 4 | trestbps | Resting blood pressure (in mm Hg on admission to the hospital)
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 4 | trestbps | 静息血压（入院时以mm Hg为单位） |'
- en: '| 5 | chol | Serum cholesterol in mg/dl |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 5 | chol | 血清胆固醇（以mg/dl为单位） |'
- en: '| 6 | fbs | Fasting blood sugar. If > 120 mg/dl)(1 = true; 0 = false) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 6 | fbs | 空腹血糖。如果> 120 mg/dl)(1 = 真; 0 = 假) |'
- en: '| 7 | restecg | Resting electrocardiographic results:— Value 0: normal— Value
    1: having ST-T wave abnormality— Value 2: showing probable or definite left ventricular
    hypertrophy by Estes'' criteria. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 7 | restecg | 静息心电图结果：— 值0：正常— 值1：ST-T波异常— 值2：根据Estes标准显示可能或明确的左心室肥大。 |'
- en: '| 8 | thalach | Maximum heart rate achieved |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 8 | thalach | 达到的最大心率 |'
- en: '| 9 | exang | Exercise induced angina (1 = yes; 0 = no) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 9 | exang | 运动诱发的心绞痛（1 = 是; 0 = 否） |'
- en: '| 10 | oldpeak | ST depression induced by exercise relative to rest |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 10 | oldpeak | 相对于休息引起的ST段压低 |'
- en: '| 11 | slope | The slope of the peak exercise ST segment— Value 1: upsloping—
    Value 2: flat— Value 3: down-sloping |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 11 | slope | 峰值运动ST段的斜率— 值1：上斜— 值2：平坦— 值3：下斜 |'
- en: '| 12 | ca | Number of major vessels (0-3) colored by fluoroscopy |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 12 | ca | 荧光镜检查染色的主要血管数（0-3） |'
- en: '| 13 | thal | Heart rate:—Value 3 = normal;—Value 6 = fixed defect—Value 7
    = reversible defect |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 13 | thal | 心率：—值3 = 正常；—值6 = 固定缺陷—值7 = 可逆缺陷 |'
- en: '| 14 | num | Diagnosis of heart disease (angiographic disease status)— Value
    0: < 50% diameter narrowing— Value 1: > 50% diameter narrowing |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 14 | num | 心脏疾病诊断（血管造影疾病状态）— 值0：<50%直径狭窄— 值1：>50%直径狭窄 |'
- en: 'Table 1: Dataset characteristics'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：数据集特征
- en: 'A sample snapshot of the dataset is given as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的样本快照如下所示：
- en: '![Data exploration](img/00091.jpeg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![数据探索](img/00091.jpeg)'
- en: 'Figure 8: A sample snapshot of the heart diseases dataset'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：心脏疾病数据集的样本快照
- en: Developing a heart diseases predictive model
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发心脏疾病预测模型
- en: '**Step 1: Loading the required packages and APIs**'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：加载所需的包和API**'
- en: 'The following packages and APIs need to be imported for our purpose. We believe
    the packages are self-explanatory if you have the minimum working experience with
    Spark 2.0.0:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们的目的，需要导入以下包和API。我们相信，如果您对Spark 2.0.0有最低的工作经验，这些包是不言自明的：
- en: '[PRE42]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '**Step 2: Create an active Spark session**'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：创建一个活动的Spark会话**'
- en: 'The following code helps us to create the Spark session:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码帮助我们创建Spark会话：
- en: '[PRE43]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here is the `UtilityForSparkSession` class that creates and returns an active
    Spark session:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`UtilityForSparkSession`类，它创建并返回一个活动的Spark会话：
- en: '[PRE44]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note that here in the Windows 7 platform, we have set the Spark SQL warehouse
    as `E:/Exp/`, but set your path accordingly, based on your operating system.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在Windows 7平台上，我们已将Spark SQL仓库设置为`E:/Exp/`，但根据您的操作系统设置您的路径。
- en: '**Step 3: Data parsing and RDD of Labelpoint creation**'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：数据解析和标签点的RDD创建**'
- en: 'Take the input as a simple text file, parse them as a text file, and create
    an RDD of the label point that will be used for the classification and regression
    analysis. Also specify the input source and number of partition. Adjust the number
    of partition based on your dataset size. Here the number of partition has been
    set to 2:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入作为简单文本文件，将它们解析为文本文件，并创建一个用于分类和回归分析的标签点的RDD。还要指定输入源和分区数。根据数据集大小调整分区数。这里分区数已设置为2：
- en: '[PRE45]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Since `JavaRDD` cannot be created directly from the text files, we have created
    a simple RDDs so that we can convert them to `JavaRDD` when necessary. Now let''s
    create the `JavaRDD` with a Label Point. However, we first need to convert the
    RDD to `JavaRDD` to serve our purpose as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`JavaRDD`无法直接从文本文件创建，我们已经创建了一个简单的RDD，以便在必要时将它们转换为`JavaRDD`。现在让我们创建一个带有标签点的`JavaRDD`。但是，我们首先需要将RDD转换为`JavaRDD`以满足我们的目的，如下所示：
- en: '[PRE46]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Using the `replaceAll()` method, we have handled the invalid values such as
    the missing values that are specified in the original file using the *?* character.
    To get rid of the missing or invalid values we have replaced them with a very
    large value that has no side effect to the original classification or predictive
    results. The reason for this is that missing or sparse data can lead you to highly
    misleading results.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`replaceAll()`方法，我们处理了原始文件中指定的缺失值等无效值，使用了*?*字符。为了摆脱缺失或无效值，我们用一个非常大的值替换它们，这对原始分类或预测结果没有副作用。原因是缺失或稀疏数据可能导致高度误导性的结果。
- en: '**Step 4: Splitting the RDD of the label point into training and test sets**'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：将标签点的RDD分割为训练集和测试集**'
- en: 'In the previous step, we created RDD label point data that can be used for
    the regression or classification task. Now we need to split the data into training
    and test sets as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一步中，我们创建了可以用于回归或分类任务的RDD标签点数据。现在我们需要将数据分割为训练集和测试集，如下所示：
- en: '[PRE47]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: If you look at the preceding code segments, you will find that we have split
    the RDD label point as 70% for the training and 30% for the test set. The `randomSplit()`
    method performs this split. Note that we have set this RDD's storage level to
    persist its values across operations after the first time it is computed. This
    can only be used to assign a new storage level if the RDD does not have a storage
    level set yet. The split seed value is a long integer that signifies that the
    split would be random, but the result would not be a change in each run or iteration
    during the model building or training.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看前面的代码段，您会发现我们已将RDD标签点分为70%的训练集和30%的测试集。`randomSplit()`方法执行此分割。请注意，我们已将此RDD的存储级别设置为在第一次计算后跨操作持久化其值。只有在RDD尚未设置存储级别时才能用于分配新的存储级别。拆分种子值是一个长整数，表示拆分将是随机的，但结果不会在模型构建或训练的每次运行或迭代中发生变化。
- en: '**Step 5: Train the model**'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：训练模型**'
- en: 'First we will train the linear regression model, which is the simplest regression
    classifier:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们将训练线性回归模型，这是最简单的回归分类器：
- en: '[PRE48]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: As you can see, the preceding code trains a linear regression model with no
    regularization using the Stochastic Gradient Descent. This solves the least squares
    regression formulation f *(weights) = 1/n ||A weights-y||^2^*, which is the mean
    squared error. Here the data matrix has *n* rows, and the input RDD holds the
    set of rows of A, each with its corresponding right-hand side label y. Also, to
    train the model, it takes the training set, number of iterations, and the step
    size. We provide some random values for the last two parameters here.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，前面的代码使用随机梯度下降训练了一个没有正则化的线性回归模型。这解决了最小二乘回归公式 f *(weights) = 1/n ||A weights-y||^2^*，即均方误差。这里数据矩阵有*n*行，输入RDD保存了A的一组行，每行都有其相应的右手边标签y。此外，为了训练模型，它需要训练集、迭代次数和步长。我们在这里为最后两个参数提供了一些随机值。
- en: '**Step 6: Model saving for future use**'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：将模型保存以备将来使用**'
- en: 'Now let''s save the model that we just created for future use. It''s pretty
    simple - just use the following code by specifying the storage location as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们保存刚刚创建的模型以备将来使用。很简单 - 只需使用以下代码指定存储位置如下：
- en: '[PRE49]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Once the model is saved in your desired location, you will see the following
    output in your Eclipse console:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型保存在您想要的位置，您将在Eclipse控制台中看到以下输出：
- en: '![Developing a heart diseases predictive model](img/00047.jpeg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![开发心脏疾病预测模型](img/00047.jpeg)'
- en: 'Figure 9: The log after the model is saved to the storage'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：模型保存到存储后的日志
- en: '**Step 7: Evaluate the model with a test set**'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：使用测试集评估模型**'
- en: 'Now let''s calculate the prediction score on the test dataset:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在测试数据集上计算预测分数：
- en: '[PRE50]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Predict the accuracy of the prediction:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 预测预测的准确性：
- en: '[PRE51]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output appears as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE52]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '**Step 8: Predictive analytics using a different classifier**'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8：使用不同的分类器进行预测分析**'
- en: 'Unfortunately, there is no prediction accuracy at all, right? There might be
    several reasons for that, including the following:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，根本没有预测准确性，对吧？可能有几个原因，包括以下原因：
- en: The dataset characteristic
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集特征
- en: Model selection
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型选择
- en: Parameters selection - also called hyperparameter tuning
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数选择 - 也称为超参数调整
- en: 'For simplicity, we assume the dataset is okay since, as we have already said,
    it is a widely used dataset used for machine learning research used by many researchers
    around the globe. Now, what next? Let''s consider another classifier algorithm,
    for example, a Random forest or decision tree classifier. What about the Random
    forest? Let''s go for the random forest classifier at second place. Just use the
    following code to train the model using the training set:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，我们假设数据集是可以接受的，因为正如我们已经说过的那样，它是一个广泛使用的数据集，被全球许多研究人员用于机器学习研究。现在，接下来呢？让我们考虑另一个分类器算法，例如随机森林或决策树分类器。随机森林呢？让我们选择随机森林分类器。只需使用以下代码使用训练集训练模型：
- en: '[PRE53]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now use the `HashMap` to restrict the delicacy in the tree construction:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用`HashMap`来限制树构造中的细微差别：
- en: '[PRE54]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now declare the other parameters needed to train the Random Forest classifier:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在声明训练随机森林分类器所需的其他参数：
- en: '[PRE55]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We believe the parameters used by the `trainClassifier()` method are self-explanatory
    and so we''ll leave it to the readers to get to know the significance of each
    parameter. Fantastic! We have trained the model using the Random forest classifier
    and managed the cloud to save the model for future use. Now if you reuse the same
    code that we described in the *Evaluate the model with test set* step, you should
    have the following output:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为`trainClassifier()`方法使用的参数是不言自明的，所以我们将让读者了解每个参数的重要性。太棒了！我们已经使用随机森林分类器训练了模型，并管理云端保存了模型以备将来使用。现在，如果您重用我们在*使用测试集评估模型*步骤中描述的相同代码，您应该会得到以下输出：
- en: '[PRE56]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Now the predictive accuracy should be much better. If you are still not satisfied,
    you can try with another classifier model such as the Naive Bayes classifier and
    carry out the hyperparameter tuning discussed in [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在预测准确度应该会好得多。如果您仍然不满意，可以尝试使用另一个分类器模型，比如朴素贝叶斯分类器，并进行[第7章](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "第7章。调整机器学习模型")中讨论的超参数调整，*调整机器学习模型*。
- en: '**Step 9: Making the model adaptable for a new dataset**'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9：使模型适应新数据集**'
- en: We already mentioned that we have saved the model for future use, now we should
    take the opportunity to use the same model for new datasets. The reason, if you
    recall the steps, is that we have trained the model using the training set and
    evaluated it using the test set. Now, if you have more data or new data available
    to be used, what will you do? Will you go for re-training the model? Of course
    not, since you will have to iterate several steps and you will have to sacrifice
    valuable time and cost too.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到我们已经保存了模型以备将来使用，现在我们应该抓住机会使用相同的模型来处理新数据集。原因是，如果您回忆一下步骤，我们已经使用训练集训练了模型，并使用测试集进行了评估。现在，如果您有更多数据或新数据可供使用，您会怎么做？您会重新训练模型吗？当然不会，因为您将不得不迭代多个步骤，而且还要牺牲宝贵的时间和成本。
- en: 'Therefore, it would be wise to use the already trained model and predict the
    performance on a new dataset. Well, now let''s reuse the stored model. Note that
    you will have to reuse the same model that is to be trained for the same model.
    For example, if you have done the model training using the Random forest classifier
    and saved the model while reusing it, you will have to use the same classifier
    model to load the saved model. Therefore, we will use the Random forest to load
    the model while using the new dataset. Use the following code to do that. Now
    create an RDD label point from the new dataset (that is, the Hungarian database
    with the same 14 attributes):'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，明智的做法是使用已经训练好的模型，并预测在新数据集上的性能。好了，现在让我们重用存储的模型。请注意，您将不得不重用要训练相同模型的模型。例如，如果您使用随机森林分类器进行模型训练并保存了模型，那么在重用时，您将不得不使用相同的分类器模型来加载保存的模型。因此，我们将使用随机森林来加载模型，同时使用新数据集。使用以下代码来实现这一点。现在从新数据集（即具有相同14个属性的匈牙利数据库）创建一个RDD标签点：
- en: '[PRE57]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now let''s load the saved model using the Random forest model algorithm as
    follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用随机森林模型算法加载保存的模型如下：
- en: '[PRE58]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now let''s calculate the prediction on the test set:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算测试集上的预测：
- en: '[PRE59]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now calculate the accuracy of the prediction as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在按如下方式计算预测的准确度：
- en: '[PRE60]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We should have the following output:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该得到以下输出：
- en: '[PRE61]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Now train the NaÃ¯ve Bayesian classifier and see the predictive performance.
    Just download the source code for the Naive Bayesian classifier and run the code
    as a Maven-friendly project using the `pom.xml` file that includes all the dependencies
    of the required JARs and APIs.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在训练朴素贝叶斯分类器并查看预测性能。只需下载朴素贝叶斯分类器的源代码，并使用包含所需JAR和API依赖项的`pom.xml`文件将代码作为Maven友好项目运行。
- en: 'The following table shows a comparison of the predictive accuracies among three
    classifiers (that is, Linear Regression, Random Forest, and the Naive Bayesian
    classifier). Note that, depending upon the training, the model you get might have
    different output since we randomly split the dataset into training and testing:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表显示了三个分类器（即线性回归、随机森林和朴素贝叶斯分类器）之间的预测准确性的比较。请注意，根据训练，您得到的模型可能会有不同的输出，因为我们随机将数据集分成训练集和测试集。
- en: '| **Classifier** | **Model building time** | **Model saving time** | **Accuracy**
    |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| **分类器** | **模型构建时间** | **模型保存时间** | **准确度** |'
- en: '| Linear regression | 1199 ms | 2563 ms | 0.0% |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 线性回归 | 1199毫秒 | 2563毫秒 | 0.0% |'
- en: '| NaÃ¯ve Bayes | 873 ms | 2514 ms | 45% |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 873毫秒 | 2514毫秒 | 45% |'
- en: '| Random forest | 2120 ms | 2538 ms | 91% |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 2120毫秒 | 2538毫秒 | 91% |'
- en: 'Table 2: Comparison between three classifiers'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：三个分类器之间的比较
- en: Note
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We avail the preceding output in a machine with a Windows 7(64-bit), Core i7
    (2.90GHz) processor, and 32GB of main memory. Therefore, depending upon your OS
    type and hardware configuration, you might receive different results.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一台Windows 7（64位）、Core i7（2.90GHz）处理器和32GB主内存的机器上获得了上述输出。因此，根据您的操作系统类型和硬件配置，您可能会收到不同的结果。
- en: This way the ML model can be made adaptable for the new data type. However,
    make sure that you use the same classifier or regressor to train and reuse the
    model to make the ML application adaptable.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，ML模型可以适应新的数据类型。但是，请确保您使用相同的分类器或回归器来训练和重用模型，以使ML应用程序具有适应性。
- en: Machine learning in dynamic environments
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态环境中的机器学习
- en: Making a prediction in dynamic environments does not always succeed in producing
    desired outcomes, particularly in complex and unstructured data.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在动态环境中进行预测并不总是能够产生期望的结果，特别是在复杂和非结构化的数据中。
- en: There are several reasons for that. For example, how do you infer a realistic
    outcome from a bit of data or deal with unstructured and high dimensional data
    that has been found too tedious? Moreover, model revision with efficient strategies
    to control the realistic environments is also costly.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个原因。例如，如何从少量数据中推断出真实的结果，或者处理被发现过于繁琐的非结构化和高维数据？此外，使用有效策略对现实环境进行模型修订也是昂贵的。
- en: Furthermore, sometimes the dimensionality of the input dataset is high. Consequently,
    data might be too dense or very sparse. In that case, how you deal with very large
    settings and how to apply the static models in emerging application areas such
    as robotics, image processing, deep learning, computer vision, or web mining is
    challenging. On the other hand, ensemble methods are becoming more popular for
    selecting and combining models from existing models to make the ML model more
    adaptable. A hierarchical and dynamic environment-based learning is shown in *Figure
    10:*
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有时输入数据集的维度很高。因此，数据可能过于密集或非常稀疏。在这种情况下，如何处理非常大的设置以及如何将静态模型应用于新兴应用领域，如机器人技术、图像处理、深度学习、计算机视觉或网络挖掘，是具有挑战性的。另一方面，集成方法越来越受欢迎，用于从现有模型中选择和组合模型，使ML模型更具适应性。*图10*显示了基于分层和动态环境的学习：
- en: '![Machine learning in dynamic environments](img/00021.jpeg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![动态环境中的机器学习](img/00021.jpeg)'
- en: 'Figure 10: The hierarchy of machine learning in a dynamic environment'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：动态环境中机器学习的层次结构
- en: 'In this case, ML techniques such as neural networks and statistically-based
    learning are also becoming popular for their success with numerous applications
    in industry and research such as biological systems. In particular, classical
    learning algorithms such as neural networks, decision trees, or vector quantizes
    are often restricted to purely feedforward settings, and simple vectorial data,
    instead of dynamic environments. The feature of vectorization often provides a
    better prediction because of the rich structure. In summary, there are three challenges
    in developing ML applications in a dynamic environment:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，诸如神经网络和基于统计的学习等ML技术也因其在生物系统等行业和研究中的成功而变得越来越受欢迎。特别是，诸如神经网络、决策树或矢量量化等经典学习算法通常受到纯前馈设置和简单矢量数据的限制，而不是动态环境。矢量化的特征通常提供更好的预测，因为具有丰富的结构。总之，在开发动态环境中的ML应用程序时存在三个挑战：
- en: How does the data structure emerge to shape in an autonomous environment?
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据结构如何在自主环境中形成？
- en: How do we deal with input data that is statistically sparse and high dimensional?
    More specifically, what about making predictive analysis using online algorithms
    for large-scale datasets, applying the dimensionality reduction and so on?
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何处理统计稀疏和高维的输入数据？更具体地说，如何使用在线算法对大规模数据集进行预测分析，应用降维等？
- en: With only limited reinforcement signals, ill-posed domains, or partially underspecified
    settings, how do we develop controlled and effective strategies in dynamic environments?
    Considering these issues and promising advancement in the research, in this section,
    we will provide some insights into online learning techniques through a statistical
    and adversarial model. Since learning in a dynamic environment such as streaming
    will be discussed in [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced
    Machine Learning with Streaming and Graph Data*, we will not discuss streaming-based
    learning in this chapter.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在只有有限的强化信号、不适当的域或部分未规定的设置下，我们如何在动态环境中开发受控和有效的策略？考虑到这些问题和研究中的有希望的进展，在本节中，我们将通过统计和对抗模型提供一些关于在线学习技术的见解。由于学习动态环境，如流式处理将在[第9章](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "第9章。流式和图数据的高级机器学习")中讨论，我们将不在本章讨论基于流式处理的学习。
- en: Online learning
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线学习
- en: 'Batch learning techniques generate the best predictor by learning on the entire
    training dataset at once and are often called static learning. Static learning
    algorithms take batches of training data to train a model, then a prediction is
    made using the test sample and the found relationship, whereas online learning
    algorithms take an initial guess model and then pick up a one-one observation
    from the training population and recalibrate the weights on each input parameter.
    Data usually becomes available in a sequential order as batches. The sequential
    data is used to update the best predictor of the outcome at each step as outlined
    in *Figure 11*. There are three use cases of online-based learning:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理学习技术通过一次性学习整个训练数据集生成最佳预测器，通常被称为静态学习。静态学习算法使用训练数据的批次来训练模型，然后使用测试样本和找到的关系进行预测，而在线学习算法则使用初始猜测模型，然后从训练人口中挑选一个观察值，并重新校准每个输入参数的权重。数据通常按顺序作为批次可用。顺序数据用于在每个步骤更新结果的最佳预测器，如*图11*中所述。在线学习有三种用例：
- en: Firstly, where it is computationally infeasible to train an ML model over the
    entire dataset, an online learning is commonly used
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，当在整个数据集上训练ML模型在计算上是不可行的时，通常使用在线学习
- en: Secondly, it is also used in a situation where it is necessary for the algorithm
    to dynamically adapt to new patterns in the data
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，它还用于需要算法动态适应数据中新模式的情况
- en: Thirdly, it is used when the data itself is generated as a function of time,
    for example, the stock price prediction
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，当数据本身是随时间生成的函数时，例如股票价格预测
- en: 'Online learning, therefore, requires out-of-core algorithms, that is, algorithms
    that can perform considering the constraints of networks. There are two general
    modeling strategies that exist for online learning models:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在线学习需要考虑网络约束的算法，即可以执行的算法。存在两种用于在线学习模型的一般建模策略：
- en: '**Statistical learning models**: For example, stochastic gradient descent and
    perceptron'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计学习模型**：例如，随机梯度下降和感知器'
- en: '**Adversarial models**: For example, spam filtering falls into this category,
    as the adversary will dynamically generate new spam based on the current behavior
    of the spam detector'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗模型**：例如，垃圾邮件过滤属于这一类别，因为对手将根据垃圾邮件检测器的当前行为动态生成新的垃圾邮件。'
- en: Although online and incremental learning techniques are similar, they also differ
    slightly. In online, it's generally a single pass (epoch=1) or a number of epochs
    that could be configured, whereas, incremental would mean that you already have
    a model. No matter how it is built, the model can be mutable by new examples.
    Also, a combination of online and incremental is often what is required.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在线和增量学习技术相似，但也略有不同。在线学习通常是单次通过（epoch=1）或可以配置的若干次通过，而增量意味着您已经有一个模型。无论模型是如何构建的，新的示例都可以改变模型。此外，通常需要在线和增量的组合。
- en: Data is being generated in an unprecedented way everywhere, every day. This
    huge data imposes an enormous challenge to building ML tools that can handle data
    with high volume, velocity, and veracity. In short, data generated online is also
    big data. Therefore, we need to know the technique by which to learn about the
    online learning algorithms that are meant to handle data with such high volume
    and velocity with limited performance machines.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 数据正在以前所未有的方式随处生成。这些庞大的数据对于构建能够处理高容量、高速度和高准确性数据的机器学习工具构成了巨大挑战。简而言之，在线生成的数据也是大数据。因此，我们需要了解学习处理这种高容量和高速度数据的在线学习算法的技术，而这些算法是为了在性能有限的机器上处理数据而设计的。
- en: '![Online learning](img/00016.jpeg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![在线学习](img/00016.jpeg)'
- en: 'Figure 11: Batch (static) versus online learning, an overview'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：批处理（静态）与在线学习，概述
- en: Statistical learning model
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计学习模型
- en: As already outlined, in statistically-based learning models such as **stochastic
    gradient descents** **(SGD)** and artificial neural networks or perceptron, data
    samples are assumed to be independent of each other. In addition to this, it is
    also assumed that the dataset is identically distributed as random variables.
    In other words, they don't adapt with time. Therefore, an ML algorithm has a limited
    access to the data.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 如已概述，在基于统计的学习模型中，如随机梯度下降（SGD）和人工神经网络或感知器，数据样本被假定为彼此独立。除此之外，还假定数据集是作为随机变量相同分布的。换句话说，它们不会随时间改变。因此，机器学习算法对数据的访问是有限的。
- en: 'In the field of the statistical learning model there are two interpretations
    that are considered significant:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学习模型领域，有两种被认为是重要的解释：
- en: '**First interpretation**: This considers the stochastic gradient descent method
    as applied to the problem of minimizing the expected risks. In an infinite stream
    of data, the predictive analytics is assumed to be drawn from the normal distribution.
    Therefore only the stochastic gradient descent method is used to bind the deviation.
    This interpretation is also valid for the finite training set.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第一种解释：** 这将随机梯度下降方法视为最小化期望风险的问题。在无限的数据流中，预测分析被假定来自正态分布。因此，只使用随机梯度下降方法来限制偏差。这种解释对有限的训练集也是有效的。'
- en: '**Second interpretation:** This applies to the case of a finite training set
    and considers the SGD algorithm as an instance of incremental gradient descent
    method. In this case, one instead looks at the empirical risk: Since the gradients
    of in the incremental gradient descent, iterations are also stochastic estimates
    of the gradient of, this interpretation but applied to minimize the empirical
    risk as opposed to the expected risk. For why multiple passes through the data
    are readily allowed and actually lead to tighter bounds on the deviations.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二种解释：** 这适用于有限的训练集情况，并将SGD算法视为增量梯度下降方法的一个实例。在这种情况下，人们会看到经验风险：由于增量梯度下降的梯度，迭代也是对梯度的随机估计，这种解释适用于最小化经验风险而不是期望风险。为什么允许多次通过数据，并且实际上会导致对偏差的更严格的界限。'
- en: Adversarial model
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗模型
- en: Classical machine learning, which is especially taught in classes, emphasizes
    a static environment where usually unchanging data is used to make predictions.
    It is, therefore, formally easier compared to a statistical or causal inference
    or dynamic environment. On the other hand, finding and solving the learning problem
    as a game between two players, for example, learner versus data generator in a
    dynamic environment, is an example of an adversarial model. This kind of modeling
    and making predictive analytics is critically tedious since the world does not
    know that you are trying to model it formally.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 经典机器学习，尤其是在课堂上教授的，强调静态环境，通常使用不变的数据进行预测。因此，与统计或因果推断或动态环境相比，形式上更容易。另一方面，在动态环境中将学习问题视为两个玩家之间的游戏，例如学习者与数据生成器之间的游戏，是对对抗模型的一个例子。这种建模和进行预测分析是非常繁琐的，因为世界并不知道您试图正式对其进行建模。
- en: 'Furthermore, your model does not have any positive or negative effect on the
    world. Therefore, the ultimate goal of this kind of model is to minimize losses
    prevailing from circumstances generated by the move made and played by the other
    player. The opponent can adapt the data generated based on the output of the learning
    algorithm in run-time or dynamically. Since no distributional assumptions are
    made about the data, performing well for the entire sequence that could be viewed
    ahead of time becomes the ultimate goal. Additionally, regret is to be minimized
    on the hypothesis at the last pace. According to Cathy O. et al (*Weapons of Math
    Destruction*, Cathy O''Neil, and Crown, September 6, 2016) t adversarial-based
    machine learning can be defined as follows:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您的模型对世界没有任何积极或消极的影响。因此，这种模型的最终目标是最小化由其他玩家的举动产生的情况所造成的损失。对手可以根据学习算法的输出在运行时或动态地调整生成的数据。由于对数据没有分布假设，因此在整个可能提前查看的序列中表现良好成为最终目标。此外，遗憾应该在最后一步的假设上最小化。根据Cathy
    O.等人的说法（*数学毁灭的武器*，Cathy O'Neil和Crown，2016年9月6日），对抗性机器学习可以定义如下：
- en: Adversarial machine learning is the formal name for studying what happens when
    conceding even a slightly more realistic alternative to assumptions of these types
    (harmlessly called **relaxing assumptions**).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性机器学习是研究在假设这些类型的情况下让步甚至略微更现实的替代方案时会发生什么的正式名称（无害地称为**放松假设**）。
- en: Tip
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Up to the Spark 2.0.0 release, there was no formal algorithm implemented in
    the release. Therefore, we were unable to provide any concrete examples that could
    be further explained elaborately. Interested readers should check the latest Spark
    release to understand the updates.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0.0发布之前，没有在发布中实现正式算法。因此，我们无法提供任何可以进一步详细解释的具体示例。感兴趣的读者应该查看最新的Spark发布以了解更新情况。
- en: Summary
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we tried to cover some advanced machine learning techniques
    to make machine learning models and applications adaptable for new problem and
    data types.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们试图涵盖一些高级机器学习技术，使机器学习模型和应用程序能够适应新的问题和数据类型。
- en: We have shown several examples of machine learning algorithms that learn from
    batch or static-based learning over the data of models that are updated each time
    they see a new training instance.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了几个机器学习算法的例子，这些算法通过批处理或静态学习来学习模型数据，每次看到新的训练实例时都会更新模型。
- en: We have also discussed how to make the models adaptable through generalization,
    through incremental learning, through model reusing, and in dynamic environments.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了如何通过泛化、增量学习、模型重用和动态环境使模型具有适应性。
- en: In [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced
    Machine Learning with Streaming and Graph Data*, we will guide you on how to apply
    machine learning techniques with the help of Spark MLlib and Spark ML on streaming
    and graph data, for example, topic modeling.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d "第9章。流式和图数据的高级机器学习")中，*流式和图数据的高级机器学习*，我们将指导您如何使用Spark
    MLlib和Spark ML在流式和图数据上应用机器学习技术，例如主题建模。
