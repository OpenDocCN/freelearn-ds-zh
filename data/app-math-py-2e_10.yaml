- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Improving Your Productivity
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高生产力
- en: In this chapter, we will look at several topics that don’t fit within the categories
    that we discussed in the previous chapters of this book. Most of these topics
    are concerned with different ways to facilitate computing and otherwise optimize
    the execution of our code. Others are concerned with working with specific kinds
    of data or file formats.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论一些不属于本书前几章分类的主题。这些主题大多数与简化计算过程和优化代码执行有关。还有一些涉及处理特定类型的数据或文件格式。
- en: The aim of this chapter is to provide you with some tools that, while not strictly
    mathematical in nature, often appear in mathematical problems. These include topics
    such as distributed computing and optimization – both help you to solve problems
    more quickly, validate data and calculations, load and store data from file formats
    commonly used in scientific computation, and incorporate other topics that will
    generally help you be more productive with your code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是为您提供一些工具，虽然它们本质上并非严格的数学工具，但在数学问题中经常出现。这些工具包括分布式计算和优化等主题——它们帮助您更快速地解决问题，验证数据和计算结果，从常见的科学计算文件格式加载和存储数据，并且融入其他有助于提高代码生产力的主题。
- en: In the first two recipes, we will cover packages that help keep track of units
    and uncertainties in calculations. These are very important for calculations that
    concern data that have a direct physical application. In the next recipe, we will
    look at loading and storing data from **Network Common Data Form** (**NetCDF**)
    files. NetCDF is a file format usually used for storing weather and climate data.
    In the fourth recipe, we’ll discuss working with geographical data, such as data
    that might be associated with weather or climate data. After that, we’ll discuss
    how we can run Jupyter notebooks from the terminal without having to start up
    an interactive session. Then, we will turn to validating data for the next recipe
    and then focus on performance with tools such as Cython and Dask. Finally, we
    will give a very short overview of some techniques for writing reproducible code
    for data science.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两种配方中，我们将介绍帮助跟踪计算中的单位和不确定性的包。这些对处理具有直接物理应用的数据的计算非常重要。在接下来的配方中，我们将讨论如何从 **网络通用数据格式**（**NetCDF**）文件中加载和存储数据。NetCDF
    是一种通常用于存储天气和气候数据的文件格式。在第四个配方中，我们将讨论如何处理地理数据，如可能与天气或气候数据相关的数据。接下来，我们将讨论如何从终端运行
    Jupyter 笔记本，而无需启动交互式会话。然后，我们将转向数据验证，并在接下来的配方中关注使用 Cython 和 Dask 等工具的性能。最后，我们将简要概述一些编写数据科学可重现代码的技术。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下配方：
- en: Keeping track of units with Pint
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Pint 跟踪单位
- en: Accounting for uncertainty in calculations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算中的不确定性
- en: Loading and storing data from NetCDF files
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 NetCDF 文件加载和存储数据
- en: Working with geographical data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理地理数据
- en: Executing a Jupyter notebook as a script
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Jupyter 笔记本作为脚本执行
- en: Validating data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据验证
- en: Accelerating code with Cython
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Cython 加速代码
- en: Distributing computation with Dask
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Dask 分布式计算
- en: Writing reproducible code for data science
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为数据科学编写可重现的代码
- en: Let’s get started!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires many different packages due to the nature of the recipes
    it contains. The list of packages we need is as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章包含的内容类型，本章需要许多不同的包。我们需要的包清单如下：
- en: Pint
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pint
- en: uncertainties
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: uncertainties
- en: netCDF4
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: netCDF4
- en: xarray
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: xarray
- en: Pandas
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas
- en: Scikit-learn
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn
- en: GeoPandas
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GeoPandas
- en: Geoplot
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geoplot
- en: Jupyter
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter
- en: Papermill
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papermill
- en: Cerberus
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cerberus
- en: Cython
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cython
- en: Dask
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask
- en: 'All of these packages can be installed using your favorite package manager,
    such as `pip`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些包都可以通过您喜欢的包管理工具（如 `pip`）进行安装：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To install the Dask package, we need to install the various extras associated
    with the package. We can do this using the following `pip` command in the terminal:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 Dask 包，我们需要安装与该包相关的各种附加功能。可以通过在终端中使用以下 `pip` 命令来完成：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In addition to these Python packages, we will also need to install some supporting
    software. For the *Working with geographical data* recipe, the GeoPandas and Geoplot
    libraries have numerous lower-level dependencies that might need to be installed
    separately. Detailed instructions are given in the GeoPandas package documentation
    at [https://geopandas.org/install.html](https://geopandas.org/install.html).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些Python包，我们还需要安装一些支持软件。对于*处理地理数据*配方，GeoPandas和Geoplot库有许多较低级别的依赖项，可能需要单独安装。详细的安装说明请参考GeoPandas包文档，网址：[https://geopandas.org/install.html](https://geopandas.org/install.html)。
- en: For the *Accelerating code with Cython* recipe, we will need to have a C compiler
    installed. Instructions on how to obtain the **GNU C compiler** (**GCC**) are
    given in the Cython documentation at [https://cython.readthedocs.io/en/latest/src/quickstart/install.html](https://cython.readthedocs.io/en/latest/src/quickstart/install.html).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*使用Cython加速代码*配方，我们需要安装C编译器。有关如何获得**GNU C编译器**（**GCC**）的说明，请参见Cython文档：[https://cython.readthedocs.io/en/latest/src/quickstart/install.html](https://cython.readthedocs.io/en/latest/src/quickstart/install.html)。
- en: The code for this chapter can be found in the `Chapter 10` folder of the GitHub
    repository at [https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2010](https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2010).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在GitHub仓库的`Chapter 10`文件夹中找到，地址：[https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2010](https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2010)。
- en: Keeping track of units with Pint
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Pint追踪单位
- en: Correctly keeping track of units in calculations can be very difficult, particularly
    if there are places where different units can be used. For example, it is very
    easy to forget to convert between different units – feet/inches into meters –
    or metric prefixes – converting 1 km into 1,000 m, for instance.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正确追踪计算中的单位可能非常困难，特别是当不同单位可以互换使用时。例如，非常容易忘记转换不同的单位——英尺/英寸转化为米，或者公制前缀——例如将1千米转换为1,000米。
- en: In this recipe, we’ll learn how to use the Pint package to keep track of units
    of measurement in calculations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将学习如何使用Pint包在计算中追踪单位。
- en: Getting ready
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we need the Pint package, which can be imported as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，我们需要安装Pint包，可以通过以下方式导入：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How to do it...
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The following steps show you how to use the Pint package to keep track of units
    in calculations:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤展示了如何使用Pint包在计算中追踪单位：
- en: 'First, we need to create a `UnitRegistry` object:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个`UnitRegistry`对象：
- en: '[PRE3]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To create a quantity with a unit, we multiply the number by the appropriate
    attribute of the registry object:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建一个带有单位的量，我们将数字乘以注册对象的相应属性：
- en: '[PRE4]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can change the units of the quantity using one of the available conversion
    methods:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用其中一个可用的转换方法来更改量的单位：
- en: '[PRE5]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of these `print` statements is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些`print`语句的输出如下：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We wrap a routine to make it expect an argument in seconds and output a result
    in meters:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们封装一个例程，使其期望接受秒为单位的参数，并输出以米为单位的结果：
- en: '[PRE9]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, when we call the `calc_depth` routine with a `minute` unit, it is automatically
    converted into seconds for the calculation:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，当我们用`minute`单位调用`calc_depth`例程时，它会自动转换为秒进行计算：
- en: '[PRE14]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works...
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The Pint package provides a wrapper class for numerical types that adds unit
    metadata to the type. This wrapper type implements all the standard arithmetic
    operations and keeps track of the units throughout these calculations. For example,
    when we divide a length unit by a time unit, we will get a speed unit. This means
    that you can use Pint to make sure the units are correct after a complex calculation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Pint包提供了一个包装类，用于数值类型，给类型添加了单位元数据。这个包装类型实现了所有标准的算术操作，并在整个计算过程中追踪单位。例如，当我们将长度单位除以时间单位时，我们会得到一个速度单位。这意味着，您可以使用Pint来确保在复杂计算后单位是正确的。
- en: The `UnitRegistry` object keeps track of all the units that are present in the
    session and handles things such as conversion between different unit types. It
    also maintains a reference system of measurements, which, in this recipe, is the
    standard international system with meters, kilograms, and seconds as base units,
    denoted by `mks`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`UnitRegistry`对象追踪会话中所有存在的单位，并处理不同单位类型之间的转换等操作。它还维护着一个度量参考系统，在本配方中，是以米、千克和秒为基本单位的国际标准系统，称为`mks`。'
- en: The `wraps` functionality allows us to declare the input and output units of
    a routine, which allows Pint to make automatic unit conversions for the input
    function – in this recipe, we converted from minutes into seconds. Trying to call
    a wrapped function with a quantity that does not have an associated unit, or an
    incompatible unit, will raise an exception. This allows runtime validation of
    parameters and automatic conversion into the correct units for a routine.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`wraps` 功能允许我们声明一个例程的输入和输出单位，这使得 Pint 能够对输入函数进行自动单位转换——在这个示例中，我们将分钟转换为秒。试图使用没有关联单位的数量，或者使用不兼容单位调用包装函数，会抛出异常。这使得在运行时可以验证参数，并自动转换成例程所需的正确单位。'
- en: There’s more...
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: The Pint package comes with a large list of preprogrammed units of measurement
    that cover most globally used systems. Units can be defined at runtime or loaded
    from a file. This means that you can define custom units or systems of units that
    are specific to the application that you are working with.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Pint 包提供了一个包含大量预编程单位的测量单位列表，覆盖了大多数全球使用的系统。单位可以在运行时定义，也可以从文件中加载。这意味着你可以定义特定于应用程序的自定义单位或单位系统。
- en: Units can also be used within different contexts, which allows for easy conversion
    between different unit types that would ordinarily be unrelated. This can save
    a lot of time in situations where you need to move between units fluidly at multiple
    points in a calculation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 单位也可以在不同的上下文中使用，这使得在通常不相关的不同单位类型之间轻松转换成为可能。这可以在计算过程中需要频繁转换单位时节省大量时间。
- en: Accounting for uncertainty in calculations
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在计算中考虑不确定性
- en: Most measuring devices are not 100% accurate and instead are accurate up to
    a certain amount, usually somewhere between 0 and 10%. For instance, a thermometer
    might be accurate to 1%, while a pair of digital calipers might be accurate up
    to 0.1%. The true value in both of these cases is unlikely to be exactly the reported
    value, although it will be fairly close. Keeping track of the uncertainty in a
    value is difficult, especially when you have multiple different uncertainties
    combined in different ways. Rather than keeping track of this by hand, it is much
    better to use a consistent library to do this for you. This is what the `uncertainties`
    package does.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数测量设备并非 100% 准确，而是具有一定的准确度，通常在 0 到 10% 之间。例如，一个温度计的准确度可能为 1%，而一把数显卡尺的准确度可能为
    0.1%。在这两种情况下，真实值不太可能与报告值完全一致，尽管它们会非常接近。追踪一个值的不确定性是非常困难的，特别是当多个不确定性以不同的方式结合时。与其手动跟踪这些，不如使用一个一致的库来代替。这正是
    `uncertainties` 包的功能。
- en: In this recipe, we will learn how to quantify the uncertainty of variables and
    see how these uncertainties propagate through a calculation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将学习如何量化变量的不确定性，并观察这些不确定性是如何在计算中传播的。
- en: Getting ready
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we will need the `uncertainties` package, from which we will
    import the `ufloat` class and the `umath` module:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将需要 `uncertainties` 包，其中我们将导入 `ufloat` 类和 `umath` 模块：
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How to do it...
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The following steps show you how to quantify uncertainty on numerical values
    in calculations:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤展示了如何在计算中量化数值的不确定性：
- en: 'First, we create an uncertain float value of `3.0` plus or minus `0.4`:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个不确定浮动值 `3.0` 加减 `0.4`：
- en: '[PRE18]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we perform a calculation involving this uncertain value to obtain a new
    uncertain value:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们进行涉及该不确定值的计算，以获得一个新的不确定值：
- en: '[PRE20]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we create a new uncertain float value and apply the `sqrt` routine from
    the `umath` module and perform the reverse of the previous calculation:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个新的不确定浮动值，并应用 `umath` 模块中的 `sqrt` 例程，进行前一个计算的反向操作：
- en: '[PRE22]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As we can see, the result of the first calculation (*step 2*) is an uncertain
    float with a value of `44`, and ![](img/Formula_10_001.png) systematic error.
    This means that the true value could be anything between 32 and 56\. We cannot
    be more accurate than this with the measurements that we have.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，第一次计算（*步骤 2*）的结果是一个不确定浮动值，其数值为 `44`，并且存在 ![](img/Formula_10_001.png) 系统误差。这意味着真实值可能在
    32 到 56 之间。根据我们现有的测量数据，我们无法得出更精确的结果。
- en: How it works...
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `ufloat` class wraps around `float` objects and keeps track of the uncertainty
    throughout calculations. The library makes use of linear error propagation theory,
    which uses derivatives of non-linear functions to estimate the propagated error
    during calculations. The library also correctly handles correlation so that subtracting
    a value from itself gives zero with no error.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`ufloat`类封装了`float`对象，并在整个计算过程中追踪不确定性。该库利用线性误差传播理论，使用非线性函数的导数来估算计算中的误差传播。该库还正确处理相关性，因此从自身减去一个值时，结果为零且没有误差。'
- en: To keep track of uncertainties in standard mathematical functions, you need
    to use the versions that are provided in the `umath` module, rather than those
    defined in the Python Standard Library or a third-party package such as NumPy.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了追踪标准数学函数中的不确定性，你需要使用`umath`模块中提供的版本，而不是使用Python标准库或第三方包如NumPy中定义的版本。
- en: There’s more...
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The `uncertainties` package provides support for NumPy, and the Pint package
    mentioned in the previous recipe can be combined with `uncertainties` to make
    sure that units and error margins are correctly attributed to the final value
    of a calculation. For example, we could compute the units in the calculation from
    *step 2* of this recipe, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`uncertainties`包为NumPy提供支持，而之前例子中提到的Pint包可以与`uncertainties`结合使用，以确保单位和误差范围正确地归属于计算的最终值。例如，我们可以计算本例第*2步*中计算的单位，如下所示：'
- en: '[PRE26]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As expected, the `print` statement on the last line gives us `44+/-12` meters.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，最后一行的`print`语句输出了`44+/-12`米。
- en: Loading and storing data from NetCDF files
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从NetCDF文件加载和存储数据
- en: Many scientific applications require that we start with large quantities of
    multi-dimensional data in a robust format. NetCDF is one example of a format used
    for data that’s developed by the weather and climate industry. Unfortunately,
    the complexity of the data means that we can’t simply use the utilities from the
    Pandas package, for example, to load this data for analysis. We need the `netcdf4`
    package to be able to read and import the data into Python, but we also need to
    use `xarray`. Unlike the Pandas library, `xarray` can handle higher-dimensional
    data while still providing a Pandas-like interface.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 许多科学应用要求我们从大量多维数据开始，并使用稳健的格式。NetCDF是气象和气候行业开发的一种数据格式示例。不幸的是，数据的复杂性意味着我们不能简单地使用例如Pandas包中的工具来加载这些数据进行分析。我们需要`netcdf4`包来读取并将数据导入到Python中，但我们还需要使用`xarray`。与Pandas库不同，`xarray`可以处理更高维度的数据，同时仍提供类似Pandas的接口。
- en: In this recipe, we will learn how to load data from and store data in NetCDF
    files.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将学习如何从NetCDF文件加载数据并将数据存储到NetCDF文件中。
- en: Getting ready
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we will need to import the NumPy package as `np`, the Pandas
    package as `pd`, the Matplotlib `pyplot` module as `plt`, and an instance of the
    default random number generator from NumPy:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本例，我们将需要导入NumPy包为`np`，Pandas包为`pd`，Matplotlib的`pyplot`模块为`plt`，以及NumPy中默认随机数生成器的实例：
- en: '[PRE27]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We also need to import the `xarray` package under the `xr` alias. You will
    also need to install the Dask package, as described in the *Technical requirements*
    section, and the `netCDF4` package:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要导入`xarray`包，并以`xr`为别名。你还需要安装Dask包，如*技术要求*部分所述，以及`netCDF4`包：
- en: '[PRE28]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We don’t need to import either of these packages directly.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要直接导入这两个包。
- en: How to do it...
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Follow these steps to load and store sample data in a NetCDF file:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤加载并存储示例数据到NetCDF文件：
- en: 'First, we need to create some random data. This data consists of a range of
    dates, a list of location codes, and randomly generated numbers:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一些随机数据。这些数据包含日期范围、位置代码列表以及随机生成的数字：
- en: '[PRE29]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we create a xarray `Dataset` object containing the data. The dates and
    locations are indexes, while the `steps` and `accumulated` variables are the data:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含数据的xarray `Dataset`对象。日期和位置是索引，而`steps`和`accumulated`变量是数据：
- en: '[PRE33]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output from the `print` statement is shown here:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示了`print`语句的输出：
- en: '[PRE38]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we compute the mean over all the locations at each time index:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们计算每个时间索引下所有位置的平均值：
- en: '[PRE39]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we plot the mean accumulated values on a new set of axes:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们在新的坐标轴上绘制平均累计值：
- en: '[PRE40]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The resulting plot looks as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像如下所示：
- en: '![Figure 10.1 - Plot of accumulated means over time'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.1 - 累计均值随时间变化的图表]'
- en: '](img/B19085_10_01.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B19085_10_01.jpg)'
- en: Figure 10.1 - Plot of accumulated means over time
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 - 随时间变化的累积均值图
- en: 'Save this dataset into a new NetCDF file using the `to_netcdf` method:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`to_netcdf`方法将此数据集保存为一个新的NetCDF文件：
- en: '[PRE44]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we can load the newly created NetCDF file using the `load_dataset` routine
    from `xarray`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`xarray`中的`load_dataset`例程加载新创建的NetCDF文件：
- en: '[PRE45]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '[PRE47]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The output shows that the loaded array contains all of the data that we added
    in the earlier steps. The important steps are *5* and *6*, where we store and
    load this `"``data.nc"` data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示加载的数组包含了我们在之前步骤中添加的所有数据。关键步骤是*5*和*6*，在这两个步骤中，我们存储并加载了这个`"data.nc"`数据。
- en: How it works...
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `xarray` package provides the `DataArray` and `DataSet` classes, which are
    (roughly speaking) multi-dimensional equivalents of the Pandas `Series` and `DataFrame`
    objects. We’re using a dataset in this example because each index – a tuple of
    a date and location – has two pieces of data associated with it. Both of these
    objects expose a similar interface to their Pandas equivalents. For example, we
    can compute the mean along one of the axes using the `mean` method. The `DataArray`
    and `DataSet` objects also have a convenience method for converting into a Pandas
    `DataFrame` called `to_dataframe`. We used it in this recipe to convert the accumulated
    column from the *means* *Dataset* into a `DataFrame` for plotting, which isn’t
    really necessary because `xarray` has plotting features built into it.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`xarray`包提供了`DataArray`和`DataSet`类，它们大致上是Pandas `Series`和`DataFrame`对象的多维等效物。在这个示例中，我们使用数据集，因为每个索引——一个包含日期和位置的元组——都关联着两条数据。这两个对象都暴露了与它们的Pandas等效物相似的接口。例如，我们可以使用`mean`方法计算某个轴上的均值。`DataArray`和`DataSet`对象还有一个方便的方法，可以将其转换为Pandas
    `DataFrame`，这个方法叫做`to_dataframe`。我们在这个食谱中使用它，将*means* *Dataset*中的累积列转换为`DataFrame`以便绘图，虽然这并不是必须的，因为`xarray`本身就内置了绘图功能。'
- en: The real focus of this recipe is on the `to_netcdf` method and the `load_dataset`
    routine. The former stores a `DataSet` object in a NetCDF format file. This requires
    the `netCDF4` package to be installed, as it allows us to access the relevant
    C library for decoding NetCDF-formatted files. The `load_dataset` routine is a
    general-purpose routine for loading data into a `DataSet` object from various
    file formats, including NetCDF (again, this requires the `netCDF4` package to
    be installed).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱的真正重点是`to_netcdf`方法和`load_dataset`例程。前者将一个`DataSet`对象存储为NetCDF格式的文件。这需要安装`netCDF4`包，因为它允许我们访问解码NetCDF格式文件所需的相关C库。`load_dataset`例程是一个通用的例程，用于从各种文件格式加载数据到`DataSet`对象中，包括NetCDF格式（同样，这需要安装`netCDF4`包）。
- en: There’s more...
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The `xarray` package has support for a number of data formats in addition to
    NetCDF, such as OPeNDAP, Pickle, GRIB, and other formats that are supported by
    Pandas.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`xarray`包除了支持NetCDF格式外，还支持多种数据格式，如OPeNDAP、Pickle、GRIB等，这些格式也被Pandas支持。'
- en: Working with geographical data
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理地理数据
- en: Many applications involve working with geographical data. For example, when
    tracking global weather, we might want to plot the temperature as measured by
    various sensors around the world at their position on a map. For this, we can
    use the GeoPandas package and the Geoplot package, both of which allow us to manipulate,
    analyze, and visualize geographical data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用涉及处理地理数据。例如，在跟踪全球天气时，我们可能想要绘制世界各地传感器测量的温度，并标注它们在地图上的位置。为此，我们可以使用GeoPandas包和Geoplot包，它们都允许我们操作、分析和可视化地理数据。
- en: In this recipe, we will use the GeoPandas and Geoplot packages to load and visualize
    some sample geographical data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将使用GeoPandas和Geoplot包加载并可视化一些示例地理数据。
- en: Getting ready
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we will need the GeoPandas package, the Geoplot package, and
    the Matplotlib `pyplot` package imported as `plt`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，我们将需要导入GeoPandas包、Geoplot包以及Matplotlib的`pyplot`包，命名为`plt`：
- en: '[PRE48]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: How to do it...
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Follow these steps to create a simple plot of the capital cities plotted on
    a map of the world using sample data:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤，使用示例数据在世界地图上绘制首都城市的简单图：
- en: 'First, we need to load the sample data from the GeoPandas package, which contains
    the world geometry information:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要加载GeoPandas包中的示例数据，这些数据包含了全球的几何信息：
- en: '[PRE49]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, we need to load the data containing the name and position of each of
    the capital cities of the world:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要加载包含全球每个首都城市名称和位置的数据：
- en: '[PRE52]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, we can create a new figure and plot the outline of the world geometry
    using the `polyplot` routine:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个新的图形，并使用 `polyplot` 例程绘制世界地理轮廓：
- en: '[PRE55]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Finally, we use the `pointplot` routine to add the positions of the capital
    cities on top of the world map. We also set the axes limits to make the whole
    world visible:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用 `pointplot` 例程将首都城市的位置叠加到世界地图上。我们还设置了坐标轴的限制，以确保整个世界都能显示出来：
- en: '[PRE57]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The resulting plot of the positions of the capital cities of the world looks
    as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最终得到的图像展示了世界首都城市的位置：
- en: '![Figure 10.2 - Plot of the world’s capital cities on a map'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.2 - 世界各国首都城市在地图上的分布'
- en: '](img/B19085_10_02.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B19085_10_02.jpg)'
- en: Figure 10.2 - Plot of the world’s capital cities on a map
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 - 世界各国首都城市在地图上的分布
- en: The plot shows a rough outline of the different countries of the world. Each
    of the capital cities is indicated by a marker. From this view, it is quite difficult
    to distinguish individual cities in central Europe.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了世界不同国家的粗略轮廓。每个首都城市的位置通过标记表示。从这个视角看，很难区分中欧地区的各个城市。
- en: How it works...
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The GeoPandas package is an extension of Pandas that works with geographical
    data, while the Geoplot package is an extension of Matplotlib that’s used to plot
    geographical data. The GeoPandas package comes with a selection of sample datasets
    that we used in this recipe. `naturalearth_lowres` contains geometric figures
    that describe the boundaries of countries in the world. This data is not very
    high-resolution, as signified by its name, which means that some of the finer
    details of geographical features might not be present on the map (some small islands
    are not shown at all). `naturalearth_cities` contains the names and locations
    of the capital cities of the world. We’re using the `datasets.get_path` routine
    to retrieve the path for these datasets in the package data directory. The `read_file`
    routine imports the data into the Python session.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: GeoPandas 包是 Pandas 的一个扩展，专门用于处理地理数据，而 Geoplot 包是 Matplotlib 的一个扩展，用于绘制地理数据。GeoPandas
    包提供了一些示例数据集，我们在本节中使用了这些数据集。`naturalearth_lowres` 包含描述世界各国边界的几何图形。这些数据的分辨率不高，正如其名称所示，因此地图上可能缺少一些地理特征的细节（一些小岛根本没有显示）。`naturalearth_cities`
    包含世界各国首都城市的名称和位置。我们使用 `datasets.get_path` 例程来检索这些数据集在包数据目录中的路径。`read_file` 例程将数据导入到
    Python 会话中。
- en: The Geoplot package provides some additional plotting routines specifically
    for plotting geographical data. The `polyplot` routine plots polygonal data from
    a GeoPandas DataFrame, which might describe the geographical boundaries of a country.
    The `pointplot` routine plots discrete points on a set of axes from a GeoPandas
    DataFrame, which, in this case, describe the positions of capital cities.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Geoplot 包提供了一些额外的绘图例程，专门用于绘制地理数据。`polyplot` 例程绘制来自 GeoPandas DataFrame 的多边形数据，这些数据描述了国家的地理边界。`pointplot`
    例程则在一组坐标轴上绘制来自 GeoPandas DataFrame 的离散点，这些点在本例中表示首都城市的位置。
- en: Executing a Jupyter notebook as a script
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 Jupyter notebook 作为脚本执行
- en: Jupyter notebooks are a popular medium for writing Python code for scientific
    and data-based applications. A Jupyter notebook is really a sequence of blocks
    that is stored in a file in `ipynb` extension. Each block can be one of several
    different types, such as code or markdown. These notebooks are typically accessed
    through a web application that interprets the blocks and executes the code in
    a background kernel that then returns the results to the web application. This
    is great if you are working on a personal PC, but what if you want to run the
    code contained within a notebook remotely on a server? In this case, it might
    not even be possible to access the web interface provided by the Jupyter Notebook
    software. The `papermill` package allows us to parameterize and execute notebooks
    from the command line.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter notebook 是一种流行的工具，广泛用于编写科学计算和数据分析应用中的 Python 代码。Jupyter notebook 实际上是一系列存储在
    `.ipynb` 格式文件中的代码块。每个代码块可以是不同类型的，比如代码块或 Markdown 块。这些 notebook 通常通过 Web 应用访问，Web
    应用解释这些代码块并在后台的内核中执行代码，结果再返回给 Web 应用。如果你在个人电脑上工作，这种方式非常方便，但如果你想在远程服务器上执行 notebook
    中的代码呢？在这种情况下，可能连 Jupyter Notebook 提供的 Web 界面都无法访问。`papermill` 包允许我们在命令行中对 notebook
    进行参数化和执行。
- en: In this recipe, we’ll learn how to execute a Jupyter notebook from the command
    line using `papermill`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用 `papermill` 从命令行执行 Jupyter notebook。
- en: Getting ready
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we will need to have the `papermill` package installed, and
    also have a sample Jupyter notebook in the current directory. We will use the
    `sample.ipynb` notebook file stored in the code repository for this chapter.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此方法，我们需要安装 `papermill` 包，并且在当前目录中有一个示例 Jupyter notebook。我们将使用本章代码库中存储的 `sample.ipynb`
    notebook 文件。
- en: How to do it...
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Follow these steps to use the `papermill` command-line interface to execute
    a Jupyter notebook remotely:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤使用 `papermill` 命令行界面远程执行 Jupyter notebook：
- en: 'First, we open the sample notebook, `sample.ipynb`, from the code repository
    for this chapter. The notebook contains three code cells that hold the following
    code:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从本章的代码库中打开示例 notebook 文件 `sample.ipynb`。该 notebook 包含三个代码单元，其中包含以下代码：
- en: '[PRE59]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Next, we open the folder containing the Jupyter notebook in the terminal and
    use the following command:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在终端中打开包含 Jupyter notebook 的文件夹，并使用以下命令：
- en: '[PRE66]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now, we open the output file, `output.ipynb`, which should now contain the
    notebook that’s been updated with the result of the executed code. The scatter
    plot that’s generated in the final block is shown here:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们打开输出文件 `output.ipynb`，该文件应该包含已经更新的 notebook，其中包含执行代码的结果。最终生成的散点图如下所示：
- en: '![Figure 10.3 - Scatter plot of the random data that was generated inside a
    Jupyter notebook'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.3 - 在 Jupyter notebook 内生成的随机数据的散点图'
- en: '](img/B19085_10_03.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B19085_10_03.jpg)'
- en: Figure 10.3 - Scatter plot of the random data that was generated inside a Jupyter
    notebook
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 - 在 Jupyter notebook 内生成的随机数据的散点图
- en: Notice that the output of the `papermill` command is an entirely new notebook
    that copies the code and text content from the original and is populated with
    the output of running commands. This is useful for “freezing” the exact code that
    was used to generate the results.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`papermill` 命令的输出是一个全新的 notebook，它复制了原始代码和文本内容，并填充了运行命令后的输出。这对于“冻结”用于生成结果的确切代码非常有用。
- en: How it works...
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The `papermill` package provides a simple command-line interface that interprets
    and then executes a Jupyter notebook and stores the results in a new notebook
    file. In this recipe, we gave the first argument – the input notebook file – `sample.ipynb`,
    and the second argument – the output notebook file – `output.ipynb`. The tool
    then executes the code contained in the notebook and produces the output. The
    notebook’s file format keeps track of the results of the last run, so these results
    are added to the output notebook and stored at the desired location. In this recipe,
    this is a simple local file, but `papermill` can also store them in a cloud location
    such as **Amazon Web Services** (**AWS**) S3 storage or Azure data storage.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`papermill` 包提供了一个简单的命令行界面，它解释并执行 Jupyter notebook，并将结果存储在新的 notebook 文件中。在此方法中，我们将第一个参数——输入的
    notebook 文件——设为 `sample.ipynb`，第二个参数——输出的 notebook 文件——设为 `output.ipynb`。然后，该工具执行
    notebook 中的代码并生成输出。notebook 的文件格式会跟踪最后一次运行的结果，因此这些结果会被添加到输出 notebook 中并存储在指定位置。在这个示例中，我们将其存储为一个简单的本地文件，但
    `papermill` 也可以将其存储在云端位置，如 **Amazon Web Services** (**AWS**) S3 存储或 Azure 数据存储。'
- en: 'In *step 2*, we added the `--kernel python3` option when using the `papermill`
    command-line interface. This option allows us to specify the kernel that is used
    to execute the Jupyter notebook. This might be necessary to prevent errors if
    `papermill` tries to execute the notebook with a kernel other than the one used
    to write the notebook. A list of available kernels can be found by using the following
    command in the terminal:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 2* 中，我们在使用 `papermill` 命令行界面时添加了 `--kernel python3` 选项。该选项允许我们指定用于执行 Jupyter
    notebook 的内核。如果 `papermill` 尝试使用与编写 notebook 时不同的内核执行 notebook，可能会导致错误，因此此选项可能是必要的。可以通过在终端中使用以下命令来查看可用的内核列表：
- en: '[PRE67]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: If you get an error when executing a notebook, you could try changing to a different
    kernel.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果执行 notebook 时遇到错误，您可以尝试更换为其他内核。
- en: There’s more...
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: Papermill also has a Python interface so that you can execute notebooks from
    within a Python application. This might be useful for building web applications
    that need to be able to perform long-running calculations on external hardware
    and where the results need to be stored in the cloud. It also has the ability
    to provide parameters to a notebook. To do this, we need to create a block in
    the notebook marked with the parameters tag with the default values. Updated parameters
    can then be provided through the command-line interface using the `-p` flag, followed
    by the name of the argument and the value.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Papermill还提供了一个Python接口，使得你可以在Python应用程序中执行notebook。这对于构建需要在外部硬件上执行长时间计算并且结果需要存储在云中的Web应用程序可能很有用。它还可以向notebook提供参数。为此，我们需要在notebook中创建一个标记为参数的块，并设置默认值。然后，可以通过命令行接口使用`-p`标志提供更新的参数，后跟参数名称和相应的值。
- en: Validating data
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证数据
- en: Data is often presented in a raw form and might contain anomalies or incorrect
    or malformed data, which will obviously present a problem for later processing
    and analysis. It is usually a good idea to build a validation step into a processing
    pipeline. Fortunately, the Cerberus package provides a lightweight and easy-to-use
    validation tool for Python.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通常以原始形式呈现，可能包含异常、错误或格式不正确的数据，这显然会给后续的处理和分析带来问题。通常，在处理管道中构建一个验证步骤是个好主意。幸运的是，Cerberus包为Python提供了一个轻量级且易于使用的验证工具。
- en: For validation, we have to define a *schema*, which is a technical description
    of what the data should look like and the checks that should be performed on the
    data. For example, we can check the type and place bounds on the maximum and minimum
    values. Cerberus validators can also perform type conversions during the validation
    step, which allows us to plug data loaded directly from CSV files into the validator.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于验证，我们必须定义一个*模式*，它是数据应该是什么样子以及应该执行哪些检查的技术描述。例如，我们可以检查类型并设定最大和最小值的边界。Cerberus验证器还可以在验证步骤中执行类型转换，这使我们能够将直接从CSV文件加载的数据插入到验证器中。
- en: In this recipe, we will learn how to use Cerberus to validate data loaded from
    a CSV file.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将学习如何使用Cerberus验证从CSV文件加载的数据。
- en: Getting ready
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'For this recipe, we need to import the `csv` module from the Python Standard
    Library ([https://docs.python.org/3/library/csv.html](https://docs.python.org/3/library/csv.html)),
    as well as the Cerberus package:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们需要从Python标准库中导入`csv`模块（[https://docs.python.org/3/library/csv.html](https://docs.python.org/3/library/csv.html)），以及Cerberus包：
- en: '[PRE68]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: We will also need the `sample.csv` file from the code repository ([https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2010](https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2010))
    for this chapter.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要此章节中的`sample.csv`文件，来自代码仓库（[https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2010](https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2010)）。
- en: How to do it...
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In the following steps, we will validate a set of data that’s been loaded from
    CSV using the Cerberus package:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将验证一组已从CSV加载的数据，使用Cerberus包进行验证：
- en: 'First, we need to build a schema that describes the data we expect. To do this,
    we must define a simple schema for floating-point numbers:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要构建一个描述我们期望数据的模式。为此，我们必须定义一个简单的浮点数模式：
- en: '[PRE69]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Next, we build the schema for individual items. These will be the rows of our
    data:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们为单个项构建模式。这些将是我们数据的行：
- en: '[PRE71]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now, we can define the schema for the whole document, which will contain a
    list of items:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以为整个文档定义一个模式，该模式将包含一个项目列表：
- en: '[PRE81]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Next, we create a `Validator` object with the schema we just defined:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个带有我们刚定义的模式的`Validator`对象：
- en: '[PRE87]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Then, we load the data using a `DictReader` from the `csv` module:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用`csv`模块中的`DictReader`加载数据：
- en: '[PRE88]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Next, we use the `validate` method on `validator` to validate the document:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用`validate`方法在`validator`上验证文档：
- en: '[PRE91]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Then, we retrieve the errors from the validation process from the `validator`
    object:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们从`validator`对象中获取验证过程中的错误：
- en: '[PRE92]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Finally, we can print any error messages that appeared:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以打印出现的任何错误信息：
- en: '[PRE93]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The output of the error messages is as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 错误消息的输出如下：
- en: '[PRE95]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: This has identified four rows that do not conform to the schema that we set
    out, which limits the float values in “lower” and “upper” to those between `-1.0`
    and `1.0`.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这已识别出四行不符合我们设定的模式，这限制了“lower”和“upper”中的浮动值仅限于`-1.0`到`1.0`之间。
- en: How it works...
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The schema that we created is a technical description of all the criteria that
    we need to check our data against. This will usually be defined as a dictionary
    with the name of the item as the key and a dictionary of properties, such as the
    type or bounds on the value in a dictionary, as the value. For example, in *step
    1*, we defined a schema for floating-point numbers that limits the numbers so
    that they’re between the values of -1 and 1\. Note that we include the `coerce`
    key, which specifies the type that the value should be converted into during the
    validation. This allows us to pass in data that’s been loaded from a CSV document,
    which only contains strings, without having to worry about its type.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的架构是所有需要检查数据标准的技术描述。通常，它会被定义为一个字典，字典的键是项目名称，值是一个包含属性（如类型或值范围）的字典。例如，在*第1步*中，我们为浮点数定义了一个架构，限制这些数字的值在-1和1之间。请注意，我们包含了`coerce`键，它指定了在验证过程中值应该转换为的类型。这使得我们可以传入从CSV文档中加载的数据，尽管它仅包含字符串，而无需担心其类型。
- en: The `validator` object takes care of parsing documents so that they’re validated
    and checking the data they contain against all the criteria described by the schema.
    In this recipe, we provided the schema to the `validator` object when it was created.
    However, we could also pass the schema into the `validate` method as a second
    argument. The errors are stored in a nested dictionary that mirrors the structure
    of the document.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`validator`对象负责解析文档，以便验证并检查它们包含的数据是否符合架构中描述的所有标准。在这个食谱中，我们在创建`validator`对象时提供了架构。然而，我们也可以将架构作为第二个参数传递给`validate`方法。错误信息被存储在一个嵌套的字典中，字典的结构与文档的结构相对应。'
- en: Accelerating code with Cython
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Cython加速代码
- en: Python is often criticized for being a slow programming language – an endlessly
    debatable statement. Many of these criticisms can be addressed by using a high-performance
    compiled library with a Python interface – such as the scientific Python stack
    – to greatly improve performance. However, there are some situations where it
    is difficult to avoid the fact that Python is not a compiled language. One way
    to improve performance in these (fairly rare) situations is to write a C extension
    (or even rewrite the code entirely in C) to speed up the critical parts. This
    will certainly make the code run more quickly, but it might make it more difficult
    to maintain the package. Instead, we can use Cython, which is an extension of
    the Python language that is transpiled into C and compiled for great performance
    improvements.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Python经常被批评为一种慢编程语言——这一点常常被辩论。许多批评可以通过使用具有Python接口的高性能编译库来解决，比如科学Python栈，从而大大提高性能。然而，有些情况下，我们无法避免Python不是一种编译语言的事实。在这些（相对罕见）情况下，改善性能的一种方式是编写C扩展（甚至将代码完全用C重写），以加速关键部分。这肯定会让代码运行得更快，但可能会让维护该包变得更加困难。相反，我们可以使用Cython，它是Python语言的扩展，通过将Python代码转换为C并编译，从而实现极大的性能提升。
- en: 'For example, we can consider some code that’s used to generate an image of
    the Mandelbrot set. For comparison, the pure Python code – which we assume is
    our starting point – is as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以考虑一些用来生成曼德尔布罗集图像的代码。为了对比，我们假设纯Python代码作为起点，如下所示：
- en: '[PRE96]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'The reason why this code is relatively slow in pure Python is fairly obvious:
    the nested loops. For demonstration purposes, let’s assume that we can’t vectorize
    this code using NumPy. A little preliminary testing shows that using these functions
    to generate the Mandelbrot set using 320 × 240 points and 255 steps takes approximately
    6.3 seconds. Your times may vary, depending on your system.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码在纯Python中相对较慢的原因相当明显：嵌套的循环。为了演示目的，假设我们无法使用NumPy向量化这段代码。初步测试表明，使用这些函数生成曼德尔布罗集图像，使用320
    × 240的点和255步大约需要6.3秒。你的测试时间可能会有所不同，取决于你的系统。
- en: In this recipe, we will use Cython to greatly improve the performance of the
    preceding code in order to generate an image of the Mandelbrot set.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将使用Cython大幅提升前述代码的性能，以生成曼德尔布罗集图像。
- en: Getting ready
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we will need the NumPy package and the Cython package to be
    installed. You will also need a C compiler such as the GCC installed on your system.
    For example, on Windows, you can obtain a version of the GCC by installing MinGW.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，我们需要安装NumPy包和Cython包。你还需要在系统上安装一个C编译器，如GCC。例如，在Windows上，你可以通过安装MinGW来获得GCC版本。
- en: How to do it...
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Follow these steps to use Cython to greatly improve the performance of the
    code for generating an image of the Mandelbrot set:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤使用Cython大幅提高生成曼德尔布罗特集图像代码的性能：
- en: 'Start a new file called `cython_mandel.pyx` in the `mandelbrot` folder. In
    this file, we will add some simple imports and type definitions:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`mandelbrot`文件夹中开始一个名为`cython_mandel.pyx`的新文件。在此文件中，我们将添加一些简单的导入和类型定义：
- en: '[PRE97]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Next, we define a new version of the `in_mandel` routine using the Cython syntax.
    We add some declarations to the first few lines of this routine:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用Cython语法定义`in_mandel`例程的新版本。在该例程的前几行添加一些声明：
- en: '[PRE103]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'The rest of the function is identical to the Python version of the function:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其余部分与Python版本的函数完全相同：
- en: '[PRE108]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Next, we define a new version of the `compute_mandel` function. We add two
    decorators to this function from the Cython package:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义`compute_mandel`函数的新版本。我们向该函数添加了两个来自Cython包的装饰器：
- en: '[PRE116]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Then, we define the constants, just as we did in the original routine:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义常量，就像在原始例程中一样：
- en: '[PRE119]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'We use the `linspace` and `empty` routines from the NumPy package in exactly
    the same way as in the Python version. The only addition here is that we declare
    the `i` and `j` variables, which are of the `Int` type:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用与Python版本完全相同的方式调用NumPy包中的`linspace`和`empty`例程。唯一的不同是，我们声明了`i`和`j`变量，它们是`Int`类型：
- en: '[PRE123]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'The remainder of the definition is exactly the same as in the Python version:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义的其余部分与Python版本完全相同：
- en: '[PRE130]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'Next, we create a new file called `setup.py` in the `mandelbrot` folder and
    add the following imports to the top of this file:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在`mandelbrot`文件夹中创建一个新的文件，命名为`setup.py`，并在文件顶部添加以下导入：
- en: '[PRE135]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '[PRE137]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'After that, we define an extension module with the source pointing to the original
    `python_mandel.py` file. Set the name of this module to `hybrid_mandel`:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们定义一个扩展模块，源文件指向原始的`python_mandel.py`文件。将此模块的名称设置为`hybrid_mandel`：
- en: '[PRE139]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'Now, we define a second extension module with the source set as the `cython_mandel.pyx`
    file that we just created:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义第二个扩展模块，源文件设置为刚才创建的`cython_mandel.pyx`文件：
- en: '[PRE146]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'Next, we add both these extension modules to a list and call the `setup` routine
    to register these modules:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将这两个扩展模块添加到一个列表中，并调用`setup`例程来注册这些模块：
- en: '[PRE153]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: Create a new empty file called `__init__.py` in the `mandelbrot` folder to make
    this into a package that can be imported into Python.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`mandelbrot`文件夹中创建一个名为`__init__.py`的新空文件，使其成为一个可以导入到Python中的包。
- en: 'Open the terminal inside the `mandelbrot` folder and use the following command
    to build the Cython extension modules:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`mandelbrot`文件夹中的终端，使用以下命令构建Cython扩展模块：
- en: '[PRE159]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: 'Now, start a new file called `run.py` and add the following `import` statements:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，开始一个名为`run.py`的新文件，并添加以下`import`语句：
- en: '[PRE160]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: 'Import the various `compute_mandel` routines from each of the modules we have
    defined: `python_mandel` for the original; `hybrid_mandel` for the Cythonized
    Python code; and `cython_mandel` for the compiled pure Cython code:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们定义的每个模块中导入不同的`compute_mandel`例程：`python_mandel`为原始版本；`hybrid_mandel`为Cython化的Python代码；`cython_mandel`为编译后的纯Cython代码：
- en: '[PRE164]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '[PRE167]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: 'Define a simple timer decorator that we will use to test the performance of
    the routines:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个简单的计时器装饰器，我们将用它来测试例程的性能：
- en: '[PRE170]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: '[PRE173]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE174]'
- en: '[PRE175]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '[PRE177]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE179]'
- en: 'Apply the `timer` decorator to each of the imported routines, and define some
    constants for testing:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`timer`装饰器应用到每个导入的例程，并为测试定义一些常量：
- en: '[PRE180]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '[PRE182]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE182]'
- en: '[PRE183]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE183]'
- en: '[PRE184]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE184]'
- en: '[PRE185]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE185]'
- en: 'Run each of the decorated routines with the constants we set previously. Record
    the output of the final call (the Cython version) in the `vals` variable:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们之前设置的常量运行每个已装饰的例程。将最后一次调用（Cython版本）的输出记录在`vals`变量中：
- en: '[PRE186]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE186]'
- en: '[PRE187]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '[PRE188]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE188]'
- en: 'Finally, plot the output of the Cython version to check that the routine computes
    the Mandelbrot set correctly:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，绘制Cython版本的输出，以检查该例程是否正确计算曼德尔布罗特集：
- en: '[PRE189]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '[PRE190]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE191]'
- en: 'Running the `run.py` file will print the execution time of each of the routines
    to the terminal, as follows:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`run.py`文件将会打印每个例程的执行时间到终端，如下所示：
- en: '[PRE192]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: Note
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: These timings are not as good as in the first edition, which is likely due to
    the way Python is installed on the author’s PC. Your timings may vary.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这些时间不如第一版那样好，这可能是由于作者的PC上Python安装的方式。你的时间可能会有所不同。
- en: 'The plot of the Mandelbrot set can be seen in the following figure:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 曼德尔布罗特集的图像可以在下图中看到：
- en: '![Figure 10.4 - Image of the Mandelbrot set computed using Cython code'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.4 - 使用Cython代码计算的曼德尔布罗特集图像](img/B19085_10_04.jpg)'
- en: '](img/B19085_10_04.jpg)'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B19085_10_04.jpg)'
- en: Figure 10.4 - Image of the Mandelbrot set computed using Cython code
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 - 使用Cython代码计算的曼德尔布罗特集图像
- en: This is what we expect for the Mandelbrot set. Some of the finer detail is visible
    around the boundary.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对曼德尔布罗集合的预期效果。细节部分在边界处有所显示。
- en: How it works...
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: There is a lot happening in this recipe, so let’s start by explaining the overall
    process. Cython takes code that is written in an extension of the Python language
    and compiles it into C code, which is then used to produce a C extension library
    that can be imported into a Python session. In fact, you can even use Cython to
    compile ordinary Python code directly to an extension, although the results are
    not as good as when using the modified language. The first few steps in this recipe
    define the new version of the Python code in the modified language (saved as a
    `.pyx` file), which includes type information in addition to the regular Python
    code. In order to build the C extension using Cython, we need to define a setup
    file, and then we create a file that we run to produce the results.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱中有很多内容需要说明，因此我们先从解释整体过程开始。Cython 将用 Python 语言扩展编写的代码编译成 C 代码，然后生成一个可以导入到
    Python 会话中的 C 扩展库。实际上，你甚至可以使用 Cython 将普通的 Python 代码直接编译成扩展，尽管结果不如使用修改后的语言时好。食谱中的前几步定义了用修改后的语言编写的
    Python 代码的新版本（保存为 `.pyx` 文件），其中除了常规的 Python 代码外，还包含了类型信息。为了使用 Cython 构建 C 扩展，我们需要定义一个设置文件，然后创建一个文件运行它来生成结果。
- en: The final compiled version of the Cython code runs considerably faster than
    its Python equivalent. The Cython-compiled Python code (hybrid, as we called it
    in this recipe) performs slightly better than the pure Python code. This is because
    the produced Cython code still has to work with Python objects with all of their
    caveats. By adding the typing information to the Python code, in the `.pyx` file,
    we start to see major improvements in performance. This is because the `in_mandel`
    function is now effectively defined as a C-level function that has no interaction
    with Python objects, and instead operates on primitive data types.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: Cython 编译后的最终版本比其 Python 等效版本运行得要快得多。Cython 编译后的 Python 代码（我们在本食谱中称之为混合代码）比纯
    Python 代码稍微快一些。这是因为生成的 Cython 代码仍然需要与 Python 对象进行交互，且必须考虑所有相关问题。通过在 `.pyx` 文件中的
    Python 代码中添加类型信息，我们开始看到性能有了显著的提升。这是因为 `in_mandel` 函数现在有效地被定义为一个 C 级别的函数，不再与 Python
    对象交互，而是直接操作原始数据类型。
- en: There are some small, but very important differences, between the Cython code
    and the Python equivalent. In *step 1*, you can see that we imported the NumPy
    package as usual but that we also used the `cimport` keyword to bring some C-level
    definitions into the scope. In *step 2*, we used the `cdef` keyword instead of
    the `def` keyword when we defined the `in_mandel` routine. This means that the
    `in_mandel` routine is defined as a C-level function that cannot be used from
    the Python level, which saves a significant amount of overhead when calling this
    function (which happens a lot).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: Cython 代码与 Python 等效代码之间有一些小但非常重要的差异。在 *步骤 1* 中，你可以看到我们照常导入了 NumPy 包，但我们还使用了
    `cimport` 关键字将一些 C 级别的定义引入作用域。在 *步骤 2* 中，我们在定义 `in_mandel` 函数时使用了 `cdef` 关键字，而不是
    `def` 关键字。这意味着 `in_mandel` 函数被定义为一个 C 级别的函数，不能从 Python 级别调用，这样在调用该函数时（这会发生很多次）就节省了大量开销。
- en: The only other real differences regarding the definition of this function are
    the inclusion of some type declarations in the signature and the first few lines
    of the function. The two decorators we applied here disable the checking of bounds
    when accessing elements from a list (array). The `boundscheck` decorator disables
    checking whether the index is valid (between 0 and the size of the array), while
    the `wraparound` decorator disables the negative indexing. Both of these give
    a modest improvement to speed during execution, although they disable some of
    the safety features built into Python. In this recipe, it is OK to disable these
    checks because we are using a loop over the valid indices of the array.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 关于该函数定义的唯一其他实际差异是签名中和函数的前几行中包含了一些类型声明。我们在这里应用的两个装饰器禁用了访问列表（数组）元素时的边界检查。`boundscheck`
    装饰器禁用了检查索引是否有效（即是否在 0 和数组大小之间），而 `wraparound` 装饰器禁用了负索引。这两个装饰器都能在执行过程中带来适度的速度提升，尽管它们会禁用
    Python 内置的一些安全功能。在这个食谱中，禁用这些检查是可以接受的，因为我们正在遍历数组的有效索引。
- en: The setup file is where we tell Python (and therefore Cython) how to build the
    C extension. The `cythonize` routine from Cython is the key here, as it triggers
    the Cython build process. In *steps 9* and *10*, we defined extension modules
    using the `Extension` class from `setuptools` so that we could define some extra
    details for the build; specifically, we set an environment variable for the NumPy
    compilation and added the `include` files for the NumPy C headers. This is done
    via the `define_macros` keyword argument for the `Extension` class. The terminal
    command we used in *step 13* uses `setuptools` to build the Cython extensions,
    and the addition of the `--inplace` flat means that the compiled libraries will
    be added to the current directory, rather than being placed in a centralized location.
    This is good for development.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 设置文件是我们告诉 Python（因此也告诉 Cython）如何构建 C 扩展的地方。Cython 中的 `cythonize` 例程是关键，它触发了
    Cython 构建过程。在 *步骤 9* 和 *步骤 10* 中，我们使用 `setuptools` 中的 `Extension` 类定义了扩展模块，以便为构建定义一些额外的细节；具体来说，我们为
    NumPy 编译设置了一个环境变量，并添加了 NumPy C 头文件的 `include` 文件。这是通过 `Extension` 类的 `define_macros`
    关键字参数完成的。我们在 *步骤 13* 中使用的终端命令使用 `setuptools` 构建 Cython 扩展，并且添加 `--inplace` 标志意味着编译后的库将被添加到当前目录，而不是放置在集中位置。这对于开发来说非常方便。
- en: 'The run script is fairly simple: import the routines from each of the defined
    modules – two of these are actually C extension modules – and time their execution.
    We have to be a little creative with the import aliases and routine names to avoid
    collisions.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本非常简单：从每个已定义的模块中导入例程——其中有两个实际上是 C 扩展模块——并测量它们的执行时间。我们需要在导入别名和例程名称上稍作创意，以避免冲突。
- en: There’s more...
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: Cython is a powerful tool for improving the performance of some aspects of your
    code. However, you must always be careful to spend your time wisely while optimizing
    code. Using a profile such as cProfile that is provided in the Python Standard
    Library can be used to find the places where performance bottlenecks occur in
    your code. In this recipe, it was fairly obvious where the performance bottleneck
    was occurring. Cython is a good remedy to the problem in this case because it
    involves repetitive calls to a function inside a (double) `for` loop. However,
    it is not a universal fix for performance issues and, more often than not, the
    performance of code can be greatly improved by refactoring it so that it makes
    use of high-performance libraries.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: Cython 是一个强大的工具，可以提高代码某些方面的性能。然而，在优化代码时，你必须始终谨慎，明智地分配时间。可以使用 Python 标准库中提供的配置文件（如
    cProfile）来找到代码中性能瓶颈发生的地方。在这个案例中，性能瓶颈的位置相当明显。Cython 是解决这个问题的一个好办法，因为它涉及到在（双重）`for`
    循环中重复调用一个函数。然而，它并不是解决所有性能问题的万能方法，通常情况下，重构代码使其能够利用高性能库，往往能显著提高代码的性能。
- en: Cython is well integrated with Jupyter Notebook and can be used seamlessly in
    the code blocks of a notebook. Cython is also included in the Anaconda distribution
    of Python, so no additional setup is required for using Cython with Jupyter notebooks
    when it’s been installed using the Anaconda distribution.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: Cython 与 Jupyter Notebook 紧密集成，可以无缝地在笔记本的代码块中使用。当 Cython 使用 Anaconda 发行版安装时，它也包含在
    Anaconda 中，因此不需要额外的设置就可以在 Jupyter 笔记本中使用 Cython。
- en: There are alternatives to Cython when it comes to producing compiled code from
    Python. For example, the Numba package ([http://numba.pydata.org/](http://numba.pydata.org/))
    provides a **Just-in-Time** (**JIT**) compiler that optimizes Python code at runtime
    by simply placing a decorator on specific functions. Numba is designed to work
    with NumPy and other scientific Python libraries and can also be used to leverage
    GPUs to accelerate code.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他替代方案可以将 Python 代码编译为机器码。例如，Numba 包 ([http://numba.pydata.org/](http://numba.pydata.org/))
    提供了一个 **即时编译**（**JIT**）编译器，通过简单地在特定函数上添加装饰器，在运行时优化 Python 代码。Numba 旨在与 NumPy 和其他科学
    Python 库一起使用，也可以用来利用 GPU 加速代码。
- en: There is also a general-purpose JIT compiler for Python available through the
    `pyjion` package ([https://www.trypyjion.com/](https://www.trypyjion.com/)). This
    can be used in a variety of situations, unlike the Numba library, which is primarily
    for numerical code. The `jax` library discussed in [*Chapter 3*](B19085_03.xhtml#_idTextAnchor078)
    also has a JIT compiler built in, but this too is limited to numerical code.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `pyjion` 包（[https://www.trypyjion.com/](https://www.trypyjion.com/)）还可以使用一个通用的
    JIT 编译器。这可以在各种场景中使用，不像 Numba 库主要用于数值代码。`jax` 库中也有一个内置的 JIT 编译器，如在[*第 3 章*](B19085_03.xhtml#_idTextAnchor078)所讨论的，但它也仅限于数值代码。
- en: Distributing computing with Dask
  id: totrans-402
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Dask 进行分布式计算
- en: Dask is a library that’s used for distributing computing across multiple threads,
    processes, or even computers in order to effectively perform computation on a
    huge scale. This can greatly improve performance and throughput, even if you are
    working on a single laptop computer. Dask provides replacements for most of the
    data structures from the Python scientific stack, such as NumPy arrays and Pandas
    DataFrames. These replacements have very similar interfaces, but under the hood,
    they are built for distributed computing so that they can be shared between multiple
    threads, processes, or computers. In many cases, switching to Dask is as simple
    as changing the `import` statement, and possibly adding a couple of extra method
    calls to start concurrent computations.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 是一个用于在多个线程、进程甚至计算机之间分布式计算的库，旨在有效地进行大规模计算。即使在单台笔记本电脑上工作，它也能大幅提升性能和吞吐量。Dask
    提供了 Python 科学计算栈中大多数数据结构的替代品，比如 NumPy 数组和 Pandas DataFrame。这些替代品具有非常相似的接口，但底层是为分布式计算而构建的，可以在多个线程、进程或计算机之间共享。在许多情况下，切换到
    Dask 就像改变 `import` 语句那么简单，可能还需要添加几个额外的方法调用来启动并发计算。
- en: In this recipe, we will learn how to use Dask to do some simple computations
    on a DataFrame.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，我们将学习如何使用 Dask 在 DataFrame 上进行一些简单的计算。
- en: Getting ready
  id: totrans-405
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we will need to import the `dataframe` module from the Dask
    package. Following the convention set out in the Dask documentation, we will import
    this module under the `dd` alias:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实例，我们需要从 Dask 包中导入 `dataframe` 模块。按照 Dask 文档中的约定，我们将以 `dd` 别名导入此模块：
- en: '[PRE193]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: We will also need the `sample.csv` file from the code repository for this chapter.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要本章代码库中的 `sample.csv` 文件。
- en: How to do it...
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Follow these steps to use Dask to perform some computations on a DataFrame
    object:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤使用 Dask 对 DataFrame 对象执行一些计算：
- en: 'First, we need to load the data from `sample.csv` into a Dask DataFrame. The
    type of the `number` column is set to `"object"` because otherwise, Dask’s type
    inference will fail (since this column contains `None` but is otherwise integers):'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要将数据从 `sample.csv` 加载到 Dask DataFrame 中。`number` 列的类型设置为 `"object"`，因为否则
    Dask 的类型推断会失败（因为此列包含 `None`，但其余部分为整数）：
- en: '[PRE194]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE195]'
- en: 'Next, we perform a standard calculation on the columns of the DataFrame:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们对 DataFrame 的列执行标准计算：
- en: '[PRE196]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[PRE197]'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE197]'
- en: 'Unlike Pandas DataFrames, the result is not a new DataFrame. The `print` statement
    gives us the following information:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Pandas DataFrame 不同，结果不是一个新的 DataFrame。`print` 语句为我们提供了以下信息：
- en: '[PRE198]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE198]'
- en: 'To actually get the result, we need to use the `compute` method:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要实际获取结果，我们需要使用 `compute` 方法：
- en: '[PRE199]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE200]'
- en: 'The result is now shown as expected:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 结果现在如预期所示：
- en: '[PRE201]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: 'We compute the means of the final two columns in exactly the same way we would
    with a Pandas DataFrame, but we need to add a call to the `compute` method to
    execute the calculation:'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算最后两列的均值，方法和使用 Pandas DataFrame 完全相同，只是需要调用 `compute` 方法来执行计算：
- en: '[PRE202]'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE203]'
- en: 'The result, as printed, is exactly as we expect it to be:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如我们所预期的那样打印出来：
- en: '[PRE204]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE204]'
- en: How it works...
  id: totrans-429
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Dask builds a *task graph* for the computation, which describes the relationships
    between the various operations and calculations that need to be performed on the
    collection of data. This breaks down the steps of the calculation so that calculations
    can be done in the right order across the different workers. This task graph is
    then passed into a scheduler that sends the actual tasks to the workers for execution.
    Dask comes with several different schedulers: synchronous, threaded, multiprocessing,
    and distributed. The type of scheduler can be chosen in the call to the `compute`
    method or set globally. Dask will choose a sensible default if one is not given.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 为计算构建了一个*任务图*，该图描述了需要在数据集合上执行的各种操作和计算之间的关系。这将计算步骤拆解开来，以便可以在不同的工作节点上按照正确的顺序进行计算。这个任务图会被传递给调度器，调度器将实际任务发送到工作节点进行执行。Dask
    提供了几种不同的调度器：同步、线程化、进程化和分布式。调度器的类型可以在调用 `compute` 方法时选择，或者全局设置。如果没有指定，Dask 会选择一个合理的默认值。
- en: The synchronous, threaded, and multiprocessing schedulers work on a single machine,
    while the distributed scheduler is for working with a cluster. Dask allows you
    to change between schedulers in a relatively transparent way, although for small
    tasks, you might not get any performance benefits because of the overhead of setting
    up more complicated schedulers.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 同步、线程化和进程化调度器适用于单台机器，而分布式调度器则用于集群。Dask 允许你以相对透明的方式在调度器之间进行切换，尽管对于小任务来说，由于设置更复杂的调度器所带来的开销，你可能不会获得任何性能提升。
- en: The `compute` method is the key to this recipe. The methods that would ordinarily
    perform the computation on Pandas DataFrames now just set up a computation that
    is to be executed through the Dask scheduler. The computation isn’t started until
    the `compute` method is called. This is similar to the way that a `Future` (such
    as from the asyncio standard library package) is returned as a proxy for the result
    of an asynchronous function call, which isn’t fulfilled until the computation
    is complete.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute` 方法是本食谱的关键。通常会对 Pandas DataFrame 执行计算的方法，现在仅仅是设置一个通过 Dask 调度器执行的计算。直到调用
    `compute` 方法，计算才会开始。这类似于 `Future`（如来自 asyncio 标准库的 `Future`）的方式，它作为异步函数调用结果的代理，直到计算完成时，才会返回实际结果。'
- en: There’s more...
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Dask provides interfaces for NumPy arrays, as well as the DataFrames shown in
    this recipe. There is also a machine learning interface called `dask_ml` that
    exposes similar capabilities to the `scikit-learn` package. Some external packages,
    such as `xarray`, also have a Dask interface. Dask can also work with GPUs to
    further accelerate computations and load data from remote sources, which is useful
    if the computation is distributed across a cluster.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 提供了用于 NumPy 数组的接口，以及在本食谱中展示的 DataFrame 接口。还有一个名为 `dask_ml` 的机器学习接口，提供与
    `scikit-learn` 包类似的功能。一些外部包，如 `xarray`，也有 Dask 接口。Dask 还可以与 GPU 协同工作，以进一步加速计算并从远程源加载数据，这在计算分布在集群中的时候非常有用。
- en: Writing reproducible code for data science
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写可重复的数据科学代码
- en: One of the fundamental principles of the scientific method is the idea that
    results should be reproducible and independently verifiable. Sadly, this principle
    is often undervalued in favor of “novel” ideas and results. As practitioners of
    data science, we have an obligation to do our part to make our analyses and results
    as reproducible as possible.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 科学方法的基本原则之一是，结果应该是可重复的并且可以独立验证的。遗憾的是，这一原则常常被低估，反而更重视“新颖”的想法和结果。作为数据科学的从业者，我们有责任尽力使我们的分析和结果尽可能具有可重复性。
- en: Since data science is typically done entirely on computers – that is, it doesn’t
    usually involve instrumental errors involved in measurements – some might expect
    that all data science is inherently reproducible. This is certainly not the case.
    It is easy to overlook simple things such as seeding randomness (see [*Chapter
    3*](B19085_03.xhtml#_idTextAnchor078)) when using randomized hyperparameter searches
    or stochastic gradient descent-based optimization. Moreover, more subtle non-deterministic
    factors (such as use of threading or multiprocessing) can dramatically change
    results if you are not aware of them.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据科学通常完全在计算机上进行——也就是说，它通常不涉及测量中的仪器误差——有些人可能会认为所有的数据科学工作本质上都是可重复的。但事实并非如此。在使用随机化的超参数搜索或基于随机梯度下降的优化时，容易忽略一些简单的细节，比如种子设置（参见[*第3章*](B19085_03.xhtml#_idTextAnchor078)）。此外，一些更微妙的非确定性因素（例如使用线程或多进程）如果不加以注意，可能会显著改变结果。
- en: In this recipe, we’ll look at an example of a basic data analysis pipeline and
    implement some basic steps to make sure you can reproduce the results.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例程中，我们将展示一个基本数据分析管道的示例，并实施一些基本步骤，以确保您可以重复结果。
- en: Getting ready
  id: totrans-439
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we will need the NumPy package, imported as `np`, as usual,
    the Pandas package, imported as `pd`, the Matplotlib `pyplot` interface imported
    as `plt`, and the following imports from the `scikit-learn` package:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例程，我们将需要NumPy包，通常以`np`导入，Pandas包，以`pd`导入，Matplotlib的`pyplot`接口，以`plt`导入，以及从`scikit-learn`包中导入以下内容：
- en: '[PRE205]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE205]'
- en: 'We’re going to simulate our data (rather than having to acquire it from elsewhere),
    so we need to set up an instance of the default random number generator with a
    seed value (for reproducibility):'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模拟我们的数据（而不是从其他地方获取数据），因此需要使用一个具有种子值的默认随机数生成器实例（以确保可重复性）：
- en: '[PRE206]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE206]'
- en: 'To generate the data, we define the following routine:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成数据，我们定义了以下例程：
- en: '[PRE207]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE207]'
- en: We’re using this function in place of some other method of loading the data
    into Python, such as reading from a file or downloading from the internet.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这个函数来代替其他将数据加载到Python中的方法，例如从文件中读取或从互联网上下载。
- en: How to do it…
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'Follow the steps below to create a very simple and reproducible data science
    pipeline:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照以下步骤创建一个非常简单且可重复的数据科学管道：
- en: 'First, we need to “load” our data using the `get_data` routine we defined previously:'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要使用之前定义的`get_data`例程“加载”数据：
- en: '[PRE208]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE208]'
- en: Since our data is acquired dynamically, it is a good idea to store the data
    alongside any results that we generate.
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的数据是动态获取的，最好将数据与我们生成的任何结果一起存储。
- en: '[PRE209]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE209]'
- en: '[PRE210]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE210]'
- en: 'Now, we need to split the data into a training cohort and a testing cohort
    using the `train_test_split` routine from `scikit-learn`. We split the data 80/20
    (%) train/test, and make sure the random state is set so this can be repeated
    (although we will save the indices for reference in the next step):'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要使用`scikit-learn`中的`train_test_split`例程将数据分为训练集和测试集。我们将数据按80/20的比例进行划分，并确保设置了随机状态，以便可以重复这个过程（尽管我们将在下一步保存索引以供参考）：
- en: '[PRE211]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE211]'
- en: '[PRE212]'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE212]'
- en: 'Now, we make sure that we save the indices of the training and test cohorts
    so we know precisely which observations were taken in each sample. We can use
    the indices along with the data stored in *step 2* to completely reconstruct the
    cohorts later:'
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们确保保存训练集和测试集的索引，以便我们确切知道每个样本中的观测值。我们可以将这些索引与*步骤2*中存储的数据一起使用，以便稍后完全重建这些数据集：
- en: '[PRE213]'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE214]'
- en: '[PRE215]'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE215]'
- en: '[PRE216]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE216]'
- en: 'Now, we can set up and train the classifier. We’re using a simple `DecisionTreeClassifier`
    for this example, but this choice is not important. Since the training process
    involves some randomness, make sure to set the `random_state` keyword argument
    to seed this randomness:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以设置并训练分类器。在这个示例中，我们使用一个简单的`DecisionTreeClassifier`，但这个选择并不重要。由于训练过程涉及一些随机性，请确保将`random_state`关键字参数设置为种子值，以便控制这种随机性：
- en: '[PRE217]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '[PRE218]'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE218]'
- en: 'Before we go any further, it is a good idea to gather some information about
    the trained model and store it along with the results. The interesting information
    will vary from model to model. For this model, the feature importance information
    might be useful, so we record this in a CSV file:'
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在继续之前，最好先收集一些关于训练模型的信息，并将其与结果一起存储。不同模型的有趣信息会有所不同。对于这个模型，特征重要性信息可能很有用，因此我们将其记录在一个CSV文件中：
- en: '[PRE219]'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE219]'
- en: '[PRE220]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE220]'
- en: '[PRE221]'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE221]'
- en: '[PRE222]'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE222]'
- en: '[PRE223]'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE223]'
- en: 'Now, we can proceed to check the performance of our model. We’ll evaluate the
    model on both the training data and the test data, which we will later compare
    to the true labels:'
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以继续检查模型的表现。我们将在训练数据和测试数据上评估模型，稍后我们会将其与真实标签进行比较：
- en: '[PRE224]'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE224]'
- en: '[PRE225]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE225]'
- en: 'Always save the results of this kind of prediction task (or regression, or
    any other final results that will in some way be part of the report). We convert
    these into `Series` objects first to make sure the indices are set correctly:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 始终保存此类预测任务（或回归任务，或其他将以某种方式成为报告一部分的最终结果）的结果。我们首先将它们转换为`Series`对象，以确保索引设置正确：
- en: '[PRE226]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE226]'
- en: '[PRE227]'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE227]'
- en: '[PRE228]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE228]'
- en: '[PRE229]'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE229]'
- en: '[PRE230]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE230]'
- en: '[PRE231]'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE231]'
- en: 'Finally, we can produce any graphics or metrics that will inform how we proceed
    with the analysis. Here, we’ll produce a confusion matrix plot for both training
    and testing cohorts and print out some accuracy summary scores:'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以生成任何图形或度量，以帮助我们决定如何继续进行分析。在这里，我们将为训练和测试队列分别生成一个混淆矩阵图，并打印出一些准确度总结评分：
- en: '[PRE232]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE232]'
- en: '[PRE233]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE233]'
- en: '[PRE234]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE234]'
- en: '[PRE235]'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE235]'
- en: '[PRE236]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE236]'
- en: '[PRE237]'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE237]'
- en: '[PRE238]'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE238]'
- en: '[PRE239]'
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE239]'
- en: '[PRE240]'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE240]'
- en: '[PRE241]'
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE241]'
- en: '[PRE242]'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE242]'
- en: '[PRE243]'
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE243]'
- en: '[PRE244]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE244]'
- en: '[PRE245]'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE245]'
- en: 'The resulting confusion matrices are shown in *Figure 10**.5*:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的混淆矩阵见*图 10.5*：
- en: '![Figure 10.5 - Confusion matrices for a simple classification task'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.5 - 简单分类任务的混淆矩阵'
- en: '](img/B19085_10_05.jpg)'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B19085_10_05.jpg)'
- en: Figure 10.5 - Confusion matrices for a simple classification task
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 - 简单分类任务的混淆矩阵
- en: 'The test results for this example are not spectacular, which should not be
    a surprise because we spent no time choosing the most appropriate model or tuning,
    and our sample size was pretty small. Producing an accurate model for this data
    was not the aim. In the current directory (wherever the script was run), there
    should be a number of new CSV files containing all the intermediate data we wrote
    to the disk: `data.csv`, `labels.csv`, `train_index.csv`, `test_index.csv`, `feature_importance.csv`,
    `train_predictions.csv`, and `test_predictions.csv`.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子的测试结果并不出色，这不应该令人惊讶，因为我们没有花时间选择最合适的模型或进行调优，并且我们的样本量相当小。为这些数据生成一个准确的模型并不是目标。在当前目录中（脚本运行所在的目录），应该会有一些新的CSV文件，包含我们写入磁盘的所有中间数据：`data.csv`、`labels.csv`、`train_index.csv`、`test_index.csv`、`feature_importance.csv`、`train_predictions.csv`
    和 `test_predictions.csv`。
- en: How it works…
  id: totrans-501
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The are no definitive *right* answers when it comes to reproducibility, but
    there are certainly wrong answers. We’ve only touched on a few ideas of how to
    make your code more reproducible here, but there are many more things one can
    do. (See *There’s more…*). In the recipe, we really focused on storing intermediate
    values and results more than anything else. This is often overlooked in favor
    of producing plots and graphs – since these are usually the way results will be
    presented. However, we should not have to rerun the whole pipeline in order to
    change the styling of a plot. Storing intermediate values allows you to audit
    various parts of the pipeline and check that what you did was sensible and appropriate
    and that you can reproduce the results from these intermediate values.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 在可重复性方面没有明确的*正确*答案，但肯定有错误的答案。我们这里只触及了如何使代码更具可重复性的一些想法，但还有很多可以做的事情。（参见*更多内容…*）。在这个过程中，我们实际上更专注于存储中间值和结果，而不是其他任何东西。这一点常常被忽视，大家更倾向于生成图表和图形——因为这些通常是展示结果的方式。然而，我们不应该为了更改图表的样式而重新运行整个流程。存储中间值可以让你审计流程中的各个部分，检查你做的事情是否合理和适当，并确保你能从这些中间值中重现结果。
- en: 'Generally speaking, a data science pipeline will consist of five steps:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，数据科学流程包含五个步骤：
- en: Data acquisition
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据获取
- en: Data preprocessing and feature selection
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据预处理和特征选择
- en: Model and hyperparameter tuning
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型和超参数调优
- en: Model training
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练
- en: Evaluation and results generation
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估和结果生成
- en: In the recipe, we replaced the data acquisition with a function that randomly
    generates data. As mentioned in the introduction, this step will usually involve
    loading data from disk (from CSV files or databases), downloading it from the
    internet, or gathering it directly from measurement devices. We cached the results
    of our data acquisition because we are assuming that this is an expensive operation.
    Of course, this is not always the case; if you load all of the data directly from
    disk (via a CSV file, for example) then there is obviously no need to store a
    second copy of this data. However, if you generate the data by querying a large
    database, then storing a flat copy of the data will dramatically improve the speed
    at which you can iterate on your pipeline.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将数据获取步骤替换为一个随机生成数据的函数。如引言中所述，这一步通常会涉及从磁盘加载数据（来自 CSV 文件或数据库）、从互联网下载数据，或直接从测量设备采集数据。我们缓存了数据获取的结果，因为我们假设这是一个开销较大的操作。当然，这并非总是如此；如果你直接从磁盘加载所有数据（例如通过
    CSV 文件），那么显然不需要存储这份数据的第二份副本。然而，如果你通过查询一个大型数据库生成数据，那么存储数据的平面副本将大大提高你在数据管道上迭代的速度。
- en: Our preprocessing consists only of splitting the data into training and testing
    cohorts. Again, we store enough data after this step to recreate these cohorts
    independently later – we stored just the IDs corresponding to each cohort. Since
    we’re storing these sets, it isn’t totally necessary to seed the randomness in
    the `train_test_split` routine, but it is usually a good idea. If your preprocessing
    involves more intensive operations, then you might consider caching the processed
    data or the generated features that you will use in the pipeline (we will cover
    caching in more detail shortly). If your preprocessing step involves selecting
    features from the columns of your data, then you should absolutely save those
    selected features to disk alongside the results.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预处理仅包括将数据分割成训练集和测试集。再次说明，我们在这一步之后存储了足够的数据，以便稍后可以独立地重建这些数据集——我们只存储了与每个数据集对应的
    ID。由于我们已经存储了这些数据集，因此在 `train_test_split` 函数中种子随机数并非绝对必要，但通常来说这是一个好主意。如果你的预处理涉及更为密集的操作，你可能会考虑缓存处理过的数据或在数据管道中使用的生成特征（我们稍后将更详细地讨论缓存）。如果你的预处理步骤涉及从数据的列中选择特征，那么你应该绝对保存这些选择的特征到磁盘，并与结果一起存储。
- en: Our model is very simple and doesn’t have any (non-default) hyperparameters.
    If you have done some hyperparameter tuning, you should store these, along with
    any other metadata that you might need to reconstruct the model. Storing the model
    itself (via pickling or otherwise) can be useful but remember that a pickled model
    might not be readable by another party (for example, if they are using a different
    version of Python).
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型非常简单，且没有任何（非默认）超参数。如果你做过超参数调整，你应该将这些参数以及任何可能需要用来重建模型的元数据进行存储。存储模型本身（通过序列化或其他方式）是有用的，但请记住，序列化后的模型可能无法被其他方读取（例如，如果他们使用的是不同版本的
    Python）。
- en: You should always store the numerical results from your model. It is impossible
    to compare plots and other summary figures when you’re checking that your results
    are the same on subsequent runs. Moreover, this allows you to quickly regenerate
    figures or values later should this be required. For example, if your analysis
    involves a binary classification problem, then storing the values used to generate
    a **Receiver Operating Characteristic** (**ROC**) curve is a good idea, even if
    one also produces a plot of the ROC curve and reports the area under the curve.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该始终存储来自模型的数值结果。当你检查后续运行时结果是否相同，比较图表和其他摘要图形几乎是不可能的。此外，这样做可以让你在之后快速重新生成图形或数值，如果有需要的话。例如，如果你的分析涉及二元分类问题，那么存储用于生成**接收者操作特征**（**ROC**）曲线的值是个好主意，即使你已经绘制了
    ROC 曲线的图形并报告了曲线下的面积。
- en: There’s more…
  id: totrans-513
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: There is a lot we have not discussed here. First, let’s address an obvious point.
    Jupyter notebooks are a common medium for producing data science pipelines. This
    is fine, but users should understand that this format has several shortcomings.
    First, and probably most importantly, is the fact that Jupyter notebooks can be
    run out of order and that later cells might have non-trivial dependencies on earlier
    cells. To address this, make sure that you always run a notebook on a clean kernel
    in its entirety, rather than simply rerunning each cell in a current kernel (using
    tools such as Papermill from the *Executing a Jupyter notebook as a script* recipe,
    for example.) Second, the results stored inside the notebook might not correspond
    to the code that is written in the code cells. This happens when the notebook
    is run and the code is modified after the fact without a rerun. It might be a
    good idea to keep a master copy of the notebook without any stored results and
    make copies of this that are populated with results and are never modified further.
    Finally, Jupyter notebooks are often executed in environments where it is challenging
    to properly cache the results of intermediate steps. This is partially addressed
    by the internal caching mechanism inside the notebook, but this is not always
    totally transparent.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里还没有讨论很多内容。首先，让我们处理一个显而易见的问题。Jupyter notebooks 是一种常用的数据科学工作流工具。这没问题，但用户应该了解，这种格式有几个缺点。首先，最重要的一点是，Jupyter
    notebooks 可以无序运行，后面的单元格可能会依赖于前面的单元格，这些依赖关系可能不是很直观。为了解决这个问题，确保你每次都在一个干净的内核上完整地运行整个
    notebook，而不是仅仅在当前内核中重新运行每个单元格（例如，使用 *Executing a Jupyter notebook as a script*
    这一配方中的 Papermill 工具）。其次，notebook 中存储的结果可能与代码单元格中编写的代码不一致。这种情况发生在 notebook 被运行过并且代码在事后被修改，但没有重新运行的情况下。一个好的做法是，保留一份没有任何存储结果的主副本，然后创建多个副本，其中包含结果，并且这些副本不再进行修改。最后，Jupyter
    notebooks 通常在一些难以正确缓存中间步骤结果的环境中执行。这部分问题由 notebook 内部的缓存机制部分解决，但它并不总是完全透明的。
- en: 'Let’s address two general concerns of reproducibility now: configuration and
    caching. Configuration refers to the collection of values that are used to control
    the setup and execution of the pipeline. We don’t have any obvious configuration
    values in the recipe except for the random seeds used in the `train_test_split`
    routine and the model (and the data generation, but let’s ignore this), and the
    percentage of values to take in the train/test split. These are hardcoded in the
    recipe, but this is probably not the best idea. At the very least, we want to
    be able to record the configuration used in any given run of the analysis. Ideally,
    the configuration should be loaded (exactly once) from a file and then finalized
    and cached before the pipeline runs. What this means is that the full configuration
    is loaded from one or more sources (config files, command-line arguments, or environmental
    variables), consolidated into a single source of truth, and then serialized into
    a machine- and human-readable format such as JSON alongside the results. This
    is so you know precisely what configuration was used to generate the results.'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论两个与可重复性相关的一般性问题：配置和缓存。配置是指用于控制工作流设置和执行的值的集合。在这个配方中，我们没有明显的配置值，除了在 `train_test_split`
    例程中使用的随机种子和模型（以及数据生成，暂时不考虑这些），以及训练/测试集拆分时所取的百分比。这些值是硬编码在配方中的，但这可能不是最好的做法。至少，我们希望能够记录每次运行分析时使用的配置。理想情况下，配置应该从文件中加载（仅加载一次），然后在工作流运行之前被最终确定并缓存。这意味着，完整的配置应该从一个或多个来源（配置文件、命令行参数或环境变量）加载，汇总为一个“真实来源”，然后将其序列化为机器可读和人类可读的格式，如
    JSON，并与结果一起存储。这样你就可以确切知道是使用了什么配置生成了这些结果。
- en: Caching is the process of storing intermediate results so they can be reused
    later to decrease the running time on subsequent runs. In the recipe, we did store
    the intermediate results, but we didn’t build the mechanism to reuse the stored
    data if it exists and is valid. This is because the actual mechanism for checking
    and loading the cached values is complicated and somewhat dependent on the exact
    setup. Since our project is very small, it doesn’t necessarily make any sense
    to cache values. However, for larger projects that have multiple components, this
    absolutely makes a difference. When implementing a caching mechanism, you should
    build a system to check whether the cache is valid by, for example, using the
    SHA-2 hash of the code file and any data sources on which it depends.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存是存储中间结果的过程，以便在随后的运行中可以重用，从而减少运行时间。在本食谱中，我们确实存储了中间结果，但我们没有建立机制来重用已存储的数据（如果它存在且有效）。这是因为实际的检查和加载缓存值的机制较为复杂，并且有些依赖于具体的配置。由于我们的项目非常小，所以缓存值未必有意义。然而，对于有多个组件的大型项目来说，缓存确实能带来差异。在实现缓存机制时，您应该建立一个系统来检查缓存是否有效，例如，可以使用代码文件及其依赖的任何数据源的SHA-2哈希值。
- en: 'When it comes to storing results, it is generally a good idea to store all
    the results together in a timestamped folder or similar. We don’t do this in the
    recipe, but it is relatively easy to achieve. For example, using the `datetime`
    and `pathlib` modules from the standard library, we can easily create a base path
    in which results can be stored:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储结果时，通常的好做法是将所有结果一起存储在一个带有时间戳的文件夹或类似的地方。我们在本食谱中没有这样做，但其实很容易实现。例如，使用标准库中的`datetime`和`pathlib`模块，我们可以轻松创建一个用于存储结果的基础路径：
- en: '[PRE246]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE246]'
- en: You must be a little careful if you are using multiprocessing to run multiple
    analyses in parallel since each new process will generate a new `RESULTS_OUT`
    global variable. A better option is to incorporate this into the configuration
    process, which would also allow the user to customize the output path.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用多进程并行运行多个分析任务，必须小心，因为每个新进程都会生成一个新的`RESULTS_OUT`全局变量。更好的选择是将其纳入配置过程，这样用户还可以自定义输出路径。
- en: Besides the actual code in the script that we have discussed so far, there is
    a great deal one can do at the project level to make the code more reproducible.
    The first, and probably most important step, is to make the code available as
    far as possible, which includes specifying the license under which the code can
    be shared (if at all). Moreover, good code will be robust enough that it can be
    used for analyzing multiple data (obviously, the data should be of the same kind
    as the data originally used). Also important is making use of version control
    (Git, Subversion, and so on) to keep track of changes. This also helps distribute
    the code to other users. Finally, the code needs to be well documented and ideally
    have automated tests to check that the pipeline works as expected on an example
    dataset.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们迄今为止讨论的脚本中的实际代码外，在项目层面上还有很多可以做的事情，以提高代码的可重现性。第一步，也是最重要的一步，是尽可能使代码可用，这包括指定代码可以共享的许可证（如果可以的话）。此外，好的代码应该足够健壮，以便可以用于分析多个数据集（显然，这些数据应该与最初使用的数据类型相同）。同样重要的是使用版本控制（如Git、Subversion等）来跟踪更改。这也有助于将代码分发给其他用户。最后，代码需要有良好的文档说明，并且理想情况下应有自动化测试，以检查管道在示例数据集上的预期表现。
- en: See also...
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见...
- en: 'Here are some additional sources of information about reproducible coding practices:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关于可重现编码实践的额外信息来源：
- en: '*The Turing Way*. Handbook on reproducible, ethical, and collaborative data
    science produced by the Alan Turing Institute. [https://the-turing-way.netlify.app/welcome](https://the-turing-way.netlify.app/welcome
    )'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图灵之路*。《可重现、伦理和协作数据科学手册》，由艾伦·图灵研究所编写。[https://the-turing-way.netlify.app/welcome](https://the-turing-way.netlify.app/welcome)'
- en: 'Review criteria for the Journal of Open Source Software: *Good practice guidelines
    to follow with your own code, even if it is not intended to be* *published*: [https://joss.readthedocs.io/en/latest/review_criteria.html](https://joss.readthedocs.io/en/latest/review_criteria.html)'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《开放源代码软件期刊》审稿标准：*即使代码不是为了公开发布，也应遵循的良好实践指南*：[https://joss.readthedocs.io/en/latest/review_criteria.html](https://joss.readthedocs.io/en/latest/review_criteria.html)
- en: This concludes the 10th and final chapter of the book. Remember that we have
    barely scratched the surface of what is possible when doing mathematics with Python,
    and you should read the documentation and sources mentioned throughout this book
    for much more information about what these packages and techniques are capable
    of.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了本书的第十章，也是最后一章。请记住，我们才刚刚触及了使用 Python 做数学时可能实现的表面，您应该阅读本书中提到的文档和资源，以了解这些包和技术能够实现的更多信息。
