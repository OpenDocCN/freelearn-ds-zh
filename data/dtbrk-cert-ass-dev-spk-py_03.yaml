- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Spark Architecture and Transformations
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark架构和转换
- en: Spark approaches data processing differently than traditional tools and technologies.
    To understand Spark’s unique approach, we will have to understand its basic architecture.
    A deep dive into Spark’s architecture and its components will give you an idea
    of how Spark achieves its ground-breaking processing speeds for big data analytics.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在数据处理方面与传统工具和技术不同。为了理解Spark的独特方法，我们必须了解其基本架构。深入了解Spark的架构及其组件将给你一个关于Spark如何实现其在大数据分析中突破性处理速度的思路。
- en: 'In this chapter, you will learn about the following broader topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解以下更广泛的话题：
- en: Spark architecture and execution hierarchy
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark架构和执行层次结构
- en: Different Spark components
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的不同组件
- en: The roles of the Spark driver and Spark executor
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark驱动程序和Spark执行器的角色
- en: Different deployment modes in Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的不同部署模式
- en: Transformations and actions as Spark operations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为Spark操作，转换和动作
- en: By the end of this chapter, you will have valuable insights into Spark’s inner
    workings and know how to apply this knowledge effectively for your certification
    test.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将对Spark的内部工作原理有宝贵的见解，并知道如何有效地应用这些知识以通过你的认证考试。
- en: Spark architecture
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark架构
- en: In the previous chapters, we discussed that Apache Spark is an open source,
    distributed computing framework designed for big data processing and analytics.
    Its architecture is built to handle various workloads efficiently, offering speed,
    scalability, and fault tolerance. Understanding the architecture of Spark is crucial
    for comprehending its capabilities in processing large volumes of data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了Apache Spark是一个开源的分布式计算框架，旨在处理大数据分析和处理。其架构旨在高效地处理各种工作负载，提供速度、可扩展性和容错性。理解Spark的架构对于理解其处理大量数据的能力至关重要。
- en: 'The components of Spark architecture work in collaboration to process data
    efficiently. The following major components are involved:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Spark架构的组成部分协同工作以高效处理数据。以下是一些主要组件：
- en: Spark driver
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark驱动程序
- en: SparkContext
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkContext
- en: Cluster manager
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群管理器
- en: Worker node
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点
- en: Spark executor
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark执行器
- en: Task
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务
- en: Before we talk about any of these components, it’s important to understand their
    execution hierarchy to know how each component interacts when a Spark program
    starts.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论这些组件之前，了解它们的执行层次结构对于知道每个组件在Spark程序启动时如何交互非常重要。
- en: Execution hierarchy
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行层次结构
- en: 'Let’s look at the execution flow of a Spark application with the help of the
    architecture depicted in *Figure 3**.1*:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们借助*图3.1*中的架构来查看Spark应用程序的执行流程：
- en: '![Figure 3.1: Spark architecture](img/B19176_03_01.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1：Spark架构](img/B19176_03_01.jpg)'
- en: 'Figure 3.1: Spark architecture'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：Spark架构
- en: 'These steps outline the flow from submitting a Spark job to freeing up resources
    when the job is completed:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤概述了从提交Spark作业到作业完成后释放资源的流程：
- en: Spark executions start with a user submitting a `spark-submit` request to the
    Spark engine. This will create a Spark application. Once an action is performed,
    it will result in a **job** being created.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark执行开始于用户向Spark引擎提交`spark-submit`请求。这将创建一个Spark应用程序。一旦执行了操作，就会创建一个**作业**。
- en: This request will initiate communication with the cluster manager. In turn,
    the cluster manager initializes the Spark driver to execute the `main()` method
    of the Spark application. To execute this method, `SparkSession` is created.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此请求将启动与集群管理器的通信。作为回应，集群管理器初始化Spark驱动程序以执行Spark应用程序的`main()`方法。为了执行此方法，将创建`SparkSession`。
- en: The driver starts communicating with the cluster manager and asks for resources
    to start planning for execution.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 驱动程序开始与集群管理器通信，并请求资源以开始规划执行。
- en: The cluster manager then starts the executors, which can communicate with the
    driver directly.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群管理器随后启动执行器，它们可以直接与驱动程序通信。
- en: The driver creates a logical plan, known as a **directed acyclic graph** (**DAG**),
    and physical plan for execution based on the total number of tasks required to
    be executed.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 驱动程序创建一个逻辑计划，称为**有向无环图**（DAG），以及基于需要执行的任务总数的执行计划。
- en: The driver also divides data to be run on each executor, along with tasks.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 驱动程序还将数据分配给每个执行器运行，包括任务。
- en: Once each task finishes running, the driver gets the results.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个任务运行完成后，驱动程序会获取结果。
- en: When the program finishes running, the `main()` method exits and Spark frees
    all executors and driver resources.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当程序运行完成后，`main()`方法退出，Spark释放所有executor和驱动器资源。
- en: Now that you understand the execution hierarchy, let’s discuss each of Spark’s
    components in detail.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了执行层次结构，让我们详细讨论Spark的每个组件。
- en: Spark components
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark组件
- en: Let’s dive into the inner workings of each Spark component to understand how
    each of them plays a crucial role in empowering efficient distributed data processing.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解每个Spark组件的内部工作原理，以了解它们如何在每个组件中发挥关键作用，从而实现高效的分布式数据处理。
- en: Spark driver
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark驱动器
- en: The Spark driver is the core of the intelligent and efficient computations in
    Spark. Spark follows an architecture that is commonly known as the **master-worker
    architecture** in network topology. Consider the Spark driver as a master and
    Spark executors as slaves. The driver has control and knowledge of all the executors
    at any given time. It is the responsibility of the driver to know how many executors
    are present and if any executor has failed so that it can fall back on its alternative.
    The Spark driver also maintains communication with executors all the time. The
    driver runs on the master node of a machine or cluster. When a Spark application
    starts running, the driver keeps up with all the required information that is
    needed to run the application successfully.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Spark驱动器是Spark中智能和高效计算的核心。Spark遵循一种在网络拓扑中通常被称为**主从架构**的架构。将Spark驱动器视为主节点，将Spark
    executors视为从节点。驱动器在任何给定时间都控制并了解所有executor。知道有多少executor存在以及是否有executor失败是驱动器的责任，以便它可以回退到其替代方案。Spark驱动器还始终与executor保持通信。驱动器在机器或集群的主节点上运行。当Spark应用程序开始运行时，驱动器会跟踪所有成功运行应用程序所需的信息。
- en: As shown in *Figure 3**.1*, the driver node contains `SparkSession`, which is
    the entry point of the Spark application. Previously, this was known as the `SparkContext`
    object, but in Spark 2.0, `SparkSession` handles all contexts to start execution.
    The application’s main method runs on the driver to coordinate the whole application.
    It runs on its own **Java Virtual Machine** (**JVM**). Spark driver can run as
    an independent process or it can run on one of the worker nodes, depending on
    the architecture.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图3.1*所示，驱动器节点包含`SparkSession`，这是Spark应用程序的入口点。以前，这被称为`SparkContext`对象，但在Spark
    2.0中，`SparkSession`处理所有上下文以启动执行。应用程序的主方法在驱动器上运行以协调整个应用程序。它在自己的**Java虚拟机**（**JVM**）上运行。Spark驱动器可以作为一个独立进程运行，也可以根据架构在工作者节点之一上运行。
- en: The Spark driver is responsible for dividing the application into smaller entities
    for execution. These entities are known as **tasks**. You will learn more about
    tasks in the upcoming sections of this chapter. The Spark driver also decides
    what data the executor will work on and what tasks are run on which executor.
    These tasks are scheduled to run on the executor nodes with the help of the cluster
    manager. This information that is driven by the driver enables fault tolerance.
    Since the driver has all the information about the number of available workers
    and the tasks that are running on each of them alongside data in case a worker
    fails, that task can be reassigned to a different cluster. Even if a task is taking
    too long to run, it can be assigned to another executor if that gets free. In
    that case, whichever executor returns the task earlier would prevail. The Spark
    driver also maintains metadata about the **Resilient Distributed Dataset** (**RDD**)
    and its partitions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Spark驱动器负责将应用程序划分为更小的执行实体。这些实体被称为**任务**。你将在本章接下来的部分中了解更多关于任务的内容。Spark驱动器还决定executor将处理哪些数据以及哪些任务将在哪个executor上运行。这些任务将在集群管理器的帮助下在executor节点上调度运行。由驱动器驱动的这些信息使得容错成为可能。由于驱动器拥有所有关于可用工作者数量以及每个工作者上运行的任务的信息，以及数据，以防工作者失败，因此可以将该任务重新分配到不同的集群。即使一个任务运行时间过长，如果另一个executor空闲，它也可以被分配到另一个executor。在这种情况下，哪个executor先返回任务，哪个就占上风。Spark驱动器还维护关于**弹性分布式数据集**（**RDD**）及其分区的元数据。
- en: It is the responsibility of the Spark driver to design the complete execution
    map. It determines which tasks run on which executors, as well as how the data
    is distributed across these executors. This is done by creating RDDs internally.
    Based on this distribution of data, the operations that are required are determined,
    such as transformations and actions that are defined in the program. A DAG is
    created based on these decisions. The Spark driver optimizes the logical plan
    (DAG) and finds the best possible execution strategy for the DAG, in addition
    to determining the most optimal location for the execution of a particular task.
    These executions are done in parallel. The executors simply follow these commands
    without doing any optimization on their end.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 设计完整的执行图是 Spark 驱动程序的责任。它确定哪些任务在哪些执行器上运行，以及数据如何在这些执行器之间分布。这是通过内部创建 RDDs 来实现的。基于这种数据分布，确定所需的操作，例如程序中定义的转换和动作。基于这些决策创建一个
    DAG。Spark 驱动程序优化逻辑计划（DAG），并为 DAG 寻找最佳执行策略，除了确定特定任务执行的最优位置。这些执行是并行进行的。执行器只是遵循这些命令，而不会在其端进行任何优化。
- en: For performance considerations, it is optimal to have the Spark driver work
    close to the executor. This reduces the latency by a great deal. This means that
    there would be less delay in the response time of the processes. Another point
    to note here is that this is true for the data as well. The executor reading the
    data close to it would have better performance than otherwise. Ideally, the driver
    and worker nodes should be run in the same **local area network** (**LAN**) for
    the best performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑性能因素，Spark 驱动程序靠近执行器工作是最优的。这大大减少了延迟。这意味着在进程的响应时间上会有更少的延迟。这里要注意的另一点是，这也适用于数据。读取数据的执行器靠近它会有比其他情况下更好的性能。理想情况下，驱动程序和工作节点应该在同一个
    **局域网**（**LAN**）中运行以获得最佳性能。
- en: The Spark driver also creates a web UI for the execution details. This UI is
    very helpful in determining the performance of the application. In cases where
    troubleshooting is required and some bottlenecks need to be identified in the
    Spark process, this UI is very helpful.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 驱动程序还会为执行细节创建一个 Web UI。这个 UI 在确定应用程序性能方面非常有帮助。在需要故障排除并需要在 Spark 过程中识别瓶颈的情况下，这个
    UI 非常有用。
- en: SparkSession
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SparkSession
- en: '`SparkSession` is the main point of entry and interaction with Spark. As discussed
    earlier, in the previous versions of Spark, `SparkContext` used to play this role,
    but in Spark 2.0, `SparkSession` can be created for this purpose. The Spark driver
    creates a `SparkSession` object to interact with the cluster manager and get resource
    allocation through it.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession` 是与 Spark 交互的主要入口点。如前所述，在 Spark 的早期版本中，`SparkContext` 扮演着这个角色，但在
    Spark 2.0 中，可以为此创建 `SparkSession`。Spark 驱动程序创建一个 `SparkSession` 对象来与集群管理器交互，并通过它获取资源分配。'
- en: In the lifetime of the application, `SparkSession` is also used to interact
    with all the underlying Spark APIs. We talked about different Spark APIs in [*Chapter
    2*](B19176_02.xhtml#_idTextAnchor030) namely, SparkSQL, Spark Streaming, MLlib,
    and GraphX. All of these APIs use `SparkSession` from its core to interact with
    the Spark application.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序的生命周期中，`SparkSession` 也用于与所有底层 Spark API 交互。我们曾在 [*第 2 章*](B19176_02.xhtml#_idTextAnchor030)
    中讨论了不同的 Spark API，即 SparkSQL、Spark Streaming、MLlib 和 GraphX。所有这些 API 都从其核心使用 `SparkSession`
    来与 Spark 应用程序交互。
- en: '`SparkSession` keeps track of Spark executors throughout the application’s
    execution.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession` 会跟踪整个应用程序执行过程中的 Spark 执行器。'
- en: Cluster manager
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群管理器
- en: Spark is a distributed framework, which requires it to have access to computing
    resources. This access is governed and controlled by a process known as the cluster
    manager. It is the responsibility of the cluster manager to allocate computing
    resources for the Spark application when the application execution starts. These
    resources become available at the request of the application master. In the Apache
    Spark ecosystem, the **application master** plays a crucial role in managing and
    coordinating the execution of Spark applications within a distributed cluster
    environment. It’s an essential component that’s responsible for negotiating resources,
    scheduling tasks, and monitoring the application’s execution.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个分布式框架，它需要访问计算资源。这种访问由称为集群管理器的过程进行管理和控制。当应用程序开始执行时，集群管理器的责任是为 Spark
    应用程序分配计算资源。这些资源在应用程序主节点的请求下变得可用。在 Apache Spark 生态系统中，**应用程序主节点**在管理和协调分布式集群环境中
    Spark 应用程序的执行中起着至关重要的作用。它是一个基本组件，负责协商资源、调度任务和监控应用程序的执行。
- en: Once the resources are available, the driver is made aware of those resources.
    It’s the responsibility of the driver to manage these resources based on tasks
    that need to be executed by the Spark application. Once the application has finished
    execution, these resources are released back to the cluster manager.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦资源可用，驱动程序就会知道这些资源。根据 Spark 应用程序需要执行的任务，管理这些资源是驱动程序的责任。一旦应用程序完成执行，这些资源就会释放回集群管理器。
- en: Applications have their dedicated executor processes that parallelize how tasks
    are run. The advantage is that each application is independent of the other and
    runs on its own schedule. Data also becomes independent for each of these applications,
    so data sharing can only take place by writing data to disk so that it can be
    shared across applications.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序有自己的专用执行程序进程，这些进程并行运行任务。其优势是每个应用程序都是独立的，并且按照自己的时间表运行。数据对于这些应用程序中的每一个也是独立的，因此数据共享只能通过将数据写入磁盘来实现，以便可以在应用程序之间共享。
- en: Cluster modes
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群模式
- en: Cluster modes define how Spark applications utilize cluster resources, manage
    task execution, and interact with cluster managers for resource allocation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 集群模式定义了 Spark 应用程序如何利用集群资源、管理任务执行以及与集群管理器进行资源分配的交互。
- en: 'If there is more than one user sharing resources on the cluster, be it Spark
    applications or other applications that need cluster resources, they have to be
    managed based on different modes. There are two types of modes available for cluster
    managers – standalone client mode and cluster mode. The following table highlights
    some of the differences between the two:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果集群上有多个用户共享资源，无论是 Spark 应用程序还是需要集群资源的其他应用程序，它们必须根据不同的模式进行管理。集群管理器提供了两种模式类型——独立客户端模式和集群模式。以下表格突出了这两种模式之间的一些差异：
- en: '| **Client Mode** | **Cluster Mode** |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **客户端模式** | **集群模式** |'
- en: '| --- | --- |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| In client mode, the driver program runs on the machine where the Spark application
    is submitted. | In cluster mode, the driver program runs within the cluster, on
    one of the worker nodes. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 在客户端模式中，驱动程序程序在提交 Spark 应用程序的机器上运行。 | 在集群模式中，驱动程序程序在集群内部运行，在工作节点之一上。 |'
- en: '| The driver program is responsible for orchestrating the execution of the
    Spark application, including creating `SparkContext` and coordinating tasks. |
    The cluster manager is responsible for launching the driver program and allocating
    resources for execution. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 驱动程序程序负责协调 Spark 应用程序的执行，包括创建 `SparkContext` 和协调任务。 | 集群管理器负责启动驱动程序程序并为执行分配资源。
    |'
- en: '| The client machine interacts directly with the cluster manager to request
    resources and launch executors on worker nodes. | Once the driver program is launched,
    it coordinates with the cluster manager to request resources and distribute tasks
    to worker nodes. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 客户端机器直接与集群管理器交互，请求资源并在工作节点上启动执行程序。 | 一旦启动驱动程序程序，它就会与集群管理器协调，请求资源并将任务分配给工作节点。
    |'
- en: '| It may not be suitable for production deployments with large-scale applications.
    | It is commonly used for production deployments as it allows for better resource
    utilization and scalability. It also ensures fault tolerance. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 它可能不适合大规模应用程序的生产部署。 | 它通常用于生产部署，因为它允许更好的资源利用和可伸缩性。它还确保了容错性。 |'
- en: 'Table 3.1: Client mode versus cluster mode'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.1：客户端模式与集群模式
- en: 'Now, we will talk about different deployment modes and their corresponding
    managers in Spark:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将讨论不同的部署模式和 Spark 中相应的管理器：
- en: '**Built-in standalone mode** (**Spark’s native manager**): A simple cluster
    manager bundled with Spark that’s suitable for small to medium-scale deployments
    without external dependencies.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内置独立模式**（**Spark 的原生管理器**）：Spark 随带的一个简单集群管理器，适用于无需外部依赖的小到中等规模部署。'
- en: '**Apache YARN** (**Hadoop’s resource manager**): Integrated with Spark, YARN
    enables Spark applications to share Hadoop’s cluster resources efficiently.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache YARN**（**Hadoop 的资源管理器**）：与 Spark 集成，YARN 使 Spark 应用程序能够高效地共享 Hadoop
    的集群资源。'
- en: '**Apache Mesos** (**resource sharing platform**): Mesos offers efficient resource
    sharing across multiple applications, allowing Spark to run alongside other frameworks.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Mesos**（**资源共享平台**）：Mesos 提供了跨多个应用程序的高效资源共享，允许 Spark 与其他框架并行运行。'
- en: We will talk more about deployment modes later in this chapter.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面更详细地讨论部署模式。
- en: Spark executors
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 执行器
- en: Spark executors are the processes that run on the worker node and execute tasks
    sent by the driver. The data is stored in memory primarily but can also be written
    to disk storage closest to them. Driver launches the executors based on the DAG
    that Spark generates for its execution. Once the tasks have finished executing,
    executors send the results back to the driver.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 执行器是在工作节点上运行的进程，执行由驱动程序发送的任务。数据主要存储在内存中，但也可以写入它们最近的磁盘存储。驱动程序根据 Spark 为其执行生成的
    DAG 启动执行器。一旦任务执行完成，执行器将结果发送回驱动程序。
- en: Since the driver is the main controller of the Spark application, if an executor
    fails or takes too long to execute a task, the driver can choose to send that
    task over to other available executors. This ensures reliability and fault tolerance
    in Spark. We will read more about this later in this chapter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于驱动程序是 Spark 应用的主要控制器，如果执行器失败或执行任务花费时间过长，驱动程序可以选择将任务发送到其他可用的执行器。这确保了 Spark
    的可靠性和容错性。我们将在本章后面了解更多关于这一点。
- en: It is the responsibility of the executor to read data from external sources
    that are needed to run the tasks. It can also write its partitioned data to the
    disk as needed. All processing for a task is done by the executor.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器的责任是从外部读取运行任务所需的数据。它还可以根据需要将其分区数据写入磁盘。一个任务的所有处理都由执行器完成。
- en: 'The key functions of an executor are as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器的主要功能如下：
- en: '**Task execution**: Executors run tasks assigned by the Spark application,
    processing data stored in RDDs or DataFrames'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务执行**：执行器运行 Spark 应用程序分配的任务，处理存储在 RDD 或 DataFrame 中的数据'
- en: '**Resource allocation**: Each Spark application has a set of executors allocated
    by the cluster manager for managing resources such as CPU cores and memory'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源分配**：每个 Spark 应用程序都有一组由集群管理器分配的执行器，用于管理资源，如 CPU 内核和内存'
- en: 'In Apache Spark, the concepts of job, stage, and task form the fundamental
    building blocks of its distributed computing framework. Understanding these components
    is essential to grasp the core workings of Spark’s parallel processing and task
    execution. See *Figure 3**.2* to understand the relationship between these concepts
    while we discuss them in detail:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Apache Spark 中，作业、阶段和任务的概念构成了其分布式计算框架的基本构建块。理解这些组件对于掌握 Spark 并行处理和任务执行的核心工作至关重要。见图
    *3*.2* 了解这些概念之间的关系，同时我们详细讨论它们：
- en: '![Figure 3.2: Interaction between jobs, stages, and tasks](img/B19176_03_02.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2：作业、阶段和任务之间的交互](img/B19176_03_02.jpg)'
- en: 'Figure 3.2: Interaction between jobs, stages, and tasks'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：作业、阶段和任务之间的交互
- en: 'Let’s take a closer look:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看：
- en: '`collect`). We will learn more about actions later. When an action (such as
    `collect` or `count`) is invoked on a dataset, it triggers the execution of one
    or more jobs.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collect`）。我们将在后面了解更多关于动作的内容。当在数据集上调用动作（如 `collect` 或 `count`）时，它将触发一个或多个作业的执行。'
- en: A job consists of several stages, each containing tasks that execute a set of
    transformations on data partitions.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个作业由几个阶段组成，每个阶段包含执行数据分区上一系列转换的任务。
- en: '**Stage**: Each job is divided into stages that may depend on other stages.
    Stages act as transformation boundaries – they are created at the boundaries of
    wide transformations that require data shuffling across partitions. If a stage
    is dependent on outputs from a previous stage, then this stage would not begin
    execution until the previous dependent stages have finished execution.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阶段**：每个作业被划分为可能依赖于其他阶段的阶段。阶段充当转换边界 - 它们在需要跨分区进行数据洗牌的宽转换的边界处创建。如果一个阶段依赖于前一个阶段的输出，那么这个阶段将不会开始执行，直到依赖的前一个阶段完成执行。'
- en: Each stage is divided into a set of tasks to be executed on the cluster nodes,
    processing data in parallel.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个阶段被划分为一组在集群节点上执行的任务，以并行方式处理数据。
- en: '**Task**: A task is the smallest unit of execution in Spark. It is the smallest
    object compiled and run by Spark to perform a group of operations. It is executed
    on a Spark executor. Tasks are essentially a series of operations such as filter,
    groupBy, and others.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务**：在Spark中，任务是最小的执行单元。它是Spark编译和运行以执行一组操作的最小对象。它在Spark执行器上执行。任务本质上是一系列操作，如过滤、groupBy等。'
- en: Tasks run in parallel across executors. They can be run on multiple nodes and
    are independent of each other. This is done with the help of slots. Each task
    processes a portion of the data partition. Occasionally, a group of these tasks
    has to finish execution to begin the next task’s execution.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务在执行器上并行运行。它们可以在多个节点上运行，并且彼此独立。这是通过槽位来实现的。每个任务处理数据分区的一部分。偶尔，一组任务需要完成执行才能开始下一个任务的执行。
- en: 'Now that we understand these concepts, let’s see why they are significant in
    Spark:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了这些概念，让我们看看为什么它们在Spark中很重要：
- en: '**Parallel processing**: Executors, jobs, stages, and tasks collaborate to
    enable parallel execution of computations, optimizing performance by leveraging
    distributed computing'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行处理**：执行器、作业、阶段和任务协作以实现计算的并行执行，通过利用分布式计算优化性能。'
- en: '**Task granularity and efficiency**: Tasks divide computations into smaller
    units, facilitating efficient resource utilization and parallelism across cluster
    nodes'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务粒度和效率**：任务将计算分解成更小的单元，便于在集群节点间实现高效的资源利用和并行处理。'
- en: Next, we will move on to discuss a significant concept that enhances efficiency
    in computation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论一个增强计算效率的重要概念。
- en: Partitioning in Spark
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark中的分区
- en: 'In Apache Spark, partitioning is a critical concept that’s used to divide data
    across multiple nodes in a cluster for parallel processing. Partitioning improves
    data locality, enhances performance, and enables efficient computation by distributing
    data in a structured manner. Spark supports both static and dynamic partitioning
    strategies to organize data across the cluster nodes:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在Apache Spark中，分区是一个关键概念，用于在集群的多个节点上划分数据以实现并行处理。分区提高了数据局部性，增强了性能，并通过以结构化的方式分配数据来实现高效的计算。Spark支持静态和动态分区策略来组织集群节点上的数据：
- en: '**Static partitioning of resources**: Static partitioning is available on all
    cluster managers. With static partitioning, maximum resources are allocated to
    each application and these resources remain dedicated to these applications during
    their lifetime.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源的静态分区**：静态分区在所有集群管理器上可用。使用静态分区时，每个应用程序都分配了最大资源，并且这些资源在其生命周期内保持专用。'
- en: '**Dynamic sharing of resources**: Dynamic partitioning is only available on
    Mesos. When dynamically sharing resources, the Spark application gets fixed and
    independent memory allocation, such as static partitioning. The major difference
    is that when the tasks are not being run by an application, these cores can be
    used by other applications as well.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态资源共享**：动态分区仅在Mesos上可用。当动态共享资源时，Spark应用程序获得固定的独立内存分配，类似于静态分区。主要区别在于，当任务没有被应用程序运行时，这些核心也可以被其他应用程序使用。'
- en: 'Let’s discuss why partitioning is significant:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下为什么分区很重要：
- en: '**Performance optimization**: Effective partitioning strategies, whether static
    or dynamic, significantly impact Spark’s performance by improving data locality
    and reducing data shuffle'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能优化**：有效的分区策略，无论是静态还是动态，都能通过提高数据局部性和减少数据洗牌来显著影响Spark的性能。'
- en: '**Adaptability and flexibility**: Dynamic partitioning provides adaptability
    to varying data sizes or distribution patterns without manual intervention'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适应性和灵活性**：动态分区能够在无需人工干预的情况下适应变化的数据大小或分布模式。'
- en: '**Control and predictability**: Static partitioning offers control and predictability
    over data distribution, which can be advantageous in specific use cases'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制和可预测性**：静态分区提供了对数据分布的控制和可预测性，这在某些用例中可能是有利的。'
- en: In summary, partitioning strategies – whether static or dynamic – in Spark play
    a crucial role in optimizing data distribution across cluster nodes, improving
    performance, and ensuring efficient parallel processing of data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在 Spark 中，无论是静态还是动态的分区策略，在优化跨集群节点数据分布、提高性能和确保数据高效并行处理方面都发挥着至关重要的作用。
- en: Apache Spark offers different cluster and deployment modes to run applications
    across distributed computing environments. We’ll take a look at them in the next
    section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 提供了不同的集群和部署模式，以在分布式计算环境中运行应用程序。我们将在下一节中探讨它们。
- en: Deployment modes
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署模式
- en: There are different deployment modes available in Spark. These deployment modes
    define how Spark applications are launched, executed, and managed in diverse computing
    infrastructures. Based on these different deployment modes, it gets decided where
    the Spark driver, executor, and cluster manager will run.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中有几种不同的部署模式。这些部署模式定义了 Spark 应用程序如何在不同的计算基础设施中启动、执行和管理。基于这些不同的部署模式，将决定
    Spark 驱动程序、执行器和集群管理器将在哪里运行。
- en: 'The different deployment modes that are available in Spark are as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中可用的不同部署模式如下：
- en: '**Local**: In local mode, the Spark driver and executor run on a single JVM
    and the cluster manager runs on the same host as the driver and executor.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地**：在本地模式下，Spark 驱动程序和执行器在单个 JVM 上运行，集群管理器在驱动程序和执行器相同的宿主机上运行。'
- en: '**Standalone**: In standalone mode, the driver can run on any node of the cluster
    and the executor will launch its own independent JVM. The cluster manager can
    remain on any of the hosts in the cluster.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立**：在独立模式下，驱动程序可以在集群的任何节点上运行，执行器将启动自己的独立 JVM。集群管理器可以保留在集群中的任何主机上。'
- en: '**YARN (client)**: In this mode, the Spark driver runs on the client and YARN’s
    resource manager allocates containers for executors on NodeManagers.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YARN (客户端)**：在此模式下，Spark 驱动程序在客户端运行，YARN 的资源管理器在 NodeManagers 上为执行器分配容器。'
- en: '**YARN (cluster)**: In this mode, the Spark driver runs with the YARN application
    master while YARN’s resource manager allocates containers for executors on NodeManagers.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YARN (集群)**：在此模式下，Spark 驱动程序与 YARN 应用程序主控一起运行，而 YARN 的资源管理器在 NodeManagers
    上为执行器分配容器。'
- en: '**Kubernetes**: In this mode, the driver runs in Kubernetes pods. Executors
    have their own pods.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes**：在此模式下，驱动程序在 Kubernetes 容器中运行。执行器有自己的容器。'
- en: 'Let’s look at some points of significance regarding the different deployment
    modes:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看关于不同部署模式的一些重要点：
- en: '**Resource utilization**: Different deployment modes optimize resource utilization
    by determining where the driver program runs and how resources are allocated between
    the client and the cluster.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源利用率**：不同的部署模式通过确定驱动程序程序运行的位置以及客户端和集群之间如何分配资源来优化资源利用率。'
- en: '**Accessibility and control**: Client mode offers easy accessibility to driver
    logs and outputs, facilitating development and debugging, while cluster mode utilizes
    cluster resources more efficiently for production workloads.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可访问性和控制**：客户端模式提供对驱动程序日志和输出的轻松访问，便于开发和调试，而集群模式则更有效地利用集群资源来处理生产工作负载。'
- en: '**Integration with container orchestration**: Kubernetes deployment mode enables
    seamless integration with containerized environments, leveraging Kubernetes’ orchestration
    capabilities for efficient resource management.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与容器编排集成**：Kubernetes 部署模式允许与容器化环境无缝集成，利用 Kubernetes 的编排能力进行高效的资源管理。'
- en: 'There are some considerations to keep in mind while choosing deployment modes:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 选择部署模式时有一些考虑事项需要注意：
- en: '**Development versus production**: Client mode is suitable for development
    and debugging, while cluster mode is ideal for production workloads'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发和生产**：客户端模式适合开发和调试，而集群模式则适用于生产工作负载。'
- en: '**Resource management**: Evaluate the allocation of resources between client
    and cluster nodes based on the application’s requirements'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源管理**：根据应用程序的需求评估客户端和集群节点之间资源的分配'
- en: '**Containerization needs**: Consider Kubernetes deployment for containerized
    environments, leveraging Kubernetes features for efficient container management'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器化需求**：考虑在容器化环境中使用 Kubernetes 部署，利用 Kubernetes 的功能进行高效的容器管理'
- en: In summary, deployment modes in Apache Spark provide flexibility in how Spark
    applications are launched and executed, catering to different development, production,
    and containerized deployment scenarios.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Apache Spark的部署模式提供了灵活性，以适应不同的开发、生产和容器化部署场景。
- en: Next, we will look at RDDs, which serve as foundational data abstractions in
    Apache Spark, enabling distributed processing, fault tolerance, and flexibility
    in handling large-scale data operations. While RDDs continue to be a fundamental
    concept, Spark’s DataFrame and Dataset APIs offer advancements in structured data
    processing and performance optimization.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨RDD，它是Apache Spark中的基础数据抽象，使分布式处理、容错性和处理大规模数据操作时的灵活性成为可能。虽然RDD仍然是一个基本概念，但Spark的DataFrame和Dataset
    API在结构化数据处理和性能优化方面提供了进步。
- en: RDDs
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD
- en: Apache Spark’s RDD stands as a foundational abstraction that underpins the distributed
    computing capabilities within the Spark framework. RDDs serve as the core data
    structure in Spark, enabling fault-tolerant and parallel operations on large-scale
    distributed datasets and they are immutable. This means that they cannot be changed
    over time. For any operations, a new RDD has to be generated from the existing
    RDD. When a new RDD originates from the original RDD, the new RDD has a pointer
    to the RDD it is generated from. This is the way Spark documents the lineage for
    all the transformations taking place on an RDD. This lineage enables **lazy evaluation**
    in Spark, which generates DAGs for different operations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark的RDD作为基础抽象，支撑了Spark框架内的分布式计算能力。RDD是Spark中的核心数据结构，它使对大规模分布式数据集进行容错性和并行操作成为可能，并且它们是不可变的。这意味着它们不能随时间改变。对于任何操作，都必须从现有的RDD生成一个新的RDD。当一个新RDD从原始RDD起源时，新RDD有一个指向其起源的RDD的指针。这是Spark记录RDD上所有变换谱系的方式。这种谱系使得Spark中的**延迟评估**成为可能，它为不同的操作生成DAGs。
- en: This immutability and lineage gives Spark the ability to reproduce any DataFrame
    in case of failure and it makes fault-tolerant by design. Since RDD is the lowest
    level of abstraction in Spark, all other datasets built on top of RDDs share these
    properties. The high-level DataFrame API is built on top of the low-level RDD
    API as well, so DataFrames also share the same properties.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不可变性和谱系赋予了Spark在失败情况下重新生成任何DataFrame的能力，并使其设计上具有容错性。由于RDD是Spark中抽象层次最低的，因此所有建立在RDD之上的其他数据集都共享这些属性。高级DataFrame
    API也是建立在低级RDD API之上的，因此DataFrame也共享相同的属性。
- en: RDDs are also partitioned by Spark and each partition is distributed to multiple
    nodes in the cluster.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: RDD也被Spark分区，并且每个分区被分配到集群中的多个节点。
- en: 'Here are some of the key characteristics of Spark RDDs:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是Spark RDD的一些关键特性：
- en: '**Immutable nature**: RDDs are immutable, ensuring that once created, they
    cannot be altered, allowing for a lineage of transformations.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可变性质**：RDD是不可变的，确保一旦创建，就不能被修改，从而允许有变换的谱系。'
- en: '**Resilience through lineage**: RDDs store lineage information, enabling reconstruction
    of lost partitions in case of failures. Spark is designed to be fault-tolerant.
    Therefore, if an executor on a worker node fails while calculating an RDD, that
    RDD can be recomputed by another executor using the lineage that Spark has created.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过谱系实现弹性**：RDD存储谱系信息，在发生故障时能够重建丢失的分区。Spark被设计成具有容错性。因此，如果一个工作节点上的执行器在计算RDD时失败，可以使用Spark创建的谱系由另一个执行器重新计算该RDD。'
- en: '**Partitioned data**: RDDs divide data into partitions, distributed across
    multiple nodes in a cluster for parallel processing.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区数据**：RDD将数据划分为分区，这些分区分布在集群中的多个节点上以实现并行处理。'
- en: '**Parallel execution**: Spark executes operations on RDDs in parallel across
    distributed partitions, enhancing performance.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行执行**：Spark在分布式分区上并行执行RDD上的操作，从而提高性能。'
- en: Let’s discuss some more characteristics in detail.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论一些其他特性。
- en: Lazy computation
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 懒计算
- en: RDDs support lazy evaluation, deferring execution of transformations until an
    action is invoked. The way Spark achieves its efficiency in processing and fault
    tolerance is through lazy evaluation. Code execution in Spark is delayed. Unless
    an action is called an operation, Spark does not start code execution. This helps
    Spark achieve optimization as well. For all the transformations and actions, Spark
    keeps track of the steps in the code that need to be executed by creating a DAG
    for these operations. Because Spark creates the query plan before execution, it
    can make smart decisions about the hierarchy of execution as well. To achieve
    this, one of the features Spark uses is called **predicate pushdown**.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: RDDs 支持惰性评估，将转换的执行延迟到动作被调用。Spark 在处理和容错方面的效率是通过惰性评估实现的。Spark 中的代码执行是延迟的。除非调用一个动作操作，否则
    Spark 不会开始代码执行。这有助于 Spark 实现优化。对于所有转换和动作，Spark 通过为这些操作创建一个 DAG 来跟踪代码中需要执行的步骤。因为
    Spark 在执行之前创建查询计划，所以它可以就执行层次结构做出明智的决策。为了实现这一点，Spark 使用的一个功能称为**谓词下推**。
- en: Predicate pushdown means that Spark can prioritize the operations to make them
    the most efficient. One example can be a filter operation. A filter operation
    would generally reduce the amount of data that the subsequent operations have
    to work with if the filter operation can be applied before other transformations.
    This is exactly how Spark operates. It will execute filters as early in the process
    as possible, thus making the next operations more performant.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 谓词下推意味着 Spark 可以优先执行操作，使其最有效。一个例子可以是一个过滤操作。如果过滤操作可以在其他转换之前应用，那么过滤操作通常会减少后续操作需要处理的数据量。这正是
    Spark 的操作方式。它会在流程中尽可能早地执行过滤操作，从而使后续操作更高效。
- en: This also implies that Spark jobs would fail only at execution time. Since Spark
    uses lazy evaluation, until an action is called, the code is not executed and
    certain errors can be missed. To catch these errors, Spark code would need to
    have an action for execution and hence error handling.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这也意味着 Spark 作业只有在执行时才会失败。由于 Spark 使用惰性评估，直到调用动作之前，代码不会执行，某些错误可能会被忽略。为了捕获这些错误，Spark
    代码需要有一个动作用于执行和错误处理。
- en: Transformations
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换
- en: Transformations create new RDDs by applying functions to existing RDDs (for
    example, `map`, `filter`, and `reduce`). Transformations are operations that do
    not result in any code execution. These statements result in Spark creating a
    DAG for execution. Once that DAG is created, Spark would need an action operation
    in the end to run the code. Due to this, when certain developers try to time the
    code from Spark, they see that certain operations’ runtime is very fast. The reason
    could be that the code is only comprised of transformations until that point.
    Since no action is present, the code doesn’t run. To accurately measure the runtime
    of each operation, actions have to be called to force Spark to execute those statements.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 转换通过将函数应用于现有的 RDDs（例如，`map`、`filter` 和 `reduce`）来创建新的 RDDs。转换是那些不会导致任何代码执行的操作。这些语句会导致
    Spark 为执行创建一个 DAG。一旦创建了 DAG，Spark 就需要在最后运行代码时需要一个动作操作。由于这个原因，当某些开发者尝试测量 Spark
    中的代码时间时，他们会看到某些操作的运行时间非常快。可能的原因是，代码直到那个点只包含转换。由于没有动作，代码不会运行。为了准确测量每个操作的运行时间，必须调用动作来强制
    Spark 执行这些语句。
- en: 'Here are some of the operations that can be classified as transformations:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可以被归类为转换的操作：
- en: '`orderBy()`'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`orderBy()`'
- en: '`groupBy()`'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groupBy()`'
- en: '`filter()`'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter()`'
- en: '`select()`'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`select()`'
- en: '`join()`'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`join()`'
- en: When these commands are executed, they are evaluated lazily. This means all
    these operations on DataFrames result in a new DataFrame, but they are not executed
    until an action is followed by them. This would return a DataFrame or RDD when
    it is triggered by an action.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些命令执行时，它们是惰性评估的。这意味着所有这些对 DataFrames 的操作都会产生一个新的 DataFrame，但它们不会执行，直到一个动作跟随它们。当触发动作时，这将返回一个
    DataFrame 或 RDD。
- en: Actions and computation execution
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作和计算执行
- en: Actions (for example, `collect`, `count`, and `saveAsTextFile`) prompt the execution
    of transformations on RDDs. Execution is triggered by actions only, not by transformations.
    When an action is called, this is when Spark starts execution on the DAG it created
    during the analysis phase of code. With the DAG created, Spark creates multiple
    query plans based on its internal optimizations. Then, it executes the plan that
    is the most efficient and cost-effective. We will discuss query plans later in
    this book.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 行动（例如，`collect`、`count` 和 `saveAsTextFile`）会触发 RDD 上的变换执行。执行仅由行动触发，而不是由变换触发。当调用行动时，这是
    Spark 开始在代码分析阶段创建的 DAG 上执行的时候。有了创建的 DAG，Spark 会根据其内部优化创建多个查询计划。然后，它执行最有效和成本效益最高的计划。我们将在本书的后面讨论查询计划。
- en: 'Here are some of the operations that can be classified as actions:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些可以归类为行动的操作：
- en: '`show()`'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`show()`'
- en: '`take()`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`take()`'
- en: '`count()`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count()`'
- en: '`collect()`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collect()`'
- en: '`save()`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save()`'
- en: '`foreach()`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`foreach()`'
- en: '`first()`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`first()`'
- en: All of these operations would result in Spark triggering code execution and
    thus operations are run.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些操作都会导致 Spark 触发代码执行，因此操作会运行。
- en: 'Let’s take a look at the following code to understand these concepts better:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下代码，以更好地理解这些概念：
- en: '[PRE0]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, until line 2, nothing would be executed. On line 3, an
    action is triggered and thus it triggers the whole code execution. Therefore,
    if you give the wrong data path in line 1 or the wrong column names in line 2,
    Spark will not detect this until it runs line 3\. This is a different paradigm
    than most other programming paradigms. This is what we call lazy evaluation in
    Spark.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，直到第 2 行，什么都不会执行。在第 3 行，触发了一个行动，因此触发了整个代码的执行。因此，如果你在第 1 行提供了错误的数据路径或在第
    2 行提供了错误的列名，Spark 不会在执行到第 3 行之前检测到这一点。这与大多数其他编程范式不同。这就是我们所说的 Spark 中的懒加载。
- en: Actions bring about computation and collect results to be sent to the driver
    program.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 行动会导致计算并收集结果以发送给驱动程序。
- en: Now that we’ve covered the basics of transformations and actions in Spark, let’s
    move on to understanding the two types of transformations it offers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了 Spark 中变换和行动的基础知识，让我们继续了解它提供的两种变换类型。
- en: Types of transformations
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变换类型
- en: Apache Spark’s transformations are broadly categorized into narrow and wide
    transformations, each serving distinct purposes in the context of distributed
    data processing.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 的变换大致分为窄变换和宽变换，每种变换在分布式数据处理环境中都服务于不同的目的。
- en: Narrow transformations
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 窄变换
- en: Narrow transformations, also known as local transformations, operate on individual
    partitions of data without shuffling or redistributing data across partitions.
    These transformations enable Spark to process data within a single partition independently.
    In narrow transformations, Spark will work with a single input partition and a
    single output partition. This means that these types of transformations would
    result in an operation that can be performed on a single partition. The data doesn’t
    have to be taken from multiple partitions or written back to multiple partitions.
    This results in operations that don’t require shuffle.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 窄变换，也称为本地变换，在数据单个分区上操作，不涉及在分区之间洗牌或重新分配数据。这些变换使 Spark 能够独立地在单个分区内处理数据。在窄变换中，Spark
    将与单个输入分区和单个输出分区一起工作。这意味着这些类型的变换会导致可以在单个分区上执行的操作。数据不必从多个分区中取出或写回到多个分区。这导致不需要洗牌的操作。
- en: 'Here are some of their characteristics:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是它们的一些特征：
- en: '**Partition-level operation**: Narrow transformations process data at the partition
    level, performing computations within each partition'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区级操作**：窄变换在分区级别处理数据，在每个分区内执行计算。'
- en: '**Independence and local processing**: They do not require data movement or
    communication across partitions, allowing parallel execution within partitions'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立性和本地处理**：它们不需要数据在分区之间移动或通信，允许在分区内并行执行。'
- en: '`map`, `filter`, and `flatMap` are typical examples of narrow transformations'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map`、`filter` 和 `flatMap` 是窄变换的典型例子。'
- en: 'Now, let’s look at their significance:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看它们的重要性：
- en: '**Efficiency and speed**: Narrow transformations are efficient as they involve
    local processing within partitions, reducing communication overhead'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率和速度**：窄变换是高效的，因为它们涉及分区内的本地处理，减少了通信开销。'
- en: '**Parallelism**: They facilitate maximum parallelism by operating on partitions
    independently, optimizing performance'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行性**：它们通过独立地对分区进行操作，实现最大并行性，从而优化性能'
- en: Wide transformations
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 宽转换
- en: Wide transformations, also termed global or shuffle-dependent transformations,
    involve operations that require data shuffling and redistribution across partitions.
    These transformations involve dependencies between partitions, necessitating data
    exchange. With wide transformations, Spark will use the data present on multiple
    partitions and it could also write back the results to multiple partitions. These
    transformations would force a shuffle operation, so they are also referred to
    as shuffle transformations.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 宽转换，也称为全局或洗牌相关转换，涉及需要跨分区进行数据洗牌和重新分配的操作。这些转换涉及分区之间的依赖关系，需要数据交换。使用宽转换时，Spark将使用多个分区上的数据，并且也可能将结果写回到多个分区。这些转换将强制进行洗牌操作，因此它们也被称为洗牌转换。
- en: Wide transformations are complex operations. They would need to write the results
    out in between operations if needed and they also have to aggregate data across
    different machines in certain cases.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 宽转换是复杂的操作。如果需要，它们需要在操作之间写入结果，并且在某些情况下还必须跨不同机器聚合数据。
- en: 'Here are some of their characteristics:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些它们的特征：
- en: '**Data shuffling**: Wide transformations reorganize data across partitions
    by reshuffling or aggregating data from multiple partitions'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据洗牌**：宽转换通过重新排列或聚合来自多个分区的数据来跨分区重新组织数据'
- en: '**Dependency on multiple partitions**: They depend on data from various partitions,
    leading to the exchange and reorganization of data across the cluster'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对多个分区的依赖性**：它们依赖于来自各个分区的数据，导致在集群中跨分区进行数据交换和重组'
- en: '`groupBy`, `join`, and `sortByKey` are typical examples of wide transformations'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groupBy`、`join`和`sortByKey`是宽转换的典型例子'
- en: 'Now, let’s look at their significance:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看它们的显著性：
- en: '**Network and disk overhead**: Wide transformations introduce network and disk
    overhead due to data shuffling, impacting performance'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络和磁盘开销**：宽转换由于数据洗牌而引入网络和磁盘开销，影响性能'
- en: '**Stage boundary creation**: They define stage boundaries within a Spark job,
    resulting in distinct stages during job execution'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阶段边界创建**：它们在Spark作业中定义阶段边界，导致在作业执行期间出现不同的阶段'
- en: 'The following are the differences between narrow and wide transformations:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是比较窄转换和宽转换之间的差异：
- en: '**Data movement**: Narrow transformations process data within partitions locally,
    minimizing data movement, while wide transformations involve data shuffling and
    movement across partitions'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据移动**：窄转换在分区内部本地处理数据，最小化数据移动，而宽转换涉及数据洗牌和跨分区的数据移动'
- en: '**Performance impact**: Narrow transformations typically offer higher performance
    due to reduced data movement, whereas wide transformations involve additional
    overhead due to data shuffling'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能影响**：窄转换通常由于数据移动减少而提供更高的性能，而宽转换由于数据洗牌而涉及额外的开销'
- en: '**Parallelism scope**: Narrow transformations enable maximum parallelism within
    partitions, while wide transformations might limit parallelism due to dependency
    on multiple partitions'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行性范围**：窄转换在分区内实现最大并行性，而宽转换可能会由于对多个分区的依赖而限制并行性'
- en: In Apache Spark, understanding the distinction between narrow and wide transformations
    is crucial. Narrow transformations excel in local processing within partitions,
    optimizing performance, while wide transformations, although necessary for certain
    operations, introduce overhead due to data shuffling and global reorganization
    across partitions.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在Apache Spark中，理解窄转换和宽转换之间的区别至关重要。窄转换在分区内的本地处理中表现出色，优化性能，而宽转换，尽管对于某些操作是必要的，但由于数据洗牌和跨分区的全局重组而引入了开销。
- en: 'Let’s look at the significance of Spark RDDs:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Spark RDDs的重要性：
- en: '**Distributed data processing**: RDDs enable distributed processing of large-scale
    data across a cluster of machines, promoting parallelism and scalability'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式数据处理**：RDDs允许在机器集群上分布式处理大规模数据，促进并行性和可伸缩性'
- en: '**Fault tolerance and reliability**: Their immutability and lineage-based recovery
    ensure fault tolerance and reliability in distributed environments'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性和可靠性**：它们的不可变性和基于血缘的恢复确保了在分布式环境中的容错性和可靠性'
- en: '**Flexibility in operations**: RDDs support a wide array of transformations
    and actions, allowing diverse data manipulations and processing operations'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作的灵活性**：RDDs 支持广泛的转换和操作，允许多样化的数据处理和操作。'
- en: Evolution and alternatives
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进化与替代
- en: While RDDs remain fundamental, Spark’s DataFrame and Dataset APIs offer optimized,
    higher-level abstractions suitable for structured data processing and optimization.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 RDDs 仍然是基础性的，但 Spark 的 DataFrame 和 Dataset API 提供了优化、高级别的抽象，适合结构化数据处理和优化。
- en: Spark RDDs serve as the bedrock of distributed data processing within the Apache
    Spark framework, providing immutability, fault tolerance, and the foundational
    structure for performing parallel operations on distributed datasets. Although
    RDDs are fundamental, Spark’s DataFrame and Dataset APIs offer advancements in
    performance and structured data processing, catering to various use cases and
    preferences within the Spark ecosystem.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Spark RDDs 是 Apache Spark 框架内分布式数据处理的基础，提供不可变性、容错性和在分布式数据集上执行并行操作的基础结构。尽管 RDDs
    是基础性的，但 Spark 的 DataFrame 和 Dataset API 提供了性能和结构化数据处理方面的进步，满足 Spark 生态系统中的各种用例和偏好。
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about Spark’s architecture and its inner workings.
    This exploration of Spark’s distributed computing landscape covered different
    Spark components, such as the Spark driver and `SparkSession`. We also talked
    about the different types of cluster managers available in Spark. Then, we touched
    on different types of partitioning regarding Spark and its deployment modes.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了 Spark 的架构及其内部工作原理。这次对 Spark 分布式计算景观的探索涵盖了不同的 Spark 组件，如 Spark 驱动程序和
    `SparkSession`。我们还讨论了 Spark 中可用的不同类型的集群管理器。然后，我们简要介绍了 Spark 及其部署模式的不同分区类型。
- en: Next, we discussed Spark executors, jobs, stages, and tasks and highlighted
    the differences between them before learning about RDDs and their transformation
    types, learning more about narrow and wide transformations.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论了 Spark 执行器、作业、阶段和任务，在学习 RDDs 及其转换类型之前，强调了它们之间的区别，并更多地了解了窄转换和宽转换。
- en: These concepts form the foundation for harnessing Spark’s immense capabilities
    in distributed data processing and analytics.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念构成了利用 Spark 在分布式数据处理和分析中巨大能力的基础。
- en: In the next chapter, we will discuss Spark DataFrames and their corresponding
    operations.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论 Spark DataFrame 及其相应的操作。
- en: Sample questions
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样题
- en: '**Question 1:**'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1：**'
- en: What’s true about Spark’s execution hierarchy?
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Spark 的执行层次结构，以下哪项是正确的？
- en: In Spark’s execution hierarchy, a job may reach multiple stage boundaries.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Spark 的执行层次结构中，一个作业可能会达到多个阶段边界。
- en: In Spark’s execution hierarchy, manifests are one layer above jobs.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Spark 的执行层次结构中，作业描述文件位于作业之上的一层。
- en: In Spark’s execution hierarchy, a stage comprises multiple jobs.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Spark 的执行层次结构中，一个阶段包含多个作业。
- en: In Spark’s execution hierarchy, executors are the smallest unit.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Spark 的执行层次结构中，执行器是最小的单元。
- en: In Spark’s execution hierarchy, tasks are one layer above slots.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Spark 的执行层次结构中，任务位于槽位之上的一层。
- en: '**Question 2:**'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2：**'
- en: What do executors do?
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器的作用是什么？
- en: Executors host the Spark driver on a worker-node basis.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器在每个工作节点上托管 Spark 驱动程序。
- en: Executors are responsible for carrying out work that they get assigned by the
    driver.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器负责执行由驱动程序分配给它们的工作。
- en: After the start of the Spark application, executors are launched on a per-task
    basis.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 应用程序启动后，每个任务都会启动执行器。
- en: Executors are located in slots inside worker nodes.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器位于工作节点内的槽位中。
- en: The executors’ storage is ephemeral and as such it defers the task of caching
    data directly to the worker node thread.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器的存储是短暂的，因此它将直接缓存数据的任务推迟到工作节点线程。
- en: Answers
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 答案
- en: A
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: B
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: 'Part 3: Spark Operations'
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：Spark 操作
- en: In this part, we will cover Spark DataFrames and their operations, emphasizing
    their role in structured data processing and analytics. This will include DataFrame
    creation, manipulation, and various operations such as filtering, aggregations,
    joins, and groupings, demonstrated through illustrative examples. Then, we will
    discuss advanced operations and optimization techniques, including broadcast variables,
    accumulators, and custom partitioning. This part also talks about performance
    optimization strategies, highlighting the significance of adaptive query execution
    and offering practical tips for enhancing Spark job performance. Furthermore,
    we will explore SQL queries in Spark, focusing on its SQL-like querying capabilities
    and interoperability with the DataFrame API. Examples will illustrate complex
    data manipulations and analytics through SQL queries in Spark.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分，我们将涵盖Spark DataFrame及其操作，强调它们在结构化数据处理和分析中的作用。这包括DataFrame的创建、操作以及各种操作，如过滤、聚合、连接和分组，通过示例进行演示。然后，我们将讨论高级操作和优化技术，包括广播变量、累加器和自定义分区。本部分还将讨论性能优化策略，强调自适应查询执行的重要性，并提供提高Spark作业性能的实用技巧。此外，我们将探索Spark中的SQL查询，重点关注其类似SQL的查询能力和与DataFrame
    API的互操作性。示例将通过Spark中的SQL查询展示复杂的数据操作和分析。
- en: 'This part has the following chapters:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 4*](B19176_04.xhtml#_idTextAnchor071), *Spark DataFrames and their
    Operations*'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B19176_04.xhtml#_idTextAnchor071), *Spark DataFrame及其操作*'
- en: '[*Chapter 5*](B19176_05.xhtml#_idTextAnchor115), *Advanced Operations and Optimizations
    in Spark*'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B19176_05.xhtml#_idTextAnchor115), *Spark中的高级操作和优化*'
- en: '[*Chapter 6*](B19176_06.xhtml#_idTextAnchor164)*,* *SQL Queries in Spark*'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B19176_06.xhtml#_idTextAnchor164), *Spark中的SQL查询*'
