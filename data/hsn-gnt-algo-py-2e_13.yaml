- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Reinforcement Learning with Genetic Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用遗传算法进行强化学习
- en: In this chapter, we will demonstrate how genetic algorithms can be applied to
    **reinforcement learning** – a fast-developing branch of machine learning that
    is capable of tackling complex tasks. We will do this by solving two benchmark
    environments from the *Gymnasium* (formerly *OpenAI Gym*) toolkit. We will start
    by providing an overview of reinforcement learning, followed by a brief introduction
    to *Gymnasium*, a toolkit that can be used to compare and develop reinforcement
    learning algorithms, as well as a description of its Python-based interface. Then,
    we will explore two Gymnasium environments, *MountainCar* and *CartPole*, and
    develop genetic algorithm-based programs to solve the challenges they present.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将展示如何将遗传算法应用于**强化学习**——这一快速发展的机器学习分支，能够解决复杂的任务。我们将通过解决来自*Gymnasium*（前身为*OpenAI
    Gym*）工具包的两个基准环境来实现这一目标。我们将首先概述强化学习，随后简要介绍*Gymnasium*，这是一个可用于比较和开发强化学习算法的工具包，并描述其基于Python的接口。接下来，我们将探索两个Gymnasium环境，*MountainCar*和*CartPole*，并开发基于遗传算法的程序来解决它们所面临的挑战。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding the basic concepts of reinforcement learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解强化学习的基本概念
- en: Becoming familiar with the *Gymnasium* project and its shared interface
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉*Gymnasium*项目及其共享接口
- en: Using genetic algorithms to solve the *Gymnasium* *MountainCar* environment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用遗传算法解决*Gymnasium*的*MountainCar*环境
- en: Using genetic algorithms, in combination with a neural network, to solve the
    *Gymnasium* *CartPole* environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用遗传算法结合神经网络解决*Gymnasium*的*CartPole*环境
- en: We will start this chapter by outlining the basic concepts of reinforcement
    learning. If you are a seasoned data scientist, feel free to skip this introductory
    section.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过概述强化学习的基本概念来开始本章。如果你是经验丰富的数据科学家，可以跳过这一介绍部分。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use Python 3 with the following supporting libraries:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Python 3及以下支持库：
- en: '**deap**'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**deap**'
- en: '**numpy**'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**numpy**'
- en: '**scikit-learn**'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scikit-learn**'
- en: '**gymnasium** – *introduced in* *this chapter*'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**gymnasium** – *在本章中介绍*'
- en: '**pygame** – *introduced in* *this chapter*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pygame** – *在本章中介绍*'
- en: Important Note
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you use the **requirements.txt** file we provide (See [*Chapter 3*](B20851_03.xhtml#_idTextAnchor091)),
    these libraries are already included in your environment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用我们提供的**requirements.txt**文件（参见[*第3章*](B20851_03.xhtml#_idTextAnchor091)），这些库已经包含在你的环境中了。
- en: The *Gymnasium* environments that will be used in this chapter are *MountainCar-v0*
    ([https://gymnasium.farama.org/environments/classic_control/mountain_car/](https://gymnasium.farama.org/environments/classic_control/mountain_car/))
    and *CartPole-v1* ([https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用的*Gymnasium*环境是*MountainCar-v0*（[https://gymnasium.farama.org/environments/classic_control/mountain_car/](https://gymnasium.farama.org/environments/classic_control/mountain_car/)）和*CartPole-v1*（[https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)）。
- en: The programs that will be used in this chapter can be found in this book’s GitHub
    repository at [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_10](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_10).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的程序可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_10](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_10)。
- en: 'Check out the following video to see the code in action: [https://packt.link/OEBOd](https://packt.link/OEBOd).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，看看代码如何运行：[https://packt.link/OEBOd](https://packt.link/OEBOd)。
- en: Reinforcement learning
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: In the previous chapters, we covered several topics related to machine learning
    and focused on **supervised learning** tasks. While supervised learning is immensely
    important and has a lot of real-life applications, reinforcement learning currently
    seems to be the most exciting and promising branch of machine learning. The reasons
    for this excitement include the complex, everyday-life-like tasks that reinforcement
    learning has the potential to handle. In March 2016, *AlphaGo*, a reinforcement
    learning-based system specializing in the highly complex game of *Go*, was able
    to defeat the person considered to be the greatest Go player of the past decade
    in a competition that was widely covered by the media.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了与机器学习相关的多个主题，重点介绍了**监督学习**任务。尽管监督学习非常重要，并且有许多现实生活中的应用，但目前看来，强化学习是机器学习中最令人兴奋和最有前景的分支。人们对这一领域的兴奋之情源自于强化学习能够处理的复杂且类似于日常生活的任务。2016年3月，基于强化学习的*AlphaGo*系统成功战胜了被认为是过去十年最强围棋选手的选手，并且这一比赛得到了广泛的媒体报道。
- en: While supervised learning requires **labeled data** for training – in other
    words, pairs of inputs and matching outputs – reinforcement learning does not
    present immediate right/wrong feedback; instead, it provides an environment where
    a longer-term, cumulative **reward** is sought after. This means that, sometimes,
    an algorithm will need to take a momentary step backward to eventually reach a
    longer-term goal, as we will demonstrate in the first example of this chapter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然监督学习需要**标注数据**进行训练——换句话说，需要输入与匹配输出的对——但强化学习并不会立即给出对错反馈；相反，它提供了一个寻求长期、累积**奖励**的环境。这意味着，有时算法需要暂时后退一步，才能最终实现长期目标，正如我们将在本章的第一个例子中展示的那样。
- en: 'The two main components of reinforcement learning task are the **environment**
    and the **agent**, as illustrated in the following diagram:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习任务的两个主要组件是**环境**和**智能体**，如下面的图示所示：
- en: '![Figure 10.1: Reinforcement learning represented as an interaction between
    the agent and the environment](img/B20851_10_1.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1: 强化学习表示为智能体与环境之间的互动](img/B20851_10_1.jpg)'
- en: 'Figure 10.1: Reinforcement learning represented as an interaction between the
    agent and the environment'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.1: 强化学习表示为智能体与环境之间的互动'
- en: The *agent* represents an algorithm that interacts with the *environment* and
    attempts to solve a given problem by maximizing the cumulative reward.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*智能体*代表了一种与*环境*互动的算法，它通过最大化累积奖励来尝试解决给定的问题。'
- en: The exchange that takes place between the agent and the environment can be expressed
    as a series of **steps**. In each step, the environment presents the agent with
    a certain **state** (*s*), also called an observation. The agent, in turn, performs
    an **action** (*a*). The environment responds with a new state (*s’*), as well
    as an intermediate reward value (*R*). This exchange repeats until a certain stopping
    condition is met. The agent’s goal is to maximize the sum of the reward values
    that are collected along the way.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体与环境之间发生的交换可以表示为一系列的**步骤**。在每一步中，环境会向智能体呈现一个特定的**状态**（*s*），也称为观察。智能体则执行一个**动作**（*a*）。环境会回应一个新的状态（*s’*），以及一个中间奖励值（*R*）。这一交换会一直重复，直到满足某个停止条件。智能体的目标是最大化沿途收集的奖励值的总和。
- en: Despite the simplicity of this formulation, it can be used to describe extremely
    complex tasks and situations, which makes reinforcement learning applicable to
    a wide range of applications, such as game theory, healthcare, control systems,
    supply-chain automation, and operations research.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这一表述非常简单，但它可以用来描述极其复杂的任务和情境，这也使得强化学习适用于广泛的应用场景，如博弈论、医疗保健、控制系统、供应链自动化和运筹学。
- en: The versatility of genetic algorithms will be demonstrated once more in this
    chapter, since we will harness them to assist with reinforcement learning tasks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将再次展示遗传算法的多功能性，因为我们将利用它们来辅助强化学习任务。
- en: Genetic algorithms and reinforcement learning
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 遗传算法与强化学习
- en: Various dedicated algorithms have been developed to carry out reinforcement
    learning tasks – *Q-Learning*, *SARSA*, and *DQN*, to name a few. However, since
    reinforcement learning tasks involve maximizing a long-term reward, we can think
    of them as optimization problems. As we have seen throughout this book, genetic
    algorithms can be used to solve optimization problems of various types. Therefore,
    genetic algorithms can be utilized for reinforcement learning as well, and in
    several different ways – two of them will be demonstrated in this chapter. In
    the first case, our genetic algorithm-based solution will directly provide the
    agent’s optimal series of actions. In the second case, it will supply the optimal
    parameters for the neural controller that provides these actions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行强化学习任务，已经开发了多种专用算法——如 *Q-Learning*、*SARSA* 和 *DQN* 等。然而，由于强化学习任务涉及最大化长期奖励，我们可以将它们视为优化问题。正如本书中所展示的，遗传算法可以用于解决各种类型的优化问题。因此，遗传算法也可以用于强化学习，并且有几种不同的方式——本章将演示其中的两种。在第一种情况下，我们基于遗传算法的解决方案将直接提供智能体的最佳动作序列。在第二种情况下，它将为提供这些动作的神经控制器提供最佳参数。
- en: Before we start applying genetic algorithms to reinforcement learning tasks,
    let’s get acquainted with the toolkit that will be used to conduct these tasks
    – **Gymnasium**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始将遗传算法应用于强化学习任务之前，让我们先了解将用于执行这些任务的工具包——**Gymnasium**。
- en: Gymnasium
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gymnasium
- en: '*Gymnasium* ([https://gymnasium.farama.org/](https://gymnasium.farama.org/))
    – a fork and the official successor of *OpenAI Gym* – is an open source library
    that was written to allow access to a standardized set of reinforcement learning
    tasks. It provides a toolkit that can be used to compare and develop reinforcement
    learning algorithms.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*Gymnasium* ([https://gymnasium.farama.org/](https://gymnasium.farama.org/))
    —— 这是 *OpenAI Gym* 的一个分支和官方继任者 —— 是一个开源库，旨在提供对标准化强化学习任务集合的访问。它提供了一个工具包，用于比较和开发强化学习算法。'
- en: '*Gymnasium* consists of a collection of environments, all presenting a common
    interface called `env`. This interface decouples the various environments from
    the agents, which can be implemented in any way we like – the only requirement
    from the agent is that it can interact with the environment(s) via the `env` interface.
    This will be described in the next subsection.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*Gymnasium* 是由一系列环境组成的集合，这些环境都呈现一个共同的接口，称为 `env`。这个接口将各种环境与智能体解耦，智能体可以以任何我们喜欢的方式实现——智能体唯一的要求是能够通过
    `env` 接口与环境进行交互。这个内容将在下一小节中进行描述。'
- en: 'The basic package, `gymnasium`, provides access to several environments and
    can be installed as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基本包 `gymnasium` 提供对多个环境的访问，可以通过以下方式进行安装：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To allow us to render and animate our test environments, the *PyGame* library
    needs to be installed as well. This can be done using the following command:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们能够渲染和动画化测试环境，还需要安装 *PyGame* 库。可以使用以下命令进行安装：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Several other packages are available, such as “Atari,” “Box2D,” and “MuJoCo,”
    that provide access to numerous and diverse additional environments. Some of these
    packages have system dependencies and may only be available for certain operating
    systems. More information is available at [https://github.com/Farama-Foundation/Gymnasium#installation](https://github.com/Farama-Foundation/Gymnasium#installation).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他的包可用，例如“Atari”、“Box2D”和“MuJoCo”，它们提供对多个多样化环境的访问。这些包有些具有系统依赖性，可能只适用于某些操作系统。更多信息请访问
    [https://github.com/Farama-Foundation/Gymnasium#installation](https://github.com/Farama-Foundation/Gymnasium#installation)。
- en: The next subsection describes interaction with the `env` interface.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下一小节将描述如何与 `env` 接口进行交互。
- en: The env interface
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: env 接口
- en: 'To create an environment, we need to use the `make()` method and the name of
    the desired environment, as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个环境，我们需要使用 `make()` 方法并提供所需环境的名称，如下所示：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: observation, info = env.observation object, describing the initial state of
    the environment, and a dictionary, info, that may contain auxiliary information
    complementing observation. The content of observation is environment-dependent.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: observation, info = env.observation 对象，描述环境的初始状态，以及一个字典 info，可能包含补充 observation
    的辅助信息。observation 的内容依赖于环境。
- en: 'Conforming with the reinforcement learning cycle that we described in the previous
    section, the ongoing interaction with the environment consists of sending it an
    *action* and, in return, receiving an *intermediate reward* and a new *state*.
    This is implemented by the `step()` method, as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在上一节中描述的强化学习周期一致，与环境的持续互动包括发送一个 *动作*，然后接收一个 *中间奖励* 和一个新的 *状态*。这一过程通过 `step()`
    方法实现，如下所示：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: env.render_mode, which can be set when the environment is created. A value of
    "human", for example, will result in the environment being continuously rendered
    in the current display or terminal, while the default value of None will result
    in no rendering.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`env.render_mode` 可以在创建环境时进行设置。例如，设置为 "human" 会使环境在当前显示器或终端中持续渲染，而默认值 None
    则不会进行渲染。'
- en: 'Finally, an environment can be closed to invoke any necessary cleanup, as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以关闭环境以调用任何必要的清理操作，如下所示：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If this method isn’t called, the environment will automatically close itself
    the next time Python runs its *garbage collection* process (the process of identifying
    and freeing memory that is no longer in use by the program), or when the program
    exits.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有调用此方法，环境将在下一次 Python 执行 *垃圾回收* 进程（即识别并释放程序不再使用的内存）时自动关闭，或者当程序退出时关闭。
- en: Note
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Detailed information about the env interface is available at [https://gymnasium.farama.org/api/env/](https://gymnasium.farama.org/api/env/).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 env 接口的详细信息，请参见 [https://gymnasium.farama.org/api/env/](https://gymnasium.farama.org/api/env/)。
- en: The complete cycle of interaction with the environment will be demonstrated
    in the next section, where we’ll encounter our first gymnasium challenge – the
    *MountainCar* environment.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与环境的完整交互周期将在下一节中演示，在那里我们将遇到第一个 Gymnasium 挑战——*MountainCar* 环境。
- en: Solving the MountainCar environment
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 解决 MountainCar 环境
- en: 'The `MountainCar-v0` environment simulates a car on a one-dimensional track,
    situated between two hills. The simulation starts with the car placed between
    the hills, as shown in the following rendered output:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`MountainCar-v0` 环境模拟了一辆位于两座山丘之间的单维轨道上的汽车。模拟开始时，汽车被放置在两座山丘之间，如下图所示：'
- en: '![Figure 10.2: The MountainCar simulation – the starting point](img/B20851_10_2.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2：MountainCar 模拟——起点](img/B20851_10_2.jpg)'
- en: 'Figure 10.2: The MountainCar simulation – the starting point'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2：MountainCar 模拟——起点
- en: 'The goal is to get the car to climb up the taller hill – the one on the right
    – and ultimately hit the flag:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是让汽车爬上更高的山丘——右侧的山丘——并最终触碰到旗帜：
- en: '![Figure 10.3: The MountainCar simulation – the car climbing the hill on the
    right](img/B20851_10_3.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3：MountainCar 模拟——汽车爬上右侧山丘](img/B20851_10_3.jpg)'
- en: 'Figure 10.3: The MountainCar simulation – the car climbing the hill on the
    right'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3：MountainCar 模拟——汽车爬上右侧山丘
- en: 'The simulation is set up with the car’s engine being too weak to directly climb
    the taller hill. The only way to reach the goal is to drive the car back and forth
    until enough momentum is built for climbing. Climbing the left hill can help to
    achieve this goal, as reaching the left peak will bounce the car back to the right,
    as shown in the following screenshot:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模拟设置的情景是汽车的引擎太弱，无法直接爬上更高的山丘。达到目标的唯一方法是让汽车前后行驶，直到积累足够的动能以供攀爬。爬上左侧山丘有助于实现这一目标，因为到达左侧山顶会使汽车反弹到右侧，以下截图展示了这一过程：
- en: '![Figure 10.4: The MountainCar simulation – the car bouncing off the hill on
    the left](img/B20851_10_4.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4：MountainCar 模拟——汽车从左侧山丘反弹](img/B20851_10_4.jpg)'
- en: 'Figure 10.4: The MountainCar simulation – the car bouncing off the hill on
    the left'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4：MountainCar 模拟——汽车从左侧山丘反弹
- en: This simulation is a great example where intermediate loss (moving left) can
    help to achieve the ultimate goal (going all the way to the right).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模拟是一个很好的例子，表明中间的损失（向左移动）可以帮助实现最终目标（完全向右移动）。
- en: 'The expected *action* value in this simulation is an integer with of one of
    the three following values:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模拟中的预期 *动作* 值是一个整数，取以下三个值之一：
- en: '0: Push left'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '0: 向左推动'
- en: '1: No push'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1: 不推动'
- en: '2: Push right'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2: 向右推动'
- en: 'The `observation` object contains two floats that describe the position and
    the velocity of the car, such as the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`observation` 对象包含两个浮动值，描述了汽车的位置和速度，如下所示：'
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Finally, the `reward` value is -1 for each time step, until the goal (located
    at position 0.5) is reached. The simulation will stop after 200 steps if the goal
    is not reached beforehand.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`reward`值在每个时间步为-1，直到达到目标（位于位置0.5）。如果在200步之前没有达到目标，模拟将会停止。
- en: The goal of this environment is to reach the flag placed on top of the right
    hill as quickly as possible, and therefore, the agent is penalised with a reward
    of -1 for each timestep used.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 该环境的目标是尽可能快速地到达位于右侧山丘顶部的旗帜，因此，智能体在每个时间步上都会被扣除-1的奖励。
- en: More information about the *MountainCar-v0* environment can be found at
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 关于*MountainCar-v0*环境的更多信息可以在这里找到：
- en: '[https://gymnasium.farama.org/environments/classic_control/mountain_car/](https://gymnasium.farama.org/environments/classic_control/mountain_car/).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://gymnasium.farama.org/environments/classic_control/mountain_car/](https://gymnasium.farama.org/environments/classic_control/mountain_car/)。'
- en: In our implementation, we will attempt to hit the flag using the least amount
    of steps, as we apply a sequence of preselected actions from a fixed starting
    position. To find a sequence of actions that will get the car to climb the tall
    hill and hit the flag, we will craft a genetic algorithm-based solution. As usual,
    we will start by defining what a candidate solution for this challenge will look
    like.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们将尝试使用最少的步数来撞击旗帜，因为我们会从固定的起始位置应用一系列预选的动作。为了找到一个能够让小车爬上高山并撞击旗帜的动作序列，我们将设计一个基于遗传算法的解决方案。像往常一样，我们将首先定义这个挑战的候选解应如何表现。
- en: Solution representation
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 解的表示
- en: 'Since *MountainCar* is controlled by a sequence of actions, each with a value
    of 0 (push left), 1 (no push), or 2 (push right), and there can be up to 200 actions
    in a single episode, one obvious way to represent a candidate solution is by using
    a list of length 200, containing values of 0, 1, or 2\. An example of this is
    as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*MountainCar*是通过一系列动作来控制的，每个动作的值为0（向左推动）、1（不推动）或2（向右推动），并且在单个回合中最多可以有200个动作，因此表示候选解的一种显而易见的方法是使用长度为200的列表，列表中的值为0、1或2。一个示例如下：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The values in the list will be used as actions to control the car and, hopefully,
    drive it to the flag. If the car made it to the flag in less than 200 steps, the
    last few items in the list will not be used.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的值将用作控制小车的动作，并且希望能够把它驱动到旗帜。如果小车在少于200步的时间内到达了旗帜，列表中的最后几项将不会被使用。
- en: Next, we need to determine how to evaluate a given solution of this form.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要确定如何评估这种形式的给定解。
- en: Evaluating the solution
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 评估解的质量
- en: 'While evaluating a given solution, or when comparing two solutions, it is apparent
    that the reward value alone may not provide us with sufficient information. With
    the way the reward is defined, its value will always be -200 if we don’t hit the
    flag. When we compare two candidate solutions that don’t hit the flag, we would
    still like to know which one got closer to it and consider it a better solution.
    Therefore, we will use the final position of the car, in addition to the reward
    value, to determine the score of the solution:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估给定解时，或者在比较两个解时，很明显，单独的奖励值可能无法提供足够的信息。根据当前奖励的定义，如果我们没有撞到旗帜，它的值将始终为-200。当我们比较两个没有撞到旗帜的候选解时，我们仍然希望知道哪一个更接近旗帜，并将其视为更好的解。因此，除了奖励值外，我们还将使用小车的最终位置来确定解的得分：
- en: If the car did not hit the flag, the score will be the distance from the flag.
    Therefore, we will look for a solution that minimizes the score.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果小车没有撞到旗帜，得分将是与旗帜的距离。因此，我们将寻找一个能够最小化得分的解。
- en: If the car hits the flag, the base score will be zero, and from that, we deduct
    an additional value based on how many unused steps were left, making the score
    negative. Since we are looking for the lowest score possible, this arrangement
    will encourage solutions to hit the flag using the smallest possible amount of
    actions.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果小车撞到旗帜，基础得分将为零，从此基础上根据剩余未使用的步骤数扣除一个额外的值，使得得分为负。由于我们寻求最低的得分，这种安排将鼓励解通过尽可能少的动作撞击旗帜。
- en: This score evaluation procedure is implemented by the `MountainCar` class, which
    is explored in the following subsection.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 该评分评估过程由`MountainCar`类实现，下面的子章节中将对其进行详细探讨。
- en: The Python problem representation
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Python问题表示
- en: To encapsulate the MountainCar challenge, we’ve created a Python class called
    `MountainCar`. This class is contained in the `mountain_car.py` file, which is
    located at [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/mountain_car.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/mountain_car.py).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了封装MountainCar挑战，我们创建了一个名为`MountainCar`的Python类。该类包含在`mountain_car.py`文件中，文件位于[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/mountain_car.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/mountain_car.py)。
- en: 'The class is initialized with a random seed and provides the following methods:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该类通过一个随机种子初始化，并提供以下方法：
- en: '**getScore(actions)**: Calculates the score of a given solution, represented
    by the list of actions. The score is calculated by initiating an episode of the
    **MountainCar** environment and running it with the provided actions, and this
    can be negative if we hit the target with fewer than 200 steps. The lower the
    score is, the better.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**getScore(actions)**：计算给定解决方案的得分，解决方案由动作列表表示。得分是通过启动一个**MountainCar**环境的回合并用提供的动作运行它来计算的，如果在少于200步的情况下击中目标，得分可能为负值。得分越低越好。'
- en: '**saveActions(actions)**: Saves a list of actions to a file using **pickle**
    (Python’s object serialization and deserialization module).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**saveActions(actions)**：使用**pickle**（Python的对象序列化和反序列化模块）将动作列表保存到文件。'
- en: '**replaySavedActions()**: Deserializes the last saved list of actions and replays
    it using the **replay** method.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**replaySavedActions()**：反序列化最后保存的动作列表，并使用**replay**方法重放它。'
- en: '**replay(actions)**: Renders the environment using the “human” **render_mode**
    and replays the list of actions given to it, visualizing a given solution.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**replay(actions)**：使用“human”**render_mode**渲染环境，并重放给定的动作列表，展示给定的解决方案。'
- en: The main method of the class can be used after a solution has been found, serialized,
    and saved using the `saveActions()` method. The main method will initialize the
    class and call `replaySavedActions()` to render and animate the last saved solution.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 类的主要方法可以在找到解决方案、序列化并使用`saveActions()`方法保存后使用。主方法将初始化类并调用`replaySavedActions()`以渲染和动画展示最后保存的解决方案。
- en: We typically use the main method to animate the best solution that’s found by
    the genetic algorithm-based program. This will be explored in the following subsection.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用主方法来展示由遗传算法程序找到的最佳解决方案的动画。接下来的小节将详细探讨这一点。
- en: Genetic algorithms solution
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传算法解决方案
- en: To tackle the *MountainCar* challenge using the genetic algorithms approach,
    we’ve created a Python program, `01_solve_mountain_car.py`, which is located at
    [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/01_solve_mountain_car.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/01_solve_mountain_car.py).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用遗传算法方法解决*MountainCar*挑战，我们创建了一个Python程序`01_solve_mountain_car.py`，该程序位于[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/01_solve_mountain_car.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/01_solve_mountain_car.py)。
- en: Since the solution representation we chose for this problem is a list containing
    the 0, 1, or 2 integer values, this program bears resemblance to the one we used
    to solve the knapsack 0-1 problem in [*Chapter 4*](B20851_04.xhtml#_idTextAnchor155),
    *Combinatorial Optimization*, where solutions were represented as lists with the
    values 0 and 1.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们为此问题选择的解决方案表示方法是包含0、1或2整数值的列表，因此这个程序与我们在[*第4章*](B20851_04.xhtml#_idTextAnchor155)《组合优化》中用来解决0-1背包问题的程序相似，在那里解决方案是以包含0和1的列表表示的。
- en: 'The following steps describe how to create the main parts of this program:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤描述了如何创建该程序的主要部分：
- en: 'We start by creating an instance of the **MountainCar** class, which will allow
    us to score the various solutions for the *MountainCar* challenge:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过创建**MountainCar**类的实例开始，这将允许我们为*MountainCar*挑战打分，评估各种解决方案：
- en: '[PRE7]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '2.  Since our goal is to minimize the score – in other words, hit the flag
    with the minimum step count, or get as close as possible to the flag – we define
    a single objective, minimizing the fitness strategy:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.  由于我们的目标是最小化得分——换句话说，使用最少的步数击中旗帜，或者尽可能接近旗帜——我们定义了一个单一目标，最小化适应度策略：
- en: '[PRE8]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '3.  Now, we need to create a toolbox operator that can produce one of the three
    allowed action values – 0, 1, or 2:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3.  现在，我们需要创建一个工具箱操作符，用来生成三个允许的动作值之一——0、1或2：
- en: '[PRE9]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '4.  This is followed by an operator that fills up an individual instance with
    these values:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 4.  接下来是一个操作符，它用这些值填充个体实例：
- en: '[PRE10]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '5.  Then, we instruct the genetic algorithm to use the `getScore()` method,
    which we described in the previous subsection, initiates an episode of the *MountainCar*
    environment and uses the given individual – a list of actions – as the inputs
    to the environment until the episode is done. Then, it evaluates the score – the
    lower the better – according to the final location of the car. If the car hit
    the flag, the score can even be negative, based on the number of unused steps
    left:'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5.  然后，我们指示遗传算法使用`getScore()`方法，该方法在前一小节中描述过，启动一个*MountainCar*环境的回合，并使用给定的个体——一组动作——作为环境的输入，直到回合结束。然后，根据汽车的最终位置评估分数——分数越低越好。如果汽车撞到旗帜，分数甚至可能是负数，具体取决于剩余未使用步骤的数量：
- en: '[PRE11]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 6.  As for the genetic operators, we start with the usual *tournament selection*,
    with a tournament size of 2\. Since our solution representation is a list of the
    0, 1, or 2 integer values, we can use the *two-point crossover* operator, just
    like we did when the solution was represented by a list of 0 and 1 values.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 6.  至于遗传操作符，我们从通常的*锦标赛选择*开始，锦标赛规模为2。由于我们的解表示是由0、1或2组成的整数值列表，我们可以像解表示为0和1值列表时那样，使用*二点交叉*操作符。
- en: 'For *mutation*, however, rather than the *FlipBit* operator, which is typically
    used for the binary case, we need to use the *UniformInt* operator, which is used
    for a range of integer values, and configure it for the range of 0-2:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于*变异*，与通常用于二进制情况的*FlipBit*操作符不同，我们需要使用*UniformInt*操作符，它适用于一系列整数值，并将其配置为0到2的范围：
- en: '[PRE12]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '7.  In addition, we continue to use the *elitist approach*, where the **hall
    of fame** (**HOF**) members – the current best individuals – are always passed
    untouched to the next generation:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 7.  此外，我们继续使用*精英方法*，即**名人堂**（**HOF**）成员——当前的最佳个体——总是会原封不动地传递到下一代：
- en: '[PRE13]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '8.  After the run, we print the best solution and save it so that we can later
    animate it, using the replay capability we built into the **MountainCar** class:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 8.  运行结束后，我们打印出最佳解并将其保存，以便稍后使用我们在**MountainCar**类中构建的重放功能进行动画演示：
- en: '[PRE14]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Running the algorithm for 80 generations and with a population size of 100,
    we get the following outcome:'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行该算法80代，种群规模为100时，我们得到以下结果：
- en: '[PRE15]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: From the preceding output, we can see that, after about 50 generations, the
    best solution(s) started hitting the flag, producing score values of zero or less.
    From hereon, the best solutions hit the flag in fewer steps, thereby producing
    increasingly negative score values.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以看到，在大约50代之后，最佳解开始撞击旗帜，产生零分或更低的分数值。从此以后，最佳解在更少的步骤中撞击旗帜，导致越来越低的分数值。
- en: 'As we already mentioned, the best solution was saved at the end of the run,
    and we can now replay it by running the `mountain_car` program. This replay illustrates
    how the actions of our solution drive the car back and forth between the two peaks,
    climbing higher each time, until the car is able to climb the lower hill on the
    left. Then, it bounces back, which means we have enough momentum to continue climbing
    the higher hill on the right, ultimately hitting the flag, as shown in the following
    screenshot:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，最佳解在运行结束时被保存，现在我们可以通过运行`mountain_car`程序来重放它。这个重放展示了我们的解如何驱动汽车在两个山峰之间来回摆动，每次都爬得更高，直到汽车能够爬上左侧的低山。然后，它会反弹回来，这意味着我们已经积累了足够的动能，可以继续爬上右侧的更高山峰，最终撞击旗帜，以下面的截图所示：
- en: '![Figure 10.5: The MountainCar simulation – the car reaching the goal](img/B20851_10_5.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.5：MountainCar 仿真——汽车到达目标](img/B20851_10_5.jpg)'
- en: 'Figure 10.5: The MountainCar simulation – the car reaching the goal'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5：MountainCar 仿真——汽车到达目标
- en: While solving it was a lot of fun, the way this environment is set up did not
    require us to dynamically interact with it. We were able to climb the hill using
    a sequence of actions that our algorithm put together, based on the initial location
    of the car. In contrast, the next environment we are about to tackle – named *CartPole*
    – requires us to dynamically calculate our action at any time step, based upon
    the latest observation produced. Read on to find out how we can make this work.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管解决它非常有趣，但这个环境的设置并不要求我们与其进行动态交互。我们能够通过一系列由我们算法根据小车的初始位置组成的动作来爬上山坡。与此不同，我们即将面对的下一个环境——名为*CartPole*——要求我们根据最新的观察结果，在任何时间步骤动态计算我们的动作。继续阅读，了解如何实现这一点。
- en: Solving the CartPole environment
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 解决CartPole环境
- en: The *CartPole-v1* environment simulates a balancing act of a pole, hinged at
    its bottom to a cart, which moves left and right along a track. Balancing the
    pole upright is carried out by applying to the cart 1 unit of force – to the right
    or the left – at a time.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*CartPole-v1*环境模拟了一个杆平衡的过程，杆底部铰接在一个小车上，小车沿着轨道左右移动。保持杆竖直通过施加1个单位的力到小车上——每次向右或向左。'
- en: 'The pole, acting as a pendulum in this environment, starts upright within a
    small random angle, as shown in the following rendered output:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中，杆像一个摆锤一样开始竖立，并以一个小的随机角度出现，如下图所示：
- en: '![Figure 10.6: The CartPole simulation – the starting point](img/B20851_10_6.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图10.6：CartPole仿真—起始点](img/B20851_10_6.jpg)'
- en: 'Figure 10.6: The CartPole simulation – the starting point'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：CartPole仿真—起始点
- en: 'Our goal is to keep the pendulum from falling over to either side for as long
    as possible – that is, up to 500 time steps. For every time step that the pole
    remains upright, we get a reward of +1, so the maximum total reward is 500\. The
    episode will end prematurely if one of the following occurs during the run:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是尽可能长时间地保持摆锤不倾倒到任一侧——即，最多500个时间步骤。每当杆保持竖直时，我们将获得+1的奖励，因此最大总奖励为500。若在运行过程中发生以下任何情况，回合将提前结束：
- en: The angle of the pole from the vertical position exceeds 15 degrees
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆的角度偏离垂直位置超过15度
- en: The cart’s distance from the center exceeds 2.4 units
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小车距离中心的距离超过2.4单位
- en: Consequently, the total reward in these cases will be smaller than 500.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这些情况下，最终的奖励将小于500。
- en: 'The expected `action` value in this simulation is an integer of one of the
    two following values:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个仿真中，期望的`action`值是以下两个值之一的整数：
- en: '0: Push the cart to the left'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0：将小车推向左侧
- en: '1: Push the cart to the right'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1：将小车推向右侧
- en: 'The `observation` object contains four floats that hold the following information:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`observation`对象包含四个浮动值，保存以下信息：'
- en: '**Cart position**, between -2.4 and 2.4'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小车位置**，在-2.4到2.4之间'
- en: '**Cart velocity**, between -Inf and Inf'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小车速度**，在-Inf到Inf之间'
- en: '**Pole angle**, between -0.418 rad (-24°) and 0.418 rad (24°)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**杆角度**，在-0.418弧度（-24°）到0.418弧度（24°）之间'
- en: '**Pole angular velocity**, between -Inf and Inf'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**杆角速度**，在-Inf到Inf之间'
- en: For example, we could have an `observation` of `[0.33676587, 0.3786464, -``0.00170739,
    -0.36586074]`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以有一个`observation`为`[0.33676587, 0.3786464, -0.00170739, -0.36586074]`。
- en: More information about the CartPole-v1 environment is available at [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 有关CartPole-v1环境的更多信息，请访问[https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)。
- en: In our proposed solution, we will use these values as inputs at every time step
    to determine which action to take. We will do this with the aid of a neural network-based
    controller. This is described in more detail in the following subsection.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们提出的解决方案中，我们将在每个时间步骤使用这些值作为输入，以决定采取什么行动。我们将借助基于神经网络的控制器来实现这一点。详细描述见下一个小节。
- en: Controlling the CartPole with a neural network
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络控制CartPole
- en: To successfully carry out the *CartPole* challenge, we would like to dynamically
    respond to the changes in the environment. For example, when the pole starts leaning
    in one direction, we should probably move the cart in that direction but possibly
    stop pushing when it starts to stabilize. So, the reinforcement learning task
    here can be thought of as teaching a controller to balance the pole by mapping
    the four available inputs – cart position, cart velocity, pole angle, and pole
    velocity – to the appropriate action at each time step. How can we implement such
    mapping?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功地完成*CartPole*挑战，我们希望能够动态响应环境的变化。例如，当杆子开始向一个方向倾斜时，我们可能应该把小车朝那个方向移动，但当杆子开始稳定时，可能需要停止推动。因此，这里的强化学习任务可以被看作是教一个控制器通过将四个可用的输入——小车位置、小车速度、杆子角度和杆子速度——映射到每个时间步的适当动作，来保持杆子的平衡。我们如何实现这种映射呢？
- en: 'One good way to implement this mapping is by using a **neural network**. As
    we saw in [*Chapter 9*](B20851_09.xhtml#_idTextAnchor257), *Architecture Optimization
    of Deep Learning Networks*, a neural network, such as a **multilayer perceptron**
    (**MLP**), can implement complex mappings between its inputs and outputs. This
    mapping is done with the aid of the network parameters – namely, the *weights
    and biases* of the active nodes in the network, as well as the *transfer functions*
    that are implemented by these nodes. In our case, we will use a network with a
    single *hidden layer* of four nodes. In addition, the *input layer* consists of
    four nodes, one for each of the input values provided by the environment, while
    the *output layer* has a single node, since we only have one output value – the
    action to be taken. This network structure can be illustrated with the following
    diagram:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这种映射的一个好方法是使用**神经网络**。正如我们在[*第9章*](B20851_09.xhtml#_idTextAnchor257)《深度学习网络的架构优化》中看到的那样，神经网络，比如**多层感知器**（**MLP**），可以实现其输入和输出之间的复杂映射。这个映射是通过网络的参数来完成的——即，网络中活跃节点的*权重和偏置*，以及这些节点实现的*传递函数*。在我们的案例中，我们将使用一个包含四个节点的单一*隐藏层*的网络。此外，*输入层*由四个节点组成，每个节点对应环境提供的一个输入值，而*输出层*则有一个节点，因为我们只有一个输出值——即需要执行的动作。这个网络结构可以通过以下图示来表示：
- en: '![Figure 10.7: The structure of the neural network that’s used to control the
    cart](img/B20851_10_7.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7：用于控制小车的神经网络结构](img/B20851_10_7.jpg)'
- en: 'Figure 10.7: The structure of the neural network that’s used to control the
    cart'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7：用于控制小车的神经网络结构
- en: As we have seen already, the values of the weights and biases of a neural network
    are typically set during a process in which the network is trained. The interesting
    part is that, so far, we have only seen this kind of neural network being trained
    using the backpropagation algorithm while implementing *supervised learning* –
    that is, in each of the previous cases, we had a training set of inputs and matching
    outputs, and the network was trained to map each given input to its matching given
    output. Here, however, as we practice *reinforcement learning*, we don’t have
    such training information available. Instead, we only know how well the network
    did at the end of the episode. This means that instead of using the conventional
    training algorithms, we need a method that will allow us to find the best network
    parameters – weights and biases – based on the results that are obtained by running
    the environment’s episodes. This is exactly the kind of optimization that genetic
    algorithms are good at – finding a set of parameters that will give us the best
    results, as long as you have a way to evaluate and compare them. To do that, we
    need to figure out how to represent the network’s parameters, as well as how to
    evaluate a given set of those parameters. Both of these topics will be covered
    in the following subsection.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经看到的，神经网络的权重和偏置值通常是在网络训练的过程中设置的。值得注意的是，到目前为止，我们仅仅看到了在使用反向传播算法实施*监督学习*的过程中训练神经网络——也就是说，在之前的每一种情况中，我们都有一组输入和匹配的输出，网络被训练来将每个给定的输入映射到其匹配的输出。然而，在这里，当我们实践*强化学习*时，我们并没有这种训练信息。相反，我们只知道网络在每一轮训练结束时的表现如何。这意味着我们需要一种方法来根据通过运行环境的训练轮次获得的结果来找到最佳的网络参数——即权重和偏置，而不是使用传统的训练算法。这正是遗传算法擅长的优化任务——找到一组能够为我们提供最佳结果的参数，只要你有评估和比较这些参数的方法。为了做到这一点，我们需要弄清楚如何表示网络的参数，并且如何评估一组给定的参数。这两个问题将在下一个小节中讨论。
- en: Solution representation and evaluation
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案表示与评估
- en: 'Since we have decided to control the cart in the CartPole challenge using a
    neural network of the *MLP* type, the set of parameters that we will need to optimize
    are the network’s weights and biases, as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们决定使用*MLP*类型的神经网络来控制CartPole挑战中的小车，因此我们需要优化的参数集合为网络的权重和偏置，具体如下：
- en: '**Input layer**: This layer does not participate in the network mapping; instead,
    it receives the input values and forwards them to every neuron in the next layer.
    Therefore, no parameters are needed for this layer.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：该层不参与网络映射；相反，它接收输入值并将其传递给下一层的每个神经元。因此，这一层不需要任何参数。'
- en: '**Hidden layer**: Each node in this layer is fully connected to each of the
    four inputs and, therefore, requires four weights in addition to a single bias
    value.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：这一层中的每个节点与四个输入完全连接，因此除了一个偏置值外，还需要四个权重。'
- en: '**Output layer**: The single node in this layer is connected to each of the
    four nodes in the hidden layer and, therefore, requires four weights in addition
    to a single bias value.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：这一层的单个节点与隐藏层中的每个四个节点相连，因此除了一个偏置值外，还需要四个权重。'
- en: 'In total, we have 20 weight values and 5 bias values we need to find, all of
    the `float` type. Therefore, each potential solution can be represented as a list
    of 25 `float` values, like so:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有20个权重值和5个偏置值需要找到，所有值都为`float`类型。因此，每个潜在解决方案可以表示为25个`float`值的列表，如下所示：
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Evaluating a given solution means creating our MLP with the correct dimensions
    – four inputs, a four-node hidden layer, and a single output – and assigning the
    weight and bias values from our list of floats to the various nodes. Then, we
    need to use this MLP as the controller for the cart pole during one episode. The
    resulting total reward of the episode is used as the score value for this solution.
    In contrast to the previous task, here, we aim to *maximize* the score that’s
    achieved. This score evaluation procedure is implemented by the `CartPole` class,
    which will be explored in the following subsection.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 评估给定的解决方案意味着创建一个具有正确维度的MLP——四个输入，一个四节点的隐藏层和一个输出——并将我们浮动列表中的权重和偏置值分配到不同的节点上。然后，我们需要使用这个MLP作为小车摆杆的控制器，运行一个回合。回合的总奖励作为此解决方案的得分值。与之前的任务相比，在这里我们旨在*最大化*得分。这个得分评估过程由`CartPole`类实现，接下来将深入讨论。
- en: The Python problem representation
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Python问题表示
- en: To encapsulate the *CartPole* challenge, we’ve created a Python class called
    `CartPole`. This class is contained in the `cart_pole.py` file, which is located
    at [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/cart_pole.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/cart_pole.py).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了封装*CartPole*挑战，我们创建了一个名为`CartPole`的Python类。该类包含在`cart_pole.py`文件中，位于[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/cart_pole.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/cart_pole.py)。
- en: 'The class is initialized with an optional random seed and provides the following
    methods:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 该类通过一个可选的随机种子初始化，并提供以下方法：
- en: '**initMlp()**: Initializes an MLP *regressor* with the desired network architecture
    (layers) and network parameters (weights and biases), which are derived from the
    list of floats that represent a candidate solution.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**initMlp()**：使用所需的网络架构（层）和网络参数（权重和偏置）初始化一个MLP *回归器*，这些参数来自表示候选解决方案的浮动列表。'
- en: '**getScore()**: Calculates the score of a given solution, represented by the
    list of float-valued network parameters. This is done by creating a corresponding
    MLP regressor, initiating an episode of the *CartPole* environment, and running
    it with the MLP controlling the actions, all while using the observations as inputs.
    The higher the score is, the better.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**getScore()**：计算给定解决方案的得分，该解决方案由一组浮点值表示的网络参数表示。通过创建一个相应的MLP回归器，初始化*CartPole*环境的一个回合，并在使用观察作为输入的同时，利用MLP控制行动来实现这一点。得分越高，效果越好。'
- en: '**saveParams()**: Serializes and saves a list of network parameters using **pickle**.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**saveParams()**：使用**pickle**序列化并保存网络参数列表。'
- en: '**replayWithSavedParams()**: Deserializes the latest saved list of network
    parameters and uses it to replay an episode using the **replay** method.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**replayWithSavedParams()**：反序列化最新保存的网络参数列表，并使用这些参数通过**replay**方法重放一个回合。'
- en: '**replay()**: Renders the environment and uses the given network parameters
    to replay an episode, visualizing a given solution.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**replay()**：渲染环境，并使用给定的网络参数重放一个回合，展示给定的解决方案。'
- en: The main method of the class should be used after a solution has been serialized
    and saved, using the `saveParams()` method. The main method will initialize the
    class and call `replayWithSavedParams()` to render and animate the saved solution.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 类的主要方法应该在解决方案已序列化并保存后使用，使用`saveParams()`方法。主方法将初始化类并调用`replayWithSavedParams()`来渲染并动画化保存的解决方案。
- en: We will typically use the main method to animate the best solution that’s found
    by our genetic algorithm-driven solution, as explored in the following subsection.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常会使用主方法来动画化遗传算法驱动的解决方案所找到的最佳解决方案，正如下面小节所探讨的那样。
- en: A genetic algorithm solution
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传算法解决方案
- en: To interact with the *CartPole* environment and solve it using a genetic algorithm,
    we’ve created a Python program, `02_solve_cart-pole.py`, which is located at [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/02_solve_cart_pole.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/02_solve_cart_pole.py).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与*CartPole*环境进行交互并使用遗传算法来解决它，我们创建了一个Python程序`02_solve_cart-pole.py`，该程序位于[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/02_solve_cart_pole.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/02_solve_cart_pole.py)。
- en: Since we will use a list of floats to represent a solution – the network’s weights
    and biases – this program is very similar to the function optimization programs
    we looked at in [*Chapter 6*](B20851_06.xhtml#_idTextAnchor197), *Optimizing Continuous
    Functions*, such as the one we used for the *Eggholder* *function*’s optimization.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用浮动数值列表来表示解决方案——即网络的权重和偏差——这个程序与我们在[*第6章*](B20851_06.xhtml#_idTextAnchor197)中看到的函数优化程序非常相似，*优化连续函数*，例如我们用于*Eggholder*
    *函数*优化的程序。
- en: 'The following steps describe how to create the main parts of this program:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤描述了如何创建此程序的主要部分：
- en: 'We start by creating an instance of the **CartPole** class, which will allow
    us to test the various solutions for the *CartPole* challenge:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先创建一个**CartPole**类的实例，这将使我们能够测试*CartPole*挑战的各种解决方案：
- en: '[PRE17]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '2.  Next, we set the upper and lower boundaries for the float values we will
    search for. Since all of our values represent weights and biases within a neural
    network, this range should be between -1.0 and 1.0 in every dimension:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.  接下来，我们设置浮动数值的上下边界。由于我们所有的数值表示神经网络中的权重和偏差，因此这个范围应该在每个维度内都介于-1.0和1.0之间：
- en: '[PRE18]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '3.  As you may recall, our goal in this challenge is to *maximize* the score
    – how long we can keep the pole balanced. To do so, we define a single objective,
    maximizing the fitness strategy:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3.  如你所记得，我们在这个挑战中的目标是*最大化*分数——即我们能保持杆子平衡的时间。为此，我们定义了一个单一目标，最大化适应度策略：
- en: '[PRE19]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '4.  Now, we need to create a helper function to create random real numbers
    that are uniformly distributed within a given range. This function assumes that
    the range is the same for every dimension, as is the case in our solution:'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 4.  现在，我们需要创建一个辅助函数，用于在给定范围内均匀分布地生成随机实数。此函数假设每个维度的范围都是相同的，就像我们解决方案中的情况一样：
- en: '[PRE20]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '5.  Now, we use this function to create an operator that randomly returns a
    list of floats in the desired range that we set earlier:'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5.  现在，我们使用此函数创建一个操作符，它会随机返回一个在我们之前设定的范围内的浮动数值列表：
- en: '[PRE21]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '6.  This is followed by an operator that fills up an individual instance using
    the preceding operator:'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 6.  紧接着是一个操作符，使用之前的操作符填充个体实例：
- en: '[PRE22]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'def score(individual):'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def score(individual):'
- en: return cartPole.getScore(individual),
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return cartPole.getScore(individual),
- en: toolbox.register("evaluate", score)
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: toolbox.register("evaluate", score)
- en: '[PRE23]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '7.  It’s time to choose our genetic operators. Once again, we’ll use *tournament
    selection* with a tournament size of 2 as our *selection* operator. Since our
    solution representation is a list of floats in a given range, we’ll use the specialized
    *continuous bounded crossover* and *mutation* operators provided by the DEAP framework
    – **cxSimulatedBinaryBounded** and **mutPolynomialBounded**, respectively:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 7.  现在是选择遗传操作符的时候了。我们将再次使用*锦标赛选择*，并且锦标赛大小为2，作为我们的*选择*操作符。由于我们的解决方案表示为一个在给定范围内的浮动数值列表，我们将使用DEAP框架提供的专用*连续有界交叉*和*变异*操作符——分别是**cxSimulatedBinaryBounded**和**mutPolynomialBounded**：
- en: '[PRE24]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '8.  And, as usual, we use the *elitist approach*, where the HOF members – the
    current best individuals – are always passed untouched to the next generation:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另外，像往常一样，我们使用*精英策略*，即当前最好的个体——HOF成员——始终会直接传递到下一代：
- en: '[PRE25]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '9.  After the run, we print the best solution and save it so that we can animate
    it, using the replay capability we built into the **MountainCar** class:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行结束后，我们打印出最佳解并保存，以便通过我们在**MountainCar**类中构建的回放功能进行动画演示：
- en: '[PRE26]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '10.  In addition, we will run 100 consecutive episodes using our best individual,
    randomly initiating the CartPole problem each time, so each episode starts from
    a slightly different starting condition and can potentially yield a different
    result. We will then calculate the average of all the results:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，我们将使用我们最好的个体运行100次连续的实验，每次都随机初始化CartPole问题，因此每个实验都从稍微不同的起始条件开始，可能会得到不同的结果。然后我们将计算所有结果的平均值：
- en: '[PRE27]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'It is time to find out how well we did in this challenge. By running the algorithm
    for 10 generations with a population size of 30, we get the following outcome:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看我们在这个挑战中表现得如何了。通过运行10代，每代30个个体的遗传算法，我们得到了以下结果：
- en: '[PRE28]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: From the preceding output, we can see that, after just five generations, the
    best solution(s) reached the maximum score of 500, balancing the pole for the
    entire episode’s time.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中可以看到，在仅仅五代之后，最好的解达到了500的最高分，在整个实验期间平衡了杆子。
- en: 'Looking at the results of our additional test, it seems that all 100 tests
    ended with a perfect score of 500:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们额外测试的结果来看，似乎所有100次测试都以完美的500分结束：
- en: '[PRE29]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As we mentioned previously, each of these 100 runs is done with a slightly different
    random starting point. However, the controller is powerful enough to balance the
    pole for the entire run, each and every time. To watch the controller in action,
    we can play a CartPole episode – or several episodes – with the saved results
    by launching the `cart_pole` program. The animation illustrates how the controller
    dynamically responds to the pole’s movement by applying actions that keep the
    pole balanced on the cart for the entire episode.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，每次这100次实验都以略有不同的随机起始点开始。然而，控制器足够强大，可以每次都在整个实验过程中保持杆子的平衡。为了观察控制器的实际效果，我们可以通过启动`cart_pole`程序来播放CartPole实验——或播放多个实验——并查看保存的结果。动画展示了控制器如何通过采取行动动态地响应杆子的运动，使其在整个实验过程中保持在小车上的平衡。
- en: If you would like to contrast these results with less-than-perfect ones, you
    are encouraged to repeat this experiment with three (or even two) nodes in the
    hidden layer instead of four – just change the `HIDDEN_LAYER` constant value accordingly
    in the `CartPole` class. Alternatively, you can reduce the number of generations
    and/or population size of the genetic algorithm.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想将这些结果与不完美的结果进行对比，建议你在`CartPole`类中将`HIDDEN_LAYER`常量的值改为三（甚至两个）个节点，而不是四个。或者，你可以减少遗传算法的代数和/或种群规模。
- en: Summary
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you were introduced to the basic concepts of **reinforcement
    learning**. After getting acquainted with the **Gymnasium** toolkit, you were
    presented with the *MountainCar* challenge, where a car needs to be controlled
    in a way that will allow it to climb the taller of two mountains. After solving
    this challenge using genetic algorithms, you were introduced to the next challenge,
    *CartPole*, where a cart is to be precisely controlled to keep an upright pole
    balanced. We were able to solve this challenge by combining the power of a neural
    network-based controller with genetic algorithm-guided training.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了**强化学习**的基本概念。在熟悉了**Gymnasium**工具包后，你遇到了*MountainCar*挑战，在这个挑战中，需要控制一辆车使其能够爬上两座山中的较高一座。在使用遗传算法解决了这个挑战后，你接着遇到了下一个挑战——*CartPole*，在这个挑战中，需要精确控制一辆小车以保持竖直的杆子平衡。我们通过结合基于神经网络的控制器和遗传算法引导的训练成功解决了这个挑战。
- en: While we have primarily focused on problems involving structured numerical data
    thus far, the next chapter will shift its focus to applications of genetic algorithms
    in **Natural Language Processing** (**NLP**), a branch of machine learning that
    empowers computers to comprehend, interpret, and process human language.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们迄今为止主要关注的是涉及结构化数值数据的问题，但下一章将转向遗传算法在**自然语言处理**（**NLP**）中的应用，这是机器学习的一个分支，使计算机能够理解、解释和处理人类语言。
- en: Further reading
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information, refer to the following resources:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参考以下资源：
- en: '*Mastering Reinforcement Learning with Python*, Enes Bilgin, December 18 2020'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用Python精通强化学习*，Enes Bilgin，2020年12月18日'
- en: '*Deep Reinforcement Learning Hands-On, 2nd Edition*, Maksim Lapan, January
    21, 2020'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度强化学习实战，第2版*，Maksim Lapan，2020年1月21日'
- en: 'Gymnasium documentation:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gymnasium文档：
- en: '[https://gymnasium.farama.org/](https://gymnasium.farama.org/)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://gymnasium.farama.org/](https://gymnasium.farama.org/)'
- en: '*OpenAI Gym* (white paper), Greg Brockman, Vicki Cheung, Ludwig Pettersson,
    Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*OpenAI Gym*（白皮书），Greg Brockman，Vicki Cheung，Ludwig Pettersson，Jonas Schneider，John
    Schulman，Jie Tang，Wojciech Zaremba：'
- en: '[https://arxiv.org/abs/1606.01540](https://arxiv.org/abs/1606.01540)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1606.01540](https://arxiv.org/abs/1606.01540)'
- en: '[PRE30]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
