- en: Aggregating and Summarizing Data into Useful Reports
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据聚合和汇总为有用的报告
- en: In this chapter, we will learn how to aggregate and summarize data into useful
    reports. We will learn how to calculate averages with `map` and `reduce` functions,
    perform faster average computation, and use pivot tables with key-value pair data
    points.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何将数据聚合和汇总为有用的报告。我们将学习如何使用 `map` 和 `reduce` 函数计算平均值，执行更快的平均计算，并使用键值对数据点的数据透视表。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将涵盖以下主题：
- en: Calculating averages with `map` and `reduce`
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `map` 和 `reduce` 计算平均值
- en: Faster average computations with aggregate
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用聚合进行更快的平均计算
- en: Pivot tabling with key-value paired data points
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用键值对数据点进行数据透视表
- en: Calculating averages with map and reduce
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 map 和 reduce 计算平均值
- en: 'We will be answering the following three main questions in this section:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回答以下三个主要问题：
- en: How do we calculate averages?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何计算平均值？
- en: What is a map?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 map？
- en: What is reduce?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 reduce？
- en: You can check the documentation at [https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.map](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.map).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.map](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.map)上查看文档。
- en: The `map` function takes two arguments, one of which is optional. The first
    argument to `map` is `f`, which is a function that gets applied to the RDD throughout
    by the `map` function. The second argument, or parameter, is the `preservesPartitioning`
    parameter, which is `False` by default.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`map` 函数接受两个参数，其中一个是可选的。`map` 的第一个参数是 `f`，它是一个应用于整个 RDD 的函数。第二个参数或参数是 `preservesPartitioning`
    参数，默认值为 `False`。'
- en: 'If we look at the documentation, it says that `map` simply returns a new RDD
    by applying a function to each element of this RDD, and obviously, this function
    refers to `f` that we feed into the `map` function itself. There''s a very simple
    example in the documentation, where it says if we parallelize an `rdd` method
    that contains a list of three characters, `b`, `a`, and `c`, and we map a function
    that creates a tuple of each element, then we''ll create a list of three-tuples,
    where the original character is placed in the first elements of the tuple, and
    the `1` integer is placed in the second as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看文档，它说 `map` 通过将函数应用于此 RDD 的每个元素来简单地返回一个新的 RDD，显然，此函数指的是我们输入到 `map` 函数本身的
    `f`。文档中有一个非常简单的例子，如果我们并行化一个包含三个字符 `b`、`a` 和 `c` 的 `rdd` 方法，并且我们映射一个创建每个元素的元组的函数，那么我们将创建一个包含三个元组的列表，其中原始字符放在元组的第一个元素中，整数
    `1` 放在第二个元素中，如下所示：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will give us the following output:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `reduce` function takes only one argument, which is `f`. `f` is a function to
    reduce a list into one number. From a technical point of view, the specified commutative
    and associative binary operator reduces the elements of this RDD.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce` 函数只接受一个参数，即 `f`。`f` 是一个将列表减少为一个数字的函数。从技术角度来看，指定的可交换和可结合的二进制运算符减少了此
    RDD 的元素。'
- en: 'Let''s take an example using the KDD data we have been using. We launch our
    Jupyter Notebook instance that links to a Spark instance, as we have done previously.
    We then create a `raw_data` variable by loading a `kddcup.data.gz` text file from
    the local disk as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们一直在使用的 KDD 数据来举个例子。我们启动我们的 Jupyter Notebook 实例，它链接到一个 Spark 实例，就像我们以前做过的那样。然后我们通过从本地磁盘加载
    `kddcup.data.gz` 文本文件来创建一个 `raw_data` 变量，如下所示：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next thing to do is to split this file into `csv`, and then we will filter
    for rows where feature 41 includes the word `normal`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来要做的是将此文件拆分为 `csv`，然后我们将过滤包含单词 `normal` 的特征 41 的行：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then we use the `map` function to convert this data into an integer, and then,
    finally, we can use the `reduce` function to compute the `total_duration`, and
    then we can print the `total_duration` as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用 `map` 函数将这些数据转换为整数，最后，我们可以使用 `reduce` 函数来计算 `total_duration`，然后我们可以打印
    `total_duration` 如下：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will then get the following output:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将得到以下输出：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The next thing to do is to divide `total_duration` by the counts of the data
    as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来要做的是将 `total_duration` 除以数据的计数，如下所示：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will give us the following output:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: And after a little computation, we would have created two counts using `map`
    and `reduce`. We have just learned how we can calculate averages with PySpark,
    and what the `map` and `reduce` functions are in PySpark.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微计算后，我们将使用 `map` 和 `reduce` 创建两个计数。我们刚刚学会了如何使用 PySpark 计算平均值，以及 PySpark 中的
    `map` 和 `reduce` 函数是什么。
- en: Faster average computations with aggregate
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用聚合进行更快的平均计算
- en: In the previous section, we saw how we can use `map` and `reduce` to calculate
    averages. Let's now look at faster average computations with the `aggregate` function.
    You can refer to the documentation mentioned in the previous section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了如何使用 `map` 和 `reduce` 计算平均值。现在让我们看看如何使用 `aggregate` 函数进行更快的平均计算。您可以参考前一节中提到的文档。
- en: The `aggregate` is a function that takes three arguments, none of which are
    optional.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregate` 是一个带有三个参数的函数，其中没有一个是可选的。'
- en: The first one is the `zeroValue` argument, where we put in the base case of
    the aggregated results.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是 `zeroValue` 参数，我们在其中放入聚合结果的基本情况。
- en: The second argument is the sequential operator (`seqOp`), which allows you to
    stack and aggregate values on top of `zeroValue`. You can start with `zeroValue`,
    and the `seqOp` function that you feed into `aggregate` takes values from your
    RDD, and stacks or aggregates it on top of `zeroValue`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数是顺序运算符 (`seqOp`)，它允许您在 `zeroValue` 之上堆叠和聚合值。您可以从 `zeroValue` 开始，将您的 RDD
    中的值传递到 `seqOp` 函数中，并将其堆叠或聚合到 `zeroValue` 之上。
- en: The last argument is `combOp`, which stands for combination operation, where
    we simply take the `zeroValue` argument that is now aggregated through the `seqOp`
    argument, and combine it into one value so that we can use this to conclude the
    aggregation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个参数是`combOp`，表示组合操作，我们只需将通过`seqOp`参数聚合的`zeroValue`参数组合成一个值，以便我们可以使用它来完成聚合。
- en: 'So, here we are aggregating the elements of each partition and then the results
    for all the partitions using a combined function and a neutral zero value. Here,
    we have two things to note:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们正在聚合每个分区的元素，然后使用组合函数和中性零值对所有分区的结果进行聚合。在这里，我们有两件事需要注意：
- en: The `op` function is allowed to modify `t1`, but it should not modify `t2`
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`op`函数允许修改`t1`，但不应修改`t2`'
- en: The first function `seqOp` can return a different result type `U`
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个函数`seqOp`可以返回不同的结果类型`U`
- en: In this case, we all need one operation for merging a `T` into `U`, and one
    operation for merging the two Us.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们都需要一个操作来将`T`合并到`U`，以及一个操作来合并这两个`U`。
- en: 'Let''s go to our Jupyter Notebook to check how this is done. `aggregate` allows
    us to calculate both the total duration and the count at the same time. We call
    the `duration_count` function. We then take `normal_data` and we aggregate it.
    Remember that there are three arguments to aggregate. The first one is the initial
    value; that is, the zero value, `(0,0)`. The second one is a sequential operation,
    as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们去我们的Jupyter Notebook检查这是如何完成的。`aggregate`允许我们同时计算总持续时间和计数。我们调用`duration_count`函数。然后我们取`normal_data`并对其进行聚合。请记住，聚合有三个参数。第一个是初始值；也就是零值，`(0,0)`。第二个是一个顺序操作，如下所示：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We need to specify a `lambda` function with two arguments. The first argument
    is the current accumulator, or the aggregator, or what can also be called a database
    (`db`). Then, we have the second argument in our `lambda` function as `new_value`,
    or the current value we're processing in the RDD. We simply want to do the right
    thing to the database, so to say, where we know that our database looks like a
    tuple with the sum of duration on the first element and the count on the second
    element. Here, we know that our database looks like a tuple, where the sum of
    duration is the first element, and the count is the second element. Whenever we
    look at a new value, we need to add the new value to the current running total and
    add `1` to the current running counts.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要指定一个具有两个参数的`lambda`函数。第一个参数是当前的累加器，或者聚合器，或者也可以称为数据库（`db`）。然后，在我们的`lambda`函数中，我们有第二个参数`new_value`，或者我们在RDD中处理的当前值。我们只是想对数据库做正确的事情，也就是说，我们知道我们的数据库看起来像一个元组，第一个元素是持续时间的总和，第二个元素是计数。在这里，我们知道我们的数据库看起来像一个元组，持续时间的总和是第一个元素，计数是第二个元素。每当我们查看一个新值时，我们需要将新值添加到当前的运行总数中，并将`1`添加到当前的运行计数中。
- en: The running total is the first element, `db[0]`. And we then simply need to
    add `1` to the second element `db[1]`, which is the count. That's the sequential
    operation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 运行总数是第一个元素，`db[0]`。然后我们只需要将`1`添加到第二个元素`db[1]`，即计数。这是顺序操作。
- en: 'Every time we get a `new_value`, as shown in the previous code block, we simply
    add it to the running total. And, because we''ve added `new_value` to the running
    total, we need to increment the counts by `1`. Secondly, we need to put in the
    combinator operation. Now, we simply need to combine the respective elements of
    two separate databases, `db1` and `db2`, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们得到一个`new_value`，如前面的代码块所示，我们只需将其添加到运行总数中。而且，因为我们已经将`new_value`添加到运行总数中，我们需要将计数增加`1`。其次，我们需要放入组合器操作。现在，我们只需要将两个单独的数据库`db1`和`db2`的相应元素组合如下：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Since the duration counts is a tuple that collects our total duration on the
    first element, and counts how many durations we looked at in the second element,
    computing the average is very simple. We need to divide the first element by the
    second element as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于持续时间计数是一个元组，它在第一个元素上收集了我们的总持续时间，在第二个元素上记录了我们查看的持续时间数量，计算平均值非常简单。我们需要将第一个元素除以第二个元素，如下所示：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will give us the following output:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can see that it returns the same results as we saw in the previous section,
    which is great. In the next section, we are going to look at pivot tabling with
    key-value paired data points.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到它返回了与我们在上一节中看到的相同的结果，这很棒。在下一节中，我们将看一下带有键值对数据点的数据透视表。
- en: Pivot tabling with key-value paired data points
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有键值对数据点的数据透视表
- en: Pivot tables are very simple and easy to use. What we are going to do is use
    big datasets, such as the KDD cup dataset, and group certain values by certain
    keys.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数据透视表非常简单且易于使用。我们将使用大型数据集，例如KDD杯数据集，并根据某些键对某些值进行分组。
- en: For example, we have a dataset of people and their favorite fruits. We want
    to know how many people have apple as their favorite fruit, so we will group the
    number of people, which is the value, against a key, which is the fruit. This
    is the simple concept of a pivot table.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们有一个包含人和他们最喜欢的水果的数据集。我们想知道有多少人把苹果作为他们最喜欢的水果，因此我们将根据水果将人数进行分组，这是值，而不是键。这就是数据透视表的简单概念。
- en: 'We can use the `map` function to move the KDD datasets into a key-value pair
    paradigm. We map feature `41` of the dataset using a `lambda` function in the `kv` key
    value, and we append the value as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`map`函数将KDD数据集移动到键值对范例中。我们使用`lambda`函数将数据集的特征`41`映射到`kv`键值，并将值附加如下：
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We use feature `41` as the key, and the value is the data point, which is `x`.
    We can use the `take` function to take one of these transformed rows to see how
    it looks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用特征`41`作为键，值是数据点，即`x`。我们可以使用`take`函数来获取这些转换行中的一个，以查看其外观。
- en: Let's now try something similar to the previous example. To figure out the total
    duration against each type of value that is present in feature `41`, we can use
    the `map` function again and simply take the `41` feature as our key. We can take the
    float of the first number in the data point as our value. We will use the `reduceByKey`
    function to reduce each duration by its key.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试类似于前面的例子。为了找出特征`41`中每种数值的总持续时间，我们可以再次使用`map`函数，简单地将`41`特征作为我们的键。我们可以将数据点中第一个数字的浮点数作为我们的值。我们将使用`reduceByKey`函数来减少每个键的持续时间。
- en: So, instead of just reducing all of the data points regardless of which key
    they belong to, `reduceByKey` reduces duration numbers depending on which key
    it is associated with. You can view the documentation at [https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.reduceByKey](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.reduceByKey).
    `reduceByKey` merges the values for each key using an associative and commutative
    `reduce` function. It performs local merging on each mapper before sending the
    results to the reducer, which is similar to a combiner in MapReduce.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`reduceByKey`不仅仅是减少所有数据点，而是根据它们所属的键来减少持续时间数字。您可以在[https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.reduceByKey](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.reduceByKey)上查看文档。`reduceByKey`使用关联和交换的`reduce`函数合并每个键的值。它在将结果发送到减速器之前在每个映射器上执行本地合并，这类似于MapReduce中的组合器。
- en: 'The `reduceByKey` function simply takes one argument. We will be using the
    `lambda` function. We take two different durations and add them together, and
    PySpark is smart enough to apply this reduction function depending on a key, as
    follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey`函数只需一个参数。我们将使用`lambda`函数。我们取两个不同的持续时间并将它们相加，PySpark足够聪明，可以根据键应用这个减少函数，如下所示：'
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The resulting output is shown in the following screenshot:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出如下截图所示：
- en: '![](img/dc4b7020-d1a2-48ee-8e9f-6f152fefc69f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc4b7020-d1a2-48ee-8e9f-6f152fefc69f.png)'
- en: 'If we collect the key-value duration data, we can see that the duration is
    collected by the value that appears in feature `41`. If we are using pivot tables
    in Excel, there is a convenience function that is the `countByKey` function, which
    does the exact same thing, demonstrated as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们收集键值持续时间数据，我们可以看到持续时间是由出现在特征`41`中的值收集的。如果我们在Excel中使用数据透视表，有一个方便的函数是`countByKey`函数，它执行的是完全相同的操作，如下所示：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will give us the following output:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '![](img/0e2d8e34-d50e-447b-818e-d2139cdaa519.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e2d8e34-d50e-447b-818e-d2139cdaa519.png)'
- en: You can see that calling the `kv.countByKey()` function is the same as calling
    the `reduceByKey` function, preceded by a mapping from the key to the duration.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '您可以看到调用`kv.countByKey()`函数与调用`reduceByKey`函数相同，先前是从键到持续时间的映射。 '
- en: Summary
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned how to calculate averages with `map` and `reduce`.
    We also learned faster average computations with `aggregate`. Finally, we learned
    that pivot tables allow us to aggregate data based on different values of features,
    and that, with pivot tables in PySpark, we can leverage handy functions, such
    as `reducedByKey` or `countByKey`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用`map`和`reduce`计算平均值。我们还学习了使用`aggregate`进行更快的平均计算。最后，我们了解到数据透视表允许我们根据特征的不同值对数据进行聚合，并且在PySpark中，我们可以利用`reducedByKey`或`countByKey`等方便的函数。
- en: In the next chapter, we will learn about MLlib, which involves machine learning,
    which is a very hot topic.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习关于MLlib的内容，其中涉及机器学习，这是一个非常热门的话题。
