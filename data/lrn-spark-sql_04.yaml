- en: Using Spark SQL for Data Munging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL进行数据整理
- en: In this code-intensive chapter, we will present key data munging techniques
    used to transform raw data to a usable format for analysis. We start with some
    general data munging steps that are applicable in a wide variety of scenarios.
    Then, we shift our focus to specific types of data including time-series data,
    text, and data preprocessing steps for Spark MLlib-based machine learning pipelines.
    We will use several Datasets to illustrate these techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码密集的章节中，我们将介绍用于将原始数据转换为可用格式进行分析的关键数据整理技术。我们首先介绍适用于各种场景的一些通用数据整理步骤。然后，我们将把重点转移到特定类型的数据，包括时间序列数据、文本和用于Spark
    MLlib机器学习流水线的数据预处理步骤。我们将使用几个数据集来说明这些技术。
- en: 'In this chapter, we shall learn:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习：
- en: What is data munging?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是数据整理？
- en: Explore data munging techniques
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据整理技术
- en: Combine data using joins
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用连接合并数据
- en: Munging on textual data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据整理
- en: Munging on time-series data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列数据整理
- en: Dealing with variable length records
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理可变长度记录
- en: Data preparation for machine learning pipelines
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习流水线准备数据
- en: Introducing data munging
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍数据整理
- en: Raw data is typically messy and requires a series of transformations before
    it becomes useful for modeling and analysis work. Such Datasets can have missing
    data, duplicate records, corrupted data, incomplete records, and so on. In its
    simplest form, data munging, or data wrangling, is basically the transformation
    of raw data into a usable format. In most projects, this is the most challenging
    and time-consuming step.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据通常混乱不堪，需要经过一系列转换才能变得有用，用于建模和分析工作。这样的数据集可能存在缺失数据、重复记录、损坏数据、不完整记录等问题。在其最简单的形式中，数据整理或数据整理基本上是将原始数据转换为可用格式。在大多数项目中，这是最具挑战性和耗时的步骤。
- en: However, without data munging your project can reduce to a garbage-in, garbage-out
    scenario.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果没有数据整理，您的项目可能会陷入垃圾进垃圾出的境地。
- en: Typically, you will execute a bunch of functions and processes such as subset,
    filter, aggregate, sort, merge, reshape, and so on. In addition, you will also
    do type conversions, add new fields/columns, rename fields/columns, and so on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您将执行一系列函数和过程，如子集、过滤、聚合、排序、合并、重塑等。此外，您还将进行类型转换、添加新字段/列、重命名字段/列等操作。
- en: A large project can comprise of several different kinds of data with varying
    degrees of data quality. There can be a mix of numerical, textual, time-series,
    structured, and unstructured data including audio and video data used together
    or separately for analysis. A substantial part of such projects consist of cleansing
    and transformation steps combined with some statistical analyses and visualization.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个大型项目可能包含各种数据，数据质量不同。可能会混合使用数字、文本、时间序列、结构化和非结构化数据，包括音频和视频数据，一起或分开用于分析。这类项目的一个重要部分包括清洗和转换步骤，结合一些统计分析和可视化。
- en: 'We will use several Datasets to demonstrate the key data munging techniques
    required for preparing the data for subsequent modeling and analyses. These Datasets
    and their sources are listed as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用几个数据集来演示为准备数据进行后续建模和分析所需的关键数据整理技术。以下是这些数据集及其来源：
- en: '**Individual household electric power consumption Dataset**: The original source
    for the Dataset provided by Georges Hebrail, Senior Researcher, EDF R&D, Clamart,
    France and Alice Berard, TELECOM ParisTech Master of Engineering Internship at
    EDF R&D, Clamart, France. The Dataset consists of measurements of electric power
    consumption in one household at one-minute intervals for a period of nearly four
    years. This Dataset is available for download from the UCI Machine Learning Repository
    from the following URL:'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个人家庭电力消耗数据集**：数据集的原始来源是法国EDF R&D的高级研究员Georges Hebrail和法国Clamart的TELECOM ParisTech工程师实习生Alice
    Berard。该数据集包括近四年内一个家庭每分钟的电力消耗测量。该数据集可以从以下网址下载：'
- en: '[https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption)。'
- en: '**Machine Learning based ZZAlpha Ltd Stock Recommendations 2012-2014 Dataset**:
    This Dataset contains recommendations made, for various US traded stock portfolios,
    the morning of each day during a three year period from Jan 1, 2012 to Dec 31,
    2014\. This Dataset can be downloaded from the following URL:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于机器学习的ZZAlpha Ltd. 2012-2014股票推荐数据集**：该数据集包含了在2012年1月1日至2014年12月31日期间，每天早上针对各种美国交易的股票组合所做的推荐。该数据集可以从以下网址下载：'
- en: '[https://archive.ics.uci.edu/ml/datasets/Machine+Learning+based+ZZAlpha+Ltd.+Stock+Recommendations+2012-2014](https://archive.ics.uci.edu/ml/datasets/Machine+Learning+based+ZZAlpha+Ltd.+Stock+Recommendations+2012-2014).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/datasets/Machine+Learning+based+ZZAlpha+Ltd.+Stock+Recommendations+2012-2014](https://archive.ics.uci.edu/ml/datasets/Machine+Learning+based+ZZAlpha+Ltd.+Stock+Recommendations+2012-2014)。'
- en: '**Paris weather history Dataset**: This Dataset contains the daily weather
    report for Paris. We downloaded historical data covering the same time period
    as in the household electric power consumption Dataset. This Dataset can downloaded
    from the following URL:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**巴黎天气历史数据集**：该数据集包含了巴黎的每日天气报告。我们下载了与家庭电力消耗数据集相同时间段的历史数据。该数据集可以从以下网址下载：'
- en: '[https://www.wunderground.com/history/airport/LFPG](https://www.wunderground.com/history/airport/LFPG).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.wunderground.com/history/airport/LFPG](https://www.wunderground.com/history/airport/LFPG)。'
- en: '**Original 20 newsgroups data**: This data set consists of 20,000 messages
    taken from 20 Usenet newsgroups. The original owner and donor of this Dataset
    was Tom Mitchell, School of Computer Science, Carnegie Mellon University. Approximately
    a thousand Usenet articles were taken from each of the 20 newsgroups. Each newsgroup
    is stored in a subdirectory and each article stored as a separate file. The Dataset
    can be downloaded from the following URL:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原始20个新闻组数据**：该数据集包括来自20个Usenet新闻组的20,000条消息。该数据集的原始所有者和捐赠者是Carnegie Mellon大学计算机科学学院的Tom
    Mitchell。大约每个新闻组中取了一千篇Usenet文章。每个新闻组存储在一个子目录中，每篇文章都存储为一个单独的文件。该数据集可以从以下网址下载：'
- en: '[http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html).'
- en: '**Yahoo finance data**: This Dataset comprises of historical daily stock prices
    for six stocks for one year duration (from 12/04/2015 to 12/04/2016). The data
    for each of the ticker symbols chosen can been downloaded from the following site:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Yahoo财务数据**：该数据集包括了为期一年（从2015年12月4日至2016年12月4日）的六只股票的历史每日股价。所选股票符号的数据可以从以下网站下载：'
- en: '[ ](http://finance.yahoo.com/)[http://finance.yahoo.com/](http://finance.yahoo.com/).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[ ](http://finance.yahoo.com/)[http://finance.yahoo.com/](http://finance.yahoo.com/).'
- en: Exploring data munging techniques
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据整理技术
- en: In this section, we will introduce several data munging techniques using household
    electric consumption and weather Datasets. The best way to learn these techniques
    is to practice the various ways to manipulate the data contained in various publically
    available Datasets (in addition to the ones used here). The more you practice,
    the better you will get at it. In the process, you will probably evolve your own
    style, and develop several toolsets and techniques to achieve your munging objectives.
    At a minimum, you should get very comfortable working with and moving between
    RDDs, DataFrames, and Datasets, computing counts, distinct counts, and various
    aggregations to cross-check your results and match your intuitive understanding
    the Datasets. Additionally, it is also important to develop the ability to make
    decisions based on the pros and cons of executing any given munging step.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍使用家庭电力消耗和天气数据集的几种数据整理技术。学习这些技术的最佳方法是练习操纵各种公开可用数据集中包含的数据的各种方法（除了这里使用的数据集）。你练习得越多，你就会变得越擅长。在这个过程中，你可能会发展出自己的风格，并开发出几种工具集和技术来实现你的整理目标。至少，你应该非常熟悉并能够在RDD、DataFrame和数据集之间进行操作，计算计数、不同计数和各种聚合，以交叉检查你的结果并匹配你对数据集的直觉理解。此外，根据执行任何给定的整理步骤的利弊来做出决策的能力也很重要。
- en: 'We will attempt to accomplish the following objectives in this section:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试实现以下目标：
- en: Pre-process the household electric consumption Dataset--read the input Dataset,
    define case class for the rows,  count the number of records, remove the header
    and rows with missing data values, and create a DataFrame.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理家庭电力消耗数据集--读取输入数据集，为行定义case类，计算记录数，删除标题和包含缺失数据值的行，并创建DataFrame。
- en: Compute basic statistics and aggregations
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算基本统计数据和聚合
- en: Augment the Dataset with new information relevant to the analysis
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与分析相关的新信息增强数据集
- en: Execute other miscellaneous processing steps, if required
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行其他必要的杂项处理步骤
- en: Pre-process the weather Dataset--similar to step 1
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理天气数据集--类似于步骤1
- en: Analyze missing data
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析缺失数据
- en: Combine the Datasets using JOIN and analyze the results
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用JOIN合并数据集并分析结果
- en: Start the Spark shell, at this time, and follow along as you read through this
    and the subsequent sections.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此时启动Spark shell，并随着阅读本节和后续节的内容进行操作。
- en: 'Import all required classes used in this section:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 导入本节中使用的所有必需类：
- en: '![](img/00085.gif)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00085.gif)'
- en: Pre-processing of the household electric consumption Dataset
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 家庭电力消耗数据集的预处理
- en: 'Create a `case` class for household electric power consumption called `HouseholdEPC`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为家庭电力消耗创建一个名为`HouseholdEPC`的`case`类：
- en: '![](img/00086.gif)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00086.gif)'
- en: Read the input Dataset into a RDD and count the number of rows in it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入数据集读入RDD并计算其中的行数。
- en: '![](img/00087.gif)![](img/00088.gif)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00087.gif)![](img/00088.gif)'
- en: 'Next, remove the header and all other rows containing missing values, (represented
    as `?''s` in the input), as shown in the following steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，删除标题和包含缺失值的所有其他行（在输入中表示为`?`），如下面的步骤所示：
- en: '![](img/00089.gif)![](img/00090.gif)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00089.gif)![](img/00090.gif)'
- en: In the next step, convert the `RDD [String]` to a `RDD` with the `case` class,
    we defined earlier, and convert the RDD a DatFrame of `HouseholdEPC` objects.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，将`RDD [String]`转换为我们之前定义的`case`类的`RDD`，并将RDD转换为`HouseholdEPC`对象的DataFrame。
- en: '![](img/00091.gif)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00091.gif)'
- en: Display a few sample records in the DataFrame, and count the number of rows
    in it to verify that the number of rows in the DataFrame matches the expected
    number of rows in your input Dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在DataFrame中显示一些样本记录，并计算其中的行数，以验证DataFrame中的行数是否与输入数据集中预期的行数匹配。
- en: '![](img/00092.gif)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00092.gif)'
- en: Computing basic statistics and aggregations
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算基本统计数据和聚合
- en: Next, compute and display some basic statistics for the numeric columns in the
    DataFrame to get a feel for the data, we will be working with.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，计算并显示DataFrame中数值列的一些基本统计数据，以了解我们将要处理的数据。
- en: '![](img/00093.gif)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00093.gif)'
- en: We can also display the basic statistics for some or all of the columns rounded
    to four decimal places. We can also rename each of the columns by prefixing a `r`
    to the column names to differentiate them from the original column names.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以显示一些或所有列的基本统计数据，四舍五入到四位小数。我们还可以通过在列名前加上`r`来重命名每个列，以使它们与原始列名区分开来。
- en: '![](img/00094.gif)![](img/00095.gif)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00094.gif)![](img/00095.gif)'
- en: 'In addition, we count the distinct number of days, for which the data is contained
    in the DataFrame using an aggregation function:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们使用聚合函数计算包含在DataFrame中的数据的不同日期的数量：
- en: '![](img/00096.gif)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00096.gif)'
- en: Augmenting the Dataset
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强数据集
- en: We can augment the DataFrame with new columns for the day of the week, day of
    the month, month and year information. For example, we may be interested in studying
    power consumption on weekdays versus weekends. This can help achieve a better
    understanding of the data through visualization or pivoting based on these fields.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为星期几、每月的日期、月份和年份信息在DataFrame中增加新的列。例如，我们可能对工作日和周末的用电量感兴趣。这可以通过可视化或基于这些字段的数据透视来更好地理解数据。
- en: '![](img/00097.gif)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00097.gif)'
- en: Executing other miscellaneous processing steps
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行其他杂项处理步骤
- en: If required we can choose to execute a few more steps to help cleanse the data
    further, study more aggregations, or to convert to a typesafe data structure,
    and so on.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，我们可以选择执行更多步骤来帮助进一步清洗数据，研究更多的聚合，或者转换为类型安全的数据结构等。
- en: We can drop the time column and aggregate the values in various columns using
    aggregation functions such as sum and average on the values of each day's readings.
    Here, we rename the columns with a `d` prefix to represent daily values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以删除时间列，并使用聚合函数（如sum和average）对各列的值进行聚合，以获取每天读数的值。在这里，我们使用`d`前缀来重命名列，以表示每日值。
- en: '![](img/00098.gif)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00098.gif)'
- en: 'We display a few sample records from this DataFrame:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从这个DataFrame中显示一些样本记录：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](img/00099.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00099.jpeg)'
- en: Here, we group the readings by year and month, and then count the number of
    readings and display them for each of the months. The first month's number of
    readings is low as the data was captured in half a month.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们按年份和月份对读数进行分组，然后计算每个月的读数数量并显示出来。第一个月的读数数量较低，因为数据是在半个月内捕获的。
- en: '![](img/00100.gif)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.gif)'
- en: 'We can also convert our DataFrame to a Dataset using a `case` class, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`case`类将DataFrame转换为数据集，如下所示：
- en: '![](img/00101.gif)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00101.gif)'
- en: At this stage, we have completed all the steps for pre-processing the household
    electric consumption Dataset. We now shift our focus to processing the weather
    Dataset.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经完成了预处理家庭电力消耗数据集的所有步骤。现在我们将把重点转移到处理天气数据集上。
- en: Pre-processing of the weather Dataset
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 天气数据集的预处理
- en: First, we define a `case` class for the weather readings.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为天气读数定义一个`case`类。
- en: '![](img/00102.gif)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00102.gif)'
- en: Next, we read in the four files of the daily weather readings (downloaded from
    the Paris Weather website) approximately matching the same duration as the household
    electric power consumption readings.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们读取了四个文件的每日天气读数（从巴黎天气网站下载），大致与家庭电力消耗读数的持续时间相匹配。
- en: '![](img/00103.gif)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00103.gif)'
- en: 'Remove the headers from each of the input files shown as follows. We have shown
    the output of header values so you get a sense of the various weather reading
    parameters captured in these Datasets:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下显示的每个输入文件中删除标题。我们已经显示了标题值的输出，以便您了解这些数据集中捕获的各种天气读数参数：
- en: '![](img/00104.gif)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00104.gif)'
- en: Analyzing missing data
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析缺失数据
- en: 'If we wanted to get a sense of the number of rows containing one or more missing
    fields in the RDD, we can create a RDD with these rows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要了解RDD中包含一个或多个缺失字段的行数，我们可以创建一个包含这些行的RDD：
- en: '![](img/00105.gif)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00105.gif)'
- en: 'We can also do the same, if our data was available in a DataFrame as shown:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据以DataFrame的形式可用，我们也可以做同样的操作，如下所示：
- en: '![](img/00106.gif)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00106.gif)'
- en: A quick check of the Dataset reveals that most of the rows with missing data
    also have missing values for the Events and Max Gust Speed Km/h columns. Filtering
    on these two column values actually, captures all the rows with missing field
    values. It also matches the results for missing values in the RDD.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 快速检查数据集发现，大多数具有缺失数据的行也在“事件”和“最大阵风速度公里/小时”列中具有缺失值。根据这两列的值进行过滤实际上捕获了所有具有缺失字段值的行。这也与RDD中的缺失值的结果相匹配。
- en: '![](img/00107.gif)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00107.gif)'
- en: As there are many rows that contain one or more missing fields, we choose to
    retain these rows to ensure we do not lose valuable information. In the following
    function, we insert `0` in all the missing fields of an RDD.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有许多行包含一个或多个缺失字段，我们选择保留这些行，以确保不丢失宝贵的信息。在下面的函数中，我们在RDD的所有缺失字段中插入`0`。
- en: '![](img/00108.gif)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00108.gif)'
- en: 'We can replace `0` inserted in the previous step with an `NA` in the string
    fields, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用字符串字段中的`0`替换前一步骤中插入的`NA`，如下所示：
- en: '![](img/00109.gif)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00109.gif)'
- en: At this stage, we can combine the rows of the four Datasets into a single Dataset
    using the `union` operation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以使用`union`操作将四个数据集的行合并成一个数据集。
- en: '![](img/00110.gif)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00110.gif)'
- en: At this stage, the processing of our second Dataset containing weather data
    is complete. In the next section, we combine these pre-processed Datasets using
    a `join` operation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们第二个包含天气数据的数据集的处理已经完成。在接下来的部分，我们将使用`join`操作来合并这些预处理的数据集。
- en: Combining data using a JOIN operation
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用JOIN操作合并数据
- en: In this section, we will introduce the JOIN operation, in which the daily household
    electric power consumption is combined with the weather data. We have assumed
    the locations of readings taken for the household electric power consumption and
    the weather readings are in close enough proximity to be relevant.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍JOIN操作，其中每日家庭电力消耗与天气数据进行了合并。我们假设家庭电力消耗的读数位置和天气读数的位置足够接近，以至于相关。
- en: Next, we use the join operation to combine the daily household electric power
    consumption Dataset with the weather Dataset.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用JOIN操作将每日家庭电力消耗数据集与天气数据集进行合并。
- en: '![](img/00111.gif)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00111.gif)'
- en: 'Verify the number of rows in the final DataFrame obtained with the number of
    rows expected subsequent to the `join` operation shown as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 验证最终DataFrame中的行数是否与`join`操作后预期的行数相匹配，如下所示：
- en: '![](img/00112.gif)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00112.gif)'
- en: 'You can compute a series of correlations between various columns in the newly
    joined Dataset containing columns from each of the two original Datasets to get
    a feel for the strength and direction of relationships between the columns, as
    follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以计算新连接的数据集中各列之间的一系列相关性，以了解列之间的关系强度和方向，如下所示：
- en: '![](img/00113.gif)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00113.gif)'
- en: Similarly, you can join the Datasets grouped by year and month to get a higher-level
    summarization of the data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以连接按年和月分组的数据集，以获得数据的更高级别的总结。
- en: '![](img/00114.gif)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00114.gif)'
- en: 'In order to visualize the summarized data, we can execute the preceding statements
    in an Apache Zeppelin notebook. For instance, we can plot the monthly **Global
    Reactive Power** (**GRP**) values by transforming `joinedMonthlyDF` into a table
    and then selecting the appropriate columns from it, as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化总结的数据，我们可以在Apache Zeppelin笔记本中执行前面的语句。例如，我们可以通过将`joinedMonthlyDF`转换为表，并从中选择适当的列来绘制月度**全球反应功率**（**GRP**）值，如下所示：
- en: '![](img/00115.jpeg)![](img/00116.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00115.jpeg)![](img/00116.jpeg)'
- en: 'Similarly, if you want to analyze readings by the day of the week then follow,
    the steps as shown:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果您想按星期几分析读数，则按照以下步骤进行：
- en: '![](img/00117.gif)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00117.gif)'
- en: 'Finally, we print the schema of the joined Dataset (augmented with the day
    of the week column) so you can further explore the relationships between various
    fields of this DataFrame:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们打印连接的数据集的模式（增加了星期几列），以便您可以进一步探索此数据框架的各个字段之间的关系：
- en: '![](img/00118.gif)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00118.gif)'
- en: In the next section, we shift our focus to munging textual data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把重点转移到整理文本数据上。
- en: Munging textual data
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整理文本数据
- en: In this section, we explore data munging techniques for typical text analysis
    situations. Many text-based analyses tasks require computing word counts, removing
    stop words, stemming, and so on. In addition, we will also explore how you can
    process multiple files, one at a time, from HDFS directories.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨典型文本分析情况下的数据整理技术。许多基于文本的分析任务需要计算词频、去除停用词、词干提取等。此外，我们还将探讨如何逐个处理HDFS目录中的多个文件。
- en: 'First, we import all the classes that will be used in this section:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入本节中将使用的所有类：
- en: '![](img/00119.gif)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00119.gif)'
- en: Processing multiple input data files
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理多个输入数据文件
- en: In the next few steps, we initialize a set of variables for defining the directory
    containing the input files, and an empty RDD. We also create a list of filenames
    from the input HDFS directory. In the following example, we will work with files
    contained in a single directory; however, the techniques can easily be extended
    across all 20 newsgroup sub-directories.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个步骤中，我们初始化一组变量，用于定义包含输入文件的目录和一个空的RDD。我们还从输入HDFS目录创建文件名列表。在下面的示例中，我们将处理包含在单个目录中的文件；但是，这些技术可以很容易地扩展到所有20个新闻组子目录。
- en: '![](img/00120.gif)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00120.gif)'
- en: 'Next, we write a function to compute the word counts for each file and collect
    the results in an `ArrayBuffer`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写一个函数，计算每个文件的词频，并将结果收集到一个`ArrayBuffer`中：
- en: '![](img/00121.gif)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00121.gif)'
- en: 'We have included a print statement to display the file names as they are picked
    up for processing, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经包含了一个打印语句，以显示选定的文件名进行处理，如下所示：
- en: '![](img/00122.gif)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00122.gif)'
- en: 'We add the rows into a single RDD using the `union` operation:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`union`操作将行添加到单个RDD中：
- en: '![](img/00123.gif)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00123.gif)'
- en: 'We could have directly executed the union step as each file is processed, as
    follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接执行联合步骤，因为每个文件被处理时，如下所示：
- en: '![](img/00124.gif)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00124.gif)'
- en: However, using `RDD.union()` creates a new step in the lineage graph requiring
    an extra set of stack frames for each new RDD. This can easily lead to a Stack
    Overflow condition. Instead, we use `SparkContext.union()` which executes the
    `union` operation all at once without the extra memory overheads.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用`RDD.union()`会在血统图中创建一个新步骤，需要为每个新RDD添加额外的堆栈帧。这很容易导致堆栈溢出。相反，我们使用`SparkContext.union()`，它会一次性执行`union`操作，而不会产生额外的内存开销。
- en: 'We can cache and print sample rows from our output RDD as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以缓存并打印输出RDD中的样本行，如下所示：
- en: '![](img/00125.gif)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00125.gif)'
- en: In the next section, we show you ways of filtering out stop words. For simplicity,
    we focus only on well-formed words in the text. However, you can easily add conditions
    to filter out special characters and other anomalies in our data using String
    functions and regexes (for a detailed example, refer [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c), *Developing
    Applications with Spark SQL)*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将向您展示过滤停用词的方法。为简单起见，我们只关注文本中格式良好的单词。但是，您可以使用字符串函数和正则表达式轻松添加条件，以过滤数据中的特殊字符和其他异常情况（有关详细示例，请参阅[第9章](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c)，*使用Spark
    SQL开发应用程序*）。
- en: Removing stop words
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去除停用词
- en: In our example, we create a set of stop words and filter them out from the words
    in contained in each file. Normally, a Spark operation executing on a remote node
    works on a separate copy of the variables used in the function. We can use a broadcast
    variable to maintain a read-only, cached copy of the set of stop words at each
    node in the cluster instead of shipping a copy of it with the tasks to be executed
    on the nodes. Spark attempts to distribute the broadcast variables efficiently
    to reduce the overall communication overheads. Further more, we also filter out
    empty lists returned by the function as a result of our filtering process and
    stop words removal.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们创建了一组停用词，并从每个文件中的单词中过滤掉它们。通常，在远程节点上执行的Spark操作会在函数中使用的变量的单独副本上工作。我们可以使用广播变量在集群中的每个节点上维护一个只读的缓存副本，而不是将其与要在节点上执行的任务一起传输。Spark尝试有效地分发广播变量，以减少总体通信开销。此外，我们还过滤掉由于我们的过滤过程和停用词移除而返回的空列表。
- en: '![](img/00126.gif)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00126.gif)'
- en: 'We can extract the words from each of the tuples in the RDD and create a DataFrame
    containing them, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从RDD中的每个元组中提取单词，并创建包含它们的DataFrame，如下所示：
- en: '![](img/00127.gif)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00127.gif)'
- en: In the following example, we show another method for filtering out the stop
    words from our list of words. In order to improve the word matches between the
    two lists, we process the stop words file in a similar manner to the words extracted
    from the input files. We read the file containing stop words, remove leading and
    trailing spaces, convert to lower case, replace special characters, filter out
    empty words, and, finally, create a DataFrame (containing the stop words).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们展示了另一种从单词列表中过滤出停用词的方法。为了改善两个列表之间的单词匹配，我们以与从输入文件中提取的单词类似的方式处理停用词文件。我们读取包含停用词的文件，去除开头和结尾的空格，转换为小写，替换特殊字符，过滤掉空单词，最后创建一个包含停用词的DataFrame。
- en: We use the list of stop words available at [http://algs4.cs.princeton.edu/35applications/stopwords.txt](http://algs4.cs.princeton.edu/35applications/stopwords.txt)
    in our example.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在示例中使用的停用词列表可在[http://algs4.cs.princeton.edu/35applications/stopwords.txt](http://algs4.cs.princeton.edu/35applications/stopwords.txt)中找到。
- en: '![](img/00128.gif)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00128.gif)'
- en: Here, we use a `regex` to filter out special characters contained in the file.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`regex`来过滤文件中包含的特殊字符。
- en: '![](img/00129.gif)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00129.gif)'
- en: Next, we compare the number of words in our list before and after the removal
    of the stop words from our original list of words. The final number of words remaining
    suggests that a majority of words in our input files were stop words.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较在去除原始单词列表中的停用词之前和之后列表中单词的数量。剩下的最终单词数量表明我们输入文件中的大部分单词是停用词。
- en: '![](img/00130.gif)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00130.gif)'
- en: For a more detailed coverage of text data processing (of an annual `10-K` financial
    filing document and other document corpuses) including building pre-processing
    data pipelines, identifying themes in document corpuses, using Naïve Bayes classifiers,
    and developing a machine learning application, refer [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c), *Developing
    Applications with Spark SQL.*
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 有关文本数据处理的更详细覆盖范围（包括年度`10-K`财务申报文件和其他文档语料库的处理、识别文档语料库中的主题、使用朴素贝叶斯分类器和开发机器学习应用程序），请参阅[第9章](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c)，*使用Spark
    SQL开发应用程序*。
- en: In the next section, we shift our focus to munging time-series data using the
    `spark-time-series` library from Cloudera.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将把重点转移到使用Cloudera的`spark-time-series`库对时间序列数据进行整理。
- en: Munging time series data
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整理时间序列数据
- en: Time series data is a sequence of values linked to a timestamp. In this section,
    we use Cloudera's `spark-ts` package for analyzing time-series data.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据是与时间戳相关联的一系列值。在本节中，我们使用Cloudera的`spark-ts`包来分析时间序列数据。
- en: Refer to *Cloudera Engineering Blog*, *A New Library for Analyzing Time-Series
    Data with Apache Spark*, for more details on time-series data and its processing
    using `spark-ts`. This blog is available at: [https://github.com/sryza/spark-timeseries](https://github.com/sryza/spark-timeseries).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 有关时间序列数据及其使用`spark-ts`进行处理的更多详细信息，请参阅*Cloudera Engineering Blog*，*使用Apache Spark分析时间序列数据的新库*。该博客位于：[https://github.com/sryza/spark-timeseries](https://github.com/sryza/spark-timeseries)。
- en: 'The `spark-ts` package can be downloaded and built using instructions available
    at:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark-ts`包可以通过以下说明进行下载和构建：'
- en: '[https://github.com/sryza/spark-timeseries](https://github.com/sryza/spark-timeseries).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/sryza/spark-timeseries](https://github.com/sryza/spark-timeseries)。'
- en: 'We will attempt to accomplish the following objectives in the following sub-sections:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子部分中，我们将尝试实现以下目标：
- en: Pre-processing of the time-series Dataset
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理时间序列数据集
- en: Processing date fields
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理日期字段
- en: Persisting and loading data
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久化和加载数据
- en: Defining a date-time index
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义日期时间索引
- en: Using the  `TimeSeriesRDD` object
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`TimeSeriesRDD`对象
- en: Handling missing time-series data
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失的时间序列数据
- en: Computing basic statistics
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算基本统计数据
- en: 'For this section, specify inclusion of the `spark-ts.jar` file while starting
    the Spark shell as shown:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节，请在启动Spark shell时指定包含`spark-ts.jar`文件。
- en: '![](img/00131.gif)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00131.gif)'
- en: We download Datasets containing pricing and volume data for six stocks over
    a one year period from the Yahoo Finance site. We will need to pre-process the
    data before we can use the `spark-ts` package for time-series data analysis.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Yahoo Finance网站下载了包含六只股票一年期价格和成交量数据的数据集。在使用`spark-ts`包进行时间序列数据分析之前，我们需要对数据进行预处理。
- en: Import the classes required in this section.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 导入本节所需的类。
- en: '![](img/00132.gif)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00132.gif)'
- en: Pre-processing of the time-series Dataset
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理时间序列数据集
- en: Read the data from the input data files and define a `case` class Stock containing
    the fields in the Dataset plus a field to hold the ticker symbol.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入数据文件中读取数据，并定义一个包含数据集中字段的`case`类Stock，以及一个用于保存股票代码的字段。
- en: '![](img/00133.gif)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00133.gif)'
- en: Next, we remove the header from each of the files, map our RDD row using the
    `case` class, include a string for the ticker symbol, and convert the RDD to a
    DataFrame.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从每个文件中移除标题，使用`case`类映射我们的RDD行，包括一个用于股票代码的字符串，并将RDD转换为DataFrame。
- en: '![](img/00134.gif)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00134.gif)'
- en: Next, we combine the rows from each of our DataFrames using `union`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`union`将每个DataFrame的行合并起来。
- en: '![](img/00135.gif)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00135.gif)'
- en: Processing date fields
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理日期字段
- en: In the next step, we separate out the date column into three separate fields
    containing the day, month, and the year information.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将日期列分成包含日期、月份和年份信息的三个单独字段。
- en: '![](img/00136.gif)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00136.gif)'
- en: Persisting and loading data
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化和加载数据
- en: 'At this stage, we can persist our DataFrame to a CSV file using the `DataFrameWriter`
    class. The overwrite mode lets you overwrite the file, if it is already present
    from a previous execution of the `write` operation:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以使用`DataFrameWriter`类将我们的DataFrame持久化到CSV文件中。覆盖模式允许您覆盖文件，如果它已经存在于`write`操作的先前执行中：
- en: '![](img/00137.gif)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00137.gif)'
- en: 'For loading the time series Dataset written to disk in the previous step, we
    define a function to load our observations from a file and return a DataFrame:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载上一步写入磁盘的时间序列数据集，我们定义一个从文件加载观测值并返回DataFrame的函数：
- en: '![](img/00138.gif)![](img/00139.gif)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00138.gif)![](img/00139.gif)'
- en: Defining a date-time index
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义日期时间索引
- en: We define a date-time index for the period for which we have the data so that
    each record (for a specific ticker symbol) includes a time series represented
    as an array of `366` positions for each of the days in the year (plus one extra
    day as we have downloaded the data from 12/04/2015 to 12/04/2016). The Business
    Day Frequency specifies that the data is available for the business days of the
    year only.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为我们拥有数据的期间定义一个日期时间索引，以便每条记录（针对特定的股票代码）包括一个时间序列，表示为一年中每一天的`366`个位置的数组（加上额外的一天，因为我们已经从2015年12月4日下载了数据到2016年12月4日）。工作日频率指定数据仅适用于一年中的工作日。
- en: '![](img/00140.gif)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00140.gif)'
- en: Using the  TimeSeriesRDD object
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`TimeSeriesRDD`对象
- en: The main abstraction in the `spark-ts` library is a RDD called `TimeSeriesRDD`.
    The data is a set of observations represented as a tuple of (timestamp, key, value).
    The key is a label used to identify the time series. In the following example,
    our tuple is (timestamp, ticker, close). Each series in the RDD has the ticker
    symbol as the key and the daily closing price of the stock as the value.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark-ts`库中的主要抽象是称为`TimeSeriesRDD`的RDD。数据是一组以元组（时间戳、键、值）表示的观测值。键是用于标识时间序列的标签。在下面的示例中，我们的元组是（时间戳、股票代码、收盘价）。RDD中的每个系列都将股票代码作为键，将股票的每日收盘价作为值。'
- en: '[PRE1]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can cache and display the number of rows in the RDD which should be equal
    to the number of stocks in our example:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以缓存并显示RDD中的行数，这应该等于我们示例中的股票数量：
- en: '![](img/00141.gif)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00141.gif)'
- en: 'Display a couple of rows from the RDD to see the data in each row:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 显示RDD中的几行以查看每行中的数据：
- en: '![](img/00142.gif)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00142.gif)'
- en: Handling missing time-series data
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失的时间序列数据
- en: Next, we check the RDD for missing data. The missing data is marked with `NaN`
    values. Computing basic statistics with `NaN` values present will give errors.
    Hence, we need to replace these missing values with approximations. Our example
    data does not contain any missing fields. However, as an exercise, we delete a
    few values from the input datasets to simulate these `NaN` values in the RDD,
    and then impute these values using linear interpolation. Other approximations
    available include next, previous, and nearest values.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检查RDD中是否有缺失数据。缺失数据标记为`NaN`值。在存在`NaN`值的情况下计算基本统计数据会导致错误。因此，我们需要用近似值替换这些缺失值。我们的示例数据不包含任何缺失字段。但是，作为练习，我们从输入数据集中删除一些值，以模拟RDD中的这些`NaN`值，然后使用线性插值来填补这些值。其他可用的近似值包括下一个、上一个和最近的值。
- en: 'We fill in the approximate values for the missing values, as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们填写缺失值的近似值，如下所示：
- en: '![](img/00143.gif)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00143.gif)'
- en: Computing basic statistics
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算基本统计数据
- en: 'Finally, we compute the mean, standard deviation, max and min values for each
    of our series, as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算每个系列的均值、标准差、最大值和最小值，如下所示：
- en: '![](img/00144.gif)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00144.gif)'
- en: There are many other useful functions available for exploratory data analysis
    and data munging using the `TimeSeriesRDD` object. These include collecting the
    RDD as a local time series, finding specific time series, various filters and
    slicing functionality, sorting and re-partitioning the data, writing out the time
    series to CSV files, and many more.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`TimeSeriesRDD`对象进行探索性数据分析和数据整理还有许多其他有用的函数。这些包括将RDD收集为本地时间序列、查找特定时间序列、各种过滤和切片功能、对数据进行排序和重新分区、将时间序列写入CSV文件等等。
- en: Dealing with variable length records
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理可变长度记录
- en: In this section, we will explore a way of dealing with variable length records.
    Our approach essentially converts each of the rows to a fixed length record equal
    to the maximum length record. In our example, as each row represents a portfolio
    and there is no unique identifier, this method is useful for manipulating data
    into the familiar fixed length records case. We will generate the requisite number
    of fields to equal the maximum number of stocks in the largest portfolio. This
    will lead to empty fields where the number of stocks is less than the maximum
    number of stocks in any portfolio. Another way to deal with variable length records
    is to use the `explode()` function to create new rows for each stock in a given
    portfolio (for an example of using the `explode()` function, refer [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c), *Developing
    Applications with Spark SQL).*
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将探讨处理可变长度记录的方法。我们的方法基本上将每一行转换为等于最大长度记录的固定长度记录。在我们的例子中，由于每行代表一个投资组合并且没有唯一标识符，这种方法对将数据转换为熟悉的固定长度记录情况非常有用。我们将生成所需数量的字段，使其等于最大投资组合中的股票数量。这将导致在股票数量少于任何投资组合中的最大股票数量时出现空字段。处理可变长度记录的另一种方法是使用`explode()`函数为给定投资组合中的每支股票创建新行（有关使用`explode()`函数的示例，请参阅[第9章](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c)，*使用Spark
    SQL开发应用程序)。*
- en: To avoid repeating all the steps from previous examples to read in all the files,
    we have combined the data into a single input file in this example.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免重复之前示例中的所有步骤来读取所有文件，我们在本例中将数据合并为一个单独的输入文件。
- en: 'First, we import the classes required and read the input file into an RDD:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的类并将输入文件读入RDD：
- en: '![](img/00145.gif)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00145.gif)'
- en: We count the total number of portfolios and print a few records from the RDD.
    You can see that while the first and the second portfolios contain one stock each,
    the third one contains two stocks.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算投资组合的总数，并打印RDD中的一些记录。您可以看到，第一个和第二个投资组合各包含一支股票，而第三个投资组合包含两支股票。
- en: '![](img/00146.gif)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00146.gif)'
- en: Converting variable-length records to fixed-length records
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将可变长度记录转换为固定长度记录
- en: In our example Dataset, there are no fields missing, hence, we can use the number
    of commas in each row to derive the varying number stock-related fields in each
    of the portfolios. Alternatively, this information can be extracted from the strings
    contained in last field of the RDD.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例数据集中，没有缺失的字段，因此，我们可以使用每行逗号的数量来推导出每个投资组合中不同数量的股票相关字段。或者，这些信息可以从RDD的最后一个字段中提取出来。
- en: Next, we create a UDF to count the number of stocks indirectly by counting the
    number of commas in each row. We use `describe` to find the maximum number of
    commas across all rows in the dataset.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个UDF来间接计算每行中逗号的数量，通过计算数据集中所有行中逗号的最大数量来使用`describe`。
- en: '![](img/00147.gif)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00147.gif)'
- en: In the next step, we augment the DataFrame with a column containing the number
    of commas.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们用一个包含逗号数量的列来增加DataFrame。
- en: '![](img/00148.gif)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00148.gif)'
- en: 'Then we write a function to insert the correct number of commas in each row
    at the appropriate location:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们编写一个函数，在适当的位置插入每行中正确数量的逗号：
- en: '![](img/00149.gif)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00149.gif)'
- en: 'Next, we drop the number of commas column, as it is not required in the subsequent
    steps:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们去掉逗号数量列，因为在后续步骤中不需要它：
- en: '![](img/00150.gif)![](img/00151.gif)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00150.gif)![](img/00151.gif)'
- en: 'At this stage, if you want to get rid of duplicate rows in the DataFrame, then
    you can use the `dropDuplicates` method shown as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，如果你想要去掉DataFrame中的重复行，那么你可以使用`dropDuplicates`方法，如下所示：
- en: '![](img/00152.gif)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00152.gif)'
- en: In the next step, we define a `case` class for the `Portfolio` with the maximum
    number of stocks in the largest portfolio.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们为最大投资组合中的最大股票数定义一个`Portfolio`的`case`类。
- en: '![](img/00153.gif)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00153.gif)'
- en: 'Next, we convert the RDD into a DataFrame. For convenience, we will demonstrate
    the operations using fewer stock-related columns; however, the same can be extended
    to fields for other stocks in the portfolio:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将RDD转换为DataFrame。为了方便起见，我们将演示使用较少的与股票相关的列进行操作；然而，同样的操作可以扩展到投资组合中其他股票的字段：
- en: '![](img/00154.gif)![](img/00155.gif)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00154.gif)![](img/00155.gif)'
- en: 'We can replace empty fields for stocks in the smaller portfolios with `NA` , as
    follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用`NA`替换较小投资组合中股票的空字段，如下所示：
- en: '![](img/00156.gif)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00156.gif)'
- en: Extracting data from "messy" columns
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从“混乱”的列中提取数据
- en: In this section, we continue on from the previous section, however, we will
    work with a single stock to demonstrate the data manipulations required to modify
    the data fields to a state where we end up with cleaner and richer data than we
    started with.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将继续上一节的工作，但是我们将只处理一个股票，以演示修改数据字段所需的数据操作，使得最终得到的数据比起开始时更加干净和丰富。
- en: 'As most of the fields contain several pieces of information, we will execute
    a series of statements to separate them out into their own independent columns:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数字段包含多个信息，我们将执行一系列语句，将它们分开成独立的列：
- en: '![](img/00157.gif)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00157.gif)'
- en: 'In the next step, we remove the first underscore with a space in the `datestr`
    column. This results in separating out the date field:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将`datestr`列中的第一个下划线替换为一个空格。这样就分离出了日期字段：
- en: '![](img/00158.gif)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00158.gif)'
- en: 'Next, we separate out the information in the stock column, as it contains several
    pieces of useful information including the ticker symbol, ratio of the selling
    price and purchase price, and the selling price and purchase price. First, we
    get rid of the `=` in the stock column by replacing it with an empty string:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们分离股票列中的信息，因为它包含了几个有用的信息，包括股票代码、卖出价格和购买价格的比率，以及卖出价格和购买价格。首先，我们通过用空字符串替换股票列中的`=`来去掉`=`：
- en: '![](img/00159.gif)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00159.gif)'
- en: 'Next, the values in each column separated by spaces in each column are converted
    into an array of the values:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将每列中由空格分隔的值转换为值的数组：
- en: '![](img/00160.gif)![](img/00161.gif)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00160.gif)![](img/00161.gif)'
- en: Next, we use a `UDF` to pick certain elements from the arrays in each column
    into their own separate columns.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`UDF`从每列的数组中挑选出特定元素，放到它们自己的独立列中。
- en: '![](img/00162.gif)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00162.gif)'
- en: 'The file column is not particularly useful for our analysis, except for extracting
    the information at the beginning of the filename that denotes the pool of stocks
    from which the stocks for any given portfolio were picked. We do that next, as
    follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 文件列对我们的分析来说并不特别有用，除了提取文件名开头的信息，表示任何给定投资组合的股票池。我们接下来就这样做：
- en: '![](img/00163.gif)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00163.gif)'
- en: The following is the final version of the DataFrame that is ready for further
    analysis. In this example, we have worked with a single stock however you can
    easily extend the same techniques to all stocks in any given portfolio to arrive
    at the final, clean and rich, DataFrame ready for querying, modeling, and analysis.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是准备进行进一步分析的DataFrame的最终版本。在这个例子中，我们只处理了一个股票，但是你可以很容易地将相同的技术扩展到给定投资组合中的所有股票，得到最终的、干净且丰富的DataFrame，可以用于查询、建模和分析。
- en: '![](img/00164.gif)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00164.gif)'
- en: In the next section, we briefly introduce the steps required for preparing data
    for use with Spark MLlib machine learning algorithms for classification problems.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们简要介绍了为了使用Spark MLlib机器学习算法解决分类问题而准备数据所需的步骤。
- en: Preparing data for machine learning
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为机器学习准备数据
- en: 'In this section, we introduce the process of preparing the input data prior
    to applying Spark MLlib algorithms. Typically, we need to have two columns called
    label and features for using Spark MLlib classification algorithms. We will illustrate
    this with the following example described:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们介绍了在应用Spark MLlib算法之前准备输入数据的过程。通常情况下，我们需要有两列，称为标签和特征，用于使用Spark MLlib分类算法。我们将用下面描述的例子来说明这一点：
- en: 'We import the required classes for this section:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入了本节所需的类：
- en: '[PRE2]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Pre-processing data for machine learning
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为机器学习预处理数据
- en: We define a set of `UDFs` used in this section. These include, for example,
    checking whether a string contains a specific substring or not, and returning
    a `0.0` or `1.0` value to create the label column. Another `UDF` is used to create
    a features vector from the numeric fields in the DataFrame.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中定义了一组在本节中使用的`UDF`。这些包括，例如，检查字符串是否包含特定子字符串，并返回`0.0`或`1.0`值以创建标签列。另一个`UDF`用于从DataFrame中的数字字段创建特征向量。
- en: 'For example, we can convert the day of week field to a numeric value by binning
    shown as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以通过以下方式将星期几字段转换为数字值进行分箱显示：
- en: '![](img/00165.gif)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00165.gif)'
- en: In our example, we create a `label` from the `Events` column of the household
    electric consumption Dataset based on whether it rained on given a day or not.
    For illustrative purposes, we use the columns from the household's electric power
    consumption readings in the joined DataFrame from before, even though readings
    from weather Dataset are probably a better predictor of rain.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们根据某一天是否下雨，从家庭电力消耗数据集的`Events`列中创建一个`label`。为了说明的目的，我们使用了之前连接的DataFrame中的家庭电力消耗读数的列，尽管来自天气数据集的读数可能更好地预测雨水。
- en: '![](img/00166.gif)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00166.gif)'
- en: Finally, we can also split our DataFrame to create training and test Datasets
    containing 70% and 30% of the readings, chosen randomly, respectively. These Datasets
    are used for training and testing machine learning algorithms.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还可以将DataFrame拆分，创建包含随机选择的70%和30%读数的训练和测试数据集。这些数据集用于训练和测试机器学习算法。
- en: '![](img/00167.gif)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00167.gif)'
- en: Creating and running a machine learning pipeline
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建和运行机器学习管道
- en: In this section, we present an example of a machine learning pipeline that uses
    the indexers and the training data to train a Random Forest model. We will not
    present detailed explanations for the steps, as our primary purpose here is to
    only demonstrate how the preparatory steps in the previous section are actually
    used.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一个使用索引器和训练数据来训练随机森林模型的机器学习管道的示例。我们不会对步骤进行详细解释，因为我们在这里的主要目的是演示前一节中的准备步骤实际上是如何使用的。
- en: '[PRE3]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/00168.jpeg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00168.jpeg)'
- en: '[PRE4]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](img/00169.jpeg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00169.jpeg)'
- en: More details on specific data structures and operations including vectors, processing
    categorical variables, and so on, for Spark MLlib processing, are covered in [Chapter
    6](part0103.html#3279U0-e9cbc07f866e437b8aa14e841622275c), **Using Spark SQL in
    Machine Learning Applications,** and [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c),
    *Developing Applications with Spark SQL*. Additionally, techniques for preparing
    data for graph applications are presented in [Chapter 7](part0134.html#3VPBC0-e9cbc07f866e437b8aa14e841622275c),
    **Using Spark SQL in Graph Applications**.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 有关特定数据结构和操作的更多详细信息，包括向量、处理分类变量等等，用于Spark MLlib处理的内容，将在[第6章](part0103.html#3279U0-e9cbc07f866e437b8aa14e841622275c)中进行介绍，**在机器学习应用中使用Spark
    SQL**，以及[第9章](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c)中进行介绍，*使用Spark
    SQL开发应用程序*。此外，有关为图应用程序准备数据的技术将在[第7章](part0134.html#3VPBC0-e9cbc07f866e437b8aa14e841622275c)中进行介绍，**在图应用程序中使用Spark
    SQL**。
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored using Spark SQL for performing some basic data
    munging/wrangling tasks. We covered munging textual data, working with variable
    length records, extracting data from "messy" columns, combining data using JOIN,
    and preparing data for machine learning applications. In addition, we used `spark-ts`
    library to work with time-series data.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用Spark SQL执行一些基本的数据整理/处理任务。我们涵盖了整理文本数据，处理可变长度记录，从“混乱”的列中提取数据，使用JOIN组合数据，并为机器学习应用程序准备数据。此外，我们使用了`spark-ts`库来处理时间序列数据。
- en: In the next chapter, we will shift our focus to Spark Streaming applications.
    We will introduce you to using Spark SQL in such applications. We will also include
    extensive hands-on sessions for demonstrating the use of Spark SQL in implementing
    the common use cases in Spark Streaming applications.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把重点转向Spark Streaming应用程序。我们将介绍如何在这些应用程序中使用Spark SQL。我们还将包括大量的实践课程，演示在Spark
    Streaming应用程序中实现常见用例时如何使用Spark SQL。
