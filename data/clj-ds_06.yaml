- en: Chapter 6. Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 聚类
- en: '|   | *Things that have a common quality ever quickly seek their kind.* |  
    |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *具有共同特质的事物总会很快地寻找到它们自己的同类。* |   |'
- en: '|   | --*Marcus Aurelius* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*马库斯·奥勒留* |'
- en: 'In previous chapters, we covered multiple learning algorithms: linear and logistic
    regression, C4.5, naive Bayes, and random forests. In each case we were required
    to train the algorithm by providing features and a desired output. In linear regression,
    for example, the desired output was the weight of an Olympic swimmer, whereas
    for the other algorithms we provided a class: whether the passenger survived or
    perished. These are examples of **supervised learning algorithms**: we tell our
    algorithm the desired output and it will attempt to learn a model that reproduces
    it.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们涵盖了多个学习算法：线性回归和逻辑回归，C4.5，朴素贝叶斯和随机森林。在每种情况下，我们需要通过提供特征和期望输出来训练算法。例如，在线性回归中，期望输出是奥运游泳选手的体重，而对于其他算法，我们提供了一个类别：乘客是否生还。这些是**监督学习算法**的示例：我们告诉算法期望的输出，它将尝试学习生成相似的模型。
- en: There is another class of learning algorithm referred to as **unsupervised learning**.
    Unsupervised algorithms are able to operate on the data without a set of reference
    answers. We may not even know ourselves what structure lies within the data; the
    algorithm will attempt to determine the structure for itself.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一类学习算法称为**无监督学习**。无监督算法能够在没有参考答案集的情况下操作数据。我们甚至可能不知道数据内部的结构；算法将尝试自行确定这种结构。
- en: 'Clustering is an example of an unsupervised learning algorithm. The results
    of cluster analysis are groupings of input data that are more similar to each
    other in some way. The technique is general: any set entities that have a conceptual
    similarity or distance from each other can be clustered. For example, we could
    cluster groups of social media accounts by similarity in terms of shared followers,
    or we could cluster the results of market research by measuring the similarity
    of respondents'' answers to a questionnaire.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是无监督学习算法的一个例子。聚类分析的结果是输入数据的分组，这些数据在某种方式上更相似于彼此。该技术是通用的：任何具有概念上相似性或距离的实体都可以进行聚类。例如，我们可以通过共享粉丝的相似性来聚类社交媒体账户组，或者通过测量受访者问卷答案的相似性来聚类市场调研的结果。
- en: One common application of clustering is to identify documents that share similar
    subject matter. This provides us with an ideal opportunity to talk about text
    processing, and this chapter will introduce a variety of techniques specific to
    dealing with text.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的一个常见应用是识别具有相似主题的文档。这为我们提供了一个理想的机会来讨论文本处理，本章将介绍一系列处理文本的特定技术。
- en: Downloading the data
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载数据
- en: 'This chapter makes use of the **Reuters-21578** dataset: a venerable collection
    of articles that were published on the Reuters newswire in 1987\. It is one of
    the most widely used for testing the categorization and classification of text.
    The copyright for the text of articles and annotations in the Reuters-21578 collection
    resides with Reuters Ltd. Reuters Ltd. and Carnegie Group, Inc. have agreed to
    allow the free distribution of this data for research purposes only.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用**Reuters-21578**数据集：这是一组在1987年通过路透社新闻线发布的文章。它是最广泛用于测试文本分类和分类的数据集之一。文章文本和Reuters-21578集合中的注释版权归Reuters
    Ltd.所有。Reuters Ltd.和Carnegie Group, Inc.已同意仅为研究目的免费分发这些数据。
- en: Note
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You can download the example code for this chapter from the Packt Publishing's
    website or from [https://github.com/clojuredatascience/ch6-clustering](https://github.com/clojuredatascience/ch6-clustering).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从Packt Publishing的网站或[https://github.com/clojuredatascience/ch6-clustering](https://github.com/clojuredatascience/ch6-clustering)下载本章的示例代码。
- en: 'As usual, within the sample code is a script to download and unzip the files
    to the data directory. You can run it from within the project directory with the
    following command:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，在示例代码中有一个脚本用于下载并解压文件到数据目录。您可以在项目目录中使用以下命令运行它：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Alternatively, at the time of writing, the Reuters dataset can be downloaded
    from [http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz](http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz).
    The rest of this chapter will assume that the files have been downloaded and installed
    to the project's data directory.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，在写作时，Reuters数据集可以从[http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz](http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz)下载。本章的其余部分将假设文件已被下载并安装到项目的`data`目录中。
- en: Extracting the data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取数据
- en: After you run the preceding script, the articles will be unzipped to the directory
    `data/reuters-sgml`. Each `.sgm` file in the extract contains around 1,000 short
    articles that have been wrapped in XML-style tags using Standard Generalized Markup
    Language (SGML). Rather than write our own parser for the format, we can make
    use of the one already written in the Lucene text indexer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的脚本后，文章将被解压到目录`data/reuters-sgml`中。每个`.sgm`文件包含大约1,000篇短文章，这些文章使用标准通用标记语言（SGML）被包装在XML样式的标签中。我们不需要自己编写解析器，而是可以利用已经在Lucene文本索引器中写好的解析器。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here we're making use of Clojure's Java interop to simply call the extract method
    on Lucene's `ExtractReuters` class. Each article is extracted as its own text
    file.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们利用Clojure的Java互操作性，简单地调用Lucene的`ExtractReuters`类中的提取方法。每篇文章都被提取为一个独立的文本文件。
- en: 'This code can be run by executing:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过执行以下代码运行：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: on the command line within the project directory. The output will be a new directory,
    `data/reuters-text`, containing over 20,000 individual text files. Each file contains
    a single Reuters newswire article.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目目录内的命令行中运行。输出将是一个新目录`data/reuters-text`，其中包含超过20,000个独立的文本文件。每个文件包含一篇单独的路透社新闻文章。
- en: 'If you''re short on disk space you can delete the `reuters-sgml` and `reuters21578.tar.gz`
    files now: the contents of the `reuters-text` directory are the only files we
    will be using in this chapter. Let''s look at a few now.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果磁盘空间不足，现在可以删除`reuters-sgml`和`reuters21578.tar.gz`文件：`reuters-text`目录中的内容是本章中我们唯一会使用的文件。现在让我们看看其中的几个文件。
- en: Inspecting the data
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查数据
- en: 'The year 1987 was the year of "Black Monday". On 19th October stock markets
    around the world crashed and the Dow Jones Industrial Average declined 508 points
    to 1738.74\. Articles such as the one contained in `reut2-020.sgm-962.txt` describe
    the event:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 1987年是“黑色星期一”那一年。10月19日，全球股市暴跌，道琼斯工业平均指数下降了508点，降至1738.74。像`reut2-020.sgm-962.txt`中包含的文章描述了这一事件：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The structure of this article is representative of the majority of articles
    in the corpus. The first line is the timestamp indicating when the article was
    published, followed by a blank line. The article has a headline which is often—but
    not always—in upper case, and then another blank line. Finally comes the article
    body text. As is often the case when working with semi-structured text such as
    this, there are multiple spaces, odd characters, and abbreviations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章的结构代表了语料库中大多数文章的结构。第一行是时间戳，表示文章的发布时间，后面是一个空行。文章有一个标题，通常——但并非总是——是大写字母，然后是另一个空行。最后是文章正文文本。与处理此类半结构化文本时常见的情况一样，文章中存在多个空格、奇怪的字符和缩写。
- en: 'Other articles are simply headlines, for example in `reut2-020.sgm-761.txt`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其他文章只是标题，例如在`reut2-020.sgm-761.txt`中：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These are the files on which we will be performing our cluster analysis.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文件是我们将进行聚类分析的对象。
- en: Clustering text
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类文本
- en: Clustering is the process of finding groups of objects that are similar to each
    other. The goal is that objects within a cluster should be more similar to each
    other than to objects in other clusters. Like classification, it is not a specific
    algorithm so much as a general class of algorithms that solve a general problem.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是找到彼此相似的对象组的过程。目标是，簇内的对象应该比簇外的对象更加相似。与分类一样，聚类并不是一个特定的算法，而是解决一般问题的算法类别。
- en: 'Although there are a variety of clustering algorithms, all rely to some extent
    on a distance measure. For an algorithm to determine whether two objects belong
    in the same or different clusters it must be able to determine a quantitative
    measure of the distance (or, if you prefer, the similarity) between them. This
    calls for a numeric measure of distance: the smaller the distance, the greater
    the similarity between two objects.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有多种聚类算法，但它们都在某种程度上依赖于距离度量。为了让算法判断两个对象是否属于同一簇或不同簇，它必须能够确定它们之间的距离（或者，如果你愿意，可以认为是相似度）的定量度量。这就需要一个数字化的距离度量：距离越小，两个对象之间的相似度就越大。
- en: 'Since clustering is a general technique that can be applied to diverse data
    types, there are a large number of possible distance measures. Nonetheless, most
    data can be represented by one of a handful of common abstractions: a set, a point
    in space, or a vector. For each of these there exists a commonly-used measure.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于聚类是一种可以应用于多种数据类型的通用技术，因此有大量可能的距离度量方法。尽管如此，大多数数据都可以通过一些常见的抽象表示：集合、空间中的点或向量。对于这些，每种情况都有一种常用的度量方法。
- en: Set-of-words and the Jaccard index
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单词集合与 Jaccard 指数
- en: 'If your data can be represented as a set of things the Jaccard index, also
    known as the **Jaccard similarity**, can be used. It''s one of the simplest measures
    conceptually: it is the set intersection divided by the set union, or the number
    of shared elements in common out of the total unique elements in the sets:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据可以表示为事物的集合，则可以使用 Jaccard 指数，也称为**Jaccard 相似度**。它在概念上是最简单的度量之一：它是集合交集除以集合并集，或者是集合中共同元素的数量占总独特元素数量的比例：
- en: '![Set-of-words and the Jaccard index](img/7180OS_06_01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![单词集合与 Jaccard 指数](img/7180OS_06_01.jpg)'
- en: Many things can be represented as sets. Accounts on social networks can be represented
    as sets of friends or followers, and customers can be represented as sets of products
    purchased or viewed. For our text documents, a set representation could simply
    be the set of unique words used.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 许多事物可以表示为集合。社交网络上的账户可以表示为朋友或关注者的集合，顾客可以表示为购买或查看过的商品集合。对于我们的文本文件，集合表示可以简单地是使用的唯一单词的集合。
- en: '![Set-of-words and the Jaccard index](img/7180OS_06_100.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![单词集合与 Jaccard 指数](img/7180OS_06_100.jpg)'
- en: 'The Jaccard index is very simple to calculate in Clojure:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard 指数在 Clojure 中非常简单计算：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It has the advantage that the sets don't have to be of the same cardinality
    for the distance measure to make sense. In the preceding diagram, **A** is "larger"
    than **B**, yet the intersection divided by the union is still a fair reflection
    of their similarity. To apply the Jaccard index to text documents, we need to
    translate them into sets of words. This is the process of **tokenization**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 它的优点是，即使集合的基数不同，距离度量依然有意义。在前面的图示中，**A**“比”**B**要“大”，但交集除以并集仍然能公平地反映它们的相似度。要将
    Jaccard 指数应用于文本文件，我们需要将它们转化为单词集合。这就是**词元化**的过程。
- en: Tokenizing the Reuters files
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词元化 Reuters 文件
- en: Tokenization is the name for the technique of taking a string of text and splitting
    it into smaller units for the purpose of analysis. A common approach is to split
    a text string into individual words. An obvious separator would be whitespace
    so that `"tokens like these"` become `["tokens" "like" "these"]`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 词元化是将一串文本拆分成更小的单元以便进行分析的技术名称。常见的方法是将文本字符串拆分成单个单词。一个明显的分隔符是空格，因此 `"tokens like
    these"` 会变成 `["tokens" "like" "these"]`。
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is convenient and simple, but unfortunately, language is subtle and few
    simple rules can be applied universally. For example, our tokenizer treats apostrophes
    as whitespace:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这既方便又简单，但不幸的是，语言是微妙的，很少有简单的规则可以普遍适用。例如，我们的词元化器将撇号视为空格：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Hyphens are treated as whitespace too:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 连字符也被视为空格：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'and removing them rather changes the meaning of the sentence. However, not
    all hyphens should be preserved:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 而删除它们则会改变句子的含义。然而，并非所有的连字符都应该被保留：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The terms `"New"`, `"York"`, and `"based"` correctly represent the subject of
    the phrase, but it would be preferable to group `"New York"` into a single term,
    since it represents a specific name and really ought to be preserved intact. `York-based`,
    on the other hand, would be a meaningless token on its own.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`"New"`、`"York"` 和 `"based"` 正确地表示了短语的主体，但最好将 `"New York"` 归为一个词，因为它代表了一个特定的名称，应该完整保留。另一方面，`York-based`
    单独作为词元没有意义。'
- en: In short, text is messy, and parsing meaning reliably from free text is an extremely
    rich and active area of research. In particular, for extracting names (e.g. `"New
    York"`) from text, we need to consider the context in which the terms are used.
    Techniques that label tokens within a sentence by their grammatical function are
    called **parts-of-speech taggers**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，文本是杂乱的，从自由文本中可靠地解析出意义是一个极其丰富和活跃的研究领域。特别是，对于从文本中提取名称（例如，“纽约”），我们需要考虑术语使用的上下文。通过其语法功能标注句子中词汇的技术被称为**词性标注器**。
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on advanced tokenization and parts-of-speech tagging, see
    the `clojure-opennlp` library at [https://github.com/dakrone/clojure-opennlp](https://github.com/dakrone/clojure-opennlp).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于高级分词和词性标注的信息，请参见`clojure-opennlp`库：[https://github.com/dakrone/clojure-opennlp](https://github.com/dakrone/clojure-opennlp)。
- en: In this chapter we have the luxury of a large quantity of documents and so we'll
    continue to use our simple tokenizer. We'll find that—in spite of its deficiencies—it
    will perform well enough to extract meaning from the documents.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们有足够多的文档可以使用，因此我们将继续使用我们的简单分词器。我们会发现，尽管它存在一些缺点，它仍然能够足够好地从文档中提取意义。
- en: 'Let''s write a function to return the tokens for a document from its file name:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个函数，根据文档的文件名返回文档的标记：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We're removing the timestamp from the top of the file and making the text lower-case
    before tokenizing. In the next section, we'll see how to measure the similarity
    between tokenized documents.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在去除文件顶部的时间戳，并在分词之前将文本转换为小写。在下一部分，我们将看到如何衡量分词后的文档相似性。
- en: Applying the Jaccard index to documents
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用Jaccard指数到文档上
- en: 'Having tokenized our input documents, we can simply pass the resulting sequence
    of tokens to our `jaccard-similarity` function defined previously. Let''s compare
    the similarity of a couple of documents from the Reuters corpus:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在对输入文档进行分词之后，我们可以将结果的标记序列简单地传递给我们之前定义的`jaccard-similarity`函数。让我们比较一下来自路透社语料库的几篇文档相似性：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The Jaccard index outputs a number between zero and one, so it has judged these
    documents to be 25 percent similar based on the words in their headlines. Notice
    how we''ve lost the order of the words in the headline. Without further tricks
    that we''ll come to shortly, the Jaccard index looks only at the items in common
    between two sets. Another aspect we''ve lost is the number of times a term occurs
    in the document. A document that repeats the same word many times may in some
    sense regard that word as more important. For example, `reut2-020.sgm-932.txt`
    has a headline like this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard指数输出一个介于零和一之间的数字，因此它认为这两篇文档在标题中的词汇相似度为25%。注意我们丢失了标题中词汇的顺序。没有我们稍后会讲到的其他技巧，Jaccard指数只关注两个集合中共同的项目。我们丢失的另一个方面是一个词在文档中出现的次数。重复出现同一个词的文档，可能会在某种意义上视该词为更重要。例如，`reut2-020.sgm-932.txt`的标题如下：
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: NYSE appears twice in the headline. We could infer that this headline is especially
    about the New York Stock Exchange, perhaps more so than a headline that mentioned
    NYSE only once.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: NYSE在标题中出现了两次。我们可以推测这个标题特别关注纽约证券交易所，可能比只提到一次NYSE的标题更为重要。
- en: The bag-of-words and Euclidean distance
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词袋模型和欧几里得距离
- en: A possible improvement over the set-of-words approach is the **bag-of-words
    approach**. This preserves the word count of the terms within the document. The
    term count can be incorporated by distance measures for a potentially more accurate
    result.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能优于词集方法的改进是**词袋模型方法**。这种方法保留了文档中各个词汇的词频。通过距离度量可以将词频纳入考虑，从而可能得到更准确的结果。
- en: 'One of the most common conceptions of distance is the Euclidean distance measure.
    In geometry, the Euclidean measure is how we calculate the distance between two
    points in space. In two dimensions, the Euclidean distance is given by the **Pythagoras
    formula**:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的距离概念之一是欧几里得距离度量。在几何学中，欧几里得度量是我们计算空间中两点之间距离的方式。在二维空间中，欧几里得距离由**毕达哥拉斯公式**给出：
- en: '![The bag-of-words and Euclidean distance](img/7180OS_06_02.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![词袋模型和欧几里得距离](img/7180OS_06_02.jpg)'
- en: This represents the difference between two points as the length of the straight-line
    distance between them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示两个点之间的差异是它们之间直线距离的长度。
- en: '![The bag-of-words and Euclidean distance](img/7180OS_06_110.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![词袋模型和欧几里得距离](img/7180OS_06_110.jpg)'
- en: 'This can be extended to three dimensions:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以扩展到三维：
- en: '![The bag-of-words and Euclidean distance](img/7180OS_06_03.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![词袋模型和欧几里得距离](img/7180OS_06_03.jpg)'
- en: 'and generalized to *n* dimensions:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 并且推广到 *n* 维度：
- en: '![The bag-of-words and Euclidean distance](img/7180OS_06_04.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![词袋模型和欧几里得距离](img/7180OS_06_04.jpg)'
- en: where *A*[i] and *B*[i] are the values of *A* or *B* at dimension *i*. The distance
    measure is thus the overall similarity between two documents, having taken into
    account how many times each word occurs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *A*[i] 和 *B*[i] 是在维度 *i* 上 *A* 或 *B* 的值。因此，距离度量就是两个文档之间的整体相似性，考虑了每个单词出现的频率。
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Since each word now represents a dimension in space, we need to make sure that
    when we calculate the Euclidean distance measure we are comparing the magnitude
    in the same dimension of each document. Otherwise, we may literally be comparing
    "apples" with "oranges".
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个单词现在代表空间中的一个维度，我们需要确保在计算欧几里得距离时，我们是在比较每个文档中同一维度上的大小。否则，我们可能会字面意义上比较“苹果”和“橙子”。
- en: Representing text as vectors
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将文本表示为向量
- en: Unlike the Jaccard index, the Euclidean distance relies on a consistent ordering
    of words into dimensions. The word count, or term frequency, represents the position
    of that document in a large multi-dimensional space, and we need to ensure that
    when we compare values we do so in the correct dimension. Let's represent our
    documents as term **frequency vectors**.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与Jaccard指数不同，欧几里得距离依赖于将单词一致地排序为各个维度。词频（term frequency）表示文档在一个大型多维空间中的位置，我们需要确保在比较值时，比较的是正确维度中的值。我们将文档表示为术语**频率向量**。
- en: Imagine all the words that could appear in a document being given a unique number.
    For example, the word "apple" could be assigned the number 53, the word "orange"
    could be assigned the number 21,597\. If all numbers are unique, they could correspond
    to the index that a word appears in a term vector.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，文档中可能出现的所有单词都被赋予一个唯一的编号。例如，单词“apple”可以被赋值为53，单词“orange”可以被赋值为21,597。如果所有数字都是唯一的，它们可以对应于单词在术语向量中出现的索引。
- en: The dimension of these vectors can be very large. The maximum number of dimensions
    possible is the cardinality of the vector. The value of the element at the index
    corresponding to a word is usually the number of occurrences of the word in the
    document. This is known as the **term frequency** (**tf**), weighting.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量的维度可能非常大。最大维度数是向量的基数。对应于单词的索引位置的元素值通常是该单词在文档中出现的次数。这被称为**术语频率**（**tf**）加权。
- en: In order to be able to compare text vectors it's important that the same word
    always appears at the same index in the vector. This means that we must use the
    same word/index mapping for each vector that we create. This word/index mapping
    is our dictionary.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够比较文本向量，重要的是相同的单词始终出现在向量中的相同索引位置。这意味着我们必须为每个创建的向量使用相同的单词/索引映射。这个单词/索引映射就是我们的词典。
- en: Creating a dictionary
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建词典
- en: 'To create a valid dictionary, we need to make sure that the indexes for two
    words don''t clash. One way to do this is to have a monotonically increasing counter
    which is incremented for each word added to the dictionary. The count at the point
    the word is added becomes the index of the word. To both add a word to the dictionary
    and increment a counter in a thread-safe way, we can use an atom:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个有效的词典，我们需要确保两个单词的索引不会冲突。做到这一点的一种方法是使用一个单调递增的计数器，每添加一个单词到词典中，计数器就增加一次。单词被添加时的计数值将成为该单词的索引。为了线程安全地同时将单词添加到词典并递增计数器，我们可以使用原子操作：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: To perform an update to an atom, we have to execute our code in a `swap!` function.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对原子进行更新，我们必须在`swap!`函数中执行我们的代码。
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Adding another word will cause the count to increase:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 添加另一个单词将导致计数增加：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'And adding the same word twice will have no effect:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 并且重复添加相同单词不会产生任何效果：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Performing this update inside an atom ensures that each word gets its own index
    even when the dictionary is being simultaneously updated by multiple threads.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在原子操作中执行此更新可以确保即使在多个线程同时更新词典时，每个单词也能获得自己的索引。
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Building a whole dictionary is as simple as reducing our `add-term-to-dict!`
    function over a supplied dictionary atom with a collection of terms.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 构建整个词典就像在提供的词典原子上使用`add-term-to-dict!`函数并对一组术语进行简化操作一样简单。
- en: Creating term frequency vectors
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建术语频率向量
- en: To calculate the Euclidean distance, let's first create a vector from our dictionary
    and document. This will allow us to easily compare the term frequencies between
    documents because they will occupy the same index of the vector.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算欧几里得距离，首先让我们从字典和文档中创建一个向量。这将使我们能够轻松地比较文档之间的术语频率，因为它们将占据向量的相同索引。
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `term-frequencies` function creates a map of term ID to frequency count
    for each term in the document. The `map->vector` function simply takes this map
    and associates the frequency count at the index of the vector given by the term
    ID. Since there may be many terms, and the vector may be very long, we're using
    Clojure's `transient!` and `persistent!` functions to temporarily create a mutable
    vector for efficiency.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`term-frequencies` 函数为文档中的每个术语创建一个术语 ID 到频率计数的映射。`map->vector` 函数简单地接受此映射，并在由术语
    ID 给出的向量索引位置关联频率计数。由于可能有许多术语，且向量可能非常长，因此我们使用 Clojure 的 `transient!` 和 `persistent!`
    函数暂时创建一个可变向量以提高效率。'
- en: 'Let''s print the document, dictionary, and resulting vector for `reut2-020.sgm-742.txt`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印 `reut2-020.sgm-742.txt` 的文档、字典和生成的向量：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is shown as follows (the formatting has been adjusted for legibility):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示（格式已调整以提高可读性）：
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: With 12 terms in the input, there are 12 terms in the dictionary and a vector
    of 12 elements returned.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输入中有 12 个术语，字典中有 12 个术语，并返回了一个包含 12 个元素的向量。
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Like the Jaccard index, the Euclidean distance cannot decrease below zero. Unlike
    the Jaccard index, though, the value can grow indefinitely.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Jaccard 指数类似，欧几里得距离不能低于零。然而，不同于 Jaccard 指数，欧几里得距离的值可以无限增长。
- en: The vector space model and cosine distance
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量空间模型与余弦距离
- en: The vector space model can be considered a generalization of the set-of-words
    and bag-of-words models. Like the bag-of-words model, the vector space model represents
    each document as a vector, each element of which represents a term. The value
    at each index is a measure of importance of the word, which may or may not be
    the term frequency.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 向量空间模型可以视为词集（set-of-words）和词袋（bag-of-words）模型的推广。与词袋模型类似，向量空间模型将每个文档表示为一个向量，每个元素表示一个术语。每个索引位置的值是该词的重要性度量，可能是也可能不是术语频率。
- en: If your data conceptually represents a vector (that is to say, a magnitude in
    a particular direction), then the cosine distance may be the most appropriate
    choice. The cosine distance measure determines the similarity of two elements
    as the cosine of the angle between their vector representations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据从概念上表示一个向量（也就是说，一个特定方向上的大小），那么余弦距离可能是最合适的选择。余弦距离度量通过计算两个元素的向量表示之间夹角的余弦值来确定它们的相似度。
- en: '![The vector space model and cosine distance](img/7180OS_06_120.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![向量空间模型与余弦距离](img/7180OS_06_120.jpg)'
- en: 'If both vectors point in the same direction, then the angle between them will
    be zero and the cosine of zero is one. The cosine similarity can be defined in
    the following way:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个向量指向相同的方向，那么它们之间的夹角为零，而零的余弦值为 1。余弦相似度可以通过以下方式定义：
- en: '![The vector space model and cosine distance](img/7180OS_06_05.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![向量空间模型与余弦距离](img/7180OS_06_05.jpg)'
- en: This is a more complicated equation than the ones we've covered previously.
    It relies on calculating the dot product of the two vectors and the magnitude
    of each.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个比我们之前讨论的方程更为复杂的方程。它依赖于计算两个向量的点积及其大小。
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Examples of the cosine similarity are shown as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度的例子如下所示：
- en: '![The vector space model and cosine distance](img/7180OS_06_130.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![向量空间模型与余弦距离](img/7180OS_06_130.jpg)'
- en: 'The cosine similarity is often used as a similarity measure in high-dimensional
    spaces where each vector contains a lot of zeros because it can be very efficient
    to evaluate: only the non-zero dimensions need to be considered. Since most text
    documents use only a small fraction of all words (and therefore are zero for a
    large proportion of dimensions), the cosine measure is often used for clustering
    text.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度通常作为高维空间中的相似度度量使用，其中每个向量包含很多零，因为它计算起来非常高效：只需要考虑非零维度。由于大多数文本文档只使用了所有单词中的一小部分（因此在大多数维度上是零），余弦度量通常用于文本聚类。
- en: In the vector space model, we need a consistent strategy for measuring the importance
    of each term. In the set-of-words model, all terms are counted equally. This is
    equivalent to setting the value of the vector at that point to one. In the bag-of-words
    model, the term frequencies were counted. We'll continue to use the term frequency
    for now, but we'll see shortly how to use a more sophisticated measure of importance,
    called **term frequency-inverse document frequency** (**TF-IDF**).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在向量空间模型中，我们需要一个一致的策略来衡量每个词项的重要性。在词集合模型中，所有词项被平等对待。这相当于将该点的向量值设置为1。在词袋模型中，计算了词项的频率。我们目前将继续使用词频，但很快我们将看到如何使用一种更复杂的重要性度量，称为**词频-逆文档频率**（**TF-IDF**）。
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The closer the cosine value is to `1`, the more similar the two entities are.
    To convert `cosine-similarity` to a distance measure, we can simply subtract the
    `cosine-similarity` from `1`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦值越接近`1`，这两个实体越相似。为了将`余弦相似度`转化为距离度量，我们可以简单地将`余弦相似度`从`1`中减去。
- en: Although all the measures mentioned earlier produce different measures for the
    same input, they all satisfy the constraint that the distance between *A* and
    *B* should be the same as the difference between *B* and *A*. Often the same underlying
    data can be transformed to represent a set (Jaccard), a point in space (Euclidean),
    or a vector (Cosine). Sometimes the only way to know which is right is to try
    it and see how good the results are.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面提到的所有度量方法对相同输入产生不同的度量值，但它们都满足一个约束，即* A *和* B *之间的距离应该与* B *和* A *之间的差异相同。通常，相同的底层数据可以被转换为表示集合（Jaccard）、空间中的一个点（欧几里得）或一个向量（余弦）。有时，唯一的方法是尝试并查看结果如何，才能知道哪个是正确的。
- en: The number of unique words that appear in one document is typically small compared
    to the number of unique words that appear in any document in a collection being
    processed. As a result, these high-dimensional document vectors are quite sparse.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 出现在一个文档中的唯一单词数量通常比处理中的文档集合中任何文档中出现的唯一单词数量要少。因此，这些高维文档向量通常是非常稀疏的。
- en: Removing stop words
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去除停用词
- en: Much of the similarity between the headlines has been generated by often-occurring
    words that don't add a great deal of meaning to the content. Examples are "a",
    "says", and "and". We should filter these out in order to avoid generating spurious
    similarities.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 许多头条新闻之间的相似性是由一些经常出现的词汇所产生的，这些词汇对内容的意义贡献不大。例如，“a”，“says”和“and”。我们应该过滤掉这些词，以避免产生虚假的相似性。
- en: 'Consider the following two idioms:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下两个习语：
- en: '`"Music is the food of love"`'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"音乐是爱的食粮"`'
- en: '`"War is the locomotive of history"`'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"战争是历史的动力"`'
- en: 'We could calculate the cosine similarity between them using the following Clojure
    code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下Clojure代码来计算它们之间的余弦相似度：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The two documents are showing a similarity of `0.5` in spite of the fact that
    the only words they share in common are `is`, `the`, and `of`. Ideally we'll want
    to remove these.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两个文档之间唯一的共同单词是`is`、`the`和`of`，它们的相似度为`0.5`。理想情况下，我们希望移除这些词。
- en: Stemming
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词干提取
- en: 'Now let''s consider an alternative phrase:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一个替代表达：
- en: '`"Music is the food of love"`'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"音乐是爱的食粮"`'
- en: '`"It''s lovely that you''re musical"`'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"你很有音乐天赋，真好"`'
- en: 'Let''s compare their cosine similarity as well:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也来比较它们的余弦相似度：
- en: '[PRE26]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In spite of the fact that the two sentences refer to music and positive feelings,
    the two phrases have a cosine similarity of zero: there are no words in common
    between the two phrases. This makes sense but does not express the behavior we
    usually want, which is to capture the similarity between "concepts", rather than
    the precise words that were used.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两个句子都提到音乐和积极情感，但这两个短语的余弦相似度为零：两个短语之间没有共同的单词。这是有道理的，但并没有表达我们通常想要的行为，即捕捉“概念”之间的相似性，而不是精确使用的词汇。
- en: One way of tackling this problem is to **stem** words, which reduces them to
    their roots. Words which share a common meaning are more likely to stem to the
    same root. The Clojure library stemmers ([https://github.com/mattdw/stemmers](https://github.com/mattdw/stemmers))
    will do this for us, and fortunately they will also remove stop words too.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是**词干提取**，它将单词简化为其词根。具有共同意义的单词更可能提取到相同的词根。Clojure库的词干提取器（[https://github.com/mattdw/stemmers](https://github.com/mattdw/stemmers)）可以为我们完成这项工作，幸运的是，它们也会去除停用词。
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Much better. After stemming and stop word removal, the similarity between the
    phrases has dropped from 0.0 to 0.82\. This is a good outcome since, although
    the sentences used different words, the sentiments they expressed were related.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 好得多了。经过词干提取和去除停用词后，短语之间的相似度从 0.0 降低到 0.82。这是一个很好的结果，因为尽管句子使用了不同的词汇，它们表达的情感是相关的。
- en: Clustering with k-means and Incanter
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 k-means 和 Incanter 进行聚类
- en: Finally, having tokenized, stemmed, and vectorized our input documents—and with
    a selection of distance measures to choose from—we're in a position to run clustering
    on our data. The first clustering algorithm we'll look at is called *k-means clustering*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在对输入文档进行标记化、词干提取和向量化处理，并且选择了不同的距离度量方式后，我们可以开始对数据进行聚类。我们将首先探讨的聚类算法是 *k-means
    聚类*。
- en: '*k*-means is an iterative algorithm that proceeds as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means 是一个迭代算法，步骤如下：'
- en: Randomly pick *k* cluster centroids.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择 *k* 个聚类中心点。
- en: Assign each of the data points to the cluster with the closest centroid.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个数据点分配到与其最近的聚类中心点所属的聚类中。
- en: Adjust each cluster centroid to the mean of its assigned data points.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整每个聚类中心点的位置，使其位于其分配数据点的均值位置。
- en: Repeat until convergence or the maximum number of iterations reached.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复进行，直到收敛或达到最大迭代次数。
- en: 'The process is visualized in the following diagram for *k=3* clusters:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程在以下图示中展示，适用于 *k=3* 个聚类：
- en: '![Clustering with k-means and Incanter](img/7180OS_06_140.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![使用 k-means 和 Incanter 进行聚类](img/7180OS_06_140.jpg)'
- en: In the preceding figure, we can see that the initial cluster centroids at iteration
    1 don't represent the structure of the data well. Although the points are clearly
    arranged in three groups, the initial centroids (represented by crosses) are all
    distributed around the top area of the graph. The points are colored according
    to their closest centroid. As the iterations proceed, we can see how the cluster
    centroids are moved closer to their "natural" positions in the center of each
    of the groups of points.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以看到，在第一次迭代时，初始的聚类中心点并没有很好地代表数据的结构。尽管这些点显然被分成了三组，但初始的聚类中心点（以十字标记表示）都分布在图形的上方区域。这些点的颜色表示它们与最近的聚类中心的关系。随着迭代的进行，我们可以看到聚类中心点如何逐步靠近每个点组的“自然”位置，即组中心。
- en: 'Before we define the main *k*-means function, it''s useful to define a couple
    of utility functions first: a function to calculate the centroid for a cluster,
    and a function to group the data into their respective clusters.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义主要的 *k*-means 函数之前，先定义几个实用的辅助函数是有用的：一个用于计算聚类中心点的函数，另一个则是将数据分组到各自聚类的函数。
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `centroid` function simply calculates the mean of each column of the input
    matrix.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`centroid` 函数简单地计算输入矩阵每列的均值。'
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `clusters` function splits a larger matrix up into a sequence of smaller
    matrices based on the supplied cluster IDs. The cluster IDs are provided as a
    sequence of elements the same length as the clustered points, listing the cluster
    ID of the point at that index in the sequence. Items that share a common cluster
    ID will be grouped together. With these two functions in place, here''s the finished
    `k-means` function:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`clusters` 函数根据提供的聚类 ID 将较大的矩阵拆分为一系列较小的矩阵。聚类 ID 被提供为与聚类点长度相同的元素序列，列出该序列中每个点对应的聚类
    ID。共享相同聚类 ID 的项目将被分到一起。通过这两个函数，我们得到了完整的 `k-means` 函数：'
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We start by picking `k` random cluster centroids by sampling the input data.
    Then, we use loop/recur to continuously update the cluster centroids until `previous-cluster-ids`
    are the same as `cluster-ids`. At this point, no documents have moved cluster,
    so the clustering has converged.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过抽样输入数据随机选择 `k` 个聚类中心点。接着，使用循环/递归不断更新聚类中心点，直到 `previous-cluster-ids` 和
    `cluster-ids` 相同。此时，所有文档都没有移动到其他聚类，因此聚类过程已收敛。
- en: Clustering the Reuters documents
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对路透社文档进行聚类
- en: Let's use our `k-means` function to cluster the Reuters documents now. Let's
    go easy on our algorithm to start with, and pick a small sample of larger documents.
    Larger documents will make it more likely that the algorithm will be able to determine
    meaningful similarities between them. Let's set the minimum threshold at 500 characters.
    This means that at the very least our input documents will have a headline and
    a couple of sentences of body text to work with.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用 `k-means` 函数对路透社文档进行聚类。我们先让算法简单一些，选取一些较大的文档样本。较大的文档更容易让算法识别它们之间的相似性。我们将最低字符数设定为
    500 字符。这意味着我们的输入文档至少有一个标题和几句正文内容。
- en: '[PRE31]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We're using the `fs` library ([https://github.com/Raynes/fs](https://github.com/Raynes/fs))
    to create a list of files to perform our clustering on by calling `fs/glob` with
    a pattern that matches all the text files. We remove those which are too short,
    tokenize the first 100, and add them to the dictionary. We create `tf` vectors
    for our inputs and then call `k-means` on them.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`fs`库（[https://github.com/Raynes/fs](https://github.com/Raynes/fs)）通过调用`fs/glob`，并使用匹配所有文本文件的模式，来创建文件列表进行聚类。我们删除那些太短的文件，标记前100个，并将它们添加到字典中。我们为输入创建`tf`向量，然后对它们调用`k-means`。
- en: If you run the preceding example, you'll receive a list of clustered document
    vectors, which isn't very useful. Let's create a `summary` function that uses
    the dictionary to report the most common terms in each of the clusters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行前面的示例，你将得到一组聚类文档向量，但这些并不太有用。让我们创建一个`summary`函数，利用字典报告每个聚类中最常见的术语。
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '*k*-means is by its nature a stochastic algorithm, and is sensitive to the
    starting position for the centroids. I get the following output, but yours will
    almost certainly differ:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-均值算法本质上是一个随机算法，对质心的初始位置敏感。我得到了以下输出，但你的结果几乎肯定会不同：'
- en: '[PRE33]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Unfortunately, we don't seem to be getting very good results. The first cluster
    contains two articles about rockets and space, and the third seems to consist
    of articles about Iran. The most popular word in most of the articles is "said".
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们似乎没有得到很好的结果。第一个聚类包含两篇关于火箭和太空的文章，第三个聚类似乎由关于伊朗的文章组成。大多数文章中最常见的词汇是“said”。
- en: Better clustering with TF-IDF
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TF-IDF进行更好的聚类
- en: '**Term** **Frequency-Inverse Document Frequency** (**TF-IDF**) is a general
    approach to weighting terms within a document vector so that terms that are popular
    across the whole dataset are not weighted as highly as terms that are less usual.
    This captures the intuitive conviction—and what we observed earlier—that words
    such as "said" are not a strong basis for building clusters.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**词汇** **频率-逆文档频率**（**TF-IDF**）是一种通用方法，用于在文档向量中加权术语，以便在整个数据集中流行的术语不会像那些较不常见的术语那样被高估。这捕捉了直观的信念——也是我们之前观察到的——即“said”这样的词汇并不是构建聚类的强有力基础。'
- en: Zipf's law
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Zipf定律
- en: 'Zipf''s law states that the frequency of any word is inversely proportional
    to its rank in the frequency table. Thus, the most frequent word will occur approximately
    twice as often as the second most frequent word and three times as often as the
    next most frequent word, and so on. Let''s see if this applies across our Reuters
    corpus:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Zipf定律指出，任何词汇的频率与其在频率表中的排名成反比。因此，最频繁的词汇出现的频率大约是第二常见词汇的两倍，第三常见词汇的三倍，依此类推。让我们看看这一规律是否适用于我们的路透社语料库：
- en: '[PRE34]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Using the preceding code we can calculate the frequency graph of the top 25
    most popular terms in the first 1,000 Reuters documents.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述代码，我们可以计算出前1,000篇路透社文档中前25个最流行词汇的频率图。
- en: '![Zipf''s law](img/7180OS_06_150.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![Zipf定律](img/7180OS_06_150.jpg)'
- en: In the first 1,000 documents, the most popular term appears almost 10,000 times.
    The 25^(th) most popular term appears around 1,000 times overall. In fact, the
    data is showing that words are appearing more commonly in the Reuters corpus than
    their placement in the frequency table would suggest. This is most likely due
    to the bulletin nature of the Reuters corpus, which tends to re-use the same short
    words repeatedly.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在前1,000篇文档中，最流行的词汇出现了将近10,000次。第25^(th)个最流行的词汇总共出现了大约1,000次。实际上，数据显示，词汇在路透社语料库中的出现频率比频率表中的位置所暗示的要高。这很可能是由于路透社语料库的公告性质，导致相同的短词被反复使用。
- en: Calculating the TF-IDF weight
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算TF-IDF权重
- en: Calculating TF-IDF only requires two modifications to the code we've created
    already. Firstly, we must keep track of how many documents a given term appears
    in. Secondly, we must weight the term appropriately when building the document
    vector.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 计算TF-IDF只需要对我们已经创建的代码进行两个修改。首先，我们必须追踪给定术语出现在哪些文档中。其次，在构建文档向量时，我们必须适当地加权该术语。
- en: Since we've already created a dictionary of terms, we may as well store the
    document frequencies for each term there.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经创建了一个术语字典，我们不妨将每个术语的文档频率存储在其中。
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `build-df-dictionary` function earlier accepts a dictionary and a sequence
    of terms. We build the dictionary from the distinct terms and look up the `term-id`
    for each one. Finally, we iterate over the term IDs and increment the `:df` for
    each one.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`build-df-dictionary` 函数之前接受一个字典和一个术语序列。我们从不同的术语中构建字典，并查找每个术语的 `term-id`。最后，我们遍历术语
    ID 并为每个 ID 增加 `:df`。'
- en: 'If a document has words *w*[1], …, *w*[n], then the inverse document frequency
    for word *w*[i] is defined as:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个文档包含词语 *w*[1]、…、*w*[n]，那么词语 *w*[i] 的逆文档频率定义为：
- en: '![Calculating the TF-IDF weight](img/7180OS_06_06.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![计算 TF-IDF 权重](img/7180OS_06_06.jpg)'
- en: 'That is, the reciprocal of the number of documents it appears in. If a word
    occurs commonly across a collection of documents, its *DF* value is large and
    its *IDF* value is small. With a large number of documents, it''s common to normalize
    the *IDF* value by multiplying it by a constant number, usually the document count
    *N*, so the *IDF* equation looks like this:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 即，词语出现在文档中的倒数。如果一个词语在一组文档中频繁出现，它的 *DF* 值较大，而 *IDF* 值较小。在文档数量很大的情况下，通常通过乘以一个常数（通常是文档总数
    *N*）来对 *IDF* 值进行归一化，因此 *IDF* 公式看起来像这样：
- en: '![Calculating the TF-IDF weight](img/7180OS_06_07.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![计算 TF-IDF 权重](img/7180OS_06_07.jpg)'
- en: 'The TF-IDF weight *W*[i] of word *w*[i] is given by the product of the term
    frequency and the inverse document frequency:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 词语 *w*[i] 的 TF-IDF 权重 *W*[i] 由词频和逆文档频率的乘积给出：
- en: '![Calculating the TF-IDF weight](img/7180OS_06_08.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![计算 TF-IDF 权重](img/7180OS_06_08.jpg)'
- en: 'However, the *IDF* value in the preceding equation is still not ideal since
    for large corpora the range of the *IDF* term is usually much greater than the
    *TF* and can overwhelm its effect. To reduce this problem, and balance the weight
    of the *TF* and the *IDF* terms, the usual practice is to use the logarithm of
    the *IDF* value instead:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，前述公式中的 *IDF* 值仍然不理想，因为对于大型语料库，*IDF* 项的范围通常远大于 *TF*，并且可能会压倒 *TF* 的效果。为了减少这个问题并平衡
    *TF* 和 *IDF* 项的权重，通常的做法是使用 *IDF* 值的对数：
- en: '![Calculating the TF-IDF weight](img/7180OS_06_09.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![计算 TF-IDF 权重](img/7180OS_06_09.jpg)'
- en: 'Thus, the TF-IDF weight *w*[i] for a word *w*[i] becomes:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，词语 *w*[i] 的 TF-IDF 权重 *w*[i] 变为：
- en: '![Calculating the TF-IDF weight](img/7180OS_06_10.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![计算 TF-IDF 权重](img/7180OS_06_10.jpg)'
- en: 'This is a classic TF-IDF weighting: common words are given a small weight and
    terms that occur infrequently get a large weight. The important words for determining
    the topic of a document usually have a high *TF* and a moderately large *IDF*,
    so the product of the two becomes a large value, thereby giving more importance
    to these words in the resulting vector.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这是经典的 TF-IDF 权重：常见词语的权重较小，而不常见的词语的权重较大。确定文档主题的重要词语通常具有较高的 *TF* 和适中的 *IDF*，因此二者的乘积成为一个较大的值，从而在结果向量中赋予这些词语更多的权重。
- en: '[PRE36]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The preceding code calculates the TF-IDF from `term-frequencies` defined previously
    and `document-frequencies` extracted from our dictionary.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码计算了先前定义的 `term-frequencies` 和从字典中提取的 `document-frequencies` 的 TF-IDF 值。
- en: k-means clustering with TF-IDF
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TF-IDF 的 k-means 聚类
- en: 'With the preceding adjustments in place, we''re in a position to calculate
    the TF-IDF vectors for the Reuters documents. The following example is a modification
    of `ex-6-12` using the new `tfidf-vector` function:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行前述调整后，我们可以计算路透社文档的 TF-IDF 向量。以下示例是基于新的 `tfidf-vector` 函数修改的 `ex-6-12`：
- en: '[PRE37]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The preceding code is very similar to the previous example, but we have substituted
    our new `build-df-dictionary` and `tfidf-vector` functions. If you run the example,
    you should see output that looks a little better than before:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码与之前的示例非常相似，但我们已经替换了新的 `build-df-dictionary` 和 `tfidf-vector` 函数。如果你运行该示例，应该会看到比之前稍微改进的输出：
- en: '[PRE38]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Although the top words may be hard to interpret because they have been stemmed,
    these represent the most unusually common words within each of the clusters. Notice
    that "said" is no longer the most highly rated word across all clusters.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管由于词语已被词干化，顶级词语可能难以解释，但这些词语代表了每个聚类中最为不寻常的常见词。注意，“said”不再是所有聚类中评分最高的词语。
- en: Better clustering with n-grams
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更好的 n-gram 聚类
- en: It should be clear from looking at the earlier lists of words how much has been
    sacrificed by reducing our documents to unordered sequences of terms. Without
    the context of a sentence, it's very hard to get more than a vague sense of what
    each cluster might be about.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面列出的词汇表来看，应该清楚地意识到，通过将文档简化为无序的词汇序列，我们丧失了多少信息。如果没有句子的上下文，我们很难准确把握每个聚类的含义。
- en: There is, however, nothing inherent in the vector space model that precludes
    maintaining the order of our input tokens. We can simply create a new term to
    represent a combination of words. The combined term, representing perhaps several
    input words in sequence, is called an ***n*-gram**.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，向量空间模型本身并没有什么内在的限制，阻止我们保持输入词语的顺序。我们可以简单地创建一个新的术语来表示多个词的组合。这个组合词，可能表示多个连续的输入词，被称为
    ***n*-gram**。
- en: An example of an *n*-gram might be "new york", or "stock market". In fact, because
    they contain two terms, these are called **bigrams**. *n*-grams can be of arbitrary
    length. The longer an *n*-gram, the more context it carries, but also the rarer
    it is.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *n*-gram 的例子可能是“new york”（纽约）或“stock market”（股市）。事实上，因为它们包含两个词，所以这些被称为 **bigrams**（二元组）。*n*-grams
    可以是任意长度的。*n*-gram 越长，携带的上下文信息越多，但它的出现也就越为罕见。
- en: '*n*-grams are closely related to the concept of **shingling**. When we shingle
    our *n*-grams, we''re creating overlapping sequences of terms. The term shingling
    comes from the way the terms overlap like roof shingles.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*-grams 与 **瓦片法（shingling）**的概念密切相关。当我们对 *n*-grams 进行瓦片化处理时，我们是在创建重叠的词组序列。瓦片化（shingling）一词来源于这些词语像屋顶瓦片一样重叠的方式。'
- en: '[PRE39]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Already, using 2-grams would allow us (for example) to distinguish between
    the following uses of the word "coconut" in the dataset: "coconut oil", "coconut
    planters", "coconut plantations", "coconut farmers", "coconut association", "coconut
    authority", "coconut products", "coconut exports", "coconut industry", and the
    rather pleasing "coconut chief". Each of these pairs of words defines a different
    concept—sometimes subtly different—that we can capture and compare across documents.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，使用 2-grams 就能让我们（例如）区分数据集中“coconut”一词的不同用法：“coconut oil”（椰子油）、“coconut planters”（椰子种植者）、“coconut
    plantations”（椰子种植园）、“coconut farmers”（椰子农民）、“coconut association”（椰子协会）、“coconut
    authority”（椰子管理机构）、“coconut products”（椰子产品）、“coconut exports”（椰子出口）、“coconut industry”（椰子产业）和相当吸引人的“coconut
    chief”（椰子首领）。这些词对定义了不同的概念——有时是微妙的不同——我们可以在不同的文档中捕捉并进行比较。
- en: 'We can get the best of both worlds with *n*-grams and shingling by combining
    the results of multiple lengths of *n*-gram:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过结合不同长度的 *n*-gram 的结果，来实现 *n*-gram 和瓦片化的双重优势：
- en: '[PRE40]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: While stemming and stop word removal had the effect of reducing the size of
    our dictionary, and using TF-IDF had the effect of improving the utility of the
    weight for each term in a document, producing *n*-grams has the effect of massively
    expanding the number of terms we need to accommodate.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然词干提取（stemming）和停用词移除（stop word removal）起到了缩减字典规模的作用，而使用 TF-IDF 则提高了每个词在文档中的权重效用，但生成
    *n*-grams 的效果是大大增加了我们需要处理的词汇数量。
- en: This explosion of features is going to immediately overwhelm our implementation
    of *k*-means in Incanter. Fortunately, there's a machine learning library called
    **Mahout** that's specifically designed to run algorithms such as *k*-means on
    very large quantities of data.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 特征的爆炸性增长将立刻使我们在 Incanter 中实现 *k*-means 的效果超出负荷。幸运的是，有一个叫做 **Mahout** 的机器学习库，专门设计用于在海量数据上运行像
    *k*-means 这样的算法。
- en: Large-scale clustering with Mahout
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Mahout 进行大规模聚类
- en: Mahout ([http://mahout.apache.org/](http://mahout.apache.org/)) is a machine
    learning library intended for use in distributed computing environments. Version
    0.9 of the library targets Hadoop and is the version we'll be using here.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Mahout ([http://mahout.apache.org/](http://mahout.apache.org/)) 是一个机器学习库，旨在分布式计算环境中使用。该库的
    0.9 版本支持 Hadoop，并且是我们将在此使用的版本。
- en: Note
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of writing, Mahout 0.10 has just been released and also targets
    Spark. Spark is an alternative distributed computing framework that we'll be introducing
    in the next chapter.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文撰写时，Mahout 0.10 刚刚发布，并且同样支持 Spark。Spark 是一个替代性的分布式计算框架，我们将在下一章介绍。
- en: 'We saw in the previous chapter that one of Hadoop''s abstractions is the sequence
    file: a binary representation of Java keys and values. Many of Mahout''s algorithms
    expect to operate on sequence files, and we''ll need to create one as input to
    Mahout''s *k*-means algorithm. Mahout''s *k*-means algorithm also expects to receive
    its input as a vector, represented by one of Mahout''s vector types.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章看到，Hadoop 的一个抽象概念是序列文件：Java 键和值的二进制表示。许多 Mahout 的算法期望在序列文件上操作，我们需要创建一个序列文件作为
    Mahout 的 *k*-means 算法的输入。Mahout 的 *k*-means 算法也期望将其输入作为向量，表示为 Mahout 的向量类型之一。
- en: Although Mahout contains classes and utility programs that will extract vectors
    from text, we'll use this as an opportunity to demonstrate how to use Parkour
    and Mahout together. Not only will we have finer-grained control over the vectors
    that are created, but it will allow us to demonstrate more of the capabilities
    of Parkour for specifying Hadoop jobs.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Mahout 包含提取向量的类和实用程序，我们将借此机会演示如何将 Parkour 和 Mahout 一起使用。这样不仅能让我们对创建的向量有更细粒度的控制，还可以展示更多
    Parkour 在指定 Hadoop 作业时的能力。
- en: Converting text documents to a sequence file
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将文本文件转换为序列文件
- en: 'We won''t define a custom job to convert our text documents into a sequence
    file representation, though: Mahout already defines a useful `SequenceFilesFromDirectory`
    class to convert a directory of text files. We''ll use this to create a single
    file representing the entire contents of the `reuters-txt` directory.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不会定义自定义作业来将文本文档转换为序列文件表示：Mahout 已经定义了一个有用的 `SequenceFilesFromDirectory`
    类，用来转换文本文件目录。我们将使用这个工具来创建一个代表整个 `reuters-txt` 目录内容的单一文件。
- en: Though the sequence file may be physically stored in separate chunks (on HDFS,
    for example), it is logically one file, representing all the input documents as
    key/value pairs. The key is the name of the file, and the value is the file's
    text contents.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管序列文件可能物理上分布在不同的块中（例如在 HDFS 上），但它在逻辑上是一个文件，表示所有输入文档作为键/值对。键是文件名，值是文件的文本内容。
- en: '![Converting text documents to a sequence file](img/7180OS_06_160.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![将文本文档转换为序列文件](img/7180OS_06_160.jpg)'
- en: 'The following code will handle the conversion:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将处理转换：
- en: '[PRE41]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`SequenceFilesFromDirectory` is a Mahout utility class, part of a suite of
    classes designed to be called on the command line.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`SequenceFilesFromDirectory` 是 Mahout 的一个实用类，是一套可以在命令行调用的类之一。'
- en: Tip
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Since running the preceding example is a prerequisite for subsequent examples,
    it''s also available on the command line:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 由于运行前面的示例是后续示例的先决条件，它也可以在命令行上运行：
- en: '[PRE42]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We're calling the `main` function directly, passing the arguments we would otherwise
    pass on the command line as a string array.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们直接调用 `main` 函数，传递我们通常在命令行上传递的参数，作为字符串数组。
- en: Using Parkour to create Mahout vectors
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Parkour 创建 Mahout 向量
- en: Now that we have a sequence file representation of the Reuters corpus, we need
    to transform each document (now represented as a single key/value pair) into a
    vector. We saw how to do this earlier using a shared dictionary modeled as a Clojure
    atom. The atom ensures that each distinct term gets its own ID even in a multi-threaded
    environment.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了 Reuters 语料库的序列文件表示，我们需要将每个文档（现在表示为一个单一的键/值对）转换为向量。我们之前已经看到如何使用共享字典（被建模为
    Clojure 原子）来实现这一点。原子确保即使在多线程环境中，每个不同的术语也会得到自己的 ID。
- en: We will be using Parkour and Hadoop to generate our vectors, but this presents
    a challenge. How can we assign a unique ID to each word when the nature of MapReduce
    programming is that mappers operate in parallel and share no state? Hadoop doesn't
    provide the equivalent of a Clojure atom for sharing mutable state across nodes
    in a cluster, and in fact minimizing shared state is key to scaling distributed
    applications.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Parkour 和 Hadoop 来生成向量，但这带来了一个挑战。由于 MapReduce 编程的特点是映射器并行工作且不共享状态，我们如何为每个单词分配一个唯一的
    ID 呢？Hadoop 并未提供类似于 Clojure 原子的机制来在集群中的节点之间共享可变状态，实际上，最小化共享状态是扩展分布式应用程序的关键。
- en: 'Creating a shared set of unique IDs therefore presents an interesting challenge
    for our Parkour job: let''s see how we can produce unique IDs for our dictionary
    in a distributed way.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，创建一个共享的唯一 ID 集合对我们的 Parkour 作业来说是一个有趣的挑战：让我们看看如何以分布式方式为我们的字典生成唯一的 ID。
- en: Creating distributed unique IDs
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建分布式唯一 ID
- en: Before we look at Hadoop-specific solutions, though, it's worth noting that
    one easy way of creating a cluster-wide unique identifier is to create a universally
    unique identifier, or UUID.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看 Hadoop 特定的解决方案之前，值得注意的是，创建一个集群范围内唯一标识符的一个简单方法是创建一个通用唯一标识符，或 UUID。
- en: '[PRE43]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This creates a long string of bytes in the form: `3a65c7db-6f41-4087-a2ec-8fe763b5f185`
    that is virtually guaranteed not to clash with any other UUID generated anywhere
    else in the world.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这会创建一个形式为 `3a65c7db-6f41-4087-a2ec-8fe763b5f185` 的长字节字符串，这几乎可以保证不会与世界上任何地方生成的其他
    UUID 冲突。
- en: 'While this works for generating unique IDs, the number of possible IDs is astronomically
    large, and Mahout''s sparse vector representation needs to be initialized with
    the cardinality of the vector expressed as an integer. IDs generated with `uuid`
    are simply too big. Besides, it doesn''t help us coordinate the creation of IDs:
    every machine in the cluster will generate different UUIDs to represent the same
    terms.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这对于生成唯一 ID 有效，但可能的 ID 数量是天文数字，而 Mahout 的稀疏向量表示需要以整数的形式初始化向量的基数。使用 `uuid` 生成的
    ID 太大了。此外，这并不能帮助我们协调 ID 的创建：集群中的每台机器都会生成不同的 UUID 来表示相同的术语。
- en: One way of getting around this is to use the term itself to generate a unique
    ID. If we used a consistent hashing function to create an integer from each input
    term, all machines in the cluster would generate the same ID. Since a good hashing
    function is likely to produce a unique output for unique input terms, this technique
    is likely to work well. There will be some hash collisions (where two words hash
    to the same ID) but this should be a small percentage of the overall.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是使用术语本身来生成唯一 ID。如果我们使用一致性哈希函数从每个输入术语创建一个整数，集群中的所有机器都会生成相同的 ID。由于良好的哈希函数可能会为唯一的输入术语产生唯一的输出，这个技巧可能会很好地工作。虽然会有一些哈希冲突（即两个词哈希到相同的
    ID），但这应该只是整体中的一个小比例。
- en: Note
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The method of hashing the features themselves to create a unique ID is often
    referred to as the "hashing trick". Although it's commonly used for text vectorization,
    it can be applied to any problem that involves large numbers of features.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希特征本身以创建唯一 ID 的方法通常被称为“哈希技巧”。尽管它通常用于文本向量化，但它可以应用于任何涉及大量特征的问题。
- en: 'However, the challenge of producing distinct IDs that are unique across the
    whole cluster gives us the opportunity to talk about a useful feature of Hadoop
    that Parkour exposes: the distributed cache.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，生成跨整个集群唯一的区分 ID 的挑战，给了我们一个讨论 Hadoop 中 Parkour 所揭示的一个有用功能的机会：分布式缓存。
- en: Distributed unique IDs with Hadoop
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Hadoop 分布式唯一 ID
- en: 'Let''s consider what our Parkour mapper and reducer might look like if we were
    to calculate unique, cluster-wide IDs. The mapper is easy: we''ll want to calculate
    the document frequency for each term we encounter, so the following mapper simply
    returns a vector for each unique term: the first element of the vector (the key)
    is the term itself, and the second element (the value) is `1`.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要计算唯一的集群范围 ID，考虑一下我们的 Parkour 映射器和化简器可能是什么样子。映射器很简单：我们希望计算每个遇到的术语的文档频率，因此以下映射器简单地为每个唯一术语返回一个向量：向量的第一个元素（键）是术语本身，第二个元素（值）是
    `1`。
- en: '[PRE44]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The reducer's job will be to take these key/value pairs of terms to document
    count and reduce them such that each unique term has a unique ID. A trivial way
    of doing this would be to ensure that there is only one reducer on the cluster.
    Since all the terms would all be passed to this single process, the reducer could
    simply keep an internal counter and assign each term an ID in a similar way to
    what we did with the Clojure atom earlier. This isn't taking advantage of Hadoop's
    distributed capabilities, though.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 化简器的任务是将这些术语的键/值对（术语与文档计数）进行归约，以确保每个唯一术语都有一个唯一的 ID。做这件事的一个简单方法是确保集群上只有一个化简器。由于所有术语都会传递给这个单一的进程，化简器可以简单地保持一个内部计数器，并像我们之前用
    Clojure 原子做的那样为每个术语分配一个 ID。然而，这并没有利用 Hadoop 的分布式能力。
- en: 'One feature of Parkour that we haven''t introduced yet is the runtime context
    that''s accessible from within every mapper and reducer. Parkour binds the `parkour.mapreduce/*context*`
    dynamic variable to the Hadoop task context of the task within which our mappers
    and reducers run. The task context contains, amongst other things, the following
    properties:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未介绍 Parkour 的一个特点，即每个映射器（mapper）和归约器（reducer）内部可以访问的运行时上下文。Parkour 将 `parkour.mapreduce/*context*`
    动态变量绑定到执行我们映射器和归约器的 Hadoop 任务的任务上下文中。任务上下文包含以下属性（其中之一）：
- en: '| Property | Type | Description |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `mapred.job.id` | String | The job''s ID |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `mapred.job.id` | 字符串 | 作业的 ID |'
- en: '| `mapred.task.id` | int | The task attempt ID |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| `mapred.task.id` | int | 任务尝试的 ID |'
- en: '| `mapred.task.partition` | int | The ID of the task within the job |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| `mapred.task.partition` | int | 任务在作业中的 ID |'
- en: The last of these, the `mapred.task.partition` property, is the number of the
    task assigned by Hadoop, guaranteed to be a monotonically increasing integer unique
    across the cluster. This number is our task's global offset. Within each task
    we can also keep a local offset and output both with each word processed. The
    two offsets together—global and local—provide a unique identifier for the term
    across the cluster.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最后一个属性 `mapred.task.partition`，是 Hadoop 分配的任务编号，保证是一个单调递增的唯一整数，且在整个集群中唯一。这个数字就是我们任务的全局偏移量。在每个任务内部，我们还可以保持一个本地偏移量，并在处理每个单词时输出全局和本地偏移量。全局偏移量和本地偏移量一起为集群中的每个术语提供了一个唯一的标识符。
- en: 'The following diagram visualizes the process for eight terms processed on three
    separate mappers:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了在三个独立的映射器上处理的八个术语的过程：
- en: '![Distributed unique IDs with Hadoop](img/7180OS_06_170.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![分布式唯一 ID 与 Hadoop](img/7180OS_06_170.jpg)'
- en: Each mapper is only aware of its own partition number and the term's local offset.
    However, these two numbers are all that's required to calculate a unique, global
    ID. The preceding **Calculate Offsets** box determines what the global offset
    should be for each task partition. Partition **1** has a global offset of **0**.
    Partition **2** has a global offset of **3**, because partition **1** processed
    **3** words. Partition **3** has an offset of **5**, because partitions **1**
    and **2** processed **5** words between them, and so on.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 每个映射器只知道它自己的分区号和术语的本地偏移量。然而，这两个数字就是计算唯一全局 ID 所需的全部信息。前面提到的 **计算偏移量** 盒子确定了每个任务分区的全局偏移量是什么。分区
    **1** 的全局偏移量为 **0**。分区 **2** 的全局偏移量为 **3**，因为分区 **1** 处理了 **3** 个单词。分区 **3** 的偏移量为
    **5**，因为分区 **1** 和 **2** 处理了总共 **5** 个单词，依此类推。
- en: 'For the preceding approach to work, we need to know three things: the global
    offset of the mapper, the local offset of the term, and the total number of terms
    processed by each mapper. These three numbers can be used to define a unique,
    cluster-wide ID for each term. The reducer that creates these three numbers is
    defined as follows. It introduces a couple of new concepts that we''ll discuss
    shortly.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 要使上述方法工作，我们需要知道三件事：映射器的全局偏移量、术语的本地偏移量，以及每个映射器处理的术语总数。这三组数字可以用来为每个术语定义一个全局唯一的集群
    ID。生成这三组数字的归约器定义如下。它引入了一些新概念，稍后我们将详细讨论。
- en: '[PRE45]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The first step the reducer performs is to fetch the `global-offset`, the task
    partition for this particular reducer. We're using `mapcat-state`, a function
    defined in the transduce library ([https://github.com/brandonbloom/transduce](https://github.com/brandonbloom/transduce))
    to build up a sequence of tuples in the format `[[:data ["apple" [1 4]] [:data
    ["orange" [1 5]] ...]` where the vector of numbers `[1 4]` represents the global
    and local offsets respectively. Finally, when we've reached the end of this reduce
    task, we append a tuple to the sequence in the format `[:counts [1 5]]`. This
    represents the final local count, `5`, for this particular reducer partition,
    `1`. Thus, a single reducer is calculating all three of the elements we require
    to calculate all the term IDs.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 归约器执行的第一步是获取 `global-offset`，即此归约器对应的任务分区。我们使用 `mapcat-state`，这是在 transduce
    库中定义的一个函数（[https://github.com/brandonbloom/transduce](https://github.com/brandonbloom/transduce)），来构建一系列元组，格式为
    `[[:data ["apple" [1 4]] [:data ["orange" [1 5]] ...]`，其中数字向量 `[1 4]` 分别表示全局和本地偏移量。最后，当我们到达此归约任务的末尾时，我们会将一个元组以
    `[:counts [1 5]]` 的格式添加到序列中。这代表了该特定归约器分区 `1` 的最终本地计数 `5`。因此，单个归约器计算了我们计算所有术语 ID
    所需的三项元素。
- en: 'The keyword provided to `::mr/source-as` is not one we''ve encountered previously.
    In the previous chapter, we saw how the shaping options `:keyvals`, `:keys`, and
    `:vals` let Parkour know how we wanted our data provided, and the structure of
    the data we''d be providing in return. For reducers, Parkour describes a more
    comprehensive set of shaping functions that account for the fact that inputs may
    be grouped. The following diagram illustrates the available options:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 提供给`::mr/source-as`的关键字是我们之前没有遇到过的。在上一章中，我们看到了如何通过`：keyvals`、`:keys`和`:vals`等塑形选项，让Parkour知道我们希望如何提供数据，以及我们将返回的数据结构。对于聚合器，Parkour描述了一个更全面的塑形函数集，考虑到输入可能是分组的。下图展示了可用的选项：
- en: '![Distributed unique IDs with Hadoop](img/7180OS_06_175.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![分布式唯一ID与Hadoop](img/7180OS_06_175.jpg)'
- en: 'The option provided to `::mr/sink-as` is not one we''ve encountered before
    either. The `parkour.io.dux` namespace provides options for de-multiplexing outputs.
    In practice this means that, by sinking as `dux/named-keyvals`, a single reducer
    can write to several different outputs. In other words, we''ve introduced a fork
    into our data pipeline: some data is written to one branch, the rest to another.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 提供给`::mr/sink-as`的选项我们也没有遇到过。`parkour.io.dux`命名空间提供了输出去多路复用的选项。实际上，这意味着通过将sink指定为`dux/named-keyvals`，单个聚合器可以写入多个不同的输出。换句话说，我们在数据管道中引入了一个分支：部分数据写入一个分支，其余数据写入另一个分支。
- en: Having set a sink specification of `dux/named-keyvals`, the first element of
    our tuple will be interpreted as the destination to write to; the second element
    of our tuple will be treated as the key/value pair to be written. As a result,
    we can write out the `:data` (the local and global offset) to one destination
    and the `:counts` (number of terms processed by each mapper) to another.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 设置了`dux/named-keyvals`的sink规范后，我们元组的第一个元素将被解释为写入的目标；元组的第二个元素将被视为要写入的键值对。因此，我们可以将`:data`（本地和全局偏移量）写入一个目标，将`:counts`（每个映射器处理的术语数量）写入另一个目标。
- en: The job that makes use of the mapper and reducer that we've defined is presented
    next. As with the Parkour job we specified in the previous chapter, we chain together
    an input, map, partition, reduce, and output step.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来展示的是使用我们定义的映射器和聚合器的作业。与上一章中指定的Parkour作业类似，我们将输入、映射、分区、归约和输出步骤串联起来。
- en: '[PRE46]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'There are two primary differences between the preceding code and the job specification
    we''ve seen previously. Firstly, our output specifies two named sinks: one for
    each of the outputs of our reducer. Secondly, we''re using the `parkour.io.avro`
    namespace as `mra` to specify a schema for our data with `(mra/dsink [:string
    long-pair])`.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码和我们之前看到的作业规范之间有两个主要区别。首先，我们的输出指定了两个命名的sink：每个聚合器的输出各一个。其次，我们使用`parkour.io.avro`命名空间作为`mra`来为我们的数据指定模式，使用`(mra/dsink
    [:string long-pair])`。
- en: 'In the previous chapter we made use of Tesser''s `FressianWritable` to serialize
    arbitrary Clojure data structures to disk. This worked because the contents of
    the `FressianWritable` did not need to be interpreted by Hadoop: the value was
    completely opaque. With Parkour, we have the option to define custom key/value
    pair types. Since the key and value do need to be interpreted as separate entities
    by Hadoop (for the purpose of reading, partitioning, and writing sequence files),
    Parkour allows us to define a "tuple schema" using the `parkour.io.avro` namespace,
    which explicitly defines the type of the key and the value. `long-pair` is a custom
    schema used to store both the local and global offset in a single tuple:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用了Tesser的`FressianWritable`将任意Clojure数据结构序列化到磁盘。这是因为`FressianWritable`的内容不需要Hadoop解析：该值是完全不透明的。使用Parkour时，我们可以定义自定义的键/值对类型。由于Hadoop需要将键和值作为独立实体进行解析（用于读取、分区和写入序列文件），Parkour允许我们使用`parkour.io.avro`命名空间定义“元组模式”，该模式明确地定义了键和值的类型。`long-pair`是一个自定义模式，用于将本地和全局偏移量存储在单个元组中。
- en: '[PRE47]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'And, since schemas are composable, we can refer to the `long-pair` schema when
    defining our output schema: `(mra/dsink [:string long-pair])`.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模式是可组合的，我们可以在定义输出模式时引用`long-pair`模式：`(mra/dsink [:string long-pair])`。
- en: Note
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Parkour uses the library `Acbracad` to serialize Clojure data structures using
    Avro. For more information about serialization options consult the documentation
    for Abracad at [https://github.com/damballa/abracad](https://github.com/damballa/abracad).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Parkour使用`Acbracad`库来通过Avro序列化Clojure数据结构。有关序列化选项的更多信息，请参考Abracad文档，网址：[https://github.com/damballa/abracad](https://github.com/damballa/abracad)。
- en: 'Let''s look at another feature of Hadoop that Parkour exposes which allows
    our term ID job to be more efficient than it would otherwise be: the distributed
    cache.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看Parkour暴露的Hadoop的另一个功能，它使得我们的术语ID任务比其他方式更高效：分布式缓存。
- en: Sharing data with the distributed cache
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用分布式缓存共享数据
- en: As we discussed in the previous section, if we know the local offset of each
    word for a particular mapper, and we know how many records each mapper processed
    overall, then we're in a position to calculate a unique, contiguous ID for each
    word.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在上一节中讨论的那样，如果我们知道每个映射器中每个单词的本地偏移量，并且我们知道每个映射器处理了多少记录，那么我们就能计算出每个单词的唯一连续ID。
- en: 'The diagram that showed the process a few pages ago contained two central boxes
    each labeled **Calculate Offsets** and a **Global ID**. Those boxes map directly
    to the functions that we present next:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 几页前展示的图示包含了两个中央框，每个框上标有**计算偏移量**和**全局ID**。这些框直接对应于我们接下来要展示的函数：
- en: '[PRE48]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Once we've calculated the map of offsets to use for generating unique IDs, we'd
    really like them to be available to all of our map and reduce tasks as a shared
    resource. Having generated the offsets in a distributed fashion, we'd like to
    consume it in a distributed fashion too.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算出了用于生成唯一ID的偏移量映射，我们希望这些映射能够作为共享资源对所有的映射任务和归约任务可用。既然我们已经以分布式的方式生成了偏移量，我们也希望以分布式的方式进行使用。
- en: The distributed cache is Hadoop's way of allowing tasks to access common data.
    This is a much more efficient way of sharing small quantities of data (data that's
    small enough to reside in memory) than through potentially costly data joins.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式缓存是Hadoop让任务能够访问公共数据的一种方式。这比通过可能昂贵的数据连接共享少量数据（足够小可以存储在内存中的数据）要高效得多。
- en: '![Sharing data with the distributed cache](img/7180OS_06_180.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![使用分布式缓存共享数据](img/7180OS_06_180.jpg)'
- en: 'Before reading from the distributed cache, we have to write something to it.
    This can be achieved with Parkour''s `parkour.io.dval` namespace:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在从分布式缓存读取数据之前，我们需要向它写入一些数据。这可以通过Parkour的`parkour.io.dval`命名空间来实现：
- en: '[PRE49]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Here, we''re writing two sets of data to the distributed cache with the `dval/edn-dval`
    function. The first is the result of the `calculate-offsets` function just defined
    which is passed to the `word-id-m` mappers for their use. The second set of data
    written to the distributed cache is their output. We''ll see how this is generated
    in the `word-id-m` function, as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`dval/edn-dval`函数将两组数据写入分布式缓存。第一组数据是刚刚定义的`calculate-offsets`函数的结果，它将传递给`word-id-m`映射器使用。写入分布式缓存的第二组数据是它们的输出。我们将看到如何在`word-id-m`函数中生成这些数据，如下所示：
- en: '[PRE50]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The value returned by `dval/edn-dval` implements the `IDRef` interface. This
    means that we can use Clojure's `deref` function (or the deref macro character
    `@`) to retrieve the value that it wraps, just as we do with Clojure's atoms.
    Dereferencing the distributed value the first time causes the data to be downloaded
    from the distributed cache to a local mapper cache. Once the data is available
    locally, Parkour takes care of reconstructing the Clojure data structure (the
    map of offsets) that we wrote to it in EDN format.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`dval/edn-dval`返回的值实现了`IDRef`接口。这意味着我们可以使用Clojure的`deref`函数（或者`@`反引用宏）来获取它所包装的值，就像我们操作Clojure的原子值一样。第一次对分布式值进行反引用时，会从分布式缓存中下载数据到本地映射器缓存中。一旦数据在本地可用，Parkour会负责重新构建我们以EDN格式写入的数据结构（偏移量的映射）。'
- en: Building Mahout vectors from input documents
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从输入文档构建Mahout向量
- en: In the previous sections, we took a detour to introduce several new Parkour
    and Hadoop concepts, but we're finally in a position to build text vectors for
    Mahout using unique IDs for every term. Some further code is omitted for brevity
    but the whole job is available to view in the `cljds.ch6.vectorizer` example code
    namespace.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们绕了一些路介绍了几个新的Parkour和Hadoop概念，但现在我们终于可以为Mahout构建文本向量，并为每个术语使用唯一的ID。为了简洁起见，部分代码被省略，但整个任务可以在`cljds.ch6.vectorizer`示例代码命名空间中查看。
- en: 'As mentioned previously, Mahout''s implementation of *k*-means expects us to
    provide a vector representation of our input using one of its vector classes.
    Since our dictionary is large, and most documents use few of these terms, we''ll
    be using a sparse vector representation. The following code makes use of a `dictionary`
    distributed value to create a `org.apache.mahout.math.RandomAccessSparseVector`
    for every input document:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Mahout的*k*-means实现要求我们使用其向量类之一提供输入的向量表示。由于我们的字典很大，并且大多数文档只使用其中少数术语，因此我们将使用稀疏向量表示。以下代码利用了一个`dictionary`分布式值，为每个输入文档创建一个`org.apache.mahout.math.RandomAccessSparseVector`：
- en: '[PRE51]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Finally, we make use of the `create-tfidf-vectors-m` function, which brings
    everything we''ve covered together into a single Hadoop job:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们利用`create-tfidf-vectors-m`函数，它将我们所学的内容汇聚成一个单一的Hadoop作业：
- en: '[PRE52]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This task handles the creation of the dictionary, writing the dictionary to
    the distributed cache, and then using the dictionary—with the mapper we just defined—to
    convert each input document to a Mahout vector. To ensure sequence file compatibility
    with Mahout, we set the key/value classes of our final output to be `Text` and
    `VectorWritable`, where the key is the original filename of the document and the
    value is the Mahout vector representation of the contents.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务处理字典的创建，将字典写入分布式缓存，然后使用我们刚定义的映射器，将每个输入文档转换为Mahout向量。为了确保与Mahout的序列文件兼容，我们将最终输出的键/值类设置为`Text`和`VectorWritable`，其中键是文档的原始文件名，值是文档内容的Mahout向量表示。
- en: 'We can call this job by running:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下命令来调用此作业：
- en: '[PRE53]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The job will write the dictionary out to the `dictionary-path` (we'll be needing
    it again), and the vectors out to the `vector-path`.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 该作业将字典写入`dictionary-path`（我们稍后还需要它），并将向量写入`vector-path`。
- en: Tip
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Since running the preceding example is a prerequisite for subsequent examples,
    it''s also available on the command line:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 由于运行前面的例子是后续示例的先决条件，它也可以通过命令行访问：
- en: '[PRE54]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Next, we'll discover how to use these vectors to actually perform clustering
    with Mahout.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何使用这些向量来实际执行Mahout的聚类。
- en: Running k-means clustering with Mahout
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Mahout运行k-means聚类
- en: Now that we have a sequence file of vectors suitable for consumption by Mahout,
    it's time to actually run *k*-means clustering on the whole dataset. Unlike our
    local Incanter version, Mahout won't have any trouble dealing with the full Reuters
    corpus.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经有了一个适合Mahout使用的向量序列文件，接下来就该在整个数据集上实际运行*k*-means聚类了。与我们本地的Incanter版本不同，Mahout在处理完整的Reuters语料库时不会遇到任何问题。
- en: As with the `SequenceFilesFromDirectory` class, we've created a wrapper around
    another of Mahout's command-line programs, `KMeansDriver`. The Clojure variable
    names make it easier to see what each command-line argument is for.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 与`SequenceFilesFromDirectory`类一样，我们已经为Mahout的另一个命令行程序`KMeansDriver`创建了一个封装器。Clojure变量名使得我们更容易理解每个命令行参数的作用。
- en: '[PRE55]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We're providing the string `org.apache.mahout.common.distance.CosineDistanceMeasure`
    to indicate to the driver that we'd like to use Mahout's implementation of the
    cosine distance measure. Mahout also includes a `EuclideanDistanceMeasure` and
    a `TanimotoDistanceMeasure` (similar to the Jaccard distance, the complement of
    the Jaccard index, but one that will operate on vectors rather than sets). Several
    other distance measures are also defined; consult the Mahout documentation for
    all the available options.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供字符串`org.apache.mahout.common.distance.CosineDistanceMeasure`，以指示驱动程序我们希望使用Mahout的余弦距离度量实现。Mahout还包括`EuclideanDistanceMeasure`和`TanimotoDistanceMeasure`（类似于Jaccard距离，是Jaccard指数的补集，但将作用于向量而非集合）。还有几种其他的距离度量可以选择；请参考Mahout文档以了解所有可用选项。
- en: 'With the preceding `run-kmeans` function in place, we simply need to let Mahout
    know where to access our files. As in the previous chapter, we assume Hadoop is
    running in local mode and all file paths are relative to the project root:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 有了前面的`run-kmeans`函数后，我们只需要告诉Mahout在哪里访问我们的文件。与上一章一样，我们假设Hadoop在本地模式下运行，所有文件路径都相对于项目根目录：
- en: '[PRE56]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: This example may run for a little while as Mahout iterates over our large dataset.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子可能会运行一段时间，因为Mahout需要对我们的大数据集进行迭代。
- en: Viewing k-means clustering results
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看k-means聚类结果
- en: Once it's finished, we'll want to see a cluster summary for each cluster as
    we did with our Incanter implementation. Fortunately, Mahout defines a `ClusterDumper`
    class that will do exactly this for us. We need to provide the location of our
    clusters, of course, but we'll provide the location of our dictionary, too. Providing
    the dictionary means that the output can return the top terms for each cluster.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们希望能看到每个聚类的聚类总结，就像我们在Incanter实现中所做的那样。幸运的是，Mahout定义了一个`ClusterDumper`类，正是用来做这个事情的。我们需要提供聚类的位置，当然，还需要提供字典的位置。提供字典意味着输出将返回每个聚类的顶部术语。
- en: '[PRE57]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Next, we define the code that will actually call the `run-cluster-dump` function:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义实际调用`run-cluster-dump`函数的代码：
- en: '[PRE58]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: We're making use of the `me.raynes.fs` library once again to determine which
    directory the final clusters are contained by. Mahout will append `-final` to
    the directory containing the final clusters, but we don't know ahead of time which
    directory this will be. The `fs/glob` function will find a directory that matches
    the pattern `clusters-*-final`, and replace `*` with whichever iteration number
    the true directory name contains.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用`me.raynes.fs`库来确定最终聚类所在的目录。Mahout会在包含最终聚类的目录名后附加`-final`，但我们事先并不知道哪个目录会是这个最终目录。`fs/glob`函数会查找与`clusters-*-final`模式匹配的目录，并将`*`替换为实际目录名称中包含的迭代编号。
- en: Interpreting the clustered output
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释聚类输出
- en: 'If you open the file created by the previous example, `data/kmeans-clusterdump`,
    in any text editor, you''ll see output representing the top terms of the Mahout
    clusters. The file will be large, but an excerpt is provided next:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在任何文本编辑器中打开之前示例创建的文件`data/kmeans-clusterdump`，你将看到表示Mahout聚类的顶部术语的输出。该文件可能很大，但下面提供了一个摘录：
- en: '[PRE59]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The first line contains information about the cluster: the ID (in this case
    `VL-11417`) followed by curly braces containing the size of the cluster and the
    location of the cluster centroid. Since the text has been converted to weights
    and numeric IDs, the centroid is impossible to interpret on its own. The top terms
    beneath the centroid description hint at the contents of the cluster, though;
    they''re the terms around which the cluster has coalesced.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行包含关于聚类的信息：ID（在本例中为`VL-11417`），后面跟着包含聚类大小和聚类质心位置的大括号。由于文本已转换为权重和数字ID，单独解读质心是不可行的。不过，质心描述下方的顶部术语暗示了聚类的内容；它们是聚类汇聚的核心术语。
- en: '[PRE60]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The two clusters earlier hint at two clear topics present in the data set, although
    your clusters may be different due to the stochastic nature of the *k*-means algorithm.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 前面提到的两个聚类暗示了数据集中存在的两个明确主题，尽管由于*k*-means算法的随机性，你的聚类可能会有所不同。
- en: Depending on your initial centroids and how many iterations you let the algorithm
    run, you may see clusters that appear "better" or "worse" in some respect. This
    will be based on an instinctive response to how well the clustered terms go together.
    But often it's not clear simply by looking at the top terms how well clustering
    has performed. In any case, instinct is not a very reliable way of judging the
    quality of an unsupervised learning algorithm. What we'd ideally like is some
    quantitative measure for how well the clustering has performed.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 根据初始质心和算法运行的迭代次数，你可能会看到某些聚类在某些方面看起来“更好”或“更差”。这将基于对聚类词汇如何组合在一起的直觉反应。但通常，仅通过查看顶部术语并不能清楚地判断聚类的效果如何。无论如何，直觉并不是判断无监督学习算法质量的可靠方法。我们理想的情况是有一个定量指标来衡量聚类的效果。
- en: Cluster evaluation measures
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类评估指标
- en: 'At the bottom of the file we looked at in the previous section, you''ll see
    some statistics that suggest how well the data has been clustered:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上一节查看的文件底部，你会看到一些统计信息，表明数据的聚类效果如何：
- en: '[PRE61]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: These two numbers can be considered as the equivalent to the variance within
    and the variance between measures we have seen in [Chapter 2](ch02.xhtml "Chapter 2. Inference"),
    *Inference* and [Chapter 3](ch03.xhtml "Chapter 3. Correlation"), *Correlation*.
    Ideally, we are seeking a lower variance (or a higher density) within clusters
    compared to the density between clusters.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个数字可以看作是我们在[第2章](ch02.xhtml "第2章. 推理")，*推理*和[第3章](ch03.xhtml "第3章. 相关性")，*相关性*中看到的组内方差和组间方差的等价物。理想情况下，我们希望聚类内的方差较低（或密度较高），而聚类间的密度较低。
- en: Inter-cluster density
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类间密度
- en: Inter-cluster density is the average distance between cluster centroids. Good
    clusters probably don't have centers that are too close to each other. If they
    did, it would indicate the clustering is creating groups with similar features,
    and perhaps drawing distinctions between cluster members that are hard to support.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类间密度是聚类质心之间的平均距离。好的聚类通常不会有太靠近的质心。如果它们太近，那就意味着聚类正在创建具有相似特征的组，并可能在区分聚类成员方面存在难以支持的情况。
- en: '![Inter-cluster density](img/7180OS_06_190.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![聚类间密度](img/7180OS_06_190.jpg)'
- en: Thus, ideally our clustering will produce clusters with a **large inter-cluster
    distance**.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，理想情况下，我们的聚类应产生具有**大聚类间距离**的聚类。
- en: Intra-cluster density
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类内部密度
- en: By contrast, the intra-cluster density is a measure of how compact the clusters
    are. Ideally, clustering will identify groups of items that are similar to each
    other. Compact clusters indicate that all of the items within a cluster are strongly
    alike.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，聚类内密度是衡量聚类紧凑性的指标。理想情况下，聚类会识别出彼此相似的项目组。紧凑的聚类表示聚类中的所有项目彼此高度相似。
- en: '![Intra-cluster density](img/7180OS_06_200.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![聚类内部密度](img/7180OS_06_200.jpg)'
- en: The best clustering outcomes therefore produce compact, distinct clusters with
    a **high intra-cluster density** and a **low inter-cluster density**.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最佳的聚类结果会产生紧凑、独特的聚类，具有**高聚类内密度**和**低聚类间密度**。
- en: It is not always clear how many clusters are justified by the data, though.
    Consider the following that shows the same dataset grouped into varying numbers
    of clusters. It's hard to say with any degree of confidence what the ideal number
    of clusters is.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据到底应该有多少个聚类并不总是清楚。考虑以下示例，它展示了同一数据集以不同聚类数分组的情况。很难有足够的信心判断理想的聚类数是多少。
- en: '![Intra-cluster density](img/7180OS_06_210.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![聚类内部密度](img/7180OS_06_210.jpg)'
- en: Although the preceding illustration is contrived, it illustrates a general issue
    with clustering data. There is often no one, clear "best" number of clusters.
    The most effective clustering will depend to a large degree on the ultimate use
    of the data.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的示意图是人为构造的，但它展示了聚类数据时的一个普遍问题。通常没有一个明确的“最佳”聚类数。最有效的聚类将很大程度上取决于数据的最终用途。
- en: We can however infer which might be better values of *k* by determining how
    the value of some quality score varies with the number of clusters. The quality
    score could be a statistic such as the inter- or intra-cluster density. As the
    number of clusters approaches its ideal, we would expect the value of this quality
    score to improve. Conversely, as the number of clusters diverges from its ideal
    we would expect the quality to decrease. To get a reasonable idea of how many
    clusters are justified in the dataset, therefore, we should run the algorithm
    many times for different values of *k*.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以通过确定某些质量评分如何随着聚类数量的变化来推断可能的较优 *k* 值。质量评分可以是像聚类间密度或聚类内密度这样的统计数据。当聚类数接近理想值时，我们期望该质量评分的值会提高。相反，当聚类数偏离理想值时，我们期望质量评分会下降。因此，为了合理地估算数据集中多少个聚类是合理的，我们应该为不同的
    *k* 值多次运行算法。
- en: Calculating the root mean square error with Parkour
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Parkour 计算均方根误差
- en: 'One of the most common measures of cluster quality is the **sum of squared
    errors** (**SSE**). For each point, the error is the measured distance to the
    nearest cluster centroid. The total clustering SSE is therefore the sum over all
    clusters for a clustered point to its corresponding centroid:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的聚类质量度量之一是**平方误差和**（**SSE**）。对于每个点，误差是测量到最近聚类质心的距离。因此，总的聚类 SSE 是聚类点到其相应质心的所有聚类的和：
- en: '![Calculating the root mean square error with Parkour](img/7180OS_06_11.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Parkour 计算均方根误差](img/7180OS_06_11.jpg)'
- en: where *µ*[i] is the centroid of points in cluster *S*[i], *k* is the total number
    of clusters and *n* is the total number of points.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *µ*[i] 是聚类 *S*[i] 中点的质心，*k* 是聚类的总数，*n* 是点的总数。
- en: To calculate the *RMSE* in Clojure we therefore need to be able to relate each
    point in the cluster to its corresponding cluster centroid. Mahout saves cluster
    centroids and clustered points in two separate files, so in the next section we'll
    combine them.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 Clojure 中计算 *RMSE* 时，我们需要能够将聚类中的每个点与其对应的聚类质心关联起来。Mahout 将聚类质心和聚类点保存在两个不同的文件中，因此在下一节中我们将把它们合并。
- en: Loading clustered points and centroids
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载聚类点和质心
- en: Given a parent directory (e.g. `data/reuters-kmeans/kmeans-10`), the following
    function will load points into vectors stored in a map indexed by cluster ID using
    Parkour's `seqf/dseq` function to load key/value pairs from a sequence file. In
    this case, the key is the cluster ID (as an integer) and the value is the TF-IDF
    vector.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个父目录（例如 `data/reuters-kmeans/kmeans-10`），以下函数将使用 Parkour 的 `seqf/dseq` 函数从序列文件加载键/值对，并将点加载到按聚类
    ID 索引的向量映射中。在这种情况下，键是聚类 ID（整数），值是 TF-IDF 向量。
- en: '[PRE62]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The output of the preceding function is a map keyed by cluster ID whose values
    are sequences of clustered points. Likewise, the following function will convert
    each cluster into a map, keyed by cluster ID, whose values are maps containing
    the keys `:id` and `:centroid`.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数的输出是一个按聚类 ID 索引的映射，其值是聚类点的序列。同样，以下函数将把每个聚类转换为一个按聚类 ID 索引的映射，其值是包含 `:id`
    和 `:centroid` 键的映射。
- en: '[PRE63]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Having two maps keyed by cluster ID means that combining the clustered points
    and cluster centroids is a simple matter of calling `merge-with` on the maps supplying
    a custom merging function. In the following code, we merge the clustered points
    into the map containing the cluster `:id` and `:centroid`.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有两个按聚类 ID 索引的映射意味着将聚类的点与聚类中心结合起来，只需对映射调用 `merge-with` 并提供一个自定义的合并函数即可。在以下代码中，我们将聚类的点合并到包含聚类
    `:id` 和 `:centroid` 的映射中。
- en: '[PRE64]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The final output is a single map, keyed by cluster ID, with each value as a
    map of `:id`, `:centroid` and `:points`. We'll use this map in the next section
    to calculate the clustering RMSE.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出是一个单一的映射，按聚类 ID 索引，其中每个值都是一个包含 `:id`、`:centroid` 和 `:points` 的映射。我们将在下一节中使用这个映射来计算聚类的
    RMSE。
- en: Calculating the cluster RMSE
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算聚类 RMSE
- en: To calculate the RMSE, we need to be able to establish the distance between
    every point and its associated cluster centroid. Since we used Mahout's `CosineDistanceMeasure`
    to perform the initial clustering, we should use the cosine distance to evaluate
    the clustering as well. In fact, we can simply make use of Mahout's implementation.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算 RMSE，我们需要能够确定每个点与其关联的聚类中心之间的距离。由于我们使用了 Mahout 的 `CosineDistanceMeasure`
    来执行初始聚类，因此我们也应该使用余弦距离来评估聚类。事实上，我们可以直接利用 Mahout 的实现。
- en: '[PRE65]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: If the RMSE is plotted against the number of clusters, you'll find that it declines
    as the number of clusters increases. A single cluster will have the highest RMSE
    error (the variance of the original dataset from the mean), whereas the lowest
    RMSE will be the degenerate case when every point is in its own cluster (an RMSE
    of zero). Clearly either of these extremes will provide a poor explanation for
    the structure of the data. However, the RMSE doesn't decline in a straight line.
    It declines sharply as the number of clusters is increased from 1, but will fall
    more slowly once the "natural" number of clusters has been exceeded.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将 RMSE 与聚类数绘制成图，你会发现随着聚类数的增加，RMSE 会逐渐下降。单一聚类将具有最高的 RMSE 错误（原始数据集与均值的方差），而最低的
    RMSE 将是每个点都在自己的聚类中的退化情况（RMSE 为零）。显然，这两个极端都无法很好地解释数据的结构。然而，RMSE 不是线性下降的。当聚类数从 1
    增加时，它会急剧下降，但一旦超过“自然”聚类数后，下降的速度就会变慢。
- en: Therefore, one way of judging the ideal number of clusters is to plot how the
    RMSE changes with respect to the number of clusters. This is called the **elbow
    method**.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，判断理想聚类数的一种方法是绘制 RMSE 随聚类数变化的图表。这就是所谓的 **肘部法**。
- en: Determining optimal k with the elbow method
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用肘部法确定最佳的 k 值
- en: In order to determine the value of *k* using the elbow method, we're going to
    have to re-run *k*-means a number of times. The following code accomplishes this
    for all *k* between `2` and `21`.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用肘部法确定 *k* 的值，我们需要多次重新运行 *k*-均值聚类。以下代码实现了对 `2` 到 `21` 之间的所有 *k* 值进行聚类。
- en: '[PRE66]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'This will take a little while to run, so it might be time to go and make a
    hot drink: the `println` statement will log each clustering run to let you know
    how much progress has been made. On my laptop the whole process takes about 15
    minutes.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要一些时间，所以不妨去泡杯热饮：`println` 语句会记录每次聚类的运行情况，让你知道进展如何。在我的笔记本电脑上，整个过程大约需要 15
    分钟。
- en: 'Once it''s complete, you should be able to run the example to generate a scatter
    plot of the RMSE for each of the clustered values:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，你应该能够运行示例，生成每个聚类值的 RMSE 散点图：
- en: '[PRE67]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This should return a plot similar to the following:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该返回一个类似于以下的图表：
- en: '![Determining optimal k with the elbow method](img/7180OS_06_220.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![使用肘部法确定最佳 k 值](img/7180OS_06_220.jpg)'
- en: The preceding scatter plot shows the RMSE plotted against the number of clusters.
    It should be clear how the rate of RMSE change slows as *k* exceeds around 13
    clusters and increasing the number of clusters further yields diminishing returns.
    Therefore, the preceding chart suggests for our Reuters data that around 13 clusters
    is a good choice.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的散点图展示了RMSE与簇数之间的关系。应该能看出，当*k*超过大约13个簇时，RMSE的变化速率放慢，进一步增加簇数会带来递减的收益。因此，上图表明对于我们的路透社数据，约13个簇是一个不错的选择。
- en: The elbow method provides an intuitive means to determine the ideal number of
    clusters, but it is sometimes hard to apply in practice. This is because we must
    interpret the shape of the curve defined by the RMSE for each *k*. If *k* is small,
    or the RMSE contains a lot of noise, it may not be apparent where the elbow falls,
    or if there is an elbow at all.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 肘部法则提供了一种直观的方式来确定理想的簇数，但在实践中有时难以应用。这是因为我们必须解释每个*k*对应的RMSE曲线的形状。如果*k*很小，或者RMSE包含较多噪声，可能不容易看出肘部在哪里，甚至是否存在肘部。
- en: Note
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Since clustering is an unsupervised learning algorithm, we assume here that
    the internal structure of the clusters is the only means of validating the quality
    of clustering. If true cluster labels are known then it's possible to use external
    validation measures (such as entropy) of the kind that we encountered in [Chapter
    4](ch04.xhtml "Chapter 4. Classification"), *Classification* to validate the success
    of the model.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 由于聚类是一种无监督学习算法，我们在此假设簇的内部结构是验证聚类质量的唯一手段。如果已知真实的簇标签，则可以使用外部验证度量（例如熵）来验证模型的成功，这在[第4章](ch04.xhtml
    "第4章. 分类")，*分类*中我们曾遇到过。
- en: Other clustering evaluation schemes aim to provide a clearer means of determining
    the precise number of clusters. The two we'll cover are the Dunn index and the
    Davies-Bouldin index. Both are internal evaluation schemes, meaning that they
    only look at the structure of the clustered data. Each aims to identify the clustering
    that has produced the most compact, well-separated clusters, in different ways.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 其他聚类评估方案旨在提供更清晰的方式来确定精确的簇数。我们将讨论的两种方法是邓恩指数和戴维斯-鲍尔丁指数。它们都是内部评估方案，意味着它们只关注聚类数据的结构。每种方法都旨在以不同的方式识别产生最紧凑、最分离的簇的聚类。
- en: Determining optimal k with the Dunn index
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用邓恩指数确定最优k
- en: 'The Dunn index offers an alternative way to choose the optimal number of *k*.
    Rather than considering the average error remaining in the clustered data, the
    Dunn index instead considers the ratio of two "worst-case" situations: the minimum
    distance between two cluster centroids, divided by the maximum cluster diameter.
    A higher index therefore indicates better clustering since in general we would
    like large inter-cluster distances and small intra-cluster distances.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 邓恩指数提供了另一种选择最优*k*的方式。邓恩指数不考虑聚类数据中的平均误差，而是考虑两种“最坏情况”的比率：两个簇中心之间的最小距离，除以最大簇直径。因此，较高的邓恩指数表示更好的聚类，因为通常我们希望簇间距离较大，而簇内距离较小。
- en: 'For *k* clusters we can express the Dunn index in the following way:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*k*个簇，我们可以通过以下方式表达邓恩指数：
- en: '![Determining optimal k with the Dunn index](img/7180OS_06_12.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![使用邓恩指数确定最优k](img/7180OS_06_12.jpg)'
- en: where *δ(C[i],C[j])* distance between the two clusters *C*[i] and *C*[j] and
    ![Determining optimal k with the Dunn index](img/7180OS_06_13.jpg) represents
    the size (or scatter) of the largest cluster.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*δ(C[i],C[j])*是两个簇*C*[i]和*C*[j]之间的距离，而![使用邓恩指数确定最优k](img/7180OS_06_13.jpg)表示最大簇的大小（或散布）。
- en: There are several possible ways to calculate the scatter of a cluster. We could
    take the distance between the furthest two points inside a cluster, or the mean
    of all the pairwise distances between data points inside the cluster, or the mean
    of each data point from the cluster centroid itself. In the following code, we
    calculate the size by taking the median distance from the cluster centroid.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 计算一个簇的散布有几种可能的方法。我们可以计算簇内最远两个点之间的距离，或者计算簇内所有数据点之间成对距离的平均值，或者计算每个数据点到簇中心的距离的平均值。在以下代码中，我们通过计算从簇中心到各点的中位距离来确定大小。
- en: '[PRE68]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The preceding code makes use of the `combinations` function from `clojure.math.combinatorics`
    ([https://github.com/clojure/math.combinatorics/](https://github.com/clojure/math.combinatorics/))
    to produce a lazy sequence of all pairwise combinations of clusters.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码使用了 `clojure.math.combinatorics` 中的 `combinations` 函数（[https://github.com/clojure/math.combinatorics/](https://github.com/clojure/math.combinatorics/)）来生成所有聚类的懒序列对。
- en: '[PRE69]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We use the `dunn-index` function in the preceding code to generate a scatter
    plot for the clusters from *k=2* to *k=20*:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上面的代码中使用了 `dunn-index` 函数来为 *k=2* 到 *k=20* 的聚类生成散点图：
- en: '![Determining optimal k with the Dunn index](img/7180OS_06_230.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Dunn 指数确定最优的 k 值](img/7180OS_06_230.jpg)'
- en: A higher Dunn index indicates a better clustering. Thus, it appears that the
    best clustering is for *k=2*, followed by *k=6*, with *k=12* and *k=13* following
    closely behind. Let's try an alternative cluster evaluation scheme and compare
    the results.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的 Dunn 指数表示更好的聚类效果。因此，最佳聚类似乎是 *k=2*，接下来是 *k=6*，然后是 *k=12* 和 *k=13*，它们紧随其后。我们来尝试一种替代的聚类评估方案，并比较结果。
- en: Determining optimal k with the Davies-Bouldin index
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Davies-Bouldin 指数确定最优的 k 值
- en: 'The Davies-Bouldin index is an alternative evaluation scheme that measures
    the mean ratio of size and separation for all values in the cluster. For each
    cluster, an alternative cluster is found that maximizes the ratio of the sum of
    cluster sizes divided by the inter-cluster distance. The Davies-Bouldin index
    is defined as the mean of this value for all clusters in the data:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: Davies-Bouldin 指数是一种替代的评估方案，用于衡量聚类中所有值的大小和分离度的平均比率。对于每个聚类，找到一个替代聚类，使得聚类大小之和与聚类间距离的比率最大化。Davies-Bouldin
    指数被定义为所有聚类的此值的平均值：
- en: '![Determining optimal k with the Davies-Bouldin index](img/7180OS_06_14.jpg)![Determining
    optimal k with the Davies-Bouldin index](img/7180OS_06_15.jpg)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Davies-Bouldin 指数确定最优的 k 值](img/7180OS_06_14.jpg)![使用 Davies-Bouldin 指数确定最优的
    k 值](img/7180OS_06_15.jpg)'
- en: 'where *δ(C[i],C[j])* is the distance between the two cluster centroids *C*[i]
    and *C*[j], and *S*[i] and *S*[j] are the scatter. We can calculate the Davies-Bouldin
    index using the following code:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *δ(C[i],C[j])* 是两个聚类中心 *C*[i] 和 *C*[j]* 之间的距离，*S*[i] 和 *S*[j]* 是散布度。我们可以使用以下代码计算
    Davies-Bouldin 指数：
- en: '[PRE70]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Let''s now plot the Davies-Bouldin on a scatter plot for clusters *k=2* to
    *k=20*:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在散点图中绘制 *k=2* 到 *k=20* 的 Davies-Bouldin 指数：
- en: '[PRE71]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'This should generate the following plot:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![Determining optimal k with the Davies-Bouldin index](img/7180OS_06_240.jpg)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Davies-Bouldin 指数确定最优的 k 值](img/7180OS_06_240.jpg)'
- en: Unlike the Dunn index, the Davies-Bouldin index is minimized for good clustering
    schemes since in general we seek out clusters that are compact in size and have
    high inter-cluster distances. The preceding chart suggests that *k=2* is the ideal
    cluster size followed by *k=13*.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Dunn 指数不同，Davies-Bouldin 指数对于良好的聚类方案是最小化的，因为一般来说，我们寻找的是紧凑的聚类，并且聚类间的距离较大。前面的图表表明，*k=2*
    是理想的聚类大小，其次是 *k=13*。
- en: The drawbacks of k-means
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*k*-均值算法的缺点'
- en: '*k*-means is one of the most popular clustering algorithms due to its relative
    ease of implementation and the fact that it can be made to scale well to very
    large datasets. In spite of its popularity, there are several drawbacks.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-均值算法是最流行的聚类算法之一，因为它相对易于实现，并且可以很好地扩展到非常大的数据集。尽管它很受欢迎，但也有一些缺点。'
- en: '*k*-means is stochastic, and does not guarantee to find the global optimum
    solution for clustering. In fact, the algorithm can be very sensitive to outliers
    and noisy data: the quality of the final clustering can be highly dependent on
    the position of the initial cluster centroids. In other words, *k*-means will
    regularly discover a local rather than global minimum.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-均值算法是随机的，并不能保证找到全局最优的聚类解。事实上，该算法对离群点和噪声数据非常敏感：最终聚类的质量可能高度依赖于初始聚类中心的位置。换句话说，*k*-均值算法通常会发现局部最优解，而非全局最优解。'
- en: '![The drawbacks of k-means](img/7180OS_06_250.jpg)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![k-means 算法的缺点](img/7180OS_06_250.jpg)'
- en: 'The preceding diagram illustrates how *k*-means may converge to a local minimum
    based on poor initial cluster centroids. Non-optimal clustering may even occur
    if the initial cluster centroids are well-placed, since *k*-means prefers clusters
    with similar sizes and densities. Where clusters are not approximately equal in
    size and density, *k*-means may fail to converge to the most natural clustering:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 上图说明了如何根据不良的初始聚类质心，*k*均值可能会收敛到局部最小值。如果初始聚类质心的位置合适，非最优聚类仍然可能发生，因为 *k* 均值倾向于选择具有相似大小和密度的聚类。在聚类大小和密度不大致相等时，*k*
    均值可能无法收敛到最自然的聚类：
- en: '![The drawbacks of k-means](img/7180OS_06_260.jpg)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![k-means的缺点](img/7180OS_06_260.jpg)'
- en: Also, *k*-means strongly prefers clusters that are "globular" in shape. Clusters
    with more intricate shapes are not well-identified by the *k*-means algorithm.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，*k* 均值算法强烈倾向于选择“球形”的聚类。形状较为复杂的聚类往往不能被 *k* 均值算法很好地识别。
- en: 'In the next chapter, we''ll see how a variety of dimensionality reduction techniques
    can help work around these problems. But before we get there, let''s develop an
    intuition for an alternative way of defining distance: as a measure of how far
    away from a "group" of things an element is.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将看到各种降维技术如何帮助解决这些问题。但在那之前，我们先来培养一种直觉，理解另一种定义距离的方式：作为衡量某个元素距离一“组”物体的远近。
- en: The Mahalanobis distance measure
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马哈拉诺比斯距离度量
- en: 'We saw at the beginning of the chapter how some distance measures may be more
    appropriate than others, given your data, by showing how the Jaccard, Euclidean,
    and cosine measures relate to data representation. Another factor to consider
    when choosing a distance measure and clustering algorithm is the internal structure
    of your data. Consider the following scatter plot:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时，我们通过展示 Jaccard、欧几里得和余弦距离如何与数据表示相关，看到了一些距离度量在特定数据下可能比其他度量更合适。选择距离度量和聚类算法时需要考虑的另一个因素是数据的内部结构。请考虑以下散点图：
- en: '![The Mahalanobis distance measure](img/7180OS_06_270.jpg)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![马哈拉诺比斯距离度量](img/7180OS_06_270.jpg)'
- en: 'It''s "obvious" that the point indicated by the arrow is distinct from the
    other points. We can clearly see that it''s far from the distribution of the others
    and therefore represents an anomaly. Yet, if we calculate the Euclidean distance
    of all points from the mean (the "centroid"), the point will be lost amongst the
    others that are equivalently far, or even further, away:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 很“显然”，箭头所指的点与其他点不同。我们可以清楚地看到它远离其他点的分布，因此代表了一个异常点。然而，如果我们计算所有点到均值（“质心”）的欧几里得距离，这个点将被其他距离相等或更远的点所掩盖：
- en: '[PRE72]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The preceding code generates the following chart:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下图表：
- en: '![The Mahalanobis distance measure](img/7180OS_06_280.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![马哈拉诺比斯距离度量](img/7180OS_06_280.jpg)'
- en: 'The Mahalanobis distance takes into account the covariance among the variables
    in calculating distances. In two dimensions, we can imagine the Euclidean distance
    as a circle growing out from the centroid: all points at the edge of the circle
    are equidistant from the centroid. The Mahalanobis distance stretches and skews
    this circle to correct for the respective scales of the different variables, and
    to account for correlation amongst them. We can see the effect in the following
    example:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 马哈拉诺比斯距离在计算距离时考虑了变量之间的协方差。在二维空间中，我们可以将欧几里得距离想象成一个从质心发出的圆：圆上所有的点与质心的距离相等。马哈拉诺比斯距离将这个圆拉伸并扭曲，以纠正不同变量的尺度差异，并考虑它们之间的相关性。我们可以在以下示例中看到这种效果：
- en: '[PRE73]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The preceding code uses the function provided by `incanter.stats` to plot the
    Mahalanobis distance between the same set of points. The result is shown on the
    following chart:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用了 `incanter.stats` 提供的函数来绘制相同数据点的马哈拉诺比斯距离。结果显示在下图中：
- en: '![The Mahalanobis distance measure](img/7180OS_06_290.jpg)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![马哈拉诺比斯距离度量](img/7180OS_06_290.jpg)'
- en: This chart clearly identifies one point in particular as being much more distant
    than the other points. This matches our perception that this point in particular
    should be considered as being further away from the others.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 该图清晰地标识出一个点与其他点相比，距离要远得多。这与我们对这个点应该被认为比其他点更远的看法一致。
- en: The curse of dimensionality
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度灾难
- en: 'There is one fact that the Mahalanobis distance measure is unable to overcome,
    though, and this is known as the curse of dimensionality. As the number of dimensions
    in a dataset rises, every point tends to become equally far from every other point.
    We can demonstrate this quite simply with the following code:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，马哈拉诺比斯距离度量无法克服一个事实，这就是所谓的维度灾难。随着数据集中的维度数量增加，每个点都趋向于和其他点一样远。我们可以通过下面的代码简单地演示这一点：
- en: '[PRE74]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The preceding code finds both the minimum and the maximum distance between
    any two pairs of points in a synthetic generated dataset of 100 points. As the
    number of dimensions approaches the number of elements in the set, we can see
    how the minimum and the maximum distance between each pair of elements approach
    one another:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码找出了在一个合成生成的100个点的数据集中，任意两个点对之间的最小距离和最大距离。随着维度数接近数据集中的元素数量，我们可以看到每对元素之间的最小距离和最大距离逐渐接近：
- en: '![The curse of dimensionality](img/7180OS_06_300.jpg)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![维度灾难](img/7180OS_06_300.jpg)'
- en: 'The effect is striking: as the number of dimensions increases, the distance
    between the closest two points rises too. The distance between the furthest two
    points rises as well, but at a slower rate. Finally, with 100 dimensions and 100
    data points, every point appears to be equally far from every other.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 这一效果非常显著：随着维度数量的增加，最接近的两个点之间的距离也在增加。最远的两个点之间的距离也增加，但增长速度较慢。最后，当维度为100，数据点为100时，每个点似乎与其他每个点的距离都相等。
- en: Of course, this is synthetic, randomly generated data. If we're attempting to
    cluster our data, we implicitly hope that it will have a discernible internal
    structure that we can tease out. Nonetheless, this structure will become more
    and more difficult to identify as the number of dimensions rises.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是合成的随机生成数据。如果我们尝试对数据进行聚类，我们隐含地希望数据中会有一个可识别的内部结构，我们可以将其提取出来。然而，随着维度数量的增加，这种结构会变得越来越难以识别。
- en: Summary
  id: totrans-411
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've learned about the process of clustering and covered the
    popular *k*-means clustering algorithm to cluster large numbers of text documents.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了聚类的过程，并介绍了流行的*k*-均值聚类算法，用于聚类大量的文本文件。
- en: This provided an opportunity to cover the specific challenges presented by text
    processing where data is often messy, ambiguous, and high-dimensional. We saw
    how both stop words and stemming can help to reduce the number of dimensions and
    how TF-IDF can help identify the most important dimensions. We also saw how *n*-grams
    and shingling can help to tease out context for each word at the cost of a vast
    proliferation of terms.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个机会，探讨文本处理所带来的具体挑战，其中数据通常是杂乱的、模糊的，并且是高维的。我们看到停用词和词干提取如何帮助减少维度的数量，以及TF-IDF如何帮助识别最重要的维度。我们还看到如何通过*n*-gram和shingling技术提取每个词的上下文，代价是大量的术语扩展。
- en: We've explored Parkour in greater detail and seen how it can be used to write
    sophisticated, scalable, Hadoop jobs. In particular, we've seen how to make use
    of the distributed cache and custom tuple schemas to write Hadoop job process
    data represented as Clojure data structures. We used both of these to implement
    a method for generating unique, cluster-wide term IDs.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经更详细地探讨了Parkour，并且看到了它如何用来编写复杂的、可扩展的Hadoop作业。特别是，我们看到了如何利用分布式缓存和自定义元组模式来编写处理Clojure数据结构表示的数据的Hadoop作业。我们用这两者实现了生成唯一的、跨集群的术语ID的方法。
- en: 'Finally, we witnessed the challenge presented by very high-dimensional spaces:
    the so-called "curse of dimensionality". In the next chapter, we''ll cover this
    topic in more detail and describe a variety of techniques to combat it. We''ll
    continue to explore the concepts of "similarity" and "difference" as we consider
    the problem of recommendation: how we can match users and items together.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们见证了高维空间带来的挑战：所谓的“维度灾难”。在下一章，我们将更详细地探讨这个话题，并描述一系列应对技术。我们将继续探索“相似性”和“差异性”的概念，同时考虑推荐问题：我们如何将用户与物品匹配起来。
