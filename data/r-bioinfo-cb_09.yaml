- en: Useful Statistical and Machine Learning Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有用的统计和机器学习方法
- en: In bioinformatics, the statistical analysis of datasets of varied size and composition
    is a frequent task. R is, of course, a hugely powerful statistical language with
    abundant options for all sorts of tasks. In this chapter, we will focus a little
    on some of those useful but not so often discussed methods that, while none of
    them make up an analysis in and of themselves, can be powerful additions to the
    analyses that you likely do quite often. We'll look at recipes for simulating
    datasets and machine learning methods for class prediction and dimensionality
    reduction.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物信息学中，对不同大小和组成的数据集进行统计分析是常见的任务。R无疑是一个功能强大的统计语言，拥有丰富的选项来处理各种任务。在本章中，我们将重点关注一些有用但不常讨论的方法，尽管这些方法本身并不构成完整的分析，但它们可以成为你经常进行的分析的有力补充。我们将查看模拟数据集的食谱，以及用于类别预测和降维的机器学习方法。
- en: 'The following recipes will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下食谱：
- en: Correcting p-values to account for multiple hypotheses
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 校正p值以考虑多重假设
- en: Generating a simulated dataset to represent a background
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成一个模拟数据集来表示背景
- en: Learning groupings within data and classifying with kNN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据中学习分组并使用kNN进行分类
- en: Predicting classes with random forests
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林预测类别
- en: Predicting classes with SVM
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SVM预测类别
- en: Learning groups in data without prior information
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在没有先验信息的情况下对数据进行分组学习
- en: Identifying the most important variables in data with random forests
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林识别数据中最重要的变量
- en: Identifying the most important variables in data with PCA
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PCA识别数据中最重要的变量
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The sample data you'll need is available from this book's GitHub repository
    at [https://github.com/PacktPublishing/R-Bioinformatics-Cookbook](https://github.com/PacktPublishing/R-Bioinformatics-Cookbook). If
    you want to use the code examples as they are written, then you will need to make
    sure that this data is in a sub-directory of whatever your working directory is.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要的示例数据可以从本书的GitHub仓库获取，网址是[https://github.com/PacktPublishing/R-Bioinformatics-Cookbook](https://github.com/PacktPublishing/R-Bioinformatics-Cookbook)。如果你想按原样使用代码示例，那么你需要确保这些数据位于你工作目录的子目录中。
- en: 'Here are the R packages that you''ll need. In general, you can install these
    with `install.packages("package_name")`. The packages listed under `Bioconductor`
    need to be installed with the dedicated installer. If you need to do anything
    further, installation will be described in the recipes in which the packages are
    used:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是你需要的R包。通常，你可以使用`install.packages("package_name")`来安装这些包。列在`Bioconductor`下的包需要使用专门的安装器安装。如果需要进一步的操作，安装过程将在使用这些包的食谱中进行描述：
- en: '`Bioconductor`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Bioconductor`'
- en: '`Biobase`'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Biobase`'
- en: '`caret`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`caret`'
- en: '`class`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class`'
- en: '`dplyr`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dplyr`'
- en: '`e1071`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`e1071`'
- en: '`factoextra`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`factoextra`'
- en: '`fakeR`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fakeR`'
- en: '`magrittR`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`magrittR`'
- en: '`randomForest`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`randomForest`'
- en: '`RColorBrewer`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RColorBrewer`'
- en: '`Bioconductor` is huge and has its own installation manager. You can install
    the manager with the following code:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`Bioconductor`非常庞大，并且有自己的安装管理器。你可以使用以下代码安装该管理器：'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, you can install the packages with this code:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用以下代码安装这些包：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Further information is available at [https://www.bioconductor.org/install/](https://www.bioconductor.org/install/).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息可以在[https://www.bioconductor.org/install/](https://www.bioconductor.org/install/)找到。
- en: Normally, in R, a user will load a library and use the functions directly by
    name. This is great in interactive sessions but it can cause confusion when many
    packages are loaded. To clarify which package and function I'm using at a given
    moment, I will occasionally use the `packageName::functionName()` convention.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在R中，用户会加载一个库并通过名称直接使用函数。这在交互式会话中非常方便，但当加载了多个包时，可能会引起混淆。为了明确我在使用哪个包和函数，我会偶尔使用`packageName::functionName()`这种约定。
- en: 'Sometimes, in the middle of a recipe, I''ll interrupt the code so you can see
    some intermediate output or the structure of an object that''s important to understand.
    Whenever that happens, you''ll see a code block where each line begins with `##`
    (double hash) symbols. Consider the following command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，在执行食谱的过程中，我会暂停代码，以便你能看到一些中间输出或对象的结构，这对理解非常重要。每当发生这种情况时，你会看到一个代码块，每行以`##`（双井号）符号开头。请考虑以下命令：
- en: '`letters[1:5]`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`letters[1:5]`'
- en: 'This will give us the following output:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '`## a b c d e`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`## a b c d e`'
- en: Note that the output lines are prefixed with `##`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出行的前缀是`##`。
- en: Correcting p-values to account for multiple hypotheses
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 校正p值以考虑多重假设
- en: In bioinformatics, particularly in genomics projects, we often perform statistical
    tests thousands of times in an analysis. But this can be a source of significant
    error in our results. Consider a gene expression experiment that has small numbers
    of measurements per treatment (often only three) but has tens of thousands of
    genes. A user doing a statistical test at *p <= 0.05* will reject the null hypothesis
    incorrectly five percent of the time. Correcting for performing multiple hypotheses
    allows us to reduce the error rate from such analyses. We will look at a simple-to-apply
    method for making such a correction.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物信息学，特别是在基因组学项目中，我们常常在分析中执行数千次统计检验。但这可能会导致我们结果中的显著误差。考虑一个基因表达实验，每个处理的测量数很少（通常只有三次），但基因数量有成千上万。一个执行统计检验的用户，在*p
    <= 0.05*的情况下，会错误地拒绝零假设5%的时间。对进行多个假设检验的校正可以帮助我们减少此类分析中的错误率。我们将查看一种简单的校正方法。
- en: Getting ready
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: All of the functions we need are base R and we will create our own data with
    code.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所有需要的函数都是R的基础函数，我们将通过代码创建自己的数据。
- en: How to do it...
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Correcting p-values to account for multiple hypotheses can be done using the
    following steps:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 校正p值以考虑多个假设的步骤如下：
- en: 'Run 10,000 t-tests:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行10,000次t检验：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Assess the number of p*-*values, `<= 0.05`:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估p*-*值数量，`<= 0.05`：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Adjust the p-values:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整p值：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Re-assess the number of p-values, `<= 0.05`:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新评估`<= 0.05`的p值数量：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works...
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The first line in *Step 1* simply fixes the random number generator so that
    we get consistent results between computers; you won't need this other than to
    compare the results in this book. The next part is to create a custom function
    that creates two sets (*x* and *y*) of 10 random numbers, then performs a t-test
    and returns the p-value. As these are just random numbers from the same distribution,
    there is no real difference. The final line uses the `sapply()` function to run
    our custom function and create a vector of 10,000 p-values.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 1*中的第一行代码简单地固定了随机数生成器，以便我们能够在不同计算机之间获得一致的结果；除了为了对比本书中的结果，你不需要这部分代码。接下来是创建一个自定义函数，生成两组（*x*和*y*）10个随机数，然后执行t检验并返回p值。由于这些只是来自相同分布的随机数，因此没有实际差异。最后一行使用`
    sapply()`函数运行我们自定义的函数并创建一个包含10,000个p值的向量。'
- en: 'In *Step 2*, we simply count the number of p-values that are lower than 0.05\.
    We get this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 2*中，我们仅仅统计p值小于0.05的数量。我们得到如下结果：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This indicates that we have 506 falsely called significant results.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示我们有506个错误判定为显著的结果。
- en: In *Step 3*, we use the `p.adjust()` function to apply a correction method.
    The `argument` method can be one of several available methods. In practice, it's
    best to try `holm` or `BH` (Benjamini Hochberg) as these give accurate false detection
    rates. A widely used but not very useful method is `Bonferroni`; avoid this in
    most cases.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 3*中，我们使用`p.adjust()`函数应用一种校正方法。`argument`方法可以是几种可用方法之一。实践中，最好尝试`holm`或`BH`（Benjamini
    Hochberg），因为这些方法提供了准确的假阳性率。一种广泛使用但效果不太好的方法是`Bonferroni`；大多数情况下应避免使用它。
- en: 'In *Step 4*, we re-assess the number of p-values that are lower than 0.05\.
    This time, it''s as we expect:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们重新评估小于0.05的p值数量。这次，结果正如我们预期的那样：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Generating a simulated dataset to represent a background
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成一个代表背景的模拟数据集
- en: Constructing simulated datasets for sensible controls, making appropriate comparisons
    to an expected background distribution, and having a proper background population
    from which to draw samples can be important aspects of many studies. In this recipe,
    we'll look at various ways of generating these either from scratch or by mixing
    up an existing dataframe.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模拟数据集以作为合理的对照，进行与预期背景分布的适当比较，并拥有一个合适的背景人群来抽取样本，这是许多研究的重要方面。在本配方中，我们将探讨从头开始或通过混合现有数据框来生成这些数据的各种方式。
- en: Getting ready
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: We'll use the `fakeR` package and the `iris` built-in dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`fakeR`包和内置的`iris`数据集。
- en: How to do it...
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Generating a simulated dataset to represent a background can be done using
    the following steps:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 生成一个代表背景的模拟数据集可以按照以下步骤进行：
- en: 'Make a random dataset with the same characteristics as a given set:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个具有与给定数据集相同特征的随机数据集：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Make a vector of normal random numbers with the mean and standard deviation
    of a given vector:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个均值和标准差与给定向量相同的正态分布随机数向量：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Make a vector of uniform random integers in a range:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个范围内创建一个均匀分布的随机整数向量：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Make a vector of the number of binomial successes:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个二项分布成功次数的向量：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Make a vector of random selections from a list, with a different probability
    for each:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从列表中随机选择元素，且每个元素的选择概率不同：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How it works...
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: '*Step* *1* uses the `fakeR` package function called `simulate_dataset()` to
    create a new dataset with the same number of values, identical column names, the
    same number of factor levels and level names, and the same number of rows as the
    source dataset (`iris`). The values are randomized but, otherwise, the dataframe
    is identical. Note how using the `str()` function reports identical structures
    for `iris` and the new `fake_iris` object:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤* *1* 使用了`fakeR`包中的`simulate_dataset()`函数，生成一个新的数据集，该数据集与源数据集（`iris`）具有相同数量的值、相同的列名、相同数量的因子水平和水平名称，以及相同数量的行。值是随机化的，但数据框架是完全相同的。注意，使用`str()`函数报告`iris`和新的`fake_iris`对象的结构是完全相同的：'
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In *Step 2*, our objective is to make a vector of random numbers with the same
    mean and standard deviation as those in the iris `Sepal.Length` column. To that
    end, we first calculate those quantities with `mean()` and `sd()`. Then, we use
    them as parameter values for the `mean` and `sd` arguments of the `rnorm()` function.
    Running `hist()` to plot the resulting `random_sepal_lengths` vector confirms
    the distribution and parameters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 2*中，我们的目标是生成一个随机数向量，它的均值和标准差与`iris`数据集中`Sepal.Length`列的均值和标准差相同。为此，我们首先使用`mean()`和`sd()`函数计算这些数值。然后，我们将这些值作为参数传递给`rnorm()`函数的`mean`和`sd`参数。通过运行`hist()`绘制生成的`random_sepal_lengths`向量，我们可以确认其分布和参数。
- en: 'In *Step 3*, we wish to create a vector of numeric (floating point) values
    that can occur with equal probability—this is analogous to repeated rolls of a
    dice: each option is equally likely. Indeed, in this recipe, we set the low value
    of the range (`low_num`) to 1 and the high value (`high_num`) to 6 to mimic that.
    We ask the `runif()` function for 1,500 values with those low and high values
    and, by plotting the result with `hist()`again, we can see the relatively level
    frequencies in each bin, confirming the uniformity of those values.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 3*中，我们希望创建一个数值（浮动点）向量，这些数值能够以相等的概率出现——这类似于反复掷骰子：每个选项的可能性是一样的。事实上，在这个步骤中，我们将范围的低值（`low_num`）设置为1，将高值（`high_num`）设置为6，以模拟这个过程。我们通过`runif()`函数生成1,500个符合这些低值和高值的随机数，并通过再次使用`hist()`绘制结果，我们可以看到每个区间内的频率相对均匀，从而确认这些数值的均匀性。
- en: In *Step 4*, we wish to mimic a coin-toss style probability experiment—a so-called
    binomial success probability distribution. We first must decide on the number
    of trials each time—in a coin-toss experiment, this is the number of coins we
    toss. Here, we set the `number_of_coins` variable to 1\. We must also decide the
    probability of success. Again, mimicking a coin-toss means we set the `p_heads`
    variable to 0.5\. To run the simulation, we pass these values to the `rbinom()` function,
    asking for 1,500 separate repeats of the experiment. The `hist()` function shows
    us the frequency of 0 successes (a tails toss) and 1 success (a heads toss) over
    all 1,500 repeats is roughly equal. Next, we change the number of trials to 5,
    by changing the value of the `number_of_coins` variable. This mimics an experiment
    where we are using five coins at every repetition. We again use `rbinom()` and
    plot the result with `hist()`, this time observing that two and three successes
    (heads) are the most common outcomes from a trial with five coins.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们希望模拟一个掷硬币式的概率实验——所谓的二项成功概率分布。我们首先必须决定每次实验的试验次数——在掷硬币实验中，这指的是我们掷的硬币数量。在这里，我们将`number_of_coins`变量设置为1。我们还必须决定成功的概率。同样，模拟掷硬币实验时，我们将`p_heads`变量设置为0.5。为了进行模拟，我们将这些值传递给`rbinom()`函数，请求进行1,500次独立的实验重复。`hist()`函数显示，0次成功（掷出反面）和1次成功（掷出正面）的频率在所有1,500次重复中大致相等。接下来，我们通过改变`number_of_coins`变量的值，将试验次数改为5，模拟每次重复实验时使用五枚硬币。我们再次使用`rbinom()`函数并通过`hist()`绘制结果，这时我们可以观察到，两个和三个成功（正面）是每次五枚硬币实验中最常见的结果。
- en: Finally, in *Step 5*, we look at selecting items from a vector with the `sample()` function.
    The first argument to sample is the vector to sample from—so, here, the integers
    1 to 10\. The second argument is the number of items to select—here, we select
    10\. Note that, by default, `sample()` will select without replacement, so that
    no item will appear twice, though each item in the vector has an equal probability
    of being selected each time. The second use of `sample()` sets the value of the
    `replacement` argument to `TRUE`, meaning that items can be selected repeatedly.
    This use also sets the `prob` argument—a vector containing the probabilities of
    selecting each value in the initial vector. Running this sample and putting the
    result through the `table()` function confirms that we get selections in the approximate
    probabilities expected.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在*步骤 5*中，我们通过`sample()`函数来选择向量中的项目。`sample`的第一个参数是要从中采样的向量——这里是整数1到10。第二个参数是要选择的项目数量——这里我们选择10个。请注意，默认情况下，`sample()`会进行不放回抽样，因此每个项目不会被选择两次，尽管向量中的每个项目每次都有相等的被选中概率。`sample()`的第二种用法将`replacement`参数设置为`TRUE`，意味着可以重复选择项目。此用法还设置了`prob`参数——一个包含选择初始向量中每个值的概率的向量。运行此采样并将结果传递给`table()`函数确认我们获得的选择符合预期的近似概率。
- en: Learning groupings within data and classifying with kNN
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据中的学习分组与kNN分类
- en: The **k-Nearest Neighbors** (**kNN**) algorithm is a supervised learning algorithm that,
    given a data point, will try to classify it based on its similarity to a set of
    training examples of known classes. In this recipe, we'll look at taking a dataset,
    dividing it into a test and train set, and predicting the test classes from a
    model built on the training set. These sorts of approaches are widely applicable
    in bioinformatics and can be invaluable in clustering when we have some known
    examples of our target classes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**k-最近邻（kNN）**算法是一种监督学习算法，它会根据给定的数据点，尝试根据其与一组已知类别的训练样本的相似性对其进行分类。在这个食谱中，我们将学习如何使用数据集，将其划分为测试集和训练集，并根据训练集构建的模型预测测试集的类别。这种方法广泛应用于生物信息学，并且在聚类分析中非常有价值，尤其是当我们有一些已知的目标类别示例时。'
- en: Getting ready
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we''ll need a few new packages: `caret`, `class`, `dplyr`,
    and `magrittr`. As a dataset, we will use the built-in `iris` dataset.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，我们需要几个新的包：`caret`、`class`、`dplyr`和`magrittr`。作为数据集，我们将使用内置的`iris`数据集。
- en: How to do it...
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何进行...
- en: 'Learning groupings within data and classifying with kNN can be done using the
    following steps:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下步骤可以在数据中进行学习分组并进行kNN分类：
- en: 'Scale the data and remove non-numeric columns:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准化数据并删除非数值列：
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Extract a training and test dataset:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取训练集和测试集：
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Make the model and predictions on the test set:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型并对测试集进行预测：
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Compare the prediction with the actual class:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较预测结果与实际类别：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How it works...
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 1*, we initially use `set.seed()` to ensure random number reproducibility
    and then scale each column of the dataset using the `dplyr mutate_if()` function.
    The first argument of `mutate_if()` is a condition to be tested; the `.funs` argument
    is the function to be applied if the condition is true. Here, then, we're applying
    the `scale()` function to a column of the `iris` dataframe and if it is numeric,
    returning a dataframe we call `scaled_iris`. Performing scaling between columns
    is very important in kNN as the magnitude of the actual values can have a strong
    effect, so we need them to be of similar scale between columns. Next, we make
    a copy of the `Species` column from the data as this contains the class labels
    and remove it from the dataframe by assigning `NULL` to the column—for the next
    steps, the dataframe should contain only numeric data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 1*中，我们首先使用`set.seed()`确保随机数的可复现性，然后使用`dplyr mutate_if()`函数对数据集的每一列进行标准化。`mutate_if()`的第一个参数是要测试的条件；`.funs`参数是如果条件为真则应用的函数。在这里，我们对`iris`数据框的一列应用`scale()`函数，如果该列是数值型，则返回我们称之为`scaled_iris`的数据框。列间的标准化在kNN中非常重要，因为实际值的大小可能会有很大影响，所以我们需要确保它们在各列之间的尺度相似。接下来，我们从数据中复制`Species`列，因为它包含类标签，并通过将列赋值为`NULL`将其从数据框中删除——接下来的步骤中，数据框应仅包含数值数据。
- en: In *Step 2*, we decide which rows should be in our training set and our test
    set. We use the `sample()` function to select from a vector of 1 to the number
    of rows in `iris`; we select 80% of the row numbers without a replacement so that
    `train_rows` is a vector of integers giving the rows from `scaled_iris`, which
    we will use in our training set. In the rest of this step, we use subsetting and
    negative subsetting to prepare the subsets of `scaled_iris` we will need.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 2*中，我们决定哪些行应该包含在训练集和测试集中。我们使用`sample()`函数从1到`iris`行数的向量中选择；我们无替代地选择80%的行号，因此`train_rows`是一个整数向量，表示`scaled_iris`中的行，我们将在训练集中使用它。在这一步的其余部分，我们使用子集和负子集来准备我们需要的`scaled_iris`子集。
- en: In *Step 3*, we apply the kNN algorithm with the `knn()` function to build the
    model and classify the test set in a single operation. The `train` argument gets
    the portion of the data we set aside for training, the `test` argument the portion
    for testing, and the `cl` (class) argument gets the labels for the training set.
    The `k` argument is the number of neighbors that should be used in classifying
    each unknown test point. The function returns a vector of predicted classes for
    each row in the test data, which we save in `test_set_predictions`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 3*中，我们应用kNN算法，通过`knn()`函数构建模型，并在一次操作中对测试集进行分类。`train`参数表示我们为训练预留的数据部分，`test`参数表示我们为测试预留的数据部分，`cl`（类别）参数表示训练集的标签。`k`参数是用于分类每个未知测试点的邻居数。该函数返回一个包含测试数据中每行预测类别的向量，我们将其保存在`test_set_predictions`中。
- en: 'In *Step 4*, we assess the predictions using the `caret` package function,
    `confusionMatrix().` This takes the predicted classes and real classes and creates
    a set of statistics, including the following table, which contains the `Real`
    labels in the rows and the `Predicted` labels in the columns. This model predicted
    one `versicolor` row as `virginica`, incorrectly, with all other predictions correct:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们使用`caret`包的`confusionMatrix()`函数评估预测结果。该函数接受预测类别和真实类别，并生成一组统计数据，包括以下表格，其中行表示`Real`标签，列表示`Predicted`标签。该模型错误地将一个`versicolor`行预测为`virginica`，但其余所有预测都正确：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Predicting classes with random forests
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林进行类别预测
- en: Random forests is another supervised learning algorithm that uses ensembles
    of decision trees to make many class predictions so that the most frequently called
    class becomes the model's final prediction. Random forests is useful generally
    as it will work with categorical and numerical data together and can be applied
    to classification and regression, and we'll use it again for predicting the most
    important variables in our data in the *Identifying the most important variables
    in data with random forests* recipe in this chapter. In this recipe, we'll use
    random forests to predict classes of data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是另一种监督学习算法，它使用决策树的集合来进行多次类别预测，最终预测最常出现的类别作为模型的最终预测。随机森林通常是有用的，因为它可以同时处理分类和数值数据，并且可以应用于分类和回归任务。在本章的*使用随机森林识别数据中最重要的变量*配方中，我们将再次使用它来预测数据中最重要的变量。在这个配方中，我们将使用随机森林来预测数据的类别。
- en: Getting ready
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 做好准备
- en: For this recipe, we'll need the `caret`and `randomForest` packages and the built-in
    `iris` dataset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，我们需要`caret`和`randomForest`包以及内置的`iris`数据集。
- en: How to do it...
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何执行此操作...
- en: 'Predicting classes with random forests can be done using the following steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机森林进行类别预测可以通过以下步骤完成：
- en: 'Prepare a training set from the `iris` data:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`iris`数据集中准备训练集：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Build a model on the training data:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据上构建模型：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Use the model to make predictions on the test data:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型对测试数据进行预测：
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How it works...
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The whole of *Step 1* is the preparation of training and test sets. We use the `sample()` function
    to select from a vector of 1 to the number of rows in `iris`; we select 80% of
    the row numbers without a replacement so that `train_rows` is a vector of integers
    giving the rows from `iris`, which we will use in our training set. In the rest
    of this step, we use subsetting and negative subsetting to prepare the subsets
    of `iris` we will need.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 1*的整个过程是准备训练集和测试集。我们使用`sample()`函数从1到`iris`行数的向量中选择；我们无替代地选择80%的行号，因此`train_rows`是一个整数向量，表示`iris`中的行，我们将在训练集中使用它。在这一步的其余部分，我们使用子集和负子集来准备我们需要的`iris`子集。'
- en: In *Step 2*, we proceed directly to build a model we make predictions with.
    The `randomForest()` function takes, at its first argument, an R formula naming
    the column to be predicted (in other words, `Species`, the response variable),
    and the dataframe columns to use as training data—here, we use all columns, which
    we express as a `.` character. The `data` argument is the name of the source dataframe
    and the `mtry` argument is a tunable parameter that tells the algorithm how many
    splits to use. The best value of this is usually around the square root of the
    number of columns, but optimizing it can be helpful. The resulting model is saved
    in a variable called `model`, which can be printed for inspection.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 2* 中，我们直接进行模型构建和预测。`randomForest()`函数以其第一个参数为 R 公式，命名要预测的列（即`Species`，响应变量），并且是训练数据的数据框列——在这里，我们使用所有列，表示为一个`.`字符。`data`参数是源数据框的名称，`mtry`参数是一个可调参数，告诉算法使用多少分裂。这个最佳值通常是列数的平方根，但优化它可能会有帮助。生成的模型保存在一个名为`model`的变量中，可以打印进行检查。
- en: 'At *Step 3*, we use the `predict()` function with `model`, the `test_set` data,
    and the `type` argument set to `class` to predict the classes of the test set.
    We then assess them with `caret::confusionMatrix()`to give the following result:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 3* 中，我们使用`predict()`函数与`model`、`test_set`数据和设置为`class`的`type`参数来预测测试集的类别。然后，我们使用`caret::confusionMatrix()`来评估它们，得到以下结果：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The result indicates that the test set was classified perfectly.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明测试集完美分类。
- en: There's more
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容
- en: 'It is possible to perform regression (the prediction of a numeric value) with
    a very similar approach. Look at the similarity of the following code for building
    a regression and doing an assessment. Here, we predict sepal length based on the
    other columns. After model building, we run the prediction as before; note how
    we drop the `type` argument (as regression is actually the default). Finally,
    we assess by calculating the **Mean Squared Error **(**MSE**), in which we square
    the difference between the prediction and the actual value for sepal length and
    then take the mean of both:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用非常相似的方法进行回归（预测数值）预测。看看以下建立回归模型和进行评估的代码的相似性。在这里，我们基于其他列预测萼片长度。模型构建后，我们像以前一样运行预测；注意我们如何删除`type`参数（因为实际上回归是默认的）。最后，我们通过计算**均方误差**（**MSE**）来评估，在其中我们平方预测值和萼片长度的实际值之间的差异，然后取均值：
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Predicting classes with SVM
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SVM 预测类别
- en: The **support vector machine** (**SVM**) algorithm is a classifier that works
    by finding the maximum distance between classes in multiple dimensions of data—effectively
    the largest gap between classes—and uses the middle point of that gap as a boundary
    for classification. In this recipe, we'll look at using the SVM for peforming
    supervised class prediction and illustrating the boundary graphically.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）算法是一种分类器，通过在数据的多个维度中找到类之间的最大距离——有效地是类之间最大的间隙——并使用该间隙的中点作为分类的边界。在这个配方中，我们将使用
    SVM 来执行监督类别预测，并通过图形化地说明边界。'
- en: Getting ready
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: We'll continue to use the built-in `iris` dataset and the `e1071` package.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用内置的`iris`数据集和`e1071`包。
- en: How to do it...
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Predicting classes with SVM can be done using the following steps:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SVM 预测类别可以通过以下步骤完成：
- en: 'Build the training and test sets:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建训练集和测试集：
- en: '[PRE24]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Construct the model:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型：
- en: '[PRE25]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Plot the boundary of the model:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制模型边界：
- en: '[PRE26]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Make predictions on the test set:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上进行预测：
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: How it works...
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In *Step 1*, we have the probably familiar train and test set generation step
    we discussed in the previous recipes. Briefly, here, we create a vector of row
    numbers to use as a training set and use subsetting and negative subsetting to
    extract to new sub-datasets.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 1* 中，我们有可能熟悉的训练集和测试集生成步骤，我们在前面的配方中讨论过。简而言之，在这里，我们创建一个行号向量作为训练集，并使用子集和负子集来提取新的子数据集。
- en: In *Step 2*, we proceed to create the model using the `svm()` function. The
    first argument is an R formula that specifies the column to use as the classes
    (the response variable, `Species`), and after `~`, we use the `.` character to
    mean that all other columns are to be used as the data from which to build the
    model. We set the `data` argument to the `train_set` dataframe and select appropriate
    values for the `kernel` and `gamma` type. `type` may be classification- or regression-based; `kernel`
    is one of a variety of functions that are designed for different data and problems;
    and `gamma` is a parameter for the kernel. You may wish to check the function
    documentation for details. These values can also be optimized empirically.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2步*中，我们使用`svm()`函数创建模型。第一个参数是一个R公式，指定了要用作类别的列（响应变量`Species`），在`~`后，我们使用`.`字符表示所有其他列将作为构建模型的数据。我们将`data`参数设置为`train_set`数据框，并为`kernel`和`gamma`类型选择合适的值。`type`可以是基于分类或回归的；`kernel`是为不同数据和问题设计的多种函数之一；而`gamma`是核函数的一个参数。你可能希望查看函数文档以获取详细信息，这些值也可以通过经验进行优化。
- en: In *Step 3*, we create some objects that we can use to render the four-dimensional
    boundary in two dimensions. First, we select the columns we don't want to plot
    (those to hold constant), then we use the `lapply()` function to iterate over
    a character vector of those column names and apply a function to calculate the
    mean of the named column. We add column names to the resultant list in the `cols_to_hold` variable. We
    then use the generic `plot()` function, passing the model, the training data to
    plot, the two dimensions to plot as a formula (`Petal.Width ~ Petal.Length`),
    and a `slice` argument that takes our means from the other columns in the `held_constant`
    list.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3步*中，我们创建了一些对象，用来在二维空间中呈现四维边界。首先，我们选择不需要绘制的列（这些列将保持不变），然后使用`lapply()`函数遍历这些列名的字符向量，应用一个函数计算命名列的均值。我们将列名添加到`cols_to_hold`变量中的结果列表中。接着，我们使用通用的`plot()`函数，传入模型、训练数据、绘图的两个维度（通过公式`Petal.Width
    ~ Petal.Length`指定），以及一个`slice`参数，从`held_constant`列表中提取其他列的均值。
- en: 'The result looks like this, showing the margins in colors for each class:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示，显示了每个类别的边界颜色：
- en: '![](img/95c47bd6-d945-4fc3-a22d-3c00437a076f.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95c47bd6-d945-4fc3-a22d-3c00437a076f.png)'
- en: In *Step 4*, we repeat the predictions on the test set using `predict()` and
    generate the confusion matrix with `caret::confusionMatrix()` to see the accuracy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4步*中，我们使用`predict()`对测试集进行预测，并通过`caret::confusionMatrix()`生成混淆矩阵以查看准确性。
- en: Learning groups in data without prior information
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在没有先验信息的情况下学习数据中的分组
- en: It is common in bioinformatics to want to classify things into groups without
    first knowing what or how many groups there may be. This process is usually known
    as clustering and is a type of unsupervised machine learning. A common place for
    this approach is in genomics experiments, particularly RNAseq and related expression
    technologies. In this recipe, we'll start with a large gene expression dataset
    of around 150 samples, learn how to estimate how many groups of samples there
    are, and apply a method to cluster them based on the reduction of dimensionality
    with **Principal Component Analysis **(**PCA**), followed by a k-means cluster.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物信息学中，通常需要在没有预先知道分组信息或数量的情况下将事物分类。这一过程通常被称为聚类，是一种无监督的机器学习方法。这种方法常见于基因组学实验，特别是在RNA测序及相关表达技术中。在本教程中，我们将从一个包含约150个样本的大型基因表达数据集开始，学习如何估算样本的分组数量，并应用一种基于**主成分分析**（**PCA**）降维的方法进行聚类，接着使用k均值聚类。
- en: Getting ready
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we'll need the `factoextra` and `biobase` libraries (the latter
    from `Bioconductor`) and the `modencodefly_eset.RData` file from the `datasets/ch1`
    folder of this book's repository.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们需要`factoextra`和`biobase`库（后者来自`Bioconductor`），以及本书仓库中的`datasets/ch1`文件夹中的`modencodefly_eset.RData`文件。
- en: How to do it...
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Learning about groups in data without prior information can be done using the
    following steps:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有先验信息的情况下，了解数据中的分组可以通过以下步骤完成：
- en: 'Load the data and run a PCA:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据并运行PCA：
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Extract the principal components and estimate the optimal clusters:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取主成分并估算最佳聚类：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Perform k-means clustering and visualizing:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行k均值聚类并进行可视化：
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: How it works...
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In *Step* *1*, we use the `load()` function to import the `modencodefly.eset`
    object into memory; this is a gene expression dataset. Then, we use the `Biobase`
    function, called `exprs()` to extract the expression measurements as a rectangular
    matrix and pass that to the `prcomp()` function, which  performs PCA and returns
    a PCA object, which we store in the `expr_pca` variable.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤1*中，我们使用`load()`函数将`modencodefly.eset`对象导入内存；这是一个基因表达数据集。然后，我们使用`Biobase`函数`exprs()`提取表达测量值，并将其传递给`prcomp()`函数，后者执行PCA并返回一个PCA对象，我们将其存储在`expr_pca`变量中。
- en: 'We then plot the PCA with the `factoextra` function, `fviz_screeplot()`, and
    see the following diagram:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用`factoextra`函数`fviz_screeplot()`绘制PCA图，并看到以下图示：
- en: '![](img/d0a8b873-4af6-4684-b24c-7bd281470469.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0a8b873-4af6-4684-b24c-7bd281470469.png)'
- en: This shows how much of the variance within the data is captured by each principal
    component. The first three components capture over 70% of the variance. Hence,
    we can use these three instead of the whole 150-column dataset, simplifying the
    process and speeding up the analysis greatly.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了每个主成分所捕获的数据变异度。前三个成分捕获了超过70%的变异度。因此，我们可以使用这三个成分，而不是整个150列的数据集，从而大大简化过程并加速分析。
- en: 'In *Step 2*, we extract the main components using subsetting on the rotation
    slot of the `expr_pca` object, extracting the first three columns—these correspond
    to the first three components. We save these in a variable called `main_components`
    and use the `fviz_nbclust()` function on `main_components` and the `kmeans` function
    to create the following diagram:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤2*中，我们通过对子集化`expr_pca`对象的旋转槽提取主要成分，提取前三列——这些列对应于前三个成分。我们将它们保存在一个名为`main_components`的变量中，并使用`fviz_nbclust()`函数对`main_components`和`kmeans`函数进行处理，生成以下图示：
- en: '![](img/236e1075-e0a6-4720-97b5-72b15747af67.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/236e1075-e0a6-4720-97b5-72b15747af67.png)'
- en: In this function, the data is divided into increasing amounts of clusters and
    the `wss` (**Within Sum of Squares**), a measure of variability within the cluster.
    The diagram shows that the **Within Sum of Squares** measure decreases greatly
    up until about 5 clusters, after which no improvement is seen, indicating that
    the data contains about 5 clusters.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，数据被划分为越来越多的聚类，同时计算`wss` (**组内平方和**)，这是衡量聚类内变异性的一个指标。图示表明，**组内平方和**在约5个聚类之前大幅下降，之后没有明显改善，表明数据大约包含5个聚类。
- en: 'In *Step 3*, we perform a k-means cluster using the `kmeans()` function, providing
    `main_components` as data for the first argument and `5` for the number of clusters
    as the second argument. The values for the `nstart` and `iter.max` arguments are
    reasonable options for most runs of the algorithm. Finally, we pass the `kmeans_clust`
    object to the `fviz_cluster()` function and set some display options to get the
    following diagram:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤3*中，我们使用`kmeans()`函数执行k-means聚类，将`main_components`作为第一个参数的数据，将`5`作为聚类数的第二个参数。`nstart`和`iter.max`参数的值对于大多数算法运行来说是合理的选择。最后，我们将`kmeans_clust`对象传递给`fviz_cluster()`函数，并设置一些显示选项，生成以下图示：
- en: '![](img/7358aec3-8fc4-4b59-82dd-606ade79fcfb.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7358aec3-8fc4-4b59-82dd-606ade79fcfb.png)'
- en: There's more
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多
- en: 'We have performed k-means clustering for the samples or columns of this dataset.
    If you wish to do the same for genes or rows, extract the main components from
    the unrotated data in the *x* slot in *Step 2*:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经对这个数据集的样本或列进行了k-means聚类。如果你希望对基因或行做同样的操作，可以从*步骤2*中的未旋转数据的*x*槽中提取主要成分：
- en: '[PRE31]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If you wish to get the actual cluster IDs for each sample, that is stored in
    the `cluster` slot of the `kmeans_clus` object:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望获取每个样本的实际聚类ID，可以在`kmeans_clus`对象的`cluster`槽中找到：
- en: '[PRE32]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Identifying the most important variables in data with random forests
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林识别数据中最重要的变量
- en: We've already seen the random forests algorithm in use in this chapter, in the
    *Predicting classes with random forests* recipe, where we used it for class prediction
    and regression. Here, we're going to use it for a different purpose—to try and
    work out which of the variables in a dataset contribute most to the classification
    or regression accuracy of the trained model. This requires only a simple change
    to the code we already have and a new function or two.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到过随机森林算法的应用，在*使用随机森林预测类别*的示例中，我们用它进行类别预测和回归。这里，我们将用它来做不同的事情——尝试找出数据集中哪些变量对训练模型的分类或回归准确度贡献最大。这只需要对已有代码做一个简单的修改，并使用一个或两个新函数。
- en: Getting ready
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: We'll need the `randomForest` package and the built-in `iris` dataset.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要`randomForest`包和内置的`iris`数据集。
- en: How to do it...
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Identifying the most important variables in data with random forests can be
    done using the following steps:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机森林识别数据中最重要的变量可以通过以下步骤完成：
- en: 'Prepare the training and test data:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备训练数据和测试数据：
- en: '[PRE33]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Train the model and create the `importance` plot:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型并创建`importance`图：
- en: '[PRE34]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: How it works...
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 1*, we perform a similar dataset split to those in several previous
    recipes. Using the `sample()` function, we create a list of 80% of the row numbers
    of the original `iris` data and then, using subsetting and negative subsetting,
    we extract the rows.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤1*中，我们进行类似于之前几个配方中的数据集拆分。使用`sample()`函数，我们创建了一个包含原始`iris`数据80%行号的列表，然后，使用子集和负子集提取这些行。
- en: 'In *Step 2*, we train the model using the `randomForest()` function. The first
    argument here is a formula; we''re specifying that `Species` is the value we wish
    to predict based on all other variables, which are described by `. `. `data` is
    our `train_set` object. The key in this recipe is to make sure we set the `importance`
    variable to `TRUE`, meaning the model will test variables that, when left out
    of the model building, cause the biggest decrease in accuracy. Once the model
    is built and tested, we can visualize the importance of each variable with the
    `varImpPlot()` function. In doing so, we get the following diagram:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤2*中，我们使用`randomForest()`函数训练模型。这里的第一个参数是一个公式；我们指定`Species`是我们希望预测的值，基于所有其他变量，这些变量由`.`描述。`data`是我们的`train_set`对象。此配方的关键是确保将`importance`变量设置为`TRUE`，这意味着模型将测试哪些变量在从模型构建中省略时，会导致准确度的最大下降。一旦模型构建并测试完毕，我们可以使用`varImpPlot()`函数可视化每个变量的重要性。这样，我们得到以下图表：
- en: '![](img/c17eef5b-627c-46bf-8368-9da91dad763d.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c17eef5b-627c-46bf-8368-9da91dad763d.png)'
- en: We can see that it is the `Petal.Width` and `Petal.Length` variables that, when
    left out, cause the greatest decrease in model accuracy, so are, by this measure,
    the most important.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当省略`Petal.Width`和`Petal.Length`变量时，会导致模型准确度的最大下降，因此，根据这一标准，它们是最重要的。
- en: Identifying the most important variables in data with PCA
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PCA识别数据中最重要的变量
- en: We've seen PCA in use in the *Learning groups in data without prior information* recipe
    as a dimensionality reduction technique—a method for reducing the size of our
    dataset whilst retaining the important information. As you might imagine, that
    means that we can get an idea of which of the original variables are contributing
    most to our reduced representation and we can, therefore, work out which are the
    most important. We'll see how that works in this recipe.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在*无先验信息的数据分组学习*配方中看到过PCA的应用，它是一种降维技术——一种在保留重要信息的同时减少数据集大小的方法。正如你所想，这意味着我们可以了解哪些原始变量对降维后的表示贡献最大，因此可以确定哪些是最重要的。我们将在本配方中看到这一点。
- en: Getting ready
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: For this recipe, we'll use the `factoextra` package and the built-in `iris`
    dataset.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此配方，我们将使用`factoextra`包和内置的`iris`数据集。
- en: How to do it...
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Identifying the most important variables in data with PCA can be done using
    the following steps:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA识别数据中最重要的变量可以通过以下步骤完成：
- en: 'Perform PCA:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行PCA：
- en: '[PRE35]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Create a variable plot:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建变量图：
- en: '[PRE36]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works...
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This brief recipe begins in *Step 1* with the simple construction of `pca_result`
    from the `prcomp()` function. We pass the `iris` data as the first argument (without
    the fifth categorical column) and scale and center the data—this stops magnitude
    differences from measurements in different scales taking up inappropriate weights.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简短的配方在*步骤1*开始时简单构建了从`prcomp()`函数获得的`pca_result`。我们将`iris`数据作为第一个参数（不包括第五个分类列），并对数据进行缩放和中心化——这可以避免不同量纲的测量差异占用不当的权重。
- en: 'With the `pca_result` constructed, we can plot the variables using the `fviz_pca_var()`
    function to get the following diagram:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 构建好`pca_result`后，我们可以使用`fviz_pca_var()`函数绘制变量，得到以下图表：
- en: '![](img/4c0dc688-49cd-460f-a55c-fddc5a822b85.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c0dc688-49cd-460f-a55c-fddc5a822b85.png)'
- en: In it, we can see arrows depicting each variable. The angle at which an arrow
    moves away from the center indicates a characteristic of the variable; the closer
    the arrows are, the more similar the variables—hence, `Petal.Length` and `Petal.Width`
    are highly correlated variables. The color of the arrows indicates a complicated
    quantity (called `cos2`), which represents the quality of the contribution of
    the variable. The higher the contribution of the variable, the higher `cos2`.
    Here, we can see that `Sepal.Width` and `Petal.Length` contribute well to the
    PCA. `Petal.Width` is too similar to be considered. This is a different result
    to that of the *Identifying the most important variables in data with random forests* recipe,
    as the two techniques are asking different questions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中，我们可以看到箭头表示每个变量。箭头从中心移动的角度表示变量的一个特征；箭头之间的距离越近，变量越相似——因此，`Petal.Length`和`Petal.Width`是高度相关的变量。箭头的颜色表示一个复杂的量（称为`cos2`），它代表变量贡献的质量。变量的贡献越高，`cos2`就越高。在这里，我们可以看到`Sepal.Width`和`Petal.Length`对主成分分析（PCA）的贡献较大。`Petal.Width`由于过于相似，无法被考虑。这与*通过随机森林识别数据中最重要的变量*的结果不同，因为这两种技术提出的问题不同。
