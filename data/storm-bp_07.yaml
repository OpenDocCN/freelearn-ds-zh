- en: Chapter 7. Integrating Druid for Financial Analytics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 集成Druid进行金融分析
- en: In this chapter, we will extend the use of Trident to create a real-time financial
    analytics dashboard. The system will process financial messages to provide stock
    pricing information over time at various levels of granularity. The system will
    demonstrate integration with a non-transactional system using custom state implementations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将扩展Trident的使用，创建一个实时金融分析仪表板。该系统将处理金融消息，以在不同粒度上随时间提供股票定价信息。该系统将展示与非事务性系统的集成，使用自定义状态实现。
- en: In the previous example, we used Trident to tally running totals of events over
    time. It was sufficient for the simple use case that analyzed only a single dimension
    of the data, but the architectural design was not flexible. To introduce a new
    dimension would have required Java development and the deployment of new code.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们使用Trident来统计随时间变化的事件总数。对于分析数据的简单用例来说已经足够了，但是架构设计并不灵活。要引入新的维度需要Java开发和部署新代码。
- en: Traditionally, data warehousing techniques and business intelligence platforms
    are used to compute and store dimensional analytics. The warehouses are deployed
    as part of an **On-line Analytics Processing** (**OLAP**) system, which is separated
    out from the **On-line Transaction Processing** (**OLTP**). Data propagates down
    to the OLAP system, but typically after some lag. This is a sufficient model for
    retrospective analytics, but does not suffice in situations that require real-time
    analytics.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，数据仓库技术和商业智能平台用于计算和存储维度分析。数据仓库作为**On-line Analytics Processing** (**OLAP**)系统的一部分部署，与**On-line
    Transaction Processing** (**OLTP**)分开。数据传播到OLAP系统，但通常有一定的滞后。这对于回顾性分析是足够的，但在需要实时分析的情况下不够。
- en: Similarly, other approaches use batch-processing techniques to empower data
    scientists. Data scientists use languages such as PIG to express their queries.
    Then, these queries compile down into jobs that run over large sets of data. Fortunately,
    they run on platforms such as Hadoop that distribute the processing across many
    machines, but this still introduces a substantial delay.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，其他方法使用批处理技术来赋予数据科学家能力。数据科学家使用诸如PIG之类的语言来表达他们的查询。然后，这些查询编译成在大量数据集上运行的作业。幸运的是，它们在分布式处理的平台上运行，如Hadoop，但这仍然引入了相当大的延迟。
- en: Both of these approaches fall short for financial systems, which cannot afford
    such a lag in the availability of the analytics. The overhead alone of spinning
    up a batch-processing job might be too much of a delay for the real-time demands
    of a financial system.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法对于金融系统来说都不够，金融系统无法承受分析数据的可用性出现滞后。仅仅启动批处理作业的开销可能对金融系统实时需求造成太大延迟。
- en: In this chapter, we will extend our use of Storm to deliver a flexible system
    that requires only minimal effort to introduce new dimensions, while simultaneously
    providing real-time analytics. By that, we mean only a short delay between data
    ingestion and availability of the dimensional analytics.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将扩展我们对Storm的使用，以提供一个灵活的系统，只需要很少的工作就可以引入新的维度，同时提供实时分析。这意味着数据摄入和维度分析的可用性之间只有很短的延迟。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Custom state implementations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义状态实现
- en: Integration with non-transactional storage
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与非事务性存储的集成
- en: Use of ZooKeeper for distributed state
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ZooKeeper进行分布式状态
- en: Druid and real-time aggregate analytics
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Druid和实时聚合分析
- en: Use case
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例
- en: In our use case, we will tap into orders for shares of stock in a financial
    system. Using this information, we will deliver pricing information over time,
    which is available via a **REpresentational State Transfer** (**REST**) interface.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的用例中，我们将利用金融系统中股票订单的信息。利用这些信息，我们将随时间提供定价信息，这些信息可以通过**REpresentational State
    Transfer** (**REST**)接口获得。
- en: The canonical message format in the financial industry is the **Financial Information
    eXchange** (**FIX**) format. The specification for this format can be found at
    [http://www.fixprotocol.org/](http://www.fixprotocol.org/).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 金融行业中的规范消息格式是**Financial Information eXchange** (**FIX**)格式。该格式的规范可以在[http://www.fixprotocol.org/](http://www.fixprotocol.org/)找到。
- en: 'An example FIX message is shown as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个FIX消息的示例如下：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: FIX messages are essentially streams of key-value pairs. The ASCII character
    01, which is **Start of Header** (**SOH**), delimits the pairs. FIX refers to
    the keys as tags. As shown in the preceding message, tags are identified by integers.
    Each tag has an associated field name and data type. For a full reference of tag
    types go to [http://www.fixprotocol.org/FIXimate3.0/en/FIX.4.2/fields_sorted_by_tagnum.html](http://www.fixprotocol.org/FIXimate3.0/en/FIX.4.2/fields_sorted_by_tagnum.html).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: FIX消息本质上是键值对流。ASCII字符01，即**Start of Header** (**SOH**)，分隔这些键值对。FIX将键称为标签。如前面的消息所示，标签由整数标识。每个标签都有一个关联的字段名和数据类型。要查看标签类型的完整参考，请转到[http://www.fixprotocol.org/FIXimate3.0/en/FIX.4.2/fields_sorted_by_tagnum.html](http://www.fixprotocol.org/FIXimate3.0/en/FIX.4.2/fields_sorted_by_tagnum.html)。
- en: 'The important fields for our use case are shown in the following table:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用例中的重要字段显示在以下表格中：
- en: '| Tag ID | Field name | Description | Data type |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 标签ID | 字段名 | 描述 | 数据类型 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `11` | `CIOrdID` | This is the unique identifier for message. | String |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| `11` | `CIOrdID` | 这是消息的唯一标识符。 | 字符串 |'
- en: '| `35` | `MsgType` | This is the type of the FIX message. | String |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| `35` | `MsgType` | 这是FIX消息的类型。 | 字符串 |'
- en: '| `44` | `Price` | This is the stock price per share. | Price |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| `44` | `价格` | 这是每股股票的股价。| 价格 |'
- en: '| `55` | `Symbol` | This is the stock symbol. | String |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `55` | `符号` | 这是股票符号。 | 字符串 |'
- en: FIX is a layer on top of the TCP/IP protocol. Thus, in a real system, these
    messages are received over TCP/IP. For ease of integration with Storm, the system
    could queue those messages in Kafka. However, for our example, we will simply
    ingest a file filled with the FIX messages. FIX supports multiple message types.
    Some are used for control messages (for example, Logon, Heartbeat, and so on).
    We will filter out those messages, passing only the types that include price information
    to the analytics engine.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: FIX是TCP/IP协议的一层。因此，在实际系统中，这些消息是通过TCP/IP接收的。为了与Storm轻松集成，系统可以将这些消息排队在Kafka中。然而，在我们的示例中，我们将简单地摄取一个填满FIX消息的文件。FIX支持多种消息类型。有些用于控制消息（例如，登录，心跳等）。我们将过滤掉这些消息，只传递包含价格信息的类型到分析引擎。
- en: Integrating a non-transactional system
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成非事务系统
- en: To extend on our previous example, we could develop a framework for the configuration
    that would allow the user to specify the dimensions along which they would like
    to aggregate events. Then, we could use that configuration in our topology to
    maintain a set of in-memory data sets to accumulate the aggregations, but any
    in-memory store is susceptible to faults. To address fault-tolerance, we could
    then make those aggregations persist in a database.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展我们之前的示例，我们可以开发一个配置框架，允许用户指定他们想要对事件进行聚合的维度。然后，我们可以在我们的拓扑中使用该配置来维护一组内存数据集来累积聚合，但任何内存存储都容易出现故障。为了解决容错性，我们可以将这些聚合持久存储在数据库中。
- en: We would need to anticipate and support all the different types of aggregations
    the user would like to perform (for example, sum, average, geospatial, and so
    on). This seems like a substantial endeavor.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要预期并支持用户想要执行的所有不同类型的聚合（例如，总和，平均，地理空间等）。这似乎是一项重大的努力。
- en: 'Fortunately, there are options for real-time analytics engines. One popular
    open-source option is Druid. The following article is taken from their whitepaper
    found at [http://static.druid.io/docs/druid.pdf](http://static.druid.io/docs/druid.pdf):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有实时分析引擎的选项。一个流行的开源选项是Druid。以下文章摘自他们在[http://static.druid.io/docs/druid.pdf](http://static.druid.io/docs/druid.pdf)找到的白皮书：
- en: Druid is an open source, real-time analytical data store that supports fast
    ad-hoc queries on large-scale data sets. The system combines a column-oriented
    data layout, a shared-nothing architecture, and an advanced indexing structure
    to allow for the arbitrary exploration of billion-row tables with sub-second latencies.
    Druid scales horizontally and is the core engine of the Metamarkets data analytics
    platform.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Druid是一个开源的、实时的分析数据存储，支持对大规模数据集进行快速的自由查询。该系统结合了列导向的数据布局、共享无内容架构和先进的索引结构，允许对十亿行表进行任意探索，延迟在亚秒级。Druid可以水平扩展，是Metamarkets数据分析平台的核心引擎。
- en: From that excerpt, Druid exactly fits our requirements. Now, the challenge is
    integrating it with Storm.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述摘录中，Druid正好符合我们的要求。现在，挑战是将其与Storm集成。
- en: Druid's technology stack fits naturally into a Storm-based ecosystem. Like Storm,
    it uses ZooKeeper to coordinate between its nodes. Druid also supports direct
    integration with Kafka. For some cases, this may be appropriate. In our example,
    to demonstrate integration of a non-transactional system, we will integrate Druid
    with Storm directly.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Druid的技术堆栈自然地适应了基于Storm的生态系统。与Storm一样，它使用ZooKeeper在其节点之间进行协调。Druid还支持与Kafka的直接集成。对于某些情况，这可能是合适的。在我们的示例中，为了演示非事务系统的集成，我们将直接将Druid与Storm集成。
- en: 'We will include a brief description of Druid here. However, for more detailed
    information on Druid, refer to the following website:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里简要介绍Druid。但是，有关Druid的更详细信息，请参阅以下网站：
- en: '[https://github.com/metamx/druid/wiki](https://github.com/metamx/druid/wiki)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/metamx/druid/wiki](https://github.com/metamx/druid/wiki)'
- en: Druid collects information via its **Real-time** nodes. Based on a configurable
    granularity, the **Real-time** nodes collect the event information into segments
    that are persisted permanently in a deep storage mechanism. Druid persistently
    stores the metadata for those segments in MySQL. The **Master** node recognizes
    the new segment, identifies **Compute** nodes for that segment based on rules,
    and notifies the **Compute** nodes to pull the new segment. A **Broker** node
    sits in front of the **Compute** nodes, receives `REST` queries from consumers,
    and distributes those queries to the appropriate **Compute** nodes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Druid通过其**实时**节点收集信息。根据可配置的粒度，**实时**节点将事件信息收集到永久存储在深度存储机制中的段中。Druid持久地将这些段的元数据存储在MySQL中。**主**节点识别新段，根据规则为该段识别**计算**节点，并通知**计算**节点拉取新段。**代理**节点坐在**计算**节点前面，接收来自消费者的`REST`查询，并将这些查询分发给适当的**计算**节点。
- en: 'Thus, an architecture that integrates Storm with Druid looks similar to what
    is shown in the following diagram:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将Storm与Druid集成的架构看起来与以下图表所示的类似：
- en: '![Integrating a non-transactional system](img/8294OS_07_01.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![集成非事务系统](img/8294OS_07_01.jpg)'
- en: As depicted in the preceding diagram, there are three data storage mechanisms
    involved. The **MySQL** database is a simple metadata repository. It contains
    all the metadata information for all of the segments. The **Deep Storage** mechanism
    contains the actual segment information. Each segment contains a merged index
    of the events for a specific time period based on the dimensions and aggregations
    defined in a configuration file. As such, segments can be large (for example,
    2 GB blobs). In our example, we will use Cassandra as our deep storage mechanism.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，涉及三种数据存储机制。**MySQL**数据库是一个简单的元数据存储库。它包含所有段的所有元数据信息。**深度存储**机制包含实际的段信息。每个段包含根据配置文件中定义的维度和聚合而基于特定时间段的事件的合并索引。因此，段可以很大（例如，2GB的blob）。在我们的示例中，我们将使用Cassandra作为我们的深度存储机制。
- en: Finally, the third data storage mechanism is **ZooKeeper**. The storage in **ZooKeeper**
    is transient and is used for control information only. When a new segment is available,
    the **Master** node writes an ephemeral node in **ZooKeeper**. The **Compute**
    Node is subscribed to the same path, and the ephemeral node triggers the **Compute**
    node to pull the new segment. After the segment is successfully retrieved, the
    **Compute** node removes the ephemeral node from **ZooKeeper**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第三种数据存储机制是ZooKeeper。ZooKeeper中的存储是瞬态的，仅用于控制信息。当一个新的段可用时，Master节点会在ZooKeeper中写入一个临时节点。Compute节点订阅相同的路径，临时节点触发Compute节点拉取新的段。在成功检索段后，Compute节点会从ZooKeeper中删除临时节点。
- en: 'For our example, the entire sequence of events is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，事件的整个序列如下：
- en: '![Integrating a non-transactional system](img/8294OS_07_02.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![集成非事务性系统](img/8294OS_07_02.jpg)'
- en: The preceding diagram lays out the event processing downstream from Storm. What
    is important to recognize in many real-time analytics engines is the inability
    to revert a transaction. The analytics systems are highly optimized to process
    speed and aggregation. The sacrifice is transactional integrity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表展示了从Storm下游的事件处理。在许多实时分析引擎中，重要的是要认识到无法撤销事务。分析系统被高度优化以处理速度和聚合。牺牲的是事务完整性。
- en: 'If we re-examine Trident''s state classifications, there are three different
    flavors of state: Transactional, Opaque, and Non-Transactional. A Transactional
    state requires the contents of each batch to be constant over time. An Opaque
    Transactional state can tolerate batch composition changing over time. Finally,
    a Non-Transactional state cannot guarantee exactly one semantic at all.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果重新审视Trident的状态分类，有三种不同的状态：事务性、不透明和非事务性。事务状态要求每个批次的内容随时间保持不变。不透明事务状态可以容忍随时间变化的批次组合。最后，非事务状态无法保证确切的一次语义。
- en: 'Summarizing the Javadoc for the `storm.trident.state.State` object, there are
    three different kinds of state:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 总结`storm.trident.state.State`对象的Javadoc，有三种不同类型的状态：
- en: '| **Non-Transactional state** | In this state, commits are ignored.No rollback
    can be done.Updates are permanent. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **非事务状态** | 在这种状态下，提交被忽略。无法回滚。更新是永久的。'
- en: '| **Repeat Transactional state** | The system is idempotent as long as all
    batches are identical. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **重复事务状态** | 只要所有批次都是相同的，系统就是幂等的。'
- en: '| **Opaque Transactional state** | State transitions are incremental. The previous
    state is stored along with the batch identifier to tolerate changing batch composition
    in the event of replay. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **不透明事务状态** | 状态转换是增量的。在重播事件中，先前的状态与批次标识符一起存储以容忍批次组合的变化。'
- en: It is important to realize that introducing state into a topology effectively
    sequences any writes to storage. This can impact performance dramatically. When
    possible, the best approach is to ensure the entire system is idempotent. If all
    writes are idempotent, then you need not introduce transactional storage (or state)
    at all, because the architecture naturally tolerates tuple replay.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要意识到，将状态引入拓扑实际上会将任何写入存储的顺序化。这可能会对性能产生重大影响。在可能的情况下，最好的方法是确保整个系统是幂等的。如果所有写入都是幂等的，那么你根本不需要引入事务性存储（或状态），因为架构自然容忍元组重播。
- en: 'Often, if state persistence is backed by a database over which you control
    the schema, you can adjust the schema to add the additional information to participate
    in transactions: last committed batch identifier for repeat transactional and
    previous state for opaque transactional. Then, in the state implementation, you
    can leverage this information to ensure that your state object aligns with the
    type of spout you are using.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果状态持久性由你控制架构的数据库支持，你可以调整架构以添加额外的信息来参与事务：重复事务的最后提交批次标识符和不透明事务的上一个状态。然后，在状态实现中，你可以利用这些信息来确保你的状态对象与你正在使用的spout类型相匹配。
- en: However, this is not always the case, especially in systems that perform aggregations
    such as counting, summing, averaging, and so on. Counter mechanisms in Cassandra
    have exactly this constraint. It is impossible to undo an addition to a counter,
    and it is impossible to make the addition idempotent. If a tuple is replayed,
    the counter is again incremented, and you have most likely overcounted elements
    in the system. For this reason, any state implementation backed by Cassandra counters
    is considered non-transactional.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不总是适用，特别是在执行计数、求和、平均值等聚合的系统中。Cassandra中的计数器机制正是具有这种约束。无法撤销对计数器的增加，也无法使增加幂等。如果元组被重播，计数器将再次递增，你很可能在系统中过度计数元素。因此，任何由Cassandra计数器支持的状态实现都被视为非事务性的。
- en: Likewise, Druid is non-transactional. Once Druid consumes an event, the event
    cannot be undone. Thus, if a batch within Storm is partially consumed by Druid
    and then the batch is replayed, or the composition changes, there is no way for
    the aggregate dimensional analytics to recover. For this reason, it is interesting
    to consider integration between Druid and Storm, the steps we can take to address
    replays, and the power of such a coupling.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Druid是非事务性的。一旦Druid消费了一个事件，该事件就无法撤销。因此，如果Storm中的一个批次被Druid部分消费，然后重新播放批次，或者组合发生变化，聚合维度分析就无法恢复。因此，考虑Druid和Storm之间的集成，以及我们可以采取的步骤来解决重播的问题，以及这种耦合的力量，这是很有趣的。
- en: In short, to connect Storm to Druid, we will leverage the characteristics of
    a transactional spout to minimize the risk of overcounting when connecting to
    a non-transactional state mechanism like Druid.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，要将Storm连接到Druid，我们将利用事务spout的特性，以最小化连接到非事务状态机制（如Druid）时的过度计数的风险。
- en: The topology
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拓扑结构
- en: 'With the architectural concepts in place, let''s return to the use case. To
    keep things focused on the integration, we will keep the topology simple. The
    following diagram depicts the topology:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有了架构概念，让我们回到用例。为了将重点放在集成上，我们将保持拓扑的简单。以下图表描述了拓扑结构：
- en: '![The topology](img/8294OS_07_03.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![拓扑结构](img/8294OS_07_03.jpg)'
- en: The **FIX Spout** emits tuples containing simple FIX messages. Then the filter
    checks the type of the message, filtering for stock orders that contain pricing
    information. Then, those filtered tuples flow to the `DruidState` object, which
    is the bridge to Druid.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**FIX喷口**发出包含简单FIX消息的元组。然后过滤器检查消息的类型，过滤包含定价信息的股票订单。然后，这些经过过滤的元组流向`DruidState`对象，它是与Druid连接的桥梁。'
- en: 'The code for this simple topology is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单拓扑的代码如下所示：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The spout
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 喷口
- en: There are many parsers for the FIX message format. In the spout, we will use
    the FIX Parser, which is a Google project. For more information on this project,
    you can refer to [https://code.google.com/p/fixparser/](https://code.google.com/p/fixparser/).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: FIX消息格式有许多解析器。在喷口中，我们将使用FIX解析器，这是一个Google项目。关于这个项目的更多信息，您可以参考[https://code.google.com/p/fixparser/](https://code.google.com/p/fixparser/)。
- en: 'Just like the previous chapter, the spout itself is straightforward. It simply
    returns references to a coordinator and an emitter, as shown in the following
    code:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 就像前一章一样，喷口本身很简单。它只是返回一个协调器和一个发射器的引用，如下面的代码所示：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As shown in the preceding code, the `Spout` declares a single output field:
    `message`. This will contain the `FixMessageDto` object that is generated by the
    `Emitter`, as shown in the following code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，`Spout`声明了一个单一的输出字段：`message`。这将包含`Emitter`生成的`FixMessageDto`对象，如下面的代码所示：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: From the preceding code, you can see that we reparse the file for each batch.
    As we stated previously, in a real-time system we will probably receive the messages
    via TCP/IP and queue them in Kafka. Then, we would use the Kafka spout to emit
    the messages. It is a matter of preference; but, to fully encapsulate the data
    processing in Storm, the system would most likely queue the raw message text.
    In that design, we would parse the text in a function rather than the spout.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，您可以看到我们为每个批次重新解析文件。正如我们之前所述，在实时系统中，我们可能会通过TCP/IP接收消息，并将它们排队在Kafka中。然后，我们将使用Kafka喷口发出这些消息。这是一个偏好问题；但是，为了完全封装Storm中的数据处理，系统很可能会排队原始消息文本。在这种设计中，我们将在一个函数中解析文本，而不是在喷口中。
- en: Although this `Spout` is only sufficient for this example, note that the composition
    of each batch is the same. Specifically, each batch contains all messages from
    the file. Since our state design relies on this characteristic, in a real system,
    we would need to use `TransactionalKafkaSpout`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个“喷口”只适用于这个例子，但请注意每个批次的组成是相同的。具体来说，每个批次包含文件中的所有消息。由于我们的状态设计依赖于这一特性，在一个真实的系统中，我们需要使用“TransactionalKafkaSpout”。
- en: The filter
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤器
- en: 'Like the spout, the filter is straightforward. It examines the `msgType` object
    and filters messages that are not fill orders. Fill orders are effectively stock
    purchase receipts. They contain the average price executed for that trade and
    the symbol for the stock purchased. The following code is the filter for this
    message type:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与喷口一样，过滤器也很简单。它检查“msgType”对象并过滤掉不是填单的消息。填单实际上是股票购买收据。它们包含了该交易执行的平均价格和所购买股票的符号。以下代码是这种消息类型的过滤器：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This provides a good opportunity to point out the importance of serializability
    in Storm. Note that in the preceding code the filter is operating on a `FixMessageDto`
    object. It would have been easier to simply use the `SimpleFixMessage` object,
    but `SimpleFixMessage` is not serializable. This will not cause any problems when
    running on a local cluster. However, since tuples are exchanged between hosts
    during data processing in Storm, all the elements within a tuple must be serializable.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个很好的机会来指出Storm中可序列化性的重要性。请注意，在前面的代码中，过滤器操作的是一个`FixMessageDto`对象。使用`SimpleFixMessage`对象可能更容易，但`SimpleFixMessage`不可序列化。这在本地集群上运行时不会造成任何问题。然而，在Storm中进行数据处理时，元组在主机之间交换，元组中的所有元素都必须是可序列化的。
- en: Tip
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Developers often commit changes to data objects within tuples that are not
    serializable. This causes downstream deployment issues. To ensure that all objects
    in a tuple remain serializable, add unit tests that verify that objects are serializable.
    The test is a simple one; use the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员经常对不可序列化的元组中的数据对象进行更改。这会导致下游部署问题。为了确保元组中的所有对象保持可序列化，添加一个验证对象可序列化的单元测试。这个测试很简单，使用以下代码：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The state design
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态设计
- en: Now, let us proceed to the most interesting aspects of this example. In order
    to integrate Druid with Storm, we will embed a real-time Druid server into our
    topology and implement the necessary interfaces to connect the tuple stream to
    it. To mitigate the inherent risks of connecting to a non-transactional system,
    we leverage ZooKeeper to persist state information. That persistence will not
    prevent anomalies due to failures, but it will help identify what data is at risk
    when a failure occurs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续讨论这个例子最有趣的方面。为了将Druid与Storm集成，我们将在我们的拓扑中嵌入一个实时的Druid服务器，并实现必要的接口将元组流连接到它。为了减轻连接到非事务性系统的固有风险，我们利用ZooKeeper来持久化状态信息。这种持久化不会防止由于故障而导致的异常，但它将有助于确定故障发生时哪些数据处于风险之中。
- en: 'The high-level design is shown as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 高级设计如下所示：
- en: '![The state design](img/8294OS_07_04.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![状态设计](img/8294OS_07_04.jpg)'
- en: At a high level, Storm creates state objects within worker JVM processes by
    using a factory. A state object is created for every partition in the batch. The
    state factory object ensures that the real-time server is running before it returns
    any state objects and starts the server if it is not running. The state object
    then buffers those messages until Storm calls commit. When Storm calls commit,
    the state object unblocks the Druid **Firehose**. This sends the signals to Druid
    that the data is ready for aggregation. Then, we block Storm in the commit method,
    while the real-time server begins pulling the data via the **Firehose**.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，Storm通过使用工厂在worker JVM进程中创建状态对象。为批次中的每个分区创建一个状态对象。状态工厂对象确保在返回任何状态对象之前，实时服务器正在运行，并在服务器未运行时启动服务器。然后状态对象缓冲这些消息，直到Storm调用commit。当Storm调用commit时，状态对象解除Druid
    **Firehose**的阻塞。这向Druid发送信号，表明数据已准备好进行聚合。然后，在commit方法中阻塞Storm，而实时服务器通过**Firehose**开始拉取数据。
- en: To ensure that every partition is processed at most once, we associate a partition
    identifier with each partition. The partition identifier is a combination of the
    batch identifier and the partition index, which uniquely identifies a set of data
    since we are using a transactional spout.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保每个分区最多被处理一次，我们将分区标识符与每个分区关联起来。分区标识符是批次标识符和分区索引的组合，可以唯一标识一组数据，因为我们使用了事务性spout。
- en: The **Firehose** persists the identifier in **ZooKeeper** to maintain the state
    of the partition.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**Firehose**将标识符持久化在**ZooKeeper**中以维护分区的状态。'
- en: 'There are three states in **ZooKeeper**:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**ZooKeeper**中有三种状态：'
- en: '| State | Description |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | 描述 |'
- en: '| --- | --- |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| inProgress | This `Zookeeper` path contains the partition identifiers that
    Druid is processing. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| inProgress | 这个`Zookeeper`路径包含了Druid正在处理的分区标识符。|'
- en: '| Limbo | This `Zookeeper` path contains the partition identifiers that Druid
    consumed in their entirety, but which may not be committed. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Limbo | 这个`Zookeeper`路径包含了Druid完全消耗但可能尚未提交的分区标识符。|'
- en: '| Completed | This `Zookeeper` path contains the partition identifiers that
    Druid successfully committed. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 完成 | 这个`Zookeeper`路径包含了Druid成功提交的分区标识符。|'
- en: While a batch is in process, the **Firehose** writes the partition identifier
    to the inProgress path. When Druid has pulled the entirety of a Storm partition,
    the partition identifier is moved to **Limbo**, and we release Storm to continue
    processing while we wait for the commit message from Druid.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理批次时，**Firehose**将分区标识符写入inProgress路径。当Druid完全拉取了Storm分区的全部数据时，分区标识符被移动到**Limbo**，我们释放Storm继续处理，同时等待Druid的提交消息。
- en: Upon receiving the commit message from Druid, the **Firehose** moves the partition
    identifier to the **Completed** path. At this point, we assume the data has been
    written to disk. We are still susceptible to losing data in the event of a disk
    failure. However, if we assume that we can reconstruct the aggregations using
    batch processing, then this is most likely an acceptable risk.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 收到Druid的提交消息后，**Firehose**将分区标识符移动到**Completed**路径。此时，我们假设数据已写入磁盘。然而，在磁盘故障的情况下，我们仍然容易丢失数据。但是，如果我们假设可以使用批处理重建聚合，那么这很可能是可以接受的风险。
- en: 'The following state machine captures the different phases of processing:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下状态机捕捉了处理的不同阶段：
- en: '![The state design](img/8294OS_07_05.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![状态设计](img/8294OS_07_05.jpg)'
- en: 'As depicted in the diagram, there is a loop between **Buffering Messages**
    and **Aggregating Messages**. The main control loop switches rapidly between these
    two states, splitting its time between the Storm processing loop and the Druid
    aggregation loop. The states are mutually exclusive: either the system is aggregating
    a batch, or it is buffering the next batch.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，在**缓冲消息**和**聚合消息**之间存在一个循环。主控制循环在这两种状态之间快速切换，将其时间分配给Storm处理循环和Druid聚合循环。这些状态是互斥的：系统要么在聚合一个批次，要么在缓冲下一个批次。
- en: The third state is triggered when Druid has written the information to disk.
    When that happens (as we will see later), the **Firehose** is notified and we
    can update our persistence mechanism to indicate that the batch was safely processed.
    Until that commit is called, the batches consumed by Druid must remain in **Limbo**.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种状态是当Druid将信息写入磁盘时触发的。当发生这种情况（稍后我们将看到），**Firehose**会收到通知，我们可以更新我们的持久化机制，以指示批次已安全处理。在调用commit之前，Druid消耗的批次必须保持在**Limbo**中。
- en: While in **Limbo**, no assumptions can be made about the data. Druid may or
    may not have aggregated the records.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在**Limbo**中，不能对数据做任何假设。Druid可能已经聚合了记录，也可能没有。
- en: 'In the event of a failure, Storm may leverage other `TridentState` instances
    to complete the processing. Thus, for every partition, the **Firehose** must execute
    the following steps:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在发生故障时，Storm可能利用其他`TridentState`实例来完成处理。因此，对于每个分区，**Firehose**必须执行以下步骤：
- en: The **Firehose** must check to see if the partition was already completed. If
    so, the partition is a replay, probably due to a downstream failure. Since the
    batch is guaranteed to have the same contents as before, it can safely be ignored
    since Druid has already aggregated its contents. The system may log a warning
    message.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Firehose**必须检查分区是否已经完成。如果是，那么分区是一个重播，可能是由于下游故障。由于批次保证与之前相同，可以安全地忽略，因为Druid已经聚合了其内容。系统可能会记录警告消息。'
- en: The **Firehose** must check to see if the partition is in limbo. If this is
    the case, then Druid fully consumed the partition, but never called commit or
    the system failed after commit was called but before the **Firehose** updated
    **ZooKeeper**. The system should raise an alert. It should not attempt to complete
    the batch since it was fully consumed by Druid and we do not know the status of
    the aggregation. It simply returns, enabling Storm to continue to the next batch.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Firehose**必须检查分区是否处于悬空状态。如果是这种情况，那么Druid完全消耗了分区，但从未调用commit，或者在调用commit之后但在**Firehose**更新**ZooKeeper**之前系统失败了。系统应该发出警报。它不应该尝试完成批处理，因为它已被Druid完全消耗，我们不知道聚合的状态。它只是返回，使Storm可以继续进行下一批处理。'
- en: The **Firehose** must check to see if the partition is in progress. If this
    is the case, then for some reason, somewhere on the network, the partition is
    being processed by another instance. This should not happen during ordinary processing.
    In this case, the system should raise an alert for this partition. In our simple
    system, we will simply proceed, leaving it to our offline batch processing to
    correct the aggregation.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Firehose**必须检查分区是否正在进行中。如果是这种情况，那么由于某种原因，在网络的某个地方，分区正在被另一个实例处理。这在普通处理过程中不应该发生。在这种情况下，系统应该为该分区发出警报。在我们简单的系统中，我们将简单地继续进行，留待离线批处理来纠正聚合。'
- en: In many large scale real-time systems, the users are willing to tolerate slight
    discrepancies in the real-time analytics as long as the skews are infrequent and
    can be remedied fairly quickly.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多大规模实时系统中，用户愿意容忍实时分析中的轻微差异，只要偏差不经常发生并且可以很快得到纠正。
- en: It is important to note that this approach succeeds because we are using a transactional
    spout. The transactional spout guarantees that each batch has the same composition.
    Furthermore, for this approach to work, each partition within the batch must have
    the same composition. This is true if and only if the partitioning in the topology
    is deterministic. With deterministic partitioning and a transactional spout, each
    partition will contain the same data, even in the event of a replay. Had we used
    shuffle grouping, this approach would not work. Our example topology is deterministic.
    This guarantees that a batch identifier, when combined with a partition index,
    represents a consistent set of data over time.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，这种方法成功的原因是我们使用了事务性spout。事务性spout保证每个批次具有相同的组成。此外，为了使这种方法有效，批处理中的每个分区必须具有相同的组成。只有在拓扑中的分区是确定性的情况下才成立。有了确定性的分区和事务性spout，即使在重放的情况下，每个分区也将包含相同的数据。如果我们使用了洗牌分组，这种方法就不起作用。我们的示例拓扑是确定性的。这保证了批处理标识符与分区索引结合表示了随时间一致的数据集。
- en: Implementing the architecture
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施架构
- en: 'With the design in place, we can turn our attention to the implementation.
    The sequence diagram for the implementation is shown as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有了设计之后，我们可以将注意力转向实施。实施的序列图如下所示：
- en: '![Implementing the architecture](img/8294OS_07_06.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![实施架构](img/8294OS_07_06.jpg)'
- en: The preceding diagram implements the state machine shown in the design. Once
    the real-time server is started, Druid polls the `StormFirehose` object using
    the `hasMore()` method. The contract with Druid specifies that the `Firehose`
    object's implementation should block until data is available. While Druid is polling
    and the `Firehose` object is blocking, Storm delivers tuples into the `DruidState`
    object`'s` message buffer. When the batch is complete, Storm calls the `commit()`
    method on the `DruidState` object. At that point, the PartitionStatus is updated.
    The partition is put in progress and the implementation unblocks the `StormFirehose`
    object.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图实现了设计中显示的状态机。一旦实时服务器启动，Druid使用`hasMore()`方法轮询`StormFirehose`对象。与Druid的合同规定，`Firehose`对象的实现应该在数据可用之前阻塞。当Druid在轮询而`Firehose`对象在阻塞时，Storm将元组传递到`DruidState`对象的消息缓冲区中。当批处理完成时，Storm调用`DruidState`对象的`commit()`方法。在那时，PartitionStatus被更新。分区被放置在进行中，并且实现解除`StormFirehose`对象的阻塞。
- en: Druid begins pulling data from the `StormFirehose` object via the `nextRow()`
    method. When the `StormFirehose` object exhausts the contents of the partition,
    it places the partition in limbo, and releases control back to Storm.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Druid开始通过`nextRow()`方法从`StormFirehose`对象中拉取数据。当`StormFirehose`对象耗尽分区的内容时，它将分区置于悬空状态，并将控制权释放给Storm。
- en: Finally, when the commit method is called on the StormFirehose, the implementation
    returns a `Runnable`, which is what Druid uses to notify a Firehose that the partition
    is persisted. When Druid calls `run()`, the implementation moves the partition
    to completion.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当在StormFirehose上调用commit方法时，实现会返回一个`Runnable`，这是Druid用来通知Firehose分区已持久化的方式。当Druid调用`run()`时，实现会将分区移动到完成状态。
- en: DruidState
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DruidState
- en: First, we will look at the Storm side of the equation. In the previous chapter,
    we extended the `NonTransactionalMap` class to persist a state. That abstraction
    shielded us from the details of sequential batch processing. We simply implemented
    the `IBackingMap` interface to support the `multiGet` and `multiPut` calls, and
    the superclass took care of the rest.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将看一下风暴方面的情况。在上一章中，我们扩展了`NonTransactionalMap`类以持久化状态。这种抽象使我们免受顺序批处理细节的影响。我们只需实现`IBackingMap`接口来支持`multiGet`和`multiPut`调用，超类就会处理其余部分。
- en: 'In this scenario, we need more control over the persistence process than what
    the default implementations provide. Instead, we need to implement the base `State`
    interfaces ourselves. The following class diagram depicts the class hierarchy:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要比默认实现提供的更多对持久化过程的控制。相反，我们需要自己实现基本的`State`接口。以下类图描述了类层次结构：
- en: '![DruidState](img/8294OS_07_07.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![DruidState](img/8294OS_07_07.jpg)'
- en: As evident in the diagram, the `DruidStateFactory` class manages the embedded
    real-time node. An argument could be made for the updater managing the embedded
    server. However, since there should be only a single instance of the real-time
    server per JVM and that instance needs to exist before any state objects, the
    lifecycle management of the embedded server seemed to fit more naturally in the
    factory.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图中所示，`DruidStateFactory`类管理嵌入式实时节点。可以提出一个论点，认为更新程序管理嵌入式服务器。然而，由于每个JVM应该只有一个实时服务器实例，并且该实例需要在任何状态对象之前存在，因此嵌入式服务器的生命周期管理似乎更自然地适合工厂。
- en: 'The following code snippet contains the relevant sections of the `DruidStateFactory`
    class:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段包含了`DruidStateFactory`类的相关部分：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Without going into too much detail, the preceding code starts a real-time node
    if one had not been started already. Also, it registers the `StormFirehoseFactory`
    class with that real-time node.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 不详细介绍，前面的代码如果尚未启动实时节点，则启动一个实时节点。此外，它将`StormFirehoseFactory`类注册到该实时节点。
- en: 'The factory also implements the `StateFactory` interface from Storm, which
    allows Storm to use this factory to create new `State` objects. The `State` object
    itself is fairly simple:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 工厂还实现了来自Storm的`StateFactory`接口，允许Storm使用此工厂创建新的`State`对象。`State`对象本身非常简单：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see in the preceding code, the `State` object is a message buffer.
    It delegates the actual commit logic to the `Firehose` object, which we will examine
    shortly. However, there are a few critical lines in this class that implement
    the failure detection we outlined earlier.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，`State`对象是一个消息缓冲区。它将实际的提交逻辑委托给`Firehose`对象，我们将很快进行检查。然而，在这个类中有一些关键的行，实现了我们之前概述的故障检测。
- en: The conditional logic in the `commit()` method on the `State` object checks
    the ZooKeeper status to determine if this partition was already successfully processed
    (`inCompleted`), failed to commit (`inLimbo`), or failed during processing (`inProgress`).
    We will dive deeper into the state storage when we examine the `DruidPartitionStatus`
    object.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`State`对象上`commit()`方法中的条件逻辑检查ZooKeeper状态，以确定此分区是否已成功处理（`inCompleted`），未能提交（`inLimbo`）或在处理过程中失败（`inProgress`）。当我们检查`DruidPartitionStatus`对象时，我们将更深入地了解状态存储。'
- en: It is also important to note that the `commit()` method is called by Storm directly,
    but the `aggregateMessage()` method is called by the updater. Even though Storm
    should never call those methods concurrently, we chose to use a thread-safe vector
    anyway.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意的是，`commit()`方法由Storm直接调用，但`aggregateMessage()`方法由更新程序调用。即使Storm不应该同时调用这些方法，我们还是选择使用线程安全的向量。
- en: 'The DruidStateUpdater code is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: DruidStateUpdater代码如下：
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As shown in the preceding code, the updater simply loops through the tuples
    and passes them to the state object to buffer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，更新程序只是简单地循环遍历元组，并将它们传递给状态对象进行缓冲。
- en: Implementing the StormFirehose object
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现StormFirehose对象
- en: 'Before we turn our attention to the Druid side of the implementation, we should
    probably take a step back and discuss Druid in more detail. Druid feeds are configured
    via a spec file. In our example, this is `realtime.spec`, as shown in the following
    code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们转向Druid实现的一侧之前，我们可能应该退一步，更详细地讨论一下Druid。Druid的数据源是通过一个规范文件进行配置的。在我们的示例中，这是`realtime.spec`，如下面的代码所示：
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For our example, the important elements in the preceding spec file are `schema`
    and `firehose`. The `schema` element defines the data and the aggregations that
    Druid should perform on that data. In our example, Druid will count the number
    of times we see a stock symbol in the `orders` field and track the total price
    paid in the `totalPrice` field. The `totalPrice` field will be used to calculate
    the running stock price average over time. Additionally, you need to specify an
    `indexGranularity` object that specifies the temporal granularity of the index.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，在前面的规范文件中，重要的元素是`schema`和`firehose`。`schema`元素定义了数据和Druid应该对该数据执行的聚合。在我们的示例中，Druid将计算我们在`orders`字段中看到股票符号的次数，并跟踪`totalPrice`字段中支付的总价格。`totalPrice`字段将用于计算随时间变化的股票价格平均值。此外，您需要指定一个`indexGranularity`对象，该对象指定索引的时间粒度。
- en: The `firehose` element contains the configuration for the `Firehose` object.
    As we saw in the `StateFactory` interface, an implementation registers a `FirehoseFactory`
    class with Druid when the real-time server is started. That factory is registered
    as a `Jackson` subtype. When the real-time spec file is parsed, the type in the
    `firehose` element of the JSON is used to link back to the appropriate `FirehoseFactory`
    for a stream of data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`firehose`元素包含`Firehose`对象的配置。正如我们在`StateFactory`接口中看到的，实现在实时服务器启动时向Druid注册了一个`FirehoseFactory`类。该工厂被注册为`Jackson`子类型。当解析实时规范文件时，JSON中`firehose`元素中的类型用于链接回适用于数据流的适当`FirehoseFactory`。'
- en: 'For more information on the JSON polymorphism, refer to the following website:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 有关JSON多态性的更多信息，请参考以下网站：
- en: '[http://wiki.fasterxml.com/JacksonPolymorphicDeserialization](http://wiki.fasterxml.com/JacksonPolymorphicDeserialization)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://wiki.fasterxml.com/JacksonPolymorphicDeserialization](http://wiki.fasterxml.com/JacksonPolymorphicDeserialization)'
- en: 'For more information on the spec file, refer to the following website:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 有关规范文件的更多信息，请参考以下网站：
- en: '[https://github.com/metamx/druid/wiki/Realtime](https://github.com/metamx/druid/wiki/Realtime)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/metamx/druid/wiki/Realtime](https://github.com/metamx/druid/wiki/Realtime)'
- en: Now, we can turn our attention to the Druid side of the implementation. `Firehose`
    is the main interface one must implement to contribute data into a Druid real-time
    server.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以把注意力转向Druid实现的一侧。`Firehose`是必须实现的主要接口，以将数据贡献到Druid实时服务器中。
- en: 'The code for our `StormFirehoseFactory` class is as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`StormFirehoseFactory`类的代码如下：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The factory implementation is straightforward. In this case, we simply return
    a static singleton object. Note that the object is annotated with `@JsonTypeName`
    and `@JsonCreator`. As stated in the preceding code, `Jackson` is the means through
    which `FirehoseFactory` objects are registered. Thus, the name specified as the
    `@JsonTypeName` must align with the type specified in the spec file.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 工厂实现很简单。在这种情况下，我们只返回一个静态的单例对象。请注意，该对象带有`@JsonTypeName`和`@JsonCreator`注解。如前面的代码所述，`Jackson`是`FirehoseFactory`对象注册的手段。因此，`@JsonTypeName`指定的名称必须与规范文件中指定的类型一致。
- en: 'The meat of the implementation is in the `StormFirehose` class. Within this
    class, there are four critical methods that we will examine one by one: `hasMore()`,
    `nextRow()`, `commit()`, and `sendMessages()`.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的核心在`StormFirehose`类中。在这个类中，有四个关键方法，我们将逐一检查：`hasMore()`，`nextRow()`，`commit()`和`sendMessages()`。
- en: 'The `sendMessages()` method is the entry point into the `StormFirehose` class.
    It is effectively the handoff point between Storm and Druid. The code for this
    method is as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`sendMessages()`方法是进入`StormFirehose`类的入口点。这实际上是Storm和Druid之间的交接点。该方法的代码如下：'
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This method is synchronized to prevent concurrency issues. Note that it does
    not do anything more than copy the message buffer into a queue and notify the
    `hasMore()` method to release the batch. Then, it blocks waiting for Druid to
    fully consume the batch.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法是同步的，以防止并发问题。请注意，它除了将消息缓冲区复制到队列中并通知`hasMore()`方法释放批处理外，不做任何其他操作。然后，它会阻塞等待Druid完全消耗批处理。
- en: 'Then, the flow proceeds to the `nextRow()` method, which is shown as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，流程继续到`nextRow()`方法，如下所示：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This method pulls a message off of the queue. If it is not null, the data is
    added to a map that is passed along to Druid as a `MapBasedInputRow` method. If
    there are no remaining messages in the queue, the `sendMessages()` method that
    we examined in the preceding code is released. From Storm's perspective, the batch
    is complete. Druid now owns the data. However, from a system perspective, the
    data is in limbo because Druid may not have persisted the data to disk. We are
    at a risk of losing the data entirely in the event of a hardware failure.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法从队列中取出一条消息。如果不为空，则将数据添加到一个映射中，并作为`MapBasedInputRow`方法传递给Druid。如果队列中没有剩余消息，则释放前面代码中检查的`sendMessages()`方法。从Storm的角度来看，批处理已完成。Druid现在拥有数据。但是，从系统的角度来看，数据处于悬而未决状态，因为Druid可能尚未将数据持久化到磁盘。在硬件故障的情况下，我们有丢失数据的风险。
- en: 'Druid will then poll the `hasMore()` method, which is shown in the following
    code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后Druid将轮询`hasMore()`方法，如下所示：
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Since the queue is empty, the method will block until `sendMessage()` is called
    again.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 由于队列为空，该方法将阻塞，直到再次调用`sendMessage()`。
- en: 'This leaves only one remaining piece of the puzzle, the `commit()` method.
    It is shown in the following code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在只剩下一个谜题的部分，`commit()`方法。它在以下代码中显示：
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This method returns `Runnable`, which is called by Druid after it's finished
    persisting the messages. Although all the other methods in the `Firehose` object
    are called from a single thread, the `Runnable` is called from a different thread
    and, therefore, must be thread-safe. For that reason, we copy the transactions
    in limbo into a separate list and pass it into the `Runnable` object's constructor.
    As you can see in the following code, the `Runnable` does nothing but moves the
    transactions into the completed state in `Zookeeper`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法返回`Runnable`，在Druid完成持久化消息后被调用。尽管`Firehose`对象中的所有其他方法都是从单个线程调用的，但`Runnable`是从不同的线程调用的，因此必须是线程安全的。因此，我们将悬而未决的事务复制到一个单独的列表中，并将其传递给`Runnable`对象的构造函数。如下代码所示，`Runnable`除了将事务移动到`Zookeeper`中的已完成状态外，什么也不做。
- en: '[PRE15]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Implementing the partition status in ZooKeeper
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在ZooKeeper中实现分区状态
- en: Now that we have examined all of the code, we can take a look at how the state
    is persisted in ZooKeeper. This enables the system to coordinate the distributed
    processing, especially in the event of a failure.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经检查了所有的代码，我们可以看一下状态如何在ZooKeeper中持久化。这使得系统能够协调分布式处理，特别是在发生故障时。
- en: The implementation leverages ZooKeeper to persist the partition-processing status.
    ZooKeeper is another open source project. For more information, you can refer
    to [http://zookeeper.apache.org/](http://zookeeper.apache.org/).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 该实现利用ZooKeeper来持久化分区处理状态。ZooKeeper是另一个开源项目。更多信息，请参考[http://zookeeper.apache.org/](http://zookeeper.apache.org/)。
- en: ZooKeeper maintains a tree of nodes. Each node has an associated path, much
    like a file system. The implementation uses ZooKeeper through a framework called
    Curator. For more information, you can refer to [http://curator.incubator.apache.org/](http://curator.incubator.apache.org/).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ZooKeeper维护一个节点树。每个节点都有一个关联的路径，就像文件系统一样。实现使用ZooKeeper通过一个叫做Curator的框架。更多信息，请参考[http://curator.incubator.apache.org/](http://curator.incubator.apache.org/)。
- en: When connecting to ZooKeeper through Curator, you supply a namespace. Effectively,
    this is the top-level node under which the application data is stored. In our
    implementation, the namespace is `stormdruid`. The application then maintains
    three paths underneath that, where it stores batch status information.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Curator连接到ZooKeeper时，您提供一个命名空间。实际上，这是应用数据存储在其中的顶级节点。在我们的实现中，命名空间是`stormdruid`。然后应用在其中维护三个路径，用于存储批处理状态信息。
- en: 'The paths correspond to the states described in the design and are as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 路径对应于设计中描述的状态，如下所示：
- en: '`/stormdruid/current`: This corresponds to the current state'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/stormdruid/current`：这对应于当前状态'
- en: '`/stormdruid/limbo`: This corresponds to the limbo state'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/stormdruid/limbo`：这对应于悬而未决的状态'
- en: '`/stormdruid/completed`: This corresponds to the completed state'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/stormdruid/completed`：这对应于已完成的状态'
- en: In our implementation, all ZooKeeper's interactions for partition status are
    run through the `DruidPartitionStatus` class.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，所有关于分区状态的ZooKeeper交互都通过`DruidPartitionStatus`类运行。
- en: 'The code for this class is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的代码如下：
- en: '[PRE16]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the interest of space, we have only shown the constructor and the methods
    related to the limbo status. In the constructor, the client connects to ZooKeeper
    and creates the three base paths as described in the preceding code. Then, it
    provides query methods to test if a transaction is in progress, limbo, or completed.
    It also provides methods that move a transaction between those states.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 出于空间考虑，我们只显示了构造函数和与limbo状态相关的方法。在构造函数中，客户端连接到ZooKeeper并创建了前面代码中描述的三个基本路径。然后，它提供了查询方法来测试事务是否正在进行中、处于limbo状态或已完成。它还提供了将事务在这些状态之间移动的方法。
- en: Executing the implementation
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行实现
- en: Enough with the code, let's get on with the demo! We start the topology using
    the main method of the `FinancialAnalyticsTopology` class. For a better demo,
    we introduce random prices between zero and one hundred. (Refer back to the `Emitter`
    code.)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 够了，不要再看代码了，让我们进行演示吧！我们使用“FinancialAnalyticsTopology”类的主方法启动拓扑。为了更好的演示，我们引入了零到一百之间的随机价格。（参考“Emitter”代码。）
- en: 'Once the topology is started, you will see the following output:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦拓扑启动，您将看到以下输出：
- en: '[PRE17]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can interrogate the processing from multiple dimensions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从多个维度对处理进行审查。
- en: 'Using the ZooKeeper client, you can examine the status of transactions. Take
    a look at the following listing; it shows the transaction/batch identifiers and
    their statuses:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ZooKeeper客户端，您可以检查事务的状态。看一下下面的列表；它显示了事务/批处理标识符及其状态：
- en: '[PRE18]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'For alerting and monitoring, please note the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于警报和监控，请注意以下内容：
- en: If ever there is more than one batch in the `current` path, then alerts should
    go out
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果“current”路径中有多个批处理，那么应该发出警报
- en: If ever there are batch identifiers in `limbo` that are not sequential, or substantially
    behind the current identifier, alerts should go out
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果“limbo”中有不连续的批处理标识符，或者明显落后于当前标识符，应该发出警报
- en: 'To clean up the state in ZooKeeper, you can execute the following code:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要清理ZooKeeper中的状态，您可以执行以下代码：
- en: '[PRE19]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To monitor the segment propagation, you can use the MySQL client. Using the
    default schema, you will find segments by selecting them out of the `prod_segments`
    table with the following code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 要监控段的传播，您可以使用MySQL客户端。使用默认模式，您可以通过以下代码从“prod_segments”表中选择出段：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Examining the analytics
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 审查分析
- en: 'Now, the moment we have all been waiting for; we can see average stock prices
    over time by using the REST API that Druid provides. To use the REST API, it is
    not necessary to run a full-blown Druid cluster. You will only be able to query
    the data seen by the singular embedded real-time node, but each node is capable
    of servicing requests and this makes testing easier. Using curl, you can issue
    a query of a real-time node using the following command:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们一直在等待的时刻到了；我们可以通过Druid提供的REST API看到随时间变化的平均股价。要使用REST API，不需要运行一个完整的Druid集群。您只能查询单个嵌入式实时节点看到的数据，但每个节点都能够处理请求，这使得测试更容易。使用curl，您可以使用以下命令查询实时节点：
- en: '[PRE21]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The final parameter of the `curl` statement references a file, the contents
    of which will be included as the body of the `POST` request. The file contains
    the following details:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: “curl”语句的最后一个参数引用一个文件，该文件的内容将作为“POST”请求的正文包含在其中。该文件包含以下细节：
- en: '[PRE22]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'There are two types of aggregations happening in Druid. There are aggregations
    that happen as part of the indexing and there are aggregations that happen at
    query time. The aggregations that happen during indexing are defined in the spec
    file. If you recall, we had two aggregations in the spec file:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Druid中有两种聚合类型。索引过程中发生的聚合和查询时发生的聚合。索引期间发生的聚合在规范文件中定义。如果你还记得，我们在规范文件中有两种聚合：
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The events we are aggregating have two fields: `symbol` and `price`. The preceding
    aggregations are applied at indexing time, and introduce two additional fields:
    `totalPrice` and `orders`. Recall that `totalPrice` is the sum of the prices on
    each event for that slice of time. The `orders` field contains the total count
    of events in that slice of time.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在聚合的事件有两个字段：“symbol”和“price”。前面的聚合是在索引时间应用的，并引入了两个额外的字段：“totalPrice”和“orders”。请记住，“totalPrice”是该时间段内每个事件的价格总和。“orders”字段包含了该时间段内事件的总数。
- en: 'Then, when we perform the query, Druid applies a second set of aggregations
    based on the `groupBy` statement. In our query, we group by `symbol` at a granularity
    of a minute. The aggregations then introduce two new fields: `cumulativeCount`
    and `cumulativePrice`. These fields contain the sums of the previous aggregations.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在执行查询时，Druid根据“groupBy”语句应用了第二组聚合。在我们的查询中，我们按分钟对“symbol”进行分组。然后聚合引入了两个新字段：“cumulativeCount”和“cumulativePrice”。这些字段包含了前面聚合的总和。
- en: Finally, we introduce a `postaggregation` method to calculate the average for
    that slice of time. The `postaggregation` method divides (`""fn":"/"`) the two
    cumulative fields to yield a new `avg_price` field.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们引入了一个“postaggregation”方法来计算该时间段的平均值。该“postaggregation”方法将两个累积字段进行除法（“fn”：“/”），得到一个新的“avg_price”字段。
- en: 'Issuing the `curl` statement to a running server results in the following response:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 向运行中的服务器发出“curl”语句会得到以下响应：
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Since we updated the code to generate random prices between zero and one hundred,
    it is no surprise that the averages are approximately fifty. (Woo hoo!)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 自从我们更新了代码以生成零到一百之间的随机价格，平均价格大约是五十。（哇呼！）
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we gained a deeper appreciation for the Trident State API.
    We created a direct implementation of the `State` and `StateUpdater` interfaces
    instead of relying on default implementations. Specifically, we implemented these
    interfaces to bridge the gap between a transactional spout and a non-transactional
    system, namely Druid. Although it is impossible to establish exactly-once semantics
    into a non-transactional store, we put mechanisms in place to alert when the system
    encounters issues. Ostensibly, upon failure we could then use a batch processing
    mechanism to reconstruct any suspect aggregation segments.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们更加深入地了解了Trident State API。我们创建了`State`和`StateUpdater`接口的直接实现，而不是依赖于默认实现。具体来说，我们实现了这些接口来弥合事务型spout和非事务型系统（即Druid）之间的差距。虽然在非事务型存储中无法确保精确一次语义，但我们已经采取了机制来在系统遇到问题时发出警报。显然，一旦失败，我们可以使用批处理机制来重建任何可疑的聚合段。
- en: For future investigation, it would be beneficial to establish an idempotent
    interface between Storm and Druid. To do this, we could publish a single segment
    for each batch within Storm. Since segment propagation is atomic within Druid,
    this would give us a mechanism to commit each batch atomically to Druid. Additionally,
    batches could then be processed in parallel, improving throughput. Druid supports
    an ever-expanding set of query types and aggregation mechanisms. It is incredibly
    powerful, and the marriage of Storm and Druid is a powerful one.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了未来的调查，建立Storm和Druid之间的幂等接口将是有益的。为了做到这一点，我们可以在Storm中为每个批次发布一个单独的段。由于在Druid中段的传播是原子的，这将为我们提供一种机制，将每个批次原子地提交到Druid中。此外，批次可以并行处理，从而提高吞吐量。Druid支持日益扩大的查询类型和聚合机制。它非常强大，Storm和Druid的结合是非常强大的。
