- en: Chapter 9. Databricks Visualization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。Databricks可视化
- en: 'This chapter builds on the work done in [Chapter 8](ch08.html "Chapter 8. Spark
    Databricks"), *Spark Databricks*, and continues to investigate the functionality
    of the Apache Spark-based service at [https://databricks.com/](https://databricks.com/).
    Although I will use Scala-based code examples in this chapter, I wish to concentrate
    on the Databricks functionality rather than the traditional Spark processing modules:
    MLlib, GraphX, Streaming, and SQL. This chapter will explain the following Databricks
    areas:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是在[第8章](ch08.html "第8章。Spark Databricks")*Spark Databricks*中完成的工作的基础上继续研究基于Apache
    Spark的服务的功能[https://databricks.com/](https://databricks.com/)。尽管我在本章中将使用基于Scala的代码示例，但我希望集中在Databricks功能上，而不是传统的Spark处理模块：MLlib、GraphX、Streaming和SQL。本章将解释以下Databricks领域：
- en: Data visualization using Dashboards
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用仪表板的数据可视化
- en: An RDD-based report
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于RDD的报告
- en: A Data stream-based report
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于数据流的报告
- en: The Databricks Rest interface
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks Rest接口
- en: Moving data with Databricks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Databricks移动数据
- en: So, this chapter will examine the functionality in Databricks to analytically
    visualize data via reports, and dashboards. It will also examine the REST interface,
    as I believe it to be a useful tool for both, remote access, and integration purposes.
    Finally, it will examine the options for moving data, and libraries, into a Databricks
    cloud instance.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本章将审查Databricks中通过报告和仪表板进行数据分析可视化的功能。它还将检查REST接口，因为我认为它是远程访问和集成目的的有用工具。最后，它将检查将数据和库移动到Databricks云实例的选项。
- en: Data visualization
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Databricks provides tools to access S3, and the local file system-based files.
    It offers the ability to import data into tables, as already shown. In the last
    chapter, raw data was imported into the shuttle table to provide the table-based
    data that SQL could be run against, to filter against rows and columns, allow
    data to be sorted, and then aggregated. This is very useful, but we are still
    left looking at raw data output when images, and reports, present information
    that can be more readily, and visually, interpreted.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks提供了访问S3和基于本地文件系统的文件的工具。它提供了将数据导入表格的能力，如已经显示的。在上一章中，原始数据被导入到航天飞机表中，以提供可以针对其运行SQL的表格数据，以针对行和列进行过滤，允许数据进行排序，然后进行聚合。这非常有用，但当图像和报告呈现可以更容易和直观地解释的信息时，我们仍然在查看原始数据输出。
- en: Databricks provides a visualization interface, based on the tabular result data
    that your SQL session produces. The following screenshot shows some SQL that has
    been run. The resulting data, and the visualization drop-down menu under the data,
    show the possible options.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks提供了一个可视化界面，基于您的SQL会话产生的表格结果数据。以下截图显示了一些已经运行的SQL。生成的数据和数据下面的可视化下拉菜单显示了可能的选项。
- en: '![Data visualization](img/B01989_09_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![数据可视化](img/B01989_09_01.jpg)'
- en: 'There is a range of visualization options here, starting with the more familiar
    **Bar** graphs, and **Pie** charts through to **Quantiles**, and **Box** plots.
    I''m going to change my SQL so that I get more options to plot a graph, which
    is as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一系列的可视化选项，从更熟悉的**柱状图**和**饼图**到**分位数**和**箱线图**。我将更改我的SQL，以便获得更多绘制图形的选项，如下所示：
- en: '![Data visualization](img/B01989_09_02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![数据可视化](img/B01989_09_02.jpg)'
- en: Then, having selected the visualization option; **Bar** graph, I will select
    the **Plot** options which will allow me to choose the data for the graph vertices.
    It will also allow me to select a data column to pivot on. The following screenshot
    shows the values that I have chosen.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在选择了可视化选项；**柱状图**后，我将选择**绘图**选项，这将允许我选择图形顶点的数据。它还将允许我选择要在其上进行数据列的数据列。以下截图显示了我选择的值。
- en: '![Data visualization](img/B01989_09_03.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![数据可视化](img/B01989_09_03.jpg)'
- en: 'The All fields section, from the **Plot** options display, shows all of the
    fields available for the graph display from the SQL statement result data. The
    **Keys** and **Values** sections define the data fields that will form the graph
    axes. The **Series grouping** field allows me to define a value, education, to
    pivot on. By selecting **Apply**, I can now create a graph of total balance against
    a job type, grouped by the education type, as the following screenshot shows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**绘图**选项显示的**所有字段**部分显示了可以从SQL语句结果数据中用于图形显示的所有字段。**键**和**值**部分定义了将形成图形轴的数据字段。**系列分组**字段允许我定义一个值，教育，进行数据透视。通过选择**应用**，我现在可以创建一个根据教育类型分组的工作类型的总余额图表，如下截图所示：'
- en: '![Data visualization](img/B01989_09_04.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![数据可视化](img/B01989_09_04.jpg)'
- en: If I were an accountant trying to determine the factors affecting wage costs,
    and groups of employees within the company that cost the most, I would then see
    the green spike in the previous graph. It seems to indicate that the **management**
    employees with a tertiary education are the most costly group within the data.
    This can be confirmed by changing the SQL to filter on a **tertiary education**,
    ordering the result by balance descending, and creating a new bar graph.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我是一名会计师，试图确定影响工资成本的因素，以及公司内成本最高的员工群体，那么我将看到上一个图表中的绿色峰值。它似乎表明具有高等教育的**管理**员工是数据中成本最高的群体。这可以通过更改SQL以过滤**高等教育**来确认，按余额降序排序结果，并创建一个新的柱状图。
- en: '![Data visualization](img/B01989_09_05.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![数据可视化](img/B01989_09_05.jpg)'
- en: Clearly, the **management** grouping is approximately **14 million**. Changing
    the display option to **Pie** represents the data as a pie graph, with clearly
    sized segments and colors, which visually and clearly present the data, and the
    most important items.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，**管理**分组约为**1400万**。将显示选项更改为**饼图**，将数据表示为饼图，具有清晰大小的分段和颜色，从视觉上清晰地呈现数据和最重要的项目。
- en: '![Data visualization](img/B01989_09_06.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![数据可视化](img/B01989_09_06.jpg)'
- en: I cannot examine all of the display options in this small chapter, but what
    I did want to show is the world map graph that can be created using geographic
    information. I have downloaded the `Countries.zip` file from [http://download.geonames.org/export/dump/](http://download.geonames.org/export/dump/).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我无法在这个小章节中检查所有的显示选项，但我想展示的是可以使用地理信息创建的世界地图图表。我已从[http://download.geonames.org/export/dump/](http://download.geonames.org/export/dump/)下载了`Countries.zip`文件。
- en: 'This will offer a sizeable data set of around 281 MB compressed, which can
    be used to create a new table. It is displayed as a world map graph. I have also
    sourced an ISO2 to ISO3 set of mapping data, and stored it in a Databricks table
    called `cmap`. This allows me to convert ISO2 country codes in the data above
    i.e “AU” to ISO3 country codes i.e “AUS” (needed by the map graph I am about to
    use). The first column in the data that we will use for the map graph, must contain
    the geo location data. In this instance, the country codes in the ISO 3 format.
    So from the countries data, I will create a count of records for each country
    by ISO3 code. It is also important to ensure that the plot options are set up
    correctly in terms of keys, and values. I have stored the downloaded country-based
    data in a table called `geo1`. The SQL used is shown in the following screenshot:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供一个约281MB压缩的庞大数据集，可用于创建新表。它显示为世界地图图表。我还获取了一个ISO2到ISO3的映射数据集，并将其存储在一个名为`cmap`的Databricks表中。这使我能够将数据中的ISO2国家代码（例如“AU”）转换为ISO3国家代码（例如“AUS”）（地图图表所需）。我们将用于地图图表的数据的第一列必须包含地理位置数据。在这种情况下，ISO
    3格式的国家代码。因此，从国家数据中，我将按ISO3代码为每个国家创建记录计数。还要确保正确设置绘图选项的键和值。我已将下载的基于国家的数据存储在一个名为`geo1`的表中。以下截图显示了使用的SQL：
- en: '![Data visualization](img/B01989_09_07.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![数据可视化](img/B01989_09_07.jpg)'
- en: 'As shown previously, this gives two columns of data an ISO3-based value called
    `country`, and a numeric count called `value`. Setting the display option to `Map`
    creates a color-coded world map, shown in the following screenshot:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所示，这给出了两列数据，一个基于ISO3的值称为`country`，和一个称为`value`的数字计数。将显示选项设置为`地图`会创建一个彩色世界地图，如下截图所示：
- en: '![Data visualization](img/B01989_09_08.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![数据可视化](img/B01989_09_08.jpg)'
- en: These graphs show how data can be visually represented in various forms, but
    what can be done if a report is needed for external clients or a dashboard is
    required? All this will be covered in the next section.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表展示了数据可以以各种形式进行视觉呈现，但如果需要为外部客户生成报告或需要仪表板怎么办？所有这些将在下一节中介绍。
- en: Dashboards
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仪表板
- en: In this section, I will use the data in the table called `geo1`, which was created
    in the last section for a map display. It was made to create a simple dashboard,
    and publish the dashboard to an external client. From the **Workspace** menu,
    I have created a new dashboard called `dash1`. If I edit the controls tab of this
    dashboard, I can start to enter SQL, and create graphs, as shown in the following
    screenshot. Each graph is represented as a view and can be defined via SQL. It
    can be resized, and configured using the plot options as per the individual graphs.
    Use the **Add** drop-down menu to add a view. The following screenshot shows that
    `view1` is already created, and added to `dash1`. `view2` is being defined.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将使用上一节中创建的名为`geo1`的表中的数据进行地图显示。它被用来创建一个简单的仪表板，并将仪表板发布给外部客户。从**工作区**菜单中，我创建了一个名为`dash1`的新仪表板。如果我编辑此仪表板的控件选项卡，我可以开始输入SQL，并创建图表，如下截图所示。每个图表都表示为一个视图，并可以通过SQL定义。它可以通过绘图选项调整大小和配置，就像每个图表一样。使用**添加**下拉菜单添加一个视图。以下截图显示`view1`已经创建，并添加到`dash1`。`view2`正在被定义。
- en: '![Dashboards](img/B01989_09_09.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![仪表板](img/B01989_09_09.jpg)'
- en: 'Once all the views have been added, positioned, and resized, the edit tab can
    be selected to present the finalized dashboard. The following screenshot now shows
    the finalized dashboard called `dash1` with three different graphs in different
    forms, and segments of the data:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有视图都被添加、定位和调整大小，可以选择编辑选项卡来呈现最终的仪表板。以下截图现在显示了名为`dash1`的最终仪表板，其中包含三种不同形式的图表和数据的部分：
- en: '![Dashboards](img/B01989_09_10.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![仪表板](img/B01989_09_10.jpg)'
- en: This is very useful for giving a view of the data, but this dashboard is within
    the Databricks cloud environment. What if I want a customer to see this? There
    is a **publish** menu option in the top-right part of the dashboard screen, which
    allows you to publish the dashboard. This displays the dashboard under a new publicly
    published URL, as shown in the following screenshot. Note the new URL at the top
    of the following screenshot. You can now share this URL with your customers to
    present results. There are also options to periodically update the display to
    represent updates in the underlying data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于展示数据非常有用，但这个仪表板是在Databricks云环境中。如果我想让客户看到呢？仪表板屏幕右上角有一个**发布**菜单选项，允许您发布仪表板。这将在新的公开发布的URL下显示仪表板，如下截图所示。请注意以下截图顶部的新URL。您现在可以与客户分享此URL以呈现结果。还有定期更新显示以表示基础数据更新的选项。
- en: '![Dashboards](img/B01989_09_11.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![仪表板](img/B01989_09_11.jpg)'
- en: This gives you an idea of the available display options. All of the reports,
    and dashboards created so far have been based upon SQL, and the data returned.
    In the next section, I will show that reports can be created programmatically
    using a Scala-based Spark RDD, and streamed data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了可用的显示选项的概念。到目前为止，所有创建的报告和仪表板都是基于SQL和返回的数据。在下一节中，我将展示可以使用基于Scala的Spark RDD和流数据以编程方式创建报告。
- en: An RDD-based report
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于RDD的报告
- en: 'The following Scala-based example uses a user-defined class type called `birdType`,
    based on the bird name, and the volume encountered. An RDD is created of the bird
    type records, and then converted into a data frame. The data frame is then displayed.
    Databricks allows the displayed data to be presented as a table or using plot
    options as a graph. The following image shows the Scala that is used:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下基于Scala的示例使用了一个名为`birdType`的用户定义类类型，基于鸟的名称和遇到的数量。 创建了一个鸟类记录的RDD，然后转换为数据框架。
    然后显示数据框架。 Databricks允许将显示的数据呈现为表格或使用绘图选项呈现为图形。 以下图片显示了使用的Scala：
- en: '![An RDD-based report](img/B01989_09_12.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![基于RDD的报告](img/B01989_09_12.jpg)'
- en: 'The bar graph, which this Scala example allows to be created, is shown in the
    following screenshot. The previous Scala code and the following screenshot are
    less important than the fact that this graph has been created programmatically
    using a data frame:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Scala示例允许创建的条形图显示在以下截图中。 前面的Scala代码和下面的截图不如这个图表是通过数据框架以编程方式创建的这一事实重要：
- en: '![An RDD-based report](img/B01989_09_13.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![基于RDD的报告](img/B01989_09_13.jpg)'
- en: This opens up the possibility of programmatically creating data frames, and
    temporary tables from calculation-based data sources. It also allows for streamed
    data to be processed, and the refresh functionality of dashboards to be used,
    to constantly present a window of streamed data. The next section will examine
    a stream-based example of report generation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这打开了以编程方式从基于计算的数据源创建数据框架和临时表的可能性。 它还允许处理流数据，并使用仪表板的刷新功能，以不断呈现流数据的窗口。 下一节将介绍基于流的报告生成示例。
- en: A stream-based report
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于流的报告
- en: 'In this section, I will use Databricks capability to upload a JAR-based library,
    so that we can run a Twitter-based streaming Apache Spark example. In order to
    do this, I must first create a Twitter account, and a sample application at: [https://apps.twitter.com/](https://apps.twitter.com/).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将使用Databricks的能力上传基于JAR的库，以便我们可以运行基于Twitter的流式Apache Spark示例。 为了做到这一点，我必须首先在[https://apps.twitter.com/](https://apps.twitter.com/)上创建一个Twitter帐户和一个示例应用程序。
- en: The following screenshot shows that I have created an application called `My
    example app`. This is necessary, because I need to create the necessary access
    keys, and tokens to create a Scala-based twitter feed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示我创建了一个名为`My example app`的应用程序。 这是必要的，因为我需要创建必要的访问密钥和令牌来创建基于Scala的Twitter
    feed。
- en: '![A stream-based report](img/B01989_09_14.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![基于流的报告](img/B01989_09_14.jpg)'
- en: 'If I now select the application name, I can see the application details. This
    provides a menu option, which provides access to the application details, settings,
    access tokens, and permissions. There is also a button which says **Test OAuth**,
    this enables the access and token keys that will be created to be tested. The
    following screenshot shows the application menu options:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我现在选择应用程序名称，我可以看到应用程序详细信息。 这提供了一个菜单选项，该选项提供对应用程序详细信息、设置、访问令牌和权限的访问。 还有一个按钮，上面写着**测试OAuth**，这使得将要创建的访问和令牌密钥可以进行测试。
    以下截图显示了应用程序菜单选项：
- en: '![A stream-based report](img/B01989_09_15.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![基于流的报告](img/B01989_09_15.jpg)'
- en: By selecting the **Keys and Access Tokens** menu option, the access keys, and
    the access tokens can be generated for the application. Each of the application
    settings and tokens, in this section, have an API key, and a secret key. The top
    of the form, in the following screenshot, shows the consumer key, and consumer
    secret (of course, the key and account details have been removed from these images
    for security reasons).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择**密钥和访问令牌**菜单选项，可以为应用程序生成访问密钥和访问令牌。 在本节中，每个应用程序设置和令牌都有一个API密钥和一个秘密密钥。 在以下截图的表单顶部显示了消费者密钥和消费者秘钥（当然，出于安全原因，这些图像中的密钥和帐户详细信息已被删除）。
- en: '![A stream-based report](img/B01989_09_16.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![基于流的报告](img/B01989_09_16.jpg)'
- en: 'There are also options in the previous screenshot to regenerate the keys, and
    set permissions. The next screenshot shows the application access token details.
    There is an access token, and an access token secret. It also has the options
    to regenerate the values, and revoke access:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一张截图中还有重新生成密钥和设置权限的选项。 下一张截图显示了应用程序访问令牌的详细信息。 有一个访问令牌和一个访问令牌秘钥。 还有重新生成值和撤销访问的选项：
- en: '![A stream-based report](img/B01989_09_17.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![基于流的报告](img/B01989_09_17.jpg)'
- en: 'Using these four alpha numeric value strings, it is possible to write a Scala
    example to access a Twitter stream. The values that will be needed are as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这四个字母数字值字符串，可以编写一个Scala示例来访问Twitter流。 需要的值如下：
- en: Consumer Key
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者密钥
- en: Consumer Secret
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者秘钥
- en: Access Token
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问令牌
- en: Access Token Secret
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问令牌秘钥
- en: 'In the following code sample, I will remove my own key values for security
    reasons. You just need to add your own values to get the code to work. I have
    developed my library, and run the code locally to check whether it will work.
    I did this before loading it to Databricks, in order to reduce time, and costs
    due to debugging. My Scala code sample looks like the following code. First, I
    define a package, import Spark streaming, and twitter resources. Then, I define
    an object class called `twitter1`, and create a main function:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码示例中，出于安全原因，我将删除自己的密钥值。 您只需要添加自己的值即可使代码正常工作。 我已经开发了自己的库，并在本地运行代码以检查它是否能正常工作。
    我在将其加载到Databricks之前就这样做了，以减少调试所需的时间和成本。 我的Scala代码示例如下。 首先，我定义一个包，导入Spark流和Twitter资源。
    然后，我定义了一个名为`twitter1`的对象类，并创建了一个主函数：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, I create a Spark configuration object using an application name. I have
    not used a Spark master URL, as I will let both, `spark-submit`, and Databricks
    assign the default URL. From this, I will create a Spark context, and define the
    Twitter consumer, and access values:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我使用应用程序名称创建一个Spark配置对象。 我没有使用Spark主URL，因为我将让`spark-submit`和Databricks分配默认URL。
    从这里，我将创建一个Spark上下文，并定义Twitter消费者和访问值：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'I set the Twitter access properties using the `System.setProperty` call, and
    use it to set the four `twitter4j` `oauth` access properties using the access
    keys, which were generated previously:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用`System.setProperty`调用设置了Twitter访问属性，并使用它来设置四个`twitter4j` `oauth`访问属性，使用之前生成的访问密钥：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'A streaming context is created from the Spark context, which is used to create
    a Twitter-based Spark DStream. The stream is split by spaces to create words,
    and it gets filtered by the words starting with `#`, to select hash tags:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark上下文创建了一个流上下文，用于创建基于Twitter的Spark DStream。流被空格分割以创建单词，并且通过以`#`开头的单词进行过滤，以选择哈希标签：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The function used below to get a singleton SQL Context is defined at the end
    of this example. So, for each RDD in the stream of hash tags, a single SQL context
    is created. This is used to import implicits which allows an RDD to be implicitly
    converted to a data frame using `toDF`. A data frame is created from each `rdd`
    called `dfHashTags`, and this is then used to register a temporary table. I have
    then run some SQL against the table to get a count of rows. The count of rows
    is then printed. The horizontal banners in the code are just used to enable easier
    viewing of the output of results when using `spark-submit`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下面用于获取单例SQL上下文的函数在本示例的末尾定义。因此，对于哈希标签流中的每个RDD，都会创建一个单独的SQL上下文。这用于导入隐式，允许RDD通过`toDF`隐式转换为数据框。从每个`rdd`创建了一个名为`dfHashTags`的数据框，然后用它注册了一个临时表。然后我对表运行了一些SQL以获取行数。然后打印出行数。代码中的横幅只是用来在使用`spark-submit`时更容易查看输出结果：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'I have also output a list of the top five tweets by volume in my current tweet
    stream data window. You might recognize the following code sample. It is from
    the Spark examples on GitHub. Again, I have used the banner to help with the results
    that will be seen in the output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我还输出了当前推文流数据窗口中前五条推文的列表。你可能会认出以下代码示例。这是来自GitHub上Spark示例。同样，我使用了横幅来帮助输出结果的查看：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, I have used `start` and `awaitTermination`, via the Spark stream context
    `ssc`, to start the application, and keep it running until stopped:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我使用Spark流上下文`ssc`的`start`和`awaitTermination`来启动应用程序，并保持其运行直到停止：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, I have defined the singleton SQL context function, and the `dataframe`
    `case` `class` for each row in the hash tag data stream `rdd`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我已经定义了单例SQL上下文函数，并且为哈希标签数据流`rdd`中的每一行定义了`dataframe` `case` `class`：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'I compiled this Scala application code using SBT into a JAR file called `data-bricks_2.10-1.0.jar`.
    My `SBT` file looks as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用SBT编译了这个Scala应用程序代码，生成了一个名为`data-bricks_2.10-1.0.jar`的JAR文件。我的`SBT`文件如下：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'I downloaded the correct version of Apache Spark onto my cluster to match the
    current version used by Databricks at this time (1.3.1). I then installed it under
    `/usr/local/` on each node in my cluster, and ran it in local mode with spark
    as the cluster manager. My `spark-submit` script looks as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我下载了正确版本的Apache Spark到我的集群上，以匹配Databricks当前使用的版本（1.3.1）。然后我在集群中的每个节点下安装了它，并以spark作为集群管理器在本地模式下运行。我的`spark-submit`脚本如下：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'I won''t go through the details, as it has been covered quite a few times,
    except to note that the class value is now `nz.co.semtechsolutions.twitter1`.
    This is the package class name, plus the application object class name. So, when
    I run it locally, I get an output as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会详细介绍，因为已经涵盖了很多次，除了注意现在类值是`nz.co.semtechsolutions.twitter1`。这是包类名，加上应用对象类名。所以，当我在本地运行时，我得到以下输出：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This tells me that the application library works. It connects to Twitter, creates
    a data stream, is able to filter the data into hash tags, and creates a temporary
    table using the data. So, having created a JAR library for Twitter data streaming,
    and proving that it works, I'm now able to load it onto the Databricks cloud.
    The following screenshot shows that a job has been created from the Databricks
    cloud jobs menu called `joblib1`. The **Set Jar** option has been used to upload
    the JAR library that was just created. The full package-based name to the `twitter1`
    application object class has been specified.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我应用程序库起作用了。它连接到Twitter，创建数据流，能够将数据过滤为哈希标签，并使用数据创建临时表。因此，创建了一个用于Twitter数据流的JAR库，并证明它有效后，我现在可以将其加载到Databricks云上。以下截图显示了从Databricks云作业菜单创建了一个名为`joblib1`的作业。**设置Jar**选项已用于上传刚刚创建的JAR库。已指定了到`twitter1`应用对象类的完整基于包的名称。
- en: '![A stream-based report](img/B01989_09_18.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![基于流的报告](img/B01989_09_18.jpg)'
- en: The following screenshot shows the `joblib1` job, which is ready to run. A Spark-based
    cluster will be created on demand, as soon as the job is executed using the **Run
    Now** option, under the **Active runs** section. No scheduling options have been
    specified, although the job can be defined to run at a given date and time.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了名为`joblib1`的作业，已准备就绪。基于Spark的集群将根据需要创建，一旦使用**立即运行**选项执行作业，将在**活动运行**部分下立即执行。虽然没有指定调度选项，但可以定义作业在特定日期和时间运行。
- en: '![A stream-based report](img/B01989_09_19.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![基于流的报告](img/B01989_09_19.jpg)'
- en: I selected the **Run Now** option to start the job run, as shown in the following
    screenshot. This shows that there is now an active run called `Run 1` for this
    job. It has been running for six seconds. It was launched manually, and is pending
    while a on-demand cluster is created. By selecting the run name `Run 1`, I can
    see details about the job, especially the logged output.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了**立即运行**选项来启动作业运行，如下截图所示。这显示现在有一个名为`Run 1`的活动运行。它已经运行了六秒。它是手动启动的，正在等待创建按需集群。通过选择运行名称`Run
    1`，我可以查看有关作业的详细信息，特别是已记录的输出。
- en: '![A stream-based report](img/B01989_09_20.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![基于流的报告](img/B01989_09_20.jpg)'
- en: The following screenshot shows an example of the output for `Run 1` of `joblib1`.
    It shows the time started and duration, it also shows the running status and job
    details in terms of class and JAR file. It would have shown class parameters,
    but there were none in this case. It also shows the details of the 54 GB on-demand
    cluster. More importantly, it shows the list of the top five tweet hash tag values.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了`joblib1`的`Run 1`输出的示例。它显示了开始时间和持续时间，还显示了运行状态和作业详细信息，包括类和JAR文件。它本应该显示类参数，但在这种情况下没有。它还显示了54GB按需集群的详细信息。更重要的是，它显示了前五个推文哈希标签值的列表。
- en: '![A stream-based report](img/B01989_09_21.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![基于流的报告](img/B01989_09_21.jpg)'
- en: The following screenshot shows the same job run output window in the Databricks
    cloud instance. But this shows the output from the SQL `count(*)`, showing the
    number of tweet hash tags in the current data stream tweet window from the temporary
    table.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了Databricks云实例中相同作业运行输出窗口。但这显示了来自SQL `count(*)`的输出，显示了当前数据流推文窗口中临时表中的推文哈希标签数量。
- en: '![A stream-based report](img/B01989_09_22.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![基于流的报告](img/B01989_09_22.jpg)'
- en: So, this proves that I can create an application library locally, using Twitter-based
    Apache Spark streaming, and convert the data stream into data frames, and a temporary
    table. It shows that I can reduce costs by developing locally, and then port my
    library to my Databricks cloud. I am aware that I have neither visualized the
    temporary table, nor the DataFrame in this example, into a Databricks graph, but
    time scales did not allow me to do this. Also, another thing that I would have
    done, if I had time, would be to checkpoint, or periodically save the stream to
    file, in case of application failure. However, this topic is covered in [Chapter
    3](ch03.html "Chapter 3. Apache Spark Streaming"), *Apache Spark Streaming* with
    an example, so you can take a look there if you are interested. In the next section,
    I will examine the Databricks REST API, which will allow better integration between
    your external applications, and your Databricks cloud instance.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这证明了我可以在本地创建一个应用程序库，使用基于Twitter的Apache Spark流处理，并将数据流转换为数据框架和临时表。它表明我可以通过在本地开发，然后将我的库移植到Databricks云来降低成本。我知道在这个例子中我既没有将临时表可视化，也没有将DataFrame可视化为Databricks图表，但时间不允许我这样做。另外，如果有时间，我会做的另一件事是在应用程序失败时进行检查点或定期保存流到文件。然而，这个主题在[第3章](ch03.html
    "第3章。Apache Spark Streaming")中有所涵盖，*Apache Spark Streaming*中有一个例子，所以如果您感兴趣，可以在那里看一下。在下一节中，我将检查Databricks
    REST API，它将允许您的外部应用程序与Databricks云实例更好地集成。
- en: REST interface
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: REST接口
- en: 'Databricks provides a REST interface for Spark cluster-based manipulation.
    It allows for cluster management, library management, command execution, and the
    execution of contexts. To be able to access the REST API, the port `34563` must
    be accessible for your instance in the AWS EC2-based Databricks cloud. The following
    Telnet command shows an attempt to access the port `34563` of my Databricks cloud
    instance. Note that the Telnet attempt has been successful:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks为基于Spark集群的操作提供了REST接口。它允许集群管理、库管理、命令执行和上下文的执行。要能够访问REST API，AWS EC2基础的Databricks云中的实例必须能够访问端口`34563`。以下是尝试访问我Databricks云实例端口`34563`的Telnet命令。请注意，Telnet尝试已成功：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you do not receive a Telnet session, then contact Databricks via `<[help@databricks.com](mailto:help@databricks.com)>`.
    The next sections provide examples of REST interface access to your instance on
    the Databricks cloud.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有收到Telnet会话，请通过`<[help@databricks.com](mailto:help@databricks.com)>`联系Databricks。接下来的部分提供了访问Databricks云实例的REST接口示例。
- en: Configuration
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置
- en: In order to use the interface, I needed to whitelist the IP address that I use
    to access my Databricks cluster instance. This is the IP address of the machine
    from which I will be running the REST API commands. By whitelisting the IP addresses,
    Databricks can ensure that a secure list of users access each Databricks cloud
    instance.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用接口，我需要将我用于访问Databricks集群实例的IP地址加入白名单。这是我将运行REST API命令的机器的IP地址。通过将IP地址加入白名单，Databricks可以确保每个Databricks云实例都有一个安全的用户访问列表。
- en: 'I contacted Databricks support via the previous help email address, but there
    is also a Whitelist IP Guide, found in the **Workspace** menu in your cloud instance:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过之前的帮助电子邮件地址联系了Databricks支持，但在您的云实例的**工作区**菜单中还有一个白名单IP指南：
- en: '**Workspace** | **databricks_guide** | **DevOps Utilitie**s | **Whitelist IP**.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**工作区** | **databricks_guide** | **DevOps工具** | **白名单IP**。'
- en: REST API calls can now be submitted to my Databricks cloud instance, from the
    Linux command line, using the Linux `curl` command. The example general form of
    the `curl` command is shown next using my Databricks cloud instance username,
    password, cloud instance URL, REST API path, and parameters.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以使用Linux `curl`命令从Linux命令行向我的Databricks云实例提交REST API调用。下面显示了`curl`命令的示例通用形式，使用了我的Databricks云实例用户名、密码、云实例URL、REST
    API路径和参数。
- en: 'The Databricks forum, and the previous help email address can be used to gain
    further information. The following sections will provide some REST API worked
    examples:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks论坛和之前的帮助电子邮件地址可用于获取更多信息。接下来的部分将提供一些REST API的工作示例：
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Cluster management
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群管理
- en: 'You will still need to create Databricks Spark clusters from your cloud instance
    user interface. The list REST API command is as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您仍然需要从您的云实例用户界面创建Databricks Spark集群。列表REST API命令如下：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It needs no parameters. This command will provide a list of your clusters,
    their status, IP addresses, names, and the port numbers that they run on. The
    following output shows that the cluster `semclust1` is in a pending state in the
    process of being created:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 它不需要任何参数。此命令将提供您的集群列表、它们的状态、IP地址、名称以及它们运行的端口号。以下输出显示，集群`semclust1`处于挂起状态，正在创建过程中：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The same REST API command run when the cluster is available, shows that the
    cluster called `semcust1` is running, and has one worker:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当集群可用时运行相同的REST API命令，显示名为`semcust1`的集群正在运行，并且有一个worker：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Terminating this cluster, and creating a new one called `semclust` changes
    the results of the REST API call as shown:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 终止此集群，并创建一个名为`semclust`的新集群，将更改REST API调用的结果，如下所示：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The execution context
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行上下文
- en: 'With these API calls, you can create, show the status of, or delete an execution
    context. The REST API calls are as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些API调用，您可以创建、显示或删除执行上下文。REST API调用如下：
- en: '`/api/1.0/contexts/create`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/api/1.0/contexts/create`'
- en: '`/api/1.0/contexts/status`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/api/1.0/contexts/status`'
- en: '`/api/1.0/contexts/destroy`'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/api/1.0/contexts/destroy`'
- en: In the following REST API call example, submitted via `curl`, a Scala context
    has been created for the cluster `semclust` identified by it's cluster ID.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下REST API调用示例中，通过`curl`提交，为标识为其集群ID的`semclust`创建了一个Scala上下文。
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The result returned is either an error, or a context ID. The following three
    example return values show an error caused by an invalid URL, and two successful
    calls returning context IDs:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的结果要么是错误，要么是上下文ID。以下三个示例返回值显示了由无效URL引起的错误，以及两个成功调用返回的上下文ID：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Command execution
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命令执行
- en: 'These commands allow you to run a command, list a command status, cancel a
    command, or show the results of a command. The REST API calls are as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令允许您运行命令、列出命令状态、取消命令或显示命令的结果。REST API调用如下：
- en: /api/1.0/commands/execute
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: /api/1.0/commands/execute
- en: /api/1.0/commands/cancel
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: /api/1.0/commands/cancel
- en: /api/1.0/commands/status
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: /api/1.0/commands/status
- en: 'The following example shows an SQL statement being run against an existing
    table called `cmap`. The context must exist, and must be of the SQL type. The
    parameters have been passed on to the HTTP GET call via a `–d` option. The parameters
    are language, the cluster ID, the context ID, and the SQL command. The command
    ID is returned as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例显示了针对名为`cmap`的现有表运行的SQL语句。上下文必须存在，并且必须是SQL类型。参数已通过`-d`选项传递给HTTP GET调用。参数是语言、集群ID、上下文ID和SQL命令。命令ID返回如下：
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Libraries
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 库
- en: 'The REST API also allows for libraries to be uploaded to a cluster and their
    statuses checked. The REST API call paths are as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: REST API还允许上传库到集群并检查它们的状态。REST API调用路径如下：
- en: '`/api/1.0/libraries/upload`'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/api/1.0/libraries/upload`'
- en: '`/api/1.0/libraries/list`'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/api/1.0/libraries/list`'
- en: 'An example is given next of a library upload to the cluster instance called
    `semclust`. The parameters passed on to the HTTP GET API call via a `–d` option
    are the language, cluster ID, the library name and URI. A successful call results
    in the name and URI of the library, which is as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来给出了一个上传到名为`semclust`的集群实例的库的示例。通过`-d`选项将参数传递给HTTP GET API调用的语言、集群ID、库名称和URI。成功的调用将返回库的名称和URI，如下所示：
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note that this REST API can change by content and version overtime, so check
    in the Databricks forum, and use the previous help email address to check the
    API details with Databricks support. I do think though that, with these simple
    example calls, it is clear that this REST API can be used to integrate Databricks
    with the external systems, and ETL chains. In the next section, I will provide
    an overview of data movement within the Databricks cloud.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此REST API可能会随内容和版本而更改，因此请在Databricks论坛中检查，并使用以前的帮助电子邮件地址与Databricks支持检查API详细信息。我认为，通过这些简单的示例调用，很明显这个REST
    API可以用于将Databricks与外部系统和ETL链集成。在下一节中，我将概述Databricks云内的数据移动。
- en: Moving data
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据移动
- en: Some of the methods of moving data in and out of Databricks have already been
    explained in [Chapter 8](ch08.html "Chapter 8. Spark Databricks"), *Spark Databricks*
    and [Chapter 9](ch09.html "Chapter 9. Databricks Visualization"), *Databricks
    Visualization*. What I would like to do in this section is provide an overview
    of all of the methods available for moving data. I will examine the options for
    tables, workspaces, jobs, and Spark code.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在Databricks中移动数据的一些方法已经在[第8章](ch08.html "第8章。Spark Databricks") *Spark Databricks*和[第9章](ch09.html
    "第9章。Databricks Visualization") *Databricks Visualization*中进行了解释。我想在本节中概述所有可用的移动数据方法。我将研究表、工作区、作业和Spark代码的选项。
- en: The table data
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表数据
- en: The table import functionality for Databricks cloud allows data to be imported
    from an AWS **S3** bucket, from the **Databricks file system** (**DBFS**), via
    JDBC and finally from a local file. This section gives an overview of each type
    of import, starting with **S3**. Importing the table data from AWS **S3** requires
    the AWS Key, the AWS secret key, and the **S3** bucket name. The following screenshot
    shows an example. I have already provided an example of **S3** bucket creation,
    including adding an access policy, so I will not cover it again.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks云的表导入功能允许从AWS **S3**存储桶、**Databricks文件系统**（**DBFS**）、通过JDBC以及从本地文件导入数据。本节概述了每种类型的导入，从**S3**开始。从AWS
    **S3**导入表数据需要AWS密钥、AWS秘钥和**S3**存储桶名称。以下屏幕截图显示了一个示例。我已经提供了一个**S3**存储桶创建的示例，包括添加访问策略，因此我不会再次介绍它。
- en: '![The table data](img/B01989_09_23.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![表数据](img/B01989_09_23.jpg)'
- en: 'Once the form details are added, you will be able to browse your **S3** bucket
    for a data source. Selecting `DBFS` as a table data source enables your `DBFS`
    folders, and files to be browsed. Once a data source is selected, it can display
    a preview as the following screenshot shows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦添加了表单详细信息，您就可以浏览您的**S3**存储桶以获取数据源。选择`DBFS`作为表数据源可以浏览您的`DBFS`文件夹和文件。选择数据源后，可以显示预览，如下面的屏幕截图所示：
- en: '![The table data](img/B01989_09_24.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![表数据](img/B01989_09_24.jpg)'
- en: 'Selecting `JDBC` as a table data source allows you to specify a remote SQL
    database as a data source. Just add an access **URL**, **Username**, and **Password**.
    Also, add some SQL to define the table, and columns to source. There is also an
    option of adding extra properties to the call via the **Add Property** button,
    as the following screenshot shows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 选择`JDBC`作为表格数据源允许您指定远程SQL数据库作为数据源。只需添加一个访问**URL**、**用户名**和**密码**。还可以添加一些SQL来定义表和源列。还有一个通过**添加属性**按钮添加额外属性的选项，如下面的屏幕截图所示：
- en: '![The table data](img/B01989_09_25.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![表格数据](img/B01989_09_25.jpg)'
- en: Selecting the **File** option to populate a Databricks cloud instance table,
    from a file, creates a drop down or browse. This upload method was used previously
    to upload CSV-based data into a table. Once the data source is specified, it is
    possible to specify a data separator string or header row, define column names
    or column types and preview the data before creating the table.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 选择**文件**选项以从文件中填充Databricks云实例表，创建一个下拉或浏览。此上传方法先前用于将基于CSV的数据上传到表中。一旦指定了数据源，就可以指定数据分隔符字符串或标题行，定义列名或列类型，并在创建表之前预览数据。
- en: '![The table data](img/B01989_09_26.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![表格数据](img/B01989_09_26.jpg)'
- en: Folder import
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件夹导入
- en: 'From either a workspace, or a folder drop-down menu, it is possible to import
    an item. The following screenshot shows a compound image from the **Import Item**
    menu option:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从工作区或文件夹下拉菜单中，可以导入项目。以下屏幕截图显示了**导入项目**菜单选项的复合图像：
- en: '![Folder import](img/B01989_09_27.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![文件夹导入](img/B01989_09_27.jpg)'
- en: This creates a file drop or browse window, which when clicked, allows you to
    browse the local server for the items to import. Selecting the `All Supported
    Types` option shows that the items to import can be JAR files, dbc archives, Scala,
    Python, or SQL files.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个文件拖放或浏览窗口，当点击时，允许您浏览本地服务器以导入项目。选择“所有支持的类型”选项显示可以导入的项目可以是JAR文件、dbc存档、Scala、Python或SQL文件。
- en: Library import
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 库导入
- en: 'The following screenshot shows the **New Library** functionality, from the
    Workspace and folder menu options. This allows an externally created and tested
    library to be loaded to your Databricks cloud instance. The library can be in
    the form of a Java or Scala JAR file, a Python Egg or a Maven coordinate for repository
    access. In the following screenshot, a JAR file is being selected from the local
    server via a browse window. This functionality has been used in this chapter to
    test stream-based Scala programming:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了来自Workspace和文件夹菜单选项的**新库**功能。这允许将外部创建和测试的库加载到您的Databricks云实例中。该库可以是Java或Scala
    JAR文件、Python Egg或用于访问存储库的Maven坐标。在下面的屏幕截图中，正在从本地服务器通过浏览窗口选择一个JAR文件。本章中使用了此功能来测试基于流的Scala编程：
- en: '![Library import](img/B01989_09_28.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![库导入](img/B01989_09_28.jpg)'
- en: Further reading
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Before summing up this chapter, and the last for cloud-based Apache Spark usage
    in Databricks, I wanted to mention some resources for gaining extra information
    on both, Apache Spark, and Databricks. First, there is the Databricks forum available
    at: [forums.databricks.com/](http://forums.databricks.com/) for questions, and
    answers related to the use of [https://databricks.com/](https://databricks.com/).
    Also, within your Databricks instance, under the Workspace menu option, there
    will be a Databricks guide that contains a lot of useful information. The Apache
    Spark website at [http://spark.apache.org/](http://spark.apache.org/) also contains
    a lot of useful information, as well as module-based API documentation. Finally,
    there is the Spark mailing list, `<[user@spark.apache.org](mailto:user@spark.apache.org)>`,
    which provides a great deal of Spark usage information, and problem solving.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结本章之前，也是Databricks云端使用Apache Spark的最后一章，我想提及一些关于Apache Spark和Databricks的额外信息资源。首先，有Databricks论坛可供访问：[forums.databricks.com/](http://forums.databricks.com/)，用于与[https://databricks.com/](https://databricks.com/)的使用相关的问题和答案。此外，在您的Databricks实例中，在Workspace菜单选项下，将有一个包含许多有用信息的Databricks指南。Apache
    Spark网站[http://spark.apache.org/](http://spark.apache.org/)也包含许多有用信息，以及基于模块的API文档。最后，还有Spark邮件列表，`<[user@spark.apache.org](mailto:user@spark.apache.org)>`，提供了大量关于Spark使用信息和问题解决的信息。
- en: Summary
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: '[Chapter 8](ch08.html "Chapter 8. Spark Databricks"), *Spark Databricks* and
    [Chapter 9](ch09.html "Chapter 9. Databricks Visualization"), *Databricks Visualization*,
    have provided an introduction to Databricks in terms of cloud installation, and
    the use of Notebooks and folders. Account and cluster management have been examined.
    Also, job creation, the idea of remote library creation, and importing have been
    examined. The functionality of the Databricks `dbutils` package, and the Databricks
    file system was explained in [Chapter 8](ch08.html "Chapter 8. Spark Databricks"),
    *Spark Databricks*. Tables, and an example of data import was also shown so that
    SQL can be run against a dataset.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章](ch08.html "第8章。Spark Databricks")、*Spark Databricks*和[第9章](ch09.html
    "第9章。Databricks可视化")、*Databricks可视化*，已经介绍了Databricks在云安装方面的情况，以及Notebooks和文件夹的使用。已经检查了帐户和集群管理。还检查了作业创建、远程库创建的概念以及导入。解释了Databricks
    `dbutils`包的功能，以及Databricks文件系统在[第8章](ch08.html "第8章。Spark Databricks")、*Spark
    Databricks*中。还展示了表格和数据导入的示例，以便对数据集运行SQL。'
- en: The idea of data visualization has been examined, and a variety of graphs have
    also been created. Dashboards have been made to show how easy it is to both, create,
    and share this kind of data presentation. The Databricks REST interface has been
    shown via worked examples, as an aid to using a Databricks cloud instance remotely,
    and integrating it with external systems. Finally, the data and library movement
    options have been examined in terms of workspace, folders, and tables.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 已经检查了数据可视化的概念，并创建了各种图表。已经创建了仪表板，以展示创建和共享这种数据呈现的简易性。通过示例展示了Databricks REST接口，作为远程使用Databricks云实例并将其与外部系统集成的辅助。最后，已经检查了关于工作区、文件夹和表的数据和库移动选项。
- en: You might ask why I have committed two chapters to a cloud-based service such
    as Databricks. The reason is that Databricks seems to be a logical, cloud-based
    progression, from Apache Spark. It is supported by the people who originally developed
    Apache Spark and although in it's infancy as a service and subject to change still
    capable of providing a Spark cloud based production service. This means that a
    company wishing to use a Spark could use Databricks and grow their cloud as demand
    grows and have access to dynamic Spark-based machine learning, graph processing,
    SQL, streaming and visualization functionality.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会问为什么我要把两章内容都献给像Databricks这样的基于云的服务。原因是Databricks似乎是从Apache Spark发展而来的一个逻辑上的基于云的进展。它得到了最初开发Apache
    Spark的人的支持，尽管作为一个服务还处于初期阶段，可能会发生变化，但仍然能够提供基于Spark的云生产服务。这意味着一家希望使用Spark的公司可以使用Databricks，并随着需求增长而扩展他们的云，并且可以访问动态的基于Spark的机器学习、图处理、SQL、流处理和可视化功能。
- en: As ever, these Databricks chapters have just scratched the surface of the functionality
    available. The next step will be to create an AWS and Databricks account yourself,
    and use the information provided here to gain practical experience.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正如以往一样，这些Databricks章节只是触及了功能的表面。下一步将是自己创建一个AWS和Databricks账户，并使用这里提供的信息来获得实际经验。
- en: 'As this is the last chapter, I will provide my contact details again. I would
    be interested in the ways that people are using Apache Spark. I would be interested
    in the size of clusters you are creating, and the data that you are processing.
    Are you using Spark as a processing engine? Or are you building systems on top
    of it? You can connect with me at LinkedIn at: [linkedin.com/profile/view?id=73219349](http://linkedin.com/profile/view?id=73219349).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是最后一章，我将再次提供我的联系方式。我对人们如何使用Apache Spark感兴趣。我对您创建的集群规模以及您处理的数据感兴趣。您是将Spark作为处理引擎使用吗？还是在其上构建系统？您可以在LinkedIn上与我联系：[linkedin.com/profile/view?id=73219349](http://linkedin.com/profile/view?id=73219349)。
- en: 'You can contact me via my website at `semtech-solutions.co.nz` or finally,
    by email at: `<[info@semtech-solutions.co.nz](mailto:info@semtech-solutions.co.nz)>`.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过我的网站`semtech-solutions.co.nz`或最后通过电子邮件联系我：`<[info@semtech-solutions.co.nz](mailto:info@semtech-solutions.co.nz)>`。
- en: 'Finally, I maintain a list of open-source-software-related presentations when
    I have the time. Anyone is free to use, and download them. They are available
    on SlideShare at: [http://www.slideshare.net/mikejf12/presentations](http://www.slideshare.net/mikejf12/presentations).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我在有空的时候会维护一个与开源软件相关的演示文稿列表。任何人都可以免费使用和下载它们。它们可以在SlideShare上找到：[http://www.slideshare.net/mikejf12/presentations](http://www.slideshare.net/mikejf12/presentations)。
- en: If you have any challenging opportunities or problems, please feel free to contact
    me using the previous details.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有任何具有挑战性的机会或问题，请随时使用上述联系方式与我联系。
