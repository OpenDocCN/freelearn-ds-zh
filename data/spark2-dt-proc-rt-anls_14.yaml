- en: Unsupervised Clustering with Apache Spark 2.0
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark 2.0进行无监督聚类
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Building a KMeans classification system in Spark 2.0
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中构建KMeans分类系统
- en: Bisecting KMeans, the new kid on the block in Spark 2.0
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中，二分KMeans作为新星登场
- en: Using Gaussian Mixture and Expectation Maximization (EM) in Spark 2.0 to classify
    data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用高斯混合模型和期望最大化（EM）算法进行数据分类
- en: Classifying the vertices of a graph using Power Iteration Clustering (PIC) in
    Spark 2.0
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用幂迭代聚类（PIC）对图的顶点进行分类
- en: Using Latent Dirichlet Allocation (LDA) to classify documents and text into
    topics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用潜在狄利克雷分配（LDA）将文档和文本分类为主题
- en: Streaming KMeans to classify data in near real time
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流式KMeans在接近实时的情况下对数据进行分类
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Unsupervised machine learning is a type of learning technique in which we try
    to draw inferences either directly or indirectly (through latent factors) from
    a set of unlabeled observations. In simple terms, we are trying to find the hidden
    knowledge or structures in a set of data without initially labeling the training
    data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习是一种学习技术，我们试图直接或间接（通过潜在因素）从一组未标记的观察中得出推断。简而言之，我们试图在未对训练数据进行初始标记的情况下，从一组数据中发现隐藏的知识或结构。
- en: While most machine learning library implementation break down when applied to
    large datasets (iterative, multi-pass, a lot of intermediate writes), the Apache
    Spark Machine Library succeeds by providing machine library algorithms designed
    for parallelism and extremely large datasets using memory for intermediate writes
    out of the box.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数机器学习库的实现在大数据集上应用时会崩溃（迭代、多次遍历、大量中间写入），但Apache Spark机器学习库通过提供为并行性和极大数据集设计的算法，并默认使用内存进行中间写入，从而取得了成功。
- en: 'At the most abstract level, we can think of unsupervised learning as:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在最抽象的层面上，我们可以将无监督学习视为：
- en: Building a KMeans classifying system in Spark 2.0
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0中构建KMeans分类系统
- en: In this recipe, we will load a set of features (for example, x, y, z coordinates)
    using a LIBSVM file and then proceed to use `KMeans()` to instantiate an object.
    We will then set the number of desired clusters to three and then use `kmeans.fit()`
    to action the algorithm. Finally, we will print the centers for the three clusters
    that we found.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用LIBSVM文件加载一组特征（例如，x，y，z坐标），然后使用`KMeans()`实例化一个对象。接着，我们将设置期望的簇数为三个，并使用`kmeans.fit()`执行算法。最后，我们将打印出我们找到的三个簇的中心。
- en: It is really important to note that Spark *does not* implement KMeans++, contrary
    to popular literature, instead it implements KMeans || (pronounced as KMeans Parallel).
    See the following recipe and the sections following the code for a complete explanation
    of the algorithm as it is implemented in Spark.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Spark*并未*实现KMeans++，这与流行文献相反，而是实现了KMeans ||（发音为KMeans Parallel）。请参阅以下教程以及代码之后的部分，以获得对Spark中实现的算法的完整解释。
- en: How to do it...
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了获取集群访问权限并使用`Log4j.Logger`减少Spark产生的输出量，需要导入必要的Spark上下文包：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的日志输出：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create Spark''s Session object:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的Session对象：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How it works...
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We read a LIBSVM file with a set of coordinates (can be interpreted as a tuple
    of three numbers) and then created a `KMean()` object, but changed the default
    number of clusters from 2 (out of the box) to 3 for demonstration purposes. We
    used the `.fit()` to create the model and then used `model.summary.predictions.show()`
    to display which tuple belongs to which cluster. In the last step, we printed
    the cost and the center of the three clusters. Conceptually, it can be thought
    of as having a set of 3D coordinates as data and then assigning each individual
    coordinate to one of the three clusters using KMeans algorithms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取了一个 LIBSVM 文件，其中包含一组坐标（可以解释为三个数字的元组），然后创建了一个 `KMean()` 对象，但将默认簇数从 2（开箱即用）更改为
    3，以便演示。我们使用 `.fit()` 创建模型，然后使用 `model.summary.predictions.show()` 显示哪个元组属于哪个簇。在最后一步中，我们打印了成本和三个簇的中心。从概念上讲，可以将其视为拥有一组
    3D 坐标数据，然后使用 KMeans 算法将每个单独的坐标分配给三个簇之一。
- en: KMeans is a form of unsupervised machine learning algorithm, with its root in
    signal processing (vector quantization) and compression (grouping similar vectors
    of items together to achieve a higher compression rate). Generally speaking, the
    KMeans algorithm attempts to group a series of observations {X[1,] X[2], ....
    , X[n]} into a series of clusters {C[1,] C[2 .....] C[n]} using a form of distance
    measure (local optimization) that is optimized in an iterative manner.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: KMeans 是一种无监督机器学习算法，其根源在于信号处理（矢量量化）和压缩（将相似的物品矢量分组以实现更高的压缩率）。一般来说，KMeans 算法试图将一系列观察值
    {X[1,] X[2], .... , X[n]} 分组到一系列簇 {C[1,] C[2 .....] C[n]} 中，使用一种距离度量（局部优化），该度量以迭代方式进行优化。
- en: There are three main types of KMeans algorithm that are in use. In a simple
    survey, we found 12 specialized variations of the KMeans algorithm. It is important
    to note that Spark implements a version called KMeans || (KMeans Parallel) and
    *not* KMeans++ or standard KMeans as referenced in some literature or videos.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 目前使用的 KMeans 算法主要有三种类型。在一项简单的调查中，我们发现了 12 种 KMeans 算法的专门变体。值得注意的是，Spark 实现了一个名为
    KMeans ||（KMeans 并行）的版本，而不是文献或视频中提到的 KMeans++ 或标准 KMeans。
- en: 'The following figure depicts KMeans in a nutshell:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下图简要描绘了 KMeans 算法：
- en: '![](img/870bc634-f412-46c1-9857-83ed6e05183f.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/870bc634-f412-46c1-9857-83ed6e05183f.png)'
- en: 'Source: Spark documentation'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：Spark 文档
- en: KMeans (Lloyd Algorithm)
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KMeans（Lloyd 算法）
- en: 'The steps for basic KMeans implementation (Lloyd algorithm) are:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 基本 KMeans 实现（Lloyd 算法）的步骤如下：
- en: Randomly select K datacenters from observations as the initial centroids.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从观察结果中随机选择 K 个数据中心作为初始中心。
- en: 'Keep iterating till the convergence criteria is met:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持续迭代直至满足收敛条件：
- en: Measure the distance from a point to each centroid
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量一个点到每个中心的距离
- en: Include each data point in a cluster which is the closest centroid
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个数据点包含在与其最接近的中心对应的簇中
- en: Calculate new cluster centroids based on a distance formula (proxy for dissimilarity)
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据距离公式（作为不相似性的代理）计算新的簇中心
- en: Update the algorithm with new center points
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用新的中心点更新算法
- en: 'The three generations are depicted in the following figure:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描绘了三代算法：
- en: '![](img/bc4b1c43-9ed0-4396-a06a-d12c2a3d181d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc4b1c43-9ed0-4396-a06a-d12c2a3d181d.png)'
- en: KMeans++ (Arthur's algorithm)
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KMeans++（亚瑟算法）
- en: The next improvement over standard KMeans is the KMeans++ proposed by David
    Arthur and Sergei Vassilvitskii in 2007\. Arthur's algorithm improves the initial
    Lloyd's KMeans by being more selective during the seeding process (the initial
    step).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对标准 KMeans 的下一个改进是 David Arthur 和 Sergei Vassilvitskii 于 2007 年提出的 KMeans++。亚瑟算法通过在种子过程（初始步骤）中更加挑剔来改进初始的
    Lloyd 的 KMeans。
- en: KMeans++, rather than picking random centres (random centroids) as starting
    points, picks the first centroid randomly and then picks the data points one by
    one and calculates `D(x)`. Then it chooses one more data point at random and,
    using proportional probability distribution `D(x)2`, it then keeps repeating the
    last two steps until all *K* numbers are picked. After the initial seeding, we
    finally run the KMeans or a variation with the newly seeded centroid. The KMeans++
    algorithm is guaranteed to find a solution in an *Omega= O(log k)* complexity.
    Even though the initial seeding takes extra steps, the accuracy improvements are
    substantial.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: KMeans++并非随机选择初始中心（随机质心），而是随机选取第一个质心，然后逐个选取数据点并计算`D(x)`。接着，它随机选择另一个数据点，并使用比例概率分布`D(x)2`，重复最后两个步骤，直到选出所有*K*个数。初始播种完成后，我们最终运行KMeans或其变体，使用新播种的质心。KMeans++算法保证在*Omega=
    O(log k)*复杂度内找到解决方案。尽管初始播种步骤较多，但准确性提升显著。
- en: KMeans|| (pronounced as KMeans Parallel)
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KMeans||（发音为KMeans Parallel）
- en: KMeans || is optimized to run in parallel and can result in one-two orders of
    magnitude improvement over Lloyd's original algorithm. The limitation of KMeans++
    is that it requires K-passes over the dataset, which can severely limit the performance
    and practicality of running KMeans with large or extreme datasets. Spark's KMeans||
    parallel implementation runs faster because it takes fewer passes (a lot less)
    over the data by sampling m points and oversampling in the process.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: KMeans || 经过优化，可并行运行，相较于Lloyd的原始算法，性能提升可达一到两个数量级。KMeans++的局限性在于它需要对数据集进行K次遍历，这在大规模或极端数据集上运行KMeans时会严重限制其性能和实用性。Spark的KMeans||并行实现运行更快，因为它通过采样m个点并在过程中进行过采样，减少了数据遍历次数（大幅减少）。
- en: 'The core of the algorithm and the math is depicted in the following figure:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的核心及数学原理在下图中展示：
- en: '![](img/e50796d1-a612-4089-b44f-1ffb5c933bab.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e50796d1-a612-4089-b44f-1ffb5c933bab.png)'
- en: In a nutshell, the highlight of the KMeans || (Parallel ...
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，KMeans ||（并行...）的亮点在于...
- en: There's more...
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: There is also a streaming version of KMeans implementation in Spark that allows
    you to classify the features on the fly.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中还有一个流式KMeans实现，允许您实时对特征进行分类。
- en: 'There is also a class that helps you to generate RDD data for KMeans. We found
    this to be very useful during our application development process:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个类帮助您生成KMeans的RDD数据。我们在应用程序开发过程中发现这非常有用：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This call uses Spark context to create RDDs while allowing you to specify the
    number of points, clusters, dimensions, and partitions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此调用使用Spark上下文创建RDD，同时允许您指定点数、簇数、维度和分区数。
- en: 'A useful related API is: `generateKMeansRDD()`. Documentation for `generateKMeansRDD`
    can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator%24) for
    generate an RDD containing test data for KMeans.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关的实用API是：`generateKMeansRDD()`。关于`generateKMeansRDD`的文档可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator%24)找到，用于生成供KMeans使用的测试数据RDD。
- en: See also
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'We need two pieces of objects to be able to write, measure, and manipulate
    the parameters of the KMeans || algorithm in Spark. The details of these two pieces
    of objects can be found at the following websites:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要两个对象来编写、测量和操作Spark中KMeans ||算法的参数。这两个对象的详细信息可以在以下网站找到：
- en: '`KMeans()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KMeans()`：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans)'
- en: '`KMeansModel()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KMeansModel()`：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel)'
- en: Bisecting KMeans, the new kid on the block in Spark 2.0
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bisecting KMeans，Spark 2.0中的新秀
- en: In this recipe, we will download the glass dataset and try to identify and label
    each glass using a bisecting KMeans algorithm. The Bisecting KMeans is a hierarchical
    version of the K-Mean algorithm implemented in Spark using the `BisectingKMeans()`
    API. While this algorithm is conceptually like KMeans, it can offer considerable
    speed for some use cases where the hierarchical path is present.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将下载玻璃数据集，并尝试使用Bisecting KMeans算法来识别和标记每种玻璃。Bisecting KMeans是K-Mean算法的层次化版本，在Spark中通过`BisectingKMeans()`API实现。虽然该算法在概念上类似于KMeans，但在存在层次路径的情况下，它可以为某些用例提供显著的速度优势。
- en: The dataset we used for this recipe is the Glass Identification Database. The
    study of the classification of types of glass was motivated by criminological
    research. Glass could be considered as evidence if it is correctly identified.
    The data can be found at NTU (Taiwan), already in LIBSVM format.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中使用的数据集是玻璃识别数据库。对玻璃类型分类的研究源于犯罪学研究。如果玻璃能被正确识别，它可能被视为证据。数据可在台湾大学（NTU）找到，已采用LIBSVM格式。
- en: How to do it...
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: We downloaded the prepared data file in LIBSVM from: [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale)
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从以下链接下载了LIBSVM格式的预处理数据文件：[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale)
- en: The dataset contains 11 features and 214 rows.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含11个特征和214行数据。
- en: The original dataset and data dictionary is also available at the UCI website: [http://archive.ics.uci.edu/ml/datasets/Glass+Identification](http://archive.ics.uci.edu/ml/datasets/Glass+Identification)
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始数据集及数据字典亦可在UCI网站上获取：[http://archive.ics.uci.edu/ml/datasets/Glass+Identification](http://archive.ics.uci.edu/ml/datasets/Glass+Identification)
- en: 'ID number: 1 to 214'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ID号：1至214
- en: 'RI: Refractive index'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'RI: 折射率'
- en: 'Na: Sodium (unit measurement: weight percent in corresponding oxide, as are
    attributes 4-10)'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Na: 钠（单位测量：相应氧化物中的重量百分比，属性4-10也是如此）'
- en: 'Mg: Magnesium'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mg: 镁'
- en: 'Al: Aluminum'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Al: 铝'
- en: 'Si: Silicon'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Si: 硅'
- en: 'K: Potassium'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'K: 钾'
- en: 'Ca: Calcium'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ca: 钙'
- en: 'Ba: Barium'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ba: 钡'
- en: 'Fe: Iron'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fe: 铁'
- en: 'Type of glass: Will find our class attributes or clusters using `BisectingKMeans()`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 玻璃类型：我们将使用`BisectingKMeans()`来寻找我们的类别属性或簇：
- en: '`building_windows_float_processed`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`building_windows_float_processed`'
- en: '`building_windows_non-_float_processed`'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`building_windows_non-_float_processed`'
- en: '`vehicle_windows_float_processed`'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vehicle_windows_float_processed`'
- en: How it works...
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this session, we explored the Bisecting KMeans model, which is new in Spark
    2.0\. We utilized the glass dataset in this session and tried to assign a glass
    type using `BisectingKMeans()`, but changed k to 6 so we have sufficient clusters.
    As usual, we loaded the data into a dataset with Spark's libsvm loading mechanism.
    We split the dataset randomly into 80% and 20%, with 80% used to train the model
    and 20% used for testing the model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了Spark 2.0中新引入的Bisecting KMeans模型。我们利用了玻璃数据集，并尝试使用`BisectingKMeans()`来指定玻璃类型，但将k值调整为6，以便拥有足够的簇。按照惯例，我们使用Spark的libsvm加载机制将数据加载到数据集中。我们将数据集随机分为80%和20%，其中80%用于训练模型，20%用于测试模型。
- en: We created the `BiSectingKmeans()` object and used the `fit(x)` function to
    create the model. We then used the `transform(x)` function for the testing dataset
    to explore the model prediction and printed out the result in the console output.
    We also output the cost of computing the clusters (sum of error squared) and then
    displayed the cluster centers. Finally, we printed the features with their assigned
    cluster number and stop operation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了`BiSectingKmeans()`对象，并使用`fit(x)`函数来构建模型。随后，我们使用`transform(x)`函数对测试数据集进行模型预测，并在控制台输出结果。我们还输出了计算簇的成本（误差平方和），并展示了簇中心。最后，我们打印了特征及其分配的簇编号，并停止操作。
- en: 'Approaches to hierarchical clustering include:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类的方法包括：
- en: '**Divisive**: Top down approach (Apache Spark implementation)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分割型**：自上而下的方法（Apache Spark实现）'
- en: '**Agglomerative**: Bottom up approach'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合型**：自下而上的方法'
- en: There's more...
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'More about the Bisecting KMeans can be found at:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Bisecting KMeans的更多信息，请访问：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans)'
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel)'
- en: We use clustering to explore the data and get a feel for what the outcome looks
    like as clusters. The bisecting KMeans is an interesting case of hierarchical
    analysis versus KMeans clustering.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用聚类来探索数据，并对聚类结果的外观有所了解。二分K均值是层次分析与K均值聚类的一个有趣案例。
- en: The best way to conceptualize it is to think of bisecting KMeans as a recursive
    hierarchical KMeans. The bisecting KMeans algorithm divides the data using similarity
    measurement techniques like KMeans but uses a hierarchical scheme to increase
    accuracy. It is particularly prevalent ...
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的理解方式是将二分K均值视为递归层次的K均值。二分K均值算法通过类似K均值的相似度测量技术分割数据，但采用层次结构以提高准确性。它在...中尤为普遍...
- en: See also
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'There are two approaches to implementing hierarchical clustering--Spark uses
    a recursive top-down approach in which a cluster is chosen and then splits are
    performed in the algorithm as it moves down the hierarchy:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 实现层次聚类有两种方法——Spark采用递归自顶向下的方法，在其中选择一个簇，然后在算法向下移动层次时执行分割：
- en: Details about the hierarchical clustering approach can be found at [https://en.wikipedia.org/wiki/Hierarchical_clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于层次聚类方法的详细信息可在[https://en.wikipedia.org/wiki/Hierarchical_clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)找到
- en: Spark 2.0 documentation for Bisecting K-Mean can be found at [http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means](http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0关于二分K-均值的文档可在[http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means](http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means)找到
- en: A paper describing how to use Bisecting KMeans to classify web logs can be found
    at [http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf](http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一篇描述如何使用二分K均值对网络日志进行分类的论文可在[http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf](http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf)找到
- en: Using Gaussian Mixture and Expectation Maximization (EM) in Spark to classify
    data
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark中使用高斯混合和期望最大化（EM）进行数据分类
- en: In this recipe, we will explore Spark's implementation of **expectation maximization**
    (**EM**) `GaussianMixture()`*,* which calculates the maximum likelihood given
    a set of features as input. It assumes a Gaussian mixture in which each point
    can be sampled from K number of sub-distributions (cluster memberships).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将探讨Spark对**期望最大化**（**EM**）的实现`GaussianMixture()`，它计算给定一组特征输入的最大似然。它假设每个点可以从K个子分布（簇成员）中采样的高斯混合。
- en: How to do it...
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作方法...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Import the necessary packages for vector and matrix manipulation:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入用于向量和矩阵操作的必要包：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create Spark''s session object:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的会话对象：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let us take a look at the dataset and examine the input file. The Simulated
    SOCR Knee Pain Centroid Location Data represents the centroid location for the
    hypothetical knee-pain locations for 1,000 subjects. The data includes the X and
    Y coordinates of the centroids.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们查看数据集并检查输入文件。模拟的SOCR膝痛质心位置数据代表了1000名受试者假设的膝痛位置的质心位置。数据包括质心的X和Y坐标。
- en: This dataset can be used to illustrate the Gaussian Mixture and Expectation
    Maximization. The data is available at: [http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集可用于说明高斯混合和期望最大化。数据可在[http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409)获取
- en: 'The sample data looks like the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 样本数据如下所示：
- en: '**X**: The *x* coordinate of the centroid location for one subject and one
    view.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X**：一个受试者和一个视图的质心位置的*x*坐标。'
- en: '**Y**: The *y* coordinate of the centroid location for one subject and one
    view.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Y**：一个受试者和一个视图的质心位置的*y*坐标。'
- en: X, Y
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: X, Y
- en: 11 73
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 11 73
- en: 20 88
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 20 88
- en: 19 73
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 19 73
- en: 15 65
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 15 65
- en: 21 57
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 21 57
- en: 26 101
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 26 101
- en: 24 117
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 24 117
- en: 35 106
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 35 106
- en: 37 96
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 37 96
- en: 35 147
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 35 147
- en: 41 151
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 41 151
- en: 42 137
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 42 137
- en: 43 127
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 43 127
- en: 41 206
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 41 206
- en: 47 213
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 47 213
- en: 49 238
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 49 238
- en: 40 229
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 40 229
- en: 'The following figure depicts a knee-pain map based on the SOCR dataset from
    `wiki.stat.ucla`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 下图基于`wiki.stat.ucla`的SOCR数据集描绘了一个膝痛地图：
- en: '![](img/18ed269a-1079-4fd8-a2c0-6a75e4e93fcb.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18ed269a-1079-4fd8-a2c0-6a75e4e93fcb.png)'
- en: We place the data file in a data directory (you can copy the data file to any
    location you prefer).
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据文件放置在一个数据目录中（您可以将数据文件复制到您喜欢的任何位置）。
- en: 'The data file contains 8,666 entries:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数据文件包含8,666条记录：
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then load the data file into RDD:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们将数据文件加载到RDD中：
- en: '[PRE9]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We now create a GaussianMixture model and set the parameters for the model.
    We set the K value to 4, since the data was collected by four views: **Left Front**
    (**LF**), **Left Back** (**LB**), **Right Front** (**RF**), and **Right Back**
    (**RB**). We set the convergence to the default value of 0.01, and the maximum
    iteration counts to 100:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们创建一个高斯混合模型并设置模型参数。我们将K值设为4，因为数据是通过四个视角收集的：**左前**（**LF**）、**左后**（**LB**）、**右前**（**RF**）和**右后**（**RB**）。我们将收敛值设为默认值0.01，最大迭代次数设为100：
- en: '[PRE10]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We run the model algorithm:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们运行模型算法：
- en: '[PRE11]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We print out the key values for the GaussianMixture model after the training:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练后，我们打印出高斯混合模型的关键值：
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Since we set the K value to 4, we will have four sets of values printed out
    in the console logger:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们将K值设为4，因此控制台记录器将打印出四组值：
- en: '![](img/bc6b63fe-5651-462d-8966-636bec6ba119.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc6b63fe-5651-462d-8966-636bec6ba119.png)'
- en: 'We also print out the first 50 cluster-labels based on the GaussianMixture
    model predictions:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还根据高斯混合模型预测打印出前50个聚类标签：
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The sample output in the console will show the following:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制台中的样本输出将显示以下内容：
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We then close the program by stopping the Spark context:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过停止Spark上下文来关闭程序：
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: How it works...
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In the previous recipe, we observed that KMeans can discover and allocate membership
    to one and only one cluster based on an iterative method using similarity (Euclidian,
    and so on). One can think of KMeans as a specialized version of a Gaussian mixture
    model with EM models in which a discrete (hard) membership is enforced.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个示例中，我们观察到KMeans能够发现并基于迭代方法（如欧几里得距离等）将成员分配到一个且仅一个集群。可以将KMeans视为高斯混合模型中EM模型的专用版本，其中强制执行离散（硬）成员资格。
- en: 'But there are cases that have overlap, which is often the case in medicine
    or signal processing, as depicted in the following figure:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 但存在重叠情况，这在医学或信号处理中很常见，如下图所示：
- en: '![](img/86a123c5-915b-40b8-ae00-55fac3375b57.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86a123c5-915b-40b8-ae00-55fac3375b57.png)'
- en: In such cases, we need a probability density function that can express the membership
    in each sub-distribution. The Gaussian Mixture models with **Expectation Maximization**
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要一个能够表达每个子分布中成员资格的概率密度函数。采用**期望最大化**算法的高斯混合模型
- en: New GaussianMixture()
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新建GaussianMixture()
- en: 'This constructs a default instance. The default parameters that control the
    behavior of the model are:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这构建了一个默认实例。控制模型行为的默认参数如下：
- en: '![](img/46e18b3f-0a38-44c0-bebf-24f741ebe8f6.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46e18b3f-0a38-44c0-bebf-24f741ebe8f6.png)'
- en: The Gaussian Mixture models with Expectation Maximization are a form of soft
    clustering in which a membership can be inferred using a log maximum likelihood
    function. In this scenario, a probability density function with mean and covariance
    is used to define the membership or likelihood of a membership to K number of
    clusters. It is flexible in the sense that the membership is not quantified which
    allows for overlapping membership based on probability (indexed to multiple sub-distributions).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 采用期望最大化算法的**高斯混合模型**是一种软聚类形式，其中可以通过对数最大似然函数推断出成员资格。在此情况下，使用具有均值和协方差的概率密度函数来定义属于K个集群的成员资格或似然性。其灵活性在于，成员资格未量化，这允许基于概率（索引到多个子分布）的成员资格重叠。
- en: 'The following figure is a snapshot of the EM algorithm:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是EM算法的一个快照：
- en: '![](img/1eac50f4-acad-4b14-a479-aa390fe87a9b.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1eac50f4-acad-4b14-a479-aa390fe87a9b.png)'
- en: 'Here are the steps to the EM algorithm:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是EM算法的步骤：
- en: Assume *N* number of Gaussian distribution.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设有*N*个高斯分布。
- en: 'Iterate until we have convergence:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代直至达到收敛：
- en: For each point Z drawn with conditional probability of being drawn from distribution
    Xi written as *P (Z | Xi)*
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个点Z，其条件概率为从分布Xi中抽取，记作*P(Z | Xi)*
- en: Adjust the parameter's mean and variance so that they fit the points that are
    assigned to the sub-distribution
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整参数的均值和方差，使其适合分配给子分布的点
- en: 'For a more mathematical explanation, including detailed work on maximum likelihood,
    see the following link: [http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf](http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更数学化的解释，包括关于最大似然的详细工作，请参阅以下链接：[http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf](http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf)
- en: There's more...
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The following figure provides a quick reference point to highlight some of
    the differences between hard versus soft clustering:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 下图提供了一个快速参考点，以突出硬聚类与软聚类之间的一些差异：
- en: '![](img/f1c9d196-7b32-40ea-9303-66fa819f52f7.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1c9d196-7b32-40ea-9303-66fa819f52f7.png)'
- en: See also
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for constructor GaussianMixture can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造器GaussianMixture的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture)找到
- en: Documentation for constructor GaussianMixtureModel can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造器GaussianMixtureModel的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel)找到
- en: Classifying the vertices of a graph using Power Iteration Clustering (PIC) in
    Spark 2.0
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用幂迭代聚类（PIC）对图的顶点进行分类
- en: This is a classification method for the vertices of a graph given their similarities
    as defined by their edges. It uses the GraphX library which is ships out of the
    box with Spark to implement the algorithm. Power Iteration Clustering is similar
    to other Eigen Vector/Eigen Value decomposition algorithms but without the overhead
    of matrix decomposition. It is suitable when you have a large sparse matrix (for
    example, graphs depicted as a sparse matrix).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种基于顶点相似性（由边定义）对图的顶点进行分类的方法。它使用随Spark一起提供的GraphX库来实现算法。幂迭代聚类类似于其他特征向量/特征值分解算法，但没有矩阵分解的开销。当您有一个大型稀疏矩阵（例如，以稀疏矩阵表示的图）时，它很适用。
- en: GraphFrames will be the replacement/interface proper for the GraphX library
    going forward ([https://databricks.com/blog/2016/03/03/introducing-graphframes.html](https://databricks.com/blog/2016/03/03/introducing-graphframes.html)).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，GraphFrames将成为GraphX库的替代/接口（[https://databricks.com/blog/2016/03/03/introducing-graphframes.html](https://databricks.com/blog/2016/03/03/introducing-graphframes.html)）。
- en: How to do it...
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE16]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Spark上下文导入必要的包以访问集群，并导入`Log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE17]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Set up the logger level to ERROR only to reduce the output:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志级别设置为ERROR，仅以减少输出：
- en: '[PRE18]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Create Spark''s configuration and SQL context so we can have access to the
    cluster and be able to create and use a DataFrame as needed:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark配置和SQL上下文，以便我们可以访问集群并能够根据需要创建和使用DataFrame：
- en: '[PRE19]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We create a training dataset with a list of datasets and use the Spark `sparkContext.parallelize()`
    function to create Spark RDD:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用Spark的`sparkContext.parallelize()`函数创建包含一系列数据集的训练数据集，并创建Spark RDD：
- en: '[PRE20]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We create a `PowerIterationClustering` object and set the parameters. We set
    the `K` value to `3` and max iteration count to `15`:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个`PowerIterationClustering`对象并设置参数。我们将`K`值设置为`3`，最大迭代次数设置为`15`：
- en: '[PRE21]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We then let the model run:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后让模型运行：
- en: '[PRE22]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We print out the cluster assignment based on the model for the training data:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据模型打印出训练数据的集群分配情况：
- en: '[PRE23]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The console output will show the following information:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制台输出将显示以下信息：
- en: '![](img/1fbb5236-33c4-443b-b46c-c506f0c66782.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fbb5236-33c4-443b-b46c-c506f0c66782.png)'
- en: 'We also print out the model assignment data in a collection for each cluster:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还为每个聚类在集合中打印出模型分配数据：
- en: '[PRE24]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The console output will display the following information (in total, we have
    three clusters which were set in the preceding parameters):'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制台输出将显示以下信息（总共，我们在前面的参数中设置了三个聚类）：
- en: '[PRE25]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We then close the program by stopping the Spark context:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过停止Spark上下文来关闭程序：
- en: '[PRE26]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How it works...
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其工作原理...
- en: 'We created a list of edges and vertices for a graph and then proceeded to create
    the object and set the parameters:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个图的边和顶点列表，然后继续创建对象并设置参数：
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The next step was the model of training data:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是训练数据模型：
- en: '[PRE28]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The clusters were then outputted for inspection. The code near the end prints
    out the model assignment data in a collection for each cluster using Spark transformation
    operators.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后输出聚类以供检查。代码末尾附近的代码使用Spark转换运算符在集合中为每个聚类打印出模型分配数据。
- en: At the core **PIC** (**Power Iteration Clustering**) is an eigenvalue class
    algorithm which avoids matrix decomposition by producing an Eigen Value plus an
    Eigen Vector to satisfy *Av* = λ*v.* Because PIC avoids the decomposition of the
    matrix A, it is suitable when the input matrix A (describing a graph in ...
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**PIC**（**幂迭代聚类**）的核心是一种避免矩阵分解的特征值类算法，它通过生成一个特征值加上一个特征向量来满足*Av* = λ*v*。由于PIC避免了矩阵A的分解，因此它适用于输入矩阵A（描述图...'
- en: There's more...
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: For a more detailed mathematical treatment of the subject (power iteration),
    see the following white paper from Carnegie Mellon University: [http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf](http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如需对主题（幂迭代）进行更详细的数学处理，请参阅卡内基梅隆大学提供的以下白皮书：[http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf](http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf)
- en: See also
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for the constructor `PowerIterationClustering()` can be found
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数`PowerIterationClustering()`的文档可以在以下位置找到：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering)
- en: Documentation for the constructor `PowerIterationClusteringModel()` can be found
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数`PowerIterationClusteringModel()`的文档可以在以下位置找到：[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel)
- en: Latent Dirichlet Allocation (LDA) to classify documents and text into topics
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用潜在狄利克雷分配（LDA）对文档和文本进行主题分类
- en: In this recipe, we will explore the **Latent Dirichlet Allocation** (**LDA**)
    algorithm in Spark 2.0\. The LDA we use in this recipe is completely different
    from linear discrimination analysis. Both Latent Dirichlet Allocation and linear
    discrimination analysis are referred to as LDA, but they are extremely different
    techniques. In this recipe, when we use the LDA, we refer to Latent Dirichlet
    Allocation. The chapter on text analytics is also relevant to understanding the
    LDA.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将探讨Spark 2.0中的**潜在狄利克雷分配**（**LDA**）算法。本食谱中使用的LDA与线性判别分析完全不同。潜在狄利克雷分配和线性判别分析都称为LDA，但它们是截然不同的技术。在本食谱中，当我们使用LDA时，我们指的是潜在狄利克雷分配。关于文本分析的章节也与理解LDA相关。
- en: LDA is often used in natural language processing which tries to classify a large
    body of the document (for example, emails from the Enron fraud case) into a discrete
    number of topics or themes so it can be understood. LDA is also a good candidate
    for selecting articles based on one's interest (for example, as you turn a page
    and spend time on a specific topic) in a given magazine article or page.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: LDA常用于自然语言处理，试图将大量文档（例如安然欺诈案中的电子邮件）分类为有限数量的主题或主题，以便于理解。LDA也是根据个人兴趣选择文章的良好候选方法（例如，当你翻页并花时间在特定主题上时），在给定的杂志文章或页面上。
- en: How to do it...
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE29]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Import the necessary packages:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE30]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We set up the necessary Spark Session to gain access to the cluster:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置必要的Spark会话以访问集群：
- en: '[PRE31]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We have a sample LDA dataset, which is located at the following relative path
    (you can use an absolute path). The sample file is provided with any Spark distribution
    and ...
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有一个LDA样本数据集，位于以下相对路径（您也可以使用绝对路径）。该样本文件随任何Spark发行版提供，并且...
- en: How it works...
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: LDA assumes that the document is a mixture of different topics with Dirichlet
    prior distribution. The words in the document are assumed to have an affinity
    towards a specific topic which allows LDA to classify the overall document (compose
    and assign a distribution) that best matches a topic.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: LDA假设文档是具有Dirichlet先验分布的不同主题的混合体。文档中的单词被认为对特定主题有亲和力，这使得LDA能够对整体文档（构成并分配分布）进行分类，以最佳匹配主题。
- en: A topic model is a generative latent model for discovering abstract themes (topics)
    that occur in the body of documents (often too large for humans to handle). The
    models are a pre-cursor to summarize, search, and browse a large set of unlabeled
    documents and their contents. Generally speaking, we are trying to find a cluster
    of features (words, sub-images, and so on) that occur together.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型是一种生成潜在模型，用于发现文档主体中出现的抽象主题（主题）（通常对于人类来说太大而无法处理）。这些模型是总结、搜索和浏览大量未标记文档及其内容的先驱。一般来说，我们试图找到一起出现的特征（单词、子图像等）的集群。
- en: 'The following figure depicts the overall LDA scheme:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描绘了LDA的整体方案：
- en: Please be sure to refer to the white paper cited here for completeness [http://ai.stanford.edu/~ang/papers/nips01-lda.pdf](http://ai.stanford.edu/~ang/papers/nips01-lda.pdf)
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，请务必参考此处引用的白皮书：[http://ai.stanford.edu/~ang/papers/nips01-lda.pdf](http://ai.stanford.edu/~ang/papers/nips01-lda.pdf)
- en: '![](img/a85bbb8a-37a8-4880-ac6f-e8ede49bdb67.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a85bbb8a-37a8-4880-ac6f-e8ede49bdb67.png)'
- en: 'The steps for the LDA algorithm are as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: LDA算法的步骤如下：
- en: 'Initialize the following parameters (controls concentration and smoothing):'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化以下参数（控制集中度和平滑度）：
- en: Alpha parameter (high alpha makes documents more similar to each other and contain
    similar topics )
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alpha参数（高alpha值使得文档间更为相似，且包含相似的主题）
- en: Beta parameter ( high beta means each topic is most likely to contain a mix
    of most of the words)
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Beta参数（高beta值意味着每个主题最可能包含大多数单词的混合）
- en: Randomly initialize the topic assignment.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化主题分配。
- en: 'Iterate:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代：
- en: For each document.
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个文档。
- en: For each word in the document.
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于文档中的每个单词。
- en: Resample the topic for each word.
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个单词重新采样主题。
- en: With respect to all other words and their current assignment (for the current
    iteration).
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相对于所有其他单词及其当前分配（对于当前迭代）。
- en: Get the result.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取结果。
- en: Model evaluation
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型评估
- en: In statistics, Dirichlet distribution Dir(alpha) is a family of continuous multivariate
    probability distributions parameterized by a vector α of positive real numbers.
    For a more in-depth treatment of LDA, see the original paper in the
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，Dirichlet分布Dir(alpha)是一族由正实数向量α参数化的连续多元概率分布。关于LDA的更深入探讨，请参阅原始论文：
- en: Journal of Machine Learning at [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习杂志上的原论文链接：[http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
- en: The LDA does not assign any semantics to a topic and does not care what the
    topics are called. It is only a generative model that uses the distribution of
    fine-grained items (for example, words about cats, dogs, fish, cars) to assign
    an overall topic that scores the best. It does not know, cares, or understand
    about topics called dogs or cats.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: LDA不对主题赋予任何语义，也不关心主题的名称。它只是一个生成模型，使用细粒度项（例如，关于猫、狗、鱼、汽车的单词）的分布来分配总体主题，该主题得分最高。它不知道、不关心，也不理解被称为狗或猫的主题。
- en: We often have to tokenize and vectorize the document via TF-IDF prior to input
    to an LDA algorithm.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常需要通过TF-IDF对文档进行分词和向量化，然后才能输入到LDA算法中。
- en: There's more...
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The following figure depicts the LDA in a nutshell:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 下图简要描绘了LDA：
- en: '![](img/f3315b50-1445-4633-a0d5-ffaf8256afd5.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f3315b50-1445-4633-a0d5-ffaf8256afd5.png)'
- en: 'There are two approaches to document analysis. We can simply use matrix factorization
    to decompose a large matrix of datasets to a smaller matrix (topic assignments)
    times a vector (topics themselves):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 文档分析有两种方法。我们可以简单地使用矩阵分解将大型数据集矩阵分解为较小的矩阵（主题分配）乘以向量（主题本身）：
- en: '![](img/1f6891f4-b0ce-476a-bda5-d52d10745a60.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f6891f4-b0ce-476a-bda5-d52d10745a60.png)'
- en: See also
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '**LDA**: documentation for a constructor can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LDA**：构造函数的文档可在 [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)'
- en: '[**LDAModel**: documentation for a constructor can be found at ](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LDAModel**：构造函数的文档可在 ](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)'
- en: 'See also, via Spark''s Scala API, documentation links for the following:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 另请参阅，通过Spark的Scala API，以下文档链接：
- en: DistributedLDAModel
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DistributedLDAModel
- en: EMLDAOptimizer
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMLDAOptimizer
- en: LDAOptimizer
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDAOptimizer
- en: LocalLDAModel
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LocalLDAModel
- en: OnlineLDAOptimizer
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OnlineLDAOptimizer
- en: Streaming KMeans to classify data in near real-time
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式KMeans用于近实时分类数据
- en: Spark streaming is a powerful facility which lets you combine near real-time
    and batch in the same paradigm. The streaming KMeans interface lives at the intersection
    of ML clustering and Spark streaming, and takes full advantage of the core facilities
    provided by Spark streaming itself (for example, fault tolerance, exactly once
    delivery semantics, and so on).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Spark流式处理是一个强大的功能，它允许您在同一范式中结合近实时和批处理。流式KMeans接口位于ML聚类和Spark流式处理的交叉点，充分利用了Spark流式处理本身提供的核心功能（例如，容错、精确一次交付语义等）。
- en: How to do it...
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Import the necessary packages for streaming KMeans:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入流式KMeans所需的包：
- en: '`package spark.ml.cookbook.chapter14`.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`package spark.ml.cookbook.chapter14`.'
- en: 'Import the necessary packages for streaming KMeans:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入流式KMeans所需的包：
- en: '[PRE32]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We set up the following parameters for the streaming KMeans program. The training
    directory will be the directory to send the training data file. The KMeans clustering
    model utilizes the training data to run algorithms and calculations. The `testDirectory`
    will be the test data for predictions. The `batchDuration` is a number in seconds
    for a batch run. In the following case, the program will check every 10 seconds
    to see if there is any new data files for recalculations.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为流式KMeans程序设置了以下参数。训练目录将是发送训练数据文件的目录。KMeans聚类模型利用训练数据运行算法和计算。`testDirectory`将用于预测的测试数据。`batchDuration`是以秒为单位的批处理运行时间。在以下情况下，程序将每10秒检查一次是否有新的数据文件用于重新计算。
- en: 'The cluster is set to `2`, and the data dimensions will be `3`:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群设置为`2`，数据维度将为`3`：
- en: '[PRE33]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'With the preceding settings, the sample training data will contain data like
    the following (in the format of [*X[1], X[2], ...X[n]*], where *n* is `numDimensions`:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上述设置，示例训练数据将包含以下格式的数据（以[*X[1], X[2], ...X[n]*]格式，其中*n*是`numDimensions`）：
- en: '[0.0,0.0,0.0]'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[0.0,0.0,0.0]'
- en: '[0.1,0.1,0.1]'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[0.1,0.1,0.1]'
- en: '[0.2,0.2,0.2]'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[0.2,0.2,0.2]'
- en: '[9.0,9.0,9.0]'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[9.0,9.0,9.0]'
- en: '[9.1,9.1,9.1]'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[9.1,9.1,9.1]'
- en: '[9.2,9.2,9.2]'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[9.2,9.2,9.2]'
- en: '[0.1,0.0,0.0]'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[0.1,0.0,0.0]'
- en: '[0.2,0.1,0.1]'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[0.2,0.1,0.1]'
- en: '....'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '....'
- en: 'The test data file will contain data like the following (in the format of (*y,
    [X1, X2, .. Xn]*), where *n* is `numDimensions` and `y` is an identifier):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据文件将包含以下格式的数据（以（*y, [X1, X2, .. Xn]*）格式，其中*n*是`numDimensions`，`y`是标识符）：
- en: (7,[0.4,0.4,0.4])
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: (7,[0.4,0.4,0.4])
- en: (8,[0.1,0.1,0.1])
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: (8,[0.1,0.1,0.1])
- en: (9,[0.2,0.2,0.2])
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: (9,[0.2,0.2,0.2])
- en: (10,[1.1,1.0,1.0])
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: (10,[1.1,1.0,1.0])
- en: (11,[9.2,9.1,9.2])
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: (11,[9.2,9.1,9.2])
- en: (12,[9.3,9.2,9.3])
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: (12,[9.3,9.2,9.3])
- en: 'We set up the necessary Spark context to gain access to the cluster:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置必要的Spark上下文以访问集群：
- en: '[PRE34]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Define the streaming context and micro-batch window:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义流式上下文和微批处理窗口：
- en: '[PRE35]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following code will create data by parsing the data file in the preceding
    two directories into `trainingData` and `testData RDDs`:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码将通过解析上述两个目录中的数据文件创建`trainingData`和`testData RDDs`：
- en: '[PRE36]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We create the `StreamingKMeans` model and set the parameters:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建`StreamingKMeans`模型并设置参数：
- en: '[PRE37]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The program will train the model using the training dataset and predict using
    the test dataset:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序将使用训练数据集训练模型，并使用测试数据集进行预测：
- en: '[PRE38]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We start the streaming context, and the program will run the batch every 10
    seconds to see if a new dataset is available for training and if there is any
    new test dataset for prediction. The program will exit if a termination signal
    is received (exit the batch running):'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们启动流式上下文，程序将每10秒运行一次批处理，以检查是否有新的训练数据集可用，以及是否有新的测试数据集用于预测。如果收到终止信号（退出批处理运行），程序将退出。
- en: '[PRE39]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We copy the `testKStreaming1.txt` data file into the preceding `testDir` set
    and see the following printed out in the console logs:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`testKStreaming1.txt`数据文件复制到上述`testDir`设置中，并在控制台日志中看到以下打印输出：
- en: '![](img/d39d25e2-be84-46b3-917a-52c3907e3122.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d39d25e2-be84-46b3-917a-52c3907e3122.png)'
- en: For a Windows machine, we copied the `testKStreaming1.txt` file into the directory: `C:\spark-2.0.0-bin-hadoop2.7\data\sparkml2\chapter8\testDir\`.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于Windows机器，我们将`testKStreaming1.txt`文件复制到了目录：`C:\spark-2.0.0-bin-hadoop2.7\data\sparkml2\chapter8\testDir\`。
- en: We can also check the SparkUI for more information: `http://localhost:4040/`.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以通过访问`http://localhost:4040/`来检查SparkUI以获取更多信息。
- en: 'The job panel will display streaming jobs, as shown in the following figure:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 作业面板将显示流式作业，如图所示：
- en: '![](img/6ef1d102-cb06-48cd-b26c-9f71ac6bbb9d.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ef1d102-cb06-48cd-b26c-9f71ac6bbb9d.png)'
- en: 'As shown in the following figure, the streaming panel will show the preceding
    Streaming KMeans matrix as the matrix displayed, the batch job running every 10
    seconds in this case:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，流式面板将显示上述流式KMeans矩阵，显示批处理作业每10秒运行一次：
- en: '![](img/b14bd318-6028-4597-bf45-ca1e5a3dcd9d.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b14bd318-6028-4597-bf45-ca1e5a3dcd9d.png)'
- en: 'You can get more details on the streaming batch by clicking on any of the batches,
    as shown in the following figure:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过点击任何批处理，如图所示，获取有关流式批处理的更多详细信息：
- en: '![](img/1a690bac-440b-4891-a843-1270709933a4.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a690bac-440b-4891-a843-1270709933a4.png)'
- en: How it works...
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In certain situations, we cannot use batch methods to load and capture the events
    and then react to them. We can use creative methods of capturing events in the
    memory or a landing DB and then rapidly marshal that over to another system for
    processing, but most of these systems fail to act as streaming systems and often
    are very expensive to build.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们不能使用批处理方法来加载和捕获事件，然后对其做出反应。我们可以使用在内存或着陆数据库中捕获事件的创造性方法，然后快速将其转移到另一个系统进行处理，但大多数这些系统无法作为流式系统运行，并且通常构建成本非常高。
- en: Spark provides a near real-time (also referred to as subjective real time) that
    can receive incoming sources, such as Twitter feeds, signals, and so, on via connectors
    (for example, a Kafka connector) and then process and present them as an RDD interface.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了一种近乎实时的（也称为主观实时）方式，可以接收来自Twitter feeds、信号等的传入源，通过连接器（例如Kafka连接器）进行处理，并以RDD接口的形式呈现。
- en: 'These are the elements needed to build and construct streaming KMeans in Spark:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是构建和构造Spark中流式KMeans所需的元素：
- en: Use the streaming context as opposed to the ...
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用流式上下文而不是...
- en: There's more...
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Streaming KMeans are special cases of KMeans implementation in which the data
    can arrive at a near real-time and be classified into a cluster (hard classification)
    as needed. For a reference to Voronoi diagrams, see the following URL: [https://en.wikipedia.org/wiki/Voronoi_diagram](https://en.wikipedia.org/wiki/Voronoi_diagram)
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 流式KMeans是KMeans实现的一种特殊情况，其中数据可以近乎实时地到达，并根据需要被分类到集群（硬分类）中。关于Voronoi图的参考，请参见以下URL：[https://en.wikipedia.org/wiki/Voronoi_diagram](https://en.wikipedia.org/wiki/Voronoi_diagram)
- en: 'Currently, there are other algorithms besides streaming KMeans in the Spark
    Machine Library, as shown in the following figure:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Spark机器学习库中除了流式KMeans外还有其他算法，如图所示：
- en: '![](img/7e339c28-5370-47c9-bd81-086eb04f7ca0.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e339c28-5370-47c9-bd81-086eb04f7ca0.png)'
- en: See also
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for Streaming KMeans can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式KMeans文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)找到。
- en: Documentation for Streaming KMeans Model can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest)
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式KMeans模型文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest)找到。
- en: Documentation for Streaming Test--very useful for data generation--can be found
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式测试文档——对数据生成非常有用——可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)找到。
