- en: '*Chapter 4*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*'
- en: Dimensionality Reduction and Unsupervised Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维和无监督学习
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将能够：
- en: Compare hierarchical cluster analysis (HCA) and k-means clustering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较层次聚类分析（HCA）和k-means聚类
- en: Conduct an HCA and interpret the output
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行层次聚类分析（HCA）并解读输出结果
- en: Tune a number of clusters for k-means clustering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整k-means聚类的聚类数
- en: Select an optimal number of principal components for dimension reduction
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个最佳的主成分数进行降维
- en: Perform supervised dimension compression using linear discriminant function
    analysis (LDA)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用线性判别分析（LDA）进行监督式降维
- en: This chapter will cover various concepts that fall under dimensionality reduction
    and unsupervised learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍降维和无监督学习下的各种概念。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In unsupervised learning, **descriptive models** are used for exploratory analysis
    to uncover patterns in unlabeled data. Examples of unsupervised learning tasks
    include algorithms for **clustering** and those for **dimension reduction**. In
    clustering, observations are assigned to groups in which there is high within-group
    homogeneity and between-group heterogeneity. Simply put, observations are placed
    into clusters of samples with other observations that are very similar. Use cases
    for clustering algorithms are vast. For example, analysts seeking to elevate sales
    by targeting selected customers for marketing advertisements and promotions separate
    customers by their shopping behavior.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，**描述性模型**用于探索性分析，以揭示未标记数据中的模式。无监督学习任务的例子包括**聚类**算法和**降维**算法。在聚类中，观察值被分配到组中，其中组内同质性高而组间异质性大。简而言之，观察值被归类到与其他非常相似的观察值的样本群体中。聚类算法的应用场景广泛。例如，分析师通过根据顾客的购物行为将顾客分开，进而为特定顾客群体定向营销广告和促销活动，从而提高销售额。
- en: Note
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Additionally, hierarchical clustering has been implemented in academic neuroscience
    and motor behavior research (https://www.researchgate.net/profile/Ming-Yang_Cheng/project/The-Effect-of-SMR-Neurofeedback-Training-on-Mental-Representation-and-Golf-Putting-Performance/attachment/57c8419808aeef0362ac36a5/AS:401522300080128@1472741784217/download/Schack+-+2012+-+Measuring+mental+representations.pdf?context=ProjectUpdatesLog)
    and k-means clustering has been used in fraud detection (https://www.semanticscholar.org/paper/Fraud-Detection-in-Credit-Card-by-Clustering-Tech/3e98a9ac78b5b89944720c2b428ebf3e46d9950f).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，层次聚类已被应用于学术神经科学和运动行为研究（https://www.researchgate.net/profile/Ming-Yang_Cheng/project/The-Effect-of-SMR-Neurofeedback-Training-on-Mental-Representation-and-Golf-Putting-Performance/attachment/57c8419808aeef0362ac36a5/AS:401522300080128@1472741784217/download/Schack+-+2012+-+Measuring+mental+representations.pdf?context=ProjectUpdatesLog），而k-means聚类已被用于欺诈检测（https://www.semanticscholar.org/paper/Fraud-Detection-in-Credit-Card-by-Clustering-Tech/3e98a9ac78b5b89944720c2b428ebf3e46d9950f）。
- en: However, when building descriptive or predictive models, it can be a challenge
    to determine which features to include in a model to improve it, and which features
    to exclude because they diminish a model. Too many features can be troublesome
    because the greater the number of variables in a model, the higher the probability
    of multicollinearity and subsequent overfitting of a model. Additionally, numerous
    features expand the complexity of a model and increase the time for model tuning
    and fitting.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在构建描述性或预测性模型时，确定哪些特征应纳入模型以提高模型效果，以及哪些特征应排除因为它们会削弱模型，可能是一个挑战。特征过多可能会导致问题，因为模型中变量的数量越多，多重共线性和模型过拟合的可能性就越高。此外，过多的特征增加了模型的复杂性，并延长了模型调优和拟合的时间。
- en: This becomes troublesome with larger datasets. Fortunately, another use case
    for unsupervised learning is to reduce the number of features in a dataset by
    creating combinations of the original features. Reducing the number of features
    in data helps eliminate multicollinearity and converges on a combination of features
    to best produce a model that performs well on unseen test data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集变大时，这会变得更加棘手。幸运的是，另一种无监督学习的应用场景是通过创建原始特征的组合来减少数据集中的特征数量。减少数据中的特征数量有助于消除多重共线性，并汇聚出一组特征组合，从而生成一个在未见测试数据上表现良好的模型。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Multicollinearity is a situation in which at least two variables are correlated.
    It is a problem in linear regression models because it does not allow the isolation
    of the relationship between each independent variable and the outcome measure.
    Thus, coefficients and p-values become unstable and less precise.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 多重共线性是指至少有两个变量之间存在相关性。这在回归模型中是一个问题，因为它无法孤立地描述每个独立变量与结果变量之间的关系。因此，系数和p值变得不稳定，且精确度较低。
- en: 'In this chapter, we will be covering two widely used unsupervised clustering
    algorithms: *Hierarchical Cluster Analysis (HCA)* and *k-means clustering*. Additionally,
    we will explore dimension reduction using *principal component analysis (PCA)*
    and observe how reducing dimensionality can improve model performance. Lastly,
    we will implement linear discriminant function analysis *(LDA)* for supervised
    dimensionality reduction.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍两种广泛使用的无监督聚类算法：*层次聚类分析（HCA）*和*k均值聚类*。此外，我们还将探索使用*主成分分析（PCA）*进行降维，并观察降维如何改善模型性能。最后，我们将实现线性判别函数分析*(LDA)*用于监督式降维。
- en: Hierarchical Cluster Analysis (HCA)
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次聚类分析（HCA）
- en: Hierarchical cluster analysis (HCA) is best implemented when the user does not
    have a priori number of clusters to build. Thus, it is a common approach to use
    HCA as a precursor to other clustering techniques where a predetermined number
    of clusters is recommended. HCA works by merging observations that are similar
    into clusters and continues merging clusters that are closest in proximity until
    all observations are merged into a single cluster.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类分析（HCA）最适用于用户不知道预先聚类数量的情况。因此，HCA常作为其他聚类技术的前驱方法，其中建议使用预定数量的聚类。HCA的工作原理是将相似的观测值合并成聚类，并继续合并距离最近的聚类，直到所有观测值合并为一个单一的聚类。
- en: HCA determines similarity as the Euclidean distance between and among observations
    and creates links at the distance in which the two points lie.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: HCA通过计算观测值之间的欧几里得距离来确定相似性，并根据两个点之间的距离建立链接。
- en: 'With the number of features indicated by *n*, the Euclidean distance is calculated
    using the formula:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征数* n *表示，欧几里得距离的计算公式如下：
- en: '![Figure 4.1: The Euclidean distance](img/C13322_04_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1: 欧几里得距离](img/C13322_04_01.jpg)'
- en: 'Figure 4.1: The Euclidean distance'
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.1: 欧几里得距离'
- en: After the distance between observations and cluster have been calculated, the
    relationships between and among all observations are displayed using a dendrogram.
    Dendrograms are tree-like structures displaying horizontal lines as the distance
    between links.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了观测值和聚类之间的距离后，通过树状图显示所有观测值之间的关系。树状图是类似树的结构，通过水平线表示链接之间的距离。
- en: Dr. Thomas Schack (https://www.researchgate.net/profile/Ming-Yang_Cheng/project/The-Effect-of-SMR-Neurofeedback-Training-on-Mental-Representation-and-Golf-Putting-Performance/attachment/57c8419808aeef0362ac36a5/AS:401522300080128@1472741784217/download/Schack+-+2012+-+Measuring+mental+representations.pdf?context=ProjectUpdatesLog)
    relates this structure to the human brain in which each observation is a node
    and the links between observations are neurons.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Thomas Schack 博士（[链接](https://www.researchgate.net/profile/Ming-Yang_Cheng/project/The-Effect-of-SMR-Neurofeedback-Training-on-Mental-Representation-and-Golf-Putting-Performance/attachment/57c8419808aeef0362ac36a5/AS:401522300080128@1472741784217/download/Schack+-+2012+-+Measuring+mental+representations.pdf?context=ProjectUpdatesLog)）将这种结构与人脑相联系，在人脑中，每个观测值都是一个节点，观测值之间的链接则是神经元。
- en: 'This creates a hierarchical structure in which items that are closely related
    are "chunked" together into clusters. An example dendrogram is displayed here:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个层次结构，其中关系紧密的项目会被“打包”到一起，形成聚类。这里展示了一个示例树状图：
- en: '![Figure 4.2: An example dendrogram'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2: 一个示例树状图](img/C13322_04_02.jpg)'
- en: '](img/C13322_04_02.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_04_02.jpg)'
- en: 'Figure 4.2: An example dendrogram'
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.2: 一个示例树状图'
- en: The y-axis indicates the Euclidean distance, while the x-axis indicates the
    row index for each observation. Horizontal lines denote links between observations;
    links closer to the x-axis indicate shorter distance and a subsequent closer relationship.
    In this example, there appear to be three clusters. The first cluster includes
    observations colored in green, the second cluster includes observations colored
    in red, and the third cluster includes observations colored in turquoise.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: y轴表示欧几里得距离，x轴表示每个观察值的行索引。水平线表示观察值之间的连接，越接近x轴的连接表示越短的距离及其更紧密的关系。在此示例中，似乎有三个聚类。第一个聚类包含绿色标记的观察值，第二个聚类包含红色标记的观察值，第三个聚类包含青绿色标记的观察值。
- en: 'Exercise 34: Building an HCA Model'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习34：构建HCA模型
- en: 'To demonstrate HCA, we will be use an adapted version of the glass dataset
    from the University of California – Irvine (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter04).
    This data contains 218 observations and 9 features corresponding to the percent
    weight of various oxides found in glass:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示HCA，我们将使用加利福尼亚大学欧文分校（https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter04）提供的适配版本的玻璃数据集。该数据集包含218个观察值和9个特征，对应于玻璃中各种氧化物的质量百分比：
- en: 'RI: refractive index'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RI：折射率
- en: 'Na: weight percent in sodium'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Na：钠的质量百分比
- en: 'Mg: weight percent in magnesium'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mg：镁的质量百分比
- en: 'Al: weight percent in aluminum'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al：铝的质量百分比
- en: 'Si: weight percent in silicon'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Si：硅的质量百分比
- en: 'K: weight percent in potassium'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K：钾的质量百分比
- en: 'Ca: weight percent in calcium'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ca：钙的质量百分比
- en: 'Ba: weight percent in barium'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ba：钡的质量百分比
- en: 'Fe: weight percent in iron'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fe：铁的质量百分比
- en: In this exercise, we will use the refractive index (RI) and weight percent in
    each oxide to segment the glass type.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用折射率（RI）和每种氧化物的质量百分比来对玻璃类型进行分段。
- en: 'To get started, we will import pandas and read the `glass.csv` file using the
    following code:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将导入pandas并使用以下代码读取`glass.csv`文件：
- en: '[PRE0]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Look for some basic data frame information by printing `df.info()` to the console
    using the following code:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用以下代码打印`df.info()`到控制台，查看一些基本的DataFrame信息：
- en: '[PRE1]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Figure 4.3: DataFrame information'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.3：DataFrame信息'
- en: '](img/C13322_04_03.jpg)'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_04_03.jpg)'
- en: 'Figure 4.3: DataFrame information'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.3：DataFrame信息
- en: 'To remove any possible order effects in the data, we will shuffle the rows
    prior to building any models and save it as a new data frame object, as follows:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了去除数据中的可能的顺序效应，我们将在构建任何模型之前打乱行，并将其保存为新的数据框对象，如下所示：
- en: '[PRE2]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Transform each observation into a z-score by fitting and transforming shuffled
    data using:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过拟合并转换打乱后的数据，使用以下方法将每个观察值转换为z分数：
- en: '[PRE3]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Perform hierarchical clustering using the linkage function on `scaled_features`.
    The following code will show you how:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`scaled_features`执行分层聚类，使用连接函数。以下代码将向你展示如何操作：
- en: '[PRE4]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Congratulations! You've successfully built an HCA model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已成功构建HCA模型。
- en: 'Exercise 35: Plotting an HCA Model and Assigning Predictions'
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习35：绘制HCA模型并分配预测标签
- en: Now that the HCA model has been built, we will continue with the analysis by
    visualizing clusters using a dendrogram and using the visualization to generate
    predictions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在HCA模型已经构建完成，我们将继续分析，使用树状图可视化聚类，并利用该可视化生成预测。
- en: 'Display the dendrogram by plotting the linkage model as follows:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过绘制连接模型来显示树状图，如下所示：
- en: '[PRE5]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Figure 4.4: Dendogram for glass data'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.4：玻璃数据的树状图'
- en: '](img/C13322_04_04.jpg)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_04_04.jpg)'
- en: 'Figure 4.4: Dendogram for glass data'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.4：玻璃数据的树状图
- en: Note
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The index for each observation or row in a dataset is on the x-axis. The Euclidean
    distance is on the y-axis. Horizontal lines are links between and among observations.
    By default, scipy will color code the different clusters that it finds.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集中的每个观察值或行的索引在x轴上。欧几里得距离在y轴上。水平线表示观察值之间的连接。默认情况下，scipy会为它找到的不同聚类进行颜色编码。
- en: Now that we have the predicted clusters of observations, we can use the `fcluster`
    function to generate an array of labels that correspond to rows in `df_shuffled`.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经得到了预测的观察值聚类，我们可以使用`fcluster`函数生成一个标签数组，该数组与`df_shuffled`中的行对应。
- en: 'Generate predicted labels of the cluster which an observation belongs to using
    the following code:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码生成预测的标签，表示一个观察值属于哪个聚类：
- en: '[PRE6]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Add the labels array as a column in the shuffled data and preview the first
    five rows using the following code:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将标签数组作为一列添加到打乱的数据中，并预览前五行：
- en: '[PRE7]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Check the output in the following figure:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看下图中的输出：
- en: '![Figure 4.5: The first five rows of df_shuffled after predictions have been
    matched to observations.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5：在预测与观察值匹配后，df_shuffled 的前五行数据。](img/C13322_04_05.jpg)'
- en: '](img/C13322_04_05.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_04_05.jpg)'
- en: 'Figure 4.5: The first five rows of df_shuffled after predictions have been
    matched to observations.'
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.5：在预测与观察值匹配后，df_shuffled 的前五行数据。
- en: We have successfully learned the difference between supervised and unsupervised
    learning, how to build an HCA model, how to visualize and interpret the HCA dendrogram,
    and how to assign the predicted cluster label to the appropriate observation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功地学习了监督学习与无监督学习的区别，如何构建 HCA 模型，如何可视化和解释 HCA 的树状图，以及如何将预测的聚类标签分配给相应的观察值。
- en: 'Here, we have utilized HCA to cluster our data into three groups and matched
    the observations with their predicted cluster. Some pros of HCA models include:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 HCA 将数据分成了三组，并将观察值与其预测的聚类匹配起来。HCA 模型的一些优点包括：
- en: They are easy to build
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们容易构建
- en: There is no need to specify the number of clusters in advance
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需提前指定聚类的数量
- en: Visualizations are easy to interpret
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化结果容易理解
- en: 'However, some drawbacks of HCA include:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，HCA 的一些缺点包括：
- en: Vagueness in terms of the termination criteria (that is, when to finalize the
    number of clusters)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 终止标准的模糊性（即何时最终确定聚类数量）
- en: The algorithm cannot adjust once the clustering decisions have been made
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦做出聚类决策，算法无法进行调整
- en: Can be very computationally expensive to build HCA models on large datasets
    with many features
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大数据集且特征较多的情况下，构建 HCA 模型可能会非常耗费计算资源
- en: Next, we will introduce you to another clustering algorithm, k-means clustering.
    This algorithm addresses some of the HCA shortcomings by having the ability to
    adjust when the clusters have been initially generated. It is more computationally
    frugal than HCA.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为您介绍另一种聚类算法——K-means 聚类。该算法通过能够在初始生成聚类后进行调整，解决了 HCA 的一些不足。它比 HCA 更加节省计算资源。
- en: K-means Clustering
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means 聚类
- en: Like HCA, K-means also uses distance to assign observations into clusters not
    labeled in data. However, rather than linking observations to each other as in
    HCA, k-means assigns observations to *k* (user-defined number) clusters.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 与 HCA 相似，K-means 也使用距离将未标记数据的观察值分配到聚类中。然而，与 HCA 将观察值相互连接不同，K-means 会将观察值分配到
    *k*（用户定义数量）个聚类中。
- en: 'To determine the cluster to which each observation belongs, kcluster centers
    are randomly generated, and observations are assigned to the cluster in which
    its Euclidean distance is closest to the cluster center. Like the starting weights
    in artificial neural networks, cluster centers are initialized at random. After
    cluster centers have been randomly generated there are two phases:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定每个观察值所属的聚类，K-means 会随机生成聚类中心，并将观察值分配到其欧几里得距离与聚类中心最接近的聚类中。类似于人工神经网络中的起始权重，聚类中心是随机初始化的。在聚类中心被随机生成后，算法分为两个阶段：
- en: Assignment phase
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指派阶段
- en: Updating phase
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新阶段
- en: Note
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The randomly generated cluster centers are important to remember, and we will
    be visiting it later in this chapter. Some refer to this random generation of
    cluster centers as a weakness of the algorithm, because results vary between fitting
    the same model on the same data, and it is not guaranteed to assign observations
    to the appropriate cluster. We can turn it into an advantage by leveraging the
    power of loops.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机生成的聚类中心非常重要，我们将在本章稍后进行讨论。有人认为这种随机生成聚类中心的方式是算法的弱点，因为在对相同数据拟合相同模型时，结果会有所不同，且无法确保将观察值分配到合适的聚类。我们可以通过利用循环的强大功能将其转化为优势。
- en: 'In the assignment phase, observations are assigned to the cluster from which
    it has the smallest Euclidean distance, as shown in the following figure:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在指派阶段，观察值会被分配到距离它最近的聚类，如下图所示：
- en: '![Figure 4.6: A scatterplot of observations and the cluster centers as denoted
    by the star, triangle, and diamond.](img/C13322_04_06.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6：观察值的散点图以及用星号、三角形和菱形表示的聚类中心。](img/C13322_04_06.jpg)'
- en: 'Figure 4.6: A scatterplot of observations and the cluster centers as denoted
    by the star, triangle, and diamond.'
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.6：观察值的散点图以及用星号、三角形和菱形表示的聚类中心。
- en: 'Next, in the updating phase, cluster centers are shifted to the mean position
    of the points in that cluster. These cluster means are known as the centroids,
    as shown in the following figure:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在更新阶段，聚类中心会移动到该聚类中各点的均值位置。这些聚类均值被称为质心，如下图所示：
- en: '![Figure 4.7: Shifting of the cluster centers to the cluster centroid.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7：聚类中心向质心移动。'
- en: '](img/C13322_04_07.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_04_07.jpg)'
- en: 'Figure 4.7: Shifting of the cluster centers to the cluster centroid.'
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.7：聚类中心向质心移动。
- en: 'However, once the centroids have been calculated, some of the observations
    are reassigned to a different cluster due to being closer to the new centroid
    than the previous cluster center. Thus, the model must update its centroids once
    again. This is shown in the following figure:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一旦计算出质心，由于某些观察比以前的聚类中心更接近新的质心，这些观察将被重新分配到不同的聚类。因此，模型必须再次更新其质心。此过程在下图中展示：
- en: '![Figure 4.8: Updating of the centroids after observation reassignment.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8：观察重新分配后质心的更新。'
- en: '](img/C13322_04_08.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_04_08.jpg)'
- en: 'Figure 4.8: Updating of the centroids after observation reassignment.'
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.8：观察重新分配后质心的更新。
- en: 'This process of updating centroids continues until there are no further observation
    reassignments. The final centroid is shown in the following figure:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 更新质心的过程持续进行，直到没有进一步的观察重新分配。最终的质心如下图所示：
- en: '![Figure 4.9: The final centroid position and cluster assignments.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.9：最终质心位置和聚类分配。'
- en: '](img/C13322_04_09.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_04_09.jpg)'
- en: 'Figure 4.9: The final centroid position and cluster assignments.'
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.9：最终质心位置和聚类分配。
- en: Using the same glass dataset from *Exercise 34*, *Building an HCA Model*, we
    will fit a k-means model with user-defined number of clusters. Next, because of
    the randomness in which group centroids are chosen, we will increase the confidence
    in our predictions by building an ensemble of k-means models with a given number
    of clusters and assigning each observation to the mode of the predicted clusters.
    After that, we will tune the optimal number of clusters by monitoring the mean
    *inertia*, or within-cluster sum of squares, by number of clusters, and finding
    the point at which there are diminishing returns in inertia by adding more clusters.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与*练习 34*中相同的玻璃数据集，*构建 HCA 模型*，我们将拟合一个具有用户定义聚类数量的 K-Means 模型。接下来，由于质心的选择具有随机性，我们将通过构建一个具有给定聚类数量的
    K-Means 模型集成，并将每个观察分配给预测聚类的众数，从而提高我们预测的可靠性。之后，我们将通过监控平均*惯性*（或聚类内平方和）随聚类数量的变化，来调节最佳的聚类数，并找出增加聚类数时，惯性减少的临界点。
- en: 'Exercise 36: Fitting k-means Model and Assigning Predictions'
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 36：拟合 K-Means 模型并分配预测
- en: Since our data has already been prepared (see *Exercise 34, Building an HCA
    Model*), and we understand concepts behind the k-Means algorithm, we will learn
    how easy it is to fit a k-means model, generate predictions, and assign these
    predictions to the appropriate observation.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据已经准备好（请参见*练习 34，构建 HCA 模型*），并且我们理解 K-Means 算法背后的概念，我们将学习如何轻松地拟合 K-Means
    模型，生成预测，并将这些预测分配到相应的观察。
- en: 'After the glass dataset has been imported, shuffled, and standardized:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入、打乱和标准化玻璃数据集后：
- en: 'Instantiate a KMeans model with an arbitrary number of, in this case, two clusters,
    as follows:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码实例化一个 KMeans 模型，假设聚类数为任意数，这里为两个聚类：
- en: '[PRE8]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Fit the model to `scaled_features` using the following line of code:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码行将模型拟合到 `scaled_features`：
- en: '[PRE9]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Save the cluster labels from our model into the array, labels, using the following:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将模型的聚类标签保存到数组 labels 中：
- en: '[PRE10]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Generate a frequency table of the labels:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成标签的频率表：
- en: '[PRE11]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To get a better idea, refer to the following screenshot:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了更好地理解，请参见以下截图：
- en: '![Figure 4.10: Frequency table of two clusters'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.10：两个聚类的频率表'
- en: '](img/C13322_04_10.jpg)'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_04_10.jpg)'
- en: 'Figure 4.10: Frequency table of two clusters'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.10：两个聚类的频率表
- en: Using two clusters, 61 observations were placed into the first cluster and 157
    observations were grouped into the second cluster.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用两个聚类，将 61 个观察分配到第一个聚类，157 个观察分配到第二个聚类。
- en: 'Add the labels array as the ''`Predicted Cluster`'' column into the `df_shuffled`
    data frame and preview the first five rows using the following code:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将标签数组作为 '`预测聚类`' 列添加到 `df_shuffled` 数据框中，并预览前五行：
- en: '[PRE12]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Check the output in the following figure:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看以下图中的输出：
- en: '![Figure 4.11: First five rows of df_shuffled'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.11：`df_shuffled` 的前五行'
- en: '](img/C13322_04_11.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_04_11.jpg)'
- en: 'Figure 4.11: First five rows of df_shuffled'
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.11：`df_shuffled` 的前五行
- en: 'Activity 12: Ensemble k-means Clustering and Calculating Predictions'
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 12：集成 K-Means 聚类与预测计算
- en: When algorithms use randomness as part of their method for finding the optimal
    solution (that is, in artificial neural networks and k-means clustering), running
    identical models on the same data may result in different conclusions, limiting
    the confidence we have in our predictions. It is advised to run these models many
    times and generate predictions using a summary measure across all models (that
    is, mean, median, and mode). In this activity, we will build an ensemble of 100
    k-means clustering models.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当算法使用随机性作为寻找最佳解决方案的一部分时（即在人工神经网络和 k-means 聚类中），在相同数据上运行相同的模型可能会得出不同的结论，从而限制我们对预测结果的信心。因此，建议多次运行这些模型，并使用所有模型的汇总度量（即均值、中位数和众数）生成预测。在本活动中，我们将构建
    100 个 k-means 聚类模型的集成。
- en: 'After the glass dataset has been imported, shuffled, and standardized (see
    *Exercise 34*, *Building an HCA Model*):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入、打乱和标准化玻璃数据集之后（请参见 *练习 34*，*构建 HCA 模型*）：
- en: Instantiate an empty data frame to append the labels for each model and save
    it as the new data frame object `labels_df`.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个空数据框架，以便为每个模型附加标签，并将其保存为新的数据框架对象 `labels_df`。
- en: 'Using a for loop, iterate through 100 models, appending the predicted labels
    to `labels_df` as a new column at each iteration. Calculate the mode for each
    row in `labels_df` and save it as a new column in `labels_df`. The output should
    be as follows:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 for 循环，迭代 100 个模型，在每次迭代时将预测标签作为新列附加到 `labels_df` 中。计算 `labels_df` 中每行的众数，并将其作为新列保存到
    `labels_df` 中。输出应如下所示：
- en: '![Figure 4.12: First five rows of labels_df'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.12：labels_df 的前五行'
- en: '](img/C13322_04_12.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_04_12.jpg)'
- en: 'Figure 4.12: First five rows of labels_df'
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.12：labels_df 的前五行
- en: Note
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 356.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解答可以在第 356 页找到。
- en: We have drastically increased the confidence in our predictions by iterating
    through numerous models, saving the predictions at each iteration, and assigning
    the final predictions as the mode of these predictions. However, these predictions
    were generated by models using a predetermined number of clusters. Unless we know
    the number of clusters a priori, we will want to discover the optimal number of
    clusters to segment our observations.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通过迭代多个模型并在每次迭代中保存预测结果，我们极大地提高了对预测结果的信心，并将最终预测结果作为这些预测的众数。然而，这些预测是由使用预定聚类数的模型生成的。除非我们事先知道聚类数，否则我们需要发现最佳的聚类数来分割我们的观测数据。
- en: 'Exercise 37: Calculating Mean Inertia by n_clusters'
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 37：通过 n_clusters 计算平均惯性
- en: The k-means algorithm groups observations into clusters by minimizing the within-cluster
    sum of squares, or inertia. Thus, to improve our confidence in the tuned number
    of clusters for our k-means model, we will place the loop we created in A*ctivity
    12, Ensemble k-means Clustering and Calculating Predictions* (with a few minor
    adjustments) inside of another loop which will iterate through a range of `n_clusters`.
    This creates a nested loop which iterates through 10 possible values for `n_clusters`
    and builds 100 models at each iteration. At each of the 100 inner iterations,
    model inertia will be calculated. For each of the 10 outer iterations, mean inertia
    over the 100 models will be computed, resulting in the mean inertia value for
    each `n_clusters` value.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法通过最小化簇内平方和（或惯性）来将观测数据分组到不同的簇中。因此，为了提高我们对 k-means 模型的聚类数调优的信心，我们将把在
    *活动 12，集成 k-means 聚类与计算预测* 中创建的循环（经过少许调整）放入另一个循环中，后者将迭代一个 `n_clusters` 范围。这会创建一个嵌套循环，迭代
    10 个 `n_clusters` 的可能值，并在每次迭代时构建 100 个模型。在 100 次内层迭代中的每一次中，模型惯性将被计算出来。对于 10 次外层迭代中的每一次，将计算
    100 个模型的平均惯性，从而得出每个 `n_clusters` 值的平均惯性值。
- en: 'After the glass dataset has been imported, shuffled, and standardized (see *Exercise 34,
    Building an HCA Model*):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入、打乱和标准化玻璃数据集之后（请参见 *练习 34，构建 HCA 模型*）：
- en: 'Import the packages we need outside of the loop as shown here:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，在循环外部导入我们所需的包：
- en: '[PRE13]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It is easier to build and comprehend nested loops by working from the inside-out.
    First, instantiate an empty list, `inertia_list`, for which we will append inertia
    values after each iteration of the inside loop as shown here:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从内到外构建和理解嵌套循环会更容易。首先，实例化一个空列表 `inertia_list`，我们将在内部循环的每次迭代后将惯性值附加到该列表，如下所示：
- en: '[PRE14]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the for loop, we will iterate through 100 models using the following code:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 for 循环中，我们将使用以下代码迭代 100 个模型：
- en: '[PRE15]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Inside the loop, build a `KMeans` model with `n_clusters=x`, as follows:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在循环内部，构建一个 `KMeans` 模型，使用 `n_clusters=x`，如下所示：
- en: '[PRE16]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The value for x is determined by the outer for loop, which we have not covered
    yet, but we will cover in detail very shortly.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: x 的值由外部 for 循环确定，我们还没有讲解这个部分，但很快会详细介绍。
- en: 'Fit the model to `scaled_features` as shown here:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，将模型拟合到 `scaled_features`：
- en: '[PRE17]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Get the inertia value and save it to the object inertia as follows:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取惯性值并将其保存为对象惯性，如下所示：
- en: '[PRE18]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Append inertia to `inertia_list` using the following code:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将惯性添加到 `inertia_list` 中：
- en: '[PRE19]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Move to the outside loop, instantiate another empty list to store the average
    inertia values, as shown here:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移动到外部循环，实例化另一个空列表来存储平均惯性值，如下所示：
- en: '[PRE20]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Iterate through the values 1 through 10 for `n_clusters` using the following
    code:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码循环遍历 `n_clusters` 从 1 到 10 的值：
- en: '[PRE21]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After the inside for loop has run through 100 iterations, and the inertia value
    for each of the 100 models have been appended to `inertia_list`, compute the mean
    of this list and save as the object, `mean_inertia` as follows:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在内部 for 循环运行完 100 次迭代，并将 100 个模型的惯性值添加到 `inertia_list` 后，计算该列表的均值并保存为对象 `mean_inertia`，如下面所示：
- en: '[PRE22]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Append `mean_inertia` to `mean_inertia_list` as shown here:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，将 `mean_inertia` 添加到 `mean_inertia_list` 中：
- en: '[PRE23]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: After 100 iterations have been completed 10 times for a total of 1000 iterations,
    `mean_inertia_list` contains 10 values that are the average inertia values for
    each value of `n_clusters`.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在完成 100 次迭代并进行 10 次，合计 1000 次迭代后，`mean_inertia_list` 包含 10 个值，这些值是每个 `n_clusters`
    值的平均惯性值。
- en: 'Print `mean_inertia_list` as shown in the following code. The values are shown
    in the following figure:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下代码打印 `mean_inertia_list`。这些值在下图中显示：
- en: '[PRE24]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![Figure 4.13: mean_inertia_list'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.13：mean_inertia_list'
- en: '](img/C13322_04_13.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_04_13.jpg)'
- en: 'Figure 4.13: mean_inertia_list'
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.13：mean_inertia_list
- en: 'Exercise 38: Plotting Mean Inertia by n_clusters'
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 38：按 n_clusters 绘制均值惯性
- en: 'Continuing from Exercise 38:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 继续进行练习 38：
- en: Now that we have generated mean inertia over 100 models for each value of `n_clusters`,
    we will plot mean inertia by `n_clusters`. Then, we will discuss how to visually
    assess the best value to use for `n_clusters`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为每个 `n_clusters` 值生成了 100 个模型的均值惯性，接下来我们将按 `n_clusters` 绘制均值惯性。然后，我们将讨论如何从视觉上评估选择
    `n_clusters` 的最佳值。
- en: 'First, import matplotlib as follows:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，按如下方式导入 matplotlib：
- en: '[PRE25]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create a list of numbers and save it as the object x, so we can plot it on
    the x-axis as shown here:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数字列表并将其保存为对象 x，以便在 x 轴上绘制，如下所示：
- en: '[PRE26]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Save `mean_inertia_list`, as the object y as shown here:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `mean_inertia_list` 保存为对象 y，如下所示：
- en: '[PRE27]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Plot the mean inertia by number of clusters, as follows:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，按聚类数量绘制均值惯性：
- en: '[PRE28]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Set the plot title to read ''`Mean Inertia by n_clusters`'' using the following:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将图表标题设置为 '`Mean Inertia by n_clusters`'：
- en: '[PRE29]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Label the x-axis ''`n_clusters`'' using `plt.xlabel(''n_clusters'')`, and label
    the y-axis ''`Mean Inertia`'' using the following code:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `plt.xlabel('n_clusters')` 将 x 轴标签标记为 '`n_clusters`'，使用以下代码将 y 轴标签标记为 '`Mean
    Inertia`'：
- en: '[PRE30]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Set the tick labels on the x-axis as the values in x using the following:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将 x 轴的刻度标签设置为 x 中的值：
- en: '[PRE31]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Display the plot used in `plt.show()`. To better understand, refer to the following
    code:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `plt.show()` 显示图表。为了更好地理解，参见以下代码：
- en: '[PRE32]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'For the resultant output, refer to the following screenshot:'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于结果输出，请参见以下截图：
- en: '![Figure 4.14: Mean inertia by n_clusters'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.14：按 n_clusters 计算的均值惯性'
- en: '](img/C13322_04_14.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_04_14.jpg)'
- en: 'Figure 4.14: Mean inertia by n_clusters'
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.14：按 n_clusters 计算的均值惯性
- en: To determine the best number of `n_clusters`, we will use the "elbow method."
    That is, the point in the plot where there are diminishing returns for the added
    complexity of more clusters. From Figure 4.14, we can see that there are rapid
    decreases in mean inertia from `n_clusters` 1 to 3\. After `n_clusters` equals
    3, the decreases in mean inertia seem to become less rapid and the decrease in
    inertia may not be worth the added complexity of adding additional clusters. Thus,
    the appropriate number of `n_clusters` in this situation is 3.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定最佳的 `n_clusters` 数量，我们将使用“肘部法则”。即在图中，随着聚类数量增加，新增聚类带来的复杂度增加，而均值惯性减少的幅度逐渐减缓。从图
    4.14 中可以看出，从 `n_clusters` 为 1 到 3 时，均值惯性急剧下降。而当 `n_clusters` 等于 3 后，均值惯性的下降似乎变得缓慢，并且惯性减少可能不足以抵消增加额外聚类的复杂性。因此，在这种情况下，合适的
    `n_clusters` 数量是 3。
- en: However, if the data has too many dimensions, the k-means algorithm can fall
    subject to the curse of dimensionality by inflated Euclidean distances and subsequent
    erroneous results. Thus, before fitting a k-Means model, using a dimension reduction
    strategy is encouraged.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果数据的维度过多，k-means算法可能会受到维度灾难的影响，因为欧几里得距离膨胀，导致结果错误。因此，在拟合k-Means模型之前，建议使用降维策略。
- en: Reducing the number of dimensions helps to eliminate multicollinearity and decreases
    the time to fit the model. **Principal component analysis** (**PCA**) is a common
    method to reduce the number of dimensions by discovering a set of underlying linear
    variables in the data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 降低维度有助于消除多重共线性，并减少拟合模型的时间。**主成分分析**（**PCA**）是一种常见的降维方法，通过发现数据中一组潜在的线性变量来减少维度。
- en: Principal Component Analysis (PCA)
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: At a high level, PCA is a technique for creating uncorrelated linear combinations
    from the original features termed **components**. Of the principal components,
    the first component explains the greatest proportion of variance in data, while
    the following components account for progressively less variance.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，PCA是一种通过原始特征创建不相关线性组合的技术，这些组合被称为**主成分**。在主成分中，第一个成分解释了数据中最大比例的方差，而后续的成分则逐渐解释较少的方差。
- en: 'To demonstrate PCA, we will:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示PCA，我们将：
- en: Fit PCA model with all principal components
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用所有主成分拟合PCA模型
- en: Tune the number of principal components by setting a threshold of explained
    variance to remain in data
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设置解释方差的阈值来调整主成分的数量，以便保留数据中的信息。
- en: Fit those components to a k-means cluster analysis and compare k-means performance
    before and after the PCA transformation
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些成分拟合到k-means聚类分析中，并比较PCA转换前后k-means的性能
- en: 'Exercise 39: Fitting a PCA Model'
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习39：拟合PCA模型
- en: In this exercise, you will learn to fit a generic PCA model using data we prepared
    in *Exercise 34, Building an HCA Model* and the brief explanation of PCA.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，您将学习如何使用我们在*练习34：构建HCA模型*中准备的数据和PCA简要说明来拟合一个通用的PCA模型。
- en: 'Instantiate a PCA model as shown here:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式实例化PCA模型：
- en: '[PRE33]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Fit the PCA model to `scaled_features`, as shown in the following code:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将PCA模型拟合到`scaled_features`，如下代码所示：
- en: '[PRE34]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Get the proportion of explained variance in the data for each component, save
    the array as the object `explained_var_ratio`, and print the values to the console
    as follows:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据中每个主成分的解释方差比例，将数组保存为对象`explained_var_ratio`，并将值打印到控制台，如下所示：
- en: '[PRE35]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'For the resultant output, refer to the following screenshot:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于结果输出，请参阅以下屏幕截图：
- en: '![Figure 4.15: Explained variance in the data for each principal component'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.15：每个主成分的数据解释方差'
- en: '](img/C13322_04_15.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_04_15.jpg)'
- en: 'Figure 4.15: Explained variance in the data for each principal component'
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.15：每个主成分的数据解释方差
- en: Each principal component explains a proportion of the variance in data. In this
    exercise, the first principal component explained .35 of the variance in data,
    the second explained. 25, the third .13%, and so on. Altogether, these nine components
    explain 100% of the variance in data. The goal of dimensionality reduction is
    to decrease the number of dimensions in data with the objectives of limiting overfitting
    and time to fit the subsequent model. Thus, we will not keep all nine components.
    However, if we retain too few components, the percent of explained variance in
    the data will be low and the subsequent model will under fit. Therefore, a challenge
    for data scientists exists in determining the number of `n_components` that minimize
    over fitting and under fitting.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 每个主成分解释了数据中的一部分方差。在本练习中，第一个主成分解释了数据中35%的方差，第二个主成分解释了25%，第三个主成分解释了13%，依此类推。总的来说，这九个成分解释了数据中100%的方差。降维的目标是减少数据中的维度，以限制过拟合和后续模型拟合的时间。因此，我们不会保留所有九个成分。然而，如果我们保留的成分太少，数据中的解释方差比例将很低，后续模型将出现欠拟合。因此，数据科学家的挑战在于确定最小化过拟合和欠拟合的`n_components`数量。
- en: 'Exercise 40: Choosing n_components using Threshold of Explained Variance'
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习40：使用解释方差阈值选择n_components
- en: In *Exercise 39*, *Fitting PCA Model*, you learned to fit a PCA model with all
    available principal components. However, keeping all of the principal components
    does not reduce the number of dimensions in data. In this exercise, we will reduce
    the number of dimensions in data by retaining the components that explain a threshold
    of variance in it.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在*练习39*，*拟合PCA模型*中，你学会了用所有可用的主成分拟合PCA模型。然而，保留所有主成分并不会减少数据的维度。在本练习中，我们将通过保留解释一定方差阈值的主成分来减少数据的维度。
- en: 'Determine the number of principal components in which a minimum of 95% of the
    variance in the data is explained by calculating the cumulative sum of explained
    variance by the principal component. Let''s look at the following code, to see
    how it''s done:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算每个主成分解释的方差的累计和，确定最少95%数据方差由多少个主成分解释。让我们看以下代码，看看它是如何实现的：
- en: '[PRE36]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'For the resultant output, refer to the following screenshot:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于结果输出，请参考以下截图：
- en: '![Figure 4.16: The cumulative sum of the explained variance for each principal
    component'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.16：每个主成分的解释方差的累计和'
- en: '](img/C13322_04_16.jpg)'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_04_16.jpg)'
- en: 'Figure 4.16: The cumulative sum of the explained variance for each principal
    component'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.16：每个主成分的解释方差的累计和
- en: 'Set the threshold for the percent of variance to keep in data as 95%, as follows:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据中保持的方差百分比阈值设置为95%，如下所示：
- en: '[PRE37]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Using this threshold, we will loop through the list of cumulative explained
    variance and see where they explain no less than 95% of the variance in data.
    Since we will be looping through the indices of `cum_sum_explained_var`, we will
    instantiate our loop using the following:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个阈值，我们将遍历累计解释方差的列表，看看它们是否解释了数据中不少于95%的方差。由于我们将循环遍历`cum_sum_explained_var`的索引，因此我们将使用以下方式实例化我们的循环：
- en: '[PRE38]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Check to see if the item in `cum_sum_explained_var` is greater than or equal
    to 0.95, as shown here:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查`cum_sum_explained_var`中的项是否大于或等于0.95，如下所示：
- en: '[PRE39]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'If that logic is met, then we will add 1 to that index (because we cannot have
    0 principal components), save the value as an object, and break the loop. To do
    this, we will use `best_n_components = i+1` inside of the if statement and break
    in the next line. Look at the following code to get an idea:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果满足该逻辑，则我们将在该索引上加1（因为我们不能有0个主成分），将值保存为一个对象，并退出循环。为此，我们将在if语句中使用`best_n_components
    = i+1`，并在下一行执行break。看看以下代码，了解如何操作：
- en: '[PRE40]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The last two lines in the if statement instruct the loop not to do anything
    if the logic is not met:'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: if语句中的最后两行指示循环如果逻辑不满足时不做任何操作：
- en: '[PRE41]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Print a message detailing the best number of components using the following
    code:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码打印一条消息，详细说明最佳的主成分数量：
- en: '[PRE42]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'View the output from the previous line of code:'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查看上一行代码的输出：
- en: '![Figure 4.17: The output message displaying number of components'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.17：显示组件数量的输出信息'
- en: '](img/C13322_04_17.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_04_17.jpg)'
- en: 'Figure 4.17: The output message displaying number of components'
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.17：显示组件数量的输出信息
- en: The value for `best_n_components` is 6\. We can refit another PCA model with
    `n_components = 6`, transform the data into principal components, and use these
    components in a new k-means model to lower the inertia values. Additionally, we
    can compare the inertia values across `n_clusters` values for the models built
    using PCA transformed data to those using data that was not PCA transformed.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`best_n_components`的值为6。我们可以用`n_components = 6`重新拟合一个PCA模型，将数据转换为主成分，并在新的k-means模型中使用这些主成分来降低惯性值。此外，我们可以将使用PCA变换后的数据构建的模型与使用未经PCA变换的数据构建的模型在`n_clusters`值上的惯性值进行比较。'
- en: 'Activity 13: Evaluating Mean Inertia by Cluster after PCA Transformation'
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动13：通过PCA变换后按聚类评估平均惯性
- en: Now that we know the number of components to retain at least 95% of the variance
    in the data, how to transform our features into principal components, and a way
    to tune the optimal number of clusters for k-means clustering with a nested loop,
    we will put them all together in this activity.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了保留至少95%数据方差的主成分数量，知道如何将特征转换为主成分，并且有了一个方法来通过嵌套循环调整k-means聚类的最优聚类数量，接下来我们将在这个活动中将它们整合起来。
- en: 'Continuing from *Exercise 40*:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 接着进行*练习40*：
- en: Instantiate a PCA model with the value for the `n_components` argument equal
    to `best_n_components` (that is, remember, `best_n_components = 6`).
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`n_components`参数等于`best_n_components`的值实例化PCA模型（也就是说，记住，`best_n_components
    = 6`）。
- en: Fit the model to `scaled_features` and transform it into the first six principal
    components
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型拟合到`scaled_features`并将其转换为前六个主成分
- en: Using a nested loop, calculate the mean inertia over 100 models at values 1
    through 10 for `n_clusters` (see *Exercise 40*, *Choosing n_components using Threshold
    of Explained Variance*).
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用嵌套循环，计算在`n_clusters`值从1到10之间的100个模型的均值惯性（参见*练习 40*，*使用方差解释阈值选择n_components*）。
- en: '![Figure 4.18: mean_inertia_list_PCA'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.18：mean_inertia_list_PCA'
- en: '](img/C13322_04_18.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_04_18.jpg)'
- en: 'Figure 4.18: mean_inertia_list_PCA'
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.18：mean_inertia_list_PCA
- en: Now, much like in *Exercise 38*, *Plotting Mean Inertia by n_clusters*, we have
    a mean inertia value for each value of `n_clusters` (1 through 10). However, `mean_inertia_list_PCA`
    contains the mean inertia value for each value of `n_clusters` after PCA transformation.
    But, how do we know if the k-means model performs better after PCA transformation?
    In the next exercise, we will visually compare the mean inertia values before
    and after PCA transformation at each value of `n_clusters`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，就像在*练习 38*中所做的那样，*按n_clusters绘制均值惯性*，我们有每个`n_clusters`值（1到10）的均值惯性值。然而，`mean_inertia_list_PCA`包含PCA变换后每个`n_clusters`值的均值惯性值。但是，我们如何知道PCA变换后k-means模型的表现是否更好呢？在下一个练习中，我们将视觉对比每个`n_clusters`值下PCA变换前后的均值惯性。
- en: Note
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 357.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解答可以在第357页找到。
- en: 'Exercise 41: Visual Comparison of Inertia by n_clusters'
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 41：按n_clusters进行惯性视觉对比
- en: 'To visually compare mean inertia by `n_clusters` before and after PCA transformation,
    we will slightly modify the plot created in *Exercise 38, Plotting Mean Inertia
    by n_clusters,* by:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了视觉对比PCA变换前后的均值惯性，我们将稍微修改在*练习 38，按n_clusters绘制均值惯性*中创建的图表，方法如下：
- en: Adding a second line to the plot showing mean inertia by `n_clusters` after
    PCA transformation
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向图表中添加一条显示PCA变换后按`n_clusters`绘制的均值惯性曲线
- en: Creating a legend distinguishing the lines
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个图例来区分各条线
- en: Changing the title
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改标题
- en: Note
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For this visualization to work properly, `mean_inertia_list` from *Exercise
    38*, *Plotting Mean Inertia by n_clusters*, must still be in the environment.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了让这个可视化正常工作，*练习 38*中的`mean_inertia_list`，*按n_clusters绘制均值惯性*，必须仍然存在于环境中。
- en: 'Continuing from *Activity 13*:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 继续进行*活动 13*：
- en: 'Import Matplotlib using the following code:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码导入Matplotlib：
- en: '[PRE43]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create a list of numbers and save it as the object x, so we can plot it on
    the x-axis as follows:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数字列表并将其保存为对象x，以便我们可以在x轴上绘制，方法如下：
- en: '[PRE44]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Save `mean_inertia_list_PCA` as the object y using the following code:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将`mean_inertia_list_PCA`保存为对象y：
- en: '[PRE45]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Save `mean_inertia_list` as the object y2 using the following:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将`mean_inertia_list`保存为对象y2：
- en: '[PRE46]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Plot mean inertia after a PCA transformation by number of clusters using the
    following code:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码按聚类数绘制PCA变换后的均值惯性：
- en: '[PRE47]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Add our second line of mean inertia before a PCA transformation by number of
    clusters using the following:'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用以下方法在PCA变换之前，按聚类数添加我们的第二条均值惯性线：
- en: '[PRE48]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Set the plot title to read ''`Mean Inertia by n_clusters for Original Features
    and PCA Transformed Features`'' as follows:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图表标题设置为'`Mean Inertia by n_clusters for Original Features and PCA Transformed
    Features`'，如下所示：
- en: '[PRE49]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Label the x-axis ''`n_clusters`'' using the following code:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将x轴标签标记为'`n_clusters`'：
- en: '[PRE50]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Label the y-axis ''`Mean Inertia`'' using:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将y轴标签标记为'`Mean Inertia`'：
- en: '[PRE51]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Set the tick labels on the x-axis as the values in x using `plt.xticks(x)`.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plt.xticks(x)`将x轴的刻度标签设置为x中的值。
- en: 'Show a legend using and display the plot as follows:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用并显示图表如下所示，显示图例：
- en: '[PRE52]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![Figure 4.19: Mean inertia by n_clusters for original features (orange) and
    PCA transformed features (blue)](img/C13322_04_19.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.19：原始特征（橙色）和PCA变换特征（蓝色）按n_clusters的均值惯性](img/C13322_04_19.jpg)'
- en: 'Figure 4.19: Mean inertia by n_clusters for original features (orange) and
    PCA transformed features (blue)'
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.19：原始特征（橙色）和PCA变换特征（蓝色）按n_clusters的均值惯性
- en: From the plot, we can see that inertia is lower at every number of clusters
    in the model using the PCA transformed features. This indicates that there was
    less distance between the group centroids and observations in each cluster after
    the PCA transformation relative to before the transformation. Thus, using a PCA
    transformation on the original features, we were able to decrease the number of
    features and simultaneously improve our model by decreasing the within-cluster
    sum of squares (that is, inertia).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中，我们可以看到，在模型中使用 PCA 转换后的特征时，每个聚类数的惯性较低。这表明，在 PCA 转换后，相比于转换前，各聚类的组质心与观测值之间的距离更小。因此，通过对原始特征进行
    PCA 转换，我们能够减少特征数量，并同时通过减少组内平方和（即惯性）来改进我们的模型。
- en: HCA and k-means clustering are two widely-used unsupervised learning techniques
    used for segmentation. PCA can be used to help reduce the number of dimensions
    in our data and improve models in an unsupervised fashion. Linear discriminant
    function analysis (LDA), on the other hand, is a supervised method for reducing
    the number of dimensions via data compression.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: HCA 和 k-means 聚类是两种广泛使用的无监督学习技术，用于数据分割。PCA 可用于帮助减少数据维度，并以无监督的方式改进模型。而线性判别函数分析（LDA）则是一种监督方法，通过数据压缩来减少维度。
- en: Supervised Data Compression using Linear Discriminant Analysis (LDA)
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用线性判别分析（LDA）进行监督数据压缩
- en: As discussed previously, PCA transforms features into a set of variables to
    maximize the variance among the features. In PCA, the output labels are not considered
    when fitting the model. Meanwhile, LDA uses the dependent variable to help compress
    data into features that best discriminate the classes of the outcome variable.
    In this section, we will walk through how to use LDA as a supervised data compression
    technique.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，PCA 将特征转换为一组最大化特征间方差的变量。在 PCA 中，输出标签在拟合模型时不被考虑。与此同时，LDA 使用因变量来帮助将数据压缩成能最好区分结果变量类别的特征。在这一部分中，我们将演示如何使用
    LDA 作为监督数据压缩技术。
- en: 'To demonstrate using LDA as supervised dimensionality compression technique,
    we will:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示如何使用 LDA 作为监督的降维压缩技术，我们将：
- en: Fit an LDA model with all possible `n_components`
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用所有可能的 `n_components` 拟合一个 LDA 模型
- en: Transform our features to `n_components`
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们的特征转换为 `n_components`
- en: Tune the number of `n_components`
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整 `n_components` 的数量
- en: 'Exercise 42: Fitting LDA Model'
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 42：拟合 LDA 模型
- en: To fit the model as a supervised learner using the default parameters of the
    LDA algorithm we will be using a slightly different glass data set, `glass_w_outcome.csv`.
    (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter04)
    This dataset contains the same nine features as glass, but also an outcome variable,
    Type, corresponding to the type of glass. Type is labeled 1, 2, and 3 for building
    windows float processed, building windows non float processed, and headlamps,
    respectively.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 LDA 算法的默认参数将模型作为监督学习者拟合，我们将使用一个稍有不同的玻璃数据集，`glass_w_outcome.csv`。（https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter04）此数据集包含与玻璃数据集相同的九个特征，但还包括一个结果变量
    Type，表示玻璃的类型。Type 被标记为 1、2 和 3，分别对应建筑窗户浮法处理、建筑窗户非浮法处理和车灯。
- en: 'Import the `glass_w_outcome.csv` file and save it as the object df using the
    following code:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `glass_w_outcome.csv` 文件，并使用以下代码将其保存为对象 df：
- en: '[PRE53]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Shuffle the data to remove any ordering effects and save it as the data frame
    `df_shuffled` as follows:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打乱数据以消除任何顺序效应，并将其保存为数据框 `df_shuffled`，如下所示：
- en: '[PRE54]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Save ‘`Type`’ as `DV` (I.e., dependent variable) as follows:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `Type` 保存为 `DV`（即，因变量），如下所示：
- en: '[PRE55]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Split the shuffled data into features (i.e., X) and outcome (i.e., y) using
    `X = df_shuffled.drop(DV, axis=1)` and `y = df_shuffled[DV]`, respectively.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `X = df_shuffled.drop(DV, axis=1)` 和 `y = df_shuffled[DV]` 将打乱后的数据拆分为特征（即
    X）和结果（即 y）。
- en: 'Split X and y into testing and training as follows:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照如下方式将 X 和 y 拆分为测试集和训练集：
- en: '[PRE56]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Scale `X_train` and `X_test` separately using the following code:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码分别对 `X_train` 和 `X_test` 进行缩放：
- en: '[PRE57]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Instantiate the LDA model and save it as model. The following will show you
    how.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化 LDA 模型并将其保存为模型。以下将向您展示如何操作。
- en: '[PRE58]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Note
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: By instantiating an LDA model with no argument `for n_components` we will return
    all possible components.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过实例化一个不带参数 `for n_components` 的 LDA 模型，我们将返回所有可能的成分。
- en: 'Fit the model to the training data using the following:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将模型拟合到训练数据：
- en: '[PRE59]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'See the resultant output below:![Figure 4.20: Output from fitting linear discriminant
    function analysis'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请参见下面的结果输出：![图 4.20：线性判别函数分析的拟合结果输出]
- en: '](img/C13322_04_20.jpg)'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_04_20.jpg)'
- en: 'Figure 4.20: Output from fitting linear discriminant function analysis'
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.20：拟合线性判别分析函数后的输出
- en: Much like in PCA, we can return the percentage of variance explained by each
    component.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与 PCA 类似，我们可以返回每个组件解释的方差百分比。
- en: '[PRE60]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The output is shown in the following figure.
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下图所示。
- en: '![Figure 4.21: Explained variance by component.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.21：按组件解释的方差。'
- en: '](img/C13322_04_21.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_04_21.jpg)'
- en: 'Figure 4.21: Explained variance by component.'
  id: totrans-323
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.21：按组件解释的方差。
- en: Note
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The first component explains 95.86% of the variance in the data and the second
    component explains 4.14% of the variance in the data for a total of 100%.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个组件解释了数据中 95.86% 的方差，第二个组件解释了数据中 4.14% 的方差，总共为 100%。
- en: We have successfully fit an LDA model to compress our data from nine features
    to two features. Decreasing the features to two cuts the time to tune and fit
    machine learning models. However, prior to using these features in a classifier
    model we must transform the training and testing features into their two components.
    In the next exercise, we will show how this is done.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地将 LDA 模型拟合到数据中，将数据从九个特征压缩到两个特征。将特征减少到两个可以减少调优和拟合机器学习模型的时间。然而，在将这些特征应用于分类器模型之前，我们必须将训练和测试特征转换为其两个组成部分。在下一个练习中，我们将展示如何实现这一过程。
- en: 'Exercise 43: Using LDA Transformed Components in Classification Model'
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 43：在分类模型中使用 LDA 转换的组件
- en: Using supervised data compression, we will transform our training and testing
    features (i.e., `X_train_scaled` and `X_test_scaled`, respectively) into their
    components and fit a `RandomForestClassifier` model on them.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 通过监督式数据压缩，我们将把训练和测试特征（即 `X_train_scaled` 和 `X_test_scaled`）转换为其组成部分，并在其上拟合 `RandomForestClassifier`
    模型。
- en: 'Continuing from *Exercise 42*:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 继续进行*练习 42*：
- en: 'Compress `X_train_scaled` into its components as follows:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`X_train_scaled`压缩为其组成部分，如下所示：
- en: '[PRE61]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Compress `X_test` into its components using:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下方式将`X_test`压缩为其组成部分：
- en: '[PRE62]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Instantiate a `RandomForestClassifier` model as follows:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式实例化一个`RandomForestClassifier`模型：
- en: '[PRE63]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Note
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: We will be using the default hyperparameters of the `RandomForestClassifier`
    model because tuning hyperparameters is beyond the scope of this chapter.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将使用`RandomForestClassifier`模型的默认超参数，因为超参数调优超出了本章的范围。
- en: 'Fit the model to the compressed training data using the following code:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将模型拟合到压缩后的训练数据：
- en: '[PRE64]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'See the resultant output below:'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参见下方的结果输出：
- en: '![Figure 4.22: Output after fitting random forest classifier model'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.22：拟合随机森林分类器模型后的输出'
- en: '](img/C13322_04_22.jpg)'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_04_22.jpg)'
- en: 'Figure 4.22: Output after fitting random forest classifier model'
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.22：拟合随机森林分类器模型后的输出
- en: 'Generate predictions on `X_test_LDA` and save them as the array, predictions
    using the following code:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码在`X_test_LDA`上生成预测并将其保存为数组predictions：
- en: '[PRE65]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Evaluate model performance by comparing predictions to `y_test` using a confusion
    matrix. To generate and print a confusion matrix see the code below:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将预测与 `y_test` 进行比较，使用混淆矩阵评估模型性能。要生成并打印混淆矩阵，请参见以下代码：
- en: '[PRE66]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The output is shown in the following figure:'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下图所示：
- en: '![Figure 4.23: 3x3 confusion matrix for evaluating RandomForestClassifier model
    performance using the LDA compressed data](img/C13322_04_23.jpg)'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.23：使用 LDA 压缩数据评估随机森林分类器模型性能的 3x3 混淆矩阵](img/C13322_04_23.jpg)'
- en: 'Figure 4.23: 3x3 confusion matrix for evaluating RandomForestClassifier model
    performance using the LDA compressed data'
  id: totrans-350
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.23：使用 LDA 压缩数据评估随机森林分类器模型性能的 3x3 混淆矩阵
- en: Summary
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter introduced you to two widely used unsupervised, clustering algorithms,
    HCA and k-means clustering. While learning about k-means clustering, we leveraged
    the power of loops to create ensembles of models for tuning the number of clusters
    and to gain more confidence in our predictions. During the PCA section, we determined
    the number of principal components for dimensionality reduction and fit the components
    to a k-means model. Additionally, we compared the differences in k-means model
    performance before and after PCA transformation. We were introduced to an algorithm,
    LDA, which reduces dimensionality in a supervised manner. Lastly, we tuned the
    number of components in LDA by iterating through all possible values for components
    and programmatically returning the value resulting in the best accuracy score
    from a Random Forest classifier model. You should now feel comfortable with dimensionality
    reduction and unsupervised learning techniques.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向你介绍了两种广泛使用的无监督聚类算法——HCA和k-means聚类。在学习k-means聚类时，我们利用循环的力量创建了多个模型集，以调节聚类的数量，并提高我们预测的可靠性。在PCA部分，我们确定了用于降维的主成分数量，并将这些成分拟合到k-means模型中。此外，我们比较了PCA转换前后k-means模型性能的差异。我们还介绍了一种算法——LDA，它以监督方式减少维度。最后，我们通过遍历所有可能的成分值，并通过编程返回使得随机森林分类器模型获得最佳准确度得分的值，来调节LDA中的成分数量。现在，你应该已经对降维和无监督学习技术感到得心应手。
- en: We were briefly introduced to creating plots in this chapter; however, in the
    next chapter, we will learn about structured data and how to work with XGboost
    and Keras libraries
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要介绍了如何创建图表；然而，在下一章，我们将学习结构化数据以及如何使用XGboost和Keras库。
