- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Applying Machine Learning at Work
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在工作中应用机器学习
- en: 'You''ve heard a lot about creating business value with intelligent algorithms:
    it''s finally time to roll up our sleeves and make it happen. In this chapter,
    we are going to experience what it means to apply machine learning to tangible
    cases by going through a few step-by-step tutorials. Our companion KNIME is back
    on stage: we will learn how to build workflows for implementing machine learning
    models using real-world data. We are going to meet a few specific algorithms and
    learn the intuitive mechanisms behind how they operate. We''ll glimpse into their
    underlying mathematical models, focusing on the basics to comprehend their results
    and leverage them in our work.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经听说过如何通过智能算法创造商业价值：现在是时候大显身手了。在本章中，我们将通过几个逐步教程，亲身体验将机器学习应用于具体案例的过程。我们的伙伴 KNIME
    回到了舞台上：我们将学习如何构建工作流来使用实际数据实施机器学习模型。我们将接触几个特定的算法，了解它们背后直观的工作原理。我们将窥视它们的数学模型，重点理解基本原理，以便理解它们的结果并在工作中加以利用。
- en: 'This practical chapter will answer several questions, including:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本实用章节将回答几个问题，包括：
- en: How do I make predictions using supervised machine learning algorithms in KNIME?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何使用 KNIME 中的监督式机器学习算法进行预测？
- en: How can I check whether a model is performing well?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何检查模型是否表现良好？
- en: How do we avoid the risk of overfitting?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何避免过拟合的风险？
- en: What techniques can I use to improve the performance of a model?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以使用什么技巧来提高模型的性能？
- en: How can I group similar elements together using clustering algorithms?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何使用聚类算法将相似的元素分组？
- en: 'The tutorials included in this chapter cover three of the most recurrent cases
    when you can rely on machine learning as part of your work: predicting numbers,
    classifying entities, and grouping elements. Think of them as "templates" that
    you can widely reapply after you reach the last page of the chapter and that you
    are likely to keep using as a reference. The steps of the tutorials are also organized
    in the same order they would unfold in everyday practice, including the "back
    and forth" iterations required for improving the performance of your model. This
    will prepare you to face the actual use of real-life machine learning, which often
    follows a circuitous route made of trial and error attempts.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的教程涵盖了你可以在工作中依赖机器学习的三种最常见情况：预测数字、分类实体和分组元素。你可以把它们看作是“模板”，在你翻到本章最后一页后，可以广泛地重新应用，并且你很可能会继续将其作为参考。教程的步骤也按它们在日常实践中展开的顺序进行组织，包括为了提高模型性能所需的“反复迭代”。这将帮助你准备面对实际的机器学习应用，现实中机器学习往往需要经历一条曲折的试错路线。
- en: Within each tutorial, you will encounter one or two machine learning algorithms
    (specifically, **linear regression** in the first, **decision tree** and **random
    forest** in the second, and **k-means** in the third) that will be introduced
    and explained before being seen in action. Let's get started with some first predictions!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个教程中，你将遇到一两个机器学习算法（具体来说，第一个是**线性回归**，第二个是**决策树**和**随机森林**，第三个是**K-means**），这些算法将在实际应用之前进行介绍和解释。让我们从一些初步的预测开始吧！
- en: Predicting numbers through regressions
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过回归预测数字
- en: 'For this tutorial, you will assume the—somewhat—enviable role of a real estate
    agent based in Rome, Italy. The company you work for owns multiple agencies specialized
    in rentals of properties located in the broader metropolitan area of the Eternal
    City. Your passion for data analytics got you noticed by the CEO: she asked you
    to figure out a way to support agents in objectively evaluating the fair monthly
    rent of a property based on its features. She noticed that the business greatly
    suffers when the rent set for a property is not aligned with the market. In fact,
    if the rent is too low, the agency fee (which is a fixed percentage of the agreed
    rent) will end up being lower than what it could have been, leaving profit on
    the table. On the other hand, if the ask is too high, revenues for the agency
    will take longer to materialize, causing a substantial impact on the cash flow.
    The traditional approach to set the monthly rent for new properties is a "negotiation"
    between owners and agents, who will use their market understanding (and sometimes
    the benchmark of similar properties) to convince the owners about the right rent
    to ask for.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将假设自己扮演的是一名位于意大利罗马的房地产经纪人的—有点—令人羡慕的角色。你所在的公司拥有多家专注于出租位于永恒之城更广泛都市区的物业的代理机构。你对数据分析的热情引起了首席执行官的注意：她要求你找到一种方法，帮助代理商根据物业的特点客观评估合理的月租。她注意到，当物业的租金与市场不符时，业务将受到严重影响。事实上，如果租金过低，代理费（这是约定租金的固定百分比）最终会低于本应有的水平，导致利润流失。另一方面，如果租金过高，代理机构的收入将需要更长时间才能显现，影响现金流。为新物业设定月租的传统方法是房东与代理商之间的“谈判”，代理商会利用他们对市场的了解（有时还会参考类似物业的基准）来说服房东设定合适的租金。
- en: 'You are sure that machine learning has the potential to make a difference,
    and you are resolute in finding an ML way to improve this business process. The
    idea that comes to mind is to use the database of the monthly rent of previously
    rented properties (for which we have available their full description) to predict
    the right monthly rent of future properties based on their objective characteristics.
    Such a data-driven approach, if well communicated, can ease the price-setting
    process and result in a mutual advantage for all the parties involved: the landlord
    and the agency will get a quick and profitable transaction, and the tenant will
    obtain a fair rent.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你确信机器学习有潜力带来改变，并决心找到一种机器学习的方法来改善这一业务流程。你想到的一个方法是使用以前租赁物业的月租数据库（这些物业有完整的描述）来预测未来物业的合理月租，基于其客观特征。这样一种数据驱动的方法，如果传达得当，可以简化定价过程，带来各方的共同利益：房东和代理机构能够迅速达成有利可图的交易，租客则能够获得公正的租金。
- en: 'The prospect of building a machine able to predict rental prices is exhilarating
    and makes you impatient to start. You manage to obtain an extraction of the last
    4,000 rental agreements signed at the agency (`RomeHousing-History.xlsx`). The
    table contains, for each property:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 能够预测租金价格的机器的前景令人兴奋，让人迫不及待地想开始。你设法获取了最近签署的 4,000 份租赁协议的提取数据（`RomeHousing-History.xlsx`）。表格包含了每个物业的以下信息：
- en: '*House_ID*: a unique identifier of the property.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*House_ID*: 物业的唯一标识符。'
- en: '*Neighborhood*: the name of the area where the property lies, ranging from
    the fancy surroundings of `Piazza` `Navona` to the tranquil, lakeside towns of
    `Castelli` `Romani`. *Figure 5.1* shows a map of the Rome area with some of these
    neighborhoods.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Neighborhood*: 物业所在区域的名称，从`Piazza` `Navona`周围的豪华区域到宁静的湖畔小镇`Castelli` `Romani`。*图5.1*展示了罗马地区的地图，标出了这些社区的一部分。'
- en: '*Property_type*: a string clarifying if the property is a `flat`, a `house`,
    a `villa`, or a `penthouse`.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Property_type*: 一个字符串，说明物业是`flat`（公寓）、`house`（房屋）、`villa`（别墅）还是`penthouse`（顶层公寓）。'
- en: '*Rooms*: the number of available rooms in the property, including bathrooms.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Rooms*: 物业中可用房间的数量，包括浴室。'
- en: '*Surface*: the usable floor area of the property in square meters.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Surface*: 物业的可用楼面面积，单位为平方米。'
- en: '*Elevator*: a binary category indicating if an elevator is available (`1`)
    or not (`0`).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elevator*: 一个二元类别，表示物业是否有电梯（`1`表示有，`0`表示没有）。'
- en: '*Floor_type*: a category showing if the property is on a `Mezzanine`, a `Ground`
    floor, or an `Upper` level.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Floor_type*: 一个类别，显示物业所在的楼层是`Mezzanine`（夹层）、`Ground`（底层）还是`Upper`（上层）。'
- en: '*Floor_number*: the floor number on which the property is situated, based on
    the European convention (`0` is for the ground floor, `0.5` is the mezzanine,
    `1` is for the first level above the ground, and so on).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Floor_number*：物业所在的楼层，按照欧洲惯例（`0`表示底层，`0.5`表示夹层，`1`表示地面以上的第一层，依此类推）。'
- en: '*Rent*: the all-inclusive, monthly rent in euros on the final rental agreement.![Map'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Rent*：最终租赁协议中的所有包括项，即月租金，以欧元计算。[地图图像]'
- en: Description automatically generated](img/B17125_05_01.png)
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[自动生成的描述](img/B17125_05_01.png)'
- en: 'Figure 5.1: The Rome neighborhoods covered by our real estate. Have you visited
    any of these places already?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：我们的房地产覆盖的罗马邻里。你去过这些地方吗？
- en: 'Before building the model, you wisely stop for a second and think through the
    ways you are going to practically leverage it once ready. You realize that the
    potential business value for completing this endeavor is two-fold:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型之前，你明智地停下来，认真思考一旦模型完成后，如何实际运用它。你意识到，完成这一工作潜在的商业价值有两个方面：
- en: 'First, by interpreting how the model works, you can find out some insightful
    evidence on the market price formation mechanisms. You might be able to find answers
    to the questions: *what features really do make a difference in the pricing?*,
    *does the floor number impact the value greatly?*, and *which neighborhoods prove
    to be most expensive ones, at parity of all other characteristics of the property?*.
    Some of the answers will reinforce the market understanding that your agency already
    has, adding the benefit of making this knowledge explicit and formally described.
    More interestingly, other findings might be truly unexpected and unveil original
    dynamics you did not know about.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，通过解读模型如何运作，你可以发现一些关于市场价格形成机制的深刻证据。你可能能够找到以下问题的答案：*哪些特征在定价中真正起到了决定性作用？*、*楼层数是否对价值有很大影响？*以及*在其他特征相同的情况下，哪些邻里最为昂贵？*其中一些答案将强化你所在机构已有的市场理解，同时增加将这些知识明确化并正式描述的好处。更有趣的是，其他发现可能是完全意外的，并揭示出你之前未曾了解的原始动态。
- en: Second, your model can be used to generate data-based recommendations on the
    rent to be set for new properties as they go on the market and enter the portfolio
    of the agency. To make things more interesting on this front, the owner shares
    with you a list (`RomeHousing-NewProperties.xlsx`) of 10 incoming properties for
    which the rental price has not been fixed yet, using the same features (such as
    *Neighborhood*, *Property_type*, and so on) available in the historical database.
    Once ready, you will apply your model to these sample properties as an illustration
    of how it works.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，你的模型可以用来生成基于数据的租金建议，当新物业进入市场并加入机构的投资组合时。为了让这方面的事情更加有趣，业主向你分享了一份名为(`RomeHousing-NewProperties.xlsx`)的列表，里面列出了10个尚未确定租金的即将上市的物业，这些物业使用与历史数据库中相同的特征（如*Neighborhood*、*Property_type*等）。一旦模型完成，你将运用它来对这些样本物业进行示范，展示它的工作原理。
- en: 'You are now clear on what the business requires, and you can finally translate
    it into definite machine learning terms, building on what we have learned in the
    previous chapter. You need to build a machine that predicts "unknown" rental prices
    by learning from some "known" examples: the database of previously rented properties
    is your *labeled* dataset, as it has examples of your target variable, in this
    case, the *Rent*. Going through the catalog of machine learning algorithms (*Figure
    4.5*), you realize we are clearly in the category of *supervised* machine learning.
    More specifically, you need to predict numbers (rent in euros), so you definitely
    need to leverage an algorithm for doing a *regression*.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经清楚了业务需求，最终可以将其转化为具体的机器学习术语，基于我们在上一章学到的内容。你需要建立一个通过学习一些“已知”示例来预测“未知”租金价格的机器：以前租赁过的物业数据库就是你的*标签化*数据集，因为它包含了目标变量的示例，在本案例中，即*Rent*。浏览机器学习算法的目录（*图4.5*），你意识到我们显然处于*监督式*机器学习的范畴。更具体地说，你需要预测数值（欧元租金），因此你确实需要利用一个进行*回归*的算法。
- en: 'The ML way to solve this business opportunity is now clear in front of your
    eyes: you can finally get KNIME started and create a new workflow (**File** |
    **New…** | **New KNIME Workflow**):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，解决这个商业机会的机器学习方法已经清晰地出现在你面前：你终于可以启动KNIME并创建一个新的工作流（**文件** | **新建...** | **新建KNIME工作流**）：
- en: As a very first step, you load your labeled dataset by dragging and dropping
    the file(`RomeHousing-History.xlsx`) into your blank workflow or by implementing
    the**Excel Reader**node. In either case, KNIME will have recognized the structure
    of the file, and you just need to accept its default configuration. After running
    the node, you obtain the dataset shown in *Figure 5.2*, where you find the nine
    columns you expected:![](img/B17125_05_02.png)
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为第一步，你通过将文件（`RomeHousing-History.xlsx`）拖放到空白工作流中，或者通过使用**Excel Reader**节点来加载带标签的数据集。在这两种情况下，KNIME都会识别文件的结构，你只需接受其默认配置。运行节点后，你将获得如*图
    5.2*所示的数据集，其中包含你预期的九列：![](img/B17125_05_02.png)
- en: 'Figure 5.2: Historical rental data loaded into KNIME: 4,000 properties to learn
    from'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.2：加载到KNIME中的历史租赁数据：4,000个房产供学习使用
- en: When you build a machine learning model, you will interact in various ways with
    the columns of your data table. It is sensible to get an understanding of what
    you are going to deal with by exploring the columns right at the beginning. Fortunately,
    the **Statistics** node helps as it displays at once the most important things
    you need to know about your columns.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当你构建机器学习模型时，你将以各种方式与数据表中的列进行交互。通过在一开始就探索这些列，了解你将要处理的数据是非常明智的。幸运的是，**统计**节点可以帮助你，它会立即显示你需要了解的列的最重要信息。
- en: '![](img/NEW_statistics_node.png) *Statistics*'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![](img/NEW_statistics_node.png) *统计*'
- en: 'This node (**Analytics > Statistics**) calculates summary statistics for each
    column available in the input table. The checkbox appearing at the top of its
    configuration dialog (*Figure 5.3*) lets you decide whether to **Calculate median
    values** of the numeric columns: this calculation might be computationally expensive
    for large datasets, so you will tick it only if necessary. The column selector
    in the middle lets you decide which columns should be treated as **Nominal**.
    For these columns, the node will count the number of instances of each unique
    value: this is useful for categorical columns when you want to quickly assess
    the relative footprint of every category in a table. The main summary metrics
    calculated by the node are minimum (**Min**), average (**Mean**), **Median**,
    maximum (**Max**), standard deviations (**Std. Dev.**), **Skewness**, **Kurtosis**,
    count of non-numeric values such as missing values (**No. Missing**), and plus
    or minus infinite (**No. +∞**, **No. –∞**). The node will also output the histograms
    showing the distributions of the values and, for nominal columns, the list of
    the most and least numerous categories identified in the dataset:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个节点（**分析 > 统计**）计算输入表中每一列的摘要统计信息。其配置对话框顶部出现的复选框（*图 5.3*）让你决定是否**计算数值列的中位数值**：对于大数据集来说，这个计算可能会很耗费计算资源，因此只有在必要时才勾选。中间的列选择器让你决定哪些列应该被视为**名义型**。对于这些列，节点会计算每个唯一值的实例数：这对于分类列非常有用，当你想快速评估表格中每个类别的相对分布时。节点计算的主要摘要指标包括最小值（**Min**）、平均值（**Mean**）、**中位数**、最大值（**Max**）、标准差（**Std.
    Dev.**）、**偏度**、**峰度**、非数值数据（如缺失值）的计数（**No. Missing**），以及正负无穷大（**No. +∞**、**No.
    –∞**）。节点还会输出显示值分布的直方图，并且对于名义型列，会列出数据集中最常见和最少见的类别：
- en: '**Skewness** and **Kurtosis** are certainly the least known summary statistics
    among the ones mentioned above. However, they are useful in telling you quickly
    how much the shape of a distribution differs from the iconic bell-shaped curve
    of a pure Gaussian distribution. Skewness tells you about the symmetry of the
    distribution: if it has a positive value, it is skewed on the left while if it
    has a negative value, it is skewed on the right. Kurtosis tells you about the
    flatness of the distribution: if negative it is flatter than a bell curve, while
    if positive it shows a sharper peak.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏度**和**峰度**无疑是上述摘要统计中最不为人知的。然而，它们对于迅速告诉你分布的形状与纯高斯分布的典型钟形曲线有多大不同非常有用。偏度告诉你分布的对称性：如果它的值为正，则表示分布左偏；如果值为负，则表示分布右偏。峰度告诉你分布的平坦程度：如果为负值，则分布比钟形曲线更平坦；如果为正值，则分布的峰值更尖锐。'
- en: '![](img/B17125_05_03.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_03.png)'
- en: 'Figure 5.3: Configuration of Statistics: explore the data with its summary
    statistics'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：统计配置：通过其摘要统计探索数据
- en: 'Implement the **Statistics** node and connect it with the previous one. When
    configuring it, check the first box so we can have a look at the median values
    of the numeric columns. In the selector of the nominal values, keep only the string-typed
    columns (*Neighborhood*, *Property_type*, and *Floor_type*) plus *Elevator*. Although
    formally numeric, this latter column splits our samples into two categories, the
    properties equipped with the elevator and the ones missing it: it will be interesting
    to read a count of how many properties fall into each category, so we shall treat
    this column as nominal. If you run the node and display its main output (just
    press *Shift* + *10* or, after you execute the node, right-click on it and select
    **View: Statistics View**) you will obtain a window with three useful tabs. The
    first one (*Figure 5.4*) gives you all the highlights on the numeric columns:
    we learn that the average rent of the properties in our database is slightly above
    €1,000 and that the median floor surface is around 70 square meters. We also learn
    that there are no missing values: this is good news as we don''t need to engage
    in clean up chores:![A picture containing table'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现**Statistics**节点并将其与之前的节点连接。当配置时，勾选第一个框，以便我们可以查看数值列的中位数值。在名义值的选择器中，仅保留字符串类型的列（*Neighborhood*，*Property_type*，和*Floor_type*）以及*Elevator*。尽管这个列在形式上是数值型的，但它将我们的样本分为两类：配备电梯的房产和没有电梯的房产：读取每个类别中有多少房产将会很有意思，因此我们将把这个列视为名义列。如果你运行节点并显示其主要输出（只需按*Shift*
    + *10*，或者在执行节点后，右击它并选择**查看：统计视图**），你将获得一个包含三个有用标签的窗口。第一个标签（*图 5.4*）展示了数值列的所有重点：我们了解到，数据库中房产的平均租金略高于€1,000，中位数的楼层面积约为70平方米。我们还了解到没有缺失值：这对我们来说是好消息，因为我们无需进行清理工作：![A
    picture containing table
- en: Description automatically generated](img/B17125_05_04.png)
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_04.png)
- en: 'Figure 5.4: Numeric panel within the Statistics output: how are my numeric
    features distributed?'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.4：统计输出中的数值面板：我的数值特征如何分布？
- en: 'The second and third (*Figure 5.5*) tabs tell you about the nominal columns:
    we learn that some neighborhoods (such as `Magliana` and `Portuense`) are much
    less represented in our dataset than others. By looking at the values in the *Property_type*
    column, we also learn that the vast majority of our rented properties have been
    flats:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二和第三个（*图 5.5*）标签向你展示了名义列：我们了解到，某些社区（如`Magliana`和`Portuense`）在我们的数据集中比其他社区的表示要少得多。通过查看*Property_type*列的值，我们还了解到，我们租赁的绝大多数房产都是公寓：
- en: '![](img/B17125_05_05.png)'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17125_05_05.png)'
- en: 'Figure 5.5: Top/bottom panel within the Statistics output: check the values
    of your categorical columns'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.5：统计输出中的顶部/底部面板：检查你的类别列的值
- en: 'Now that we have explored the dataset and have become acquainted with the main
    characteristics of its columns, we can proceed with the fun part and design our
    model. To build a robust supervised machine learning model, we need to rely on
    the typical flow that we encountered in the previous chapter. Let''s refresh our
    memory on this critical point: in order to stay away from the trap of overfitting,
    we need to partition our labeled data into training and test sets, learn on the
    training set, predict on the test set, and—finally—assess the expected performance
    of the model by scoring the predicted values. You can go back to *Chapter 4*,
    *What is Machine Learning?*, and check *Figure 4.13* out to see once again the
    full process: we are always required to follow this approach when implementing
    a machine that can predict something useful. So, the very first step is to randomly
    partition all our labeled data rows into two separate subsets. This is exactly
    the "specialty" of our next node: **Partitioning**.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经浏览了数据集，并了解了各列的主要特征，我们可以继续进行有趣的部分，设计我们的模型。为了构建一个强健的监督学习模型，我们需要依赖上一章中遇到的典型流程。让我们在这一关键点上刷新记忆：为了避免过拟合的陷阱，我们需要将标记数据随机划分为训练集和测试集，在训练集上学习，在测试集上预测，并最终通过评分预测值来评估模型的预期性能。你可以回到*第4章*，*什么是机器学习？*，查看*图
    4.13*，再次了解完整的过程：在实现一个能够预测有用内容的机器时，我们总是需要遵循这一方法。因此，第一步是将所有标记数据行随机划分为两个独立的子集。这正是我们下一个节点的“专长”：**Partitioning**。
- en: '![](img/image007.png) *Partitioning*'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![](img/image007.png) *划分*'
- en: 'This node (**Manipulation > Row > Transform**) performs a row-wise split of
    the input table into two tables corresponding to the upper (first partition) and
    lower (second partition) output ports. The selector at the top of its configuration
    window (*Figure 5.6*) lets you set the size of the first partition (upper output
    port). You can either specify the number of rows to be included (**Absolute**)
    or the relative size of the partition in percentage points (**Relative[%]**).
    The second selector specifies the method used for splitting the rows into the
    two partitions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个节点（**Manipulation > Row > Transform**）执行输入表格的按行拆分，将其拆分为两个表格，分别对应上（第一部分）和下（第二部分）输出端口。其配置窗口顶部的选择器（*图
    5.6*）允许你设置第一部分（上输出端口）的大小。你可以指定要包含的行数（**绝对值**），或者指定该部分的相对大小（以百分比表示，**相对[%]**）。第二个选择器指定了将行拆分成两个部分时所使用的方法：
- en: '**Take from top**: if you select this option, the split will happen according
    to the current sorting order. The top rows of the input table will end up in the
    first partition while all others, after a certain threshold, will go to the second.
    The position of the threshold depends on the size of the partition that you have
    already decided above.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从顶部取**：如果选择此选项，拆分将按照当前排序顺序进行。输入表格的顶部行将进入第一部分，而所有其他行在超过某个阈值后将进入第二部分。阈值的位置取决于你在上面已决定的部分大小。'
- en: '**Linear sampling**: also, in this case, the order of the input table rows
    is preserved: every *n*^(th) row will go to an output port, alternating regularly
    across the two partitions. If, for instance, you run a linear sampling for creating
    two equally sized partitions (each having half of the original rows), you will
    end up with all the odd rows in a partition and all the even ones in the other.
    If, instead, the split is one-third and two-thirds, you will have every third
    row in the first partition and all others in the second one. This is particularly
    useful when your dataset is a time series, with records sorted chronologically.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性抽样**：在这种情况下，输入表格行的顺序得以保持：每隔 *n*^(行)将会进入一个输出端口，两个部分交替分配。如果，举例来说，你进行线性抽样，目的是创建两个大小相等的部分（每部分包含原始行数的一半），那么你将会得到所有奇数行在一个部分，所有偶数行在另一个部分。如果拆分是三分之一和三分之二，那么第一部分会有每三行中的一行，第二部分则包含所有其他行。如果你的数据集是时间序列，并且记录按时间顺序排列，这种方法特别有用。'
- en: '**Draw randomly**: if you go for this option, you obtain a random sampling.
    The only thing you can be sure of is that the number of rows in the first partition
    will be exactly what you have set in the first selector.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机抽取**：如果选择此选项，你将获得一个随机抽样。唯一可以确保的是，第一部分中的行数将恰好是你在第一个选择器中设置的行数。'
- en: '**Stratified sampling**: in this case, you also run a random sampling but,
    you force the distribution of a nominal column to be preserved in both output
    partitions. For example: if you have an input table describing 1,000 patients,
    out of which 90% are labeled as `negative` and 10% as `positive`, you can use
    stratified sampling to retain the ratio between positive and negative patients
    in each partition. In this case, if you want to have 700 rows to go to the first
    partition, you will end up with exactly 630 negative patients and 70 positive
    ones: the proportion is kept.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分层抽样**：在这种情况下，你同样执行随机抽样，但你强制保持一个名义列的分布在两个输出部分中。例如：如果你有一个描述1,000名患者的输入表格，其中90%标记为`negative`（阴性），10%标记为`positive`（阳性），你可以使用分层抽样来保持每个拆分部分中阳性与阴性患者的比例不变。在这种情况下，如果你希望将700行分配到第一部分，你将得到恰好630个阴性患者和70个阳性患者：比例保持不变。'
- en: 'If you have selected a splitting method based on a random selection (the last
    two options in the list above), you can protect the reproducibility of your workflow
    by ticking the **Use random seed** optional box. When you specify a constant number
    for initializing the random sampling, you are "fixing" the random behavior: as
    a result, you will always obtain the same partitions every time you execute the
    node. This is handy when you want to keep the partitioning constant as you go
    back and forth in the construction of your workflow or when you want other people
    to get the same partitioning on their machines:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择了基于随机选择的拆分方法（上面列表中的最后两个选项），你可以通过勾选**使用随机种子**选框来保护工作流的可重现性。当你指定一个常数值来初始化随机抽样时，你是在“固定”随机行为：结果是每次执行节点时，你都会得到相同的拆分。这在你希望在构建工作流时保持拆分一致，或者希望其他人在他们的机器上得到相同拆分时非常方便：
- en: '![Graphical user interface, text, application'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，文本，应用程序'
- en: Description automatically generated](img/B17125_05_06.png)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_06.png)
- en: 'Figure 5.6: Configuration dialog of Partitioning: how do you want to split
    your dataset?'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：分割配置对话框：你希望如何分割数据集？
- en: One thing that computers really struggle with is behaving randomly and doing
    anything "unexpected" as they are built and programmed to follow a deterministic
    set of steps. For this reason, computers leverage special algorithms for generating
    sequences of **pseudo-random numbers** that "look" as if they are truly random.
    Notably, the starting point of these sequences (the **random seed**) can determine
    the full progression of numbers. When needed, a computer can still generate a
    random seed by looking at a quickly changing state (like the number of clock cycles
    of the CPU from the last boot) or by measuring some microscopic physical quantities
    (like a voltage on a port) that are affected by uncontrollable phenomena, such
    as thermal noise and other quantic effects. It's interesting how computers struggle
    with what would take us just a flip of a coin!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机真正难以应对的一件事是表现得随机并执行任何“意外”的操作，因为它们是根据一套确定性的步骤来构建和编程的。因此，计算机利用特殊算法生成**伪随机数**序列，这些序列“看起来”就像是真正的随机数。值得注意的是，这些序列的起始点（**随机种子**）可以决定数字的完整进程。必要时，计算机仍然可以通过查看快速变化的状态（如从上次启动以来CPU时钟周期的数量）或通过测量一些微观物理量（如端口上的电压），这些量受到不可控现象的影响，如热噪声和其他量子效应，从而生成随机种子。很有趣的是，计算机在处理这些任务时会遇到我们只需抛个硬币就能完成的事情！
- en: 'Let''s start our supervised learning typical flow and split our full housing
    dataset into training and test subsets:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始典型的监督学习流程，并将完整的房屋数据集分割为训练集和测试集：
- en: Let's implement the **Partitioning** node and connect it with the output of
    the **Excel Reader** output (you can keep the **Statistics** node unhooked as
    we don't need to use its outputs). In the configuration dialog, let's make sure
    that we select the **Relative[%]** option with the value `70`. This means that,
    out of the 4,000 properties available at the inputs, 70% of them will be used
    for training (which is a fair thing to do since, as anticipated in *Chapter 4*,
    *What is Machine Learning?*, the training set should normally cover between 70%
    and 80% of the total full dataset). We want the partitioning to happen randomly.
    In the previous step, we noticed that some nominal columns (like *Neighborhood*)
    display an unbalanced distribution across their values. This means that we have
    the risk of having the very few properties in a smaller neighborhood (like the
    26 rows referring to `Magliana`) ending up solely in a partition. Although this
    is not strictly required, we better avoid any unbalance that can affect our learning
    and select **Stratified sampling** on *Neighborhood* in the dialog. You can also
    click on the bottom tick box and, on the right, type in a random seed, like `12345`,
    so that you can count on the same partitioning over and over. When you run the
    node, you find that in the upper output port (right-click on the node and select
    **First partition**) you find 2,800 rows that are exactly 70% of the original
    dataset. This is a good sign and we can move ahead with the learning step.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实现**分割**节点，并将其与**Excel 阅读器**的输出连接（你可以保持**统计**节点未连接，因为我们不需要使用它的输出）。在配置对话框中，我们确保选择**相对[%]**选项，值为`70`。这意味着，在输入的4,000个属性中，将有70%的数据用于训练（这是合理的做法，因为如*第4章*中所述，*什么是机器学习？*，训练集通常应涵盖总数据集的70%到80%）。我们希望分割是随机进行的。在上一步中，我们注意到一些名义列（如*Neighborhood*）在其值的分布上存在不平衡。这意味着，我们有可能在一个较小的邻里（如与`Magliana`相关的26行数据）中仅仅获得少量的属性，进而仅将这些数据分配到一个分区。虽然这并非严格要求，但我们最好避免任何可能影响学习的不平衡，因此在对话框中选择**分层抽样**，并对*Neighborhood*进行处理。你也可以勾选下方的复选框，并在右侧输入一个随机种子，如`12345`，这样你就可以确保每次都得到相同的分区。当你运行节点时，你会发现，在上方的输出端口（右键点击节点并选择**第一个分区**）中，你将找到2,800行数据，正好是原始数据集的70%。这是一个好兆头，我们可以继续进行学习步骤。
- en: At this point, we need to add the nodes (both learner and predictor) that implement
    the specific machine learning algorithm we want to use. The simplest algorithm
    for predicting numbers is **linear regression,** which is what we are going to
    use in this tutorial. It's worth introducing first the underlying mathematical
    model so that we can get ready to interpret its results.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们需要添加实现特定机器学习算法的节点（包括学习器和预测器）。预测数字的最简单算法是**线性回归**，这也是我们在本教程中使用的算法。值得先介绍一下其基础数学模型，以便我们为理解其结果做好准备。
- en: Linear regression algorithm
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归算法
- en: 'The linear regression model is a generalization of the simple regression we
    have used to predict second-hand car prices in *Chapter 4*, *What is Machine Learning?*.
    In that case, we modeled the price as a straight line, following the simple equation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型是我们在*第4章*《什么是机器学习？》中用来预测二手车价格的简单回归的推广。在那种情况下，我们将价格建模为一条直线，遵循简单的方程：
- en: '![](img/B17125_05_001.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_001.png)'
- en: 'where *y* was the dependent variable, so the target of our prediction (the
    price of the car), *x*, was the only independent variable (in that case, the age
    of the car in years) and ![](img/B17125_05_002.png) and ![](img/B17125_05_003.png)
    were the parameters of the model, defining the *height* of the line (also known
    as the *offset* or *intercept*) and its *slope*, respectively:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*y* 是因变量，因此是我们预测的目标（汽车的价格），*x* 是唯一的自变量（在那个例子中是汽车的年龄，单位为年），而 ![](img/B17125_05_002.png)
    和 ![](img/B17125_05_003.png) 是模型的参数，分别定义了直线的*高度*（也称为*偏移量*或*截距*）和*斜率*：
- en: '![](img/B17125_05_07.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_07.png)'
- en: 'Figure 5.7: Linear regression of car prices: the line shows the prediction
    as the age varies'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：汽车价格的线性回归：随着年龄变化，直线显示了预测值
- en: 'Specifically, as you can see in *Figure 5.7*, we have ![](img/B17125_05_004.png)
    (it''s where the model line encounters the vertical axis) and ![](img/B17125_05_005.png)
    so the price of the car is predicted through the simple model:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，正如你在*图 5.7*中看到的那样，我们有 ![](img/B17125_05_004.png)（它是模型线与纵轴相交的地方）和 ![](img/B17125_05_005.png)，因此汽车的价格可以通过这个简单模型预测：
- en: '![](img/B17125_05_006.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_006.png)'
- en: 'The price of a 2-year-old car will be estimated to be $12,600, since:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一辆2年车龄的汽车的价格将被估计为 $12,600，因为：
- en: '![](img/B17125_05_007.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_007.png)'
- en: The purpose of the *learner* algorithm of a simple linear regression is to find
    the right parameters (![](img/B17125_05_008.png) and ![](img/B17125_05_009.png))
    that minimize the error of a prediction, while the *predictor* algorithm will
    just apply the model on new numbers, like we did when we came up with the estimated
    price of a 2-year-old car.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归的*学习器*算法的目的是找到正确的参数（![](img/B17125_05_008.png) 和 ![](img/B17125_05_009.png)），以最小化预测误差，而*预测器*算法则会将模型应用于新的数据，就像我们在得出2年车龄汽车估计价格时所做的那样。
- en: 'Linear regression is a generalization of the simple model that we have just
    seen in action. Its underlying mathematical description is:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是我们刚刚看到的简单模型的推广。其基础数学描述为：
- en: '![](img/B17125_05_010.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_010.png)'
- en: 'where *y* is still the (single) target variable that we are trying to predict,
    the various *x*[i] values represent the (many) independent variables that correspond
    to the features we have available, and the ![](img/B17125_05_011.png) values are
    the parameters of the model that define its "shape." Since we have several independent
    variables this time (for this reason, we call it a **multivariate model**), we
    cannot "visualize" it any longer with a simple line on a 2D chart. Still, its
    underlying mathematical model is quite simple because it assumes that every feature
    is "linearly" connected with the target variable. Here you go: you have just met
    the multivariate linear regression model.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*y* 仍然是我们试图预测的（单一的）目标变量，各种 *x*[i] 值代表对应我们所拥有的特征的（多个）自变量，![](img/B17125_05_011.png)
    值是定义模型“形状”的参数。由于这次我们有多个自变量（因此我们称之为**多变量模型**），我们无法再通过简单的二维图表中的直线进行“可视化”。尽管如此，它的基础数学模型仍然相当简单，因为它假设每个特征与目标变量之间是“线性”关联的。好了，你刚刚遇到了多变量线性回归模型。
- en: 'If we apply this model to the prediction of the rental prices, our target variable
    is represented by the column *Rent* while the features (independent variables)
    are all the other columns, like *Rooms*, *Surface*, and so on. The multivariate
    linear regression model will look like:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这个模型应用于租金预测，我们的目标变量是*Rent*列，而特征（自变量）则是所有其他列，如*Rooms*、*Surface*等。多变量线性回归模型将会是：
- en: '![](img/B17125_05_012.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_012.png)'
- en: and the aim of the learner algorithm implementing this model will be to find
    the "best" values of ![](img/B17125_05_013.png), ![](img/B17125_05_014.png), ![](img/B17125_05_015.png),
    and so on that minimize the error produced on the training set.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 并且实现此模型的学习算法的目标是找到“最佳”值，如![](img/B17125_05_013.png)、![](img/B17125_05_014.png)、![](img/B17125_05_015.png)等，这些值能够最小化在训练集上产生的误差。
- en: 'There are ways to find analytically (meaning through a set of given formulas,
    nothing overly complex) the set of parameters ![](img/B17125_05_016.png) that
    minimize the error of a linear regression model. The simplest one is called **Ordinary
    Least Squares (OLS)**: it minimizes the sum of the squared errors of a linear
    regression. Do you remember the **Root Mean Squared Error** (**RMSE**) metric
    introduced in *Chapter 4*? By using the ordinary least squares procedure, we are
    going to minimize the RMSE, which is exactly what we need to do here.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一组给定的公式（并不复杂），我们可以分析性地找到能够最小化线性回归模型误差的参数集![](img/B17125_05_016.png)。最简单的方法称为**普通最小二乘法（OLS）**：它最小化线性回归的平方误差和。你还记得在*第4章*中介绍的**均方根误差**（**RMSE**）指标吗？通过使用普通最小二乘法程序，我们将最小化RMSE，这正是我们在这里需要做的。
- en: 'The model above expects every independent variable to be a number. So, how
    do we deal with the nominal features we have in our dataset like *Floor_type*?
    We can solve this apparent limitation with a common trick used in machine learning:
    creating the so-called **dummy variables**. The idea is very simple: we transform
    every nominal variable into multiple numerical variables. Let''s take the example
    of *Floor_type*: this is a categorical variable whose value can be either `Upper`,
    `Mezzanine`, or `Ground floor`. In this case we would replace this categorical
    variable by creating three numeric dummy variables: *Floor_type=Upper*, *Floor_type=Mezzanine*,
    and *Floor_type=Ground*. The dummy variables will take as values either `1` or
    `0`, depending on the category: for a given row, only one dummy variable will
    take `1` and all others will take `0`. For example, if a row refers to an `Upper`
    floor property, the dummy variable *Floor_type=Upper* will be `1` and the other
    two will be `0`.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的模型期望每个自变量都是一个数值。那么，我们如何处理数据集中像*Floor_type*这样的名义特征呢？我们可以通过机器学习中的一个常用技巧来解决这个显而易见的限制：创建所谓的**虚拟变量**。这个想法非常简单：我们将每个名义变量转换为多个数值变量。以*Floor_type*为例：这是一个类别变量，其值可以是`Upper`、`Mezzanine`或`Ground
    floor`。在这种情况下，我们将通过创建三个数值虚拟变量来替换这个类别变量：*Floor_type=Upper*、*Floor_type=Mezzanine*和*Floor_type=Ground*。虚拟变量的值可以是`1`或`0`，取决于类别：对于某一行数据，只有一个虚拟变量的值为`1`，其余的都为`0`。例如，如果某一行数据指的是`Upper`楼层的房产，那么虚拟变量*Floor_type=Upper*的值将为`1`，其他两个将为`0`。
- en: Thanks to this trick, we can apply a linear regression model on any categorical
    variables as well; we just need to "convert" them into multiple additional dummy
    variables.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 得益于这个技巧，我们可以将线性回归模型应用于任何类别变量；我们只需要将它们“转换”成多个额外的虚拟变量。
- en: We have all we need to give the linear regression model a try by introducing
    the KNIME node that implements its learning algorithm.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好通过引入实现其学习算法的KNIME节点，来尝试线性回归模型。
- en: '![A picture containing logo'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![一张包含logo的图片'
- en: Description automatically generated](img/image024.png) *Linear Regression Learner*
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/image024.png) *线性回归学习器*
- en: 'This node (**Analytics > Mining > Linear/Polynomial Regression**) trains a
    multivariate linear regression model for predicting a numeric quantity. For its
    configuration (see *Figure 5.8*) you will have to specify the numeric column to
    be predicted by picking it in the **Target** drop-down menu at the top:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个节点（**分析 > 挖掘 > 线性/多项式回归**）训练一个多变量线性回归模型，用于预测一个数值量。对于其配置（见*图5.8*），你需要在顶部的**目标**下拉菜单中选择要预测的数值列：
- en: '![Graphical user interface, application'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，应用程序'
- en: Description automatically generated](img/B17125_05_08.png)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_08.png)
- en: 'Figure 5.8: Configuration dialog of Linear Regression Learner: choose what
    to predict and the features to use'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：线性回归学习器的配置对话框：选择要预测的内容和要使用的特征
- en: 'Then, in the central box, you can select which columns should be used as features:
    only the columns that appear on the green box on the right will be considered
    as independent variables in the model. The nominal columns, such as strings, will
    be automatically converted by the node into dummy variables.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在中央框中，您可以选择哪些列应作为特征使用：只有右侧绿色框中出现的列才会被视为模型中的自变量。名义列，如字符串，将自动由节点转换为虚拟变量。
- en: 'If a nominal column (like *Type*) admits *N* unique values (like `A`, `B`,
    and `C`), this node will actually create not *N*, but *N-1* dummy variables (*Type=A*
    and *Type=B*). In fact, one of the nominal values can be covered by the combination
    of all zeros: in our case, if *Type* is `C`, both *Type=A* and *Type=B* will be
    zero, implying that the only possible value for that row is `C`. In this way,
    we make the model simpler and avoid the so-called dummy variable trap, which might
    make our model parameters impossible to calculate. The node takes care of this
    automatically, so you don''t have to worry about it: just keep this in mind when
    reading the model parameters related to dummy variables.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个名义列（如*类型*）具有*N*个唯一值（如`A`、`B`和`C`），那么该节点实际上会创建 *N-1* 个虚拟变量（*Type=A* 和 *Type=B*）。实际上，名义值中的一个可以通过所有零的组合来表示：在我们的例子中，如果*类型*是`C`，那么*Type=A*和*Type=B*都会是零，意味着该行唯一可能的值是`C`。这样，我们使模型变得更简单，并避免了所谓的虚拟变量陷阱，这可能导致我们的模型参数无法计算。该节点会自动处理这一点，因此您不必担心：只需要在阅读与虚拟变量相关的模型参数时记住这一点。
- en: By clicking on the **Predefined Offset Value** tick box, you can "force" the
    offset value of the linear regression model (we also called it ![](img/B17125_05_017.png)
    or intercept earlier) to a certain value or remove it, by setting it to zero.
    This reduces the "flexibility" of the model to minimize the error so it will reduce
    its accuracy. However, this trick might be helpful when you are trying to reduce
    the complexity of the model and improve its explain ability, as we have one less
    parameter to interpret. By default, this node will fail if there are some missing
    values in the input data. To manage this, you can either manage them earlier in
    the workflow, using the **Missing Value** node, or select the **Ignore rows with
    missing value** option at the bottom-left corner of the configuration dialog.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击**预定义偏移值**复选框，您可以“强制”将线性回归模型的偏移值（我们之前也叫它 ![](img/B17125_05_017.png) 或截距）设置为某个值，或者通过将其设置为零来移除它。这会减少模型最小化误差的“灵活性”，因此会降低其准确性。然而，当您试图降低模型复杂度并提高模型的可解释性时，这个技巧可能会有所帮助，因为我们少了一个参数需要解释。默认情况下，如果输入数据中存在缺失值，此节点将无法执行。为了处理这个问题，您可以提前在工作流中管理它们，使用**缺失值**节点，或者选择配置对话框左下角的**忽略缺失值的行**选项。
- en: 'Once executed, the node will return at its first output port the regression
    model, which can then be used by a predictor node for making predictions. The
    second output is a table (*Figure 5.9*) that contains a summary view of the regression
    model parameters, where for each variable (including the dummy ones) you can find:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 执行后，节点将在其第一个输出端口返回回归模型，然后可以通过预测节点用于预测。第二个输出是一个表格（*图 5.9*），其中包含回归模型参数的摘要视图，对于每个变量（包括虚拟变量），您可以找到：
- en: '**Coeff.**: this is the **parameter** (also called coefficient) of the variable.
    This is the ![](img/B17125_05_018.png) parameter we have seen in the regression
    model formula.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Coeff.**：这是变量的**参数**（也叫系数）。这是我们在回归模型公式中看到的 ![](img/B17125_05_018.png) 参数。'
- en: '**Std. Err.**: this is **the standard deviation of the error** expected for
    this parameter. If you compare it with the value of the parameter, you get a rough
    idea of how "precise" the estimation of that parameter can be. You can use it
    also to get a rough confidence interval for the given parameter as we did in *Chapter
    4*, *What is Machine Learning?*, when talking about RMSE. In the case of the car
    price regression, if the parameter for the variable *Age* is -1.7 and the standard
    error is 0.1, you can say that 95% of the time, the price of a car declines by
    $M 1.7 ± 0.2 (2 times the standard error) every year.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准误差**：这是该参数的**误差标准差**。如果将其与参数值进行比较，你可以大致了解该参数的估计值有多“精确”。你还可以用它来大致获取该参数的置信区间，正如我们在*第4章*《什么是机器学习？》中讨论RMSE时所做的那样。在汽车价格回归的例子中，如果*年龄*变量的参数是-1.7，标准误差是0.1，你可以说，95%的情况下，汽车的价格每年下降$M
    1.7 ± 0.2（2倍标准误差）。'
- en: '**t-value** and **P>|t|**: these are two summary statistics (**t-value** and
    **p-value**) generated by the application of the Student test, which clarifies
    how significant a variable is for the model. The smaller the p-value, the more
    confident you can be in rejecting the possibility that that parameter looks significant
    just "by chance" (it''s called **null hypothesis**). As a general rule of thumb,
    when the p-value (the last column in this table) is above 0.05, you should remove
    that variable from the model, as it is likely insignificant:![Table'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**t值**和**P>|t|**：这两个统计值（**t值**和**p值**）是通过应用学生t检验生成的，用于说明某个变量对于模型的重要性。p值越小，你越能有信心拒绝该参数仅仅是“偶然”显著的可能性（这被称为**零假设**）。作为一个通用规则，当p值（该表格的最后一列）大于0.05时，你应该从模型中删除该变量，因为它可能是不显著的：![表格'
- en: Description automatically generated](img/B17125_05_09.png)
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[自动生成的描述](img/B17125_05_09.png)'
- en: 'Figure 5.9: The summary output of the Linear Regression Learner node: find
    out what the parameters of the regression are and if they turn out significant
    or not'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.9：线性回归学习节点的摘要输出：找出回归的参数，并确定它们是否显著
- en: 'If you right-click on the node after it is executed, you can open an additional
    graphical view (select **View: Linear Regression Scatterplot View**) where you
    can visually compare the individual features against the target to look for steep
    slopes and other patterns.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在节点执行后右键点击，可以打开一个额外的图形视图（选择**视图：线性回归散点图视图**），在这里你可以直观地比较各个特征与目标之间的关系，寻找陡峭的斜率和其他模式。
- en: 'Let''s now put this node to work with our Rome properties and see what it''s
    got:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们用我们的罗马房产数据来实际操作这个节点，看看它能带来什么：
- en: Implement the **Linear Regression Learner** node and connect it with the upper
    output of the **Partitioning** node, which is the training set (a 70% random sample
    of the historical database of rents). In the configuration window, double-check
    that *Rent* is set as the **Target** variable on top. Feature-wise, at this point,
    we can keep all of them to see if they are significant or not. However, we can
    already remove one, *House_ID*, as we already know we don't want it to be used.
    We don't want to make use of the unique identifier of the property to infer the
    rental price. That number has been assigned artificially when the property was
    added to the database, and it is not connected with features of the property itself,
    so we don't want to consider it in a predictive model. Run the model and open
    the second output port to obtain the summary view of the model parameters.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现**线性回归学习者**节点，并将其与**分区**节点的上输出端口连接，该输出端口是训练集（来自历史租金数据库的70%随机样本）。在配置窗口中，再次检查*租金*是否被设置为顶部的**目标**变量。就特征而言，此时我们可以保留所有特征，看看哪些是显著的。然而，我们可以删除其中一个，*房屋ID*，因为我们已经知道不希望使用它。我们不希望用房产的唯一标识符来推断租金。该编号是在房产加入数据库时人为分配的，并且与房产本身的特征无关，因此我们不希望在预测模型中考虑它。运行模型并打开第二个输出端口以获取模型参数的摘要视图。
- en: 'This summary view will look similar to what is displayed in *Figure 5.9*, although
    the numbers could differ given that the random partitioning might have generated
    in your case different partitions: welcome to the world of probabilistic models!
    However, we can already notice that some parameters display a p-value (the last
    column of the table, **P>|t|**) higher than 0.05\. This means we can come back
    to this step later and do some cleaning and improve the performance of the model.
    For now, let''s proceed further so that we can make some predictions and score
    the model.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个总结视图将类似于*图5.9*中显示的内容，尽管数字可能不同，因为随机划分可能在你的案例中生成了不同的划分：欢迎来到概率模型的世界！然而，我们已经注意到，一些参数的p值（表格的最后一列，**P>|t|**）高于0.05。这意味着我们可以稍后返回此步骤，进行清理并提高模型的性能。目前，让我们继续进行预测并评估模型。
- en: '![Icon'
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![图标'
- en: Description automatically generated](img/image028.png) *Regression Predictor*
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/image028.png) *回归预测器*
- en: 'This node (**Analytics > Mining > Linear/Polynomial Regression**) applies a
    regression model (given as an input in the first blue port on the left) to a dataset
    (second port) and returns the result of the prediction for each input row. The
    node does not require any configuration and can be used in conjunction with either
    the **Linear Regression Learner** node, introduced above, or the **Polynomial
    Regression** **Learner** node: you can check this one out by yourself if you want
    to build linear regressions on different polynomial degrees as we did in *Chapter
    4*, *What is Machine Learning?* (have a look at *Figure 4.9*).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个节点（**分析 > 挖掘 > 线性/多项式回归**）应用回归模型（作为输入给出的第一个蓝色端口）到数据集（第二个端口），并返回每个输入行的预测结果。该节点不需要任何配置，可以与**线性回归学习器**节点或**多项式回归**
    **学习器**节点一起使用：如果你想像我们在*第4章*《什么是机器学习？》中那样在不同多项式度数下构建线性回归，可以自行查看该节点（参见*图4.9*）。
- en: 'Let''s add the **Regression Predictor** node to the workflow and make the connections:
    link the blue square output of the **Linear Regression Learner** to the upper
    input port of the predictor and connect the bottom output port of the **Partitioning**
    (the test set) to the second input port. No configuration is needed so you can
    execute the node and look at the output, which is similar to what you find in
    *Figure 5.10*:![Table'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将**回归预测器**节点添加到工作流中并建立连接：将**线性回归学习器**的蓝色方形输出连接到预测器的上输入端口，并将**划分**（测试集）的底部输出端口连接到第二个输入端口。无需配置，因此你可以执行节点并查看输出，输出结果与*图5.10*中的类似：![表格
- en: Description automatically generated](img/B17125_05_10.png)
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_10.png)
- en: 'Figure 5.10: Output of the Regression Predictor node: we finally have a prediction
    of the rental price.'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.10：回归预测器节点的输出：我们最终得到了租金价格的预测。
- en: 'You can look with pride at the last column on the right, called *Prediction
    (Rent)*: for each row in the test set (which has not been "seen" by the learner
    node) the node has generated a prediction of the rent. This prediction was obtained
    by just "applying" the parameters of the regression model to the values of the
    rows in the test set. Let''s see how this works with an example: consider the
    parameters in *Figure 5.9*. In this case the intercept (last row) is 569.9, the
    parameter of *Rooms* is around 25.8, the one for *Surface* is 9.6, the parameter
    of the dummy variable associated with the `Collatino` neighborhood (*Neighborhood=Collatino*)
    is -561.9, and so on. When the predictor had to come up with a prediction for
    the first row in the test set (see the first line in *Figure 5.10*), it had to
    just apply the formula of the regression model, with the parameters found by the
    learner, to this property (with 3 rooms, 80 square meters, based in `Collatino`,
    and so on). Hence, the resulting calculation for the **Regression Predictor**
    node is:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以自豪地查看右侧的最后一列，称为*预测（租金）*：对于测试集中的每一行（学习器节点未“见过”的行），该节点生成了租金的预测。这个预测是通过将回归模型的参数直接“应用”到测试集中的每一行的值上获得的。让我们通过一个例子来看这个是如何工作的：考虑*图5.9*中的参数。在这种情况下，截距（最后一行）为569.9，*房间数*的参数约为25.8，*面积*的参数为9.6，与`Collatino`社区相关的虚拟变量（*Neighborhood=Collatino*）的参数为-561.9，依此类推。当预测器需要为测试集中的第一行（见*图5.10*中的第一行）生成预测时，它只需将学习器找到的回归模型公式应用到该属性（具有3个房间，80平方米，位于`Collatino`等）中。因此，**回归预测器**节点的结果计算如下：
- en: '![](img/B17125_05_019.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_019.png)'
- en: 'In this specific case, if you add all the other features that were not reported
    in the preceding formula, we come up with a final prediction of €896.4, making
    around €50 of error versus the actual rental price, which we know is €950: not
    bad for our first prediction! To have a complete view of the performance of the
    current model, we would need to check the difference between predicted and real
    rents for all rows in the test set, using the **Numeric Scorer** node.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特定情况下，如果加上所有未在前述公式中报告的其他特征，我们得出的最终预测为€896.4，与实际租金价格€950相比，误差约为€50：对我们的第一个预测来说，这并不差！要完整了解当前模型的性能，我们需要检查测试集中所有行的预测和实际租金之间的差异，使用**数值评分器**节点。
- en: '![A picture containing text'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![包含文本的图片'
- en: Description automatically generated](img/image032.png) *Numeric Scorer*
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/image032.png) *数值评分器*
- en: 'This node (**Analytics > Mining > Scoring**) calculates the summary performance
    metrics of a regression by comparing two numeric columns. Its only required configuration
    (*Figure 5.11*) is the selection of the two columns to be compared: you can select
    the target column of the regression, containing the actual values, in the **Reference
    column** dropdown, and the predictions in the next one, labeled as **Predicted
    column**. If you want to output the performance scores as variables as well (this
    is useful when doing hyperparameter optimization), you need to tick the **Output
    scores as flow variables** box at the bottom. The node outputs the most popular
    scoring metrics of a regression, including the **Coefficient of Determination**,
    **R**², and the **RMSE**:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此节点（**分析 > 挖掘 > 评分**）通过比较两个数值列来计算回归的摘要性能指标。它唯一需要的配置（*图 5.11*）是选择要比较的两列：您可以在**参考列**下拉菜单中选择回归的目标列，其中包含实际值，然后在下一个标记为**预测列**的列中选择预测值。如果您还想将性能评分作为流变量输出（在进行超参数优化时很有用），则需要勾选底部的**将分数作为流变量输出**框。该节点输出回归的最流行评分指标，包括**确定系数**、**R**²
    和 **RMSE**：
- en: '![Graphical user interface, text, application, email'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，文本，应用程序，电子邮件'
- en: Description automatically generated](img/B17125_05_11.png)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_11.png)
- en: 'Figure 5.11: Configuration dialog of the Numeric Scorer node: select the columns
    to compare.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：数值评分器节点的配置对话框：选择要比较的列。
- en: 'Implement the **Numeric Scorer** node (watch out: don''t get confused with
    the **Scorer** node, which is used for classifications) and connect the output
    of the **Regression Predictor** with its input port. For its configuration, just
    double-check that you have *Rent* and *Prediction (Rent)* in the drop-down menus
    at the top and run the node. Its output (*Figure 5.12*) is very encouraging (of
    course, you can get slightly different results from what you find in these figures
    and that''s normal):![Table'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现**数值评分器**节点（注意：不要与用于分类的**评分器**节点混淆），并将**回归预测器**的输出连接到其输入端口。在其配置中，请仔细检查顶部下拉菜单中是否包含*租金*和*预测（租金）*，然后运行节点。其输出（*图
    5.12*）非常令人鼓舞（当然，您可能会得到与这些图中稍有不同的结果，这是正常的）：![表
- en: Description automatically generated](img/B17125_05_12.png)
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_12.png)
- en: 'Figure 5.12: Performance metrics as returned by the Numeric Scorer node:not
    bad for your first regression'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.12：作为数值评分器节点返回的性能指标：作为您的第一个回归不错
- en: 'We obtained an R² of 0.92, which means that our current model accounts for
    around 92% of the full variability of rental prices in Rome. Considering the limited
    sample and the few features available, this looks quite good already. Also, the
    RMSE is €110, which means that 68% of the time (one standard deviation) we will
    make a prediction error that is, in absolute terms, below €110, and 95% of the
    time our error will be below €220 (two times the RMSE). The last performance metric,
    **Mean Absolute Percentage Error** (**MAPE**) tells us that, on average, our predicted
    rent will differ from the actual rent by around 10%: again, not bad at all.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个R²值为0.92，这意味着我们当前的模型解释了罗马租金价格的大约92%的完全变异性。考虑到样本有限和可用特征较少，这看起来已经相当不错了。此外，RMSE为€110，这意味着我们将在68%的时间内（一个标准偏差）做出预测误差，绝对值低于€110，而在95%的时间内，我们的误差将低于€220（RMSE的两倍）。最后一个性能指标，**平均绝对百分比误差**（**MAPE**），告诉我们，我们的预测租金与实际租金的平均差异约为10%：再次非常不错。
- en: 'Still, we strive for the best and question ourselves if we can do anything
    to improve the model. The simplest thing to do will be to consider whether we
    can improve the selection of features. Let''s go back and have a look at the parameters
    obtained by the regression (*Figure 5.9*) and if we can remove some unneeded (or
    damaging) features. When we remove excess features from a model, we obtain at
    least two advantages: first, we make the model simpler and more explanatory to
    other human beings, as we have fewer parameters to explain. Secondly, we reduce
    the possibility for the model to overfit on the training set and, so, we increase
    its general robustness.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们依然追求最优，并不断质疑自己是否能做些什么来改进模型。最简单的方法是考虑是否可以改善特征的选择。让我们回顾一下回归分析得到的参数（*图5.9*），看看是否能移除一些不必要（或有害）的特征。当我们从模型中移除多余的特征时，至少会带来两个好处：首先，我们让模型更简洁，便于其他人理解，因为我们需要解释的参数变少了。其次，我们减少了模型在训练集上的过拟合可能性，从而增强了模型的泛化能力。
- en: 'Another reason for removing features is to avoid the risk of **multicollinearity**,
    which happens when features are correlated with each other. Correlated features
    are redundant: they can produce degradation of the predictive performance of your
    model and should be removed. The **Linear Correlation** node can help you calculate
    the correlation across all pairs of numeric columns in a table. As an alternative,
    you can use the **Variance Inflation Filter** (**VIF**) component, available in
    the KNIME Hub: as a rule of thumb, all variables showing a VIF higher than 5 should
    be removed.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 移除特征的另一个原因是避免**多重共线性**的风险，这种情况发生在特征之间相互关联时。相关的特征是冗余的：它们会降低模型的预测性能，应当被移除。**线性相关性**节点可以帮助你计算表格中所有数字列对之间的相关性。作为替代方案，你可以使用KNIME
    Hub中的**方差膨胀因子滤波器**（**VIF**）组件：根据经验法则，所有VIF值大于5的变量应该被移除。
- en: 'Let''s have a look at the p-values (last column of the table) and see if we
    can unveil some opportunities. Remember, the higher they are, the less meaningful
    their associated features proved to be. For sure we notice that the feature *Elevator*
    should be removed: its p-value is way above the thumb-rule threshold of 0.05 so
    we can go ahead and remove it. Also, the variable *Property_type* shall be removed:
    the p-values of their dummy variables are high, with the exception of *Property_type=Penthouse*
    (indicating that `Penthouse` is the only type that seems to be significant in
    affecting the value of the rent). Still, considering how few penthouses we have
    in the dataset, it''s worth removing this feature and further simplifying the
    model. Let''s give this simplification a try and see what happens:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看p值（表格的最后一列），看看是否能揭示一些机会。记住，p值越高，相关特征的意义就越小。我们可以明显看到特征*Elevator*应该被移除：它的p值远高于0.05的规则阈值，因此我们可以直接将其移除。同时，变量*Property_type*也应该被移除：它的虚拟变量的p值较高，唯一例外是*Property_type=Penthouse*（表明`Penthouse`是唯一一个似乎对租金有显著影响的类型）。不过，考虑到数据集中顶层公寓的数量非常少，还是值得移除这个特征，进一步简化模型。让我们尝试一下这种简化，看看会发生什么：
- en: Open the configuration dialog of the **Linear Regression Learner** node and
    move *Elevator* and *Property_type* to the left box of the column selector, so
    as to remove them as features of the model.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开**线性回归学习器**节点的配置对话框，将*Elevator*和*Property_type*移到列选择器的左侧框中，以便将它们移除作为模型的特征。
- en: 'Now let''s run the full model and see if something changed. To do so, it will
    be enough to execute the **Numeric Scorer** node: all previous nodes will be forced
    to run as well.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来运行完整模型，看看是否有变化。为此，只需执行**Numeric Scorer**节点：之前的所有节点将被强制一起运行。
- en: 'By removing these two features (see the updated results in *Figure 5.13*),
    we managed to keep the same performance levels, proving that they were unneeded.
    Actually, the performance has marginally increased (notice the lower RMSE), probably
    showing that we were slightly overfitting because of these uninformative variables.
    Additionally, we simplified the model, making it simpler to explain. Now we can
    predict the rental price of a property in Rome by knowing only the neighborhood,
    the number of rooms, the surface, and its floor (number and type):'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过移除这两个特征（见*图5.13*中的更新结果），我们成功保持了相同的性能水平，证明它们是不必要的。实际上，性能略微提高了（注意到RMSE更低），这可能表明由于这些无信息的变量，我们稍微出现了过拟合现象。此外，我们简化了模型，使得模型更容易解释。现在，只需要知道一个房产在罗马的所在区域、房间数、面积和楼层（编号及类型），就能预测它的租金。
- en: '![Graphical user interface, table'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图形用户界面，表格'
- en: Description automatically generated](img/B17125_05_13.png)
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_13.png)
- en: 'Figure 5.13: Updated parameters and performance scores after the removal of
    two features: every little helps'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.13：移除两个特征后的更新参数和性能评分：每一点改进都很有帮助
- en: These last two steps have shown us the value of selecting features wisely. As
    anticipated in *Chapter 4*, *What is Machine Learning?*, feature selection is
    an important practice in machine learning, indeed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后的两步展示了我们明智选择特征的价值。正如*第4章*《什么是机器学习？》中所预测的那样，特征选择在机器学习中是一个重要的实践，的确如此。
- en: In this case, we applied feature selection "by hand," checking the parameters
    manually and selecting the least meaningful ones. There are more systemic and
    semi-automated techniques to find out the best subset of features to use in a
    machine learning model. If you are curious, check the KNIME nodes for **Feature
    Selection** loops and have a look at the sample workflow available on the KNIME
    Hub called **Performing a Forward Feature Selection**.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们是“手动”进行特征选择的，通过手动检查参数并选择最不重要的那些。实际上，还有一些系统化和半自动化的技术来找出最适合用于机器学习模型的特征子集。如果你感兴趣，可以查看KNIME中的**特征选择**节点，并查看KNIME
    Hub上名为**执行前向特征选择**的示例工作流。
- en: 'Before concluding, we need to do one last thing: it''s time to apply our model
    to the 10 incoming properties for which the rental price is not available yet.
    This will be a way to illustrate our findings to the owner of the company. It
    will also be an opportunity for us to understand how predictive models are used
    in real life after they are built. In fact, once models are constructed (and validated
    against overfitting, as we did through the partitioning in training and test sets,
    and so on) they are **operationalized** in a way that they can be applied to future
    samples (in this case, the 10 new properties) whenever a prediction is needed.
    Let''s see this in action with our properties:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束之前，我们需要做最后一件事：是时候将我们的模型应用于10个尚未提供租金价格的待租房产了。这将是向公司所有者展示我们发现的一个方式，也为我们提供了一个机会来理解在构建完模型后，如何在实际生活中使用预测模型。事实上，一旦模型构建完成（并且经过过拟合验证，如我们通过在训练集和测试集上进行划分所做的那样），它们会被**投入使用**，以便在需要进行预测时，可以将其应用于未来的样本（在本例中，即10个新房产）。让我们通过这些房产来看一下实际操作：
- en: Load the Excel file with the new properties *(*`RomeHousing-NewProperties.xlsx`*)
    by dragging and dropping it into your workflow or implementing an* **Excel Reader**
    *node*. Once executed, you will find a short table that has exactly the same columns
    as the historical database, but—of course—lacks the *Rent* value.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过拖拽或实现一个**Excel读取器**节点，加载包含新房产的Excel文件*（`RomeHousing-NewProperties.xlsx`）*。执行后，你会看到一个简短的表格，其列与历史数据库完全相同，但——当然——缺少*租金*值。
- en: Implement a new **Regression Predictor** node (or copy/paste the existing one)
    and connect it as displayed in *Figure 5.14*. You should link the output of the
    **Linear Regression Learner** (yes—we are going to reuse the model we learned
    earlier) to the first input of the predictor. Then connect the **Excel Reader**
    output (the 10 new properties) to the second input of the predictor. You can now
    execute the node and have a look at the output:![](img/B17125_05_14.png)
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个新的**回归预测器**节点（或者复制/粘贴现有的节点），并按照*图 5.14*中的显示方式连接它。你应该将**线性回归学习器**的输出（是的，我们将重用之前学到的模型）连接到预测器的第一个输入。然后，将**Excel读取器**的输出（10个新房产）连接到预测器的第二个输入。现在，你可以执行该节点并查看输出：![](img/B17125_05_14.png)
- en: 'Figure 5.14: Full workflow for the Rome rent prediction'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14：罗马租金预测的完整工作流
- en: 'At this point, you have all you need to go back to the owner of the real estate
    with your output table (which will have a similar format to what you find in *Figure
    5.15*) and wait impatiently for her reaction, which turns out to be very positive!
    She loves it, as she finds that the estimates make, at least at a first glance,
    a lot of sense:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经具备了所有必要的条件，可以将输出表格（格式与*图 5.15*中类似）带回给房地产所有者，并焦急地等待她的反应，结果反应非常积极！她很喜欢，因为她发现这些估算结果至少从表面看起来非常有道理：
- en: '![Table'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![表格'
- en: Description automatically generated](img/B17125_05_15.png)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_15.png)
- en: 'Figure 5.15: The predicted rental prices on the new properties: do you fancy
    a 145 square meter flat near Piazza Navona at this price?'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15：新房产的预测租金：你喜欢在这个价格下租一套位于纳沃纳广场附近的145平方米公寓吗？
- en: 'The understandable initial stress turns quickly to a broad sense of enthusiasm.
    The model you created responds to the initial business objectives. In fact:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 初步的紧张感很快转变为一种广泛的热情。你创建的模型回应了最初的商业目标。事实上：
- en: The interpretation of the parameters of the model tells us something quite useful
    about the price formation mechanisms. For instance, you have found that the presence
    of the elevators and the type of flat doesn't count as much as the surface, the
    number of rooms, the floors, and, very importantly, the neighborhood to which
    the property belongs. By looking at the parameters of the neighborhood dummy variables
    (*Figure 5.13*), you find out what additional value each neighborhood brings (of
    course to be added to the rest of the components of your regression). For instance,
    Piazza Navona is by far the most expensive area while Castelli Romani seems to
    offer (at parity of characteristics) the most accessible rent.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数的解释告诉我们一些关于价格形成机制的非常有用的信息。例如，你发现电梯的存在和公寓的类型并不像面积、房间数量、楼层以及非常重要的，物业所属的邻里那样重要。通过查看邻里虚拟变量的参数（*图
    5.13*），你可以发现每个邻里所带来的额外价值（当然，这些价值需要加到回归模型的其他组成部分上）。例如，纳沃纳广场是迄今为止最贵的地区，而卡斯特尔利·罗马尼则似乎提供（在相同特征下）最具可及性的租金。
- en: On top of this, you now have a simple approach to quickly generate a recommendation
    of what fair rent looks like, which could be the basis for the discussion with
    the prospective landlord when fixing the rental price. By having a data-based
    number to start from, the agents can aim at a smoother negotiation session, which
    will more likely end up with a quicker and more profitable matching of demand
    and offer in the housing market.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，你现在有了一个简单的方法，可以快速生成一个公平租金的推荐，这可能成为与潜在房东讨论租金时的基础。通过拥有一个基于数据的起始数字，代理商可以更顺利地进行谈判，最终更有可能实现需求和供应在住房市场上更快、更有利的匹配。
- en: 'Congratulations on completing your first regression model! It''s now time to
    move on and challenge ourselves with a different undertaking: anticipating consumers''
    behavior.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了第一个回归模型！现在是时候向前迈进，挑战自己去做一个不同的任务：预测消费者行为。
- en: Anticipating preferences with classification
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用分类预测偏好
- en: 'In this tutorial, you will step into the role of a marketing analyst working
    for a mid-sized national consumer bank, offering services such as accounts, personal
    loans, and mortgages to around 300,000 customers in the country. The bank is currently
    trying to launch a new type of low-cost savings account, providing essential services
    and a pre-paid card that can be fully managed online. The product manager of this
    new account is not very pleased with how things are going and invites you to join
    a review meeting. You can see he is tense as he presents the outcome of a pilot
    telemarketing campaign run to support the launch. As part of this pilot, 10,000
    people were randomly selected among the full bank customer base and were phoned
    by an outbound call center. The outcome was apparently not so bad: 1,870 of the
    contacted customers (19% of the total) signed up for a new account. However, the
    calculation of the **Return On Investment** (**ROI**) pulled the entire audience
    back to the unsettling reality. The average cost of attempting to contact a customer
    through a call center is $15 per person while the incremental revenue resulting
    from a confirmed sale is estimated to be, on average, $60\. The math is simple:
    the pilot telemarketing campaign cost $150,000 and generated revenues amounting
    only to $112,200, implying a net loss of $37,800\. Now it is clear why the product
    manager looked disappointed: repeating the same campaign on more customers would
    be financially devastating.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将扮演一位中型全国性消费者银行的市场分析师，负责为大约30万名客户提供账户、个人贷款和抵押贷款等服务。该银行目前正在尝试推出一种新的低成本储蓄账户，提供基本服务并配备可以完全在线管理的预付卡。这个新账户的产品经理对目前的情况并不满意，并邀请你参加一次评审会议。从他紧张的表现来看，显然他对试点电话营销活动的结果并不满意。作为这个试点的一部分，从整个银行客户群中随机选取了1万名客户，并通过外呼中心进行了电话联系。结果似乎还不错：1,870名被联系的客户（占总数的19%）签约开设了新账户。然而，**投资回报率**（**ROI**）的计算让全场观众回到了令人不安的现实。通过呼叫中心联系客户的平均成本为每人15美元，而确认销售所带来的增量收入平均为60美元。算式很简单：试点电话营销活动花费了15万美元，产生的收入仅为112,200美元，意味着净损失37,800美元。现在大家都明白为什么产品经理看起来失望：如果再对更多客户重复这个活动，结果将是财务上的灾难。
- en: You timidly raise your hand and ask whether the outcomes of the pilot calls
    could be used to rethink the campaign target and improve the ROI of the marketing
    efforts. You explain that some machine learning algorithms might be able to predict
    whether a customer is willing or not to buy a product by learning from previous
    examples. As it normally happens in these cases, you instantly earn the opportunity
    to try what you suggested, and your manager asks you to put together a proposal
    on an ML way to support the launch of the new savings account.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你腼腆地举手问是否可以使用试点电话营销的结果来重新思考活动目标，并提高市场营销工作的投资回报率（ROI）。你解释说，一些机器学习算法可能能够通过学习之前的案例预测客户是否愿意购买产品。像往常一样，你立即获得了尝试你所建议的机会，经理要求你提出一个支持新储蓄账户推出的机器学习方案。
- en: 'You have mixed feelings about what just happened: on one hand, you are wondering
    whether you were a bit too quick in sharing the idea. On the other hand, you are
    very excited as you get to try leveraging algorithms to impact the business on
    such an important case. You are impatient to start and ask for all the available
    information related to the customers that were involved in the pilot. The file
    you receive (`BankTelemarketing.csv`) contains the following columns:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你对刚才发生的事情感到复杂的心情：一方面，你在想自己是不是分享想法有点太急了；另一方面，你也很兴奋，因为你将有机会尝试利用算法来影响这样一个重要的商业案例。你迫不及待想要开始，并请求所有与参与试点的客户相关的信息。你收到的文件（`BankTelemarketing.csv`）包含以下几列：
- en: '*Age*: the age of the customer.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Age*：客户的年龄。'
- en: '*Job*: a string describing the job family of the customer, like `blue-collar`,
    `management`, `student`, `unemployed`, and `retired`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Job*：描述客户工作类别的字符串，例如 `蓝领`、`管理`、`学生`、`失业` 和 `退休`。'
- en: '*Marital*: the marital status, which could be `married`, `single`, `divorced`,
    or `unknown`.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Marital*：婚姻状况，可以是 `已婚`、`单身`、`离婚` 或 `未知`。'
- en: '*Education*: the highest education level reached to date by the customer, ranging
    from `illiterate` and `basic.4y` (4 years of basic education in total) to `university.degree`.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Education*：客户目前达到的最高教育水平，范围从 `文盲` 和 `基础教育4年`（总共4年基础教育）到 `大学学位`。'
- en: '*Default*: this tells us whether we know that the customer has defaulted due
    to extended payment delinquency or not. Only a few customers end up being marked
    as defaulted (`yes`): most of them either show a good rating history (`no`) or
    do not have enough history to be assigned in a category (`unknown`).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*违约*：这告诉我们客户是否因为长期未支付款项而违约。只有少数客户最终会被标记为违约（`yes`）：大多数客户要么有良好的信用记录（`no`），要么没有足够的历史记录，无法归类为某个类别（`unknown`）。'
- en: '*Mortgage* and *Loan*: tells us whether the user has ever requested a housing
    mortgage or a personal loan, respectively.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*抵押贷款* 和 *贷款*：分别告诉我们用户是否曾申请过住房抵押贷款或个人贷款。'
- en: '*Contact*: indicates if the telephone number provided as a preferred contact
    method is a `landline` or a `mobile` `phone`.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*联系方式*：指示提供的首选联系电话是`固定电话`还是`手机`。'
- en: '*Outcome*: a string recording the result of the call center contact during
    the pilot campaign. It can be `yes` or `no`, depending on whether the customer
    opened the new savings account or decided to decline the offer.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结果*：记录试点活动中呼叫中心联系结果的字符串。它可以是`yes`或`no`，具体取决于客户是否开设了新储蓄账户，或者是否决定拒绝此项优惠。'
- en: 'Before you get cracking, you have a chat with the product manager to get clear
    on what would be the most valuable outputs for the business given the situation:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，你需要和产品经理进行一次沟通，以明确在当前情况下，什么样的输出对业务最有价值：
- en: First of all, it would be very useful to understand and document what characteristics
    make a customer most likely to buy the new banking product. Given its novelty,
    it is not clear yet who will find its proposition particularly appealing. Having
    some more clues on this aspect can help to build more tailored campaigns, personalize
    their content, and—by doing so—transfer the learnings from the call center pilot
    to other types of media touchpoints.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，了解并记录哪些特征使得客户最有可能购买新银行产品是非常有用的。鉴于该产品的创新性，目前还不清楚哪些人群会特别看中它的提议。了解更多关于这一点的线索有助于构建更具针对性的营销活动，个性化其内容，并且——通过这样做——将呼叫中心试点的经验教训转移到其他媒体接触点。
- en: 'Given that the pilot covered only a relatively small subset of customers—around
    3% of the total—it would be useful to identify "who else" to call within the other
    97% to maximize the ROI of the marketing initiative. In fact, we can assume that
    the same features we found in our pilot dataset—such as age, job, marital status,
    and so on—are available for the entire customer database. If we were able to *score*
    the remaining customers in terms of their *propensity* to buy the product, we
    would be focusing our efforts on the most inclined ones and greatly improving
    the campaign''s effectiveness. In other words, we should create a **propensity
    model** that will score current (and future) customers to enable a better marketing
    targeting. We will use the propensity scores to "limit" the next marketing efforts
    to a selected subset of the total customer base where the percentage of people
    in the new product is higher than 19% (as it was in our pilot): by doing so, we
    would increase the ROI of our marketing efforts.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到试点只覆盖了一个相对较小的客户子集——约占总数的3%——识别“其他”97%的客户进行联系，以最大化营销投资回报率是非常有用的。事实上，我们可以假设，在我们的试点数据集中找到的相同特征——例如年龄、职业、婚姻状况等——也适用于整个客户数据库。如果我们能够根据客户购买产品的*倾向*为剩余客户进行`评分`，那么我们就能将精力集中在最有可能的客户身上，从而大大提高营销活动的效果。换句话说，我们应该建立一个**倾向模型**，对当前（以及未来）客户进行评分，以便更好地进行市场营销定位。我们将使用倾向得分来“限制”下一步营销活动的范围，选择出新产品的客户比例高于19%的客户子集（就像我们的试点一样）：通过这样做，我们可以提高营销工作的投资回报率。
- en: 'From a machine learning standpoint, you need to create a machine able to predict
    whether a consumer will buy or will not open a savings account before you make
    the call. This is still a clear case of supervised learning, since you aim at
    predicting something based on previous examples (the pilot calls). In contrast
    with the Rome real estate case, where we had to predict a number (the rental price)
    using *regression* algorithms, here we need to predict the value of the categorical
    column *Outcome*. We will then need to implement *classification* algorithms,
    such as decision trees and random forest, which we are going to meet shortly.
    We are clear on the business need, the available data, and the type of machine
    learning route we want to take: we have all we need to start getting serious about
    this challenge. After creating a new workflow in KNIME, we load the data into
    it:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从机器学习的角度来看，在拨打电话之前，你需要创建一个能预测消费者是否会购买或是否会开设储蓄账户的机器。这仍然是一个典型的监督学习案例，因为你是基于先前的示例（试点电话）来预测某些内容。与罗马房地产案例不同，后者是用*回归*算法预测一个数字（租金价格），在这里，我们需要预测分类列*Outcome*的值。因此，我们需要实现*分类*算法，如决策树和随机森林，我们将很快遇到这些方法。我们已经清楚了业务需求、可用数据以及我们希望采取的机器学习路线：我们具备了开始认真对待这个挑战的所有条件。在
    KNIME 中创建一个新的工作流后，我们将数据加载进去：
- en: Drag and drop the file `BankTelemarketing.csv` onto the blank workflow. After
    the **CSV Reader** node dialog appears, we can quickly check that all is in order
    and close the window by clicking on **OK**. Once executed, the output of the node
    (*Figure 5.16*) confirms that our dataset is ready to go:![Table
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件`BankTelemarketing.csv`拖放到空白工作流中。弹出**CSV Reader**节点对话框后，我们可以快速检查一切正常，然后通过点击**OK**按钮关闭窗口。执行后，节点的输出（*图
    5.16*）确认我们的数据集已经准备好：![表格
- en: Description automatically generated](img/B17125_05_16.png)
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_16.png)
- en: 'Figure 5.16: The pilot campaign data: 10,000 customers through 8 features and
    for which we know the outcome of their call center contact'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.16：试点活动数据：10,000 个客户通过 8 个特征，且我们知道他们与呼叫中心联系的结果
- en: 'As usual, we implement the node **Statistics**, to explore the characteristics
    of our dataset. After confirming its default configuration, we check the **Top/bottom**
    tab of its main view (press *F10* or right-click and select **View: Statistics
    View** to open it). It seems that there are no missing values and that all seems
    to be in line with what we knew about the pilot campaign: the *Outcome* column
    shows 1,870 rows with `yes`, which is what the product manager managed in his
    presentation. We also notice that the *Default* column has only one row referring
    to a defaulted customer. This column might still be useful as it differentiates
    between customers who never defaulted and ones we don''t have any certainty about,
    so we decide to keep it and move on:![Text'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按惯例，我们实现了**统计**节点，来探索数据集的特征。确认其默认配置后，我们检查其主视图的**Top/bottom**选项卡（按*F10*或右键点击选择**视图：统计视图**以打开）。似乎没有缺失值，所有数据都符合我们对试点活动的预期：*Outcome*列显示
    1,870 行标注为`yes`，这正是产品经理在其演示中展示的内容。我们还注意到，*Default*列仅有一行表示一个违约客户。这个列仍然可能有用，因为它区分了从未违约的客户和我们不确定的客户，所以我们决定保留它并继续：![文本
- en: Description automatically generated](img/B17125_05_17.png)
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_17.png)
- en: 'Figure 5.17: The Top/bottom output of the Statistics node: only one person
    in this sample defaulted—good for everyone!'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.17：统计节点的Top/bottom输出：该样本中仅有一个人违约——对大家有利！
- en: 'Since we are in the supervised learning scenario, we need to implement the
    usual partitioning/learn/predict/score structure in order to validate against
    the risk of overfitting. We start by adding the **Partitioning** node and connecting
    it downstream to the **CSV Reader** node. In its configuration dialog, we leave
    the **Relative** 70% size for the training partition and we decide to protect
    the distribution of the target variable *Outcome* in both partitions, selecting
    the **Stratified sampling** option. Additionally, we put a static number in the
    random seed box (you can put `12345` as you see in *Figure 5.18*) and tick the
    adjacent checkbox:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们处于监督学习场景中，我们需要实现通常的分割/学习/预测/评分结构，以便验证是否存在过拟合的风险。我们首先添加**Partitioning**节点，并将其下游连接到**CSV
    Reader**节点。在其配置对话框中，我们将训练集的**相对**大小设置为 70%，并选择保护目标变量*Outcome*在两个分区中的分布，勾选**分层抽样**选项。此外，我们在随机种子框中填写一个静态数字（如*图
    5.18*所示，你可以填写`12345`），并勾选旁边的复选框：
- en: 'As a general rule, always perform a stratified sampling on the target variable
    of a classification. This will reduce the impact of imbalanced classes when learning
    and validating your model. There are other ways to restore a balance in the distribution
    of classes, such as under-sampling the majority class or over-sampling the minority
    one. One interesting approach is the creation of synthetic (and realistic) additional
    samples using algorithms like the **Synthetic Minority Over-sampling Technique**:
    check out the **SMOTE** node to learn more.'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作为一般规则，在分类的目标变量上始终执行分层抽样。这将减少学习和验证模型时类不平衡的影响。在恢复类分布平衡的其他方法中，可以通过对多数类进行欠采样或对少数类进行过采样来实现。一个有趣的方法是使用诸如**合成少数过采样技术**（SMOTE）这样的算法创建合成（和真实的）额外样本：了解更多信息，请查看**SMOTE**节点。
- en: '![Graphical user interface, text, application'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图形用户界面、文本、应用'
- en: Description automatically generated](img/B17125_05_18.png)
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成描述](img/B17125_05_18.png)
- en: 'Figure 5.18: Performing a stratified sampling using the Partitioning node:
    this way, we ensure a fair presence of yes and no customers in each partition'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.18：使用分区节点执行分层抽样：这样可以确保每个分区中都有足够的是和否客户
- en: 'Now that we have a training and test dataset readily available, we can proceed
    with implementing our first classification algorithm: **decision trees**. Let''s
    get a hint of how it works.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个训练和测试数据集，可以继续实施我们的第一个分类算法：**决策树**。让我们先了解一下它是如何工作的。
- en: Decision tree algorithm
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树算法
- en: 'Decision trees are simple models that describe a decision-making process. Have
    a look at the tree shown in *Figure 5.19* to get an idea of how they work. Their
    hierarchical structure resembles an upside-down tree. The root on top corresponds
    to the first question: according to the possible answers, there is a split between
    two or more subsequent *branches*. Every branch can either lead to additional
    questions (and respective splits into more branches) or terminate in *leaves*,
    indicating the outcome of the decision:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是描述决策过程的简单模型。看一下*图5.19*中显示的树，以了解它们的工作原理。它们的分层结构类似于倒置的树。顶部的根对应于第一个问题：根据可能的答案，会在两个或更多的*分支*之间进行分割。每个分支可以通向额外的问题（及相应的更多分支），也可以终止在*叶子*上，指示决策的结果：
- en: '![](img/B17125_05_19.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_19.png)'
- en: 'Figure 5.19: How will you go to work tomorrow? A decision tree can help you
    make up your mind'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19：明天你会如何去上班？决策树可以帮助你做出决定。
- en: 'Decision trees can be used to describe the process that assigns an entity to
    a class: in this case, we call it a **classification tree**. Think about a table
    where each entity (corresponding to a row) is described by multiple features (columns)
    and is assigned to one specific class, among different alternatives. For example,
    a classification tree that assigns consumers to multiple classes will answer the
    question *to which class does the consumer belong?*: every branching will correspond
    to different outcomes of a test on the features (like *is the age of the consumer
    higher than 35?* or *is the person married?*) while each terminal leaf will be
    one of the possible classes. Once you have defined the decision tree, you can
    apply it to all consumers (current and future). For every consumer in the table,
    you follow the decision tree: the features of the consumer will dictate which
    specific path to follow and result in a single leaf to be assigned as the class
    of the consumer.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可用于描述将实体分配给类别的过程：在这种情况下，我们称之为**分类树**。想象一张表，其中每个实体（对应一行）由多个特征（列）描述，并被分配到一种特定的类别中，其中有多种替代方案。例如，将消费者分配到多个类别的分类树将回答消费者属于哪个类别的问题：每个分支对应于对特征进行测试的不同结果（例如*消费者的年龄是否大于35岁？*或*这个人是否已婚？*），而每个终端叶子将是可能的类别之一。一旦定义了决策树，就可以将其应用于所有消费者（当前和未来）。对于表中的每个消费者，都要按照决策树进行操作：消费者的特征将决定跟随哪条具体路径，并导致一个单一叶子被分配为消费者的类别。
- en: 'There are many tree-based learning algorithms available for classification.
    They are able to "draw" trees by learning from labeled examples. These algorithms
    can find out the right splits and paths that end up with a decision model able
    to predict classes of new, unlabeled entities. The simplest version of a decision
    tree learning algorithm will proceed by iteration, starting from the root of the
    tree and checking what the "best possible" next split to make is so as to differentiate
    classes in the least ambiguous way. This concept will become clear by means of
    a practical example. Let''s imagine that we want to build a decision tree in order
    to predict which drink fast-food customers are going to order (among soda, wine,
    or beer), based on the food menu they had (the delicious alternatives are pizza,
    burger, or salad) and the composition of the table (whether it is among kids,
    couples, or groups of adults). The dataset to learn from will look like the one
    shown in *Figure 5.20*: we have 36 rows, each referring to a previous customer,
    and three columns, one for each feature (*Menu* and *Type*) and the target class
    (the *Favorite drink*).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多基于树的学习算法可用于分类。这些算法能够通过从标记的示例中学习来“绘制”决策树。它们可以找出正确的分裂和路径，最终生成一个能够预测新标签数据类别的决策模型。最简单的决策树学习算法会通过迭代进行，从树的根节点开始，检查“最佳的”下一个分裂点，以便以最不模糊的方式区分类别。通过一个实际示例，这个概念将变得清晰。假设我们想构建一个决策树，以预测快餐顾客将会点哪种饮料（可选饮料有汽水、葡萄酒或啤酒），根据他们选择的食物菜单（美味的选择包括比萨、汉堡或沙拉）和桌面构成（是否是孩子、情侣或成人团体）。我们将要学习的数据集将像*图
    5.20*所示：我们有36行数据，每行代表一个先前的顾客，三列数据分别对应特征（*菜单*和*类型*）和目标类别（*最爱饮料*）。
- en: '![A picture containing text, electronics'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![一张包含文本、电子设备的图片  '
- en: Description automatically generated](img/B17125_05_20.png)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_20.png)
- en: 'Figure 5.20: Drink preferences for 36 fast-food customers: can you predict
    their preferences based on their food menu and type?'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.20：36位快餐顾客的饮料偏好：你能根据他们的食物菜单和类型预测他们的饮料偏好吗？  '
- en: 'Since we have only two features, the resulting decision tree can only have
    two levels, resulting in two alternative shapes: either the first split is by
    *Menu* and the second, at the level below, by *Type*, or the other way around.
    The learning algorithm will pick the split that makes the most sense by looking
    at the count of the items falling into each branch and checking which splits make
    the "clearest cut" among classes.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只有两个特征，因此生成的决策树只能有两个层级，导致有两种可能的分裂方式：要么第一次分裂是按*菜单*，第二次分裂（在下一级）是按*类型*，要么反过来。学习算法会通过查看每个分支中项目的数量，并检查哪些分裂在类别之间做出“最清晰的区分”，来选择最合适的分裂。
- en: 'In this specific case, the alternative choices for the first split are the
    ones drawn in *Figure 5.21*: you can find the number of customers falling into
    each branch, separated by alternative class (beer, soda, or wine). Have a look
    at the number and ask yourself: between the *Menu* split on the left and the *Type*
    split on the right, which one is differentiating in the "purest" way among the
    three classes?'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特定情况下，第一次分裂的备选选择就是*图 5.21*中绘制的分裂方式：你可以看到每个分支中顾客的数量，并按不同饮料类别（啤酒、汽水或葡萄酒）进行区分。看看这些数字，问问自己：在左侧按*菜单*进行的分裂和右侧按*类型*进行的分裂之间，哪一个在三种类别中做出了“最纯粹”的区分？
- en: '![A picture containing diagram'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![一张包含图表的图片  '
- en: Description automatically generated](img/B17125_05_21.png)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_21.png)
- en: 'Figure 5.21: Which of these two alternative splits gives you the most help
    in anticipating the choice of drinks?'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.21：这两种分裂方式中，哪一种能在预测饮料选择上提供更多帮助？
- en: 'In this case, it seems that the *Type* split on the right is a no-brainer:
    kids are consistently going for sodas (with the exception of 2 customers who—hopefully—got
    served with alcohol-free beer), groups prefer beers, while couples go mainly with
    wine. The other alternative (split by *Menu*) is messier: for those having salad
    and, to some extent, burger, there is no such clear cut drinks choice. Our preference
    for the option on the right is guided by human intuition: for an algorithm, we
    need to have a more deterministic way to make a decision. Tree learning algorithms
    use, in fact, metrics to decide which splits are best to pick. One of these metrics
    is called the **Gini index**, or **Impurity index**. Its formula is quite simple:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，右侧按*类型*划分似乎是显而易见的：孩子们一致选择苏打水（除了2个顾客——希望——他们得到的是无酒精啤酒），团体更喜欢啤酒，而情侣则主要选择葡萄酒。另一种选择（按*菜单*划分）则复杂一些：对于吃沙拉和在某种程度上吃汉堡的人来说，饮料的选择没有这么明确。我们选择右侧选项的偏好是基于人类的直觉：对于算法来说，我们需要一个更具确定性的方法来做出决策。实际上，决策树学习算法使用度量标准来决定哪些划分是最合适的。这些度量标准之一叫做**基尼指数**，或者**不纯度指数**。它的公式非常简单：
- en: '![](img/B17125_05_020.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_020.png)'
- en: 'where *f*[i] is the relative frequency of *i-n*^(th) class (it''s in the *%*
    column in *Figure 5.21*), among the *M* possible classes. The algorithm will calculate
    the *I*[G] for each possible branching of a split and average the results out.
    The option with the lowest Gini index (meaning, with the least "impure" cut) will
    win among the others. In our fast-food case, the overall Gini index for the option
    on the left will be the average of:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*f*[i]是第*i-n*^(th)类的相对频率（它位于*图 5.21*中的*%*列），在*M*个可能的类别中。算法会计算每个可能划分的*I*[G]，并对结果进行平均。基尼指数最低的选项（意味着“最纯”的划分）将胜出。在我们的快餐案例中，左侧选项的总体基尼指数将是以下内容的平均值：
- en: '![](img/B17125_05_021.png)![](img/B17125_05_022.png)![](img/B17125_05_023.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_021.png)![](img/B17125_05_022.png)![](img/B17125_05_023.png)'
- en: 'By averaging them out, we find that the Gini index for the left option is 0.60
    while the one for the right option is 0.38\. These metrics are confirming our
    intuition: the option on the right (the split by *Type*) is "purer" as demonstrated
    by the lower Gini index. Now you have all the elements to see how the decision
    tree learning algorithm works: it will iteratively calculate the average *I*[G]
    for all possible splits (at least one for each available feature), pick the one
    with the lowest index, and repeat the same at the levels below, for all possible
    branches, until it is not possible to split further. In the end, the leaves are
    assigned by just looking at where the majority of the known examples fall. For
    instance, take the branching on the right in *Figure 5.21*: if this was the last
    level of a tree, kids will be classified with soda, couples with wine, and groups
    with beer. You can see in *Figure 5.22* the resulting full decision tree you would
    obtain by using the fast-food data we presented above:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对它们进行平均，我们发现左侧选项的基尼指数为0.60，而右侧选项的基尼指数为0.38。这些度量标准验证了我们的直觉：右侧选项（按*类型*划分）更“纯净”，因为其基尼指数较低。现在你已经具备了所有元素，能够理解决策树学习算法是如何工作的：它将迭代计算所有可能划分的平均*I*[G]（至少对每个可用特征有一个），选择基尼指数最低的一个，并在下一级的所有可能分支中重复同样的过程，直到无法进一步划分。最后，叶子节点将根据已知示例的大多数归属情况进行分配。例如，看看*图
    5.21*中的右侧分支：如果这是树的最后一层，孩子们将被分类为选择苏打水，情侣选择葡萄酒，团体选择啤酒。在*图 5.22*中，你可以看到使用我们上面介绍的快餐数据得到的完整决策树：
- en: '![](img/B17125_05_22.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_22.png)'
- en: 'Figure 5.22: Decision tree for classifying fast-food customers according to
    their favorite drink.In which path would you normally be?'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.22：根据顾客最喜欢的饮料分类的快餐顾客决策树。你通常会在哪条路径上？
- en: 'By looking at the obtained decision tree, you will notice that not all branches
    at the top level incur further splits at the level below. Take the example of
    the *Type*=`Kids` branch on the top left: the vast majority of kids (10 out of
    12) go for `Soda`. There are not enough remaining examples to make a meaningful
    further split by *Menu*, so the tree just stops there. On top of this basic stopping
    criterion, you can implement additional (and more stringent) conditions that limit
    the growth of the tree by removing less meaningful branches: these are called—quite
    appropriately, I must say—**pruning mechanisms**. By pruning a decision tree,
    you end up with a less complex model: this is very handy to use when you want
    to avoid model overfitting. Think about this: if you have many features and examples,
    your tree can grow massively.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看得到的决策树，你会注意到并非所有顶层的分支都会在下面的层级继续分裂。以左上方的*Type*=`Kids`分支为例：绝大多数孩子（12个中有10个）选择了`Soda`。剩余的样本不足以通过*Menu*做出有意义的进一步分裂，因此树就停在了这里。除了这个基本的停止标准之外，你还可以实现其他（更严格的）条件，通过移除不太有意义的分支来限制树的生长：这些被称为——我必须说，这个名字非常恰当——**修剪机制**。通过修剪决策树，你最终得到的是一个更简单的模型：当你想避免模型过拟合时，这非常有用。想一想：如果你有很多特征和样本，树可能会极其庞大。
- en: 'Every combination of values might, in theory, produce a very specific path.
    Chances are that these small branches cover an insignificant case that just happened
    to be in the training set but has no general value: this is a typical case of
    overfitting that we want to avoid as much as possible. That is why, as you will
    soon see in KNIME, you might need to activate some of the pruning mechanisms to
    avoid overfitting when growing trees.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 每一种值的组合理论上都可能产生一个非常特定的路径。很有可能这些小分支覆盖的只是训练集中偶然出现的一个无关紧要的案例，而这个案例没有普遍价值：这是典型的过拟合问题，我们希望尽可能避免。这就是为什么，正如你很快在KNIME中看到的那样，你可能需要激活一些修剪机制，以避免在生长树时过拟合。
- en: 'Let''s make another consideration related to numeric features in decision trees.
    In the fast-food example, we only had nominal features, which make every split
    quite simple to imagine: every underlying branch covered a possible value of the
    categorical column. If you have a numeric column to be considered, the algorithm
    will check what the Gini index would be if you split your samples using a numeric
    threshold. The algorithm will try multiple thresholds and pick the best split
    that minimizes impurity. Let''s imagine that in our example we had an additional
    feature, called *Size*, that counts the number of people sitting at each table.
    The algorithm will test multiple thresholds and will check what the Gini index
    would be if you divided your samples according to these conditions, which are
    questions like "is *Size* > 3?", "is *Size* > 5?", and "is *Size* > 7?". If one
    of these conditions is meaningful, the split will be made according to the numeric
    variable: all samples having *Size* lower than the threshold will go to the left
    branch, and all others to the right branch. The Gini indices resulting from all
    the thresholds on the numeric features will be compared across all other indices
    coming from the categorical variables as we saw earlier: at each step, the purest
    split will win, irrespectively of its type. This is how decision trees can cleverly
    mix all types of features when classifying samples.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再考虑一下与决策树中数值特征相关的问题。在快餐示例中，我们只有名义特征，这使得每一次分裂都很容易想象：每个底层分支涵盖了类别列的一个可能值。如果你有一个数值列需要考虑，算法会检查如果使用数值阈值来划分样本时，基尼指数会是什么。算法会尝试多个阈值，并选择最佳的分裂方式来最小化杂质。假设在我们的示例中，我们有一个额外的特征，叫做*Size*，它统计了每张桌子上坐着的人数。算法会测试多个阈值，并检查如果根据这些条件划分样本时基尼指数会是什么，这些条件包括像“*Size*
    > 3?”、“*Size* > 5?”和“*Size* > 7?”。如果这些条件中的一个有意义，那么分裂会根据数值变量来进行：所有*Size*低于阈值的样本将进入左分支，其他样本进入右分支。来自所有数值特征的基尼指数将与我们之前看到的所有来自类别变量的指数进行比较：在每一步中，最纯净的分裂会胜出，不管它是什么类型。这就是决策树在分类样本时如何巧妙地混合各种特征类型的方式。
- en: 'Decision tree models can be extended to predict numbers and, so, become **regression
    trees**. In these trees, each leaf is labeled with a different value of the target
    variable. Normally, the value of the leaf is just the average of all the samples
    that ended up in such a leaf node, after going through a construction mechanism
    similar to the ones for classification trees (using Gini indices and all that).
    You can build regression trees in KNIME as well: have a look at the **simple regression
    tree** nodes in the repository.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树模型可以扩展为预测数字，从而成为**回归树**。在这些树中，每个叶子节点标注了目标变量的不同值。通常，叶子的值只是所有进入该叶子节点的样本的平均值，这些样本经过类似分类树构建机制的过程（使用基尼指数等）。你也可以在KNIME中构建回归树：请查看存储库中的**简单回归树**节点。
- en: 'Now that we know what decision trees are, let''s grow one to classify our bank
    customers according to the outcome of the telemarketing campaign. We''ll use a
    new node for that: the **Decision Tree Learner**.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了决策树是什么，让我们生长一棵树，用它来根据电话营销活动的结果对我们的银行客户进行分类。我们将使用一个新的节点：**决策树学习器**。
- en: '![Logo'
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![Logo'
- en: Description automatically generated](img/image0491.png) *Decision Tree Learner*
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/image0491.png) *决策树学习器*
- en: 'This node (**Analytics > Mining > Decision tree**) trains a decision tree model
    for predicting nominal variables (classification). The most important fields to
    be set in its configuration dialog (see *Figure 5.23*) are:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点（**分析 > 挖掘 > 决策树**）训练一个决策树模型，用于预测名义变量（分类）。在其配置对话框中（见*图 5.23*）需要设置的最重要字段是：
- en: '**Class column**: you need to specify your nominal target variable to be predicted.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别列**：你需要指定你要预测的名义目标变量。'
- en: '**Quality measure**: this is the metric used to decide how to make the splits.
    The default value is the **Gini index** we have encountered above. You can also
    select the information for **Gain ratio**, which would tend to create more numerous
    and smaller branches. There is not a good and bad choice, and in most cases both
    measures generate very similar trees: you can try them both and see which one
    produces the best results.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量度量**：这是用来决定如何进行分裂的度量标准。默认值是我们上面遇到的**基尼指数**。你也可以选择**增益比**的信息，这通常会创建更多且更小的分支。没有“好”与“不好”的选择，在大多数情况下，两种度量都会生成非常相似的树：你可以尝试它们两者，看看哪个能产生更好的结果。'
- en: '**Pruning method**: you can use this selector to activate a robust pruning
    technique called **MDL** (**Minimum Description Length**) that removes the less
    meaningful branches and generates a balanced tree.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剪枝方法**：你可以使用此选择器来激活一种强大的剪枝技术，称为**MDL**（**最小描述长度**），它去除不太重要的分支并生成一个平衡的树。'
- en: '**Min number records per node**: you can control the tree growth-stopping criterion
    by setting a minimum number of samples for allowing a further split. By default,
    this hyperparameter is set to `2`: this means that no branch will be generated
    with less than 2 samples. As you increase this number, you will prune more branches
    and obtain smaller and smaller trees: this is an effective way for tuning the
    complexity of the trees and obtaining an optimal, well-fitted model. By activating
    the MDL technique in the earlier selector, you go the "easy way" as it will automatically
    guess the right level of pruning.![](img/B17125_05_23.png)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个节点的最小记录数**：你可以通过设置允许进一步分裂的最小样本数来控制树的生长停止标准。默认情况下，此超参数设置为`2`：这意味着没有分支会生成少于
    2 个样本的节点。随着这个数字的增加，你将剪去更多的分支，并得到越来越小的树：这是调整树复杂度并获得最佳拟合模型的有效方法。通过在之前的选择器中激活MDL技术，你走的是“捷径”，因为它会自动推测正确的剪枝水平。![](img/B17125_05_23.png)'
- en: 'Figure 5.23: Configuration window of the Decision Tree Learner node: are you
    up for some pruning today?'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.23：决策树学习器节点的配置窗口：今天要剪枝吗？
- en: 'The output of the node is the definition of the tree model, which can be explored
    by opening its main view (right-click on the node and select **View: Decision
    Tree View**). In *Figure 5.24*, you will find the KNIME output of the fast-food
    classification tree we obtained earlier (see, for comparison, *Figures 5.22* and
    *5.21*): at each node of the tree, you find the number of training samples falling
    into each value of the class. You can expand and collapse the branches by clicking
    on the circled **+** and **–** signs appearing at each split:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '节点的输出是树模型的定义，可以通过打开其主视图来探索（右键单击节点，选择**View: Decision Tree View**）。在*图 5.24*中，您将找到我们之前获取的快餐分类树的
    KNIME 输出（可参见比较*图 5.22*和*5.21*）：在树的每个节点处，您会发现落入每个类值的训练样本数。您可以通过单击每个分割点处显示的带有圆形
    **+** 和 **–** 标志来展开和折叠分支：'
- en: '![](img/B17125_05_24.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_24.png)'
- en: 'Figure 5.24: The fast-food classification tree, as outputted by the Decision
    Tree Learner node in KNIME.The gray rows correspond to the majority class'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24：快餐分类树，由 KNIME 中的 Decision Tree Learner 节点输出。灰色行对应于主要类别
- en: 'Drag and drop the **Decision Tree Learner** node from the repository and connect
    the upper output of the **Partitioning node** (the training set) with it. Let''s
    leave all the default values for now in its configuration (we will have the opportunity
    for some pruning later): the only selector to double-check is the one setting
    the **Class column** that in our case is *Outcome*. If you run the node and open
    its decision tree view (select the node and press *F10*), you will meet the tree
    you have just grown:![](img/B17125_05_25.png)'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从存储库中拖放**Decision Tree Learner**节点，并将其连接到**Partitioning node**的上输出（训练集）。暂时保留其配置的所有默认值（稍后我们将有机会进行一些修剪）：唯一需要双重检查的选择器是设置**Class
    column**的选择器，在我们的案例中为*Outcome*。如果您运行节点并打开其决策树视图（选择节点并按 *F10*），您将会看到刚刚生成的树：![](img/B17125_05_25.png)
- en: 'Figure 5.25: A first tree classifying bank customers by Outcome: this is just
    a partial view of the many levels and branches available'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.25：首个根据 Outcome 对银行客户进行分类的树：这只是可用的许多级别和分支的部分视图
- en: 'As you expand some of the branches, you realize that the tree is very wide
    and deep: *Figure 5.25* shows an excerpt of what the tree might look like (depending
    on your random partitioning, you might end up with a different tree, which is
    fine). In this case, we noticed that the top split divided customers into mobile
    and landline users. This is what happened: the Gini index was calculated across
    all features and scored the lowest for *Contact*, making this the single most
    important variable to differentiate customers according to their *Outcome*. Let''s
    see whether this tree is good enough and predict the outcomes in the test set.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 当您展开某些分支时，您会意识到树非常宽和深：*图 5.25*展示了树可能的部分外观（根据随机分区，您可能会得到不同的树，这是正常的）。在这种情况下，我们注意到顶部分割将客户分为移动用户和固定电话用户。事情是这样的：对所有特征计算了基尼指数，并且在*Contact*方面得分最低，使其成为区分客户*Outcome*最重要的变量。让我们看看这棵树是否足够好并预测测试集中的结果。
- en: '![Icon'
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![图标'
- en: Description automatically generated](img/image0531.png) *Decision Tree Predictor*
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/image0531.png) *Decision Tree Predictor*
- en: This (**Analytics > Mining > Decision tree**) applies a decision tree model
    (provided as an input in the first port) to a dataset (second port) and returns
    the prediction for each input row. This node will not require any configuration
    and will produce a similar table to the one provided in the input with an additional
    column that includes the result of the classification.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这个（**Analytics > Mining > Decision tree**）应用决策树模型（作为第一个端口的输入）到数据集（第二个端口），并返回每个输入行的预测结果。这个节点不需要任何配置，并且将生成与输入中提供的表格类似的表格，其中包含一个额外的列，包含分类结果。
- en: Let's implement the **Decision Tree Predictor** node and wire it in such a way
    it gets as inputs the tree model outputted by the **Decision Tree Learner** node
    and the second outport of the **Partitioning** node, which is our test set. As
    you execute the node, you will find an output that the precious additional column
    called *Prediction (Outcome)*.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实施**Decision Tree Predictor**节点，并将其连接到如下所示的方式：该节点获取由**Decision Tree Learner**节点输出的树模型以及**Partitioning**节点的第二输出端口，即我们的测试集。当您执行该节点时，您将找到一个输出，其中包含名为*Prediction
    (Outcome)*的宝贵附加列。
- en: 'At this point, we can finally assess the performance of the model by calculating
    the metrics used for classification. Do you remember the accuracy, precision,
    sensitivity measures, and confusion matrix we obtained in the cute dog versus
    muffin example? It''s time to calculate these metrics by using the right node:
    **Scorer**.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以通过计算用于分类的指标来评估模型的性能。你还记得在“小狗与松饼”示例中我们获得的准确度、精确度、敏感性和混淆矩阵吗？现在是使用正确的节点：**评分器**来计算这些指标的时候了。
- en: '![A picture containing graphical user interface'
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![图形用户界面图片'
- en: Description automatically generated](img/image054.png) *Scorer*
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/image054.png) *评分器*
- en: 'This node (**Analytics > Mining > Scoring**) calculates the summary performance
    scores of classification by comparing two nominal columns. The only step required
    for its configuration (*Figure 5.26*) is the selection of the columns to be compared:
    you should select the column carrying the observed (actual) values in the **First
    Column** dropdown, while predictions go in the **Second Column** selector. The
    node outputs the most important metrics for assessing a classification performance,
    namely: the Confusion Matrix, provided as a table in the first output (columns
    will refer to the predictions, while actual values will go as rows) and summary
    metrics such as **Accuracy**, **Precision**, and **Sensitivity**, which you can
    find in the second output of the node.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点（**分析 > 数据挖掘 > 评分**）通过比较两个名义列来计算分类的总体性能评分。其配置步骤（*图 5.26*）非常简单，只需选择要比较的列：你应该在**第一列**下拉框中选择包含观察到的（实际）值的列，而预测值则选择在**第二列**下拉框中。该节点输出评估分类性能的最重要指标，即：混淆矩阵，作为表格显示在第一个输出中（列表示预测值，而行表示实际值），以及总结性指标，如**准确率**、**精确度**和**敏感性**，你可以在节点的第二个输出中找到这些指标。
- en: 'Some of the performance metrics for a classification will depend on which class
    you decide to be considered as `Positive`: have a look at *Figure 4.8* in the
    previous chapter to get a refresher. In the second output of the Scorer node,
    you will find one row for every possible class: each row contains the metrics
    calculated under the assumption that one specific class is labeled as `Positive`
    and all the other classes are `Negative`.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 一些分类的性能指标将取决于你决定将哪个类别视为`正类`：可以查看上一章的*图 4.8*来复习。在评分器节点的第二个输出中，你将看到每个可能类别的一行：每一行包含假设某一特定类别被标记为`正类`，而其他所有类别为`负类`时计算的指标。
- en: '![Graphical user interface, text, application, email'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，文本，应用程序，电子邮件'
- en: Description automatically generated](img/B17125_05_26.png)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_26.png)
- en: 'Figure 5.26: The configuration window of the Scorer node: just select the columns
    to compare across'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.26：评分器节点的配置窗口：只需选择要比较的列
- en: 'We can now add the **Scorer** node (make sure you don''t get confused and pick
    the **Numeric Scorer** node, which can only be used for regressions) to the workflow
    and connect it downstream to the **Decision Tree Predictor**. In the configuration
    window, we can leave everything as it is, just checking that we have *Outcome*
    as **First Column** and *Prediction (Outcome)* as **Second Column**. Execute the
    node and open its main view (*F10* or right-click and select **View: Confusion
    Matrix**).'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以将**评分器**节点添加到工作流中（确保不要选错，选择了只能用于回归的**数值评分器**节点）。然后将它连接到下游的**决策树预测器**。在配置窗口中，我们可以保持默认设置，只需确保**第一列**是*结果*，**第二列**是*预测（结果）*。执行该节点并打开其主视图（*F10*
    或右键点击并选择**查看：混淆矩阵**）。
- en: 'The output of the **Scorer** node (*Figure 5.27*) tells us that we get an accuracy
    level of 78.3%: out of 100 predictions, 78 of them turn out to be correct. The
    confusion matrix helps us understand whether the model can bring value to our
    business case:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**评分器**节点的输出（*图 5.27*）告诉我们，模型的准确率为78.3%：在100个预测中，有78个是正确的。混淆矩阵帮助我们理解模型是否能为我们的业务场景带来价值：'
- en: '![](img/B17125_05_27.png)'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17125_05_27.png)'
- en: 'Figure 5.27: The output of the node Scorer after our first classification:
    78% accuracy is not bad as a starting point'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.27：第一次分类后评分器节点的输出：78%的准确率作为起点并不差
- en: 'In the case shown in *Figure 5.27*, we have 450 customers (180 + 270) in the
    test set that were predicted as interested in the account (*Prediction (Outcome)*
    = `yes`). Out of this, only 180 (40%, which corresponds to the precision of our
    model) were predicted correctly, meaning that these customers ended up buying
    the product. The number seems to be low, but it is already encouraging: the algorithm
    can help to find a subset of customers that are more likely to buy the product.
    If we indiscriminately called every customer—as we know from the pilot—we would
    have achieved a success rate of 19% while, by focusing on the (fewer) customers
    that the algorithm identified as potential (*Prediction (Outcome)* = `yes`), the
    success rate would double and reach 40%.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 5.27*所示的情况下，我们在测试集中有 450 名客户（180 + 270），被预测为对账户感兴趣（*预测（结果）* = `yes`）。其中只有
    180 人（40%，即我们的模型的精度）被正确预测，这意味着这些客户最终购买了产品。这个数字看起来很低，但已经是一个鼓舞人心的结果：算法能够帮助找到更有可能购买产品的客户子集。如果我们不加区分地对所有客户进行预测——正如我们从试点中所知道的那样——我们会获得
    19% 的成功率，而通过集中关注算法识别为潜在客户的（较少的）客户（*预测（结果）* = `yes`），成功率将翻倍，达到 40%。
- en: 'Let''s now think about what we can do to improve the results of the modeling.
    We remember that our decision tree was deep and wide: some of the branches were
    leading to very "specific" cases, which interested only a handful of examples
    in the training set. This doesn''t look right: a decision tree that adapted so
    closely to the training set might produce high errors in future cases as it is
    not able to comprehend the essential patterns of general validity. We might be
    overfitting! Let''s equip ourselves with a good pair of pruning shears: we can
    try to fix the overfitting by reducing the complexity of the tree, making some
    smart cuts here and there:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们思考如何改进建模结果。我们记得我们的决策树既深又宽：一些分支指向了非常“具体”的案例，这些案例只对训练集中的少数几个示例感兴趣。这看起来不太对：一棵过度适应训练集的决策树可能会在未来的案例中产生高误差，因为它无法理解具有普遍有效性的基本模式。我们可能在过拟合！让我们准备好一把修剪剪刀：我们可以通过减少树的复杂性来修正过拟合，在某些地方做出明智的修剪：
- en: 'Sometimes, the Decision Tree Predictor node generates null predictions (red
    `?` in KNIME tables, which caused the warning message you see at the top of *Figure
    5.27*). This is a sign that the tree might be overfitted: its paths are too "specific"
    and do not encompass the set of values that require a prediction (this "pathology"
    is called **No True Child**). Besides taking care of the overfitting, one trick
    you can apply to solve the missing values is to open the **PMMLSettings** panel
    (second tab in the **Decision Tree Learner** configuration) and set **No true
    child strategy** to **returnLastPrediction**.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，决策树预测节点会生成空预测（在 KNIME 表格中显示为红色`?`，这就是你在*图 5.27*顶部看到的警告信息）。这表明树可能存在过拟合问题：其路径过于“具体”，无法涵盖需要预测的值集合（这种“病态”被称为**没有真实子节点**）。除了处理过拟合问题外，解决缺失值的一个技巧是打开**PMML设置**面板（**决策树学习器**配置中的第二个选项卡），并将**无真实子节点策略**设置为**返回最后的预测**。
- en: 'Open the configuration dialog of the **Decision Tree Learner** and select **MDL**
    as the **Pruning method**. This is the simplest and quickest way to prune our
    tree: we could have also iterated through higher values of **Min number records
    per node** (give it a try to check how it works), but MDL is a safe approach to
    get quick improvements.'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开**决策树学习器**的配置对话框，并选择**MDL**作为**修剪方法**。这是修剪我们树的最简单和最快的方法：我们也可以尝试增加**每个节点的最小记录数**（可以试试看它的效果），但
    MDL 是一种安全的方法，可以快速获得改进。
- en: Let's see if it worked. We don't need to change anything else, so let's just
    execute the **Scorer** node and open its main view to see what happened.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看是否有效。我们不需要改变其他内容，所以只需执行**评分器**节点并打开其主视图查看发生了什么。
- en: 'When you look at the results (*Figure 5.28*) you feel a thrill of excitement:
    things got better. The accuracy raised to 83% and, most importantly, the precision
    of the model greatly increased. Out of the 175 customers in the test set who are
    now predicted as *Outcome*=`yes`, 117 would have ended up actually buying the
    product. If we followed the recommendation of the model (which we can assume will
    keep a similar predictive performance on customers we didn''t call yet—so the
    remaining 97% of our customer base), the success rate of our marketing campaign
    will move to 67%, which is more than 3 times better than our initial baseline
    of 19%!'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当你看到结果时（*图 5.28*），你会感到一阵兴奋：情况变得更好。准确率提高到了 83%，最重要的是，模型的精度大大提升。在测试集中175名现在被预测为*Outcome*=`yes`的客户中，117人最终会真正购买产品。如果我们遵循模型的推荐（我们可以假设它在我们尚未联系的客户群体中也会保持类似的预测效果——即剩余的97%客户群体），那么我们的营销活动的成功率将提升至67%，这比最初的19%基准提高了三倍多！
- en: '![](img/B17125_05_28.png)'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17125_05_28.png)'
- en: 'Figure 5.28: The output of the node Scorer after our tree pruning: the precision'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.28：树修剪后节点评分器的输出：精度
- en: 'The model was previously overfitting and some pruning clearly helped. If you
    now open the tree view of the **Decision Tree Learner** node, you will find a
    much simpler model that can be explored and, finally, interpreted. You can expand
    all branches at once by selecting the root node (just left-click on it) and then
    clicking on **Tree** | **Expand Selected Branch** from the top menu. By looking
    at the tree, which might be similar to the one shown in *Figure 5.29*, we can
    finally attempt some interpretation of the model. Look at the different percentages
    of the `yes` category within each node: we found some buckets of customers that
    are disproportionally interested in our product:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 模型之前存在过拟合问题，一些修剪明显有所帮助。如果你现在打开**决策树学习器**节点的树状视图，你会发现一个更简单的模型，它可以被探索并最终进行解释。你可以通过选择根节点（只需左键点击它），然后点击顶部菜单中的**树**
    | **展开选定分支**来一次性展开所有分支。通过查看这棵树（可能类似于*图 5.29*所示），我们最终可以尝试对模型进行一些解释。观察每个节点中`yes`类别的不同百分比：我们发现一些客户群体对我们的产品表现出不成比例的兴趣：
- en: '![](img/B17125_05_29.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_29.png)'
- en: 'Figure 5.29: An excerpt of the decision tree classifying bank customers by
    Outcome: students, retired, and 60+ customers using landlines are showing the
    most interest in our new savings account'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.29：决策树摘录，按结果分类的银行客户：学生、退休人员和使用固定电话的60岁以上客户表现出对我们新储蓄账户的最大兴趣
- en: 'For example, we find out that customers falling into these three segments:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们发现这三个细分市场的客户群体：
- en: Mobile users who are students
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动设备用户中的学生
- en: Mobile users who are retired
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动设备用户中退休的人群
- en: Landline users who are 60+ years old
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用固定电话的60岁以上用户
- en: 'responded much more to our pilot campaign than all others, having more than
    50% of the samples ending up with opening a new savings account. We have a quick
    chat with the product manager and show these results to him. He is very excited
    about the findings and, after some thinking, he confirms that what the algorithm
    spotted makes perfect sense from a business standpoint. The new type of account
    has less fixed costs than the others, so this explains while its proposition proves
    more compelling to lower-income customers, such as students and the retired. Additionally,
    this account includes a free prepaid card, which is a great tool for students,
    who can get their balance topped up progressively, but also for older customers,
    who do not fully trust yet the usage of traditional credit cards and prefer keeping
    the risk of fraud under control. The account manager is very pleased with what
    you shared with him and does not stop thanking you: by having data-based evidence
    of the characteristics that make a customer more likely to buy his new product,
    he can now finetune the marketing concept, highlighting benefits and reinforcing
    the message to share with prospective customers.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于其他所有样本，我们的试点活动受到了更多的响应，超过50%的样本最终成功开设了新的储蓄账户。我们和产品经理快速聊了一下，并向他展示了这些结果。他对这些发现感到非常兴奋，经过一番思考后，他确认算法所发现的内容在商业上完全有意义。新类型的账户比其他账户具有更低的固定成本，这也解释了为什么它的提案对低收入客户（如学生和退休人员）更具吸引力。此外，这种账户包括一张免费的预付卡，这是一个很棒的工具，既适合学生逐步充值，也适合那些尚未完全信任传统信用卡的年长客户，他们更倾向于控制欺诈风险。账户经理对你分享的信息非常满意，不停地感谢你：通过基于数据的证据，了解到哪些特征使得客户更可能购买他的新品，他现在可以精细调整营销概念，突出利益并强化向潜在客户传递的信息。
- en: 'The positive feedback you just received was invigorating and you want to quickly
    move to the second part of the challenge: building a propensity model able to
    "score" the 97% of the customers that have not been contacted yet. To do so, we
    will first need to introduce another classification algorithm particularly well
    suited for anticipating propensities: **random forest**.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚收到的积极反馈让你精神焕发，接下来你想迅速进入挑战的第二部分：构建一个能够为**97%**尚未联系的客户“评分”的倾向模型。为此，我们首先需要引入另一种特别适合预测倾向的分类算法：**随机森林**。
- en: Random forest algorithm
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林算法
- en: 'One approach used in machine learning to obtain better performance is **ensemble
    learning**. The idea behind it is very simple: instead of building a single model,
    you combine multiple *base* models together and obtain an *ensemble* model that,
    collectively, produces stronger results than any of the underlying models. If
    we apply this concept to decision trees, we will grow multiple models in parallel
    and obtain… a forest. However, if we run the decision tree algorithm we''ve seen
    in the previous pages to the same data set multiple times, we will just obtain
    "copies" of identical trees. Think about it: the procedure we described earlier
    (with the calculation of the Gini index and the building of subsequent branches)
    is completely deterministic and will always produce the same outputs when using
    the same inputs. To encourage "diversity" across the base models, we need to force
    some variance in the inputs: one way to do so is to randomly sample subsets of
    rows and columns of our input dataset, and offer them as different training sets
    to independently growing base models. Then, we will just need to aggregate the
    results of the several base models into a single ensemble model. This is called
    **Bagging**, short for **Bootstrap Aggregation**, which is the secret ingredient
    that we are going to use to move from decision trees to random forests.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，为了获得更好的性能，一种常用的方法是**集成学习**。其背后的理念非常简单：不是构建一个单一的模型，而是将多个*基础*模型结合起来，得到一个*集成*模型，集体上比任何单一模型都能产生更强的结果。如果我们将这个概念应用到决策树上，我们将并行构建多个模型，得到一个……森林。然而，如果我们对相同的数据集多次运行前面提到的决策树算法，我们只会得到“复制”的相同树。想想看：我们之前描述的过程（计算基尼指数并构建后续分支）是完全确定性的，当输入相同的时候，始终会产生相同的输出。为了促使基础模型之间的“多样性”，我们需要在输入数据中强制引入一些变化：一种方法是随机抽取输入数据集中的行列子集，并将它们作为不同的训练集提供给独立生长的基础模型。然后，我们只需将多个基础模型的结果聚合成一个单一的集成模型。这被称为**Bagging**，即**Bootstrap聚合**，它是我们将从决策树过渡到随机森林的秘密武器。
- en: 'To understand how it works, let''s visualize it in a practical example: *Figure
    5.30* shows both a simple decision tree and a random forest (made of four trees)
    built on our bank telemarketing example:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解它是如何工作的，让我们通过一个实际例子来可视化：*图 5.30* 展示了一个简单的决策树和一个基于我们的银行电话营销案例构建的随机森林（由四棵树组成）。
- en: '![](img/B17125_05_30.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_30.png)'
- en: 'Figure 5.30: A decision tree and random forest compared: with the forest you
    get a propensity score and higher accuracy'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.30：决策树与随机森林对比：使用森林模型可以得到倾向得分和更高的准确性
- en: 'Thanks to a random sampling of rows and columns, we managed to grow four different
    trees, starting from the same initial dataset. Look at the tree on the bottom
    left (marked as *#1* in the figure): it only had the *Mortgage* and the *Contact*
    columns available to learn from, as they were the ones randomly sampled in its
    case. Given the subset of rows that were offered to it (that were also randomly
    drawn as part of the bootstrap process), the model applies the decision tree algorithm
    and produces a tree that differs from all other base models (you can check the
    four trees at the bottom—they are all different). Given the four trees that make
    our forest, let''s imagine that we want to predict the outcome for a 63-year-old
    retired customer, who has a mortgage and gets contacted by landline. The *same*
    customer will follow four *different* paths (one for each tree), which will lead
    to different outcomes. In this case, 3 trees out of 4 agree that the prediction
    should be `yes`. The resulting ensemble prediction will be made in a very democratic
    manner, by voting. Since the majority believes that this customer is a `yes`,
    the final outcome will be `yes` with a **Propensity score** of 0.75 (3 divided
    by 4).'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对行和列进行随机抽样，我们成功地从相同的初始数据集中生成了四棵不同的树。看看图中左下角的树（标记为 *#1*）：它只有 *Mortgage* 和 *Contact*
    列可以学习，因为它们是该树在其情况下随机抽取的列。考虑到它所提供的行子集（这些行也是通过引导过程随机抽取的），模型应用决策树算法，生成了一棵与其他所有基础模型不同的树（你可以检查底部的四棵树——它们都不同）。考虑到构成我们森林的四棵树，让我们假设我们想预测一个63岁退休客户的结果，该客户有房贷并通过固定电话被联系。*相同*的客户将遵循四条*不同*的路径（每棵树一条），最终会导致不同的结果。在这种情况下，4棵树中有3棵一致认为预测结果应该是`yes`。最终的集成预测将通过投票的方式民主产生。由于大多数树认为该客户是`yes`，因此最终的结果将是`yes`，倾向得分为0.75（3除以4）。
- en: 'The assumption we make is that the more trees that are in agreement with a
    customer being classified as `yes`, the "closer" the customer is to buying our
    product. Of course, we normally build many more trees than just four: the diversity
    of the different branching each tree displays will make our ensemble model more
    "sensitive" to the smaller nuances of feature combinations that can tell us something
    useful about the propensity of a customer. Every tree offers a slightly "different"
    point of view on how to classify a customer: by bringing all these contributions
    together—in a sort of decisions crowdsourcing—we obtain more robust collective
    predictions: this is yet another proof of the universal value of diversity in
    life!'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做出的假设是：越多的树一致认为某个客户被分类为 `yes`，那么该客户“越接近”购买我们的产品。当然，我们通常会建立比四棵树更多的树：每棵树在不同的分支上展示的多样性将使我们的集成模型更“敏感”于特征组合中的微小差异，这些差异能够告诉我们关于客户倾向的有用信息。每棵树都会提供一个稍微“不同”的视角来对客户进行分类：通过将所有这些贡献汇集起来——在一种类似决策众包的方式中——我们获得了更强大的集体预测：这又是多样性在生活中普遍价值的另一个证明！
- en: 'Although the propensity score is related to the probability that a classification
    is correct, they are not the same thing. We are still in the uncertain world of
    probabilistic models: even if 100% of the trees agree on a specific classification,
    you cannot be 100% sure that the classification is right.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管倾向得分与分类正确的概率相关，但它们并不是同一个概念。我们仍然处于不确定的概率模型世界中：即使100%的树对某一特定分类达成一致，也不能100%确信该分类是正确的。
- en: 'Let''s get acquainted with the KNIME node that can grow forests: meet the **Random
    Forest Learner** node.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们认识一下可以生成森林的KNIME节点：见 **随机森林学习器** 节点。
- en: '![](img/image060.png) *Random Forest Learner*'
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![](img/image060.png) *随机森林学习器*'
- en: 'This node (**Analytics > Mining > Decision Tree Ensemble > Random Forest >
    Classification**) trains a random forest model for classification. At the top
    of its configuration window (*Figure 5.31*) you can select the nominal column
    to use as the target of the classification (**Target Column**). Then, in the column
    selector in the middle, you can choose which columns to use as features (the ones
    appearing on the **Include** box on the right): all others will be ignored by
    the learning algorithm. The option **Save target distribution…** will record the
    number of samples that fell into each leaf of the underlying tree models: although
    it is memory expensive, it can help to generate more accurate propensity scores,
    by means of the **soft voting** technique, which we will talk about later.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这个节点（**分析 > 挖掘 > 决策树集成 > 随机森林 > 分类**）训练一个用于分类的随机森林模型。在其配置窗口的顶部（*图5.31*），你可以选择用作分类目标的名义列（**目标列**）。然后，在中间的列选择器中，你可以选择哪些列作为特征（在右侧**包括**框中显示的列）：所有其他列将被学习算法忽略。选项**保存目标分布…**将记录每个叶节点中样本的数量：尽管这会消耗大量内存，但它可以帮助通过**软投票**技术生成更准确的倾向得分，关于这一点我们稍后会讨论。
- en: 'Toward the bottom of the window, you will find also a box that lets you choose
    how many trees you want to grow (**Number of models**). Lastly, you can decide
    to check a tick box (labeled as **Use static random seed**) that, similarly to
    what you found in the **Partitioning** node, lets you "fix" the initialization
    seed of the pseudo-random number generator used for the random sampling of rows
    and columns: in this case, you will obtain, at parity of input and configuration
    parameters, always the same forest generated:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在窗口底部，你还会看到一个框，让你选择想要生成的树木数量（**模型数量**）。最后，你可以决定勾选一个复选框（标记为**使用静态随机种子**），它类似于你在**分区**节点中找到的内容，让你“固定”用于随机抽取行和列的伪随机数生成器的初始化种子：在这种情况下，输入和配置参数相同的情况下，你将始终获得相同的森林：
- en: '![Graphical user interface, application'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，应用程序'
- en: Description automatically generated](img/B17125_05_31.png)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_31.png)
- en: 'Figure 5.31: Configuration window of the Random Forest Learner node: how many
    trees you want to see in the forest?'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.31：随机森林学习器节点的配置窗口：你想在森林中看到多少棵树？
- en: 'Let''s implement the **Random Forest Learner** node and connect the training
    set (the first output port of the **Partitioning** node) with its input: there
    is no harm in reusing the same training and test sets used for the decision tree
    learner. If we execute the node and open its main view (*F10* or right-click and
    then select **View: Tree Views**), we will find a tree-like output, as in the
    case of the decision trees: however, this time, we have a little selector at the
    top that lets us scroll across all 100 trees of the forest.'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实现**随机森林学习器**节点，并将训练集（**分区**节点的第一个输出端口）连接到它的输入：重新使用决策树学习器使用的相同训练和测试集是没有问题的。如果我们执行该节点并打开其主视图（*F10*或右键点击然后选择**视图：树视图**），我们将看到类似树形结构的输出，就像决策树的情况一样：然而，这次，我们在顶部有一个小的选择器，允许我们滚动查看森林中的100棵树。
- en: 'Random forests are **black box** models as they are hard to interpret: going
    through 100 different trees would not offer us a hint for explaining how the predictions
    are made. However, there is a simple way to check which features proved to be
    most meaningful. Open the second outport of the **Random Forest Learner** node
    (right-click and click on **Attribute statistics**). The first column—called *#splits
    (level 0)*—tells you how many times that feature was selected as the top split
    of a tree. The higher that number, the more useful that feature has been in the
    learning process of the model.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是**黑箱**模型，因为它们难以解释：浏览100棵不同的树并不能提供关于如何做出预测的线索。然而，有一种简单的方法可以检查哪些特征最为重要。打开**随机森林学习器**节点的第二个输出端口（右键点击并选择**属性统计**）。第一列——称为*#划分（级别0）*——告诉你该特征作为树的顶部划分被选择了多少次。这个数字越高，说明该特征在模型的学习过程中越有用。
- en: '![](img/image062.png) *Random Forest Predictor*'
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![](img/image062.png) *随机森林预测器*'
- en: 'This node (**Analytics > Mining > Decision Tree Ensemble > Random Forest >
    Classification**) applies a random forest model (which needs to be provided in
    the first gray input port) to a dataset (second port) and returns the ensemble
    prediction for each input row. As part of its configuration, you can decide whether
    you want to output the propensity scores for each individual class (**Append individual
    class probabilities**). If you tick the **Use soft voting** box, you enable a
    more accurate estimation of propensity: in this case, the vote of each tree will
    be weighted by a factor that depends on how many samples fell in each leaf during
    the learning process. The more samples a leaf has "seen," the more confident we
    can be about its estimation. To use this feature, you will have to select the
    option **Save target distribution…** in the **Random Forest Learning** node, which
    is upstream.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点（**分析 > 数据挖掘 > 决策树集成 > 随机森林 > 分类**）应用随机森林模型（需要在第一个灰色输入端口提供）到一个数据集（第二个端口），并返回每一行输入的集成预测。作为其配置的一部分，您可以决定是否要输出每个单独类的倾向分数（**附加个别类概率**）。如果勾选**使用软投票**框，您可以启用更准确的倾向估计：在这种情况下，每棵树的投票将根据学习过程中每个叶子中包含的样本数量进行加权。一个叶子“看到”的样本越多，我们就越能相信它的估计。要使用此功能，您必须在上游的**随机森林学习**节点中选择**保存目标分布…**选项。
- en: '![](img/B17125_05_32.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_32.png)'
- en: 'Figure 5.32: The configuration dialog of Random Forest Learner node. You can
    decide whether you want to see propensity scores or not.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.32：随机森林学习器节点的配置对话框。您可以决定是否查看倾向分数。
- en: Drag and drop the **Random Forest Predictor** node onto the workflow and connect
    its inputs with the forest model, outputted by the **Random Forest Learner** and
    the training set, meaning the bottom outport of the **Partitioning** node. Configure
    the node by unticking the **Append overall prediction confidence** box, and ticking
    both the **Append individual class probabilities** (we need the propensity score)
    and the **Use soft voting** boxes. After you execute it, you will find at its
    output the test set enriched with the prediction, *Prediction (Outcome)*, and
    the propensity scores by class. Specifically, the propensity of a customer being
    interested in our product is *P (Outcome=Yes)*.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将**随机森林预测器**节点拖放到工作流中，并将其输入与由**随机森林学习器**输出的森林模型和训练集连接，即**划分**节点的底部输出端口。配置节点时，取消选中**附加整体预测置信度**框，勾选**附加个别类概率**（我们需要的是倾向分数）和**使用软投票**框。执行后，您将在其输出中找到已被预测、*预测（结果）*以及按类划分的倾向分数的测试集。具体来说，顾客对我们产品感兴趣的倾向是*P（结果=是）*。
- en: Implement a new **Scorer** node (for simplicity, you can copy/paste the one
    you used for the decision tree) and connect it downstream to the **Random Forest
    Predictor**. For its configuration, just make sure you select *Outcome* and *Prediction
    (Outcome)* in the first two drop-down menus. Execute it and open its main output
    view (*F10*).
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个新的**Scorer**节点（为了简便，您可以复制/粘贴您为决策树使用的那个），并将其连接到**随机森林预测器**的下游。对于它的配置，只需确保在前两个下拉菜单中选择*结果*和*预测（结果）*。执行后，打开其主输出视图（*F10*）。
- en: 'The results of **Scorer** (*Figure 5.33*) confirm that, at least in this case,
    the ensemble model comes with better performance metrics. Accuracy has increased
    by a few decimal points and, most importantly (as it directly affects the ROI
    of our marketing campaigns), precision has reached 72% (open the **Accuracy statistics**
    outport to check it or compute it easily from the confusion matrix):'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Scorer**的结果（*图 5.33*）证实，至少在这个案例中，集成模型的性能指标更好。准确度提高了几个小数点，最重要的是（因为它直接影响我们营销活动的投资回报率），精确度达到了72%（可以打开**准确度统计**输出端口查看，或者从混淆矩阵中轻松计算出来）：'
- en: '![](img/B17125_05_33.png)'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17125_05_33.png)'
- en: 'Figure 5.33: The Scorer node output for our random forest. Both accuracy and
    precision increased versus the decision tree: diversity helps'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.33：我们随机森林的**Scorer**节点输出。与决策树相比，准确度和精确度都有所提高：多样性有所帮助。
- en: Now that we have confirmation that we have built a robust model at hand, let's
    concentrate on the propensity score we calculated and see what we can do with
    it.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确认建立了一个强健的模型，让我们集中精力在计算出的倾向分数上，看看我们可以用它做什么。
- en: 'Open the output of the **Random Forest Predictor** node and sort the rows by
    decreasing level of propensity (click on the header of column *P (Outcome=yes)*
    and then on **Sort Descending**): you will obtain a view similar to the one shown
    in *Figure 5.34*:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 打开**随机森林预测器**节点的输出，并按倾向值从高到低对行进行排序（点击*P（Outcome=yes）*列的标题，然后点击**降序排序**）：你将获得类似于*图
    5.34*所示的视图：
- en: '![](img/B17125_05_34.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_34.png)'
- en: 'Figure 5.34: The predictions generated by the random forest in descending order
    of propensity, P (Outcome=yes): the more we go down the list, the less interested
    customers (column Outcome) we find'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.34：随机森林生成的按倾向值降序排列的预测，P（Outcome=yes）：我们在列表中往下翻时，会发现越来越多的不感兴趣的客户（Outcome列中显示no）
- en: 'At the top of the list, we have the customers in the test set that most decision
    trees identified as interested. In fact, if you look at the column *Outcome*,
    we find that most rows show a `yes`, proving that, indeed, these customers were
    very interested in the product (when called, they agreed to open the savings account).
    If you scroll down the list, the propensity will go down and you will start finding
    increasingly more `no` values in column *Outcome*. Now, let''s think about the
    business case once again: now that we have a model able to predict the level of
    propensity, we could run it on the other 97% of customers that were not contacted
    as part of the pilot. If we then sorted our customer list by decreasing level
    of propensity (as we just did on the test set), we will obtain a prioritized list
    of the next people to call about our product. We will expect that the first calls
    (the ones directed to the most inclined people) will end up with a very high success
    rate (like we noticed in the test set).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表顶部，我们有测试集中的客户，这些客户是大多数决策树认为感兴趣的。事实上，如果你查看*Outcome*列，会发现大多数行显示`yes`，证明这些客户确实对产品非常感兴趣（当联系他们时，他们同意开设储蓄账户）。如果你向下滚动列表，倾向值会逐渐降低，你会开始在*Outcome*列中看到越来越多的`no`值。现在，让我们再次思考这个商业案例：既然我们有一个能够预测倾向水平的模型，那么我们可以在没有参与试点的其他97%的客户身上运行它。如果我们按照倾向值的降序排列客户名单（就像我们在测试集上做的那样），我们将获得一份优先的客户呼叫名单。我们预计，最初的电话（打给倾向性最强的人）会有很高的成功率（就像我们在测试集中看到的那样）。
- en: 'Then, little by little, the success rate will decay: more and more people will
    start saying `no` and, at some point, it will start to become counterproductive
    to make a call. So, the key question becomes: at what point should we "stop" to
    get the maximum possible ROI from the initiative? How many calls should we make?
    What is the minimum level of propensity, below which we should avoid attempting
    to make a sale? The exciting part of propensity modeling is that you can find
    an answer to these questions before making *any* call!'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，随着时间的推移，成功率会逐渐下降：越来越多的人会开始说`no`，并且在某一时刻，打电话就会变得没有意义。那么，关键问题是：我们应该在什么时刻“停止”以获得该行动的最大投资回报率？我们应该打多少电话？什么是最小的倾向水平，低于这个水平我们应该避免尝试销售？倾向建模的一个令人兴奋的地方在于，你可以在打*任何*电话之前就找到这些问题的答案！
- en: 'In fact, if we assume that the customers that were part of the pilot were a
    fair sample of the total population, then we can use our test set (which has not
    been "seen" by the training algorithm, so there is no risk of overfitting) as
    a base for simulating the ROI of a marketing campaign where we call customers
    by following a decreasing level of propensity. This is exactly what we are going
    to do right now: we will need to first sort the test set by decreasing level of
    propensity (the temporary sorting we did earlier did not impact the permanent
    order of the rows in the underlying table); then, we calculate the cumulative
    profit we would make by "going through the list," using the cost and revenue estimates
    shared by the product manager. We check at which level of propensity we maximized
    our profit, so that we have a good estimate of the number of calls that we will
    need to make in total to optimize the ROI. Let''s get cracking!'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，如果我们假设参与试点的客户是整个客户群的一个公平样本，那么我们可以将测试集（它没有被训练算法“看到”，因此不存在过拟合的风险）作为基础，模拟通过按照倾向值降序呼叫客户的营销活动的ROI。这正是我们现在要做的：我们需要先按倾向值降序对测试集进行排序（之前进行的临时排序不会影响底层表中行的永久顺序）；然后，我们计算通过“遍历名单”所能获得的累计利润，使用产品经理提供的成本和收入估算数据。我们检查在哪个倾向水平下，我们的利润最大化，从而能准确估算出优化ROI所需打的电话数量。开始吧！
- en: Implement a **Sorter** node and connect it at the output of the **Random Forest
    Predictor** node. We want to sort the customers in the test set by decreasing
    level of propensity, so select column *P (Outcome=yes)* and go for the **Descending**
    option.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个**排序器**节点，并将其连接到**随机森林预测器**节点的输出端。我们希望按倾向性降序对测试集中的客户进行排序，因此选择列*P (Outcome=yes)*并选择**降序**选项。
- en: Implement a **Rule Engine** node to calculate the marginal profit we make on
    each individual customer. We know that every call we make costs us $15, irrespective
    of its outcome. We also know that every account opening brings an incremental
    revenue of $60\. Hence, every customer that ends up buying the product (*Outcome*=`Yes`)
    brings $45 of profit while all others hit us by $–15\. Let's create a column (we
    can call it *Profit*) that implements this simple logic, as shown in *Figure 5.35*:![Graphical
    user interface, text, application
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个**规则引擎**节点来计算我们在每个客户身上赚取的边际利润。我们知道每次通话都会花费我们15美元，无论结果如何。我们还知道每个账户开通都会带来60美元的增量收入。因此，每个最终购买产品的客户（*Outcome*=`Yes`）带来的利润是45美元，而其他所有客户的损失是-15美元。我们来创建一个列（可以称之为*Profit*），来实现这个简单的逻辑，如*图
    5.35*所示：![图形用户界面，文本，应用程序
- en: Description automatically generated](img/B17125_05_35.png)
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_35.png)
- en: 'Figure 5.35: The Rule Engine node for calculating the marginal profit for each
    individual customer'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.35：用于计算每个客户边际利润的规则引擎节点
- en: To calculate the cumulative profit we will need to use a new node, called **Moving
    Aggregation**.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算累计利润，我们需要使用一个新的节点，叫做**移动聚合**。
- en: '![A picture containing text, clipart'
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![一张包含文本的图片，剪贴画'
- en: Description automatically generated](img/image067.png) *Moving Aggregation*
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/image067.png) *移动聚合*
- en: 'As the name suggests, this node (**Other Data Types > Time Series > Smoothing**)
    aggregates values on moving windows and calculates cumulative summarizations.
    To use a moving window, you will have to declare the **Window length** in terms
    of the number of rows to be considered and the **Window type** (meaning the direction
    of movement of the window in the table). For example, if you select **3** as the
    length and **Backward** as the type, the previous 3 rows will be aggregated together.
    If you want to aggregate by cumulating values from the first row to the last,
    you need to check the **Cumulative computation** box. Similarly to a Group By
    node, the **Aggregation settings** tab will let you select which columns should
    be aggregated and using which method:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，这个节点（**其他数据类型 > 时间序列 > 平滑**）会在移动窗口上聚合值并计算累计汇总。要使用移动窗口，你需要声明**窗口长度**（即要考虑的行数）和**窗口类型**（即窗口在表格中的移动方向）。例如，如果选择**3**作为长度，**向后**作为类型，则会将前3行聚合在一起。如果你想通过从第一行到最后一行累积值进行聚合，则需要勾选**累计计算**框。与“分组”节点类似，**聚合设置**标签会让你选择要聚合的列以及使用的聚合方法：
- en: '![Graphical user interface, text, application, email'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，文本，应用程序，电子邮件'
- en: Description automatically generated](img/B17125_05_36.png)
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_36.png)
- en: 'Figure 5.36: Configuration dialog of the Moving Aggregation node: you can aggregate
    through moving windows or by progressively cumulating'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.36：移动聚合节点的配置对话框：你可以通过移动窗口或逐步累积的方式进行聚合。
- en: Implement the **Moving Aggregation** node and connect it downstream from the
    **Rule Engine**. Check the **Cumulative Computation** box, double-click on the
    *Profit* column on the left, and select **Sum** as the aggregation method. Execute
    the node and open its outport view.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现**移动聚合**节点，并将其连接到**规则引擎**的下游。勾选**累计计算**框，双击左侧的*Profit*列，选择**求和**作为聚合方法。执行节点并打开其输出视图。
- en: The **Moving Aggregation** node has cumulated the marginal profit generated
    by each customer. If we scroll the list (similar to the one displayed in *Figure
    5.37*) and keep an eye on the last column, *Sum(Profit)*, we noticed that the
    profit peaks when we are slightly below the first third of the full list. When
    the *P (Outcome=yes)* propensity is near 0.23, we obtain a profit of around $8,200\.
    This means that by calling only people above this level of propensity (called
    the **Cutoff** point), we maximize the ROI of our campaign.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**移动聚合**节点已经累积了每个客户产生的边际利润。如果我们滚动列表（类似于*图 5.37*中显示的列表），并且注意到最后一列*Sum(Profit)*，我们会发现当我们略低于完整列表的前三分之一时，利润达到峰值。当*P
    (Outcome=yes)*的倾向性接近0.23时，我们获得大约8,200美元的利润。这意味着，通过只联系倾向性高于这一水平的人（即**分界点**），我们最大化了我们活动的投资回报率（ROI）。'
- en: '![](img/B17125_05_37.png)'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17125_05_37.png)'
- en: 'Figure 5.37: The output of the Moving Aggregation node: it seems that we reach
    maximum profit when we call people having a propensity of around 0.23.'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.37：移动聚合节点的输出：似乎当我们呼叫倾向得分大约为0.23的人时，我们达到了最大利润。
- en: To make this concept clearer, let's visualize the changing profit by employing
    a line chart.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这个概念更清晰，我们通过使用线图来可视化变化的利润。
- en: '![A picture containing text'
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![一张含有文字的图片'
- en: Description automatically generated](img/image070.png) *Line Plot (local)*
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/image070.png) *线图（本地）*
- en: This node (**View > Local (Swing)**) generates a line plot. The only configuration
    that might be needed is the box labeled **No. of rows to display**, which you
    can use to extend the limit of rows considered for creating the plot.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 此节点（**视图 > 本地（Swing）**）生成一个线图。唯一可能需要配置的选项是标记为**显示行数**的框，您可以使用它来扩展用于创建图表的行数限制。
- en: Implement a **Line Plot (local)** node, extend the number of rows to display
    to at least 3,000 (the size of the test set), execute it, and open its view at
    once (*Shift* + *F10*). In the **Column Selection** tab, keep only *Sum(Profit)*
    on the right and remove all other columns.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个**线图（本地）**节点，将显示的行数扩展到至少3,000行（测试集的大小），执行它，并立刻打开其视图（*Shift* + *F10*）。在**列选择**标签中，仅保留右侧的*Sum(Profit)*，并移除其他所有列。
- en: 'The output of the chart (shown in *Figure 5.38*) confirms what we noticed in
    the table and makes it more evident: if we use the propensity score to decide
    the calling order of customers, our profit will follow the shape of the curve
    in the figure. We will start with a steep increase of profit (see the first segment
    on the left), as most of the first people we call (which are top prospects, given
    their high propensity score) will actually buy the product. Then, at around one-third
    of the list (when we know that the propensity score is near 0.23), we reach the
    maximum possible profit. After that, it will drop fast as we will encounter fewer
    and fewer interested customers. If we called all the people on the list, we will
    end up with a significant loss, as we have painfully learned as part of the pilot
    campaign:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图表的输出（如*图5.38*所示）证实了我们在表格中注意到的内容，并使其更加明显：如果我们使用倾向得分来决定客户的呼叫顺序，我们的利润将跟随图中的曲线形状。我们会从利润的急剧增加开始（见左侧的第一段），因为我们打电话给的第一批人（这些人是潜在客户，因为他们的倾向得分很高）实际上会购买产品。接下来，大约在列表的三分之一处（当我们知道倾向得分接近0.23时），我们达到了最大可能的利润。之后，随着我们遇到越来越少的感兴趣的客户，利润将迅速下降。如果我们打完列表上的所有人，我们将遭遇显著的损失，这也是我们在试点活动中痛苦学到的经验：
- en: '![Graphical user interface, chart'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，图表'
- en: Description automatically generated](img/B17125_05_38.png)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_38.png)
- en: 'Figure 5.38: The cumulative profit curve for our machine learning-assisted
    telemarketing campaign: we maximize the ROI at around one-third of the list sorted
    by propensity'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.38：我们机器学习辅助的电话营销活动的累积利润曲线：我们在按倾向排序的列表的三分之一处最大化了投资回报率
- en: 'Thanks to this simulation, we have discovered that if we limit our campaign
    to customers with a propensity score higher than 0.23 (which will be around one-third
    of the total population), we will maximize our profit. By doing the required proportions
    (our simulation covered *only* the test set, so 3,000 customers in total), we
    can estimate how much profit we would make if we applied our propensity model
    to the *entire* bank database. In this case, we would use the scores to decide
    who to call within the remaining 97% of the customer base. The overall "size of
    the prize" of conducting a mass telemarketing campaign will bring around $800,000
    of profit, if we were to call one-third of the bank''s customers. Considering
    that it might not be viable to make so many calls, we might stop earlier in the
    list: in any case, we will make some considerable profit by following the list
    that our random forest can now generate. The simulation that we just did can be
    used as a tool for planning the marketing spend and sizing the right level of
    investment. The product manager and your boss are pleased with the great work
    you pulled together. You definitely proved that spotting (and following) the ML
    way can bring sizeable value to the business: in this case, you completely reversed
    the potential outcome of a marketing campaign. The heavy losses in the pilot can
    now be transformed into a meaningful value, thanks to data, algorithms, and—most
    importantly—your expertise in leveraging them. It was a terrific result, and it
    took only 12 KNIME nodes (*Figure 5.39*) to put all of this together!'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这次模拟，我们发现，如果将活动限定在倾向得分高于 0.23 的客户（大约占总人口的三分之一），我们将最大化利润。通过计算所需的比例（我们的模拟仅涵盖了*测试集*，总共有
    3,000 名客户），我们可以估算如果将我们的倾向模型应用到*整个*银行数据库，我们将获得多少利润。在这种情况下，我们将使用这些得分来决定在剩余 97% 的客户群中呼叫谁。进行大规模电话营销活动的整体“奖池”将带来约
    80 万美元的利润，如果我们呼叫三分之一的银行客户。考虑到打这么多电话可能不可行，我们可能会在列表的前面停下来：无论如何，按照我们随机森林现在可以生成的列表进行操作，我们将获得可观的利润。我们刚刚进行的模拟可以作为计划营销支出和确定正确投资水平的工具。产品经理和你的老板对你所做的出色工作感到满意。你无疑证明了识别（并遵循）机器学习方法能够为业务带来可观的价值：在这个案例中，你完全扭转了营销活动的潜在结果。由于数据、算法，以及最重要的，你在利用这些技术方面的专业知识，原本在试点中的巨大损失现在可以转化为有意义的价值。这是一个了不起的结果，而这一切只用了
    12 个 KNIME 节点（*图 5.39*）就完成了！
- en: '![](img/B17125_05_39.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_39.png)'
- en: 'Figure 5.39: Full workflow for the bank telemarketing optimization'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.39：银行电话营销优化的完整工作流程
- en: Segmenting consumers with clustering
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用聚类对消费者进行细分
- en: In this tutorial, you will re-enter the shoes of the business analyst working
    for the online retailer we encountered in *Chapter 3*, *Transforming Data*. This
    time, instead of automating the creation of a set of financial reports, you are
    after a seemingly sexier objective. The **Customer Relationship Management** (**CRM**)
    team is looking for a smarter way to communicate with those customers who opted-in
    to receive regular newsletters. Instead of sending a weekly email equal for all,
    the CRM manager asked you to find a data-based approach for creating a few meaningful
    consumer segments. Once segments are defined, the CRM team can build multiple
    messages, one for each segment. By doing so, they will offer a more personalized
    (and engaging) experience for the entire customer base, which will ultimately
    affect customer loyalty and drive sustainable revenue growth.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将再次扮演我们在*第三章*《数据转化》中遇到的在线零售商的业务分析师。这一次，你不再是自动化创建一组财务报告，而是追求一个看起来更具吸引力的目标。**客户关系管理**（**CRM**）团队正在寻找一种更智能的方式，与那些选择接收定期新闻通讯的客户进行沟通。与其发送一封统一的每周电子邮件，CRM
    经理要求你找到一种基于数据的方法来创建几个有意义的消费者细分。一旦定义了细分，CRM 团队可以为每个细分创建多个消息。通过这样做，他们将为整个客户群提供更加个性化（且更具吸引力）的体验，最终影响客户忠诚度并推动可持续的收入增长。
- en: 'Unsupervised learning offers a proven methodology that can meet this business
    need: by using a clustering algorithm, we can create several groups of customers
    that *look similar* in terms of their characteristics (such as age, family composition,
    and income level) and the consumption patterns they displayed through previous
    purchases (like the average price of the products they selected, the overall amount
    of money they spent, or the frequency of their orders). This is the ML way of
    helping the business: use clustering to segment consumers appropriately.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习提供了一种经过验证的方法论，可以满足这一业务需求：通过使用聚类算法，我们可以根据客户的特征（如年龄、家庭组成和收入水平）以及他们通过先前购买所表现出的消费模式（例如他们选择的产品的平均价格、总花费金额或订单频率）将客户划分为几个“相似”的群体。这就是机器学习帮助业务的方式：使用聚类对消费者进行适当的细分。
- en: 'The CRM manager has already initiated the gathering of some basic consumer-level
    data and obtained a CSV file (`eCommerce-CRM.csv`), which has 4,157 rows—one for
    each customer—and four columns:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: CRM 经理已经开始收集一些基本的消费者级别数据，并获得了一个 CSV 文件（`eCommerce-CRM.csv`），该文件有 4,157 行——每一行代表一个客户——以及四列数据：
- en: '*Customer_ID*: a unique identifier of the customer.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Customer_ID*：客户的唯一标识符。'
- en: '*Average Price*: the average unit price for all purchases made by each customer.
    It gives us a directional view of the "premiumness" of the former shopping choices
    displayed by the customer.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Average Price*：每个客户所有购买商品的平均单价。它为我们提供了客户过去购物选择的“高端性”方向性视图。'
- en: '*Basket Size*: the average number of units purchased within any single order
    created by the customers. This measure indicates whether they prefer to go for
    "bulk" shopping with fat baskets or smaller, occasion-driven purchase acts.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Basket Size*：客户在每次下单中购买的平均商品数量。这个指标显示了他们是否更倾向于进行“大宗”购物，即购买大量商品，还是偏好小规模的、基于需求的购物行为。'
- en: '*Unique Products*: the average number of different articles that the customer
    buys on each occasion. This metric indicates the breadth of the assortment "tried"
    by each customer. It gives us an idea of the customer''s willingness to explore
    new products versus their preference of "keep buying" the same articles all the
    time.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Unique Products*：客户每次购物时购买的不同商品的平均数量。这个指标显示了每个客户“尝试”的商品种类的广度。它为我们提供了客户是否愿意探索新产品，还是更倾向于“一直购买”相同商品的一个线索。'
- en: 'As we exchange thoughts with the CRM manager about this dataset, she confirms
    what we had already noticed: the three consumption metrics included in the data
    (the last three columns) are far from giving us a comprehensive picture of each
    customer''s preferences. If we wanted, we could have generated many more columns
    by aggregating the transactions history: think about the absolute number of purchases
    by customer, the total generated value, the "mix" of purchased categories and
    subcategories, the premiumness of the purchased products within each category
    and subcategory, and also the customer characteristics, like their age, the average
    income of the neighborhood they live in, and so on. Still, we decide to go ahead
    and leverage the power of machine learning on this first dataset: we can always
    increase the level of the model sophistication later if we want. Now, the important
    thing is to "start rocking" and pragmatically prove some first business value
    from this new way of operating. In terms of deliverables, you align with your
    business partner the need to assign each customer to a small number of clusters
    and put together some visualizations to interpret what differentiates clusters.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们与 CRM 经理讨论这个数据集时，她确认了我们已经注意到的一个问题：数据中包含的三个消费指标（最后三列）远未能为我们提供每个客户偏好的全面图景。如果我们愿意的话，可以通过汇总交易历史生成更多列：比如按客户计算的购买总数、产生的总价值、购买类别和子类别的“组合”、每个类别和子类别内购买产品的高端程度，还有客户的一些特征，比如他们的年龄、居住区的平均收入等。然而，我们决定继续使用机器学习的力量来处理这个第一个数据集：如果以后需要，我们总可以提高模型的复杂度。现在，重要的是要“开始行动”，并务实地证明从这种新的运营方式中能提取出一些初步的业务价值。在交付物方面，你与业务伙伴达成一致，决定将每个客户分配到少数几个簇中，并整理一些可视化图表来解释簇与簇之间的差异。
- en: 'It''s time to power KNIME on, create a new workflow, and load our CRM extract
    into it:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候启动 KNIME，创建一个新的工作流，并将我们的 CRM 提取数据加载到其中：
- en: Load the file `eCommerce-CRM.csv` onto the workflow editor. As the **CSV Reader**
    node dialog pops up, we can check that all four columns are showing in the preview
    and click **OK** to confirm the default setting. After executing the node, we
    can look at its output view (*Figure 5.40*) and move to the next step:![Table
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件`eCommerce-CRM.csv`加载到工作流编辑器中。当**CSV Reader**节点对话框弹出时，我们可以检查预览中是否显示了所有四列，然后点击**OK**确认默认设置。执行节点后，我们可以查看输出视图（*图
    5.40*）并继续进行下一步：![Table
- en: Description automatically generated](img/B17125_05_40.png)
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_40.png)
- en: 'Figure 5.40: The CRM extract once loaded: for every customer, we have three
    metrics, each one giving us a hint of their shopping habits'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.40：加载后的CRM提取数据：对于每个客户，我们有三个指标，每个指标都能给我们一些关于其购物习惯的线索
- en: Creating homogenous groups of elements, such as customers in our case, requires
    the use of a clustering algorithm. Let's make acquaintance with possibly the most
    popular clustering algorithm available today – **k-means**.
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建同质的元素组，例如我们案例中的客户，需要使用聚类算法。让我们认识一下今天可能最流行的聚类算法——**k-均值**算法。
- en: K-means algorithm
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-均值算法
- en: 'The k-means algorithm is perhaps the easiest (and yet probably the most used)
    approach used for clustering. The big idea is elementary and can be summarized
    in two lines: each element in a dataset is assigned to the closest cluster. At
    each step of the process, the position of the clusters gets updated, so they become
    more and more compact.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: k-均值算法可能是最简单（也是最常用）的一种聚类方法。它的核心思想非常简单，可以用两行话总结：数据集中的每个元素都被分配到最接近的聚类中。在每一步中，聚类的位置都会被更新，因此它们会变得越来越紧凑。
- en: 'Let''s imagine we want to cluster a set of points displayed on a bi-dimensional
    scatter plot. Each point is described employing two numbers that represent the
    horizontal and the vertical coordinates, respectively. The distance between any
    two points can be easily calculated through the Pythagorean theorem (yes, the
    same used for calculating the sides of a right triangle—see *Figure 5.41* for
    a refresher):'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要对一个二维散点图上显示的点集进行聚类。每个点由两个数字描述，分别代表水平和垂直坐标。两点之间的距离可以通过毕达哥拉斯定理轻松计算（是的，就是那个用来计算直角三角形边长的定理——有关细节，请参见*图
    5.41*）：
- en: '![Chart, line chart'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '![Chart, line chart'
- en: Description automatically generated](img/B17125_05_41.png)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_41.png)
- en: 'Figure 5.41: Calculating the distance between two points using the Pythagorean
    theorem: you make the square root of the sum of the squared differences for each
    coordinate'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.41：使用毕达哥拉斯定理计算两点之间的距离：你需要计算每个坐标差的平方和的平方根
- en: 'The goal of the k-means algorithm is to create a given number (*k*) of homogenous
    groups formed by points that are relatively close to one another. Like many other
    machine learning algorithms, k-means has an iterative approach: at each iteration,
    it groups the points based on their proximity to some special points called the
    **centroids** of each cluster. Every point is associated with its closest centroid.
    The algorithm then updates the position of the centroids iteratively: at each
    iteration, the groups will tend to be more and more homogenous, meaning that the
    points forming these clusters will be gradually closer and closer to each other.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: k-均值算法的目标是创建指定数量（*k*）的同质组，这些组由彼此相对接近的点组成。像许多其他机器学习算法一样，k-均值采用迭代方法：每次迭代时，它根据点与称为**质心**的特殊点的接近程度进行分组。每个点都与其最近的质心关联。然后，算法通过迭代更新质心的位置：在每次迭代中，聚类将趋向于越来越同质化，这意味着这些聚类中的点将逐渐越来越接近。
- en: 'Let''s see in detail the sequence of steps that make the k-means algorithm:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看k-均值算法的步骤：
- en: '**Initialization**: the first step of the algorithm is making the initial choice
    of the centroids, one per cluster. There are different ways to make this choice.
    The simplest way is to randomly select *k* points in our dataset.'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化**：算法的第一步是选择初始质心，每个聚类一个质心。选择方法有很多种。最简单的方式是从我们的数据集中随机选择*k*个点。'
- en: '**Grouping**: the algorithm now calculates the distance of each point from
    each centroid (using the Pythagorean theorem), and each point is matched with
    its closest centroid (the one lying at the smallest distance). In this way, all
    the points near a centroid are grouped together as they belong to the same cluster.'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分组**：算法现在计算每个点到每个质心的距离（使用勾股定理），然后每个点都会与其最近的质心匹配（即距离最小的质心）。通过这种方式，靠近一个质心的所有点将被分在一起，因为它们属于同一个簇。'
- en: '**Update**: the algorithm now calculates the centroid of each cluster again
    by making an average of the coordinates of all the points that belong to the cluster.
    Basically, the centroid is updated so that it matches the center of mass of the
    newly formed group.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新**：算法现在重新计算每个簇的质心，通过对属于该簇的所有点的坐标进行平均来完成。基本上，质心会更新，以便它与新形成的组的质心一致。'
- en: 'At this point of the process, we return to step *b* to start a new iteration
    and repeat steps *b* and *c* as long as it is possible to improve the centroids.
    At every iteration, the clusters will converge, meaning that they will become
    increasingly more meaningful. We will stop when the update step produces no change
    in the way in which points are assigned to clusters. When this happens, the algorithm
    terminates: a stable solution has been found, and the current definition of clusters
    is returned as the resulting output. Should this convergence not take place, the
    algorithm will stop in any case once a preset number of maximum iterations is
    reached.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程的这一阶段，我们返回到步骤 *b* 开始新的迭代，并在可能改善质心的情况下重复步骤 *b* 和 *c*。每次迭代时，簇会收敛，意味着它们会变得越来越有意义。当更新步骤不再改变点与簇的分配时，我们就停止。此时，算法终止：一个稳定的解决方案已经找到，当前的簇定义将作为结果输出。如果收敛没有发生，算法无论如何都会在达到预设的最大迭代次数时停止。
- en: 'This process might still look complicated but let me stress how simple the
    underlying mathematics is: random draws, averages, squares, and the square roots
    in the Pythagorean theorem are all the math we need to implement the k-means algorithm.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程看起来可能仍然比较复杂，但让我强调一下其背后数学的简单性：随机抽样、平均值、平方和勾股定理中的平方根就是我们实现 k-means 算法所需的全部数学。
- en: 'To better understand how the algorithm works, let''s go through a concrete
    example and use some charts to display the evolution of the various iterations
    graphically. For the sake of simplicity, we will use a simple dataset formed only
    by two columns: by having only two columns, we can visualize the distances between
    the various points on 2-dimensional scatter plots (Cartesian diagrams).'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解算法是如何工作的，我们通过一个具体的例子来讲解，并使用一些图表直观展示不同迭代的演变。为了简化起见，我们将使用一个仅包含两列的简单数据集：通过仅使用两列数据，我们可以在二维散点图（笛卡尔坐标图）上可视化不同点之间的距离。
- en: 'When we work with datasets with more than two columns (as is usually the case),
    the concept of distance becomes more difficult to visualize in our human mind.
    While, with three columns, we can still imagine the algorithm working on a 3-dimensional
    space, with 4, 5, or 10 columns, we will necessarily need to delegate the task
    to machines. Luckily, they are much more at ease than humans when navigating multidimensional
    spaces. The good news is that the basic formula for calculating distances (the
    Pythagorean theorem you found in *Figure 5.41*) stays the same: you will have
    to calculate the squares of the distances across *all* dimensions—no matter how
    many they are—and sum them across.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理包含多于两列的数据集时（通常情况就是如此），距离的概念变得更加难以在人类的思维中可视化。虽然在三列数据时，我们仍然能想象算法在三维空间中工作，但在四列、五列或十列数据时，我们就必须将任务交给机器了。幸运的是，它们在多维空间中比人类更得心应手。好消息是，计算距离的基本公式（你在
    *图 5.41* 中找到的勾股定理）保持不变：你只需要计算 *所有* 维度的距离的平方——无论维度有多少——并对它们进行求和。
- en: 'Going back to the real estate sector for a second, let''s imagine that we have
    a dataset describing 16 properties utilizing their price per square meter and
    their age in years (see *Figure 5.42* on the left). We want to cluster these properties
    in three homogeneous clusters. The business reason we want to create such a cluster
    is immediate: should a client show interest in any of these properties, we want
    to immediately recommend considering all other properties in the same cluster
    since they should exhibit *similar* features. This example looks naïve with 16
    properties: we wouldn''t need k-means to identify similarities with so little
    data involved. However, the beauty of k-means is that it could easily scale to
    many dimensions and properties, while our human brain would start struggling with
    a few more data points:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 回到房地产领域，我们假设有一个数据集，描述了16个物业的每平方米价格和年龄（见左侧的*图 5.42*）。我们希望将这些物业聚类为三个同质聚类。我们想要创建这种聚类的商业理由是显而易见的：如果客户对其中任何一处物业表现出兴趣，我们希望立即推荐考虑同一聚类中的所有其他物业，因为它们应该具有*相似*的特征。这个例子在16个物业中看起来很简单：对于这么少的数据，我们不需要k-means来识别相似性。然而，k-means的美妙之处在于它可以轻松扩展到许多维度和物业，而我们的大脑在面对更多数据点时就会开始挣扎：
- en: '![Chart, scatter chart'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '![图表，散点图'
- en: Description automatically generated](img/B17125_05_42.png)
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_42.png)
- en: 'Figure 5.42: Kicking k-means off: out of the 16 properties with different prices
    and ages (left), three are randomly picked and elected as initial centroids (right)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.42：启动k-means：从16个具有不同价格和年龄的物业中（左），随机选择三处并选为初始质心（右）
- en: 'The first step to run is the initialization: the algorithm will draw at random
    three properties, as three is the number of requested clusters (*k=3*). The algorithm
    has randomly extracted properties **C**, **G**, and **I**, as you can see on the
    right side of *Figure 5.42*. As part of the first iteration, the algorithm will
    proceed with the grouping step: first, it will use the Pythagorean theorem to
    calculate the distances between each property and each centroid and will associate
    every property to its closest centroid out of the three. Let''s follow how k-means
    proceeds at each iteration with the help of the figures. As you can see in the
    left handside of *Figure 5.43*, the grouping step has created three first cluster
    compositions, each one represented by a different color. The blue-colored properties
    (**C**, **A**, **B**, and **D**) are the closest ones to the blue centroid that
    overlaps with property **C**. The ones belonging to the red cluster (**G**, **E**,
    **F**, and **H**) are, instead, closest to the red centroid, **G**. Finally, the
    green cluster is made of the points (**I**, **L**, **M**, **N**, **O**, **P**,
    **Q**, and **R**) whose closest centroid is **I**. The next step for the algorithm
    is to update the centroids: considering the points falling into each cluster,
    it will be enough to calculate the actual center of mass of the cluster by averaging
    out the prices and the ages of the properties belonging to it. For example, let''s
    look at the green cluster: the properties forming this cluster tend to be older,
    leading the new centroid to be placed on the right side of the scatter plot. The
    centroid in the red cluster has instead moved toward the top: indeed, the properties
    associated with this cluster all have in common a higher price compared to point
    **C** (the old centroid):'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是初始化：算法将随机选择三处物业，因为请求的聚类数量是三（*k=3*）。算法随机选取了物业**C**、**G**和**I**，如*图 5.42*右侧所示。在第一次迭代中，算法将进行分组步骤：首先，它将使用毕达哥拉斯定理计算每处物业与每个质心之间的距离，并将每处物业与最接近的质心进行关联。让我们通过图形来跟踪k-means在每次迭代中的进展。正如你在*图
    5.43*左侧所看到的，分组步骤创建了三个初步的聚类，每个聚类由不同的颜色表示。蓝色的物业（**C**、**A**、**B**和**D**）是最接近蓝色质心的，它与物业**C**重合。属于红色聚类的物业（**G**、**E**、**F**和**H**）则最接近红色质心**G**。最后，绿色聚类由物业（**I**、**L**、**M**、**N**、**O**、**P**、**Q**和**R**）组成，它们的最近质心是**I**。接下来的步骤是更新质心：考虑到每个聚类中的点，只需通过计算属于该聚类的物业的价格和年龄的平均值，就能得出聚类的实际质心。例如，我们来看绿色聚类：形成这个聚类的物业往往较老，这使得新的质心出现在散点图的右侧。红色聚类中的质心则向上移动：事实上，与这个聚类相关的物业普遍具有比**C**（旧质心）更高的价格：
- en: '![Chart, scatter chart'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '![图表，散点图'
- en: Description automatically generated](img/B17125_05_43.png)
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_43.png)
- en: 'Figure 5.43: The first full iteration of k-means: with the update step, the
    centroids make a move'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.43：k-means 算法的第一次完整迭代：更新步骤后，质心发生了移动
- en: 'Now we can finally start the second iteration (*Figure 5.44*). Once again,
    we begin by grouping the points using the centroid we have just recalculated.
    As a consequence of this shift in the centroids, the clusters have changed, and
    some properties switched color: for instance, property **E** used to be red and
    is now blue as its closest centroid is now the blue one, and no longer the red
    one. The same applies to points **I** and **L**, which used to be green and are
    now red. It could appear that our algorithm has taken the right road as it is
    converging to a solution that makes sense: after this iteration, the clusters
    have changed in a way that makes their elements closer to each other. In the second
    step of the iteration, the algorithm will again update the centroids, taking into
    account the new compositions of the clusters. The most remarkable change is now
    in the red cluster, whose centroid has moved toward the bottom (where prices are
    lower), given the addition of properties **I** and **L** to the group:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于可以开始第二次迭代（*图 5.44*）。我们再次从利用刚刚重新计算的质心对点进行分组开始。由于质心发生了变化，簇的组成发生了变化，一些属性的颜色也发生了变化：例如，属性
    **E** 以前是红色的，现在变成了蓝色，因为它最近的质心变成了蓝色的，而不再是红色的。**I** 和 **L** 点也是如此，它们曾是绿色的，现在变成了红色的。看起来我们的算法已经走上了正确的道路，因为它正在趋向一个有意义的解决方案：经过这次迭代后，簇的组成发生了变化，使得它们的元素更加接近。第二步迭代中，算法将再次更新质心，考虑到簇的新组成。最显著的变化是红色簇的质心，它已向底部移动（价格较低的地方），这是由于属性
    **I** 和 **L** 被添加到该组：
- en: '![Chart, scatter chart'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '![图表，散点图'
- en: Description automatically generated](img/B17125_05_44.png)
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_44.png)
- en: 'Figure 5.44: The second iteration of k-means: the groups make more and more
    sense'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.44：k-means 算法的第二次迭代：分组变得越来越有意义
- en: 'In the third iteration (*Figure 5.45*), the algorithm repeats the grouping
    step, and other properties change color (for instance, **M** moves from green
    to red, and **F** becomes blue). However, something new happens: despite having
    updated the centroids, the composition of the cluster does not change at all.
    This is the sign that our algorithm has found a stable solution and can be terminated,
    returning our final cluster composition:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三次迭代（*图 5.45*）中，算法重复了分组步骤，其他属性的颜色发生了变化（例如，**M** 从绿色变为红色，**F** 变为蓝色）。然而，发生了一些新的变化：尽管质心已经更新，但簇的组成并没有发生任何变化。这是我们的算法已找到一个稳定解并可以终止的标志，返回最终的簇组成：
- en: '![Chart'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '![图表'
- en: Description automatically generated](img/B17125_05_45.png)
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_45.png)
- en: 'Figure 5.45: The third and last iteration of k-means: no more updates are possible
    and the algorithm converges'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.45：k-means 算法的第三次也是最后一次迭代：无法再进行更新，算法收敛
- en: 'This final cluster composition seems to be making a lot of sense. By looking
    at the scatter plots, we can also attempt a business interpretation of each cluster:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最终的簇组成看起来非常合理。通过观察散点图，我们还可以对每个簇进行商业解读：
- en: Blue properties (**A**, **B**, **C**, **D**, **E**, and **F**) are in the top-left
    corner of our diagram. They were all recently built and, as new properties, they
    tend to display a higher price than the rest.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝色属性（**A**、**B**、**C**、**D**、**E** 和 **F**）位于图表的左上角。它们都是最近建造的，作为新建属性，它们的价格通常高于其他属性。
- en: Red properties (**G**, **H**, **I**, **L**, and **M**) are in the bottom central
    part of the diagram and refer to buildings built in the seventies with lower quality
    materials; hence, their price is more accessible.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 红色属性（**G**、**H**、**I**、**L** 和 **M**）位于图表的底部中央部分，代表的是七十年代建造的建筑，这些建筑使用了较低质量的材料，因此它们的价格更为亲民。
- en: Finally, the green points (**N**, **O**, **P**, **Q**, and **R**) are associated
    with older buildings, which tend to be more prestigious and come with a higher
    price tag.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，绿色点（**N**、**O**、**P**、**Q** 和 **R**）与老旧建筑相关，这些建筑通常更具声望，且价格较高。
- en: The clusters we obtained after only a handful of iterations of the k-means algorithm
    can certainly help real estate agents present convincing alternatives to potential
    buyers. Not bad for an algorithm repeating a set of simple mathematical steps.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 k-means 算法仅经过几次迭代获得的簇，肯定能帮助房地产经纪人向潜在买家展示令人信服的选择。对于一个仅重复一组简单数学步骤的算法来说，表现相当不错。
- en: 'A natural question that comes to mind when using k-means is: what is the right
    value of *k* or, in other words, how many clusters should I create? Even though
    there are some numerical techniques (check out the **Elbow method**, for instance)
    to infer an optimal value for *k*, the business practice of machine learning demands
    taking another, less mathematically rigorous approach. When choosing the number
    of clusters, the advice is to take a step back and think of the actual business
    utilization of the cluster definitions. The right question to ask becomes: how
    many clusters shall I create so that the result can be used in practice in my
    business case? In the example of segmenting consumers for personalizing communication,
    is it reasonable to create—let''s say—100 clusters of consumers if I can only
    afford to produce three versions of a newsletter at most? We will often use the
    business constraints for deciding a range of reasonable values of *k* and then
    pick the one that looks most interpretable. The moral of this story is that data
    analytics is a mix of art and science, and human judgment is often needed to guide
    algorithms to the right path.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 使用k-means时，自然会有一个问题浮现：*k*的正确值是多少，或者换句话说，我应该创建多少个聚类？尽管有一些数值技术（例如**肘部法则**）可以推断出*k*的最佳值，但机器学习的商业实践要求我们采取另一种不那么数学化的方法。在选择聚类数量时，建议后退一步，考虑聚类定义的实际商业用途。需要问的正确问题是：我应该创建多少个聚类，以便结果可以在我的商业案例中实际应用？以消费者细分和个性化沟通为例，如果我最多只能制作三种版本的新闻通讯，那么创建100个消费者聚类是否合理？我们通常会根据商业约束来决定*k*的合理范围，然后选择最易解释的那个值。这段话的寓意是：数据分析既是艺术也是科学，通常需要人类判断来引导算法走上正确的道路。
- en: 'Before moving back to our tutorial flow, let''s go through a couple of considerations
    regarding "what can go wrong" when using a distance-based approach like k-means
    and how to avoid it:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在回到我们的教程流程之前，先考虑几个关于使用基于距离的方法（如k-means）时“可能出错”的问题，以及如何避免这些问题：
- en: '**Outliers can spoil the game**. If some points in your dataset exhibit extreme
    values, they will naturally "stay apart" from the rest, making the clustering
    exercise less meaningful. For example, imagine that in our real estate case, we
    have a single property with a price ten times higher than every other property:
    this exceptional property will probably make a cluster by itself. Most times,
    we don''t want this to happen, so we remove outliers upfront. The **Numeric Outliers**
    node will do the job for us.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离群值会破坏游戏规则**。如果数据集中某些数据点存在极端值，它们自然会“与其他点分开”，从而使得聚类过程失去意义。例如，想象一下在我们的房地产案例中，有一套房产的价格是其他房产的十倍：这套特殊房产可能会单独形成一个聚类。大多数情况下，我们不希望发生这种情况，因此我们会在一开始就去除离群值。**数值离群值**节点将为我们完成这项工作。'
- en: '**Extreme range differences can make distance calculations unbalanced**. This
    one is easy to see through an example. Think again about the formula in *Figure
    5.41* for calculating distances: in the real estate example, it would leverage
    the differences in house prices (which are in the thousands of dollars) and the
    age differences (which, instead, vary in the area of dozens of years). The massive
    gap between the two orders of magnitude becomes even wider when you square them,
    as the formula provides. This means that the house prices will count disproportionally
    more than the age, making the latter almost meaningless. To fix this numeric disadvantage,
    we need to normalize all the measures used in k-means and reduce their scale to
    a common range (generally from zero to one) while keeping the differences across
    data points. This is what the **Normalizer** node (and its inverse companion,
    the **Denormalizer** node) will do for us in KNIME.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**极端的数值范围差异会导致距离计算不平衡**。这一点通过一个例子很容易理解。再看看*图5.41*中用于计算距离的公式：在房地产示例中，它会利用房价差异（单位是几千美元）和房龄差异（单位通常是几十年）。当你对这两个数量级差异进行平方计算时，差距会变得更加显著，如公式所示。这意味着房价在计算中会占据不成比例的比重，而房龄的影响几乎可以忽略不计。为了解决这个数值上的劣势，我们需要对在k-means中使用的所有度量进行标准化，并将它们的范围缩小到一个共同的范围（通常是从零到一），同时保持数据点之间的差异。这就是**标准化器**节点（以及它的逆节点**反标准化器**节点）在KNIME中为我们做的事情。'
- en: 'To avoid these issues, remember this general advice: always remove outliers
    and normalize your data before applying k-means. With more practice and expertise,
    you might be able to "bend" these rules to meet your specific business needs at
    best, but in most cases, these two steps can only improve your clustering results,
    so they are no-brainers. Let''s now see how to apply them in KNIME.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这些问题，请记住以下通用建议：在应用 k-means 之前，始终删除离群值并规范化数据。通过更多的实践和专业知识，你可能能够“弯曲”这些规则，以最佳方式满足你特定的业务需求，但在大多数情况下，这两个步骤只能改善你的聚类结果，因此它们是显而易见的。现在，让我们看看如何在
    KNIME 中应用它们。
- en: '![A picture containing icon'
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![图标图片  '
- en: Description automatically generated](img/image079.png) *Numeric Outliers*
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/image079.png) *Numeric Outliers*
- en: 'This node (**Analytics > Statistics**) identifies outliers in a data table
    and manages them according to the needs. At the top of its configuration window
    (*Figure 5.46*), you can select which columns to consider in the outliers detection:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这个节点 (**Analytics > Statistics**) 在数据表中识别离群值，并根据需要管理它们。在其配置窗口的顶部 (*图 5.46*)，你可以选择考虑哪些列来进行离群值检测：
- en: '![Graphical user interface'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面  '
- en: Description automatically generated](img/B17125_05_46.png)
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_46.png)
- en: 'Figure 5.46: Configuration window of Numeric Outliers node: what do you want
    to do with your extreme values?'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.46：Numeric Outliers 节点的配置窗口：你希望如何处理极端值？
- en: In the **Outlier Treatment** panel on the bottom right, you can decide how to
    manage outliers once detected. In particular, the **Treatment option** drop-down
    menu lets you choose whether you want to **Remove outlier rows** (so as to ignore
    them in the rest of the workflow), **Remove non-outlier rows** (so you keep *only*
    the outliers and study them further), or **Replace outliers values** (by either
    assigning them a missing value status or substituting them with the closest value
    within the permitted range—you can specify your preference in the **Replacement
    strategy** menu).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在右下角的 **Outlier Treatment** 面板中，你可以决定如何处理检测到的离群值。特别地，**Treatment option** 下拉菜单让你选择是要
    **删除离群值行**（以便在其余工作流程中忽略这些行）、**删除非离群值行**（以便只保留 *离群值* 并进一步研究它们），还是 **替换离群值**（通过将它们设为缺失值状态，或将其替换为允许范围内的最接近值——你可以在
    **Replacement strategy** 菜单中指定偏好）。
- en: 'The key parameter for setting the sensitivity to use in detecting outliers
    is the **Interquartile range multiplier (k)**, which you can set on the bottom-left
    area of the configuration window. To understand how it works, have a look at the
    **box plot** shown in *Figure 5.47*:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 设置离群值检测灵敏度的关键参数是 **四分位距倍数 (k)**，你可以在配置窗口的左下角进行设置。要理解它是如何工作的，看看 **图 5.47** 中的
    **箱型图**：
- en: '![Diagram'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '![图示'
- en: Description automatically generated](img/B17125_05_47.png)
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_47.png)
- en: 'Figure 5.47: How to interpret a box-and-whisker plot: the box in the middle
    covers the central 50% of points in a distribution. Beyond the whiskers, you find
    outliers'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.47：如何解读箱线图：中间的盒子覆盖了分布中的 50% 的数据点。超出“胡须”的部分是离群值。
- en: 'Box plots show us at a glance the key features of a numeric distribution: quartile
    values (see in the picture **Q1**, **Q2**, which is the **Median**, and **Q3**)
    tell us where we could "cut" a population of sorted numbers so as to get 25% of
    the values in each slice. Now, look at the central box, whose length is called
    **Interquartile range** (**IQR**): within this range, we will find nearly 50%
    of the values of the population—this is the *core* of our distribution. Keeping
    this in mind, outliers can be defined as the values that lie *far* from this core.
    Typically, the values that are further than 1.5 times the interquartile range
    above the third quartile or below the first quartile are considered **mild outliers**.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 箱型图让我们一眼看出数字分布的关键特征：四分位数值（见图中的 **Q1**、**Q2**，即 **中位数**，和 **Q3**）告诉我们如何“切割”排序后的数字群体，从而使每一段中包含
    25% 的值。现在，看看中间的盒子，其长度被称为 **四分位距**（**IQR**）：在这个范围内，我们将找到几乎 50% 的值——这是我们分布的 *核心*。牢记这一点，离群值可以定义为远离这个核心的值。通常，位于第三四分位数以上
    1.5 倍四分位距或第一四分位数以下 1.5 倍四分位距之外的值被视为 **轻微离群值**。
- en: They are represented as circles in *Figure 5.47*, while the limit of mild outliers
    is represented by the dashed "whiskers" you see above and below the central box
    (this is why box plots are also known as box-and-whisker plots). If you increase
    the multiplier of the interquartile range to 3.0, you find the **extreme outliers**,
    which are shown as crosses in the figure. By editing the interquartile range multiplier
    parameter in the configuration dialog, you can tell the node how "aggressive"
    it should be in detecting outliers.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在*图 5.47*中表示为圆形，而轻微离群值的界限则由你在中央框的上下方看到的虚线“胡须”表示（这也是箱线图又被称为箱形图和胡须图的原因）。如果你将四分位数范围的乘数增加到
    3.0，你会发现**极端离群值**，它们在图中显示为十字形。通过在配置对话框中编辑四分位数范围乘数参数，你可以告诉节点它在检测离群值时应该有多“激进”。
- en: 'Let''s leverage our new node straight away on the CRM dataset:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们立即在 CRM 数据集上利用我们新的节点：
- en: Implement a **Numeric Outliers** node and connect it with the output port of
    the CSV reader. In its configuration window, deselect the column *Customer_ID*
    since we don't want to use it in our clustering. Since we are after extreme outliers,
    set `3.0` as the **Interquartile range multiplier (k)**, and select **Remove outlier
    rows** as the **Treatment option**. Finally, execute the node and have a look
    at its output ports.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个**Numeric Outliers**节点，并将其与 CSV 读取器的输出端口连接。在它的配置窗口中，取消选择*Customer_ID*列，因为我们不想在聚类中使用它。由于我们关注的是极端离群值，将`3.0`设置为**四分位数范围乘数
    (k)**，并选择**删除离群值行**作为**处理选项**。最后，执行该节点并查看它的输出端口。
- en: 'The first output (**Treated table**) is the cleaned-up version of the table,
    showing only 3,772 rows: this means that we removed 10% of rows as they were considered
    outliers according to some columns. We could have played with the IQR multiplier
    value and increased it to 5.0 or more, so as to focus on more extreme values and
    remove fewer rows, but for the sake of this exercise, we can carry on with this.
    The second output of the node (**Summary**, shown in *Figure 5.48*) tells us the
    number of rows regarded as outliers according to each individual column (*Basket
    Size* seems to be the one displaying more extreme values):'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个输出（**处理后的表格**）是已清理的表格版本，显示只有 3,772 行：这意味着我们删除了 10% 的行，因为它们根据某些列被认为是离群值。我们本可以调整
    IQR 乘数值，将其增加到 5.0 或更高，以便关注更极端的值并删除更少的行，但为了本练习的方便，我们可以继续使用当前的设置。该节点的第二个输出（**总结**，如*图
    5.48*所示）告诉我们根据每个单独的列被认为是离群值的行数（*Basket Size*似乎是显示出更多极端值的列）：
- en: '![Graphical user interface, table'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图形用户界面，表格'
- en: Description automatically generated](img/B17125_05_48.png)
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_48.png)
- en: 'Figure 5.48: Summary output view of the Numeric Outliers node: which columns
    are causing most of the outliers?'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.48：Numeric Outliers 节点的总结输出视图：哪些列引起了最多的离群值？
- en: 'Let''s proceed with the second preparation step before applying k-means: normalize
    the data to a set range through the **Normalizer** node.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用 k-means 之前，让我们进行第二步准备：通过**Normalizer**节点将数据标准化到一个特定的范围。
- en: '![Diagram, schematic'
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![图表，示意图'
- en: Description automatically generated](img/image083.png) *Normalizer*
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/image083.png) *Normalizer*
- en: 'This node (**Manipulation > Column > Transform** in the node repository) normalizes
    all values in selected numerical columns of a dataset. In its configuration window
    (*Figure 5.49*), you first choose which columns to normalize and, then, pick a
    normalization method. The most useful one (especially indicated in conjunction
    with distance-based procedures like k-means clustering) is the **Min-Max Normalization**,
    which linearly projects the original range onto a predefined range (usually 0
    to 1, but you can manually edit the boundaries using the text boxes provided).
    With this normalization approach, the original minimum value is transformed to
    0, the maximum to 1, and everything in the middle is proportionally assigned to
    a value within the 0 to 1 range. Another popular normalization method is the **Z-Score
    Normalization (Gaussian)**, also known as **Standardization**. Using this method,
    each value is transformed into the number of standard deviations by which it is
    above or below the population''s mean. For instance, a Z-score of –3 means that
    the value is three standard deviations below the population''s average. This is
    useful when you want to assess how much your points deviate from their mean:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点（在节点库中为**操作 > 列 > 转换（Manipulation > Column > Transform）**）将归一化选定数据集中所有数值列的值。在其配置窗口中（*图
    5.49*），首先选择要归一化的列，然后选择归一化方法。最有用的归一化方法（特别适合与基于距离的算法如 k-means 聚类结合使用）是**最小-最大归一化（Min-Max
    Normalization）**，它将原始范围线性映射到预定义的范围（通常为 0 到 1，但你可以手动编辑文本框中的边界）。使用这种归一化方法，原始的最小值被转换为
    0，最大值转换为 1，中间的所有值按比例分配到 0 到 1 范围内。另一种常见的归一化方法是**Z-分数归一化（高斯标准化）**，也叫做**标准化（Standardization）**。使用这种方法，每个值都转化为其相对于总体平均值的标准差数。例如，Z
    分数为 -3 表示该值低于总体平均值三个标准差。当你想评估数据点与均值的偏离程度时，这非常有用：
- en: '![Graphical user interface, application'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，应用程序]'
- en: Description automatically generated](img/B17125_05_49.png)
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '[描述自动生成](img/B17125_05_49.png)'
- en: 'Figure 5.49: Configuration window of the Normalizer node:select the columns
    to normalize and the method to apply'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.49：归一化节点的配置窗口：选择要归一化的列和应用的方法
- en: 'The node has two outputs: the upper output port returns the table with normalized
    values, and the bottom (the cyan square) holds the normalization model. Such a
    model can restate the original values using the **Denormalizer** node, which we
    will encounter in a few pages.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点有两个输出：上方的输出端口返回归一化值的表格，下方（青色方块）保存归一化模型。此类模型可以使用**反归一化器（Denormalizer）**节点重新表达原始值，我们将在几页后遇到该节点。
- en: We now have all we need to proceed and normalize our outliers-less CRM data
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经具备了继续进行归一化的所有条件，数据中已没有异常值，CRM 数据准备就绪。
- en: 'Pick the **Normalizer** node from the repository and connect its input to the
    first output of **Numeric Outliers**. The node configuration is straightforward:
    exclude the *Customer­­_ID* column from the normalization process by double-clicking
    on it and making sure it appears on the red box on the right. The default settings
    of the normalization method work well for us: indeed, the **Min-Max Normalization**
    with a range between 0 and 1 is great for calculating distances with algorithms
    such as k-means. Finally, click on **OK** and execute the node.'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从库中选择**归一化器（Normalizer）**节点，并将其输入连接到**数值异常值（Numeric Outliers）**的第一个输出。节点配置非常简单：通过双击*Customer­­_ID*列并确保它出现在右侧的红框中，来排除该列参与归一化过程。归一化方法的默认设置适合我们：事实上，**最小-最大归一化（Min-Max
    Normalization）**，其范围在 0 和 1 之间，特别适合使用 k-means 等算法计算距离。最后，点击**确定**并执行节点。
- en: 'If you look at the first output of the **Normalizer** node, you will notice
    how the values of the affected columns are now falling in the desired range, which
    is exactly what we needed. Now, all columns will have the same weight in calculating
    distances based on the Pythagorean theorem. We can finally move on and introduce
    the critical node of the workflow, allowing us to cluster our customers: **k-Means**.'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你查看**归一化器（Normalizer）**节点的第一个输出，你会注意到受影响列的值现在都落在了所需的范围内，这正是我们所需要的。现在，所有列在计算基于勾股定理的距离时将具有相同的权重。我们终于可以继续并介绍工作流中的关键节点，这将帮助我们对客户进行聚类：**k-Means**。
- en: '![Diagram'
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![图示]'
- en: Description automatically generated](img/image0851.png) *k-Means*
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '[描述自动生成](img/image0851.png) *k-Means*'
- en: 'This node (**Analytics > Mining > Clustering**) clusters the rows of the input
    table using the k-means algorithm. The first parameter to be set as part of its
    configuration (*Figure 5.50*) is the **Number of clusters**, which can be chosen
    by entering an integer in the textbox at the very top. You can then choose the
    method for the **Centroid initialization**, which, by default, happens by random
    draw (you can still set a static random seed to make the process repeatable),
    and the maximum number of iterations used to force termination (it is preset to
    99, which, in most cases, is good enough since k-means would naturally converge
    in fewer iterations). The last configuration step is to choose which numeric columns
    to consider when clustering, which can be done using the **Column selection**
    panel at the bottom:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '这个节点（**Analytics > Mining > Clustering**）使用k-means算法对输入表格的行进行聚类。作为其配置的一部分，第一个需要设置的参数（*图
    5.50*）是**簇的数量**，可以通过在顶部的文本框中输入整数来选择。然后，你可以选择**中心初始化**的方法，默认情况下是通过随机抽取（你仍然可以设置一个静态随机种子，使过程可重复），以及强制终止时使用的最大迭代次数（默认设置为99，通常情况下，这已经足够，因为k-means通常在更少的迭代中就会收敛）。最后的配置步骤是选择在聚类时要考虑的数字列，可以通过底部的**列选择**面板完成：  '
- en: '![Graphical user interface, application'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，应用程序  '
- en: Description automatically generated](img/B17125_05_50.png)
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '描述自动生成](img/B17125_05_50.png)  '
- en: 'Figure 5.50: Configuration window of the k-Means node: select the columns to
    normalize and the method to apply'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.50：k-Means节点的配置窗口：选择要标准化的列以及应用的方法  '
- en: Let's apply our new node to the normalized data and see what happens.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们将新的节点应用到标准化的数据上，看看会发生什么。  '
- en: Implement the **k-Means** node and connect it downstream to the first output
    of the **Normalizer** node. We can keep its configuration simple, ensuring that
    the **Number of clusters** is set to 3 and deselecting *Customer_ID* from the
    list since we don't want to consider the column in the clustering exercise. Click
    on **OK** and then execute the node and open its main view (*Shift* + *F10*, or
    right-click and then select **Execute and Open Views...**).
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '实现**k-Means**节点并将其连接到**Normalizer**节点的第一个输出端口。我们可以保持其配置简单，确保**簇的数量**设置为3，并从列表中取消选择*Customer_ID*，因为我们不想在聚类操作中考虑此列。点击**确定**，然后执行节点并打开其主视图（*Shift*
    + *F10*，或右键点击然后选择**执行并打开视图...**）。  '
- en: 'The main view of the **k-Means** node (right-click on the node and then select
    **View: Cluster View** to make it appear if needed) will look similar to what
    you find in *Figure 5.51*:'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**k-Means**节点的主视图（右键点击节点然后选择**视图：簇视图**，如果需要可以让它显示）将类似于你在*图 5.51*中看到的内容'
- en: '![Graphical user interface, text, application, email'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图形用户界面，文本，应用程序，电子邮件  '
- en: Description automatically generated](img/B17125_05_51.png)
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '描述自动生成](img/B17125_05_51.png)  '
- en: 'Figure 5.51: Summary view of the k-Means node: we can start seeing what the
    three clusters look like'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 5.51：k-Means节点的摘要视图：我们可以开始看到三个簇的样子  '
- en: 'This summary view is already telling us a lot: k-means segmented our customer
    base into three different groups of 830, 1,126, and 1,816 customers, respectively
    (see the **coverage** labels in the figure). If you open the different clusters
    (click on the **+** button on the left), you find a numeric description of the
    three centroids. According to what you see in *Figure 5.51*, for example, the
    first cluster (generically named **cluster_0** by KNIME) shows the smallest *Basket
    Size* of the three and the highest *Unique Products*. If you open the first output
    port of the node (right-click on the node and then select **Labeled input**),
    you will see that every row has been assigned to one of the three clusters, as
    indicated in the additional *Cluster* column (see *Figure 5.52*):'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '这个摘要视图已经给我们提供了很多信息：k-means将我们的客户群体分成了三组，分别是830、1,126和1,816个客户（参见图中的**覆盖率**标签）。如果你展开不同的簇（点击左侧的**+**按钮），你会看到三个中心点的数字描述。例如，参考*图
    5.51*，第一个簇（KNIME通常称之为**cluster_0**）显示出三者中最小的*购物篮大小*和最高的*独特产品*。如果你打开节点的第一个输出端口（右键点击节点然后选择**标记输入**），你将看到每一行已经被分配到三个簇中的一个，正如附加的*簇*列所示（参见*图
    5.52*）：  '
- en: '![Table'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![表格  '
- en: Description automatically generated](img/B17125_05_52.png)
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '描述自动生成](img/B17125_05_52.png)  '
- en: 'Figure 5.52: Output of the k-Means node: every customer—whether they like it
    or not—gets assigned to a cluster'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 5.52：k-Means节点的输出：每个客户—无论他们是否喜欢—都会被分配到一个簇  '
- en: As aligned with our business partner, the CRM manager, we need to go one step
    ahead and build a couple of visualizations to simplify the process of interpreting
    our clustering results.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的业务合作伙伴——CRM经理达成一致后，我们需要迈出一步，构建几个可视化图表，以简化解释我们聚类结果的过程。
- en: Before doing that, we realize that our values are still normalized and forced
    to fall within the 0 to 1 range. To make our visuals easier to interpret, we would
    prefer to come back to the original scales instead. To do so, we can revert the
    normalization by leveraging the **Denormalizer** node.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行此操作之前，我们意识到我们的数值仍然被标准化并强制限制在 0 到 1 范围内。为了让我们的可视化更容易解读，我们更愿意恢复到原始的尺度。为此，我们可以通过利用**去标准化器**节点来撤销标准化。
- en: '![Diagram, schematic'
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![图示，示意图'
- en: Description automatically generated](img/image087.png) *Denormalizer*
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/image087.png) *去标准化器*
- en: 'This node (**Manipulation > Column > Transform**) brings the values in a dataset
    back to their original range. It requires two input connections: the first one
    is the model generated by the previous **Normalizer** node, which carries a description
    of the normalization method and parameters. The second input is the normalized
    table to be denormalized. The node does not require any configuration.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点（**操作 > 列 > 转换**）将数据集中的值恢复到其原始范围。它需要两个输入连接：第一个输入是由之前的**标准化器**节点生成的模型，包含标准化方法和参数的描述。第二个输入是需要去标准化的标准化表。该节点不需要任何配置。
- en: Implement the **Denormalizer** node and set up the wiring. The cyan output of
    the **Normalizer** node should be connected to the first input of the **Denormalizer
    node.** The first output of the **k-Means** node should be connected, instead,
    to the second input port of the **Denormalizer** node. You can have a sneak view
    of the final workflow in *Figure 5.57* to see how to get the connections right.
    After executing the node, you can see how the values have been reverted to their
    original range.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现**去标准化器**节点并设置连接。**标准化器**节点的青色输出应连接到**去标准化器**节点的第一个输入端口。**k-Means**节点的第一个输出应连接到**去标准化器**节点的第二个输入端口。您可以在*图
    5.57*中预览最终的工作流，查看如何正确连接。执行节点后，您可以看到值已恢复到原始范围。
- en: To build the visuals, we will need three more nodes. The first one (**Color
    Manager**) is required for assigning colors to the various rows of the dataset
    (according to the cluster), while the other two (**Scatter Matrix (local)** and
    **Conditional Box Plot**) will generate a couple of nice charts.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建可视化，我们需要另外三个节点。第一个（**颜色管理器**）用于根据数据集的不同行（根据簇）分配颜色，而其他两个（**散点矩阵（局部）**和**条件箱线图**）将生成几个漂亮的图表。
- en: '![A picture containing icon'
  id: totrans-411
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![一个包含图标的图片'
- en: Description automatically generated](img/Image73183.png) *Color Manager*
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/Image73183.png) *颜色管理器*
- en: 'This node (**Views > Property**) assigns colors to each row of a dataset. Its
    configuration window (*Figure 5.53*) asks you to select two things. First, you
    specify the nominal column used to evaluate what color to assign: every possible
    value associated with that column will correspond to a specific color. Second,
    you need to select the color set to adopt. On top of the three default color sets,
    you can also manually define which color to assign to each possible value of the
    nominal column. To do so, you will have to select **Custom** in the **Palettes**
    tab and then use one of the tabs on the right (such as **Swatches**, **RGB**,
    and **CMYK**) to pick the right color for each nominal value manually:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点（**视图 > 属性**）为数据集的每一行分配颜色。其配置窗口（*图 5.53*）要求您选择两项内容。首先，您需要指定用于评估分配颜色的标称列：与该列相关的每个可能值都将对应一个特定颜色。其次，您需要选择要采用的颜色集。在三个默认颜色集的基础上，您还可以手动定义为标称列的每个可能值分配哪种颜色。为此，您必须在**调色板**选项卡中选择**自定义**，然后使用右侧的选项卡（如**色板**、**RGB**和**CMYK**）手动选择每个标称值的正确颜色：
- en: '![Graphical user interface, chart'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，图表'
- en: Description automatically generated](img/B17125_05_53.png)
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_53.png)
- en: 'Figure 5.53: Configuration of the Color Manager node: you can pick which color
    to assign to which value of the nominal column of your choice'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.53：颜色管理器节点的配置：您可以选择为标称列的每个值分配哪种颜色
- en: Add a **Color Manager** node and connect it to the output of the **Denormalizer**.
    Confirm the *Cluster* column in the drop-down menu at the top, and then select
    the color set of your choice. In the specific example of *Figure 5.53*, a custom
    palette has been manually created so that blue, orange, and green could be assigned
    to the three clusters.
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个**颜色管理器**节点，并将其连接到**反规范化器**的输出。确认顶部下拉菜单中的*集群*列，然后选择你想要的颜色集。在*图 5.53*的具体示例中，已手动创建一个自定义调色板，以便将蓝色、橙色和绿色分别分配给三个集群。
- en: Now that the colors are set, it's finally time to pull together the first chart
    with the help of a new node.
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在颜色已设置好，是时候借助新节点汇总第一个图表了。
- en: '![A close-up of a logo'
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![徽标的特写'
- en: Description automatically generated with low confidence](img/image084.png) *Scatter
    Matrix (local)*
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成，信心较低](img/image084.png) *散点矩阵（本地）*
- en: This node (**Views > Local (Swing)**) generates a matrix of scatter plots, displaying
    multiple combinations of variables in a single view. The node does not require
    any configuration, but you can optionally increase the maximum number of points
    that will be plotted.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点（**视图 > 本地（Swing）**）生成一个散点图矩阵，在单一视图中展示多个变量组合。该节点无需配置，但你可以选择性地增加最大绘图点数。
- en: 'Implement the **Scatter Matrix (local)** node after **Color Manager**. Execute
    and open its main view (*F10* after selecting the node). From the **Column Selection**
    tab at the bottom, you can choose which variables to display. In our case, let''s
    make sure we have only *Average Price*, *Basket Size*, and *Unique Products* selected
    on the right: you will end up with a visual similar to *Figure 5.54*:![Graphical
    user interface, application'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**颜色管理器**之后实施**散点矩阵（本地）**节点。执行并打开其主视图（选择节点后按*F10*）。在底部的**列选择**标签页中，你可以选择显示哪些变量。在我们的例子中，确保在右侧只选择了*平均价格*、*购物篮大小*和*独特产品*：你将得到一个类似于*图
    5.54*的视觉效果：![图形用户界面，应用程序
- en: Description automatically generated](img/B17125_05_54.png)
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_54.png)
- en: 'Figure 5.54: Output view of the Scatter Matrix (local) node: your customers
    have become colored points. By looking at how the cloud of dots is scattered,
    you can interpret what each cluster is all about'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.54：散点矩阵（本地）节点的输出视图：你的客户变成了彩色点。通过观察点云的分布，你可以解读每个集群的含义
- en: The scatter matrix we just obtained renders the result of the clustering in
    a more human-friendly way. As we look at it together with the CRM manager, we
    notice some initial clear patterns. For example, look at the chart at the top-right
    corner of *Figure 5.54*, which shows *Average Price* on the vertical axis and
    *Unique Products* on the horizontal axis. The blue cluster (cluster_0) clearly
    dominates the right-hand side of the chart, confirming that this is the segment
    of consumers that tend to try a more diverse set of products (high values of *Unique
    Products*). At the same time, the orange cluster (cluster_1) has customers that
    seem to go for less unique products and lower prices. Instead, the green cluster
    (cluster_2) includes those willing to pay more premium prices when shopping at
    our website. This is all starting to make sense, and the visual is already a big
    help in understanding how our clustering worked.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚获得的散点矩阵以更人性化的方式呈现了聚类的结果。当我们与CRM经理一起查看时，我们注意到一些初步的清晰模式。例如，看看*图 5.54*右上角的图表，该图表显示了*平均价格*（纵轴）和*独特产品*（横轴）。蓝色集群（cluster_0）明显占据图表的右侧，确认这是一个倾向于尝试更多样化产品（高值的*独特产品*）的消费者群体。与此同时，橙色集群（cluster_1）的客户似乎偏好较少独特的产品和较低的价格。相反，绿色集群（cluster_2）包括那些在我们网站购物时愿意支付更高价格的消费者。这一切开始变得有意义，视觉效果已经在帮助我们理解聚类结果方面发挥了巨大作用。
- en: 'Let''s add one last visual to clarify even further the composition of our segments:
    meet the **Conditional Box Plot** node.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再添加一个视觉元素，以进一步阐明我们片段的组成：请见**条件箱型图**节点。
- en: '![A picture containing text'
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![一张包含文本的图片'
- en: Description automatically generated](img/image086.png) *Conditional Box Plot*
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/image086.png) *条件箱型图*
- en: 'This node (**Views > JavaScript**) produces an interactive view with multiple
    box plots, one for each value in a given categorical column. Such a view enables
    the parallel comparison of distributions. Its configuration window (*Figure 5.55*)
    requires selecting the **Category Column** to be used for differentiating parallel
    box plots and the choice of the numeric columns whose distribution will be visualized:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点（**视图 > JavaScript**）生成一个交互式视图，其中包含多个箱形图，每个箱形图代表给定类别列中的一个值。这样的视图可以并行比较各分布。其配置窗口（*图
    5.55*）要求选择用于区分并行箱形图的**类别列**，并选择要可视化分布的数字列：
- en: '![Graphical user interface, text, application'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，文本，应用程序'
- en: Description automatically generated](img/B17125_05_55.png)
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_55.png)
- en: 'Figure 5.55: Configuration dialog of the Conditional Box Plot node: which distributions
    are you interested in comparing between?'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.55：条件箱形图节点的配置对话框：您有兴趣比较哪些分布？
- en: 'Drag and drop the **Conditional Box Plot** node onto the workflow editor and
    connect it to the output port of the **Denormalizer** node. Select *Cluster* as
    **Category Column** and ensure that only **Average Price**, **Basket Size**, and
    **Unique Products** are on the right of the column selector placed at the center
    of the dialog. Click on **OK** and then press *Shift* + *F10* to execute it and
    open its main view. In the interactive window that appears, you can swap which
    distribution to visualize by operating on the **Selected Column** drop-down menu:
    you can find this selector by clicking on the icon at the far top-right of the
    interactive window.'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拖放**条件箱形图**节点到工作流编辑器，并将其连接到**去归一化器**节点的输出端口。选择*Cluster*作为**类别列**，确保只有**平均价格**、**购物篮大小**和**独特产品**出现在对话框中部的列选择器右侧。点击**确定**，然后按*Shift*
    + *F10*来执行并打开其主视图。在弹出的交互式窗口中，您可以通过操作**选择的列**下拉菜单来切换可视化的分布：您可以通过点击交互式窗口右上角的图标找到此选择器。
- en: 'The output views of the **Conditional Box Plot** node (*Figure 5.56*) clarify
    even better the essential features of each cluster. The k-means algorithm was
    able to produce three homogeneous clusters with peculiar and differentiating characteristics.
    The box plots are great at showing such differences. As an example, take the third
    plot in the figure, which refers to *Unique Products*. The blue cluster dominates
    when it comes to this measure: the median number of unique products purchased
    by customers belonging to this segment is 32, while for the others it is near
    10\. The lack of visual overlap in height between the blue box and the other two
    means this difference is meaningful. On the other hand, the orange and the green
    clusters seem to be quite similar in terms of unique products, as the boxes are
    almost coinciding:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '**条件箱形图**节点的输出视图（*图 5.56*）更清晰地阐明了每个集群的关键特征。k-means 算法能够生成三个具有独特和区分特征的均匀集群。箱形图非常适合显示这些差异。举个例子，取图中的第三个图，它涉及到*独特产品*。在这个度量方面，蓝色集群占主导地位：属于该细分的客户购买的独特产品的中位数为
    32，而其他集群的中位数接近 10。蓝色箱形图与其他两个箱形图在高度上没有视觉重叠，意味着这种差异是有意义的。另一方面，橙色和绿色集群在独特产品方面似乎相似，因为它们的箱形图几乎重合：'
- en: '![Chart, box and whisker chart'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '![图表，箱形图'
- en: Description automatically generated](img/B17125_05_56.png)
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_56.png)
- en: 'Figure 5.56: Outputs of the Conditional Box Plot node: you can readily appreciate
    the differences in the distributions across clusters'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.56：条件箱形图节点的输出：您可以轻松地欣赏各集群间分布的差异
- en: 'We can now sit together with the CRM manager and, having the scatter matrix
    and the conditional box plots at hand, we can finally describe each customer segment
    and give a business-oriented interpretation of their meaning:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以与 CRM 经理一起坐下来，手头有散点矩阵和条件箱形图，我们终于可以描述每个客户细分，并为其含义提供面向业务的解释：
- en: The blue cluster includes those **curious customers** who are willing to try
    different products. In our communication with this segment, we can give disproportionate
    space to the "new arrivals" and intrigue them with an ample selection of products
    they haven't tried yet.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝色集群包括那些**好奇的客户**，他们愿意尝试不同的产品。在与这个细分群体的沟通中，我们可以为“新到货”产品提供更多空间，并通过提供他们尚未尝试过的丰富产品选择来吸引他们。
- en: The orange cluster possibly comprises **small retailers** who buy "in bulk"
    from our website to resell their shops. They tend to buy relatively few products
    but in large quantities. We can offer them quantity discounts and regularly communicate
    the list of best-selling products, hopefully leading them to add our best-selling
    articles to their assortment for mutual business growth.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 橙色聚类可能包括**小型零售商**，他们从我们的网站大量购买商品以转售给自己的商店。他们通常购买的产品种类较少，但数量较大。我们可以向他们提供批量折扣，并定期沟通畅销产品列表，
    希望能够促使他们将我们的畅销商品纳入他们的商品组合，以实现双方的业务增长。
- en: The green cluster is made up of our **high-value customers**, who systematically
    put quality ahead of price in their shopping choices. Therefore, when communicating
    with them, we should advertise the premium end of the products portfolio and focus
    on topics such as the quality and the safety of our assortment, deprioritizing
    price-cut offers, and other types of promotional levers.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绿色聚类由我们的**高价值客户**组成，他们在购物选择中系统性地将质量置于价格之上。因此，与他们沟通时，我们应该宣传产品组合的高端部分，专注于产品质量和安全性等话题，降低价格折扣和其他促销手段的优先级。
- en: By using only 8 KNIME nodes (see the full workflow in *Figure 5.57*), we came
    up with a simple segmentation of customers and a first proposition of how to drive
    the most value when personalizing their experience. By uniting the business expertise
    of our partners (the CRM managers in this case) with the power of data and algorithms,
    such as k-means, we can make the magic happen!
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仅使用 8 个 KNIME 节点（请参见*图 5.57*中的完整工作流），我们提出了一种简单的客户细分方法，并初步提出了如何在个性化体验中驱动最大价值的建议。通过将我们的合作伙伴（在本例中是
    CRM 管理员）的业务专业知识与数据和算法的力量（如 K-均值）结合起来，我们能够实现这种神奇的效果！
- en: '![Diagram'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '![图示'
- en: Description automatically generated](img/B17125_05_57.png)
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_57.png)
- en: 'Figure 5.57: The full workflow for segmenting consumers using clustering'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.57：使用聚类进行消费者细分的完整工作流
- en: Summary
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you touched on the serious potential behind data analytics
    and machine learning with your own hands. You have solved three real-world problems
    by putting data and algorithms at work.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你亲自触摸到了数据分析和机器学习背后巨大的潜力。你通过让数据和算法发挥作用，解决了三个现实世界中的问题。
- en: In the first tutorial, you managed to predict with a decent level of accuracy
    the rental price of properties, collecting, in the process, a few interesting
    insights into real estate price formation. You have now acquired a proven methodology,
    based on the linear regression model, that you can replicate on many business
    cases where you have to predict numeric quantities.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个教程中，你成功地预测了房地产租赁价格，准确度相当不错，并在此过程中收集到了一些关于房地产价格形成的有趣见解。你现在已经掌握了一种经过验证的方法论，基于线性回归模型，可以在许多商业案例中复制应用，尤其是在需要预测数值量的场景中。
- en: In the second tutorial, you entered the fascinating world of classification
    and propensity modeling, experiencing firsthand the game-changing role of data
    analytics in marketing. You were able to put together a couple of classification
    models through which you met multiple business needs. First, you were able to
    reveal the "unwritten rules" that make a product generally attractive to customers
    by building and interpreting a decision tree model. Then, you built a random forest
    model that proved effective in anticipating the level of propensity of individual
    bank customers. Lastly, you managed to estimate the possible ROI of further marketing
    campaigns, unlocking serious value creation opportunities for a business. Also,
    in this case, you gained a series of general-purpose techniques that you can easily
    reapply in your own work every time you need to predict anything of business relevancy.
    By going through the tutorial, you also experienced the "back and forth" iterations
    needed to fine-tune machine learning models to fit your business needs.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个教程中，你进入了分类和倾向建模的迷人世界，亲身体验了数据分析在营销中的变革性作用。你能够组建几个分类模型，满足多个业务需求。首先，你通过构建和解释决策树模型，揭示了使产品对客户普遍具有吸引力的“潜规则”。然后，你构建了一个随机森林模型，有效预测了个别银行客户的倾向水平。最后，你成功估算了进一步营销活动的可能
    ROI，为企业创造了巨大的价值机会。同样，在此过程中，你还掌握了一系列通用技术，可以在每次需要预测与业务相关的任何内容时轻松重新应用。通过完成这个教程，你还体验到了调整机器学习模型以适应业务需求所需的“反复迭代”过程。
- en: 'In our third tutorial, you experienced the power of unsupervised learning.
    You were able to put together a meaningful and straightforward customer segmentation
    that can be used to design personalized communication strategies and maximize
    the overall customer value. With this new algorithm, k-means, in your backpack,
    you can potentially cluster anything: stores, products, contracts, defects, events,
    virtually any business entity that can benefit from the algorithmic tidying that
    comes with clustering. Think about the value you can create by applying this new
    concept to the work items you deal with on a daily basis. In the process of learning
    k-means, we also got acquainted with the fundamental statistical concept of outliers
    and saw how to spot and manage them systematically.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第三个教程中，你体验了无监督学习的强大功能。你能够组建一个有意义且简洁的客户细分模型，这可以用来设计个性化的沟通策略，并最大化整体客户价值。有了这个新的算法k-means在你的“背包”中，你可以潜在地对任何事物进行聚类：商店、产品、合同、缺陷、事件，几乎任何可以从聚类算法带来的整洁性中受益的业务实体。想一想，通过将这一新概念应用到你日常处理的工作项目中，你能创造出什么样的价值。在学习k-means的过程中，我们还接触到了异常值这一基本统计概念，并且看到了如何系统地识别和处理异常值。
- en: 'Let''s now move on and learn how we can make our data accessible to our business
    partners through self-service dashboards. It''s time to meet our next travel companion
    in the data analytics journey: Power BI.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续前进，学习如何通过自助式仪表板将我们的数据呈现给业务伙伴。是时候认识我们在数据分析旅程中的下一个“旅行伙伴”：Power BI。
