- en: Chapter 2. The Spark Programming Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 Spark编程模型
- en: Large-scale data processing using thousands of nodes with built-in fault tolerance
    has become widespread due to the availability of open source frameworks, with
    Hadoop being a popular choice. These frameworks are quite successful in executing
    specific tasks such as **Extract, Transform, and Load** (**ETL**) and storage
    applications that deal with web-scale data. However, developers were left with
    a myriad of tools to work with, along with the well-established Hadoop ecosystem.
    There was a need for a single, general-purpose development platform that caters
    to batch, streaming, interactive, and iterative requirements. This was the motivation
    behind Spark.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模数据处理使用数千个具有内置容错能力的节点已经变得普遍，这是由于开源框架的可用性，Hadoop是一个受欢迎的选择。这些框架在执行特定任务（如**提取、转换和加载**（**ETL**）以及处理网络规模数据的存储应用程序）方面非常成功。然而，开发人员在使用这些框架时需要使用大量的工具，以及成熟的Hadoop生态系统。需要一个单一的、通用的开发平台，满足批处理、流式处理、交互式和迭代式需求。这就是Spark背后的动机。
- en: The previous chapter outlined the big data analytics challenges and how Spark
    addressed most of them at a very high level. In this chapter, we will examine
    the design goals and choices involved in the making of Spark to get a clearer
    understanding of its suitability as a data science platform for big data. We will
    also cover the core abstraction **Resilient Distributed Dataset** (**RDD**) in
    depth with examples.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章概述了大数据分析的挑战，以及Spark在很高的层次上解决了大部分问题。在本章中，我们将深入研究Spark的设计目标和选择，以更清楚地了解其作为大数据科学平台的适用性。我们还将深入介绍核心抽象**弹性分布式数据集**（**RDD**）并提供示例。
- en: 'As a prerequisite for this chapter, a basic understanding of Python or Scala
    along with elementary understanding of Spark is needed. The topics covered in
    this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章之前，需要基本了解Python或Scala以及对Spark的初步了解。本章涵盖的主题如下：
- en: The programming paradigm - language support and design benefits
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程范式 - 语言支持和设计优势
- en: Supported programming languages
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持的编程语言
- en: Choosing the right language
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择正确的语言
- en: The Spark engine - Spark core components and their implications
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark引擎 - Spark核心组件及其影响
- en: Driver program
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动程序
- en: Spark shell
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark shell
- en: SparkContext
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkContext
- en: Worker nodes
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点
- en: Executors
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行器
- en: Shared variables
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享变量
- en: Flow of execution
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行流程
- en: The RDD API - understanding the RDD fundamentals
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD API - 理解RDD基础
- en: RDD basics
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD基础
- en: Persistence
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久性
- en: RDD operations - let's get your hands dirty
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD操作 - 让我们动手做
- en: Getting started with the shell
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用shell
- en: Creating RDDs
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建RDD
- en: Transformations on normal RDDs
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对普通RDD的转换
- en: Transformations on pair RDDs
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对成对RDD的转换
- en: Actions
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作
- en: The programming paradigm
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编程范式
- en: For Spark to address the big data challenges and serve as a platform for data
    science and other scalable applications, it was built with well-thought-out design
    considerations and language support.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决大数据挑战并作为数据科学和其他可扩展应用程序的平台，Spark在设计时考虑周全，并提供了语言支持。
- en: There are Spark APIs designed for varieties of application developers to create
    Spark-based applications using standard API interfaces. Spark provides APIs for
    Scala, Java, R and Python programming languages, as explained in the following
    sections.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了专为各种应用程序开发人员设计的API，使用标准API接口创建基于Spark的应用程序。Spark提供了Scala、Java、R和Python编程语言的API，如下节所述。
- en: Supported programming languages
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持的编程语言
- en: With built-in support for so many languages, Spark can be used interactively
    through a shell, which is otherwise known as **Read-Evaluate-Print-Loop** (**REPL**),
    in a way that will feel familiar to developers of any language. The developers
    can use the language of their choice, leverage existing libraries, and seamlessly
    interact with Spark and its ecosystem. Let us see the ones supported on Spark
    and how they fit into the Spark ecosystem.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Spark内置对多种语言的支持，可以通过一个称为**读取-求值-打印-循环**（**REPL**）的shell进行交互式使用，这对任何语言的开发人员来说都会感到熟悉。开发人员可以使用他们选择的语言，利用现有的库，并与Spark及其生态系统无缝交互。让我们看看Spark支持的语言以及它们如何适应Spark生态系统。
- en: Scala
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Scala
- en: Spark itself is written in Scala, a **Java Virtual Machine** (**JVM**) based
    functional programming language. The Scala compiler generates byte code that executes
    on the JVM. So, it can seamlessly integrate with any other JVM-based systems such
    as HDFS, Cassandra, HBase, and so on. Scala was the language of choice because
    of its concise programming interface, an interactive shell, and its ability to
    capture functions and efficiently ship them across the nodes in a cluster. Scala
    is an extensible (scalable, hence the name), statically typed, efficient multi-paradigm
    language that supports functional and object-oriented language features.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Spark本身是用Scala编写的，Scala是一种基于**Java虚拟机**（**JVM**）的函数式编程语言。Scala编译器生成的字节码在JVM上执行。因此，它可以与任何其他基于JVM的系统（如HDFS、Cassandra、HBase等）无缝集成。Scala是首选语言，因为它具有简洁的编程接口、交互式shell以及捕获函数并有效地在集群中的节点之间传输的能力。Scala是一种可扩展（可伸缩，因此得名）、静态类型的、高效的多范式语言，支持函数式和面向对象的语言特性。
- en: Apart from the full-blown applications, Scala also supports shell (Spark shell)
    for interactive data analysis on Spark.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了完整的应用程序外，Scala还支持shell（Spark shell），用于在Spark上进行交互式数据分析。
- en: Java
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Java
- en: Since Spark is JVM based, it naturally supports Java. This helps existing Java
    developers to develop data science applications along with other scalable applications.
    Almost all the built-in library functions are accessible from Java. Coding in
    Java for data science assignments is comparatively difficult in Spark, but someone
    very hands-on with Java might find it easy.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark是基于JVM的，它自然地支持Java。这有助于现有的Java开发人员开发数据科学应用程序以及其他可扩展的应用程序。几乎所有内置库函数都可以从Java中访问。在Spark中使用Java进行数据科学任务的编码相对困难，但对Java非常熟悉的人可能会觉得很容易。
- en: This Java API only lacks a shell-based interface for interactive data analysis
    on Spark.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Java API只缺少一个基于shell的接口，用于在Spark上进行交互式数据分析。
- en: Python
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python
- en: Python is supported on Spark through PySpark, which is built on top of Spark's
    Java API (using Py4J). From now on, we will be using the term **PySpark** to refer
    to the Python environment on Spark. Python was already very popular amongst developers
    for data wrangling, data munging, and other data science related tasks. Support
    for Python on Spark became even more popular as Spark could address the scalable
    computation challenge.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Python通过PySpark在Spark上得到支持，它是建立在Spark的Java API（使用Py4J）之上的。从现在开始，我们将使用术语**PySpark**来指代Spark上的Python环境。Python在数据整理、数据处理和其他数据科学相关任务方面已经非常受开发人员欢迎。随着Spark能够解决可伸缩计算的挑战，对Python在Spark上的支持变得更加流行。
- en: Through Python's interactive shell on Spark (PySpark), interactive data analysis
    at scale is possible.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Python在Spark上的交互式shell（PySpark），可以进行大规模的交互式数据分析。
- en: R
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: R
- en: R is supported on Spark through SparkR, an R package through which Spark's scalability
    is accessible through R. SparkR empowered R to address its limitation of single-threaded
    runtime, because of which computation was limited only to a single node.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: R通过SparkR支持Spark，这是一个R包，通过它可以通过R访问Spark的可伸缩性。SparkR使R能够解决单线程运行时的限制，因此计算仅限于单个节点。
- en: Since R was originally designed only for statistical analysis and machine learning,
    it was already enriched with most of the packages. Data scientists can now work
    on huge data at scale with a minimal learning curve. R is still a default choice
    for many data scientists.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于R最初只设计用于统计分析和机器学习，它已经丰富了大部分的包。数据科学家现在可以在大规模数据上工作，学习曲线很小。R仍然是许多数据科学家的首选。
- en: Choosing the right language
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择合适的语言
- en: 'Apart from the developer''s language preference, at times there are other constraints
    that may draw attention. The following aspects could supplement your development
    experience while choosing one language over the other:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了开发人员的语言偏好之外，有时还有其他约束条件可能会引起注意。在选择一种语言而不是另一种语言时，以下方面可能会补充您的开发经验：
- en: An interactive shell comes in handy when developing complex logic. All languages
    supported by Spark except Java have an interactive shell.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开发复杂逻辑时，交互式shell非常方便。除了Java之外，Spark支持的所有语言都有交互式shell。
- en: R is the lingua franca of data scientists. It is definitely more suitable for
    pure data analytics because of its richer set of libraries. R support was added
    in Spark 1.4.0 so that Spark reaches out to data scientists working on R.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R是数据科学家的通用语言。由于其更丰富的库集，它绝对更适合纯数据分析。R支持是在Spark 1.4.0中添加的，以便Spark能够接触到使用R的数据科学家。
- en: Java has a broader base of developers. Java 8 has included lambda expressions
    and hence the functional programming aspect. Nevertheless, Java tends to be verbose.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java拥有更广泛的开发人员基础。Java 8已经包含了lambda表达式，因此具有函数式编程方面。尽管如此，Java往往冗长。
- en: Python is gradually gaining more popularity in the data science space. The availability
    of Pandas and other data processing libraries, and its simple and expressive nature,
    make Python a strong candidate. Python gives more flexibility than R in scenarios
    such as data aggregation from different sources, data cleaning, natural language
    processing, and so on.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python在数据科学领域逐渐变得更受欢迎。Pandas和其他数据处理库的可用性，以及其简单和表达性的特性，使Python成为一个强有力的选择。Python在数据聚合、数据清洗、自然语言处理等方面比R更灵活。
- en: Scala is perhaps the best choice for real-time analytics because this is the
    closest to Spark. The initial learning curve for developers coming from other
    languages should not be a deterrent for serious production systems. The latest
    inclusions to Spark are usually first available in Scala. Its static typing and
    sophisticated type inference improve efficiency as well as compile-time checks.
    Scala can draw from Java's libraries as Scala's own library base is still at an
    early stage, but catching up.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala可能是实时分析的最佳选择，因为它与Spark最接近。对于来自其他语言的开发人员来说，初始学习曲线不应成为严重生产系统的障碍。Spark的最新增加通常首先在Scala中可用。其静态类型和复杂的类型推断提高了效率以及编译时检查。Scala可以利用Java的库，因为Scala自己的库基础仍处于早期阶段，但正在迎头赶上。
- en: The Spark engine
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark引擎
- en: To program with Spark, a basic understanding of Spark components is needed.
    In this section, some of the important Spark components along with their execution
    mechanism will be explained so that developers and data scientists can write programs
    and build applications.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Spark进行编程，需要对Spark组件有基本的了解。在本节中，将解释一些重要的Spark组件以及它们的执行机制，以便开发人员和数据科学家可以编写程序和构建应用程序。
- en: 'Before getting into the details, we suggest you take a look at the following
    diagram so that the descriptions of the Spark gears are more comprehensible as
    you read further:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入细节之前，我们建议您查看以下图表，以便在阅读更多内容时更容易理解Spark齿轮的描述：
- en: '![The Spark engine](img/image_02_001.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![The Spark engine](img/image_02_001.jpg)'
- en: Driver program
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 驱动程序
- en: The Spark shell is an example of a driver program. A driver program is a process
    that executes in the JVM and runs the user's *main* function on it. It has a SparkContext
    object which is a connection to the underlying cluster manager. A Spark application
    is initiated when the driver starts and it completes when the driver stops. The
    driver, through an instance of SparkContext, coordinates all processes within
    a Spark application.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Spark shell是驱动程序的一个示例。驱动程序是在JVM中执行并在其上运行用户的*main*函数的进程。它具有一个SparkContext对象，它是与底层集群管理器的连接。驱动程序启动时启动Spark应用程序，并在驱动程序停止时完成。通过SparkContext的实例，驱动程序协调Spark应用程序中的所有进程。
- en: Primarily, an RDD lineage **Directed Acyclic Graph** (**DAG**) is built on the
    driver side with data sources (which may be RDDs) and transformations. This DAG
    is submitted to the DAG scheduler when an *action* method is encountered. The
    DAG scheduler then splits the DAG into logical units of work (for example, map
    or reduce) called stages. Each stage, in turn, is a set of tasks, and each task
    is assigned to an executor (worker) by the task scheduler. Jobs may be executed
    in FIFO order or round robin, depending on the configuration.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 主要是在驱动程序端使用数据源（可能是RDD）和转换构建RDD谱系有向无环图（DAG）。当遇到*action*方法时，此DAG被提交给DAG调度程序。然后DAG调度程序将DAG拆分为逻辑工作单元（例如map或reduce）称为阶段。每个阶段又是一组任务，每个任务由任务调度程序分配给执行者（工作节点）。作业可以按FIFO顺序或循环顺序执行，具体取决于配置。
- en: Tip
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Inside a single Spark application, multiple parallel jobs can run simultaneously
    if they were submitted from separate threads.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个Spark应用程序中，如果从不同的线程提交，多个并行作业可以同时运行。
- en: The Spark shell
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark shell
- en: The Spark shell is none other than the interface provided by Scala and Python.
    It looks very similar to any other interactive shell. It has a SparkContext object
    (created by default for you) that lets you leverage the distributed cluster. An
    interactive shell is quite useful for exploratory or ad hoc analysis. You can
    develop your complex scripts step by step through the shell without going through
    the compile-build-execute cycle.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Spark shell实际上就是由Scala和Python提供的接口。它看起来非常类似于任何其他交互式shell。它有一个SparkContext对象（默认为您创建），让您利用分布式集群。交互式shell非常适用于探索性或临时分析。您可以通过shell逐步开发复杂的脚本，而无需经历编译-构建-执行的周期。
- en: SparkContext
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SparkContext
- en: SparkContext is the entry point to the Spark core engine. This object is required
    to create and manipulate RDDs and create shared variables on a cluster. The SparkContext
    object connects to a cluster manager, which is responsible for resource allocation.
    Spark comes with its own standalone cluster manager. Since the cluster manager
    is a pluggable component in Spark, it can be managed through external cluster
    managers such as Apache Mesos or YARN.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: SparkContext是Spark核心引擎的入口点。此对象用于在集群上创建和操作RDD，并创建共享变量。SparkContext对象连接到负责资源分配的集群管理器。Spark自带其自己的独立集群管理器。由于集群管理器在Spark中是可插拔的组件，因此可以通过外部集群管理器（如Apache
    Mesos或YARN）进行管理。
- en: When you start a Spark shell, a SparkContext object is created by default for
    you. You can also create it by passing a SparkConf object that is used to set
    various Spark configuration parameters as key value pairs. Please note that there
    can be only one SparkContext object in a JVM.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当启动Spark shell时，默认会为您创建一个SparkContext对象。您也可以通过传递一个用于设置各种Spark配置参数的SparkConf对象来创建它。请注意，在一个JVM中只能有一个SparkContext对象。
- en: Worker nodes
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作节点
- en: Worker nodes are the nodes that run the application code in a cluster, obeying
    the driver program. The real work is actually executed by the worker nodes. Each
    machine in the cluster may have one or more worker instances (default one). A
    worker node executes one or more executors that belong to one or more Spark applications.
    It consists of a *block manager* component, which is responsible for managing
    data blocks. The blocks can be cached RDD data, intermediate shuffled data, or
    broadcast data. When the available RAM is not sufficient, it automatically moves
    some data blocks to disk. Data replication across nodes is another responsibility
    of block manager.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点是在集群中运行应用程序代码的节点，遵循驱动程序。实际工作实际上是由工作节点执行的。集群中的每台机器可能有一个或多个工作实例（默认一个）。工作节点执行属于一个或多个Spark应用程序的一个或多个执行者。它包括一个*块管理器*组件，负责管理数据块。这些块可以是缓存的RDD数据、中间洗牌数据或广播数据。当可用的RAM不足时，它会自动将一些数据块移动到磁盘上。块管理器的另一个责任是在节点之间复制数据。
- en: Executors
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行者
- en: Each application has a set of executor processes. Executors reside on worker
    nodes and communicate directly with the driver once the connection is made by
    the cluster manager. All executors are managed by SparkContext. An executor is
    a single JVM instance that serves a single Spark application. An executor is responsible
    for managing computation through tasks, storage, and caching on each worker node.
    It can run multiple tasks concurrently.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每个应用程序都有一组执行者进程。执行者驻留在工作节点上，并一旦由集群管理器建立连接，就直接与驱动程序通信。所有执行者都由SparkContext管理。执行者是一个单独的JVM实例，为单个Spark应用程序提供服务。执行者负责通过任务、存储和缓存在每个工作节点上管理计算。它可以同时运行多个任务。
- en: Shared variables
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共享变量
- en: Normally, the code is shipped to partitions along with separate copies of variables.
    These variables cannot be used to propagate results (for example, intermediate
    work counts) back to the driver program. Shared variables are used for this purpose.
    There are two kinds of shared variables, **broadcast variables** and **accumulators**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，代码会与变量的单独副本一起传输到分区。这些变量不能用于将结果（例如中间工作计数）传播回驱动程序。共享变量用于此目的。共享变量有两种，即**广播变量**和**累加器**。
- en: Broadcast variables enable the programmers to retain a read-only copy cached
    on each node rather than shipping a copy of it with tasks. If large, read-only
    data is used in multiple operations, it can be designated as broadcast variables
    and shipped only once to all worker nodes. The data broadcast in this way is cached
    in serialized form and is deserialized before running each task. Subsequent operations
    can access these variables along with the local variables moved along with the
    code. Creating broadcast variables is not necessary in all cases, except the ones
    where tasks across multiple stages need the same read-only copy of the data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量使程序员能够保留只读副本，而不是将其与任务一起传输到每个节点。如果大型只读数据在多个操作中使用，可以将其指定为广播变量，并且只传输一次到所有工作节点。以这种方式广播的数据以序列化形式缓存，并在运行每个任务之前进行反序列化。后续操作可以访问这些变量以及与代码一起移动的本地变量。在所有情况下都不需要创建广播变量，除非跨多个阶段的任务需要相同的只读数据副本。
- en: Accumulators are variables that are always incremented, such as counters or
    cumulative sums. Spark natively supports accumulators of numeric types, but allows
    programmers to add support for new types. Please note that the worker nodes cannot
    read the value of accumulators; they can only modify their values.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 累加器是始终递增的变量，例如计数器或累积和。Spark本身支持数值类型的累加器，但允许程序员为新类型添加支持。请注意，工作节点无法读取累加器的值；它们只能修改它们的值。
- en: Flow of execution
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行流程
- en: A Spark application consists of a set of processes with one *driver* program
    and multiple *worker* (*executor*) programs. The driver program contains the application's
    *main* function and a SparkContext object, which represents a connection to the
    Spark cluster. Coordination between driver and the other processes happens through
    the SparkContext object.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Spark应用程序由一个*驱动*程序和多个*工作*(*执行器*)程序组成。驱动程序包含应用程序的*main*函数和一个代表与Spark集群的连接的SparkContext对象。驱动程序和其他进程之间的协调通过SparkContext对象进行。
- en: 'A typical Spark client program performs the following steps:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的Spark客户端程序执行以下步骤：
- en: When a program is run on a Spark shell, it is called the driver program with
    the user's `main` method in it. It gets executed in the JVM of the system where
    you are running the driver program.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当程序在Spark shell上运行时，它被称为驱动程序，其中包含用户的`main`方法。它在运行驱动程序的系统的JVM中执行。
- en: The first step is to create a SparkContext object with the required configuration
    parameters. When you run the PySpark or Spark shell, it is instantiated by default,
    but for other applications, you have to create it explicitly. SparkContext is
    actually the gateway to Spark.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是使用所需的配置参数创建一个SparkContext对象。当您运行PySpark或Spark shell时，默认情况下会实例化它，但对于其他应用程序，您必须显式创建它。SparkContext实际上是通往Spark的入口。
- en: The next step is to define one or more RDDs, either by loading a file or programmatically
    by passing an array of items, referred to parallel collection
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是定义一个或多个RDD，可以通过加载文件或通过以并行集合引用项目数组来以编程方式定义
- en: Then more RDDs can be defined by a sequence of transformations, which are tracked
    and managed by a **lineage graph**. These RDD transformations may be viewed as
    piped UNIX commands where the output of one command becomes the input to the next
    command and so on. Each resulting RDD of a *transformation* step has a pointer
    to its parent RDD and also has a function for calculating its data. The RDD is
    acted on only after encountering an *action* statement. So, the *transformations*
    are lazy operations used to define new RDDs and *actions* launch a computation
    to return a value to the program or write data to external storage. We will discuss
    this aspect a little more in the following sections.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，更多的RDD可以通过一系列的转换来定义，这些转换由一个**血统图**跟踪和管理。这些RDD转换可以被视为管道UNIX命令，其中一个命令的输出成为下一个命令的输入，依此类推。每个*转换*步骤的结果RDD都有一个指向其父RDD的指针，并且还有一个用于计算其数据的函数。只有在遇到*操作*语句后，RDD才会被执行。因此，*转换*是用于定义新RDD的惰性操作，而*操作*会启动计算以将值返回给程序或将数据写入外部存储。我们将在接下来的部分中更详细地讨论这一方面。
- en: At this stage, Spark creates an execution graph where nodes represent the RDDs
    and edges represent the transformation steps. Spark breaks the job into multiple
    tasks to run on separate machines. This is how Spark sends the **compute** to
    the data across the nodes in a cluster, rather than getting all the data together
    and computing it.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个阶段，Spark创建一个执行图，其中节点表示RDD，边表示转换步骤。Spark将作业分解为多个任务在单独的机器上运行。这就是Spark如何在集群中的节点之间发送**计算**，而不是将所有数据聚集在一起进行计算。
- en: The RDD API
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD API
- en: The RDD is a read-only, partitioned, fault-tolerant collection of records. From
    a design perspective, there was a need for a single data structure abstraction
    that hides the complexity of dealing with a wide variety of data sources, be it
    HDFS, filesystems, RDBMS, NOSQL data structures, or any other data source. The
    user should be able to define the RDD from any of these sources. The goal was
    to support a wide array of operations and let users compose them in any order.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RDD是一个只读的、分区的、容错的记录集合。从设计的角度来看，需要一个单一的数据结构抽象，隐藏处理各种各样的数据源的复杂性，无论是HDFS、文件系统、RDBMS、NOSQL数据结构还是任何其他数据源。用户应该能够从这些源中定义RDD。目标是支持各种操作，并让用户以任何顺序组合它们。
- en: RDD basics
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD基础
- en: Each dataset is represented as an object in Spark's programming interface called
    RDD. Spark provides two ways for creating RDDs. One way is to parallelize an existing
    collection. The other way is to reference a dataset in an external storage system
    such as a filesystem.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集在Spark的编程接口中表示为一个名为RDD的对象。Spark提供了两种创建RDD的方式。一种方式是并行化现有集合。另一种方式是引用外部存储系统中的数据集，例如文件系统。
- en: An RDD is composed of one or more data sources, maybe after performing a series
    of transformations including several operators. Every RDD or RDD partition knows
    how to recreate itself in case of failure. It has the log of transformations,
    or a *lineage* that is required to recreate itself from stable storage or another
    RDD. Thus, any program using Spark can be assured of built-in fault tolerance,
    regardless of the underlying data source and the type of RDD.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个RDD由一个或多个数据源组成，可能经过一系列的转换，包括几个操作符。每个RDD或RDD分区都知道如何在发生故障时重新创建自己。它具有转换的日志，或者是从稳定存储或另一个RDD重新创建自己所需的*血统*。因此，使用Spark的任何程序都可以确保具有内置的容错性，而不管底层数据源和RDD的类型如何。
- en: 'There are two kinds of methods available on RDDs: transformations, and actions.
    Transformations are the methods that are used to create RDDs. Actions are the
    methods that utilize RDDs. RDDs are usually partitioned. Users may choose to persist
    RDDs that may be reused in their programs.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: RDD上有两种方法可用：转换和操作。转换是用于创建RDD的方法。操作是利用RDD的方法。RDD通常是分区的。用户可以选择持久化RDD，以便在程序中重复使用。
- en: RDDs are immutable (read-only) data structures, so any transformation results
    in the creation of a new RDD. The transformations are applied lazily, only when
    any action is applied on them, and not when an RDD is defined. An RDD is recomputed
    every time it is used in an action unless the user explicitly persists the RDD
    in memory. Saving in memory saves a lot of time. If the memory is not sufficient
    to accommodate the RDD fully, the remaining portion of that RDD will be stored
    (spilled) on the hard disk automatically. One advantage of lazy transformations
    is that it is possible to optimize the transformation steps. For example, if the
    action is to return the first line, Spark computes only a single partition and
    skips the rest.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: RDD是不可变（只读）的数据结构，因此任何转换都会创建一个新的RDD。转换是懒惰地应用的，只有当对它们应用任何操作时，而不是在定义RDD时。除非用户明确将RDD持久化在内存中，否则每次在操作中使用RDD时都会重新计算RDD。保存在内存中可以节省大量时间。如果内存不足以容纳整个RDD，剩余部分将自动存储（溢出）到硬盘上。懒惰转换的一个优点是可以优化转换步骤。例如，如果操作是返回第一行，Spark只计算一个分区并跳过其余部分。
- en: An RDD may be viewed as a set of partitions (splits) with a list of dependencies
    on parent RDDs and a function to compute a partition given its parents. Sometimes,
    each partition of a parent RDD is used by a single child RDD. This is called *narrow
    dependency*. Narrow dependency is desirable because when a parent RDD partition
    is lost, only a single child partition needs to be recomputed. On the other hand,
    computing a single child RDD partition that involves operations such as *group-by-keys*
    depends on several parent RDD partitions. Data from each parent RDD partition
    in turn is required in creating data in several child RDD partitions. Such a dependency
    is called *wide dependency*. In the case of narrow dependency, it is possible
    to keep both parent and child RDD partitions on a single node (co-partition).
    But this is not possible in the case of wide dependency because parent data is
    scattered across several partitions. In such cases, data should be *shuffled*
    across partitions. Data shuffling is a resource-intensive operation that should
    be avoided to the extent possible. Another issue with wide dependency is that
    all child RDD partitions need to be recomputed even when a single parent RDD partition
    is lost.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: RDD可以被视为一组分区（拆分），具有对父RDD的依赖关系列表和一个计算分区的函数。有时，父RDD的每个分区被单个子RDD使用。这被称为*窄依赖*。窄依赖是可取的，因为当父RDD分区丢失时，只需要重新计算一个子分区。另一方面，计算涉及*group-by-keys*等操作的单个子RDD分区依赖于多个父RDD分区。每个父RDD分区的数据依次用于创建多个子RDD分区的数据。这样的依赖被称为*宽依赖*。在窄依赖的情况下，可以将父RDD分区和子RDD分区都保留在单个节点上（共同分区）。但在宽依赖的情况下是不可能的，因为父数据分散在多个分区中。在这种情况下，数据应该在分区之间*洗牌*。数据洗牌是一个资源密集型的操作，应尽量避免。宽依赖的另一个问题是，即使丢失一个父RDD分区，所有子RDD分区也需要重新计算。
- en: Persistence
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持久性
- en: RDDs are computed on the fly every time they are acted upon through an action
    method. The developer has the ability to override this default behavior and instruct
    to *persist* or *cache* a dataset in memory across partitions. If this dataset
    is required to participate in several actions, then persisting saves a significant
    amount of time, CPU cycles, disk I/O, and network bandwidth. The fault-tolerance
    mechanism is applicable to the cached partitions too. When any partition is lost
    due to node failure, it is recomputed using a lineage graph. If the available
    memory is insufficient, Spark gracefully spills the persisted partitions on to
    the disk. The developer may remove unwanted RDDs using *unpersist*. Nevertheless,
    Spark automatically monitors the cache and removes old partitions using **Least
    Recently Used** (**LRU**) algorithms.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: RDD在每次通过操作方法进行操作时都是即时计算的。开发人员有能力覆盖这种默认行为，并指示在分区之间*持久化*或*缓存*数据集。如果这个数据集需要参与多个操作，那么持久化可以节省大量的时间、CPU周期、磁盘I/O和网络带宽。容错机制也适用于缓存分区。当任何分区由于节点故障而丢失时，它将使用一个血统图进行重新计算。如果可用内存不足，Spark会优雅地将持久化的分区溢出到磁盘上。开发人员可以使用*unpersist*来删除不需要的RDD。然而，Spark会自动监视缓存，并使用**最近最少使用**（**LRU**）算法删除旧的分区。
- en: Tip
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '`Cache()` is the same as `persist()` or `persist (MEMORY_ONLY)`. While the
    `persist()` method can have many other arguments for different levels of persistence,
    such as only memory, memory and disk, only disk, and so on, the `cache()` method
    is designed only for persistence in the memory.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cache()`与`persist()`或`persist(MEMORY_ONLY)`相同。虽然`persist()`方法可以有许多其他参数用于不同级别的持久性，比如仅内存、内存和磁盘、仅磁盘等，但`cache()`方法仅设计用于在内存中持久化。'
- en: RDD operations
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD操作
- en: Spark programming usually starts by choosing a suitable interface that you are
    comfortable with. If you intend to do interactive data analysis, then a shell
    prompt would be the obvious choice. However, choosing a Python shell (PySpark)
    or Scala shell (Spark-Shell) depends on your proficiency with these languages
    to some extent. If you are building a full-blown scalable application then proficiency
    matters a great deal, so you should develop the application in your language of
    choice between Scala, Java, and Python, and submit it to Spark. We will discuss
    this aspect in more detail later in the book.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Spark编程通常从选择一个合适的接口开始，这取决于您的熟练程度。如果您打算进行交互式数据分析，那么shell提示符将是显而易见的选择。然而，选择Python
    shell（PySpark）或Scala shell（Spark-Shell）在某种程度上取决于您对这些语言的熟练程度。如果您正在构建一个完整的可扩展应用程序，那么熟练程度就非常重要，因此您应该选择Scala、Java和Python中的一种语言来开发应用程序，并将其提交给Spark。我们将在本书的后面更详细地讨论这个方面。
- en: Creating RDDs
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建RDDs
- en: In this section, we will use both a Python shell (PySpark) and a Scala shell
    (Spark-Shell) to create an RDD. Both of these shells have a predefined, interpreter-aware
    SparkContext that is assigned to a variable `sc`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Python shell（PySpark）和Scala shell（Spark-Shell）来创建一个RDD。这两个shell都有一个预定义的、解释器感知的SparkContext，分配给一个名为`sc`的变量。
- en: 'Let us get started with some simple code examples. Note that the code assumes
    the current working directory is Spark''s home directory. The following code snippet
    initiates the Spark interactive shell, reads a file from the local filesystem,
    and prints the first line from that file:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些简单的代码示例开始。请注意，代码假定当前工作目录是Spark的主目录。以下代码片段启动了Spark交互式shell，从本地文件系统读取文件，并打印该文件的第一行：
- en: '**Python**:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE0]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Scala**:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In both the preceding examples, the first line has invoked the interactive shell.
    The SparkContext variable `sc` is already defined as expected. We have created
    an RDD by the name `fileRDD` that points to a file `RELEASE`. This statement is
    just a transformation and will not be executed until an action is encountered.
    You can try giving a nonexistent filename but you will not get any error until
    you execute the next statement, which happens to be an *action* statement.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的两个例子中，第一行已经调用了交互式shell。SparkContext变量`sc`已经按预期定义。我们已经创建了一个名为`fileRDD`的RDD，指向一个名为`RELEASE`的文件。这个语句只是一个转换，直到遇到一个动作才会被执行。你可以尝试给一个不存在的文件名，但直到执行下一个语句（也就是一个*动作*语句）时才会得到任何错误。
- en: We have completed the whole cycle of initiating a Spark application (shell),
    creating an RDD, and consuming it. Since RDDs are recomputed every time an action
    is executed, `fileRDD` is not persisted in the memory or hard disk. This allows
    Spark to optimize the sequence of steps and execute intelligently. In fact, in
    the previous example, the optimizer would have just read one partition of the
    input file because `first()` does not require a complete file scan.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了启动Spark应用程序（shell）、创建RDD和消耗它的整个循环。由于RDD在执行动作时每次都会重新计算，`fileRDD`没有被持久化在内存或硬盘上。这使得Spark能够优化步骤序列并智能地执行。实际上，在前面的例子中，优化器可能只读取了输入文件的一个分区，因为`first()`不需要完整的文件扫描。
- en: 'Recall that there are two ways to create an RDD: one way is to create a pointer
    to a data source and the other is to parallelize an existing collection. The previous
    examples covered one way, by loading a file from a storage system. We will now
    see the second way, which is parallelizing an existing collection. RDD creation
    by passing in-memory collections is simple but may not work very well for large
    collections, because the input collection should fit completely in the driver
    node''s memory.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，创建RDD有两种方式：一种是创建一个指向数据源的指针，另一种是并行化一个现有的集合。前面的例子涵盖了一种方式，即从存储系统加载文件。现在我们将看到第二种方式，即并行化现有集合。通过传递内存中的集合来创建RDD是简单的，但对于大型集合可能效果不佳，因为输入集合应该完全适合驱动节点的内存。
- en: 'The following example creates an RDD by passing a Python/Scala list with the
    `parallelize` function:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例通过使用`parallelize`函数传递Python/Scala列表来创建一个RDD：
- en: '**Python**:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**:'
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: A lambda function is an unnamed function, typically used as function arguments
    to other functions. A Python lambda function can be a single expression only.
    If your logic requires multiple steps, create a separate function and use it in
    the lambda expression.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda函数是一个无名函数，通常用作其他函数的函数参数。Python lambda函数只能是一个单一表达式。如果你的逻辑需要多个步骤，创建一个单独的函数并在lambda表达式中使用它。
- en: '**Scala**:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**:'
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we saw in the previous example, we were able to pass a Scala/Python collection
    to create an RDD and we also had the liberty to specify the number of partitions
    to cut those collections into. Spark runs one task for each partition of the cluster,
    so it has to be carefully decided to optimize the computation effort. Though Spark
    sets the number of partitions automatically based on the cluster, we have the
    liberty to set it manually by passing it as a second argument to the `parallelize`
    function (for example, `sc.parallelize(data, 3)`). The following is a diagrammatic
    representation of an RDD which is created with a dataset with, say, 14 records
    (or tuples) and is partitioned into 3, distributed across 3 nodes:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的例子中看到的，我们能够传递一个Scala/Python集合来创建一个RDD，并且我们也有自由来指定将这些集合切分成的分区数。Spark对集群的每个分区运行一个任务，因此必须仔细决定以优化计算工作。虽然Spark根据集群自动设置分区数，但我们可以通过将其作为`parallelize`函数的第二个参数手动设置（例如，`sc.parallelize(data,
    3)`）。以下是一个RDD的图形表示，它是使用一个包含14条记录（或元组）的数据集创建的，并且被分区为3个，分布在3个节点上：
- en: '![Creating RDDs](img/1-1.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![创建RDDs](img/1-1.jpg)'
- en: 'Writing a Spark program usually consists of transformations and actions. Transformations
    are lazy operations that define how to build an RDD. Most of the transformations
    accept a single function argument. All these methods convert one data source to
    another. Every time you perform a transformation on any RDD, a new RDD will be
    generated, even if it is a small change as shown in the following diagram:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 编写Spark程序通常包括转换和动作。转换是延迟操作，定义了如何构建RDD。大多数转换接受一个函数参数。所有这些方法都将一个数据源转换为另一个数据源。每次对任何RDD执行转换时，都会生成一个新的RDD，即使是一个小的改变，如下图所示：
- en: '![Creating RDDs](img/image_02_003.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![创建RDDs](img/image_02_003.jpg)'
- en: This is because the RDDs are immutable (read-only) abstractions by design. The
    resulting output from an action can either be written back to the storage system
    or it can be returned to the driver program for local computation if needed to
    produce the final output.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为RDD是不可变（只读）的抽象设计。从动作中产生的输出可以被写回到存储系统，也可以返回给驱动程序进行本地计算，以便产生最终输出。
- en: So far, we have seen some simple transformations that define RDDs and some actions
    to process them and generate some output. Let us go on a quick tour of some handy
    transformations and actions followed by transformations on pair RDDs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了一些简单的转换来定义RDD，并进行了一些处理和生成一些输出的动作。让我们快速浏览一些方便的转换和转换对配对RDD的转换。
- en: Transformations on normal RDDs
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对普通RDD的转换
- en: The Spark API includes a rich set of transformation operators, and developers
    can compose them in arbitrary ways. Try out the following examples on the interactive
    shell to gain a better understanding of these operations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Spark API包括丰富的转换操作符，开发人员可以以任意方式组合它们。尝试在交互式shell上尝试以下示例，以更好地理解这些操作。
- en: The filter operation
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: filter操作
- en: The `filter` operation returns an RDD with only those elements that satisfy
    a `filter` condition, similar to the `WHERE` condition in SQL.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter`操作返回一个只包含满足`filter`条件的元素的RDD，类似于SQL中的`WHERE`条件。'
- en: '**Python**:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Scala**:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The distinct operation
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: distinct操作
- en: 'The distinct (`[numTasks]`) operation returns an RDD with a new dataset after
    eliminating duplicates:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: distinct(`[numTasks]`)操作在消除重复后返回一个新数据集的RDD。
- en: '**Python**:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Scala**:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The intersection operation
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交集操作
- en: 'The intersection operation takes another dataset as input. It returns a dataset
    that contains common elements:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: intersection操作接受另一个数据集作为输入。它返回一个包含共同元素的数据集。
- en: '**Python**:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Scala**:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The union operation
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: union操作
- en: 'The union operation takes another dataset as input. It returns a dataset that
    contains elements of itself and the input dataset supplied to it. If there are
    common values in both sets, then they will appear as duplicate values in the resulting
    set after union:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: union操作接受另一个数据集作为输入。它返回一个包含自身元素和提供给它的输入数据集的元素的数据集。如果两个集合中有共同的值，则它们将在联合后的结果集中出现为重复值。
- en: '**Python**:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Scala**:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The map operation
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: map操作
- en: 'The map operation returns a distributed dataset formed by executing an input
    function on each of the elements in the input dataset:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: map操作通过在输入数据集的每个元素上执行输入函数来返回一个分布式数据集。
- en: '**Python**:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Scala**:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The flatMap operation
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: flatMap操作
- en: 'The flatMap operation is similar to the `map` operation. While `map` returns
    one element per input element, `flatMap` returns a list of zero or more elements
    for each input element:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: flatMap操作类似于`map`操作。而`map`为每个输入元素返回一个元素，`flatMap`为每个输入元素返回零个或多个元素的列表。
- en: '**Python**:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Scala**:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The keys operation
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: keys操作
- en: 'The keys operation returns an RDD with the key of each tuple:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: keys操作返回每个元组的键的RDD。
- en: '**Python**:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Scala**:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The cartesian operation
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cartesian操作
- en: 'The `cartesian` operation takes another dataset as argument and returns the
    Cartesian product of both datasets. This can be an expensive operation, returning
    a dataset of size `m` x `n` where `m` and `n` are the sizes of input datasets:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`cartesian`操作接受另一个数据集作为参数，并返回两个数据集的笛卡尔积。这可能是一个昂贵的操作，返回一个大小为`m` x `n`的数据集，其中`m`和`n`是输入数据集的大小。'
- en: '**Python**:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Scala**:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Transformations on pair RDDs
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对成对RDD的转换
- en: Some Spark operations are available only on RDDs of key value pairs. Note that
    most of these operations, except counting operations, usually involve shuffling,
    because the data related to a key may not always reside on a single partition.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一些Spark操作仅适用于键值对的RDD。请注意，除了计数操作之外，这些操作通常涉及洗牌，因为与键相关的数据可能并不总是驻留在单个分区上。
- en: The groupByKey operation
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: groupByKey操作
- en: 'Similar to the SQL `groupBy` operation, this groups input data based on the
    key and you can use `aggregateKey` or `reduceByKey` to perform aggregate operations:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于SQL的`groupBy`操作，这根据键对输入数据进行分组，您可以使用`aggregateKey`或`reduceByKey`执行聚合操作。
- en: '**Python**:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Scala**:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE21]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The join operation
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: join操作
- en: 'The join operation takes another dataset as input. Both datasets should be
    of the key value pairs type. The resulting dataset is yet another key value dataset
    having keys and values from both datasets:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: join操作接受另一个数据集作为输入。两个数据集都应该是键值对类型。结果数据集是另一个具有来自两个数据集的键和值的键值数据集。
- en: '**Python**:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE22]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Scala**:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The reduceByKey operation
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: reduceByKey操作
- en: 'The reduceByKey operation merges the values for each key using an associative
    reduce function. This will also perform the merging locally on each mapper before
    sending results to a reducer and producing hash-partitioned output:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: reduceByKey操作使用关联的reduce函数合并每个键的值。这也会在将结果发送到reducer并生成哈希分区输出之前在每个mapper上本地执行合并。
- en: '**Python**:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE24]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Scala**:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE25]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The aggregate operation
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: aggregate操作
- en: 'The aggregrate operation returns an RDD with the keys of each tuple:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: aggregrate操作返回每个元组的键的RDD。
- en: '**Python**:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE26]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Scala**:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE27]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that in the preceding aggregate examples, the resultant strings (for example,
    `abcd`, `xxabxcd`, `53`, `01`) you get need not match the output shown here exactly.
    It depends on the order in which the individual tasks return their output.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的聚合示例中，您得到的结果字符串（例如`abcd`，`xxabxcd`，`53`，`01`）不一定要与此处显示的输出完全匹配。这取决于各个任务返回其输出的顺序。
- en: Actions
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作
- en: Once an RDD has been created, the various transformations get executed only
    when an *action* is performed on it. The result of an action can either be data
    written back to the storage system or returned to the driver program that initiated
    this for further computation locally to produce the final result.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了RDD，各种转换只有在对其执行*动作*时才会执行。动作的结果可以是写回存储系统的数据，也可以返回给启动此操作的驱动程序，以便在本地进行进一步计算以生成最终结果。
- en: We have already covered some of the action functions in the previous examples
    of transformations. The following are a few more, but there are a lot more that
    you have to explore.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在之前的转换示例中涵盖了一些动作函数。以下是一些更多的示例，但还有很多需要您去探索。
- en: The collect() function
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: collect()函数
- en: The `collect()` function returns all the results of an RDD operation as an array
    to the driver program. This is usually useful for operations that produce sufficiently
    small datasets. Ideally, the result should easily fit in the memory of the system
    that's hosting the driver program.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`collect()`函数将RDD操作的所有结果作为数组返回给驱动程序。这通常对于生成数据集足够小的操作非常有用。理想情况下，结果应该很容易适应托管驱动程序的系统的内存。'
- en: The count() function
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: count()函数
- en: This returns the number of elements in a dataset or the resulting output of
    an RDD operation.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回数据集中的元素数量或RDD操作的结果输出。
- en: The take(n) function
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: take(n)函数
- en: The `take(n)` function returns the first (`n`) elements of a dataset or the
    resulting output of an RDD operation.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`take(n)`函数返回数据集的前(`n`)个元素或RDD操作的结果输出。'
- en: The first() function
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: first()函数
- en: The `first()` function returns the first element of the dataset or the resulting
    output of an RDD operation. It works similarly to the `take(1)` function.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`first()`函数返回数据集的第一个元素或RDD操作的结果输出。它的工作方式类似于`take(1)`函数。'
- en: The takeSample() function
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: takeSample()函数
- en: 'The `takeSample(withReplacement, num, [seed])` function returns an array with
    a random sample of elements from a dataset. It has three arguments as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`takeSample(withReplacement, num, [seed])`函数返回数据集中元素的随机样本数组。它有三个参数如下：'
- en: '`withReplacement`/`withoutReplacement`: This indicates sampling with or without
    replacement (while taking multiple samples, it indicates whether to replace the
    old sample back to the set and then take a fresh sample or sample without replacing).
    For `withReplacement`, argument should be `True` and `False` otherwise.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`withReplacement`/`withoutReplacement`：这表示采样是否有放回（在取多个样本时，它表示是否将旧样本放回集合然后取新样本或者不放回取样）。对于`withReplacement`，参数应为`True`，否则为`False`。'
- en: '`num`: This indicates the number of elements in the sample.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num`：这表示样本中的元素数量。'
- en: '`Seed`: This is a random number generator seed (optional).'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Seed`：这是一个随机数生成器的种子（可选）。'
- en: The countByKey() function
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: countByKey()函数
- en: The `countByKey()` function is available only on RDDs of type key value. It
    returns a table of (`K`, `Int`) pairs with the count of each key.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`countByKey()`函数仅适用于键值类型的RDD。它返回一个(`K`, `Int`)对的表，其中包含每个键的计数。'
- en: 'The following are some example code snippets on Python and Scala:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关于Python和Scala的示例代码片段：
- en: '**Python**:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE28]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Scala**:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE29]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Summary
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we touched upon the supported programming languages, their
    advantages and when to choose one language over the other. We discussed the design
    of the Spark engine along with its core components and their execution mechanism.
    We saw how Spark sends the data to be computed across many cluster nodes. We then
    discussed some RDD concepts. We learnt how to create RDDs and perform transformations
    and actions on them through both Scala and Python. We also discussed some advanced
    operations on RDDs.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涉及了支持的编程语言，它们的优势以及何时选择一种语言而不是另一种语言。我们讨论了Spark引擎的设计以及其核心组件及其执行机制。我们看到了Spark如何将要计算的数据发送到许多集群节点上。然后我们讨论了一些RDD概念。我们学习了如何通过Scala和Python在RDD上创建RDD并对其执行转换和操作。我们还讨论了一些RDD的高级操作。
- en: In the next chapter, we will learn about DataFrames in detail and how they justify
    their suitability for all sorts of data science requirements.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将详细了解DataFrame以及它们如何证明适用于各种数据科学需求。
- en: References
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'Scala language:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Scala语言：
- en: '[http://www.scala-lang.org](http://www.scala-lang.org)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.scala-lang.org](http://www.scala-lang.org)'
- en: 'Apache Spark architecture:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark架构：
- en: '[http://lintool.github.io/SparkTutorial/slides/day1_context.pdf](http://lintool.github.io/SparkTutorial/slides/day1_context.pdf)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://lintool.github.io/SparkTutorial/slides/day1_context.pdf](http://lintool.github.io/SparkTutorial/slides/day1_context.pdf)'
- en: 'The Spark programming guide is the primary resource for concepts; refer to
    the language-specific API documents for a complete list of operations available:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Spark编程指南是概念的主要资源；参考特定语言的API文档以获取可用操作的完整列表。
- en: '[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html)'
- en: 'Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory
    Cluster Computing by Matei Zaharia and others is the original source for RDD basics:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性分布式数据集：Matei Zaharia等人的《内存集群计算的容错抽象》是RDD基础知识的原始来源：
- en: '[https://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf](https://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf](https://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf)'
- en: '[http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf](http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf](http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf)'
- en: 'Spark Summit, the official event series of Apache Spark, has a wealth of the
    latest information. Check out past events'' presentations and videos:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Summit是Apache Spark的官方活动系列，提供了大量最新信息。查看过去活动的演示文稿和视频：
- en: '[https://spark-summit.org/2016/](https://spark-summit.org/2016/)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark-summit.org/2016/](https://spark-summit.org/2016/)'
