- en: Chapter 8. Ensemble Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 集成方法
- en: 'In this chapter, we will look at the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下内容：
- en: Understanding Ensemble – the bagging Method
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解集成方法——袋装法（Bagging）
- en: Understanding Ensemble – the boosting Method, AdaBoost
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解集成方法——提升法（Boosting），AdaBoost
- en: Understanding Ensemble – the gradient Boosting
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解集成方法——梯度提升（Gradient Boosting）
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, we are going to look at recipes covering the ensemble methods.
    When we are faced with the challenge of uncertainty while making a decision in
    real life, we typically approach multiple friends for opinion. We make our decision
    based on the collective knowledge that we receive from those friends. Ensemble
    is a similar concept in machine learning. In the previous chapters, we built a
    single model for our datasets, and used the prediction of that model on unseen
    test data. What if we build a lot of models on that data and make our final prediction
    based on the prediction from all these individual models? This is the idea behind
    Ensemble. Using the Ensemble approach for a given problem, we proceed with building
    a lot of models, and use all of them to make our final prediction on unseen datasets.
    For a regression problem, the final output may be the average prediction value
    from all the models. In a classification context, a majority vote is taken to
    decide the output class.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论涉及集成方法的内容。当我们在现实生活中面临不确定性决策时，通常会向多个朋友寻求意见。我们根据从这些朋友那里获得的集体知识做出决策。集成方法在机器学习中的概念类似。前几章中，我们为我们的数据集构建了单一模型，并使用该模型对未见过的测试数据进行预测。如果我们在这些数据上构建多个模型，并根据所有这些模型的预测结果做出最终预测，会怎么样呢？这就是集成方法的核心思想。使用集成方法解决某个问题时，我们会构建多个模型，并利用它们所有的预测结果来对未见过的数据集做出最终预测。对于回归问题，最终输出可能是所有模型预测值的平均值。在分类问题中，则通过多数投票来决定输出类别。
- en: The fundamental idea is to have a lot of models, each one of them producing
    slightly different results on the training dataset. Some models learn certain
    aspects of the data very well as compared to the others. The belief is that the
    final output from all these models should be better than the output produced by
    just any one of them.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是拥有多个模型，每个模型在训练数据集上产生略有不同的结果。有些模型比其他模型更好地学习数据的某些方面。其信念是，所有这些模型的最终输出应该优于任何单一模型的输出。
- en: As mentioned earlier, the idea of ensemble is to combine many models together.
    These models can be of the same or of different types. For example, we can combine
    a neural network model output with a Bayesian model. We will restrict our discussions
    to using an ensemble of the same type of models in this chapter. Combining the
    same kind of models is vastly used in the Data Science community through techniques
    like Bagging and Boosting.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，集成方法的核心是将多个模型结合在一起。这些模型可以是相同类型的，也可以是不同类型的。例如，我们可以将神经网络模型的输出与贝叶斯模型的输出结合在一起。本章中我们将限制讨论使用相同类型的模型进行集成。通过像袋装法（Bagging）和提升法（Boosting）这样的技术，数据科学社区广泛使用相同类型模型的集成方法。
- en: Bootstrap aggregation, commonly known as Bagging, is an elegant technique for
    generating a lot of models and combining their output to make a final prediction.
    Every model in a Bagging ensemble uses only a portion of the training data. The
    idea behind Bagging is to reduce the overfitting of data. As stated before, we
    want each of the model to be slightly different from the others. So we sample
    the data with replacement for training each of the models, and thus introduce
    variability. Another way to introduce variation in the model is to sample the
    attributes. We don't provide all the attributes to the model, but different models
    get a different set of attributes. Bagging can be easily parallelized. Based on
    the parallel processing framework available, models can be constructed in parallel
    with different samples of the training dataset. Bagging doesn't work with linear
    predictors like linear regression.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自助聚合（Bootstrap aggregation），通常称为袋装法（Bagging），是一种优雅的技术，用于生成大量模型并将它们的输出结合起来以做出最终预测。袋装法中的每个模型只使用部分训练数据。袋装法的核心思想是减少数据的过拟合。如前所述，我们希望每个模型与其他模型略有不同。因此，我们对每个模型的训练数据进行有放回的抽样，从而引入了变异性。引入模型变异的另一种方式是对属性进行抽样。我们不会将所有属性提供给模型，而是让不同的模型获得不同的属性集。袋装法可以轻松并行化。基于现有的并行处理框架，可以在不同的训练数据子集上并行构建模型。袋装法不适用于线性预测器，如线性回归。
- en: 'Boosting is an ensemble technique which produces a sequence of increasingly
    complex models. It works sequentially by training the newer models based on the
    errors in the previous models. Every model that is trained is associated with
    a weight, which is calculated based on how well the model has performed on the
    given data. When the final prediction is made, these weights decides the amount
    of influence that a particular model has over the final output. Boosting does
    not lend itself to parallelism as naturally as Bagging. Since the models are built
    in a sequence, it cannot be parallelized. Errors made by classifiers coming early
    in the sequence are considered as hard instances to classify. The framework is
    designed in such a way that models coming later in the sequence pick up those
    misclassified or erroneous predictions made by the previous predictor, and try
    to improve upon them. Typically, very weak classifiers are used in Boosting, for
    example a decision stump, which is a decision tree with a single splitting node
    and two leaves, is used inside the ensemble. A very famous success story about
    Boosting is the Viola Jone Face Detection algorithm where several weak classifiers
    (decision stumps) were used to find good features. You can read more about this
    success story at the following website:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting 是一种集成技术，能够产生一系列逐渐复杂的模型。它是按顺序工作的，通过基于前一个模型的错误来训练新的模型。每一个训练出来的模型都会有一个与之关联的权重，这个权重是根据该模型在给定数据上表现的好坏来计算的。当最终做出预测时，这些权重决定了某个特定模型对最终结果的影响程度。与
    Bagging 不同，Boosting 不太容易进行并行化处理。由于模型是按序列构建的，它无法并行化处理。序列中较早的分类器所犯的错误被视为难以分类的实例。该框架的设计是让序列中后续的模型拾取前一个预测器所犯的错误或误分类，并尝试改进它们。Boosting
    通常使用非常弱的分类器，例如决策树桩，即一个只有一个分裂节点和两个叶子的决策树，作为集成中的一部分。关于 Boosting 的一个非常著名的成功案例是 Viola-Jones
    人脸检测算法，其中多个弱分类器（决策树桩）被用来找出有效特征。你可以在以下网站上查看更多关于该成功案例的内容：
- en: '[https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework)'
- en: In this chapter, we will study the Bagging and Boosting Methods in detail. We
    will extend our discussion to a special type of Boosting called Gradient Boosting
    in the final recipe. We will also take a look at both regression and classification
    problems, and see how they can be addressed by ensemble learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细研究 Bagging 和 Boosting 方法。我们将在最后的配方中扩展讨论一种特殊类型的 Boosting，叫做梯度 Boosting。我们还将探讨回归和分类问题，并看看如何通过集成学习来解决这些问题。
- en: Understanding Ensemble – Bagging Method
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解集成学习—Bagging 方法
- en: Ensemble methods belong to the family of methods known as committee-based learning.
    Instead of leaving the decision of classification or regression to a single model,
    a group of models is used to make decisions in an ensemble. Bagging is a famous
    and widely used ensemble method.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法属于一种叫做基于委员会学习的方法家族。与其将分类或回归的决策交给单一模型，不如通过一组模型来共同决策。Bagging 是一种著名且广泛使用的集成方法。
- en: Bagging is also known as bootstrap aggregation. Bagging can be made effective
    only if we are able to introduce variability in the underlying models, that is,
    if we can successfully introduce variability in the underlying dataset, it will
    lead to models with slight variations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 也叫做自助聚合。只有当我们能够在基础模型中引入变异性时，Bagging 才能发挥作用。也就是说，如果我们能够成功地在基础数据集中引入变异性，就会导致具有轻微变化的模型。
- en: We leverage Bootstrapping to fed to these models variability in our dataset.
    Bootstrapping is the process by which we randomly sample the given dataset for
    a specified number of instances, with or without replacement. In bagging, we leverage
    bootstrapping to generate, say, `m` is the different datasets and construct a
    model for each of them. Finally, we average the output of all the models to produce
    the final prediction in case of regression problems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用自助采样为这些模型引入数据集的变异性。自助采样是通过随机抽取给定数据集中的实例，指定次数地进行采样，且可以选择是否有放回。 在 Bagging
    中，我们通过自助采样生成 `m` 个不同的数据集，并为每个数据集构建一个模型。最后，我们对所有模型的输出进行平均，来产生回归问题中的最终预测结果。
- en: 'Let us say we bootstrap the data m times, we would have `m` models, that is,
    `y m` values, and our final prediction would be as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们对数据进行 `m` 次自助采样，我们将得到 `m` 个模型，也就是 `y m` 个值，最终的预测结果如下：
- en: '![Understanding Ensemble – Bagging Method](img/B04041_08_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成学习 – Bagging方法](img/B04041_08_01.jpg)'
- en: In case of classification problems, the final output is decided based on voting.
    Let us say we have one hundred models in our ensemble, and we have a two-class
    classification problem with class labels as {+1,-1}. If more than 50 models predict
    the output as +1, we declare the prediction as +1\.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，最终的输出是通过投票决定的。假设我们的集成模型中有一百个模型，并且我们有一个二类分类问题，类别标签为{+1,-1}。如果超过50个模型预测输出为+1，我们就将预测结果判定为+1。
- en: Randomization is another technique by which variability can be introduced in
    the model building exercise. An example is to pick randomly a subset of attributes
    for each model in the ensemble. That way, different models will have different
    sets of attributes. This technique is called the random subspaces method.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随机化是另一种可以在模型构建过程中引入变化的技术。例如，可以随机选择每个集成模型的一个属性子集。这样，不同的模型将拥有不同的属性集。这种技术叫做随机子空间方法。
- en: With very stable models, Bagging may not achieve very great results. Bagging
    helps most if the underlying classifier is very sensitive to even small changes
    to the data. For example, Decision trees, which are very unstable. Unpruned decision
    trees are a good candidate for Bagging. But say a Nearest Neighbor Classifier,
    K, is a very stable model. However, we can leverage the random subspaces, and
    introduce some instability into the nearest neighbor methods.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非常稳定的模型，Bagging可能无法取得很好的效果。如果基础分类器对数据的微小变化非常敏感，Bagging则会非常有效。例如，决策树非常不稳定，未剪枝的决策树是Bagging的良好候选模型。但是，如果是一个非常稳定的模型，比如最近邻分类器K，我们可以利用随机子空间方法，为最近邻方法引入一些不稳定性。
- en: In the following recipe, you will learn how to leverage Bagging and Random subspaces
    on a K-Nearest Neighbor algorithm. We will take up a classification problem, and
    the final prediction will be based on majority voting.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的方案中，你将学习如何在K最近邻算法中应用Bagging和随机子空间方法。我们将解决一个分类问题，最终的预测将基于多数投票。
- en: Getting ready…
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作…
- en: We will leverage the Scikit learn classes' `KNeighborsClassifier` for classification
    and `BaggingClassifier` for applying the bagging principle. We will generate data
    for this recipe using the `make_classification` convenience function.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Scikit learn中的`KNeighborsClassifier`进行分类，并使用`BaggingClassifier`来应用Bagging原理。我们将通过`make_classification`便捷函数生成本方案的数据。
- en: How to do it
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'Let us import the necessary libraries, and write a function `get_data()` to
    provide us with a dataset to work through this recipe:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入必要的库，并编写一个`get_data()`函数，为我们提供数据集，以便我们可以处理这个方案：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let us proceed to write three functions:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始编写三个函数：
- en: Function build_single_model to make a simple KNearest neighbor model with the
    given data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`build_single_model`用给定数据构建一个简单的K最近邻模型。
- en: Function build_bagging_model, a function which implements the Bagging routine.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`build_bagging_model`实现了Bagging过程。
- en: 'The function view_model to inspect the model that we have built:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`view_model`用于检查我们已经构建的模型：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, we will write our main function, which will call the other functions:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将编写主函数，调用其他函数：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How it works…
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Let us start with the main method. We first call the `get_data` function to
    return the dataset as a matrix x of predictors and a vector y for the response
    variable. Let us look into the `get_data` function:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从主方法开始。首先调用`get_data`函数，返回一个包含预测变量的矩阵x和响应变量的向量y。让我们来看一下`get_data`函数：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Take a look at the parameters passed to the `make_classification` method. The
    first parameter is the number of instances required; in this case, we say we need
    500 instances. The second parameter is the number of, attributes that are required
    per instance. We say that we need `30` of them as defined by the variable `no_features`.
    The third parameter, `flip_y`, randomly interchanges 3 percent of the instances.
    This is done to introduce some noise in our data. The next parameter specifies
    the number of features out of those `30` that should be informative enough to
    be used in our classification. We have specified that 60 percent of our features,
    that is, 18 out of 30 should be informative. The next parameter is about redundant
    features. These are generated as a linear combination of the informative features
    in order to introduce a correlation among the features. Finally, repeated features
    are the duplicate features which are drawn randomly from both informative features
    and redundant features.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下传递给`make_classification`方法的参数。第一个参数是所需的实例数；在这种情况下，我们说需要500个实例。第二个参数是每个实例所需的属性数。我们说我们需要`30`个属性，这由变量`no_features`定义。第三个参数，`flip_y`，会随机交换3%的实例。这是为了在我们的数据中引入一些噪声。下一个参数指定了这`30`个特征中应该有多少个是足够信息量大的，可以用于我们的分类。我们指定60%的特征，也就是30个中的18个应该是有信息量的。下一个参数是关于冗余特征的。这些特征是由信息性特征的线性组合生成的，用于在特征之间引入相关性。最后，重复特征是从信息性特征和冗余特征中随机抽取的重复特征。
- en: 'Let us split the data into a training and a testing set using `train_test_split`.
    We reserve 30 percent of our data for testing:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`train_test_split`将数据划分为训练集和测试集。我们保留30%的数据用于测试：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Once again we leverage train_test_split to split our test data into dev and
    test.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次利用`train_test_split`将我们的测试数据分为开发集和测试集。
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Having divided the data for building, evaluating, and testing the model, we
    proceed to build our models. We are going to initially build a single model using
    `KNeighborsClassifier` by invoking the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在划分好数据以构建、评估和测试模型后，我们继续构建模型。我们将首先通过调用以下代码构建一个单一模型，使用`KNeighborsClassifier`：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Inside this function, we create an object of type `KNeighborsClassifier` and
    fit our data, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数内部，我们创建了一个`KNeighborsClassifier`类型的对象，并将我们的数据进行拟合，如下所示：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As explained in the previous section, `KNearestNeighbor` is a very stable algorithm.
    Let us see how this model performs. We perform our predictions on the training
    data and look at our model metrics:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，`KNearestNeighbor`是一个非常稳定的算法。让我们看看这个模型的表现。我们在训练数据上执行预测，并查看我们的模型指标：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`classification_report` is a convenient function under the module metric in
    Scikit learn. It gives a table for `precision`, `recall`, and `f1-score`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`classification_report`是Scikit learn模块中的一个方便函数。它给出一个包含`precision`、`recall`和`f1-score`的表格：'
- en: '![How it works…](img/B04041_08_02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作…](img/B04041_08_02.jpg)'
- en: 'Out of `350` instances, our precision is 87 percent. With this figure, let
    us proceed to build our bagging model:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在`350`个实例中，我们的精确度为87%。有了这个结果，让我们继续构建我们的集成模型：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We invoke the function `build_bagging_model` with our training data to build
    a bag of classifiers, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用训练数据调用`build_bagging_model`函数，构建一个分类器集合，如下所示：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Inside the method, we invoke the `BaggingClassifier` class. Let us look at the
    arguments that we pass to this class to initialize it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方法中，我们调用了`BaggingClassifier`类。让我们看一下我们传递给这个类的参数，以便初始化它。
- en: The first argument is the underlying estimator or model. By passing `KNeighborClassifier`,
    we are telling the bagging classifier that we want to build a bag of `KNearestNeighbor`
    classifiers. The next parameter specifies the number of estimators that we will
    build. In this case, we are saying we need `100` of them. The `random_state` argument
    is the seed to be used by the random number generator. In order to be consistent
    during different runs, we set this to an integer value.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是底层估计器或模型。通过传递`KNeighborClassifier`，我们告诉集成分类器我们想要构建一个由`KNearestNeighbor`分类器组成的集合。下一个参数指定了我们将构建的估计器的数量。在这种情况下，我们说需要`100`个估计器。`random_state`参数是随机数生成器使用的种子。为了在不同的运行中保持一致性，我们将其设置为一个整数值。
- en: Our next parameter is max_samples, where we specify the number of instances
    to be selected for one estimator when we bootstrap from our input dataset. In
    this case, we are asking the bagging routine to select all the instances.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个参数是max_samples，指定在从输入数据集进行自助采样时，每个估计器选择的实例数。在这种情况下，我们要求集成程序选择所有实例。
- en: Next, the parameter max_features specifies the number of attributes that are
    to be included while bootstrapping for an estimator. We say that we want to include
    only 70 percent of the attributes. So for each estimator/model inside the ensemble,
    it will be using a different subset of the attributes to build the model. This
    is the random space methodology that we introduced in the previous section. The
    function proceeds to fit the model and return the model to the calling function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，参数max_features指定在为估计器进行自助采样时要包含的属性数量。我们假设只选择70%的属性。因此，在集成中的每个估计器/模型将使用不同的属性子集来构建模型。这就是我们在上一节中介绍的随机空间方法。该函数继续拟合模型并将模型返回给调用函数。
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let us look at the model accuracy:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型的准确性：
- en: '![How it works…](img/B04041_08_03.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![How it works…](img/B04041_08_03.jpg)'
- en: You can see a big jump in the model metrics.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到模型指标有了大幅提升。
- en: 'Before we test our models with our dev dataset, let us look at the attributes
    that were allocated to the different models, by invoking the view_model function:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试模型之前，让我们先查看通过调用view_model函数分配给不同模型的属性：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We print the attributes selected for the first ten models, as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打印出前十个模型选择的属性，如下所示：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![How it works…](img/B04041_08_04.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![How it works…](img/B04041_08_04.jpg)'
- en: As you can make out from the result, we have assigned attributes to every estimator
    pretty much randomly. In this way, we introduced variability into each of our
    estimator.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中可以看出，我们已经几乎随机地为每个估计器分配了属性。通过这种方式，我们为每个估计器引入了变异性。
- en: 'Let us proceed to check how our single classifier and bag of estimators have
    performed in our dev set:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续检查我们单一分类器和估计器集合在开发集中的表现：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![How it works…](img/B04041_08_05.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![How it works…](img/B04041_08_05.jpg)'
- en: As expected, our bag of estimators has performed better in our dev set as compared
    to our single classifier.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，与单一分类器相比，我们的估计器集合在开发集中的表现更好。
- en: There's more…
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'As we said earlier, in the case of classification, the label with the majority
    number of votes is considered as the final prediction. Instead of the voting scheme,
    we can ask the constituent models to output the prediction probabilities for the
    labels. An average of the probabilities can be finally taken to decide the final
    output label. In Scikit''s case, the documentation of the API provides the details
    on how the final prediction is performed:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说，对于分类问题，获得最多票数的标签将被视为最终预测。除了投票方案，我们还可以要求组成模型输出标签的预测概率。最终，可以取这些概率的平均值来决定最终的输出标签。在Scikit的情况下，API文档提供了关于如何执行最终预测的详细信息：
- en: '*''The predicted class of an input sample is computed as the class with the
    highest mean predicted probability. If base estimators do not implement a *predict
    phobia* method, then it resorts to voting.''*'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*''输入样本的预测类别是通过选择具有最高平均预测概率的类别来计算的。如果基础估计器没有实现*predict phobia*方法，则会使用投票。''*'
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)'
- en: In the last chapter, we discussed cross validation. Although cross validation
    may look very similar to bagging, they have different uses in reality. In cross
    validation, we create K-Folds and, based on the model output from those folds,
    we may choose our parameters for the model, as we selected the alpha value for
    ridge regression. This is done primarily to avoid exposing our test data in the
    model building exercise. Cross validation can be used in Bagging to determine
    the number of estimators we need to add to our bagging module.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了交叉验证。虽然交叉验证看起来与Bagging非常相似，但它们在实际使用中是不同的。在交叉验证中，我们创建K折，并根据这些折的模型输出来选择模型的参数，就像我们为岭回归选择alpha值一样。这样做主要是为了避免在模型构建过程中暴露我们的测试数据。交叉验证可以用于Bagging，以确定我们需要向Bagging模块添加的估计器数量。
- en: However, a drawback with Bagging is that we loose the interpretability of a
    model. Consider a Simple Decision tree derived after pruning. It is very easy
    to explain the decision tree model. But once we have a bag of 100 such models,
    it become a black box. For increased accuracy, we do a trading of interpretability.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Bagging的一个缺点是我们失去了模型的可解释性。考虑一个经过剪枝的简单决策树，它很容易解释。但一旦我们有了100个这样的模型集合，它就变成了一个黑箱。为了提高准确性，我们牺牲了可解释性。
- en: 'Please refer to the following paper by Leo Breiman for more information about
    bagging:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Bagging的更多信息，请参阅Leo Breiman的以下论文：
- en: '*Leo Breiman. 1996\. Bagging predictors.*Mach. Learn.*24, 2 (August 1996),
    123-140\. DOI=10.1023/A:1018054314350 http://dx.doi.org/10.1023/A:1018054314350*'
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Leo Breiman. 1996\. Bagging predictors.*Mach. Learn.*24, 2 (August 1996),
    123-140\. DOI=10.1023/A:1018054314350 http://dx.doi.org/10.1023/A:1018054314350*'
- en: See also
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: '*Using cross validation iterators* recipe in [Chapter 7](ch07.xhtml "Chapter 7. Machine
    Learning 2"), *Machine Learning 2*'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用交叉验证迭代器*，请参阅[第7章](ch07.xhtml "Chapter 7. Machine Learning 2")，《机器学习2》'
- en: '*Building Decision Trees to solve Multi-Class Problems* recipe in [Chapter
    6](ch06.xhtml "Chapter 6. Machine Learning 1"), *Machine Learning 1*'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*构建决策树解决多类问题*，请参阅[第6章](ch06.xhtml "Chapter 6. Machine Learning 1")，《机器学习1》'
- en: Understanding Ensemble – Boosting Method
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解集成 - 提升方法
- en: Boosting is a powerful ensemble technique. It's pretty much used in most of
    the Data Science applications. In fact, it is one of the most essential tools
    in a Data Scientist tool kit. The Boosting technique utilizes a bag of estimators
    similar to Bagging. But that is where the similarity ends. Let us quickly see
    how Boosting acts as a very effective ensemble technique before jumping into our
    recipe.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting是一种强大的集成技术。它几乎被用在大多数数据科学应用中。事实上，它是数据科学家工具包中最重要的工具之一。Boosting技术利用了类似于Bagging的一组估计器。但在这里相似性就结束了。在我们深入研究我们的方法之前，让我们快速看一下Boosting如何作为一种非常有效的集成技术。
- en: 'Let''s take the familiar two-class classification problem, where the input
    is a set of predictors (`X`) and the output is a response variable (`Y`) which
    can either take `0` or `1` as value. The input for the classification problem
    is represented as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个熟悉的二类分类问题，其中输入是一组预测变量（`X`），输出是一个响应变量（`Y`），它可以取`0`或`1`作为值。分类问题的输入表示如下：
- en: '![Understanding Ensemble – Boosting Method](img/B04041_08_06.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成 - 提升方法](img/B04041_08_06.jpg)'
- en: 'The job the classifier is to find a function which can approximate:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的任务是找到一个可以近似的函数：
- en: '![Understanding Ensemble – Boosting Method](img/B04041_08_07.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成 - 提升方法](img/B04041_08_07.jpg)'
- en: 'The misclassification rate of the classifier is defined as:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的误分类率定义为：
- en: '![Understanding Ensemble – Boosting Method](img/B04041_08_08.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成 - 提升方法](img/B04041_08_08.jpg)'
- en: 'Let us say we build a very weak classifier whose error rate is slightly better
    than random guessing. In Boosting, we build a sequence of weak classifiers on
    a slightly modified set of data. We modify the data slightly for every classifier,
    and finally, we end up with M classifiers:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们构建了一个非常弱的分类器，其错误率略好于随机猜测。在Boosting中，我们在略微修改的数据集上构建了一系列弱分类器。我们为每个分类器轻微修改数据，并最终得到M个分类器：
- en: '![Understanding Ensemble – Boosting Method](img/B04041_08_09.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成 - 提升方法](img/B04041_08_09.jpg)'
- en: 'Finally, the predictions from all of them are combined through a weighted majority
    vote:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，它们所有的预测通过加权多数投票进行组合：
- en: '![Understanding Ensemble – Boosting Method](img/B04041_08_10.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成 - 提升方法](img/B04041_08_10.jpg)'
- en: This method is called AdaBoost.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法称为AdaBoost。
- en: The weight alpha and the sequential way of model building is where Boosting
    differs from Bagging. As mentioned earlier, Boosting builds a sequence of weak
    classifiers on a slightly modified data set for each classifier. Let us look at
    what that slight data modification refers to. It is from this modification that
    we derive our weight alpha.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 权重alpha和模型构建的顺序方式是Boosting与Bagging不同的地方。正如前面提到的，Boosting在每个分类器上构建了一系列略微修改的数据集上的弱分类器。让我们看看这个微小的数据修改指的是什么。正是从这个修改中我们得出了我们的权重alpha。
- en: 'Initially for the first classifier, m=1, we set the weight of each instance
    as 1/N, that is, if there are a hundred records, each record gets a weight of
    0.001\. Let us denote the weight by w—now we have a hundred such weights:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最初对于第一个分类器，m=1，我们将每个实例的权重设置为1/N，也就是说，如果有一百条记录，每条记录的权重为0.001。让我们用w来表示权重-现在我们有一百个这样的权重：
- en: '![Understanding Ensemble – Boosting Method](img/B04041_08_11.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成 - 提升方法](img/B04041_08_11.jpg)'
- en: 'All the records now have an equal chance of being selected by a classifier.
    We build the classifier, and test it against our training data to get the misclassification
    rate. Refer to the misclassification rate formula given earlier in this section.
    We are going to change it slightly by including the weights, as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所有记录现在都有相等的机会被分类器选择。我们构建分类器，并在训练数据上测试它，以获得误分类率。请参考本节前面给出的误分类率公式。我们将稍微修改它，加入权重，如下所示：
- en: '![Understanding Ensemble – Boosting Method](img/B04041_08_12.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成方法 – 提升方法](img/B04041_08_12.jpg)'
- en: 'Where abs stands for the absolute value of the results. With this error rate,
    we calculate our alpha (model weight) as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其中abs表示结果的绝对值。根据这个误差率，我们计算我们的alpha（模型权重）如下：
- en: '![Understanding Ensemble – Boosting Method](img/B04041_08_13.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成方法 – 提升方法](img/B04041_08_13.jpg)'
- en: Where epsilon is a very small value.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其中epsilon是一个非常小的值。
- en: 'Let us say our model 1 has got an error rate of 0.3, that is, the model was
    able to classify 70 percent of the records correctly. Therefore, the weight for
    that model will be 0.8, approximately, which, is a good weight. Based on this,
    we go will back and set the weights of individual records, as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的模型1的误差率是0.3，也就是说，模型能够正确分类70%的记录。因此，该模型的权重大约是0.8，这是一个不错的权重。基于此，我们将返回并设置个别记录的权重，如下所示：
- en: '![Understanding Ensemble – Boosting Method](img/B04041_08_14.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成方法 – 提升方法](img/B04041_08_14.jpg)'
- en: As you can see, the weights of all the attributes which were misclassified will
    increase. This increases the chance of the misclassified record being selected
    by the next classifier. Thus, the classifier coming next in the sequence selects
    the instances with more weight and tries to fit it. In this way, all the future
    classifiers start concentrating on the records misclassified by the previous classifier.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，所有被错误分类的属性的权重都会增加。这增加了被下一个分类器选择的错误分类记录的概率。因此，下一个分类器在序列中选择权重更大的实例并尝试拟合它。通过这种方式，所有未来的分类器将开始集中处理之前分类器错误分类的记录。
- en: This is the power of boosting. It is able to turn several weak classifiers into
    one powerful ensemble.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是提升方法的威力。它能够将多个弱分类器转化为一个强大的集成模型。
- en: Let us see boosting in action. As we proceed with our code, we will also see
    a small variation to AdaBoost known as SAMME.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看提升方法如何应用。在我们编写代码的过程中，我们还将看到AdaBoost的一个小变种，称为SAMME。
- en: Getting Started…
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用…
- en: We will leverage the scikit learn classes `DecisionTreeClassifier` for classification
    and the `AdaBoostClassifier` for applying the Boosting principle. We will generate
    data for this recipe using the `make_classification` convenience function.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用scikit-learn的`DecisionTreeClassifier`类进行分类，并使用`AdaBoostClassifier`应用提升原理。我们将使用`make_classification`便捷函数生成数据。
- en: How to do it
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: Let us import the necessary libraries, and write a function `get_data()` to
    provide us with a dataset to work through this recipe.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入必要的库，并编写一个函数`get_data()`，为我们提供一个数据集来实现这个方案。
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let us proceed and write the following three functions:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续并编写以下三个函数：
- en: The function build_single_model to make a simple Decision Tree model with the
    given data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 函数build_single_model用于使用给定数据构建一个简单的决策树模型。
- en: The function build_boosting_model, a function which implements the Boosting
    routine.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 函数build_boosting_model，它实现了提升算法。
- en: The function view_model, to inspect the model that we have built.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 函数view_model，用于检查我们构建的模型。
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We then write a function called number_estimators_vs_err_rate. We use this function
    to see how our error rates change with respect to the number of models in our
    ensemble.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们编写一个名为number_estimators_vs_err_rate的函数。我们使用此函数来查看随着集成模型中模型数量的变化，我们的误差率是如何变化的。
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Finally, we will write our main function, which will call the other functions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将编写主函数，它将调用其他函数。
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works…
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Let us start with the main method. We first call the `get_data` function to
    return the dataset as a matrix x of predictors and a vector y for the response
    variable. Let us look into the `get_data` function:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从主方法开始。我们首先调用`get_data`函数返回数据集，其中x为预测变量矩阵，y为响应变量向量。让我们深入了解`get_data`函数：
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Take a look at the parameters passed to the `make_classification` method. The
    first parameter is the number of instances required; in this case, we say we need
    500 instances. The second parameter gives the number of attributes that are required
    per instance. We say that we need 30 of them, as defined by the variable `no_features`.
    The third parameter `flip_y`, randomly interchanges 3 percent of the instances.
    This is done to introduce some noise in our data. The next parameter specifies
    the number of features out of those 30 which should be informative enough to be
    used in our classification. We have specified that 60 percent of our features,
    that is, 18 out of 30 should be informative. The next parameter is about redundant
    features. These are generated as a linear combination of the informative features
    for introducing a correlation among the features. Finally, repeated features are
    the duplicate features which are drawn randomly from both, informative features
    and redundant features.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 查看传递给`make_classification`方法的参数。第一个参数是所需的实例数量；在这种情况下，我们说我们需要500个实例。第二个参数给出了每个实例所需的属性数量。我们说我们需要30个属性，正如`no_features`变量所定义的那样。第三个参数`flip_y`，随机交换3%的实例。这是为了在数据中引入一些噪音。接下来的参数指定了这些30个特征中应该有多少个是足够有用的，可以用于分类。我们指定60%的特征，即30个特征中的18个应该是有信息量的。接下来的参数是冗余特征。这些特征是通过有信息特征的线性组合生成的，用来引入特征之间的相关性。最后，重复特征是从有信息特征和冗余特征中随机选取的重复特征。
- en: Let us split the data into a training and a testing set using `train_test_split`.
    We reserve 30 percent of our data for testing.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`train_test_split`将数据分为训练集和测试集。我们将30%的数据保留用于测试。
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Once again, we leverage train_test_split to split our test data into dev and
    test.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们利用`train_test_split`将我们的测试数据分为开发集和测试集。
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Having divided the data for building, evaluating, and testing the model, we
    proceed to build our models.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据分为用于构建、评估和测试模型的部分后，我们开始构建我们的模型。
- en: 'Let us start by fitting a single decision tree, and look at the performance
    of the tree on training set:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先拟合一棵单一的决策树，并查看该树在训练集上的表现：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We build a model by invoking `build_single_model` function with the predictors
    and response variable. Inside this we fit a single decision tree and return the
    tree back to the calling function.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过调用`build_single_model`函数，传入预测变量和响应变量来构建模型。在这个过程中，我们拟合一棵单一的决策树，并将树返回给调用函数。
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Let us evaluate how good the model is using `classification_report`, a utility
    function from Scikit learn which displays a set of metrics including `precision`,
    `recall` and `f1-score`; we also display the misclassification rate.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`classification_report`评估模型的好坏，这是来自Scikit learn的一个实用函数，它显示一组指标，包括`precision`（精确度）、`recall`（召回率）和`f1-score`；我们还会显示误分类率。
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![How it works…](img/B04041_08_15.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_08_15.jpg)'
- en: 'As you can see, our decision tree model has done a perfect job of fitting the
    data—our misclassification rate is 0\. Before we test this model on our dev data,
    let us build our ensemble:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的决策树模型完美地拟合了数据——我们的误分类率为0。在我们在开发集上测试该模型之前，让我们构建我们的集成模型：
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Using the method build_boosting_model, we build our ensemble as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`build_boosting_model`方法，我们按如下方式构建我们的集成模型：
- en: '[PRE26]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We leverage `AdaBoostClassifier` from Scikit learn to build our Boosting ensemble.
    We instantiate the class with the following parameters:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用Scikit learn中的`AdaBoostClassifier`构建我们的提升集成。我们使用以下参数实例化该类：
- en: An estimator—in our case, we say we want to build an ensemble of decision trees.
    Hence, we pass the `DecisionTreeClassifier` object.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器——在我们的案例中，我们说我们想要构建一个决策树的集成。因此，我们传入`DecisionTreeClassifier`对象。
- en: '`max_depth`—We don''t want to have fully grown trees in our ensemble. We need
    only stumps—trees with just two leaf nodes and one splitting node. Hence, we set
    the `max_depth` parameter to 1\.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth`——我们不希望在集成中使用完全生长的树木。我们只需要树桩——只有两个叶子节点和一个分割节点的树。因此，我们将`max_depth`参数设置为1。'
- en: With the n_estimators parameters, we specify the number of trees that we want
    to grow; in this case, we will grow 86 trees.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`n_estimators`参数，我们指定要生成的树木数量；在此案例中，我们将生成86棵树。
- en: Finally, we have a parameter called algorithm, which is set to `SAMME`. `SAMME`
    stands for Stage wise Additive Modeling using Multi-class Exponential loss function.
    `SAMME` is an improvement over the AdaBoosting algorithm. It tries to put more
    weights on the misclassified records. The model weight alpha is where `SAMME`
    differs from AdaBoost.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有一个参数叫做algorithm，它被设置为`SAMME`。`SAMME`代表逐阶段加法建模，使用多类指数损失函数。`SAMME`是对AdaBoost算法的改进。它试图将更多的权重分配给误分类的记录。模型权重α是`SAMME`与AdaBoost的区别所在。
- en: '![How it works…](img/B04041_08_16.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_08_16.jpg)'
- en: 'We have ignored the constant 0.5 in the preceding formula. Let us look at the
    new addition: log (K-1). If K = 2, then the preceding equation reduces to AdaBoost.
    Here, K is the number of classes in our response variable. For a two-class problem,
    SAMME reduces to AdaBoost, as stated earlier.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的公式中忽略了常数0.5。让我们来看一下新的添加项：log(K-1)。如果K=2，那么前面的公式就简化为AdaBoost。在这里，K是响应变量中的类别数。对于二分类问题，SAMME就会简化为AdaBoost，正如前面所述。
- en: 'Let us fit the model, and return it to our calling function. We run this model
    on our training dataset, and once again look at how the model has performed:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拟合模型，并将其返回给调用函数。我们在训练数据集上运行该模型，再次查看模型的表现：
- en: '[PRE27]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![How it works…](img/B04041_08_17.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_08_17.jpg)'
- en: The result is not very different from our original model. We have correctly
    classified almost 98 percent of the records with this ensemble.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与我们原始模型的表现没有太大不同。我们已经正确分类了几乎98%的记录。
- en: 'Before testing it on our dev set, let us proceed to look at the Boosting ensemble
    that we have built:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在对开发集进行测试之前，让我们首先查看我们构建的Boosting集成模型：
- en: '[PRE28]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Inside view_model, we first print out the weights assigned to each classifier
    in our ensemble:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在view_model内部，我们首先打印出分配给每个分类器的权重：
- en: '[PRE29]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![How it works…](img/B04041_08_18.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_08_18.jpg)'
- en: Here we have shown the weights of the first 20 ensembles. Based on their misclassification
    rate, we have assigned different weights to these estimators.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们展示了前20个集成的权重。根据它们的误分类率，我们为这些估计器分配了不同的权重。
- en: 'Let us proceed to plot a graph showing the estimator weight versus the error
    thrown by each estimator:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续绘制一个图表，显示估计器权重与每个估计器所产生错误之间的关系：
- en: '[PRE30]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![How it works…](img/B04041_08_19.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_08_19.jpg)'
- en: As you can see, the models which are classifying properly are assigned more
    weights than the ones with higher errors.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，正确分类的模型被分配了比错误率较高的模型更多的权重。
- en: 'Let us now look at the way single tree and the bag of tree have performed against
    the dev data:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看单一决策树和集成决策树在开发数据上的表现：
- en: '[PRE31]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Pretty much as we did with the training data, we print the classification report
    and the misclassification rate:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们对训练数据做的那样，我们打印出分类报告和误分类率：
- en: '![How it works…](img/B04041_08_20.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_08_20.jpg)'
- en: As you can see, the single tree has performed poorly. Though it displayed a
    100 percent accuracy with the training data, with the dev data it has misclassified
    almost 40 percent of the records—a sign of overfitting. In contrast, the Boosting
    model is able to make a better fit of the dev data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，单一决策树表现不佳。尽管它在训练数据上显示了100%的准确率，但在开发数据上却误分类了近40%的记录——这是过拟合的迹象。相比之下，Boosting模型能够更好地拟合开发数据。
- en: How do we go about improving the Boosting model? One way to do it is to test
    the error rate in the training set against the number of ensembles that we want
    to include in our bagging.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何改进Boosting模型？其中一种方法是测试训练集中的错误率与我们想要在集成中包含的分类器数量之间的关系。
- en: '[PRE32]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following function proceeds to fit with the increasing number of ensembles
    and plot the error rate:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数会根据集成的数量递增并绘制错误率：
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you can see, we declare a list, starting with 20 and ending with 120 in step
    size of 10s. Inside the `for` loop, we pass each element of this list as the number
    of estimator parameter to `build_boosting_model`, and then proceed to access the
    error rate of the model. We then check the error rates in the dev set. Now we
    have two lists—one which has all the error rates from the training data and another
    with the error rates from the dev data. We plot them both, where the *x* axis
    is the number of estimators and *y* axis is the misclassification rate in the
    dev and train sets.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们声明了一个列表，起始值为20，结束值为120，步长为10。在`for`循环中，我们将列表中的每个元素作为估计器参数传递给`build_boosting_model`，然后继续访问模型的错误率。接着我们检查开发集中的错误率。现在我们有两个列表—一个包含训练数据的所有错误率，另一个包含开发数据的错误率。我们将它们一起绘制，*x*轴是估计器的数量，*y*轴是开发集和训练集中的错误分类率。
- en: '![How it works…](img/B04041_08_21.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_08_21.jpg)'
- en: The preceding plot gives a clue that in around 30 to 40 estimators, the error
    rate in dev is very low. We can further experiment with the tree model parameters
    to arrive at a good model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表给出了一个线索，在大约30到40个估计器时，开发集中的错误率非常低。我们可以进一步实验树模型参数，以获得一个良好的模型。
- en: There's more…
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'Boosting was introduced in the following seminal paper:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting方法首次在以下开创性论文中提出：
- en: '*Freund, Y. & Schapire, R. (1997), ''A decision theoretic generalization of
    on-line learning and an application to boosting'', Journal of Computer and System
    Sciences 55(1), 119–139.*'
  id: totrans-179
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Freund, Y. & Schapire, R. (1997), ''A decision theoretic generalization of
    on-line learning and an application to boosting'', Journal of Computer and System
    Sciences 55(1), 119–139.*'
- en: 'Initially, most of the Boosting methods reduced the multiclass problems into
    two-class problems and multiple two-class problems. The following paper extends
    AdaBoost to the multiclass problems:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，大多数Boosting方法将多类问题简化为二类问题和多个二类问题。以下论文将AdaBoost扩展到多类问题：
- en: '*Multi-class AdaBoost Statistics and Its Interface, Vol. 2, No. 3\. (2009),
    pp. 349-360,doi:10.4310/sii.2009.v2.n3.a8 by Trevor Hastie, Saharon Rosset, Ji
    Zhu, Hui Zou*'
  id: totrans-181
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Multi-class AdaBoost Statistics and Its Interface, Vol. 2, No. 3 (2009), pp.
    349-360, doi:10.4310/sii.2009.v2.n3.a8 by Trevor Hastie, Saharon Rosset, Ji Zhu,
    Hui Zou*'
- en: This paper also introduces SAMME, the method that we have used in our recipe.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 本文还介绍了SAMME，这是我们在配方中使用的方法。
- en: See also
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: '*Building Decision Trees to solve Multi-Class Problems* recipe in [Chapter
    6](ch06.xhtml "Chapter 6. Machine Learning 1"), *Machine Learning I*'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.xhtml "第6章. 机器学习 I")中，*构建决策树解决多类问题*的配方，*机器学习 I*
- en: '*Using cross validation iterators* recipe in[Chapter 7](ch07.xhtml "Chapter 7. Machine
    Learning 2"), *Machine Learning II*'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.xhtml "第7章. 机器学习 II")中，*使用交叉验证迭代器*的配方，*机器学习 II*
- en: '*Understanding Ensemble – Bagging Method* recipe in[Chapter 8](ch08.xhtml "Chapter 8. Ensemble
    Methods"), *Model Selection and Evaluation*'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第8章](ch08.xhtml "第8章. 集成方法")中，*理解集成方法 – 自助法*的配方，*模型选择与评估*
- en: Understanding Ensemble – Gradient Boosting
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解集成方法 – 梯度Boosting
- en: Let us recall the Boosting algorithm explained in the previous recipe. In boosting,
    we fitted an additive model in a forward, stage-wise manner. We built the classifiers
    sequentially. After building each classifier, we estimated the weight/importance
    of the classifiers. Based on weights/importance, we adjusted the weights of the
    instances in our training set. Misclassified instances were weighted higher than
    the correctly classified ones. We would like the next model to pick those incorrectly
    classified instances and train on them. Instances from the dataset which didn't
    fit properly were identified using these weights. Another way of looking at it
    is that those records were the shortcomings of the previous model. The next model
    tries to overcome those shortcomings.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下前面配方中解释的Boosting算法。在Boosting中，我们以逐步的方式拟合加法模型。我们顺序地构建分类器。构建每个分类器后，我们估计分类器的权重/重要性。根据权重/重要性，我们调整训练集中实例的权重。错误分类的实例比正确分类的实例权重更高。我们希望下一个模型能够选择那些错误分类的实例并在其上进行训练。训练集中的那些没有正确拟合的实例会通过这些权重被识别出来。换句话说，这些记录是前一个模型的不足之处。下一个模型试图克服这些不足。
- en: Gradient Boosting uses gradients instead of weights to identify those shortcomings.
    Let us quickly see how we can use gradients to improve models.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度Boosting使用梯度而非权重来识别这些不足之处。让我们快速了解如何使用梯度来改进模型。
- en: Let us take a simple regression problem, where we are given the required predictor
    variable *X* and the response *Y*, which is a real number.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个简单的回归问题为例，假设我们已知所需的预测变量*X*和响应变量*Y*，其中*Y*是一个实数。
- en: '![Understanding Ensemble – Gradient Boosting](img/B04041_08_22.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成 – 梯度提升](img/B04041_08_22.jpg)'
- en: 'Gradient boosting proceeds as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升过程如下：
- en: It starts with a very simple model, say, mean value.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 它从一个非常简单的模型开始，比如均值模型。
- en: '![Understanding Ensemble – Gradient Boosting](img/B04041_08_23.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成 – 梯度提升](img/B04041_08_23.jpg)'
- en: The predicted value is simply the mean value of the response variables.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值仅仅是响应变量的均值。
- en: It then proceeds to fit the residuals. Residual is the difference between the
    actual value y and the predicted value y hat.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它会继续拟合残差。残差是实际值y与预测值y_hat之间的差异。
- en: '![Understanding Ensemble – Gradient Boosting](img/B04041_08_24.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成 – 梯度提升](img/B04041_08_24.jpg)'
- en: 'The next classifier is trained on the data set as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的分类器是在如下数据集上进行训练的：
- en: '![Understanding Ensemble – Gradient Boosting](img/B04041_08_25.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成 – 梯度提升](img/B04041_08_25.jpg)'
- en: The subsequent model is trained on the residual of the previous model, and thus,
    the algorithm proceeds to build the required number of models inside the ensemble.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 随后模型会在前一个模型的残差上进行训练，因此算法会继续构建所需数量的模型，最终形成集成模型。
- en: 'Let us try and understand why we train on residuals. By now it should be clear
    that Boosting makes additive models. Let us say we build two models `F1 (X)` and
    `F2(X)` to predict `Y1`. By the additive principle, we can combine these two models
    as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解为什么我们要在残差上进行训练。现在应该清楚，Boosting方法构建的是加性模型。假设我们建立两个模型`F1(X)`和`F2(X)`来预测`Y1`。根据加性原理，我们可以将这两个模型结合起来，如下所示：
- en: '![Understanding Ensemble – Gradient Boosting](img/B04041_08_26.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成 – 梯度提升](img/B04041_08_26.jpg)'
- en: That is, we combine the prediction from both the models to predict Y_1.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们结合两个模型的预测结果来预测Y_1。
- en: 'Equivalently, we can say that:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 等价地，我们可以这样说：
- en: '![Understanding Ensemble – Gradient Boosting](img/B04041_08_27.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成 – 梯度提升](img/B04041_08_27.jpg)'
- en: Residual is the part where the model has not done well, or to put it simply,
    residuals are the short comings of the previous model. Hence, we use the residual
    improve our model, that is, improving the shortcomings of the previous model.
    Based on this discussion, you would wonder why the method is called Gradient Boosting
    instead of Residual Boosting.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 残差是模型未能很好拟合的部分，或者简单来说，残差就是前一个模型的不足之处。因此，我们利用残差来改进模型，即改进前一个模型的不足。基于这个讨论，你可能会好奇为什么这种方法叫做梯度提升（Gradient
    Boosting）而不是残差提升（Residual Boosting）。
- en: 'Given a function which is differentiable, Gradient stands for the first-order
    derivatives of that function at certain values. In the case of regression, the
    objective function is:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个可微的函数，梯度表示该函数在某些值处的一阶导数。在回归问题中，目标函数为：
- en: '![Understanding Ensemble – Gradient Boosting](img/B04041_08_28.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成 – 梯度提升](img/B04041_08_28.jpg)'
- en: where `F(xi)` is our regression model.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，`F(xi)`是我们的回归模型。
- en: 'The linear regression problem is about minimizing this preceding function.
    We find the first-order derivative of that function at value `F(xi)`, and if we
    update our weights'' coefficients with the negative of that derivative value,
    we will move towards a minimum solution in the search space. The first-order derivative
    of the preceding cost function with respect to `F(xi)` is `F(xi ) – yi`. Please
    refer to the following link for the derivation:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归问题是通过最小化前述函数来解决的。我们在`F(xi)`的值处求该函数的一阶导数，如果用该导数值的负值来更新权重系数，我们将朝着搜索空间中的最小解前进。前述成本函数关于`F(xi)`的一阶导数是`F(xi
    ) – yi`。请参阅以下链接了解推导过程：
- en: '[https://en.wikipedia.org/wiki/Gradient_descent](https://en.wikipedia.org/wiki/Gradient_descent)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://zh.wikipedia.org/wiki/%E6%A0%B9%E5%87%BB%E4%BD%8D%E9%9A%8F](https://zh.wikipedia.org/wiki/%E6%A0%B9%E5%87%BB%E4%BD%8D%E9%9A%8F)'
- en: '`F(xi ) – yi`, the gradient, is the negative of our residual `yi – F(xi)`,
    and hence the name Gradient Boosting.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`F(xi ) – yi`，即梯度，是我们残差`yi – F(xi)`的负值，因此得名梯度提升（Gradient Boosting）。'
- en: With this theory in mind, let us jump into our recipe for gradient boosting.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个理论，我们可以进入梯度提升的具体步骤。
- en: Getting Started…
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用……
- en: 'We are going to use the Boston dataset to demonstrate Gradient Boosting. The
    Boston data has 13 attributes and 506 instances. The target variable is a real
    number, the median value of houses in thousands. the Boston data can be downloaded
    from the UCI link:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用波士顿数据集来演示梯度提升。波士顿数据集有13个特征和506个实例。目标变量是一个实数，即房屋的中位数价格（单位为千美元）。波士顿数据集可以从UCI链接下载：
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)'
- en: We intend to generate Polynomial Features of degree 2, and consider only the
    interaction effects.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打算生成2次方的多项式特征，并仅考虑交互效应。
- en: How to do it
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'Let us import the necessary libraries and write a function `get_data()` to
    provide us with a dataset to work through this recipe:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入必要的库，并编写一个`get_data()`函数来提供我们需要使用的数据集，以便完成这个任务：
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Let us write the following three functions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写以下三个函数。
- en: The function build _model, which implements the Gradient Boosting routine.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: build_model函数实现了梯度提升算法。
- en: 'The functions view_model and model_worth, which are used to inspect the model
    that we have built:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: view_model和model_worth函数，用于检查我们构建的模型：
- en: '[PRE35]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we write our main function which will call the other functions:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们编写主函数，该函数将调用其他函数：
- en: '[PRE36]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works…
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何运作…
- en: 'Let us start with the main module and follow the code. We load the predictor
    x and response variable y using the get_data function:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从主模块开始，跟随代码进行操作。我们使用get_data函数加载预测变量x和响应变量y：
- en: '[PRE37]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The function invokes the Scikit learn's convenience function `load_boston()`
    to retrieve the Boston house pricing dataset as numpy arrays.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数调用Scikit learn的便捷函数`load_boston()`来获取波士顿房价数据集，并将其作为numpy数组加载。
- en: We proceed to divide the data into the train and test sets using the train_test_split
    function from Scikit library. We reserve 30 percent of our dataset for testing.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续使用Scikit库中的train_test_split函数将数据划分为训练集和测试集。我们保留30%的数据集用于测试。
- en: '[PRE38]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Out of this 30 percent, we again extract the dev set in the next line:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 从这30%的数据中，我们在下一行再次提取开发集：
- en: '[PRE39]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We proceed to build the polynomial features as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来构建多项式特征如下：
- en: '[PRE40]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As you can see, we have set interaction_only to True. By having interaction_only
    set to true, given say x1 and x2 attribute, only x1*x2 attribute is created. Square
    of x1 and square of x2 are not created, assuming that the degree is 2\. The default
    degree is 2.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们已将interaction_only设置为True。将interaction_only设置为True时，给定x1和x2属性时，只会创建x1*x2的交互项，而不会创建x1的平方和x2的平方，假设多项式的度数为2。默认的度数是2。
- en: '[PRE41]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Using the transform function, we transform our train, dev, and test datasets
    to include polynomial features:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用transform函数，我们将训练集、开发集和测试集转换为包含多项式特征的数据集：
- en: 'Let us proceed to build our model:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续构建我们的模型：
- en: '[PRE42]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Inside the build_model function, we instantiate the GradientBoostingRegressor
    class as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在build_model函数内部，我们按如下方式实例化GradientBoostingRegressor类：
- en: '[PRE43]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Let us look at the parameters. The first parameter is the number of models in
    the ensemble. The second parameter is verbose—when this is set to a number greater
    than 1, it prints the progress as every model, trees in this case is built. The
    next parameter is subsample, which dictates the percentage of training data that
    will be used by the models. In this case, 0.7 indicates that we will use 70 percent
    of the training dataset. The next parameter is the learning rate. It's the shrinkage
    parameter to control the contribution of each tree. Max_depth, the next parameter,
    determines the size of the tree built. The random_state parameter is the seed
    to be used by the random number generator. In order to stay consistent during
    different runs, we set this to an integer value.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些参数。第一个参数是集成模型的数量。第二个参数是verbose——当设置为大于1的数字时，它会在每个模型（在此为树）的构建过程中打印进度。下一个参数是subsample，它决定了模型将使用的训练数据百分比。在本例中，0.7表示我们将使用70%的训练数据集。下一个参数是学习率。它是一个收缩参数，用于控制每棵树的贡献。接下来的参数max_depth决定了构建树的大小。random_state参数是随机数生成器使用的种子。为了在不同的运行中保持一致性，我们将其设置为一个整数值。
- en: 'Since we have set our verbose parameter to more than 1, as we fit our model,
    we see the following results on the screen during each model iteration:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将verbose参数设置为大于1，在拟合模型时，我们会在每次模型迭代过程中看到以下结果：
- en: '![How it works…](img/B04041_08_29.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![如何运作...](img/B04041_08_29.jpg)'
- en: As you can see, the training loss reduces with each iteration. The fourth column
    is the out-of-bag improvement score. With the subsample, we had selected only
    70 percent of the dataset; the OOB score is calculated with the rest 30 percent.
    There is an improvement in loss as compared to the previous model. For example,
    in iteration 2, we have an improvement of 10.32 when compared with the model built
    in iteration 1.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，训练损失随着每次迭代而减少。第四列是袋外改进得分。在子采样中，我们仅选择了数据集的 70%；袋外得分是用剩下的 30%计算的。与前一个模型相比，损失有所改善。例如，在第二次迭代中，相较于第一次迭代构建的模型，我们有
    10.32 的改进。
- en: 'Let us proceed to check performance of the ensemble on the training data:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续检查集成模型在训练数据上的表现：
- en: '[PRE44]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![How it works…](img/B04041_08_30.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_08_30.jpg)'
- en: As you can see, our boosting ensemble has fit the training data perfectly.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的提升集成模型已经完美地拟合了训练数据。
- en: 'The model_worth function prints some more details of the model. They are as
    follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: model_worth 函数打印出模型的更多细节，具体如下：
- en: '![How it works…](img/B04041_08_31.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_08_31.jpg)'
- en: 'The score of each of the different models, which we saw in the verbose output
    is stored as an attribute in the model object, and is retrieved as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细输出中看到的每个不同模型的得分，都作为属性存储在模型对象中，可以通过以下方式检索：
- en: '[PRE45]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let us plot this in a graph:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在图表中展示这个结果：
- en: '[PRE46]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The *x* axis represents the model number and the y axis displays the training
    score. Remember that boosting is a sequential process, and every model is an improvement
    over the previous model.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '*x* 轴表示模型编号，y 轴显示训练得分。记住，提升是一个顺序过程，每个模型都是对前一个模型的改进。'
- en: '![How it works…](img/B04041_08_32.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_08_32.jpg)'
- en: As you can see in the graph, the mean square error, which is the model score
    decreases with every successive model.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，均方误差（即模型得分）随着每一个后续模型的增加而减小。
- en: 'Finally, we can also see the importance associated with each feature:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还可以看到与每个特征相关的重要性：
- en: '[PRE47]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Let us see how the features are stacked against each other.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看各个特征之间的堆叠情况。
- en: '![How it works…](img/B04041_08_33.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_08_33.jpg)'
- en: 'Gradient Boosting unifies feature selection and model building into a single
    operation. It can naturally discover the non-linear relationship between features.
    Please refer to the following paper on how Gradient boosting can be used for feature
    selection:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升将特征选择和模型构建统一为一个操作。它可以自然地发现特征之间的非线性关系。请参考以下论文，了解如何将梯度提升用于特征选择：
- en: '*Zhixiang Xu, Gao Huang, Kilian Q. Weinberger, and Alice X. Zheng. 2014\. Gradient
    boosted feature selection. In *Proceedings of the 20th ACM SIGKDD international
    conference on Knowledge discovery and data mining*(KDD ''14). ACM, New York, NY,
    USA, 522-531\.*'
  id: totrans-266
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Zhixiang Xu, Gao Huang, Kilian Q. Weinberger, 和 Alice X. Zheng. 2014\. 梯度提升特征选择。在
    *第20届 ACM SIGKDD国际知识发现与数据挖掘大会论文集*(KDD ''14)。ACM, 纽约, NY, USA, 522-531。*'
- en: 'Let us apply the dev data to the model and look at its performance:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将开发数据应用到模型中并查看其表现：
- en: '[PRE48]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![How it works…](img/B04041_08_34.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_08_34.jpg)'
- en: Finally, we look at the test set performance.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来看一下测试集上的表现。
- en: '![How it works…](img/B04041_08_35.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_08_35.jpg)'
- en: As you can see, our ensemble has performed extremely well in our test set as
    compared to the dev set.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，与开发集相比，我们的集成模型在测试集上的表现极为出色。
- en: There's more…
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'For more information about Gradient Boosting, please refer to the following
    paper:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于梯度提升的信息，请参考以下论文：
- en: '*Friedman, J. H. (2001). Greedy function approximation: a gradient boosting
    machine. Annals of Statistics, pages 1189–1232.*'
  id: totrans-275
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Friedman, J. H. (2001). 贪婪函数逼近：一种梯度提升机。统计年鉴，第1189–1232页。*'
- en: ''
  id: totrans-276
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*In this receipt we explained gradient boosting with a squared loss function.
    However Gradient Boosting should be viewed as a framework and not as a method.
    Any differentiable loss function can be used in this framework. Any learning method
    and a differentiable loss function can be chosen by users and apply it into the
    Gradient Boosting framework.*'
  id: totrans-277
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*在这份报告中，我们用平方损失函数解释了梯度提升。然而，梯度提升应该被视为一个框架，而不是一个方法。任何可微分的损失函数都可以在这个框架中使用。用户可以选择任何学习方法和可微损失函数，并将其应用于梯度提升框架。*'
- en: ''
  id: totrans-278
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Scikit Learn also provides a Gradient Boosting method for classification,
    called GradientBosstingClassifier.*'
  id: totrans-279
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Scikit Learn 还提供了一种用于分类的梯度提升方法，称为 GradientBoostingClassifier。*'
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)'
- en: See also
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*Understanding Ensemble, Bagging Method* recipe in [Chapter 8](ch08.xhtml "Chapter 8. Ensemble
    Methods"), *Model Selection and Evaluation*'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解集成方法, Bagging方法* 配方见 [第8章](ch08.xhtml "Chapter 8. Ensemble Methods")，*模型选择与评估*'
- en: '*Understanding Ensemble*, *Boosting Method AdaBoost* recipe in [Chapter 8](ch08.xhtml
    "Chapter 8. Ensemble Methods"), *Model Selection and Evaluation*'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解集成方法*，*AdaBoost增强方法* 配方见 [第8章](ch08.xhtml "Chapter 8. Ensemble Methods")，*模型选择与评估*'
- en: '*Predicting real valued numbers using regression* recipe in [Chapter 7](ch07.xhtml
    "Chapter 7. Machine Learning 2"), *Machine Learning II*'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用回归预测实值数* 配方见 [第7章](ch07.xhtml "Chapter 7. Machine Learning 2")，*机器学习 II*'
- en: '*Variable Selection using LASSO Regression* recipe in[Chapter 7](ch07.xhtml
    "Chapter 7. Machine Learning 2"), *Machine Learning II*'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用LASSO回归进行变量选择* 配方见 [第7章](ch07.xhtml "Chapter 7. Machine Learning 2")，*机器学习
    II*'
- en: '*Using cross validation iterators* recipe in [Chatper 7](ch07.xhtml "Chapter 7. Machine
    Learning 2"), *Machine Learning II*'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用交叉验证迭代器* 配方见 [第7章](ch07.xhtml "Chapter 7. Machine Learning 2")，*机器学习 II*'
