- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Designing Monitored Data Workﬂows
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计受监控的数据工作流程
- en: Logging code is a good practice that allows developers to debug faster and provide
    maintenance more effectively for applications or systems. There is no strict rule
    when inserting logs, but knowing when not to spam your monitoring or alerting
    tool while using it is excellent. Creating several logging messages unnecessarily
    will obfuscate the instance when something significant happens. That’s why it
    is crucial to understand the best practices when inserting logs into code.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日志代码是一种良好的实践，它允许开发者更快地调试并提供更有效的应用程序或系统维护。插入日志时没有严格的规则，但了解在使用时何时不要过度使用您的监控或警报工具是非常好的。不必要地创建多个日志消息会模糊重要事件发生的实例。这就是为什么理解在代码中插入日志的最佳实践至关重要。
- en: This chapter will show how to create efficient and well-formatted logs using
    Python and PySpark for data pipelines with practical examples that can be applied
    in real-world projects.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将展示如何使用 Python 和 PySpark 为数据管道创建高效且格式良好的日志，并通过实际示例说明，这些示例可以应用于现实世界项目。
- en: 'In this chapter, we have the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们有以下食谱：
- en: Inserting logs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插入日志
- en: Using log-level types
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用日志级别类型
- en: Creating standardized logs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建标准化的日志
- en: Monitoring our data ingest ﬁle size
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控我们的数据摄入文件大小
- en: Logging based on data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于数据的日志记录
- en: Retrieving SparkSession metrics
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取 SparkSession 指标
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code from this chapter in the GitHub repository at [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从本章节的 GitHub 仓库中找到代码，网址为 [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook)。
- en: Inserting logs
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 插入日志
- en: As mentioned in the introduction of this chapter, adding logging functionality
    to your applications is essential for debugging or making improvements later on.
    However, creating several log messages without necessity may generate confusion
    or even cause us to miss crucial alerts. In any case, knowing which kind of message
    to show is indispensable.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章引言中所述，将日志功能添加到您的应用程序对于调试或以后进行改进至关重要。然而，不必要地创建多个日志消息可能会引起混淆，甚至导致我们错过关键警报。无论如何，了解显示哪种类型的信息是必不可少的。
- en: This recipe will cover how to create helpful log messages using Python and when
    to insert them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将介绍如何使用 Python 创建有用的日志消息，以及何时插入它们。
- en: Getting ready
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will use only Python code. Make sure you have Python version 3.7 or above.
    You can use the following command to check it on your **command-line** **interface**
    (**CLI**):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将仅使用 Python 代码。请确保您有 Python 3.7 或更高版本。您可以使用以下命令来检查您的 **命令行界面**（**CLI**）：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The following code execution can be done on a Python shell or a Jupyter notebook.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码执行可以在 Python 壳或 Jupyter 笔记本中完成。
- en: How to do it…
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'To perform this exercise, we will make a function that reads and returns the
    first line of a CSV file using the best logging practices. Here is how we do it:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行这个练习，我们将创建一个函数，该函数使用最佳日志记录实践读取并返回 CSV 文件的第一行。以下是我们的操作方法：
- en: 'First, let’s import the libraries we will use and set the primary configuration
    for our `logging` library:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入我们将使用的库，并设置我们的 `logging` 库的主要配置：
- en: '[PRE1]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice that we passed a filename parameter to the `basicConfig` method. Our
    logs will be stored there.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们向 `basicConfig` 方法传递了一个文件名参数。我们的日志将存储在那里。
- en: 'Next, we will create a simple function to read and return a CSV file’s first
    line. Observe that `logging.info()` calls are inserted inside the functions with
    a message, as follows:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个简单的函数来读取并返回 CSV 文件的第一行。请注意，`logging.info()` 调用带有消息插入到函数内部，如下所示：
- en: '[PRE2]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, let’s call our function, passing a CSV file as an example. Here, I will
    use the `listings.csv` file, which you can find in the GitHub repository as follows:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们调用我们的函数，传递一个 CSV 文件作为示例。在这里，我将使用 `listings.csv` 文件，您可以在 GitHub 仓库中找到它，如下所示：
- en: '[PRE3]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should see the following output:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下输出：
- en: '![Figure 8.1 – gets_csv_first_line function output](img/Figure_8.01_B19453.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – gets_csv_first_line 函数输出](img/Figure_8.01_B19453.jpg)'
- en: Figure 8.1 – gets_csv_first_line function output
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – gets_csv_first_line 函数输出
- en: 'Let’s check the directory where we executed our Python script or Jupyter notebook.
    You should see a file named `our_application.log`. If you click on it, the result
    should be as follows:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查我们执行 Python 脚本或 Jupyter 笔记本所在的目录。您应该看到一个名为 `our_application.log` 的文件。如果您点击它，结果应该如下所示：
- en: '![Figure 8.2 – our_application.log content](img/Figure_8.02_B19453.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – our_application.log内容](img/Figure_8.02_B19453.jpg)'
- en: Figure 8.2 – our_application.log content
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – our_application.log内容
- en: 'As you can see, we had two different outputs: one with the function results
    (*step 3*) and another that creates a file containing the log messages (*step
    4*).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们有两个不同的输出：一个包含函数结果（*步骤3*）的输出，另一个创建包含日志消息的文件（*步骤4*）。
- en: How it works…
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Let’s start understanding how the code works by looking at the first lines:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看前几行来开始理解代码是如何工作的：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: After importing the built-in logging library, we called a method named `basicConfig()`,
    which sets the primary configuration for the subsequent calls in our function.
    The `filename` parameter indicates we want to save the logs into a file, and the
    `level` parameter sets the log level at which we want to start seeing messages.
    This will be covered in more detail in the *Using log-level types* recipe later
    in this chapter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入内置的日志库后，我们调用了一个名为`basicConfig()`的方法，它为我们的函数中的后续调用设置了主要配置。`filename`参数表示我们想要将日志保存到文件中，而`level`参数设置了我们想要开始看到消息的日志级别。这将在本章后面的“使用日志级别类型”菜谱中更详细地介绍。
- en: 'Then, we proceeded by creating our function and inserting the logging calls.
    Looking closely, we inserted the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续创建我们的函数并插入日志调用。仔细观察，我们插入了以下内容：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: These two logs are informative and track an action or inform us as we pass through
    a part of the code or module. The best practice is to keep it as clean and objective
    as possible, so the next person (or even yourself) can identify where to start
    to look to solve a problem.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个日志是信息性的，跟踪一个动作或在我们通过代码或模块的一部分时通知我们。最佳实践是尽可能保持其清洁和客观，这样下一个人（甚至你自己）就可以确定从哪里开始解决问题。
- en: 'The next log informs us of an error, as you can see here:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个日志通知我们一个错误，正如你在这里可以看到的：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The way to make the call for this error method is similar to the `.info()` ones.
    In this case, the best practice is to use only exception clauses and pass the
    error as a string function, as we did by passing the `e` variable in curly brackets.
    This way, even if we cannot see the Python traceback, we will store it in a file
    or monitoring application.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此错误方法的方式与`.info()`类似。在这种情况下，最佳实践是只使用异常子句并将错误作为字符串函数传递，就像我们通过在大括号中传递`e`变量所做的那样。这样，即使我们看不到Python跟踪信息，我们也会将其存储在文件或监控应用程序中。
- en: Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is a common practice to encapsulate the exception output in a variable, as
    in `except Exception as e`. It allows us to control how we show or get a part
    of the error message.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将异常输出封装在变量中是一种常见的做法，例如`except Exception as e`。这允许我们控制如何显示或获取错误消息的一部分。
- en: 'Since our function was executed successfully, we don’t expect to see any error
    message in our `our_application.log` file, as you can see here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的函数执行成功，我们预计在`our_application.log`文件中不会看到任何错误消息，正如你在这里可以看到的：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If we look closely at the structure of the saved log, we will notice a pattern.
    The first word on each line, `INFO`, indicates the log level; after this, we see
    the `root` word, which indicates the logging hierarchy; and finally, we get the
    message we inserted into the code.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细观察保存的日志结构，我们会注意到一个模式。每行的第一个单词`INFO`表示日志级别；之后我们看到`root`这个词，表示日志层次结构；最后，我们得到我们插入到代码中的消息。
- en: We can optimize and format our logs in many ways, but we won’t worry about this
    for now. We will cover the logging hierarchy in more detail in the *Formatting*
    *logs* recipe.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过多种方式优化和格式化我们的日志，但我们现在不会担心这个问题。我们将在“格式化日志”菜谱中更详细地介绍日志层次结构。
- en: See also
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'See more about initiating the logs in Python in the official documentation
    here: [https://docs.python.org/3/howto/logging.xhtml#logging-to-a-file](https://docs.python.org/3/howto/logging.xhtml#logging-to-a-file)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里了解更多关于在Python中初始化日志的官方文档：[https://docs.python.org/3/howto/logging.xhtml#logging-to-a-file](https://docs.python.org/3/howto/logging.xhtml#logging-to-a-file)
- en: Using log-level types
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用日志级别类型
- en: Now that we have been introduced to how and where to insert logs, let’s understand
    log types or levels. Each log level has its own degree of relevance inside any
    system. For instance, the console output does not show debug messages by default.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了如何以及在哪里插入日志，让我们了解日志类型或级别。每个日志级别在系统内部都有其自己的相关性程度。例如，默认情况下控制台输出不会显示调试信息。
- en: We already covered how to log levels using PySpark in the *Inserting formatted
    SparkSession logs to facilitate your work recipe* in [*Chapter 6*](B19453_06.xhtml#_idTextAnchor195).
    Now we will do the same using only Python. This recipe aims to show how to set
    logging levels at the beginning of your script and insert the different levels
    inside your code to create a hierarchy of priority for your logs. With this, you
    can create a structured script that allows you or your team to monitor and identify
    errors.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[*第6章*](B19453_06.xhtml#_idTextAnchor195)的*将格式化的SparkSession日志插入以方便您的工作流程*中介绍了如何使用PySpark记录日志级别。现在我们将仅使用Python来完成同样的操作。这个菜谱旨在展示如何在脚本开始时设置日志级别，并在代码中插入不同级别的日志以创建日志的优先级层次结构。有了这个，您可以创建一个结构化的脚本，允许您或您的团队能够监控和识别错误。
- en: Getting ready
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will use only Python code. Make sure you have Python version 3.7 or above.
    You can use the following command on your CLI to check your version:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将仅使用Python代码。请确保您有Python 3.7或更高版本。您可以在CLI上使用以下命令来检查您的版本：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The following code execution can be done on a Python shell or a Jupyter notebook.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码执行可以在Python shell或Jupyter笔记本上完成。
- en: How to do it…
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Let’s use the same example we had in the previous, *Inserting logs* recipe,
    and make some enhancements:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前在*插入日志*菜谱中使用的相同示例，并进行一些改进：
- en: 'Let’s start by importing the libraries and defining `basicConfig`. This time,
    we will set the log level to `DEBUG`:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入库和定义`basicConfig`开始。这次，我们将日志级别设置为`DEBUG`：
- en: '[PRE9]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, before declaring the function, we will insert a `DEBUG` log informing
    that we are about to test this script:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在声明函数之前，我们将插入一个`DEBUG`日志，告知我们即将测试此脚本：
- en: '[PRE10]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, as we saw in the *Inserting logs* recipe, we will build a function that
    reads a CSV file and returns the first line but with slight changes. Let’s insert
    a `DEBUG` message after the first line of the CSV is executed successfully, and
    a `CRITICAL` message if we enter the exception:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，正如我们在*插入日志*菜谱中所见，我们将构建一个函数，该函数读取CSV文件并返回第一行，但有一些细微的变化。让我们在CSV的第一行执行成功后插入一个`DEBUG`消息，如果遇到异常则插入一个`CRITICAL`消息：
- en: '[PRE11]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, before we make the call to the function, let’s insert a warning message
    informing that we are about to start it:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在我们调用函数之前，让我们插入一个警告消息，告知我们即将开始执行：
- en: '[PRE12]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After calling the function, you should see the following output in the `our_application.log`
    file:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在调用函数后，您应该在`our_application.log`文件中看到以下输出：
- en: '![Figure 8.3 – our_application.log updated with different log levels](img/Figure_8.03_B19453.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – our_application.log更新了不同的日志级别](img/Figure_8.03_B19453.jpg)'
- en: Figure 8.3 – our_application.log updated with different log levels
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – our_application.log更新了不同的日志级别
- en: It informs us the function was correctly executed and no error occurred.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 它告诉我们函数已正确执行且没有发生错误。
- en: 'Let’s now simulate an error. You should now see the following message inside
    the `our_application.log` file:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们模拟一个错误。您现在应该在`our_application.log`文件中看到以下消息：
- en: '![Figure 8.4 – our_application.log showing an ERROR message](img/Figure_8.04_B19453.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – our_application.log显示的错误消息](img/Figure_8.04_B19453.jpg)'
- en: Figure 8.4 – our_application.log showing an ERROR message
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – our_application.log显示的错误消息
- en: As you can see, we entered the exception, and we can see the `ERROR` and `CRITICAL`
    messages.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们遇到了异常，并且我们可以看到`ERROR`和`CRITICAL`消息。
- en: How it works…
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作…
- en: 'Although it may seem irrelevant, we have made beneficial improvements to our
    function. Each log level corresponds to a different degree of criticality relating
    to what is happening. Let’s take a look at the following figure , which shows
    the weight of each level:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能看起来无关紧要，但我们已经对我们的函数进行了有益的改进。每个日志级别都对应着不同程度的关键性，与正在发生的事情相关。让我们看看以下图表，它显示了每个级别的权重：
- en: '![Figure 8.5 – Diagram of log level weight according to criticality](img/Figure_8.05_B19453.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 根据关键性日志级别的权重图](img/Figure_8.05_B19453.jpg)'
- en: Figure 8.5 – Diagram of log level weight according to criticality
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 根据关键性日志级别的权重图
- en: Depending on where the log level is inserted, it can prevent the script from
    continuing and creating a chain of errors since we can add different error handlers
    according to the level.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 根据日志级别插入的位置，它可以防止脚本继续执行并创建错误链，因为我们可以根据级别添加不同的错误处理器。
- en: 'By default, the Python logging library is configured to show messages only
    from the `DEBUG`, as you can see here:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Python日志库配置为仅显示`DEBUG`级别的消息，正如您在这里所看到的：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The purpose of showing only **WARNING** messages and above is to avoid spamming
    the console output or a log file with unnecessary system information. In the following
    figure, you can see how Python internally organizes its log levels:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 仅显示**警告**消息及以上的目的是为了避免在控制台输出或日志文件中充斥着不必要的系统信息。在以下图中，您可以查看Python内部如何组织其日志级别：
- en: '![Figure 8.6 – Log level detailed description and when it’s best to use them
    (source: https://docs.python.org/3/howto/logging.xhtml)](img/Figure_8.06_B19453.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 日志级别详细描述及其最佳使用时机（来源：https://docs.python.org/3/howto/logging.xhtml)](img/Figure_8.06_B19453.jpg)'
- en: 'Figure 8.6 – Log level detailed description and when it’s best to use them
    (source: [https://docs.python.org/3/howto/logging.xhtml](https://docs.python.org/3/howto/logging.xhtml))'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 日志级别详细描述及其最佳使用时机（来源：[https://docs.python.org/3/howto/logging.xhtml](https://docs.python.org/3/howto/logging.xhtml)）
- en: You can use this table as a reference when setting your log messages inside
    your code. It can also be found in the official Python documentation at [https://docs.python.org/3/howto/logging.xhtml#when-to-use-logging](https://docs.python.org/3/howto/logging.xhtml#when-to-use-logging).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在设置代码中的日志消息时使用此表作为参考。它也可以在官方Python文档中找到：[https://docs.python.org/3/howto/logging.xhtml#when-to-use-logging](https://docs.python.org/3/howto/logging.xhtml#when-to-use-logging)。
- en: In this recipe, we tried to cover all the log severity levels to demonstrate
    the recommended places to insert them. Even though it may seem like simple stuff,
    knowing when each level should be used makes all the difference and brings maturity
    to your application.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们试图涵盖所有日志严重级别，以展示推荐的插入位置。尽管这看起来可能很简单，但知道何时使用每个级别可以带来很大的不同，并使您的应用程序更加成熟。
- en: There’s more…
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: Usually, each language has its structured form of logging levels. However, there
    is an *agreement* in the software engineering world about how the levels should
    be used. The following figure shows a fantastic decision diagram created by *Taco
    Jan Osinga* about the behavior of logging levels at the **Operating System** (**OS**)
    level.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，每种语言都有其结构化的日志级别形式。然而，在软件工程领域，关于如何使用这些级别存在一种*共识*。以下图显示了由*Taco Jan Osinga*创建的关于操作系统（**OS**）级别日志级别行为的出色决策图。
- en: '![Figure 8.7 – Decision diagram of log levels by Taco Jan Osinga (source: https://stackoverflow.com/users/3476764/taco-jan-osinga?tab=profile)](img/Figure_8.07_B19453.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – Taco Jan Osinga的日志级别决策图（来源：https://stackoverflow.com/users/3476764/taco-jan-osinga?tab=profile)](img/Figure_8.07_B19453.jpg)'
- en: 'Figure 8.7 – Decision diagram of log levels by Taco Jan Osinga (source: https://stackoverflow.com/users/3476764/taco-jan-osinga?tab=profile)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – Taco Jan Osinga的日志级别决策图（来源：https://stackoverflow.com/users/3476764/taco-jan-osinga?tab=profile)
- en: See also
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For more detailed information about the foundations of Python logs, refer to
    the official documentation: [https://docs.python.org/3/howto/logging.xhtml](https://docs.python.org/3/howto/logging.xhtml)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于Python日志基础的详细信息，请参阅官方文档：[https://docs.python.org/3/howto/logging.xhtml](https://docs.python.org/3/howto/logging.xhtml)
- en: Creating standardized logs
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建标准化的日志
- en: Now that we know the best practices for inserting logs and using log levels,
    we can add more relevant information to our logs to help us monitor our code.
    Information such as date and time or the module or function executed helps us
    determine where an issue occurred or where improvements are required.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了插入日志和使用日志级别的最佳实践，我们可以向日志中添加更多相关信息，以帮助我们监控代码。例如，日期和时间或执行过的模块或函数等信息，有助于我们确定问题发生的位置或需要改进的地方。
- en: Creating standardized formatting for application logs or (in our case) data
    pipeline logs makes the debugging process more manageable, and there are a variety
    of ways to do this. One way of doing it is to create `.ini` or `.conf` files that
    hold the configuration on how the logs will be formatted and applied to our wider
    Python code, for instance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 创建应用程序日志或（在我们的情况下）数据管道日志的标准格式，可以使调试过程更加易于管理，并且有各种方法可以实现这一点。其中一种方法就是创建`.ini`或`.conf`文件，这些文件包含日志的格式化和应用配置，例如。
- en: In this recipe, we will learn how to create a configuration file that will dictate
    how the logs will be formatted across the code and shown in the execution output.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将学习如何创建一个配置文件，该文件将决定日志在代码中的格式以及执行输出中的显示方式。
- en: Getting ready
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Let’s use the same code as the previous *Using log-level types* recipe, but
    with more improvements!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用与之前*使用日志级别类型*菜谱相同的代码，但进行更多改进！
- en: 'You can use the following code to follow the steps of this recipe in a new
    file or notebook, or reuse the function from the *Using log-level types* recipe.
    I prefer to make a copy so the first piece of code is left intact:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下代码在新的文件或笔记本中遵循此配方的步骤，或者重用 *使用日志级别类型* 配方中的函数。我更喜欢复制，这样第一段代码就保持不变：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How to do it…
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Here are the steps to perform this recipe:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此配方的步骤如下：
- en: To start our exercise, let’s create a file called `logging.conf`. My recommendation
    is to store it in the same location as your Python scripts. However, feel free
    to keep it somewhere else, but do remember we will need the file’s path later.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了开始我们的练习，让我们创建一个名为 `logging.conf` 的文件。我的建议是将它存储在与你的 Python 脚本相同的目录中。然而，你也可以自由地将它放在其他地方，但请记住我们稍后需要文件的路径。
- en: 'Next, paste the following code inside the `logging.conf` file and save it:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将以下代码粘贴到 `logging.conf` 文件中并保存：
- en: '[PRE15]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, insert the following `import` statements, the `config.fileConfig()` method,
    and the `logger` variable before the `gets_csv_first_line()` function, as you
    can see here:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，插入以下 `import` 语句、`config.fileConfig()` 方法以及 `logger` 变量，在 `gets_csv_first_line()`
    函数之前，如你所见：
- en: '[PRE16]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Observe that we are passing `logging.conf` as a parameter for the `config.fileConfig()`
    method. Pass the whole path if you stored it in a different directory level of
    your Python script.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们正在将 `logging.conf` 作为参数传递给 `config.fileConfig()` 方法。如果你将其存储在 Python 脚本的不同目录级别，请传递整个路径。
- en: 'Now, let’s call our function by passing a CSV file. As usual, I will use the
    `listings.csv` file for this exercise:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们通过传递一个 CSV 文件来调用我们的函数。像往常一样，我将使用 `listings.csv` 文件进行此练习：
- en: '[PRE17]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You should see the following output in your notebook cell or Python shell:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在笔记本单元格或 Python 命令行界面中看到以下输出：
- en: '![Figure 8.8 – Console output with formatted logs](img/Figure_8.08_B19453.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.8 – 带有格式化日志的控制台输出](img/Figure_8.08_B19453.jpg)'
- en: Figure 8.8 – Console output with formatted logs
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 – 带有格式化日志的控制台输出
- en: 'Then, check your directory. You should see a file named `data_ingest.log`.
    Open it, and you should see something like the following screenshot:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，检查你的目录。你应该看到一个名为 `data_ingest.log` 的文件。打开它，你应该看到以下截图类似的内容：
- en: '![Figure 8.9 – The data_ingest.log file with formatted logs](img/Figure_8.09_B19453.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.9 – 带有格式化日志的 data_ingest.log 文件](img/Figure_8.09_B19453.jpg)'
- en: Figure 8.9 – The data_ingest.log file with formatted logs
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – 带有格式化日志的 data_ingest.log 文件
- en: As you can observe, we created a standardized log format for both console and
    file output. Let’s now understand how we did it in the next section.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所观察到的，我们为控制台和文件输出创建了一个标准化的日志格式。现在，让我们在下一节中了解我们是如何做到这一点的。
- en: How it works…
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Before jumping into the code, let’s first understand what a configuration file
    is. `.conf` or `.ini` file extension, offer a useful way to create customized
    applications to interact with other applications. You can find some of them inside
    your OS in the `/etc` or `/``var` directories.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在跳入代码之前，让我们首先了解配置文件是什么。`.conf` 或 `.ini` 文件扩展名提供了一种创建自定义应用程序以与其他应用程序交互的有用方式。你可以在操作系统的
    `/etc` 或 `/var` 目录中找到一些。
- en: Our case is no different. At the beginning of our recipe, we created a configuration
    file called `logging.conf` that holds the pattern for the Python logs we will
    apply across our application.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的情况并无不同。在我们的配方开始时，我们创建了一个名为 `logging.conf` 的配置文件，该文件包含了我们将应用于整个应用程序的 Python
    日志模式。
- en: 'Now, let’s take a look inside the `logging.conf` file. Looking closely, it
    is possible to see some values inside square brackets. Let’s start with the first
    three, as you can see here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 `logging.conf` 文件内部。仔细观察，你可以在方括号内看到一些值。让我们从这里看到的第一个三个开始：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'These parameters are modular components of the Python logging library, allowing
    easy customization due to their detachment from each other. In short, they represent
    the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数是 Python 日志库的模块化组件，由于它们彼此分离，因此易于定制。简而言之，它们代表以下内容：
- en: Loggers are used by the code and expose the interface for itself. By default,
    there is a `root` logger used by Python. For new loggers, we use the `key` argument.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录器被代码使用，并暴露了自身的接口。默认情况下，Python 使用一个 `root` 日志记录器。对于新的日志记录器，我们使用 `key` 参数。
- en: 'Handlers send the logs to the configured destination. In our case, we created
    two: `fileHandler` and `consoleHandler`.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理器将日志发送到配置的目标位置。在我们的例子中，我们创建了两个：`fileHandler` 和 `consoleHandler`。
- en: Formatters create a layout for the log records.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式化器为日志记录创建布局。
- en: 'After declaring the basic parameters, we inserted two customized loggers and
    handlers, as you can observe in the following piece of code:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在声明基本参数后，我们插入了两个自定义的日志记录器和处理器，如下所示：
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Creating a customization for the `root` `Logger` is not mandatory, but here
    we wanted to change the default log level to `DEBUG` and always send it to `fileHandler`.
    For `logger_data_ingest`, we also passed `consoleHandler`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为 `root` `Logger` 创建自定义不是强制性的，但在这里我们想要将默认日志级别更改为 `DEBUG` 并始终将其发送到 `fileHandler`。对于
    `logger_data_ingest`，我们还传递了 `consoleHandler`。
- en: Speaking of handlers, they have a fundamental role here. Although they share
    the same log level and `Formatter`, they inherit different classes. The `StreamHandler`
    class catches the log records, and with `args=(sys.stdout,)` it gets all the system
    outputs for display in the console output. `FileHandler` works similarly, saving
    all the results at the `DEBUG` level and above.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 说到处理器，它们在这里起着基本的作用。尽管它们共享相同的日志级别和 `Formatter`，但它们继承不同的类。`StreamHandler` 类捕获日志记录，通过
    `args=(sys.stdout,)` 获取所有系统输出以在控制台输出中显示。`FileHandler` 的工作方式类似，将所有结果保存到 `DEBUG`
    级别及以上。
- en: Finally, `Formatter` dictates how the log will be displayed. There are many
    ways to set the format, even passing the line of the code where the log was executed.
    You can see all the possible attributes at https://docs.python.org/3/library/logging.xhtml#logrecord-attributes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`Formatter` 决定了日志的显示方式。有许多方法可以设置格式，甚至可以传递执行日志的代码行。你可以在 https://docs.python.org/3/library/logging.xhtml#logrecord-attributes
    查看所有可能的属性。
- en: The official Python documentation has an excellent diagram, shown in the following
    figure, that outlines the relationship between these modifiers and another one
    we didn’t cover here, called `Filter`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 官方 Python 文档有一个优秀的图表，如下所示，概述了这些修饰符与我们未在此处介绍的其他修饰符之间的关系，称为 `Filter`。
- en: '![Figure 8.10 – Logging flow diagram (source: https://docs.python.org/3/howto/logging.xhtml#logging-flow)](img/Figure_8.10_B19453.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.10 – 日志流程图（来源：https://docs.python.org/3/howto/logging.xhtml#logging-flow）](img/Figure_8.10_B19453.jpg)'
- en: 'Figure 8.10 – Logging flow diagram (source: https://docs.python.org/3/howto/logging.xhtml#logging-flow)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 日志流程图（来源：https://docs.python.org/3/howto/logging.xhtml#logging-flow）
- en: 'In our exercise, we created a simple logger handler called `data_ingest` along
    with the `gets_csv_first_line()` function. Now, imagine how it could be expanded
    through a whole application or system. Using a single configuration file, we can
    create several patterns with different applications for different scripts or ETL
    phases. Let’s take a look at the first lines of our code:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的练习中，我们创建了一个简单的日志处理器，名为 `data_ingest`，以及 `gets_csv_first_line()` 函数。现在，想象一下它如何在整个应用程序或系统中扩展。使用单个配置文件，我们可以为不同的脚本或
    ETL 阶段创建不同的模式。让我们看看我们代码的第一行：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`config.fileConfig()` loads the configuration file and `logging.getLogger()`
    loads the `Logger` instance to use. It will use the `root` as default if it doesn’t
    find the proper `Logger`.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`config.fileConfig()` 加载配置文件，`logging.getLogger()` 加载要使用的 `Logger` 实例。如果没有找到合适的
    `Logger`，它将使用 `root` 作为默认值。'
- en: Software engineers commonly use this best practice in a real-world application
    to avoid code redundancy and create a centralized solution.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程师通常在现实世界的应用程序中使用这种最佳实践来避免代码冗余并创建集中式解决方案。
- en: There’s more…
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: There are some other acceptable file formats with which to create log configurations.
    For example, we can use a **YAML Ain’t Markup Language** (**YAML**) file or a
    Python dictionary.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些其他可接受的文件格式可以用来创建日志配置。例如，我们可以使用 **YAML Ain’t Markup Language**（**YAML**）文件或
    Python 字典。
- en: '![Figure 8.11 – Configuration file formatting with YAML (source: https://docs.python.org/3/howto/logging.xhtml#configuring-logging)](img/Figure_8.11_B19453.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11 – 使用 YAML 格式配置文件（来源：https://docs.python.org/3/howto/logging.xhtml#configuring-logging）](img/Figure_8.11_B19453.jpg)'
- en: 'Figure 8.11 – Configuration file formatting with YAML (source: https://docs.python.org/3/howto/logging.xhtml#configuring-logging)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 使用 YAML 格式配置文件（来源：https://docs.python.org/3/howto/logging.xhtml#configuring-logging）
- en: 'If you want to know more about using the `logging.config` package to create
    improved YAML or dictionary configurations, refer to the official documentation
    here: [https://docs.python.org/3/library/logging.config.xhtml#logging-config-api](https://docs.python.org/3/library/logging.config.xhtml#logging-config-api)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于使用 `logging.config` 包来创建改进的 YAML 或字典配置的信息，请参阅以下官方文档：[https://docs.python.org/3/library/logging.config.xhtml#logging-config-api](https://docs.python.org/3/library/logging.config.xhtml#logging-config-api)
- en: See also
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'To read and understand more about how handlers work, refer to the official
    documentation here: [https://docs.python.org/3/library/logging.handlers.xhtml](https://docs.python.org/3/library/logging.handlers.xhtml)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要阅读和理解更多关于处理器如何工作的信息，请参阅以下官方文档：[https://docs.python.org/3/library/logging.handlers.xhtml](https://docs.python.org/3/library/logging.handlers.xhtml)
- en: Monitoring our data ingest ﬁle size
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控我们的数据摄入文件大小
- en: When ingesting data, we can track a few items to ensure the incoming information
    is what we expect. One of the most important of these items is the data size we
    are ingesting, which can mean file size or the size of chunks of streaming data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在摄入数据时，我们可以跟踪一些项目以确保传入的信息是我们预期的。其中最重要的项目之一是我们摄入的数据大小，这可能意味着文件大小或流数据的块大小。
- en: Logging the size of incoming data allows the creation of intelligent and efficient
    monitoring. If at some point the size of incoming data diverges from what is expected,
    we can take action to investigate and resolve the issue.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 记录摄入数据的尺寸允许创建智能和高效的监控。如果在某个时刻，摄入数据的尺寸与预期不符，我们可以采取行动进行调查和解决问题。
- en: In this recipe, we will create simple Python code that logs the size of ingested
    files, which is very valuable in data monitoring.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们将创建简单的 Python 代码来记录摄入文件的尺寸，这在数据监控中非常有价值。
- en: Getting ready
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will use only Python code. Make sure you have Python version 3.7 or above.
    You can use the following command on your CLI to check your version:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只使用 Python 代码。请确保您拥有 Python 3.7 或更高版本。您可以在您的命令行界面使用以下命令来检查您的版本：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The following code execution can be done using a Python shell or a Jupyter notebook.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码执行可以使用 Python shell 或 Jupyter notebook 完成。
- en: How to do it…
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'This exercise will create a simple Python function to read a file path and
    return its size in bytes by default. If we want to return the value in megabytes,
    we only need to pass the input parameter as `True`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习将创建一个简单的 Python 函数，用于读取文件路径并默认返回其字节数。如果我们想以兆字节为单位返回值，我们只需要将输入参数作为 `True`
    传递：
- en: 'Let’s start by importing the `os` library:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入 `os` 库开始：
- en: '[PRE22]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we declare our function that requires a file path, along with an optional
    parameter to convert the size to megabytes:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们声明我们的函数，该函数需要一个文件路径，以及一个可选参数来将大小转换为兆字节：
- en: '[PRE23]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s use `os.stat()` to retrieve information from the file:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 `os.stat()` 从文件中检索信息：
- en: '[PRE24]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Since it is optional, we can create an `if` condition to convert the `bytes`
    value to `megabytes`. If not flagged as `True`, we return the value in `bytes`,
    as you can see in the following code:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于它是可选的，我们可以创建一个 `if` 条件来将 `bytes` 值转换为 `megabytes`。如果没有标记为 `True`，我们将以 `bytes`
    的形式返回值，如下面的代码所示：
- en: '[PRE25]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, let’s call our function, passing a dataset we already used:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们调用我们的函数，传递我们已使用的数据集：
- en: '[PRE26]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For the `listings.csv` file, you should see the following output:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `listings.csv` 文件，您应该看到以下输出：
- en: '![Figure 8.12 – File size in bytes](img/Figure_8.12_B19453.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.12 – 字节单位的文件大小](img/Figure_8.12_B19453.jpg)'
- en: Figure 8.12 – File size in bytes
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 – 字节单位的文件大小
- en: 'If we execute it by passing `s_megabytes` as `True`, we will see the following
    output:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过传递 `s_megabytes` 作为 `True` 来执行它，我们将看到以下输出：
- en: '![Figure 8.13 – File size in megabytes](img/Figure_8.13_B19453.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.13 – 兆字节单位的文件大小](img/Figure_8.13_B19453.jpg)'
- en: Figure 8.13 – File size in megabytes
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13 – 兆字节单位的文件大小
- en: Feel free to test it using any file path on your machine and check whether the
    size is the same as that indicated in the console output.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以自由地使用您机器上的任何文件路径进行测试，并检查大小是否与控制台输出中指示的一致。
- en: How it works…
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: A file’s size estimation is convenient when working with data. Let’s understand
    the pieces of code we used to achieve this estimation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理数据时，文件大小的估计很方便。让我们了解我们用来实现这种估计的代码片段。
- en: 'The first operation we used was the `os.stat()` method to retrieve information
    about the file, as you can see here:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用的是 `os.stat()` 方法来检索文件信息，正如您在这里看到的：
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This method interacts directly with your OS. If we execute it in isolation,
    we will have the following output for the `listings.csv` file:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法直接与您的操作系统交互。如果我们单独执行它，对于 `listings.csv` 文件，我们将得到以下输出：
- en: '![Figure 8.14  – Attributes of the listings.csv file when using os.stat_result](img/Figure_8.14_B19453.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.14 – 使用 os.stat_result 时 listings.csv 文件的特征](img/Figure_8.14_B19453.jpg)'
- en: Figure 8.14 – Attributes of the listings.csv file when using os.stat_result
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 – 使用 os.stat_result 时 listings.csv 文件的特征
- en: 'In our case, we only need `st_size` to bring the `bytes` estimation, so we
    called it later in the `return` clause as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们只需要`st_size`来估算`bytes`，所以我们后来在`return`子句中这样调用它：
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If you want to know more about the other results shown, you can refer to the
    official documentation page here: [https://docs.python.org/3/library/stat.xhtml#stat.filemode](https://docs.python.org/3/library/stat.xhtml#stat.filemode)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于其他显示结果的信息，您可以参考以下官方文档页面：[https://docs.python.org/3/library/stat.xhtml#stat.filemode](https://docs.python.org/3/library/stat.xhtml#stat.filemode)
- en: 'Lastly, to provide the result in megabytes, we only need to do a simple conversion
    using the `st_size` value, where 1 KB is 1,024 bytes, and 1 MB is equal to 1,024
    KB. You can see the conversion formula here:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了以兆字节为单位提供结果，我们只需要使用`st_size`值进行简单的转换，其中1 KB等于1,024字节，1 MB等于1,024 KB。您可以在以下链接中查看转换公式：
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: There’s more…
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: This recipe showed how easy it is to create a Python function that retrieves
    the file size. Unfortunately, at the time of writing, there was no straightforward
    solution to perform the same thing using PySpark.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方展示了创建一个Python函数来检索文件大小是多么容易。不幸的是，在撰写本文时，没有直接的方法来使用PySpark执行相同的事情。
- en: 'Glenn Franxman, a software engineer, proposed on his GitHub a workaround solution
    using Spark internals to estimate the size of a DataFrame. You can see his code
    on his GitHub at the following link – make sure to give the proper credits if
    you do use it: [https://gist.github.com/gfranxman/4fd0719ff2618039182dd7ea1a702f8e](https://gist.github.com/gfranxman/4fd0719ff2618039182dd7ea1a702f8e)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程师Glenn Franxman在他的GitHub上提出了一种使用Spark内部功能来估算DataFrame大小的解决方案。您可以在以下链接中查看他的代码
    – 如果您确实使用了它，请确保给予适当的致谢：[https://gist.github.com/gfranxman/4fd0719ff2618039182dd7ea1a702f8e](https://gist.github.com/gfranxman/4fd0719ff2618039182dd7ea1a702f8e)
- en: 'Let’s use Glenn’s code in an example to estimate the DataFrame size and see
    how it works:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来使用Glenn的代码估算DataFrame的大小，看看它是如何工作的：
- en: '[PRE30]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To execute the preceding code, you must have a SparkSession initiated. Once
    you have this and a DataFrame, execute the code and call the `estimate_df_size()`
    function as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行前面的代码，您必须有一个已启动的SparkSession。一旦您有了这个和DataFrame，执行代码并调用`estimate_df_size()`函数，如下所示：
- en: '[PRE31]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You should see the following output in bytes, depending on which DataFrame
    you are using:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下以字节为单位的输出，具体取决于您使用的是哪个DataFrame：
- en: '![Figure 8.15 – DataFrame size in bytes](img/Figure_8.15_B19453.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图8.15 – DataFrame的字节大小](img/Figure_8.15_B19453.jpg)'
- en: Figure 8.15 – DataFrame size in bytes
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 – DataFrame的字节大小
- en: Remember that this solution will only work if you pass a DataFrame as a parameter.
    Our Python code works well for other file estimations and doesn’t have performance
    issues when estimating big files.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这个解决方案只有在您将DataFrame作为参数传递时才会工作。我们的Python代码对其他文件估算工作良好，并且在估算大文件时没有性能问题。
- en: See also
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'Unlike PySpark, Scala has a `SizeEstimator` function to return the size of
    a DataFrame. You can find more here: [https://spark.apache.org/docs/latest/api/java/index.xhtml?org/apache/spark/util/SizeEstimator.xhtml](https://spark.apache.org/docs/latest/api/java/index.xhtml?org/apache/spark/util/SizeEstimator.xhtml)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 与PySpark不同，Scala有一个`SizeEstimator`函数可以返回DataFrame的大小。您可以在以下链接中找到更多信息：[https://spark.apache.org/docs/latest/api/java/index.xhtml?org/apache/spark/util/SizeEstimator.xhtml](https://spark.apache.org/docs/latest/api/java/index.xhtml?org/apache/spark/util/SizeEstimator.xhtml)
- en: Logging based on data
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于数据的日志记录
- en: As mentioned in the *Monitoring our data ingest ﬁle size* recipe, logging our
    ingest is a good practice in the data field. There are several ways to explore
    our ingestion logs to increase the process’s reliability and our confidence in
    it. In this recipe, we will start to get into the data operations field (or **DataOps**),
    where the goal is to track the behavior of data from the source until it reaches
    its final destination.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在*监控我们的数据摄取文件大小*配方中提到的，在数据领域记录摄取是一个好的实践。有几种方法可以探索我们的摄取日志，以增加流程的可靠性和我们对它的信心。在这个配方中，我们将开始进入数据操作领域（或**DataOps**），其目标是跟踪数据从源头到最终目的地的行为。
- en: This recipe will explore other metrics we can track to create a reliable data
    pipeline.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方将探索我们可以跟踪的其他指标，以创建一个可靠的数据管道。
- en: Getting ready
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this exercise, let’s imagine we have two simple data ingests, one from
    a database and another from an API. Since this is a straightforward pipeline,
    let’s visualize it with the following diagram:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，让我们假设我们有两个简单的数据摄取，一个来自数据库，另一个来自API。由于这是一个直接的管道，让我们用以下图表来可视化它：
- en: '![Figure 8.16 – Data ingestion phases](img/Figure_8.16_B19453.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图8.16 – 数据摄取阶段](img/Figure_8.16_B19453.jpg)'
- en: Figure 8.16 – Data ingestion phases
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16 – 数据摄取阶段
- en: With this in mind, let’s explore the instances we can log to make monitoring
    efficient.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个前提下，让我们探索我们可以记录以使监控高效的实例。
- en: How to do it…
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Let’s define the essential metrics based on each layer (or step) we saw in
    the preceding diagram:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们根据前图中我们看到的每一层（或步骤）定义基本指标：
- en: '**Data sources**: Let’s start with the first layer of the ingestion—the sources.
    Knowing we are handling two different data sources, we must create additional
    metrics for them. See the following figure for reference:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据源**：让我们从摄取的第一层——源开始。我们知道我们正在处理两个不同的数据源，因此我们必须为它们创建额外的指标。参见以下图示以供参考：'
- en: '![Figure 8.17 – Database metrics to monitor](img/Figure_8.17_B19453.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.17 – 要监控的数据库指标](img/Figure_8.17_B19453.jpg)'
- en: Figure 8.17 – Database metrics to monitor
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.17 – 要监控的数据库指标
- en: '**Ingestion**: Now that the source is logged and monitored, let’s move on to
    the ingestion layer. As we saw previously in this chapter, we can log information
    such as errors, informative parts of the code execution, file size, and so on.
    Let’s insert more content here, such as the schema and the time taken to retrieve
    or process data. We will end up with a diagram similar to the following:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**摄取**：现在，既然源已经进行了日志记录和监控，让我们继续到摄取层。正如我们在本章前面所看到的，我们可以记录诸如错误、代码执行的信息部分、文件大小等信息。让我们在这里插入更多内容，例如模式以及检索或处理数据所需的时间。我们将得到一个类似于以下图表的图表：'
- en: '![Figure 8.18 – Data ingestion metrics to monitor](img/Figure_8.18_B19453.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.18 – 要监控的数据摄取指标](img/Figure_8.18_B19453.jpg)'
- en: Figure 8.18 – Data ingestion metrics to monitor
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.18 – 要监控的数据摄取指标
- en: '**Staging layer**: Lastly, let’s cover the final layer after ingestion. The
    goal here is to ensure we maintain the integrity of the data, so verifying whether
    the schema still matches the data is crucial. We can also add logs to monitor
    the number of Parquet files and their sizes. See the following figure for reference:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**暂存层**：最后，让我们讨论摄取后的最后一层。目标是确保我们保持数据的完整性，因此验证模式是否仍然与数据匹配是至关重要的。我们还可以添加日志来监控
    Parquet 文件的数量和大小。参见以下图示以供参考：'
- en: '![Figure 8.19 – Staging layer metrics to monitor](img/Figure_8.19_B19453.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.19 – 暂存层指标要监控](img/Figure_8.19_B19453.jpg)'
- en: Figure 8.19 – Staging layer metrics to monitor
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.19 – 暂存层指标要监控
- en: Now that we have covered the essential topics to be monitored, let’s understand
    why they were chosen.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了需要监控的基本主题，让我们了解为什么选择了它们。
- en: How it works…
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Since the first recipe of this chapter, we have perpetually reinforced how logs
    are relevant to getting your system to work correctly. Here, we put it all together
    to see, albeit from a high level, how storing specific information can help us
    with monitoring.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 自从本章的第一个配方以来，我们一直在强调日志对于使系统正确工作的重要性。在这里，我们将所有这些内容综合起来，尽管是从高层次来看，但我们可以看到存储特定信息如何帮助我们进行监控。
- en: Starting with the data source layer, the metrics chosen were based on the response
    of the data and the availability to retrieve data. Understanding whether we can
    begin the ingestion process is fundamental, and even more important is knowing
    whether the data size is what we expect.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据源层开始，选择的指标基于数据的响应和检索数据的可用性。了解我们是否可以开始摄取过程是基本的，更重要的是知道数据大小是否符合我们的预期。
- en: 'Imagine the following scenario: every day, we ingest 50 MB of data from an
    API. One day, however, we received 10 KB. With proper logging and monitoring functionalities,
    we can quickly review the issue in terms of historic events. We can expand the
    data size check to the subsequent layers we covered in the recipe.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 想象以下场景：我们每天从 API 中摄取 50 MB 的数据。然而，有一天我们只收到了 10 KB。通过适当的日志记录和监控功能，我们可以快速回顾历史事件中的问题。我们可以将数据大小检查扩展到我们在配方中覆盖的后续层。
- en: Note
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We purposely intercalate the words “step” and “layer” when referring to the
    phases of the ingestion process since it can vary in different works of literature
    and in different companies’ internal processes.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们故意交替使用“步骤”和“层”这两个词来指代摄取过程的各个阶段，因为在不同的文献和不同公司的内部流程中可能会有所不同。
- en: Another way to log and monitor our data is by using schema validation. **Schema
    validation** (when applicable) guarantees that nothing has changed at the source.
    Therefore, the results for transformation or aggregation tend to be linear. We
    can also implement an auxiliary function or job to check that fields containing
    **sensitive** or **Personally Identifiable Information** (**PII**) are adequately
    anonymized.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种记录和监控我们的数据的方式是使用模式验证。**模式验证**（当适用时）保证源数据没有发生变化。因此，转换或聚合的结果往往呈线性。我们还可以实现一个辅助函数或作业来检查包含**敏感**或**个人身份信息**（**PII**）的字段是否得到了充分的匿名化。
- en: Monitoring the *parquet file size or count* is crucial to verify that quality
    is being maintained. As seen in [*Chapter 7*](B19453_07.xhtml#_idTextAnchor227),
    the number of parquet files can interfere with other applications’ reading quality
    or even the ETL’s subsequent phases.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 监控*parquet文件大小或数量*对于验证质量是否得到保持至关重要。如[第7章](B19453_07.xhtml#_idTextAnchor227)中所示，parquet文件的数量可能会影响其他应用程序的读取质量，甚至影响ETL的后续阶段。
- en: Finally, it is essential to point out that we covered logs here to ensure the
    quality and reliability of our data ingestion. Remember that the best practice
    is to align the records we got from the code with the examples we saw here.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，必须指出，我们在这里讨论日志是为了确保数据摄取的质量和可靠性。请记住，最佳实践是将我们从代码中获取的记录与这里看到的示例对齐。
- en: There’s more…
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: The content of this recipe is part of a more extensive subject called **data
    observability**. Data observability is a union of data operations, quality, and
    governance. The objective is to centralize everything to make the management and
    monitoring of data processes efficient and reliable, bringing health to data.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方的内容是更广泛主题的一部分，称为**数据可观察性**。数据可观察性是数据操作、质量和治理的结合。目标是集中一切，使数据过程的管理和监控高效且可靠，为数据带来健康。
- en: We will discuss this further in [*Chapter 12*](B19453_12.xhtml#_idTextAnchor433).
    However, if you are curious about the topic, Databand (an IBM company) has a good
    introduction at [https://databand.ai/data-observability/](https://databand.ai/data-observability/).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第12章](B19453_12.xhtml#_idTextAnchor433)中进一步讨论这个问题。然而，如果你对这个主题感兴趣，Databand（一家IBM公司）有一个很好的介绍，链接如下：[https://databand.ai/data-observability/](https://databand.ai/data-observability/)
- en: See also
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Find out more about monitoring ETL pipelines at the DataGaps blog page, here:
    [https://www.datagaps.com/blog/monitoring-your-etl-test-data-pipelines-in-production-dataops-suite/](https://www.datagaps.com/blog/monitoring-your-etl-test-data-pipelines-in-production-dataops-suite/)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在DataGaps博客页面上了解更多关于监控ETL管道的信息，链接如下：[https://www.datagaps.com/blog/monitoring-your-etl-test-data-pipelines-in-production-dataops-suite/](https://www.datagaps.com/blog/monitoring-your-etl-test-data-pipelines-in-production-dataops-suite/)
- en: Retrieving SparkSession metrics
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取SparkSession指标
- en: Until now, we created our logs to provide more information and be more useful
    for monitoring. Logging allows us to build customized metrics based on the necessity
    of our pipeline and code. However, we can also take advantage of built-in metrics
    from frameworks and programming languages.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们创建日志是为了提供更多信息，使其在监控方面更有用。日志使我们能够根据管道和代码的需求构建自定义指标。然而，我们也可以利用框架和编程语言内置的指标。
- en: When we create a `SparkSession`, it provides a web UI with useful metrics that
    can be used to monitor our pipelines. Using this, the following recipe shows you
    how to access and retrieve metric information from SparkSession, and use it as
    a tool when ingesting or processing a DataFrame.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建一个`SparkSession`时，它提供了一个带有有用指标的web UI，这些指标可以用来监控我们的管道。使用这个UI，以下配方展示了如何访问和检索SparkSession的指标信息，并将其用作摄取或处理DataFrame的工具。
- en: Getting ready
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You can execute this recipe using the PySpark command line or the Jupyter Notebook.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用PySpark命令行或Jupyter Notebook执行此配方。
- en: 'Before exploring the Spark UI metrics, let’s create a simple `SparkSession`
    using the following code:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索Spark UI指标之前，让我们使用以下代码创建一个简单的`SparkSession`：
- en: '[PRE32]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, let’s read a JSON file and call the `.show()` method as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们读取一个JSON文件，并按如下方式调用`.show()`方法：
- en: '[PRE33]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: I am using a dataset called `github_events.json`, which we worked with previously
    in [*Chapter 4*](B19453_04.xhtml#_idTextAnchor127). However, feel free to use
    whatever you prefer, since the objective here is not to observe the schema of
    the dataset, but to see what we can find out from the Spark UI.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在使用一个名为`github_events.json`的数据集，我们之前在[第4章](B19453_04.xhtml#_idTextAnchor127)中处理过。然而，请随意使用你喜欢的任何数据集，因为这里的目的是观察数据集的模式，而不是从Spark
    UI中找出我们能了解什么。
- en: How to do it…
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'With the `SparkSession` initiated as outlined in the *Getting ready* section,
    we can use the `spark` command to retrieve a link to the Spark UI, as follows:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如同在*准备就绪*部分概述的那样，我们使用`spark`命令检索Spark UI的链接，如下所示：
- en: '[PRE34]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You should see the following output:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到以下输出：
- en: '![Figure 8.20 – spark command output](img/Figure_8.20_B19453.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图8.20 – spark命令输出](img/Figure_8.20_B19453.jpg)'
- en: Figure 8.20 – spark command output
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20 – spark命令输出
- en: 'Click on **Spark UI**, and your browser will open a new tab. You should see
    a page like this:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**Spark UI**，你的浏览器将打开一个新标签页。你应该会看到一个像这样的页面：
- en: '![Figure 8.21 – Spark UI: Jobs page view](img/Figure_8.21_B19453.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图8.21 – Spark UI：作业页面视图](img/Figure_8.21_B19453.jpg)'
- en: 'Figure 8.21 – Spark UI: Jobs page view'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.21 – Spark UI：作业页面视图
- en: If you are not using the Jupyter Notebook, you can access this interface by
    pointing your browser to [http://localhost:4040/](http://localhost:4040/).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有使用Jupyter Notebook，你可以通过将浏览器指向[http://localhost:4040/](http://localhost:4040/)来访问这个界面。
- en: My page looks more crowded because I expanded **Event Timeline** and **Completed
    Jobs** – you can do the same by clicking on them.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我的页面看起来更拥挤，因为我展开了**事件时间线**和**完成的作业** – 你可以通过点击它们来做到同样的效果。
- en: 'Next, let’s explore the first completed job further. Click on **showString
    at NativeMethodAccessorImpl.java:0** and you should see the following page:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们进一步探索第一个完成的作业。点击**showString at NativeMethodAccessorImpl.java:0**，你应该会看到一个以下页面：
- en: '![Figure 8.22 – Spark UI: Stage page view for a specific job](img/Figure_8.22_B19453.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图8.22 – Spark UI：特定作业的阶段页面视图](img/Figure_8.22_B19453.jpg)'
- en: 'Figure 8.22 – Spark UI: Stage page view for a specific job'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.22 – Spark UI：特定作业的阶段页面视图
- en: Here, we can see the task status of this job in more detail, covering things
    such as how much memory it used, the time taken to execute it, and so on.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以更详细地看到这个作业的任务状态，包括它使用了多少内存、执行它所需的时间等等。
- en: Note also that it switched to the **Stages** tab at the top menu.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，它切换到了顶部菜单的**阶段**标签。
- en: 'Now, click on the **Executors** button at the top of the page. You should see
    a page similar to this:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，点击页面顶部的**Executors**按钮。你应该会看到一个类似于这样的页面：
- en: '![Figure 8.23 – Spark UI: Executors page view](img/Figure_8.23_B19453.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图8.23 – Spark UI：执行器页面视图](img/Figure_8.23_B19453.jpg)'
- en: 'Figure 8.23 – Spark UI: Executors page view'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.23 – Spark UI：执行器页面视图
- en: All the metrics here are related to the Spark drivers and nodes.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这里所有的指标都与Spark驱动器和节点相关。
- en: 'Then, click on the **SQL** button in the top menu. You should see the following
    page:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，点击顶部菜单中的**SQL**按钮。你应该会看到以下页面：
- en: '![Figure 8.24 – Spark UI: SQL page view](img/Figure_8.24_B19453.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图8.24 – Spark UI：SQL页面视图](img/Figure_8.24_B19453.jpg)'
- en: 'Figure 8.24 – Spark UI: SQL page view'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.24 – Spark UI：SQL页面视图
- en: On this page, it is possible to see the queries executed by Spark internally.
    If we used an explicit query in our code, we would see here how it was performed
    internally.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个页面上，你可以看到Spark内部执行的查询。如果我们代码中使用了显式查询，我们就会在这里看到它是如何内部执行的。
- en: You don’t need to worry about the `.``show()` method.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要担心`.show()`方法。
- en: How it works…
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Now that we have explored Spark UI, let’s understand how each tab is organized
    and some of the steps we did with them.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了Spark UI，让我们了解每个标签是如何组织的，以及我们使用它们的步骤。
- en: 'In *step 2*, we had a first glance at the interface. This interface makes it
    possible to see an event timeline with information about when the driver was created
    and executed. Also, we can observe the jobs marked on the timeline, as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤2*中，我们首次浏览了界面。这个界面使我们能够看到包含驱动器创建和执行时间信息的事件时间线。我们还可以观察到时间线上标记的作业，如下所示：
- en: '![Figure 8.25 – Detailed view of the Event Timeline expanded menu](img/Figure_8.25_B19453.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图8.25 – 事件时间线展开菜单的详细视图](img/Figure_8.25_B19453.jpg)'
- en: Figure 8.25 – Detailed view of the Event Timeline expanded menu
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.25 – 事件时间线展开菜单的详细视图
- en: We can observe how they interact when working with bigger jobs and more complex
    parallel tasks. Unfortunately, we would need a dedicated project and several datasets
    to simulate this, but you now know where to look for future reference.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到它们在处理更大作业和更复杂的并行任务时的交互。不幸的是，我们需要一个专门的项目和几个数据集来模拟这种情况，但现在你知道了未来参考的地方。
- en: Then, we selected **showString at NativeMethodAccessorImpl.java:0**, which redirected
    us to the **Stages** page. This page offers more detailed information about Spark’s
    tasks, whether the task was successful or not.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们选择了 **NativeMethodAccessorImpl.java:0 中的 showString**，这引导我们进入 **阶段**页面。这个页面提供了关于
    Spark 任务的更详细信息，包括任务是否成功。
- en: 'An excellent metric and visualization tool is **DAG Visualization** (referring
    to directed acyclic graphs), which can be expanded and will show something like
    this:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 一个优秀的指标和可视化工具是 **DAG 可视化**（指有向无环图），它可以扩展并显示如下内容：
- en: '![Figure 8.26 – DAG Visualization of a job](img/Figure_8.26_B19453.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.26 – 作业的 DAG 可视化](img/Figure_8.26_B19453.jpg)'
- en: Figure 8.26 – DAG Visualization of a job
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.26 – 作业的 DAG 可视化
- en: This offers an excellent overview of each step performed at each stage. We can
    also consult this to understand which part was problematic in the event of an
    error based on the traceback message.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了每个阶段每个步骤执行的优秀概览。我们还可以参考这些信息来了解在出现错误时，基于跟踪消息的哪个部分存在问题。
- en: 'Since we selected a specific task (or job), it showed its stages and details.
    However, we can display all the steps executed if we go directly to **Stages**.
    Doing so, you should see something like this:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们选择了特定的任务（或作业），它显示了其阶段和详细信息。然而，如果我们直接进入 **阶段**，我们可以显示所有执行的步骤。这样做，你应该会看到如下内容：
- en: '![Figure 8.27 – Spark UI: Stages overview with all jobs executed](img/Figure_8.27_B19453.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.27 – Spark UI：所有执行作业的阶段概览](img/Figure_8.27_B19453.jpg)'
- en: 'Figure 8.27 – Spark UI: Stages overview with all jobs executed'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.27 – Spark UI：所有执行作业的阶段概览
- en: Although the description messages are not straightforward, we can get the gist
    of what each of them is doing. `Stage id 0` refers to the reading JSON function,
    and `Stage id 1` with the `showString` message refers to the `.show()` method
    call.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管描述信息并不直接，但我们仍能把握每个描述所表达的核心内容。`Stage id 0` 指的是读取 JSON 的函数，而带有 `showString`
    消息的 `Stage id 1` 则指的是 `.show()` 方法的调用。
- en: The **Executors** page shows the metrics related to the core of Spark and how
    it is performing. You can use this information to understand your cluster’s behavior
    and whether any tuning is needed. For more detailed information about each field,
    refer to the Spark official documentation at https://spark.apache.org/docs/latest/monitoring.xhtml#executor-metrics.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '**执行器**页面显示了与 Spark 核心相关的指标以及其性能表现。你可以使用这些信息来了解你的集群行为以及是否需要调整。有关每个字段的更详细信息，请参阅
    Spark 官方文档：https://spark.apache.org/docs/latest/monitoring.xhtml#executor-metrics。'
- en: 'Last but not least, we saw the **SQL** page, where it was possible to see how
    Spark internally shuffles and aggregates the data behind the scenes, like **Stages**,
    taking advantage of a more visual form of execution, as you can see in the following
    screenshot:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们看到了 **SQL** 页面，在这里可以看到 Spark 在幕后如何内部洗牌和聚合数据，就像 **阶段**一样，利用更直观的执行形式，如下面的截图所示：
- en: '![Figure 8.28 – Flow diagram of the SQL query internally executed](img/Figure_8.28_B19453.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.28 – 内部执行的 SQL 查询流程图](img/Figure_8.28_B19453.jpg)'
- en: Figure 8.28 – Flow diagram of the SQL query internally executed
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.28 – 内部执行的 SQL 查询流程图
- en: Here, we can see that the query is related to the `.show()` method. There is
    helpful information inside it, including the number of output rows, the files
    read, and their sizes.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到查询与 `.show()` 方法相关。其中包含有用的信息，包括输出行数、读取的文件及其大小。
- en: There’s more…
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Even though Spark metrics are handy, you might wonder how to use them when hosting
    your PySpark jobs on cloud providers such as AWS or Google Cloud.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Spark指标很方便，但你可能会想知道如何在云提供商（如AWS或Google Cloud）上托管PySpark作业时使用它们。
- en: '**AWS** provides a simple solution to enable Spark UI when using **AWS Glue**.
    You can find out more about it at [https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-jobs.xhtml](https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-jobs.xhtml).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS** 为在使用 **AWS Glue** 时启用 Spark UI 提供了一个简单的解决方案。你可以在此了解更多信息：[https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-jobs.xhtml](https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-jobs.xhtml)。'
- en: '**Google Data Proc** provides a web interface for its cluster, where you can
    also see metrics for **Hadoop** and **YARN**. Since Spark runs on top of YARN,
    you won’t find a link directly for Spark UI, but you can use the YARN interface
    to access it. You can find out more here: [https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces](https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '**Google Data Proc** 提供了其集群的 Web 界面，您还可以在此查看 **Hadoop** 和 **YARN** 的指标。由于 Spark
    运行在 YARN 之上，因此您不会找到直接指向 Spark UI 的链接，但您可以使用 YARN 界面来访问它。您可以在此处了解更多信息：[https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces](https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces)。'
- en: See also
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: '*Towards Data Science* has a fantastic article about Spark metrics: https://towardsdatascience.com/monitoring-of-spark-applications-3ca0c271c4e0'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '*《走向数据科学》* 上有一篇关于 Spark 指标非常棒的文章：https://towardsdatascience.com/monitoring-of-spark-applications-3ca0c271c4e0'
- en: Further reading
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: https://spark.apache.org/docs/latest/monitoring.xhtml#executor-task-metrics
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: https://spark.apache.org/docs/latest/monitoring.xhtml#executor-task-metrics
- en: '[https://developer.here.com/documentation/metrics-and-logs/user_guide/topics/spark-ui.xhtml](https://developer.here.com/documentation/metrics-and-logs/user_guide/topics/spark-ui.xhtml)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Here 开发者文档中的 Spark UI 主题](https://developer.here.com/documentation/metrics-and-logs/user_guide/topics/spark-ui.xhtml)'
- en: '[https://github.com/LucaCanali/Miscellaneous/blob/master/Spark_Notes/Spark_TaskMetrics.md](https://github.com/LucaCanali/Miscellaneous/blob/master/Spark_Notes/Spark_TaskMetrics.md)'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LucaCanali 的 Miscellaneous 仓库中的 Spark Notes/Spark_TaskMetrics.md](https://github.com/LucaCanali/Miscellaneous/blob/master/Spark_Notes/Spark_TaskMetrics.md)'
- en: '[https://docs.python.org/3/howto/logging.xhtml](https://docs.python.org/3/howto/logging.xhtml)'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 3 文档中的日志处理指南](https://docs.python.org/3/howto/logging.xhtml)'
- en: '[datadoghq.com/blog/python-logging-best-practices/](http://datadoghq.com/blog/python-logging-best-practices/)'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[datadoghq.com 博客中的 Python 日志最佳实践](http://datadoghq.com/blog/python-logging-best-practices/)'
- en: '[https://coralogix.com/blog/python-logging-best-practices-tips/](https://coralogix.com/blog/python-logging-best-practices-tips/)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 日志最佳实践和技巧](https://coralogix.com/blog/python-logging-best-practices-tips/)'
