- en: Chapter 4. Real-time Trend Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。实时趋势分析
- en: In this chapter, we will introduce you to trend analysis techniques using Storm
    and Trident. Real-time trend analysis involves identifying patterns in data streams,
    such as recognizing when the occurrence rate or count of certain events reaches
    a certain threshold. Common examples include trending topics in social media,
    such as when a specific hashtag becomes popular on Twitter or identifying trending
    search terms in a search engine. Storm originated as a project to perform real-time
    analytics on Twitter data, and it provides many of the core primitives required
    for analytical computation.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍使用Storm和Trident的趋势分析技术。实时趋势分析涉及识别数据流中的模式，例如识别特定事件的发生率或计数达到一定阈值时。常见的例子包括社交媒体中的热门话题，例如特定标签在Twitter上变得流行，或者识别搜索引擎中的热门搜索词。Storm最初是一个在Twitter数据上执行实时分析的项目，并且提供了许多用于分析计算所需的核心原语。
- en: In the previous chapters, the spout implementations used were primarily simulations
    that used static sample data or randomly generated data. In this chapter, we will
    introduce an open source spout that emits data from a queue (Apache Kafka) and
    supports all three types of the Trident spout transaction (Non-transaction, Repeat
    Transaction, and Opaque Transactional). We will also implement a simple, generic
    method to populate the Kafka queue using a popular logging framework that will
    enable you to quickly begin real-time analysis of the existing applications and
    data with little or no source code modifications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，spout实现主要是使用静态样本数据或随机生成的数据的模拟。在本章中，我们将介绍一个开源的spout，它从队列（Apache Kafka）发出数据，并支持Trident
    spout事务的所有三种类型（非事务、重复事务和不透明事务）。我们还将实现一种简单的通用方法，用于使用流行的日志框架填充Kafka队列，从而使您能够快速开始对现有应用程序和数据进行实时分析，几乎不需要进行任何源代码修改。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Logging data to Apache Kafka and streaming it to Storm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将日志数据记录到Apache Kafka并将其流式传输到Storm
- en: Streaming an existing application's log data to Storm for analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将现有应用程序的日志数据流式传输到Storm进行分析
- en: Implementing an exponentially weighted moving average Trident function
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施指数加权移动平均Trident函数
- en: Using the XMPP protocol with Storm to send alerts and notifications
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用XMPP协议与Storm发送警报和通知
- en: Use case
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用案例
- en: In our use case, we have an application or set of applications (websites, enterprise
    applications, and so on) that use the popular logback framework ([http://logback.qos.ch](http://logback.qos.ch))
    for logging structured messages to disk (access logs, errors, and so on). Currently,
    the only way to perform analytics on that data is to process the files in batches
    using something like Hadoop. The latency introduced by that process dramatically
    slows down our reaction time; patterns gleaned from the log data only emerge hours,
    sometimes days, after a particular event occurred and the opportunity to take
    responsive action has passed. It is much more desirable to be actively notified
    of patterns as they emerge, rather than after the fact.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的用例中，我们有一个应用程序或一组应用程序（网站，企业应用程序等），它们使用流行的logback框架（[http://logback.qos.ch](http://logback.qos.ch)）将结构化消息记录到磁盘（访问日志，错误等）。目前，对该数据进行分析的唯一方法是使用类似Hadoop的东西批处理处理文件。该过程引入的延迟大大减慢了我们的反应时间；从日志数据中获取的模式通常要在特定事件发生后数小时，有时甚至数天后才出现，错失了采取响应行动的机会。更希望在模式出现后立即被主动通知，而不是事后才得知。
- en: 'This use case represents a common theme and has a broad range of applications
    across many business scenarios, including the following applications:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这个用例代表了一个常见的主题，并在许多业务场景中有广泛的应用，包括以下应用：
- en: 'Application Monitoring: For example, to notify system administrators when certain
    network errors reach a certain frequency'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序监控：例如，在某些网络错误达到一定频率时通知系统管理员
- en: 'Intrusion Detection: For example, to detect suspicious activity such as an
    increase in failed login attempts'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入侵检测：例如，检测到失败的登录尝试增加等可疑活动
- en: 'Supply Chain Management: For example, to identify spikes in sales of specific
    products and adjusting just-in-time delivery accordingly'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 供应链管理：例如，识别特定产品销售量的激增，并相应调整及时交付
- en: 'Online Advertising: For example, to recognize popular trends and dynamically
    changing ad delivery'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线广告：例如，识别热门趋势和动态更改广告投放
- en: Architecture
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: 'The architecture of our application is depicted in the following diagram, and
    it will include the following components:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用程序的架构如下图所示，并将包括以下组件：
- en: '![Architecture](img/8294OS_04_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![架构](img/8294OS_04_01.jpg)'
- en: The source application
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 源应用程序
- en: The source application component is any application that uses the logback framework
    for logging arbitrary log messages. For our purposes, we will create a simple
    application that logs structured messages at certain intervals. However, as you'll
    see, any existing application that uses either the logback or slf4j frameworks
    can be substituted with a simple configuration change.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 源应用程序组件是使用logback框架记录任意日志消息的任何应用程序。对于我们的目的，我们将创建一个简单的应用程序，以在特定间隔记录结构化消息。但是，正如您将看到的，任何现有应用程序使用logback或slf4j框架都可以通过简单的配置更改来替换。
- en: The logback Kafka appender
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: logback Kafka appender
- en: The logback framework has an extension mechanism that allows you to add additional
    appenders to its configuration. A logback appender is simply a Java class that
    receives logging events and does something with them. The most commonly used appenders
    are one of several `FileAppender` subclasses that simply format and write log
    messages to a file on disk. Other appender implementations write log data to network
    sockets, relational databases, and to SMTP for e-mail notifications. For our purposes,
    we will implement an appender that writes log messages to an Apache Kafka queue.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: logback框架具有扩展机制，允许您向其配置添加附加器。logback附加器只是一个接收日志事件并对其进行处理的Java类。最常用的附加器是几个`FileAppender`子类之一，它们只是将日志消息格式化并写入磁盘上的文件。其他附加器实现将日志数据写入网络套接字、关系数据库和SMTP以进行电子邮件通知。为了我们的目的，我们将实现一个将日志消息写入Apache
    Kafka队列的附加器。
- en: Apache Kafka
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Kafka
- en: Apache Kafka ([http://kafka.apache.org](http://kafka.apache.org)) is an open
    source distributed publish-subscribe messaging system. Kafka is specifically designed
    and optimized for high-throughput, persistent real-time streams. Like Storm, Kafka
    is designed to scale horizontally on commodity software to support hundreds of
    thousands of messages per second.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka ([http://kafka.apache.org](http://kafka.apache.org)) 是一个开源的分布式发布-订阅消息系统。Kafka专门设计和优化用于高吞吐量、持久的实时流。与Storm一样，Kafka被设计为在通用软件上水平扩展，以支持每秒数十万条消息。
- en: Kafka spout
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka spout
- en: The Kafka spout reads data from a Kafka queue and emits it to a Storm or Trident
    topology. The Kafka spout was originally authored by Nathan Marz, and it is still
    a part of the storm-contrib project on GitHub ([https://github.com/nathanmarz/storm-contrib](https://github.com/nathanmarz/storm-contrib)).
    Prebuilt binaries of the Kafka spout are available from the `clojars.org` Maven
    repository ([https://clojars.org/storm/storm-kafka](https://clojars.org/storm/storm-kafka)).
    We will use the Kafka spout to read messages from the Kafka queue and stream them
    into our topology.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka spout从Kafka队列中读取数据并将其发射到Storm或Trident拓扑。Kafka spout最初由Nathan Marz编写，并且仍然是GitHub上storm-contrib项目的一部分
    ([https://github.com/nathanmarz/storm-contrib](https://github.com/nathanmarz/storm-contrib))。Kafka
    spout的预构建二进制文件可从`clojars.org` Maven存储库 ([https://clojars.org/storm/storm-kafka](https://clojars.org/storm/storm-kafka))
    获取。我们将使用Kafka spout从Kafka队列中读取消息并将其流入我们的拓扑。
- en: Our topology will consist of a collection of both built-in and custom Trident
    components (functions, filters, state, and so on) that detect patterns in the
    source data stream. When a pattern is detected, the topology will emit a tuple
    to a function that will send an XMPP message to an XMPP server to notify end users
    via an **instant message** (**IM**).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的拓扑将由一系列内置和自定义的Trident组件（函数、过滤器、状态等）组成，用于检测源数据流中的模式。当检测到模式时，拓扑将向一个函数发出元组，该函数将向XMPP服务器发送XMPP消息，以通过**即时消息**
    (**IM**) 通知最终用户。
- en: The XMPP server
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XMPP服务器
- en: '**Extensible Messaging and Presence Protocol** (**XMPP**) ([http://xmpp.org](http://xmpp.org))
    is an XML-based standard for instant messaging, presence information, and contact
    list maintenance. Many IM clients such as Adium (for OSX) ([http://adium.im](http://adium.im))
    and Pidgin (for OSX, Linus, and Windows) ([http://www.pidgin.im](http://www.pidgin.im))
    support the XMPP protocol, and if you have ever used Google Talk for instant messaging,
    you have used XMPP.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**可扩展消息和出席协议** (**XMPP**) ([http://xmpp.org](http://xmpp.org)) 是一种基于XML的即时消息、出席信息和联系人列表维护的标准。许多即时消息客户端，如Adium（用于OSX）([http://adium.im](http://adium.im))和Pidgin（用于OSX、Linus和Windows）([http://www.pidgin.im](http://www.pidgin.im))支持XMPP协议，如果您曾经使用过Google
    Talk进行即时消息传递，那么您已经使用过XMPP。'
- en: We will use the open source OpenFire XMPP server ([http://www.igniterealtime.org/projects/openfire/](http://www.igniterealtime.org/projects/openfire/))
    for its ease of setup and compatibility with OSX, Linux, and Windows.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用开源的OpenFire XMPP服务器 ([http://www.igniterealtime.org/projects/openfire/](http://www.igniterealtime.org/projects/openfire/))，因为它易于设置并且与OSX、Linux和Windows兼容。
- en: Installing the required software
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装所需软件
- en: 'We''ll begin by installing the necessary software: Apache Kafka and OpenFire.
    Although Kafka is a distributed messaging system, it will work just fine installed
    as a single node, or even locally as part of a development environment. In a production
    environment, you will need to set up a cluster of one or more machines depending
    on your scaling requirements. The OpenFire server is not a clustered system and
    can be installed on a single node or locally.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先安装必要的软件：Apache Kafka和OpenFire。虽然Kafka是一个分布式消息系统，但作为单节点安装，甚至作为开发环境的一部分本地安装都可以正常工作。在生产环境中，您需要根据扩展需求设置一个或多个机器的集群。OpenFire服务器不是一个集群系统，可以安装在单个节点或本地。
- en: Installing Kafka
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Kafka
- en: Kafka depends on ZooKeeper for storing certain state information, much like
    Storm. Since Storm imposes a relatively light load on ZooKeeper, in many cases
    it is acceptable to share the same ZooKeeper cluster between both Kafka and Storm.
    Since we've already covered ZooKeeper installation in [Chapter 2](ch02.html "Chapter 2. Configuring
    Storm Clusters"), *Configuring Storm Clusters*, here we'll just cover the running
    of the local ZooKeeper server that ships with Kafka and is suitable for a development
    environment.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka依赖于ZooKeeper来存储某些状态信息，就像Storm一样。由于Storm对ZooKeeper的负载相对较轻，在许多情况下可以接受在Kafka和Storm之间共享相同的ZooKeeper集群。由于我们已经在[第2章](ch02.html
    "第2章。配置Storm集群")中介绍了ZooKeeper的安装，*配置Storm集群*，这里我们只介绍与Kafka一起提供的本地ZooKeeper服务器的运行，适用于开发环境。
- en: 'Begin by downloading the 0.7.x release of Apache Kafka from the following website:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先从以下网站下载Apache Kafka的0.7.x版本：
- en: '[http://kafka.apache.org/downloads.html](http://kafka.apache.org/downloads.html)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://kafka.apache.org/downloads.html](http://kafka.apache.org/downloads.html)'
- en: 'Next, unpack the source distribution and change the existing directory to the
    following directory:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，解压源分发并将现有目录更改为以下目录：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Kafka is written in the Scala JVM language ([http://www.scala-lang.org](http://www.scala-lang.org))
    and uses `sbt` (**Scala Build Tool**) ([http://www.scala-sbt.org](http://www.scala-sbt.org))
    for compiling and packaging. Fortunately, the Kafka source distribution includes
    `sbt` and can be built with the following command:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka是用Scala JVM语言（[http://www.scala-lang.org](http://www.scala-lang.org)）编写的，并使用`sbt`（**Scala
    Build Tool**）（[http://www.scala-sbt.org](http://www.scala-sbt.org)）进行编译和打包。幸运的是，Kafka源代码分发包括`sbt`，可以使用以下命令构建：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Before starting Kafka, unless you already have a ZooKeeper service running,
    you will need to start the ZooKeeper service bundled with Kafka using the following
    command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动Kafka之前，除非您已经运行了ZooKeeper服务，否则您需要使用以下命令启动与Kafka捆绑的ZooKeeper服务：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, in a separate terminal window, start the Kafka service with the following
    command:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在一个单独的终端窗口中，使用以下命令启动Kafka服务：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The Kafka service is now ready to use.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka服务现在可以使用了。
- en: Installing OpenFire
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装OpenFire
- en: 'OpenFire is available as an installer for OSX and Windows as well as a package
    for various Linux distributions, and it can be downloaded from the following website:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFire可作为OSX和Windows的安装程序以及各种Linux发行版的软件包提供，并且可以从以下网站下载：
- en: '[http://www.igniterealtime.org/downloads/index.jsp](http://www.igniterealtime.org/downloads/index.jsp)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.igniterealtime.org/downloads/index.jsp](http://www.igniterealtime.org/downloads/index.jsp)'
- en: 'To install OpenFire, download the installer for your operating system and follow
    the appropriate installation instructions that can be found at the following website:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装OpenFire，请下载适用于您操作系统的安装程序，并按照以下网站上找到的适当安装说明进行操作：
- en: '[http://www.igniterealtime.org/builds/openfire/docs/latest/documentation/index.html](http://www.igniterealtime.org/builds/openfire/docs/latest/documentation/index.html)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.igniterealtime.org/builds/openfire/docs/latest/documentation/index.html](http://www.igniterealtime.org/builds/openfire/docs/latest/documentation/index.html)'
- en: Introducing the sample application
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍示例应用程序
- en: 'The application component is a simple Java class that uses the **Simple Logging
    Facade for Java** (**SLF4J**) ([http://www.slf4j.org](http://www.slf4j.org)) to
    log messages. We will simulate an application that begins by generating warning
    messages at a relatively slow rate, then switches to a state where it generates
    warning messages at a much faster rate, and finally returns to the slow state
    as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 应用组件是一个简单的Java类，使用**Simple Logging Facade for Java**（**SLF4J**）（[http://www.slf4j.org](http://www.slf4j.org)）记录消息。我们将模拟一个应用程序，开始以相对较慢的速率生成警告消息，然后切换到以更快的速率生成警告消息的状态，最后返回到慢速状态，如下所示：
- en: Log a warning message every 5 seconds for 30 seconds (slow state)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每5秒记录一次警告消息，持续30秒（慢速状态）
- en: Log a warning message every second for 15 seconds (rapid state)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每秒记录一次警告消息，持续15秒（快速状态）
- en: Log a warning message every 5 seconds for 30 seconds (slow state)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每5秒记录一次警告消息，持续30秒（慢速状态）
- en: 'The goal of the application is to generate a simple pattern that our storm
    topology can recognize and react to by sending notifications when certain patterns
    emerge and state changes occur as shown in the following code snippet:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序的目标是生成一个简单的模式，我们的风暴拓扑可以识别并在出现特定模式和状态变化时发送通知，如下面的代码片段所示：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Sending log messages to Kafka
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将日志消息发送到Kafka
- en: The logback framework provides a simple extension mechanism that allows you
    to plug in additional appenders. In our case, we want to implement an appender
    that can write log message data to Kafka.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: logback框架提供了一个简单的扩展机制，允许您插入附加的附加器。在我们的情况下，我们想要实现一个可以将日志消息数据写入Kafka的附加器。
- en: 'Logback includes the `ch.qos.logback.core.AppenderBase` abstract class that
    makes it easy to implement the `Appender` interface. The `AppenderBase` class
    defines a single abstract method as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Logback包括`ch.qos.logback.core.AppenderBase`抽象类，使得实现`Appender`接口变得容易。`AppenderBase`类定义了一个抽象方法如下：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `eventObject` parameter represents a logging event and includes properties
    such as the date of the event, the log level (`DEBUG`, `INFO`, `WARN`, and so
    on), as well as the log message itself. We will override the `append()` method
    to write the `eventObject` data to Kafka.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`eventObject`参数表示日志事件，并包括事件日期、日志级别（`DEBUG`、`INFO`、`WARN`等）以及日志消息本身等属性。我们将重写`append()`方法，将`eventObject`数据写入Kafka。'
- en: 'In addition to the `append()` method, the `AppenderBase` class defines two
    additional lifecycle methods that we will need to override:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`append()`方法之外，`AppenderBase`类还定义了两个我们需要重写的附加生命周期方法：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `start()` method is called during the initialization of the logback framework,
    and the `stop()` method is called upon deinitialization. We will override these
    methods to set up and tear down our connection to the Kafka service.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`start()`方法在logback框架初始化期间调用，`stop()`方法在去初始化时调用。我们将重写这些方法来建立和拆除与Kafka服务的连接。'
- en: 'The source code for the `KafkaAppender` class is listed as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`KafkaAppender`类的源代码如下所示：'
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you will see, the JavaBean-style accessors in this class allow us to configure
    the associated values via dependency injection at runtime when the logback framework
    initializes. The setters and getters for the `zookeeperHosts` property are used
    to initialize the `KafkaProducer` client, configuring it to discover Kafka hosts
    that have registered with ZooKeeper. An alternative method would be to supply
    a static list of Kafka hosts, but for simplicity's sake it is easier to use an
    auto-discovery mechanism. The `topic` property is used to tell the `KafkaConsumer`
    client from which Kafka topic it should read.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将看到的，这个类中的JavaBean风格的访问器允许我们在logback框架初始化时通过依赖注入配置相关值。`zookeeperHosts`属性的setter和getter用于初始化`KafkaProducer`客户端，配置它以发现已在ZooKeeper注册的Kafka主机。另一种方法是提供一个静态的Kafka主机列表，但为了简单起见，使用自动发现机制更容易。`topic`属性用于告诉`KafkaConsumer`客户端应该从哪个Kafka主题读取。
- en: 'The `Formatter` property is somewhat special. It is an interface we''ve defined
    that provides an extension point for handling structured (that is, parseable)
    log messages as shown in the following code snippet:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`Formatter`属性有些特殊。这是一个我们定义的接口，提供了处理结构化（即可解析的）日志消息的扩展点，如下面的代码片段所示：'
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'A `Formatter` implementation''s job is to take an `ILoggingEvent` object and
    turn it into a machine-readable string that can be processed by a consumer. A
    simple implementation listed in the following code snippet simply returns the
    log message, discarding any additional metadata:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`Formatter`实现的工作是将`ILoggingEvent`对象转换为可被消费者处理的机器可读字符串。下面的简单实现只是返回日志消息，丢弃任何额外的元数据：'
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following logback configuration file illustrates the usage of the appender.
    This example does not define a custom `Formatter` implementation, so the `KafkaAppender`
    class will default to using the `MessageFormatter` class and just write the log
    message data to Kafka and discard any additional information contained in the
    logging event, as shown in the following code snippet:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的logback配置文件展示了appender的使用。这个例子没有定义自定义的`Formatter`实现，所以`KafkaAppender`类将默认使用`MessageFormatter`类，只会将日志消息数据写入Kafka并丢弃日志事件中包含的任何额外信息，如下面的代码片段所示：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The Storm application we''re building is time sensitive: if we''re tracking
    the rate at which each event occurs, we need to know exactly when an event occurs.
    A naïve approach would be to simply assign the event a time using the `System.currentTimeMillis()`
    method when the data enters our topology. However, Trident''s batching mechanism
    doesn''t guarantee that tuples will be delivered to a topology at the same rate
    with which they were received.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在构建的Storm应用程序是时间敏感的：如果我们正在跟踪每个事件发生的速率，我们需要准确知道事件发生的时间。一个天真的方法是当数据进入我们的拓扑时，简单地使用`System.currentTimeMillis()`方法为事件分配一个时间。然而，Trident的批处理机制不能保证元组以与接收到的速率相同的速率传递到拓扑。
- en: In order to account for this situation, we need to capture the time of the event
    when it occurs and include it in the data when we write to the Kafka queue. Fortunately,
    the `ILoggingEvent` class includes a timestamp, in milliseconds since the epoch,
    that the event occurred.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这种情况，我们需要在事件发生时捕获事件的时间并在写入Kafka队列时包含在数据中。幸运的是，`ILoggingEvent`类包括一个时间戳，表示事件发生时距离纪元的毫秒数。
- en: 'To include the metadata included in `ILoggingEvent`, we''ll create a custom
    `Formatter` implementation that encodes the log event data in JSON format as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了包含`ILoggingEvent`中包含的元数据，我们将创建一个自定义的`Formatter`实现，将日志事件数据编码为JSON格式，如下所示：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The bulk of the `JsonMessageFormatter` class code uses a `java.lang.StringBuilder`
    class to create JSON from the `ILoggingEvent` object. While we could have used
    a JSON library to do the work, the JSON data we're generating is simple and adding
    an additional dependency just to generate JSON would be overkill.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`JsonMessageFormatter`类的大部分代码使用`java.lang.StringBuilder`类从`ILoggingEvent`对象创建JSON。虽然我们可以使用JSON库来完成工作，但我们生成的JSON数据很简单，添加额外的依赖只是为了生成JSON会显得过度。'
- en: The one JavaBean property exposed by `JsonMessageFormatter` is the `expectJson`
    Boolean used to specify whether the log message passed to the `Formatter` implementation
    should be treated as JSON. If set to `False`, the log message will be treated
    as a string and wrapped in double quotes, otherwise the message will be treated
    as a JSON object (`{...}`) or array (`[...]`).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`JsonMessageFormatter`公开的一个JavaBean属性是`expectJson`布尔值，用于指定传递给`Formatter`实现的日志消息是否应被视为JSON。如果设置为`False`，日志消息将被视为字符串并用双引号括起来，否则消息将被视为JSON对象（`{...}`）或数组（`[...]`）。'
- en: 'The following is a sample logback configuration file that illustrates the usage
    of the `KafkaAppender` and `JsonFormatter` classes:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例的logback配置文件，展示了`KafkaAppender`和`JsonFormatter`类的使用：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Since the analytics topology we are building is more concerned with event timing
    than message content, the log messages we generate will be strings, so we set
    the `expectJson` property to `False`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在构建的分析拓扑更关注事件时间而不是消息内容，我们生成的日志消息将是字符串，因此我们将`expectJson`属性设置为`False`。
- en: Introducing the log analysis topology
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍日志分析拓扑
- en: 'With the means to write our log data to Kafka, we''re ready to turn our attention
    to the implementation of a Trident topology to perform the analytical computation.
    The topology will perform the following operations:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有了将日志数据写入Kafka的手段，我们准备将注意力转向实现一个Trident拓扑来执行分析计算。拓扑将执行以下操作：
- en: Receive and parse the raw JSON log event data.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接收并解析原始JSON日志事件数据。
- en: Extract and emit necessary fields.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取并发出必要的字段。
- en: Update an exponentially-weighted moving average function.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新指数加权移动平均函数。
- en: Determine if the moving average has crossed a specified threshold.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定移动平均是否越过了指定的阈值。
- en: Filter out events that do not represent a state change (for example, rate moved
    above/below threshold).
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤掉不代表状态改变的事件（例如，速率移动超过/低于阈值）。
- en: Send an instant message (XMPP) notification.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发送即时消息（XMPP）通知。
- en: 'The topology is depicted in the following diagram with the Trident stream operations
    at the top and stream processing components at the bottom:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑结构如下图所示，三叉戟流操作位于顶部，流处理组件位于底部：
- en: '![Introducing the log analysis topology](img/8294OS_04_02.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![介绍日志分析拓扑](img/8294OS_04_02.jpg)'
- en: Kafka spout
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka spout
- en: 'The first step in creating the log analysis topology is to configure the Kafka
    spout to stream data received from Kafka into our topology as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 创建日志分析拓扑的第一步是配置Kafka spout，将从Kafka接收的数据流入我们的拓扑，如下所示：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This code first creates a new `TridentTopology` instance, and then uses the
    Kafka Java API to create a list of Kafka hosts with which to connect (since we''re
    running a single, unclustered Kafka service locally, we specify a single host:
    `localhost`). Next, we create the `TridentKafkaConfig` object, passing it the
    host list and a unique identifier.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码首先创建了一个新的`TridentTopology`实例，然后使用Kafka Java API创建了一个Kafka主机列表，用于连接（因为我们在本地运行单个、非集群的Kafka服务，所以我们指定了一个主机：`localhost`）。接下来，我们创建了`TridentKafkaConfig`对象，将主机列表和唯一标识符传递给它。
- en: The data our application writes to Kafka is a simple Java string, so we use
    Storm-Kafka built-in `StringScheme` class. The `StringScheme` class will read
    data from Kafka as a string and output it in a tuple field named `str`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用程序写入Kafka的数据是一个简单的Java字符串，因此我们使用Storm-Kafka内置的`StringScheme`类。`StringScheme`类将从Kafka读取数据作为字符串，并将其输出到名为`str`的元组字段中。
- en: 'By default, upon deployment the Kafka spout will attempt to read from the Kafka
    queue where it last left off by querying ZooKeeper for state information. This
    behavior can be overridden by calling the `forceOffsetTime(long time)` method
    of the `TridentKafkaConfig` class. The time parameter can be one of the following
    three values:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，在部署时，Kafka spout将尝试从Kafka队列中上次离开的地方读取状态信息。这种行为可以通过调用`TridentKafkaConfig`类的`forceOffsetTime(long
    time)`方法来覆盖。时间参数可以是以下三个值之一：
- en: '**-2 (earliest offset)**: The spout will *rewind* and start reading from the
    beginning of the queue'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**-2（最早的偏移）**：spout将*倒带*并从队列的开头开始读取'
- en: '**-1 (latest offset)**: The spout will *fast forward* and read from the end
    of the queue'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**-1（最新的偏移）**：spout将*快进*并从队列的末尾读取'
- en: '**Time in milliseconds**: Given a specific date in milliseconds (for example,
    `java.util.Date.getTime()`), the spout will attempt to begin reading from that
    point in time'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以毫秒为单位的时间**：给定特定日期的毫秒数（例如，`java.util.Date.getTime()`），spout将尝试从那个时间点开始读取'
- en: After setting up the spout configuration, we create an instance of the *Opaque
    Transactional* Kafka spout and set up a corresponding Trident stream.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好spout配置之后，我们创建了一个*Opaque Transactional* Kafka spout的实例，并设置了相应的Trident流。
- en: The JSON project function
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JSON项目函数
- en: 'The data stream coming from the Kafka spout will contain a single field (`str`)
    containing the JSON data from the log event. We''ll create a Trident function
    to parse the incoming data and output, or project requested fields as tuple values
    as shown in the following code snippet:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Kafka spout的数据流将包含一个字段（`str`），其中包含来自日志事件的JSON数据。我们将创建一个Trident函数来解析传入的数据，并输出或投影请求的字段作为元组值，如下面的代码片段所示：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `JsonProjectFunction` constructor takes a `Fields` object parameter that
    will determine what values to emit as a list of key names to look up from the
    JSON. When the function receives a tuple, it will parse the JSON in the tuple's
    `str` field, iterate the `Fields` object's values, and emit the corresponding
    value from the input JSON.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`JsonProjectFunction`构造函数接受一个`Fields`对象参数，该参数将确定要作为要查找的键名称列表从JSON中发出的值。当函数接收到一个元组时，它将解析元组的`str`字段中的JSON，迭代`Fields`对象的值，并从输入JSON中发出相应的值。'
- en: 'The following code creates a `Fields` object with a list of field names to
    extract from the JSON. It then creates a new `Stream` object from the spout stream,
    selects the `str` tuple field as the input to the `JsonProjectFunction` constructor,
    constructs the `JsonProjectFunction` constructor, and specifies that the fields
    selected from the JSON will also be output from the function:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码创建了一个`Fields`对象，其中包含要从JSON中提取的字段名称列表。然后，它从spout流创建了一个新的`Stream`对象，选择`str`元组字段作为`JsonProjectFunction`构造函数的输入，构造了`JsonProjectFunction`构造函数，并指定从JSON中选择的字段也将从函数中输出：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Consider that the following JSON message is received from the Kafka spout:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到以下JSON消息是从Kafka spout接收到的：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This would mean that the function would output the following tuple values:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着该函数将输出以下元组值：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Calculating a moving average
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算移动平均
- en: In order to calculate the rate at which log events occur, without the need to
    store an inordinate amount of state, we will implement a function that performs
    what is known in statistics as an **exponentially weighted moving average**.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算日志事件发生的速率，而无需存储过多的状态，我们将实现一个函数，执行统计学中所谓的**指数加权移动平均**。
- en: 'A moving average calculation is often used to smooth out short-term fluctuations
    and expose long-term trends in time series data. One of the most common examples
    of a moving average is its use in graphing the fluctuation of prices in the stock
    market, as shown in the following screenshot:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 移动平均计算经常用于平滑短期波动，并暴露时间序列数据中的长期趋势。移动平均的最常见的例子之一是在股票市场价格波动的图表中使用，如下面的屏幕截图所示：
- en: '![Calculating a moving average](img/8294OS_04_03.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![计算移动平均](img/8294OS_04_03.jpg)'
- en: The smoothing effect of a moving average is achieved by taking into account
    historical values in the calculation. A moving average calculation can be performed
    with a very minimal amount of state. For a time series, we need only keep the
    time of the last event and the last calculated average.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 移动平均的平滑效果是通过在计算中考虑历史值来实现的。移动平均计算可以以非常少量的状态执行。对于时间序列，我们只需要保留上一个事件的时间和上一个计算的平均值。
- en: 'In pseudo code, the calculation would look something like the following code
    snippet:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在伪代码中，计算看起来像以下代码片段：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `alpha` value in the preceding calculation is a constant value between `0`
    and `1`. The `alpha` value determines the amount of smoothing that occurs over
    time. The closer the `alpha` value is to `1`, the more the historical values affect
    the current average. In other words, an `alpha` value closer to `0` will result
    in less smoothing and the moving average will be closer to the current value.
    An `alpha` value closer to `1` will have the opposite effect. The current average
    will be less affected by wild fluctuations and the historical values will have
    more weight in determining the current average.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 上述计算中的`alpha`值是介于`0`和`1`之间的常量值。`alpha`值确定随时间发生的平滑程度。`alpha`值越接近`1`，历史值对当前平均值的影响就越大。换句话说，`alpha`值越接近`0`，平滑效果就越小，移动平均值就越接近当前值。`alpha`值越接近`1`，效果就相反。当前平均值受到的波动影响就越小，历史值在确定当前平均值时的权重就越大。
- en: Adding a sliding window
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加一个滑动窗口
- en: In some cases, we may want to discount historical values to reduce their effects
    on the moving average, for example, to reset the smoothing effect if a large amount
    of time has passed between receiving events. In case of a low alpha value, this
    may not be necessary since the smoothing effect is minimal. In the event of a
    high alpha, however, it may be desirable to counteract the smoothing effect.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能希望打折历史值以减少它们对移动平均值的影响，例如，如果在接收事件之间经过了很长时间，我们可能希望重置平滑效果。在低alpha值的情况下，这可能是不必要的，因为平滑效果很小。然而，在高alpha值的情况下，抵消平滑效果可能是可取的。
- en: Consider the following example.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例。
- en: We have an event (such as a network error and so on) that occurs infrequently.
    Occasionally, small spikes in frequency occur, but that's usually okay. So, we
    want to smooth out the small spikes. What we want to be notified of is if a *sustained*
    spike occurs.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个（例如网络错误等）偶尔发生的事件。偶尔会出现小的频率波动，但通常没关系。因此，我们希望消除小的波动。我们希望被通知的是如果发生了*持续*的波动。
- en: If the event occurs once a week on average (well below our notification threshold),
    but one day spikes to many occurrences within an hour (above our notification
    threshold), the smoothing effect of the high alpha may negate the spike such that
    a notification is never triggered.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果事件平均每周发生一次（远低于我们的通知阈值），但有一天在一个小时内发生了多次（超过我们的通知阈值），高alpha的平滑效果可能会抵消波动，以至于永远不会触发通知。
- en: 'To counteract this effect, we can introduce the concept of a **sliding window**
    into our moving average calculation. Since we''re already keeping track of both
    the time of the last event, and the current average, implementing a sliding window
    is simple as illustrated in the following pseudo code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了抵消这种影响，我们可以在移动平均值计算中引入**滑动窗口**的概念。由于我们已经在跟踪上一个事件的时间和当前平均值，因此实现滑动窗口就像在以下伪代码中所示的那样简单：
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'An implementation of an exponentially weighted moving average is listed as
    follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 指数加权移动平均的实现如下所示：
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `EWMA` implementation defines three useful constant `alpha` values: `ONE_MINUTE_ALPHA`,
    `FIVE_MINUTE_ALPHA`, and `FIFTEEN_MINUTE_ALPHA`. These correspond to the standard
    `alpha` values used to calculate load averages in UNIX. The `alpha` value can
    also be specified manually, or as a function of an *alpha* window.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`EWMA`实现定义了三个有用的常量`alpha`值：`ONE_MINUTE_ALPHA`，`FIVE_MINUTE_ALPHA`和`FIFTEEN_MINUTE_ALPHA`。这些对应于UNIX中用于计算负载平均值的标准`alpha`值。`alpha`值也可以手动指定，或者作为*alpha*窗口的函数。'
- en: 'The implementation uses a fluent-style *builder* API. For example, you can
    create an `EWMA` instance with a sliding window of one minute and an `alpha` value
    equivalent to the UNIX one-minute interval, as shown in use the following code
    snippet:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 该实现使用流畅的*构建器*API。例如，您可以创建一个具有一分钟滑动窗口和等效于UNIX一分钟间隔的`alpha`值的`EWMA`实例，如下面的代码片段所示：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `mark()` methods are used to update the moving average. Without arguments,
    the `mark()` method will use the current time to calculate the average. Because
    we want to use the original timestamp from the log event, we overload the `mark()`
    method to allow the specification of a specific time.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`mark（）`方法用于更新移动平均值。如果没有参数，`mark（）`方法将使用当前时间来计算平均值。因为我们想要使用日志事件的原始时间戳，我们重载`mark（）`方法以允许指定特定时间。'
- en: The `getAverage()` method returns the average time between calls to `mark()`
    in milliseconds. We also added the convenient `getAverageIn()`method, which will
    return the average in the specified time unit of measure (seconds, minutes, hours,
    and so on). The `getAverageRatePer()` method returns the rate of calls to `mark()`
    in a specific time measurement.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`getAverage（）`方法以毫秒为单位返回`mark（）`调用之间的平均时间。我们还添加了方便的`getAverageIn（）`方法，它将返回指定时间单位（秒，分钟，小时等）的平均值。`getAverageRatePer（）`方法返回特定时间测量中`mark（）`调用的速率。'
- en: As you'll probably notice, using an exponentially weighted moving average can
    be somewhat tricky. Finding the right set of values for an alpha as well as the
    optional sliding window varies quite a bit depending on the specific use case,
    and finding the right value is largely a matter of trial and error.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能注意到的那样，使用指数加权移动平均值可能有些棘手。找到合适的alpha值以及可选滑动窗口的正确值在很大程度上取决于特定用例，并且找到正确的值在很大程度上是一个反复试验的问题。
- en: Implementing the moving average function
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现移动平均函数
- en: 'To use our `EWMA` class in a Trident topology, we''ll create a subclass of
    Trident''s `BaseFunction` abstract class named `MovingAverageFunction` that wraps
    an instance of `EWMA,` as shown in the following code snippet:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Trident拓扑中使用我们的`EWMA`类，我们将创建Trident的`BaseFunction`抽象类的子类，命名为`MovingAverageFunction`，它包装了一个`EWMA`实例，如下面的代码片段所示：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `MovingAverage.execute()` method gets the `Long` value of the incoming
    tuple''s first field, uses the value to call the `mark()` method to update the
    current average, and emits the current average rate. Functions in Trident are
    additive, meaning they add values to the tuples in a stream. So, for example,
    consider that the tuple coming into our function looks like the following code
    snippet:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`MovingAverage.execute()`方法获取传入元组的第一个字段的`Long`值，使用该值调用`mark()`方法来更新当前平均值，并发出当前平均速率。Trident中的函数是累加的，这意味着它们将值添加到流中的元组中。因此，例如，考虑传入我们函数的元组如下代码片段所示：'
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This means that after processing, the tuple might look like the following code
    snippet:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在处理后，元组可能看起来像下面的代码片段：
- en: '[PRE24]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, the new value represents the new average rate.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，新值代表了新的平均速率。
- en: 'To use the function, we create an instance of the `EWMA` class and pass it
    to the `MovingAverageFunction` constructor. We apply the function to the stream
    with the `each()` method, selecting the `timestamp` field as the input, as shown
    in the following code snippet:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用该函数，我们创建了一个`EWMA`类的实例，并将其传递给`MovingAverageFunction`构造函数。我们使用`each()`方法将该函数应用于流，选择`timestamp`字段作为输入，如下面的代码片段所示：
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Filtering on thresholds
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阈值过滤
- en: For our use case, we want to be able to define a rate threshold that triggers
    a notification when exceeded. We also want notifications when the average rate
    falls back below that threshold (that is, returns to normal). We can accomplish
    this functionality using a combination of an additional function and a simple
    Trident filter.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的用例，我们希望能够定义一个触发通知的速率阈值。当超过阈值时，我们还希望在平均速率再次低于该阈值时收到通知（即恢复正常）。我们可以使用额外的函数和简单的Trident过滤器的组合来实现这个功能。
- en: 'The job of the function will be to determine whether the new value of the average
    rate field crosses a threshold, and if that represents a change from the previous
    value (that is, whether it has changed from *below threshold* to *above threshold*
    or vice versa). If the new average represents a state change, the function will
    emit the Boolean value `True`, otherwise it will emit `False`. We will leverage
    that value to filter out events that do not represent a state change. We''ll implement
    the threshold tracking function in the `ThresholdFilterFunction` class as shown
    in the following code snippet:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的作用是确定平均速率字段的新值是否越过了阈值，并且它是否代表了与先前值的变化（即它是否从*低于阈值*变为*高于阈值*或反之）。如果新的平均值代表了状态变化，函数将发出布尔值`True`，否则它将发出`False`。我们将利用该值来过滤掉不代表状态变化的事件。我们将在`ThresholdFilterFunction`类中实现阈值跟踪功能，如下面的代码片段所示：
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `ThresholdFilterFunction` class defines an inner enumeration to represent
    the state (above threshold or below). The constructor takes a double argument
    that establishes the threshold we compare against. In the `execute()` method,
    we get the current rate value and determine whether it is below or above the threshold.
    We then compare it to the last state to see if it has changed and emit that value
    as a Boolean. Finally, we update the internal above/below state to the newly calculated
    value.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`ThresholdFilterFunction`类定义了一个内部枚举来表示状态（高于阈值或低于阈值）。构造函数接受一个双精度参数，用于建立我们要比较的阈值。在`execute()`方法中，我们获取当前速率值，并确定它是低于还是高于阈值。然后，我们将其与上一个状态进行比较，看它是否已经改变，并将该值作为布尔值发出。最后，我们将内部的高于/低于状态更新为新计算的值。'
- en: 'After passing through the `ThresholdFilterFunction` class, tuples in the input
    stream will contain a new Boolean value that we can use to easily filter out events
    that don''t trigger a state change. To filter out non-state-change events, we''ll
    use a simple `BooleanFilter` class as shown in the following code snippet:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`ThresholdFilterFunction`类后，输入流中的元组将包含一个新的布尔值，我们可以使用它来轻松过滤掉不触发状态变化的事件。为了过滤掉非状态变化的事件，我们将使用一个简单的`BooleanFilter`类，如下面的代码片段所示：
- en: '[PRE27]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `BooleanFilter.isKeep()` method simply reads a field from a tuple as a Boolean
    value and returns that value. Any tuples containing `False` for the input value
    will be filtered out of the resulting stream.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`BooleanFilter.isKeep()`方法只是从元组中读取一个字段作为布尔值并返回该值。任何包含输入值为`False`的元组将被过滤出结果流。'
- en: 'The following code fragment illustrates the usage of the `ThresholdFilterFuncation`
    class and the `BooleanFilter` class:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段说明了`ThresholdFilterFuncation`类和`BooleanFilter`类的用法：
- en: '[PRE28]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The first line creates a `ThresholdFilterFunction` instance with a threshold
    of `50.0`. We then create a new stream using the `averageStream` as input to the
    threshold function, and select the `average` tuple field as input. We also assign
    names (`change` and `threshold`) to the fields added by the function. Finally,
    we apply the `BooleanFilter` class to create a new stream that will only contain
    tuples that represent a change in threshold comparison.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行创建了一个具有阈值`50.0`的`ThresholdFilterFunction`实例。然后，我们使用`averageStream`作为输入创建了一个新的流，并选择`average`元组字段作为输入。我们还为函数添加的字段分配了名称（`change`和`threshold`）。最后，我们应用`BooleanFilter`类创建一个新的流，该流将只包含代表阈值比较变化的元组。
- en: At this point, we have everything necessary to implement notifications. The
    `filteredStream` we've created will only contain tuples that represent a threshold
    state change.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经有了实现通知所需的一切。我们创建的`filteredStream`将只包含代表阈值状态变化的元组。
- en: Sending notifications with XMPP
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用XMPP发送通知
- en: 'The XMPP protocol provides all the typical features you would expect in an
    instant messaging standard:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: XMPP协议提供了即时消息标准中所期望的所有典型功能：
- en: Rosters (contact lists)
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花名册（联系人列表）
- en: Presence (knowing when others are online and their availability status)
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线状态（知道其他人何时在线以及他们的可用状态）
- en: User-to-user instant messaging
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户之间的即时消息
- en: Group chats
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 群聊
- en: The XMPP protocol uses an XML format for its communication protocol, but there
    are numerous high-level client libraries that handle most of the low-level details
    with a simple API. We will use the Smack API ([http://www.igniterealtime.org/projects/smack/](http://www.igniterealtime.org/projects/smack/))
    as it is one of the most straightforward XMPP client implementations.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: XMPP协议使用XML格式进行通信，但有许多高级客户端库可以处理大部分低级细节，并提供简单的API。我们将使用Smack API（[http://www.igniterealtime.org/projects/smack/](http://www.igniterealtime.org/projects/smack/)），因为它是最直接的XMPP客户端实现之一。
- en: 'The following code snippet demonstrates the usage of the Smack API to send
    a simple instant message to another user:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段演示了使用Smack API向另一个用户发送简单即时消息的用法：
- en: '[PRE29]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The code connects to the XMPP server at [jabber.org](http://jabber.org) and
    logs in with a username and password. Behind the scenes, the Smack library handles
    the low-level communications with the server. When the client connects and authenticates,
    it also sends a presence message to the server. This allows a user's contacts
    (other users listed in their XMPP roster) to receive a notification that the person
    is now connected. Finally, we create and send a simple message addressed to `"myfriend@jabber.org"`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码连接到[jabber.org](http://jabber.org)的XMPP服务器，并使用用户名和密码登录。在幕后，Smack库处理与服务器的低级通信。当客户端连接并进行身份验证时，它还向服务器发送了一个出席消息。这允许用户的联系人（在其XMPP花名册中列出的其他用户）收到通知，表明该用户现在已连接。最后，我们创建并发送一个简单的消息，地址为`"myfriend@jabber.org"`。
- en: Based on this simple example, we will create a class named `XMPPFunction` that
    sends XMPP notifications when it receives a Trident tuple. The class will establish
    a long-lived connection to an XMPP server in the `prepare()` method. Also, in
    the `execute()` method it will create an XMPP message based on the tuple received.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个简单的例子，我们将创建一个名为`XMPPFunction`的类，当它接收到Trident元组时，会发送XMPP通知。该类将在`prepare()`方法中建立与XMPP服务器的长连接。此外，在`execute()`方法中，它将根据接收到的元组创建一个XMPP消息。
- en: 'To make the `XMPPFunction` class more reusable, we''ll introduce the `MessageMapper`
    interface that defines a method to format the data from a Trident tuple to a string
    suitable for an instant message notification, as shown in the following code snippet:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使`XMPPFunction`类更具可重用性，我们将引入`MessageMapper`接口，该接口定义了一种方法，用于将Trident元组的数据格式化为适合即时消息通知的字符串，如下所示的代码片段所示：
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We''ll delegate message formatting to an instance of `MessageMapper` in the
    `XMPPFunction` class as shown in the following code snippet:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在`XMPPFunction`类中委托消息格式化给一个`MessageMapper`实例，如下所示的代码片段所示：
- en: '[PRE31]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The `XMPPFunction` class begins by defining several string constants that are
    used to look up values from the Storm configuration passed to the `prepare()`
    method, and it follows with the declaration of the instance variables that we'll
    populate when the function becomes active. The class' constructor takes a `MessageMapper`
    instance as a parameter that will be used in the `execute()` method to format
    the body of the notification message.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`XMPPFunction`类首先定义了几个字符串常量，用于从传递给`prepare()`方法的Storm配置中查找值，然后声明了实例变量，当函数激活时我们将填充这些变量。该类的构造函数接受一个`MessageMapper`实例作为参数，该实例将在`execute()`方法中用于格式化通知消息的正文。'
- en: In the `prepare()` method, we look up the configuration parameters (`server`,
    `username`, `to address`, and so on) for the `XMPPConnection` class and open the
    connection. When a topology that uses this function is deployed, the `XMPP` client
    will send a presence packet and other users who have the configured user in their
    roster (buddy list) will receive a notification indicating that the user is now
    online.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在`prepare()`方法中，我们查找`XMPPConnection`类的配置参数（`server`、`username`、`to address`等），并打开连接。当部署使用此函数的拓扑时，`XMPP`客户端将发送出席数据包，其他用户如果在其花名册（好友列表）中有配置的用户，则会收到通知，指示该用户现在在线。
- en: 'The final necessary piece of our notification mechanism is to implement a `MessageMapper`
    instance to format the contents of a tuple into a human-readable message body
    as shown in the following code snippet:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通知机制的最后一个必要部分是实现一个`MessageMapper`实例，将元组的内容格式化为人类可读的消息正文，如下所示的代码片段所示：
- en: '[PRE32]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The final topology
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终的拓扑结构
- en: 'We now have all the components necessary to build our log analysis topology
    as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有构建日志分析拓扑所需的所有组件，如下所示：
- en: '[PRE33]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then, the `buildTopology()` method creates all the stream connections between
    the Kafka spout and our Trident functions and filters. The `main()` method then
    submits the topology to a cluster: a local cluster if the topology is being run
    in the local mode or a remote cluster when run in the distributed mode.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`buildTopology()`方法创建Kafka spout和我们的Trident函数和过滤器之间的所有流连接。然后，`main()`方法将拓扑提交到集群：如果拓扑在本地模式下运行，则提交到本地集群，如果在分布式模式下运行，则提交到远程集群。
- en: We begin by configuring the Kafka spout to read from the same topic that our
    application is configured to write log events. Because Kafka persists all the
    messages it receives, and because our application may have been running for some
    time (and thus logging many events), we tell the spout to fast-forward to the
    end of the Kafka queue by calling the `forceStartOffsetTime()` method with a value
    of `-1`. This will avoid the replay of all the old messages that we may not be
    interested in. Using a value of `-2` will force the spout to rewind to the beginning
    of the queue, and using a specific date in milliseconds will force it to rewind
    to a specific point in time. If the `forceFromStartTime()` method is not called,
    the spout will attempt to resume where it last left off by looking up an offset
    in ZooKeeper.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先配置Kafka spout以从我们的应用程序配置为写入日志事件的相同主题中读取。因为Kafka会持久化它接收到的所有消息，并且因为我们的应用程序可能已经运行了一段时间（因此记录了许多事件），我们告诉spout通过调用`forceStartOffsetTime()`方法并使用值`-1`来快进到Kafka队列的末尾。这将避免重放我们可能不感兴趣的所有旧消息。使用值`-2`将强制spout倒带到队列的开头，并使用毫秒级的特定日期将强制它倒带到特定时间点。如果没有调用`forceFromStartTime()`方法，spout将尝试通过在ZooKeeper中查找偏移量来恢复上次离开的位置。
- en: Next, we set up the `JsonProjectFunction` class to parse the raw JSON received
    from Kafka and emit the values that we're interested in. Recall that the Trident
    functions are additive. This means that our tuple stream, in addition to all the
    values extracted from the JSON, will also contain the original unparsed JSON string.
    Since we no longer need that data, we call the `Stream.project()` method with
    a list of fields we want to keep. The `project()` method is useful for paring
    down tuple streams to just the essential fields, and it is especially important
    while repartitioning streams that have large amounts of data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置`JsonProjectFunction`类来解析从Kafka接收到的原始JSON，并发出我们感兴趣的值。请记住，Trident函数是可加的。这意味着我们的元组流，除了从JSON中提取的所有值之外，还将包含原始未解析的JSON字符串。由于我们不再需要这些数据，我们调用`Stream.project()`方法，提供我们想要保留的字段列表。`project()`方法对于将元组流减少到只包含基本字段非常有用，尤其是在重新分区具有大量数据的流时非常重要。
- en: The resulting stream now contains just the data we need. We set up an `EWMA`
    instance with a sliding window of one minute and configure the `MovingAverageFunction`
    class to emit the current rate in minutes. We create the `ThresholdFunction` class
    with a value of `50.0`, so we'll receive a notification any time the average rate
    goes above or falls below 50 events per minute.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在生成的流只包含我们需要的数据。我们使用一个滑动窗口为一分钟的`EWMA`实例，并配置`MovingAverageFunction`类以发出每分钟的当前速率。我们使用值`50.0`创建`ThresholdFunction`类，因此每当平均速率超过或低于每分钟50个事件时，我们都会收到通知。
- en: Finally, we apply the `BooleanFilter` class and connect the resulting stream
    to the `XMPPFunction` class.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应用`BooleanFilter`类，并将生成的流连接到`XMPPFunction`类。
- en: The `main()` method of the topology simply populates a `Config` object with
    the properties needed by the `XMPPFunction` class and submits the topology.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑的`main()`方法只是用所需的属性填充一个`Config`对象，并提交拓扑。
- en: Running the log analysis topology
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行日志分析拓扑
- en: To run the analysis topology, first make sure that ZooKeeper, Kafka, and OpenFire
    are all up and running by using the procedures outlined earlier in the chapter.
    Then, run the `main()` method of the topology.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行分析拓扑，首先确保ZooKeeper、Kafka和OpenFire都已经按照本章前面概述的步骤启动并运行。然后，运行拓扑的`main()`方法。
- en: 'When the topology activates, the *storm* XMPP user will connect to the XMPP
    server and trigger a presence event. If you are logged into the same server with
    an XMPP client and have the *storm* user in your buddy list, you will see it become
    available. This is shown in the following screenshot:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当拓扑激活时，*storm* XMPP用户将连接到XMPP服务器并触发存在事件。如果您使用XMPP客户端登录到同一服务器，并且在好友列表中有*storm*用户，您将看到它变为可用。如下面的屏幕截图所示：
- en: '![Running the log analysis topology](img/8294OS_04_04.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![运行日志分析拓扑](img/8294OS_04_04.jpg)'
- en: 'Next, run the `RogueApplication` class and wait for a minute. You should receive
    an instant message notification indicating that the threshold has been exceeded,
    which will be followed by one indicating a return to normal (below threshold),
    as shown in the following screenshot:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，运行`RogueApplication`类并等待一分钟。您应该收到即时消息通知，指示已超过阈值，随后将收到一条指示返回正常（低于阈值）的消息，如下面的屏幕截图所示：
- en: '![Running the log analysis topology](img/8294OS_04_05.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![运行日志分析拓扑](img/8294OS_04_05.jpg)'
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we've introduced you to real-time analytics by creating a simple
    yet powerful topology that can be adapted to a wide range of applications. The
    components we've built are generic and can easily be reused and extended in other
    projects. Finally, we introduced a real-world spout implementation that can be
    used for a multitude of purposes.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过创建一个简单但功能强大的拓扑介绍了实时分析，该拓扑可以适应各种应用程序。我们构建的组件是通用的，可以轻松地在其他项目中重用和扩展。最后，我们介绍了一个真实世界的spout实现，可以用于多种目的。
- en: While the topic of real-time analytics is very broad, and admittedly we've only
    been able to scratch the surface in this chapter, we encourage you to explore
    the techniques presented in other chapters of this book and consider how they
    may be incorporated into your analytics toolbox.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然实时分析的主题非常广泛，而且诚然，我们在本章中只能触及表面，但我们鼓励您探索本书其他章节中提出的技术，并考虑如何将它们纳入您的分析工具箱中。
- en: In the next chapter, we'll introduce you to Trident's distributed state mechanism
    by building an application that continuously writes Storm-processed data to a
    graph database.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '在下一章中，我们将通过构建一个应用程序，将Storm处理的数据持续写入图形数据库，向您介绍Trident的分布式状态机制。 '
