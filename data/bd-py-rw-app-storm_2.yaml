- en: Chapter 2. The Storm Anatomy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。风暴解剖
- en: 'This chapter gives a detailed view of the internal structure and processes
    of the Storm technology. We will cover the following topics in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细介绍了风暴技术的内部结构和流程。本章将涵盖以下主题：
- en: Storm processes
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风暴流程
- en: Storm-topology-specific terminologies
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风暴拓扑特定术语
- en: Interprocess communication
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进程间通信
- en: Fault tolerance in Storm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风暴中的容错
- en: Guaranteed tuple processing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保证元组处理
- en: Parallelism in Storm—scaling a distributed computation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风暴中的并行性-扩展分布式计算
- en: As we advance through the chapter, you will understand Storm's processes and
    their role in detail. In this chapter, various Storm-specific terminologies will
    be explained. You will learn how Storm achieves fault tolerance for different
    types of failure. We will see what guaranteed message processing is and, most
    importantly, how to configure parallelism in Storm to achieve fast and reliable
    processing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们在本章中的深入，您将详细了解Storm的流程及其作用。在本章中，将解释各种特定于Storm的术语。您将了解Storm如何实现不同类型故障的容错。我们将看到什么是消息处理的保证，最重要的是如何配置Storm中的并行性以实现快速可靠的处理。
- en: Storm processes
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 风暴流程
- en: We will start with Nimbus first, which is actually the entry-point daemon in
    Storm. Just to compare with Hadoop, Nimbus is actually the job tracker of Storm.
    Nimbus's job is to distribute code to all supervisor daemons of a cluster. So,
    when topology code is submitted, it actually reaches all physical machines in
    the cluster. Nimbus also monitors failure of supervisors. If a supervisor continues
    to fail, then Nimbus reassigns those workers' jobs to other workers of a different
    physical machine. The current version of Storm allows only one instance of the
    Nimbus daemon to run. Nimbus is also responsible for assigning tasks to supervisor
    nodes. If you lose Nimbus, the workers will still continue to compute. Supervisors
    will continue to restart workers as and when they die. Without Nimbus, a worker's
    task won't be reassigned to another machine worker within the cluster.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先从Nimbus开始，实际上Nimbus是Storm中的入口守护程序。仅仅与Hadoop相比，Nimbus实际上是Storm的作业跟踪器。Nimbus的工作是将代码分发到集群的所有监督守护程序。因此，当拓扑代码被提交时，它实际上会到达集群中的所有物理机器。Nimbus还监视监督守护程序的故障。如果监督守护程序继续失败，那么Nimbus会将这些工作重新分配给集群中不同物理机器的其他工作程序。当前版本的Storm只允许运行一个Nimbus守护程序实例。Nimbus还负责将任务分配给监督节点。如果丢失Nimbus，工作程序仍将继续计算。监督守护程序将在工作程序死亡时继续重新启动工作程序。没有Nimbus，工作程序的任务将不会重新分配到集群中的另一台机器上的工作程序。
- en: There is no alternative Storm process that will take over if Nimbus dies, and
    no process will even try to restart it. There is nothing to worry about, however,
    since it can be restarted anytime. In a production environment, alerts can also
    be set when Nimbus dies. In future, we may see highly available Nimbus.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Nimbus死亡，没有替代的风暴流程会接管，也没有进程会尝试重新启动它。然而，不用担心，因为它可以随时重新启动。在生产环境中，当Nimbus死亡时也可以设置警报。在未来，我们可能会看到高可用的Nimbus。
- en: Supervisor
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督守护程序
- en: A supervisor manages all the workers of the respective machine. Distributed
    computation in Storm is possible due to the supervisor daemon, as there is one
    supervisor per machine in your cluster. The supervisor daemon listens for the
    work assigned by Nimbus to the machine that it runs, and distributes it among
    workers. Due to any runtime exception, workers can die anytime, and the supervisor
    restarts them when there is no heartbeat from dead workers. Each worker process
    executes a part of a topology. Similar to the Hadoop ecosystem, supervisor is
    a task tracker of Storm. It tracks the tasks of workers of the same machine. The
    maximum number of possible workers depends on the number of ports defined in `storm.yaml`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 监督守护程序管理各自机器的所有工作程序。由于在您的集群中每台机器上都有一个监督守护程序，因此风暴中的分布式计算是可能的。监督守护程序监听Nimbus分配给其运行的机器的工作，并将其分配给工作程序。由于任何运行时异常，工作程序随时可能会死亡，当没有来自死亡工作程序的心跳时，监督守护程序会重新启动它们。每个工作程序进程执行拓扑的一部分。与Hadoop生态系统类似，监督守护程序是风暴的任务跟踪器。它跟踪同一台机器上的工作程序的任务。可能的工作程序的最大数量取决于`storm.yaml`中定义的端口数量。
- en: Zookeeper
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动物园管理员
- en: In addition to its own components, Storm relies on a Zookeeper cluster (one
    or more Zookeeper servers) to perform the coordination job between Nimbus and
    the supervisors. Apart from using Zookeeper for coordination purposes, Nimbus
    and the supervisors also store all their states in Zookeeper, and Zookeeper stores
    them on a local disk where it is running. Having more than one Zookeeper daemon
    increases the reliability of the system, because if one daemon goes down, another
    becomes the leader.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自己的组件外，风暴还依赖于一个动物园管理员集群（一个或多个动物园管理员服务器）来在Nimbus和监督守护程序之间执行协调工作。除了用于协调目的，Nimbus和监督守护程序还将它们所有的状态存储在动物园管理员中，并且动物园管理员将它们存储在其运行的本地磁盘上。拥有多个动物园管理员守护程序可以增加系统的可靠性，因为如果一个守护程序崩溃，另一个守护程序将成为领导者。
- en: The Storm UI
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 风暴UI
- en: Storm is also equipped with a web-based user interface. It should be started
    on a machine that also runs Nimbus. The Storm UI provides a report of the entire
    cluster, such as the sum of all active supervisor machines, the total number of
    workers available, allotted to each topology and how many remaining, and topology-level
    diagnostics such as tuples stats (how many tuples were emitted, and the ACK between
    spout to bolt or bolt to bolt). The Storm UI also shows the total number of workers,
    which is actually sum of all workers available of all supervisors' machines.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 风暴还配备了基于Web的用户界面。它应该在运行Nimbus的机器上启动。风暴UI提供了整个集群的报告，例如所有活动监督机器的总和，可用的工作程序总数，分配给每个拓扑的工作程序数量以及剩余的数量，以及拓扑级诊断，例如元组统计（发射了多少元组，spout到bolt或bolt到bolt之间的ACK）。风暴UI还显示了工作程序的总数，实际上是所有监督机器的所有可用工作程序的总和。
- en: 'The following screenshot shows a sample screen of the Storm UI:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了风暴UI的示例屏幕：
- en: '![The Storm UI](img/B03471_02_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![风暴UI](img/B03471_02_01.jpg)'
- en: 'Following is the explanation of Storm UI:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是风暴UI的解释：
- en: '**Topology stats**: Under **Topology stats**, you can click and see the stats
    of the last 10 minutes, 3 hours, or all time.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拓扑统计**：在**拓扑统计**下，您可以点击并查看最近10分钟、3小时或所有时间的统计信息。'
- en: '**Spouts (All time)**: This displays the number of executors and tasks assigned
    for this spout, along with the stats of emitted tuples and other latency stats.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**喷口（所有时间）**：显示了为此喷口分配的执行者和任务数量，以及发射的元组和其他延迟统计信息的状态。'
- en: '**Bolts (All time)**: This displays a list of all bolts, along with the assigned
    executors/tasks. When you are doing performance tuning, keep the **Capacity**
    column close to `1`. In the preceding example for **aggregatorBolt**, it is `1.500`,
    so instead of `200` executors/tasks, we can use `300`. The **Capacity** column
    helps us decide the right degree of parallelism. The idea is very simple; if the
    **Capacity** column reads more than `1`, try increasing the executors and tasks
    in the same ratio. If the value of executors/tasks is high and the **Capacity**
    column is close to zero, try reducing the number of executors/tasks. You can do
    this until you get the best configuration.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**螺栓（所有时间）**：显示了所有螺栓的列表，以及分配的执行者/任务。在进行性能调优时，保持**容量**列接近`1`。在前面的**aggregatorBolt**示例中，它是`1.500`，所以我们可以使用`300`而不是`200`个执行者/任务。**容量**列帮助我们决定正确的并行度。这个想法非常简单；如果**容量**列读数超过`1`，尝试以相同比例增加执行者和任务。如果执行者/任务的值很高，而**容量**列接近零，尝试减少执行者/任务的数量。您可以一直这样做，直到获得最佳配置。'
- en: Storm-topology-specific terminologies
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 风暴拓扑特定术语
- en: 'A topology is a logical separation of programming work into many small-scale
    processing units called spout and bolt, which is similar to MapReduce in Hadoop.
    A topology can be written in many languages, including Java, Python, and lot more
    supported languages. In visual depictions, a topology is shown as a graph of connecting
    spouts and bolts. Spouts and bolts execute tasks across the cluster. Storm has
    two modes of operation, called local mode and distributed mode:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑是将编程工作逻辑上分成许多小规模处理单元的分离，称为喷口和螺栓，类似于Hadoop中的MapReduce。拓扑可以用许多语言编写，包括Java、Python和更多支持的语言。在视觉描述中，拓扑显示为连接喷口和螺栓的图形。喷口和螺栓在集群中执行任务。Storm有两种操作模式，称为本地模式和分布式模式：
- en: In local mode, all processes of Storm and workers run within your code development
    environment. This is good for testing and development of topologies.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地模式下，Storm和工作者的所有进程都在您的代码开发环境中运行。这对于拓扑的测试和开发很有用。
- en: In distributed mode, Storm operates as a cluster of machines. When you submit
    topology code to the Nimbus, Nimbus takes care of distributing the code and allocating
    workers to run your topology based on your configuration.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分布式模式下，Storm作为一组机器的集群运行。当您将拓扑代码提交给Nimbus时，Nimbus负责分发代码并根据您的配置分配工作者来运行您的拓扑。
- en: In the following figure, we have purple bolts; these receive a tuple or records
    from the spout above them. A tuple supports most of the data types available in
    the programming language in which the topology code is being written. It flows
    as an independent unit from a spout to a bolt or a bolt to another bolt. An unbounded
    flow of tuples is called a stream. In a single tuple, you can have many key-value
    pairs to pass together.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们有紫色的螺栓；这些从它们上面的喷口接收元组或记录。元组支持拓扑代码所使用的编程语言中的大多数数据类型。它作为一个独立单元从喷口流向螺栓或从一个螺栓流向另一个螺栓。无限的元组流称为流。在一个元组中，您可以有许多键值对一起传递。
- en: The next figure illustrates streams in more detail. A spout is connected to
    a source of tuples and generates continuous tuples for the topology as a stream.
    What you emit from the spout as a key-value pair can be received by the bolt using
    the same key.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下图更详细地说明了流。喷口连接到元组的源并为拓扑生成连续的元组流。从喷口发出的键值对可以被螺栓使用相同的键接收。
- en: '![Storm-topology-specific terminologies](img/B03471_02_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![风暴拓扑特定术语](img/B03471_02_02.jpg)'
- en: The worker process, executor, and task
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作者进程、执行者和任务
- en: 'Storm distinguishes between the following three main entities, which are used
    to actually run a topology in a Storm cluster:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Storm区分以下三个主要实体，用于在Storm集群中实际运行拓扑：
- en: Worker
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作者
- en: Executor
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行者
- en: Task
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务
- en: 'Let''s say we have decided to keep two workers, one spout executor, three **Bolt1**
    executors, and two **Bolt2** executors. Assume that the ratio of the number of
    executors and tasks is the same. The total sum of executors is six for spout and
    bolt. Out of six executors, some will run within the scope of worker 1, and some
    will be in control of worker 2; this decision is taken by the supervisor. This
    is explained in the following figure:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们决定保留两个工作者，一个喷口执行者，三个**Bolt1**执行者和两个**Bolt2**执行者。假设执行者和任务数量的比例相同。喷口和螺栓的总执行者数为六。在六个执行者中，一些将在工作者1的范围内运行，一些将由工作者2控制；这个决定由监督者负责。这在下图中有解释。
- en: '![The worker process, executor, and task](img/B03471_02_03.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![工作者进程、执行者和任务](img/B03471_02_03.jpg)'
- en: 'The next figure explains the position of the workers and executors within the
    scope of the supervisor that is running on a machine:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下图解释了在运行在一台机器上的监督者范围内工作者和执行者的位置：
- en: '![The worker process, executor, and task](img/B03471_02_04.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![工作者进程、执行者和任务](img/B03471_02_04.jpg)'
- en: The number of executors and tasks is set while building the topology code. In
    the preceding figure, we have two workers (1 and 2), run and managed by the supervisor
    of that machine. Assume that **Executor 1** is running one task, because the ratio
    of executors to tasks is the same (for example, 10 executors means 10 tasks, which
    makes the ratio 1:1). But **Executor 2** is running two tasks sequentially, so
    the ratio of tasks to executors is 2:1 (for example, 10 executors means 20 tasks,
    which makes the ratio 2:1). Having more tasks never means higher processing speed,
    but this is true for more executors, as tasks run sequentially.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建拓扑代码时设置执行器和任务的数量。在上图中，我们有两个工作进程（1和2），由该机器的监督者运行和管理。假设**执行器1**正在运行一个任务，因为执行器与任务的比例相同（例如，10个执行器意味着10个任务，这使得比例为1:1）。但是**执行器2**正在顺序运行两个任务，因此任务与执行器的比例为2:1（例如，10个执行器意味着20个任务，这使得比例为2:1）。拥有更多的任务并不意味着更高的处理速度，但对于更多的执行器来说是正确的，因为任务是顺序运行的。
- en: Worker processes
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作进程
- en: A single worker process executes a portion of a topology and runs on its own
    JVM. Workers are allocated during topology submission. A worker process is linked
    to a specific topology and can run one or more executors for one or more spouts
    or bolts of that topology. A running topology consists of many such workers running
    on many machines within a Storm cluster.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 单个工作进程执行拓扑的一部分，并在自己的JVM上运行。工作进程在拓扑提交期间分配。工作进程与特定的拓扑相关联，并且可以为该拓扑的一个或多个spout或螺栓运行一个或多个执行器。运行中的拓扑由许多这样的工作进程组成，这些工作进程在Storm集群中的许多机器上运行。
- en: Executors
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行器
- en: An executor is a thread run within the scope of a worker's JVM. An executor
    may run one or more tasks for a spout or bolt sequentially.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器是在工作进程的JVM范围内运行的线程。执行器可以顺序运行一个或多个spout或螺栓的任务。
- en: 'An executor always runs on one thread for all its tasks, which means that tasks
    run serially on an executor. The number of executors can be changed after the
    topology has been started without shutdown, using the `rebalance` command:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器始终在一个线程上运行其所有任务，这意味着任务在执行器上顺序运行。在拓扑启动后，可以使用`rebalance`命令更改执行器的数量而无需关闭。
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tasks
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务
- en: A task performs data processing and runs within its parent executor's thread
    of execution. The default value of the number of tasks is the same as the number
    of executors. While building the topology, we can keep a higher number of tasks
    as well. It can help to increase the number of executors in the future, which
    keeps the scope of scaling open. Initially, we can have 10 executors and 20 tasks,
    so the ratio is 2:1\. This means two tasks per executor. A future rebalancing
    action can make 20 executors and 20 tasks, which will make the ratio 1:1.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 任务执行数据处理，并在其父执行器的执行线程中运行。任务数量的默认值与执行器的数量相同。在构建拓扑时，我们也可以保留更多的任务数量。这有助于在未来增加执行器的数量，从而保持扩展的范围。最初，我们可以有10个执行器和20个任务，因此比例为2:1。这意味着每个执行器有两个任务。未来的重新平衡操作可以使20个执行器和20个任务，这将使比例为1:1。
- en: Interprocess communication
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进程间通信
- en: The following figure illustrates communication between the Storm submitter (client),
    the Nimbus thrift server, Zookeeper, supervisors, workers of supervisors, executors,
    and tasks. Each worker process runs as a separate JVM.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示了Storm提交者（客户端）、Nimbus thrift服务器、Zookeeper、监督者、监督者的工作进程、执行器和任务之间的通信。每个工作进程都作为一个单独的JVM运行。
- en: '![Interprocess communication](img/B03471_02_05.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![进程间通信](img/B03471_02_05.jpg)'
- en: A physical view of a Storm cluster
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm集群的物理视图
- en: The next figure explains the physical position of each process. There can be
    only one Nimbus. However, more than one Zookeeper is there to support failover,
    and per machine, there is one supervisor.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下图解释了每个进程的物理位置。只能有一个Nimbus。但是，有多个Zookeeper来支持故障转移，并且每台机器上都有一个监督者。
- en: '![A physical view of a Storm cluster](img/B03471_02_06.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![Storm集群的物理视图](img/B03471_02_06.jpg)'
- en: Stream grouping
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流分组
- en: 'A stream grouping controls the flow of tuples between from spout to bolt or
    bolt to bolt. In Storm, we have four types of groupings. Shuffle and field grouping
    are most commonly used:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 流分组控制元组在spout和螺栓之间或螺栓之间的流动。在Storm中，我们有四种类型的分组。Shuffle和字段分组是最常用的：
- en: '**Shuffle grouping**: Tuple flow between two random tasks in this grouping'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Shuffle分组**：此分组中两个随机任务之间的元组流'
- en: '**Field grouping**: A tuple with a particular field key is always delivered
    to the same task of the downstream bolt'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字段分组**：具有特定字段键的元组始终传递到下游螺栓的相同任务'
- en: '**All grouping**: Sends the same tuple to all tasks of the downstream bolt'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**所有分组**：将相同的元组发送到下游螺栓的所有任务'
- en: '**Global grouping**: Tuples from all tasks reach one task'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局分组**：所有任务的元组都到达一个任务'
- en: 'The subsequent figure gives a diagrammatic explanation of all the four types
    of groupings:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下图解释了所有四种分组类型的图解说明：
- en: '![Stream grouping](img/B03471_02_07.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![流分组](img/B03471_02_07.jpg)'
- en: Fault tolerance in Storm
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Storm中的容错
- en: 'Supervisor runs a synchronization thread to get assignment information (what
    part of topology I am supposed to run) from Zookeeper and write to the local disk.
    This local filesystem information helps keep the worker up to date:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 监督者运行同步线程，从Zookeeper获取分配信息（我应该运行拓扑的哪一部分）并写入本地磁盘。这个本地文件系统信息有助于保持工作进程的最新状态：
- en: '**Case 1**: This is the ideal case for most of the times. When the cluster
    works normally, the worker''s heartbeat goes back to the supervisors and Nimbus
    via Zookeeper.![Fault tolerance in Storm](img/B03471_02_08.jpg)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情况1**：这在大多数情况下都是理想的情况。当集群正常工作时，工作节点的心跳通过Zookeeper返回给监督者和Nimbus。![Storm中的容错](img/B03471_02_08.jpg)'
- en: '**Case 2**: If a supervisor dies, processing still continues, but the assignment
    is never synchronized. Nimbus will reassign the work to another supervisor of
    a different machine. Those workers will be running, but will not receive any new
    tuples. Do set an alert to restart the supervisor or use a Unix tool that can
    restart the supervisor.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例2**：如果监督员死亡，处理仍将继续，但任务将永远不会同步。Nimbus将重新分配工作给另一台不同机器的监督员。这些工人将在运行，但不会接收任何新的元组。确保设置警报以重新启动监督员或使用可以重新启动监督员的Unix工具。'
- en: '**Case 3**: If Nimbus dies, the topologies will continue to function normally.
    Processing will still continue, but topology life cycle operations and reassigning
    to another machine will not be possible.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例3**：如果Nimbus死亡，拓扑将继续正常运行。处理仍将继续，但拓扑生命周期操作和重新分配到另一台机器将不可能。'
- en: '**Case 4**: If a worker dies (as the heartbeat stops arriving), the supervisor
    will try to restart the worker process and processing will continue. If a worker
    dies repeatedly, Nimbus will reassign the work to other nodes in the cluster.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例4**：如果工作人员死亡（因为心跳停止到达），监督员将尝试重新启动工作进程并继续处理。如果工作人员反复死亡，Nimbus将重新分配工作给集群中的其他节点。'
- en: Guaranteed tuple processing in Storm
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Storm中的元组处理保证
- en: 'As Storm is already equipped to deal with various process-level failures, another
    important feature is the ability to deal with failure of tuples that occurs when
    a worker dies. This is just to give an idea of bitwise XOR: the XOR of two sets
    of the same bits is 0\. This is called XOR magic, and it can help us know whether
    the delivery of a tuple to the next bolt is successful or not. Storm uses 64 bits
    to track tuples. Every tuple gets a 64-bit tuple ID. This 64-bit ID, along with
    the task ID, is kept at ACKer.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Storm已经配备了处理各种进程级故障的能力，另一个重要特性是处理工作人员死亡时发生的元组失败的能力。这只是为了给出按位XOR的概念：相同位的两组的XOR为0。这被称为XOR魔术，它可以帮助我们知道元组传递到下一个螺栓是否成功。Storm使用64位来跟踪元组。每个元组都有一个64位的元组ID。此64位ID连同任务ID一起保存在ACKer中。
- en: 'In the next figure, ACKing and a replay case is explained:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个图中，解释了ACKing和重播案例：
- en: '![Guaranteed tuple processing in Storm](img/B03471_02_09.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Storm中的元组处理保证](img/B03471_02_09.jpg)'
- en: XOR magic in acking
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ACKing中的XOR魔术
- en: 'A spout tuple is not fully processed until all the tuples in the linked tuple
    tree are completed. If the tuple tree is not completed within a configured timeout
    (the default value is `topology.message.timeout.secs: 30`), the spout tuple is
    replayed.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '在链接的元组树中的所有元组完成之前，喷口元组不会被完全处理。如果在配置的超时时间内未完成元组树（默认值为`topology.message.timeout.secs:
    30`），则会重播喷口元组。'
- en: In the preceding diagram, the first acker gets `10101` (for simplicity of explanation,
    we are keeping 5 bits) for tuple 1 from the spout. Once **Bolt 1** receives the
    same tuple, it also ACK to acker. From both sources, acker gets `10101`. This
    means `10101` XOR `10101 = 0`. Tuple 1 is successfully received by **Bolt 1**.
    The same process repeats between bolts 1 and 2\. At last, **Bolt 2** sends ack
    to acker, and the tuple tree is completed. This creates a signal to call the spout's
    `success` function. Any failure in tuple processing can trigger the spout's `fail`
    function call, which gives an indication to send the tuple back for processing
    again.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，第一个acker从喷口获得了`10101`（为了简单起见，我们保留了5位）的元组1。一旦**Bolt 1**接收到相同的元组，它也会向acker发送ACK。从两个来源，acker都获得了`10101`。这意味着`10101`
    XOR `10101 = 0`。元组1已成功被**Bolt 1**接收。同样的过程在螺栓1和2之间重复。最后，**Bolt 2**向acker发送ACK，元组树完成。这会创建一个信号来调用喷口的`success`函数。元组处理中的任何失败都可以触发喷口的`fail`函数调用，这表明需要将元组发送回进行再次处理。
- en: Storm's acker tracks the completion of the tuple tree by performing XOR between
    the sender's tuple and the receiver's tuple. Each time a tuple is sent, its value
    is XORed into the checksum maintained by acker, and each time a tuple is acked,
    its value is XORed in again at acker.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Storm的acker通过对发送者的元组和接收者的元组执行XOR来跟踪元组树的完成。每次发送元组时，它的值都会被XOR到acker维护的校验和中，每次确认元组时，它的值都会再次XOR到acker中。
- en: If all tuples have been successfully acked, the checksum will be zero. Ackers
    are system-level executors.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有元组都已成功确认，校验和将为零。Ackers是系统级执行者。
- en: In the spout, we have a choice of two emit functions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在喷口中，我们可以选择两个发射函数。
- en: '`emit([tuple])`: This is a simple emit'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emit([tuple])`：这是一个简单的发射'
- en: '`storm.emit([tuple], id=the_value)`: This creates a reliable spout, but only
    if you can re-emit a tuple using `the_value`'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`storm.emit([tuple], id=the_value)`：这将创建一个可靠的喷口，但只有在您可以使用`the_value`重新发射元组时才能实现'
- en: 'In the Spout, we also have two ACK functions:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在喷口中，我们还有两个ACK函数：
- en: '`fail(the_value)`: This function is called when a timeout occurs or the tuple
    fails'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fail(the_value)`：当发生超时或元组失败时调用此函数'
- en: '`ack(the_value)`: This function is called when the last bolt of the topology
    ACK the tuple tree'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ack(the_value)`：当拓扑的最后一个螺栓确认元组树时调用此函数'
- en: This ID field should be a random and unique value to replay from the spout's
    `fail` function. Using this ID, we can re-emit it from the `fail` function. If
    successful, the jn `success` function will call and it can remove successful tuples
    from the global list or recreate from the source.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此ID字段应该是一个随机且唯一的值，以便从喷口的`fail`函数中重播。使用此ID，我们可以从`fail`函数中重新发射它。如果成功，将调用`success`函数，并且可以从全局列表中删除成功的元组或从源中重新创建。
- en: 'You will be able to recreate the same tuple if you have a reliable spout in
    the topology. To create a reliable spout, emit a unique message ID (`the_value`)
    from the spout''s next tuple function along with the tuple:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果拓扑中有一个可靠的喷口，您将能够重新创建相同的元组。要创建可靠的喷口，请从喷口的下一个元组函数中发射一个唯一的消息ID（`the_value`）以及元组：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Whether a tuple is not ACKed within a configured period of time, or the programming
    code fails a tuple due to some error condition, both are valid cases of replay.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果元组在配置的时间段内未被确认，或者编程代码由于某些错误条件而失败了元组，这两种情况都是重播的有效情况。
- en: When the `fail` function is called, the code can read from the source of the
    spout using the same message ID, and when the `success` function is called, an
    action such as removing a message from the queue can be taken.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用`fail`函数时，代码可以使用相同的消息ID从喷口的源中读取，并且当调用`success`函数时，可以执行诸如从队列中删除消息之类的操作。
- en: The message ID is an application-specific key that can help you recreate a tuple
    and emit it back from the spout. An example of a message ID can be a queue message
    ID, or a primary key of a table. A tuple is considered failed if a timeout occurs
    or due to any other reason.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 消息ID是一个特定于应用程序的键，可以帮助您重新创建一个元组并从喷口重新发出。消息ID的一个示例可以是队列消息ID，或者是表的主键。如果发生超时或由于其他原因，元组被视为失败。
- en: Storm has a fault tolerance mechanism that guarantees at-least-once processing
    for all tuples emitted only from a reliable spout.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Storm具有容错机制，可以保证所有仅从可靠喷口发出的元组至少处理一次。
- en: Once you have a reliable spout in place, you can make the bolt do the linking
    between the input and output tuples, which creates a tuple tree. Once a tuple
    tree is established, acker knows any failure in the linked tree, and the original
    message ID is used to create the entire tuple tree again.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦有了可靠的喷口，就可以让螺栓在输入和输出元组之间进行链接，从而创建一个元组树。一旦建立了元组树，确认者就知道了链接树中的任何故障，并且使用原始消息ID再次创建整个元组树。
- en: 'In the bolt, there are two functions:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在螺栓中，有两个函数：
- en: '`emit([tuple])`: There is no tuple tree linking. We can''t track which original
    message ID was used.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emit([tuple])`: 没有元组树链接。我们无法追踪使用了哪个原始消息ID。'
- en: '`storm.emit([tuple], anchors=[message_key])`: With linking in place, the original
    tuple can now be replayed.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`storm.emit([tuple], anchors=[message_key])`: 有了链接，现在可以重放原始元组。'
- en: 'The following figure explains how tuple B is generated from tuple A:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下图解释了元组B是如何从元组A生成的：
- en: '![XOR magic in acking](img/B03471_02_10.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![XOR magic in acking](img/B03471_02_10.jpg)'
- en: 'The next figure illustrates the bolt performing **ACK**:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了执行**ACK**的螺栓：
- en: '![XOR magic in acking](img/B03471_02_11.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![XOR magic in acking](img/B03471_02_11.jpg)'
- en: 'The following figure illustrates the failure condition, where the signal reaches
    the spout upon failure:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了故障情况，信号在故障时到达喷口：
- en: '![XOR magic in acking](img/B03471_02_12.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![XOR magic in acking](img/B03471_02_12.jpg)'
- en: 'A successful **ACK** is demonstrated as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的**ACK**演示如下：
- en: '![XOR magic in acking](img/B03471_02_13.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![XOR magic in acking](img/B03471_02_13.jpg)'
- en: 'The following figure illustrates a condition of a big tuple tree without a
    bolt, and there is no failure:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了没有螺栓的大元组树的情况，也没有失败：
- en: '![XOR magic in acking](img/B03471_02_14.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![XOR magic in acking](img/B03471_02_14.jpg)'
- en: 'The next figure demonstrates an example of failure in a tuple tree—in the middle
    of the tuple tree:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下图演示了元组树中的故障示例 - 在元组树的中间：
- en: '![XOR magic in acking](img/B03471_02_15.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![XOR magic in acking](img/B03471_02_15.jpg)'
- en: Tuning parallelism in Storm – scaling a distributed computation
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm中的并行性调优 - 扩展分布式计算
- en: 'To explain parallelism of Storm, we will configure three parameters:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释Storm的并行性，我们将配置三个参数：
- en: The number of workers
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作人员的数量
- en: The number of executors
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行者的数量
- en: The number of tasks
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务的数量
- en: 'The following figure gives a diagrammatic explanation of an example where we
    have a topology with just one spout and one bolt. In this case, we will set different
    values for the numbers of workers, executors, and tasks at the spout and bolt
    levels, and see how parallelism works in each case:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下图给出了一个拓扑结构的示例，其中只有一个喷口和一个螺栓。在这种情况下，我们将为喷口和螺栓级别的工作人员、执行者和任务设置不同的值，并看看在每种情况下并行性是如何工作的：
- en: '![Tuning parallelism in Storm – scaling a distributed computation](img/B03471_02_16.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![Storm中的并行性调优 - 扩展分布式计算](img/B03471_02_16.jpg)'
- en: '[PRE2]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For this configuration, we will have two workers, which will run in separate
    JVMs (worker 1 and worker 2).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配置，我们将有两个工作人员，它们将在单独的JVM中运行（工作人员1和工作人员2）。
- en: For the spout, there is one executor, and the default number of tasks is one,
    which makes the ratio 1:1 (one task per executor).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于喷口，有一个执行者，任务的默认数量是一个，这使得比例为1:1（每个执行者一个任务）。
- en: For the bolt, there are two executors and four tasks, which makes it 4/2 = two
    tasks per executor. These two executors run under worker 2, with each having two
    tasks, while the executor of worker 1 gets only one task.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于螺栓，有两个执行者和四个任务，这使得4/2 = 每个执行者两个任务。这两个执行者在工作人员2下运行，每个执行者有两个任务，而工作人员1的执行者只有一个任务。
- en: 'This can be illustrated nicely using the following figure:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用下图很好地说明：
- en: '![Tuning parallelism in Storm – scaling a distributed computation](img/B03471_02_17.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![Storm中的并行性调优 - 扩展分布式计算](img/B03471_02_17.jpg)'
- en: 'Let''s change the configuration in the bolt to two executors and two tasks:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将螺栓的配置更改为两个执行者和两个任务：
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This can be illustrated well here:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这在这里可以很好地说明：
- en: '![Tuning parallelism in Storm – scaling a distributed computation](img/B03471_02_18.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![Storm中的并行性调优 - 扩展分布式计算](img/B03471_02_18.jpg)'
- en: The number of workers is two again. As the bolt has two executors and two tasks,
    that makes it 2/2, or one task per executor. Now you can see that both executors
    get one task each. In terms of performance, both cases are exactly the same, as
    the tasks run sequentially within the executor thread. More executors means a
    higher degree of parallelism, and more workers means using resources such as CPU
    and RAM more effectively. Memory allocation is done at the worker level using
    the `worker`.`childopts` setting. We should also monitor the maximum amount of
    memory a particular worker process is holding. This plays an important role in
    deciding the total number of workers. It can be seen using the `ps -ef` option.
    Always keep the tasks and executors in the same ratio, and derive the correct
    value for the number of executors using the capacity column of the Storm UI. As
    an important note, we should keep the shorter duration transaction in the bolt
    and try to tune it via splitting code into more bolts or reducing the batch size
    tuple. The batch size is the number of records received by the bolt in a single
    tuple delivery. Also, don't block the `nextTuple` method of the spout due to the
    longer holding transaction.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 工作人员的数量再次是两个。由于螺栓有两个执行者和两个任务，这使得它成为2/2，或者每个执行者一个任务。现在您可以看到，两个执行者都分别获得一个任务。在性能方面，两种情况完全相同，因为任务在执行者线程内顺序运行。更多的执行者意味着更高的并行性，更多的工作人员意味着更有效地使用CPU和RAM等资源。内存分配是在工作人员级别使用`worker`.`childopts`设置完成的。我们还应该监视特定工作人员进程所持有的最大内存量。这在决定工作人员的总数时起着重要作用。可以使用`ps
    -ef`选项来查看。始终保持任务和执行者的比例相同，并使用Storm UI的容量列推导出执行者数量的正确值。作为一个重要的注意事项，我们应该将较短的持续时间事务保留在螺栓中，并尝试通过将代码拆分为更多的螺栓或减少批处理大小元组来调整它。批处理大小是螺栓在单个元组传递中接收的记录数量。此外，不要因为较长的持续时间事务而阻塞喷口的`nextTuple`方法。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As this chapter approaches its end, you must have got a brief idea about the
    Nimbus, supervisor, UI, and Zookeeper processes. This chapter also taught you
    how to tune parallelism in Storm by playing with the number of workers, executors,
    and tasks. You became familiar with the important problem of distributing computation,
    that is, failures and overcoming failures by different kinds of fault tolerance
    available in the system. And most importantly, you learned how to write a "reliable"
    spout to achieve guaranteed message processing and linking in bolts.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本章接近尾声，您一定对Nimbus、supervisor、UI和Zookeeper进程有了一个简要的了解。本章还教会了您如何通过调整Storm中的并行性来玩弄工作人员、执行者和任务的数量。您熟悉了分布式计算的重要问题，即系统中可用的不同类型的容错机制来克服故障和故障。最重要的是，您学会了如何编写一个“可靠”的喷口，以实现消息处理和在螺栓中的链接的保证。
- en: The next chapter will give you information about how to build a simple topology
    using a Python library called Petrel. Petrel addresses some limitations of Storm's
    built-in Python support, providing simpler and more streamlined development.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将为您提供有关如何使用名为Petrel的Python库构建简单拓扑的信息。Petrel解决了Storm内置Python支持的一些限制，提供了更简单、更流畅的开发。
