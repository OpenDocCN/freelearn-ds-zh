- en: Building Enterprise Search Platform
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建企业搜索平台
- en: 'After learning data ingestions and data persistence approaches, let''s learn
    about searching the data. In this chapter, we will learn about the following important
    things:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习了数据摄取和数据持久化方法之后，让我们来学习如何搜索数据。在本章中，我们将学习以下重要内容：
- en: Data search techniques
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据搜索技术
- en: Building real-time search engines.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建实时搜索引擎
- en: Searching real-time, full-text data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索实时全文数据
- en: Data indexing techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据索引技术
- en: Building a real-time data search pipeline
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建实时数据搜索管道
- en: The data search concept
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据搜索的概念
- en: In our everyday life, we always keep on searching something. In the morning,
    we search for a toothbrush, newspaper, search stock prices, bus schedule, office
    bag, and so on. The list goes on and on. This search activity stops when we go
    to bed at the end of the day. We use a lot of tools and techniques to search these
    things to minimize the actual search time. We use Google to search most of the
    things such as news, stock prices, bus schedule, and anything and everything we
    need. To search a particular page of a book, we use the book's index. So, the
    point is that search is a very important activity of our life. There are two important
    concepts can be surfaced out of this, that is, search tool and search time. Just
    think of a situation where you want to know about a particular stock price of
    a company and it takes a few minutes to load that page. You will definitely get
    very annoyed. It is because the Search Time in this case is not acceptable to
    you. So then the question is, *How to reduce this search time?* We will learn
    that in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的日常生活中，我们总是在不断寻找某些东西。早上，我们寻找牙刷、报纸、搜索股价、公交时刻表、公文包等等。这个清单可以一直列下去。当我们在一天结束时上床睡觉时，这种搜索活动就停止了。我们使用很多工具和技术来搜索这些事物，以最大限度地减少实际搜索时间。我们使用谷歌搜索大多数事情，比如新闻、股价、公交时刻表以及我们需要的任何东西。为了搜索一本书的特定页面，我们使用书的索引。所以，重点是搜索是我们生活中非常重要的活动。从这个过程中可以提炼出两个重要的概念，那就是搜索工具和搜索时间。想想看，如果你想知道一家公司的特定股价，而加载这个页面需要几分钟，你肯定会非常烦恼。这是因为在这种情况下，搜索时间对你来说是不可接受的。那么问题来了，*如何减少搜索时间？*
    我们将在本章中学习这一点。
- en: The need for an enterprise search engine
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 企业搜索引擎的需求
- en: Just like we all need a tool to search our own things, every company also needs
    a search engine to build so that internal and external entities can find what
    they want.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们都需要一个工具来搜索自己的东西一样，每个公司也需要一个搜索引擎来构建，以便内部和外部实体可以找到他们想要的东西。
- en: For example, an employee has to search for his/her PTO balance, paystub of a
    particular month, and so on. The HR department may search for employees who are
    in finance group or so. In an e-commerce company, a product catalog is the most
    searchable object. It is a very sensitive object because it directly impacts the
    revenue of the company. If a customer wants to buy a pair of shoes, the first
    thing he/she can do is search the company product catalog. If the search time
    is more than a few seconds, the customer may lose interest in the product. It
    may also be possible that the same customer goes to another website to buy a pair
    of shoes, resulting in a loss of revenue.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，员工需要搜索他的/她的年假余额、特定月份的工资条等等。人力资源部门可能需要搜索在财务组工作的员工等等。在一家电子商务公司中，产品目录是最可搜索的对象。这是一个非常敏感的对象，因为它直接影响到公司的收入。如果客户想要买一双鞋，他/她首先可以做的就是搜索公司的产品目录。如果搜索时间超过几秒钟，客户可能会对产品失去兴趣。也可能发生这样的情况，同一个客户会去另一个网站买鞋，从而导致收入损失。
- en: 'It appears that even with all the tech and data in the world, we can''t do
    much without two crucial components:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，即使拥有世界上所有的技术和数据，如果没有两个关键组件，我们也做不了太多：
- en: Data search
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据搜索
- en: Data index
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据索引
- en: Companies such as Google, Amazon, and Apple have changed the word's expectations
    of search. We all expect them to search anything, anytime, and using any tool
    such as website, mobile, and voice-activated tools like Google Echo, Alexa, and
    HomePad. We expect these tools to answer all our questions, from *How's the weather
    today?* to give me a list of gas stations near me.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，像谷歌、亚马逊和苹果这样的公司已经改变了世界对搜索的期望。我们都期望他们能够随时随地进行搜索，使用任何工具，如网站、移动设备和像谷歌Echo、Alexa和HomePad这样的语音激活工具。我们期望这些工具能够回答我们所有的问题，从“今天天气怎么样？”到给我列出附近的所有加油站。
- en: As these expectations are growing, the need to index more and more data is also
    growing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这些期望的增长，索引更多数据的需要也在增长。
- en: Tools for building an enterprise search engine
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建企业搜索引擎的工具
- en: 'The following are some popular tools/products/technologies available:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些流行的工具/产品/技术：
- en: Apache Lucene
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Lucene
- en: Elasticsearch
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: Apache Solr
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Solr
- en: Custom (in-house) search engine
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义（内部）搜索引擎
- en: In this chapter, I will focus on Elasticsearch in detail. I will discuss Apache
    Solr on a conceptual level only.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将详细讨论Elasticsearch。我将在概念层面上讨论Apache Solr。
- en: Elasticsearch
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: Elasticsearch is an open source search engine. It is based on Apache Lucene.
    It is distributed and supports multi-tenant capability. It uses schema-free JSON
    documents and has a built-in, HTTP-based web interface. It also supports analytical
    RESTful query workloads. It is a Java-based database server. Its main protocol
    is HTTP/JSON.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch是一个开源的搜索引擎。它基于Apache Lucene。它是分布式的，并支持多租户能力。它使用无模式的JSON文档，并具有基于HTTP的内置网络接口。它还支持分析RESTful查询工作负载。它是一个基于Java的数据库服务器。其主要协议是HTTP/JSON。
- en: Why Elasticsearch?
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择Elasticsearch？
- en: 'Elasticsearch is the most popular data indexing tool as of today. It is because
    of its the following features:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，Elasticsearch是最流行的数据索引工具。这是因为以下特性：
- en: It is **fast**. Data is indexed at a real-time speed.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是**快速**的。数据以实时速度索引。
- en: It is **scalable**. It scales horizontally.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是**可扩展**的。它水平扩展。
- en: It is **flexible**. It supports any data format, structured, semi-structured,
    or unstructured.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是**灵活**的。它支持任何数据格式，结构化、半结构化或非结构化。
- en: It is **distributed**. If one node fails, the cluster is still available for
    business.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是**分布式**的。如果一个节点失败，集群仍然可用于业务。
- en: 'It supports data search query in any language: Java, Python Ruby, C#, and so
    on.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持任何语言的数据搜索查询：Java、Python、Ruby、C#等等。
- en: It has a **Hadoop connector,** which facilitates smooth communication between
    Elasticsearch and Hadoop.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有一个**Hadoop连接器**，这促进了Elasticsearch和Hadoop之间顺畅的通信。
- en: It supports robust data **aggregation** on huge datasets to find trends and
    patterns.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持在大型数据集上进行强大的数据**聚合**，以找到趋势和模式。
- en: The **Elastic stack** (Beats, Logstash, Elasticsearch, and Kibana) and X-Pack
    offers out-of-the-box support for data ingestion, data indexing, data visualization,
    data security, and monitoring.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Elastic stack**（Beats、Logstash、Elasticsearch和Kibana）和X-Pack为数据摄取、数据索引、数据可视化、数据安全和监控提供开箱即用的支持。'
- en: Elasticsearch components
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch组件
- en: Before we take a deep dive, let's understand a few important components of Elasticsearch.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨之前，让我们了解Elasticsearch的一些重要组件。
- en: Index
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 索引
- en: Elasticsearch index is a collection of JSON documents. Elasticsearch is a data
    store that may contain multiple indices. Each index may be divided into one or
    many types. A type is a group of similar documents. A type may contain multiple
    documents. In terms of database analogy, an index is a database and each of its
    types is a table. Each JSON document is a row in that table.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch索引是一组JSON文档。Elasticsearch是一个可能包含多个索引的数据存储。每个索引可以分成一个或多个类型。类型是相似文档的集合。一个类型可以包含多个文档。从数据库的类比来说，索引是一个数据库，而它的每个类型都是一个表。每个JSON文档是表中的一行。
- en: Indices created in Elasticsearch 6.0.0 or later may only contain a single mapping
    type.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在Elasticsearch 6.0.0或更高版本中创建的索引可能只能包含一个映射类型。
- en: Mapping types will be completely removed in Elasticsearch 7.0.0.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 映射类型将在Elasticsearch 7.0.0中完全移除。
- en: Document
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档
- en: 'Document in Elasticsearch means a JSON document. It is a basic unit of data
    to be stored in an index. An index comprises multiple documents. In the RDBMS
    world, a document is nothing but a row in a table. For example, a customer document
    may look like the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在Elasticsearch中的文档意味着一个JSON文档。它是存储在索引中的基本数据单元。一个索引由多个文档组成。在关系型数据库管理系统（RDBMS）的世界里，文档不过是表中的一行。例如，一个客户文档可能看起来像以下这样：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Mapping
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 映射
- en: Mapping is schema definition of an index. Just like a database, we have to define
    a data structure of a table. We have to create a table, its columns, and column
    data types. In Elasticsearch, we have define a structure of an index during its
    creation. We may have to define which field can be indexed, searchable, and storable.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 映射是索引的架构定义。就像数据库一样，我们必须定义一个表的数据库结构。我们必须创建一个表，它的列和列数据类型。在Elasticsearch中，我们必须在其创建期间定义索引的结构。我们可能需要定义哪些字段可以被索引、可搜索和可存储。
- en: The good news is that, Elasticsearch supports **dynamic mapping**. It means
    that mapping is not mandatory at index creation time. An index can be created
    without mapping. When a document is sent to Elasticsearch for indexing, Elasticsearch
    automatically defines the data structure of each field and make each field a searchable
    field.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，Elasticsearch 支持 **动态映射**。这意味着在索引创建时映射不是必需的。可以创建没有映射的索引。当文档被发送到 Elasticsearch
    进行索引时，Elasticsearch 会自动定义每个字段的 数据结构，并将每个字段设置为可搜索字段。
- en: Cluster
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群
- en: 'Elasticsearch is a collection of nodes (servers). Each node may store part
    of the data in index and provides federated indexing and search capabilities across
    all nodes. Each cluster has a unique name, `elasticsearch`, by default. A cluster
    is divided into multiple types of nodes, namely Master Node and Data Node. But
    an Elasticsearch cluster can be created using just one node having both Master
    and Data nodes installed on the same node:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 是由节点（服务器）组成的集合。每个节点可能存储索引中的部分数据，并为所有节点提供联邦索引和搜索功能。每个集群默认都有一个独特的名称，即
    `elasticsearch`。集群被划分为多种类型的节点，即主节点和数据节点。但可以使用仅安装了主节点和数据节点的单个节点来创建 Elasticsearch
    集群：
- en: '**Master node**: This controls the entire cluster. There can be more than one
    master node in a cluster (three are recommended). Its main function is index creation
    or deletion and allocation of shards (partitions) to data nodes.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主节点**：它控制整个集群。集群中可以有多个主节点（建议三个）。其主要功能是索引创建或删除以及将分片（分区）分配给数据节点。'
- en: '**Data node**: This stores the actual index data in shards. They support all
    data-related operations such as aggregations, index search, and so on.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据节点**：它存储实际的索引数据在分片中。它们支持所有数据相关操作，如聚合、索引搜索等。'
- en: Type
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类型
- en: Documents are divided into various logical types for example, order document,
    product document, customer document, and so on. Instead of creating three separate
    order, product, and customer indices, a single index can be logically divided
    into order, product and customer types. In RDBMS analogy, a type is nothing but
    a Table in a database. So, a type is a logical partition of an index.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 文档被划分为各种逻辑类型，例如订单文档、产品文档、客户文档等。而不是创建三个单独的订单、产品、客户索引，单个索引可以逻辑上划分为订单、产品、客户类型。在
    RDBMS 类比中，类型就是数据库中的一个表。因此，类型是索引的逻辑分区。
- en: Type is deprecated in Elasticsearch version 6.0.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 类型在 Elasticsearch 6.0 版本中已弃用。
- en: How to index documents in Elasticsearch?
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在 Elasticsearch 中索引文档？
- en: Let's learn how Elasticsearch actually works by indexing these three sample
    documents. While learning this, we will touch upon a few important functions/concepts
    of Elasticsearch.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过索引这三个示例文档来学习 Elasticsearch 实际是如何工作的。在学习这个过程中，我们将涉及到 Elasticsearch 的几个重要功能/概念。
- en: 'These are the three sample JSON documents to be indexed:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是要索引的三个示例 JSON 文档：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Elasticsearch installation
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch 安装
- en: First things first. Let's install Elasticsearch.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的。让我们安装 Elasticsearch。
- en: Please do the following steps to install Elasticsearch on your server. It is
    assumed that you are installing Elasticsearch using CentOS 7 on your server.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照以下步骤在您的服务器上安装 Elasticsearch。假设您正在使用 CentOS 7 在服务器上安装 Elasticsearch。
- en: What minimum Hardware is required?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 需要的最小硬件配置是什么？
- en: '**RAM**: 4 GB'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RAM**：4 GB'
- en: '**CPU**: 2'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CPU**：2 核'
- en: 'Which JDK needs to be installed? We need JDK 8\. If you don''t have JDK 8 installed
    on your server, do the following steps to install JDK 8:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 需要安装哪个 JDK？我们需要 JDK 8。如果您服务器上没有安装 JDK 8，请按照以下步骤安装 JDK 8：
- en: 'Change to home folder:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到主目录：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Download JDK RPM:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 JDK RPM：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Install RMP using YUM (it is assumed that you have `sudo` access):'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 YUM 安装 RPM（假设您有 `sudo` 权限）：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Since, we have installed JDK 8 on our server successfully, let's start installing
    Elasticsearch.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经成功在服务器上安装了 JDK 8，让我们开始安装 Elasticsearch。
- en: Installation of Elasticsearch
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch 安装
- en: 'For detailed installation steps, please refer to the following URL:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于详细的安装步骤，请参阅以下 URL：
- en: '[**https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html**](https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[**https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html**](https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html)'
- en: 'The RPM for Elasticsearch v6.2.3 can be downloaded from the website and installed
    as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Elasticsearch v6.2.3 的 RPM 可以从网站上下载，并按照以下步骤安装：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To configure Elasticsearch to start automatically when the system boots up,
    run the following commands.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要配置 Elasticsearch 在系统启动时自动启动，请运行以下命令。
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Elasticsearch can be started and stopped as follows:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Elasticsearch可以如下启动和停止：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The main configuration file is located in the config folder called `elasticsearch.yml`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 主要配置文件位于名为`elasticsearch.yml`的`config`文件夹中。
- en: 'Let''s do the following initial config changes in `elasticsearch.yml`. Find
    and replace the following parameters:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在`elasticsearch.yml`中进行以下初始配置更改。找到并替换以下参数：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now start Elasticsearch:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在启动Elasticsearch：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Check whether Elasticsearch is running using the following URL:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下URL检查Elasticsearch是否正在运行：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will get the following response:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下响应：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, our Elasticsearch is working fine. Let's create an index to store our documents.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的Elasticsearch运行良好。让我们创建一个索引来存储我们的文档。
- en: Create index
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建索引
- en: 'We will use the following `curl` command to create our first index named `my_index`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下`curl`命令创建我们的第一个名为`my_index`的索引：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will get this response:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下响应：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the index creation URL, we used settings, shards, and replica. Let's understand
    what is meant by shard and replica.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引创建URL中，我们使用了设置、分片和副本。让我们了解分片和副本的含义。
- en: Primary shard
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主分片
- en: We have created index with three shards. It means Elasticsearch will divide
    index into three partitions. Each partition is called a **shard**. Each shard
    is a full-fledged, independent Lucene index. The basic idea is that Elasticsearch
    will store each shard on a separate data node to increase the scalability. We
    have to mention how many shards we want at the time of index creation. Then, Elasticsearch
    will take care of it automatically. During document search, Elasticsearch will
    aggregate all documents from all available shards to consolidate the results so
    as to fulfill a user search request. It is totally transparent to the user. So
    the concept is that index can be divided into multiple shards and each shard can
    be hosted on each data node. The placement of shards will be taken care of by
    Elasticsearch itself. If we don't specify the number of shards in the index creation
    URL, Elasticsearch will create five shards per index by default.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已创建包含三个分片的索引。这意味着Elasticsearch会将索引划分为三个分区。每个分区称为**分片**。每个分片都是一个完整的、独立的Lucene索引。基本思想是Elasticsearch会将每个分片存储在不同的数据节点上，以增加可伸缩性。在创建索引时，我们必须指定我们想要的分片数量。然后，Elasticsearch将自动处理。在文档搜索过程中，Elasticsearch将聚合来自所有可用分片的所有文档，以合并结果以满足用户的搜索请求。这对用户来说是完全透明的。因此，概念是索引可以被划分为多个分片，并且每个分片可以托管在每个数据节点上。分片的放置将由Elasticsearch本身负责。如果我们没有在创建索引的URL中指定分片数量，Elasticsearch将默认为每个索引创建五个分片。
- en: Replica shard
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 副本分片
- en: We have created index with one replica. It means Elasticsearch will create one
    copy (replica) of each shard and place each replica on separate data node other
    than the shard from which it is copied. So, now there are two shards, primary
    shard (the original shard) and replica shard (the copy of the primary shard).
    During a high volume of search activity, Elasticsearch can provide query results
    either from primary shards or from replica shards placed on different data nodes.
    This is how Elasticsearch increases the query throughput because each search query
    may go to different data nodes.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已创建包含一个副本的分片索引。这意味着Elasticsearch将为每个分片创建一个副本（副本），并将每个副本放置在除原始分片外的单独数据节点上。因此，现在有两个分片：主分片（原始分片）和副本分片（主分片的副本）。在高量搜索活动期间，Elasticsearch可以从放置在不同数据节点上的主分片或副本分片提供查询结果。这就是Elasticsearch如何通过每个搜索查询可能访问不同的数据节点来增加查询吞吐量的方式。
- en: In the summary, both, primary shards and replica shards provide horizontal scalability
    and throughput. It scales out your search volume/throughput since searches can
    be executed on all replicas in parallel.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，主分片和副本分片都提供了横向可伸缩性和吞吐量。它通过在所有副本上并行执行搜索来扩展您的搜索量/吞吐量。
- en: Elasticsearch is a distributed data store. It means data can be divided into
    multiple data nodes. For example, assume if we have just one data node and we
    keep on ingesting and indexing documents on the same data node, it may possible
    that after reaching out the hardware capacity of that node, we will not be to
    ingest documents. Hence, in order to accommodate more documents, we have to add
    another data node to the existing Elasticsearch cluster. If we add another data
    node, Elasticsearch will re-balance the shards to the newly created data node.
    So now, user search queries can be accommodated to both the data nodes. If we
    created one replica shard, then two replicas per shard will be created and placed
    on these two data nodes. Now, if one of the data nodes goes down, then still user
    search queries will be executed using just one data node.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch是一个分布式数据存储。这意味着数据可以被分成多个数据节点。例如，假设我们只有一个数据节点，并且我们一直在同一个数据节点上摄取和索引文档，那么在达到该节点的硬件容量后，我们可能无法再摄取文档。因此，为了容纳更多文档，我们必须向现有的Elasticsearch集群添加另一个数据节点。如果我们添加另一个数据节点，Elasticsearch将重新平衡分片到新创建的数据节点。因此，现在用户搜索查询可以同时适应两个数据节点。如果我们创建了一个副本分片，那么每个分片将创建两个副本并放置在这两个数据节点上。现在，如果一个数据节点出现故障，用户搜索查询仍然可以使用一个数据节点执行。
- en: 'This picture shows how user search queries are executed from both the data
    nodes:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片展示了用户搜索查询是如何从数据节点执行的：
- en: '![](img/b8d46b2c-601b-444e-90d9-3b2912de3bd5.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8d46b2c-601b-444e-90d9-3b2912de3bd5.png)'
- en: 'The following picture shows that even if data nodes **A** goes down, still,
    user queries are executed from data node **B**:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片显示，即使数据节点 **A** 崩溃，用户查询仍然可以从数据节点 **B** 执行：
- en: '![](img/29ff9261-0355-4bbb-8702-cc2758b6827d.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29ff9261-0355-4bbb-8702-cc2758b6827d.png)'
- en: 'Let''s verify the newly created index:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证新创建的索引：
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will get the following response:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获得以下响应：
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s understand the response:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解响应：
- en: '**Health:** This means the overall cluster health is yellow. There are three
    statuses: green, yellow, and red. The status `Green`" means the cluster is fully
    functional and everything looks good. The status "Yellow" means cluster is fully
    available but some of the replicas are not allocated yet. In our example, since
    we are using just one node and 5 shards and 1 replica each, Elasticsearch will
    not allocate all the replicas of all the shards on just one data node. The cluster
    status "Red" means cluster is partially available and some datasets are not available.
    The reason may be that the data node is down or something else.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Health:** 这表示集群的整体健康状态为黄色。有三个状态：绿色、黄色和红色。状态 `Green` 表示集群完全功能正常，一切看起来都很好。状态
    "Yellow" 表示集群完全可用，但一些副本尚未分配。在我们的例子中，因为我们只使用一个节点和5个分片以及每个分片1个副本，Elasticsearch不会将所有分片的副本都分配到单个数据节点上。集群状态
    "Red" 表示集群部分可用，某些数据集不可用。原因可能是数据节点已关闭或其他原因。'
- en: '**Status**: `Open`. It means the cluster is open for business.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Status**: `Open`。这意味着集群对业务开放。'
- en: '**Index** : Index name. In our example, the index name is `my_index`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Index** : 索引名称。在我们的例子中，索引名称是 `my_index`。'
- en: '**Uuid** : This is unique index ID.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Uuid** : 这是唯一的索引ID。'
- en: '**Pri** : Number of primary shards.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pri** : 主分片的数量。'
- en: '**Rep** : Number of replica shards.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rep** : 副本分片的数量。'
- en: '**docs.count** : Total number of documents in an index.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**docs.count** : 索引中的文档总数。'
- en: '**docs.deleted** : Total number of documents deleted so far from an index.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**docs.deleted** : 从索引中至今已删除的文档总数。'
- en: '**store.size** : The store size taken by primary and replica shards.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**store.size** : 主分片和副本分片占用的存储大小。'
- en: '**pri.store.size** : The store size taken only by primary shards.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pri.store.size** : 仅由主分片占用的存储大小。'
- en: Ingest documents into index
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文档导入索引
- en: 'The following `curl` command can be used to ingest a single document in the
    `my_index` index:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 `curl` 命令可以用来将单个文档摄取到 `my_index` 索引中：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the previous command, we use a type called `customer`, which is a logical
    partition of an index. In a RDBMS analogy, a type is like a table in Elasticsearch.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的命令中，我们使用了一个名为 `customer` 的类型，它是索引的逻辑分区。在关系型数据库管理系统（RDBMS）的类比中，类型就像Elasticsearch中的表。
- en: Also, we used the number `1` after the type customer. It is an ID of a customer.
    If we omit it, then Elasticsearch will generate an arbitrary ID for the document.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在类型customer后面使用了数字 `1`。它是客户的ID。如果我们省略它，那么Elasticsearch将为文档生成一个任意的ID。
- en: We have multiple documents to be inserted into the `my_index` index. Inserting
    documents one by one in the command line is very tedious and time consuming. Hence,
    we can include all the documents in a file and do a bulk insert into `my_index`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多个文档需要插入到 `my_index` 索引中。在命令行中逐个插入文档非常繁琐且耗时。因此，我们可以将所有文档包含在一个文件中，然后批量插入到
    `my_index`。
- en: 'Create a `sample.json` file and include all the three documents:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 `sample.json` 文件并包含所有三个文档：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Bulk Insert
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量插入
- en: 'Let''s ingest all the documents in the file `sample.json` at once using the
    following command:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令一次性将文件 `sample.json` 中的所有文档导入：
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s verify all the records using our favorite browser. It will show all
    the three records:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们喜欢的浏览器验证所有记录。它将显示所有三个记录：
- en: '[PRE19]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Document search
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档搜索
- en: 'Since we have documents in our `my_index` index, we can search these documents:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在 `my_index` 索引中已有文档，我们可以搜索这些文档：
- en: 'Find out a document where `city = " Los Angeles?` and query is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 查找一个 `city = "Los Angeles"` 的文档，查询如下：
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Response:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If we analyze the response, we can see that the source section gives a back
    the document we were looking for. The document is in the index `my_index`, `"_type"
    : "customer"`, `"_id" : "3"`. Elasticsearch searches all `three _shards` successfully.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们分析响应，我们可以看到源部分返回了我们正在寻找的文档。该文档位于索引 `my_index` 中，`"_type" : "customer"`，`"_id"
    : "3"`。Elasticsearch 成功搜索了所有 `三个分片`。'
- en: 'Under the `hits` section, there is a field called `_score`. Elasticsearch calculates
    the relevance frequency of each field within a document and stores it in index.
    It is called the weight of the document. This weight is calculated based on four
    important factors: term frequency, inverse frequency, document frequency, and
    field length frequency. This brings up another question, *How does Elasticsearch
    index a document?*'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `hits` 部分，有一个名为 `_score` 的字段。Elasticsearch 计算文档中每个字段的关联频率并将其存储在索引中。这被称为文档的权重。这个权重是基于四个重要因素计算的：词频、逆频率、文档频率和字段长度频率。这又引出了另一个问题，*Elasticsearch
    是如何索引文档的？*
- en: 'For example, we have the following four documents to index in Elasticsearch:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们有以下四个文档需要索引到 Elasticsearch 中：
- en: I love Elasticsearch
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我喜欢 Elasticsearch
- en: Elasticsearch is a document store
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch 是一个文档存储
- en: HBase is key value data store
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBase 是键值数据存储
- en: I love HBase
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我喜欢 HBase
- en: '| **Term** | **Frequency** | **Document No.** |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| **Term** | **Frequency** | **Document No.** |'
- en: '| a | 2 | 2 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| a | 2 | 2 |'
- en: '| index | 1 | 2 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| index | 1 | 2 |'
- en: '| Elasticsearch | 2 | 1,2 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Elasticsearch | 2 | 1,2 |'
- en: '| HBase | 2 | 1 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| HBase | 2 | 1 |'
- en: '| I | 2 | 1,4 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| I | 2 | 1,4 |'
- en: '| is | 2 | 2,3 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| is | 2 | 2,3 |'
- en: '| Key | 1 | 3 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Key | 1 | 3 |'
- en: '| love | 2 | 1,4 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| love | 2 | 1,4 |'
- en: '| store | 2 | 2,3 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| store | 2 | 2,3 |'
- en: '| value | 1 | 3 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| value | 1 | 3 |'
- en: When we ingest three documents in Elasticsearch, an Inverted Index is created,
    like the following.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在 Elasticsearch 中导入三个文档时，会创建一个倒排索引，如下所示。
- en: 'Now, if we want to query term Elasticsearch, then only two documents need to
    be searched: 1 and 2\. If we run another query to find *love Elasticsearch*, then
    three documents need to be searched (documents 1,2, and 4) before sending the
    results from only the first document.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想查询术语 Elasticsearch，那么只需要搜索两个文档：1 和 2。如果我们运行另一个查询来查找 *love Elasticsearch*，那么在发送仅来自第一个文档的结果之前，需要搜索三个文档（文档
    1、2 和 4）。
- en: Also, there is one more important concept we need to understand.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个重要的概念我们需要理解。
- en: Meta fields
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元字段
- en: 'When we ingest a document into index, Elasticsearch adds a few meta fields
    to each index document. The following is the list of meta fields with reference
    to our sample `my_index`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将文档导入索引时，Elasticsearch 会为每个索引文档添加一些元字段。以下是我们示例 `my_index` 的元字段列表：
- en: '`_index`: Name of the index. `my_index`.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_index`：索引的名称。`my_index`。'
- en: '`_type`: Mapping type. "customer" (deprecated in version 6.0).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_type`：映射类型。"customer"（在 6.0 版本中已弃用）。'
- en: '`_uid`: `_type + _id` (deprecated in version 6.0).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_uid`：`_type + _id`（在 6.0 版本中已弃用）。'
- en: '`_id`: `document_id` (1).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_id`：`document_id`（1）。'
- en: '`_all`: This concatenates all the fields of an index into a searchable string
    (deprecated in version 6.0).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_all`：将索引中所有字段的值连接成一个可搜索的字符串（在 6.0 版本中已弃用）。'
- en: '`_ttl`: Life a document before it can be automatically deleted.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_ttl`：在自动删除之前，文档的生命周期。'
- en: '`_timestamp`: Provides a timestamp for a document.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_timestamp`：为文档提供时间戳。'
- en: '`_source`: This is an actual document, which is automatically indexed by default.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_source`：这是一个实际的文档，默认情况下会自动索引。'
- en: Mapping
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 映射
- en: In RDBMS analogy, mapping means defining a table schema. We always define a
    table structure, that is, column data types. In Elasticsearch, we also need to
    define the data type for each field. But then comes another question. Why did
    we not define it before when we ingested three documents into the `my_index` index?
    The answer is simple. Elasticsearch doesn't care. It is claimed that *Elasticsearch
    is a schema-less data model*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在关系型数据库的类比中，映射意味着定义表模式。我们总是定义一个表结构，即列数据类型。在Elasticsearch中，我们还需要为每个字段定义数据类型。但接下来又出现了一个问题。为什么我们在将三个文档导入`my_index`索引之前没有定义它？答案是简单的。Elasticsearch并不关心。据说*Elasticsearch是一个无模式的数据库模型*。
- en: If we don't define a mapping, Elasticsearch dynamically creates a mapping for
    us by defining all fields as text. Elasticsearch is intelligent enough to find
    out date fields to assign the `date` data type to them.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有定义映射，Elasticsearch会动态地为我们创建一个映射，将所有字段定义为文本类型。Elasticsearch足够智能，能够识别日期字段并将`date`数据类型分配给它们。
- en: 'Let''s find the existing dynamic mapping of index `my_index`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查找索引`my_index`的现有动态映射：
- en: '[PRE22]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Response:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE23]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Elasticsearch supports two mapping types as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch支持以下两种映射类型：
- en: Static mapping
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态映射
- en: Dynamic mapping
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态映射
- en: Static mapping
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 静态映射
- en: In static mapping, we always know our data and we define the appropriate data
    type for each field. Static mapping has to be defined at the time of index creation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在静态映射中，我们始终知道我们的数据，并为每个字段定义适当的数据类型。静态映射必须在索引创建时定义。
- en: Dynamic mapping
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态映射
- en: We have already used dynamic mapping for our documents in our example. Basically,
    we did not define any data type for any field. But when we ingested documents
    using `_Bulk` load, Elasticsearch transparently defined `text` and `date` data
    types appropriately for each field. Elasticsearch intelligently found our `Birthdate` as
    a date field and assigned the `date` data type to it.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在我们的示例中使用了动态映射。基本上，我们没有为任何字段定义任何数据类型。但当我们使用`_Bulk`加载文档时，Elasticsearch透明地定义了每个字段的`text`和`date`数据类型。Elasticsearch智能地识别出我们的`Birthdate`字段是一个日期字段，并将其分配了`date`数据类型。
- en: Elasticsearch-supported data types
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch支持的数据类型
- en: 'The following spreadsheet summarizes the available data types in Elasticsearch:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了Elasticsearch中可用的数据类型：
- en: '| **Common** | **Complex** | **Geo** | **Specialized** |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **常见** | **复杂** | **地理** | **专用** |'
- en: '| String | Array | Geo_Point | `ip` |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 字符串 | 数组 | 地理点 | `ip` |'
- en: '| Keyword | Object (single Json) | Geo_Shape | `completion` |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 关键词 | 对象（单个JSON） | 地理形状 | `completion` |'
- en: '| Date | Nested (Json array) |  | `token_count` |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 嵌套（JSON数组） |  | `token_count` |'
- en: '| Long |  |  | `join` |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 长整型 |  |  | `join` |'
- en: '| Short |  |  | `percolator` |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 短整型 |  |  | `percolator` |'
- en: '| Byte |  |  | `murmur3` |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 字节 |  |  | `murmur3` |'
- en: '| Double |  |  |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 双精度浮点数 |  |  |  |'
- en: '| Float |  |  |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 浮点数 |  |  |  |'
- en: '| Boolean |  |  |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 布尔型 |  |  |  |'
- en: '| Binary |  |  |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 二进制 |  |  |  |'
- en: '| Integer_range |  |  |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 整数范围 |  |  |  |'
- en: '| Float_range |  |  |  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 浮点范围 |  |  |  |'
- en: '| Long_range |  |  |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 长整型范围 |  |  |  |'
- en: '| Double_range |  |  |  |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 双精度浮点数范围 |  |  |  |'
- en: '| Date_range |  |  |  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 日期范围 |  |  |  |'
- en: 'Most of the data types need no explanation. But the following are a few explanations
    for specific data types:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据类型无需解释。但以下是对一些特定数据类型的解释：
- en: '**Geo-Point**:You can define latitude and longitude points here'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Geo-Point**：您可以在此处定义纬度和经度点'
- en: '**Geo-Shape**:This is for defining shapes'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Geo-Shape**：用于定义形状'
- en: '**Completion**:This data type is for defining auto completion of words.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Completion**：此数据类型用于定义单词自动补全。'
- en: '**Join**: To define parent/child relationships'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Join**：定义父子关系'
- en: '**Percolator**:This is for query-dsl'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Percolator**：这是用于查询-dsl。'
- en: '**Murmur3**:During index time, it is for calculations hash value and store
    it into index'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Murmur3**：在索引时，用于计算哈希值并将其存储到索引中。'
- en: Mapping example
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 映射示例
- en: 'Let''s re-create another index, `second_index`, which is similar to our `first_index` with
    static mapping, where we will define the data type of each field separately:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新创建另一个索引，`second_index`，它与我们的`first_index`类似，具有静态映射，我们将分别定义每个字段的类型：
- en: '[PRE24]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Let's understand the preceding mapping. We disable the `_source` field for the
    customer type. It means, we get rid of the default behavior, where Elasticsearch
    stores and indexes the document by default. Now, since we have disabled it, we
    will deal with each and every field separately to decide whether that field should
    be indexed stored or both.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解前面的映射。我们禁用了客户类型的`_source`字段。这意味着，我们摆脱了Elasticsearch默认存储和索引文档的行为。现在，由于我们已禁用它，我们将单独处理每个字段，以决定该字段是否应该被索引、存储或两者兼有。
- en: So, in the preceding example, we want to store only three fields, `name`, `state`
    and `zip`. Also, we don't want to index the `state` and `zip` fields. It means
    `state` and `zip` fields are not searchable.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在前面的示例中，我们只想存储三个字段，`name`、`state`和`zip`。我们也不希望索引`state`和`zip`字段。这意味着`state`和`zip`字段是不可搜索的。
- en: Analyzer
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析器
- en: We have already learned about an inverted index. We know that Elasticsearch
    stores a document into an inverted index. This transformation is known as analysis.
    This is required for a successful response of the index search query.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了倒排索引。我们知道Elasticsearch将文档存储到倒排索引中。这种转换称为分析。这是成功响应索引搜索查询所必需的。
- en: Also, many of the times, we need to use some kind of transformation before sending
    that document to Elasticsearch index. We may need to change the document to lowercase,
    stripping off HTML tags if any from the document, remove white space between two
    words, tokenize the fields based on delimiters, and so on.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，很多时候，在将文档发送到Elasticsearch索引之前，我们需要进行某种类型的转换。我们可能需要将文档转换为小写，如果有的话，移除文档中的HTML标签，删除两个单词之间的空白，根据分隔符对字段进行分词等。
- en: 'Elasticsearch offers the following built-in analyzers:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch提供了以下内置分析器：
- en: '**Standard analyzer**:It is a default analyzer. This uses standard tokenizer
    to divide text. It normalizes tokens, lowercases tokens, and also removes unwanted
    tokens.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准分析器**：这是一个默认分析器。它使用标准分词器来分割文本。它规范化标记，将标记转换为小写，并删除不需要的标记。'
- en: '**Simple analyzer**:This analyzer is composed of lowercase tokenizer.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单分析器**：这个分析器由小写分词器组成。'
- en: '**Whitespace analyzer**: This uses the whitespace tokenizer to divide text
    at spaces.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空白分析器**：它使用空白分词器在空格处分割文本。'
- en: '**Language analyzers**:Elasticsearch provides many language-specific analyzers
    such as English, and so on.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言分析器**：Elasticsearch提供了许多特定于语言的分词器，例如英语等。'
- en: '**Fingerprint analyzer**:The fingerprint analyzer is a specialist analyzer.
    It creates a fingerprint, which can be used for duplicate detection.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指纹分析器**：指纹分析器是一个专业分析器。它创建一个指纹，可以用于重复检测。'
- en: '**Pattern analyzer**:The pattern analyzer uses a regular expression to split
    the text into terms.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式分析器**：模式分析器使用正则表达式将文本分割成术语。'
- en: '**Stop analyzer**:This uses letter tokenizer to divide text. It removes stop
    words from token streams. for example, all stop words like a, an, the, is and
    so on.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停用分析器**：它使用字母分词器来分割文本。它从标记流中删除停用词。例如，所有停用词如a、an、the、is等。'
- en: '**Keyword analyzer**:This analyzer tokenizes an entire stream as a single token.
    It can be used for zip code.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键词分析器**：这个分析器将整个流作为一个单独的标记进行分词。它可以用于邮政编码。'
- en: '**Character filter**: Prepare a string before it is tokenize. Example: remove
    html tags.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字符过滤器**：在标记之前准备字符串。例如：移除HTML标签。'
- en: '**Tokenizer**: MUST have a single tokenizer. It''s used to break up the string
    into individual terms or tokens.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词器**：必须有一个分词器。它用于将字符串分解成单个术语或标记。'
- en: '**Token filter**: Change, add or remove tokens. Stemmer is a token filter,
    it is used to get base of word, for example: learned, learning => learn'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记过滤器**：更改、添加或删除标记。词干提取器是一个标记过滤器，用于获取单词的基本形式，例如：learned, learning => learn'
- en: 'Example of atandard analyzer:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 标准分析器的示例：
- en: '[PRE25]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Response:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE26]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Example of simple analyzer:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 简单分析器的示例：
- en: '[PRE27]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Response:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE28]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Elasticsearch stack components
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch堆栈组件
- en: The Elasticsearch stack consists of following
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch堆栈由以下组成
- en: Beats
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beats
- en: Logstash
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logstash
- en: Elasticsearch
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: Kibana
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kibana
- en: Let's study them in brief.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要地研究它们。
- en: Beats
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Beats
- en: Please refer to the following URL to know more about beats: [https://www.elastic.co/products/beats](https://www.elastic.co/products/beats).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下URL了解更多关于节拍的信息：[https://www.elastic.co/products/beats](https://www.elastic.co/products/beats)。
- en: Beats are lightweight data shippers. Beats are installed on to servers as agents.
    Their main function is collect the data and send it to either Logstash or Elasticsearch.
    We can configure beats to send data to Kafka topics also.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Beats是轻量级的数据传输工具。Beats作为代理安装在服务器上。它们的主要功能是收集数据并将其发送到Logstash或Elasticsearch。我们还可以配置Beats将数据发送到Kafka主题。
- en: 'There are multiple beats. Each beat is meant for collecting specific datasets
    and metrics. The following are various types of Beats:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个节拍。每个节拍都旨在收集特定的数据集和指标。以下是一些节拍类型：
- en: '**Filebeat**:For collection of log files. They simplify the collection, parsing,
    and visualization of common log formats down to a single command. Filebeat comes
    with internal modules (auditd, Apache, nginx, system, and MySQL).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Filebeat**：用于收集日志文件。它们将收集、解析和可视化常见日志格式简化为单个命令。Filebeat 内置了内部模块（auditd、Apache、nginx、系统、MySQL）。'
- en: '**Metricbeat**:For collection of metrics. They collect metrics from any systems
    and services, for example, memory, COU, and disk. Metricbeat is a lightweight
    way to send system and service statistics.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Metricbeat**：用于收集指标。它们从任何系统和服务中收集指标，例如内存、CPU 和磁盘。Metricbeat 是一种轻量级的方式，用于发送系统和服务的统计信息。'
- en: '**Packetbeat**: This is for collection of network data. Packetbeat is a lightweight
    network packet analyzer that sends data to Logstash or Elasticsearch.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Packetbeat**：用于收集网络数据。Packetbeat 是一个轻量级的网络数据包分析器，它将数据发送到 Logstash 或 Elasticsearch。'
- en: '**Winlogbeat**:For collection of Windows event data. Winlogbeat live-streams
    Windows event logs to Elasticsearch and Logstash.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Winlogbeat**：用于收集 Windows 事件数据。Winlogbeat 将 Windows 事件日志实时流式传输到 Elasticsearch
    和 Logstash。'
- en: '**Auditbeat**:For collection of audit data. Auditbeat collects audit framework
    data.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Auditbeat**：用于收集审计数据。Auditbeat 收集审计框架数据。'
- en: '**Heartbeat**:For collection of uptime monitoring data. Heartbeat ships this
    information and response time to Elasticsearch.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**心跳（Heartbeat）**：用于收集在线时间监控数据。心跳将此信息和响应时间发送到 Elasticsearch。'
- en: 'Installation of Filebeat:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Filebeat：
- en: '[PRE29]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Logstash
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Logstash
- en: Logstash is a lightweight, open source data processing pipeline. It allows collecting
    data from a wide variety of sources, transforming it on the fly, and sending it
    to any desired destination.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash 是一个轻量级、开源的数据处理管道。它允许从各种来源收集数据，实时转换，并将其发送到任何期望的目的地。
- en: It is most often used as a data pipeline for Elasticsearch, a popular analytics
    and search engine. Logstash is a popular choice for loading data into Elasticsearch
    because of its tight integration, powerful log processing capabilities, and over
    200 prebuilt open source plugins that can help you get your data indexed the way
    you want it.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 它通常用作 Elasticsearch 的数据管道，Elasticsearch 是一个流行的分析和搜索引擎。Logstash 由于其紧密集成、强大的日志处理能力以及超过
    200 个预构建的开源插件，这些插件可以帮助您以您想要的方式索引数据，因此成为加载数据到 Elasticsearch 的热门选择。
- en: 'The following is a structure of `Logstash.conf`:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为 `Logstash.conf` 的结构：
- en: '[PRE30]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Installation of Logstash:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Logstash：
- en: '[PRE31]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Kibana
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kibana
- en: Kibana is an open-source data visualization and exploration tool used for log
    and time series analytics, application monitoring, and operational intelligence
    use cases. Kibana offers tight integration with Elasticsearch, a popular analytics
    and search engine, which makes Kibana the default choice for visualizing data
    stored in Elasticsearch. Kibana is also popular due to its powerful and easy-to-use
    features such as histograms, line graphs, pie charts, heat maps, and built-in
    geospatial support**.**
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 是一款开源的数据可视化和探索工具，用于日志和时间序列分析、应用监控和运营智能用例。Kibana 与流行的分析和搜索引擎 Elasticsearch
    紧密集成，这使得 Kibana 成为可视化存储在 Elasticsearch 中的数据的默认选择。Kibana 还因其强大的易用功能而受到欢迎，例如直方图、折线图、饼图、热图和内置的地理空间支持**。**
- en: 'Installation of Kibana:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Kibana：
- en: '[PRE32]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Use case
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例
- en: 'Let''s assume that we have an application deployed on an application server.
    That application is logging on to an access log. Then how can we analyze this
    access log using a dashboard? We would like to create a real-time visualization
    of the following info:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个部署在应用服务器上的应用程序。该应用程序正在记录访问日志。那么我们如何使用仪表板分析这个访问日志呢？我们希望创建以下信息的实时可视化：
- en: Number of various response codes
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种响应代码的数量
- en: Total number of responses
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应总数
- en: List of IPs
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP 列表
- en: 'Proposed technology stack:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的技术栈：
- en: '**Filebeat**: To read access log and write to Kafka topic'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Filebeat**：读取访问日志并写入 Kafka 主题'
- en: '**Kafka:** Message queues and o buffer message'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka**：消息队列和缓冲消息'
- en: '**Logstash:** To pull messages from Kafka and write to Elasticsearch index'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Logstash**：从 Kafka 拉取消息并写入 Elasticsearch 索引'
- en: '**Elasticsearch**: For indexing messages'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Elasticsearch**：用于索引消息'
- en: '**Kibana**: Dashboard visualization'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kibana**：仪表板可视化'
- en: In order to solve this problem, we install filebeat on Appserver. Filebeat will
    read each line from the access log and write to the kafka topic in real time.
    Messages will be buffered in Kafka. Logstash will pull messages from the Kafka
    topic and write to Elasticsearch.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们在应用服务器上安装了 filebeat。Filebeat 将实时读取访问日志的每一行并将其写入 Kafka 主题。消息将在 Kafka
    中缓冲。Logstash 将从 Kafka 主题中拉取消息并写入 Elasticsearch。
- en: 'Kibana will create real-time streaming dashboard by reading messages from Elasticsearch
    index. The following is the architecture of our use case:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 将通过读取 Elasticsearch 索引中的消息来创建实时流式仪表板。以下是我们用例的架构：
- en: '**![](img/70bfafda-6da2-425d-bcbd-39bfe676906d.png)**'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/70bfafda-6da2-425d-bcbd-39bfe676906d.png)**'
- en: 'Here is the step-by-step code sample, `Acccss.log`:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一步步的代码示例，`Acccss.log`：
- en: '[PRE33]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following is the complete `Filebeat.ymal`:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为完整的 `Filebeat.ymal`：
- en: 'In the Kafka output section, we have mentioned Kafka broker details. `output.kafka`:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kafka 输出部分，我们提到了 Kafka 代理的详细信息。`output.kafka`：
- en: '[PRE34]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following is the complete `Filebeat.ymal`:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为完整的 `Filebeat.ymal`：
- en: '[PRE35]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We have to create a `logs-topic` topic in Kafka before we start ingesting messages
    into it. It is assumed that we have already installed Kafka on the server. Please
    refer to [Chapter 2](b17d8b55-1716-471b-aa8c-daf6d590172e.xhtml), *Hadoop Life
    Cycle Management* to read more about Kafka.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始将消息摄入 Kafka 之前，我们必须在 Kafka 中创建一个 `logs-topic` 主题。假设我们已经在服务器上安装了 Kafka。请参阅
    [第 2 章](b17d8b55-1716-471b-aa8c-daf6d590172e.xhtml)，*Hadoop 生命周期管理* 了解有关 Kafka
    的更多信息。
- en: 'Create logs-topic:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 logs-topic：
- en: '[PRE36]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following is the `Logstash.conf` (to read messages from Kafka and push
    them to Elasticseach):'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为 `Logstash.conf`（从 Kafka 读取消息并将其推送到 Elasticsearch）：
- en: '[PRE37]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In the Kafka section, we''ve mentioned the following things:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kafka 部分，我们提到了以下内容：
- en: '[PRE38]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the filter section**,** we are converting each message into JSON format.
    After that, we are parsing each message and dividing it into multiple fields such
    as `ip`, `timestamp`, and `status`. Also, we add the application name `myapp` field
    to each message.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤器部分**，**我们将每条消息转换为 JSON 格式。之后，我们将每条消息解析并分成多个字段，例如 `ip`、`timestamp` 和 `status`。此外，我们还在每条消息中添加了应用程序名称
    `myapp` 字段。
- en: In the output section, we are writing each message to Elasticsearch. The index
    name is `log_index-YYYY-MM-dd`.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出部分，我们将每条消息写入 Elasticsearch。索引名称为 `log_index-YYYY-MM-dd`。
- en: Summary
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you looked at the basic concepts and components of an Elasticsearch
    cluster.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了 Elasticsearch 集群的基本概念和组件。
- en: After this, we discussed how Elasticsearch indexes a document using inverted
    index. We also discussed mapping and analysis techniques. We learned how we can
    denormalize an event before ingesting into Elasticsearch. We discussed how Elasticsearch
    uses horizontal scalability and throughput. After learning about Elasticstack
    components such as Beats, Logstash, and Kibana, we handled a live use case, where
    we demonstrated how access log events can be ingested into Kafka using Filebeat.
    We developed a code to pull messages from Kafka and ingest into Elasticsearch
    using Logstash. At the end, we learned data visualization using Kibana.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们讨论了 Elasticsearch 如何使用倒排索引索引文档。我们还讨论了映射和分析技术。我们学习了如何在将事件摄入 Elasticsearch
    之前对其进行去规范化。我们讨论了 Elasticsearch 如何使用横向扩展和吞吐量。在学习了 Elasticstack 组件，如 Beats、Logstash
    和 Kibana 之后，我们处理了一个实际用例，展示了如何使用 Filebeat 将访问日志事件摄入 Kafka。我们编写了代码从 Kafka 拉取消息并使用
    Logstash 将其摄入 Elasticsearch。最后，我们学习了使用 Kibana 进行数据可视化。
- en: In the next chapter, we will see how to build analytics to design data visualization
    solutions that drive business decisions.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何构建分析来设计数据可视化解决方案，以推动业务决策。
