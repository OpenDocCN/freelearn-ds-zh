- en: Apache Spark MLlib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark MLlib
- en: MLlib is the original machine learning library that is provided with Apache
    Spark, the in-memory cluster-based open source data processing system. This library
    is still based on the RDD API. In this chapter, we will examine the functionality
    provided with the MLlib library in terms of areas such as regression, classification,
    and neural network processing. We will examine the theory behind each algorithm
    before providing working examples that tackle real problems. The example code
    and documentation on the web can be sparse and confusing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib是Apache Spark附带的原始机器学习库，Apache Spark是一个基于内存的集群式开源数据处理系统。该库仍然基于RDD API。在本章中，我们将从回归、分类和神经网络处理等领域来探讨MLlib库提供的功能。在提供解决实际问题的示例之前，我们将先探讨每种算法的理论基础。网络上的示例代码和文档可能稀少且令人困惑。
- en: 'We will take a step-by-step approach in describing how the following algorithms
    can be used and what they are capable of doing:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采取逐步的方式来描述以下算法的使用方法及其功能：
- en: Architecture
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构
- en: Classification with Naive Bayes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯进行分类
- en: Clustering with K-Means
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-Means聚类
- en: Image classification with **artificial neural networks**
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**人工神经网络**进行图像分类
- en: Architecture
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: Remember that, although Spark is used for the speed of its in-memory distributed
    processing, it doesn't provide storage. You can use the Host (local) filesystem
    to read and write your data, but if your data volumes are big enough to be described
    as big data, then it makes sense to use a cloud-based distributed storage system
    such as OpenStack Swift Object Storage, which can be found in many cloud environments
    and can also be installed in private data centers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，尽管Spark因其内存中的分布式处理速度而被使用，但它并不提供存储。您可以使用主机（本地）文件系统来读写数据，但如果您的数据量足够大，可以称之为大数据，那么使用基于云的分布式存储系统（如OpenStack
    Swift对象存储）是有意义的，该系统可以在许多云环境中找到，也可以安装在私有数据中心中。
- en: 'In case very high I/O is needed, HDFS would also be an option. More information
    on HDFS can be found here: [http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要极高的I/O性能，HDFS也是一个选项。更多关于HDFS的信息可以在这里找到：[http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)。
- en: The development environment
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发环境
- en: 'The Scala language will be used for the coding samples in this book. This is
    because, as a scripting language, it produces less code than Java. It can also
    be used from the Spark shell as well as compiled with Apache Spark applications.
    We will be using the **sbt tool** to compile the Scala code, which we have installed
    into Hortonworks HDP 2.6 Sandbox as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码示例将使用Scala语言编写。这是因为作为一种脚本语言，它产生的代码比Java少。它也可以在Spark shell中使用，以及与Apache
    Spark应用程序一起编译。我们将使用**sbt工具**来编译Scala代码，我们已经按照以下方式将其安装到Hortonworks HDP 2.6 Sandbox中：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following URL provides instructions to install sbt on other operating systems
    including Windows, Linux, and macOS: [http://www.scala-sbt.org/0.13/docs/Setup.html](http://www.scala-sbt.org/0.13/docs/Setup.html).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下URL提供了在包括Windows、Linux和macOS在内的其他操作系统上安装sbt的说明：[http://www.scala-sbt.org/0.13/docs/Setup.html](http://www.scala-sbt.org/0.13/docs/Setup.html)。
- en: We used a generic Linux account called **Hadoop**. As the previous commands
    show, we need to install `sbt` as the root account, which we have accessed via
    `sudo su -l` (switch user). We then downloaded the `sbt.rpm` file to the `/tmp`
    directory from the web-based server called `repo.scala-sbt.org` using `wget`.
    Finally, we installed the `rpm` file using the `rpm` command with the options
    `i` for install, `v` for verify, and `h` to print the hash marks while the package
    is being installed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个名为**Hadoop**的通用Linux账户。如前述命令所示，我们需要以root账户安装`sbt`，我们通过`sudo su -l`（切换用户）访问了该账户。然后，我们使用`wget`从名为`repo.scala-sbt.org`的基于网络的服务器下载了`sbt.rpm`文件到`/tmp`目录。最后，我们使用带有`i`（安装）、`v`（验证）和`h`（打印哈希标记）选项的`rpm`命令安装了`rpm`文件。
- en: 'We developed all of the Scala code for Apache Spark in this chapter on the
    Linux server, using the Linux Hadoop account. We placed each set of code within
    a subdirectory under `/home/hadoop/spark`. For instance, the following `sbt` structure
    diagram shows that the MLlib Naive Bayes code is stored in a subdirectory called
    `nbayes` under the Spark directory. What the diagram also shows is that the Scala
    code is developed within a subdirectory structure named `src/main/scala` under
    the `nbayes` directory. The files called `bayes1.scala` and `convert.scala` contain
    the Naive Bayes code that will be used in the next section:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们在Linux服务器上使用Linux Hadoop账户开发了Apache Spark的所有Scala代码。我们将每组代码放置在`/home/hadoop/spark`下的一个子目录中。例如，以下`sbt`结构图显示MLlib朴素贝叶斯代码存储在Spark目录下的名为`nbayes`的子目录中。该图还显示，Scala代码是在`nbayes`目录下的`src/main/scala`子目录结构中开发的。名为`bayes1.scala`和`convert.scala`的文件包含将在下一节中使用的朴素贝叶斯代码：
- en: '![](img/4e6ee246-91df-45b4-b62d-91322e97ea0f.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e6ee246-91df-45b4-b62d-91322e97ea0f.png)'
- en: The `bayes.sbt` file is a configuration file used by the `sbt` tool, which describes
    how to compile the Scala files within the Scala directory. (Note that if you were
    developing in Java, you would use a path of the `nbayes/src/main/java` form .)
    The contents of the `bayes.sbt` file are shown next. The `pwd` and `cat` Linux
    commands remind you of the file location and also remind you to dump the file
    contents.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`bayes.sbt`文件是`sbt`工具使用的配置文件，描述了如何编译Scala目录内的Scala文件。（注意，如果你使用Java开发，你将使用`nbayes/src/main/java`这样的路径。）接下来展示`bayes.sbt`文件的内容。`pwd`和`cat`Linux命令提醒你文件位置，并提示你查看文件内容。'
- en: The `name`, `version`, and `scalaVersion` options set the details of the project
    and the version of Scala to be used. The `libraryDependencies` options define
    where the Hadoop and Spark libraries can be located.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`name`、`version`和`scalaVersion`选项设置项目详细信息及使用的Scala版本。`libraryDependencies`选项定义Hadoop和Spark库的位置。'
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The Scala `nbayes` project code can be compiled from the `nbayes` subdirectory
    using this command:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令从`nbayes`子目录编译Scala `nbayes`项目代码：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `sbt compile` command is used to compile the code into classes. The classes
    are then placed in the `nbayes/target/scala-2.10/classes` directory. The compiled
    classes can be packaged in a JAR file with this command:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`sbt compile`命令用于将代码编译成类。这些类随后被放置在`nbayes/target/scala-2.10/classes`目录下。使用此命令可将编译后的类打包成JAR文件：'
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `sbt package` command will create a JAR file under the `nbayes/target/scala-2.10`
    directory. As we can see in the example in the **sbt structure diagram**, the
    JAR file named `naive-bayes_2.10-1.0.jar` has been created after a successful
    compile and package. This JAR file, and the classes that it contains, can then
    be used in a `spark-submit` command. This will be described later as the functionality
    in the Apache Spark MLlib module is explored.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`sbt package`命令将在`nbayes/target/scala-2.10`目录下创建一个JAR文件。如**sbt结构图**所示例中，编译打包成功后，名为`naive-bayes_2.10-1.0.jar`的JAR文件已被创建。此JAR文件及其包含的类可通过`spark-submit`命令使用。随着对Apache
    Spark MLlib模块功能的探索，这将在后面描述。'
- en: Classification with Naive Bayes
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯进行分类
- en: This section will provide a working example of the Apache Spark MLlib Naive
    Bayes algorithm. It will describe the theory behind the algorithm and will provide
    a step-by-step example in Scala to show how the algorithm may be used.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将提供一个Apache Spark MLlib朴素贝叶斯算法的实际示例。它将阐述该算法的理论基础，并提供一个逐步的Scala示例，展示如何使用该算法。
- en: Theory on Classification
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类理论
- en: 'In order to use the Naive Bayes algorithm to classify a dataset, the data must
    be linearly divisible; that is, the classes within the data must be linearly divisible
    by class boundaries. The following figure visually explains this with three datasets
    and two class boundaries shown via the dotted lines:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用朴素贝叶斯算法对数据集进行分类，数据必须是线性可分的；即数据中的类别必须能通过类别边界线性分割。下图通过三条数据集和两条虚线表示的类别边界直观解释了这一点：
- en: '![](img/5d57d6e1-50f8-4545-adac-23de9f528038.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d57d6e1-50f8-4545-adac-23de9f528038.png)'
- en: 'Naive Bayes assumes that the features (or dimensions) within a dataset are
    independent of one another; that is, they have no effect on each other. The following
    example considers the classification of e-mails as spam. If you have 100 e-mails,
    then perform the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯假设数据集内的特征（或维度）彼此独立；即它们互不影响。以下示例考虑将电子邮件分类为垃圾邮件。如果你有100封电子邮件，则执行以下操作：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s convert this example into conditional probabilities so that a Naive
    Bayes classifier can pick it up:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将此示例转换为条件概率，以便朴素贝叶斯分类器可以识别：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'What is the probability that an e-mail that contains the word buy is spam?
    Well, this would be written as *P (Spam|Buy)*. Naive Bayes says that it is described
    by the equation in the following figure:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 包含单词“buy”的电子邮件是垃圾邮件的概率是多少？这可以写为*P(Spam|Buy)*。朴素贝叶斯表示，它由以下等式描述：
- en: '![](img/3adfb0ba-fb3c-4349-9460-d2957e12edc5.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3adfb0ba-fb3c-4349-9460-d2957e12edc5.png)'
- en: 'So, using the previous percentage figures, we get the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用之前的百分比数据，我们得到以下结果：
- en: '*P(Spam|Buy) = ( 0.8 * 0.6 ) / (( 0.8 * 0.6 ) + ( 0.1 * 0.4 ) ) = ( .48 ) /
    ( .48 + .04 )*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Spam|Buy) = ( 0.8 * 0.6 ) / (( 0.8 * 0.6 ) + ( 0.1 * 0.4 ) ) = ( .48 ) /
    ( .48 + .04 )*'
- en: '*= .48 / .52 = .923*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*= .48 / .52 = .923*'
- en: This means that it is *92* percent more likely that an e-mail that contains
    the word buy is spam. That was a look at the theory; now it's time to try a real-world
    example using the Apache Spark MLlib Naive Bayes algorithm.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着包含单词“buy”的电子邮件是垃圾邮件的可能性*92%*更高。以上是理论部分；现在是时候尝试一个使用Apache Spark MLlib朴素贝叶斯算法的真实示例了。
- en: Naive Bayes in practice
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯实践
- en: The first step is to choose some data that will be used for classification.
    We have chosen some data from the UK Government data website at [http://data.gov.uk/dataset/road-accidents-safety-data](http://data.gov.uk/dataset/road-accidents-safety-data).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是选择一些用于分类的数据。我们选择了英国政府数据网站上的一些数据，网址为[http://data.gov.uk/dataset/road-accidents-safety-data](http://data.gov.uk/dataset/road-accidents-safety-data)。
- en: 'The dataset is called **Road Safety - Digital Breath Test Data 2013**, which
    downloads a zipped text file called `DigitalBreathTestData2013.txt`. This file
    contains around half a million rows. The data looks as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集名为**道路安全 - 数字呼吸测试数据2013**，下载一个名为`DigitalBreathTestData2013.txt`的压缩文本文件。该文件包含大约五十万行。数据如下所示：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In order to classify the data, we have modified both the column ...
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对数据进行分类，我们对列进行了修改...
- en: Clustering with K-Means
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用K-Means进行聚类
- en: This example will use the same test data from the previous example, but we will
    attempt to find clusters in the data using the MLlib K-Means algorithm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本例将使用与前例相同的测试数据，但我们尝试使用MLlib的K-Means算法在数据中寻找簇。
- en: Theory on Clustering
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类理论
- en: The K-Means algorithm iteratively attempts to determine clusters within the
    test data by minimizing the distance between the mean value of cluster center
    vectors, and the new candidate cluster member vectors. The following equation
    assumes dataset members that range from *X1* to *Xn*; it also assumes *K* cluster
    sets that range from *S1* to *Sk*, where *K <= n*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means算法通过迭代尝试，通过最小化簇中心向量的均值与新候选簇成员向量之间的距离，来确定测试数据中的簇。以下等式假设数据集成员范围从*X1*到*Xn*；同时也假设*K*个簇集合，范围从*S1*到*Sk*，其中*K
    <= n*。
- en: '![](img/55533695-d1e3-486e-a661-261ea6f75809.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55533695-d1e3-486e-a661-261ea6f75809.png)'
- en: K-Means in practice
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-Means实践
- en: The K-Means MLlib functionality uses the `LabeledPoint` structure to process
    its data and so it needs numeric input data. As the same data from the last section
    is being reused, we will not explain the data conversion again. The only change
    that has been made in data terms in this section, is that processing in HDFS will
    now take place under the `/data/spark/kmeans/` directory**.** Additionally, the
    conversion Scala script for the K-Means example produces a record that is all
    comma-separated.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib的K-Means功能使用`LabeledPoint`结构处理数据，因此需要数值输入数据。由于正在重复使用上一节的数据，我们将不再解释数据转换。本节中数据方面的唯一变化是，处理将在HDFS下的`/data/spark/kmeans/`目录进行。此外，K-Means示例的转换Scala脚本生成的记录全部以逗号分隔。
- en: 'The development and processing for the K-Means example has taken place under
    the `/home/hadoop/spark/kmeans` directory to separate the work from other development.
    The `sbt` configuration file is now called `kmeans.sbt` and is identical to the
    last example, except for the project name:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将工作与其他开发分开，K-Means示例的开发和处理已在`/home/hadoop/spark/kmeans`目录下进行。`sbt`配置文件现在称为`kmeans.sbt`，与上一个示例相同，只是项目名称不同：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The code for this section can be found in the software package under `chapter7\K-Means`.
    So, looking at the code for `kmeans1.scala`, which is stored under `kmeans/src/main/scala`,
    some similar actions occur. The import statements refer to the Spark context and
    configuration. This time, however, the K-Means functionality is being imported
    from MLlib. Additionally, the application class name has been changed for this
    example to `kmeans1`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本节代码可在软件包的`chapter7\K-Means`目录下找到。因此，查看存储在`kmeans/src/main/scala`下的`kmeans1.scala`代码，会发现一些类似的操作。导入语句引用了Spark上下文和配置。然而，这一次，K-Means功能是从MLlib导入的。此外，为了这个例子，应用程序类名已更改为`kmeans1`：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The same actions are being taken as in the last example to define the data
    file--to define the Spark configuration and create a Spark context:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与上例相同，正在采取行动定义数据文件——定义Spark配置并创建Spark上下文：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, the CSV data is loaded from the data file and split by comma characters
    into the `VectorData` variable:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从数据文件加载CSV数据，并通过逗号字符分割到`VectorData`变量中：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'A `KMeans` object is initialized, and the parameters are set to define the
    number of clusters and the maximum number of iterations to determine them:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`KMeans`对象被初始化，并设置参数以定义簇的数量和确定它们的最大迭代次数：'
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Some default values are defined for the initialization mode, the number of
    runs, and Epsilon, which we needed for the K-Means call but did not vary for the
    processing. Finally, these parameters were set against the `KMeans` object:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为初始化模式、运行次数和Epsilon定义了一些默认值，这些值是我们进行K-Means调用所需的，但在处理过程中并未改变。最后，这些参数被设置到`KMeans`对象上：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We cached the training vector data to improve the performance and trained the
    `KMeans` object using the vector data to create a trained K-Means model:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们缓存了训练向量数据以提高性能，并使用向量数据训练了`KMeans`对象，创建了一个经过训练的K-Means模型：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We have computed the K-Means cost and number of input data rows, and have to
    output the results via `println` statements. The cost value indicates how tightly
    the clusters are packed and how separate the clusters are:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了K-Means成本和输入数据行数，并通过`println`语句输出了结果。成本值表示簇的紧密程度以及簇之间的分离程度：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we have used the K-Means Model to print the cluster centers as vectors
    for each of the three clusters that were computed:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用K-Means模型打印出计算出的三个簇中每个簇的中心作为向量：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we use the K-Means model to predict function to create a list of cluster
    membership predictions. We then count these predictions by value to give a count
    of the data points in each cluster. This shows which clusters are bigger and whether
    there really are three clusters:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用K-Means模型的预测函数来创建一个簇成员资格预测列表。然后，我们按值计数这些预测，以给出每个簇中数据点的计数。这显示了哪些簇更大，以及是否真的存在三个簇：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'So, in order to run this application, it must be compiled and packaged from
    the `kmeans` subdirectory as the Linux `pwd` command shows here:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了运行此应用程序，必须从`kmeans`子目录进行编译和打包，正如Linux的`pwd`命令所示：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once this packaging is successful, we check HDFS to ensure that the test data
    is ready. As in the last example, we convert our data into the numeric form using
    the `convert.scala` file, provided in the software package. We will process the
    `DigitalBreathTestData2013-MALE2a.csv` data file in the HDFS directory, `/data/spark/kmeans`,
    as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦打包成功，我们检查HDFS以确保测试数据已就绪。如前例所示，我们使用软件包中提供的`convert.scala`文件将数据转换为数值形式。我们将处理HDFS目录`/data/spark/kmeans`中的`DigitalBreathTestData2013-MALE2a.csv`数据文件，如下所示：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `spark-submit` tool is used to run the K-Means application. The only change
    in this command is that the class is now `kmeans1`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`spark-submit`工具运行K-Means应用程序。此命令中唯一的更改是类名现在是`kmeans1`：
- en: '[PRE19]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output from the Spark cluster run is shown to be as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Spark集群运行的输出显示如下：
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The previous output shows the input data volume, which looks correct; it also
    shows the `K-Means cost` value. The cost is based on the **Within Set Sum of Squared
    Errors (WSSSE)** which basically gives a measure of how well the found cluster
    centroids are matching the distribution of the data points. The better they are
    matching, the lower the cost. The following link [https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/](https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/)
    explains WSSSE and how to find a good value for **k** in more detail.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的输出显示了输入数据量，看起来是正确的；它还显示了`K-Means成本`值。该成本基于**内部总和平方误差（WSSSE）**，基本上给出了找到的簇质心与数据点分布匹配程度的度量。匹配得越好，成本越低。以下链接[https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/](https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/)更详细地解释了WSSSE以及如何找到一个好的**k**值。
- en: 'Next, come the three vectors, which describe the data cluster centers with
    the correct number of dimensions. Remember that these cluster centroid vectors
    will have the same number of columns as the original vector data:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是三个向量，它们描述了具有正确维数的数据簇中心。请记住，这些簇质心向量将具有与原始向量数据相同的列数：
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, cluster membership is given for clusters 1 to 3 with cluster 1 (index
    0) having the largest membership at `407539` member vectors:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，给出了1至3簇的簇成员资格，其中簇1（索引0）拥有最大的成员资格，有`407539`个成员向量：
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: So, these two examples show how data can be classified and clustered using Naive
    Bayes and K-Means. What if I want to classify images or more complex patterns,
    and use a black box approach to classification? The next section examines Spark-based
    classification using **ANNs**, or **artificial neural networks**.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这两个示例展示了如何使用朴素贝叶斯和K-Means对数据进行分类和聚类。如果我想对图像或更复杂的模式进行分类，并使用黑盒方法进行分类呢？下一节将探讨基于Spark的分类，使用**ANNs**，即**人工神经网络**。
- en: Artificial neural networks
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: 'The following figure shows a simple biological neuron to the left. The neuron
    has dendrites that receive signals from other neurons. A cell body controls activation,
    and an axon carries an electrical impulse to the dendrites of other neurons. The
    artificial neuron to the right has a series of weighted inputs: a summing function
    that groups the inputs and a **firing mechanism** (**F(Net)**), which decides
    whether the inputs have reached a threshold, and, if so, the neuron will fire:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下图左侧展示了一个简单的生物神经元。该神经元具有接收其他神经元信号的树突。细胞体控制激活，轴突将电脉冲传递到其他神经元的树突。右侧的人工神经元有一系列加权输入：一个汇总函数，将输入分组，以及一个**触发机制**（**F(Net)**），该机制决定输入是否达到阈值，如果是，则神经元将触发：
- en: '![](img/663b4884-b77e-4a07-8718-7f41d2f5fdf1.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/663b4884-b77e-4a07-8718-7f41d2f5fdf1.png)'
- en: Neural networks are tolerant of noisy images and distortion, and so are useful
    when a black box classification method is needed for potentially ...
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络对噪声图像和失真具有容忍度，因此在需要潜在的...黑盒分类方法时非常有用。
- en: ANN in practice
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ANN实践
- en: 'In order to begin ANN training, test data is needed. Given that this type of
    classification method is supposed to be good at classifying distorted or noisy
    images, we decided to attempt to classify the images here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始ANN训练，需要测试数据。鉴于这种分类方法应该擅长分类扭曲或噪声图像，我们决定在这里尝试对图像进行分类：
- en: '![](img/19b4278b-afec-4d67-826c-20e4b767737f.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19b4278b-afec-4d67-826c-20e4b767737f.png)'
- en: They are hand-crafted text files that contain shaped blocks, created from the
    characters 1 and 0\. When they are stored on HDFS, the carriage return characters
    are removed so that the image is presented as a single line vector. So, the ANN
    will be classifying a series of shape images and then will be tested against the
    same images with noise added to determine whether the classification will still
    work. There are six training images, and they will each be given an arbitrary
    training label from 0.1 to 0.6\. So, if the ANN is presented with a closed square,
    it should return a label of 0.1\. The following image shows an example of a testing
    image with noise added.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是手工制作的文本文件，包含由1和0组成的形状块。当存储在HDFS上时，回车符会被移除，使得图像呈现为单行向量。因此，ANN将对一系列形状图像进行分类，然后与添加了噪声的相同图像进行测试，以确定分类是否仍然有效。有六张训练图像，每张图像将被赋予一个从0.1到0.6的任意训练标签。因此，如果ANN呈现一个闭合的正方形，它应该返回标签0.1。下图展示了一个带有噪声的测试图像示例。
- en: 'The noise, created by adding extra zero (0) characters within the image, has
    been highlighted:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在图像内添加额外的零（0）字符创建的噪声已被突出显示：
- en: '![](img/9a151e3d-12c8-4d62-9c5b-2b7c4c9674f9.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a151e3d-12c8-4d62-9c5b-2b7c4c9674f9.png)'
- en: 'As before, the ANN code is developed using the Linux Hadoop account in a subdirectory
    called `spark/ann`. The `ann.sbt` file exists in the `ann` directory:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，ANN代码是在Linux Hadoop账户下的`spark/ann`子目录中开发的。`ann.sbt`文件位于`ann`目录中：
- en: '[PRE23]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The contents of the `ann.sbt` file have been changed to use full paths of JAR
    library files for the Spark dependencies:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`ann.sbt`文件的内容已更改，以使用Spark依赖项JAR库文件的完整路径：'
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As in the previous examples, the actual Scala code to be compiled exists in
    a subdirectory named `src/main/scala`. We have created two Scala programs. The
    first trains using the input data and then tests the ANN model with the same input
    data. The second tests the trained model with noisy data to test the distorted
    data classification:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，实际要编译的Scala代码存放在名为`src/main/scala`的子目录中。我们创建了两个Scala程序。第一个程序使用输入数据进行训练，然后用同一输入数据测试ANN模型。第二个程序则用噪声数据测试已训练模型的扭曲数据分类能力：
- en: '[PRE25]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will examine the first Scala file and then we will just show the extra features
    of the second file, as the two examples are very similar up to the point of training
    the ANN. The code examples shown here can be found in the software package provided
    with this book under the path, `chapter2\ANN`. So, to examine the first Scala
    example, the import statements are similar to the previous examples. The Spark
    context, configuration, vectors, and `LabeledPoint` are being imported. The `RDD`
    class for RDD processing is being imported this time, along with the new ANN class, `ANNClassifier`.
    Note that the MLlib/classification routines widely use the `LabeledPoint` structure
    for input data, which will contain the features and labels that are supposed to
    be trained against:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将检查第一个Scala文件，然后仅展示第二个文件的额外特性，因为两个示例在训练ANN之前非常相似。此处展示的代码示例可在本书提供的软件包中的路径`chapter2\ANN`下找到。因此，要检查第一个Scala示例，导入语句与前例类似。正在导入Spark上下文、配置、向量和`LabeledPoint`。这次还导入了用于RDD处理的`RDD`类以及新的ANN类`ANNClassifier`。请注意，MLlib/分类例程广泛使用`LabeledPoint`结构作为输入数据，该结构将包含要训练的特征和标签：
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The application class in this example has been called `testann1`. The HDFS
    files to be processed have been defined in terms of the HDFS `server`, `path`,
    and file name:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本例中的应用程序类名为`testann1`。要处理的HDFS文件已根据HDFS的`server`、`path`和文件名定义：
- en: '[PRE27]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The Spark context has been created with the URL for the Spark instance, which
    now has a different port number--`8077`. The application name is `ANN 1`. This
    will appear on the Spark web UI when the application is run:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Spark上下文已使用Spark实例的URL创建，现在端口号不同——`8077`。应用程序名称为`ANN 1`。当应用程序运行时，这将在Spark Web
    UI上显示：
- en: '[PRE28]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The HDFS-based input training and test data files are loaded. The values on
    each line are split by space characters, and the numeric values have been converted
    into doubles. The variables that contain this data are then stored in an array
    called **inputs**. At the same time, an array called outputs is created, containing
    the labels from `0.1` to `0.6`. These values will be used to classify the input
    patterns:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 基于HDFS的输入训练和测试数据文件被加载。每行上的值通过空格字符分割，数值已转换为双精度数。包含此数据的变量随后存储在一个名为**inputs**的数组中。同时，创建了一个名为outputs的数组，包含从`0.1`到`0.6`的标签。这些值将用于对输入模式进行分类：
- en: '[PRE29]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The input and output data, representing the input data features and labels,
    are then combined and converted into a `LabeledPoint` structure. Finally, the
    data is parallelized in order to partition it for optimal parallel processing:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 代表输入数据特征和标签的输入和输出数据随后被合并并转换为`LabeledPoint`结构。最后，数据被并行化，以便为最佳并行处理进行分区：
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Variables are created to define the hidden layer topology of the ANN. In this
    case, we have chosen to have two hidden layers, each with 100 neurons. The maximum
    number of iterations is defined as well as a batch size (six patterns) and convergence
    tolerance. The tolerance refers to how big the training error can get before we
    can consider training to have worked. Then, an ANN model is created using these
    configuration parameters and the input data:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 变量用于定义人工神经网络（ANN）的隐藏层拓扑结构。在此例中，我们选择了两个隐藏层，每层各有100个神经元。同时定义了最大迭代次数、批次大小（六个模式）以及收敛容差。容差指的是训练误差达到多大时，我们可以认为训练已经成功。接着，根据这些配置参数和输入数据创建了一个ANN模型：
- en: '[PRE31]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In order to test the trained ANN model, the same input training data is used
    as testing data to obtain prediction labels. First, an input data variable is
    created called `rPredictData`. Then, the data is partitioned and, finally, the
    predictions are obtained using the trained ANN model. For this model to work,
    it must output the labels, `0.1` to `0.6`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试已训练的ANN模型，使用相同的输入训练数据作为测试数据以获取预测标签。首先，创建一个名为`rPredictData`的输入数据变量。然后，数据被分区，并最终使用已训练的ANN模型获取预测结果。为了使该模型工作，它必须输出标签，即`0.1`到`0.6`：
- en: '[PRE32]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The label predictions are printed and the script closes with a closing bracket:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 打印标签预测结果，脚本以闭合括号结束：
- en: '[PRE33]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'So, in order to run this code sample, it must first be compiled and packaged.
    By now, you must be familiar with the `sbt` command, executed from the `ann` subdirectory:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要运行此代码示例，首先必须对其进行编译和打包。至此，您应该已经熟悉了从`ann`子目录执行的`sbt`命令：
- en: '[PRE34]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The `spark-submit` command is then used from within the new `spark/spark` path
    using the new Spark-based URL at port `8077` to run the application, `testann1`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在新`spark/spark`路径内使用新的基于Spark的URL（端口`8077`）运行应用程序`testann1`，使用`spark-submit`命令：
- en: '[PRE35]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'By checking the Apache Spark web URL at `http://localhost:19080/`, it is now
    possible to see the application running. The following figure shows the `ANN 1`
    application running as well as the previously completed executions:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问Apache Spark网页URL `http://localhost:19080/`，现在可以看到应用程序正在运行。下图显示了`ANN 1`应用程序的运行情况以及先前完成的执行：
- en: '![](img/e5e83e52-c0db-4506-8eeb-3ea0c81a5c5e.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e83e52-c0db-4506-8eeb-3ea0c81a5c5e.png)'
- en: 'By selecting one of the cluster host worker instances, it is possible to see
    a list of executors that actually carry out cluster processing for that worker:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择其中一个集群主机工作实例，可以看到实际执行集群处理的工作实例的执行程序列表：
- en: '![](img/689d4c67-406b-4b41-8e9a-e437d9354d48.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/689d4c67-406b-4b41-8e9a-e437d9354d48.png)'
- en: 'Finally, by selecting one of the executors, it is possible to see its history
    and configuration as well as links to the log file and error information. At this
    level, with the log information provided, debugging is possible. These log files
    can be checked to process error messages:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过选择其中一个执行程序，可以看到其历史和配置，以及到日志文件和错误信息的链接。在这一级别，借助提供的日志信息，可以进行调试。可以检查这些日志文件以处理错误消息：
- en: '![](img/467af2aa-e9ae-4b6b-927b-8b44c9d32c6d.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467af2aa-e9ae-4b6b-927b-8b44c9d32c6d.png)'
- en: 'The `ANN 1` application provides the following output to show that it has reclassified
    the same input data correctly. The reclassification has been successful as each
    of the input patterns has been given the same label that it was trained with:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`ANN 1`应用程序提供了以下输出，以显示它已正确地对相同输入数据进行了重新分类。重新分类成功，因为每个输入模式都被赋予了与其训练时相同的标签：'
- en: '[PRE36]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'So, this shows that ANN training and test prediction will work with the same
    data. Now, we will train with the same data, but test with distorted or noisy
    data, an example of which we already demonstrated. This example can be found in
    the file called `test_ann2.scala` in your software package. It is very similar
    to the first example, so we will just demonstrate the changed code. The application
    is now called `testann2`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明ANN训练和测试预测将适用于相同的数据。现在，我们将使用相同的数据进行训练，但测试时使用扭曲或含噪声的数据，我们已展示了一个示例。该示例可在软件包中的`test_ann2.scala`文件中找到。它与第一个示例非常相似，因此我们将仅展示更改的代码。该应用程序现在称为`testann2`：
- en: '[PRE37]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'An extra set of testing data is created after the ANN model has been created
    using the training data. This testing data contains noise:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在ANN模型使用训练数据创建后，会生成一组额外的测试数据。此测试数据包含噪声：
- en: '[PRE38]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This data is processed into input arrays and partitioned for cluster processing:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据被处理成输入数组并分区以供集群处理：
- en: '[PRE39]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'It is then used to generate label predictions in the same way as the first
    example. If the model classifies the data correctly, then the same label values
    should be printed from `0.1` to `0.6`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 它随后以与第一个示例相同的方式生成标签预测。如果模型正确分类数据，则应从`0.1`到`0.6`打印相同的标签值：
- en: '[PRE40]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The code has already been compiled, so it can be run using the `spark-submit`
    command:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 代码已经编译完成，因此可以使用`spark-submit`命令运行：
- en: '[PRE41]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Here is the cluster output from this script, which shows a successful classification
    using a trained ANN model and some noisy test data. The noisy data has been classified
    correctly. For instance, if the trained model had become confused, it might have
    given a value of 0.15 for the noisy `close_square_test.img` test image in position
    one, instead of returning `0.1` as it did:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本脚本的集群输出显示了使用训练好的ANN模型对一些噪声测试数据进行成功分类的情况。噪声数据已被正确分类。例如，如果训练模型出现混淆，它可能会对位置一的噪声`close_square_test.img`测试图像给出0.15的值，而不是像实际那样返回`0.1`：
- en: '[PRE42]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter has attempted to provide you with an overview of some of the functionality
    available within the Apache Spark MLlib module. It has also shown the functionality
    that will soon be available in terms of ANNs or artificial neural networks. You
    might have been impressed by how well ANNs work. It is not possible to cover all
    the areas of MLlib due to the time and space allowed for this chapter. In addition,
    we now want to concentrate more on the SparkML library in the next chapter, which
    speeds up machine learning by supporting DataFrames and the underlying Catalyst
    and Tungsten optimizations.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本章试图为你概述Apache Spark MLlib模块中可用的一些功能。它还展示了即将在ANNs或人工神经网络方面可用的功能。你可能会对ANNs的工作效果印象深刻。由于时间和篇幅限制，本章无法涵盖MLlib的所有领域。此外，我们现在希望在下一章中更多地关注SparkML库，该库通过支持DataFrames以及底层Catalyst和Tungsten优化来加速机器学习。
- en: We saw how to develop Scala-based examples for Naive Bayes classification, K-Means
    clustering, and ANNs. You learned how to prepare test ...
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何开发基于Scala的示例，用于朴素贝叶斯分类、K-Means聚类和ANNs。你了解了如何准备测试...
