- en: Apache SparkML
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache SparkML
- en: So now that you've learned a lot about MLlib, why another ML API? First of all,
    it is a common task in data science to work with multiple frameworks and ML libraries
    as there are always advantages and disadvantages; mostly, it is a trade-off between
    performance and functionality. R, for instance, is the king when it comes to functionality--there
    exist more than 6000 R add-on packages. However, R is also one of the slowest
    execution environments for data science. SparkML, on the other hand, currently
    has relatively limited functionality but is one of the fastest libraries. Why
    is this so? This brings us to the second reason why SparkML exists.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经学了很多关于 MLlib 的知识，为什么还需要另一个 ML API 呢？首先，在数据科学中，与多个框架和 ML 库合作是一项常见任务，因为它们各有优劣；大多数情况下，这是性能和功能之间的权衡。例如，R
    在功能方面是王者——存在超过 6000 个 R 附加包。然而，R 也是数据科学执行环境中最慢的之一。另一方面，SparkML 目前功能相对有限，但却是速度最快的库之一。为什么会这样呢？这引出了
    SparkML 存在的第二个原因。
- en: The duality between RDD on the one hand and DataFrames and Datasets on the other
    is like a red thread in this book and doesn't stop influencing the machine learning
    chapters. As MLlib is designed to work on top of RDDs, SparkML works on top of
    DataFrames and Datasets, therefore making use of all the new performance benefits
    that Catalyst and Tungsten bring.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 与 DataFrames 和 Datasets 之间的二元性就像本书中的一条红线，并且不断影响着机器学习章节。由于 MLlib 设计为在 RDD
    之上工作，SparkML 在 DataFrames 和 Datasets 之上工作，因此利用了 Catalyst 和 Tungsten 带来的所有新性能优势。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Introduction to the SparkML API
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkML API 简介
- en: The concept of pipelines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道概念
- en: Transformers and estimators
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换器和估计器
- en: A working example
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个工作示例
- en: What does the new API look like?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新 API 是什么样的？
- en: 'When it comes to machine learning on Apache Spark, we are used to transforming
    data into an appropriate format and data types before we actually feed them to
    our algorithms. Machine learning practitioners around the globe discovered that
    the preprocessing tasks on a machine learning project usually follow the same
    pattern:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Apache Spark 上进行机器学习时，我们习惯于在将数据实际输入算法之前将其转换为适当的格式和数据类型。全球的机器学习实践者发现，机器学习项目中的预处理任务通常遵循相同的模式：
- en: Data preparation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Training
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练
- en: Evaluating
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估
- en: Hyperparameter tuning
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调整
- en: Therefore, the new ApacheSparkML API supports this process out of the box. It
    is called **pipelines** and is inspired by scikit-learn [http://scikit-learn.org](http://scikit-learn.org),
    a very popular machine learning library for the Python programming language. The
    central data structure is a DataFrame and all operations run on top of it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，新的 ApacheSparkML API 原生支持这一过程。它被称为 **管道**，灵感来源于 scikit-learn [http://scikit-learn.org](http://scikit-learn.org)，一个非常流行的
    Python 编程语言机器学习库。中央数据结构是 DataFrame，所有操作都在其上运行。
- en: The concept of pipelines
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道概念
- en: 'ApacheSparkML pipelines have the following components:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ApacheSparkML 管道包含以下组件：
- en: '**DataFrame**: This is the central data store where all the original data and
    intermediate results are stored in.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataFrame**：这是中央数据存储，所有原始数据和中间结果都存储于此。'
- en: '**Transformer**: As the name suggests, a transformer transforms one DataFrame
    into another by adding additional (feature) columns in most of the cases. Transformers
    are stateless, which means that they don''t have any internal memory and behave
    exactly the same each time they are used; this is a concept you might be familiar
    with when using the map function of RDDs.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换器**：顾名思义，转换器通过在大多数情况下添加额外的（特征）列将一个 DataFrame 转换为另一个。转换器是无状态的，这意味着它们没有任何内部内存，每次使用时行为完全相同；这个概念在使用
    RDD 的 map 函数时你可能已经熟悉。'
- en: '**Estimator**: In most of the cases, an estimator is some sort of machine learning
    model. In contrast to a transformer, an estimator contains an internal state representation
    and is highly dependent on the history of the data that it has already seen.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**估计器**：在大多数情况下，估计器是一种机器学习模型。与转换器不同，估计器包含内部状态表示，并且高度依赖于它已经见过的数据历史。'
- en: '**Pipeline**: This is the glue which is joining the preceding components, DataFrame,
    Transformer and Estimator, together.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**：这是将前面提到的组件——DataFrame、Transformer 和 Estimator——粘合在一起的胶水。'
- en: '**Parameter**: Machine learning algorithms have many knobs to tweak. These
    are called **hyperparameters** and the values learned by a machine learning algorithm
    to fit data are called parameters. By standardizing how hyperparameters are expressed,
    ApacheSparkML opens doors to task automation, as we will see later.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数**：机器学习算法有许多可调整的旋钮。这些被称为**超参数**，而机器学习算法为了拟合数据所学习的值被称为参数。通过标准化超参数的表达方式，ApacheSparkML为任务自动化打开了大门，正如我们稍后将看到的。'
- en: Transformers
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器
- en: Let's start with something simple. One of the most common tasks in machine learning
    data preparation is string indexing and one-hot encoding of categorical values.
    Let's see how this can be done.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单的事情开始。机器学习数据准备中最常见的任务之一是对分类值进行字符串索引和独热编码。让我们看看这是如何完成的。
- en: String indexer
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符串索引器
- en: 'Let''s assume that we have a DataFrame `df` containing a column called color
    of categorical labels--red, green, and blue. We want to encode them as integer
    or float values. This is where `org.apache.spark.ml.feature.StringIndexer` kicks
    in. It automatically determines the cardinality of the category set and assigns
    each one a distinct value. So in our example, a list of categories such as red,
    red, green, red, blue, green should be transformed into 1, 1, 2, 1, 3, 2:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个名为`df`的DataFrame，其中包含一个名为color的分类标签列——红色、绿色和蓝色。我们希望将它们编码为整数或浮点值。这时`org.apache.spark.ml.feature.StringIndexer`就派上用场了。它会自动确定类别集的基数，并为每个类别分配一个唯一值。所以在我们的例子中，一个类别列表，如红色、红色、绿色、红色、蓝色、绿色，应该被转换为1、1、2、1、3、2：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The result of this transformation is a DataFrame called indexed that, in addition
    to the colors column of the String type, now contains a column called `colorsIndexed`
    of type double.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此转换的结果是一个名为indexed的DataFrame，除了字符串类型的颜色列外，现在还包含一个名为`colorsIndexed`的double类型列。
- en: OneHotEncoder
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独热编码器
- en: 'We are only halfway through. Although machine learning algorithms are capable
    of making use of the `colorsIndexed` column, they perform better if we one-hot
    encode it. This actually means that, instead of having a `colorsIndexed` column
    containing label indexes between one and three, it is better if we have three
    columns--one for each color--with the constraint that every row is allowed to
    set only one of these columns to one, otherwise zero. Let''s do it:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅进行了一半。尽管机器学习算法能够利用`colorsIndexed`列，但如果我们对其进行独热编码，它们的表现会更好。这意味着，与其拥有一个包含1到3之间标签索引的`colorsIndexed`列，不如我们拥有三个列——每种颜色一个——并规定每行只允许将其中一个列设置为1，其余为0。让我们这样做：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Intuitively, we would expect that we get three additional columns in the encoded
    DataFrame, for example, `colorIndexedRed`, `colorIndexedGreen`, and `colorIndexedBlue
    ...`
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，我们期望在编码后的DataFrame中得到三个额外的列，例如，`colorIndexedRed`、`colorIndexedGreen`和`colorIndexedBlue`...
- en: VectorAssembler
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量汇编器
- en: 'Before we start with the actual machine learning algorithm, we need to apply
    one final transformation. We have to create one additional `feature` column containing
    all the information of the columns that we want the machine learning algorithm
    to consider. This is done by `org.apache.spark.ml.feature.VectorAssembler` as
    follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始实际的机器学习算法之前，我们需要应用最后一个转换。我们必须创建一个额外的`特征`列，其中包含我们希望机器学习算法考虑的所有列的信息。这是通过`org.apache.spark.ml.feature.VectorAssembler`如下完成的：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This transformer adds only one single column to the resulting DataFrame called
    **features**, which is of the `org.apache.spark.ml.linalg.Vector` type. In other
    words, this new column called features, created by the `VectorAssembler`, contains
    all the defined columns (in this case, `colorVec`, `field2`, `field3`, and `field4`)
    encoded in a single vector object for each row. This is the format the Apache
    SparkML algorithms are happy with.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换器只为结果DataFrame添加了一个名为**features**的列，该列的类型为`org.apache.spark.ml.linalg.Vector`。换句话说，这个由`VectorAssembler`创建的新列features包含了我们定义的所有列（在这种情况下，`colorVec`、`field2`、`field3`和`field4`），每行编码在一个向量对象中。这是Apache
    SparkML算法所喜欢的格式。
- en: Pipelines
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道
- en: 'Before we dive into estimators--we''ve already used one in `StringIndexer`--let''s
    first understand the concept of pipelines. As you might have noticed, the transformers
    add only one single column to a DataFrame and basically omit all other columns
    not explicitly specified as input columns; they can only be used in conjunction
    with `org.apache.spark.ml.Pipeline`, which glues individual transformers (and
    estimators) together to form a complete data analysis process. So let''s do this
    for our two `Pipeline` stages:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解估计器之前——我们已经在`StringIndexer`中使用过一个——让我们首先理解管道的概念。你可能已经注意到，转换器只向DataFrame添加一个单一列，并且基本上省略了所有未明确指定为输入列的其他列；它们只能与`org.apache.spark.ml.Pipeline`一起使用，后者将单个转换器（和估计器）粘合在一起，形成一个完整的数据分析过程。因此，让我们为我们的两个`Pipeline`阶段执行此操作：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The now obtained DataFrame called **transformed** contains all the ...
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在得到的DataFrame称为**transformed**，包含所有...
- en: Estimators
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计器
- en: We've used estimators before in `StringIndexer`. We've already stated that estimators
    somehow contain state that changes while looking at data, whereas this is not
    the case for transformers. So why is `StringIndexer` an estimator? This is because
    it needs to remember all the previously seen strings and maintain a mapping table
    between strings and label indexes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`StringIndexer`中已经使用过估计器。我们已经说过，估计器在查看数据时会改变其状态，而转换器则不会。那么为什么`StringIndexer`是估计器呢？这是因为它需要记住所有先前见过的字符串，并维护字符串和标签索引之间的映射表。
- en: In machine learning, it is common to use at least a training and testing subset
    of your available training data. It can happen that an estimator in the pipeline,
    such as `StringIndexer`, has not seen all the string labels while looking at the
    training dataset. Therefore, you'll get an exception when evaluating the model
    using the test dataset as the `StringIndexer` now encounters labels that it has
    not seen before. This is, in fact, a very rare case and basically could mean that
    the sample function you use to separate the training and testing datasets is not
    working; however, there is an option called `setHandleInvalid("skip")` and your
    problem is solved.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，通常至少使用可用的训练数据的一个训练和测试子集。在管道中的估计器（如`StringIndexer`）在查看训练数据集时可能没有看到所有的字符串标签。因此，当你使用测试数据集评估模型时，`StringIndexer`现在遇到了它以前未见过的标签，你会得到一个异常。实际上，这是一个非常罕见的情况，基本上可能意味着你用来分离训练和测试数据集的样本函数不起作用；然而，有一个名为`setHandleInvalid("skip")`的选项，你的问题就解决了。
- en: Another easy way to distinguish between an estimator and a transformer is the
    additional method called `fit` on the estimators. Fit actually populates the internal
    data management structure of the estimators based on a given dataset, which, in
    the case of `StringIndexer`, is the mapping table between label strings and label
    indexes. So now let's take a look at another estimator, an actual machine learning
    algorithm.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 区分估计器和转换器的另一种简单方法是查看估计器上是否有额外的`fit`方法。实际上，fit方法会根据给定数据集填充估计器的内部数据管理结构，在`StringIndexer`的情况下，这是标签字符串和标签索引之间的映射表。现在让我们来看另一个估计器，一个实际的机器学习算法。
- en: RandomForestClassifier
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RandomForestClassifier
- en: 'Let''s assume that we are in a binary classification problem setting and want
    to use `RandomForestClassifier`. All SparkML algorithms have a compatible API,
    so they can be used interchangeably. So it really doesn''t matter which one we
    use, but `RandomForestClassifier` has more (hyper)parameters than more simple
    models like logistic regression. At a later stage we''ll use (hyper)parameter
    tuning which is also inbuilt in Apache SparkML. Therefore it makes sense to use
    an algorithm where more knobs can be tweaked. Adding such a binary classifier
    to our `Pipeline` is very simple:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们处于二分类问题设置中，并希望使用`RandomForestClassifier`。所有SparkML算法都有一个兼容的API，因此它们可以互换使用。所以使用哪个并不重要，但`RandomForestClassifier`比更简单的模型如逻辑回归有更多的（超）参数。在稍后的阶段，我们将使用（超）参数调整，这也是Apache
    SparkML内置的。因此，使用一个可以调整更多参数的算法是有意义的。将这种二分类器添加到我们的`Pipeline`中非常简单：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Model evaluation
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'As mentioned before, model evaluation is built-in to ApacheSparkML and you''ll
    find all that you need in the `org.apache.spark.ml.evaluation` package. Let''s
    continue with our binary classification. This means that we''ll have to use `org.apache.spark.ml.evaluation.BinaryClassificationEvaluator`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，模型评估是ApacheSparkML内置的，你会在`org.apache.spark.ml.evaluation`包中找到所需的一切。让我们继续进行二分类。这意味着我们将不得不使用`org.apache.spark.ml.evaluation.BinaryClassificationEvaluator`：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To code previous initialized a `BinaryClassificationEvaluator` function and
    tells it to calculate the `areaUnderROC`, one of the many possible metrics to
    assess the prediction performance of a machine learning algorithm.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编码，之前初始化了一个`二元分类评估器`函数，并告诉它计算`ROC曲线下面积`，这是评估机器学习算法预测性能的众多可能指标之一。
- en: 'As we have the actual label and the prediction present in a DataFrame called
    `result`, it is simple to calculate this score and is done using the following
    line of code:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在名为`结果`的数据框中同时拥有实际标签和预测，因此计算此分数很简单，使用以下代码行完成：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: CrossValidation and hyperparameter tuning
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证和超参数调整
- en: We will be looking at one example each of `CrossValidation` and hyperparameter
    tuning. Let's take a look at `CrossValidation`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分别看一个`交叉验证`和超参数调整的例子。让我们来看看`交叉验证`。
- en: CrossValidation
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证
- en: As stated before, we've used the default parameters of the machine learning
    algorithm and we don't know if they are a good choice. In addition, instead of
    simply splitting your data into training and testing, or training, testing, and
    validation sets, `CrossValidation` might be a better choice because it makes sure
    that eventually all the data is seen by the machine learning algorithm.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们使用了机器学习算法的默认参数，我们不知道它们是否是好的选择。此外，与其简单地将数据分为训练集和测试集，或训练集、测试集和验证集，`交叉验证`可能是一个更好的选择，因为它确保最终所有数据都被机器学习算法看到。
- en: '`CrossValidation` basically splits your complete available training data into
    a number of **k** folds. This parameter **k** can be specified. Then, the whole
    `Pipeline` is run once for every fold and one machine learning model is trained
    for each fold. Finally, the different machine learning models obtained are joined.
    This is done by a voting scheme for classifiers or by averaging for regression.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`交叉验证`基本上将你全部可用的训练数据分成若干个**k**折。这个参数**k**可以指定。然后，整个`流水线`对每一折运行一次，并为每一折训练一个机器学习模型。最后，通过分类器的投票方案或回归的平均方法将得到的各种机器学习模型合并。'
- en: 'The following figure illustrates ten-fold `CrossValidation`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了十折`交叉验证`：
- en: '![](img/650d0e60-930f-48eb-b434-af3077044821.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/650d0e60-930f-48eb-b434-af3077044821.png)'
- en: Hyperparameter tuning
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整
- en: '`CrossValidation` is often used in conjunction with so-called (hyper)parameter
    tuning. What are hyperparameters? These are the various knobs that you can tweak
    on your machine learning algorithm. For example, these are some parameters of
    the Random Forest classifier:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`交叉验证`通常与所谓的（超）参数调整结合使用。什么是超参数？这些是你可以在你的机器学习算法上调整的各种旋钮。例如，以下是随机森林分类器的一些参数：'
- en: Number of trees
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树的数量
- en: Feature subset strategy
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征子集策略
- en: Impurity
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不纯度
- en: Maximal number of bins
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大箱数
- en: Maximal tree depth
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大树深度
- en: Setting these parameters can have a significant influence on the performance
    of the trained classifier. Often, there is no way of choosing them based on a
    clear recipe--of course, experience helps--but hyperparameter tuning is considered
    as black magic. Can't we just choose many different parameters and test the prediction
    performance? Of course, we can. This feature ...
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 设置这些参数可能会对训练出的分类器的性能产生重大影响。通常，没有明确的方案来选择它们——当然，经验有帮助——但超参数调整被视为黑魔法。我们不能只选择许多不同的参数并测试预测性能吗？当然可以。这个功能...
- en: Winning a Kaggle competition with Apache SparkML
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache SparkML赢得Kaggle竞赛
- en: Winning a Kaggle competition is an art by itself, but we just want to show you
    how the Apache SparkML tooling can be used efficiently to do so.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 赢得Kaggle竞赛本身就是一门艺术，但我们只是想展示如何有效地使用Apache SparkML工具来做到这一点。
- en: We'll use an archived competition for this offered by BOSCH, a German multinational
    engineering, and electronics company, on production line performance data. Details
    for the competition data can be found at [https://www.kaggle.com/c/bosch-production-line-performance/data](https://www.kaggle.com/c/bosch-production-line-performance/data).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用博世公司提供的一个存档竞赛来进行这个操作，博世是一家德国跨国工程和电子公司，关于生产线性能数据。竞赛数据的详细信息可以在[https://www.kaggle.com/c/bosch-production-line-performance/data](https://www.kaggle.com/c/bosch-production-line-performance/data)找到。
- en: Data preparation
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: The challenge data comes in three ZIP packages but we only use two of them.
    One contains categorical data, one contains continuous data, and the last one
    contains timestamps of measurements, which we will ignore for now.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战数据以三个ZIP包的形式提供，但我们只使用其中两个。一个包含分类数据，一个包含连续数据，最后一个包含测量时间戳，我们暂时忽略它。
- en: 'If you extract the data, you''ll get three large CSV files. So the first thing
    that we want to do is re-encode them into parquet in order to be more space-efficient:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你提取数据，你会得到三个大型CSV文件。因此，我们首先要做的是将它们重新编码为parquet，以便更节省空间：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First, we define a function ...
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个函数...
- en: Feature engineering
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: 'Now it is time to run the first transformer (which is actually an estimator).
    It is `StringIndexer` and needs to keep track of an internal mapping table between
    strings and indexes. Therefore, it is not a transformer but an estimator:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候运行第一个转换器（实际上是估计器）了。它是`StringIndexer`，需要跟踪字符串和索引之间的内部映射表。因此，它不是转换器，而是估计器：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As we can see clearly in the following image, an additional column called `L0_S22_F545Index`
    has been created:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，已创建一个名为`L0_S22_F545Index`的附加列：
- en: '![](img/4052e492-9d0b-4dd8-8dbd-8ad0aaca53bf.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4052e492-9d0b-4dd8-8dbd-8ad0aaca53bf.png)'
- en: Finally, let's examine some content of the newly created column and compare
    it with the source column.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们检查新创建列的一些内容，并与源列进行比较。
- en: 'We can clearly see how the category string gets transformed into a float index:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到类别字符串是如何转换为浮点索引的：
- en: '![](img/08902e98-db50-473b-9f20-c83646239384.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08902e98-db50-473b-9f20-c83646239384.png)'
- en: 'Now we want to apply `OneHotEncoder`, which is a transformer, in order to generate
    better features for our machine learning model:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要应用`OneHotEncoder`，这是一个转换器，以便为我们的机器学习模型生成更好的特征：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As you can see in the following figure, the newly created column `L0_S22_F545Vec`
    contains `org.apache.spark.ml.linalg.SparseVector` objects, which is a compressed
    representation of a sparse vector:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，新创建的列`L0_S22_F545Vec`包含`org.apache.spark.ml.linalg.SparseVector`对象，这是一种稀疏向量的压缩表示：
- en: '![](img/7566dc66-93d4-4433-97de-b09abb41cc1f.png)**Sparse vector representations**:
    The `OneHotEncoder`, as many other algorithms, returns a sparse vector of the
    `org.apache.spark.ml.linalg.SparseVector` type as, according to the definition,
    only one element of the vector can be one, the rest has to remain zero. This gives
    a lot of opportunity for compression as only the position of the elements that
    are non-zero has to be known. Apache Spark uses a sparse vector representation
    in the following format: *(l,[p],[v])*, where *l* stands for length of the vector,
    *p* for position (this can also be an array of positions), and *v* for the actual
    values (this can be an array of values). So if we get (13,[10],[1.0]), as in our
    earlier example, the actual sparse vector looks like this: (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/7566dc66-93d4-4433-97de-b09abb41cc1f.png)**稀疏向量表示**：`OneHotEncoder`与其他许多算法一样，返回一个`org.apache.spark.ml.linalg.SparseVector`类型的稀疏向量，根据定义，向量中只有一个元素可以为1，其余必须保持为0。这为压缩提供了大量机会，因为只需知道非零元素的位置即可。Apache
    Spark使用以下格式的稀疏向量表示：*(l,[p],[v])*，其中*l*代表向量长度，*p*代表位置（这也可以是位置数组），*v*代表实际值（这可以是值数组）。因此，如果我们得到(13,[10],[1.0])，如我们之前的例子所示，实际的稀疏向量看起来是这样的：(0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0)。'
- en: 'So now that we are done with our feature engineering, we want to create one
    overall sparse vector containing all the necessary columns for our machine learner.
    This is done using `VectorAssembler`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的特征工程已完成，我们想要创建一个包含机器学习器所需所有必要列的总体稀疏向量。这是通过使用`VectorAssembler`完成的：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We basically just define a list of column names and a target column, and the
    rest is done for us:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上只需定义列名列表和目标列，其余工作将自动完成：
- en: '![](img/77298190-a703-4228-b9df-e8b960a720c6.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77298190-a703-4228-b9df-e8b960a720c6.png)'
- en: 'As the view of the `features` column got a bit squashed, let''s inspect one
    instance of the feature field in more detail:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`features`列的视图有些压缩，让我们更详细地检查特征字段的一个实例：
- en: '![](img/1c6cb5ae-1cae-4f6d-aacd-3514e383fe0a.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c6cb5ae-1cae-4f6d-aacd-3514e383fe0a.png)'
- en: 'We can clearly see that we are dealing with a sparse vector of length 16 where
    positions 0, 13, 14, and 15 are non-zero and contain the following values: `1.0`,
    `0.03`, `-0.034`, and `-0.197`. Done! Let''s create a `Pipeline` out of these
    components.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，我们处理的是一个长度为16的稀疏向量，其中位置0、13、14和15是非零的，并包含以下值：`1.0`、`0.03`、`-0.034`和`-0.197`。完成！让我们用这些组件创建一个`Pipeline`。
- en: Testing the feature engineering pipeline
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试特征工程管道
- en: 'Let''s create a `Pipeline` out of our transformers and estimators:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用我们的转换器和估计器创建一个`Pipeline`：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Note that the `setStages` method of `Pipeline` just expects an array of `transformers`
    and `estimators`, which we had created earlier. As parts of the `Pipeline` contain
    estimators, we have to run `fit` on our `DataFrame` first. The obtained `Pipeline`
    object takes a `DataFrame` in the `transform` method and returns the results of
    the transformations:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`Pipeline`的`setStages`方法仅期望一个由`transformers`和`estimators`组成的数组，这些我们之前已经创建。由于`Pipeline`的部分包含估计器，我们必须先对我们的`DataFrame`运行`fit`。得到的`Pipeline`对象在`transform`方法中接受一个`DataFrame`，并返回转换的结果：
- en: As expected, ...
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的，...
- en: Training the machine learning model
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练机器学习模型
- en: 'Now it''s time to add another component to the `Pipeline`: the actual machine
    learning algorithm--RandomForest:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候向`Pipeline`添加另一个组件了：实际的机器学习算法——随机森林：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This code is very straightforward. First, we have to instantiate our algorithm
    and obtain it as a reference in `rf`. We could have set additional parameters
    to the model but we''ll do this later in an automated fashion in the `CrossValidation`
    step. Then, we just add the stage to our `Pipeline`, fit it, and finally transform.
    The `fit` method, apart from running all upstream stages, also calls fit on the
    `RandomForestClassifier` in order to train it. The trained model is now contained
    within the `Pipeline` and the `transform` method actually creates our predictions
    column:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码非常直接。首先，我们必须实例化我们的算法，并将其作为引用获取到`rf`中。我们可以为模型设置额外的参数，但我们将稍后在`CrossValidation`步骤中以自动化方式进行。然后，我们只需将阶段添加到我们的`Pipeline`，拟合它，并最终转换。`fit`方法，除了运行所有上游阶段外，还调用`RandomForestClassifier`上的拟合以训练它。训练好的模型现在包含在`Pipeline`中，`transform`方法实际上创建了我们的预测列：
- en: '![](img/8b66350b-866d-4207-9ea6-db5097902fbf.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b66350b-866d-4207-9ea6-db5097902fbf.png)'
- en: As we can see, we've now obtained an additional column called prediction, which
    contains the output of the `RandomForestClassifier` model. Of course, we've only
    used a very limited subset of available features/columns and have also not yet
    tuned the model, so we don't expect to do very well; however, let's take a look
    at how we can evaluate our model easily with Apache SparkML.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，我们现在获得了一个名为prediction的额外列，其中包含`RandomForestClassifier`模型的输出。当然，我们仅使用了可用特征/列的一个非常有限的子集，并且尚未调整模型，因此我们不期望表现很好；但是，让我们看看如何使用Apache
    SparkML轻松评估我们的模型。
- en: Model evaluation
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'Without evaluation, a model is worth nothing as we don''t know how accurately
    it performs. Therefore, we will now use the built-in `BinaryClassificationEvaluator`
    in order to assess prediction performance and a widely used measure called `areaUnderROC`
    (going into detail here is beyond the scope of this book):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 没有评估，模型一文不值，因为我们不知道它的准确性如何。因此，我们现在将使用内置的`BinaryClassificationEvaluator`来评估预测性能，并使用一个广泛使用的度量标准，称为`areaUnderROC`（深入探讨这一点超出了本书的范围）：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As we can see, there is a built-in class called `org.apache.spark.ml.evaluation.BinaryClassificationEvaluator`
    and there are some other ...
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，有一个内置类名为`org.apache.spark.ml.evaluation.BinaryClassificationEvaluator`，还有其他一些...
- en: CrossValidation and hyperparameter tuning
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证与超参数调整
- en: As explained before, a common step in machine learning is cross-validating your
    model using testing data against training data and also tweaking the knobs of
    your machine learning algorithms. Let's use Apache SparkML in order to do this
    for us, fully automated!
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，机器学习中的一个常见步骤是使用测试数据对训练数据进行交叉验证，并调整机器学习算法的旋钮。让我们使用Apache SparkML来自动完成这一过程！
- en: 'First, we have to configure the parameter map and `CrossValidator`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须配置参数映射和`CrossValidator`：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `org.apache.spark.ml.tuning.ParamGridBuilder` is used in order to define
    the hyperparameter space where the `CrossValidator` has to search and finally,
    the `org.apache.spark.ml.tuning.CrossValidator` takes our `Pipeline`, the hyperparameter
    space of our RandomForest classifier, and the number of folds for the `CrossValidation`
    as parameters. Now, as usual, we just need to call fit and transform on the `CrossValidator`
    and it will basically run our `Pipeline` multiple times and return a model that
    performs the best. Do you know how many different models are trained? Well, we
    have five folds on `CrossValidation` and five-dimensional hyperparameter space
    cardinalities between two and eight, so let''s do the math: 5 * 8 * 5 * 2 * 7
    * 7 = 19600 times!'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`org.apache.spark.ml.tuning.ParamGridBuilder`用于定义`CrossValidator`需要在其中搜索的超参数空间，而`org.apache.spark.ml.tuning.CrossValidator`则接收我们的`Pipeline`、随机森林分类器的超参数空间以及`CrossValidation`的折数作为参数。现在，按照惯例，我们只需对`CrossValidator`调用fit和transform方法，它就会基本运行我们的`Pipeline`多次，并返回一个表现最佳的模型。你知道训练了多少个不同的模型吗？我们有5折的`CrossValidation`和5维超参数空间基数在2到8之间，所以让我们计算一下：5
    * 8 * 5 * 2 * 7 * 7 = 19600次！'
- en: Using the evaluator to assess the quality of the cross-validated and tuned model
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用评估器来评估经过交叉验证和调优的模型的质量
- en: 'Now that we''ve optimized our `Pipeline` in a fully automatic fashion, let''s
    see how our best model can be obtained:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经以全自动方式优化了`Pipeline`，接下来让我们看看如何获得最佳模型：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `crossValidatorModel.bestModel` code basically returns the best `Pipeline`.
    Now we use `bestPipelineModel.stages` to obtain the individual stages and obtain
    the tuned `RandomForestClassificationModel ...`
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`crossValidatorModel.bestModel`代码基本上返回了最佳`Pipeline`。现在我们使用`bestPipelineModel.stages`来获取各个阶段，并获得经过调优的`RandomForestClassificationModel
    ...`'
- en: Summary
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: You've learned that, as in many other places, the introduction of `DataFrames`
    leads to the development of complementary frameworks that are not using RDDs directly
    anymore. This is also the case for machine learning but there is much more to
    it. `Pipeline` actually takes machine learning in Apache Spark to the next level
    as it improves the productivity of the data scientist dramatically.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解到，正如在许多其他领域一样，引入`DataFrames`促进了互补框架的发展，这些框架不再直接使用RDDs。机器学习领域亦是如此，但还有更多内容。`Pipeline`实际上将Apache
    Spark中的机器学习提升到了一个新的水平，极大地提高了数据科学家的生产力。
- en: The compatibility between all intermediate objects and well-thought-out concepts
    is just awesome. Great! Finally, we've applied the concepts that we discussed
    on a real dataset from a Kaggle competition, which is a very nice starting point
    for your own machine learning project with Apache SparkML. The next Chapter covers
    Apache SystemML, which is a 3rd party machine learning library for Apache Spark.
    Let's see why it is useful and what the differences are to SparkML.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所有中间对象之间的兼容性以及精心设计的概念简直令人惊叹。太棒了！最后，我们将讨论的概念应用于来自Kaggle竞赛的真实数据集，这对于你自己的Apache
    SparkML机器学习项目来说是一个非常好的起点。下一章将介绍Apache SystemML，这是一个第三方机器学习库，用于Apache Spark。让我们看看它为何有用以及与SparkML的区别。
