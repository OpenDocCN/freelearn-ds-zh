- en: Chapter 12. Optimizations and Performance Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。优化和性能调优
- en: This chapter covers various optimizations and performance-tuning best practices
    when working with Spark.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了在使用Spark时的各种优化和性能调优最佳实践。
- en: 'The chapter is divided into the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分为以下几个配方：
- en: Optimizing memory
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化内存
- en: Using compression to improve performance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用压缩以提高性能
- en: Using serialization to improve performance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用序列化以提高性能
- en: Optimizing garbage collection
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化垃圾收集
- en: Optimizing the level of parallelism
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化并行级别
- en: Understanding the future of optimization – project Tungsten
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解优化的未来-项目钨
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Before looking into various ways to optimize Spark, it is a good idea to look
    at the Spark internals. So far, we have looked at Spark at higher level, where
    focus was the functionality provided by the various libraries.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究各种优化Spark的方法之前，最好先了解Spark的内部情况。到目前为止，我们已经在较高级别上看待了Spark，重点是各种库提供的功能。
- en: 'Let''s start with redefining an RDD. Externally, an RDD is a distributed immutable
    collection of objects. Internally, it consists of the following five parts:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新定义一个RDD。从外部来看，RDD是一个分布式的不可变对象集合。在内部，它由以下五个部分组成：
- en: Set of partitions (`rdd.getPartitions`)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组分区（`rdd.getPartitions`）
- en: List of dependencies on parent RDDs (`rdd.dependencies`)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对父RDD的依赖列表（`rdd.dependencies`）
- en: Function to compute a partition, given its parents
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算分区的函数，给定其父级
- en: Partitioner (optional) (`rdd.partitioner`)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区器（可选）（`rdd.partitioner`）
- en: Preferred location of each partition (optional) (`rdd.preferredLocations`)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个分区的首选位置（可选）（`rdd.preferredLocations`）
- en: The first three are needed for an RDD to be recomputed, in case the data is
    lost. When combined, it is called **lineage**. The last two parts are optimizations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 前三个是RDD重新计算所需的，以防数据丢失。当组合在一起时，称为**血统**。最后两个部分是优化。
- en: A set of partitions is how data is divided into nodes. In case of HDFS, it means
    `InputSplits`, which are mostly the same as block (except when a record crosses
    block boundaries; in that case, it will be slightly bigger than a block).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一组分区是数据如何分布到节点的。在HDFS的情况下，这意味着`InputSplits`，它们大多与块相同（除非记录跨越块边界；在这种情况下，它将比块稍大）。
- en: 'Let''s revisit our `wordCount` example to understand these five parts. This
    is how the RDD graph looks for `wordCount` at dataset level view:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视我们的`wordCount`示例，以了解这五个部分。这是`wordCount`在数据集级别视图下的RDD图的样子：
- en: '![Introduction](img/3056_12_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![介绍](img/3056_12_01.jpg)'
- en: 'Basically, this is how the flow goes:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，流程如下：
- en: 'Load the `words` folder as an RDD:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`words`文件夹加载为RDD：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following are the five parts of `words` RDD:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`words` RDD的五个部分：
- en: '| **Partitions** | One partition per hdfs inputsplit/block (`org.apache.spark.rdd.HadoopPartition`)
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **分区** | 每个hdfs输入拆分/块一个分区（`org.apache.spark.rdd.HadoopPartition`）|'
- en: '| **Dependencies** | None |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **依赖** | 无 |'
- en: '| **Compute function** | Read the block |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| **计算函数** | 读取块 |'
- en: '| **Preferred location** | The hdfs block location |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **首选位置** | hdfs块位置 |'
- en: '| **Partitioner** | None |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| **分区器** | 无 |'
- en: 'Tokenize the words from `words` RDD with each word on a separate line:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`words` RDD中的单词标记化，每个单词占一行：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following are the five parts of `wordsFlatMap` RDD:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`wordsFlatMap` RDD的五个部分：
- en: '| **Partitions** | Same as parent RDD, that is, `words` (`org.apache.spark.rdd.HadoopPartition`)
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **分区** | 与父RDD相同，即`words`（`org.apache.spark.rdd.HadoopPartition`）|'
- en: '| **Dependencies** | Same as parent RDD, that is, `words` (`org.apache.spark.OneToOneDependency`)
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **依赖** | 与父RDD相同，即`words`（`org.apache.spark.OneToOneDependency`）|'
- en: '| **Compute function** | Compute parent and split each element and flattens
    the results |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **计算函数** | 计算父级并拆分每个元素并展平结果 |'
- en: '| **Preferred location** | Ask parent |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **首选位置** | 询问父母 |'
- en: '| **Partitioner** | None |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **分区器** | 无 |'
- en: 'Transform each word in `wordsFlatMap` RDD to (word,1) tuple:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`wordsFlatMap` RDD中的每个单词转换为（单词，1）元组：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following are the five parts of `wordsMap` RDD:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`wordsMap` RDD的五个部分：
- en: '| **Partitions** | Same as parent RDD, that is, wordsFlatMap (org.apache.spark.rdd.HadoopPartition)
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **分区** | 与父RDD相同，即wordsFlatMap（org.apache.spark.rdd.HadoopPartition）|'
- en: '| **Dependencies** | Same as parent RDD, that is, wordsFlatMap (org.apache.spark.OneToOneDependency)
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| **依赖** | 与父RDD相同，即wordsFlatMap（org.apache.spark.OneToOneDependency）|'
- en: '| **Compute function** | Compute parent and map it to PairRDD |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **计算函数** | 计算父级并将其映射到PairRDD |'
- en: '| **Preferred Location** | Ask parent |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **首选位置** | 询问父母 |'
- en: '| **Partitioner** | None |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **分区器** | 无 |'
- en: 'Reduce all the values for a given key and sum them up:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将给定键的所有值减少并求和：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following are the five parts of `wordCount` RDD:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`wordCount` RDD的五个部分：
- en: '| **Partitions** | One per reduce task (`org.apache.spark.rdd.ShuffledRDDPartition`)
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **分区** | 每个reduce任务一个（`org.apache.spark.rdd.ShuffledRDDPartition`）|'
- en: '| **Dependencies** | Shuffle dependency on each parent (`org.apache.spark.ShuffleDependency`)
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **依赖** | 每个父级的Shuffle依赖（`org.apache.spark.ShuffleDependency`）|'
- en: '| **Compute function** | Do addition on shuffled data |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| **计算函数** | 对洗牌数据进行加法 |'
- en: '| **Preferred location** | None |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| **首选位置** | 无 |'
- en: '| **Partitioner** | HashPartitioner (`org.apache.spark.HashPartitioner`) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **分区器** | HashPartitioner（`org.apache.spark.HashPartitioner`）|'
- en: 'This is how an RDD graph for `wordCount` looks at the partition level view:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`wordCount`在分区级别视图下的RDD图的样子：
- en: '![Introduction](img/3056_12_02.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![介绍](img/3056_12_02.jpg)'
- en: Optimizing memory
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化内存
- en: Spark is a complex distributed computing framework, and has many moving parts.
    Various cluster resources, such as memory, CPU, and network bandwidth, can become
    bottlenecks at various points. As Spark is an in-memory compute framework, the
    impact of the memory is the biggest.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个复杂的分布式计算框架，有许多组成部分。各种集群资源，如内存、CPU和网络带宽，可能在各个点成为瓶颈。由于Spark是一个内存计算框架，内存的影响最大。
- en: Another issue is that it is common for Spark applications to use a huge amount
    of memory, sometimes more than 100 GB. This amount of memory usage is not common
    in traditional Java applications.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，Spark应用程序通常使用大量内存，有时超过100GB。这种内存使用量在传统的Java应用程序中并不常见。
- en: In Spark, there are two places where memory optimization is needed, and that
    is at the driver and at the executor level.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，有两个地方需要进行内存优化，即在驱动程序和执行程序级别。
- en: 'You can use the following commands to set the driver memory:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令来设置驱动程序内存：
- en: 'Spark shell:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark shell：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Spark submit:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark提交：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can use the following commands to set the executor memory:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令来设置执行程序内存：
- en: 'Spark shell:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark shell：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Spark submit:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark提交：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To understand memory optimization, it is a good idea to understand how memory
    management works in Java. Objects reside in Heap in Java. Heap is created when
    JVM starts, and it can resize itself when needed (based on minimum and maximum
    size, that is, `-Xms` and `-Xmx`, respectively assigned in configuration).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解内存优化，了解Java中内存管理的工作原理是一个好主意。对象驻留在Java堆中。堆在JVM启动时创建，并且可以根据需要调整大小（基于配置中分配的最小和最大大小，即`-Xms`和`-Xmx`）。
- en: 'Heap is divided into two spaces or generations: young space and old space.
    The young space is reserved for the allocation of new objects. Young space consists
    of an area called **Eden** and two smaller survivor spaces. When the nursery becomes
    full, garbage is collected by running a special process called **young collection**,
    where all the objects, which have lived long enough, are promoted to old space.
    When the old space becomes full, the garbage is collected there by running a process
    called **old collection**.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 堆被分为两个空间或代：年轻空间和老年空间。年轻空间用于分配新对象。年轻空间包括一个称为**伊甸园**的区域和两个较小的幸存者空间。当幼儿园变满时，通过运行称为**年轻收集**的特殊过程来收集垃圾，其中所有已经存在足够长时间的对象都被提升到老年空间。当老年空间变满时，通过运行称为**老年收集**的过程来在那里收集垃圾。
- en: '![Optimizing memory](img/3056_12_03.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![优化内存](img/3056_12_03.jpg)'
- en: The logic behind nursery is that most objects have a very short life span. A
    young collection is designed to be fast at finding newly allocated objects and
    moving them to the old space.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 幼儿园背后的逻辑是，大多数对象的寿命非常短。年轻收集旨在快速找到新分配的对象并将它们移动到老年空间。
- en: The JVM uses mark and sweep algorithm for garbage collection. Mark and sweep
    collection consists of two phases.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: JVM使用标记和清除算法进行垃圾回收。标记和清除收集包括两个阶段。
- en: During the mark phase, all the objects, which have live references, are marked
    alive, the rest are presumed candidates for garbage collection. During the sweep
    phase, the space occupied by garbage collectable candidates is added to the free
    list, that is, they are available to be allocated to new objects.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记阶段，所有具有活动引用的对象都被标记为活动的，其余的被假定为垃圾收集的候选对象。在清除阶段，垃圾收集候选对象占用的空间被添加到空闲列表中，即它们可以分配给新对象。
- en: There are two improvements to mark and sweep. One is **concurrent mark and sweep**
    (**CMS**) and the other is parallel mark and sweep. CMS focuses on lower latency,
    while the latter focuses on higher throughput. Both strategies have performance
    trade-offs. CMS does not do compaction, while parallel **garbage collector** (**GC**)
    performs whole-heap only compaction, which results in pause times. As a thumb
    rule, for real-time streaming, CMS should be used, and parallel GC otherwise.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 标记和清除有两个改进。一个是**并发标记和清除**（**CMS**），另一个是并行标记和清除。CMS专注于较低的延迟，而后者专注于更高的吞吐量。这两种策略都有性能权衡。CMS不进行压缩，而并行**垃圾收集器**（**GC**）执行整个堆的压缩，这会导致暂停时间。作为经验法则，对于实时流处理，应该使用CMS，否则使用并行GC。
- en: If you would like to have both low latency and high throughput, Java 1.7 update
    4 onwards has another option called **garbage-first GC** (**G1**). G1 is a server-style
    garbage collector, primarily meant for multicore machines with large memories.
    It is planned as a long-term replacement for CMS. So, to modify our thumb rule,
    if you are using Java 7 onwards, simply use G1.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望同时具有低延迟和高吞吐量，Java 1.7更新4之后还有另一个选项，称为**垃圾优先GC**（**G1**）。G1是一种服务器式垃圾收集器，主要用于具有大内存的多核机器。它计划作为CMS的长期替代品。因此，为了修改我们的经验法则，如果您使用Java
    7及以上版本，只需使用G1。
- en: G1 partitions the heap into a set of equal-sized regions, where each set is
    a contiguous range of virtual memory. Each region is assigned a role like Eden,
    Survivor, and Old. G1 performs a concurrent global marking phase to determine
    the live references of objects throughout the heap. After the mark phase is over,
    G1 knows which regions are mostly empty. It collects in these regions first and
    this frees the larger amount of memory.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: G1将堆分成一组大小相等的区域，每个区域都是虚拟内存的连续范围。每个区域被分配了一个角色，如伊甸园、幸存者和老年。G1执行并发全局标记阶段，以确定整个堆中对象的活动引用。标记阶段结束后，G1知道哪些区域大部分是空的。它首先在这些区域中进行收集，从而释放更多的内存。
- en: '![Optimizing memory](img/3056_12_04.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![优化内存](img/3056_12_04.jpg)'
- en: The regions selected by G1 as candidates for garbage collection are garbage
    collected using evacuation. G1 copies objects from one or more regions of the
    heap to a single region on the heap, and it both compacts and frees up memory.
    This evacuation is performed in parallel on multiple cores to reduce pause times
    and increase throughput. So, each garbage collection round reduces fragmentation
    while working within user-defined pause times.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: G1选择的用于垃圾收集的区域使用疏散进行垃圾收集。G1将对象从堆的一个或多个区域复制到堆上的单个区域，并且它既压缩又释放内存。这种疏散是在多个核心上并行执行的，以减少暂停时间并增加吞吐量。因此，每次垃圾收集循环都会减少碎片化，同时在用户定义的暂停时间内工作。
- en: 'There are three aspects in memory optimization in Java:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中内存优化有三个方面：
- en: Memory footprint
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存占用
- en: Cost of accessing objects in memory
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问内存中的对象的成本
- en: Cost of garbage collection
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垃圾收集的成本
- en: Java objects, in general, are fast to access but consume much more space than
    the actual data inside them.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，Java对象访问速度快，但占用的空间比其中的实际数据多得多。
- en: Using compression to improve performance
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用压缩来提高性能
- en: Data compression involves encoding information using fewer bits than the original
    representation. Compression has an important role to play in big data technologies.
    It makes both storage and transport of data more efficient.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据压缩涉及使用比原始表示更少的位对信息进行编码。压缩在大数据技术中发挥着重要作用。它使数据的存储和传输更加高效。
- en: When data is compressed, it becomes smaller, so both disk I/O and network I/O
    become faster. It also saves storage space. Every optimization has a cost, and
    the cost of compression comes in the form of added CPU cycles to compress and
    decompress data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据经过压缩时，它变得更小，因此磁盘I/O和网络I/O都变得更快。它还节省了存储空间。每种优化都有成本，压缩的成本体现在增加的CPU周期上，用于压缩和解压缩数据。
- en: Hadoop needs to split data to put them into blocks, irrespective of whether
    the data is compressed or not. Only few compression formats are splittable.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop需要将数据分割成块，无论数据是否经过压缩。只有少数压缩格式是可分割的。
- en: Two most popular compression formats for big data loads are LZO and Snappy.
    Snappy is not splittable, while LZO is. Snappy, on the other hand, is a much faster
    format.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据加载的两种最流行的压缩格式是LZO和Snappy。 Snappy不可分割，而LZO可以。另一方面，Snappy是一种更快的格式。
- en: If compression format is splittable like LZO, input file is first split into
    blocks and then compressed. Since compression happened at block level, decompression
    can happen at block level as well as node level.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果压缩格式像LZO一样是可分割的，输入文件首先被分割成块，然后进行压缩。由于压缩发生在块级别，因此解压缩可以在块级别以及节点级别进行。
- en: If compression format is not splittable, compression happens at file level and
    then it is split into blocks. In this case, blocks have to be merged back to file
    before they can be decompressed, so decompression cannot happen at node level.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果压缩格式不可分割，则压缩发生在文件级别，然后将其分割成块。在这种情况下，块必须合并回文件，然后才能进行解压缩，因此无法在节点级别进行解压缩。
- en: For supported compression formats, Spark will deploy codecs automatically to
    decompress, and no action is required from the user's side.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于支持的压缩格式，Spark将自动部署编解码器进行解压缩，用户无需采取任何操作。
- en: Using serialization to improve performance
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用序列化来提高性能
- en: 'Serialization plays an important part in distributed computing. There are two
    persistence (storage) levels, which support serializing RDDs:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 序列化在分布式计算中起着重要作用。有两种支持序列化RDD的持久性（存储）级别：
- en: '`MEMORY_ONLY_SER`: This stores RDDs as serialized objects. It will create one
    byte array per partition'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_ONLY_SER`：将RDD存储为序列化对象。它将为每个分区创建一个字节数组'
- en: '`MEMORY_AND_DISK_SER`: This is similar to the `MEMORY_ONLY_SER`, but it spills
    partitions that do not fit in the memory to disk'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEMORY_AND_DISK_SER`：这类似于`MEMORY_ONLY_SER`，但会将不适合内存的分区溢出到磁盘'
- en: 'The following are the steps to add appropriate persistence levels:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是添加适当持久性级别的步骤：
- en: 'Start the Spark shell:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Import the `StorageLevel` and implicits associated with it:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入与之相关的`StorageLevel`和隐式转换：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create an RDD:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个RDD：
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Persist the RDD:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持久化RDD：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Though serialization reduces the memory footprint substantially, it adds extra
    CPU cycles due to deserialization.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管序列化大大减少了内存占用，但由于反序列化而增加了额外的CPU周期。
- en: By default, Spark uses Java's serialization. Since the Java serialization is
    slow, the better approach is to use `Kryo` library. `Kryo` is much faster and
    sometimes even 10 times more compact than the default.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Spark使用Java的序列化。由于Java序列化速度较慢，更好的方法是使用`Kryo`库。 `Kryo`要快得多，有时甚至比默认值紧凑10倍。
- en: How to do it…
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到…
- en: 'You can use `Kryo` by doing the following settings in your `SparkConf`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在`SparkConf`中进行以下设置来使用`Kryo`：
- en: 'Start the Spark shell by setting `Kryo` as serializer:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过设置`Kryo`作为序列化器启动Spark shell：
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`Kryo` automatically registers most of the core Scala classes, but if you would
    like to register your own classes, you can use the following command:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Kryo`自动注册大部分核心Scala类，但如果您想注册自己的类，可以使用以下命令：'
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Optimizing garbage collection
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化垃圾收集
- en: JVM garbage collection can be a challenge if you have a lot of short lived RDDs.
    JVM needs to go over all the objects to find the ones it needs to garbage collect.
    The cost of the garbage collection is proportional to the number of objects the
    GC needs to go through. Therefore, using fewer objects and the data structures
    that use fewer objects (simpler data structures, such as arrays) helps.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有大量短寿命的RDD，JVM垃圾收集可能会成为一个挑战。 JVM需要检查所有对象以找到需要进行垃圾回收的对象。垃圾收集的成本与GC需要检查的对象数量成正比。因此，使用更少的对象和使用更少对象的数据结构（更简单的数据结构，如数组）有助于减少垃圾收集的成本。
- en: Serialization also shines here as a byte array needs only one object to be garbage
    collected.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 序列化在这里也很出色，因为一个字节数组只需要一个对象进行垃圾回收。
- en: By default, Spark uses 60 percent of the executor memory to cache RDDs and the
    rest 40 percent for regular objects. Sometimes, you may not need 60 percent for
    RDDs and can reduce this limit so that more space is available for object creation
    (less need for GC).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Spark使用60％的执行器内存来缓存RDD，其余40％用于常规对象。有时，您可能不需要60％的RDD，并且可以减少此限制，以便为对象创建提供更多空间（减少对GC的需求）。
- en: How to do it…
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到…
- en: 'You can set the memory allocated for RDD cache to 40 percent by starting the
    Spark shell and setting the memory fraction:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过启动Spark shell并设置内存分数来将RDD缓存的内存分配设置为40％：
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Optimizing the level of parallelism
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化并行级别
- en: Optimizing the level of parallelism is very important to fully utilize the cluster
    capacity. In the case of HDFS, it means that the number of partitions is the same
    as the number of `InputSplits`, which is mostly the same as the number of blocks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 优化并行级别对充分利用集群容量非常重要。在HDFS的情况下，这意味着分区的数量与`InputSplits`的数量相同，这与块的数量大致相同。
- en: In this recipe, we will cover different ways to optimize the number of partitions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将介绍优化分区数量的不同方法。
- en: How to do it…
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到…
- en: 'Specify the number of partitions when loading a file into RDD with the following
    steps:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载文件到RDD时指定分区数量，具体步骤如下：
- en: 'Start the Spark shell:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Load the RDD with a custom number of partitions as a second parameter:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用自定义分区数量作为第二个参数加载RDD：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Another approach is to change the default parallelism by performing the following
    steps:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是通过执行以下步骤更改默认并行度：
- en: 'Start the Spark shell with the new value of default parallelism:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的默认并行度值启动Spark shell：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Check the default value of parallelism:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查默认并行度的值：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注
- en: You can also reduce the number of partitions using an RDD method called `coalesce(numPartitions)`
    where `numPartitions` is the final number of partitions you would like. If you
    would like the data to be reshuffled over the network, you can call the RDD method
    called `repartition(numPartitions)` where `numPartitions` is the final number
    of partitions you would like.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用RDD方法`coalesce(numPartitions)`来减少分区数量，其中`numPartitions`是您希望的最终分区数量。如果您希望数据在网络上重新分配，可以调用RDD方法`repartition(numPartitions)`，其中`numPartitions`是您希望的最终分区数量。
- en: Understanding the future of optimization – project Tungsten
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解优化的未来-项目钨
- en: Project Tungsten, starting with Spark Version 1.4, is the initiative to bring
    Spark closer to bare metal. The goal of this project is to substantially improve
    the memory and CPU efficiency of the Spark applications and push the limits of
    underlying hardware.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 项目钨从Spark 1.4版本开始，旨在将Spark更接近裸金属。该项目的目标是大幅提高Spark应用程序的内存和CPU效率，并推动底层硬件的极限。
- en: In distributed systems, conventional wisdom has been to always optimize network
    I/O as that has been the most scarce and bottlenecked resource. This trend has
    changed in the last few years. Network bandwidth, in the last 5 years, has changed
    from 1 gigabit per second to 10 gigabit per second.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式系统中，传统智慧一直是始终优化网络I/O，因为这一直是最稀缺和最瓶颈的资源。这一趋势在过去几年已经改变。在过去5年中，网络带宽已经从每秒1千兆位增加到每秒10千兆位。
- en: On similar lines, the disk bandwidth has increased from 50 MB/s to 500 MB/s
    and SSDs are being deployed more and more. CPU clock speed, on the other hand,
    was ~3 GHz 5 years back and is still the same. This has unseated the network and
    made CPU the new bottleneck in distributed processing.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似的情况下，磁盘带宽已经从50MB/s增加到500MB/s，SSD的部署也越来越多。另一方面，CPU时钟速度在5年前是~3GHz，现在仍然是一样的。这使得网络不再是瓶颈，而使CPU成为分布式处理中的新瓶颈。
- en: Note
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注
- en: Another trend that has put more load on CPU performance is the new compressed
    data formats such as Parquet. Both compression and serialization, as we have seen
    in the previous recipes in this chapter, lead to more CPU cycles. This trend has
    also pushed the need for CPU optimization to reduce the CPU cycle cost.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个增加CPU性能负担的趋势是新的压缩数据格式，比如Parquet。正如我们在本章的前几个示例中看到的，压缩和序列化会导致更多的CPU周期。这一趋势也推动了减少CPU周期成本的CPU优化的需求。
- en: On the similar lines, let's look at the memory footprint. In Java, GC does memory
    management. GC has done an amazing job at taking away the memory management from
    the programmer and making it transparent. To do this, Java has to put a lot of
    overhead, and that substantially increases the memory footprint. As an example,
    a simple String "abcd", which should ideally take 4 bytes, takes 48 bytes in Java.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似的情况下，让我们看看内存占用。在Java中，GC进行内存管理。GC在将内存管理从程序员手中拿走并使其透明方面做得很好。为了做到这一点，Java必须付出很大的开销，这大大增加了内存占用。例如，一个简单的字符串"abcd"，在Java中应该占用4个字节，实际上占用了48个字节。
- en: What if we do away with GC and manage memory manually like in lower-level programming
    languages such as C? Java does provide a way to do that since 1.7 version and
    it is called `sun.misc.Unsafe`. Unsafe essentially means that you can build long
    regions of memory without any safety checks. This is the first feature of project
    Tungsten.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们放弃GC，像在C等低级编程语言中那样手动管理内存会怎样？自Java 1.7版本以来，Java确实提供了一种方法来做到这一点，称为`sun.misc.Unsafe`。Unsafe基本上意味着您可以构建长区域的内存而不进行任何安全检查。这是项目钨的第一个特点。
- en: Manual memory management by leverage application semantics
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过利用应用程序语义进行手动内存管理
- en: Manual memory management by leverage application semantics, which can be very
    risky if you do not know what you are doing, is a blessing with Spark. We used
    knowledge of data schema (DataFrames) to directly layout the memory ourselves.
    It not only gets rid of GC overheads, but lets you minimize the memory footprint.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用应用程序语义进行手动内存管理，如果你不知道自己在做什么，这可能非常危险，但在Spark中是一种福音。我们利用数据模式（DataFrames）的知识直接布局内存。这不仅可以摆脱GC开销，还可以最小化内存占用。
- en: The second point is storing data in CPU cache versus memory. Everyone knows
    CPU cache is great as it takes three cycles to get data from the main memory versus
    one cycle in cache. This is the second feature of project Tungsten.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第二点是将数据存储在CPU缓存与内存中。每个人都知道CPU缓存很棒，因为从主内存获取数据需要三个周期，而缓存只需要一个周期。这是项目钨的第二个特点。
- en: Using algorithms and data structures
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用算法和数据结构
- en: Algorithms and data structures are used to exploit memory hierarchy and enable
    more cache-aware computation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 算法和数据结构被用来利用内存层次结构，实现更多的缓存感知计算。
- en: 'CPU caches are small pools of memory that store the data the CPU is going to
    need next. CPUs have two types of caches: instruction cache and data cache. Data
    caches are arranged in hierarchy of L1, L2, and L3:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: CPU缓存是存储CPU下一个需要的数据的小内存池。CPU有两种类型的缓存：指令缓存和数据缓存。数据缓存按照L1、L2和L3的层次结构排列：
- en: L1 cache is the fastest and most expensive cache in a computer. It stores the
    most critical data and is the first place the CPU looks for information.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1缓存是计算机中最快、最昂贵的缓存。它存储最关键的数据，是CPU查找信息的第一个地方。
- en: L2 cache is slightly slower than L1, but still located on the same processor
    chip. It is the second place the CPU looks for information.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2缓存比L1稍慢，但仍位于同一处理器芯片上。这是CPU查找信息的第二个地方。
- en: L3 cache is still slower, but is shared by all cores, such as DRAM (memory).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L3缓存仍然较慢，但由所有核心共享，例如DRAM（内存）。
- en: 'These can be seen in the following diagram:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可以在以下图表中看到：
- en: '![Using algorithms and data structures](img/3056_12_05.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![使用算法和数据结构](img/3056_12_05.jpg)'
- en: The third point is that Java is not very good at bytecode generation for things
    like expression evaluation. If this code generation is done manually, it is much
    more efficient. Code generation is the third feature of project Tungsten.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 第三点是，Java在字节码生成方面不太擅长，比如表达式求值。如果这种代码生成是手动完成的，效率会更高。代码生成是Tungsten项目的第三个特性。
- en: Code generation
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码生成
- en: This involves exploiting modern compliers and CPUs to allow efficient operations
    directly on binary data. Project Tungsten is in its infancy at present and will
    have much wider support in version 1.5.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及利用现代编译器和CPU，以便直接在二进制数据上进行高效操作。目前，Tungsten项目还处于起步阶段，在1.5版本中将有更广泛的支持。
