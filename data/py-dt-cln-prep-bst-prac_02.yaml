- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Importance of Data Quality
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据质量的重要性
- en: Did you know that data serves as the backbone of many important business decisions?
    Without accurate, complete, and consistent information, companies risk making
    faulty judgments that could potentially damage their reputation, client relationships,
    and business overall. Consistency issues across different datasets can cause confusion
    and prevent meaningful analysis from happening. Irrelevant or outdated data can
    misguide the judgment of decision-makers, resulting in suboptimal choices. On
    the other hand, building high-quality data products serves as a powerful asset,
    empowering organizations to make informed decisions, uncover valuable insights,
    identify trends, mitigate risks, and gain a competitive edge.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道数据是许多重要商业决策的支柱吗？没有准确、完整、一致的信息，企业可能会做出错误的判断，这可能损害企业的声誉、客户关系及整体业务。不同数据集之间的一致性问题可能会造成混乱，阻碍有意义的分析。无关或过时的数据可能会误导决策者的判断，导致做出次优选择。另一方面，构建高质量的数据产品则是一个强有力的资产，能够帮助组织做出明智的决策、发现有价值的洞察、识别趋势、降低风险并获得竞争优势。
- en: 'In this chapter, we will dive deep into the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨以下话题：
- en: Why data quality is important
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么数据质量很重要
- en: Different dimensions to measure data quality in your data products
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于衡量数据产品中数据质量的不同维度
- en: The impact of data silos on data quality
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据孤岛对数据质量的影响
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find all the code for the chapter in the following GitHub repository:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下的GitHub仓库中找到本章的所有代码：
- en: '[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter02](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter02)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter02](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter02)'
- en: Why data quality is important
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么数据质量很重要
- en: 'Allow me to unveil the reasons why data quality matters:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我来揭示一下为什么数据质量如此重要：
- en: '**Accurate data can give you a competitive advantage**: Organizations depend
    on data to determine patterns, trends, preferences, and other vital aspects governing
    their ecosystem. If your data quality is subpar, the resulting analysis and conclusions
    may be skewed, resulting in wrong moves that could jeopardize your entire business.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确的数据能带来竞争优势**：组织依赖数据来识别模式、趋势、偏好以及支配其生态系统的其他关键因素。如果你的数据质量不合格，结果分析和结论可能会失真，导致错误的决策，这可能会危及整个业务。'
- en: '**Complete data is the backbone of cost optimization**: Data forms the foundation
    of automation and optimization, which can drive up productivity and lower expenses
    when executed properly. Incomplete or low-quality data can cause bottlenecks and
    increase costs. Imagine countless man-hours wasted on fixing errors that would
    have been avoided if only there had been higher standards set for data entry.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整的数据是成本优化的基石**：数据构成了自动化和优化的基础，若能得当地执行，能够提高生产力并降低费用。不完整或低质量的数据会导致瓶颈并增加成本。试想一下，如果数据录入标准更高，许多本可以避免的错误就不会浪费无数的人工时间去修正。'
- en: '**Top-notch data can lead to satisfied customers who stick around long term**:The
    heartbeat of every business depends on satisfied clients, whose loyalty can ensure
    sustained growth over time. Wrong data about customers might translate into personalized
    experiences that don’t match customer characteristics or even into inaccurate
    billings and unfulfilled requests. These disappointed customers may take their
    business elsewhere, leaving your company struggling to survive.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顶级数据能够带来长期忠诚的满意客户**：每个企业的生命力依赖于满意的客户，客户的忠诚度能够确保企业的持续增长。关于客户的错误数据可能会导致个性化体验不匹配客户特征，甚至出现错误的账单和未满足的请求。这些失望的客户可能会把生意带到别处，导致公司面临生存困境。'
- en: '**Compliant data is a requirement to avoid unwanted legal consequences**: Several
    industries must follow specific rules concerning data precision, safety, and privacy.
    Adherence requires a high level of data quality to satisfy strict guidelines and
    prevent legal penalties along with the potential loss of consumer confidence.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规的数据是避免不必要法律后果的必要条件**：许多行业必须遵循有关数据精度、安全性和隐私的特定规则。遵守这些规则需要高质量的数据，以满足严格的指南，并防止法律处罚以及可能失去消费者信任。'
- en: '**To avoid data silos, you need to trust your data**: When various entities
    within an enterprise must work together to leverage data, ensuring its integrity
    is essential. Incompatibility or discrepancies may obstruct cooperation and can
    hinder integration efforts and create data silos.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为了避免数据孤岛，你需要信任你的数据**：当企业中的多个实体必须协作利用数据时，确保数据的完整性至关重要。数据的不兼容或差异可能会妨碍合作，阻碍集成工作，并导致数据孤岛的形成。'
- en: '**Data quality actually means trust**: Data quality directly impacts the trust
    and credibility stakeholders place in an organization. By maintaining high-quality
    data, organizations can foster trust among customers, partners, and investors.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量实际上意味着信任**：数据质量直接影响利益相关者对组织的信任和信誉。通过保持高质量的数据，组织可以在客户、合作伙伴和投资者之间建立信任。'
- en: Now that we have a clearer picture of why data quality is important, let’s move
    to the next section, where we will dive deep into the different dimensions of
    data quality.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们更清楚数据质量为何重要，接下来进入下一部分，我们将深入探讨数据质量的不同维度。
- en: Dimensions of data quality
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据质量的维度
- en: As emphasized earlier, superior data quality forms the foundation upon which
    informed decisions and strategic insights are built. With this in mind, let us
    now examine which **Key Performance Indicators** (**KPIs**) we could use to measure
    the data quality of our assets.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，卓越的数据质量是构建明智决策和战略洞察力的基础。考虑到这一点，我们现在来探讨我们可以使用哪些**关键绩效指标**（**KPIs**）来衡量我们资产的数据质量。
- en: Completeness
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整性
- en: Completeness measures the extent to which data is complete and lacks missing
    values or fields. KPIs can include metrics such as the percentage of missing data
    or missing data points per record.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 完整性衡量数据的完整程度，即数据是否缺少任何值或字段。关键绩效指标可能包括缺失数据的百分比或每条记录缺失的数据点数。
- en: 'The following code will output the completeness percentages for each column
    in your dataset. A higher percentage indicates a higher level of completeness,
    while a lower percentage suggests more missing values:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将输出数据集中每列的完整性百分比。较高的百分比表示较高的完整性水平，而较低的百分比则表示更多缺失值：
- en: 'We’ll start by importing the `pandas` library to work with the dataset:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先导入`pandas`库以处理数据集：
- en: '[PRE0]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we create a sample dataset with the following columns: `Name`, `Age`,
    `Gender`, and `City`. Some values are intentionally missing (represented as `None`):'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含以下列的示例数据集：`Name`（姓名）、`Age`（年龄）、`Gender`（性别）和`City`（城市）。其中有些值故意缺失（用`None`表示）：
- en: '[PRE1]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we create a pandas DataFrame:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个pandas DataFrame：
- en: '[PRE2]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We’ll then use the `isnull()` function to identify missing values in each column
    and use the `sum()` function to count the total number of missing values for each
    column:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用`isnull()`函数来识别每列的缺失值，并使用`sum()`函数计算每列缺失值的总数：
- en: '[PRE3]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we’ll calculate the completeness percentage:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将计算完整性百分比：
- en: '[PRE4]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will print the following output:'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印以下输出：
- en: '[PRE5]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The completeness check shows the number of missing values for each column, and
    the completeness percentage indicates the proportion of missing values in each
    column relative to the total number of records. This output indicates that the
    `Name` and `Gender` columns have 100% completeness, whereas the `Age` and `City`
    columns have 80%.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 完整性检查显示每列缺失值的数量，完整性百分比表示每列缺失值相对于总记录数的比例。该输出表明，`Name`（姓名）和`Gender`（性别）列的完整性为100%，而`Age`（年龄）和`City`（城市）列的完整性为80%。
- en: Note – completeness percentage
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注释 – 完整性百分比
- en: The completeness percentage is calculated by dividing the count of missing values
    by the total number of records in the dataset and then multiplying by 100\. It
    represents the proportion of missing values relative to the size of the dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 完整性百分比通过将缺失值的数量除以数据集中的总记录数，然后乘以100来计算。它表示缺失值相对于数据集大小的比例。
- en: '**The higher the percentage of completeness is,** **the better!**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**完整性百分比越高，** **越好！**'
- en: Accuracy
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准确性
- en: Accuracy assesses the correctness of data by comparing it to a trusted source
    or benchmark.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性通过将数据与可信来源或基准进行比较，来评估数据的正确性。
- en: 'The following code will output the accuracy percentage based on the comparison
    between the actual and expected values:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将根据实际值与期望值的比较输出准确度百分比：
- en: 'We’ll start by loading the required libraries:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载所需的库：
- en: '[PRE6]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we create a sample dataset named `data` and a reference dataset named
    `reference_data`. Both datasets have the same structure (columns: `Name`, `Age`,
    `Gender`, and `City`):'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个名为`data`的示例数据集和一个名为`reference_data`的参考数据集。两个数据集具有相同的结构（列：`Name`、`Age`、`Gender`和`City`）：
- en: '[PRE7]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We create two pandas DataFrames, named `df` and `reference_df`, using the sample
    data and the reference data, respectively:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用示例数据和参考数据分别创建两个名为`df`和`reference_df`的pandas数据框：
- en: '[PRE8]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We create an `accuracy_check` variable and assign the result of the comparison
    between `df` and `reference_df` to it. This comparison is done using the `==`
    operator, which returns `True` for matching values and `False` for non-matching
    values:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个`accuracy_check`变量，并将`df`和`reference_df`之间比较的结果赋值给它。此比较使用`==`运算符，它为匹配的值返回`True`，为不匹配的值返回`False`：
- en: '[PRE9]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can compare the column with actual values to the column with expected values
    using the `==` operator.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以使用`==`运算符将实际值列与期望值列进行比较。
- en: 'We then calculate the accuracy percentage by taking the mean of the `accuracy_check`
    DataFrame for each column. The `mean` operation treats `True` as `1` and `False`
    as `0`, so it effectively calculates the percentage of matching values in each
    column:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们然后通过对每一列的`accuracy_check`数据框求平均值来计算准确率百分比。`mean`操作将`True`视为`1`，将`False`视为`0`，因此它有效地计算了每一列中匹配值的百分比：
- en: '[PRE10]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we print the results:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印结果：
- en: '[PRE11]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE12]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The accuracy check shows `True` where the data matches the reference dataset
    and `False` where it doesn’t. The accuracy percentage indicates the proportion
    of matching values in each column relative to the total number of records. This
    output indicates that the `Age` column is the only one that needs some more attention
    in this case. All the others are 100% accurate.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确性检查显示`True`表示数据与参考数据集匹配，`False`表示不匹配。准确率百分比表示每列中匹配值相对于记录总数的比例。该输出表示在这种情况下，`Age`列是唯一需要更多关注的列。其他所有列的准确率为100%。
- en: Note – accuracy percentage
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 – 准确率百分比
- en: An accuracy percentage can be calculated by taking the mean (average) of the
    comparison results for all columns and multiplying by 100\. This represents the
    proportion of matching data values relative to the total number of data points.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率百分比可以通过对所有列的比较结果求平均值，并乘以100来计算。这个百分比代表了匹配数据值相对于总数据点数量的比例。
- en: The higher the percentage of accuracy is, the better!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率越高，结果越好！
- en: Wondering how to build the ground truth dataset?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 想知道如何构建基准数据集吗？
- en: The ground truth must be representative of the task you are trying to solve.
    This means that depending on where you are in the data life cycle, the ground
    truth is built differently and plays a different role as well.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基准数据必须能够代表你要解决的任务。这意味着，取决于数据生命周期的不同阶段，基准数据的构建方式不同，所起的作用也不同。
- en: 'Building a ground truth dataset is essential across various data personas,
    including data engineers, data analysts, and machine learning practitioners. For
    data engineers, ground truth is critical for data validation and testing. The
    good thing is that in most cases, data engineers can build the ground truth labels
    from historical data with the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 构建基准数据集对于各种数据角色至关重要，包括数据工程师、数据分析师和机器学习从业者。对于数据工程师而言，基准数据对于数据验证和测试至关重要。好消息是，在大多数情况下，数据工程师可以通过历史数据构建基准标签，方法如下：
- en: '**Data validation rules**: Establish validation rules and constraints to verify
    the accuracy of the data as it flows through the pipeline.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据验证规则**：建立验证规则和约束，以验证数据在流经管道时的准确性。'
- en: '**Manual inspection**: Manually inspect data samples to identify inconsistencies
    or errors and create a dataset of validated and corrected data. This can serve
    as the ground truth dataset.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工检查**：手动检查数据样本以识别不一致性或错误，并创建经过验证和修正的数据集。这可以作为基准数据集。'
- en: 'Data analysts rely on ground truth data to validate the accuracy of their findings,
    which can be acquired through expert annotations, historical data, and user feedback,
    ensuring that analytical insights reflect real-world phenomena:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析师依赖基准数据来验证他们的发现的准确性，这些数据可以通过专家注释、历史数据和用户反馈获得，确保分析见解反映了现实世界的现象：
- en: '**Expert annotations**: If working with unstructured or text data, domain experts
    can manually annotate data samples with the correct labels or categories, serving
    as ground truth for analysis.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专家注释**：如果处理的是非结构化或文本数据，领域专家可以手动为数据样本标注正确的标签或类别，作为分析的真实标签。'
- en: '**Historical data**: Use historical data with well-documented accuracy to serve
    as ground truth. This can be valuable when analyzing trends, patterns, or historical
    events.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**历史数据**：使用具有良好文档化准确度的历史数据作为真实标签。这在分析趋势、模式或历史事件时尤其有价值。'
- en: '**Surveys and user feedback**: Collect data from surveys or user feedback to
    validate insights and conclusions drawn from the data. These can serve as qualitative
    ground truth.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调查和用户反馈**：从调查或用户反馈中收集数据，以验证从数据中得出的见解和结论。这些可以作为定性真实标签。'
- en: 'Lastly, in the context of machine learning, ground truth data forms the backbone
    for model training and evaluation:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在机器学习的背景下，真实标签数据是模型训练和评估的基础：
- en: '**Manual labeling**: Annotate data samples manually to create a labeled dataset.
    This is common for tasks such as image classification, sentiment analysis, or
    object detection.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手动标注**：手动为数据样本添加注释，以创建标注数据集。这对于图像分类、情感分析或目标检测等任务很常见。'
- en: '**Crowdsourcing**: Use crowdsourcing platforms to collect annotations from
    multiple human workers, who collectively establish ground truth data.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**众包**：使用众包平台收集来自多个人工工人的注释，他们共同建立真实标签数据。'
- en: '**Existing datasets**: Many machine learning tasks benefit from established
    benchmark datasets that have been widely used in the research community. You can
    use and update these datasets to your needs.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现有数据集**：许多机器学习任务从已被研究社区广泛使用的基准数据集中获益。你可以根据需要使用和更新这些数据集。'
- en: '**Domain expert labels**: Consult domain experts to provide labels or annotations
    for data, especially when domain-specific knowledge is required.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域专家标签**：请咨询领域专家，为数据提供标签或注释，尤其是在需要领域特定知识时。'
- en: '**Synthetic data generation**: Create synthetic data with known ground truth
    labels to develop and test machine learning models. This is especially useful
    in the absence of real-world labeled data.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合成数据生成**：生成带有已知真实标签的合成数据，以开发和测试机器学习模型。这在没有真实标签数据的情况下尤其有用。'
- en: Regardless of the persona, it is vital to create, maintain, and continually
    assess the quality of ground truth data, being mindful of potential biases and
    limitations. This is because it significantly influences the effectiveness of
    data engineering, analysis, and machine learning efforts.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是哪种角色，创建、维护并不断评估真实标签数据的质量都是至关重要的，并要注意潜在的偏差和局限性。因为它对数据工程、分析和机器学习工作效果有着重要影响。
- en: Timeliness
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时效性
- en: Timeliness evaluates how quickly data is captured, processed, and made available
    for use. Timeliness KPIs may include metrics such as data latency (time elapsed
    between data capture and availability) or adherence to data refresh schedules.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 时效性评估数据的捕获、处理和可用的速度。时效性关键绩效指标（KPI）可能包括诸如数据延迟（数据捕获与可用之间的时间差）或遵循数据刷新计划等指标。
- en: 'Measuring timeliness in data involves assessing the freshness or currency of
    the data with respect to a specific timeframe or event. Let’s look at an example:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 测量数据的时效性涉及评估数据在特定时间范围或事件中的新鲜度或有效性。让我们来看一个例子：
- en: 'We start by importing the required libraries:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入所需的库：
- en: '[PRE13]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We then generate a random dataset with timestamps and values. The timestamps
    are randomly distributed within a given time range to simulate real-world data:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们生成一个包含时间戳和值的随机数据集。时间戳在给定时间范围内随机分布，以模拟真实世界的数据：
- en: '[PRE14]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We define a reference timestamp to compare the dataset’s timestamps to:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义一个参考时间戳，用来与数据集的时间戳进行对比：
- en: '[PRE15]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We set a timeliness threshold of 30 minutes. Data with timestamps within 30
    minutes of the reference timestamp will be considered timely:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了30分钟的时效性阈值。时间戳在参考时间戳30分钟以内的数据将被视为及时：
- en: '[PRE16]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We calculate the timeliness for each record in the dataset by computing the
    time difference in minutes between the reference timestamp and each record’s timestamp.
    We also create a Boolean column to indicate whether the record is timely based
    on the threshold:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过计算参考时间戳与每条记录时间戳之间的时间差（以分钟为单位），来计算数据集中每条记录的时效性。我们还创建了一个布尔列，用来指示记录是否符合时效性标准，根据阈值判断：
- en: '[PRE17]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we calculate the average timeliness of the dataset and display the
    results:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们计算数据集的平均时效性并显示结果：
- en: '[PRE18]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will display the following output:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将显示以下输出：
- en: '[PRE19]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This example provides a more realistic simulation of timeliness in a dataset
    with randomly generated timestamps and a timeliness threshold. *The average timeliness
    represents the average time deviation from the reference timestamp for* *the dataset*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例提供了一个更真实的数据集及时性模拟，其中包含随机生成的时间戳和及时性阈值。*平均及时性表示数据集相对于* *参考时间戳的平均时间偏差*。
- en: Good timeliness
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 良好的及时性
- en: A low average timeliness and a high percentage of timely records suggest that
    the dataset is current and aligns well with the reference timestamp. This is desirable
    in real-time applications or scenarios where up-to-date data is critical.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 低平均及时性和高及时记录的百分比表明数据集当前，并且与参考时间戳很好地对齐。这在实时应用或数据最新性至关重要的场景中是可取的。
- en: 'An important consideration here is how to define the reference timestamp. The
    reference timestamp is the point in time to which the dataset’s timestamps are
    compared. It represents the expected or desired time for the data. For example:
    a record was created when a loyalty card was scanned at a retail store, and the
    reference time is when the record was logged in the database. So, we are calculating
    the time it took from the real creation of the event to a new entry to be stored
    in the database.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个重要考虑因素是如何定义参考时间戳。参考时间戳是数据集时间戳与之比较的时间点。它代表数据的期望或预期时间。例如：在零售店扫描会员卡时创建记录的时间，参考时间是记录在数据库中登录的时间。因此，我们计算的是从事件实际创建到新条目存储在数据库中所需的时间。
- en: 'The smaller the reference threshold, the more *real-time* the application needs
    to be. On the other hand, the bigger the reference threshold is, the more time
    it takes to bring the data into your system (**batch application**). The choice
    between real-time and batch processing depends on the specific requirements of
    your application:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 参考阈值越小，应用程序需要越*实时*。另一方面，参考阈值越大，将数据带入系统所需的时间越长（**批量应用**）。实时处理和批处理之间的选择取决于应用程序的具体要求：
- en: '**Real-time processing**: Choose real-time processing when immediate responses,
    low latency, and the ability to act on data as it arrives are crucial. It’s suitable
    for applications where time-sensitive decisions are made.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时处理**：当需要即时响应、低延迟以及能够在数据到达时立即采取行动时，选择实时处理是合适的。适用于需要做出时间敏感决策的应用场景。'
- en: '**Batch processing**: Select batch processing when low latency is not a strict
    requirement, and you can tolerate some delay in data processing. Batch processing
    is often more cost-effective and suitable for tasks that can be scheduled and
    automated.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理**：当低延迟不是严格要求，且可以容忍数据处理中的一些延迟时，选择批处理通常更具成本效益，适合可以安排和自动化的任务。'
- en: How timeliness definitions change across different data personas
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不同数据角色中及时性定义的变化
- en: 'Timeliness is an essential aspect of data quality with applications across
    various roles, from data engineers to data analysts and machine learning practitioners.
    Here’s how each role can leverage timeliness in the real world:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 及时性是数据质量的一个重要方面，适用于各种角色，从数据工程师到数据分析师和机器学习从业者。以下是每个角色如何在现实世界中利用及时性：
- en: '**Data engineers**:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据工程师**：'
- en: '**Data pipeline monitoring**: Data engineers can use timeliness as a key metric
    for monitoring data pipelines. They can set up automated alerts or checks to ensure
    that data is arriving on time, identifying, and addressing delays in data ingestion.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据管道监控**：数据工程师可以将及时性作为监控数据管道的关键指标。他们可以设置自动警报或检查，确保数据按时到达，并识别和解决数据摄取中的延迟。'
- en: '**Data validation**: Data engineers can incorporate timeliness checks as part
    of their data validation processes, ensuring that data meets specified timing
    criteria before it is used for downstream processes.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据验证**：数据工程师可以将及时性检查作为其数据验证流程的一部分，确保数据在用于下游流程之前满足指定的时间条件。'
- en: '**Data analysts**:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分析师**：'
- en: '**Real-time analytics**: Analysts in domains such as finance or e-commerce
    rely on real-time data to make informed decisions. They need to ensure that the
    data they analyze is up to date and reflects the current state of affairs.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时分析**：金融或电子商务等领域的分析师依赖实时数据做出信息化决策。他们需要确保分析的数据是最新的，反映当前的情况。'
- en: '**KPI monitoring**: Timeliness is essential in monitoring KPIs. Analysts use
    timely data to track and assess the performance of various business metrics.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KPI监控**：及时性在监控关键绩效指标中至关重要。分析师使用及时数据来跟踪和评估各种业务指标的表现。'
- en: '**Machine** **learning practitioners**:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习从业者**：'
- en: '**Feature engineering**: Timeliness plays a role in feature engineering for
    machine learning models. It’s important to keep features as up-to-date as possible
    because this has a direct impact on model training and scoring.'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程**：及时性在机器学习模型的特征工程中起着重要作用。保持特征尽可能更新对模型的训练和评分有直接影响。'
- en: '**Model training and evaluation**: In real-time predictive models, model training
    and evaluation rely on timely data. Practitioners must ensure that the training
    data is current to build effective models or perform real-time inference.'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练和评估**：在实时预测模型中，模型训练和评估依赖于及时数据。从业者必须确保训练数据是当前的，以建立有效的模型或执行实时推断。'
- en: '**Concept drift detection**: Timeliness is critical in detecting concept drift,
    which occurs when the relationships within the data change over time. Machine
    learning models need to adapt to these changes, and timely data is necessary to
    monitor and detect such drift.'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概念漂移检测**：及时性对于检测概念漂移至关重要，概念漂移是指数据内部关系随时间变化的情况。机器学习模型需要适应这些变化，及时的数据监控和检测变化至关重要。'
- en: 'Here are some real applications of timeliness:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是及时性的一些实际应用：
- en: '**Finance**: In the financial sector, timeliness is crucial for stock trading,
    fraud detection, and risk management, where timely data can lead to better decisions
    and reduced risks'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金融**：在金融领域，及时性对股票交易、欺诈检测和风险管理至关重要，及时的数据可以带来更好的决策和降低风险。'
- en: '**Healthcare**: Timeliness is vital for healthcare data, particularly in patient
    monitoring and real-time health data analysis'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗保健**：及时性对于医疗数据至关重要，特别是在患者监测和实时健康数据分析中。'
- en: '**E-commerce**: Timely data is essential for e-commerce companies to monitor
    sales, customer behavior, and inventory in real-time'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电子商务**：及时的数据对电子商务公司来说至关重要，可以实时监控销售、客户行为和库存。'
- en: '**Transportation and L=logistics**: In supply chain management and logistics,
    real-time tracking and timely data are essential for route optimization and inventory
    management'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运输与物流**：在供应链管理和物流中，实时跟踪和及时的数据对路线优化和库存管理至关重要。'
- en: Let’s move on to consistency.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续谈一谈一致性。
- en: Consistency
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一致性
- en: 'Consistency measures the degree of consistency within the data and involves
    ensuring that data follows established rules, standards, and formats throughout
    a dataset. In more detail, we should check for the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性衡量数据内部一致性的程度，包括确保数据在整个数据集中遵循建立的规则、标准和格式。具体而言，我们应检查以下内容：
- en: '**Same format**: Data should follow the same format, structure, and standards
    across all records or columns. This uniformity ensures that data can be easily
    processed and compared.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相同格式**：所有记录或列的数据都应该遵循相同的格式、结构和标准。这种一致性确保数据可以轻松处理和比较。'
- en: '**Adherence to standards**: Data should adhere to predefined rules, guidelines,
    naming conventions, and reference data. For example, if a dataset contains product
    names, consistency would require that all product names follow a standardized
    naming convention.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遵守标准**：数据应符合预定义的规则、指南、命名约定和参考数据。例如，如果数据集包含产品名称，一致性要求所有产品名称都遵循标准化的命名约定。'
- en: '**Data type and format**: Consistency checks include verifying that data types
    (e.g., text, numbers, and dates) and data formats (e.g., date formats and numerical
    representations) are consistent.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据类型和格式**：一致性检查包括验证数据类型（例如文本、数字和日期）和数据格式（例如日期格式和数值表示）是否一致。'
- en: 'Let’s get a better understanding with an example:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子更好地理解：
- en: 'We start by importing the `pandas` library:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入`pandas`库：
- en: '[PRE20]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We then create a sample dataset with product information, including product
    names. In this example, we’ll check whether all product names start with `PROD`
    as per the naming convention:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们创建一个包含产品信息的样本数据集，包括产品名称。在这个例子中，我们将检查所有产品名称是否按照命名约定以`PROD`开头：
- en: '[PRE21]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s define the expected prefix:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义预期的前缀：
- en: '[PRE22]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We check the consistency of the `ProductName` column by ensuring that all product
    names start with `PROD`. Inconsistent names will be flagged:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过确保所有产品名称以`PROD`开头来检查`ProductName`列的一致性。不一致的名称将被标记：
- en: '[PRE23]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then we calculate the percentage of consistent rows:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们计算一致行的百分比：
- en: '[PRE24]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, we display the results, including the dataset with the consistency
    check results:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们展示结果，包括包含完整性检查结果的数据集：
- en: '[PRE25]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here’s the final output:'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是最终输出：
- en: '[PRE26]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In this specific dataset, three out of the five product names are consistent
    with the naming convention, resulting in an 80% consistency rate. The `Product003`
    entry is marked as inconsistent because it does not start with `PROD`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的数据集中，五个产品名称中有三个符合命名规范，因此一致性率为80%。`Product003`条目被标记为不一致，因为它没有以`PROD`开头
- en: This type of consistency check can be useful for ensuring that data adheres
    to specific standards or conventions, and the calculated percentage provides a
    quantitative measure of how many records meet the criteria.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的完整性检查对于确保数据遵循特定标准或约定非常有用，计算出的百分比提供了一个定量衡量，表示有多少记录符合标准
- en: Note
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Higher consistency percentages imply more uniformity and conformity in the values
    within the respective columns. If there is a very low percentage, then we have
    a lot of distinct values in the dataset, which is not a mistake as long as we
    understand what the column represents and there is a meaning behind all the unique
    values.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的一致性百分比意味着相应列中的值更加一致和符合标准。如果百分比很低，那么说明数据集中有许多不同的值，只要我们理解列代表的含义并且所有唯一值都有其背后的意义，这并不是一个错误
- en: Are you wondering about what other consistency checks you could apply?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否在想，还可以应用哪些其他的完整性检查？
- en: '![](img/01.jpg)![](img/02.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01.jpg)![](img/02.jpg)'
- en: Table 2.1 – Different consistency check options
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1 – 不同的完整性检查选项
- en: Let’s discuss uniqueness next.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们接下来讨论唯一性
- en: Uniqueness
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 唯一性
- en: Uniqueness measures the presence of unique values in a dataset. It can help
    identify anomalies such as duplicated keys.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一性衡量数据集中唯一值的存在。它可以帮助识别异常，例如重复的键
- en: 'The code will output the validity results for each column, indicating whether
    the values in each column conform to the defined validity rules:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将输出每一列的有效性结果，指示每一列中的值是否符合定义的有效性规则：
- en: 'We import the `pandas` library:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入`pandas`库：
- en: '[PRE27]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We then create a sample dataset containing email addresses. We want to check
    whether all email addresses in the dataset are unique:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们创建一个包含电子邮件地址的示例数据集。我们想检查数据集中的所有电子邮件地址是否唯一：
- en: '[PRE28]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We check the uniqueness of the `Email` column to ensure that no email address
    is duplicated in the dataset:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们检查`Email`列的唯一性，确保数据集中的电子邮件地址没有重复：
- en: '[PRE29]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we calculate the uniqueness percentage:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们计算唯一性百分比：
- en: '[PRE30]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we display the results, including the dataset with the uniqueness
    check results. Here’s the output:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们展示结果，包括包含唯一性检查结果的数据集。以下是输出：
- en: '[PRE31]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This output indicates that the values in the dataset are all unique.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出表示数据集中的值都是唯一的
- en: 'Uniqueness checks are commonly performed in various industries and use cases.
    Here are some common examples of uniqueness checks in real-world scenarios:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一性检查在各个行业和使用案例中都很常见。以下是一些常见的现实场景中唯一性检查的例子：
- en: '**Customer IDs**: In a customer database, each customer should have a unique
    identifier (customer ID) to prevent duplicate customer records'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户编号**：在客户数据库中，每位客户应有一个唯一的标识符（客户编号），以防止重复的客户记录'
- en: '**Product SKUs**: In inventory and e-commerce databases, each product should
    have a unique SKU to identify and manage products without duplication'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**产品SKU**：在库存和电子商务数据库中，每个产品应该有一个唯一的SKU，以便识别和管理产品，避免重复'
- en: '**Email Addresses**: Email addresses should be unique in a mailing list or
    user database to avoid sending duplicate messages or creating multiple accounts
    with the same email'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电子邮件地址**：电子邮件地址在邮件列表或用户数据库中应该是唯一的，以避免发送重复的邮件或创建多个相同电子邮件地址的账户'
- en: '**Employee IDs**: In HR databases, each employee typically has a unique employee
    ID to differentiate between employees and manage their records effectively'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**员工编号**：在人力资源数据库中，每位员工通常有一个唯一的员工编号，用以区分员工并有效管理他们的记录'
- en: '**Vehicle Identification Numbers** (**VINs**): In the automotive industry,
    VINs are unique identifiers for each vehicle to track their history and ownership'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**车辆识别号码**（**VIN**）：在汽车行业中，VIN是每辆车的唯一标识符，用于追踪其历史和所有权'
- en: '**Barcodes and QR codes**: In retail and logistics, barcodes and QR codes provide
    unique identifiers for products, packages, and items for tracking and inventory
    management'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条形码和二维码**：在零售和物流行业，条形码和二维码为产品、包裹和物品提供唯一的标识符，用于追踪和库存管理'
- en: '**Usernames and user IDs**: In online platforms and applications, usernames
    and user IDs are unique to each user to distinguish them and manage user accounts'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户名和用户ID**：在在线平台和应用中，用户名和用户ID对每个用户都是唯一的，用来区分用户并管理账户。'
- en: '**Serial Numbers**: In manufacturing, products often have unique serial numbers
    to identify and track individual items'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列号**：在制造业中，产品通常会有唯一的序列号，用于识别和跟踪单个物品。'
- en: '**Transaction IDs**: In financial systems, each transaction is assigned a unique
    transaction ID to prevent duplication and ensure proper record-keeping'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交易ID**：在金融系统中，每笔交易都会分配一个唯一的交易ID，以防止重复并确保正确的记录保存。'
- en: When you have non-unique records in a dataset, it means that there are duplicate
    entries or records with identical key attributes (e.g., IDs, names, or email addresses)
    in the dataset. Non-unique records can lead to data quality issues and potentially
    cause errors in data analysis and reporting. To fix non-unique records in a dataset
    using Python, you can use various methods, including removing duplicates, aggregating
    data, or resolving conflicts based on your specific data and requirements. We’ll
    discuss those strategies in a later chapter.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集中有非唯一记录时，意味着存在具有相同关键属性（例如，ID、姓名或电子邮件地址）的重复条目或记录。非唯一记录可能会导致数据质量问题，甚至在数据分析和报告时产生错误。要修复数据集中的非唯一记录，你可以使用多种方法，包括去除重复、汇总数据或根据具体数据和需求解决冲突。我们将在后续章节讨论这些策略。
- en: Duplication
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重复数据
- en: Duplication assesses the presence of duplicate or redundant data within the
    dataset. Having duplicates in your data means that you have the same piece of
    information or record repeated multiple times within your dataset.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 重复数据评估数据集中的重复或冗余数据的存在。如果你的数据中有重复记录，意味着同一条信息或记录在数据集中出现多次。
- en: Example – customer database
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例——客户数据库
- en: Imagine you work for a company with a customer database that tracks information
    about each customer, including their contact details, purchases, and interactions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在一个公司工作，公司拥有一个客户数据库，跟踪每个客户的信息，包括他们的联系详情、购买记录和互动信息。
- en: Issue – duplicate customer records
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题——重复的客户记录
- en: You discover that there are duplicate customer records in the database. These
    duplicates may have occurred due to data entry errors, system issues, or other
    reasons. For instance, a customer named John Smith has two separate records with
    slight variations in contact details. One record has his email address as `john.smith@email.com`,
    and the other has `jsmith@email.com`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你发现数据库中有重复的客户记录。这些重复记录可能是由于数据输入错误、系统问题或其他原因造成的。例如，一个名为John Smith的客户有两个独立的记录，联系信息略有不同。一个记录的电子邮件地址是`john.smith@email.com`，另一个是`jsmith@email.com`。
- en: 'This is generally considered undesirable for several reasons:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被认为是不可取的，原因有多个：
- en: '**Data accuracy**: When you have multiple copies of the same information, it
    becomes challenging to determine which version is correct or up-to-date. This
    can lead to data inconsistency and confusion.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据准确性**：当你有多个相同信息的副本时，很难确定哪个版本是正确或最新的。这可能导致数据不一致和混乱。'
- en: '**Storage efficiency**: Duplicated records consume unnecessary storage space.
    This is particularly important when dealing with large datasets, as it can lead
    to increased storage costs and longer data retrieval times.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储效率**：重复记录会占用不必要的存储空间。尤其在处理大数据集时，这可能导致存储成本增加和数据检索时间延长。'
- en: '**Data integrity**: Duplicates can compromise data integrity. In situations
    where data relationships are critical, duplicated records can disrupt the integrity
    of your data model.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据完整性**：重复记录可能会影响数据完整性。在数据关系至关重要的情况下，重复记录可能会破坏数据模型的完整性。'
- en: '**Efficient data processing**: Analyzing, querying, and processing datasets
    with fewer duplicates is more efficient. Data processing times are shorter, and
    results are more meaningful when you’re not dealing with repetitive information.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效的数据处理**：分析、查询和处理重复较少的数据集会更高效。处理时间更短，结果也更有意义，因为你不需要处理重复信息。'
- en: '**Data analysis**: When performing data analysis or running statistical models,
    duplicated records can skew results and lead to incorrect conclusions. Reducing
    duplicates is crucial for accurate and meaningful analysis.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分析**：在进行数据分析或运行统计模型时，重复记录可能会扭曲结果，导致错误的结论。减少重复数据对于准确和有意义的分析至关重要。'
- en: '**Cost savings**: Storing and managing duplicates incurs additional costs in
    terms of storage infrastructure and data management efforts. Eliminating duplicates
    can lead to cost savings.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本节约**：存储和管理重复记录会增加存储基础设施和数据管理的成本。消除重复记录可以带来成本节约。'
- en: Now imagine what happens if we extrapolate the same problem to a company that
    manages millions of customers. Could you see how expensive and confusing introducing
    duplicates could be?
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们将同样的问题推广到管理数百万客户的公司。你能想象引入重复记录会有多么昂贵和混乱吗？
- en: 'While it’s generally a best practice to minimize duplicate records in a dataset,
    there are some specific scenarios where accepting or allowing duplicates might
    be a reasonable choice:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在数据集中最小化重复记录通常是一种最佳实践，但在某些特定场景下，接受或允许重复记录可能是一个合理的选择：
- en: '| **Scenario** | **Data representation** |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| **场景** | **数据表示** |'
- en: '| Customer order history | Each row represents a separate order made by a customer.
    Multiple rows with the same customer ID are allowed to show order history. |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 客户订单历史 | 每一行代表客户的单独订单。允许同一客户 ID 的多行以展示订单历史。 |'
- en: '| Service requests | Records represent service requests, including multiple
    requests from the same customer or location over time. Duplicates are allowed
    to maintain a detailed history. |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 服务请求 | 记录代表服务请求，包括同一客户或地点随时间发生的多个请求。允许重复记录以保留详细历史。 |'
- en: '| Sensor data | Each row contains sensor readings, which can include multiple
    entries with the same data values over time. Duplicates are allowed for tracking
    each reading. |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 传感器数据 | 每一行包含传感器读数，可能包括多个相同数据值的条目。允许重复记录以跟踪每次读数。 |'
- en: '| Logging and audit trails | Log entries record events or actions, and some
    events may generate duplicate entries. Duplicates are preserved for detailed audit
    trails. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 日志记录和审计跟踪 | 日志条目记录事件或操作，有些事件可能生成重复条目。保留重复记录以便进行详细的审计跟踪。 |'
- en: '| User interaction data | Records capture user interactions with a website
    or application. Duplicates can represent repeated interactions for analysis of
    user behavior. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 用户交互数据 | 记录捕捉用户与网站或应用程序的交互。重复记录可以代表重复的交互，用于分析用户行为。 |'
- en: '| Change history | Data versions or document changes result in multiple records,
    including duplicates that capture historical revisions. Duplicates are maintained
    for version history. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 变更历史 | 数据版本或文档变更导致多个记录，包括捕捉历史修订的重复记录。保留重复记录用于版本历史。 |'
- en: Table 2.2 – Duplicated records scenarios
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.2 – 重复记录场景
- en: In these scenarios, allowing duplicates serves specific data management or analysis
    goals, such as preserving historical data, maintaining a record of changes, or
    capturing detailed user interactions. The data representation aligns with these
    objectives, and duplicates are *intentionally* retained to support these use cases.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些场景中，允许重复记录是为了实现特定的数据管理或分析目标，例如保留历史数据、记录变更或捕捉详细的用户交互。数据的表示方式与这些目标一致，重复记录是*故意*保留的，以支持这些用例。
- en: 'Let’s see how to track duplicates with a code example. The code will output
    the number of duplicate records found in your dataset:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个代码示例来看如何跟踪重复记录。代码将输出数据集中找到的重复记录数量：
- en: 'First, we import `pandas`:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入 `pandas`：
- en: '[PRE32]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we create a sample dataset with employee information. We’ll intentionally
    introduce duplicate employee IDs to demonstrate the identification of duplicates:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含员工信息的示例数据集。我们故意引入重复的员工 ID 来演示如何识别重复记录：
- en: '[PRE33]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We’ll use pandas to identify and mark duplicate records based on the `EmployeeID`
    column. The `duplicated()` function is used to create a Boolean mask, where `True`
    indicates a duplicated record:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用 pandas 根据 `EmployeeID` 列来识别并标记重复记录。`duplicated()` 函数用于创建布尔掩码，`True` 表示重复记录：
- en: '[PRE34]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `subset='EmployeeID'` argument specifies the column on which the duplication
    check is performed. `keep='first'` marks duplicates as `True` except for the first
    occurrence. You can change this parameter to `last` or `False` based on your requirements.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`subset=''EmployeeID''` 参数指定用于检查重复的列。`keep=''first''` 会将重复记录标记为 `True`，除了第一次出现的记录。你可以根据需要将此参数更改为
    `last` 或 `False`。'
- en: 'We then create a new column called `''IsDuplicate''` in the DataFrame to indicate
    whether each record is a duplicate or not:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在 DataFrame 中创建一个新的列 `'IsDuplicate'`，用于指示每条记录是否为重复记录：
- en: '[PRE35]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We calculate the percentage of duplicate records by dividing the number of
    duplicate records (those marked as `True` in the `''IsDuplicate''` column) by
    the total number of records and then multiplying by 100 to express it as a percentage:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过将重复记录的数量（在`IsDuplicate`列中标记为`True`的记录）除以记录总数，然后乘以100来计算重复记录的百分比，并将其表示为百分比：
- en: '[PRE36]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, we display the dataset with the `IsDuplicate` column to see which
    records are duplicates. Here’s the final output:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们显示包含`IsDuplicate`列的数据集，以查看哪些记录是重复的。以下是最终的输出：
- en: '[PRE37]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This output indicates that 28.57% of records are duplicated in the dataset.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出表示数据集中的28.57%的记录是重复的。
- en: Note
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '**The fewer duplicated records,** **the better!**'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**重复记录越少，** **越好！**'
- en: The threshold for what is considered an acceptable or “good” level of duplicated
    records in a dataset can vary depending on the specific context and the goals
    of your data management or analysis. There is no one-size-fits-all answer to this
    question, as it depends on factors such as the type of data, the purpose of the
    dataset, and industry standards.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中什么样的重复记录数量被认为是“可接受”或“良好”的标准，可能会根据具体情况和数据管理或分析的目标而有所不同。这个问题没有统一的答案，因为它取决于数据类型、数据集的目的和行业标准等因素。
- en: Data usage
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据使用
- en: Data usage assesses the extent to which data is effectively utilized within
    the organization. Data usage KPIs can include metrics such as data utilization
    rates, the number of data requests or queries, or user satisfaction surveys regarding
    data availability and quality.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 数据使用评估数据在组织内的有效利用程度。数据使用的关键绩效指标（KPI）可以包括数据利用率、数据请求或查询的数量，或者关于数据可用性和质量的用户满意度调查。
- en: Scenario – corporate business intelligence dashboard
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 场景 – 企业商业智能仪表板
- en: Imagine a large corporation that relies on data-driven decision-making to optimize
    its operations, marketing strategies, and financial performance. The corporation
    has a centralized **Business Intelligence** (**BI**) dashboard that provides various
    data analytics and insights to different departments and teams. This dashboard
    is crucial for monitoring the company’s performance and making informed decisions.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一家大型企业依赖数据驱动决策来优化其运营、营销策略和财务表现。该公司有一个集中式的**商业智能**（**BI**）仪表板，向不同部门和团队提供各种数据分析和洞察。这个仪表板对于监控公司的业绩并做出明智决策至关重要。
- en: 'In this scenario, assessing data usage metrics can be vital for optimizing
    the BI dashboard’s effectiveness and ensuring it meets the organization’s data
    needs. Here’s what we will track in the code example:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，评估数据使用指标对于优化BI仪表板的有效性以及确保它满足组织的数据需求至关重要。以下是我们在代码示例中将要追踪的内容：
- en: '**Data utilization rates**: By tracking data utilization rates for different
    departments and teams, the organization can assess how often the dashboard is
    accessed and how extensively the data within it is used. For example, the marketing
    department might have a high data utilization rate, indicating a heavy reliance
    on the dashboard for campaign performance analysis. This metric can help identify
    areas of the organization where data-driven insights are most critical.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据利用率**：通过跟踪不同部门和团队的数据利用率，组织可以评估仪表板的访问频率以及其中数据的使用广度。例如，营销部门可能有较高的数据利用率，表明他们在依赖仪表板进行活动绩效分析。此指标有助于识别组织中数据驱动洞察最为关键的领域。'
- en: '**Number of data requests or queries**: Monitoring the number of data requests
    or queries made by users provides insights into the volume of data analysis conducted
    through the dashboard. High data request numbers may indicate a strong appetite
    for data-driven decision-making. This metric can also help identify peak usage
    times and popular data sources.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据请求或查询的数量**：监控用户发出的数据请求或查询的数量，可以为我们提供关于通过仪表板进行的数据分析量的洞察。较高的数据请求数量可能表明对数据驱动决策的强烈需求。此指标还可以帮助识别使用高峰时段和受欢迎的数据源。'
- en: '**User satisfaction scores**: Collecting user satisfaction scores through surveys
    can gauge how well the BI dashboard meets user expectations. A lower average user
    satisfaction score may signal that the dashboard’s features or user experience
    need improvement. Feedback from users can guide dashboard enhancements.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户满意度得分**：通过调查收集用户满意度得分，可以衡量BI仪表板在多大程度上满足用户的期望。较低的平均用户满意度得分可能表示仪表板的功能或用户体验需要改进。来自用户的反馈可以指导仪表板的改进。'
- en: '**Organization data utilization rate**: Calculating the overall data utilization
    rate for the entire organization helps assess the dashboard’s relevance and its
    effectiveness in serving the broader business goals. It also provides a benchmark
    to measure against in terms of data utilization improvements.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组织数据利用率**：计算整个组织的总体数据利用率有助于评估仪表板的相关性及其在实现更广泛业务目标中的有效性。这还为衡量数据利用率的改进提供了一个基准。'
- en: 'To calculate the number of data requests for the last month, you would need
    to log the data requests of your application with the associated timestamps. Let’s
    see an example:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算上个月的数据请求数量，你需要记录应用程序的数据请求及其相关的时间戳。我们来看一个例子：
- en: 'First, we import the `random` library:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入`random`库：
- en: '[PRE38]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we create a function to simulate data usage metrics. In this function,
    we set the number of users in the organization to 500 users, but in a real-world
    scenario, you would replace this with the actual number of users in your organization.
    Let’s have a look at the following function:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个函数来模拟数据使用指标。在这个函数中，我们将组织中的用户数量设置为500个用户，但在实际场景中，你需要用组织中实际的用户数量来替代。让我们来看一下下面的函数：
- en: '[PRE39]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The main goal of this function is to simulate data utilization rates for each
    user in the organization. The `random.uniform(20, 90)` function generates a random
    floating-point number between 20 and 90\. We do this for each user, resulting
    in a list of utilization rates. Similarly, we simulate the number of data requests
    or queries made by each user. Here, we use `random.randint(1, 100)` to generate
    a random integer between 1 and 100 for each user, representing the number of data
    requests. Next, we calculate two organization-level metrics. The first is the
    average data utilization rate for the entire organization, and the second is the
    total number of data requests or queries across all users. We simulate user satisfaction
    scores using a scale from 1 to 5\. Each user receives a random satisfaction score.
    We calculate the average user satisfaction score for the entire organization based
    on the simulated satisfaction scores.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数的主要目标是模拟组织中每个用户的数据利用率。`random.uniform(20, 90)`函数生成一个介于20和90之间的随机浮动数值。我们对每个用户都进行这种操作，结果是一个数据利用率的列表。同样，我们模拟每个用户所做的数据请求或查询的数量。在这里，我们使用`random.randint(1,
    100)`为每个用户生成一个1到100之间的随机整数，表示数据请求的次数。接下来，我们计算两个组织级的指标，第一个是整个组织的平均数据利用率，第二个是所有用户的总数据请求或查询次数。我们使用1到5的评分来模拟用户满意度分数。每个用户都会得到一个随机的满意度分数。我们基于模拟的满意度分数来计算整个组织的平均用户满意度分数。
- en: 'We call the `simulate_data_usage()` function to run the simulation and store
    the results in the `data_usage_metrics` variable:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们调用`simulate_data_usage()`函数来运行模拟并将结果存储在`data_usage_metrics`变量中：
- en: '[PRE40]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Finally, we display the simulated data usage metrics. The output is as follows:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们展示模拟的数据显示指标。输出结果如下：
- en: '[PRE41]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Capturing the data usage of different data products is crucial for several
    reasons, especially in the context of organizations that rely on data for decision-making
    and operational effectiveness:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 捕捉不同数据产品的使用情况对于几个原因至关重要，特别是在那些依赖数据进行决策和提升运营效率的组织中：
- en: '**Optimizing resources**: By understanding how data products are used, organizations
    can allocate resources effectively. This includes identifying which data sources
    are heavily utilized and which may be underused. It helps in optimizing data storage,
    processing, and infrastructure resources.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化资源**：通过了解数据产品的使用情况，组织可以有效地分配资源。这包括识别哪些数据源被大量使用，哪些数据源可能未被充分利用。这有助于优化数据存储、处理和基础设施资源。'
- en: '**Improving data quality**: Monitoring data usage can highlight data quality
    issues. For example, if certain data products are rarely accessed, it may indicate
    that the data quality is poor or that the data is no longer relevant. Capturing
    usage can trigger data quality improvements.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升数据质量**：监控数据使用情况可以突显数据质量问题。例如，如果某些数据产品很少被访问，可能表明这些数据质量较差或已不再相关。捕捉使用情况可以促进数据质量的改进。'
- en: '**Identifying trends and patterns**: Data usage patterns can reveal insights
    into how data is consumed and what types of analyses or reports are most valuable
    to users. This input can inform data product development and enhancement strategies.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别趋势和模式**：数据使用模式可以揭示数据如何被消费，以及哪些类型的分析或报告对用户最有价值。这些信息可以为数据产品的开发和改进策略提供依据。'
- en: '**Cost management**: Knowing which data products are in high demand helps manage
    data-related costs. It enables organizations to invest resources wisely and avoid
    unnecessary expenses on maintaining or storing less-used data.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本管理**：了解哪些数据产品需求量大有助于管理与数据相关的成本。它使组织能够明智投资资源，避免在维护或存储使用较少的数据上产生不必要的费用。'
- en: '**Security and compliance**: Tracking data usage is crucial for data security
    and compliance. Organizations can identify unauthorized access or unusual usage
    patterns that may indicate security breaches. It also helps in complying with
    data privacy regulations by demonstrating control over data access.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全和合规性**：跟踪数据使用对数据安全和合规性至关重要。组织可以识别未经授权访问或异常使用模式，这可能表明安全漏洞。它还有助于通过展示对数据访问的控制来遵守数据隐私法规。'
- en: '**User satisfaction**: Understanding how data products are used and whether
    they meet user needs is vital for user satisfaction. It allows organizations to
    tailor data products to user requirements, resulting in better user experiences.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户满意度**：了解数据产品的使用方式及其是否满足用户需求对用户满意度至关重要。它允许组织根据用户要求定制数据产品，从而提供更好的用户体验。'
- en: '**Capacity planning**: Capturing usage data helps in capacity planning for
    data infrastructure. It ensures that there is enough capacity to handle data traffic
    during peak usage periods, preventing performance bottlenecks.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容量规划**：捕捉使用数据有助于数据基础设施的容量规划。它确保在高峰使用期间有足够的容量处理数据流量，避免性能瓶颈。'
- en: '**Return on Investment** (**ROI**) **measurement**: For organizations investing
    in data products, tracking usage is essential for measuring the ROI. It helps
    determine whether the resources spent on data collection, processing, and presentation
    are justified by their impact on decision-making and business outcomes.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投资回报率** (**ROI**) **测量**：对于投资于数据产品的组织来说，跟踪使用情况对于衡量投资回报率至关重要。它有助于确定数据收集、处理和呈现所耗费的资源是否因其对决策和业务结果的影响而合理。'
- en: Next, let’s discuss data compliance.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论数据合规性。
- en: Data compliance
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据合规性
- en: Data compliance evaluates the extent to which data adheres to regulatory requirements,
    industry standards, or internal data governance policies. Compliance KPIs may
    involve metrics such as the number of non-compliant data records, the percentage
    of data complying with specific regulations, or the results of data compliance
    audits. Data compliance is crucial for several important reasons, especially in
    today’s data-driven and highly regulated business environment as presented in
    the following table.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 数据合规性评估数据遵守法规要求、行业标准或内部数据治理政策的程度。合规性KPI可能涉及诸如非合规数据记录数量、符合特定法规的数据百分比或数据合规审计结果等指标。在今天的数据驱动和高度监管的业务环境中，数据合规性对几个重要原因尤为关键，如下表所示。
- en: '| **Consequence/challenge** | **Description** |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| **后果/挑战** | **描述** |'
- en: '| Legal and regulatory consequences | Non-compliance can lead to legal actions,
    fines, and penalties |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 法律和监管后果 | 不合规可能导致法律诉讼、罚款和处罚 |'
- en: '| Reputational damage | Negative publicity and loss of trust among customers
    and stakeholders |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 声誉损失 | 负面宣传和失去顾客及利益相关者的信任 |'
- en: '| Financial impact | Costs associated with fines, legal fees, data breach notifications,
    and so on |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 财务影响 | 与罚款、法律费用、数据泄露通知等相关的成本 |'
- en: '| Data breaches | Increased risk of security breaches and unauthorized access
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 数据泄露 | 增加安全漏洞和未经授权访问的风险 |'
- en: '| Data quality issues | Inaccurate or incomplete data impacts decision-making
    and efficiency |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 数据质量问题 | 不准确或不完整的数据影响决策和效率 |'
- en: '| Loss of customers | Customers discontinuing relationships with non-compliant
    organizations |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 顾客流失 | 顾客中断与不合规组织的关系 |'
- en: '| Legal liability | Potential legal liability for individuals and organizations
    |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 法律责任 | 个人和组织可能面临的法律责任 |'
- en: '| Additional monitoring and oversight | Imposition of stricter regulatory monitoring
    and oversight |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 额外的监控和监督 | 强化的法规监控和监督 |'
- en: '| Difficulty in expanding internationally | Hindrance in global expansion due
    to international non-compliance |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 难以扩展到国际市场 | 因国际非合规性而阻碍全球扩展 |'
- en: Table 2.3 – Consequences of neglecting data compliance
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.3 – 忽视数据合规性的后果
- en: 'Here’s a Python example to illustrate a simplified scenario where we check
    the compliance of data records with specific regulations using randomly generated
    data:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个Python示例，用于说明一个简化的情景，我们使用随机生成的数据检查数据记录是否符合特定规定的合规性：
- en: 'We first import the `random` library:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入`random`库：
- en: '[PRE42]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we create a function to simulate a dataset with compliance checks for
    a given number of data records:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个函数来模拟带有合规性检查的数据集，数据记录的数量由给定的值决定：
- en: '[PRE43]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Each data record consists of attributes such as `Age` and `Consent Given`,
    which are randomly generated:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每条数据记录由诸如`Age`和`Consent Given`等属性组成，这些属性是随机生成的：
- en: '[PRE44]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We define compliance rules for these attributes based on a simplified scenario
    where, for instance, individuals must be 18 or older to provide consent:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据一个简化的场景定义这些属性的合规性规则，例如，个人必须年满18岁或以上才能提供同意：
- en: '[PRE45]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We check compliance with specific regulations, and for each data record, we
    report whether it complies with `Age` and `Consent` requirements:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们检查是否符合特定的法规，并且对于每条数据记录，我们报告其是否符合`Age`和`Consent`要求：
- en: '[PRE46]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We introduce a `compliant_count` variable to keep track of the number of compliant
    records:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们引入了一个`compliant_count`变量，用于跟踪符合合规性的记录数量：
- en: '[PRE47]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Inside the loop that generates data records, we increment `compliant_count`
    whenever a record is compliant with the defined rules.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在生成数据记录的循环内部，每当记录符合定义的规则时，我们就递增`compliant_count`。
- en: 'After generating all records, we calculate the percentage of compliant records
    as `(compliant_count / num_records) * 100` and store it in the `percentage_compliant`
    variable:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生成所有记录后，我们计算符合合规性记录的百分比为`(compliant_count / num_records) * 100`，并将其存储在`percentage_compliant`变量中：
- en: '[PRE48]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We define the number of records we would like to simulate and start simulating
    the compliance checks by calling our `simulate_data_compliance` function:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了要模拟的记录数量，并通过调用我们的`simulate_data_compliance`函数开始模拟合规性检查：
- en: '[PRE49]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Finally, we display the results:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们显示结果：
- en: '[PRE50]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This will display the following output:'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示以下输出：
- en: '[PRE51]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Here’s a table summarizing popular compliance checks along with examples:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个总结常见合规性检查及其示例的表格：
- en: '| **Compliance check** | **Description** **and examples** |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| **合规性检查** | **描述** **和示例** |'
- en: '| Data privacy compliance | Ensuring the protection of **Personally Identifiable
    Information** (**PII**); an example is the secure storage of customer names and
    addresses. |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 数据隐私合规性 | 确保**个人身份信息**（**PII**）的保护；一个例子是安全存储客户姓名和地址 |'
- en: '| GDPR compliance | Complying with the GDPR; an example is handling user data
    access and deletion requests. |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| GDPR 合规性 | 遵守 GDPR；一个例子是处理用户数据访问和删除请求 |'
- en: '| HIPAA compliance | Ensuring healthcare data protection in accordance with
    HIPAA; an example is secure handling of **Electronic Protected Health** **Information**
    (**ePHI**) |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| HIPAA 合规性 | 根据 HIPAA 确保医疗数据保护；一个例子是对**电子受保护健康信息**（**ePHI**）的安全处理 |'
- en: '| PCI DSS compliance | Complying with the PCI DSS; an example is encrypting
    credit card information during payment processing |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| PCI DSS 合规性 | 遵守 PCI DSS；一个例子是支付处理过程中加密信用卡信息 |'
- en: '| Data retention compliance | Managing data retention periods and secure archiving
    or deletion |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 数据保留合规性 | 管理数据保留期限，并确保安全归档或删除 |'
- en: '| Consent compliance | Verifying explicit user consent for data collection
    and processing; an example is opt-in consent for email marketing |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 同意合规性 | 验证用户对数据收集和处理的明确同意；一个例子是电子邮件营销的选择同意 |'
- en: '| Accuracy and completeness compliance | Regularly checking and correcting
    data for accuracy and completeness |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 准确性与完整性合规性 | 定期检查和修正数据的准确性与完整性 |'
- en: '| Data classification and handling compliance | Labeling data by sensitivity
    and enforcing access controls; an example is classifying data as **Confidential**
    with restricted access. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 数据分类与处理合规性 | 根据敏感性对数据进行标记并强制访问控制；一个例子是将数据分类为**机密**并限制访问 |'
- en: '| Data encryption compliance | Encrypting sensitive data in transit and at
    rest |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 数据加密合规性 | 对敏感数据进行传输中和静态加密 |'
- en: '| Access control compliance | Implementing role-based access control to restrict
    data access |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 访问控制合规性 | 实施基于角色的访问控制以限制数据访问 |'
- en: '| Auditing and logging compliance | Maintaining audit logs of data access and
    changes |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 审计与日志记录合规性 | 维护数据访问和更改的审计日志 |'
- en: '| Data masking and anonymization Compliance | Protecting sensitive data through
    masking or anonymization |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 数据屏蔽与匿名化合规性 | 通过数据屏蔽或匿名化保护敏感数据 |'
- en: '| Data life cycle management compliance | Managing data from creation to disposal
    in accordance with policies |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 数据生命周期管理合规性 | 按照政策管理数据从创建到销毁的整个过程 |'
- en: '| Data ethics and ethical compliance | Ensuring that data practices align with
    ethical standards |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 数据伦理和道德合规 | 确保数据实践符合伦理标准 |'
- en: '| Non-discrimination compliance | Avoiding discriminatory uses of data; an
    example is fair lending practices in financial services |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 避免歧视合规性 | 避免数据的歧视性使用；例如，金融服务中的公平贷款实践 |'
- en: Table 2.4 – Key compliance checks
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.4 – 关键合规性检查
- en: In practice, organizations may choose to calculate data quality KPIs, including
    completeness, on a daily, weekly, monthly, or quarterly basis. It is important
    to strike a balance between the frequency of measurement and the resources required
    to perform the assessments effectively. Regular monitoring and adjustments to
    the frequency of calculation can help ensure that data quality is continuously
    evaluated and maintained in accordance with business needs.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 实际中，组织可能会选择按日、周、月或季度计算数据质量KPI，包括完整性。在测量频率和有效评估所需资源之间取得平衡是非常重要的。定期监控和调整计算频率可以帮助确保数据质量根据业务需求持续评估和维护。
- en: If the data has frequent updates, the monitoring metrics on the data should
    also be frequent. This ensures that any changes or updates to the data are captured
    in a timely manner and that the quality metrics remain up to date.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据有频繁的更新，数据的监控指标也应当频繁。这确保了数据的任何变化或更新都能及时捕捉，并且质量指标保持最新。
- en: 'The more critical the data, the more frequent the update on the monitoring
    metrics should be. See here what critical data means:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 数据越关键，监控指标更新的频率应该越高。这里是关键数据的定义：
- en: '| **Characteristic** | **Description** |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| **特征** | **描述** |'
- en: '| Vital for core operations | Essential for daily organizational functions
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 对核心运营至关重要 | 对日常组织功能至关重要 |'
- en: '| Key to decision-making | Instrumental in strategic, tactical, and operational
    decisions |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 决策的关键 | 在战略、战术和运营决策中起到关键作用 |'
- en: '| High value and impact | Associated with significant financial value and operational
    impact |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 高价值和影响 | 与显著的财务价值和运营影响相关 |'
- en: '| Sensitive and confidential | Often includes sensitive and confidential information
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 敏感和机密 | 经常包含敏感和机密信息 |'
- en: '| Business continuity and disaster recovery | Crucial for continuity planning
    and recovery measures |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 业务连续性和灾难恢复 | 对于连续性规划和恢复措施至关重要 |'
- en: '| Customer trust and satisfaction | Directly impacts trust and satisfaction
    |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 客户信任与满意度 | 直接影响信任与满意度 |'
- en: '| Competitive advantage | May provide a competitive edge |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 竞争优势 | 可能提供竞争优势 |'
- en: '| Strategic asset | Recognized as a strategic resource |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 战略资产 | 被视为战略资源 |'
- en: Table 2.5 – Critical data definition
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.5 – 关键数据定义
- en: If the data being measured for quality is vital for critical decision-making
    processes or sensitive operations, it may be necessary to calculate the quality
    KPIs on a more frequent basis.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用于质量衡量的数据对关键决策过程或敏感操作至关重要，则可能需要更频繁地计算质量KPI。
- en: Now that we understand how to evaluate our data products against different quality
    KPIs, let’s see at which point in the data life cycle we need to apply those.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了如何根据不同的质量KPI评估我们的数据产品，让我们看看在数据生命周期的哪些节点需要应用这些指标。
- en: Implementing quality controls throughout the data life cycle
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施数据生命周期中的质量控制
- en: 'Data quality should be a fundamental consideration throughout the entire life
    cycle of data. From data ingestion to utilization by downstream analytic teams,
    data undergoes various changes, and ensuring its quality at each step is paramount.
    Here is a diagram of the quality check life cycle:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量应当是数据整个生命周期中的根本考虑。从数据采集到下游分析团队的使用，数据经历了各种变化，确保每个步骤中的质量至关重要。以下是质量检查生命周期的示意图：
- en: '![Figure 2.1 – Quality check life cycle](img/B19801_02_1.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 质量检查生命周期](img/B19801_02_1.jpg)'
- en: Figure 2.1 – Quality check life cycle
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 质量检查生命周期
- en: 'Let’s understand more deeply what needs to happen at each step:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解每个步骤需要发生的内容：
- en: '**Data entry/ingestion**: Validating the data sources and ensuring that data
    is captured accurately and consistently while entering the system can limit errors
    in the downstream processes.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据输入/采集**：验证数据源，并确保数据在进入系统时准确、一致地捕获，可以减少下游过程中的错误。'
- en: Data persona --> data engineer
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据角色 --> 数据工程师
- en: '**Data transformation**: By incorporating quality checks into the data transformation
    layer, organizations ensure that data remains reliable, accurate, and consistent
    throughout its journey from raw sources to its final destination.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转化**：通过在数据转化层中加入质量检查，组织确保数据在从原始来源到最终目的地的整个过程中始终保持可靠、准确和一致。'
- en: Data persona --> data engineer
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据角色 --> 数据工程师
- en: '**Data integration**: When combining data from multiple sources or systems,
    data integration can introduce errors and inconsistencies. Applying quality checks
    at this level helps prevent data quality issues from propagating throughout the
    data ecosystem and supports a high level of confidence in the integrated data.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据整合**：当从多个来源或系统合并数据时，数据整合可能会引入错误和不一致。此层次的质量检查有助于防止数据质量问题在数据生态系统中传播，并支持对整合数据的高度信任。'
- en: Data persona --> data engineer and data scientist
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据角色 --> 数据工程师和数据科学家
- en: '**Data consumption**: Analytics and machine learning models heavily depend
    on the quality of input data. This is particularly crucial in today’s data-driven
    landscape, where the quality of data directly impacts an organization’s success
    and competitive advantage.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据消费**：分析和机器学习模型在很大程度上依赖于输入数据的质量。在当今以数据为驱动的环境中，这一点尤为重要，因为数据质量直接影响组织的成功与竞争优势。'
- en: Data persona --> data scientist, analyst
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据角色 --> 数据科学家，分析师
- en: As you can see in the preceding list, data flows in the system. Different teams
    collaborate on defining the quality metrics and applying the quality controls.
    Now, let’s see what would happen if there was no collaboration between the different
    teams.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面列表所示，数据在系统中流动。不同的团队合作定义质量指标并应用质量控制。现在，让我们看看如果不同团队之间没有合作，会发生什么。
- en: Data silos and the impact on data quality
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据孤岛及其对数据质量的影响
- en: Data silos, also known as isolated data repositories, are prevalent in many
    organizations today. Data silos refer to the practice of storing and managing
    data in isolated or disconnected systems or departments within an organization.
    These isolated data repositories have evolved over time, with various departments
    or business units maintaining the data separately and making it too complex to
    integrate. Organizations are increasingly aware of the limitations posed by data
    silos. They recognize that these silos hinder data-driven decision-making and
    operational efficiency. As a result, efforts to break down data silos and promote
    data integration and quality initiatives are on the rise, aimed at leveraging
    the full potential of data resources.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 数据孤岛，也称为孤立的数据存储库，在今天的许多组织中普遍存在。数据孤岛是指将数据存储和管理在组织内部孤立或不连接的系统或部门中的做法。这些孤立的数据存储库随着时间的推移发展起来，不同部门或业务单元单独维护数据，导致数据整合变得复杂。组织越来越意识到数据孤岛带来的局限性，认识到它们妨碍了数据驱动的决策和运营效率。因此，打破数据孤岛、推动数据整合和质量提升的努力正在增加，旨在充分挖掘数据资源的潜力。
- en: 'These silos pose challenges in maintaining data quality across the dimensions
    we’ve already discussed:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这些孤岛在我们已经讨论过的维度中给数据质量的维护带来了挑战：
- en: '**Not sharing data with the rest of the organization hinders its competitive
    advantage**: Data silos slow down decision-making by requiring employees to spend
    time searching for data from disparate sources, diverting their focus from gaining
    insights and taking action. Usually, data silos are associated with duplicate
    work as teams perform similar tasks independently, lacking efficient collaboration
    and information sharing. Conflicting interpretations of metrics frequently arise,
    causing confusion and disagreements among teams relying on different data sources.
    Misaligned assumptions and perspectives prevent progress and direction. Establishing
    clear communication guidelines and enforcing standardized methodologies is essential
    to align expectations and facilitate comprehension throughout the organization.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不与组织其他部门共享数据会削弱其竞争优势**：数据孤岛通过迫使员工花时间从不同的来源寻找数据，拖慢了决策过程，将他们的注意力从获取洞察力和采取行动转移开。通常，数据孤岛与重复性工作相关联，因为各团队独立完成相似任务，缺乏高效的协作和信息共享。不同的数据源之间常常会出现对指标的不同解读，导致团队之间的混淆和争议。假设和视角的不一致阻碍了进展和方向。建立清晰的沟通指南并执行标准化方法是确保对齐期望、促进整个组织理解的关键。'
- en: '**Not sharing data with the rest of the organization is very costly**: Data
    silos increase costs due to the maintenance of multiple scattered systems across
    the organization. Maintaining these disparate systems requires dedicated resources,
    both in terms of personnel and infrastructure (such as redundant storage in multiple
    places). Retrieving relevant information becomes time-consuming due to scattered
    data repositories, resulting in delays. Manually combining data from different
    sources introduces potential errors.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不与整个组织共享数据是非常昂贵的**：数据孤岛由于维护跨组织的多个分散系统而增加了成本。维护这些不同的系统需要专门的资源，包括人员和基础设施（例如在多个地方的冗余存储）。由于数据存储分散，检索相关信息变得耗时，导致延迟。手动整合来自不同来源的数据会引入潜在错误。'
- en: Now, let’s summarize what we’ve learned in this chapter.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们总结一下本章所学的内容。
- en: Summary
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we discussed the critical role of high-quality data, providing
    a solid foundation for analytics, machine learning, and informed decision-making.
    To ensure data quality, organizations implement a series of checks and measures
    at various stages of the data pipeline:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了高质量数据的重要性，它为分析、机器学习和明智决策提供了坚实的基础。为了确保数据质量，组织在数据管道的各个阶段实施了一系列检查和措施：
- en: 'Data entry/ingestion: Data sources are validated to ensure accurate and consistent
    data capture, primarily overseen by data engineers'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据录入/摄取：数据源会进行验证，以确保数据捕获的准确性和一致性，主要由数据工程师监管。
- en: 'Data transformation: Quality checks are incorporated into the transformation
    layer to maintain data reliability and accuracy, typically managed by data engineers'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转化：质量检查被纳入转化层，以保持数据的可靠性和准确性，通常由数据工程师管理。
- en: 'Data integration: Checks prevent data quality issues from propagating and support
    confidence in integrated data, involving data engineers and data scientists'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集成：检查可以防止数据质量问题的蔓延，并支持对集成数据的信任，涉及数据工程师和数据科学家。
- en: 'Data consumption: Quality data input is vital for analytics and machine learning,
    impacting user trust and competitive advantage, and is driven by data scientists
    and analysts'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据消费：高质量的数据输入对于分析和机器学习至关重要，影响用户信任和竞争优势，由数据科学家和分析师推动。
- en: These quality checks ensure that data adheres to defined standards, meets regulatory
    requirements, and is fit for its intended purpose. By implementing these checks,
    organizations maintain data accuracy, reliability, and transparency, facilitating
    better decision-making and ensuring data-driven success.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这些质量检查确保数据遵循定义的标准，符合监管要求，并适合其预定用途。通过实施这些检查，组织保持数据的准确性、可靠性和透明度，从而促进更好的决策制定，确保数据驱动的成功。
- en: In the next chapter, we will explore how profiling tools can be used to continuously
    and automatically monitor data quality.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨如何使用分析工具持续且自动地监控数据质量。
