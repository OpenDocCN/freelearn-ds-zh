- en: 7 Handling Missing Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 处理缺失数据
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的书籍社区，访问 Discord
- en: '![](img/file0.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/file0.png)'
- en: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
- en: As a data scientist, data analyst, or business analyst, you have probably discovered
    that hoping to obtain a *perfect* clean dataset is too optimistic. What is more
    common, though, is that the data you are working with suffers from flaws such
    as missing values, erroneous data, duplicate records, insufficient data, or the
    presence of outliers in the data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家、数据分析师或业务分析师，您可能已经发现，指望获得*完美*的干净数据集是过于乐观的。然而，更常见的情况是，您正在处理的数据存在缺失值、错误数据、重复记录、数据不足或数据中存在异常值等问题。
- en: Time series data is no different, and before plugging the data into any analysis
    or modeling workflow, you must investigate the data first. It is vital to understand
    the *business context around the time series data* to detect and identify these
    problems successfully. For example, if you work with stock data, the context is
    very different from COVID data or sensor data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据也不例外，在将数据输入任何分析或建模流程之前，您必须先对数据进行调查。理解*时间序列数据背后的业务背景*对于成功检测和识别这些问题至关重要。例如，如果您处理的是股票数据，其背景与
    COVID 数据或传感器数据有很大不同。
- en: Having that intuition or domain knowledge will allow you to anticipate what
    to expect and what is considered acceptable when analyzing the data. Always try
    to understand the business context around the data. For example, why is the data
    collected in the first place? How was the data collected? What business rules,
    logic, or transformations have been applied to the data? Were these modifications
    applied during the data acquisition process or built into the systems that generate
    the data?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这种直觉或领域知识将使您能够预见分析数据时的期望结果以及哪些结果是可以接受的。始终尝试理解数据背后的业务背景。例如，数据最初为什么要被收集？数据是如何收集的？数据是否已经应用了某些业务规则、逻辑或变换？这些修改是在数据采集过程中应用的，还是内置在生成数据的系统中？
- en: During the discovery phase, such prior knowledge will help you determine the
    best approach to clean and prepare your dataset for analysis or modeling. Missing
    data and outliers are two common problems that need to be dealt with during data
    cleaning and preparation. You will dive into outlier detection in *Chapter 8*,
    *Outlier Detection Using Statistical Methods*, and *Chapter 14*, *Outlier Detection
    Using Unsupervised Machine Learning*. In this chapter, you will explore techniques
    to handle missing data through **imputation** and **interpolation**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在发现阶段，这些前期知识将帮助您确定最佳的方法来清理和准备数据集，以进行分析或建模。缺失数据和异常值是数据清理和准备过程中需要处理的两个常见问题。您将在*第8章*《使用统计方法检测异常值》和*第14章*《使用无监督机器学习检测异常值》中深入探讨异常值检测。本章将探索通过**插补**和**插值**技术处理缺失数据的方法。
- en: 'Here is the list of recipes that we will cover in this chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章将涵盖的配方列表：
- en: Performing data quality checks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行数据质量检查
- en: Handling missing data with univariate imputation using pandas
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pandas 进行单变量插补处理缺失数据
- en: Handling missing data with univariate imputation using scikit-learn
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行单变量插补处理缺失数据
- en: Handling missing data with multivariate imputation
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多变量插补处理缺失数据
- en: Handling missing data with interpolation
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用插值处理缺失数据
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can download the Jupyter notebooks and the requisite datasets from the
    GitHub repository to follow along:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从 GitHub 仓库下载 Jupyter 笔记本和所需的数据集，以便跟随教程：
- en: 'Jupyter notebooks: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch7/Chapter%207.ipynb](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch7/Chapter%207.ipynb)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter 笔记本：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch7/Chapter%207.ipynb](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch7/Chapter%207.ipynb)
- en: 'Datasets: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch7](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch7)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch7](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch7)
- en: 'In this chapter and beyond, you will extensively use pandas 2.1.3 (released
    November 10, 2023). There will be four additional libraries that you will be using:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章及后续章节中，您将广泛使用 pandas 2.1.3（2023年11月10日发布）。另外，您还将使用四个额外的库：
- en: numpy (1.26.0)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: numpy (1.26.0)
- en: matplotlob (3.8.1)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: matplotlob (3.8.1)
- en: statsmodels (0.14.0)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: statsmodels (0.14.0)
- en: scikit-learn (1.3.2)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn (1.3.2)
- en: SciPy (1.11.3)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SciPy (1.11.3)
- en: 'If you are using `pip`, then you can install these packages from your terminal
    with the following command:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用`pip`，则可以通过终端使用以下命令安装这些包：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you are using `conda`, then you can install these packages with the following
    command:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用`conda`，则可以通过以下命令安装这些包：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this chapter, two datasets will be used extensively for the imputation and
    interpolation recipes: the *CO2 Emissions* dataset, and the *e-Shop Clickstream*
    dataset. The source for the Clickstream dataset comes from *clickstream data for
    online shopping* from the *UCI machine learning repository*, which you can find
    here:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，将广泛使用两个数据集进行插补和插值操作：*CO2排放*数据集和*电子商店点击流*数据集。点击流数据集的来源为*在线购物的点击流数据*，来自*UCI机器学习库*，您可以在这里找到：
- en: '[https://archive.ics.uci.edu/ml/datasets/clickstream+data+for +online+shopping](https://archive.ics.uci.edu/ml/datasets/clickstream+data+for)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/datasets/clickstream+data+for +online+shopping](https://archive.ics.uci.edu/ml/datasets/clickstream+data+for)'
- en: 'The source for the CO2 emissions dataset comes from the Annual *CO2 emissions*
    report from *Our World in Data*, which you can find here: [https://ourworldindata.org/co2-emissions](https://ourworldindata.org/co2-emissions).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: CO2排放数据集的来源为*Our World in Data*的年度*CO2排放*报告，您可以在此处找到：[https://ourworldindata.org/co2-emissions](https://ourworldindata.org/co2-emissions)。
- en: For demonstration purposes, the two datasets have been modified by removing
    observations (missing data). The original versions are provided, in addition to
    the modified versions, to be used for evaluating the different techniques discussed
    in this chapter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，两个数据集已经被修改，去除了部分观测值（缺失数据）。提供了原始版本和修改版本，用于评估本章讨论的不同技术。
- en: 'Throughout this chapter, you will follow similar steps for handling missing
    data: ingest the data into a DataFrame, identify missing data, impute missing
    data, evaluate it against the original data, and finally, visualize and compare
    the different imputation techniques.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将遵循类似的步骤来处理缺失数据：将数据导入DataFrame，识别缺失数据，对缺失数据进行插补，评估与原始数据的对比，最后可视化并比较不同的插补技术。
- en: 'These steps can be translated into functions for reusability. You can create
    functions for these steps in the process: a function to read the data into a DataFrame,
    a function to evaluate using the RMSE score, and a function to plot the results.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤可以转化为函数以便重用。您可以为这些步骤创建函数：一个用于将数据读入DataFrame的函数，一个用于使用RMSE评分评估的函数，以及一个用于绘制结果的函数。
- en: 'Start by loading the standard libraries that you will be using throughout this
    chapter:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先加载将在本章中使用的标准库：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Function 1 – read_datasets
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 函数1 – read_datasets
- en: The `read_datasets` function takes a path to the folder, CSV filename, and the
    column name that contains the date variable.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_datasets`函数接受文件夹路径、CSV文件名和包含日期变量的列名。'
- en: 'The `read_datasets` function is defined as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_datasets`函数定义如下：'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Function 2 – plot_dfs
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 函数2 – plot_dfs
- en: 'The `plot_dfs()` function takes two DataFrames: the original DataFrame (`df1`)
    with no missing data (as the baseline), and the imputed DataFrame (`df2`) to compare
    against. The function creates multiple time series subplots using the specified
    response column (`col`). Note that the imputed DataFrame will contain additional
    columns (a column for the output of each imputation technique), and the plotting
    function accommodates this fact. This is done by looping through the columns.
    The function will plot each imputation technique for visual comparison and will
    be utilized throughout this chapter.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot_dfs()`函数接受两个DataFrame：没有缺失数据的原始DataFrame（`df1`，作为基准）和经过插补的DataFrame（`df2`，用于对比）。该函数使用指定的响应列（`col`）创建多个时间序列子图。注意，插补后的DataFrame将包含额外的列（每个插补技术的输出列），绘图函数会考虑到这一点。通过遍历列来完成这一操作。该函数将为每个插补技术绘制图形以便视觉比较，并将在本章中多次使用。'
- en: 'This `plot_dfs` function is defined as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`plot_dfs`函数定义如下：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Function 3 – rmse_score
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 函数3 – rmse_score
- en: In addition to a visual comparison between imputation techniques using the `plot_dfs`
    function, you will need a method to compare the different imputation techniques
    numerically (using a statistical measure).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用`plot_dfs`函数进行插补技术的视觉比较外，您还需要一种方法来数字化比较不同的插补技术（使用统计度量）。
- en: 'This is where the `rmse_score` function will come in handy. It takes two DataFrames:
    the original DataFrame (`df1`) as the baseline and the imputed DataFrame (`df2`)
    to compare against. The function allows you to specify which column contains the
    response column (`col`) used as the basis for the calculation.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这时，`rmse_score` 函数将派上用场。它接受两个 DataFrame：原始 DataFrame（`df1`）作为基准，以及要进行比较的填补后的
    DataFrame（`df2`）。该函数允许你指定哪个列包含响应列（`col`），用于计算的基础。
- en: 'The `rmse_score` function is defined as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`rmse_score` 函数定义如下：'
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Understanding missing data
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解缺失数据
- en: Data can be missing for a variety of reasons, such as unexpected power outages,
    a device that got accidentally unplugged, a sensor that just became defective,
    a survey respondent declined to answer a question, or the data was intentionally
    removed for privacy and compliance reasons. In other words, missing data is inevitable.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据缺失可能有多种原因，例如意外的停电、设备意外断电、传感器损坏、调查参与者拒绝回答某个问题，或者出于隐私和合规性原因故意删除数据。换句话说，缺失数据是不可避免的。
- en: Generally, missing data is very common, yet sometimes it is not given the proper
    level of attention in terms of formulating a strategy on how to handle the situation.
    One approach for handling rows with missing data is to drop those observations
    (delete the rows). However, this may not be a good strategy if you have limited
    data in the first place, for example, if collecting the data is a complex and
    expensive process. Additionally, the drawback of deleting records, if done prematurely,
    is that you will not know if the missing data was due to censoring (an observation
    is only partially collected) or due to bias (for example, high-income participants
    declining to share their total household income in a survey).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，缺失数据是非常常见的，但有时在制定处理策略时并未给予足够重视。处理缺失数据行的一种方法是删除这些观察值（删除行）。然而，如果你的数据本就有限，这种方法可能不是一个好的策略。例如，若数据的收集过程复杂且昂贵，则删除记录的缺点在于，如果过早删除，你将无法知道缺失数据是由于审查（观察仅部分收集）还是由于偏差（例如，高收入参与者拒绝在调查中共享家庭总收入）造成的。
- en: A second approach may involve tagging the rows with missing data by adding a
    column describing or labeling the missing data. For example, suppose you know
    that there was a power outage on a particular day. In that case, you can add Power
    Outage to label the missing data and differentiate it from other missing data
    labeled with Missing Data if the cause is unknown.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法可能是通过添加一列描述或标记缺失数据的列来标记缺失数据的行。例如，假设你知道在某一天发生了停电。此时，你可以添加“停电”来标记缺失数据，并将其与其他标记为“缺失数据”的缺失数据区分开来，如果其原因未知的话。
- en: A third approach, which this chapter is about, is estimating the missing data
    values. The methods can range from simple and naive to more complex techniques
    leveraging machine learning and complex statistical models. But how can you measure
    the accuracy of the estimated values for data missing in the first place?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的第三种方法是估算缺失的数据值。这些方法可以从简单的、初步的，到更复杂的技术，后者使用机器学习和复杂的统计模型。但你怎么衡量最初缺失数据的估算值的准确性呢？
- en: There are different options and measures to consider, and the answer is not
    as simple. Therefore, you should explore different approaches, emphasizing a thorough
    evaluation and validation process to ensure the selected method is ideal for your
    situation. In this chapter, you will use **Root Mean Squared Error** (**RMSE**)
    to evaluate the different imputation techniques.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失数据时有多种选择和措施需要考虑，答案并非那么简单。因此，你应该探索不同的方法，强调彻底的评估和验证过程，以确保所选方法最适合你的情况。在本章中，你将使用**均方根误差**（**RMSE**）来评估不同的填补技术。
- en: 'The process to calculate the RMSE can be broken down into a few simple steps:
    first, computing the error, which is the difference between the actual values
    and the predicted or estimated values. This is done for each observation. Since
    the errors may be either negative or positive, and to avoid having a zero summation,
    the errors (differences) are squared. Finally, all the errors are summed and divided
    by the total number of observations to compute the mean. This gives you the **Mean
    Squared Error (MSE)**. RMSE is just the square root of the MSE.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 计算RMSE的过程可以分为几个简单的步骤：首先，计算误差，即实际值与预测值或估计值之间的差异。这是针对每个观测值进行的。由于误差可能是负值或正值，为了避免零求和，误差（差异）会被平方。最后，将所有误差求和并除以观测值的总数来计算平均值。这会给你**均方误差（MSE）**。RMSE只是MSE的平方根。
- en: 'The RMSE equation can be written as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE方程可以写成：
- en: '![](img/file61.jpg)In our estimate of the missing observations,![](img/file62.png)is
    the imputed value,![](img/file63.png)is the actual (original) value, and *N* is
    the number of observations.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/file61.jpg)在我们估算缺失观测值时，![](img/file62.png)是插补值，![](img/file63.png)是实际（原始）值，*N*是观测值的数量。'
- en: RMSE FOR EVALUATING MULTIPLE IMPUTATION METHODS
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用于评估多重插补方法的RMSE
- en: ''
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I want to point out that RMSE is commonly used to measure the performance of
    *predictive* models (for example, comparing regression models). Generally, a *lower*
    RMSE is desirable; it tells us that the model can fit the dataset. Simply stated,
    it tells us the average distance (error) between the predicted value and the actual
    value. You want this distance minimized.
  id: totrans-60
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我想指出，RMSE通常用于衡量*预测*模型的性能（例如，比较回归模型）。通常，较低的RMSE是理想的；它告诉我们模型能够拟合数据集。简单来说，它告诉我们预测值与实际值之间的平均距离（误差）。你希望最小化这个距离。
- en: ''
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When comparing different imputation methods, we want our imputed values to resemble
    (as close as possible) the actual data, which contains random effects (uncertainty).
    This means we are not seeking a perfect prediction, and thus a lower RMSE score
    does not necessarily indicate a better imputation method. Ideally, you would want
    to find a balance, hence, in this chapter, the use of RMSE is combined with visualization
    to help illustrate how the different techniques compare and work.
  id: totrans-62
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在比较不同的插补方法时，我们希望插补的值尽可能接近实际数据，这些数据包含随机效应（不确定性）。这意味着我们并不寻求完美的预测，因此较低的RMSE分数不一定表示更好的插补方法。理想情况下，你希望找到一个平衡，因此在本章中，RMSE与可视化结合使用，以帮助说明不同技术如何比较和工作。
- en: As a reminder, we have intentionally removed some values (synthetically causing
    missing data) but retained the original data to compare against for when using
    RMSE.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，我们有意删除了一些值（人为造成缺失数据），但保留了原始数据，以便在使用RMSE时进行对比。
- en: Performing data quality checks
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行数据质量检查
- en: '**Missing data** are values not captured or not observed in the dataset. Values
    can be missing for a *particular feature* (column), or an *entire observation*
    (row). When ingesting the data using pandas, missing values will show up as either
    `NaN`, `NaT`, or `NA`.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺失数据**是指在数据集中没有捕获或没有观察到的值。值可能会缺失于*特定特征*（列）或*整个观测*（行）。使用pandas加载数据时，缺失值将显示为`NaN`、`NaT`或`NA`。'
- en: Sometimes, in a given data set, missing observations are replaced with other
    values from the source system; for example, this can be a numeric filler such
    as `99999` or `0`, or a string such as `missing` or `N/A`. When missing values
    are represented by `0`, you need to be cautious and investigate further to determine
    whether those zero values are legitimate or if they are indicative of missing
    data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，在给定的数据集中，缺失的观测值会被源系统中的其他值替换；例如，这可以是像`99999`或`0`这样的数字填充值，或者像`missing`或`N/A`这样的字符串。当缺失值被表示为`0`时，需要小心，并进一步调查以确定这些零值是否合法，还是缺失数据的标志。
- en: In this recipe, you will explore how to identify the presence of missing data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将探索如何识别缺失数据的存在。
- en: Getting ready
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: You can download the Jupyter notebooks and requisite datasets from the GitHub
    repository. Please refer to the *Technical requirements* section of this chapter.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从GitHub仓库下载Jupyter笔记本和所需的数据集。请参考本章的*技术要求*部分。
- en: 'You will be using two datasets from the `Ch7` folder: `clicks_missing_multiple.csv`
    and `co2_missing.csv`.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用来自`Ch7`文件夹的两个数据集：`clicks_missing_multiple.csv`和`co2_missing.csv`。
- en: How to do it…
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'The `pandas` library provides convenient methods for discovering missing data
    and for summarizing data in a DataFrame:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas`库提供了方便的方法来发现缺失数据并总结DataFrame中的数据：'
- en: 'Start by reading the two CSV files (`co2_missing.csv` and `clicks_missing.csv`)
    using the `read_dataset()` function :'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`read_dataset()`函数开始读取两个CSV文件（`co2_missing.csv`和`clicks_missing.csv`）：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This should display the first five rows from the `ecom_df` DataFrame:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示`ecom_df` DataFrame的前五行：
- en: '![Figure 7.1: First five rows from the ecom_df DataFrame showing NaN and NaT](img/file64.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1：`ecom_df` DataFrame的前五行，显示了NaN和NaT](img/file64.png)'
- en: 'Figure 7.1: First five rows from the ecom_df DataFrame showing NaN and NaT'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1：`ecom_df` DataFrame的前五行，显示了NaN和NaT
- en: The output from the preceding code shows that there are five missing values
    from the source dataset. `NaN` is how pandas represents empty *numeric* values
    (`NaN` is short for **Not a Number**). `NaT` is how pandas represents missing
    `Datetime` values (`NaT` is short for **Not a Time**).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出显示源数据集中有五个缺失值。`NaN`是pandas表示空*数值*的方式（`NaN`是**Not a Number**的缩写）。`NaT`是pandas表示缺失的`Datetime`值的方式（`NaT`是**Not
    a Time**的缩写）。
- en: To count the number of missing values in both DataFrames, you can use the `DataFrame.isnull()`
    or `DataFrame.isna()` methods. This will return `True` (if missing) or `False`
    (if not missing) for each value. For example, to get the total count of missing
    values for each column, you can use `DataFrame.isnull().sum()` or `DataFrame.isna().sum()`
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要统计两个DataFrame中的缺失值数量，你可以使用`DataFrame.isnull()`或`DataFrame.isna()`方法。这会返回`True`（如果缺失）或`False`（如果不缺失）对于每个值。例如，要获取每一列缺失值的总数，你可以使用`DataFrame.isnull().sum()`或`DataFrame.isna().sum()`。
- en: 'In Python, Booleans (`True` or `False`) are a subtype of integers. `True` is
    equivalent to `1`, and `False` is equivalent to `0`. To validate this concept,
    try the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，布尔值（`True`或`False`）是整数的一个子类型。`True`等于`1`，`False`等于`0`。要验证这一概念，可以尝试以下操作：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let''s get the total number of missing values for each DataFrame:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们获取每个DataFrame中缺失值的总数：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice in the preceding code that both `.isnull()` and `.isna()` were used.
    They both can be used interchangeably since `.isnull()` is an alias of `.isna()`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上面的代码中同时使用了`.isnull()`和`.isna()`。它们可以互换使用，因为`.isnull()`是`.isna()`的别名。
- en: 'In the previous step, the `year` column from the `co2_df` and the `date` column
    from the `ecom_df` were not included in the counting result set. This is because
    `isnull()` or `isna()` focuses on the DataFrame’s columns and does not include
    the index. Our `read_datasets()` function from the *Technical Requirements* section
    was setting them as **index** columns. One simple approach is to reset the index
    to become a column as follows:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前一步中，`co2_df`的`year`列和`ecom_df`的`date`列未包含在计数结果中。这是因为`isnull()`或`isna()`关注的是DataFrame的列，而不包括索引。我们的`read_datasets()`函数在*技术要求*部分将它们设置为**索引**列。一种简单的方法是将索引重置为列，如下所示：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now if you execute the `isnull().sum()` you should see the year column from
    the `co2_df` and date column from the `ecom_df` included in the counts:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你执行`isnull().sum()`，你应该能看到`co2_df`的年份列和`ecom_df`的日期列包含在计数结果中：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: From the results, `co2_df` has `25` missing values from the `co2` column, while
    `ecom_df` has `20` missing values in total (`4` from the `date` column, `1` from
    the `price` column, `1` from the `location` column, and `14` from the `clicks`
    column).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果来看，`co2_df`的`co2`列有`25`个缺失值，而`ecom_df`总共有`20`个缺失值（其中`4`个来自`date`列，`1`个来自`price`列，`1`个来自`location`列，`14`个来自`clicks`列）。
- en: 'To get the grand total for the entire `ecom_df` DataFrame, simply just chain
    another `.sum()` function to the end of the statement:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取整个`ecom_df` DataFrame的总计，只需在语句末尾再链式调用`.sum()`函数：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Similarly, for `co2_df` you can chain another .sum()
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于`co2_df`，你可以再链式调用一个`.sum()`。
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If you inspect the `co2_missing.csv` file using a text/code editor (such as
    Excel, or Jupyter Lab) and scroll down to *rows 192-194*, you will notice that
    there are string placeholder values in there: `NA`, `N/A`, and `null`:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你使用文本/代码编辑器（如Excel或Jupyter Lab）检查`co2_missing.csv`文件，并向下滚动到*第192-194行*，你会发现其中有一些字符串占位符值：`NA`、`N/A`和`null`：
- en: '![Figure 7.2: co2_missing.csv shows string values that were converted to NaN
    (missing) by pandas](img/file65.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2：co2_missing.csv显示了由pandas转换为NaN（缺失）的字符串值](img/file65.jpg)'
- en: 'Figure 7.2: co2_missing.csv shows string values that were converted to NaN
    (missing) by pandas'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2：co2_missing.csv显示了由pandas转换为NaN（缺失）的字符串值
- en: '*Figure 7.2* shows the three string values. Interestingly, `pandas.read_csv()`
    interpreted the three string values as `NaN`. This is the default behavior in
    `read_csv()`, which can be modified through the `na_values` parameter. To see
    how pandas represents these values, you can run the following command:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.2* 显示了这三种字符串值。有趣的是，`pandas.read_csv()`将这三种字符串值解释为`NaN`。这是`read_csv()`的默认行为，可以通过`na_values`参数进行修改。要查看pandas如何表示这些值，可以运行以下命令：'
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This should produce the following output:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 7.3: pandas.read_csv() interpreted the NA, N/A, and null strings as
    a NaN type](img/file66.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3：pandas.read_csv()将NA、N/A和null字符串解析为NaN类型](img/file66.png)'
- en: 'Figure 7.3: pandas.read_csv() interpreted the NA, N/A, and null strings as
    a NaN type'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：`pandas.read_csv()`将NA、N/A和null字符串解析为NaN类型
- en: 'If all you need is to check whether the DataFrame contains any missing values,
    use `isnull().values.any()`. This will output `True` if there are any missing
    values in the DataFrame:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你只需要检查DataFrame是否包含任何缺失值，请使用`isnull().values.any()`。如果DataFrame中有任何缺失值，这将输出`True`：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: So far, `isnull()` helped identify all the missing values in the DataFrames.
    But what if the missing values were masked or replaced by other placeholder values
    such as `?` or `99999`. The presence of these values will be skipped and considered
    missing (NaN) in pandas. Technically, they are not empty cells (missing) and hold
    values. On the other hand, domain or prior knowledge will tell us that the CO2
    emission dataset is measured annually and should have values greater than 0.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，`isnull()`帮助识别了DataFrame中的所有缺失值。但如果缺失值被掩盖或替换为其他占位符值，例如`?`或`99999`，会怎样呢？这些值会被跳过并视为缺失（NaN）值。在技术上，它们并不是空单元格（缺失），而是具有值的。另一方面，领域知识或先验知识告诉我们，CO2排放数据集是按年测量的，应该具有大于0的值。
- en: Similarly, we expect the number of clicks to be numeric for the Clickstream
    data. If the column is not numeric, it should trigger an investigation as to why
    pandas could not parse the column as numeric. For example, this could be due to
    the presence of string values.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们期望点击流数据的点击次数是数值型的。如果该列不是数值型的，应该引发调查，看看为什么pandas不能将该列解析为数值型。例如，这可能是由于存在字符串值。
- en: 'To gain a better insight into the DataFrame schema and data types, you can
    use `DataFrame.info()` to display the schema, total records, column names, column
    dtypes, count of non-missing values per column, index dtype, and the DataFrame''s
    total memory usage:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解DataFrame的模式和数据类型，可以使用`DataFrame.info()`来显示模式、总记录数、列名、列的数据类型、每列非缺失值的计数、索引数据类型和DataFrame的总内存使用情况：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `co2_df` summary output looks reasonable, confirming that we have `25` missing
    values (226 total records less the 221 non-null give us 25) for the `co2` column.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`co2_df`的汇总输出看起来合理，确认我们有`25`个缺失值（226个总记录减去221个非空值，得到25个缺失值）在`co2`列中。'
- en: On the other hand, the summary for `ecom_df` indicates that the `clicks` column
    is of the `object` dtype (indicating mixed types), and not the expected `float64`.
    Let's investigate further using basic summary statistics.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`ecom_df`的汇总显示`clicks`列的数据类型为`object`（表示混合类型），而不是预期的`float64`。我们可以进一步通过基础的汇总统计信息进行调查。
- en: 'To get the summary statistics for a DataFrame, use the `DataFrame.describe()`
    method:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取DataFrame的汇总统计信息，请使用`DataFrame.describe()`方法：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.4: co2_df summary statistics indicating zero values present in the
    data](img/file67.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4：co2_df的汇总统计信息，表明数据中存在零值](img/file67.png)'
- en: 'Figure 7.4: co2_df summary statistics indicating zero values present in the
    data'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：co2_df的汇总统计信息，表明数据中存在零值
- en: Note the use of `include='all'` to replace the default value `include=None`
    . The default behavior is to show summary statistics for only numeric columns.
    By changing the value to `'all'`, the results will include all column types.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意使用`include='all'`来替代默认值`include=None`。默认行为是仅显示数字列的汇总统计信息。通过将值更改为`'all'`，结果将包括所有列类型。
- en: 'The summary statistics for the `co2_df` DataFrame confirms that we have zero
    values under the `co2` column (min = 0.00). As pointed out earlier, prior knowledge
    tells us that `0` represents a null (or missing) value. Therefore, the zeros will
    need to be replaced with `NaN` to include such values in the imputation process.
    Now, review the summary statistics for `ecom_df`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`co2_df` DataFrame 的摘要统计确认了我们在 `co2` 列下有零值（最小值 = 0.00）。正如之前所指出的，先验知识告诉我们，`0`
    代表一个空值（或缺失值）。因此，零值需要被替换为 `NaN`，以便将这些值纳入插补过程。现在，查看 `ecom_df` 的摘要统计：'
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.5: ecom_df summary statistics indicating the ? value in the clicks
    column](img/file68.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5：ecom_df 摘要统计，显示点击列中的 ? 值](img/file68.jpg)'
- en: 'Figure 7.5: ecom_df summary statistics indicating the ? value in the clicks
    column'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：ecom_df 摘要统计，显示点击列中的 `?` 值
- en: As you can see, the summary statistics for the `ecom_df` DataFrame indicate
    that we have a `?` value under the `clicks` column. This explains why pandas did
    not parse the column as numeric (due to mixed types). Similarly, the `?` values
    will need to be replaced with `NaN` to be treated as missing values for imputation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`ecom_df` DataFrame 的摘要统计表明，我们在 `clicks` 列下有一个 `?` 值。这也解释了为什么 pandas 没有将该列解析为数值类型（因为存在混合类型）。类似地，`?`
    值需要被替换为 `NaN`，以便将其视为缺失值并进行插补。
- en: 'Convert the instances of `0` and `?` values to `NaN` types. This can be accomplished
    using the `DataFrame.replace()` method:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `0` 和 `?` 的值转换为 `NaN` 类型。这可以通过使用 `DataFrame.replace()` 方法来完成：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To validate, run `DataFrame.isnull().sum()` and you should notice that the
    missing value counts have increased:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证，运行 `DataFrame.isnull().sum()`，你应该注意到缺失值的计数已经增加：
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The new numbers do a better job of reflecting the number of actual missing values
    in both DataFrames.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 新的数字更好地反映了两个 DataFrame 中实际缺失值的数量。
- en: How it works…
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: When reading the CSV files using `pandas.read_csv()`, the default behavior is
    to recognize and parse certain string values, such as `NA`, `N/A`, and `null`,
    to the `NaN` type (missing). Thus, once these values became a `NaN`, the CSV reader
    could parse the `co2` column as `float64` (numeric) based on the remaining non-null
    values.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `pandas.read_csv()` 读取 CSV 文件时，默认行为是识别并解析某些字符串值，例如 `NA`、`N/A` 和 `null`，并将其转换为
    `NaN` 类型（缺失值）。因此，一旦这些值变为 `NaN`，CSV 阅读器就可以基于剩余的非空值将 `co2` 列解析为 `float64`（数值型）。
- en: 'This is possible due to two parameters: `na_values` and `keep_default_na`.
    The `na_values` parameter, by default, contains a list of strings that are interpreted
    as `NaN`. The list includes `#N/A`, `#N/A N/A`, `#NA`, `-1.#IND`, `-1.#QNAN`,
    `-NaN`, `-nan`, `1.#IND`, `1.#QNAN`, `<NA>`, `N/A`, `NA`, `NULL`, `NaN`, `n/a`,
    `nan`, and `null`.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过两个参数来实现：`na_values` 和 `keep_default_na`。默认情况下，`na_values` 参数包含一个字符串列表，这些字符串会被解释为
    `NaN`。该列表包括 `#N/A`、`#N/A N/A`、`#NA`、`-1.#IND`、`-1.#QNAN`、`-NaN`、`-nan`、`1.#IND`、`1.#QNAN`、`<NA>`、`N/A`、`NA`、`NULL`、`NaN`、`n/a`、`nan`
    和 `null`。
- en: You can append to this list by providing additional values to the `na_values`
    parameter. Additionally, `keep_default_na` is set to `True` by default, thus using
    (appending) `na_values` with the default list for parsing.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过提供额外的值给 `na_values` 参数来将其附加到此列表中。此外，`keep_default_na` 默认设置为 `True`，因此使用（附加）`na_values`
    时，会与默认列表一起用于解析。
- en: If you change `keep_default_na` to `False` without providing new values to `na_values`,
    then none of the strings (`NA`, `N/A`, and `null`) would be parsed to `NaN` unless
    you provide a custom list. For example, if `keep_default_na` was set to `False`
    and no values provided to `na_values`, then the entire `co2` column would be parsed
    as a `string` (object), and any missing values will show up as strings; in other
    words, they will be coming in as `''`, which is an empty string.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将 `keep_default_na` 设置为 `False`，而没有为 `na_values` 提供新的值，则不会将任何字符串（如 `NA`、`N/A`
    和 `null`）解析为 `NaN`，除非你提供自定义列表。例如，如果将 `keep_default_na` 设置为 `False` 且未为 `na_values`
    提供值，则整个 `co2` 列会被解析为 `string`（对象类型），任何缺失值将以字符串的形式出现；换句话说，它们会显示为 `''`，即一个空字符串。
- en: 'Here is an example:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子：
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Notice that we did not lose any data (`226` records) but showed no `NaN` (or
    missing) values. Let''s inspect the DataFrame structure:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们没有丢失任何数据（`226` 条记录），但是没有显示任何 `NaN`（或缺失）值。我们来检查一下 DataFrame 的结构：
- en: '[PRE21]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Notice the change in *dtype* for the `co2` columns. Let''s check the data from
    index `190` to `195` again:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `co2` 列的 *dtype* 发生了变化。我们再检查一下从索引 `190` 到 `195` 的数据：
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.6: Output from the co2_df DataFrame without NaN parsing](img/file69.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6：未解析 NaN 的 co2_df DataFrame 输出](img/file69.jpg)'
- en: 'Figure 7.6: Output from the co2_df DataFrame without NaN parsing'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：没有NaN解析的co2_df DataFrame输出
- en: 'Finally, you can check how the missing values were handled:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以检查缺失值是如何处理的：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You will notice all seven rows have blank values (empty string).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到所有七行都有空值（空字符串）。
- en: 'In this recipe you explored the `.isna()` method. Once the data is read into
    a DataFrame or series, you get access to the `.isna()` and `.isnull()` interchangeable
    methods, which return `True` if data is missing and `False` otherwise. To get
    the counts for each column, we just chain a `.sum()` function, and to get the
    grand total, we chain another `.sum()` function following that:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，你探索了`.isna()`方法。一旦数据被读取到DataFrame或系列中，你可以访问`.isna()`和`.isnull()`这两个可互换的方法，如果数据缺失，它们会返回`True`，否则返回`False`。要获取每列的计数，我们只需链式调用`.sum()`函数，而要获取总计，则继续链式调用另一个`.sum()`函数：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: There's more…
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'If you know that the data will always contain `?`, which should be converted
    to `NaN` (or any other value), then you can utilize the `pd.read_csv()` function
    and update the `na_values` parameter. This will reduce the number of steps needed
    to clean the data after creating the DataFrame:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道数据中总是会包含`?`，并且应该将其转换为`NaN`（或其他值），你可以利用`pd.read_csv()`函数并更新`na_values`参数。这将减少在创建DataFrame后清理数据所需的步骤：
- en: '[PRE25]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This will replace all instances of `?` with `NaN`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把所有的`?`替换为`NaN`。
- en: See also
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: 'To learn more about the `na_values` and `keep_default_na` parameters from `pandas.read_csv()`,
    please visit the official documentation here: [https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解有关`pandas.read_csv()`中的`na_values`和`keep_default_na`参数的更多信息，请访问官方文档：[https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)
- en: 'To learn more about the `DataFrame.isna()` function, please visit the official
    documentation here: [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于`DataFrame.isna()`函数的信息，请访问官方文档：[https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html)
- en: Handling missing data with univariate imputation using pandas
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用pandas进行单变量插补处理缺失数据
- en: 'Generally, there are two approaches to imputing missing data: `univariate imputation`
    and `multivariate imputation`. This recipe will explore univariate imputation
    techniques available in pandas.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，插补缺失数据有两种方法：`单变量插补`和`多变量插补`。本示例将探讨pandas中可用的单变量插补技术。
- en: In univariate imputation, you use non-missing values in a single variable (think
    a column or feature) to impute the missing values for that variable. For example,
    if you have a sales column in the dataset with some missing values, you can use
    a univariate imputation method to impute missing sales observations using average
    sales. Here, a single column (`sales`) was used to calculate the mean (from non-missing
    values) for imputation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在单变量插补中，你使用单一变量中的非缺失值（比如某一列或特征）来填补该变量的缺失值。例如，如果你的数据集中的销售列有一些缺失值，你可以使用单变量插补方法，通过平均销售额来填补缺失的销售数据。在这里，使用了单一列（`sales`）来计算均值（来自非缺失值）进行填补。
- en: 'Some basic univariate imputation techniques include the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一些基本的单变量插补技术包括以下内容：
- en: Imputing using the **mean**.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**均值**进行填补。
- en: Imputing using the last observation forward (**forward fill**). This can be
    referred to as **Last Observation Carried Forward** (**LOCF**).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最后一个观察值进行前向填补（**前向填充**）。这可以称为**最后观察值向前填充**（**LOCF**）。
- en: Imputing using the next observation backward (**backward fill**). This can be
    referred to as **Next Observation Carried Backward** (**NOCB**).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用下一个观察值进行向后填补（**向后填充**）。这可以称为**下一个观察值向后填充**（**NOCB**）。
- en: You will use two datasets to impute missing data using different techniques
    and then compare the results.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用两个数据集，通过不同的技术进行缺失数据插补，然后比较结果。
- en: Getting ready
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: You can download the Jupyter notebooks and requisite datasets from the GitHub
    repository. Please refer to the *Technical requirements* section of this chapter.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从GitHub仓库下载Jupyter笔记本和所需的数据集。请参考本章的*技术要求*部分。
- en: 'You will be using four datasets from the `Ch7` folder: `clicks_original.csv`,
    `clicks_missing.csv`, `clicks_original.csv`, and `co2_missing_only.csv`. The datasets
    are available from the GitHub repository.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用 `Ch7` 文件夹中的四个数据集：`clicks_original.csv`，`clicks_missing.csv`，`clicks_original.csv`，和
    `co2_missing_only.csv`。这些数据集可以从 GitHub 仓库获取。
- en: How to do it…
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何执行……
- en: 'You will start by importing the libraries and then read all four CSV files.
    You will use the original versions of datasets to compare the results of the imputations
    to gain a better intuition of how they perform. For the comparison measure, you
    will use *RMSE* to evaluate each technique and then visualize the outputs to compare
    the imputation results visually:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你将从导入库开始，然后读取四个 CSV 文件。你将使用数据集的原始版本来比较插补结果，以便更好地理解它们的表现。作为比较度量，你将使用 *RMSE* 来评估每种技术，并通过可视化输出结果来直观比较插补结果：
- en: 'Use the `read_dataset()` function to read the four datasets:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `read_dataset()` 函数读取四个数据集：
- en: '[PRE26]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Visualize the CO2 DataFrames (original and missing) and specify the column
    with missing values (`co2`):'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化 CO2 数据框（原始数据与缺失数据），并指定包含缺失值的列（`co2`）：
- en: '[PRE27]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `plot_dfs` function will produce two plots: the original CO2 dataset without
    missing values, and the altered dataset with missing values.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot_dfs` 函数将生成两个图：一个是原始的无缺失值的 CO2 数据集，另一个是带有缺失值的修改后的数据集。'
- en: '![Figure 7.7: CO2 dataset showing a comparison between the missing values and
    the original](img/file70.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7：CO2 数据集，展示缺失值与原始数据的比较](img/file70.jpg)'
- en: 'Figure 7.7: CO2 dataset showing a comparison between the missing values and
    the original'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7：CO2 数据集，展示缺失值与原始数据的比较
- en: 'From *Figure 7.7*, you can see a noticeable upward trend in CO2 levels over
    time. There is missing data in three different spots. Now, visualize the Clickstream
    DataFrames:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图 7.7* 可以看到 CO2 水平随时间显著上升，并且在三个不同的地方存在缺失数据。现在，开始可视化点击流数据框：
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `plot_dfs` function will produce two plots: the original Clickstream dataset
    without missing values, and the altered dataset with missing values.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot_dfs` 函数将生成两个图：一个是原始的无缺失值的点击流数据集，另一个是带有缺失值的修改后的数据集。'
- en: '![Figure 7.8: Clickstream dataset showing a comparison between the missing
    values and the original](img/file71.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8：点击流数据集，展示缺失值与原始数据的比较](img/file71.jpg)'
- en: 'Figure 7.8: Clickstream dataset showing a comparison between the missing values
    and the original'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8：点击流数据集，展示缺失值与原始数据的比较
- en: 'Notice, the output shows missing data from May 15 to May 30\. You can confirm
    this by running the following code:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，输出显示从 5 月 15 日到 5 月 30 日的数据缺失。你可以通过运行以下代码来确认这一点：
- en: '[PRE29]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now you are ready to perform your first imputation. You will use the `.fillna()`
    method which has a `value` parameter that takes either a numeric or a string value
    to substitute for all the `NaN` instances. Additionally you will use `.ffill()`
    for forward fill, and `.bfill()` for backward fill.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你准备进行第一次插补。你将使用 `.fillna()` 方法，该方法有一个 `value` 参数，接受数字或字符串值，用来替代所有的 `NaN` 实例。此外，你还将使用
    `.ffill()` 进行前向填充，使用 `.bfill()` 进行后向填充。
- en: 'Let''s impute the missing values utilizing the `method` parameter and append
    the results as new columns in the DataFrame. Start with the CO2 DataFrame:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用 `method` 参数来插补缺失值，并将结果作为新列追加到数据框中。首先从 CO2 数据框开始：
- en: '[PRE30]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Use the `rmse_score` function to get the scores:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `rmse_score` 函数获取得分：
- en: '[PRE31]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, visualize the results using the `plot_dfs` function:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用 `plot_dfs` 函数可视化结果：
- en: '[PRE32]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding code produces the results as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码生成的结果如下：
- en: '![Figure 7.9: Comparison between the three imputation methods for the CO2 DataFrame](img/file72.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9：CO2 数据框架中三种插补方法的比较](img/file72.jpg)'
- en: 'Figure 7.9: Comparison between the three imputation methods for the CO2 DataFrame'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9：CO2 数据框架中三种插补方法的比较
- en: Compare the results in *Figure 7.9* with the original data in *Figure 7.7*.
    Notice that both `ffill` and `bfill` produce better results than when using the
    `mean`. Both techniques have favorable RMSE scores and visual representation.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *图 7.9* 中的结果与 *图 7.7* 中的原始数据进行比较。注意，无论是 `ffill` 还是 `bfill` 都比使用 `mean` 方法产生了更好的结果。两种技术都具有较低的
    RMSE 得分，并且视觉效果更好。
- en: 'Now, perform the same imputation methods on the Clickstream DataFrame:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在点击流数据框上执行相同的插补方法：
- en: '[PRE33]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, calculate the RMSE scores:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，计算 RMSE 得分：
- en: '[PRE34]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Interestingly, for the Clickstream dataset, the mean imputation had the lowest
    RMSE score, in contrast to the results from the CO2 dataset. Let''s visualize
    the results to get another perspective on performance:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，对于 Clickstream 数据集，均值插补的 RMSE 分数最低，这与 CO2 数据集的结果形成对比。我们通过可视化结果来从另一个角度审视性能：
- en: '[PRE35]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You get the plots as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 你会得到如下图形：
- en: '![Figure 7.10: Comparison between the three imputation methods for the Clickstream
    DataFrame](img/file73.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10：三种插补方法在 Clickstream 数据框中的比较](img/file73.jpg)'
- en: 'Figure 7.10: Comparison between the three imputation methods for the Clickstream
    DataFrame'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10：三种插补方法在 Clickstream 数据框中的比较
- en: Compare the results in *Figure 7.10* with the original data in *Figure 7.8*.
    Notice that from imputing two different datasets (CO2 and Clickstream), there
    is no *one-size-fits-all strategy* when it comes to handling missing data. Instead,
    each dataset requires a different strategy. Therefore, you should always inspect
    your results and align the outputs with the expectations based on the nature of
    your data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 *图 7.10* 中的结果与 *图 7.8* 中的原始数据。请注意，通过对两个不同数据集（CO2 和 Clickstream）进行插补，处理缺失数据时并没有一个
    *放之四海而皆准的策略*。相反，每个数据集都需要不同的策略。因此，你应该始终检查结果，并根据数据的性质调整输出与预期的匹配。
- en: How it works…
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Using `DataFrame.fillna()` is the simplest imputation method. In the previous
    section you used the `value` parameter within `.fillna()` where you passed the
    mean (a scalar numeric value) to use to fill for all missing values.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `DataFrame.fillna()` 是最简单的插补方法。在前一节中，你使用了 `.fillna()` 中的 `value` 参数，并传递了均值（一个标量数字值）来填充所有缺失值。
- en: Other options used were **backward filling** with`.bfill()`, which uses the
    next observation, after the missing spot(s) and fills the gaps backward. You also
    used **forward filling** with `.ffill()`, which uses the last value, before the
    missing spot(s) and fills the gaps forward.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的其他选项包括 **向后填充**（`.bfill()`），该方法使用缺失位置后的下一个观测值，并向后填充空缺。此外，还使用了 **向前填充**（`.ffill()`），该方法使用缺失位置前的最后一个值，并向前填充空缺。
- en: There's more…
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多……
- en: The `value` parameter in `.fillna()` can also take a Python **dictionary**,
    a pandas **Series**, or a pandas **DataFrame** and not just a **scalar**.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`.fillna()` 中的 `value` 参数也可以接受一个 Python **字典**、一个 pandas **Series** 或一个 pandas
    **DataFrame**，而不仅仅是一个 **标量**。'
- en: Using a Python Dictionary
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 Python 字典
- en: 'Let’s demonstrate this with another example how we can use a Python dictionary
    to impute missing values for multiple columns. Start by reading the `clicks_missing_more.csv`
    file:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过另一个示例演示如何使用 Python 字典为多列插补缺失值。首先读取 `clicks_missing_more.csv` 文件：
- en: '[PRE36]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This should produce the following:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会生成以下结果：
- en: '[PRE37]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Here we have three columns with missing values. We can use a dictionary to
    define a mapping in which each **key-value** pair corresponds to a column in the
    `clicks_missing` DataFrame. We can define different statistical measures (*median*,
    *mean*, and *mode*) for the imputation strategy for different columns. This is
    illustrated in the following code:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有三列包含缺失值。我们可以使用字典来定义一个映射，其中每个 **键值对** 对应 `clicks_missing` 数据框中的一列。我们可以为不同的列定义不同的统计量（*中位数*、*均值*
    和 *众数*）作为插补策略。以下代码展示了这一点：
- en: '[PRE38]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The preceding code should produce the following results indicating all three
    columns have their missing values filled.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码应该会生成以下结果，显示所有三列的缺失值都已填充。
- en: '[PRE39]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The `inplace=True` argument modifies the `clicks_missing` DataFrame in place,
    meaning the changes are applied directly to the DataFrame.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`inplace=True` 参数会直接修改 `clicks_missing` 数据框，意味着所做的更改会直接应用于该数据框。'
- en: Using another DataFrame
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用另一个 DataFrame
- en: You can also use another pandas DataFrame (or Series) to impute missing values,
    the column names need to match in order to map the columns appropriately. In the
    following example you will read the `clicks_missing_more.csv` file and the `clicks_original.csv`
    file for demonstration.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用另一个 pandas DataFrame（或 Series）来插补缺失值，前提是列名需要匹配，以便正确映射各列。在以下示例中，你将读取 `clicks_missing_more.csv`
    文件和 `clicks_original.csv` 文件进行演示。
- en: '[PRE40]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: You will use the `clicks_original` DataFrame to impute missing values in the
    `clicks_missing` DataFrame.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用 `clicks_original` 数据框来插补 `clicks_missing` 数据框中的缺失值。
- en: '[PRE41]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The preceding code should produce the following results indicating all three
    columns have their missing values filled.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码应该会生成以下结果，显示所有三列的缺失值都已填充。
- en: '[PRE42]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: See also
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'To learn more about `DataFrame.fillna()`, please visit the official documentation
    page here: [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html).'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`DataFrame.fillna()`的信息，请访问官方文档页面：[https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html)。
- en: In the following recipe, you will perform similar univariate imputation, but
    this time using the `Scikit-Learn` library.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下的步骤中，你将执行类似的单变量插补，但这次使用的是`Scikit-Learn`库。
- en: Handling missing data with univariate imputation using scikit-learn
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行单变量插补处理缺失数据
- en: '`Scikit-Learn` is a very popular machine learning library in Python. The `scikit-learn`
    library offers a plethora of options for everyday machine learning tasks and algorithms
    such as classification, regression, clustering, dimensionality reduction, model
    selection, and preprocessing.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`Scikit-Learn`是Python中非常流行的机器学习库。`scikit-learn`库提供了大量的选项，涵盖日常机器学习任务和算法，如分类、回归、聚类、降维、模型选择和预处理。'
- en: Additionally, the library offers multiple options for univariate and multivariate
    data imputation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，库还提供了多种选项来进行单变量和多变量数据插补。
- en: Getting ready
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: You can download the Jupyter notebooks and requisite datasets from the GitHub
    repository. Please refer to the *Technical requirements* section of this chapter.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从GitHub存储库下载Jupyter笔记本和必需的数据集。请参考本章的*技术要求*部分。
- en: This recipe will utilize the three functions prepared earlier (`read_dataset`,
    `rmse_score`, and `plot_dfs`).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 本步骤将使用之前准备好的三个函数（`read_dataset`、`rmse_score`和`plot_dfs`）。
- en: 'You will be using four datasets from the `Ch7` folder: `clicks_original.csv`,
    `clicks_missing.csv`, `co2_original.csv`, and `co2_missing_only.csv`. The datasets
    are available from the GitHub repository.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用`Ch7`文件夹中的四个数据集：`clicks_original.csv`、`clicks_missing.csv`、`co2_original.csv`和`co2_missing_only.csv`。这些数据集可以从GitHub存储库下载。
- en: How to do it…
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'You will start by importing the libraries and then read all four CSV files:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你将从导入库开始，然后读取所有四个CSV文件：
- en: 'You will be using the `SimpleImputer` class from the scikit-learn library to
    perform univariate imputation:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将使用`scikit-learn`库中的`SimpleImputer`类来执行单变量插补：
- en: '[PRE43]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`SimpleImputer` accepts different values for the `strategy` parameter, including
    `mean`, `median`, and `most_frequent`. Let''s explore all three strategies and
    see how they compare. Create a list of tuples for each method:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SimpleImputer`接受不同的`strategy`参数值，包括`mean`、`median`和`most_frequent`。让我们探索这三种策略，并看看它们的比较。为每种方法创建一个元组列表：'
- en: '[PRE44]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You can loop through the `Strategy` list to apply the different imputation
    strategies. `SimpleImptuer` has a `fit_transform` method. It combines two steps
    into one: fitting to the data (`.fit`), and then transforming the data (`.transform`).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以循环遍历`Strategy`列表，应用不同的插补策略。`SimpleImputer`有一个`fit_transform`方法。它将两个步骤合并为一个：先拟合数据（`.fit`），然后转换数据（`.transform`）。
- en: 'Keep in mind that `SimpleImputer` accepts a NumPy array, so you will need to
    use the `Series.values` property followed by the `.reshape(-1, 1)` method to create
    a 2D NumPy array. Simply, what this is doing is transforming the 1D array from
    `.values` of shape `(226, )` to a 2D array of shape `(226, 1)`, which is a column
    vector:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`SimpleImputer`接受NumPy数组，因此你需要使用`Series.values`属性，然后调用`.reshape(-1, 1)`方法来创建二维NumPy数组。简单来说，这样做的目的是将形状为`(226,
    )`的1D数组从`.values`转换为形状为`(226, 1)`的二维数组，即列向量：
- en: '[PRE45]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now, both the `clicks_missing` and `co2_missing` DataFrames have three additional
    columns, one for each of the imputation strategies implemented.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`clicks_missing`和`co2_missing`这两个DataFrame已经有了三列额外的列，每一列对应实现的插补策略。
- en: 'Using the `rmse_score` function, you can now evaluate each strategy. Start
    with the CO2 data. You should get an output like the following:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`rmse_score`函数，你现在可以评估每个策略。从CO2数据开始。你应该获得如下输出：
- en: '[PRE46]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'For the Clickstream data, you should get an output like the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Clickstream数据，你应该获得如下输出：
- en: '[PRE47]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Notice how the RMSE strategy rankings vary between the two datasets. For example,
    the `Mean` strategy performed best on the CO2 data, while the `Median` strategy
    did best on the Clickstream data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，RMSE策略排名在两个数据集之间的差异。例如，`Mean`策略在CO2数据上表现最好，而`Median`策略在Clickstream数据上表现最好。
- en: 'Finally, use the `plot_dfs` function to plot the results. Start with the CO2
    dataset:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`plot_dfs`函数绘制结果。从CO2数据集开始：
- en: '[PRE48]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'It produces the following plots:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 它会生成如下图表：
- en: '![Figure 7.11: Comparing three SimpleImputer strategies for the CO2 dataset](img/file74.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 7.11: 比较 CO2 数据集的三种 SimpleImputer 策略](img/file74.jpg)'
- en: 'Figure 7.11: Comparing three SimpleImputer strategies for the CO2 dataset'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 7.11: 比较 CO2 数据集的三种 SimpleImputer 策略'
- en: 'Compare the results in *Figure 7.11* with the original data in *Figure 7.7*.
    For the Clickstream dataset, you should use the following:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *Figure 7.11* 中的结果与 *Figure 7.7* 中的原始数据进行比较。对于 Clickstream 数据集，你应该使用以下内容：
- en: '[PRE49]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This should plot all three strategies:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该绘制出所有三种策略：
- en: '![Figure 7.12: Comparing three SimpleImputer strategies for the Clickstream
    dataset](img/file75.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 7.12: 比较 Clickstream 数据集的三种 SimpleImputer 策略](img/file75.jpg)'
- en: 'Figure 7.12: Comparing three SimpleImputer strategies for the Clickstream dataset'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 7.12: 比较 Clickstream 数据集的三种 SimpleImputer 策略'
- en: Compare the results in *Figure 7.12* with the original data in *Figure 7.8*.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *Figure 7.12* 中的结果与 *Figure 7.8* 中的原始数据进行比较。
- en: '`SimpleImputer` provides basic strategies that may be suitable with some data
    but not others. The advantage of these simple imputation strategies (including
    the ones from the previous *Handling missing data with univariate imputation using
    pandas* recipe) is that they are fast and straightforward to implement.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`SimpleImputer` 提供了一些基本策略，对某些数据可能合适，对其他数据则不一定。这些简单的填充策略（包括前一节 *使用 pandas 进行单变量填充处理缺失数据*
    中的方法）的优点在于它们快速且易于实现。'
- en: How it works…
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'You used the `SimpleImputer` class to implement three simple strategies to
    impute missing values: mean, median, and most frequent (mode).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用 `SimpleImputer` 类实现了三种简单策略来填补缺失值：均值、中位数和最频繁值（众数）。
- en: This is a univariate imputation technique, meaning only one feature or column
    was used to compute the mean, median, and most frequent value.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种单变量填充技术，意味着仅使用一个特征或列来计算均值、中位数和最频繁值。
- en: 'The `SimpleImptuer` class has three parameters that you need to know:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`SimpleImputer` 类有三个参数是你需要了解的：'
- en: '`missing_values`, which, by default, is set to `nan`, and, more specifically,
    `np.nan`. NumPy `nan` and pandas `NaN` are similar, as you can see from the following
    example:'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`missing_values` 默认设置为 `nan`，更具体地说是 `np.nan`。NumPy 的 `nan` 和 pandas 的 `NaN`
    很相似，正如下面的例子所示：'
- en: '[PRE50]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '`SimpleImputer` will impute all occurrences of the `missing_values`, which
    you can update with `pandas.NA`, an integer, float, or a string value.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`SimpleImputer` 将填补所有 `missing_values` 的出现情况，你可以使用 `pandas.NA`、整数、浮点数或字符串值来更新它。'
- en: '`strategy`, which defaults to `mean`, and takes string values.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strategy` 默认为 `mean`，接受字符串值。'
- en: '`fill_value` can be used to replace all instances from `missing_values` with
    a specific value. This can either be a string or a numeric value. If the `strategy`
    was set to `constant`, then you will need to provide your custom `fill_value`.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fill_value` 可以用特定值替换所有 `missing_values` 的实例。这可以是字符串或数值。如果 `strategy` 设置为 `constant`，则需要提供自定义的
    `fill_value`。'
- en: 'In Scikit-Learn, a common workflow for preprocessing data, such as imputation,
    involves two main steps:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scikit-Learn 中，用于预处理数据（如填补）的常见工作流程包括两个主要步骤：
- en: '**Fitting the Imputer**: First, you **fit** the imputer to the data using the
    `.fit()` method. This step involves "**training**" the imputer, which in the context
    of imputation means calculating the necessary statistics (like mean, median, etc.)
    from the provided data. The fitting process usually is done on the training dataset.'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**拟合 Imputer**：首先，你使用 `.fit()` 方法对数据进行拟合。这一步涉及“训练” imputer，在填补的上下文中意味着从提供的数据中计算所需的统计数据（如均值、中位数等）。拟合过程通常在训练数据集上完成。'
- en: '**Applying the Transform**: After fitting, you apply the imputer to the data
    with the `.transform()` method. This step actually performs the imputation, replacing
    missing values with the computed statistics.'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**应用转换**：在拟合后，使用 `.transform()` 方法将 imputer 应用于数据。这一步骤实际上执行了填充操作，用计算出的统计数据替换缺失值。'
- en: In our example, these two steps were combined into one using the `.fit_transform()`
    method. This method first fits the imputer on the data (i.e., computes the necessary
    statistics) and then immediately applies the transformation (i.e., replaces missing
    values). Using `.fit_transform()` is a convenient approach, especially during
    the initial data preprocessing phase.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，这两个步骤被合并为一个，使用 `.fit_transform()` 方法。该方法首先对数据进行拟合（即计算所需的统计数据），然后立即应用转换（即替换缺失值）。在初始数据预处理阶段使用
    `.fit_transform()` 是一种方便的方法。
- en: Additionally, the pandas DataFrame, `.fillna()`, can provide the same functionality
    as `SimpleImputer`. For example, the `mean` strategy can be accomplished by using
    the pandas `DataFrame.mean()` method and passing it to `.fillna()`.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，pandas 的 DataFrame 中，`.fillna()` 方法可以提供与 `SimpleImputer` 相同的功能。例如，`mean`
    策略可以通过使用 pandas 的 `DataFrame.mean()` 方法，并将其传递给 `.fillna()` 来实现。
- en: 'The following example illustrates this and compares the two outcomes from Scikit-Learn
    and pandas:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例说明了这一点，并比较了 Scikit-Learn 和 pandas 的两个结果：
- en: '[PRE51]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Notice how you were able to accomplish the same results as the `SimpleImputer`
    class from scikit-learn. The `.fillna()` method makes it easier to scale the imputation
    across the entire DataFrame (column by column). For example, if you have a `sales_report_data`
    DataFrame with multiple columns containing missing data, you can perform a mean
    imputation with a single line, `sales_report_data.fillna(sales_report_data.mean()).`
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 注意你是如何与 scikit-learn 的 `SimpleImputer` 类实现相同的结果的。`.fillna()` 方法使得在整个 DataFrame
    中进行插补变得更加容易（逐列处理）。例如，如果你有一个包含多个缺失数据列的 `sales_report_data` DataFrame，你可以通过一行代码
    `sales_report_data.fillna(sales_report_data.mean())` 来执行均值插补。
- en: There’s more
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多内容
- en: The add_indicator option in scikit-learn’s SimpleImputer is a useful feature
    for enhancing the imputation process. What it does, is add a MissingIndicator
    transform into the output (adds an additional binary column either indicating
    if original data was missing with 1 or observed with 0\. This can be useful for
    encoding missing information as a feature which can provide additional insights.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 中的 SimpleImputer 的 `add_indicator` 选项是一个有用的功能，可以增强插补过程。它的作用是将一个
    MissingIndicator 转换器添加到输出中（添加一个额外的二进制列，指示原始数据是否缺失，1 表示缺失，0 表示已观察到。这个功能可以用于将缺失信息编码为特征，从而提供额外的洞察）。
- en: 'The following is an example on how to enable this feature with `add_indicator=True`.
    You will use the `.fit()` followed by `.transform()` in this example:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用 `add_indicator=True` 启用此功能的示例。在此示例中，你将使用 `.fit()` 方法，接着使用 `.transform()`：
- en: '[PRE52]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'You can then use `.transform()` and add the two columns to the original `co2_missing`
    DataFrame:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以使用 `.transform()` 方法，并将两列添加到原始的 `co2_missing` DataFrame 中：
- en: '[PRE53]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The preceding code should produce the following output
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码应该会产生以下输出：
- en: '![Figure 7.13: Updating the co2_missing DataFrame with two columns](img/file76.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13：更新 co2_missing DataFrame，添加两列](img/file76.png)'
- en: 'Figure 7.13: Updating the co2_missing DataFrame with two columns'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13：更新 co2_missing DataFrame，添加两列
- en: Notice how the indicator column is added in which 0 indicates original observed
    value and 1 indicates missing value.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 注意如何添加了指示列，其中 0 表示原始观察值，1 表示缺失值。
- en: See also
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参见
- en: 'To learn more about scikit-learn''s `SimpleImputer` class, please visit the
    official documentation page here: [https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer).'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多关于 scikit-learn 的 `SimpleImputer` 类，请访问官方文档页面：[https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)。
- en: So far, you have been dealing with univariate imputation. A more powerful approach
    is multivariate imputation, which you will learn in the following recipe.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你一直在处理单变量插补。一个更强大的方法是多变量插补，你将在接下来的示例中学习到这一方法。
- en: Handling missing data with multivariate imputation
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多变量插补处理缺失数据
- en: 'Earlier, we discussed the fact that there are two approaches to imputing missing
    data: **univariate** **imputation** and **multivariate** **imputation**.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们讨论了填补缺失数据的两种方法：**单变量** **插补** 和 **多变量** **插补**。
- en: As you have seen in the previous recipes, univariate imputation involves using
    one variable (column) to substitute for the missing data, disregarding other variables
    in the dataset. Univariate imputation techniques are usually faster and simpler
    to implement, but a multivariate approach may produce better results in most situations.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在之前的示例中看到的，单变量插补是使用一个变量（列）来替代缺失的数据，而忽略数据集中的其他变量。单变量插补技术通常实现起来更快、更简单，但在大多数情况下，多变量插补方法可能会产生更好的结果。
- en: 'Instead of using a single variable (column), in a multivariate imputation,
    the method uses multiple variables within the dataset to impute missing values.
    The idea is simple: Have more variables within the dataset chime in to improve
    the predictability of missing values.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用单一变量（列）不同，在多元插补中，该方法使用数据集中的多个变量来插补缺失值。其理念很简单：让数据集中的更多变量参与进来，以提高缺失值的可预测性。
- en: In other words, univariate imputation methods handle missing values for a particular
    variable in isolation of the entire dataset and just focus on that variable to
    derive the estimates. In multivariate imputation, the assumption is that there
    is some synergy within the variables in the dataset, and collectively, they can
    provide better estimates to fill in for the missing values.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，单变量插补方法仅处理特定变量的缺失值，而不考虑整个数据集，专注于该变量来推导估算值。在多元插补中，假设数据集中的变量之间存在一定的协同作用，并且它们可以共同提供更好的估算值来填补缺失值。
- en: In this recipe, you will be working with the `Clickstream` dataset since it
    has additional variables (`clicks`, `price`, and `location` columns) to perform
    multivariate imputation for *clicks*.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，你将使用 `Clickstream` 数据集，因为它有额外的变量（`clicks`，`price` 和 `location` 列），可以用来对
    *clicks* 进行多元插补。
- en: Getting ready
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: You can download the Jupyter notebooks and requisite datasets from the GitHub
    repository. Please refer to the *Technical requirements* section of this chapter.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 GitHub 仓库下载 Jupyter 笔记本和所需的数据集。请参考本章的 *技术要求* 部分。
- en: In addition, you will leverage the three functions defined earlier in the chapter
    (`read_dataset`, `rmse_score`, and `plot_dfs`).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还将利用本章前面定义的三个函数（`read_dataset`，`rmse_score` 和 `plot_dfs`）。
- en: How to do it…
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'In this recipe, you will use scikit-learn for the multivariate imputation.
    The library provides the `IterativeImputer` class, which allows you to pass a
    regressor to predict the missing values from other variables (columns) within
    the dataset:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，你将使用 scikit-learn 进行多元插补。该库提供了 `IterativeImputer` 类，允许你传递一个回归模型来根据数据集中其他变量（列）预测缺失值：
- en: 'Start by importing the necessary libraries, methods, and classes:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入必要的库、方法和类：
- en: '[PRE54]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Load the two Clickstream datasets into DataFrames:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 将两个 Clickstream 数据集加载到数据框中：
- en: '[PRE55]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'With `IterativeImputer`, you can test different estimators. So, let''s try
    different regressors and compare the results. Create a list of the regressors
    (estimators) to be used in `IterativeImputer`:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `IterativeImputer`，你可以测试不同的估算器。那么，让我们尝试不同的回归器并比较结果。创建一个要在 `IterativeImputer`
    中使用的回归器（估算器）列表：
- en: '[PRE56]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Loop through the estimators and train on the dataset using `.fit()`, thereby
    building different models, and finally apply the imputation using `.transform()`
    on the variable with missing data. The results of each estimator will be appended
    as a new column to the `clicks_missing` DataFrame so that it can be used for scoring
    and compare the results visually:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历估算器并使用 `.fit()` 在数据集上进行训练，从而构建不同的模型，最后通过在缺失数据的变量上应用 `.transform()` 进行插补。每个估算器的结果将作为新列附加到
    `clicks_missing` 数据框中，以便进行得分并直观地比较结果：
- en: '[PRE57]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Using the `rmse_score` function, evaluate each estimator:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `rmse_score` 函数评估每个估算器：
- en: '[PRE58]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This should print the following scores:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出以下得分：
- en: '[PRE59]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Observe that Bayesian Ridge, ElasticNet, and Linear Regression produce similar
    results.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到贝叶斯岭回归、弹性网和线性回归产生了相似的结果。
- en: 'Finally, plot the results for a visual comparison between the different estimators:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，绘制结果以便于不同估算器之间的可视化比较：
- en: '[PRE60]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The output is as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.14: Comparing different estimators using IterativeImputation](img/file77.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.14：使用迭代插补比较不同的估算器](img/file77.jpg)'
- en: 'Figure 7.14: Comparing different estimators using IterativeImputation'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14：使用迭代插补比较不同的估算器
- en: Compare the results in *Figure 7.14* with the original data in *Figure 7.8*.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 *图 7.14* 中的结果与 *图 7.8* 中的原始数据。
- en: At the beginning of the chapter, we discussed that using RMSE (*Root Mean Square
    Error)* for evaluating imputation methods can be somewhat misleading. This is
    because our objective with imputation is not necessarily to achieve the 'best'
    score (i.e., the smallest RMSE value), as we would aim for in predictive modeling.
    Instead, our goal with imputation is to fill missing data in a way that closely
    resembles the true nature and distribution of the original dataset.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时，我们讨论了使用 RMSE（*均方根误差*）来评估填补方法可能会有些误导。这是因为我们进行填补的目标不一定是为了得到“最佳”分数（即最小的
    RMSE 值），就像在预测建模中我们所追求的那样。相反，填补的目标是以一种尽可能接近原始数据集的真实特性和分布的方式来填补缺失的数据。
- en: While RMSE does have limitations, it can still provide valuable insights when
    comparing different imputation methods. It helps us understand which method estimates
    the missing values more closely to their actual values, based on the available
    data.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 RMSE 存在局限性，但在比较不同填补方法时，它仍然能提供有价值的洞察。它帮助我们理解哪种方法能更接近实际值地估算缺失值，基于可用数据。
- en: However, it's crucial to recognize that a lower RMSE doesn't always mean a more
    'accurate' imputation in the context of real-world data. This is because real
    datasets often contain noise and randomness, which some imputation methods might
    fail to capture, especially those that produce the lowest RMSE scores. Methods
    like `BayesianRidge`, `ElasticNet`, and `Linear Regression` might yield lower
    RMSE values but could **oversmooth** the data (see *Figure 7.14* for those three
    estimators), failing to reflect the inherent randomness and variability present
    in real datasets.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，必须认识到，在真实数据的背景下，较低的 RMSE 并不总意味着更“准确”的填补。这是因为真实数据集通常包含噪音和随机性，而一些填补方法可能无法捕捉到这些特性，尤其是那些产生最低
    RMSE 值的方法。像 `BayesianRidge`、`ElasticNet` 和 `Linear Regression` 这样的算法可能会产生较低的 RMSE
    值，但可能会**过度平滑**数据（请参见*图 7.14*中的这三种估计器），未能反映真实数据集中固有的随机性和变异性。
- en: Later, when using the imputed data for building predictive models (like forecasting
    models), we need to acknowledge that some level of imperfection in the imputed
    values is acceptable. This is because we often don't know the true nature of the
    missing data, and our aim is to create a dataset that provides a 'good enough'
    representation for model training and analysis. In essence, the objective is to
    achieve a balance – an imputation that provides a reasonable estimate of missing
    values while preserving the overall characteristics of the data, including its
    randomness and variability.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 后续，在使用填补后的数据建立预测模型（如预测模型）时，我们需要认识到填补值的某些不完美性是可以接受的。这是因为我们通常不知道缺失数据的真实性质，而我们的目标是创建一个能为模型训练和分析提供“足够好”代表性的数据集。本质上，目标是实现平衡——一种提供合理缺失值估计的填补方法，同时保持数据的整体特性，包括其随机性和变异性。
- en: How it works…
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'The R MICE package inspired the `IterativeImputer` class from the scikit-learn
    library to implement `Multivariate Imputation by Chained Equation` ([https://www.jstatsoft.org/article/view/v045i03](https://www.jstatsoft.org/article/view/v045i03)).
    `IterativeImputer` does differ from the original implementation, which you can
    read more about here: [https://scikit-learn.org/stable/modules/impute.html#id2](https://scikit-learn.org/stable/modules/impute.html#id2).'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: R 语言中的 MICE 包启发了 `IterativeImputer` 类的设计，后者由 scikit-learn 库实现，用于执行 `多元链式方程法填补`（[https://www.jstatsoft.org/article/view/v045i03](https://www.jstatsoft.org/article/view/v045i03)）。`IterativeImputer`
    与原始实现有所不同，你可以在这里阅读更多内容：[https://scikit-learn.org/stable/modules/impute.html#id2](https://scikit-learn.org/stable/modules/impute.html#id2)。
- en: Keep in mind that `IterativeImputer` is still in experimental mode. In the next
    section, you will use another implementation of `MICE` from the `statsmodels`
    library.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`IterativeImputer` 仍处于实验模式。在下一节中，你将使用 `statsmodels` 库中的另一种 `MICE` 实现。
- en: There's more…
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多……
- en: The `statsmodels` library has an implementation of `MICE` that you can use to
    compare with `IterariveImputer`. This implementation is closer to the `MICE` implementation
    in R.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`statsmodels` 库中有一个 `MICE` 的实现，你可以用它来与 `IterativeImputer` 进行比较。这个实现更接近于 R 中的
    `MICE` 实现。'
- en: You will use the same DataFrames (`clicks_original` and `clicks_missing`) and
    append the `statsmodels` MICE imputation output to the `clicks_missing` DataFrame
    as an additional column.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用相同的 DataFrame（`clicks_original` 和 `clicks_missing`），并将 `statsmodels` MICE
    填补输出作为附加列添加到 `clicks_missing` DataFrame 中。
- en: 'Start by loading the required libraries:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载所需的库：
- en: '[PRE61]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Since your goal is to impute missing data, you can use the `MICEData` class
    to wrap the `clicks_missing` DataFrame. Start by creating an instance of `MICEData`
    and store it in a `mice_data` variable:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你的目标是插补缺失数据，你可以使用`MICEData`类来封装`clicks_missing`数据框。首先创建`MICEData`的实例，并将其存储在`mice_data`变量中：
- en: '[PRE62]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The `MICEData` prepares the dataset for MICE imputation and the `update_all()`
    method is called in a loop (20 times) to perform multiple iterations of imputations,
    each time refining the imputed values based on the other variables in the dataset.
    The `perturbation_method='gaussian'` specifies the method used for perturbing
    the missing data during the imputation process. The `'gaussian'` method adds noise
    drawn from a normal (gaussian) distribution.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '`MICEData`为MICE插补准备数据集，并在循环中调用`update_all()`方法（执行20次）进行多次插补，每次基于数据集中的其他变量优化插补值。`perturbation_method=''gaussian''`指定在插补过程中用于扰动缺失数据的方法。`''gaussian''`方法从正态（高斯）分布中添加噪声。'
- en: 'Store the results in a new column and call it `MICE`. This way, you can compare
    the scores with results from `IterativeImputer`:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 将结果存储在新列中，并命名为`MICE`。这样，你可以将得分与`IterativeImputer`的结果进行比较：
- en: '[PRE63]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Finally, visualize the results for a final comparison. This will include some
    of the imputations from `IterativeImputer`:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，绘制结果进行最终比较。这将包括来自`IterativeImputer`的一些插补结果：
- en: '[PRE64]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The output is as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.15 Comparing the statsmodels MICE implementation with the scikit-learn
    IterativeImputer](img/file78.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![图7.15 比较statsmodels MICE实现与scikit-learn IterativeImputer](img/file78.jpg)'
- en: Figure 7.15 Comparing the statsmodels MICE implementation with the scikit-learn
    IterativeImputer
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 比较statsmodels MICE实现与scikit-learn IterativeImputer
- en: Compare the results in *Figure 7.15* with the original data in *Figure 7.8*.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 将*图7.15*中的结果与*图7.8*中的原始数据进行比较。
- en: Overall, multivariate imputation techniques generally produce better results
    than univariate methods. This is true when working with more complex time-series
    datasets in terms of the number of features (columns) and records. Though univariate
    imputers are more efficient in terms of speed and simplicity to interpret, there
    is a need to balance complexity, quality, and analytical requirements.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，多元插补技术通常比单变量方法产生更好的结果。在处理具有更多特征（列）和记录的复杂时间序列数据集时，这一点尤其成立。尽管单变量插补器在速度和解释简便性方面更高效，但仍然需要平衡复杂性、质量和分析需求。
- en: See also
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: 'To learn more about `IterativeImputer`, please visit the official documentation
    page here: [https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer).'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解有关`IterativeImputer`的更多信息，请访问官方文档页面：[https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer)。
- en: 'To learn more about `statsmodels` MICE implementation, please visit the official
    documentation page here: [https://www.statsmodels.org/dev/imputation.html](https://www.statsmodels.org/dev/imputation.html).'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解有关`statsmodels` MICE实现的更多信息，请访问官方文档页面：[https://www.statsmodels.org/dev/imputation.html](https://www.statsmodels.org/dev/imputation.html)。
- en: 'An interesting library, `FancyImpute`, that originally inspired scikit-learn''s
    `IterativeImputer` offers a variety of imputation algorithms that you can check
    out here: [https://github.com/iskandr/fancyimpute](https://github.com/iskandr/fancyimpute).'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个有趣的库，`FancyImpute`，最初启发了scikit-learn的`IterativeImputer`，提供了多种插补算法，你可以在此查看：[https://github.com/iskandr/fancyimpute](https://github.com/iskandr/fancyimpute)。
- en: Handling missing data with interpolation
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用插值处理缺失数据
- en: Another commonly used technique for imputing missing values is *interpolation*.
    The pandas library provides the `DataFrame.interpolate()` method for more complex
    univariate imputation strategies.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常用的缺失值插补技术是*插值*。pandas库提供了`DataFrame.interpolate()`方法，用于更复杂的单变量插补策略。
- en: For example, one of the interpolation methods available is linear interpolation.
    **Linear interpolation** can be used to impute missing data by drawing a straight
    line between the two points surrounding the missing value (in time series, this
    means for a missing data point, it looks at a prior past value and the next future
    value to draw a line between them). A polynomial interpolation, on the other hand,
    will attempt to draw a curved line between the two points. Hence, each method
    will have a different mathematical operation to determine how to fill in for the
    missing data.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，插值方法之一是线性插值。**线性插值**可以通过在两个围绕缺失值的点之间画一条直线来填补缺失数据（在时间序列中，这意味着对于缺失的数据点，它会查看前一个过去值和下一个未来值，并在这两者之间画一条直线）。而多项式插值则尝试在两个点之间画一条曲线。因此，每种方法都会采用不同的数学运算来确定如何填充缺失数据。
- en: The interpolation capabilities in pandas can be extended further through the
    **SciPy** library, which offers additional univariate and multivariate interpolations.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: pandas的插值功能可以通过**SciPy**库进一步扩展，后者提供了额外的单变量和多变量插值方法。
- en: In this recipe, you will use the pandas `DataFrame.interpolate()` function to
    examine different interpolation methods, including linear, polynomial, quadratic,
    nearest, and spline.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，你将使用pandas的`DataFrame.interpolate()`函数来检查不同的插值方法，包括线性插值、多项式插值、二次插值、最近邻插值和样条插值。
- en: Getting ready
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: You can download the Jupyter notebooks and requisite datasets from the GitHub
    repository. Please refer to the *Technical requirements* section of this chapter.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从GitHub仓库下载Jupyter笔记本和必要的数据集。请参考本章的*技术要求*部分。
- en: You will utilize the three functions prepared earlier (`read_dataset`, `rmse_score`,
    and `plot_dfs`).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用之前准备好的三个函数（`read_dataset`，`rmse_score`和`plot_dfs`）。
- en: 'You will be using four datasets from the `Ch7` folder: `clicks_original.csv`,
    `clicks_missing.csv`, `co2_original.csv`, and `co2_missing_only.csv`. The datasets
    are available from the GitHub repository.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用来自`Ch7`文件夹的四个数据集：`clicks_original.csv`，`clicks_missing.csv`，`co2_original.csv`和`co2_missing_only.csv`。这些数据集可以从GitHub仓库中获取。
- en: How to do it…
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 操作步骤…
- en: 'You will perform multiple interpolations on two different datasets and then
    compare the results using RMSE and visualization:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 你将对两个不同的数据集进行多次插值，然后使用RMSE和可视化对比结果：
- en: 'Start by importing the libraries and reading the data into DataFrames:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先导入相关库并将数据读取到DataFrame中：
- en: '[PRE65]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Create a list of the interpolation methods to be tested: `linear`, `quadratic`,
    `nearest`, and `cubic`:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个待测试的插值方法列表：`linear`，`quadratic`，`nearest`和`cubic`：
- en: '[PRE66]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'You will loop through the list to run different interpolations using `.interpolate()`.
    Append a new column for each interpolation output to be used for comparison:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将遍历列表，使用`.interpolate()`进行不同的插值操作。为每个插值输出附加一个新列，以便进行对比：
- en: '[PRE67]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'There are two additional methods that it would be interesting to test: *spline*
    and *polynomial*. To use these methods, you will need to provide an integer value
    for the order parameter. You can try `order = 2` for the spline method, and `order
    = 5` for the polynomial method. For the spline method, for example, it would look
    like this: `.interpolate(method="spline", order = 2)`:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还有两种附加的方法值得测试：*样条插值*和*多项式插值*。要使用这些方法，你需要为`order`参数提供一个整数值。你可以尝试`order = 2`来使用样条插值方法，尝试`order
    = 5`来使用多项式插值方法。例如，样条插值方法的调用方式如下：`.interpolate(method="spline", order = 2)`：
- en: '[PRE68]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Use the `rmse_score` function to compare the results from the different interpolation
    strategies. Start with CO2 data:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`rmse_score`函数比较不同插值策略的结果。从CO2数据开始：
- en: '[PRE69]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now, let''s check the Clickstream data:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查Clickstream数据：
- en: '[PRE70]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Lastly, visualize the results to gain a better idea of how each interpolation
    worked. Start with the CO2 dataset:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，进行可视化以更好地了解每种插值方法的效果。从CO2数据集开始：
- en: '[PRE71]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'This should plot the selected columns:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会绘制出所选列：
- en: '![Figure 7.16: Comparing the different interpolation strategies on the CO2
    dataset](img/file79.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![图7.16：对CO2数据集的不同插值策略进行比较](img/file79.jpg)'
- en: 'Figure 7.16: Comparing the different interpolation strategies on the CO2 dataset'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16：对CO2数据集的不同插值策略进行比较
- en: Compare the results in *Figure 7.16* with the original data in *Figure 7.7*.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 将*图7.16*中的结果与*图7.7*中的原始数据进行对比。
- en: Both the `linear` and `nearest` methods seem to have a similar effect regarding
    how the missing values were imputed. This can be seen from the RMSE scores and
    plot.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '`线性`和`最近邻`方法在缺失值填充效果上似乎类似。可以通过RMSE评分和图表看到这一点。'
- en: 'Now, create the plots for the Clickstream dataset:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，创建 Clickstream 数据集的图表：
- en: '[PRE72]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This should plot the selected columns:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该绘制所选列：
- en: '![Figure 7.17: Comparing the different interpolation strategies on the Clickstream
    dataset](img/file80.jpg)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.17: 比较 Clickstream 数据集上的不同插值策略](img/file80.jpg)'
- en: 'Figure 7.17: Comparing the different interpolation strategies on the Clickstream
    dataset'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7.17: 比较 Clickstream 数据集上的不同插值策略'
- en: Compare the results in *Figure 7.17* with the original data in *Figure 7.8*.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 比较*图 7.17*中的结果与*图 7.8*中的原始数据。
- en: From the output, you can see how the `polynomial` method exaggerated the curve
    when using `5` as the polynomial order. On the other hand, the `Linear` method
    attempts to draw a straight line.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，你可以看到当使用`5`作为多项式阶数时，`polynomial`方法如何夸大曲线。另一方面，`Linear`方法则尝试绘制一条直线。
- en: One thing to note is that between the strategies implemented, only linear interpolation
    ignores the index, while the rest use numerical values for the index.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在实现的策略中，只有线性插值忽略了索引，而其他方法则使用数值索引。
- en: How it works…
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Overall, the interpolation technique detects patterns in neighboring data points
    (to the missing points) to predict what the missing values should be. The simplest
    form is linear interpolation, which assumes a straight line between two neighboring
    data points. On the other hand, a polynomial defines a curve between the two adjacent
    data points. Each interpolation method uses a different function and mechanism
    to predict the missing data.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，插值技术通过检测相邻数据点（缺失点周围的数据）的模式，来预测缺失值应该是什么。最简单的形式是线性插值，它假设两个相邻数据点之间存在一条直线。另一方面，多项式则定义了两个相邻数据点之间的曲线。每种插值方法使用不同的函数和机制来预测缺失数据。
- en: In pandas, you will use the `DataFrame.interpolate` function. The default interpolation
    method is the linear interpolation (`method = "linear"`). There are additional
    parameters to provide more control over how the imputation with interpolation
    is done.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pandas 中，你将使用`DataFrame.interpolate`函数。默认的插值方法是线性插值（`method = "linear"`）。还有其他参数可以提供更多控制，决定如何进行插值填充。
- en: 'The `limit` parameter allows you to set the maximum number of consecutive `NaN`
    to fill. Recall in the previous recipe, *Performing data quality checks*, that
    the Clickstream dataset had `16` consecutive missing points. You can limit the
    number of consecutive `NaN`, for example, to `5`:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '`limit`参数允许你设置填充的最大连续`NaN`数量。回想之前的配方，*执行数据质量检查*，Clickstream 数据集有`16`个连续的缺失值。你可以限制连续`NaN`的数量，例如设置为`5`：'
- en: '[PRE73]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Only 5 data points were imputed; the remaining 11 were not.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 仅填充了 5 个数据点，其余 11 个未填充。
- en: There's more…
  id: totrans-392
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Other libraries also offer interpolation, including the following:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 其他库也提供插值，包括以下内容：
- en: 'SciPy provides a more extensive selection covering univariate and multivariate
    techniques: [https://docs.scipy.org/doc/scipy/reference/interpolate.html](https://docs.scipy.org/doc/scipy/reference/interpolate.html).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SciPy 提供了更广泛的选择，涵盖了单变量和多变量技术：[https://docs.scipy.org/doc/scipy/reference/interpolate.html](https://docs.scipy.org/doc/scipy/reference/interpolate.html).
- en: 'NumPy offers a couple of interpolation options; the most widely used is the
    `numpy.interp()` function: [https://numpy.org/doc/stable/reference/generated/numpy.interp.html?highlight=interp#numpy.interp.](https://numpy.org/doc/stable/reference/generated/numpy.interp.html?highlight=interp#numpy.interp.)'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy 提供了几种插值选项；最常用的是`numpy.interp()`函数：[https://numpy.org/doc/stable/reference/generated/numpy.interp.html?highlight=interp#numpy.interp.](https://numpy.org/doc/stable/reference/generated/numpy.interp.html?highlight=interp#numpy.interp.)
- en: See also
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: 'To learn more about `DataFrame.interpolate`, please visit the official documentation
    page here: [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html).'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多关于`DataFrame.interpolate`的信息，请访问官方文档页面：[https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html).
