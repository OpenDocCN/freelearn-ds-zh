- en: Chapter 7. Unsupervised Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 无监督机器学习
- en: In the previous chapter, we learned about supervised machine learning algorithms
    and how we can use them in real-world scenarios.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们学习了监督式机器学习算法，以及如何在现实世界的场景中使用它们。
- en: Unsupervised learning is a little bit different and harder. The aim is to have
    the system learn something, but we ourselves don't know what to learn. There are
    two approaches to the unsupervised learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习有些不同且更加复杂。其目标是让系统学习某些东西，但我们自己并不知道要学什么。无监督学习有两种方法。
- en: One approach is to find the similarities/patterns in the datasets. Then we can
    create clusters of these similar points. We make the assumption that the clusters
    that we found can be classified and can be provided with a label.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是寻找数据集中的相似性/模式。然后，我们可以将这些相似的点创建为聚类。我们假设找到的聚类是可以分类的，并且可以为其分配标签。
- en: The algorithm itself cannot assign names because it doesn't have any. It can
    only find the clusters based on the similarities, but nothing more than that.
    To actually be able to find meaningful clusters, a good size of dataset is required.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 算法本身无法分配名称，因为它没有任何标签。它只能基于相似性找到聚类，但仅此而已。为了真正能够找到有意义的聚类，需要一个足够大的数据集。
- en: It is used extensively in finding similar users, recommender systems, text classification,
    and so on.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 它在寻找相似用户、推荐系统、文本分类等方面被广泛使用。
- en: 'We will discuss various clustering algorithms in detail. In this chapter, we
    will learn:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细讨论各种聚类算法。在本章中，我们将学习：
- en: Working with unlabeled data.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与无标签数据进行处理。
- en: What is unsupervised learning?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是无监督学习？
- en: What is clustering?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是聚类？
- en: Different types of clustering.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的聚类。
- en: The K-Means algorithm and Bisecting K-means. Its strengths and weaknesses.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值算法和二分K均值算法，它们的优缺点。
- en: Hierarchical clustering.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类。
- en: Agglomerative clustering. Its strengths and weaknesses.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合聚类，它的优缺点。
- en: The DBSCAN algorithm.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN算法。
- en: We should also discuss the second approach before we can start delving deep
    into clustering. It will tell us how different clustering is from this approach
    and the use cases. The second approach is a kind of reinforcement learning. This
    involves rewards to indicate success to the algorithm. There are no explicit categorizations
    done. This type of algorithm is best suited for real-world algorithms. In this
    algorithm, the system behaves on the previous rewards or the punishments it got.
    This kind of learning can be powerful because there is no prejudice and there
    are no pre-classified observations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨聚类之前，我们还应讨论第二种方法。它将告诉我们聚类与这种方法的不同之处以及使用场景。第二种方法是一种强化学习，涉及通过奖励来指示算法的成功。没有明确的分类，这种算法最适合应用于现实世界。系统根据之前的奖励或惩罚来调整行为。这种学习方式可能很强大，因为它没有偏见，也没有预先分类的观察结果。
- en: This calculates the possibility of every action and knows beforehand what action
    will lead to what kind of result.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 它计算每个动作的可能性，并提前知道哪个动作会导致什么样的结果。
- en: This trial and error method is computationally intensive and consumes a lot
    of time. Let's discuss the clustering approach that is not based on trial and
    error.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种试错方法计算量大且耗时。让我们讨论一种不依赖于试错的聚类方法。
- en: Understanding clustering
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解聚类
- en: Clustering is a technique to divide data into groups (clusters) that are useful
    and meaningful. The clusters are formed capturing the natural structure of the
    data, which have meaningful relations with each other. It is also possible that
    this is only used at the preparation or the summarization stage for the other
    algorithms or further analysis. Cluster analysis has roles in many fields, such
    as biology, pattern recognition, information retrieval, and so on.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种将数据划分为有用且有意义的组（聚类）的技术。这些聚类是通过捕捉数据的自然结构而形成的，彼此之间具有有意义的关系。也有可能它仅在其他算法或进一步分析的准备或总结阶段使用。聚类分析在许多领域都有应用，如生物学、模式识别、信息检索等。
- en: 'Clustering has applications in different fields:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类在不同领域有广泛应用：
- en: '**Information retrieval**: To segregate the information into particular clusters
    is an important step in searching and retrieving information from the numerous
    sources or a big pool of data. Let''s use the example of news aggregating websites.
    They create clusters of similar types of news making it easier for the user to
    go through the interesting sections.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息检索**：将信息划分为特定聚类是从众多来源或大量数据池中搜索和检索信息的重要步骤。以新闻聚合网站为例，它们创建了相似类型新闻的聚类，使得用户更容易浏览感兴趣的部分。'
- en: These news types can also have sub-classes creating a hierarchical view. For
    example, in the sports news section, we can have Football, Cricket, and Tennis,
    and other sports.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些新闻类型也可以有子类，形成层次结构。例如，在体育新闻部分，我们可以有足球、板球、网球等其他运动。
- en: '**Biology**: Clustering finds a great use in biology. After years of research,
    biologists have classified most of the living things in hierarchies. Using the
    features of these classes, unknowns can be classified. Also, the existing data
    can be used to find similarities and interesting patterns.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生物学**：聚类在生物学中有很大的应用价值。经过多年的研究，生物学家已将大多数生物分类为不同的层级。利用这些类别的特征，可以对未知生物进行分类。同时，现有的数据可以用来寻找相似性和有趣的模式。'
- en: '**Marketing**: Companies use customer and sales data to create clusters of
    similar users or segments where targeted promotions/campaigns can be run to get
    the maximum return on  investment.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**营销**：公司通过使用客户和销售数据，创建相似用户或细分群体，以便进行有针对性的促销/活动，从而获得最大的投资回报。'
- en: '**Weather**: Cluster analysis is used extensively in climate and weather analysis.
    Weather stations generate huge amount of data. Clustering is used to generate
    insights on this data and find out the patterns and important information.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**天气**：聚类分析在气候和天气分析中得到了广泛应用。气象站生成大量数据。聚类用于从这些数据中提取见解，发现模式和重要信息。'
- en: How are clusters formed?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类是如何形成的？
- en: 'There are many methods to form clusters. Let''s discuss some basic approaches
    of cluster creation:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 形成聚类的方法有很多。我们来讨论一些基本的聚类创建方法：
- en: Start with grouping the data objects. This grouping should only happen based
    on the data that is describing the objects.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从对数据对象进行分组开始。这种分组应仅基于描述对象的数据进行。
- en: Similar objects are grouped together. They may show a relationship with each
    other.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似的对象被聚集在一起。它们之间可能存在某种关系。
- en: Dissimilar objects are kept in other clusters.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不相似的对象被保留在其他聚类中。
- en: '![How are clusters formed?](img/image_07_001.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![聚类是如何形成的？](img/image_07_001.jpg)'
- en: The preceding plot clearly shows us some distinct clusters that are formed when
    there are more similarities between different data objects in a cluster and dissimilarities
    with data objects from other clusters.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述图清晰地展示了当不同数据对象在聚类内相似度较高，而与其他聚类的对象不相似时，形成的一些不同聚类。
- en: '![How are clusters formed?](img/image_07_002.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![聚类是如何形成的？](img/image_07_002.jpg)'
- en: But in this particular representation of the data points, we can see that there
    are no definite clusters that can be formed. This is when there is some similarity
    between the data objects of the different clusters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在这种数据点的表示中，我们可以看到没有可以形成的确定性聚类。这是因为不同聚类的数据对象之间存在一些相似性。
- en: Types of clustering
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类的类型
- en: 'There are different types of clustering mechanisms depending on the various
    factors:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类机制有多种类型，取决于不同的因素：
- en: Nested or un-nested—hierarchical or partitional
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌套或非嵌套——层次或划分
- en: Overlapping, exclusive, and fuzzy
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重叠、排他和模糊
- en: Partial versus complete
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部分与完全
- en: Hierarchical clustering
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层次聚类
- en: If the clusters do not form subsets, then the cluster is said to be un-nested.
    Therefore, partitional clustering is defined as the creation of well-defined clusters,
    which do not overlap with each other. In such a cluster, the data points are located
    in one and only one cluster alone.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果聚类没有形成子集，则该聚类被称为非嵌套的。因此，划分聚类被定义为创建明确定义的聚类，这些聚类彼此之间不重叠。在这种聚类中，数据点仅位于一个聚类中。
- en: If the clusters have subclusters within them, then it is called hierarchical
    clustering.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果聚类中有子聚类，则称为层次聚类。
- en: '![Hierarchical clustering](img/B05321_07_03.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![层次聚类](img/B05321_07_03.jpg)'
- en: The preceding diagram represents a hierarchical cluster. Hierarchical clusters
    are the clusters organized as a tree.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表示一个层次聚类。层次聚类是按树形结构组织的聚类。
- en: Here, each cluster has its own child cluster. Each node can also be thought
    of as an individual system, having its own clusters obtained through partitioning.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个聚类都有自己的子聚类。每个节点也可以被看作是一个独立的系统，具有通过划分得到的自己的聚类。
- en: Overlapping, exclusive, and fuzzy clustering
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重叠、独占和模糊聚类
- en: 'The techniques which leads to creation of different types of clusters can be
    categorized into three approaches:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 导致不同类型聚类创建的技术可以分为三种方法：
- en: '**Exclusive clusters**: In the section, How are clusters formed? We saw two
    images representing two different types of clusters. In the first image, we saw
    that the clusters are well defined and have a good separation between them. These
    are called exclusive clusters. In these clusters, the data points have a definite
    dissimilarity from the data points of the other clusters.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独占聚类**：在“聚类如何形成？”这一节中，我们看到两张图分别表示了两种不同类型的聚类。在第一张图中，聚类被清晰地定义，并且它们之间有很好的分离。这些叫做独占聚类。在这些聚类中，数据点与其他聚类的数据点有明显的异质性。'
- en: '**Overlapping clusters**: In the second image, we saw that there is no such
    definite boundary to separate two clusters. Here some of the data points can exist
    in any of the clusters. This situation comes when there is no such feature to
    distinguish the data point into any of the clusters.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重叠聚类**：在第二张图中，我们看到没有明确的边界来分隔两个聚类。在这里，一些数据点可以出现在任意一个聚类中。这种情况出现在没有明显特征将数据点区分到某一聚类时。'
- en: '**Fuzzy clustering**: Fuzzy clustering is a unique concept. Here the data point
    belongs to each and every cluster and its relationship is defined by the weight
    which is between 1 (belongs exclusively) to 0 (doesn''t belong). Therefore, clusters
    are considered as fuzzy sets. By the probabilistic rule, a constraint is added
    that the sum of weights of all the data points should be equal to 1.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模糊聚类**：模糊聚类是一个独特的概念。在这里，数据点属于每一个聚类，并且其关系通过权重来定义，权重范围从 1（完全属于）到 0（不属于）。因此，聚类被视为模糊集合。根据概率规则，添加了一个约束条件，即所有数据点的权重之和应该等于
    1。'
- en: Fuzzy clustering is also known as probabilistic clustering. Generally, to have
    a definite relation, the data point is associated with the cluster for whom it
    has the highest membership weight.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊聚类也称为概率聚类。通常，为了具有明确的关系，数据点与其具有最高隶属度的聚类关联。
- en: Differences between partial versus complete clustering
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部分聚类与完全聚类的区别
- en: In complete clustering, all the data points are assigned to a cluster because
    they accurately represent features of the cluster. These types of clusters are
    called complete clusters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全聚类中，所有数据点都会被分配到一个聚类中，因为它们准确地表示了聚类的特征。这些类型的聚类称为完全聚类。
- en: There can be some data points that may not belong to any of the clusters. This
    is when these data points represent noise or are outliers to the cluster. Such
    data points are not taken in any of the clusters, and this is called partial clustering.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有一些数据点不属于任何聚类。这是因为这些数据点代表噪声或是聚类的异常值。此类数据点不会被包含在任何聚类中，这种情况称为部分聚类。
- en: K-means clustering
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means 聚类
- en: K-means is the most popular of the clustering techniques because of its ease
    of use and implementation. It also has a partner by the name of K-medoid. These
    partitioning methods create level-one partitioning of the dataset. Let's discuss
    K-means in detail.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 是最流行的聚类技术，因为它易于使用和实现。它也有一个名为 K-medoid 的伴侣。这些划分方法创建了数据集的一级划分。让我们详细讨论
    K-means。
- en: K-means algorithm
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means 算法
- en: K-means start with a prototype. It takes centroids of data points from the dataset.
    This technique is used for the objects lying in the n-dimensional space.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 从原型开始。它从数据集中获取数据点的质心。这种技术用于位于 n 维空间中的对象。
- en: The technique involves choosing the K number of centroids. This K is specified
    by the user and is chosen considering various factors. It defines how many clusters
    we want. So, choosing a higher or lower than the required K can lead to undesired
    results.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术涉及选择 K 个质心。这个 K 是由用户指定的，根据各种因素进行选择。它定义了我们希望有多少个聚类。因此，选择比所需的 K 高或低可能会导致不理想的结果。
- en: Now going forward, each point is assigned to its nearest centroid. As many points
    get associated with a specific centroid, a cluster is formed. The centroid can
    get updated depending on the points that are part of the current cluster.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，继续进行，每个点被分配到其最近的质心。随着许多点与特定质心关联，形成一个聚类。质心可以根据当前聚类中所包含的点进行更新。
- en: This process is done repeatedly until the centroid gets constant.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会反复进行，直到质心保持不变。
- en: Algorithm of K-means
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-means 算法
- en: 'Understanding K-means algorithm will give us a better view of how to approach
    the problem. Let''s understand step by step the K-means algorithm:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 K-means 算法将帮助我们更好地理解如何解决这个问题。让我们一步步理解 K-means 算法：
- en: As per the defined K, select the number of centroids.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据定义的 K，选择质心的数量。
- en: Assign data points to the nearest centroid. This step will form the clusters.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据点分配给最近的质心。此步骤将形成聚类。
- en: Compute the centroid of the cluster again.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次计算聚类的质心。
- en: Repeat steps 2 and 3 until the centroid gets constant.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和3，直到质心保持不变。
- en: In the first step, we use the mean as the centroid.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们使用均值作为质心。
- en: Step 4 says to repeat the earlier steps of the algorithm. This can sometimes
    lead to a large number of iterations with very little change. So, we generally
    use repeat steps 2 and 3 only if the newer computed centroid has more than 1%
    change.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步要求重复之前的算法步骤。这有时会导致大量迭代且变化很小。因此，我们通常只有在新计算的质心变化超过1%时，才会重复步骤2和3。
- en: Associating the data points with the closest centroid
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据点与最接近的质心关联
- en: How do we measure the distance between the computed centroid and the data point?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何衡量计算出的质心与数据点之间的距离？
- en: We use the Euclidean (L2) distance as the measure, and we assume that the data
    points are in the Euclidean space. We can also use different proximity measures
    if required, for example, Manhattan (L1) can also be used for the Euclidean space.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用欧几里得（L2）距离作为度量，并假设数据点位于欧几里得空间中。如果需要，我们也可以使用不同的接近度度量，例如，曼哈顿（L1）距离也可以用于欧几里得空间。
- en: As the algorithm processes similarities with the different data points, it is
    good to have only the required set of features of the data points. With higher
    dimensional data, the computation increases drastically as it has to compute for
    each and every dimension iteratively.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当算法处理不同数据点之间的相似性时，最好只使用数据点的必要特征集。对于高维数据，计算量急剧增加，因为必须对每个维度进行迭代计算。
- en: 'There are some choices of the distance measure that can be used:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些可以使用的距离度量选择：
- en: '**Manhattan (L1)**: This takes a median as the centroid. It works on the function
    to minimize the sum of the L1 distance of an object from the centroid of its cluster.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**曼哈顿（L1）**：这将中位数作为质心。它作用于该函数，最小化一个对象与聚类质心之间的 L1 距离之和。'
- en: '**Squared Euclidean (L2^2)**: This takes a mean as the centroid. It works on
    the function to minimize the sum of the squared of the L2 distance of an object
    from the centroid of the cluster.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平方欧几里得（L2^2）**：这将均值作为质心。它作用于该函数，最小化一个对象与聚类质心之间的 L2 距离平方和。'
- en: '**Cosine**: This takes a mean as the centroid. It works on the function to
    maximize the sum of the cosine similarity of an object from the centroid of the
    cluster.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**余弦**：这将均值作为质心。它作用于该函数，最大化一个对象与聚类质心之间的余弦相似度之和。'
- en: '**Bregman divergence**: This takes a mean as the centroid. It minimizes the
    sum of the Bregman divergence of an object from the centroid of the cluster.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bregman 散度**：这将均值作为质心。它最小化一个对象与聚类质心之间的 Bregman 散度之和。'
- en: How to choose the initial centroids?
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何选择初始质心？
- en: This is a very important step in the K-means algorithm. We start off by choosing
    the initial centroids randomly. This generally results in very poor clusters.
    Even if these centroids are well distributed, we do not get even close to the
    desired clusters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 K-means 算法中非常重要的一步。我们首先随机选择初始质心。这通常会导致非常差的聚类结果。即使这些质心分布良好，我们也无法接近理想的聚类。
- en: There is a technique to address this problem—multiple runs with different initial
    centroids. After this, the set of the clusters is chosen, which has the minimum
    **Sum of Squares error** (**SSE**). This may not always work well and may not
    always be feasible because of the size of the dataset and the computation power
    required.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种解决此问题的技术——使用不同初始质心的多次运行。之后，选择具有最小**平方和误差**（**SSE**）的聚类集。由于数据集的大小和所需计算能力，这种方法可能并不总是有效或可行。
- en: 'In repeating the random initializing, the centroid may not be able to overcome
    the problem, but we have other techniques that we can use:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在重复随机初始化时，质心可能无法克服问题，但我们可以使用其他技术：
- en: 'Using hierarchical clustering, we can start with taking some sample points
    and use hierarchical clustering to make a cluster. Now we can take out the K number
    of clusters from this clustering and use the centroids of these clusters as the
    initial centroids. There are some constraints to this approach:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用层次聚类，我们可以从一些样本点开始，使用层次聚类来形成一个聚类。现在我们可以从这个聚类中取出K个聚类，并使用这些聚类的质心作为初始质心。这种方法有一些约束：
- en: The sample data should not be large (expensive computation).
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本数据不应该很大（计算昂贵）。
- en: With the number of the desired clusters, K should be small.
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所需的聚类数，K应该较小。
- en: 'Another technique is to get the centroid of all the points. From this centroid,
    we find the point that is separated at maximum. We follow this process to get
    the maximum distant centroids, which are also randomly chosen. But there are some
    issues with this approach:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种技术是获取所有点的质心。从这个质心中，我们找到分离最大的点。我们按照这个过程获取最大距离的质心，这些质心也是随机选择的。但是这种方法存在一些问题：
- en: It is computationally intensive to find out the farthest point.
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出最远点是计算密集型的。
- en: This approach sometimes produces undesirable results when there are outliers
    in the dataset. Therefore, we may not get the dense regions as required.
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据集中存在异常值时，这种方法有时会产生不良结果。因此，我们可能无法获得所需的密集区域。
- en: Time-space complexity of K-means algorithms
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-means算法的时间空间复杂度
- en: K-means doesn't require that much space as we only need to store the data points
    and the centroids.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: K-means并不需要那么多的空间，因为我们只需要存储数据点和质心。
- en: 'The storage requirement of a K-means algorithm *O((m+K)n)*, where:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: K-means算法的存储需求 *O((m+K)n)*，其中：
- en: '*m* is number of points'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m* 是点数'
- en: '*n* is number of attributes'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n* 是属性数'
- en: The time requirements of the K-means algorithm may vary, but generally they
    too are modest. The time increases linearly with the number of data points.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: K-means算法的时间要求可能会有所变化，但通常也是适度的。随着数据点数量的增加，时间会线性增加。
- en: 'Time requirements of a K-means algorithm: *O(I*K*m*n)*, where:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: K-means算法的时间要求：*O(I*K*m*n)*，其中：
- en: '*I* is number of iterations required to converge to a centroid'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*I* 是收敛到一个质心所需的迭代次数'
- en: K-means works best if the number of clusters required is significantly smaller
    than the number of data points on which the K-means is directly proportional to.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所需的聚类数远远小于K-means所基于的数据点数，则K-means效果最佳。
- en: Issues with K-means
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means的问题
- en: There are some issues associated with the basic K-means clustering algorithm.
    Let's discuss these issues in detail.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的K-means聚类算法存在一些问题。让我们详细讨论这些问题。
- en: Empty clusters in K-means
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-means中的空聚类
- en: 'There can be a situation where we get empty clusters. This is when there are
    no points allocated to a particular given cluster during the phase where points
    are assigned. This can be resolved as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会出现空聚类的情况。当在分配点的阶段没有分配到特定给定聚类的点时，就会发生这种情况。可以按以下方式解决：
- en: We choose a different centroid to the current choice. If it is not done, the
    squared error will be much larger than the threshold.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择一个不同的质心作为当前选择。如果不这样做，平方误差将远远大于阈值。
- en: To choose a different centroid, we follow the same approach of finding the farthest
    such point from the current centroid. This generally eliminates the point that
    was contributing to the squared error.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要选择一个不同的质心，我们遵循找到当前质心的最远点的相同方法。这通常会消除导致平方误差的点。
- en: If we are getting multiple empty clusters, then we have to repeat this process
    again several times.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们得到多个空聚类，则必须再次重复此过程。
- en: Outliers in the dataset
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集中的异常值
- en: When we are working with the squared error, then the outliers can be the decisive
    factor and can influence the clusters that are formed. This means that when there
    are outliers in the dataset, then we may not achieve the desired cluster or the
    cluster that truly represents the grouped data points may not similar features.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用平方误差时，异常值可能是决定性因素，可以影响形成的聚类。这意味着当数据集中存在异常值时，我们可能无法达到期望的聚类，或者真正代表分组数据点的聚类可能没有相似的特征。
- en: This also leads to a higher sum of squared errors. Therefore, a common practice
    is to remove the outliers before applying the clustering algorithm.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这也导致较高的平方误差和。因此，在应用聚类算法之前，通常会先移除异常值。
- en: There can also be some situations where we may not want to remove the outliers.
    Some of the points, such as unusual activity on the Web, excessive credit, and
    so on, are interesting and important to the business.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能出现一些我们不希望移除离群点的情况。例如，一些点，如 Web 上的异常活动、过度的信用等，对于业务来说是有趣和重要的。
- en: Different types of cluster
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同类型的聚类
- en: 'There are limitations with K-means. The most common limitation of K-means is
    that it faces difficulty in identifying the natural clusters. By natural clusters
    we mean:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 有一些限制。K-means 最常见的限制是它在识别自然聚类时遇到困难。所谓自然聚类是指：
- en: Non-spherical/circular in shape
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非球形/圆形的形状
- en: Clusters of different sizes
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同大小的聚类
- en: Clusters of different densities
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同密度的聚类
- en: Tip
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: K-means can fail if there are few denser clusters and a not so dense cluster.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在几个密集的聚类和一个不太密集的聚类，K-means 可能会失败。
- en: 'Here is a diagram of clusters of different sizes:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是不同大小的聚类示意图：
- en: '![Different types of cluster](img/image_07_004.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![不同类型的聚类](img/image_07_004.jpg)'
- en: The preceding figure has two images. In the first image, we have original points
    and in the second image, we have three K-means clusters. We can see that these
    are not accurate. This happens when clusters are of different sizes.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像有两张。第一张是原始点，第二张是三个 K-means 聚类。我们可以看到这些聚类并不准确。这种情况发生在聚类的大小不一样时。
- en: 'Here is a diagram of clusters of different densities:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是不同密度的聚类示意图：
- en: '![Different types of cluster](img/image_07_005.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![不同类型的聚类](img/image_07_005.jpg)'
- en: The preceding figure has two images. In the first image, we have original points
    and in the second image, we have three K-means clusters. The clusters are of different
    densities.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图有两张图片。第一张是原始点，第二张是三个 K-means 聚类。聚类具有不同的密度。
- en: 'Here is a diagram of non-globular clusters:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是非球形聚类的示意图：
- en: '![Different types of cluster](img/image_07_006.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![不同类型的聚类](img/image_07_006.jpg)'
- en: The preceding figure has two images. In the first image, we have original points
    and in the second image, we have two K-means clusters. The clusters are non-circular
    or non-globular in nature and the K-means algorithm was not able to detect them
    properly.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像有两张。第一张是原始点，第二张是两个 K-means 聚类。这些聚类是非圆形或非球形的，K-means 算法未能正确检测到它们。
- en: K-means – strengths and weaknesses
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-means —— 优势与劣势
- en: 'There are many strengths and a few weaknesses of K-means. Let''s discuss the
    strengths first:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 有许多优势，也有一些劣势。我们先来讨论其优势：
- en: K-means can be used for various types of data.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means 可以用于各种类型的数据。
- en: It is simple to understand and implement.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它易于理解和实现。
- en: It is efficient, even with repeated and multiple iterations.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它即使在重复和多次迭代的情况下也很高效。
- en: Bisecting K-means, a variant of simple K-means is more efficient. We will discuss
    that later in more detail.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二分 K-means 是简单 K-means 的一种变体，更加高效。我们将在后面详细讨论这一点。
- en: 'Some weaknesses or drawbacks of K-means clustering include:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 聚类的一些劣势或缺点包括：
- en: It is not suitable for every type of data.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它并不适用于所有类型的数据。
- en: As seen in the previous examples, it doesn't work well for clusters of different
    densities, sizes, or non-globular clusters.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前面示例所见，它在处理不同密度、大小或非球形聚类时效果不好。
- en: There are issues when there are outliers in the dataset.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据集中存在离群点时，会出现问题。
- en: K-means has a big constraint in that it makes the cluster by computing the center.
    Therefore, our data should be such that can have a "center".
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means 存在一个大限制，它通过计算中心来形成聚类。因此，我们的数据应该能够有一个“中心”。
- en: Bisecting K-means algorithm
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二分 K-means 算法
- en: Bisecting K-means is an extension of the simple K-means algorithm. Here we find
    out the K clusters by splitting the set of all points into two clusters. Then
    we take one of these clusters and split it again. The process continues until
    the K clusters are formed.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 二分 K-means 是简单 K-means 算法的扩展。在这里，我们通过将所有点集合分成两个聚类来找出 K 个聚类。然后我们选择其中一个聚类，再次进行分割。这个过程会持续进行，直到形成
    K 个聚类。
- en: 'The algorithm of bisecting K-means is:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 二分 K-means 的算法是：
- en: First we need to initialize the list of clusters that will have the cluster
    consisting of all the data points.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要初始化聚类列表，列表中将包含由所有数据点组成的一个聚类。
- en: 'Repeat the following:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复以下步骤：
- en: Now we remove one cluster from the list of the clusters
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们从聚类列表中移除一个聚类
- en: We now do trials of bisecting the cluster multiple times
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来我们多次尝试对聚类进行二分处理。
- en: For n=1 to the number of trials in the previous step
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 n=1 到前一步骤中试验的次数
- en: 'The cluster is bisected using K-means:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该簇使用 K-means 被二分：
- en: Two clusters are selected from the result that has the lowest total sum of squared
    errors
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从结果中选择两个总平方误差最小的簇
- en: These two clusters are added to the list of the clusters
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个簇被添加到簇的列表中
- en: The previous steps are performed until we have the K clusters in the list.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之前的步骤会一直执行，直到我们在列表中得到 K 个簇。
- en: 'There are several ways we can split a cluster:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过几种方式来拆分一个簇：
- en: Largest cluster
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大簇
- en: Cluster which has the largest sum of squared errors
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总平方误差最大的簇
- en: Both
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两者
- en: 'We will use the iris dataset from the RDatasets for this example:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这个例子中使用来自 RDatasets 的鸢尾花数据集：
- en: '![Bisecting K-means algorithm](img/image_07_007.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![二分K均值算法](img/image_07_007.jpg)'
- en: This is a simple example of the famous iris dataset. We are clustering the data
    points using `PetalLength` and `PetalWidth`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这是著名的鸢尾花数据集的一个简单例子。我们使用 `PetalLength` 和 `PetalWidth` 来对数据点进行聚类。
- en: 'The result is as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Bisecting K-means algorithm](img/image_07_008.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![二分K均值算法](img/image_07_008.jpg)'
- en: Getting deep into hierarchical clustering
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解层次聚类
- en: 'This is the second most used clustering technique after K-means. Let''s take
    the same example again:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这是仅次于 K-means 的第二大常用聚类技术。我们再用相同的例子来说明：
- en: '![Getting deep into hierarchical clustering](img/B05321_07_03.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![深入了解层次聚类](img/B05321_07_03.jpg)'
- en: Here, the top most root represents all the data points or one cluster. Now we
    have three sub-clusters represented by nodes. All these three clusters have two
    sub-clusters. And these sub-clusters have further sub-clusters in them. These
    sub-clusters help in finding the clusters that are pure - that means, those which
    share most of the features.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，最上面的根节点表示所有数据点或一个簇。现在我们有三个子簇，用节点表示。所有这三个簇都有两个子簇，而这些子簇中还有进一步的子簇。这些子簇有助于找到纯粹的簇——也就是说，具有大多数共同特征的簇。
- en: 'There are two ways with which we can approach hierarchical clustering:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用两种方法来进行层次聚类：
- en: '**Agglomerative**: This is based on the concept of cluster proximity. We initially
    start with treating each point as the individual cluster and then step by step
    we merge the closest pairs.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合法**：这是基于簇之间的接近度概念。我们一开始将每个点视为一个独立的簇，然后逐步合并最接近的簇对。'
- en: '**Divisive**: Here we start with one cluster containing all the data points
    and then we start splitting it until clusters having individual points are left.
    In this case, we decide how the splitting should be done.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分割法**：这里我们从一个包含所有数据点的簇开始，然后开始拆分，直到每个簇只包含一个数据点为止。在这种情况下，我们决定如何进行拆分。'
- en: Hierarchical clusters are represented as a tree-like diagram, also known as
    a dendogram. This is used to represent a cluster-subcluster relationship and how
    the clusters are merged or split (agglomerative or divisive).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类以树形图表示，也称为树状图（dendogram）。这用于表示簇与子簇之间的关系，以及簇是如何合并或拆分的（聚合法或分割法）。
- en: Agglomerative hierarchical clustering
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合层次聚类
- en: This is the bottom-up approach of hierarchical clustering. Here, each observation
    is treated as an individual cluster. Pairs of these clusters are merged together
    on the basis of similarity and we move up.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是层次聚类的自底向上方法。在这里，每个观察点被视为一个独立的簇。这些簇对会根据相似性进行合并，然后我们向上移动。
- en: These clusters are merged together based on the smallest distance. When these
    two clusters are merged, they are treated as a new cluster. These steps are repeated
    when there is one single cluster left in the pool of data points.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这些簇根据最小的距离进行合并。当这两个簇合并时，它们会被当作一个新簇。这个步骤会在数据点池中只剩一个簇时重复。
- en: 'The algorithm of agglomerative hierarchical clustering is:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合层次聚类的算法是：
- en: Firstly, the proximity matrix is computed.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，计算邻接矩阵。
- en: The two closest clusters are merged.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最接近的两个簇会被合并。
- en: The proximity matrix created in the first step is updated after the merging
    of the two clusters.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一步中创建的邻接矩阵会在合并两个簇后进行更新。
- en: Step 2 and step 3 are repeated until there is only one cluster remaining.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第 2 步和第 3 步会重复执行，直到只剩下一个簇。
- en: How proximity is computed
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何计算接近度
- en: Step 3 in the previous algorithm is a very important step. It is the proximity
    measure between the two clusters.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 前述算法中的第 3 步是一个非常重要的步骤。它是衡量两个簇之间接近度的标准。
- en: 'There are various ways to define this:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以定义这一点：
- en: '**MIN**: The two closest points of different clusters define the proximity
    of these clusters. This is the shortest distance.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MIN**：不同聚类中最接近的两个点定义了这些聚类的接近度。这是最短的距离。'
- en: '**MAX**: Opposite to MIN, MAX takes the farthest point in the clusters and
    computes the proximity between these two which is taken as the proximity of these
    clusters.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MAX**：与MIN相反，MAX取聚类中的最远点，计算这两个点之间的接近度，并将其作为这两个聚类的接近度。'
- en: '**Average**: One other approach is to take the average of all the data points
    of the different clusters and compute the proximity according to these points.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均法**：另一种方法是取不同聚类的所有数据点的平均值，并根据这些点计算接近度。'
- en: '![How proximity is computed](img/B05321_07_10.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![如何计算接近度](img/B05321_07_10.jpg)'
- en: The preceding diagram depicts the proximity measure using MIN.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示展示了使用MIN计算的接近度度量。
- en: '![How proximity is computed](img/B05321_07_11.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![如何计算接近度](img/B05321_07_11.jpg)'
- en: The preceding diagram depicts the proximity measure using MAX.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示展示了使用MAX计算的接近度度量。
- en: '![How proximity is computed](img/B05321_07_12.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![如何计算接近度](img/B05321_07_12.jpg)'
- en: The preceding diagram depicts the proximity measure using Average.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示展示了使用平均法计算的接近度度量。
- en: 'These methods are also known as:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法也被称为：
- en: '**Single Linkage**: MIN'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单链接法**：MIN'
- en: '**Complete Linkage**: MAX'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全链接法**：MAX'
- en: '**Average Linkage**: Average'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均链接法**：平均'
- en: There is also another method known as the centroid method.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种方法，称为质心法。
- en: In the centroid method, the proximity distance is computed using two mean vectors
    of the clusters. At every stage, the two clusters are combined depending on which
    has the smallest centroid distance.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在质心法中，接近度是通过聚类的两个均值向量来计算的。在每个阶段，两个聚类将根据哪一个具有最小的质心距离来合并。
- en: 'Let''s take the following example:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看以下例子：
- en: '![How proximity is computed](img/B05321_07_13.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![如何计算接近度](img/B05321_07_13.jpg)'
- en: 'The preceding diagram shows seven points in an x-y plane. If we start to do
    agglomerative hierarchical clustering, the process would be as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示展示了一个x-y平面中的七个点。如果我们开始进行凝聚型层次聚类，过程将如下：
- en: '{1},{2},{3},{4},{5},{6},{7}.'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '{1},{2},{3},{4},{5},{6},{7}。'
- en: '{1},{2,3},{4},{5},{6},{7}.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '{1},{2,3},{4},{5},{6},{7}。'
- en: '{1,7},{2,3},{4},{5},{6},{7}.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '{1,7},{2,3},{4},{5},{6},{7}。'
- en: '{1,7},{2,3},{4,5},{6}.'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '{1,7},{2,3},{4,5},{6}。'
- en: '{1,7},{2,3,6},{4,5}.'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '{1,7},{2,3,6},{4,5}。'
- en: '{1,7},{2,3,4,5,6}.'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '{1,7},{2,3,4,5,6}。'
- en: '{1,2,3,4,5,6,7}.'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '{1,2,3,4,5,6,7}。'
- en: This was broken down into seven steps to make the complete whole cluster.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程被分解为七个步骤，以形成完整的聚类。
- en: 'This can also be shown by the following dendogram:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以通过以下树状图表示：
- en: '![How proximity is computed](img/image_07_014.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![如何计算接近度](img/image_07_014.jpg)'
- en: This represents the previous seven steps of the agglomerative hierarchical clustering.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示了前述的七个步骤，属于凝聚型层次聚类。
- en: Strengths and weaknesses of hierarchical clustering
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层次聚类的优缺点
- en: 'The hierarchical clustering discussed earlier is sometimes more or less suited
    to a given problem. We will be able to comprehend this by understanding strengths
    and weaknesses of the hierarchical clustering:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 之前讨论的层次聚类方法有时更适合某些问题。我们可以通过理解层次聚类的优缺点来理解这一点：
- en: Agglomerative clustering lacks a global objective function. Such algorithms
    get the benefit of not having the local minima and no issues in choosing the initial
    points.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凝聚型聚类缺乏全局目标函数。这类算法的好处是没有局部最小值，且在选择初始点时没有问题。
- en: Agglomerative clustering handles clusters of different sizes well.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凝聚型聚类可以很好地处理不同大小的聚类。
- en: It is considered that agglomerative clustering produces better quality clusters.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认为凝聚型聚类可以产生更好的聚类质量。
- en: Agglomerative clustering is generally computationally expensive and doesn't
    work well with high-dimensional data.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凝聚型聚类通常计算量大，并且在处理高维数据时效果不佳。
- en: Understanding the DBSCAN technique
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解DBSCAN技术
- en: '**DBSCAN** refers to **Density-based Spatial Clustering of Applications with
    Noise**. It is a data clustering algorithm that uses density-based expansion of
    the seed (starting) points to find the clusters.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**DBSCAN**是**基于密度的空间聚类应用噪声**（Density-based Spatial Clustering of Applications
    with Noise）的缩写。它是一种数据聚类算法，通过基于密度的扩展种子（起始）点来找到聚类。'
- en: It locates the regions of high density and separates them from the others using
    the low densities between them.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 它定位高密度区域，并通过它们之间的低密度区域将它们与其他区域分离。
- en: So, what is density?
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么，什么是密度呢？
- en: In the center-based approach, the density is computed at a particular point
    in the dataset by using the number of points in the specified radius. This is
    easy to implement and the density of the point is dependent on the specified radius.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于中心的方法中，密度是通过计算数据集中特定点在指定半径内的点数来得出的。这种方法实现起来很简单，且点的密度依赖于指定的半径。
- en: For example, a large radius corresponds to the density at point *m*, where m
    is the number of data points inside the radius. If the radius is small, then the
    density can be 1 because only one point exists.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个较大的半径对应于点*m*处的密度，其中m是半径内的数据点数量。如果半径较小，则密度可以为1，因为只有一个点存在。
- en: How are points classified using center-based density
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用基于中心的密度来分类点
- en: '**Core point**: The points that lie inside the density-based cluster are the
    core points. These lie in the interior of the dense region.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心点**：位于基于密度的聚类内部的点就是核心点。这些点位于密集区域的内部。'
- en: '**Border point**: These points lie within the cluster, but are not the core
    points. They lie in the neighborhood of the core points. These lie on the boundary
    or edge of the dense region.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边界点**：这些点位于聚类内部，但不是核心点。它们位于核心点的邻域内，位于密集区域的边界或边缘。'
- en: '**Noise points**: The points that are not the core points or the border points
    are the noise points.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声点**：那些既不是核心点也不是边界点的点就是噪声点。'
- en: DBSCAN algorithm
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBSCAN算法
- en: Points very close to each other are put together in the same cluster. Points
    lying close to these points are also put together. Points that are very far (noise
    points) are discarded.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 非常接近的点被放在同一个聚类中。靠近这些点的点也会被放在一起。非常远的点（噪声点）会被丢弃。
- en: 'The algorithm of the DBSCAN is given by:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN算法如下：
- en: Points are labeled as core, border, or noise points.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点被标记为核心点、边界点或噪声点。
- en: Noise points are eliminated.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 噪声点会被剔除。
- en: An edge is formed between the core points using the special radius.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 核心点之间通过特殊半径形成边缘。
- en: These core points are made into a cluster.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些核心点被组成一个聚类。
- en: Border points associated to these core points are assigned to these clusters.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与这些核心点相关的边界点被分配到这些聚类中。
- en: Strengths and weaknesses of the DBSCAN algorithm
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBSCAN算法的优缺点
- en: The hierarchical clustering discussed earlier is sometimes more or less suited
    to a given problem. We will be able to comprehend this by understanding strengths
    and weaknesses of the hierarchical clustering.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，层次聚类有时更或少适合某些特定问题。我们可以通过理解层次聚类的优缺点来更好地理解这一点。
- en: DBSCAN can handle clusters of different shapes and sizes. It is able to do this
    because it creates the definition of the cluster using the density.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN能够处理不同形状和大小的聚类。它之所以能够做到这一点，是因为它通过密度定义了聚类。
- en: It is resistant to noise. It is able to perform better than K-means in terms
    of finding more clusters.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对噪声具有较强的抗干扰性。在找到更多聚类方面，它比K-means表现得更好。
- en: DBSCAN faces issues with datasets that have varied densities.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN在面对具有不同密度的数据集时会遇到问题。
- en: Also, it has issues dealing with high-dimensional data because it becomes difficult
    to find densities in such data.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，它在处理高维数据时也有问题，因为在这种数据中找到密度变得困难。
- en: It's computationally intensive when computing nearest neighbors.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算最近邻时，它的计算开销较大。
- en: Cluster validation
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类验证
- en: 'Cluster validation is important as it tells us that the generated clusters
    are relevant or not. Important points to consider when dealing with the cluster
    validation include:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类验证很重要，因为它告诉我们生成的聚类是否相关。在进行聚类验证时，需要考虑的重要点包括：
- en: It has the ability to distinguish whether non-random structure in the data actually
    exists or not
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有能力区分数据中是否真正存在非随机结构。
- en: It has the ability to determine the actual number of clusters
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它能够确定实际的聚类数量。
- en: It has the ability to evaluate how the data is fit to the cluster
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它具有评估数据是否适合聚类的能力。
- en: It should be able to compare two sets of clusters to find out which cluster
    is better
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该能够比较两组聚类，找出哪一个聚类更好。
- en: Example
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: We will be using `ScikitLearn.jl` in our example of agglomerative hierarchical
    clustering and DBSCAN.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在层次聚类和DBSCAN的示例中使用`ScikitLearn.jl`。
- en: As discussed previously, `ScikitLearn.jl` aims to provide a similar library
    such as the actual scikit-learn for Python.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`ScikitLearn.jl`旨在提供一个类似于Python中实际scikit-learn的库。
- en: 'We will first add the required packages to our environment:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要将所需的包添加到环境中：
- en: '[PRE0]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This also requires us to have the scikit-learn in our Python environment. If
    it is not already installed, we can install it using:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这也要求我们在Python环境中安装scikit-learn。如果尚未安装，我们可以使用以下命令进行安装：
- en: '[PRE1]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After this, we can start with our example. We will try out the different clustering
    algorithms available in `ScikitLearn.jl`. This is provided in the examples of
    `ScikitLearn.jl`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以从我们的示例开始。我们将尝试在`ScikitLearn.jl`中使用不同的聚类算法。这些算法在`ScikitLearn.jl`的示例中有提供：
- en: '[PRE2]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We imported the datasets from the official scikit-learn library and the clustering
    algorithms. As some of these are dependent on the distance measure of neighbors,
    we also imported kNN:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从官方的scikit-learn库导入了数据集和聚类算法。由于其中一些算法依赖于邻居的距离度量，我们还导入了kNN：
- en: '[PRE3]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This particular snippet will generate the required datasets. The dataset generated
    will be of good enough size to test these different algorithms:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将生成所需的数据集。生成的数据集大小足够大，可以用来测试这些不同的算法：
- en: '[PRE4]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We assigned names to these algorithms and colors to fill the image:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这些算法指定了名称，并为图像填充了颜色：
- en: '[PRE5]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we assign how the images will be formed for different algorithms and datasets:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们为不同的算法和数据集指定图像的生成方式：
- en: '![Example](img/B05321_07_15.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/B05321_07_15.jpg)'
- en: 'Here, we are normalizing the dataset to easily select the parameters, and initializing
    the `kneighbors_graph` for the algorithms requiring the distance measure:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对数据集进行标准化，以便更轻松地选择参数，并为需要距离度量的算法初始化`kneighbors_graph`：
- en: '![Example](img/B05321_07_16.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/B05321_07_16.jpg)'
- en: 'Here, we are creating the clustering estimators, which are required by the
    algorithms to behave accordingly to the use case:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在创建聚类估算器，这些估算器是算法根据使用案例行为所必需的：
- en: '![Example](img/B05321_07_17.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/B05321_07_17.jpg)'
- en: The similar estimators for different algorithms.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 不同算法的类似估算器。
- en: 'After this, we use these algorithms on our datasets:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将这些算法应用于我们的数据集：
- en: '[PRE6]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The result obtained is as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的结果如下：
- en: '![Example](img/image_07_018.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/image_07_018.jpg)'
- en: We can see that agglomerative clustering and DBSCAN performed really well in
    the first two datasets
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以看到，凝聚型聚类和DBSCAN在前两个数据集上表现非常好
- en: Agglomerative clustering didn't perform well in the third dataset, whereas DBSCAN
    did
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凝聚型聚类在第三个数据集上表现不佳，而DBSCAN表现良好
- en: Agglomerative clustering and DBSCAN both performed poorly on the fourth dataset
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凝聚型聚类和DBSCAN在第四个数据集上都表现不佳
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about unsupervised learning and how it is different
    from Supervised learning. We discussed various use cases where Unsupervised learning
    is used.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了无监督学习及其与监督学习的区别。我们讨论了无监督学习的各种应用场景。
- en: We went through the different Unsupervised learning algorithms and discussed
    their algorithms, and strengths and weaknesses over each other.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了不同的无监督学习算法，并讨论了它们的算法、优缺点。
- en: We discussed various clustering techniques and how clusters are formed. We learned
    how different the clustering algorithms are from each other and how they are suited
    to particular use cases.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了各种聚类技术以及聚类是如何形成的。我们学习了不同的聚类算法之间的差异，以及它们如何适应特定的应用场景。
- en: We learned about K-means, Hierarchical clustering, and DBSCAN.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解了K-means、层次聚类和DBSCAN。
- en: In the next chapter, we will learn about Ensemble learning.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习集成学习。
- en: References
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://github.com/JuliaLang/julia](https://github.com/JuliaLang/julia)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/JuliaLang/julia](https://github.com/JuliaLang/julia)'
- en: '[https://github.com/JuliaStats/Clustering.jl](https://github.com/JuliaStats/Clustering.jl)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/JuliaStats/Clustering.jl](https://github.com/JuliaStats/Clustering.jl)'
- en: '[http://juliastats.github.io/](http://juliastats.github.io/)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://juliastats.github.io/](http://juliastats.github.io/)'
- en: '[https://github.com/stevengj/PyCall.jl](https://github.com/stevengj/PyCall.jl)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/stevengj/PyCall.jl](https://github.com/stevengj/PyCall.jl)'
- en: '[https://github.com/cstjean/ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/cstjean/ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl)'
