- en: 9\. Applications in Business Use Cases and Conclusion of the Course
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9. 商业用例中的应用和课程总结
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter will allow you to utilize the skills you have learned throughout
    the course of the previous chapters. You will be able to easily handle data wrangling
    tasks for business use cases. Throughout the chapter, you will be testing the
    data wrangling skills you've acquired so far by applying them on interesting business
    problems. These tests, will help you shore up your data wrangling skills, thus
    giving you the confidence to use them to tackle interesting business problems
    in the real world.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使你能够利用在前几章课程中学到的技能。你将能够轻松处理商业用例的数据清洗任务。在整个章节中，你将通过将它们应用于有趣的企业问题来测试你迄今为止获得的数据清洗技能。这些测试将帮助你巩固数据清洗技能，从而让你有信心在现实世界中解决有趣的企业问题。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: In the previous chapter, we learned about databases. It is time to combine our
    knowledge of data wrangling and Python with a realistic scenario. Usually, data
    from one source is often inadequate to perform analysis. Generally, a data wrangler
    has to distinguish between relevant and non-relevant data and combine data from
    different sources.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了数据库。现在是时候将我们对数据清洗和Python的知识与一个现实场景结合起来。通常，来自单一来源的数据往往不足以进行分析。一般来说，数据清洗师必须区分相关数据和非相关数据，并从不同来源组合数据。
- en: The primary job of a data wrangling expert is to pull data from multiple sources,
    format and clean it (impute the data if it is missing), and finally combine it
    in a coherent manner to prepare a dataset for further analysis by data scientists
    or machine learning engineers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗专家的主要工作是从小数据源中提取数据，格式化和清洗它（如果数据缺失，则进行数据插补），最后以连贯的方式将其组合起来，为数据科学家或机器学习工程师准备用于进一步分析的数据集。
- en: In this chapter, we will try to mimic a typical task flow by downloading and
    using two different datasets from reputed web portals. Each dataset contains partial
    data pertaining to the key question that is being asked. Let's examine this more
    closely.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过下载和使用来自知名网站的两个不同数据集来尝试模拟一个典型的任务流程。每个数据集都包含与正在询问的关键问题相关的部分数据。让我们更仔细地考察一下。
- en: Applying Your Knowledge to a Data Wrangling Task
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将你的知识应用于数据清洗任务
- en: 'Suppose you are asked the following question:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你被问到以下问题：
- en: '*In India, did the enrollment in primary/secondary/tertiary education increase
    with the improvement of per capita GDP in the past 15 years? To provide an accurate
    and analyzed result, machine learning and data visualization techniques will be
    used by an expert data scientist*. The actual modeling and analysis will be done
    by a senior data scientist, who will use machine learning and data visualization
    for analysis. As a data wrangling expert, *your job will be to acquire and provide
    a clean dataset that contains educational enrollment and GDP data side by side*.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*在印度，过去15年中，随着人均GDP的提高，小学/中学/高等教育入学率是否有所增加？为了提供准确的分析结果，将由一位专家数据科学家使用机器学习和数据可视化技术*。实际的建模和分析将由一位资深数据科学家完成，他将使用机器学习和数据可视化进行分析。作为数据清洗专家，*你的工作将是获取并提供一个包含教育入学率和GDP数据的干净数据集，这些数据并排排列*。'
- en: Suppose you have a link for a dataset from the United Nations and you can download
    the dataset of education (for all the nations around the world). But this dataset
    has some missing values and, moreover, it does not have any **Gross Domestic Product**
    (**GDP**) information. Someone has also given you another separate CSV file (downloaded
    from the World Bank site) that contains GDP data but in a messy format.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个来自联合国的数据集链接，你可以下载包含全球所有国家教育数据的该数据集。但这个数据集有一些缺失值，而且它没有任何**国内生产总值**（**GDP**）信息。还有人给了你另一个单独的CSV文件（从世界银行网站下载），它包含GDP数据，但格式很混乱。
- en: 'In the following activity, we will examine how to handle these two separate
    sources and clean the data to prepare a simple final dataset with the required
    data and save it to the local drive as a SQL database file:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下活动中，我们将探讨如何处理这两个独立的数据源，并清洗数据以准备一个包含所需数据的简单最终数据集，并将其保存到本地驱动器上的SQL数据库文件中：
- en: '![Figure 9.1: Pictorial representation of merging education and economic data'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.1：教育和经济数据合并的图示'
- en: '](img/B15780_09_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15780_09_01.jpg)'
- en: 'Figure 9.1: Pictorial representation of merging education and economic data'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：教育和经济数据合并的图示
- en: You are encouraged to follow along with the code and results in the notebook
    and try to understand and internalize the nature of the data wrangling flow. You
    are also encouraged to try extracting various data from these files and answer
    your own questions about a nation's socio-economic factors and their inter-relationships.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励你跟随笔记本中的代码和结果，尝试理解并内化数据整理流程的本质。也鼓励你尝试从这些文件中提取各种数据，并回答你关于一个国家社会经济因素及其相互关系的问题。
- en: Note
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Coming up with interesting questions about social, economic, technological,
    and geo-political topics and then answering them using freely available data and
    a little bit of programming knowledge is one of the most fun ways to learn about
    any data science topic. You will get a taste of that process in this chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 提出关于社会、经济、技术和地缘政治主题的有趣问题，然后使用免费数据和一点编程知识来回答这些问题，这是了解任何数据科学主题的最有趣方式之一。你将在本章中体验这个过程。
- en: 'Let''s take a look at the following table, which shows information from a dataset
    of education from the UN data:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下表格，它显示了联合国数据集中教育数据的信息：
- en: '![Figure 9.2: UN data ](img/B15780_09_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2：联合国数据](img/B15780_09_02.jpg)'
- en: 'Figure 9.2: UN data'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：联合国数据
- en: From the preceding table, we can observe that we are missing some data. Let's
    say we decide to impute these data points by performing simple linear interpolation
    between the available data points. We can take a calculator and compute those
    values and manually create a dataset. But being a data wrangler, we will, of course,
    take advantage of Python programming, and use `pandas` imputation methods for
    this task.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的表格中，我们可以观察到我们缺少一些数据。假设我们决定通过在可用数据点之间进行简单的线性插值来填充这些数据点。我们可以拿一个计算器计算这些值并手动创建一个数据集。但作为一个数据整理员，我们当然会利用
    Python 编程，并使用 `pandas` 的插值方法来完成这项任务。
- en: But to do that, we need to create a DataFrame with missing values in it; that
    is, we need to append another DataFrame with missing values to the current DataFrame.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但要这样做，我们需要创建一个包含缺失值的 DataFrame；也就是说，我们需要将另一个包含缺失值的 DataFrame 添加到当前 DataFrame
    中。
- en: 'Activity 9.01: Data Wrangling Task – Fixing UN Data'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 9.01：数据整理任务 – 修复联合国数据
- en: The goal of this activity is to perform data analysis on the UN data to find
    out whether the enrollment in primary, secondary, or tertiary education has increased
    with the improvement of per capita GDP in the past 15 years. For this task, we
    will need to clean or wrangle the two datasets, that is, the education enrollment
    and GDP data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的目标是分析联合国数据，以找出在过去 15 年中，随着人均 GDP 的提高，小学、中学或高等教育入学率是否有所增加。为此任务，我们需要清理或整理两个数据集，即教育入学率和
    GDP 数据。
- en: The UN data is available at [https://packt.live/30ZIS4N](https://packt.live/30ZIS4N).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 联合国数据可在 [https://packt.live/30ZIS4N](https://packt.live/30ZIS4N) 获取。
- en: Note
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you download the CSV file and open it using Excel, then you will see that
    the `Footnotes` column sometimes contains useful notes. We may not want to drop
    it in the beginning. If we are interested in a particular country's data (like
    we are in this task), then it may well turn out that `Footnotes` will be `NaN`,
    that is, blank. In that case, we can drop it at the end. But for some countries
    or regions, it may contain information.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你下载了 CSV 文件并使用 Excel 打开它，那么你将看到 `脚注` 列有时包含有用的注释。我们可能不想一开始就删除它。如果我们对某个国家的数据感兴趣（就像在这个任务中一样），那么
    `脚注` 可能会是 `NaN`，即空白。在这种情况下，我们可以在最后删除它。但对于某些国家或地区，它可能包含信息。
- en: 'These steps will guide you through this activity:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将指导你完成此活动：
- en: 'Download the dataset from the UN data from GitHub from the following link:
    [https://packt.live/2AMoeu6](https://packt.live/2AMoeu6).'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下链接下载联合国数据集：[https://packt.live/2AMoeu6](https://packt.live/2AMoeu6)。
- en: The UN data contains missing values. Clean the data to prepare a simple final
    dataset with the required data and save it to your local drive as a SQL database
    file.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联合国数据包含缺失值。清理数据，准备一个包含所需数据的简单最终数据集，并将其保存到本地驱动器上的 SQL 数据库文件中。
- en: Use the `pd.read_csv` method of `pandas` to create a DataFrame.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pandas` 的 `pd.read_csv` 方法创建一个 DataFrame。
- en: Since the first row does not contain useful information, skip it using the `skiprows`
    parameter.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于第一行不包含有用的信息，请使用 `skiprows` 参数跳过它。
- en: Drop the column region/country/area and source.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除区域/国家/地区和来源列。
- en: 'Assign the following names as columns of the DataFrame: Region/County/Area,
    Year, Data, Value, and Footnotes.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下名称分配为 DataFrame 的列：地区/县/区域，年份，数据，值和脚注。
- en: Check how many unique values are present in the `Footnotes` column.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 `Footnotes` 列中存在的唯一值数量。
- en: Check the type of the `value` column.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 `value` 列的类型。
- en: Create a function to convert the value column into floating-point numbers.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数将值列转换为浮点数。
- en: Use the `apply` method to apply this function to a value.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `apply` 方法将此函数应用于一个值。
- en: Print the unique values in the data column.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印数据列中的唯一值。
- en: 'The final output should be as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出应如下所示：
- en: '![Figure 9.3: Bar plot for the enrollment in primary education in the USA'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.3：美国初等教育入学人数的条形图](img/Figure_9.3.png)'
- en: '](img/B15780_09_03.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B15780_09_03.jpg](img/B15780_09_03.jpg)'
- en: 'Figure 9.3: Bar plot for the enrollment in primary education in the USA'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：美国初等教育入学人数的条形图
- en: Note
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: The solution for this activity can be found via [this link](B15780_Solution_Final_RK.xhtml#_idTextAnchor330).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可通过 [此链接](B15780_Solution_Final_RK.xhtml#_idTextAnchor330) 获取。
- en: With this, we've reached the end of this activity. Here, we have looked into
    how to examine a particular real-life dataset to see what kind of data is missing.
    We also used the interpolate method from our DataFrame to fill in certain missing
    values.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们完成了这个活动的最后部分。在这里，我们探讨了如何检查特定的实际数据集以查看缺失了哪些类型的数据。我们还使用了 DataFrame 中的插值方法来填充某些缺失值。
- en: 'Activity 9.02: Data Wrangling Task – Cleaning GDP Data'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 9.02：数据整理任务 - 清理 GDP 数据
- en: The GDP data is available at [https://data.worldbank.org/](https://data.worldbank.org/)
    and is available on GitHub at [https://packt.live/2AMoeu6](https://packt.live/2AMoeu6).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: GDP 数据可在 [https://data.worldbank.org/](https://data.worldbank.org/) 获取，并在 GitHub
    上提供，链接为 [https://packt.live/2AMoeu6](https://packt.live/2AMoeu6)。
- en: 'In this activity, we will clean the GDP data. Follow these steps to complete
    this activity:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将清理 GDP 数据。按照以下步骤完成此活动：
- en: Create three DataFrames from the original DataFrame using filtering. Create
    the `df_primary`, `df_secondary`, and `df_tertiary` DataFrames for students enrolled
    in primary education, secondary education, and tertiary education in thousands,
    respectively.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从原始 DataFrame 中使用筛选创建三个 DataFrame。创建 `df_primary`、`df_secondary` 和 `df_tertiary`
    DataFrame，分别用于表示接受初等教育、中等教育和高等教育的学生人数（单位：千）。
- en: Plot bar charts of the enrollment of primary students in a low-income country
    such as India and a higher-income country such as the USA.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制低收入国家（如印度）和较高收入国家（如美国）初等学生入学人数的条形图。
- en: Since there is missing data, use `pandas` imputation methods to impute these
    data points by simple linear interpolation between data points. To do that, create
    a DataFrame with missing values inserted and append a new DataFrame with missing
    values to the current DataFrame.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于存在缺失数据，使用 `pandas` 的插补方法通过简单线性插值在数据点之间插补这些数据点。为此，创建一个包含缺失值的 DataFrame，并将包含缺失值的新
    DataFrame 添加到当前 DataFrame 中。
- en: '(For India) Append the rows corresponding to the missing years: `2004 – 2009`,
    `2011 – 2013`.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （针对印度）添加对应缺失年份的行：`2004 – 2009`，`2011 – 2013`。
- en: Create a dictionary of values with `np.nan`. Note that there are `9` missing
    data points, so we need to create a list with identical values repeated `9` times.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `np.nan` 创建一个包含值的字典。请注意，有 `9` 个缺失数据点，因此我们需要创建一个包含相同值重复 `9` 次的列表。
- en: Create a DataFrame of missing values (from the preceding dictionary) that we
    can append.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含缺失值的 DataFrame（来自前面的字典），我们可以将其添加。
- en: Append the DataFrames together.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 DataFrame 一起添加。
- en: Sort by year and reset the indices using `reset_index`. Use `inplace=True` to
    execute the changes on the DataFrame itself.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按年份排序并使用 `reset_index` 重置索引。使用 `inplace=True` 在 DataFrame 本身上执行更改。
- en: 'Use the interpolate method for linear interpolation. It fills all the `NaN`
    values with linearly interpolated values. See the following link for more details
    about this method: [http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html](http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html).'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用线性插值方法进行线性插值。它使用线性插值值填充所有 `NaN` 值。有关此方法的更多详细信息，请参阅以下链接：[http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html](http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html)。
- en: Repeat the same steps for USA (or other countries).
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对美国（或其他国家）重复相同的步骤。
- en: If there are values that are unfilled, use the `limit` and `limit_direction`
    parameters with the interpolate method to fill them in.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有未填充的值，使用 `limit` 和 `limit_direction` 参数与插值方法一起填充它们。
- en: Plot the final graph using the new data.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新数据绘制最终图表。
- en: Read the GDP data using the `pandas` `read_csv` method. It will generally throw
    an error.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`的`read_csv`方法读取GDP数据。它通常会抛出一个错误。
- en: To avoid errors, try using the `error_bad_lines = False` option.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了避免错误，尝试使用`error_bad_lines = False`选项。
- en: Since there is no delimiter in the file, add the `\t` delimiter.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于文件中没有分隔符，请添加`\t`分隔符。
- en: Use the `skiprows` function to remove rows that are not useful.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`skiprows`函数删除无用的行。
- en: Examine the dataset. Filter the dataset with information that states that it
    is similar to the previous education dataset.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据集。使用表示与先前教育数据集相似的信息来过滤数据集。
- en: Reset the index for this new dataset.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为这个新数据集重置索引。
- en: Drop the rows that aren't useful and re-index the dataset.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除无用的行并重新索引数据集。
- en: Rename the columns properly. This is necessary for merging the two datasets.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确重命名列。这对于合并两个数据集是必要的。
- en: We will concentrate only on the data from `2003` to `2016`. Eliminate the remaining
    data.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将只关注`2003`至`2016`年的数据。消除剩余的数据。
- en: Create a new DataFrame called `df_gdp` with rows `43` to `56`.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的DataFrame，名为`df_gdp`，包含行`43`至`56`。
- en: 'The final output should be as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出应如下所示：
- en: '![Figure 9.4: DataFrame focusing on year'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.4：关注年份的DataFrame'
- en: '](img/B15780_09_04.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15780_09_04.jpg)'
- en: 'Figure 9.4: DataFrame focusing on year'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：关注年份的DataFrame
- en: Now that we've seen how to clean and format the datasets, in the following activity,
    we'll learn how to merge these two datasets.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何清理和格式化数据集，在接下来的活动中，我们将学习如何合并这两个数据集。
- en: Note
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: The solution for this activity can be found via [this link](B15780_Solution_Final_RK.xhtml#_idTextAnchor332).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过[此链接](B15780_Solution_Final_RK.xhtml#_idTextAnchor332)找到此活动的解决方案。
- en: 'Activity 9.03: Data Wrangling Task – Merging UN Data and GDP Data'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动9.03：数据处理任务 – 合并联合国数据和GDP数据
- en: 'The aim of this activity is to merge the two datasets: UN data and GDP data.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的目的是合并两个数据集：联合国数据和GDP数据。
- en: 'The steps to merge these two databases is as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 合并这两个数据库的步骤如下：
- en: Reset the indexes for merging.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置索引以合并。
- en: Merge the two DataFrames, `primary_enrollment_india` and `df_gdp`, on the `Year`
    column.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Year`列上合并两个DataFrame，`primary_enrollment_india`和`df_gdp`。
- en: Drop the data, footnotes, and region/county/area.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除数据、脚注以及地区/县/区域。
- en: Rearrange the columns for proper viewing and presentation.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新排列列以进行适当的查看和展示。
- en: 'The output should be as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![Figure 9.5: Final output'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.5：最终输出'
- en: '](img/B15780_09_05.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15780_09_05.jpg)'
- en: 'Figure 9.5: Final output'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：最终输出
- en: In this activity, we saw how to merge two DataFrames to create a unified view
    and how to examine that view a little bit. In the next activity, we will learn
    how to store some of that data in a database.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，我们看到了如何合并两个DataFrame以创建统一视图，以及如何稍微检查该视图。在下一个活动中，我们将学习如何将其中一些数据存储到数据库中。
- en: Note
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: The solution for this activity can be found via [this link](B15780_Solution_Final_RK.xhtml#_idTextAnchor334).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过[此链接](B15780_Solution_Final_RK.xhtml#_idTextAnchor334)找到此活动的解决方案。
- en: 'Activity 9.04: Data Wrangling Task – Connecting the New Data to the Database'
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动9.04：数据处理任务 – 将新数据连接到数据库
- en: 'The steps to connect the data to the database is as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据连接到数据库的步骤如下：
- en: Import the `sqlite3` module of Python and use the `connect` function to connect
    to the database. The main database engine is embedded. But for a different database
    such as `Postgresql` or `MySQL`, we will need to connect to them using those credentials.
    We designate `Year` as the `PRIMARY KEY` of this table.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Python的`sqlite3`模块并使用`connect`函数连接到数据库。主要数据库引擎是嵌入式的。但对于像`Postgresql`或`MySQL`这样的不同数据库，我们需要使用那些凭据来连接它们。我们将`Year`指定为该表的`PRIMARY
    KEY`。
- en: Then, run a loop with the dataset rows one by one to insert them into the table.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，逐行运行数据集的循环以将它们插入到表中。
- en: 'The output: If we look at the current folder, we should see a file called `Education_GDP.db`,
    and if we examine that using a database viewer program, we will see the data transferred
    being there.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：如果我们查看当前文件夹，我们应该看到一个名为`Education_GDP.db`的文件，如果我们使用数据库查看程序检查它，我们将看到数据已传输到那里。
- en: Note
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: The solution for this activity can be found via [this link](B15780_Solution_Final_RK.xhtml#_idTextAnchor336).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过[此链接](B15780_Solution_Final_RK.xhtml#_idTextAnchor336)找到此活动的解决方案。
- en: If we look at the current folder, we should see a file called `Education_GDP.db`,
    and if we can examine that using a database viewer program, we will see that the
    data has been transferred there.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看当前文件夹，我们应该看到一个名为 `Education_GDP.db` 的文件，如果我们使用数据库查看程序来检查它，我们会看到数据已经被传输到那里。
- en: In these activities, we have examined a complete data wrangling flow, including
    reading data from the web and a local drive and filtering, cleaning, quick visualization,
    imputation, indexing, merging, and writing back to a database table. We also wrote
    custom functions to transform some of the data and saw how to handle situations
    where we may get errors upon reading the file.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些活动中，我们已经检查了一个完整的数据处理流程，包括从网络和本地驱动器读取数据，以及过滤、清洗、快速可视化、插补、索引、合并，并将数据写回数据库表。我们还编写了自定义函数来转换一些数据，并看到了在读取文件时可能遇到错误的情况如何处理。
- en: An Extension to Data Wrangling
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处理的扩展
- en: This is the concluding chapter of this book; we want to give you a broad overview
    of some of the exciting technologies and frameworks that you may need to learn
    about beyond data wrangling to work as a full-stack data scientist. Data wrangling
    is an essential part of the whole data science and analytics pipeline, but it
    is not the whole enterprise. You have learned invaluable skills and techniques
    in this book, but it is always good to broaden your horizons and look beyond to
    see what other tools that are out there that can give you an edge in this competitive
    and ever-changing world.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书的最后一章；我们希望给你一个关于一些你可能需要了解的激动人心的技术和框架的广泛概述，这些技术和框架超出了数据处理，以便作为一个全栈数据科学家工作。数据处理是整个数据科学和数据分析流程的一个基本部分，但它不是全部。你在本书中学到了宝贵的技能和技术，但总是好的，拓宽你的视野，看看其他工具，这些工具可以在竞争激烈和不断变化的世界中给你带来优势。
- en: Additional Skills Required to Become a Data Scientist
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成为数据科学家所需的其他技能
- en: 'To practice as a fully qualified data scientist/analyst, you should have some
    basic skills in your repertoire, irrespective of the particular programming language
    you choose to focus on. These skills and know-how are language-agnostic and can
    be utilized with any framework that you have to embrace, depending on your organization
    and business needs. We will describe them in brief here:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要成为一名合格的数据科学家/分析师进行实践，你应该在你选择的特定编程语言中具备一些基本技能。这些技能和知识是语言无关的，可以根据你的组织和企业需求，在任何你选择的框架中使用。我们将在下面简要描述它们：
- en: '**Git and version control**: Git and version control is what RDBMS is to data
    storage and query. It simply means that there is a huge gap between the pre and
    post Git era of version controlling your code. As you may have noticed, all the
    notebooks for this book are hosted on GitHub, and this was done to take advantage
    of the powerful Git **Version Control System** (**VCS**). It gives you, out of
    the box, version control, history, branching facilities for different code, merging
    different code branches, and advanced operations such as cherry picking, diff,
    and so on. It is a very essential tool to master as you can be almost sure that
    you will face it at one point of time in your journey.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Git和版本控制**：Git和版本控制是关系数据库管理系统（RDBMS）对数据存储和查询的作用。这意味着在Git时代之前和之后，在代码版本控制方面存在巨大的差距。正如你可能已经注意到的，本书的所有笔记本都托管在GitHub上，这是为了利用强大的Git
    **版本控制系统**（**VCS**）。它为你提供了开箱即用的版本控制、历史记录、不同代码的分支设施、合并不同代码分支以及诸如 cherry picking、diff
    等高级操作。这是你需要掌握的一个非常重要的工具，因为你几乎可以肯定，在你的旅途中你会在某个时候遇到它。'
- en: '**Linux command line**: People coming from a Windows background (or even MacOS,
    if you have not done any development before) are not very familiar, usually, with
    the command line. The superior UI of those OSes hides the low-level details of
    interaction with the OS using a command line. However, as a data professional,
    it is important that you know the command line well. There are so many operations
    that you can do by simply using the command line that it is astonishing.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Linux命令行**：来自Windows背景（或者甚至MacOS，如果你之前没有进行过任何开发）的人通常不太熟悉命令行。这些操作系统的优秀UI隐藏了使用命令行与操作系统交互的低级细节。然而，作为一名数据专业人士，了解命令行是非常重要的。你可以通过简单地使用命令行执行许多操作，这真是令人惊讶。'
- en: '**SQL and basic relational database concepts**: We dedicated an entire chapter
    to SQL and RDBMS, Chapter 8, SQL and RDBMS. However, as we have already mentioned
    there, it was really not enough. This is a vast subject and needs years of study
    to be mastered. Try to read more about it (including getting theory and practical
    experience) from books and online sources. Don''t forget that, despite all the
    other sources of data being used nowadays, we still have hundreds of millions
    of bytes of structured data stored in legacy database systems. You can be sure
    to come across one, sooner or later.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SQL和基本的关系数据库概念**：我们在第8章“SQL和RDBMS”中专门介绍了一个章节。然而，正如我们之前提到的，这远远不够。这是一个庞大的主题，需要多年的学习才能掌握。尝试从书籍和在线资源中了解更多关于它的内容（包括理论和实践经验）。别忘了，尽管现在使用了其他所有数据来源，我们仍然有数亿字节的结构化数据存储在传统的数据库系统中。你可以确信，迟早你会遇到其中之一。'
- en: '**Docker and containerization**: Since its first release in 2013, Docker has
    changed the way we distribute and deploy software in server-based applications.
    It gives you a clean and lightweight abstraction over the underlying OS and lets
    you iterate fast on development without the headache of creating and maintaining
    a proper environment. It is very useful in both the development and production
    phases. With virtually no competitor present, they are becoming the default in
    the industry very fast. We strongly advise that you explore it in great detail.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker和容器化**：自从2013年首次发布以来，Docker已经改变了我们在基于服务器的应用程序中分发和部署软件的方式。它为底层操作系统提供了一个干净且轻量级的抽象，让你在开发过程中快速迭代，无需为创建和维护适当的环境而烦恼。它在开发和生产阶段都非常有用。由于几乎没有竞争对手，它们正在非常快地成为行业中的默认选择。我们强烈建议你深入了解它。'
- en: Basic Familiarity with Big Data and Cloud Technologies
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对大数据和云计算技术的基本了解
- en: 'Big data and cloud platforms are the latest trends. We will introduce them
    here briefly and we encourage you to go ahead and learn about them as much as
    you can. If you are planning to grow as a data professional, then you can be sure
    that without these necessary skills, it will be hard for you to transition to
    the next level:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据和云计算平台是当前最新的趋势。我们将在这里简要介绍它们，并鼓励你们尽可能多地了解它们。如果你计划成为一名数据专业人士，那么你可以确信，没有这些必要的技能，你将很难过渡到下一个层次：
- en: '**Fundamental characteristics of big data**: Big data is simply data that is
    very big in size. The term size is a bit ambiguous here. It can mean one static
    chunk of data (such as the detail census data of a big country such as India or
    the US) or data that is dynamically generated as time passes, and each time it
    is huge. To give an example for the second category, we can think of how much
    data is generated by Facebook per day. It''s about 500+ TB per day. You can easily
    imagine that we will need specialized tools to deal with that amount of data.
    There are three different categories of big data, that is, structured, unstructured,
    and semi-structured. The main features that define big data are volume, variety,
    velocity, and variability.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大数据的基本特征**：大数据仅仅是非常大规模的数据。这里的“规模”一词有点模糊。它可以指一个静态的数据块（例如，像印度或美国这样的大国的详细人口普查数据）或者随着时间的推移动态生成的大量数据。为了举例说明第二类，我们可以想想Facebook每天生成多少数据。大约是每天500+
    TB。你可以轻松想象，我们将需要专门的工具来处理这么多的数据。大数据分为三个不同的类别，即结构化、非结构化和半结构化。定义大数据的主要特征是体积、种类、速度和可变性。'
- en: '**Hadoop ecosystem**: Apache Hadoop (and the related ecosystem) is a software
    framework that aims to use the MapReduce programming model to simplify the storage
    and processing of big data. It has since become one of the backbones of big data
    processing in the industry. The modules in Hadoop are designed to keep in mind
    that hardware failures are common occurrences, and they should be automatically
    handled by the framework. The four base modules of Hadoop are common, HDFS, YARN,
    and MapReduce. The Hadoop ecosystem consists of Apache Pig, Apache Hive, Apache
    Impala, Apache Zookeeper, Apache HBase, and more. They are very important bricks
    in many high-demand and cutting-edge data pipelines. We encourage you to study
    them in more depth. They are essential in any industry that aims to leverage data.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop生态系统**：Apache Hadoop（及其相关生态系统）是一个旨在使用MapReduce编程模型来简化大数据存储和处理的软件框架。它已经成为行业大数据处理的主要支柱之一。Hadoop中的模块设计时考虑到硬件故障是常见现象，并且应该由框架自动处理。Hadoop的四个基础模块是Common、HDFS、YARN和MapReduce。Hadoop生态系统包括Apache
    Pig、Apache Hive、Apache Impala、Apache Zookeeper、Apache HBase等。它们是许多高需求和前沿数据管道中非常重要的基石。我们鼓励你深入研究它们。对于任何旨在利用数据的行业来说，它们都是必不可少的。'
- en: '**Apache Spark**: Apache Spark is a general-purpose cluster computing framework
    that was initially developed at the University of California, Barkley, and released
    in 2014\. It gives you an interface to program an entire cluster of computers
    with built-in data parallelism and fault tolerance. It contains Spark Core, Spark
    SQL, Spark Streaming, MLlib (for machine learning), and GraphX. It is now one
    of the main frameworks that''s used in the industry to process a huge amount of
    data in real time based on streaming data. We encourage you to read about it and
    master it if you want to go toward real-time data engineering.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Spark**：Apache Spark是一个通用的集群计算框架，最初由加州大学伯克利分校开发，并于2014年发布。它为你提供了一个接口，可以编程整个计算机集群，内置数据并行性和容错性。它包含Spark
    Core、Spark SQL、Spark Streaming、MLlib（用于机器学习）和GraphX。它现在是行业处理基于流数据的实时大量数据的主要框架之一。如果你想要走向实时数据工程，我们鼓励你阅读并掌握它。'
- en: '**Amazon Web Services (AWS)**: Amazon Web Services (often abbreviated to AWS)
    are a bunch of managed services offered by Amazon ranging from Infrastructure-as-a-Service,
    Database-as-a-Service, Machine-Learning-as-a-Service, cache, load balancer, NoSQL
    database, to message queues and several other types. They are very useful for
    all sorts of applications. It can be a simple web app or a multi-cluster data
    pipeline. Many famous companies run their entire infrastructure on AWS (such as
    Netflix). They give us on-demand provisioning, easy scaling, a managed environment,
    a slick UI to control everything, and also a very powerful command-line client.
    They also expose a rich set of APIs, and we can find an AWS API client in virtually
    any programming language. The Python one is called Boto3\. If you are planning
    to become a data professional, then it can be said with near certainty that you
    will end up using many of their services at one point or another.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**亚马逊网络服务 (AWS)**: 亚马逊网络服务（通常简称为 AWS）是由亚马逊提供的一系列托管服务，包括基础设施即服务（IaaS）、数据库即服务（DBaaS）、机器学习即服务（MLaaS）、缓存、负载均衡器、NoSQL
    数据库，以及消息队列等多种类型。它们对于各种应用都非常有用。它可以是一个简单的Web应用，也可以是一个多集群数据管道。许多知名公司都在 AWS 上运行其整个基础设施（例如
    Netflix）。它们为我们提供了按需配置、易于扩展、管理环境、流畅的用户界面来控制一切，以及一个非常强大的命令行客户端。它们还公开了一组丰富的API，我们几乎可以在任何编程语言中找到AWS
    API客户端。Python 中的称为 Boto3。如果你计划成为一名数据专业人士，那么几乎可以肯定的是，你会在某个时候使用它们的服务。'
- en: What Goes with Data Wrangling?
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清洗与什么相关？
- en: We learned in *Chapter 1*, *Introduction to Data Wrangling with Python*, that
    the process of data wrangling lies in-between data gathering and advanced analytics,
    including visualization and machine learning. However, the boundaries that exist
    between these processes may not always be strict and rigid. It depends largely
    on organizational culture and team composition.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *第一章*，*使用 Python 进行数据清洗入门* 中了解到，数据清洗的过程位于数据收集和高级分析（包括可视化和机器学习）之间。然而，这些过程之间的界限可能并不总是严格和固定的。这很大程度上取决于组织文化和团队构成。
- en: 'Therefore, we need to not only be aware of data wrangling but also the other
    components of the data science platform to wrangle data effectively. Even if you
    are performing pure data wrangling tasks, having a good grasp over how data is
    sourced and utilized will give you an edge for coming up with unique and efficient
    solutions to complex data wrangling problems and enhance the value of those solutions
    for the machine learning scientist or the business domain expert:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不仅需要了解数据清洗，还要了解数据科学平台的其他组件，以便有效地处理数据。即使你正在执行纯粹的数据清洗任务，了解数据来源和利用方式也将为你提供优势，帮助你提出独特且高效的解决方案来解决复杂的数据清洗问题，并提高这些解决方案对机器学习科学家或业务领域专家的价值：
- en: '![Figure 9.6: Process of data wrangling'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.6：数据清洗过程'
- en: '](img/B15780_01_01.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B15780_01_01.jpg]'
- en: 'Figure 9.6: Process of data wrangling'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6：数据清洗过程
- en: Now, we have, in fact, already laid out a solid groundwork in this book for
    the data platform part, assuming that it is an integral part of data wrangling
    workflow. For example, we have covered web scraping, working with RESTful APIs,
    and database access and manipulation using Python libraries in detail.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，实际上，我们已经在本书中为数据平台部分奠定了坚实的基础，假设它是数据清洗工作流程的一个组成部分。例如，我们详细介绍了网络爬虫、使用Python库处理RESTful
    API以及数据库访问和操作。
- en: We also touched on basic visualization techniques and plotting functions in
    Python using `matplotlib`. However, there are other advanced statistical plotting
    libraries, such as `seaborn`, that you can master for more sophisticated visualization
    for data science tasks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还简要介绍了基本的可视化技术和使用`matplotlib`在Python中的绘图函数。然而，还有其他高级统计绘图库，如`seaborn`，你可以掌握这些库以进行更复杂的数据科学任务的可视化。
- en: Business logic and domain expertise is a varied topic and it can only be learned
    on the job; however, it will come eventually with experience. If you have an academic
    background and/or work experience in domains such as finance, medicine and healthcare,
    or engineering, that knowledge will come in handy in your data science career.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 商业逻辑和领域专业知识是一个多样化的主题，它只能在工作中学习；然而，随着经验的积累，它最终会到来。如果你在金融、医学和医疗保健或工程等领域有学术背景和/或工作经验，这些知识将在你的数据科学职业生涯中派上用场。
- en: The fruit of the hard work of data wrangling is realized fully in the domain
    of machine learning. It is the science and engineering of making machines learn
    patterns and insights from data for predictive analytics and intelligent, automated
    decision-making with a deluge of data that cannot be analyzed efficiently by humans.
    Machine learning has become one of the most sought-after skills in the modern
    technology landscape. It has truly become one of the most exciting and promising
    intellectual fields, with applications ranging from e-commerce to healthcare and
    virtually everything in-between. Data wrangling is intrinsically linked with machine
    learning as it prepares the data so that it's suitable for intelligent algorithms
    to process. Even if you start your career in data wrangling, it could be a natural
    progression to move to machine learning.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗的辛勤工作在机器学习领域得到了充分的体现。它是使机器从数据中学习模式和洞察力，以进行预测分析和智能、自动决策的科学和工程，这些数据量巨大，人类无法有效分析。机器学习已经成为现代技术景观中最受欢迎的技能之一。它确实已经成为一个最激动人心且最有前景的智力领域，其应用范围从电子商务到医疗保健，几乎涵盖了所有中间领域。数据清洗与机器学习内在相关，因为它为智能算法处理的数据做好了准备。即使你从数据清洗开始职业生涯，也可能自然地过渡到机器学习。
- en: Packt has published numerous books and books on this topic that you should explore.
    In the next section, we will touch upon some approaches to adopt and Python libraries
    to check out that will give you a boost in your learning.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Packt已经出版了众多关于这个主题的书籍，你应该探索一下。在下一节中，我们将讨论一些可以采用的方法和Python库，这些库将给你的学习带来帮助。
- en: Tips and Tricks for Mastering Machine Learning
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掌握机器学习的技巧与窍门
- en: 'Machine learning is difficult to start with. We have listed some structured
    MOOCs and incredible free resources that are available so that you can begin your
    journey:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习入门有一定难度。我们列出了一些结构化的MOOCs和令人难以置信的免费资源，以便您开始您的学习之旅：
- en: Understand the definition of and differentiation between the buzzwords – artificial
    intelligence, machine learning, deep learning, and data science. Cultivate the
    habit of reading great posts or listening to the expert talks on these topics
    and understand their true reach and applicability to some business problem.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解人工智能、机器学习、深度学习和数据科学等术语的定义及其区别。培养阅读关于这些主题的优秀帖子或聆听专家演讲的习惯，并理解它们在解决某些商业问题上的真实影响力和适用性。
- en: 'There are some great MOOCs that will help you understand all these and also
    learn advanced skills in machine learning and AI. Some of them are as follows:'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一些优秀的MOOC课程可以帮助你理解所有这些，并学习机器学习和人工智能的高级技能。以下是一些：
- en: a) Machine Learning – Andrew Ng, Stanford University
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 机器学习 – 斯坦福大学安德鲁·吴
- en: b) Machine Learning for Undergraduates – Nando de Freitas, University of British
    Columbia
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 适用于本科生的机器学习 – 英属哥伦比亚大学南多·德·弗雷塔斯
- en: c) Machine Learning – Tom Mitchell, CMU
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 机器学习 – 卡内基梅隆大学汤姆·米切尔
- en: d) Deep Learning – Nando de Freitas, University of Oxford
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 深度学习 – 牛津大学南多·德·弗雷塔斯
- en: 'Stay updated with the recent trends by watching videos, reading books such
    as *The Master Algorithm: How the Quest for the Ultimate Learning Machine Will
    Remake Our World*, as well as articles, and following influential blogs such as
    KDnuggets, Brandon Rohrer''s blog, Open AI''s blog about their research, Toward
    Data Science publication on Medium, and so on.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过观看视频、阅读书籍如*《大师算法：终极学习机器的探索将重塑我们的世界*》，以及文章，关注像KDnuggets、布兰登·罗赫尔的博客、Open AI关于他们研究的博客、Medium上的《走向数据科学》出版物等有影响力的博客，来保持对最新趋势的了解。
- en: As you learn new algorithms or concepts, pause and analyze how you can apply
    these machine learning concepts or algorithms in your daily work. This is the
    best method for learning and expanding your knowledge base.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你学习新的算法或概念时，暂停并分析你如何在日常工作中应用这些机器学习概念或算法。这是学习和扩展你的知识库的最佳方法。
- en: If you choose Python as your preferred language for machine learning tasks,
    then you have a great machine learning library in `scikit-learn`. It is the most
    widely used general machine learning package in the Python ecosystem. `scikit-learn`
    has a wide variety of supervised and unsupervised learning algorithms, which are
    exposed via a stable consistent interface. Moreover, it is specifically designed
    to interface seamlessly with other popular data wrangling and numerical libraries,
    such as NumPy and pandas.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你选择Python作为机器学习任务的优先语言，那么你将拥有一个优秀的机器学习库`scikit-learn`。它是Python生态系统中使用最广泛的通用机器学习包。`scikit-learn`提供了多种监督学习和无监督学习算法，这些算法通过一个稳定一致的接口进行暴露。此外，它专门设计用来无缝地与其他流行的数据处理和数值库接口，例如NumPy和pandas。
- en: Another hot skill in today's job market is deep learning. Packt has many books
    on deep learning. For Python libraries, you can learn and practice with **TensorFlow**,
    **Keras**, or **PyTorch** for deep learning.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在当今的就业市场上，深度学习是另一个热门技能。Packt有许多关于深度学习的书籍。对于Python库，你可以学习并使用**TensorFlow**、**Keras**或**PyTorch**进行深度学习。
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Data is everywhere and it is all around us. In these nine chapters, we have
    learned how data from different types and sources can be cleaned, corrected, and
    combined. Hopefully, this chapter must have tested your skills enough to shore
    up the concepts you've learned so far. If you want, you can revisit some of the
    prior chapters to practice your data wrangling skills a bit more. Using the power
    of Python and the knowledge of data wrangling and applying the tricks and tips
    that you have studied in this book, you are ready to be a data wrangler.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 数据无处不在，它环绕在我们周围。在这九个章节中，我们学习了如何清理、纠正和组合来自不同类型和来源的数据。希望这一章已经足够测试你的技能，以巩固你迄今为止学到的概念。如果你想的话，可以回顾一些前面的章节，以进一步练习你的数据处理技能。利用Python的力量、数据处理的知识以及你在本书中学到的技巧和提示，你准备好成为一名数据整理师了。
