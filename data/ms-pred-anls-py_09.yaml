- en: Chapter 9. Reporting and Testing – Iterating on Analytic Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。报告和测试 – 迭代分析系统
- en: 'In previous chapters we have considered many components of an analytical application,
    from the input data set to the choice of algorithm and tuning parameters, and
    even illustrated a potential deployment strategy using a web server. In this process,
    we considered parameters such as scalability, interpretability, and flexibility
    in making our applications robust to both later refinements of an algorithm and
    changing requirements of scale. However, these sorts of details miss the most
    important element of this application: your business partners who hope to derive
    insight from the model and the continuing needs of the organization. What metrics
    should we gather on the performance of a model to make the case for its impact?
    How can we iterate on an initial model to optimize its use for a business application?
    How can these results be articulated to stakeholders? These sorts of questions
    are key in conveying the benefit of building analytical applications for your
    organization.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们考虑了许多分析应用组件，从输入数据集到算法选择和调整参数，甚至展示了使用Web服务器的一个潜在部署策略。在这个过程中，我们考虑了可扩展性、可解释性和灵活性等参数，以使我们的应用能够应对算法的后续改进和规模需求的变化。然而，这些细节忽略了此应用最重要的元素：希望从模型中获得洞察力的业务伙伴以及组织持续的需求。我们应该收集哪些关于模型性能的指标来证明其影响？我们如何迭代初始模型以优化其在商业应用中的使用？如何将这些结果传达给利益相关者？这些问题在传达为组织构建分析应用的好处时至关重要。
- en: 'Just as we can use increasingly larger data sets to build predictive models,
    automated analysis packages and "big data" systems are making it easier to gather
    substantial amounts of information about the behavior of algorithms. Thus, the
    challenge becomes not so much if we can collect data on an algorithm or how to
    measure this performance, but to choose what statistics are most relevant to demonstrate
    value in the context of a business analysis. In order to equip you with the skills
    to better monitor the health of your predictive applications, improve them through
    iterative milestones and explain these techniques to others in this chapter, we
    will:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们可以使用越来越大的数据集来构建预测模型一样，自动分析包和“大数据”系统正在使收集大量关于算法行为的信息变得更加容易。因此，挑战不再是我们能否收集算法数据或如何衡量这种性能，而是选择哪些统计数据在商业分析环境中最能体现价值。为了使你具备更好地监控预测应用健康状况、通过迭代里程碑改进它们以及在本章中向他人解释这些技术的技能，我们将：
- en: Review common model diagnostics and performance indicators.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审查常见的模型诊断和性能指标。
- en: Describe how A/B testing may be used to iteratively improve upon a model.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述如何使用A/B测试迭代改进模型。
- en: Summarize ways in which predictive insights from predictive models can be communicated
    in reports.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结在报告中传达预测模型预测洞察的方法。
- en: Checking the health of models with diagnostics
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用诊断工具检查模型的健康状况
- en: Throughout the previous chapters, we have primarily focused on the initial steps
    of predictive modeling, from data preparation and feature extraction to optimization
    of parameters. However, it is unlikely that our customers or business will remain
    unchanging, so predictive models must typically adapt as well. We can use a number
    of diagnostics to check the performance of models over time, which serve as a
    useful benchmark to evaluate the health of our algorithms.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们主要关注预测建模的初始步骤，从数据准备和特征提取到参数优化。然而，我们的客户或业务不太可能保持不变，因此预测模型通常也需要适应。我们可以使用多种诊断工具来检查模型随时间的变化性能，这些工具作为评估我们算法健康状况的有用基准。
- en: Evaluating changes in model performance
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型性能的变化
- en: 'Let us consider a scenario in which we train a predictive model on customer
    data and evaluate its performance on a set of new records each day for a month
    afterward. If this were a classification model, such as predicting whether a customer
    will cancel their subscription in the next pay period, we could use a metric such
    as the **Area Under the Curve** (**AUC**) of the **Receiver-Operator-Characteristic**
    (**ROC**) curve that we saw previously in [Chapter 5](ch05.html "Chapter 5. Putting
    Data in its Place – Classification Methods and Analysis"), *Putting Data in its
    Place – Classification Methods and Analysis*. Alternatively, in the case of a
    regression model, such as predicting average customer spend, we can use the *R²*
    value or average squared error:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个场景，在这个场景中，我们在客户数据上训练一个预测模型，并在之后的每个月每天对一组新的记录进行性能评估。如果这是一个分类模型，例如预测客户在下一个支付周期是否会取消订阅，我们可以使用之前在[第5章](ch05.html
    "第5章。将数据放在合适的位置 – 分类方法和分析")中看到的**曲线下面积**（**AUC**）和**接收者操作特征**（**ROC**）曲线的度量，即*将数据放在合适的位置
    – 分类方法和分析*。或者，在回归模型的情况下，例如预测平均客户消费，我们可以使用*R²*值或平均平方误差：
- en: '![Evaluating changes in model performance](img/B04881_09_Formula1.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型性能变化](img/B04881_09_Formula1.jpg)'
- en: to quantify performance over time. If we observe a drop in one of these statistics,
    how can we analyze further what the root cause may be?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化随时间推移的性能。如果我们观察到这些统计数据中的一个下降，我们如何进一步分析可能的原因？
- en: '![Evaluating changes in model performance](img/B04881_09_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型性能变化](img/B04881_09_01.jpg)'
- en: In the graph above, we show such a scenario, where we measured the AUC for a
    hypothetical ad-targeting algorithm for 30 days after initial training by quantifying
    how many of the targeted users clicked on an ad sent in an e-mail and visited
    the website for our company. We see AUC begin to dip on day 18, but because the
    AUC is an overall measure of accuracy, it is not clear whether all observations
    are being poorly predicted or only a subpopulation is leading to this drop in
    performance. Thus, in addition to measuring overall AUC, we might think of calculating
    the AUC for subsets of data defined by the input features. In addition to providing
    a way of identifying problematic new data (and suggest when the model needs to
    be retrained), such reports provide a way of identifying the overall business
    impact of our model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图表中，我们展示了这样一个场景，我们通过量化有多少目标用户点击了通过电子邮件发送的广告并访问了我们的公司网站，来衡量在初始训练后的30天内一个假设的广告定位算法的AUC。我们看到AUC在第18天开始下降，但由于AUC是准确性的总体度量，因此不清楚是否所有观察结果都被错误预测，或者只有一部分子群体导致了性能下降。因此，除了测量总体AUC之外，我们还可能考虑计算由输入特征定义的数据子集的AUC。除了提供识别问题新数据（并建议何时需要重新训练模型）的方法之外，此类报告还提供了一种识别我们模型整体业务影响的方法。
- en: '![Evaluating changes in model performance](img/B04881_09_02.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型性能变化](img/B04881_09_02.jpg)'
- en: 'As an example, for our ad-targeting algorithm, we might look at overall performance
    and compare that to one of the labels in our data set: whether the user is a current
    subscriber or not. It may not be surprising that the performance on subscribers
    is usually higher, as these users were already likely to visit our site. The non-subscribers,
    who may never have visited our site before, represent the real opportunity in
    this scenario. In the preceding graph, we see that, indeed, the performance on
    non-subscribers dropped on day 18\. However, it is also worth noting that this
    does not necessarily tell the whole story of why the performance dropped. We still
    do not know why the performance on new members is lower. We can subset the data
    again and look for a correlated variable. For example, if we looked along a number
    of ad IDs (which correspond to different images displayed to a customer in an
    e-mail), we might find that the performance dip is due to one particular ad (please
    refer to the following graph). Following up with our business stakeholders, we
    might find that this particular ad was for a seasonal product and is only shown
    every 12 months. Therefore, the product was familiar to subscribers, who may have
    seen this product before, but not to non-members, who thus were unfamiliar with
    the item and did not click on it. We might be able to confirm this hypothesis
    by looking at subscriber data and seeing whether performance of the model also
    dips for subscribers with tenure less than 12 months.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们的广告定位算法为例，我们可能会查看整体性能，并将其与我们数据集中的某个标签进行比较：用户是否是当前订阅者。性能在订阅者身上通常更高可能并不令人惊讶，因为这些用户已经很可能访问过我们的网站。从未访问过我们网站的未订阅者代表了这种情况下的真正机会。在前面的图表中，我们可以看到，确实，在第18天非订阅者的性能下降了。然而，也值得注意，这并不一定能够完全解释性能下降的原因。我们仍然不知道为什么新成员的性能较低。我们可以再次对数据进行子集划分，并寻找相关的变量。例如，如果我们查看多个广告ID（这些ID对应于在电子邮件中向客户展示的不同图片），我们可能会发现性能下降是由于某个特定的广告（请参阅以下图表）。通过跟进我们的业务利益相关者，我们可能会发现这个特定的广告是为季节性产品做的，并且每年只展示12个月。因此，这个产品对订阅者来说很熟悉，他们可能之前见过这个产品，但对非会员来说则不熟悉，因此他们没有点击它。我们可能可以通过查看订阅者数据来验证这个假设，看看模型的性能是否也会在服务期限少于12个月的订阅者身上下降。
- en: '![Evaluating changes in model performance](img/B04881_09_03.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型性能的变化](img/B04881_09_03.jpg)'
- en: 'This sort of investigation can then begin to ask how to optimize this particular
    advertisement for new members, but can also indicate ways to improve our model
    training. In this scenario, it is likely that we trained the algorithm on a simple
    random sample of data that was biased for current subscribers, as we have more
    data on these customers if we took a simple random sample of event data: subscribers
    are more active, and thus both produce more impressions (as they may have registered
    for promotional e-mails) and are more likely to have clicked on ads. To improve
    our model, we might want to balance our training data between subscribers and
    non-subscribers to compensate for this bias.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种调查可以开始探讨如何优化针对新成员的特定广告，同时也可能指出改进我们模型训练的方法。在这种情况下，我们可能是在一个简单随机样本的数据上训练了算法，而这个样本对当前订阅者是有偏见的，因为我们如果对事件数据进行了简单随机抽样，那么我们关于这些客户的数据就更多了：订阅者更加活跃，因此他们产生的印象（他们可能已经注册了促销电子邮件）更多，点击广告的可能性也更大。为了改进我们的模型，我们可能想要在订阅者和非订阅者之间平衡我们的训练数据，以补偿这种偏差。
- en: In this simple example, we were able to diagnose the problem by examining performance
    of the model in only a small number of sub-segments of data. However, we cannot
    guarantee that this will always be the case, and manually searching through hundreds
    of variables will be inefficient. Thus, we might consider using a predictive model
    to help narrow down the search space. For example, consider using a **gradient
    boosted machine** (**GBM**) from [Chapter 5](ch05.html "Chapter 5. Putting Data
    in its Place – Classification Methods and Analysis"), *Putting Data in its Place
    – Classification Methods and Analysis*, with the inputs being the same data we
    used to train our predictive model, and the outputs being the misclassification
    (either a label of 1, 0 for a categorical model, or a continuous error such as
    squared error or log loss in a regression model). We now have a model that predicts
    errors in the first model. Using a method such as a GBM allows us to examine systematically
    a large number of potential variables and use the resulting variable importance
    to pinpoint a smaller number of hypotheses.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的例子中，我们能够通过检查模型在数据的一小部分子段上的性能来诊断问题。然而，我们无法保证这种情况总是会发生，手动搜索数百个变量将是不高效的。因此，我们可能考虑使用预测模型来帮助缩小搜索范围。例如，可以考虑使用来自[第5章](ch05.html
    "第5章。将数据放在合适的位置 – 分类方法和分析")的**梯度提升机**（**GBM**），*将数据放在合适的位置 – 分类方法和分析*，输入数据与训练预测模型所用的相同数据，输出数据为误分类（对于分类模型，可以是标签1或0，对于回归模型，可以是平方误差或对数损失等连续误差）。现在我们有一个预测第一个模型中错误的模型。使用GBM这样的方法允许我们系统地检查大量潜在变量，并使用由此产生的变量重要性来缩小假设的数量。
- en: Of course, the success of any these approaches hinges on the fact that the variable
    causing the drop in performance is a part of our training set, and that the issue
    has to do with the underlying algorithm or data. It is also certainly possible
    to imagine other cases where there is an additional variable we are not using
    to construct our data set for training which is causing the problem, such as poor
    connections on a given Internet service provider that are preventing users from
    clicking through an ad to our webpage, or a system problem such as failure in
    e-mail delivery.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，任何这些方法的成功都取决于导致性能下降的变量是否是我们训练集的一部分，以及问题是否与基本算法或数据有关。当然，也可以想象其他情况，其中存在一个我们没有使用来构建训练数据集的额外变量，这导致了问题，例如，在特定互联网服务提供商上的糟糕连接阻止用户点击广告到我们的网页，或者系统问题，如电子邮件投递失败。
- en: Looking at performance by segments can also help us determine if the algorithm
    is functioning as intended when we make changes. For example, if we reweighted
    our training data to emphasize non-subscribers, we would hope that performance
    of AUC on these customers would improve. If we only examined overall performance,
    we might observe increases that are improvements on existing customers, but not
    the effect we actually wished to achieve.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对各个部分进行性能分析，也可以帮助我们确定在做出更改时算法是否按预期运行。例如，如果我们重新加权我们的训练数据以强调非订阅者，我们希望这些客户的AUC性能会提高。如果我们只检查整体性能，我们可能会观察到对现有客户的改进，但不是我们实际希望实现的效果。
- en: Changes in feature importance
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性变化
- en: Besides examining the accuracy of models over time, we might also want to examine
    changes in the importance of different input data. In a regression model, we might
    examine the important coefficients as judged by magnitude and statistical significance,
    while in a decision-tree-based algorithm such as a Random Forest or GBM, we can
    look at measures of variable importance. Even if the model is performing at the
    same level as described by evaluation statistics discussed previously, shifts
    in the underlying variables may signal issues in data logging or real changes
    in the underlying data that are of business significance.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了检查模型随时间的准确性外，我们还可能想检查不同输入数据的重要性变化。在回归模型中，我们可能通过大小和统计显著性来判断重要系数，而在基于决策树的算法（如随机森林或GBM）中，我们可以查看变量重要性的度量。即使模型的表现与之前讨论的评估统计量相同，底层变量的变化可能表明数据记录的问题或底层数据中具有商业意义的真实变化。
- en: Let us consider a churn model, where we input a number of features for a user
    account (such as zip code, income level, gender, and engagement metrics such as
    hours spent per week on our website) and try to predict whether a given user will
    cancel his/her subscription at the end of each billing period. While it is useful
    to have a score predicting the likelihood of churn, as we would target these users
    with additional promotional campaigns or targeted messaging, the underlying features
    that contribute to this prediction may provide insight for more specific action.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个流失率模型，其中我们为用户账户输入一系列特征（如邮编、收入水平、性别以及每周在我们网站上花费的小时数等参与度指标），并尝试预测在每一个账单周期结束时，特定用户是否会取消其订阅。虽然有一个预测流失可能性的分数是有用的，因为我们可以针对这些用户进行额外的促销活动或定向信息，但贡献于这个预测的底层特征可能为我们提供更具体行动的见解。
- en: In this example, we generate a report of the 10 most important features in the
    predictive model each week. Historically, this list has been consistent, with
    the customer's profession and income being the top variables. However, in one
    week, we find that income is no longer in this list, and that instead zip code
    has replaced it. When we check the data flowing into the model, we find that the
    income variable is no longer being logged correctly; thus, zip code, which is
    correlated with income, becomes a substitute for this feature in the model, and
    our regular analysis of variable importance helped us detect a significant data
    issue.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们每周生成一个报告，列出预测模型中最重要的10个特征。从历史来看，这个列表一直保持一致，其中顾客的职业和收入是顶级变量。然而，在一周内，我们发现收入不再在这个列表中，取而代之的是邮编。当我们检查流入模型的数据时，我们发现收入变量不再被正确记录；因此，与收入相关的邮编在模型中成为了这个特征的替代品，而我们常规的变量重要性分析帮助我们检测到一个重大的数据问题。
- en: What if, instead the income variable was being logged correctly? In this case,
    it seems unlikely that zip code is more powerful a predictor than income if the
    underlying feature both are capturing is a customer's finances. Thus, we might
    examine whether there are particular zip codes for which churn has changed significantly
    over the past week. Upon doing so, we find that a competitor recently launched
    a site with a lower price in certain zip codes, letting us both understand the
    reason for the rise in zip code as a predictor (customers with the lower price
    option are more likely to abandon our site) and indicate market dynamics that
    are of larger interest.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果收入变量被正确记录了呢？在这种情况下，如果底层特征都在捕捉客户的财务状况，那么邮编比收入更有预测力似乎不太可能。因此，我们可能会检查在过去一周内，是否有特定的邮编其流失率发生了显著变化。经过调查，我们发现竞争对手最近在某些邮编地区推出了价格更低的网站，这让我们既理解了邮编作为预测因素上升的原因（选择更低价格选项的客户更有可能放弃我们的网站），又指出了更大兴趣的市场动态。
- en: 'This second scenario also suggests another variable we might monitor: the correlation
    between variables in our data set. While it is both computationally difficult
    and practically restrictive to comprehensively consider every pair of variables
    in large data sets, we can use dimensionality reduction techniques such as Principal
    Components Analysis described in [Chapter 6](ch06.html "Chapter 6. Words and Pixels
    – Working with Unstructured Data"), *Words and Pixels – Working with Unstructured
    Data*, to provide a high-level summary of the correlations between variables.
    This reduces the task of monitoring such correlations to examination of a few
    diagrams of the important components, which, in turn, can alert us to changes
    in the underlying structure of the data.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种第二种场景还暗示了另一个我们可能需要监控的变量：数据集中变量之间的相关性。虽然在大数据集中全面考虑每一对变量在计算上困难重重，在实践上又受到限制，但我们可以使用如[第6章](ch06.html
    "第6章。文字与像素 - 处理非结构化数据")中描述的主成分分析等降维技术，*文字与像素 - 处理非结构化数据*，来提供变量之间相关性的高级概述。这把监控这些相关性的任务简化为检查几个重要成分的图表，反过来，这又能使我们注意到数据底层结构的变化。
- en: Changes in unsupervised model performance
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督模型性能的变化
- en: The examples we looked at previously all concern a supervised model where we
    have a target to predict, and we measure performance by looking at AUC or similar
    metrics. In the case of the unsupervised models we examined in [Chapter 3](ch03.html
    "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning"),
    *Finding Patterns in the Noise – Clustering and Unsupervised Learning*, our outcome
    is a cluster membership rather than a target. What sort of diagnostics can we
    look at in this scenario?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前考察的例子都涉及一个监督模型，其中我们有一个预测的目标，并通过查看AUC或类似指标来衡量性能。在[第3章](ch03.html "第3章. 在噪声中寻找模式
    – 聚类和无监督学习")中考察的无监督模型，即《在噪声中寻找模式 – 聚类和无监督学习》，我们的结果是聚类成员资格而不是目标。在这种情况下，我们可以查看哪些诊断？
- en: In cases where we have a gold-standard label, such as a human-annotated label
    of spam versus non-spam messages if we are clustering e-mail documents, we can
    examine whether the messages end up in distinct clusters or are mixed. In a sense,
    this is comparable to looking at classification accuracy. However, for unsupervised
    models, we might frequently not have any known label, and the clustering is purely
    an exploratory tool. We might still use human-annotated examples as a guide, but
    this becomes prohibitive for larger data sets. In other scenarios, such as sentiment
    in online media, remain subjective enough that human labels may not significantly
    enrich labels derived from automated methods such as the LDA topic model we discussed
    in [Chapter 6](ch06.html "Chapter 6. Words and Pixels – Working with Unstructured
    Data"), *Words and Pixels – Working with Unstructured Data*. In this case, how
    can we judge the quality of the clustering over time?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们有黄金标准标签的情况下，例如如果我们对电子邮件文档进行聚类，我们有垃圾邮件与非垃圾邮件消息的人类标注标签，我们可以检查消息是否最终进入不同的聚类或混合。在某种程度上，这类似于查看分类精度。然而，对于无监督模型，我们可能经常没有任何已知的标签，聚类纯粹是一个探索性工具。我们仍然可以使用人类标注的示例作为指导，但对于更大的数据集来说，这可能变得不可行。在其他场景中，例如在线媒体的情感，仍然足够主观，以至于人类标签可能不会显著丰富来自如我们在[第6章](ch06.html
    "第6章. 文字和像素 – 处理非结构化数据")中讨论的LDA主题模型等自动化方法的标签。在这种情况下，我们如何判断聚类随时间的变化质量？
- en: 'In cases where the number of groups is determined dynamically, such as through
    the Affinity Propagation Clustering algorithm described in [Chapter 3](ch03.html
    "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning"),
    *Finding Patterns in the Noise – Clustering and Unsupervised Learning*, we examine
    whether the number of clusters remains fixed over time. In most cases we previously
    examined, though, the number of clusters remains fixed. Thus, we could envision
    one diagnostic in which we examine the distance between the centers of the nearest
    clusters between training cycles: for example, with a k-means model with 20 clusters,
    assign each cluster in week 1 its closest match in week 2 and compare the distribution
    of the 20 distances. If the clustering remains stable, then the distribution of
    these distances should as well. Changes could indicate that 20 is no longer a
    good number to fit the data or that the composition of the 20 clusters is significantly
    changing over time. We might also examine a value such as the sum of squares error
    in k-means clustering over time to see if the quality of the obtained clusters
    is significantly varying.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些组数是动态确定的场景中，例如通过[第3章](ch03.html "第3章. 在噪声中寻找模式 – 聚类和无监督学习")中描述的亲和传播聚类算法，即《在噪声中寻找模式
    – 聚类和无监督学习》，我们检查聚类数量是否随时间保持不变。然而，在我们之前考察的大多数情况下，聚类数量是固定的。因此，我们可以设想一种诊断方法，即检查训练周期之间最近聚类中心的距离：例如，对于一个有20个聚类的k-means模型，将第1周中的每个聚类分配给第2周中最接近的匹配，并比较20个距离的分布。如果聚类保持稳定，那么这些距离的分布也应该如此。变化可能表明20已不再是拟合数据的良好数字，或者20个聚类的组成随时间显著变化。我们还可以检查k-means聚类随时间变化的平方和误差等值，以查看获得的聚类质量是否显著变化。
- en: 'Another quality metric that is agnostic to a specific clustering algorithm
    is Silhouette analysis (Rousseeuw, Peter J. "Silhouettes: a graphical aid to the
    interpretation and validation of cluster analysis." *Journal of computational
    and applied mathematics 20* (1987): 53-65). For each data point **i** in the set,
    we ask how dissimilar (as judged by the distance metric used in the clustering
    algorithm) on average it is to other points in its cluster, giving a value *d(i)*.
    If the point **i** is appropriately assigned, then *d(i)* is near 0, as the average
    dissimilarity between **i** and other points in its cluster is low. We could also
    calculate the same average dissimilarity value for **i** for other clusters, and
    the second lowest value (the second best cluster assignment for **i**) is given
    by *d''(i)*. We then obtain a silhouette score between –1 and 1 using the formula:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个对特定聚类算法无差别的质量指标是轮廓分析（Rousseeuw, Peter J. "Silhouettes: a graphical aid to
    the interpretation and validation of cluster analysis." *Journal of computational
    and applied mathematics 20* (1987): 53-65）。对于集合中的每个数据点**i**，我们询问它与其他簇中点的平均差异（由聚类算法中使用的距离度量判断），给出一个值*d(i)*。如果点**i**被适当地分配，那么*d(i)*接近0，因为**i**与其簇中其他点的平均差异低。我们还可以为**i**计算其他簇的相同平均差异值，第二个最低值（i的第二好簇分配）由*d''(i)*给出。然后我们使用公式获得一个介于-1和1之间的轮廓分数：'
- en: '![Changes in unsupervised model performance](img/B04881_09_Formula2.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![无监督模型性能的变化](img/B04881_09_Formula2.jpg)'
- en: If a data point is well assigned to its cluster, then it is much more dissimilar
    on average to other clusters. Thus, `d'(i)` (the 'second best cluster for i')
    is larger than `d(i)`, and the ratio in the silhouette score formula is near 1\.
    Conversely, if the point is poorly assigned to its cluster, then the value of
    `d'(i)` could be less than `d(i)`, giving a negative value in the numerator of
    the silhouette score formula. Values near zero suggest the point could be reasonably
    assigned in the two clusters equally well. By looking at the distribution of silhouette
    scores over a data set, we can get a sense of how well points are being clustered
    over time.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个数据点很好地分配到其簇中，那么它在平均上与其他簇的相似度要低得多。因此，`d'(i)`（i的“第二好簇”）大于`d(i)`，并且轮廓分数公式中的比率接近1。相反，如果点分配到其簇中的效果不佳，那么`d'(i)`的值可能小于`d(i)`，在轮廓分数公式的分子中给出负值。接近零的值表明该点可以合理地分配到两个簇中。通过查看数据集上轮廓分数的分布，我们可以了解点随时间聚类的效果。
- en: Finally, we might use a bootstrap approach, where we rerun the clustering algorithm
    many times and ask how often two points end up in the same cluster. The distribution
    of these cluster co-occurrences (between 0 and 1) can also give a sense of how
    stable the assignment is over time.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可能使用一种自举方法，即多次重新运行聚类算法，并询问两个点有多少次最终落在同一个簇中。这些簇共现的分布（介于0和1之间）也可以给出关于分配随时间稳定性的感觉。
- en: Like clustering models, dimensionality reduction techniques also do not lend
    themselves easily to a gold standard by which to judge model quality over time.
    However, we can take values such as the principal components vectors of a data
    set and examine their pairwise dissimilarity (for example, using the cosine score
    described in [Chapter 3](ch03.html "Chapter 3. Finding Patterns in the Noise –
    Clustering and Unsupervised Learning"), *Finding Patterns in the Noise – Clustering
    and Unsupervised Learning*) to determine if they are changing significantly. In
    the case of matrix decomposition techniques, we could also look at the reconstruction
    error (for example, averaged squared difference over all matrix elements) between
    the original matrix and the product of the factored elements (such as the *W*
    and *H* matrices in nonnegative matrix factorization).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与聚类模型一样，降维技术也不容易找到一个金标准来衡量模型质量随时间的变化。然而，我们可以取数据集的主成分向量等值，并检查它们的成对差异（例如，使用第3章中描述的余弦分数[Chapter
    3](ch03.html "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised
    Learning")，*在噪声中寻找模式 – 聚类和无监督学习*），以确定它们是否发生了显著变化。在矩阵分解技术的情况下，我们还可以查看原始矩阵与分解元素乘积（例如，非负矩阵分解中的*W*和*H*矩阵）之间的重建误差（例如，所有矩阵元素的平均平方差）。
- en: Iterating on models through A/B testing
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过A/B测试迭代模型
- en: In the examples above and in the previous chapters of this volume, we have primarily
    examined analytical systems in terms of their predictive ability. However, these
    measures do not necessarily ultimately quantify the kinds of outcomes that are
    meaningful to the business, such as revenue and user engagement. In some cases,
    this shortcoming is overcome by converting the performance statistics of a model
    into other units that are more readily understood for a business application.
    For example, in our preceding churn model, we might multiply our prediction of
    'cancel' or 'not-cancel' to generate a predicted dollar amount lost through subscriber
    cancellation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例以及本书的前几章中，我们主要从预测能力方面分析了分析系统。然而，这些指标并不一定最终能衡量出对商业有意义的各种结果，例如收入和用户参与度。在某些情况下，这种不足可以通过将模型的性能统计数据转换为更易于商业应用理解的单位来克服。例如，在我们的先前的流失率模型中，我们可能会将我们对“取消”或“未取消”的预测乘以，以生成通过订阅者取消而损失的预测金额。
- en: In other scenarios, we are fundamentally unable to measure a business outcome
    using historical data. For example, in trying to optimize a search model, we can
    measure whether a user clicked a recommendation and whether they ended up purchasing
    anything after clicking. Through such retrospective analysis, we can only optimize
    the order of the recommendations the user was actually presented on a web page.
    However, it might be that with a better search model, we would have presented
    the user a completely different set of recommendations, which would have also
    led to greater click-through rates and revenue. However, we cannot quantify this
    hypothetical scenario, meaning we need alternative methods to assess algorithms
    as we improve them.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，我们基本上无法使用历史数据来衡量业务结果。例如，在尝试优化搜索模型时，我们可以衡量用户是否点击了推荐，以及他们点击后是否购买了任何东西。通过这种回顾性分析，我们只能优化用户在网页上实际看到的推荐顺序。然而，可能的情况是，通过更好的搜索模型，我们会向用户展示一组完全不同的推荐，这将导致更高的点击率和收入。然而，我们无法量化这种假设情景，这意味着我们需要在改进算法时采用替代方法来评估算法。
- en: 'One way to do so is through the process of experimentation, or A/B testing,
    which takes its name from the concept of comparing outcomes from test subjects
    (for example, customers) randomly assigned to treatment (for example, a search
    recommendation algorithm) A and B to determine which method generates the best
    result. In practice, there may be many more than two treatments, and the experiment
    can be randomized at the level of users, sessions (such as periods between login
    and logout on a website), products, or other units. While a truly comprehensive
    discussion of A/B testing is outside the scope of this chapter, we refer interested
    readers to more extensive references (Bailey, Rosemary A. *Design of comparative
    experiments*. Vol. 25\. Cambridge University Press, 2008; Eisenberg, Bryan, and
    John Quarto-vonTivadar. *Always be testing: The complete guide to Google website
    optimizer*. John Wiley & Sons, 2009; Finger, Lutz, and Soumitra Dutta. *Ask, Measure,
    Learn: Using Social Media Analytics to Understand and Influence Customer Behavior*.
    " O''Reilly Media, Inc.", 2014).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 做这件事的一种方法是通过实验过程，或A/B测试，这个名字来源于比较随机分配到治疗A和B的测试对象（例如，客户）的结果的概念，以确定哪种方法产生最佳结果。在实践中，可能存在许多超过两种治疗方法的情况，实验可以在用户、会话（例如，在网站上的登录和登出之间的时间段）、产品或其他单位上进行随机化。虽然对A/B测试的全面讨论超出了本章的范围，但我们建议感兴趣的读者参考更广泛的参考资料（Bailey,
    Rosemary A. *比较实验设计*. 第25卷. 剑桥大学出版社，2008年；Eisenberg, Bryan和John Quarto-vonTivadar.
    *始终在测试：Google网站优化器的完整指南*. 约翰·威利父子出版社，2009年；Finger, Lutz和Soumitra Dutta. *询问、衡量、学习：使用社交媒体分析来理解和影响客户行为*.
    "O'Reilly媒体公司"，2014年）。
- en: Experimental allocation – assigning customers to experiments
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验分配 - 将客户分配到实验中
- en: 'You have an algorithm that you wish to improve—how can you compare its performance
    improving a metric (such as revenue, retention, engagement) in comparison to an
    existing model (or no predictive model at all)? In this comparison, we want to
    make sure to remove all potential confounding factors other than the two (or more)
    models themselves. This idea underlies the concept of experimental randomization:
    if we randomly assign customers (for example) to receive search recommendations
    from two different models, any variation in customer demographics such as age,
    income, and subscription tenure should be roughly the same between the groups.
    Thus, when we compare the performance of models over time between groups following
    this random allocation, differences in performance of the algorithms can be attributed
    to the models themselves, as we have already accounted for other potential sources
    of variation through this randomization.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个希望改进的算法——如何比较其性能在提高一个指标（如收入、留存、参与度）方面与现有模型（或根本不使用预测模型）相比？在这个比较中，我们想要确保除了两个（或更多）模型本身之外，移除所有可能混淆的因素。这个想法是实验随机化的概念：如果我们随机将客户（例如）分配给接收来自两个不同模型的搜索推荐，客户人口统计学的任何变化，如年龄、收入和订阅期限，应该在两组之间大致相同。因此，当我们比较根据这种随机分配在时间上两组之间模型的性能时，算法性能的差异可以归因于模型本身，因为我们已经通过这种随机化考虑了其他潜在变化来源。
- en: How can we guarantee that users are assigned to experimental groups randomly?
    One possibility is to assign each member a random number between 0 and 1, and
    split them based on whether this number is greater than 0.5 or not. However, this
    method might have the downside that it will be difficult to replicate our analysis
    since the random number assigned to a user could change. Alternatively, we often
    will have user IDs, random numbers assigned to a given account. Assuming the form
    of this number is sufficiently randomized, we could take the modulus of this number
    (the remainder when divided by a fixed denominator, such as 2) and assign users
    to the two groups based on the modulus (for example, if 2, this would be 0 or
    1 based on if the account ID is even or odd). Thus, users are randomly allocated
    to the two groups, but we can easily recreate this assignment in the future.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何保证用户被随机分配到实验组？一种可能性是为每个成员分配一个介于0和1之间的随机数，并根据这个数是否大于0.5来划分他们。然而，这种方法可能存在缺点，即由于分配给用户的随机数可能会改变，因此很难复制我们的分析。另一种可能性是，我们通常会有用户ID，这是分配给特定账户的随机数。假设这个数的格式足够随机，我们可以取这个数的模（除以一个固定的除数，如2）的余数，并根据模数（例如，如果除数是2，这将基于账户ID是偶数还是奇数来决定是0还是1）将用户分配到两组。因此，用户被随机分配到两组，但我们可以轻松地在未来重新创建这种分配。
- en: We might also consider whether we always want a simple random stratification.
    In the ad-targeting example discussed previously, we are actually more concerned
    with the performance of the algorithm on nonsubscribers, rather than the existing
    users who would comprise most of a random allocation in our example. Thus, depending
    upon our objective, we may want to consider randomly allocating stratified samples
    in which we oversampled some accounts to compensate for inherent skew in the data.
    For example, we would enforce a roughly equal number of accounts per country to
    offset geographical bias toward a more populous region, or equal numbers of teenage
    and adult users for a service with primarily younger users.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可能考虑是否总是想要一个简单的随机分层。在之前讨论的广告定位示例中，我们实际上更关心算法在非订阅者上的性能，而不是在我们的例子中可能构成随机分配大部分的现有用户。因此，根据我们的目标，我们可能需要考虑随机分配分层样本，其中我们对某些账户进行过采样，以补偿数据中固有的偏差。例如，我们可能会强制执行每个国家账户数量大致相等，以抵消对人口较多的地区的地理偏差，或者为以年轻用户为主的服务提供相同数量的青少年和成年用户。
- en: In addition to randomly assigning users to receive an experience (such as search
    recommendations or ads sent through e-mail) dictated by a particular algorithm,
    we often need a control, a baseline to which to compare the results. In some cases,
    the control might be the outcome expected with no predictive model used at all.
    In others, we are comparing the old predictive model to a new version.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除了根据特定算法随机分配用户以接收某种体验（如搜索推荐或通过电子邮件发送的广告）之外，我们通常还需要一个控制组，以便将其结果进行比较。在某些情况下，控制组可能是没有使用任何预测模型时预期的结果。在其他情况下，我们是在比较旧的预测模型与新的版本。
- en: Deciding a sample size
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决定样本大小
- en: Now that we know what we are trying to test and have a way to randomly assign
    users, how should we determine how many to allocate to an experiment? If we have
    a control and several experimental conditions, how many users should we assign
    to each group? If our predictive model relies upon user interaction (for example,
    gauging the performance of a search model requires the user to visit a website)
    that may not be guaranteed to occur for every member of the experimental population,
    how many activities (for example, searches) do we need to accumulate to judge
    the success of our experiment? These questions all concern making estimates of
    effect size and experimental power.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了我们试图测试的内容，并且有了一种随机分配用户的方法，那么我们应该如何确定分配给实验的人数呢？如果我们有一个对照组和几个实验条件，我们应该将多少用户分配给每个组？如果我们的预测模型依赖于用户交互（例如，评估搜索模型的性能需要用户访问网站）而这种情况可能并不保证在实验人群中的每个成员都会发生，我们需要积累多少活动（例如，搜索）来判断实验的成功？这些问题都涉及到对效应大小和实验功效的估计。
- en: As you may recall from statistics, in a controlled experiment we are trying
    to determine whether the differences in outcomes between two populations (for
    example, the revenue generated from groups of users in our experimental evaluation
    of different prediction algorithms for ad targeting) are more likely due to random
    change or actual differences in the performance of an algorithm. These two options
    are also known as the null hypothesis, often represented by *H0* (that is, there
    is no difference between the two groups) and the alternative represented by *H1*.
    To determine whether an effect (for example, the difference in revenue between
    the two groups) is explained by random chance, we compare this effect to a distribution
    (frequently the t-distribution, for reasons we will discuss below) and ask what
    is the likelihood of observing this effect or greater if the true effect is **0**.
    This value—the cumulative probability of an effect greater than or equal to the
    observed given an assumption of no effect—is known as the p-value, to which we
    often apply a threshold such as 0.05 (in the example below, this is indicated
    by the shaded region on the left side of the standard normal distribution).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能从统计学中回忆起来，在控制实验中，我们试图确定两个群体（例如，我们实验评估不同广告定位预测算法时用户群体产生的收入）之间的结果差异更有可能是由于随机变化还是算法性能的实际差异。这两个选项也被称为零假设，通常用
    *H0* 表示（即，两组之间没有差异）和用 *H1* 表示的备择假设。为了确定一个效应（例如，两组之间的收入差异）是否由随机机会解释，我们将这个效应与一个分布（以下我们将讨论的原因）进行比较，并询问如果真实效应是
    **0**，观察这个效应或更大的可能性是什么。这个值——在无效应假设下，观察到的效应大于或等于给定效应的累积概率——被称为 p 值，我们通常将其应用于一个阈值，例如
    0.05（在下面的例子中，这由标准正态分布左侧的阴影区域表示）。
- en: '![Deciding a sample size](img/B04881_09_06.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![决定样本大小](img/B04881_09_06.jpg)'
- en: When we are assessing this statistical significance, we may encounter two kinds
    of errors because of the fact that any measurement of effect is subject to uncertainty.
    We never really know the true value of an effect, but rather measure this real,
    unknown effect with some error. First, we could inaccurately declare a result
    to be statistically significant when it is not. This is known as type I error
    (false positive). Secondly, we could fail to declare a result statistically significant
    when it actually is (also known as Type II error, or false negative).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们评估这种统计显著性时，我们可能会遇到两种类型的错误，因为任何效应的测量都受到不确定性的影响。我们永远不知道效应的真正值，而是用一些误差来测量这个真实、未知的效果。首先，我们可能会错误地宣布一个结果具有统计学意义，而实际上它并没有。这被称为第一类错误（假阳性）。其次，我们可能会未能宣布一个结果具有统计学意义，而实际上它确实具有（也称为第二类错误，或假阴性）。
- en: 'We can ask the question how many samples we need to declare a particular effect
    (for example, revenue difference) significant, if there really were a difference
    between our two populations. While exact applications may vary, we will assume
    for illustration that the two groups are sufficiently large and that any difference
    between measured average values (such as revenue or click through rate) follows
    a normal distribution, which is due to the **Law of Large Numbers**. We can then
    evaluate this difference using the t-distribution, which approximate the standard
    normal distribution for large samples but does not require that we know the population
    mean and variance, just the mean and variance of a sample. Then, calculating the
    necessary number of samples requires just using the following formula (for a t-test
    between samples of whose variance is potentially unequal, also known as Welch''s
    t-test):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提出问题，如果我们两个群体之间确实存在差异（例如，收入差异），我们需要多少样本才能宣布这种特定效果（例如，收入差异）具有显著性。虽然具体应用可能有所不同，但为了说明，我们将假设两组足够大，并且测量平均值的任何差异（例如收入或点击率）都遵循正态分布，这是由于**大数定律**。然后，我们可以使用t分布来评估这种差异，它近似于大样本的标准正态分布，但不需要我们知道总体均值和方差，只需知道样本的均值和方差。然后，计算所需样本数量只需要使用以下公式（对于方差可能不等的双样本t检验，也称为Welch的t检验）：
- en: '![Deciding a sample size](img/B04881_09_Formula3.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![决定样本大小](img/B04881_09_Formula3.jpg)'
- en: 'Here, *Y* is the average effect (for example, revenue per customer) of each
    group, and S (the standard deviation) is given by the following equation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*Y* 是每个组的平均效果（例如，每位客户的收入），而S（标准差）由以下方程给出：
- en: '![Deciding a sample size](img/B04881_09_Formula4.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![决定样本大小](img/B04881_09_Formula4.jpg)'
- en: 'Here, *S[1]* and *S[2]* are the sample variances, and *n[1]* and *n[2]* are
    the sample sizes of the two groups. So if we want to be able to detect a difference
    of 10, for example, with a p-value of 0.05, we solve for the sample size at which
    the t-statistic under the null yields a false positive 5% of the time (for which
    we use the normal approximation of 1.64, which is the value at which the cumulative
    distribution function of the standard normal distribution assumes the value of
    0.05). We can solve:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*S[1]* 和 *S[2]* 是两组的样本方差，而 *n[1]* 和 *n[2]* 是两组的样本大小。因此，如果我们想能够检测到10的差异，例如，以0.05的p值，我们求解在零假设下t统计量导致5%的假阳性（我们使用1.64的正常近似值，这是标准正态分布累积分布函数值为0.05的值）。我们可以求解：
- en: '![Deciding a sample size](img/B04881_09_Formula5.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![决定样本大小](img/B04881_09_Formula5.jpg)'
- en: Thus, given values for the variance of the groups in our experiment, we can
    plug in different values of *n* for the two groups and see if they are sufficient
    to fulfill the inequality. For this application, we might estimate the variance
    by looking at historical data for revenue among users of a given sample size.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，给定实验中各组方差的值，我们可以为两组输入不同的 *n* 值，并查看它们是否足够满足不等式。为此应用，我们可能通过查看给定样本大小的用户历史收入数据来估计方差。
- en: If you look at the right-hand side of the preceding equation carefully, you
    will see that (assuming reasonably similar sample variances, which is not an unreasonable
    assumption in many large scale experiments such as those conducted on consumer
    website) this value will be determined by the smaller of *n[1],n[2]*, since as
    we increase one sample size the term containing it tends toward 0\. Thus, we often
    achieve optimal power by assigning equal sample sizes to both groups. This fact
    is important in considering how to decide the relative sizes of our control and
    experimental cells. Take an example in which we have three version of an ad-targeting
    algorithm, along with no algorithm at all as a control, and measure the resulting
    click through rate. Based on the preceding calculation, we need to decide what
    our main question is. If we want to know if any algorithm is better than no algorithm,
    we should assign users evenly between control and any of the three algorithm variants.
    However, if we instead want to decide which algorithm is best compared to control,
    we want equal numbers of users in all four cells, so that control and each treatment
    are of approximately equal size.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察前面方程的右侧，你会看到（假设样本方差合理相似，这在许多大规模实验中并不算不合理，例如在消费者网站上进行的实验），这个值将由 *n[1]*
    和 *n[2]* 中的较小者决定，因为当我们增加一个样本大小时，包含它的项趋向于0。因此，我们通常通过将两组的样本量分配得相等来实现最佳功效。这个事实在考虑如何决定我们的控制组和实验组的相对大小时非常重要。以一个例子来说明，我们有三版广告定位算法，以及没有任何算法作为控制，并测量产生的点击率。根据前面的计算，我们需要决定我们的主要问题是什么。如果我们想知道是否有任何算法比没有算法更好，我们应该在控制和任何三个算法变体之间平均分配用户。然而，如果我们想决定哪个算法与控制相比最好，我们希望所有四个单元格中的用户数量相等，这样控制和每个处理都是大约相等的大小。
- en: Note that the preceding calculation assumes we are interested in a fixed difference
    of 10 in response between the two groups. We could also just ask whether there
    is any difference at all (for example, the difference is not zero). The choice
    depends whether any lift represented by the algorithm is valuable or whether a
    fixed improvement is necessary to achieve the business goal at hand.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前面的计算假设我们感兴趣的是两组之间固定的10个单位的响应差异。我们也可以简单地询问是否存在任何差异（例如，差异不是零）。这个选择取决于算法所代表的任何提升是否具有价值，或者是否需要固定的改进来实现当前的业务目标。
- en: Multiple hypothesis testing
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多重假设检验
- en: 'The last topic we will cover is somewhat subtle, but important due to the fact
    that with models with numerous tunable parameters and algorithm variations, we
    may often be performing a large number of hypothesis tests within a single A/B
    experiment. While we might evaluate each test at a significance of 0.05, if we
    perform 20 such evaluations, we have a probability of 20*0.05 = 1 (or almost certainty)
    of finding some significant result, even if it is in truth random noise. This
    issue, known as *Multiple Hypothesis Testing*, requires that we may need to recalibrate
    our significance threshold. The simplest way to do so is to divide the p-value
    threshold we use (for example, 0.05) by the number of tests performed (20) to
    obtain a new threshold for significance. This is known as Bonferonni Correction
    (Dunn, Olive Jean. "Estimation of the medians for dependent variables." *The Annals
    of Mathematical Statistics* (1959): 192-197; Dunnett, Charles W. "A multiple comparison
    procedure for comparing several treatments with a control." *Journal of the American
    Statistical Association* 50.272 (1955): 1096-1121) and, while correct, may be
    overly conservative in some scenarios. It assumes that we want a type I (false
    positive) rate of zero. However, in exploratory analyses, we often can accept
    some nonzero false positive rate as long as we are reasonably sure that a majority
    of the significant results are replicable. In this scenario, a **familywise error
    rate** (**FWER**) approach may be preferable. While a discussion of FWER is outside
    the scope of this chapter, we refer the interested reader to references on the
    subject (Shaffer, Juliet Popper. "Multiple hypothesis testing." *Annual review
    of psychology* 46 (1995): 561; Toothaker, Larry E. *Multiple comparison procedures*.
    No. 89\. Sage, 1993).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将要讨论的最后一个话题有些微妙，但很重要，因为当模型具有众多可调参数和算法变体时，我们可能在单个A/B实验中执行大量假设测试。虽然我们可能以0.05的显著性水平评估每个测试，但如果我们执行20次这样的评估，我们找到一些显著结果的可能性为20*0.05
    = 1（或几乎肯定），即使它实际上是随机噪声。这个问题被称为*多重假设检验*，它要求我们可能需要重新调整我们的显著性阈值。这样做最简单的方法是将我们使用的p值阈值（例如，0.05）除以执行的测试次数（20），以获得新的显著性阈值。这被称为Bonferroni校正（Dunn,
    Olive Jean. "Estimation of the medians for dependent variables." *The Annals of
    Mathematical Statistics* (1959): 192-197; Dunnett, Charles W. "A multiple comparison
    procedure for comparing several treatments with a control." *Journal of the American
    Statistical Association* 50.272 (1955): 1096-1121），虽然正确，但在某些情况下可能过于保守。它假设我们希望类型I（假阳性）率为零。然而，在探索性分析中，只要我们合理确信大多数显著结果是可以复制的，我们通常可以接受一些非零的假阳性率。在这种情况下，一种**全家族错误率**（**FWER**）的方法可能更可取。虽然FWER的讨论超出了本章的范围，但我们建议感兴趣的读者参考该主题的参考文献（Shaffer,
    Juliet Popper. "Multiple hypothesis testing." *Annual review of psychology* 46
    (1995): 561; Toothaker, Larry E. *Multiple comparison procedures*. No. 89\. Sage,
    1993)。'
- en: Guidelines for communication
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 沟通指南
- en: Now that we have covered debugging, monitoring and iterative testing of predictive
    models, we close with a few notes on communicating results of algorithms to a
    more general audience.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了预测模型的调试、监控和迭代测试，我们将以一些关于如何将算法结果传达给更广泛受众的注意事项作为结尾。
- en: Translate terms to business values
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将术语翻译成商业价值
- en: 'In this text, we frequently discuss evaluation statistics or coefficients whose
    interpretations are not immediately obvious, nor the difference in numerical variation
    for these values. What does it mean for a coefficient to be larger or smaller?
    What does an AUC mean in terms of customer interactions predicted? In any of these
    scenarios, it is useful to translate the underlying value into a business metric
    in explaining their significance to non-technical colleagues: for example, coefficients
    in a linear model represent the unit change in an outcome (such as revenue) for
    a 1-unit change in particular input variable. For transformed variables, it may
    be useful to relate values such as the log-odds (from logistic regression) to
    a value such as doubling the probability of an event. Additionally, as discussed
    previously, we may need to translate the outcome we predict (such as a cancelation)
    into a financial amount to make its implication clear. This sort of conversion
    is useful not only in communicating the impact of a predictive algorithm, but
    also in clarifying priorities in planning. If the development time for an algorithm
    (whose cost might be approximated by the salaries of the employees involved) is
    not offset by the estimated benefit of its performance, then this suggests it
    is not a useful application from a business perspective.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们经常讨论评估统计量或系数，其解释可能并不明显，也不明显这些值的数值变化差异。系数较大或较小意味着什么？AUC在预测客户互动方面意味着什么？在任何这些情况下，将基础值转换为业务指标以向非技术同事解释其重要性都是有用的：例如，线性模型中的系数表示特定输入变量变化1个单位时结果（如收入）的单位变化。对于转换变量，将诸如对数几率（来自逻辑回归）之类的值与诸如事件概率加倍之类的值相关联可能是有用的。此外，如前所述，我们可能需要将预测的结果（如取消）转换为财务金额，以使其含义清晰。此类转换不仅有助于传达预测算法的影响，而且有助于在规划中明确优先事项。如果一个算法的开发时间（其成本可能由涉及员工的工资来估算）不能抵消其性能的估计收益，那么这表明从业务角度来看，这不是一个有用的应用。 '
- en: Visualizing results
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化结果
- en: While not all algorithms we have discussed are amenable to visualization, many
    have elements that may be plotted for clarity. For example, regression coefficients
    can be compared using a barplot, and tree models may be represented visually by
    the branching decision points leading to a particular outcome. Such graphics help
    to turn inherently mathematical objects into more understandable results as well
    as provide ongoing insight into the performance of models, as detailed previously.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们讨论的并非所有算法都适合可视化，但许多算法都有可能通过绘图来清晰展示其元素。例如，可以通过条形图比较回归系数，而树模型可以通过导致特定结果的分支决策点进行视觉表示。此类图形有助于将本质上数学的对象转化为更易于理解的结果，同时也提供了对模型性能的持续洞察，如前所述。
- en: As a practical example of building such a service, this chapter's case study
    will walk through the generation of a custom dashboard as an extension of the
    prediction service we built in [Chapter 8](ch08.html "Chapter 8. Sharing Models
    with Prediction Services"), *Sharing Models with Prediction Services*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 作为构建此类服务的实际示例，本章的案例研究将介绍如何生成一个自定义仪表板，作为我们在[第8章](ch08.html "第8章。通过预测服务共享模型")中构建的预测服务的扩展，即*通过预测服务共享模型*。
- en: 'Case Study: building a reporting service'
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究：构建一个报告服务
- en: In [Chapter 8](ch08.html "Chapter 8. Sharing Models with Prediction Services"),
    *Sharing Models with Prediction Services*, we created a prediction service that
    uses MongoDB as a backend database to store model data and predictions. We can
    use this same database as a source to create a reporting service. Like the separation
    of concerns between the CherryPy server and the modeling service application that
    we described in [Chapter 8](ch08.html "Chapter 8. Sharing Models with Prediction
    Services"), *Sharing Models with Prediction Services*, a reporting service can
    be written without any knowledge of how the information in the database is generated,
    making it possible to generate a flexible reporting infrastructure as the modeling
    code may change over time. Like the prediction service, our reporting service
    has a few key components.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](ch08.html "第8章。通过预测服务共享模型")中，我们创建了一个使用MongoDB作为后端数据库来存储模型数据和预测的预测服务。我们可以使用这个相同的数据库作为创建报告服务的源。就像我们在[第8章](ch08.html
    "第8章。通过预测服务共享模型")中描述的CherryPy服务器和建模服务应用程序之间的关注点分离一样，报告服务可以编写而不需要了解数据库中信息的生成方式，这使得在建模代码可能随时间变化的情况下，可以生成灵活的报告基础设施。就像预测服务一样，我们的报告服务有几个关键组件。
- en: The server that will receive requests for the output of the reporting service.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将接收报告服务输出请求的服务器。
- en: The reporting application run by the server, which receive requests from the
    server and routes them to display the correct data.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器运行的报告应用程序，它接收来自服务器的请求并将它们路由以显示正确的数据。
- en: The database from which we retrieve the information required to make a plot.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从中检索用于制作图表所需信息的数据库。
- en: Charting systems that render the plots we are interested in for the end user.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为最终用户渲染我们感兴趣的图表的图表系统。
- en: Let us walk through an example of each component, which will illustrate how
    they fit together.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个分析每个组件的示例，这将说明它们是如何相互配合的。
- en: The report server
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 报告服务器
- en: Our server code is very similar to the `CherryPy` server we used in [Chapter
    8](ch08.html "Chapter 8. Sharing Models with Prediction Services"), *Sharing Models
    with Prediction Services*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务器代码与我们在[第8章](ch08.html "第8章。通过预测服务共享模型")中使用的`CherryPy`服务器非常相似，*通过预测服务共享模型*。
- en: Note
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This example was inspired by the code available at [https://github.com/adilmoujahid/DonorsChoose_Visualization](https://github.com/adilmoujahid/DonorsChoose_Visualization).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子受到了[https://github.com/adilmoujahid/DonorsChoose_Visualization](https://github.com/adilmoujahid/DonorsChoose_Visualization)上可用的代码的启发。
- en: 'The only difference is that instead of starting the modelservice application,
    we use the server to start the reportservice, as you can see in the `main` method:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别是，我们不是启动`modelservice`应用程序，而是使用服务器来启动`reportservice`，正如你在`main`方法中看到的那样：
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can test this server by simple running the following on the command line:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在命令行上简单地运行以下命令来测试这个服务器：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You should see the server begin to log information to the console as we observed
    previously for the modelserver.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到服务器开始将信息记录到控制台，就像我们之前观察到的`modelserver`一样。
- en: The report application
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 报告应用程序
- en: 'In the application code, which is also a Flask application like the model service
    we built in [Chapter 8](ch08.html "Chapter 8. Sharing Models with Prediction Services"),
    *Sharing Models with Prediction Services*, we need a few additional pieces of
    information that we didn''t use previously. The first is path variable to specify
    the location of the JavaScript and CSS files that we will need when we construct
    our charts, which are specified using the commands:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序代码中，它也是一个像我们在[第8章](ch08.html "第8章。通过预测服务共享模型")中构建的模型服务一样的Flask应用程序，*通过预测服务共享模型*，我们需要一些之前没有使用过的额外信息。首先是路径变量，用于指定我们构建图表时所需的JavaScript和CSS文件的位置，这些文件使用以下命令指定：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We also need to specify where to find the HTML pages that we render to the
    user containing our charts with the argument:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要指定包含我们图表的HTML页面的位置，这些图表将渲染给用户，使用以下参数：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'When we initialize our application, we will pass both of these as variables
    to the constructor:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们初始化应用程序时，我们将这两个变量作为变量传递给构造函数：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To return this application when called by the server, we simply return app
    in the `reportservice` function:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务器调用此应用程序时，我们只需在`reportservice`函数中返回`app`即可：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We now just need to specify the response of the application to requests forwarded
    by the server. The first is simply to render a page containing our charts:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要指定服务器转发的请求的应用程序响应。首先是简单地渲染一个包含我们图表的页面：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The template in this example is taken from [https:// github.com/keen/dashboards](https://%20github.com/keen/dashboards),
    an open source project that provides reusable templates for generating quick dashboards.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本例中的模板来自 [https://github.com/keen/dashboards](https://%20github.com/keen/dashboards)，这是一个开源项目，提供了用于快速生成仪表板的可重用模板。
- en: 'The second route will allow us to retrieve the data we will use to populate
    the chart. This is not meant to be exposed to the end user (though you would see
    a text dump of all the JSONS in our collection if you navigated to this endpoint
    in your browser): rather it is used by the client-side JavaScript code to retrieve
    the information to populate the charts. First we need to start the mongodb application
    in another terminal window using:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个路由将允许我们检索用于填充图表的数据。这不应该暴露给最终用户（尽管如果您在浏览器中导航到这个端点，您会看到我们集合中所有 JSON 的文本输出）：它是由客户端
    JavaScript 代码用来检索填充图表所需信息的。首先，我们需要在另一个终端窗口中使用以下命令启动 mongodb 应用程序：
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, in our code, we need to specify the MongoDB parameters to use in accessing
    our data. While we could have passed these as parameters in our URL, for simplicity
    in this example, we will just hard-code them at the top of the reportservice code
    to point to the results of bulk scoring the bank dataset we used to train our
    Spark Logistic Regression Model in [Chapter 8](ch08.html "Chapter 8. Sharing Models
    with Prediction Services"), *Sharing Models with Prediction Services*:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在我们的代码中，我们需要指定用于访问数据的 MongoDB 参数。虽然我们可以将这些参数作为 URL 中的参数传递，但为了简化本例，我们将在 reportservice
    代码的顶部直接硬编码它们，以便指向我们在第 8 章 [分享预测服务中的模型](ch08.html "第 8 章。使用预测服务分享模型") 中训练 Spark
    逻辑回归模型所使用的银行数据集的批量评分结果：
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note that we could just as easily have pointed to a remote data source, rather
    than one running on our machine, by changing the `MONGODB_HOST` parameter. Recall
    that when we stored the results of bulk scoring, we saved records with two elements,
    the score and the original data row. In order to plot our results, we will need
    to extract the original data row and present it along with the score using the
    following code:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们同样可以通过更改 `MONGODB_HOST` 参数，将数据源指向远程服务器，而不是我们机器上运行的本地数据源。回想一下，当我们存储批量评分的结果时，我们保存了包含两个元素的记录，即评分和原始数据行。为了绘制我们的结果，我们需要提取原始数据行，并使用以下代码将其与评分一起展示：
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now that we have all of our scored records in a single array of json strings,
    we can plot them using a bit of JavaScript and HTML.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将所有评分记录存储在一个单独的 json 字符串数组中，我们可以使用一点 JavaScript 和 HTML 来绘制它们。
- en: The visualization layer
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化层
- en: The final piece we will need is the client-side JavaScript code used to populate
    the charts, and some modifications to the `index.html` file to make use of the
    charting code. Let us look at each of these in turn.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要的是用于填充图表的客户端 JavaScript 代码，以及一些修改 `index.html` 文件以使用图表代码的修改。让我们依次查看这些内容。
- en: 'The chart generating code is a JavaScript function contained in the file `report.js`
    that you can find under `templates/assets/js` in the project directory for [Chapter
    9](ch09.html "Chapter 9. Reporting and Testing – Iterating on Analytic Systems"),
    *Reporting and Testing – Iterating on Analytic Systems*. We begin this function
    by calling for the data we need and waiting for it to be retrieved using the asynchronous
    function `d3.queue()`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 生成图表的代码是一个包含在 `report.js` 文件中的 JavaScript 函数，您可以在项目目录下的 `templates/assets/js`
    中找到它，对应于第 9 章 [报告和测试 - 在分析系统中迭代](ch09.html "第 9 章。报告和测试 - 在分析系统中迭代")。我们从这个函数开始，调用所需的数据，并使用异步函数
    `d3.queue()` 等待其检索：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Notice that this URL is the same endpoint that we specified earlier in the report
    application to retrieve the data from MongoDB. The `d3_queue` function calls this
    endpoint and waits for the data to be returned before running the `runReport`
    function. While a more extensive discussion is outside the scope of this text,
    `d3_queue` is a member of the `d3` library ([https://d3js.org/](https://d3js.org/)),
    a popular visualization framework for the javascript language.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个 URL 与我们在报告应用程序中之前指定的相同端点，用于从 MongoDB 中检索数据。`d3_queue` 函数调用此端点，并在运行 `runReport`
    函数之前等待数据返回。虽然更深入的讨论超出了本文的范围，但 `d3_queue` 是 `d3` 库的一个成员 ([https://d3js.org/](https://d3js.org/))，这是一个流行的
    JavaScript 语言可视化框架。
- en: 'Once we have retrieved the data from our database, we need to specify how to
    plot it using the `runReport` function. First we will declare the data associated
    with the function:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从数据库中检索到数据，我们需要指定如何使用`runReport`函数来绘制它。首先，我们将声明与函数相关的数据：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Though it will not be apparent until we visually examine the resulting chart,
    the `crossfilter` library ([http://square.github.io/crossfilter/](http://square.github.io/crossfilter/))
    allows us to highlight a subset of data in one plot and simultaneously highlight
    the corresponding data in another plot, even if the dimensions plotted are different.
    For example, imagine we had a histogram of ages for particular `account_ids` in
    our system, and a scatterplot of click-through-rate versus `account_id` for a
    particular ad campaign. The `Crossfilter` function would allow us to select a
    subset of the scatterplot points using our cursor and, at the same time, filter
    the histogram to only those ages that correspond to the points we have selected.
    This kind of filtering is very useful for drilling down on particular sub-segments
    of data. Next we will generate the dimensions we will use when plotting:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然直到我们视觉上检查生成的图表时才会明显，但`crossfilter`库（[http://square.github.io/crossfilter/](http://square.github.io/crossfilter/））允许我们在一个图表中突出显示数据的一个子集，并同时突出显示另一个图表中对应的数据，即使绘制的维度不同。例如，想象一下，我们有一个系统中特定`account_ids`的年龄直方图，以及一个特定广告活动的点击率与`account_id`的散点图。`Crossfilter`函数将允许我们使用我们的光标选择散点图的点的一个子集，同时过滤直方图，只显示与所选点对应的年龄。这种过滤对于深入特定数据子段非常有用。接下来，我们将生成我们在绘图时将使用的维度：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Each of these functions takes the input data and returns the requested data
    field. The dimension contains all the data points in a column and forms the superset
    from which we will filter when examining subsets of data. Using these dimensions,
    we construct groups of unique values that we can use, for example, in plotting
    histograms:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数中的每一个都接受输入数据并返回请求的数据字段。维度包含列中的所有数据点，并形成我们将用于检查数据子集的超集。使用这些维度，我们构建了唯一的值组，我们可以使用，例如，在绘制直方图时：
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For some of our dimensions, we want to add values representing that maximum
    or minimum, which we use in plotting ranges of numerical data:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的一些维度，我们想要添加表示最大值或最小值的值，我们在绘制数值数据的范围时使用这些值：
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we can specify our chart objects using `dc` ([https://dc-js.github.io/dc.js/](https://dc-js.github.io/dc.js/)),
    a charting library that uses `d3` and crossfilter to create interactive visualizations.
    The `#` tag given to each chart constructor specifies the ID we will use to reference
    it when we insert it into the HTML template later. We construct the charts using
    the following code:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`dc`（[https://dc-js.github.io/dc.js/](https://dc-js.github.io/dc.js/）），一个使用`d3`和crossfilter创建交互式可视化的图表库，来指定我们的图表对象。每个图表构造函数给出的`#`标签指定了我们在将其插入HTML模板时将使用的ID。我们使用以下代码构建图表：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we specify the dimension and axes of these charts:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们指定这些图表的维度和轴：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We just need a call to render in order to display the result:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要一个渲染调用来显示结果：
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we need to modify our `index.html` file in order to display our charts.
    If you open this file in a text editor, you will notice several places where we
    have a `<div>` tag such as:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要修改我们的`index.html`文件以显示我们的图表。如果你在这个文件中打开一个文本编辑器，你会注意到几个地方有`<div>`标签，例如：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This is where we need to place our charts using the following IDs that we specified
    in the preceding JavaScript code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们需要使用以下ID放置图表的位置，这些ID我们在前面的JavaScript代码中已经指定了：
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, in order to render the charts, we need to include our `javascript`
    code in the `<script>` arguments at the bottom of the HTML document:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了渲染图表，我们需要在HTML文档底部的`<script>`标签中包含我们的`javascript`代码：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, you should be able to navigate to the URL to which the `CherryPy` server
    points, `localhost:5000`, should now display the charts like this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该能够导航到`CherryPy`服务器指向的URL，即`localhost:5000`，现在应该显示如下图表：
- en: '![The visualization layer](img/B04881_09_07.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![可视化层](img/B04881_09_07.jpg)'
- en: Crossfilter chart highlighting other dimensions of subset of users in a given
    age range.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定年龄范围内的用户子集的其他维度上突出显示的Crossfilter图表。
- en: The data is drawn from the bank default example we used to train our model service
    in [Chapter 8](ch08.html "Chapter 8. Sharing Models with Prediction Services"),
    *Sharing Models with Prediction Services*. You can see that by selecting a subset
    of data points in the age distribution, we highlight the distribution of occupations,
    bank balance, and educations for these same users. This kind of visualization
    is very useful for drill-down diagnosis of problems points (as may be the case,
    for example, if a subset of data points is poorly classified by a model). Using
    these few basic ingredients you can now not only scale model training using the
    prediction service in [Chapter 8](ch08.html "Chapter 8. Sharing Models with Prediction
    Services"), *Sharing Models with Prediction Services*, but also visualize its
    behavior for end users using a reporting layer.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 数据来源于我们在[第8章](ch08.html "第8章。与预测服务共享模型")中使用的训练模型服务的银行违约示例，*与预测服务共享模型*。您可以看到，通过选择年龄分布中的数据点子集，我们突出了这些相同用户的职业、银行余额和教育分布。这种可视化对于深入诊断问题点（例如，如果数据点的子集被模型错误分类）非常有用。使用这些基本成分，您现在不仅可以使用[第8章](ch08.html
    "第8章。与预测服务共享模型")中提到的预测服务进行模型训练，还可以通过报告层来可视化其行为。
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned several strategies for monitoring the performance
    of predictive models following initial design and looked at a number of scenarios
    where the performance or components of the model change over time. As part of
    the process of refining models, we examined A/B testing strategies and illustrated
    how to perform basic random allocation and estimate the sample sizes needed to
    measure improvement. We also demonstrated how to leverage the infrastructure from
    our prediction service to create dashboard visualizations for monitoring, which
    can easily be extended for other use cases.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了在初始设计之后监控预测模型性能的几种策略，并观察了模型性能或组件随时间变化的多种场景。作为模型优化过程的一部分，我们探讨了A/B测试策略，并说明了如何执行基本的随机分配以及估计测量改进所需的样本量。我们还展示了如何利用我们的预测服务基础设施来创建用于监控的仪表板可视化，这可以轻松扩展到其他用例。
