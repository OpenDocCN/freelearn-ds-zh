- en: Chapter 6.  Building Scalable Machine Learning Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。 构建可扩展的机器学习管道
- en: 'The ultimate goal of machine learning is to make a machine that can automatically
    build models from data without requiring tedious and time-consuming human involvement
    and interaction. Therefore, this chapter guides the readers through creating some
    practical and widely used machine learning pipelines and applications using Spark
    MLlib and Spark ML. Both APIs will be described in detail, and a baseline use
    case will also be covered for both. Then we will focus on scaling up the ML application
    so that it can cope with increasing data loads. After reading all the sections
    in this chapter, readers will be able to differentiate between both APIs and select
    the one which best fits their requirements. In a nutshell, the following topics
    will be covered throughout this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的最终目标是使机器能够自动从数据中构建模型，而无需繁琐和耗时的人类参与和交互。 因此，本章将指导读者通过使用Spark MLlib和Spark
    ML创建一些实用和广泛使用的机器学习管道和应用。 将详细描述这两个API，并且还将为两者都涵盖一个基线用例。 然后，我们将专注于扩展ML应用程序，以便它可以应对不断增加的数据负载。
    阅读本章的所有部分后，读者将能够区分这两个API，并选择最适合其要求的API。 简而言之，本章将涵盖以下主题：
- en: Spark machine learning pipeline APIs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark机器学习管道API
- en: Cancer-diagnosis pipeline with Spark
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark Core进行癌症诊断管道
- en: Cancer-prognosis pipeline with Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行癌症预后管道
- en: Market basket analysis with Spark Core
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark Core进行市场篮子分析
- en: OCR pipeline with Spark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark中的OCR管道
- en: Topic modeling using Spark MLlib and ML
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark MLlib和ML进行主题建模
- en: Credit-risk-analysis pipeline with Spark
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行信用风险分析管道
- en: Scaling the ML pipelines
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展ML管道
- en: Tips and performance considerations
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示和性能考虑
- en: Spark machine learning pipeline APIs
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark机器学习管道API
- en: MLlib's goal is to make practical machine learning (ML) scalable and easy. Spark
    introduces the pipeline API for the easy creation and tuning of practical ML pipelines.
    As discussed in [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Knowledge
    through Feature Engineering*, a practical ML pipeline involves a sequence of data
    collection, pre-processing, feature extraction, feature selection, model fitting,
    validation, and model evaluation stages. For example, classifying the text documents
    might involve text segmentation and cleaning, extracting features, and training
    a classification model with cross-validation toward tuning. Most ML libraries
    are not designed for distributed computation, or they do not provide native support
    for pipeline creation and tuning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib的目标是使实用的机器学习（ML）可扩展且易于使用。 Spark引入了管道API，用于轻松创建和调整实用的ML管道。 如[第4章](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "第4章。通过特征工程提取知识")中所讨论的，实用的ML管道涉及一系列数据收集，预处理，特征提取，特征选择，模型拟合，验证和模型评估阶段。 例如，对文档进行分类可能涉及文本分割和清理，提取特征以及使用交叉验证训练分类模型。
    大多数ML库都不是为分布式计算而设计的，或者它们不提供管道创建和调整的本地支持。
- en: Dataset abstraction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集抽象
- en: As described in [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark"), *Introduction to Data
    Analytics with Spark*, when running SQL from within another programming language,
    the results return as a DataFrame. A DataFrame is a distributed collection of
    data organized into named columns. A Dataset, on the other hand, is an interface
    that tries to provide the benefits of RDDs out of the Spark SQL.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d "第1章。使用Spark进行数据分析简介")中所述，当在另一种编程语言中运行SQL时，结果将返回为DataFrame。
    DataFrame是一个分布式的数据集合，组织成命名列。 另一方面，数据集是一种接口，试图提供Spark SQL中RDD的好处。
- en: A Dataset can be constructed from JVM objects, which can be used both in Scala
    and Java. In the Spark pipeline design, a dataset is represented by Spark SQL's
    Dataset. An ML pipeline involves a number of the sequence of Dataset transformations
    and models. Each transformation takes an input dataset and outputs the transformed
    dataset, which becomes the input to the next stage.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以由JVM对象构建，这些对象可以在Scala和Java中使用。 在Spark管道设计中，数据集由Spark SQL的数据集表示。 ML管道涉及一系列数据集转换和模型。
    每个转换都接受输入数据集并输出转换后的数据集，这成为下一阶段的输入。
- en: 'Consequently, the data import and export are the start and end point of an
    ML pipeline. To make these easier, Spark MLlib and Spark ML provide import and
    export utilities of a Dataset, DataFrame, RDD, and model, for several application-specific
    types, including:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据导入和导出是ML管道的起点和终点。 为了使这些更容易，Spark MLlib和Spark ML提供了一些特定于应用程序的类型的数据集，DataFrame，RDD和模型的导入和导出实用程序，包括：
- en: LabeledPoint for classification and regression
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分类和回归的LabeledPoint
- en: LabeledDocument for cross-validation and **Latent Dirichlet Allocation** (**LDA**)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于交叉验证和潜在狄利克雷分配（LDA）的LabeledDocument
- en: Rating and ranking for collaborative filtering
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协同过滤的评分和排名
- en: However, real datasets usually contain numerous types, such as user ID, item
    IDs, labels, timestamps, and raw records.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，真实数据集通常包含许多类型，例如用户ID，项目ID，标签，时间戳和原始记录。
- en: Unfortunately, the current utilities of Spark implementation cannot easily handle
    datasets consisting of these types, especially time-series datasets. If you recall
    the section *Machine learning pipeline - an overview*, in [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Feature
    through Feature Engineering*, feature transformation usually forms the majority
    of a practical ML pipeline. A feature transformation can be viewed as appending
    or dropping a new column created from existing columns.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当前的Spark实现工具无法轻松处理包含这些类型的数据集，特别是时间序列数据集。如果您回忆起[第4章](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "第4章。通过特征工程提取知识")中的*机器学习管道-概述*部分，*通过特征工程提取特征*，特征转换通常占据实际ML管道的大部分。特征转换可以被视为从现有列创建新列或删除新列。
- en: 'In *Figure 1*, *Text processing for machine learning model*, you will see that
    the text tokenizer breaks a document into a bag of words. After that, the TF-IDF
    algorithm converts a bag of words into a feature vector. During the transformations,
    the labels need to be preserved for the model-fitting stage:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图1*中，*用于机器学习模型的文本处理*，您将看到文本标记器将文档分解为一袋词。之后，TF-IDF算法将一袋词转换为特征向量。在转换过程中，标签需要被保留以用于模型拟合阶段：
- en: '![Dataset abstraction](img/00034.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![数据集抽象](img/00034.jpeg)'
- en: 'Figure 1: Text processing for machine learning model (DS indicates data sources)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：用于机器学习模型的文本处理（DS表示数据源）
- en: If you recall *Figure 5* and *Figure 6* from [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Feature
    through Feature Engineering*, ID, text, and words are conceded during the transformations
    steps. They are useful in making predictions and model inspection. However, they
    are actually unnecessary for model fitting to state. According to a Databricks
    blog on ML Pipeline at [https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html](https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html),
    it doesn't provide much information if the prediction dataset only contains the
    predicted labels.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您回忆起[第4章](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "第4章。通过特征工程提取知识")中的*图5*和*图6*，*通过特征工程提取特征*，在转换步骤中，ID、文本和单词都被让步。它们在进行预测和模型检查时非常有用。但是，它们实际上对于模型拟合来说是不必要的。根据Databricks关于ML管道的博客[https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html](https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html)，如果预测数据集只包含预测标签，它并没有提供太多信息。
- en: Consequently, if you want to inspect the prediction metrics, such as the accuracy,
    precision, recall, weighted true positives, and weighted false positives, it is
    quite useful to look at the predicted labels along with the raw input text and
    tokenized words. The same recommendation also applies to other machine learning
    applications using Spark ML and Spark MLlib, too.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您想检查预测指标，如准确性、精确度、召回率、加权真正例和加权假正例，查看预测标签以及原始输入文本和标记化单词是非常有用的。相同的建议也适用于使用Spark
    ML和Spark MLlib的其他机器学习应用。
- en: Therefore, an easy conversion between RDDs, Dataset, and DataFrames has been
    made possible for in-memory, disk, or external data sources such as Hive and Avro.
    Although creating new columns from existing columns is easy with user-defined
    functions, the manifestation of Dataset is a lazy operation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，已经实现了在内存、磁盘或外部数据源（如Hive和Avro）之间进行RDD、数据集和数据框之间的简单转换。虽然使用用户定义的函数从现有列创建新列很容易，但数据集的表现是一种延迟操作。
- en: In contrast, the Dataset supports only some standard data types. However, to
    increase the usability and for making a better fit for the machine learning model,
    Spark has also added the support for the Vector type as a user-defined type that
    supports both dense and sparse feature vectors under the `mllib.linalg.DenseVector`
    and `mllib.linalg.Vector`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，数据集仅支持一些标准数据类型。然而，为了增加可用性并使其更适合机器学习模型，Spark还添加了对向量类型的支持，作为一种支持密集和稀疏特征向量的用户定义类型，支持`mllib.linalg.DenseVector`和`mllib.linalg.Vector`。
- en: Tip
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Complete DataFrame, Dataset, and RDD examples in Java, Scala, and Python can
    be found under the `examples/src/main/` folder under the Spark distribution. Interested
    readers can refer to Spark SQL's user guide at [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)
    to learn more about DataFrame, Dataset, and the operations they support.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在Spark分发的`examples/src/main/`文件夹下找到Java、Scala和Python的完整DataFrame、Dataset和RDD示例。感兴趣的读者可以参考Spark
    SQL的用户指南[http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)了解更多关于DataFrame、Dataset以及它们支持的操作。
- en: Pipeline
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道
- en: Spark provides the pipeline API under Spark ML. As previously stated, a pipeline
    is comprised of a sequence of stages consisting of transformers and estimators.
    There are two basic types of pipeline stages, called Transformer and Estimator.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在Spark ML下提供了管道API。如前所述，管道由一系列阶段组成，包括转换器和估计器。管道阶段有两种基本类型，称为转换器和估计器。
- en: A transformer takes a dataset as an input and produces an augmented dataset
    as the output so that the output can be fed to the next step. For example, **Tokenizer**
    and **H**ashingTF**** are two transformers. Tokenizer transforms a dataset with
    text into a dataset with tokenized words. A HashingTF, on the other hand, produces
    the term frequencies. The concept of tokenization and HashingTF is commonly used
    in text mining and text analytics.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器将数据集作为输入，并产生增强的数据集作为输出，以便将输出馈送到下一步。例如，**Tokenizer**和**H**ashingTF****是两个转换器。
    Tokenizer将具有文本的数据集转换为具有标记化单词的数据集。另一方面，HashingTF产生术语频率。标记化和HashingTF的概念通常用于文本挖掘和文本分析。
- en: On the contrary, an estimator must be the first on the input dataset to produce
    a model. In this case, the model itself will be used as the transformer for transforming
    the input dataset into the augmented output dataset. For example, a **Logistic
    Regression** or linear regression can be used as an estimator after fitting the
    training dataset with corresponding labels and features.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，估计器必须是输入数据集的第一个，以产生模型。在这种情况下，模型本身将被用作转换器，将输入数据集转换为增强的输出数据集。例如，在拟合训练数据集与相应的标签和特征之后，可以使用**逻辑回归**或线性回归作为估计器。
- en: 'After that, it produces a logistic or linear regression model. It implies that
    developing a pipeline is easy and simple. Well, all you need is to declare the
    required stages, then configure the related stage''s parameters; finally, chain
    them in a pipeline object, as shown in *Figure 2*:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，它产生一个逻辑或线性回归模型。这意味着开发管道是简单而容易的。好吧，你所需要做的就是声明所需的阶段，然后配置相关阶段的参数；最后，将它们链接在一个管道对象中，如*图2*所示：
- en: '![Pipeline](img/00068.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![管道](img/00068.jpeg)'
- en: 'Figure 2: Spark ML pipeline model using logistic regression estimator (DS indicates
    data store and the steps inside the dashed line only happen during pipeline fitting)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：使用逻辑回归估计器的Spark ML管道模型（DS表示数据存储，虚线内的步骤仅在管道拟合期间发生）
- en: If you look at *Figure 2*, the fitted model consists of a tokenizer, a hashingTF
    feature extractor, and a fitted logistic regression model. The fitted pipeline
    model acts as a transformer that can be used for prediction, model validation,
    model inspection, and finally, model deployment. However, increasing the performance
    in terms of prediction accuracy, the model itself needs to be tuned. We will discuss
    more about how to tune a machine learning model in [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Model*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看一下*图2*，拟合模型由分词器、哈希TF特征提取器和拟合的逻辑回归模型组成。拟合的管道模型充当了可以用于预测、模型验证、模型检查和最终模型部署的转换器。然而，为了提高预测准确性，模型本身需要进行调整。我们将在[第7章](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "第7章。调整机器学习模型")*调整机器学习模型*中更多地讨论如何调整机器学习模型。
- en: To show the pipelining technique more practically, the following section shows
    how to create a practical pipeline for cancer diagnosis using Spark ML and MLlib.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更实际地展示流水线技术，以下部分展示了如何使用Spark ML和MLlib创建癌症诊断的实际管道。
- en: Cancer-diagnosis pipeline with Spark
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark的癌症诊断管道
- en: In this section, we will look at how to develop a cancer-diagnosis pipeline
    with Spark ML and MLlib. A real dataset will be used to predict the probability
    of breast cancer, which is almost curable since the culprit genes for this cancer
    type have already been identified successfully. However, we would like to argue
    about this cancer type since in third world countries in Africa and Asia it is
    still a lethal disease.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看看如何使用Spark ML和MLlib开发癌症诊断管道。将使用真实数据集来预测乳腺癌的概率，这种癌症几乎是可以治愈的，因为这种癌症类型的罪魁祸首基因已经成功地被确定。然而，我们想要讨论一下这种癌症类型，因为在非洲和亚洲的第三世界国家，它仍然是一种致命疾病。
- en: Tip
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'We suggest the readers keep an open mind about the outcome or the status of
    this disease, as we will just show how the Spark ML API can be used to predict
    cancer by integrating and combining datasets from the Wisconsin Breast Cancer
    (original), **Wisconsin Diagnosis Breast Cancer** (**WDBC**), and **Wisconsin
    Prognosis Breast Cancer** (**WPBC**) datasets from the following website: [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议读者对这种疾病的结果或状态保持开放的态度，因为我们将展示Spark ML API如何通过整合和组合来自威斯康星乳腺癌（原始）、威斯康星诊断乳腺癌（WDBC）和威斯康星预后乳腺癌（WPBC）数据集的数据来预测癌症，这些数据集来自以下网站：[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)。
- en: Breast-cancer-diagnosis pipeline with Spark
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark乳腺癌诊断管道
- en: In this subsection, we will develop a step-by-step cancer diagnosis pipeline.
    The steps include a background study of breast cancer, dataset collection, data
    exploration, problem formalization, and Spark-based implementation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将开发一个逐步的癌症诊断管道。步骤包括对乳腺癌的背景研究、数据集收集、数据探索、问题形式化和基于Spark的实现。
- en: Background study
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 背景研究
- en: According to Salama et al. (*Breast Cancer Diagnosis on Three Different Datasets
    Using Multi-Classifiers, International Journal of Computer and Information Technology*
    (*2277 - 0764*) *Volume 01- Issue 01, September 2012*), breast cancer comes in
    fourth position after thyroid cancer, melanoma, and lymphoma, in women between
    20 and 29 years.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Salama等人的研究（*使用多分类器在三个不同数据集上进行乳腺癌诊断，国际计算机和信息技术杂志*（*2277-0764*）*第01-01期，2012年9月*），乳腺癌在20至29岁的女性中排名第四，仅次于甲状腺癌、黑色素瘤和淋巴瘤。
- en: Breast cancer develops from breast tissue that mutates due to several factors
    including sex, obesity, alcohol, family history, lack of physical exercise, and
    so on. Furthermore, according to statistics by **The Centre for Diseases Control
    and Prevention** (**TCDCP**) ([https://www.cdc.gov/cancer/breast/statistics/](https://www.cdc.gov/cancer/breast/statistics/)),
    in 2013, a total of 230,815 women and 2,109 men were diagnosed with breast cancer
    across the USA. Unfortunately, 40,860 women and 464 men died from it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌是由乳腺组织突变引起的，原因包括性别、肥胖、酒精、家族史、缺乏体育锻炼等。此外，根据**疾病控制和预防中心**（**TCDCP**）的统计数据（[https://www.cdc.gov/cancer/breast/statistics/](https://www.cdc.gov/cancer/breast/statistics/)），2013年，美国共有230,815名妇女和2,109名男性被诊断出患有乳腺癌。不幸的是，40,860名妇女和464名男性死于此病。
- en: Research has found that about 5-10% cases are due to some genetic inheritance
    from parents, including BRCA1 and BRCA2 gene mutations and so on. An early diagnosis
    could help to save thousands of breast cancer sufferers around the globe. Although
    the culprit genes have been identified, chemotherapy has not proven very effective.
    Gene silencing is becoming popular, but more research is required.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 研究发现，约5-10%的病例是由父母的一些遗传因素引起的，包括BRCA1和BRCA2基因突变等。早期诊断可以帮助拯救全球数千名乳腺癌患者。尽管罪魁祸首基因已经被确定，但化疗并不十分有效。基因沉默正在变得流行，但需要更多的研究。
- en: As mentioned previously, the learning tasks in machine learning depend heavily
    on classification, regression, and clustering techniques. Moreover, traditional
    data-mining techniques are being applied along with these machine learning techniques,
    which are the most essential and important task. Therefore, by integrating with
    Spark, these applied techniques are gaining wide acceptance and adoption in the
    area of biomedical data analytics. Furthermore, numerous experiments are being
    performed on biomedical datasets using multiclass and multilevel classifiers and
    feature-selection techniques toward cancer diagnosis and prognosis.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，机器学习中的学习任务严重依赖分类、回归和聚类技术。此外，传统的数据挖掘技术正在与这些机器学习技术一起应用，这是最基本和重要的任务。因此，通过与Spark集成，这些应用技术在生物医学数据分析领域得到了广泛的接受和应用。此外，正在使用多类和多级分类器和特征选择技术对生物医学数据集进行大量实验，以进行癌症诊断和预后。
- en: Dataset collection
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集收集
- en: '**The Cancer Genome Atlas** (**TCGA**), **Catalogue of Somatic Mutations in
    Cancer** (**COSMIC**), **International Cancer Genome Consortium** (**ICGC**) is
    the most widely used cancer and tumor-related dataset for research purposes. These
    data sources have been curated from world-renowned institutes such as MIT, Harvard,
    Oxford, and others. However, the datasets that are available are unstructured,
    complex, and multidimensional. Therefore, we cannot use them directly to show
    how to apply large-scale machine learning techniques to them. The reason is that
    these datasets require lots of pre-processing and cleaning, which requires lots
    of pages.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**癌症基因组图谱**（**TCGA**），**癌症体细胞突变目录**（**COSMIC**），**国际癌症基因组联盟**（**ICGC**）是最广泛使用的癌症和肿瘤相关数据集，用于研究目的。这些数据来源已经从麻省理工学院、哈佛大学、牛津大学等世界知名研究所进行了整理。然而，这些可用的数据集是非结构化的、复杂的和多维的。因此，我们不能直接使用它们来展示如何将大规模机器学习技术应用于它们。原因是这些数据集需要大量的预处理和清洗，这需要大量的页面。'
- en: After practising this application, we believe readers will be able to apply
    the same technique for any kind of biomedical dataset for cancer diagnosis. Due
    to the page limitation, we should use simpler datasets that are structured and
    manually curated for machine learning application development and of course, many
    of them show good classification accuracy.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过练习这个应用程序，我们相信读者将能够将相同的技术应用于任何类型的生物医学数据集，用于癌症诊断。由于页面限制，我们应该使用结构化和手动策划的简单数据集，用于机器学习应用开发，当然，其中许多显示出良好的分类准确性。
- en: For example, the Wisconsin Breast Cancer datasets from the UCI Machine Learning
    Repository available at [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)
    contains data that was donated by researchers at the University of Wisconsin and
    includes measurements from digitized images of a fine-needle aspiration of a breast
    mass. The values represent characteristics of the cell nuclei present in the digital
    image described in the following subsection.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，来自UCI机器学习库的威斯康星州乳腺癌数据集，可在[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)上找到，这些数据是由威斯康星大学的研究人员捐赠的，并包括来自乳腺肿块细针穿刺的数字图像的测量。这些值代表数字图像中细胞核的特征，如下一小节所述。
- en: Tip
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'To read more about the Wisconsin breast cancer data, refer to the authors''
    publication: *Nuclear feature extraction for breast tumor diagnosis. IS&T/SPIE
    1993 International Symposium on Electronic Imaging: Science and Technology, volume
    1905, pp 861-870 by W.N. Street, W.H. Wolberg, and O.L. Mangasarian, 1993*.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 关于威斯康星州乳腺癌数据的更多信息，请参考作者的出版物：*乳腺肿瘤诊断的核特征提取。IS＆T/SPIE 1993年国际电子成像研讨会：科学与技术，卷1905，第861-870页，作者为W.N.
    Street，W.H. Wolberg和O.L. Mangasarian，1993年*。
- en: Dataset description and preparation
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集描述和准备
- en: As shown in the **Wisconsin Breast Cancer Dataset** (**WDBC**) manual available
    at [https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names),
    the Clump thickness benign cells tend to be grouped in monolayers, while cancerous
    cells are often grouped in multilayers. Therefore, all the features and fields
    mentioned in the manual are important and before applying the machine learning
    technique since these features will help to identify if a particular cell is cancerous
    or not.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如**威斯康星州乳腺癌数据集**（**WDBC**）手册所示，可在[https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names)上找到，肿块厚度良性细胞往往成片状分组，而癌细胞通常成多层分组。因此，在应用机器学习技术之前，手册中提到的所有特征和字段都很重要，因为这些特征将有助于确定特定细胞是否患癌。
- en: The breast cancer data includes 569 samples of cancer biopsies, each with 32
    features. One feature is the identification number of the patient, another is
    the cancer diagnosis, labeled as benign or malignant, and the remainder are numeric-valued
    is called bio-assay that was identified in the molecular laboratory works. The
    diagnosis is coded as either M to indicate malignant or B to indicate benign with
    regard to the cancer diagnosis.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌数据包括569个癌症活检样本，每个样本有32个特征。一个特征是患者的识别号码，另一个是癌症诊断，标记为良性或恶性，其余的是数值型的生物测定，是在分子实验室工作中确定的。诊断编码为M表示恶性或B表示良性。
- en: 'The Class distribution is as follows: Benign: 357 (62.74%) and Malignant: 212
    (37.25%). The training and test dataset will be prepared following the dataset
    description given here. The 30 numeric measurements include the mean, standard
    error, and worst, which is the mean of the three largest values. Field 3 is the
    mean radius, 13 is the Radius SE, and 23 is the Worst Radius. The 10 real-valued
    features are computed for each cell nucleus by means of different characteristics
    of the digitized cell nuclei described in *Table 1, 10 real-valued features and
    their descriptions*:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 类别分布如下：良性：357（62.74%）和恶性：212（37.25%）。训练和测试数据集将按照此处给出的数据集描述进行准备。30个数值测量包括均值、标准误差和最坏值，即三个最大值的均值。字段3是均值半径，13是半径SE，23是最坏半径。通过对数字化的细胞核的不同特征进行计算，为每个细胞核计算了10个实值特征，这些特征描述在*表1，10个实值特征及其描述*中：
- en: '| **No.** | **Value** | **Explanation** |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| **编号** | **数值** | **解释** |'
- en: '| 1 | Radius | Mean of distances from center to points on the perimeter |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 半径 | 中心到周边点的距离的平均值 |'
- en: '| 2 | Texture | Standard deviation of gray-scale values |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 纹理 | 灰度值的标准偏差 |'
- en: '| 3 | Perimeter | The perimeter of the cell nucleus |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 周长 | 细胞核的周长 |'
- en: '| 4 | Area | Area of the cell nucleus covering the perimeter |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 面积 | 细胞核覆盖周长的面积 |'
- en: '| 5 | Smoothness | Local variation in radius lengths |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 光滑度 | 半径长度的局部变化 |'
- en: '| 6 | Compactness | Calculated as follows: (Perimeter)^2 / area - 1.0 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 紧凑性 | 计算如下：(周长)^2 / 面积 - 1.0 |'
- en: '| 7 | Concavity | Severity of concave portions of the contour |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 凹度 | 轮廓凹陷部分的严重程度 |'
- en: '| 8 | Concave points | Number of concave portions of the contour |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 凹点 | 轮廓的凹陷部分的数量 |'
- en: '| 9 | Symmetry | Indicates if the cell structure is symmetrical |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 对称性 | 表示细胞结构是否对称 |'
- en: '| 10 | Fractal dimension | Calculated as: coastline approximation - 1 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 分形维数 | 计算如下：海岸线近似 - 1 |'
- en: 'Table 1: 10 real-valued features and their descriptions'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：10个实值特征及其描述
- en: 'All feature values are recorded with four significant digits and there are
    no missing or NULL values. Therefore, we don''t need to perform any data cleaning.
    However, from the previous description, it''s really difficult for someone to
    get any good knowledge of the data. For example, you are unlikely to know how
    each field relates to benign or malignant masses unless you are an oncologist.
    These patterns will be revealed as we continue the machine learning process. A
    sample snapshot of the dataset is shown in *Figure 3*:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所有特征值都记录了四个有效数字，没有缺失或空值。因此，我们不需要进行任何数据清理。但是，从前面的描述中，很难让某人获得有关数据的任何良好知识。例如，除非您是肿瘤学家，否则您不太可能知道每个字段与良性或恶性肿块的关系。随着我们继续进行机器学习过程，这些模式将被揭示。数据集的样本快照如*图3*所示：
- en: '![Dataset description and preparation](img/00115.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![数据集描述和准备](img/00115.jpeg)'
- en: 'Figure 3: Snapshot of the data (partial)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：数据快照（部分）
- en: Problem formalization
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题形式化
- en: '*Figure 4*, *The breast cancer diagnosis and prognosis pipeline model*, describes
    the proposed breast cancer diagnosis model. The model consists of two phases,
    namely, the training and testing phases:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4*，*乳腺癌诊断和预后管道模型*，描述了提出的乳腺癌诊断模型。该模型包括两个阶段，即训练和测试阶段：'
- en: 'The training phase includes four steps: data collection, pre-processing, feature
    extraction, and feature selection'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练阶段包括四个步骤：数据收集、预处理、特征提取和特征选择
- en: The testing phase includes the same four steps as the training phase with the
    addition of the classification step
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试阶段包括与训练阶段相同的四个步骤，另外还有分类步骤
- en: 'In the data-collection step, first the pre-processing is done to check if there
    is an unwanted value or any values are missing. We have already mentioned that
    there are no missing values. However, it is always good practice to check, since
    even the unwanted value of a special character could halt the whole training process.
    After that, the feature engineering step is done through the feature extraction
    and selection process for determining the correct input vector for the subsequent
    logistic or linear regression classifier:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据收集步骤中，首先进行预处理，以检查是否存在不需要的值或任何缺失值。我们已经提到没有缺失值。但是，检查是一种良好的做法，因为即使特殊字符的不需要值也可能中断整个训练过程。之后，通过特征提取和选择过程进行特征工程步骤，以确定适用于后续逻辑或线性回归分类器的正确输入向量：
- en: '![Problem formalization](img/00105.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![问题形式化](img/00105.jpeg)'
- en: 'Figure 4: The breast cancer diagnosis and prognosis pipeline model'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：乳腺癌诊断和预后管道模型
- en: This helps to make a decision regarding the class associated to the pattern
    vectors. Based on either feature selection or feature extraction, the dimensionality
    reduction technique is accomplished. However, please note that we will not use
    any formal dimensionality reduction algorithms to develop this application. For
    more on dimensionality reduction, you can refer to the *Dimensionality reduction*
    section in [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Knowledge
    through Feature Engineering*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于对与模式向量相关联的类做出决定。基于特征选择或特征提取，完成了降维技术。但是，请注意，我们不会使用任何正式的降维算法来开发这个应用程序。有关降维的更多信息，您可以参考[第4章](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "第4章。通过特征工程提取知识")中的*降维*部分，*通过特征工程提取知识*。
- en: In the classification step, a logistic regression classifier is applied to get
    the best result for the diagnosis and prognosis of the tumor.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类步骤中，应用逻辑回归分类器以获得肿瘤诊断和预后的最佳结果。
- en: Developing a cancer-diagnosis pipeline with Spark ML
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spark ML开发癌症诊断管道
- en: As mentioned previously, the details of the attributes found in the WDBC dataset
    at [https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names)
    include patient ID, diagnosis (M = malignant, B = benign), and 10 real-valued
    features are computed for each cell nucleus, as described in *Table 1*, *10 real-valued
    features and their description*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在WDBC数据集中找到的属性的详细信息在[https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names)包括患者ID、诊断（M
    = 恶性，B = 良性）和为每个细胞核计算的10个实值特征，如*表1*、*10个实值特征及其描述*所述。
- en: These features are computed from a digitized image of a **fine needle aspiration**
    (**FNA**) of a breast mass, since we have enough knowing about the dataset. In
    this subsection, we will look at how to develop a breast cancer diagnosis machine
    learning pipeline step-by-step including taking the input of the dataset to prediction
    in the 10 steps described in *Figure 4*, as a data workflow.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征是从乳腺肿块的**细针穿刺**（**FNA**）的数字化图像计算出来的，因为我们对数据集有足够的了解。在本小节中，我们将逐步看看如何开发乳腺癌诊断机器学习流水线，包括在*图4*中描述的10个步骤中从数据集输入到预测的数据工作流程。
- en: '**Step 1: Import the necessary packages/libraries/APIs**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步：导入必要的包/库/API
- en: 'Here is the code to import the packages:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是导入包的代码：
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2: Initialize Spark session**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步：初始化Spark会话
- en: 'A Spark session can be initialized with the help of the following code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下代码初始化Spark会话：
- en: '[PRE1]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here we set the application name as `BreastCancerDetectionDiagnosis`, and the
    master URL as `local``.` The Spark Context is the entry point of the program.
    Please set these parameters accordingly.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将应用程序名称设置为`BreastCancerDetectionDiagnosis`，主URL设置为`local`。Spark上下文是程序的入口点。请相应地设置这些参数。
- en: '**Step 3: Take the breast cancer data as input and prepare JavaRDD out of the
    data**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步：将乳腺癌数据作为输入并准备JavaRDD
- en: Here is the code to prepare `JavaRDD:`
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是准备`JavaRDD`的代码：
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To learn more about the data, please refer to *Figure 3*: *Snapshot of the
    data (partial*.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于数据的信息，请参考*图3*：*数据快照（部分）*。
- en: '**Step 4: Create LabeledPoint RDDs for regression**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步：为回归创建标记点RDD
- en: 'Create `LabeledPoint` RDDs for diagnosis (B = benign and M= Malignant):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为诊断（B = 良性，M = 恶性）创建`LabeledPoint` RDDs：
- en: '[PRE3]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step 5: Create the Dataset of Row from the linesRDD and show the top features**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步：从linesRDD创建Row数据集并显示顶部特征
- en: 'Here is the code illustrated:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是所示代码：
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following figure shows the top features and their corresponding labels:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了顶部特征及其对应的标签：
- en: '![Developing a cancer-diagnosis pipeline with Spark ML](img/00161.jpeg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark ML开发癌症诊断流水线](img/00161.jpeg)'
- en: 'Figure 5: Top features and their corresponding labels'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：顶部特征及其对应的标签
- en: '**Step 6: Split the Dataset to prepare the training and test sets**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 第6步：拆分数据集以准备训练和测试集
- en: 'Here we split the original data frame into training and test set as 60% and
    40%, respectively. Here, `12345L` is the seed value. This value signifies that
    the split will be the same every time, so that the ML model produces the same
    result in each iteration. We follow the same conversion in each chapter for preparing
    the test and training set:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将原始数据框拆分为训练集和测试集，比例分别为60%和40%。在这里，`12345L`是种子值。这个值表示每次拆分都是相同的，这样ML模型在每次迭代中都会产生相同的结果。我们在每一章中都遵循相同的转换来准备测试和训练集：
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To see a quick snapshot of these two sets just write `trainingData.show()` and
    `testData.show()` for training and test sets, respectively.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速查看这两个集合的快照，只需写`trainingData.show()`和`testData.show()`分别用于训练和测试集。
- en: '**Step 7: Create a Logistic Regression classifier**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 第7步：创建一个逻辑回归分类器
- en: 'Create a logistic regression classifier by specifying the max iteration and
    regression parameter:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定最大迭代次数和回归参数创建一个逻辑回归分类器：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Tip
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Logistic regression typically takes three parameters: the number of max iteration,
    the regression parameter, and the elastic-net regularization. See the following
    lines to get a clearer idea:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归通常需要三个参数：最大迭代次数、回归参数和弹性网络正则化。请参考以下行以更清楚地了解：
- en: '[PRE7]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tip
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The preceding statements create a logistic regression model `lr` with max iteration
    `100`, regression parameter `0.01`, and elastic net parameter `0.4`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 上述语句创建了一个逻辑回归模型`lr`，最大迭代次数为`100`，回归参数为`0.01`，弹性网络参数为`0.4`。
- en: '**Step 8: Create and train the pipeline model**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 第8步：创建和训练流水线模型
- en: 'Here is the code illustrated:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是所示代码：
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here we have created a pipeline whose stages are defined by the logistic regression
    stage, which is also an estimator we have just created. Note that you could try
    creating the Tokenizer and HashingTF stages if you are dealing with a text dataset.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个流水线，其阶段由逻辑回归阶段定义，这也是我们刚刚创建的一个估计器。请注意，如果您处理的是文本数据集，可以尝试创建分词器和HashingTF阶段。
- en: However, in this cancer dataset, all of our values are numeric. Therefore, we
    don't create such stages to be chained to the pipeline.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这个癌症数据集中，所有的值都是数字。因此，我们不创建这样的阶段来链接到流水线。
- en: '**Step 9: Create a Dataset, transform the model and prediction**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 第9步：创建数据集，转换模型和预测
- en: 'Create a Dataset of type Row and transform the model to do the prediction based
    on the test dataset:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个类型为Row的数据集，并根据测试数据集进行预测转换模型：
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Step 10: Show the prediction with prediction precision**'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 第10步：显示预测及预测精度
- en: '[PRE10]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Developing a cancer-diagnosis pipeline with Spark ML](img/00037.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark ML开发癌症诊断流水线](img/00037.jpeg)'
- en: 'Figure 6: Prediction with prediction precision'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：预测及预测精度
- en: '*Figure 7* shows the prediction Dataset for the test set. The print method
    shown essentially generates output, much like the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7*显示了测试集的预测数据集。所示的打印方法本质上生成输出，就像下面的例子一样：'
- en: '![Developing a cancer-diagnosis pipeline with Spark ML](img/00155.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark ML开发癌症诊断管道](img/00155.jpeg)'
- en: 'Figure 7: Sample output toward the prediction. The first value is the feature,
    the second is the label, and the final value is the prediction value'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：朝向预测的样本输出。第一个值是特征，第二个是标签，最后一个值是预测值
- en: 'Now let''s calculate the precision score. We do this by multiplying the counter
    by 100 and then dividing the value against how many predictions were done, as
    follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算精度分数。我们通过将计数器乘以100，然后除以完成的预测数量来做到这一点，如下所示：
- en: '[PRE11]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Therefore, the precision is 100%, which is fantastic. However, if you are still
    unsatisfied or have any confusion, the following chapter will demonstrate how
    you can still tune several parameters so that the prediction accuracy increases,
    as there might have been many false-negative predictions.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，精度为100%，这是很棒的。但是，如果您仍然不满意或有任何困惑，下一章将演示如何调整几个参数，以提高预测准确性，因为可能有许多假阴性预测。
- en: Furthermore, the result might vary on your platform due to the random-split
    nature and dataset processing on your side.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于随机拆分的性质和您一侧的数据集处理，结果可能会有所不同。
- en: Cancer-prognosis pipeline with Spark
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark的癌症预后管道
- en: In the previous section, we showed how to develop a cancer diagnosis pipeline
    for predicting cancer based on two labels (Benign and Malignant). In this section,
    we will look at how to develop a cancer prognosis pipeline with Spark ML and MLlib
    APIs. The **Wisconsin Prognosis Breast Cancer** (**WPBC**) datasets will be used
    to predict the probability of breast cancer toward the prognosis for recurrent
    and non-recurrent tumor cells. Again, the dataset was downloaded from [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Prognostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Prognostic)).
    To understand the problem formalization, please refer to *Figure 1* once again
    as we will follow almost the same stages during the cancer-prognosis pipeline
    development.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们展示了如何开发一个癌症诊断管道，用于基于两个标签（良性和恶性）预测癌症。在本节中，我们将看看如何使用Spark ML和MLlib API开发癌症预后管道。**威斯康星预后乳腺癌**（**WPBC**）数据集将用于预测乳腺癌的概率，以预测复发和非复发的肿瘤细胞。同样，数据集是从[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Prognostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Prognostic))下载的。要了解问题的形式化，请再次参考*图1*，因为在癌症预后管道开发过程中，我们将几乎遵循相同的阶段。
- en: Dataset exploration
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集探索
- en: 'The details of the attributes found in the WPBC dataset in [https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.names](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.names)
    are as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在[https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.names](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.names)中找到的WPBC数据集的属性详细信息如下：
- en: ID number
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ID编号
- en: Outcome (R = recurrent, N = non-recurrent)
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果（R = 复发，N = 非复发）
- en: Time (recurrence time if field 2 => R, disease-free time if field 2 => N)
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间（如果字段2 => R，则为复发时间，如果字段2 => N，则为无病时间）
- en: '3 to 33: Ten real-valued features are computed for each cell nucleus: Radius,
    Texture, Perimeter, Area, Smoothness, Compactness, Concavity, Concave points,
    Symmetry, and Fractal dimension. Thirty-four is Tumor size and thirty-five is
    the Lymph node status, as follows:'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3到33：为每个细胞核计算了十个实值特征：半径、纹理、周长、面积、光滑度、紧凑性、凹度、凹点、对称性和分形维度。三十四是肿瘤大小，三十五是淋巴结状态，如下所示：
- en: 'Tumor size: Diameter of the excised tumor in centimeters'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肿瘤大小：切除肿瘤的直径（厘米）
- en: 'Lymph node status: The number of positive axillary lymph nodes'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 淋巴结状态：阳性腋窝淋巴结的数量
- en: If you compare *Figure 3* and *Figure 9*, you will see that the diagnosis and
    prognosis have the same features, yet the prognosis has two additional features
    (mentioned previously as 34 and 35). Note that these are observed at the time
    of surgery from the year 1988 to 1995 and out of the 198 instances, 151 are non-recurring
    (N) and 47 are recurring (R), as shown in *Figure 8*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您比较*图3*和*图9*，您会发现诊断和预后具有相同的特征，但预后有两个额外的特征（如前面提到的34和35）。请注意，这些是在1988年至1995年手术时观察到的，在198个实例中，有151个是非复发（N），47个是复发（R），如*图8*所示。
- en: 'Of course, a real cancer diagnosis and prognosis dataset today contains many
    other features and fields in a structured or unstructured way:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，今天的真实癌症诊断和预后数据集以结构化或非结构化的方式包含许多其他特征和字段：
- en: '![Dataset exploration](img/00079.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![数据集探索](img/00079.jpeg)'
- en: 'Figure 8: Snapshot of the data (partial)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：数据快照（部分）
- en: Tip
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'For more detailed discussion and meaningful insights, interested readers can
    refer to the following research paper: *The Wisconsin Breast Cancer Problem: Diagnosis
    and DFS time prognosis using probabilistic and generalized regression neural classifiers
    Oncology Reports, special issue Computational Analysis and Decision Support Systems
    in Oncology, last quarter 2005 by Ioannis A. et al. found in the following link:*
    [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.2463&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.2463&rep=rep1&type=pdf).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更详细的讨论和有意义的见解，感兴趣的读者可以参考以下研究论文：*威斯康星乳腺癌问题：使用概率和广义回归神经分类器进行诊断和DFS时间预后，2005年第四季度Ioannis
    A.等人在以下链接中找到：*[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.2463&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.2463&rep=rep1&type=pdf)。
- en: Breast-cancer-prognosis pipeline with Spark ML/MLlib
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark ML/MLlib的乳腺癌预后管道
- en: In this subsection, we will look at how to develop a breast cancer prognosis
    machine learning pipeline step-by-step, including taking the input of the dataset
    to prediction in 10 different steps that are described in *Figure 1*, as a data
    workflow.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将逐步介绍如何开发乳腺癌预后机器学习管道，包括从数据集输入到预测的10个不同步骤，这些步骤在*图1*中有描述，作为数据工作流。
- en: Tip
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Readers are advised to download the dataset and the project files, along with
    the `pom.xml` file for the Maven project configuration, from the Packt materials.
    We have advised how to make the code work in previous chapters, for example, [Chapter
    1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d "Chapter 1. Introduction
    to Data Analytics with Spark"), *Introduction to Data Analytics with Spark*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 建议读者从Packt材料中下载数据集和项目文件，以及Maven项目配置的`pom.xml`文件。我们已经在之前的章节中介绍了如何使代码工作，例如[第1章](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark")，*Spark数据分析简介*。
- en: '**Step 1: Import necessary packages/libraries/APIs**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：导入必要的包/库/API**'
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Step 2: Initialize the necessary Spark environment**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：初始化必要的Spark环境**'
- en: '[PRE13]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here we set the application name as `BreastCancerDetectionPrognosis`, the master
    URL as `local[*]`. The Spark Context is the entry point of the program. Please
    set these parameters accordingly.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将应用程序名称设置为`BreastCancerDetectionPrognosis`，主URL设置为`local[*]`。Spark Context是程序的入口点。请相应地设置这些参数。
- en: '**Step 3: Take the breast cancer data as input and prepare JavaRDD out of the
    data**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：将乳腺癌数据作为输入并准备JavaRDD数据**'
- en: '[PRE14]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Tip
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: To learn more about the data, please refer to *Figure 5* and its description
    and the Dataset exploration subsection.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于数据的信息，请参考*图5*及其描述以及数据集探索子部分。
- en: '**Step 4: Create LabeledPoint RDDs**'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：创建带标签的点RDD**'
- en: 'Create `LabeledPoint` RDDs for the prognosis for N = recurrent and R= non-recurrent,
    respectively, using the following code segments:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码段为N=复发和R=非复发的预后创建`LabeledPoint` RDD：
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Step 5: Create the Dataset from the lines RDD and show the top features**'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：从行RDD创建数据集并显示顶部特征**'
- en: '[PRE16]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The top features and their corresponding labels are shown in *Figure 9*:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部特征及其相应标签显示在*图9*中：
- en: '![Breast-cancer-prognosis pipeline with Spark ML/MLlib](img/00012.jpeg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark ML/MLlib进行乳腺癌预后管道](img/00012.jpeg)'
- en: 'Figure 9: Top features and their corresponding labels'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：顶部特征及其相应标签
- en: '**Step 6: Split the Dataset to prepare the training and test sets**'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：将数据集拆分为训练集和测试集**'
- en: 'Here we split the dataset to test and the training set as 60% and 40%, respectively.
    Please adjust these based on your requirements:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将数据集分为测试集和训练集，比例分别为60%和40%。请根据您的要求进行调整：
- en: '[PRE17]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To see a quick snapshot of these two sets, just write `trainingData.show()`
    and `testData.show()`, for training and test sets, respectively.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速查看这两个集合的快照，只需编写`trainingData.show()`和`testData.show()`，分别用于训练和测试集。
- en: '**Step 7: Create a Logistic Regression classifier**'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：创建逻辑回归分类器**'
- en: 'Create a logistic regression classifier by specifying the max iteration and
    regression parameter:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定最大迭代次数和回归参数创建逻辑回归分类器：
- en: '[PRE18]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Step 8: Create a pipeline and train the pipeline model**'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8：创建管道并训练管道模型**'
- en: '[PRE19]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here, similarly to the diagnosis pipeline, we have created the prognosis pipeline
    whose stages are defined by only the logistic regression, which is again an estimator,
    and of course a stage.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，类似于诊断管道，我们创建了预后管道，其阶段仅由逻辑回归定义，这又是一个估计器，当然也是一个阶段。
- en: '**Step 9: Create a Dataset and transform the model**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9：创建数据集并转换模型**'
- en: 'Create a Dataset and do the transformation to make a prediction based on the
    test dataset:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 创建数据集并进行转换，以基于测试数据集进行预测：
- en: '[PRE20]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Step 10: Show the prediction with prediction precision**'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤10：显示预测及其精度**'
- en: '[PRE21]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Breast-cancer-prognosis pipeline with Spark ML/MLlib](img/00092.jpeg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark ML/MLlib进行乳腺癌预后管道](img/00092.jpeg)'
- en: 'Figure 10: Prediction with prediction precision'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：预测精度
- en: '[PRE22]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This code segment will produce an output similar to that shown in *Figure 7*,
    with different features, labels, and predictions:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将产生类似于*图7*的输出，其中包含不同的特征、标签和预测：
- en: '[PRE23]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Therefore, the precision is almost 100%, which is fantastic. However, depending
    upon the data preparation, you might receive different results.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，精度几乎达到100%，这是非常棒的。然而，根据数据准备的不同，你可能会得到不同的结果。
- en: If you have any confusion, the following chapter demonstrates how to tune parameters
    so that the prediction accuracy increases, as they might have many false-negative
    predictions.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有任何困惑，下一章将演示如何调整参数，以提高预测准确性，因为可能会有许多假阴性预测。
- en: Tip
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In their book titled *Machine Learning with R, Packt Publishing, 2015*, Brett
    Lantz at el. argue that it's possible to eliminate the false negatives completely
    by classifying every mass as malignant, benign, recurrent, or non-recurrent. Obviously,
    this is not a realistic strategy. Still, it illustrates the fact that prediction
    involves striking a balance between the false-positive rate and the false-negative
    rate.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的书中《Machine Learning with R, Packt Publishing, 2015》，Brett Lantz等人认为，通过将每个肿块分类为恶性、良性、复发或非复发，可以完全消除假阴性。显然，这不是一个现实的策略。但是，这说明了预测涉及在假阳性率和假阴性率之间取得平衡的事实。
- en: If you are still unsatisfied, we will be tuning several parameters in [Chapter
    7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d "Chapter 7. Tuning
    Machine Learning Models"), *Tuning Machine Learning Models*, so that the prediction
    accuracy increases toward more sophisticated methods for measuring predictive
    accuracy that can be used to identify places where the error rate can be optimized
    depending on the costs of each type of error.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仍然不满意，我们将在[第7章](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models")中调整多个参数，*调整机器学习模型*，以便预测准确性朝着更复杂的测量预测准确性的方法增加，这些方法可以用于确定可以根据每种错误类型的成本来优化错误率的地方。
- en: Market basket analysis with Spark Core
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark Core进行市场篮分析
- en: In this section, we will look at how to develop a large-scale machine learning
    pipeline in terms of market basket analysis. Other than using the Spark ML and
    MLlib, we will demonstrate how to use Spark Core to develop such an application.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何开发大规模机器学习管道，以进行市场篮分析。除了使用Spark ML和MLlib之外，我们还将演示如何使用Spark Core来开发这样的应用程序。
- en: Background
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 背景
- en: 'In an early paper, *An Efficient Market Basket Analysis Technique with Improved
    MapReduce Framework on Hadoop: An E-commerce Perspective* (available at [http://onlinepresent.org/proceedings/vol6_2012/8.pdf](http://onlinepresent.org/proceedings/vol6_2012/8.pdf)),
    the authors have argued that the **market basket analysis** (**MBA**) technique
    is of substantial importance to everyday business decision, since customers''
    purchase rules can be extracted from the association rules by discovering what
    items they are buying frequently and together. Consequently, purchase rules can
    be revealed for frequent shoppers based on these association rules.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇早期的论文《在Hadoop上改进的MapReduce框架上的高效市场篮分析技术：电子商务视角》（可在[http://onlinepresent.org/proceedings/vol6_2012/8.pdf](http://onlinepresent.org/proceedings/vol6_2012/8.pdf)获取），作者们认为**市场篮分析**（MBA）技术对于日常业务决策非常重要，因为可以通过发现顾客频繁购买和一起购买的物品来提取顾客的购买规则。因此，可以根据这些关联规则为经常购物的顾客揭示购买规则。
- en: You might still be wondering why we need market basket analysis, why it is important,
    and why it is computationally expensive. Well, if you could identify highly specific
    association rules like, for example, if a customer prefers mango or orange jam
    along with their milk or butter, you need to have large-scale transactional data
    to be analyzed and processed. Moreover, some massive chain retailers or supermarkets,
    for example, E-mart (UK), HomePlus (Korea), Aldi (Germany), or Dunnes Stores (Ireland)
    use databases of many millions, or even billions, of transactions in order to
    find the associations among particular items with regard to brand, color, origin,
    or even flavor, to increase the probability of sales and profit.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能仍然想知道为什么我们需要市场篮分析，为什么它很重要，以及为什么它在计算上很昂贵。如果您能够识别高度特定的关联规则，例如，如果顾客喜欢芒果或橙子果酱以及牛奶或黄油，您需要有大规模的交易数据进行分析和处理。此外，一些大型连锁零售商或超市，例如E-mart（英国）、HomePlus（韩国）、Aldi（德国）或Dunnes
    Stores（爱尔兰）使用数百万甚至数十亿的交易数据库，以找到特定物品之间的关联，例如品牌、颜色、原产地甚至口味，以增加销售和利润的可能性。
- en: In this section, we will look at an efficient approach for large-scale market
    basket analysis with Spark libraries. After reading and practising this, you will
    be able to show how the Spark framework lifts the existing single-node pipeline
    to a pipeline usable on a multi-node data-mining cluster. The result is that our
    proposed association-rules mining algorithm can be reused in parallel with the
    same benefits.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨使用Spark库进行大规模市场篮分析的高效方法。阅读并实践后，您将能够展示Spark框架如何将现有的单节点管道提升到可在多节点数据挖掘集群上使用的管道。结果是我们提出的关联规则挖掘算法可以以相同的好处并行重复使用。
- en: We use the acronym SAMBA, for Spark-based Market Basket Analysis, *min_sup*
    for minimum support, and *min_conf* for minimum confidence. We also use the terms
    frequent patterns and frequent itemset interchangeably.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用SAMBA作为Spark-based Market Basket Analysis的缩写，*min_sup*表示最小支持度，*min_conf*表示最小置信度。我们还将频繁模式和频繁项集这两个术语互换使用。
- en: Motivations
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动机
- en: Traditional main memory or disk-based computing and RDBMS are not capable of
    handling ever-increasing large transactional data. Furthermore, as discussed in
    [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d "Chapter 1. Introduction
    to Data Analytics with Spark"), *Introduction to Data Analytics with Spark*, MapReduce
    has several issues with the I/O operation, algorithmic complexity, low-latency,
    and fully disk-based operation. Therefore, finding the null transactions and later
    eliminating them from the future scheme is the initial part of this approach.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的主存储器或基于磁盘的计算和关系型数据库管理系统无法处理不断增加的大规模交易数据。此外，正如[第1章](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "第1章。使用Spark进行数据分析简介")中讨论的，*使用Spark进行数据分析简介*，MapReduce在I/O操作、算法复杂性、低延迟和完全基于磁盘的操作方面存在一些问题。因此，找到空交易并随后从未来方案中消除它们是这种方法的初始部分。
- en: It is quite possible to find all the null transactions by identifying those
    transactions that do not appear against at least one frequent 1-itemset. As already
    mentioned, Spark caches the intermediate data into memory and provides an abstraction
    of **Resilient Distributed Datasets** (**RDDs**), which can be used to overcome
    these issues by making a huge difference, achieving tremendous success in the
    last three years for handling large-scale data in distributed computing systems.
    These successes are promising and motivating examples to explore this research
    work to applying Spark in market basket analysis.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 通过识别那些不出现在至少一个频繁1项集中的交易，很可能找到所有的空交易。正如前面提到的，Spark将中间数据缓存到内存中，并提供**弹性分布式数据集**（RDDs）的抽象，可以通过这种方式克服这些问题，过去三年在处理分布式计算系统中的大规模数据方面取得了巨大成功。这些成功是有希望的，也是激励人的例子，可以探索将Spark应用于市场篮分析的研究工作。
- en: Exploring the dataset
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索数据集
- en: 'Please download the grocery dataset for the market basket analysis from [https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/groceries.csv](https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/groceries.csv).
    The first five rows of the raw `grocery.csv` data are as follows in *Figure 11*.
    These lines indicate 10 separate grocery-store transactions. The first transaction
    includes four items: citrus fruit, semi-finished bread, margarine, and ready soups.
    In comparison, the third transaction includes only one item, whole milk:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 请从[https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/groceries.csv](https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/groceries.csv)下载购物篮分析的杂货数据集。原始`grocery.csv`数据的前五行如*图11*所示。这些行表示10个独立的杂货店交易。第一笔交易包括四件商品：柑橘类水果、半成品面包、人造黄油和即食汤。相比之下，第三笔交易只包括一件商品，全脂牛奶：
- en: '![Exploring the dataset](img/00008.jpeg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![Exploring the dataset](img/00008.jpeg)'
- en: 'Figure 11: A snapshot of the groceries dataset'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：杂货数据集的快照
- en: Problem statements
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述
- en: We believe we have enough motivations and reasons for why we need to analyze
    the market basket using transactional or retail datasets. Now, let us discuss
    some background studies, which are needed to apply our Spark-based market basket
    analysis technique.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信我们有足够的动机和理由来分析使用事务或零售数据集的购物篮。现在，让我们讨论一些背景研究，这些研究需要应用我们基于Spark的购物篮分析技术。
- en: Suppose you have a set of distinct items *I = {i1, i2...in}* and *n* is the
    number of distinct items. A transactional database *T = {t1, t2...tN}* is a set
    of *N* transactions and *|N|* is the number of total transactions. A set *X* ![Problem
    statements](img/00067.jpeg) is called a pattern or itemset. We assume that input
    is given as a sequence of transactions, where items are separated by a comma,
    as shown in *Table 1*.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一组不同的项目*I = {i1, i2...in}*，*n*是不同项目的数量。事务数据库*T = {t1, t2...tN}*是一组*N*个事务，*|N|*是总事务数。集合*X*![Problem
    statements](img/00067.jpeg)称为模式或项集。我们假设输入是作为事务序列给出的，其中项目用逗号分隔，如*表1*所示。
- en: 'For the sake of simplicity to describe the background study, the same transactions
    are presented with a single character in *Table 2*:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见描述背景研究，相同的交易在*表2*中用单个字符表示：
- en: '| Transaction 1Transaction 2Transaction 3Transaction 4... | crackers, ice-cream,
    coke, orange,beef, pizza, coke, breadbaguette, soda, shampoo, crackers, pepsiburger,
    cream cheese, diapers, milk... |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 交易1交易2交易3交易4... | 饼干，冰淇淋，可乐，橙子，牛肉，比萨，可乐，面包法棍，苏打水，洗发水，饼干，百事可乐汉堡，奶酪，尿布，牛奶...
    |'
- en: Table 1\. Sample transactions made by a customer
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表1. 顾客的样本交易
- en: '| **TID** | **Itemset (Sequence of items)** |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| **TID** | **Itemset (Sequence of items)** |'
- en: '| 10 | A, B, C, F |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 10 | A, B, C, F |'
- en: '| 20 | C, D, E |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 20 | C, D, E |'
- en: '| 30 | A, C, E, D |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 30 | A, C, E, D |'
- en: '| 40 | A |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 40 | A |'
- en: '| 50 | D, E, G |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 50 | D, E, G |'
- en: '| 60 | B, D |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 60 | B, D |'
- en: '| 70 | B |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 70 | B |'
- en: '| 80 | A, E, C |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 80 | A, E, C |'
- en: '| 90 | A, C, D |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 90 | A, C, D |'
- en: '| 100 | B, E, D |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 100 | B, E, D |'
- en: Table 2\. A transactional database
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 表2. 事务数据库
- en: 'If ![Problem statements](img/00117.jpeg) , it is said that *X* occurs in *t*
    or *t* contains *X*. The support count is the frequency of occurrence of an itemset
    in all transactions, which can be described as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果![Problem statements](img/00117.jpeg)，则称*X*发生在*t*中或*t*包含*X*。支持计数是项集在所有事务中出现的频率，可以描述如下：
- en: '![Problem statements](img/00093.jpeg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![Problem statements](img/00093.jpeg)'
- en: In other words, if the *support*![Problem statements](img/00033.jpeg) , we say
    that *X* is a frequent itemset. For example, in *Table 2*, the occurrences of
    itemsets *CD*, *DE,* and *CDE* are *3*, *3*, and *2*, respectively, and if the
    *min_sup* is *2*, all of these are frequent itemsets.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果*支持*![Problem statements](img/00033.jpeg)，我们说*X*是频繁项集。例如，在*表2*中，项集*CD*、*DE*和*CDE*的出现次数分别为*3*、*3*和*2*，如果*min_sup*为*2*，所有这些都是频繁项集。
- en: 'On the other hand, association rules are statements of form ![Problem statements](img/00151.jpeg)
    or more formally:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，关联规则是形式为![Problem statements](img/00151.jpeg)或更正式地：
- en: '![Problem statements](img/00110.jpeg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![Problem statements](img/00110.jpeg)'
- en: 'Therefore, we can say that an association rule is a pattern that states when
    *X* occurs, then *Y* occurs with a certain probability. Confidence for the association
    rule defined in equation 1 can be expressed as how often items in *Y* appear in
    transactions that also contain *X*, as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说关联规则是一种模式，它陈述了当*X*发生时，*Y*以一定概率发生。方程1中定义的关联规则的置信度可以表示为*Y*中的项目在包含*X*的事务中出现的频率，如下所示：
- en: '![Problem statements](img/00032.jpeg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![Problem statements](img/00032.jpeg)'
- en: 'Now we need to introduce a new parameter, called `lift`, which as a metric
    is a measure of how much more likely one item is to be purchased relative to its
    typical purchase rate, given that you know another item has been purchased. This
    is defined by the following equation:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要引入一个称为`lift`的新参数，作为一个度量，它衡量了一个项目相对于其典型购买率更有可能被购买的程度，假设您知道另一个项目已被购买。这由以下方程定义：
- en: '![Problem statements](img/00018.jpeg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![Problem statements](img/00018.jpeg)'
- en: In a nutshell, given a transactional database, now the problem of market basket
    analysis is to find the complete set of a customer's purchase rules by means of
    association rules from the frequent itemsets whose support and confidence are
    no less than the *min_sup* and *min_conf* threshold, respectively.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，给定一个事务数据库，现在购物篮分析的问题是通过关联规则找到支持和置信度都不低于*min_sup*和*min_conf*阈值的频繁项集的完整一组顾客购买规则。
- en: Large-scale market basket analysis using Spark
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark进行大规模购物篮分析
- en: As shown in *Figure 12*, we assume that transactional databases are stored in
    a distributed way in a cluster of DB servers. A DB server is a computing node
    with large storage and main memory. Therefore, it can store large datasets, so
    it can compute any task assigned to it. The Driver PC is also a computing node,
    which mainly works as a client and controls the overall process.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图12*所示，我们假设事务数据库以分布方式存储在一组DB服务器的集群中。DB服务器是具有大存储和主存储器的计算节点。因此，它可以存储大型数据集，因此可以计算分配给它的任何任务。驱动PC也是一个计算节点，主要作为客户端并控制整个过程。
- en: 'Obviously, it needs to have a large memory for processing and holding the Spark
    codes to send across the computing nodes. The codes consist of a DB server ID,
    minimum support, minimum confidence, and mining algorithm:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，它需要有大内存来处理和保存Spark代码，以便发送到计算节点。这些代码包括DB服务器ID、最小支持度、最小置信度和挖掘算法：
- en: '![Large-scale market basket analysis using Spark](img/00078.jpeg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark进行大规模市场篮分析](img/00078.jpeg)'
- en: 'Figure 12: Workflow of the SAMBA algorithm using Spark'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：使用Spark的SAMBA算法的工作流程
- en: From the patterns, frequent patterns are generated using reduce phase 1, which
    satisfies the constraints *min_sup*. The map phase is applied on the computed
    frequent patterns to generate the sub-patterns that eventually help to generate
    the association rules. From the sub-patterns, reduce phase 2 is applied to generate
    the association rules that satisfy the constraints *min_conf*.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 从模式中，使用reduce阶段1生成频繁模式，满足约束条件*min_sup*。在计算频繁模式上应用map阶段，以生成最终帮助生成关联规则的子模式。从子模式中，应用reduce阶段2生成满足约束条件*min_conf*的关联规则。
- en: The incorporation of two Map and Reduce phases is possible because of the Spark
    ecosystem toward the Spark core and associated APIs. The final results are the
    complete set of association rules with their respective support count and confidence.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark生态系统对Spark核心和相关API的支持，可以实现两个Map和Reduce阶段的结合。最终结果是完整的关联规则集，以及它们各自的支持计数和置信度。
- en: These store keepers have the full form to place their items, based on the association
    between items, to increase sales to frequent and non-frequent shoppers. Due to
    space constraints, we cannot show a step-by-step example for the sample transactional
    database presented in *Table 2*.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这些商店根据商品之间的关联关系，有完整的形式来放置它们的商品，以增加对频繁和非频繁购物者的销售。由于空间限制，我们无法展示*表2*中呈现的样本交易数据库的逐步示例。
- en: However, we believe that the workflow and the pseudo codes will suffice to understand
    the total scenario. A DB server takes the input of codes sent from the Driver
    PC and starts the computation. From an environment variable Spark session, we
    create some initial data reference or RDD objects. Then, the initial RDD objects
    are transformed to create more and brand new RDD objects in the DB server. At
    first, it reads the dataset as a plain text (or other supported format) and null
    transactions using narrow/wide transformations (that is, `flatMap`, `mapToPair`,
    and `reduceByKey`).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们相信工作流程和伪代码足以理解整个情景。DB服务器接收来自驱动PC的代码输入并开始计算。从环境变量Spark会话中，我们创建一些初始数据引用或RDD对象。然后，初始RDD对象被转换以在DB服务器中创建更多和全新的RDD对象。首先，它以纯文本（或其他支持的格式）读取数据集，并使用窄/宽转换（即`flatMap`、`mapToPair`和`reduceByKey`）来处理空事务。
- en: 'Thereby, the filter join RDD operation provides a data segment without null
    transactions. Then the RDD objects are materialized to dump the RDD into the DB
    server''s storage as filtered datasets. Spark''s inter RDD join operation allows
    for the combining of the contents of multiple RDDs within a single data node.
    In summary, we follow the steps given here before getting the filtered dataset:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，过滤连接RDD操作提供了一个没有空事务的数据段。然后，RDD对象被实现以将RDD转储到DB服务器的存储中作为筛选后的数据集。Spark的间RDD连接操作允许在单个数据节点内合并多个RDD的内容。总之，在获得筛选后的数据集之前，我们遵循这里给出的步骤：
- en: Set the system property of the distributed processing model and cluster manager
    (that is, Mesos) as true. This value can be saved on your application development
    as standard Spark code.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分布式处理模型和集群管理器（即Mesos）的系统属性设置为true。这个值可以保存在你的应用开发中作为标准的Spark代码。
- en: Set SparkConf, AppName, Master URL, Spark local IP, Spark driver host IP, Spark
    executor memory, and Spark driver memory.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置SparkConf、AppName、Master URL、Spark本地IP、Spark驱动主机IP、Spark执行器内存和Spark驱动内存。
- en: Create `JavaSparkContext` using the `SparkConf`.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`SparkConf`创建`JavaSparkContext`。
- en: Create `JavaRDD` and read the dataset as plain text, as transactions, and perform
    necessary partitioning.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`JavaRDD`并将数据集作为纯文本读取，作为事务，并执行必要的分区。
- en: Perform a `flatMap` operation over the RDD to split the transactions as items.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对RDD执行`flatMap`操作以将事务拆分为项目。
- en: Perform the `mapToPair` operation to ease finding the key/value pairs of the
    items.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行`mapToPair`操作以便于查找项目的键/值对。
- en: Perform the filter operation to remove all the null transactions.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行过滤操作以删除所有空事务。
- en: When we have the filtered databases, we materialize an action inter-RDD join
    operation to save the dataset on a DB server or partition if it does not have
    enough storage for a single machine, or cache if there's not enough memory.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有了筛选后的数据库时，我们会实现一个动作间RDD连接操作，将数据集保存在DB服务器或分区上，如果单台机器的存储空间不够，或者缓存，如果内存不够。
- en: '*Figure 12* shows the complete workflow of getting association rules as the
    final results using Spark''s APIs. On the other hand, Figure 13 shows the pseudo-code
    of the algorithm, namely, **Spark-Based Market Basket Analysis** (**SAMBA**).
    There are actually two Map and Reduce operations associated, as outlined here:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12*显示了使用Spark的API获取关联规则作为最终结果的完整工作流程。另一方面，图13显示了该算法的伪代码，即**基于Spark的市场篮分析**（**SAMBA**）。这里实际上有两个Map和Reduce操作：'
- en: '**Map/Reduce phase 1**: Mappers read the transactions from the HDFS servers
    and convert the transactions to patterns. Reducers, on the other hand, find the
    frequent patterns.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Map/Reduce阶段1**：映射器从HDFS服务器读取交易并将交易转换为模式。另一方面，减速器找到频繁模式。'
- en: '**Map/Reduce phase 2**: Mappers convert the frequent patterns into sub-patterns.
    On the other hand, a reducer generates the association rules based on the given
    constraints (`min_conf` and `lift`):![Large-scale market basket analysis using
    Spark](img/00140.jpeg)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Map/Reduce阶段2**：映射器将频繁模式转换为子模式。另一方面，减速器根据给定的约束条件（`min_conf`和`lift`）生成关联规则：![使用Spark进行大规模购物篮分析](img/00140.jpeg)'
- en: 'Figure 13: The SAMBA algorithm'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：SAMBA算法
- en: After that, the SAMBA algorithm reads the **filtered database** (**FTDB**) and
    applies map phase 1 to generate all the possible combinations of the patterns.
    Then the `mapToPair()` methods them as patterns with their respective supports.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，SAMBA算法读取**过滤数据库**（**FTDB**），并应用映射阶段1生成模式的所有可能组合。然后`mapToPair()`方法将它们作为具有相应支持的模式。
- en: The algorithm solution using Spark Core
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark Core的算法解决方案
- en: Here we will look at how to do the market basket analysis using Spark Core.
    Please note, we will not use the Spark ML or MLlib since although MLlib provides
    a technique of calculating the association rules, however, it does not show how
    to calculate some other parameters such as calculating confidence, support, and
    lift that are very needed for a complete analysis of groceries dataset. Therefore,
    we will show a complete example, step-by-step, from data exploration to association
    rules generation.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将看看如何使用Spark Core进行购物篮分析。请注意，我们将不使用Spark ML或MLlib，因为虽然MLlib提供了计算关联规则的技术，但它不显示如何计算其他参数，例如计算置信度，支持和提升，这些参数对于完整分析杂货数据集非常重要。因此，我们将逐步展示一个完整的示例，从数据探索到关联规则生成。
- en: '**Step 1: Import the necessary packages and APIs**'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步：导入必要的包和API
- en: 'Here is the code to import packages and APIs:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是导入包和API的代码：
- en: '[PRE24]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Step 2: Create the entry point by specifying the Spark session**'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步：通过指定Spark会话创建入口点
- en: 'The entry point can be created with the help of the following code:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下代码创建入口点：
- en: '[PRE25]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Step 3: Create the Java RDD for the transactions**'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步：为交易创建Java RDD
- en: 'Java RDD for the transactions can be created with the help of the following
    code:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下代码创建交易的Java RDD：
- en: '[PRE26]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Step 4: Create a method for creating the list**'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步：创建创建列表的方法
- en: 'Create a method named `toList` from the created transactions RDDs, which will
    add all the items in the transactions:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`toList`的方法，从创建的交易RDD中添加所有交易中的项目：
- en: '[PRE27]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**Step 5: Remove infrequent items and null transactions**'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步：删除不频繁的项目和空交易
- en: 'Create a method named `removeOneItemAndNullTransactions` to remove infrequent
    items and null transactions:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`removeOneItemAndNullTransactions`的方法，以删除不频繁的项目和空交易：
- en: '[PRE28]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Step 6: Flat mapping and 1-itemsets creation (map phase 1)**'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 第6步：扁平映射和创建1项集（映射阶段1）
- en: 'Do the `flatmap` and create 1-itemsets. Finally, save the patterns:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 进行`flatmap`并创建1项集。最后，保存模式：
- en: '[PRE29]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the last saving of the patterns RDD is for optional reference purposes
    and so that you can see the contents of the RDDs.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模式RDD的最后保存是为了可选的参考目的，以便您可以查看RDD的内容。
- en: 'The following is a screenshot of the 1-itemsets:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是1项集的屏幕截图：
- en: '![The algorithm solution using Spark Core](img/00152.jpeg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark Core的算法解决方案](img/00152.jpeg)'
- en: 'Figure 14: 1-itemsets'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：1项集
- en: '**Step 7: Combine and reduce frequent patterns (reduce phase 1)**'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 第7步：组合和减少频繁模式（减少阶段1）
- en: 'Combine and reduce all the frequent patterns, and save them:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 组合和减少所有频繁模式，并保存它们：
- en: '[PRE30]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following is a snapshot of the frequent patterns with their respective
    support (frequency in *Figure 15)*:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是具有相应支持的频繁模式的快照（*图15*）：
- en: '![The algorithm solution using Spark Core](img/00073.jpeg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark Core的算法解决方案](img/00073.jpeg)'
- en: 'Figure 15: Frequent patterns with their respective support (frequency)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：具有相应支持的频繁模式（频率）
- en: '**Step 8: Generate all the candidate frequent patterns (map phase 2)**'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 第8步：生成所有候选频繁模式（映射阶段2）
- en: 'Generate all the candidate frequent patterns or sub-patterns by removing the
    1-itemsets from the frequent patterns, and finally, save the candidate patterns:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从频繁模式中删除1项集来生成所有候选频繁模式或子模式，并最终保存候选模式：
- en: '[PRE31]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following is a snapshot of the sub-patterns:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是子模式的快照：
- en: '![The algorithm solution using Spark Core](img/00054.jpeg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark Core的算法解决方案](img/00054.jpeg)'
- en: 'Figure 16: Sub-patterns for the items'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：项目的子模式
- en: '**Step 9: Combine all the sub-patterns**'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 第9步：组合所有子模式
- en: 'Combine all the sub-patterns and save them on disk or persist on memory:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 组合所有子模式并将它们保存在磁盘上或持久保存在内存中：
- en: '[PRE32]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following is a screenshot of the candidate patterns (sub-patterns) in combined
    form:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是组合形式的候选模式（子模式）的屏幕截图：
- en: '![The algorithm solution using Spark Core](img/00020.jpeg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark Core的算法解决方案](img/00020.jpeg)'
- en: 'Figure 17: Candidate patterns (sub-patterns) in combined form'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：组合形式的候选模式（子模式）
- en: '**Step 10: Generate association rules**'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 第10步：生成关联规则
- en: 'Generate all the association rules from the sub-patterns (reduce phase 2) by
    specifying the `confidence` and `lift`:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定`置信度`和`提升`从子模式生成所有关联规则（减少阶段2）：
- en: '[PRE33]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The following is the output of the association rules including their confidence
    and lift. For more details on support, confidence, and lift, refer to the problem
    statement section.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是包括置信度和提升的关联规则的输出。有关支持，置信度和提升的更多详细信息，请参阅问题说明部分。
- en: '[Antecedent=>Consequent], Support, Confidence, Lift:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[前提=>结论]，支持，置信度，提升：'
- en: '![The algorithm solution using Spark Core](img/00094.jpeg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark Core的算法解决方案](img/00094.jpeg)'
- en: 'Figure 18: Association rules including their confidence and lift'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：包括置信度和提升的关联规则
- en: Tuning and setting the correct parameters in SAMBA
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在SAMBA中调整和设置正确的参数
- en: Note that if you attempt to use the default parameter settings as support =
    0.1 and confidence = 0.6, you might end up with null rules, or technically, no
    rules, to be generated. You might be wondering why. Actually, the default support
    of 0.1 means that in order to generate an association rule, an item must have
    appeared in at least *0.1 * 9385 = 938.5* transactions or 938.5 times (for the
    dataset we are using, |N| = 9385).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您尝试使用默认参数设置，如支持=0.1和置信度=0.6，可能会得到空规则，或者从技术上讲，没有规则生成。您可能会想知道为什么。实际上，0.1的默认支持意味着为了生成关联规则，一个项目必须至少出现在*0.1
    * 9385 = 938.5*交易中，或者938.5次（对于我们使用的数据集，|N| = 9385）。
- en: However, in this regard, in their book titled *Machine Learning with R, Packt
    Publishing, 2015*, Brett Lantz at el. argue that there is one way to tackle this
    issue while setting support. They suggest considering the minimum number of transactions
    needed before you would consider a pattern's interestingness. Moreover, for example,
    you could also argue that if an item is purchased twice a day (which is approximately
    60 times a month), then it may be non-trivial to consider that transaction.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这方面，在他们的书中，Brett Lantz等人认为有一种方法可以解决这个问题，同时设置支持。他们建议考虑在您认为模式有趣之前需要的最小交易数量。此外，例如，您还可以认为，如果一个项目每天购买两次（大约每月60次），那么考虑该交易可能是非平凡的。
- en: From this perspective, it is possible to estimate how to set the value of support
    needed to find only rules matching at least that many transactions. Consequently,
    you can set the value of minimum support as 0.006, because 60 out of 9,835 equals
    0.006; we'll try setting the support there first.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，可以估计如何设置支持值，以便仅找到至少匹配那么多交易的规则。因此，您可以将最小支持值设置为0.006，因为9,835中的60等于0.006；我们将首先尝试设置支持值。
- en: On the other hand, setting the minimum confidence also requires a tricky balance
    and in this regard, again, we would like to refer you to the book by Brett Lantz
    et al. titled *Machine Learning with R, Packt Publishing, 2015*. If confidence
    is too low, obviously we might be incredulous with a pretty large of unreliable
    rules false positive results.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，设置最小置信度也需要一个棘手的平衡，在这方面，我们再次想参考Brett Lantz等人的书，题为*Machine Learning with
    R, Packt Publishing, 2015*。如果置信度太低，显然我们可能会对相当多的不可靠规则产生怀疑的假阳性结果。
- en: As a result, the optimum value of the minimum confidence threshold depends heavily
    on the goals of your analysis. Consequently, if you start with conservative values,
    you can always reduce them to broaden the search if you aren't finding actionable
    intelligence. If you set the minimum confidence threshold at 0.25, it means that
    in order to be included in the results, the rule has to be correct at least 25
    percent of the time. This will eliminate the most unreliable rules while allowing
    some room for us to modify behavior with targeted promotions of the product.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最小置信度阈值的最佳值严重取决于您分析的目标。因此，如果您从保守值开始，可以随时将其降低以扩大搜索，如果您找不到可操作的情报。如果将最小置信度阈值设置为0.25，这意味着为了包含在结果中，规则必须至少有25%的时间是正确的。这将消除最不可靠的规则，同时为我们留出一些空间，以通过有针对性的产品促销来修改行为。
- en: Now, let's talk about the third parameter, the `lift`. Before suggesting how
    to set the value of the `lift`, let's see a practical example of how it might
    affect the generation of the association rules in the first place. For the third
    time, we refer to the book by Brett Lantz et al. titled *Machine Learning with
    R, Packt Publishing, 2015*.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈第三个参数，“提升”。在建议如何设置“提升”的值之前，让我们先看一个实际例子，看看它可能如何影响首次生成关联规则。这是第三次，我们参考了Brett
    Lantz等人的书，题为*Machine Learning with R, Packt Publishing, 2015*。
- en: For example, suppose at a supermarket store, many people often purchase milk
    and bread together. Therefore, naturally, you would expect to find many transactions
    that contain both milk and bread. However, if `lift` (milk => bread) is greater
    than 1, this implies that the two items are found together more often than one
    would expect by chance. As a consequence, a large `lift` value is, therefore,
    a strong indicator that a rule is important, and reflects a true connection between
    the items in the transactions.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设在超市里，很多人经常一起购买牛奶和面包。因此，自然地，您期望找到许多包含牛奶和面包的交易。然而，如果`提升`（牛奶=>面包）大于1，则意味着这两种物品一起出现的频率比预期的要高。因此，较大的`提升`值是规则重要性的强烈指标，并反映了交易中物品之间的真实联系。
- en: In summary, we need to set the values of these parameters carefully, by considering
    the preceding examples. However, as a standalone model the algorithms might take
    hours to finish. So, run the application with enough time. Alternatively, reduce
    the long transaction to reduce the time overhead.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们需要仔细考虑这些参数的值，考虑前面的例子。然而，作为一个独立的模型，算法可能需要几个小时才能完成。因此，请花足够的时间运行应用程序。或者，减少长交易以减少时间开销。
- en: OCR pipeline with Spark
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的OCR流水线
- en: Image processing and computer vision are two classical but still-emerging research
    areas that often make proper utilization of many types of machine learning algorithms.
    There are several use cases where the relationships of linking the patterns of
    image pixels to higher concepts are extremely complex and hard to define, and
    of course, computationally extensive, too.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图像处理和计算机视觉是两个经典但仍在不断发展的研究领域，它们经常充分利用许多类型的机器学习算法。有几种用例，其中将图像像素的模式与更高概念的关系联系起来是极其复杂且难以定义的，当然，也是计算上费时的。
- en: From a practical point of view, it's relatively easier for a human being to
    recognize if an object is a face, a dog, or letters or characters. However, defining
    these patterns under certain circumstances is difficult. Additionally, image-related
    datasets are often noisy.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际角度来看，人类相对容易识别物体是脸、狗，还是字母或字符。然而，在某些情况下定义这些模式是困难的。此外，与图像相关的数据集通常存在噪音。
- en: In this section, we will develop a model similar to those used at the core of
    the **Optical Character Recognition** (**OCR**) used as document scanners. This
    kind of software helps to process paper-based documents by converting printed
    or handwritten text into an electronic form to be saved in a database.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开发一个类似于用于**光学字符识别**（**OCR**）的核心的模型，用于将打印或手写文本转换为电子形式以保存在数据库中，以便处理基于纸张的文档。
- en: When OCR software first processes a document, it divides the paper, or any object,
    into a matrix such that each cell in the grid contains a single glyph (also known
    as different graphical shapes), which is just an elaborate way of referring to
    a letter, symbol, or number, or any contextual information from the paper or the
    object.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 当OCR软件首次处理文档时，它将纸张或任何对象分成一个矩阵，以便网格中的每个单元格包含一个单个字形（也称为不同的图形形状），这只是一种指代字母、符号、数字或来自纸张或对象的任何上下文信息的复杂方式。
- en: To demonstrate the OCR pipeline, we will assume that the document contains only
    alpha characters in English that match glyphs to one of the 26 letters, A to Z.
    We will use the OCR letter dataset from the UCI Machine Learning Data Repository
    ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml) ). The dataset
    was donated by W. Frey and D. J. Slate et al. To explore the dataset, we have
    found that the dataset contains 20,000 examples of 26 English alphabet capital
    letters printed using 20 different randomly reshaped and distorted black-and-white
    fonts as glyphs of different shapes.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示OCR流水线，我们将假设文档只包含英文的字母，与26个字母A到Z中的一个匹配的字形。我们将使用UCI机器学习数据存储库（[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)）中的OCR字母数据集。该数据集是由W.
    Frey和D. J. Slate等人捐赠的。我们已经发现数据集包含20,000个例子，使用20种不同的随机重塑和扭曲的黑白字体作为不同形状的字形的26个英文字母大写字母。
- en: Tip
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: For more information about these data, refer to *Letter recognition using Holland-style
    adaptive classifiers, Machine Learning, Vol. 6, pp. 161-182, by W. Frey and D.J.
    Slate (1991)*.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 有关这些数据的更多信息，请参阅W. Frey和D.J. Slate（1991年）的文章《使用荷兰式自适应分类器进行字母识别，机器学习，第6卷，第161-182页》。
- en: 'The image shown in *Figure 19* was published by Frey and Slate and provides
    an example of some of the printed glyphs. Distorted in this way, the letters are
    challenging for a computer to identify, yet are easily recognized by a human being.
    The statistical attributes for the top 20 rows are shown in *Figure 20*:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19*中显示的图像是由Frey和Slate发表的，提供了一些印刷字形的示例。以这种方式扭曲，这些字母对计算机来说很具挑战性，但对人类来说很容易识别。前20行的统计属性显示在*图20*中：'
- en: '![OCR pipeline with Spark](img/00119.jpeg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark的OCR流水线](img/00119.jpeg)'
- en: 'Figure 19: Some of the printed glyphs [courtesy of the article titled Letter
    recognition using Holland-style adaptive classifiers, Machine Learning, Vol. 6,
    pp. 161-182, by W. Frey and D.J. Slate (1991)]'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：一些印刷字形[由W. Frey和D.J. Slate（1991年）的文章《使用荷兰式自适应分类器进行字母识别，机器学习，第6卷，第161-182页》提供]
- en: Exploring and preparing the data
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索和准备数据
- en: According to the documentation provided by Frey and Slate, when the glyphs are
    scanned using an OCR reader to the computer they are automatically converted into
    pixels. Consequently, the 16 statistical attributes mentioned are recorded to
    the computer too.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Frey和Slate提供的文档，当使用OCR阅读器扫描字形到计算机时，它们会自动转换为像素。因此，提到的16个统计属性也被记录到计算机中。
- en: Note that the concentration of black pixels across the various areas of the
    box where the character is indicated should provide a way to differentiate among
    the 26 letters of the alphabet using an OCR or a machine learning algorithm to
    be trained.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，字符所在的方框各个区域的黑色像素的浓度应该提供一种区分字母表中的26个字母的方法，使用OCR或机器学习算法进行训练。
- en: Tip
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: To follow along with this example, download the `letterdata.data` file from
    the Packt Publishing website and save it to your project directory working on
    one or the other directory.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随本示例，从Packt Publishing网站下载`letterdata.data`文件，并将其保存到您的项目目录中的一个或另一个目录中。
- en: 'Before reading the data from the Spark working directory, we confirm that we
    have received the data with the 16 features that define each example of the letter
    class. As expected, the letter has 26 levels, as shown in *Figure 20*:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在从Spark工作目录中读取数据之前，我们确认已收到定义每个字母类的16个特征的数据。如预期的那样，字母有26个级别，如*图20*所示：
- en: '![Exploring and preparing the data](img/00138.jpeg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![探索和准备数据](img/00138.jpeg)'
- en: 'Figure 20: A snapshot of the dataset shown as Data Frame'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：显示为数据框的数据集的快照
- en: Recall that SVM, Naive Baseyan-based classifier, or any other classifier algorithms,
    along with their associated learners, require all the features to be numeric.
    Moreover, each feature is scaled to a fairly small interval.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，SVM、朴素贝叶斯分类器或任何其他分类器算法以及它们的相关学习器都需要所有特征都是数字。此外，每个特征都被缩放到一个相当小的区间。
- en: Also, SVM works well on dense vectorized features and consequently, will perform
    poorly against the sparse vectorized features. In our case, every feature is an
    integer. Therefore, we do not need to convert any values into numbers. On the
    other hand, some of the ranges for these integer variables appear fairly wide.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，SVM在密集向量化特征上表现良好，因此在稀疏向量化特征上表现不佳。在我们的情况下，每个特征都是整数。因此，我们不需要将任何值转换为数字。另一方面，这些整数变量的一些范围似乎相当宽。
- en: In practical cases, it might require that we normalize the data against all
    few features points.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际情况下，可能需要对所有少数特征点对数据进行归一化。
- en: OCR pipeline with Spark ML and Spark MLlib
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark ML和Spark MLlib的OCR流水线
- en: Because of its accuracy and robustness, let's see whether the SVM is up to the
    task. As you can see in *Figure 17*, we have a multiclass OCR dataset (with 26
    classes, to be more precise); therefore, we need to have a multiclass classification
    algorithm, for example, the logistic regression model, since the current implementation
    of liner SVM in Spark does not support the multi-class classification.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其准确性和健壮性，让我们看看SVM是否能胜任。正如您在*图17*中所看到的，我们有一个多类OCR数据集（具体来说有26个类）；因此，我们需要一个多类分类算法，例如逻辑回归模型，因为Spark中的线性SVM的当前实现不支持多类分类。
- en: Tip
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Please refer to the following URL for more details: [http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-support-vector-machines-svms](http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-support-vector-machines-svms).'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参阅以下网址：[http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-support-vector-machines-svms](http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-support-vector-machines-svms)。
- en: '**Step 1: Import necessary packages/libraries/APIs**'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：导入必要的包/库/接口**'
- en: 'The following is the code to import the necessary packages:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是导入必要包的代码：
- en: '[PRE34]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Step 2: Initialize necessary Spark environment**'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：初始化必要的Spark环境**'
- en: 'The following is the code to initialize a Spark environment:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是初始化Spark环境的代码：
- en: '[PRE35]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here we set the application name as `OCRPrediction`, and the master URL as `local`.
    The Spark session is the entry point of the program. Please set these parameters
    accordingly.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将应用程序名称设置为`OCRPrediction`，主URL设置为`local`。Spark会话是程序的入口点。请相应地设置这些参数。
- en: '**Step 3: Read the data file and create a corresponding Dataset and show the
    first 20 rows**'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：读取数据文件并创建相应的数据集，并显示前20行**'
- en: 'The following is the code to read the data file:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是读取数据文件的代码：
- en: '[PRE36]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: For the first 20 rows, please refer to *Figure 5*. As we can see, there are
    26 characters presented as single characters that need to be predicted; therefore,
    we need to assign each character a random double value to align the value to the
    other features. Therefore, in the next step, that is what we'll do.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前20行，请参阅*图5*。正如我们所看到的，有26个字符呈现为需要预测的单个字符；因此，我们需要为每个字符分配一个随机双精度值，以使该值与其他特征对齐。因此，在下一步中，这就是我们要做的。
- en: '**Step 4: Create a dictionary for assigning each character a double value randomly**'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：创建一个字典，为每个字符分配一个随机双精度值**'
- en: 'The following code is to create a dictionary for assigning each character a
    double value randomly:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是为每个字符分配一个随机双精度值的字典：
- en: '[PRE37]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'And here is the mapping output generated from the preceding code segment:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从前面的代码段生成的映射输出：
- en: '![OCR pipeline with Spark ML and Spark MLlib](img/00048.jpeg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark ML和Spark MLlib的OCR管道](img/00048.jpeg)'
- en: 'Figure 21: Mapping assignments'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：映射分配
- en: '**Step 5: Creating the labeled point and the feature vector**'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：创建标记点和特征向量**'
- en: 'Create the labeled points and feature vectors for the features combined from
    the 16 features (that is, 16 columns). Also, save them as Java RDD and dump or
    cache on disk or memory, and show the sample output:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 为来自16个特征（即16列）的组合特征创建标记点和特征向量。还将它们保存为Java RDD，并将其转储或缓存在磁盘或内存中，并显示样本输出：
- en: '[PRE38]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'If you look carefully at the preceding code segments, we have created an array
    named features for 16 features altogether and created a dense vector representation,
    since the dense vector representation is a more compact representation where the
    contents can be shown as in the following screenshot:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细观察前面的代码段，我们已经创建了一个名为features的数组，其中包含16个特征，并创建了密集向量表示，因为密集向量表示是一种更紧凑的表示，其中内容可以显示如下截图所示：
- en: '![OCR pipeline with Spark ML and Spark MLlib](img/00041.jpeg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark ML和Spark MLlib的OCR管道](img/00041.jpeg)'
- en: 'Figure 22: The Java RDD for the corresponding label and features as vectors'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：相应标签和特征的Java RDD
- en: '**Step 6: Generating the training and test set**'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：生成训练和测试集**'
- en: 'Here is the code for generating the test set:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成测试集的代码：
- en: '[PRE39]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'If you wish to see the snaps of the training or test datasets, you should dump
    or cache them. The following is a sample code for that:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望查看训练或测试数据集的快照，您应该将它们转储或缓存。以下是一个示例代码：
- en: '[PRE40]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Tip
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: We have randomly generated the training and the test set for the model to be
    trained and tested. In our case, it was 70% and 30%, respectively, and 11L as
    the long seed. Readjust the values based on your dataset. Note that if you add
    a seed to a Random, you get the same results each time you run your code and these
    come as primes up to 1062348.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已随机生成了要训练和测试的模型的训练集和测试集。在我们的案例中，分别为70%和30%，长种子为11L。根据您的数据集重新调整这些值。请注意，如果向随机数添加种子，每次运行代码时都会获得相同的结果，这些结果一直为质数，最多为1062348。
- en: '**Step 7: Train the model**'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：训练模型**'
- en: 'As you can see, we have a multiclass dataset with 26 classes; therefore, we
    need to have a multiclass classification algorithm, for example, the logistic
    regression model:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们有一个包含26个类的多类数据集；因此，我们需要一个多类分类算法，例如逻辑回归模型：
- en: '[PRE41]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The preceding code segment builds a model using the training datasets by specifying
    the number of classes (that is, `26`) and the feature scaling as `Boolean true`.
    As you can see, we have used the RDD version of the training datasets using `training.rdd()`,
    since the training datasets are in normal vector format.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码段通过指定类别数（即`26`）和特征缩放为`Boolean true`来使用训练数据集构建模型。正如您所看到的，我们使用了训练数据集的RDD版本，使用`training.rdd()`，因为训练数据集是以正常向量格式的。
- en: Tip
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Spark has the support of the multiclass logistic regression algorithm that supports
    the **Limited-Memory-Broyden-Fletcher-Goldfarb-Shanno** (**LBFGS**) algorithm.
    In numerical optimization, the **Broyden-Fletcher-Goldfarb-Shanno** (**BFGS**)
    algorithm is an iterative method for solving unconstrained nonlinear optimization
    problems.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持多类逻辑回归算法，支持**有限内存Broyden-Fletcher-Goldfarb-Shanno**（**LBFGS**）算法。在数值优化中，**Broyden-Fletcher-Goldfarb-Shanno**（**BFGS**）算法是用于解决无约束非线性优化问题的迭代方法。
- en: '**Step 8: Compute the raw scores on the test dataset**'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤8：计算测试数据集上的原始分数
- en: 'Here is the code to compute the raw scores:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是计算原始分数的代码：
- en: '[PRE42]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: If you look at the preceding code carefully, you will see that we are actually
    calculating the predicted features out of the model we created in *Step 7* by
    making them Java RDD.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细查看前面的代码，您会发现我们实际上是通过将它们作为Java RDD来计算模型中创建的预测特征，从而计算出*步骤7*中的预测特征。
- en: '**Step 9: Predict the outcome for label 8.0 (that is, I) and get the evaluation
    metrics**'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤9：预测标签为8.0（即I）的结果并获取评估指标
- en: 'The following code illustrates how to predict the outcome:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码说明了如何预测结果：
- en: '[PRE43]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![OCR pipeline with Spark ML and Spark MLlib](img/00050.jpeg)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark ML和Spark MLlib的OCR流水线](img/00050.jpeg)'
- en: 'Figure 23: Performance metrics for precision and recall'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 图23：精度和召回率的性能指标
- en: Therefore, the precision is 75%, which is obviously not satisfactory. However,
    if you are still unsatisfied, the following chapter will look at how to tune parameters
    so that the prediction accuracy increases.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，精度为75%，显然不令人满意。然而，如果您仍然不满意，下一章将讨论如何调整参数以提高预测准确性。
- en: Tip
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'To get an idea about how to calculate the precision, recall, true positive
    rate, and true negative rate, please refer to the Wikipedia page at [https://en.wikipedia.org/wiki/Sensitivity_and_specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity),
    which discusses the sensitivity and the specificity elaborately. You can also
    refer to *Powers, David M W (2011). Evaluation: From Precision, Recall and F-Measure
    to ROC, Informedness, Markedness & Correlation(PDF). Journal of Machine Learning
    Technologies 2 (1): 37-63*.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '要了解如何计算精度、召回率、真正率和真负率，请参阅维基百科页面[https://en.wikipedia.org/wiki/Sensitivity_and_specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)，其中详细讨论了敏感性和特异性。您还可以参考*Powers,
    David M W (2011). Evaluation: From Precision, Recall and F-Measure to ROC, Informedness,
    Markedness & Correlation(PDF). Journal of Machine Learning Technologies 2 (1):
    37-63*。'
- en: Topic modeling using Spark MLlib and ML
  id: totrans-399
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark MLlib和ML进行主题建模
- en: The topic modeling technique is widely used in the task of mining text from
    a large collection of documents. These topics can then be used to summarize and
    organize documents that include the topic terms and their relative weights. Since
    the release of Spark 1.3, MLlib supports the LDA, which is one of the most successfully
    used topic-modeling techniques in the area of text mining and **Natural Language
    Processing** (**NLP**). Moreover, LDA is also the first MLlib algorithm to adopt
    Spark GraphX.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模技术广泛用于从大量文档中挖掘文本的任务。这些主题可以用来总结和组织包括主题术语及其相对权重的文档。自Spark 1.3发布以来，MLlib支持LDA，这是文本挖掘和自然语言处理领域中最成功使用的主题建模技术之一。此外，LDA也是第一个采用Spark
    GraphX的MLlib算法。
- en: Tip
  id: totrans-401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: To get more information about how the theory behind the LDA works, please refer
    to *David M. Blei, Andrew Y. Ng and Michael I. Jordan, Latent Dirichlet Allocation,
    Journal of Machine Learning Research 3 (2003) 993-1022*.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解LDA背后的理论如何工作，请参考*David M. Blei, Andrew Y. Ng and Michael I. Jordan, Latent
    Dirichlet Allocation, Journal of Machine Learning Research 3 (2003) 993-1022*。
- en: '*Figure 24* shows the output of the topic distribution from randomly generated
    tweet text, to be discussed further in [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced
    Machine Learning with Streaming and Graph Data*. Moreover, we will provide more
    justification for why we used LDA other than other topic-modeling algorithms in
    [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d "Chapter 9. 
    Advanced Machine Learning with Streaming and Graph Data"), *Advanced Machine Learning
    with Streaming and Graph Data*:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '*图24*显示了从随机生成的推文文本中的主题分布的输出，将在[第9章](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "第9章。使用流式和图数据进行高级机器学习")中进一步讨论，*使用流式和图数据进行高级机器学习*。此外，我们将进一步解释为什么我们在[第9章](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "第9章。使用流式和图数据进行高级机器学习")中使用LDA而不是其他主题建模算法。'
- en: '![Topic modeling using Spark MLlib and ML](img/00159.jpeg)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib和ML进行主题建模](img/00159.jpeg)'
- en: 'Figure 24: The topic distribution and how it looks like'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 图24：主题分布及其外观
- en: In this section, we will look at an example of topic modeling using the LDA
    algorithm of Spark MLlib with unstructured raw tweets datasets.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍使用Spark MLlib的LDA算法处理非结构化原始推文数据集的主题建模示例。
- en: Topic modeling with Spark MLlib
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark MLlib进行主题建模
- en: In this subsection, we represent a semi-automated technique of topic modeling
    using Spark. The following steps show the topic modeling from data reading to
    printing the topics, along with their term-weights. Using other options as defaults,
    we train LDA on the dataset downloaded from the GitHub URL at [https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test](https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们使用Spark表示了一种半自动的主题建模技术。以下步骤展示了从数据读取到打印主题及其术语权重的主题建模，同时使用其他选项作为默认值，我们在从GitHub
    URL下载的数据集上训练LDA，网址为[https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test](https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test)。
- en: '**Step 1: Load required packages and APIs**'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤1：加载所需的软件包和API
- en: 'Here is the code to load the required packages:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载所需软件包的代码：
- en: '[PRE44]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**Step 2: Create a Spark session**'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2：创建Spark会话
- en: 'Here is the code to create a Spark session:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建Spark会话的代码：
- en: '[PRE45]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Step 3: Reading and seeing the content of the datasets**'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：读取和查看数据集的内容**'
- en: 'The following code illustrates how to read and see the content of the datasets:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码说明了如何读取和查看数据集的内容：
- en: '[PRE46]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Please note that the use of the character `***` indicates to read all the text
    files in the input/text directory in the project path. If we print the first 20
    rows, just use the following code and you will see the following text:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用字符`***`表示读取项目路径中input/text目录中的所有文本文件。如果要打印前20行，只需使用以下代码，您将看到以下文本：
- en: '[PRE47]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![Topic modeling with Spark MLlib](img/00006.jpeg)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行主题建模](img/00006.jpeg)'
- en: 'Figure 25: First 20 rows of the texts'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图25：文本的前20行
- en: From the preceding screenshot, it is clear that the text files we are using
    are nothing but very unstructured texts containing the column name label. They
    therefore need to be pre-processed using the feature transformation using the
    regular expression tokenizer before we can use them for our purposes.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的屏幕截图可以清楚地看出，我们使用的文本文件只是包含列名标签的非常不规则的文本。因此，我们需要使用正则表达式分词器进行特征转换预处理，然后才能用于我们的目的。
- en: '**Step 4: Feature Transformers using RegexTokenizer**'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：使用RegexTokenizer进行特征转换**'
- en: 'Here is the code for `RegexTokenizer`:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`RegexTokenizer`的代码：
- en: '[PRE48]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'If you look at the preceding code segment carefully, you will see that we have
    specified our input column name as `value` and our output column name as `labelText`
    and the pattern. Now create another data frame using the regular expression tokenizer
    we just tokenized using the following code segment:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察前面的代码段，您会发现我们指定了输入列名为`value`，输出列名为`labelText`和模式。现在使用以下代码段使用刚刚标记的正则表达式分词器创建另一个数据框：
- en: '[PRE49]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, let''s see what the new data frame `labelTextDataFrame` contains using
    the following statement:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下语句查看新数据框`labelTextDataFrame`包含什么：
- en: '[PRE50]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![Topic modeling with Spark MLlib](img/00095.jpeg)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行主题建模](img/00095.jpeg)'
- en: 'Figure 26: A new column with characters converted into the corresponding lowercase
    characters'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 图26：一个新列，其中的字符转换为相应的小写字符
- en: 'The preceding screenshot (*Figure 26*) shows that the tokenizer created a new
    column and that mostly, the uppercase words or characters have been converted
    into the corresponding lowercase characters. Since topic modeling cares about
    the term-weight and frequency of each input word, we need to separate the words
    from the label texts, which is done by using the following code segments:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图（*图26*）中显示，分词器创建了一个新列，大多数大写单词或字符已转换为相应的小写字符。由于主题建模关心每个输入词的词权重和频率，我们需要从标签文本中分离单词，这是通过使用以下代码段完成的：
- en: '[PRE51]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now let''s create another data frame and see the results of the transformation
    using the following code:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建另一个数据框，并使用以下代码查看转换的结果：
- en: '[PRE52]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![Topic modeling with Spark MLlib](img/00055.jpeg)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行主题建模](img/00055.jpeg)'
- en: 'Figure 27: Label texts as separate words separated by a comma'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 图27：标签文本作为逗号分隔的单词
- en: From the preceding screenshot (*Figure 27*), we can see that a new column, `label`
    has been added, which shows the label texts as separate words, separated by a
    comma.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的屏幕截图（*图27*）中，我们可以看到添加了一个新列`label`，其中显示标签文本作为逗号分隔的单词。
- en: 'Now, since we have a bunch of texts available, to make the prediction and topic
    modeling easier, we need to make the indexing for the words we split. But before
    that, we need to swap the `labelText` and `text` in a new data frame, as shown
    in *Figure 28*. To check if it has really happened, just print the newly created
    data frame:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于我们有一堆文本可用，为了使预测和主题建模更容易，我们需要对我们分割的单词进行索引。但在此之前，我们需要在新数据框中交换`labelText`和`text`，如*图28*所示。要检查是否真的发生了这种情况，只需打印新创建的数据框：
- en: '[PRE53]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![Topic modeling with Spark MLlib](img/00049.jpeg)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行主题建模](img/00049.jpeg)'
- en: 'Figure 28: Swapping the labelText and text in a new data frame'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 图28：在新数据框中交换labelText和text
- en: '**Step 5: Feature transformation by means of string indexer**'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：通过字符串索引器进行特征转换**'
- en: 'Here is the code for feature transformation:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是特征转换的代码：
- en: '[PRE54]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now create a new data frame for the data frame `newDF` we created in *Step
    2*, and look at the contents of the data frame. Note that we have selected the
    old column `labelText` and set the new column simply as `label`:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 现在为*步骤2*中创建的数据框`newDF`创建一个新数据框，并查看数据框的内容。请注意，我们选择了旧列`labelText`，并将新列简单设置为`label`：
- en: '[PRE55]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![Topic modeling with Spark MLlib](img/00131.jpeg)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行主题建模](img/00131.jpeg)'
- en: 'Figure 29: Corresponding labels against the labelText column'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 图29：与labelText列相对应的标签
- en: So, as shown in *Figure 29*, we got a new column, `label`, which contains the
    corresponding labels against the `labelText` column. The very next step is about
    removing the stop words.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如*图29*所示，我们得到了一个新列`label`，其中包含与`labelText`列相对应的标签。接下来的步骤是去除停用词。
- en: '**Step 6: Feature transformation (removing the stop words)**'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：特征转换（去除停用词）**'
- en: 'Here is the code for feature transformation for removing the stop words:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是去除停用词的特征转换的代码：
- en: '[PRE56]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The current implementation of the `StopWordsRemover` class of Spark contains
    the following words as stop words. Since we don''t have any precondition, we have
    used those words directly:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`StopWordsRemover`类的当前实现包含以下单词作为停用词。由于我们没有任何先决条件，我们直接使用了这些单词：
- en: '![Topic modeling with Spark MLlib](img/00074.jpeg)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行主题建模](img/00074.jpeg)'
- en: 'Figure 30: A bunch of stop words provided by Spark for text analytics'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 图30：Spark提供的用于文本分析的一些停用词
- en: '**Step 7: Create a filtered dataset by removing stop words**'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：通过去除停用词创建一个过滤后的数据集**'
- en: 'Here is the code to create a filtered dataset by removing stop words:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是通过去除停用词创建过滤后数据集的代码：
- en: '[PRE57]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now create a new data frame for the filtered words (that is, excluding the
    stop words). Let''s look at the contents of the filtered dataset:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '现在为过滤后的单词（即不包括停用词）创建一个新数据框。让我们查看过滤后数据集的内容： '
- en: '[PRE58]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '![Topic modeling with Spark MLlib](img/00107.jpeg)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行主题建模](img/00107.jpeg)'
- en: 'Figure 31: Filtered words excluding the stop words'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 图31：排除停用词的过滤词
- en: '**Step 8: Feature extraction using HashingTF**'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 第8步：使用HashingTF进行特征提取
- en: 'Here is the code for feature extraction using HashingTF:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用HashingTF进行特征提取的代码：
- en: '[PRE59]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'In the previous code, we have done the HashingTF for only five features, for
    simplicity. Now create another data frame out of the extracted features on the
    old data frame (that is, `filteredDF`) and show the same output:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们只对五个特征进行了HashingTF，以简化操作。现在从旧数据框架（即`filteredDF`）中提取特征创建另一个数据框架，并显示相同的输出：
- en: '[PRE60]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '![Topic modeling with Spark MLlib](img/00024.jpeg)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行主题建模](img/00024.jpeg)'
- en: 'Figure 32: Data frame out of the extracted features on the old data frame,
    filteredDF'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图32：从旧数据框架`filteredDF`中提取特征创建的数据框架
- en: Tip
  id: totrans-471
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: For more information and API documentation details for feature transformation,
    estimator, and hashing, please refer to the Spark website at [https://spark.apache.org/docs/latest/ml-features.html](https://spark.apache.org/docs/latest/ml-features.html).
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 有关特征转换、估计器和哈希的更多信息和API文档细节，请参考Spark网站[https://spark.apache.org/docs/latest/ml-features.html](https://spark.apache.org/docs/latest/ml-features.html)。
- en: '**Step 9: Feature extraction using IDF estimator**'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 第9步：使用IDF估计器进行特征提取
- en: '[PRE61]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The preceding code creates new features from the raw features by fitting the
    `idfModel` that takes the featured data frame (that is, `featurizedData`) in step
    5\. Now let''s create and show a new data frame for the rescaled data using the
    estimator we just created (that is, `idfModel`), which consumes the old data frame
    for the featurized data (that is, `featurizedData`):'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码通过拟合`idfModel`从原始特征中创建新特征，该模型接受第5步中的特征数据框架（即`featurizedData`）。现在让我们创建并显示使用我们刚刚创建的估计器（即`idfModel`）的重新缩放数据的新数据框架，该估计器消耗了用于特征化数据的旧数据框架（即`featurizedData`）：
- en: '[PRE62]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '![Topic modeling with Spark MLlib](img/00137.jpeg)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行主题建模](img/00137.jpeg)'
- en: 'Figure 33: The rescaled data using the estimator'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 图33：使用估计器重新缩放的数据
- en: '**Step 10: Chi-Squared feature selection**'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 第10步：卡方特征选择
- en: 'The Chi-Squared feature selection selects categorical features to use for predicting
    a categorical label. The following code segment does this selection:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 卡方特征选择选择要用于预测分类标签的分类特征。以下代码段执行此选择：
- en: '[PRE63]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now create another data frame for the selected features, as follows:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建另一个选定特征的数据框架，如下所示：
- en: '[PRE64]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '![Topic modeling with Spark MLlib](img/00130.jpeg)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行主题建模](img/00130.jpeg)'
- en: 'Figure 34: Chi-Squared feature selection'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 图34：卡方特征选择
- en: You can see from the preceding output/screenshot that our data is ready for
    the LDA model to be trained and do the topic modeling.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从前面的输出/屏幕截图中看到，我们的数据已准备好用于训练LDA模型并进行主题建模。
- en: '**Step 11: Create and train an LDA model**'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 第11步：创建并训练LDA模型
- en: 'Create and train the LDA model using the training datasets (that is, the data
    frame result) by specifying the *K* (which is the number of clusters the topic
    modeling must have to be >1, where the default value is 10) and max iteration:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据集（即数据框架结果）创建并训练LDA模型，指定*K*（主题建模必须大于1的聚类数，其中默认值为10）和最大迭代次数：
- en: '[PRE65]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now that we have the model trained, fitted, and ready for our purposes, let''s
    look at our output. However, before doing that, we need to have a data frame that
    could capture the topic-related metrics. Use the following code:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练、拟合并准备好用于我们目的的模型，让我们来看看我们的输出。但在这之前，我们需要有一个能够捕获与主题相关的指标的数据框架。使用以下代码：
- en: '[PRE66]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now let''s see the topics distribution. Look at the previous Dataset:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下主题分布。看看前面的数据集：
- en: '[PRE67]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![Topic modeling with Spark MLlib](img/00003.jpeg)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行主题建模](img/00003.jpeg)'
- en: 'Figure 35: Corresponding term weight, topic name, and term indices'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 图35：相应的术语权重、主题名称和术语索引
- en: If you look at the preceding output carefully, we have found the corresponding
    term weight, topic name, and term indices. The preceding terms and their corresponding
    weights will be used in [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced
    Machine Learning with Streaming and Graph Data*, for finding connected components
    using GraphX with Scala.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察前面的输出，我们找到了相应的术语权重、主题名称和术语索引。前述术语及其相应的权重将在[第9章](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "第9章。使用Scala进行流式和图数据的高级机器学习")中使用，*使用流式和图数据进行高级机器学习*，用于使用GraphX和Scala查找连接组件。
- en: However, we also need to have the actual terms. We will show the detailed technique
    of retrieving the terms in [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced
    Machine Learning with Streaming and Graph Data* hat extensively depends on the
    vocabulary of the terms that need to be developed  or generated using the terms
    from the bag of words concept.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们还需要实际的术语。我们将在[第9章](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "第9章。使用Scala进行流式和图数据的高级机器学习")中展示检索术语的详细技术，*使用流式和图数据进行高级机器学习*，这在很大程度上取决于需要开发或生成的术语词汇的概念。
- en: Scalability
  id: totrans-498
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性
- en: 'The previous example shows how to perform topic modeling using the LDA algorithm
    as a standalone application. However, according to a Databricks blog by Joseph
    B. at [https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html](https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html),
    the parallelization of LDA is not straightforward, and there have been many research
    papers proposing different strategies. The key obstacle in this regard is that
    all methods involve a large amount of communication. According to the blog on
    the Databricks website, here are the statistics of the dataset and related training
    and test sets that were used during the experimentation:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子展示了如何使用LDA算法进行主题建模作为独立应用。然而，根据Joseph B.在Databricks博客中的一篇文章[https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html](https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html)，LDA的并行化并不直接，已经有许多研究论文提出了不同的策略。在这方面的关键障碍是所有方法都涉及大量的通信。根据Databricks网站上的博客，以下是在实验过程中使用的数据集和相关训练和测试集的统计数据：
- en: 'Training set size: 4.6 million documents'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集大小：460万份文件
- en: 'Vocabulary size: 1.1 million terms'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇量：110万个术语
- en: 'Training set size: 1.1 billion tokens (~239 words/document)'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集大小：11亿个标记（~每份文件239个单词）
- en: 100 topics
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100个主题
- en: 16-worker EC2 cluster, for example, M4.large or M3.medium depending upon budget
    and requirements
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 16个工作节点的EC2集群，例如M4.large或M3.medium，具体取决于预算和要求
- en: 'Timing results: 176 secs/iteration on average over 10 iterations'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间结果：平均每次迭代176秒/次
- en: Credit risk analysis pipeline with Spark
  id: totrans-506
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行信用风险分析流程
- en: In this section, we will develop a credit risk pipeline that is commonly used
    in financial institutions such as banks and credit unions. First we will discuss
    what credit risk analysis is and why it is important before developing a Spark
    ML-based pipeline using a Random-Forest-based classifier. Finally, we will provide
    some performance improvement suggestions.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开发一个信用风险流程，这在银行和信用合作社等金融机构中通常使用。首先，我们将讨论信用风险分析是什么以及为什么它很重要，然后使用基于Spark
    ML的流程开发基于随机森林的分类器。最后，我们将提供一些建议以提高性能。
- en: What is credit risk analysis? Why is it important?
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是信用风险分析？为什么它很重要？
- en: When an applicant applies for a loan and a bank receives that application, based
    on the applicant's profile, the bank has to make a decision whether to approve
    the loan application or not.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 当申请人申请贷款并且银行收到申请时，基于申请人的资料，银行必须决定是否批准贷款申请。
- en: 'In this regard, there are two types of risk associated with the bank''s decision
    on the loan application:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，银行对贷款申请的决定涉及两种风险：
- en: '**Applicant is a good credit risk**: That means the client or applicant is
    more likely to repay the loan. Then, if the loan is not approved, the bank can
    potentially suffer loss of business.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**申请人是良好的信用风险**：这意味着客户或申请人更有可能偿还贷款。那么，如果贷款未获批准，银行可能会遭受业务损失。'
- en: '**Applicant is a bad credit risk**: That means that the client or applicant
    is most likely not to repay the loan. In that case, approving the loan to the
    client will result in financial loss to the bank.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**申请人是不良的信用风险**：这意味着客户或申请人很可能无法偿还贷款。在这种情况下，向客户批准贷款将导致银行财务损失。'
- en: Our common sense says that the second risk is the greater risk, as the bank
    has a higher chance of not being reimbursed the borrowed amount.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的常识告诉我们，第二种风险是更大的风险，因为银行更有可能无法收回借款金额。
- en: Therefore, most banks or credit unions evaluate the risks associated with lending
    money to a client, applicant, or customer. In business analytics, minimizing the
    risk tends to maximize the profit to the bank itself. In other words, maximizing
    the profit and minimizing the loss from a financial perspective is important.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，大多数银行或信用合作社评估向客户、申请人或顾客放贷所涉及的风险。在商业分析中，最小化风险往往会最大化银行自身的利润。换句话说，从财务角度来看，最大化利润和最小化损失是重要的。
- en: Often, the bank makes a decision about approving a loan application based on
    different factor and parameters of an applicant. For example, the demographic
    and socio-economic conditions regarding their loan application.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，银行会根据申请人的不同因素和参数对贷款申请做出决定。例如，他们的贷款申请的人口统计和社会经济状况。
- en: Developing a credit risk analysis pipeline with Spark ML
  id: totrans-516
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark ML开发信用风险分析流程
- en: In this section, we will first discuss the credit risk dataset in detail in
    order to gain some insight. After that, we will look at how to develop a large-scale
    credit risk pipeline. Finally, we will provide some performance improvement suggestions
    toward better prediction accuracy.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先详细讨论信用风险数据集，以便获得一些见解。之后，我们将看看如何开发大规模的信用风险流程。最后，我们将提供一些性能改进建议，以提高预测准确性。
- en: The dataset exploration
  id: totrans-518
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集探索
- en: 'The German Credit dataset was downloaded from the UCI Machine Learning Repository
    at [https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/).
    Although a detailed description of the dataset is available in the link, we provide
    some brief insights here in *Table 3*. The data contains credit-related data on
    21 variables and the classification of whether an applicant is considered a good
    or a bad credit risk for 1000 loan applicants. *Table 3* shows details about each
    variable that was considered before making the dataset available online:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 德国信用数据集是从UCI机器学习库[https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)下载的。尽管链接中提供了数据集的详细描述，但我们在*表3*中提供了一些简要见解。数据包含21个变量的与信用有关的数据，以及1000个贷款申请人被认为是良好还是不良信用风险的分类。*表3*显示了在将数据集提供在线之前考虑的每个变量的详细信息：
- en: '| **Entry** | **Variable** | **Explanation** |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| **条目** | **变量** | **解释** |'
- en: '| 1 | `creditability` | Capable of repaying |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `creditability` | 有偿还能力 |'
- en: '| 2 | `balance` | Current balance |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `balance` | 当前余额 |'
- en: '| 3 | `duration` | Duration of the loan being applied for |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `duration` | 申请贷款的期限 |'
- en: '| 4 | `history` | Is there any bad loan history? |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| 4 | `history` | 是否有不良贷款历史？ |'
- en: '| 5 | `purpose` | Purpose of the loan |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| 5 | `purpose` | 贷款目的 |'
- en: '| 6 | `amount` | Amount being applied for |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| 6 | `amount` | 申请金额 |'
- en: '| 7 | `savings` | Monthly saving |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| 7 | `savings` | 每月储蓄 |'
- en: '| 8 | `employment` | Employment status |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| 8 | `employment` | 就业状态 |'
- en: '| 9 | `instPercent` | Interest percent |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| 9 | `instPercent` | 利息百分比 |'
- en: '| 10 | `sexMarried` | Sex and marriage status |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| 10 | `sexMarried` | 性别和婚姻状况 |'
- en: '| 11 | `guarantors` | Are there any guarantors? |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| 11 | `guarantors` | 是否有担保人？ |'
- en: '| 12 | `residenceDuration` | Duration of residence at the current address |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| 12 | `residenceDuration` | 目前地址居住时间 |'
- en: '| 13 | `assets` | Net assets |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| 13 | `assets` | 净资产 |'
- en: '| 14 | `age` | Age of the applicant |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| 14 | `age` | 申请人年龄 |'
- en: '| 15 | `concCredit` | Concurrent credit |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| 15 | `concCredit` | 并发信用 |'
- en: '| 16 | `apartment` | Residential status |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| 16 | `apartment` | 住宅状况 |'
- en: '| 17 | `credits` | Current credits |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| 17 | `credits` | 当前信用 |'
- en: '| 18 | `occupation` | Occupation |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: 18 | `occupation` | 职业 |
- en: '| 19 | `dependents` | Number of dependents |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| 19 | `dependents` | 受抚养人数 |'
- en: '| 20 | `hasPhone` | If the applicant uses a phone |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 20 | `hasPhone` | 申请人是否使用电话 |'
- en: '| 21 | `foreign` | If the applicant is a foreigner |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| 21 | `foreign` | 申请人是否是外国人 |'
- en: 'Table 3: German credit dataset properties'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：德国信用数据集属性
- en: Note that, although *Table 3* describes the variables in the dataset, there
    is no associated header. In *Table 3*, we have shown the variable, position, and
    associated significance of each variable.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管*表3*描述了数据集中的变量，但没有相关的标题。在*表3*中，我们显示了每个变量的位置和相关重要性。
- en: Credit risk pipeline with Spark ML
  id: totrans-544
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark ML的信用风险流程
- en: There will be several steps involved, from data loading, parsing, data preparation,
    training testing set preparation, model training, model evaluation, and result
    interpretation. Let's go through the steps one by one.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及到几个步骤，从数据加载、解析、数据准备、训练测试集准备、模型训练、模型评估和结果解释。让我们逐步进行这些步骤。
- en: '**Step 1: Load required APIs and libraries**'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：加载所需的API和库**'
- en: 'The following is the code for loading the required APIs and libraries:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载所需API和库的代码：
- en: '[PRE68]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '**Step 2: Create a Spark session**'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：创建Spark会话**'
- en: 'The following is another code for creating a Spark session:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是另一个创建Spark会话的代码：
- en: '[PRE69]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '**Step 3: Load and parse the credit risk dataset**'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：加载和解析信用风险数据集**'
- en: 'Note that the dataset is in **Comma-Separated Value** (**CSV**) format. Now
    load and parse the dataset using the Databricks-provided CSV readers and prepare
    a Dataset of Row, as follows:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据集采用**逗号分隔值**（**CSV**）格式。现在使用Databricks提供的CSV读取器加载和解析数据集，并准备一个Row数据集，如下所示：
- en: '[PRE70]'
  id: totrans-554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now, show the Dataset to get to know the exact structure, as follows:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，显示数据集以了解确切的结构，如下所示：
- en: '[PRE71]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '![Credit risk pipeline with Spark ML](img/00104.jpeg)'
  id: totrans-557
  prefs: []
  type: TYPE_IMG
  zh: '![Spark ML的信用风险流程](img/00104.jpeg)'
- en: 'Figure 36: A snapshot of the credit risk dataset'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 图36：信用风险数据集快照
- en: '**Step 4: Create an RDD of type Credit**'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：创建Credit类型的RDD**'
- en: 'Create an RDD of typed class `Credit`, as follows:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个类型为`Credit`的RDD，如下所示：
- en: '[PRE72]'
  id: totrans-561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The preceding code segments creates an RDD of type `Credit` after taking the
    variable as double values by using the `parseDouble()` method, which takes a string
    and returns the corresponding value in `Double` format. The `parseDouble()` method
    goes as follows:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码段在使用`parseDouble()`方法将变量作为双精度值创建了一个`Credit`类型的RDD，该方法接受一个字符串并以`Double`格式返回相应的值。`parseDouble()`方法如下所示：
- en: '[PRE73]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Now we need to know the structure of the `Credit` class so that the structure
    itself helps to create the RDDs using the typed class.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要了解`Credit`类的结构，以便结构本身有助于使用类型化类创建RDD。
- en: 'Well, the `Credit` class is basically a singleton class that initializes all
    the setter and getter methods for the 21 variables from the dataset through the
    constructor. Here is the class:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，`Credit`类基本上是一个单例类，通过构造函数初始化数据集中的21个变量的所有setter和getter方法。以下是该类：
- en: '[PRE74]'
  id: totrans-566
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: If you look at the flow of the class, at first it declares 21 variables for
    the 21 features in the dataset. Then it initializes them using the constructor.
    The rest are simple setter and getter methods.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看类的流程，首先它声明了21个变量，对应数据集中的21个特征。然后使用构造函数对它们进行初始化。其余的是简单的setter和getter方法。
- en: '**Step 5: Create a Dataset of type Row from the RDD of type Credit**'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：从Credit类型的RDD创建类型为Row的数据集**'
- en: 'The following code shows how to create a Dataset of type Row:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示如何创建类型为Row的数据集：
- en: '[PRE75]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Now save the Dataset as a temporary view, or more formally, a table in-memory
    for query purposes, as follows:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将数据集保存为临时视图，或更正式地说，保存为内存中的表以供查询，如下所示：
- en: '[PRE76]'
  id: totrans-572
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Now let''s get to know the schema of the table as follows:'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们了解表的模式，如下所示：
- en: '[PRE77]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '![Credit risk pipeline with Spark ML](img/00102.jpeg)'
  id: totrans-575
  prefs: []
  type: TYPE_IMG
  zh: '![Spark ML的信用风险流程](img/00102.jpeg)'
- en: 'Figure 37: The schema of the Dataset'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 图37：数据集的模式
- en: '**Step 6: Create the feature vector using the VectorAssembler**'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：使用VectorAssembler创建特征向量**'
- en: 'Create a new feature vector for the 21 variables using the `VectorAssembler`
    class of Spark, as follows:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark的`VectorAssembler`类为21个变量创建一个新的特征向量，如下所示：
- en: '[PRE78]'
  id: totrans-579
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '**Step 7: Create a Dataset by combining and transforming the assembler**'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：通过组合和转换组装器创建数据集**'
- en: 'Create a Dataset by transforming the assembler using the `creditData` Dataset
    previously created, and print the first top 20 rows of the Dataset, as follows:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用先前创建的`creditData`数据集转换组装器来创建一个数据集，并打印数据集的前20行，如下所示：
- en: '[PRE79]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '![Credit risk pipeline with Spark ML](img/00087.jpeg)'
  id: totrans-583
  prefs: []
  type: TYPE_IMG
  zh: '![Spark ML的信用风险流程](img/00087.jpeg)'
- en: 'Figure 38: Newly created featured Credit Dataset'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 图38：新创建的特色信用数据集
- en: '**Step 8: Create label for making predictions**'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8：创建用于预测的标签**'
- en: 'Create a label column out of the creditability column of the preceding Dataset
    (*Figure 38*), as follows:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的数据集（*图38*）的信用度列创建一个标签列，如下所示：
- en: '[PRE80]'
  id: totrans-587
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now let''s explore the new Dataset using the `show()` method as follows:'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用`show()`方法来探索新的数据集，如下所示：
- en: '[PRE81]'
  id: totrans-589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '![Credit risk pipeline with Spark ML](img/00056.jpeg)'
  id: totrans-590
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark ML的信用风险管道](img/00056.jpeg)'
- en: 'Figure 39: Dataset with a new label column'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 图39：带有新标签列的数据集
- en: From the preceding figure, we can understand that there are only two labels
    associated with the Dataset, which are 1.0 and 0.0\. That signifies the problem
    as a binary classification problem.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以了解到与数据集相关的标签只有两个，分别是1.0和0.0。这表明问题是一个二元分类问题。
- en: '**Step 9: Prepare the training and test set**'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9：准备训练和测试集**'
- en: 'Prepare the training and test set as follows:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 按以下方式准备训练和测试集：
- en: '[PRE82]'
  id: totrans-595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Here, the ratio is 70% and 30% for the training and testing set, respectively,
    with a long seed value to disallow the random result generation in each iteration.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，训练集和测试集的比例分别为70%和30%，种子值较长，以防止在每次迭代中生成随机结果。
- en: '**Step 10: Train the Random Forest model**'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤10：训练随机森林模型**'
- en: 'To train the Random Forest model, use the following code:'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练随机森林模型，请使用以下代码：
- en: '[PRE83]'
  id: totrans-599
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'As previously mentioned, the problem is a binary classification problem. Therefore,
    we will evaluate the Random Forest model using a binary evaluator for the `label`
    column, as follows:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，问题是一个二元分类问题。因此，我们将使用二元评估器对`label`列评估随机森林模型，如下所示：
- en: '[PRE84]'
  id: totrans-601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Now we need to collect the model performance metric on the test set that goes
    as follows:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要在测试集上收集模型性能指标，如下所示：
- en: '[PRE85]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '**Step 11: Print the performance parameters**'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤11：打印性能参数**'
- en: 'We will observe several performance parameters of the binary evaluator, for
    example, accuracy after fitting the model, **Mean Square Error** (**MSE**), **Mean
    Absolutize Error** (**MAE**), **Root Mean Squared Error** (**RMSE**), R Squared
    and explained variable, and so on. Let''s do it as follows:'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将观察二元评估器的几个性能参数，例如拟合模型后的准确性，**均方误差**（**MSE**），**平均绝对误差**（**MAE**），**均方根误差**（**RMSE**），R平方和解释变量等。让我们按照以下方式进行：
- en: '[PRE86]'
  id: totrans-606
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The preceding code segment generates the following output:'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码段生成了以下输出：
- en: '[PRE87]'
  id: totrans-608
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Performance tuning and suggestions
  id: totrans-609
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能调优和建议
- en: If you look at the performance metrics in Step 11, it is obvious that the credit
    risk predictions are not satisfactory, especially in terms of accuracy, which
    is only 76.22%. That means that for the given test data, our model can predict
    if there is a credit risk with 76.22% precision. Since we need to be more careful
    about such sensitive financial sectors, therefore, more accuracy is desired no
    doubt.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看第11步的性能指标，显然信用风险预测不尽如人意，特别是在准确性方面，只有76.22%。这意味着对于给定的测试数据，我们的模型可以以76.22%的精度预测是否存在信用风险。由于我们需要对这些敏感的金融领域更加小心，因此无疑需要更高的准确性。
- en: Now, if you want to increase the prediction performance, you should try training
    your model using a model other than the Random-Forest-based classifier. For example,
    a Logistic Regression or NaÃ¯ve Baseyan-based classifier.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您想提高预测性能，您应该尝试使用除基于随机森林的分类器之外的其他模型进行模型训练。例如，逻辑回归或朴素贝叶斯分类器。
- en: Moreover, you can use the SVM-based classifier or neural-network-based Multilayer
    Perceptron classifier. In [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*,
    we will look at how to tune the hyper parameters in order to select the best model.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以使用基于SVM的分类器或基于神经网络的多层感知器分类器。在[第7章](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "第7章。调整机器学习模型")中，*调整机器学习模型*，我们将看看如何调整超参数以选择最佳模型。
- en: Scaling the ML pipelines
  id: totrans-613
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展ML管道
- en: Data mining and machine learning algorithms impose outstanding challenges on
    parallel and distributed computing platforms. Furthermore, parallelizing the machine
    learning algorithms is highly task-specific and often depends on the preceding
    questions. In [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark"), *Introduction to Data
    Analytics with Spark*, we discussed and showed how to deploy the same machine
    learning application on top of a cluster or cloud computing infrastructure (that
    is, Amazon AWS/EC2).
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 数据挖掘和机器学习算法对并行和分布式计算平台提出了巨大挑战。此外，并行化机器学习算法高度依赖于任务的特定性，并且通常取决于前面提到的问题。在[第1章](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "第1章。使用Spark进行数据分析简介")中，*使用Spark进行数据分析简介*，我们讨论并展示了如何在集群或云计算基础设施（即Amazon AWS/EC2）上部署相同的机器学习应用。
- en: Following that method, we can handle datasets with enormous batch sizes or in
    real time. In addition to this, scaling up the machine learning applications evolves
    another trade-off such as cost, complexity, run-time, and technical requirements.
    Furthermore, making task-appropriate algorithm and platform choices for large-scale
    machine learning requires an understanding of the benefits, trade-offs, and constraints
    of the available options.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这种方法，我们可以处理具有巨大批量大小或实时处理的数据集。除此之外，扩展机器学习应用还涉及到成本、复杂性、运行时间和技术要求等其他权衡。此外，为了进行大规模机器学习的任务，需要了解可用选项的优势、权衡和约束，以做出适合任务的算法和平台选择。
- en: 'To handle these issues, in this section, we will provide some theoretical aspects
    of handling big Datasets for deploying large-scale machine learning applications.
    However, before going any further, we need to know the answer to some questions.
    For example:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，在本节中，我们将提供一些处理大型数据集以部署大规模机器学习应用的理论方面。然而，在进一步进行之前，我们需要知道一些问题的答案。例如：
- en: How do we collect the big dataset to fulfil our needs?
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何收集大型数据集以满足我们的需求？
- en: How large are the big datasets and how do we handle them?
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据集有多大，我们如何处理它们？
- en: How much training data is enough to scale-up the ML application on a big dataset?
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有多少训练数据足以扩展大型数据集上的ML应用？
- en: What is an alternative approach if we don't have enough training data?
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们没有足够的训练数据，有什么替代方法？
- en: What sorts of machine learning algorithms should be used for fulfilling our
    needs?
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该使用什么样的机器学习算法来满足我们的需求？
- en: What platform should be chosen for parallel learning?
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应选择哪种平台进行并行学习？
- en: Here we discuss some important aspects of deploying and scaling up a machine
    learning application that handles the preceding big data challenges, including
    size, data skewness, cost, and infrastructure.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论了部署和扩展处理前述大数据挑战的机器学习应用的一些重要方面，包括大小、数据偏斜、成本和基础设施。
- en: Size matters
  id: totrans-624
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大小很重要
- en: Big data is data in volume, variety, veracity, velocity, and value that is too
    great to process by traditional in-memory computer systems. Scaling up machine
    learning applications by handling big data involves tasks such as classification,
    clustering, regression, feature selection, boosted decision trees, and SVMs. How
    do we handle 1 billion or 1 trillion data instances? Moreover, 5 billion cell
    phones, social networks such as Twitter produce big datasets in an unprecedented
    way. On the other hand, crowdsourcing is the reality, which is labeling of 100,000+
    data instances within a week.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据是指量、种类、真实性、速度和价值都太大，以至于传统的内存计算机系统无法处理。通过处理大数据来扩展机器学习应用涉及任务，如分类、聚类、回归、特征选择、提升决策树和支持向量机。我们如何处理10亿或1万亿的数据实例？此外，50亿部手机、Twitter等社交网络以前所未有的方式产生大数据集。另一方面，众包是现实，即在一周内标记10万个以上的数据实例。
- en: In terms of sparsity, Big datasets cannot be too sparse but dense from a content
    perspective. From the machine learning perspective, to justify this claim, let's
    think of an example of data labeling. For instance, 1M data instances cannot belong
    to 1M classes, simply because it's not practical to have 1M classes but more than
    once data instances belong to a particular class. Therefore, based on the sparsity
    and size of such a large-scale dataset, making predictive analytics is another
    challenge, which needs to be considered and handled while scaling up.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 就稀疏性而言，大数据集不能太稀疏，而从内容角度来看要密集。从机器学习的角度来看，为了证明这一点，让我们想象一个数据标记的例子。例如，100万个数据实例不可能属于100万个类别，因为拥有100万个类别是不切实际的，而且多个数据实例属于特定类别。因此，基于如此大规模数据集的稀疏性和大小，进行预测分析是另一个需要考虑和处理的挑战。
- en: Size versus skewness considerations
  id: totrans-627
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大小与偏斜度考虑
- en: Machine learning also depends on the availability of labeled data, and its trustworthiness
    is based on the learning task, such as supervised, unsupervised, or semi-supervised.
    You might have a structured dataset, but with extreme skewness. More specifically,
    suppose you have 1K labeled and 1M unlabeled data points, so the labeled and unlabeled
    ratio is 0.1%.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习还取决于标记数据的可用性，其可信度取决于学习任务，如监督、无监督或半监督。您可能拥有结构化数据，但存在极端的偏斜。更具体地说，假设您有1K个标记和1M个未标记的数据点，因此标记和未标记的比例为0.1%。
- en: Therefore, do you think that only the 1K label points are enough to train a
    supervised model? As another example, suppose, for instance, that you have 1M
    labeled and 1B unlabeled data points, where the labeled and unlabeled ratio is
    also 0.1%. Again, the same question arises, which is, is it enough to have only
    the 1M labels to train a supervised model?
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您认为只有1K标签点足以训练监督模型吗？再举一个例子，假设您有1M个标记和1B个未标记的数据点，其中标记和未标记的比例也是0.1%。同样，又出现了同样的问题，即，仅有1M个标签足以训练监督模型吗？
- en: Now the concern is what can be done or approach using existing labels as only
    guidance rather than a directive for the semi-supervised clustering, classification,
    or regression. Alternatively, label more data, either manually, or with a little
    help from the crowd. For example, suppose someone wants to cluster or classify
    analysis on a disease. More specifically, suppose we want to classify tweets,
    if particular tweets indicate an Ebola- or flu-related disease. In this case,
    we should use the semi-supervised approach for labeling the tweets.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，使用现有标签作为半监督聚类、分类或回归的指导而不是指令，可以采取什么措施或方法。或者，标记更多的数据，要么手动标记，要么在众包的帮助下。例如，假设有人想对某种疾病进行聚类或分类分析。更具体地说，假设我们想对推文进行分类，看特定的推文是否指示出与埃博拉或流感相关的疾病。在这种情况下，我们应该使用半监督方法来标记推文。
- en: However, in this case, a dataset might be very skewed, or labeling might be
    biased. Usually, the training data comes from different users, where an explicit
    user feedback might often be misleading.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这种情况下，数据集可能非常偏斜，或者标记可能存在偏见。通常，训练数据来自不同的用户，其中显式用户反馈可能经常具有误导性。
- en: Therefore, learning from the implicit feedback is a better idea; for example,
    collecting data by clicking on web search results. In these types of large-scale
    datasets, the skewness of the training data is hard to detect, as discussed in
    [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a "Chapter 4. Extracting
    Knowledge through Feature Engineering"), *Extracting Knowledge through Feature
    Engineering*. Therefore, be wary of this skewness in big Datasets.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从隐式反馈中学习是一个更好的主意；例如，通过点击网络搜索结果收集数据。在这些类型的大规模数据集中，训练数据的偏斜很难检测到，如在[第4章](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "第4章。通过特征工程提取知识")中讨论的，*通过特征工程提取知识*。因此，在大数据集中要警惕这种偏斜。
- en: Cost and infrastructure
  id: totrans-633
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本和基础设施
- en: To scale-up your machine learning application, you will need better infrastructure
    and computing power to handle such big datasets. Initially, you might want to
    utilize a local cluster. However, sometimes, the cluster might not be enough to
    scale-up your ML application if the dataset increases exponentially.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 要扩展您的机器学习应用，您将需要更好的基础设施和计算能力来处理如此庞大的数据集。最初，您可能希望利用本地集群。然而，有时，如果数据集呈指数增长，集群可能不足以扩展您的机器学习应用。
- en: As discussed in the chapter about deploying the ML pipeline on powerful infrastructure
    such as Amazon AWS cloud computing like EC2, you will have to go for pay-as-you-go
    to enjoy the cloud as Platform as a Service and Infrastructure as a Service, even
    though you use your own ML application as the Software as a Service.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署ML管道到强大基础设施（如亚马逊AWS云计算，如EC2）的章节中，您将不得不选择按使用量付费，以享受云作为平台即服务和基础设施即服务，即使您使用自己的ML应用作为软件即服务。
- en: Tips and performance considerations
  id: totrans-636
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示和性能考虑
- en: Spark also supports cross-validation for hyperparameter tuning that will be
    discussed broadly in the following chapter. Spark's view toward the cross-validation
    as a meta-algorithm, which fits the underlying estimator with user-specified combinations
    of parameters, cross-evaluates the fitted models and outputs the best one.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: Spark还支持用于超参数调整的交叉验证，这将在下一章中广泛讨论。Spark将交叉验证视为一种元算法，它使用用户指定的参数组合来拟合底层估计器，交叉评估拟合模型并输出最佳模型。
- en: However, there is no specific requirement of the underlying estimator, which
    could be a pipeline, as long as it can be paired with an evaluator that outputs
    a scalar metric, such as precision and recall, from the predictions.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于底层估计器并没有特定的要求，它可以是一个管道，只要它能够与一个评估器配对，输出预测的标量度量，如精度和召回率。
- en: 'Let''s recall the OCR prediction, where we found that the precision is 75%,
    which is obviously not satisfactory, once again. To further investigate the reason
    now, let''s prints the confusion matrix for the label 8.0 or "I". If you look
    at the following matrix in *Figure 40*, you will find the number of correctly
    predicted instances is low:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾OCR预测，我们发现精度为75%，显然是不令人满意的。现在让我们进一步调查原因，打印出标签8.0或“I”的混淆矩阵。如果您查看*图40*中的矩阵，您会发现正确预测的实例数量很少：
- en: '![Tips and performance considerations](img/00045.jpeg)'
  id: totrans-640
  prefs: []
  type: TYPE_IMG
  zh: '![提示和性能考虑](img/00045.jpeg)'
- en: 'Figure 40: The confusion matrix for the label 8.0 or "I"'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 图40：标签8.0或“I”的混淆矩阵
- en: 'Now let''s try to use the Random Forest model to do the prediction. But before
    going into the model training step, let''s do some initialization of the parameters
    that are needed for the Random Forest classifier, which also supports multiclass
    classification such as the logistic regression model with LBFGS:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试使用随机森林模型进行预测。但在进入模型训练步骤之前，让我们对随机森林分类器所需的参数进行一些初始化，它也支持多类分类，如LBFGS的逻辑回归模型：
- en: '[PRE88]'
  id: totrans-643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Now train the model by specifying the previous parameters, as follows:'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 现在通过指定先前的参数来训练模型，如下所示：
- en: '[PRE89]'
  id: totrans-645
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Now let''s see how it performs. We''ll reuse the same code segments that we
    used in step 9\. Refer to the following screenshot:'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看它的表现。我们将重用我们在第9步中使用的相同代码段。请参考以下截图：
- en: '![Tips and performance considerations](img/00027.jpeg)'
  id: totrans-647
  prefs: []
  type: TYPE_IMG
  zh: '![提示和性能考虑](img/00027.jpeg)'
- en: 'Figure 41: Performance metrics for the precision and recall'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 图41：精度和召回率的性能指标
- en: '![Tips and performance considerations](img/00154.jpeg)'
  id: totrans-649
  prefs: []
  type: TYPE_IMG
  zh: '![提示和性能考虑](img/00154.jpeg)'
- en: 'Figure 42: The improved confusion matrix for the label 8.0 or "I"'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 图42：标签8.0或“I”的改进混淆矩阵
- en: If you look at *Figure 42*, we have a significant improvement in terms of all
    the parameters printed, and the precision has been increased from 75.30% to 89.20%.
    The reason behind this is the improved interpretation of the Random Forest model
    toward the global maxima calculation for prediction accuracy and the confusion
    matrix, as shown in *Figure 38*; you will find significant improvements in the
    number of predicted instances marked by a diagonal arrow.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看*图42*，您会发现在所有打印的参数方面有显著的改善，精度已从75.30%提高到89.20%。这背后的原因是随机森林模型对于全局最大值计算的改进解释，以及混淆矩阵，如*图38*所示；您会发现由对角箭头标记的预测实例数量有显著改善。
- en: Through a process of trial and error, you can settle on a short list of algorithms
    that show promise, but how do you know which is the best? Moreover, as previously
    mentioned, it is difficult to find a well-performing machine learning algorithm
    for your dataset. Therefore, if you are still unsatisfied with the accuracy of
    89.20%, I suggest you tune the parametric values and look at the precision and
    the recall.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反复试验，您可以确定一组显示潜力的算法，但是您如何知道哪个是最好的呢？此外，如前所述，很难为您的数据集找到表现良好的机器学习算法。因此，如果您对89.20%的准确性仍然不满意，我建议您调整参数值并查看精度和召回率。
- en: Summary
  id: totrans-653
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have shown several machine learning applications and tried
    to differentiate between Spark MLlib and Spark ML. We also showed that it's really
    difficult to develop a complete machine learning application using only Spark
    ML or Spark MLlib.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了几个机器学习应用，并试图区分Spark MLlib和Spark ML。我们还表明，仅使用Spark ML或Spark MLlib开发完整的机器学习应用是非常困难的。
- en: However, we would like to argue that a combined approach, or interoperability,
    between these two APIs would be best for these purposes. In addition, we learned
    how to build an ML pipeline by using both Spark ML libraries and how to scale-up
    the basic model by considering some performance considerations.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们想要提出一个结合的方法，或者说这两个API之间的互操作性，对于这些目的来说是最好的。此外，我们学习了如何使用Spark ML库构建ML管道，以及如何通过考虑一些性能考虑因素来扩展基本模型。
- en: Tuning an algorithm or machine learning application can simply be thought of
    as a process by which one goes through to optimize the parameters that impact
    the model in order to enable the algorithm to perform at its best (in terms of
    runtime and memory usage).
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 调整算法或机器学习应用可以简单地被认为是一个过程，通过这个过程，您可以优化影响模型的参数，以使算法在其最佳状态下运行（就运行时间和内存使用而言）。
- en: In [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*,
    we will discuss more about tuning machine learning models. We will try to reuse
    some of the applications from this chapter and [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Examples*, to tune up the performance by tuning several
    parameters.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d "第7章.
    调整机器学习模型")中，*调整机器学习模型*，我们将更多地讨论调整机器学习模型的内容。我们将尝试重用本章和[第5章](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "第5章. 通过示例进行监督和无监督学习")中的一些应用，通过调整几个参数来提高性能。
