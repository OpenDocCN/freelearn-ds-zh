- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Strategies for Global Deep Learning Forecasting Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全球深度学习预测模型策略
- en: All through the last few chapters, we have been building up deep learning for
    time series forecasting. We started with the basics of deep learning, saw the
    different building blocks, practically used some of those building blocks to generate
    forecasts on a sample household, and finally, talked about attention and Transformers.
    Now, let’s slightly alter our trajectory and take a look at global models for
    deep learning. In *Chapter 10*, *Global Forecasting Models*, we saw why global
    models make sense and also saw how we can use such models in the machine learning
    context. We even got good results in our experiments. In this chapter, we will
    look at how we can apply similar concepts, but from a deep learning perspective.
    We will look at different strategies that we can use to make global deep learning
    models work better.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们一直在建立时间序列预测的深度学习模型。我们从深度学习的基础开始，了解了不同的构建模块，并实际使用了其中一些构建模块来对一个样本家庭进行预测，最后讨论了注意力和Transformer。现在，让我们稍微改变方向，看看全球深度学习模型。在*第10章*“全球预测模型”中，我们看到了为什么全球模型是有意义的，还看到了如何在机器学习背景下使用这些模型。我们在实验中甚至得到了良好的结果。在本章中，我们将探讨如何从深度学习的角度应用类似的概念。我们将探讨可以使全球深度学习模型更好运作的不同策略。
- en: 'In this chapter, we will be covering these main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Creating global deep learning forecasting models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建全球深度学习预测模型
- en: Using time-varying information
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用时变信息
- en: Using static/meta information
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用静态/元信息
- en: Using the scale of the time series
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用时间序列的规模
- en: Balancing the sampling procedure
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡采样过程
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need to set up the **Anaconda** environment by following the instructions
    in the *Preface* to get a working environment with all the libraries and datasets
    required for the code in this book. Any additional libraries will be installed
    while running the notebooks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要按照*前言*中的说明设置**Anaconda**环境，以便获取本书代码所需的所有库和数据集的工作环境。在运行笔记本时，会安装任何额外的库。
- en: 'You will need to run these notebooks:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你将需要运行这些笔记本：
- en: '`02-Preprocessing_London_Smart_Meter_Dataset.ipynb` in `Chapter02`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02-Preprocessing_London_Smart_Meter_Dataset.ipynb`在`Chapter02`'
- en: '`01-Setting_up_Experiment_Harness.ipynb` in `Chapter04`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01-Setting_up_Experiment_Harness.ipynb`在`Chapter04`'
- en: '`01-Feature_Engineering.ipynb` in `Chapter06`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01-Feature_Engineering.ipynb`在`Chapter06`'
- en: The associated code for the chapter can be found at [https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter15](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter15).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的相关代码可以在[https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter15](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter15)找到。
- en: Creating global deep learning forecasting models
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建全球深度学习预测模型
- en: In *Chapter 10*, *Global Forecasting Models*, we talked in detail about why
    a global model makes sense. We talked about the benefits regarding increased *sample
    size*, *cross-learning*, *multi-task learning*, the regularization effect that
    comes with it, and reduced *engineering complexity*. All of these are relevant
    for a deep learning model as well. Engineering complexity and sample size become
    even more important because deep learning models are data-hungry and take quite
    a bit more engineering effort and training time than other machine learning models.
    I would go to the extent that in the deep learning context, in most practical
    cases where we have to forecast at scale, global models are the only deep learning
    paradigm that makes sense.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第10章*“全球预测模型”中，我们详细讨论了为什么全球模型是有意义的。我们详细讨论了增加*样本大小*、*跨学习*、*多任务学习*以及与之相关的正则化效应，以及减少的*工程复杂性*的好处。所有这些对深度学习模型同样适用。工程复杂性和样本大小变得更为重要，因为深度学习模型对数据需求量大，并且比其他机器学习模型需要更多的工程工作和训练时间。在深度学习背景下，我认为在大多数需要大规模预测的实际情况中，全球模型是唯一有意义的深度学习范式。
- en: So, why did we spend all that time looking at individual models? Well, it’s
    easier to grasp the concept at that level, and the skills and knowledge we gained
    at that level are very easily transferred to a global modeling paradigm. In *Chapter
    13*, *Common Modeling Patterns for Time Series*, we saw how we can use a dataloader
    to sample windows from a single time series to train the model. To make the model
    a global model, all we need to do is to change the dataloader so that instead
    of sampling windows from a single time series, we sample from many time series.
    The sampling process can be thought of as a two-step process (although in practice,
    we do it in a single step, it is intuitive to think of it as two)—first, sample
    the time series we need to pick the window from, and then, sample the window from
    that time series. By doing that, we are training a single deep learning model
    to forecast all the time series together.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们要花这么多时间研究单一模型呢？其实，从这个层面上理解概念更容易，而且我们在这个层面获得的技能和知识非常容易转移到全球建模的范式中。在*第13章*，*时间序列的常见建模模式*中，我们看到如何使用数据加载器从单一时间序列中抽样窗口来训练模型。为了使模型成为一个全球模型，我们需要做的就是改变数据加载器，使其不再从单一时间序列中抽样窗口，而是从多个时间序列中抽样。这个抽样过程可以看作是一个两步走的过程（尽管在实践中我们是一气呵成的，但从直观上讲它是两个步骤）—首先，从需要选择窗口的时间序列中抽样，然后，从该时间序列中抽样窗口。通过这样做，我们正在训练一个单一的深度学习模型来一起预测所有时间序列。
- en: To make our lives easier, we are going to use the open-source libraries PyTorch
    Forecasting and `neuralforecast` from Nixtla in this text. We will be using PyTorch
    Forecasting for pedagogical purposes because it does provide more flexibility,
    but `neuralforecast` is more current and actively maintained and therefore more
    recent architectures will be added there. In *Chapter 16*, we will see how we
    can use `neuralforecast` to do forecasting, but for now, let’s pick PyTorch Forecasting
    and move ahead.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的生活更轻松，本文将使用开源库 PyTorch Forecasting 和 Nixtla 的 `neuralforecast`。我们将出于教学目的使用
    PyTorch Forecasting，因为它提供了更多的灵活性，但 `neuralforecast` 更为现代且在积极维护，因此更近期的架构将添加到该库中。在*第16章*中，我们将看到如何使用
    `neuralforecast` 进行预测，但现在让我们选择 PyTorch Forecasting 继续前进。
- en: PyTorch Forecasting aims to make time series forecasting with deep learning
    easy for both research and real-world cases alike. PyTorch Forecasting also has
    implementations for some state-of-the-art forecasting architectures, and we will
    come back to those in *Chapter 16*, *Specialized Deep Learning Architectures for
    Forecasting*. But now, let’s use the high-level API in PyTorch Forecasting. This
    will significantly reduce our work in preparing `PyTorch` datasets. The `TimeSeriesDataset`
    class in PyTorch Forecasting takes care of a lot of boilerplate code dealing with
    different transformations, missing values, padding, and so on. We will be using
    this framework in this chapter when we look at different strategies to implement
    global deep learning forecasting models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Forecasting 旨在使深度学习时间序列预测对于研究和实际应用都变得更加简便。PyTorch Forecasting 还实现了一些最先进的预测架构，我们将在*第16章*中回顾这些架构，标题为*专用深度学习架构用于预测*。但现在，让我们使用
    PyTorch Forecasting 的高级 API。这样可以大大减少我们在准备 `PyTorch` 数据集时的工作量。PyTorch Forecasting
    中的 `TimeSeriesDataset` 类处理了许多样板代码，涉及不同的转换、缺失值、填充等问题。在本章中，我们将使用这个框架来探讨实现全球深度学习预测模型的不同策略。
- en: '**Notebook alert**:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提示**：'
- en: 'To follow along with the complete code, use the notebook named `01-Global_Deep_Learning_Models.ipynb`
    in the `Chapter15` folder. There are two variables in the notebook that act as
    a switch—`TRAIN_SUBSAMPLE = True` makes the notebook run for a subset of 10 households.
    `train_model = True` makes the notebook train different models (warning: training
    models on the full data takes upward of 3 hours each). `train_model = False` loads
    the trained model weights and predicts on them.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要完整跟随代码，请使用 `Chapter15` 文件夹中的名为 `01-Global_Deep_Learning_Models.ipynb` 的笔记本。笔记本中有两个变量作为开关—`TRAIN_SUBSAMPLE
    = True` 会使笔记本仅在 10 个家庭的子集上运行。`train_model = True` 会使笔记本训练不同的模型（警告：在完整数据集上训练模型需要超过
    3 小时）。`train_model = False` 会加载训练好的模型权重并进行预测。
- en: Preprocessing the data
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'We start by loading the necessary libraries and the dataset. We are using the
    preprocessed and feature-engineered dataset we created in *Chapter 6*, *Feature
    Engineering for Time Series Forecasting*. There are different kinds of features
    in the dataset and to make our feature assignment standardized, we use `namedtuple`.
    `namedtuple()` is a factory method in collections that lets you create subclasses
    of `tuple` with named fields. These named fields can be accessed using dot notation.
    We define `namedtuple` like this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从加载必要的库和数据集开始。我们使用的是在*第六章*中创建的经过预处理和特征工程处理的数据集，*时间序列预测的特征工程*。数据集中有不同种类的特征，为了使我们的特征分配标准化，我们使用`namedtuple`。`namedtuple()`是`collections`中的一个工厂方法，允许你创建带有命名字段的`tuple`子类。这些命名字段可以通过点表示法进行访问。我们这样定义`namedtuple`：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s also quickly establish what these names mean:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速了解一下这些名称的含义：
- en: '`target`: The column name of what we are trying to forecast.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target`：我们尝试预测的目标的列名。'
- en: '`index_cols`: The columns that we need to make as an index for quick access
    to data.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index_cols`：我们需要将这些列设置为索引，以便快速访问数据。'
- en: '`static_categoricals`: These are columns that are categorical in nature and
    do not change with time. They are specific to each time series. For instance,
    the *Acorn group* in our dataset is `static_categorical` because it is categorical
    in nature and is a value pertaining to a household.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_categoricals`：这些列是分类性质的，并且不会随着时间变化。它们是每个时间序列特有的。例如，我们数据集中的*Acorn组*是`static_categorical`，因为它是分类性质的，并且是一个与家庭相关的值。'
- en: '`static_reals`: These are columns that are numeric in nature and do not change
    with time. They are specific to each time series. For instance, the average energy
    consumption in our dataset is numeric in nature and pertains to a single household.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_reals`：这些列是数值型的，并且随着时间的推移不会变化。它们是每个时间序列特有的。例如，我们数据集中的平均能耗是数值型的，且只适用于单个家庭。'
- en: '`time_varying_known_categoricals`: These are columns that are categorical in
    nature and change with time and we know the future values. They can be seen as
    quantities that keep varying with time. A prime example would be holidays, which
    are categorical and vary with time, and we know the future holidays.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_varying_known_categoricals`：这些列是分类性质的，并且随着时间变化，并且我们知道未来的值。它们可以视为随着时间不断变化的量。一个典型的例子是节假日，它是分类的，且随时间变化，我们知道未来的节假日。'
- en: '`time_varying_known_reals`: These are columns that are numeric in nature and
    change with time and we know the future values. A prime example would be temperature,
    which is numeric and varies with time, and we know the future values (provided
    the source we are getting the weather from allows for forecasted weather data
    as well).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_varying_known_reals`：这些列是数值型的，会随着时间变化，并且我们知道未来的值。一个典型的例子是温度，它是数值型的，随着时间变化，并且我们知道未来的值（前提是我们获取天气数据的来源也提供了未来天气的预报数据）。'
- en: '`time_varying_unknown_reals`: These are columns that are numeric in nature
    and change with time and we don’t know the future values. The target we are trying
    to forecast is an excellent example.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_varying_unknown_reals`：这些列是数值型的，会随着时间变化，并且我们不知道未来的值。我们尝试预测的目标就是一个很好的例子。'
- en: '`group_ids`: These columns uniquely identify each time series in the DataFrame.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`group_ids`：这些列唯一标识数据框中的每个时间序列。'
- en: 'Once defined, we can assign different values to each of these names, as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义好，我们可以为这些名称分配不同的值，如下所示：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The way of setting up a problem is slightly different in `neuralforecast` but
    the principles are the same. The different types of variables we define remain
    the same conceptually and just the parameter names we use to define them are different.
    PyTorch Forecasting needs the target to be included in `time_varying_unknown_reals`
    but `neuralforecast` doesn’t. All these minor differences will be covered when
    we use `neuralforecast` to generate forecasts.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在`neuralforecast`中，问题设置的方式略有不同，但原理是相同的。我们定义的不同类型的变量在概念上保持一致，只是我们用来定义它们的参数名称不同。PyTorch
    Forecasting需要将目标包含在`time_varying_unknown_reals`中，而`neuralforecast`则不需要。这些细微的差异将在我们使用`neuralforecast`生成预测时进行详细说明。
- en: 'We can see that we are not using all the features as we did with machine learning
    models (*Chapter 10*, *Global Forecasting Models*). There are two reasons for
    that:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们没有像在机器学习模型中那样使用所有特征（*第十章*，*全球预测模型*）。这样做有两个原因：
- en: Since we are using sequential deep learning models, a lot of the information
    we are trying to capture using rolling features and so on is already available
    to the model.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们使用的是顺序深度学习模型，因此我们尝试通过滚动特征等捕捉的许多信息，模型已经能够自动获取。
- en: Unlike robust gradient-boosted decision tree models, deep learning models aren’t
    that robust to noise. So, irrelevant features would make the model worse.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与强大的梯度提升决策树模型不同，深度学习模型对噪声的鲁棒性较差。因此，无关特征会使模型表现变差。
- en: There are a few preprocessing steps that are needed to make the dataset we have
    compatible with PyTorch Forecasting. PyTorch Forecasting needs a continuous time
    index as a proxy for time. Although we have a `timestamp` column, it has datetimes.
    So, we need to convert it to a new column, `time_idx`. The complete code is in
    the notebook, but the essence of the code is simple. We combine the train and
    test DataFrames and use a formula using the `timestamp` column to derive a new
    `time_idx` column. The formula is such that it increments every successive timestamp
    by one and is consistent between `train` and `test`. For instance, `time_idx`
    of the last timestep in `train` is `256`, and `time_idx` of the first timestep
    in `test` would be `257`. In addition to that, we also need to convert the categorical
    columns into `object` data types to play nicely with `TimeSeriesDataset` from
    PyTorch Forecasting.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的数据集与 PyTorch Forecasting 兼容，有几个预处理步骤是必需的。PyTorch Forecasting 需要一个连续的时间索引作为时间的代理。虽然我们有一个
    `timestamp` 列，但它包含的是日期时间。因此，我们需要将其转换为一个新的列 `time_idx`。完整的代码可以在笔记本中找到，但代码的核心思想很简单。我们将训练集和测试集的
    DataFrame 合并，并使用 `timestamp` 列中的公式推导出新的 `time_idx` 列。这个公式是这样的：每个连续的时间戳递增 1，并且在
    `train` 和 `test` 之间保持一致。例如，`train` 中最后一个时间步的 `time_idx` 是 `256`，而 `test` 中第一个时间步的
    `time_idx` 将是 `257`。此外，我们还需要将类别列转换为 `object` 数据类型，以便与 PyTorch Forecasting 中的 `TimeSeriesDataset`
    良好配合。
- en: 'For our experiments, we have chosen to have 2 days (96 timesteps) as the window
    and predict one single step ahead. To enable early stopping, we would need a validation
    set as well. **Early stopping** is a way of regularization (a technique to prevent
    overfitting, *Chapter 5*) where we keep monitoring the validation loss and stop
    training when the validation loss starts to increase. We have selected the last
    day of training (48 timesteps) as the validation data and 1 whole month as the
    final test data. But when we prepare these DataFrames, we need to take care of
    something: we have chosen two days as our history, and to forecast the first timestep
    in the validation or test set, we need the last two days of history along with
    it. So, we split our DataFrames as shown in the following diagram (the exact code
    is in the notebook):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们选择了 2 天（96 个时间步）作为窗口，并预测一个时间步的未来。为了启用提前停止，我们还需要一个验证集。**提前停止**是一种正则化方法（防止过拟合的技术，*第
    5 章*），它通过监控验证损失，当验证损失开始上升时停止训练。我们选择了训练的最后一天（48 个时间步）作为验证数据，并选择了 1 个月作为最终测试数据。但在准备这些
    DataFrame 时，我们需要注意一些问题：我们选择了两天作为历史数据，而为了预测验证集或测试集中的第一个时间步，我们需要将过去两天的历史数据一同考虑进去。因此，我们按照下图所示的方式划分
    DataFrame（具体代码在笔记本中）：
- en: '![Figure 15.1 – Train-validation-test split ](img/B22389_15_01.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.1 – 训练-验证-测试集划分](img/B22389_15_01.png)'
- en: 'Figure 15.1: Train-validation-test split'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.1：训练-验证-测试集划分
- en: Now, before using `TimeSeriesDataset` on our data, let’s try to understand what
    it does and what the different parameters involved are.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在使用 `TimeSeriesDataset` 处理我们的数据之前，让我们先了解它的作用以及涉及的不同参数。
- en: Understanding TimeSeriesDataset from PyTorch Forecasting
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 PyTorch Forecasting 中的 TimeSeriesDataset
- en: '`TimeSeriesDataset` automates the following tasks and more:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`TimeSeriesDataset` 自动化以下任务及更多：'
- en: 'Scaling numeric features and encoding categorical features:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数值特征进行缩放并编码类别特征：
- en: Scaling the numeric features to have the same mean and variance helps gradient
    descent-based optimization to converge faster and better.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数值特征进行缩放，使其具有相同的均值和方差，有助于基于梯度下降的优化方法更快且更好地收敛。
- en: Categorical features need to be encoded as numbers so that we can handle them
    the right way inside the deep learning models.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别特征需要编码为数字，以便我们能够在深度学习模型中正确处理它们。
- en: 'Normalizing the target variable:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化目标变量：
- en: In a global model context, the target variable can have different scales for
    different time series. For instance, a particular household typically has higher
    energy consumption, and some other households may be vacant and have little to
    no energy consumption. Scaling the target variable to a single scale helps the
    deep learning model to focus on learning the patterns rather than capturing the
    variance in scale.
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在全局模型上下文中，目标变量对于不同的时间序列可能具有不同的尺度。例如，某个家庭通常有较高的能源消耗，而其他一些家庭可能是空置的，几乎没有能源消耗。将目标变量缩放到一个单一的尺度有助于深度学习模型专注于学习模式，而不是捕捉尺度上的方差。
- en: 'Efficiently converting the DataFrame into a dictionary of PyTorch tensors:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效地将DataFrame转换为PyTorch张量字典：
- en: The dataset also takes in the information about different columns and converts
    the DataFrame into a dictionary of PyTorch tensors, separately handling the static
    and time-varying information.
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集还会接受关于不同列的信息，并将DataFrame转换为PyTorch张量的字典，分别处理静态信息和随时间变化的信息。
- en: 'These are the major parameters of `TimeSeriesDataset`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是`TimeSeriesDataset`的主要参数：
- en: '`data`: This is the pandas DataFrame holding all the data such that each row
    is uniquely identified with `time_idx` and `group_ids`.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data`：这是包含所有数据的pandas DataFrame，每一行通过`time_idx`和`group_ids`进行唯一标识。'
- en: '`time_idx`: This refers to the column name with the continuous time index we
    created earlier.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_idx`：这指的是我们之前创建的连续时间索引的列名。'
- en: '`target`, `group_ids`, `static_categoricals`, `static_reals`, `time_varying_known_categoricals`,
    `time_varying_known_reals`, `time_varying_unknown_categoricals`, and `time_varying_unknown_reals`:
    We already discussed all these parameters in the *Preprocessing the data* section.
    These hold the same meaning.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target`、`group_ids`、`static_categoricals`、`static_reals`、`time_varying_known_categoricals`、`time_varying_known_reals`、`time_varying_unknown_categoricals`
    和 `time_varying_unknown_reals`：我们已经在*数据预处理*部分讨论过所有这些参数，它们的含义相同。'
- en: '`max_encoder_length`: This sets the maximum window length given to the encoder.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_encoder_length`：设置给定编码器的最大窗口长度。'
- en: '`min_decoder_length`: This sets the minimum window given in the decoding context.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_decoder_length`：设置解码上下文中给定的最小窗口长度。'
- en: '`target_normalizer`: This takes in a Transformer that normalizes the targets.
    There are a few normalizers built into PyTorch Forecasting—`TorchNormalizer`,
    `GroupNormalizer`, and `EncoderNormalizer`. `TorchNormalizer` does standard and
    robust scaling of the targets as a whole, whereas `GroupNormalizer` does the same
    but with each group separately (a group is defined by `group_ids`). `EncoderNormalizer`
    does the scaling at runtime by normalizing using the values in each window.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_normalizer`：这是一个Transformer，用于对目标进行标准化。PyTorch Forecasting内置了几种标准化器——`TorchNormalizer`、`GroupNormalizer`和`EncoderNormalizer`。`TorchNormalizer`对整个目标进行标准化和鲁棒性缩放，而`GroupNormalizer`则对每个组分别进行相同的处理（组是由`group_ids`定义的）。`EncoderNormalizer`在运行时根据每个窗口中的值进行标准化。'
- en: '`categorical_encoders`: This parameter takes in a dictionary of scikit-learn
    Transformers as a category encoder. By default, the category encoding is similar
    to `LabelEncoder`, which replaces each unique categorical value with a number,
    adding an additional category for unknown and `NaN` values.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`categorical_encoders`：该参数接受一个字典，字典中的值是scikit-learn的Transformer，作为类别编码器。默认情况下，类别编码类似于`LabelEncoder`，它将每个独特的类别值替换为一个数字，并为未知值和`NaN`值添加额外的类别。'
- en: For the full documentation, please refer to [https://pytorch-forecasting.readthedocs.io/en/stable/data.html#time-series-data-set](https://pytorch-forecasting.readthedocs.io/en/stable/data.html#time-series-data-set).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 完整文档请参阅 [https://pytorch-forecasting.readthedocs.io/en/stable/data.html#time-series-data-set](https://pytorch-forecasting.readthedocs.io/en/stable/data.html#time-series-data-set)。
- en: Initializing TimeSeriesDataset
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化TimeSeriesDataset
- en: 'Now that we know the major parameters, let’s initialize a time series dataset
    using our data:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了主要参数，接下来用我们的数据初始化一个时间序列数据集：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that we have used `GroupNormalizer` so that each household is scaled separately
    using its own mean and standard deviation using the following well-known formula:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用了`GroupNormalizer`，使得每个家庭根据其自身的均值和标准差分别进行缩放，使用的是以下著名的公式：
- en: '![](img/B22389_15_001.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_15_001.png)'
- en: '`TimeSeriesDataset` also makes it easy to declare validation and test datasets
    as well using a factory method, `from_dataset`. It takes in another time series
    dataset as an argument and uses the same parameters, scalers, and so on, and creates
    new datasets:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`TimeSeriesDataset`还使得声明验证和测试数据集变得更加容易，通过工厂方法`from_dataset`。它接受另一个时间序列数据集作为参数，并使用相同的参数、标准化器等，创建新的数据集：'
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that we concatenate the history to both `val_df` and `test_df` to make
    sure we can predict on the entire validation and test period.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将历史数据连接到`val_df`和`test_df`中，以确保可以在整个验证和测试期间进行预测。
- en: Creating the dataloader
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建数据加载器
- en: 'All that is left to do is to create the dataloader from `TimeSeriesDataset`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的工作就是从`TimeSeriesDataset`创建数据加载器：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Before we proceed, let’s solidify our understanding of the PyTorch Forecasting
    dataloader with the help of an example. The `train` dataloader we just created
    has split the DataFrame into a dictionary of PyTorch tensors. We have chosen `512`
    as a batch size and can inspect the dataloader using the following code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们通过一个示例巩固我们对PyTorch Forecasting数据加载器的理解。我们刚创建的`train`数据加载器已经将数据框拆分成了一个PyTorch张量的字典。我们选择了`512`作为批次大小，并可以使用以下代码检查数据加载器：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will get an output as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到如下的输出：
- en: '![Figure 15.2 – Shapes of tensors in a batch of a train dataloader ](img/B22389_15_02.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.2 – 批量数据加载器中的张量形状](img/B22389_15_02.png)'
- en: 'Figure 15.2: Shapes of tensors in a batch of a train dataloader'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.2：批量数据加载器中张量的形状
- en: We can see that the dataloader and `TimeSeriesDataset` have split the DataFrame
    into PyTorch tensors and packed them into a dictionary with the encoder and decoder
    sequences separate. We can also see that the categorical and continuous features
    are also separated.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，数据加载器和`TimeSeriesDataset`已经将数据框拆分为PyTorch张量，并将它们打包进一个字典中，编码器和解码器序列被分开。我们还可以看到，类别特征和连续特征也被分开了。
- en: The main *keys* we will be using from this dictionary are `encoder_cat`, `encoder_cont`,
    `decoder_cat`, and `decoder_cont`. The `encoder_cat` and `decoder_cat` keys have
    zero dimensions because we haven’t declared any categorical features.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个字典中的主要*键*是`encoder_cat`、`encoder_cont`、`decoder_cat`和`decoder_cont`。`encoder_cat`和`decoder_cat`这两个键的维度为零，因为我们没有声明任何类别特征。
- en: Visualizing how the dataloader works
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化数据加载器的工作原理
- en: 'Let’s try to unpeel what happened here one level deeper and understand what
    `TimeSeriesDataset` has done visually:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试更深入地剖析这里发生了什么，并通过视觉化的方式理解`TimeSeriesDataset`所做的事情：
- en: '![Figure 15.3 – TimeSeriesDataset – an illustration of how it works ](img/B22389_15_03.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.3 – TimeSeriesDataset – 它是如何工作的示意图](img/B22389_15_03.png)'
- en: 'Figure 15.3: TimeSeriesDataset—an illustration of how it works'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.3：TimeSeriesDataset——它是如何工作的示意图
- en: Let’s assume we have a time series, *x*[1] to *x*[6] (this would be the target
    as well as `time_varying_unknown` in the `TimeSeriesDataset` terminology). We
    have a time-varying real, *f*[1] to *f*[6], and a time-varying categorical, *c*[1]
    to *c*[2]. In addition to that, we also have a static real, *r*, and a static
    categorical, *s*. If we set the encoder and decoder length as `3`, we will have
    the tensors constructed as shown in *Figure 15.3*. Notice how the static categorical
    and real are repeated for all timesteps. These different tensors are constructed
    so that the model encoder can be trained using the encoder tensors and the decoder
    tensors are used in the decoding process.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个时间序列，*x*[1] 到 *x*[6]（这将是目标以及`TimeSeriesDataset`术语中的`time_varying_unknown`）。我们有一个时间变化的实数，*f*[1]
    到 *f*[6]，和一个时间变化的类别，*c*[1] 到 *c*[2]。除此之外，我们还拥有一个静态实数，*r*，和一个静态类别，*s*。如果我们将编码器和解码器的长度设置为`3`，那么我们将得到如*图
    15.3*所示的张量。请注意，静态类别和实数在所有时间步长上都被重复。这些不同的张量构造是为了让模型的编码器能够使用编码器张量进行训练，而解码器张量则在解码过程中使用。
- en: Now, let’s proceed with building our first global model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始构建我们的第一个全局模型。
- en: Building the first global deep learning forecasting model
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建第一个全局深度学习预测模型
- en: PyTorch Forecasting uses PyTorch and PyTorch Lightning in the backend to define
    and train deep learning models. The models that can be used seamlessly with PyTorch
    Forecasting are essentially PyTorch Lightning models. However, the recommended
    approach is to inherit `BaseModel` from PyTorch Forecasting. The developer of
    PyTorch Forecasting has excellent documentation and tutorials to help new users
    use it the way they want. One tutorial worth mentioning here is titled *How to
    use custom data and implement custom models and metrics* (the link is in the *Further
    reading* section).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Forecasting 使用 PyTorch 和 PyTorch Lightning 在后台定义和训练深度学习模型。可以与 PyTorch
    Forecasting 无缝配合使用的模型本质上是 PyTorch Lightning 模型。然而，推荐的做法是从 PyTorch Forecasting
    继承 `BaseModel`。PyTorch Forecasting 的开发者提供了出色的文档和教程，帮助新用户按照自己的需求使用它。这里值得一提的一个教程名为
    *如何使用自定义数据并实现自定义模型和指标*（链接在 *进一步阅读* 部分）。
- en: 'I have slightly modified the basic model from the tutorial to make it more
    flexible. The implementation can be found in `src/dl/ptf_models.py` under the
    name `SingleStepRNNModel`. The class takes in two parameters:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我对教程中的基础模型做了一些修改，使其更加灵活。实现代码可以在 `src/dl/ptf_models.py` 文件中找到，名为 `SingleStepRNNModel`。该类接收两个参数：
- en: '`network_callable`: This is a callable that, when initialized, becomes a PyTorch
    model (inheriting `nn.Module`).'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_callable`：这是一个可调用对象，当初始化时，它将成为一个 PyTorch 模型（继承自 `nn.Module`）。'
- en: '`model_params`: This is a dictionary containing all the parameters necessary
    to initialize `network_callable`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_params`：这是一个字典，包含初始化 `network_callable` 所需的所有参数。'
- en: The structure is pretty simple. The `__init__` function initializes `network_callable`
    into a PyTorch model under the `network` attribute. The `forward` function sends
    the input to the network, formats the returned output the way PyTorch Forecasting
    wants, and returns it. It is a very short model because the bulk of the heavy
    lifting is done by `BaseModel`, which handles the loss calculation, logging, gradient
    descent, and so on. The benefit we get by defining a model this way is that we
    can now define standard PyTorch models and pass it to this model to make it work
    well with PyTorch Forecasting.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 结构相当简单。`__init__` 函数将 `network_callable` 初始化为一个 PyTorch 模型，并将其存储在 `network`
    属性下。`forward` 函数将输入传递给网络，格式化返回的输出，使其符合 PyTorch Forecasting 的要求，并返回结果。这个模型非常简短，因为大部分工作都由
    `BaseModel` 完成，它负责处理损失计算、日志记录、梯度下降等任务。我们通过这种方式定义模型的好处是，现在我们可以定义标准的 PyTorch 模型，并将其传递给这个模型，使其能够与
    PyTorch Forecasting 配合得很好。
- en: In addition to this, we also define an abstract class called `SingleStepRNN`,
    which takes in a set of parameters and initializes the corresponding network that
    is specified by the parameters. If the parameter specifies an LSTM, with two layers,
    then it will be initialized and saved under the `rnn` attribute. It also defines
    a fully connected layer under the `fc` attribute, which turns the output of the
    RNN into the prediction. The `forward` method is an abstract method that needs
    to be overwritten in any class subclassing this class.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我们还定义了一个抽象类 `SingleStepRNN`，它接收一组参数并初始化由这些参数指定的相应网络。如果参数指定了一个两层的 LSTM，那么它将会被初始化，并保存在
    `rnn` 属性下。它还在 `fc` 属性下定义了一个全连接层，将 RNN 的输出转化为预测结果。`forward` 方法是一个抽象方法，任何继承该类的子类都需要重写这个方法。
- en: Defining our first RNN model
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义我们的第一个 RNN 模型
- en: 'Now that we have the necessary setup, let’s define our first model inheriting
    the `SingleStepRNN` class we defined:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了必要的设置，让我们定义第一个继承 `SingleStepRNN` 类的模型：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is the most straightforward implementation. We take `encoder_cont` from
    the dictionary and pass it through the RNN, and then use a fully connected layer
    on the last hidden state from the RNN to generate the prediction. If we take the
    example in *Figure 15.3*, we used *x*[1] to *x*[3] as the history and trained
    the model to predict *x*[4] (because we are using `min_decoder_length=1`, there
    will be just one timestep in the decoder and target).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最直接的实现方式。我们从字典中取出 `encoder_cont`，并将其传递给 RNN，然后在 RNN 的最后一个隐藏状态上使用全连接层来生成预测。如果我们以
    *图 15.3* 中的示例为例，我们使用 *x*[1] 到 *x*[3] 作为历史数据，并训练模型预测 *x*[4]（因为我们使用了 `min_decoder_length=1`，所以解码器和目标中只有一个时间步）。
- en: Initializing the RNN model
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化 RNN 模型
- en: 'Now, let’s initialize the model using some parameters. I have defined two dictionaries
    for parameters:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用一些参数初始化模型。我为参数定义了两个字典：
- en: '`model_params`: This has all the parameters necessary for the `SingleStepRNN`
    model to be initialized.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_params`：这包含了初始化 `SingleStepRNN` 模型所需的所有参数。'
- en: '`other_params`: These are all the parameters, such as `learning_rate`, `loss`,
    and so on, that we pass on to `SingleStepRNNModel`.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`other_params`：这些是我们传递给`SingleStepRNNModel`的所有参数，如`learning_rate`、`loss`等。'
- en: 'Now, we can initialize the PyTorch Forecasting model using a factory method
    it supports—`from_dataset`. This factory method lets us pass a dataset and infer
    some parameters from the dataset instead of filling everything in all the time:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用PyTorch Forecasting模型支持的工厂方法`from_dataset`进行初始化。这个工厂方法允许我们传入数据集，并从数据集中推断一些参数，而不需要每次都填入所有内容：
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Training the RNN model
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练RNN模型
- en: 'Training the model is just like we have been doing in previous chapters because
    this is a PyTorch Lightning model. We can follow these steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型就像我们在前几章中所做的那样，因为这是一个PyTorch Lightning模型。我们可以按照以下步骤进行：
- en: 'Initialize the trainer with early stopping and model checkpoints:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用早期停止和模型检查点初始化训练器：
- en: '[PRE8]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Fit the model:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型：
- en: '[PRE9]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Load the best model after training:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后加载最佳模型：
- en: '[PRE10]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The training can run for some time. To save you some time, I have included the
    trained weights for each of the models we are using, and if the `train_model`
    flag is `False`, it will skip training and load the saved weights.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可能需要一些时间。为了节省您的时间，我已包含了我们使用的每个模型的训练权重，如果`train_model`标志为`False`，则会跳过训练并加载保存的权重。
- en: Forecasting with the trained model
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用训练好的模型进行预测
- en: 'Now, after training, we can predict on the `test` dataset as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，训练完成后，我们可以在`test`数据集上进行预测，方法如下：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We store the predictions in a DataFrame and evaluate them using our standard
    metrics: `MAE`, `MSE`, `meanMASE`, and `Forecast Bias`. Let’s see the results:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将预测结果存储在一个DataFrame中，并使用我们的标准指标进行评估：`MAE`、`MSE`、`meanMASE`和`Forecast Bias`。让我们看看结果：
- en: '![A close up of numbers  Description automatically generated](img/B22389_15_04.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![A close up of numbers  Description automatically generated](img/B22389_15_04.png)'
- en: 'Figure 15.4: Aggregate results using the baseline global model'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4：使用基线全局模型汇总结果
- en: 'This is not a very good model because we know from *Chapter 10*, *Global Forecasting
    Models*, that the baseline global model using LightGBM was as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不太好的模型，因为我们从*第10章*《*全局预测模型*》中知道，基线全局模型使用LightGBM的结果如下：
- en: MAE = 0.079581
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAE = 0.079581
- en: MSE = 0.027326
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MSE = 0.027326
- en: meanMASE = 1.013393
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: meanMASE = 1.013393
- en: Forecast Bias = 28.718087
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测偏差 = 28.718087
- en: Apart from Forecast Bias, our global model is nowhere close to the best. Let’s
    refer to the **global machine learning model** as **GFM**(**ML**) and the current
    model as **GFM**(**DL**) for the rest of our discussion. Now, let’s start looking
    at some strategies to make the global model better.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了预测偏差，我们的全局模型与最佳模型相差甚远。我们将**全局机器学习模型**称为**GFM**(**ML**)，将当前模型称为**GFM**(**DL**)，并在接下来的讨论中使用这两个术语。现在，让我们开始探索一些策略，改善全局模型。
- en: Using time-varying information
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用时变信息
- en: The GFM(ML) used all the available features. So, obviously, that model had access
    to a lot more information than the GFM(DL) we have built until now. The GFM(DL)
    we just built only takes in the history and nothing else. Let’s change that by
    including time-varying information. We will just use time-varying real features
    this time because dealing with categorical features is a topic I want to leave
    for the next section.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: GFM(ML)使用了所有可用的特征。因此，显然，该模型比我们目前构建的GFM(DL)访问了更多的信息。我们刚刚构建的GFM(DL)只使用了历史数据，其他的都没有。让我们通过加入时变信息来改变这一点。这次我们只使用时变的真实特征，因为处理类别特征是我希望留到下一节讨论的话题。
- en: We initialize the training dataset the same way as before, but we add `time_varying_known_reals=feat_config.time_varying_known_reals`
    to the initialization parameters. Now that we have all the datasets created, let’s
    move on to setting up the model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以与之前相同的方式初始化训练数据集，但在初始化参数中加入了`time_varying_known_reals=feat_config.time_varying_known_reals`。现在我们已经创建了所有数据集，让我们继续设置模型。
- en: To set up the model, we need to understand one concept. We are now using the
    history of the target and time-varying known features. In *Figure 15.3*, we saw
    how `TimeSeriesDataset` arranges the different kinds of variables in PyTorch tensors.
    In the previous section, we used only `encoder_cont` because there were no other
    variables to worry about. But now, we have time-varying variables along with it,
    which brings an added complication. If we take a step back and think about it,
    in the single-step-ahead forecasting context, we can see that the time-varying
    variables and the history of the target cannot be of the same timestep.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置模型，我们需要理解一个概念。我们现在使用目标的历史数据和时变已知特征。在*图 15.3*中，我们看到`TimeSeriesDataset`如何将不同类型的变量安排成PyTorch张量。在上一节中，我们只使用了`encoder_cont`，因为没有其他变量需要考虑。但现在，我们有了时变变量，这增加了复杂性。如果我们退一步思考，在单步预测的上下文中，我们可以看到时变变量和目标的历史数据不能具有相同的时间步。
- en: 'Let’s use a visual example to elucidate:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个视觉示例来阐明：
- en: '![Figure 15.5 – Using time-varying variables for training ](img/B22389_15_05.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.5 – 使用时变变量进行训练](img/B22389_15_05.png)'
- en: 'Figure 15.5: Using time-varying variables for training'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.5：使用时变变量进行训练
- en: Following the same spirit of the example from *Figure 15.3*, but reducing it
    to fit our context here, we have a time series, *x*[1] to *x*[4], and a time-varying
    real variable, *f*[1] to *f*[4]. So, for `max_encoder_length=3` and `min_decoder_length=1`,
    we would have `TimeSeriesDataset` make the tensors, as shown in *Step 1* in *Figure
    15.5*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 延续*图 15.3*示例的精神，但将其简化以适应我们的上下文，我们有一个时间序列，*x*[1] 到 *x*[4]，以及一个时变的真实变量，*f*[1]
    到 *f*[4]。所以，对于`max_encoder_length=3`和`min_decoder_length=1`，我们会让`TimeSeriesDataset`生成张量，如*图
    15.5*中的*步骤 1*所示。
- en: Now, for each timestep, we have the time-varying variable, *f*, and the history,
    *x*, in `encoder_cont`. The time-varying variable, *f*, is a variable for which
    we know the future values as well and therefore, there is no causal constraint
    on that variable. That means that for predicting the timestep, *t*, we can use
    *f*[t] because it is known. However, the history of the target variable is not.
    We do not know the future because it is the very quantity we are trying to forecast.
    That means that there is a causal constraint on *x* and, because of this, we cannot
    use *x*[t] to predict timestep *t*. But the way the tensors are formed right now,
    we have *f* and *x* aligned on timesteps and if we passed them through a model,
    we would be essentially cheating because we would be using *x*[t] to predict timestep
    *t*. Ideally, there should be an offset between the history, *x*, and the time-varying
    feature, *f*, such that at timestep *t*, the model sees *x*[t][-1], then sees
    *f*[t], and then predicts *x*[t].
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于每个时间步，我们有时变变量*f*和历史数据*x*在`encoder_cont`中。时变变量*f*是我们也知道未来值的变量，因此对该变量没有因果约束。这意味着对于预测时间步*t*，我们可以使用*f*[t]，因为它是已知的。然而，目标变量的历史数据则不是。我们无法知道未来的值，因为它正是我们想要预测的量。这意味着*x*上有因果约束，因此我们不能使用*x*[t]来预测时间步*t*。但是目前张量的形成方式是，*f*和*x*在时间步上是对齐的，如果我们将它们输入到模型中，就相当于作弊，因为我们会使用*x*[t]来预测时间步*t*。理想情况下，历史数据*x*和时变特征*f*之间应该有一个偏移量，这样在时间步*t*时，模型看到的是*x*[t][-1]，然后看到*f*[t]，然后预测*x*[t]。
- en: 'To achieve that, we do the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们执行以下操作：
- en: Concatenate `encoder_cont` and `decoder_cont` because we need to use *f*[4]
    to predict timestep *t* = 4 (*Step 2* in *Figure 15.5*).
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`encoder_cont`和`decoder_cont`连接起来，因为我们需要使用*f*[4]来预测时间步*t* = 4（*图 15.5*中的*步骤
    2*）。
- en: Shift the target history, *x*, forward by one timestep so that *f*[t] and *x*[t][-1]
    are aligned (*Step 3* in *Figure 15.5*).
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目标历史数据*x*向前移动一个时间步，使得*f*[t]和*x*[t][-1]对齐（*图 15.5*中的*步骤 3*）。
- en: Drop the first timestep because we don’t have the history to go with the first
    timestep (*Step 4* in *Figure 15.5*).
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 去掉第一个时间步，因为我们没有与第一个时间步相关的历史数据（在*图 15.5*中的*步骤 4*）。
- en: 'This is exactly what we need to implement in our `forward` method in the new
    model we defined, `DynamicFeatureRNNModel`, as well:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在新模型`DynamicFeatureRNNModel`的`forward`方法中需要实现的内容：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, let’s train this new model and see how it performs. The exact code is
    in the notebook and is exactly the same as before:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练这个新模型，看看它的表现。具体的代码在笔记本中，和之前完全相同：
- en: '![Figure 15.6 – Aggregate results using the time-varying features ](img/B22389_15_06.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.6 – 使用时变特征汇总结果](img/B22389_15_06.png)'
- en: 'Figure 15.6: Aggregate results using the time-varying features'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.6：使用时变特征汇总结果
- en: It looks like having temperature as a feature did make the model slightly better,
    but there’s still a long way to go. Not to worry; we have other features to use.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来温度作为特征确实使模型稍微改善了一些，但还有很长的路要走。别担心，我们还有其他特征可以使用。
- en: Using static/meta information
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用静态/元信息
- en: There are some features such as the Acorn group, whether dynamic pricing is
    enabled, and so on, that are specific to a household, which will help the model
    learn patterns specific to these groups. Naturally, including that information
    makes intuitive sense.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有些特征，如橡子组、是否启用动态定价等，特定于某个家庭，这将帮助模型学习特定于这些组的模式。自然地，包含这些信息是有直觉意义的。
- en: However, as we discussed in *Chapter 10*, *Global Forecasting Models*, categorical
    features do not play well with machine learning models because they aren’t numerical.
    In that chapter, we discussed a few ways of encoding categorical features into
    numerical representations. We can use any of those in a deep learning model as
    well. But there is one way of handling categorical features that is unique to
    deep learning models—**embedding vectors**.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我们在*第10章*《*全球预测模型*》中讨论的，分类特征与机器学习模型的配合不太好，因为它们不是数值型的。在那一章中，我们讨论了几种将分类特征编码为数值表示的方法。这些方法同样适用于深度学习模型。但有一种处理分类特征的方法是深度学习模型特有的——**嵌入向量**。
- en: One-hot encoding and why it is not ideal
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独热编码及其为何不理想
- en: One of the ways of converting categorical features to numerical representation
    is one-hot encoding. It encodes the categorical features in a higher dimension,
    placing the categorical values equally distant in that space. The size of the
    dimension it requires to encode the categorical values is equal to the cardinality
    of the categorical variable. For a more detailed discussion on one-hot encoding,
    refer to *Chapter 10*, *Global Forecasting Models*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 将分类特征转换为数值表示的方法之一是独热编码。它将分类特征编码为一个更高维度，将分类值等距地放置在该空间中。它需要的维度大小等于分类变量的基数。有关独热编码的详细讨论，请参阅*第10章*《*全球预测模型*》。
- en: The representation that we would get after the one-hot encoding of a categorical
    feature is what we call a **sparse representation**. If the cardinality of the
    categorical feature (number of unique values) is *C*, each row representing a
    value of the categorical feature would have *C* - 1 zeros. So, the representation
    is predominantly zeros and hence is called a sparse representation. This causes
    the overall dimension required to effectively encode a categorical feature to
    be equal to the cardinality of the vector. Therefore, one-hot encoding of a categorical
    feature with 5,000 unique values instantly adds 5,000 dimensions to the problem
    you are solving.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在对分类特征进行独热编码后，我们得到的表示被称为**稀疏表示**。如果分类特征的基数（唯一值的数量）是*C*，那么每一行代表分类特征的一个值时，将有*C*
    - 1个零。因此，该表示大部分是零，因此称为稀疏表示。这导致有效编码一个分类特征所需的总体维度等于向量的基数。因此，对一个拥有5,000个唯一值的分类特征进行独热编码会立刻给你要解决的问题添加5,000个维度。
- en: In addition to that, one-hot encoding is also completely uninformed. It places
    each categorical value equidistant from each other without any regard for the
    possible similarity between those values. For instance, if we were encoding the
    days in a week, one-hot encoding would place each day in a completely different
    dimension, making them equidistant from each other. But if we think about it,
    Saturday and Sunday should be closer together than the other weekdays on account
    of them being the weekend, right? This kind of information is not captured through
    one-hot encoding.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，独热编码也是完全没有信息的。它将每个分类值放置在相等的距离之内，而没有考虑这些值之间可能的相似性。例如，如果我们要对一周的每一天进行编码，独热编码会将每一天放在一个完全不同的维度中，使它们彼此之间距离相等。但如果我们仔细想想，周六和周日应该比其他工作日更接近，因为它们是周末，对吧？这种信息在独热编码中并没有被捕捉到。
- en: Embedding vectors and dense representations
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入向量和密集表示
- en: An embedding vector is a similar representation, but instead of a sparse representation,
    it strives to give us a dense representation of a categorical feature. We can
    achieve this by using an embedding layer. The embedding layer can be thought of
    as a mapping between each categorical value and a numerical vector, and this vector
    can have a much lower dimension than the cardinality of the categorical feature.
    The only question that remains is “*How do we know what vector to choose for each
    categorical value?*”
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入向量是一种类似的表示方式，但它不是稀疏表示，而是努力为我们提供类别特征的密集表示。我们可以通过使用嵌入层来实现这一点。嵌入层可以被视为每个类别值与一个数值向量之间的映射，而这个向量的维度可以远小于类别特征的基数。唯一剩下的问题是“*我们怎么知道为每个类别值选择哪个向量？*”
- en: The good news is that we need not because the embedding layer is trained along
    with the rest of the network. So, while training a model for some task, the model
    itself figures out what the best vector representation is for each categorical
    value. This approach is really popular in natural language processing, where thousands
    of words are embedded into dimensions as small as 200 or 300\. In PyTorch, we
    can accomplish this by using `nn.Embedding`, which is a module that is a simple
    lookup table that stores the embeddings of fixed discrete values and size.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是我们不需要知道，因为嵌入层与网络的其余部分一起训练。因此，在训练模型执行某项任务时，模型会自动找出每个类别值的最佳向量表示。这种方法在自然语言处理领域非常流行，在那里数千个单词被嵌入到维度只有200或300的空间中。在PyTorch中，我们可以通过使用`nn.Embedding`来实现这一点，它是一个简单的查找表，存储固定离散值和大小的嵌入。
- en: 'There are two mandatory parameters while initializing:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化时有两个必需的参数：
- en: '`num_embeddings`: This is the size of the dictionary of embeddings. In other
    words, this is the cardinality of the categorical feature.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_embeddings`：这是嵌入字典的大小。换句话说，这是类别特征的基数。'
- en: '`embedding_dim`: This is the size of each embedding vector.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_dim`：这是每个嵌入向量的大小。'
- en: 'Now, let’s come back to global modeling. Let’s first introduce the static categorical
    features. Please note that we are also including the time-varying categorical
    because now we know how to deal with categorical features in a deep learning model.
    The code to initialize the dataset is the same, with the addition of the following
    two parameters to the initialization:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到全局建模。首先介绍静态类别特征。请注意，我们还包括了时间变化的类别特征，因为现在我们已经知道如何在深度学习模型中处理类别特征。初始化数据集的代码是一样的，只是添加了以下两个参数：
- en: '`static_categoricals=feat_config.static_categoricals`'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_categoricals=feat_config.static_categoricals`'
- en: '`time_varying_known_categoricals=feat_config.time_varying_known_categoricals`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_varying_known_categoricals=feat_config.time_varying_known_categoricals`'
- en: Defining a model with categorical features
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义带有类别特征的模型
- en: 'Now that we have the datasets, let’s look at how we can define the `__init__`
    function in our new model, `StaticDynamicFeatureRNNModel`. In addition to invoking
    the parent model, which sets up the standard RNN and fully connected layer, we
    also set up the embedding layers using an input, `embedding_sizes`. `embedding_sizes`
    is a list of tuples (*cardinality and embedding size*) for each categorical feature:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据集，让我们看看如何在新模型`StaticDynamicFeatureRNNModel`中定义`__init__`函数。除了调用父模型来设置标准的RNN和全连接层外，我们还使用输入`embedding_sizes`设置嵌入层。`embedding_sizes`是一个包含每个类别特征的元组列表（*基数和嵌入大小*）：
- en: '[PRE13]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We used `nn.ModuleList` to store a list of `nn.Embedding` modules, one for
    each categorical feature. While initializing this model, we will need to give
    `embedding_sizes` as input. The embedding size required for each categorical feature
    is technically a hyperparameter that we can tune. But there are a few rules of
    thumb to get you started. The idea behind these thumb rules is that the bigger
    the cardinality of the categorical feature, the larger the embedding size required
    to encode the information in them. Also, the embedding size can be much smaller
    than the cardinality of the categorical feature. The rule of thumb that we have
    adopted is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`nn.ModuleList`来存储`nn.Embedding`模块的列表，每个类别特征一个。在初始化该模型时，我们需要提供`embedding_sizes`作为输入。每个类别特征所需的嵌入大小在技术上是一个超参数，我们可以进行调优。但是有一些经验法则可以帮助你入门。这些经验法则的核心思想是，类别特征的基数越大，编码这些信息所需的嵌入大小也越大。此外，嵌入大小可以远小于类别特征的基数。我们采用的经验法则如下：
- en: '![](img/B22389_15_002.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_15_002.png)'
- en: 'Therefore, we create the `embedding_sizes` list of tuples using the following
    code:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用以下代码创建`embedding_sizes`元组列表：
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, turning our attention toward the `forward` method, it is going to be similar
    to the previous model, but with an additional part to handle the categorical features.
    We essentially use the embedding layers to convert the categorical features into
    embeddings and concatenate them with the continuous features:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，转向`forward`方法，它将类似于之前的模型，但增加了一个部分来处理类别特征。我们本质上使用嵌入层将类别特征转换为嵌入，并将它们与连续特征拼接在一起：
- en: '[PRE15]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let’s train this new model with static features and see how it performs:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用静态特征来训练这个新模型，并看看它的表现如何：
- en: '![Figure 15.7 – Aggregate results using the static and time-varying features
    ](img/B22389_15_07.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.7 – 使用静态和时间变化特征的汇总结果](img/B22389_15_07.png)'
- en: 'Figure 15.7: Aggregate results using the static and time-varying features'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.7：使用静态和时间变化特征的汇总结果
- en: Adding the static variables also improved our model. Now, let’s look at another
    strategy that adds another key piece of information to the model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 添加静态变量也改善了我们的模型。现在，让我们来看另一种策略，它向模型添加了一个关键的信息。
- en: Using the scale of the time series
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用时间序列的规模
- en: We used `GroupNormlizer` in `TimeSeriesDataset` to scale each household using
    its own mean and standard deviation. We did this because we wanted to make the
    target zero mean and unit variance so that the model does not waste effort trying
    to change its parameters to capture the scale of individual household consumption.
    Although this is a good strategy, we do have some information loss here. There
    may be patterns that are specific to households whose consumption is on the larger
    side and some other patterns that are specific to households that consume much
    less. But now, they are both lumped in together and the model tries to learn common
    patterns. In such a scenario, these unique patterns seem like noise to the model
    because there is no variable to explain them.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`TimeSeriesDataset`中使用了`GroupNormlizer`来对每个家庭进行缩放，使用它们各自的均值和标准差。这样做是因为我们希望使目标具有零均值和单位方差，以便模型不必浪费精力调整其参数来捕捉单个家庭消费的规模。虽然这是一种很好的策略，但我们确实在这里丢失了一些信息。可能有一些模式是特定于消费较大家庭的，而另外一些模式则是特定于消费较少的家庭的。但现在，这些模式被混在一起，模型试图学习共同的模式。在这种情况下，这些独特的模式对模型来说就像噪音一样，因为没有变量来解释它们。
- en: The bottom line is that there is information in the scale that we removed, and
    adding that information back would be beneficial. So, how do we add it back? Definitely
    not by including the unscaled targets, which brings back the disadvantage that
    we were trying to get away from in the first place. A way to do it is to add the
    scale information as static-real features to the model. We would have kept track
    of the mean and standard deviation of each household when we scaled them in the
    first place (because we need them to do the inverse transformation and get back
    the original targets). All we need to do is make sure we include them as a static
    real variable so that the model has access to the scale information while learning
    the patterns in the time series dataset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是我们移除的尺度中包含了信息，将这些信息加回来将会是有益的。那么，我们该如何加回来呢？绝对不是通过包括未缩放的目标，这样会带回我们一开始想要避免的缺点。实现这一点的一种方式是将尺度信息作为静态真实特征添加到模型中。当我们最初进行缩放时，我们会记录每个家庭的均值和标准差（因为我们需要它们进行反向变换，并恢复原始目标）。我们需要做的就是确保将它们作为静态真实变量包含在内，这样模型在学习时间序列数据集中的模式时就能访问到尺度信息。
- en: PyTorch Forecasting makes this easier for us by having a handy parameter in
    `TimeSeriesDataset` called `add_target_scales`. If you make it `True`, then `encoder_cont`
    and `decoder_cont` will also have the mean and standard deviation of individual
    time series.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Forecasting通过在`TimeSeriesDataset`中提供一个方便的参数`add_target_scales`，使这变得更简单。如果将其设置为`True`，那么`encoder_cont`和`decoder_cont`也将包含各个时间序列的均值和标准差。
- en: 'Nothing changes in our existing model; all we need to do is add this parameter
    to `TimeSeriesDataset` while initializing it and train and predict using the model.
    Let’s see how that worked out for us:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现有的模型没有变化；我们只需要在初始化时将这个参数添加到`TimeSeriesDataset`中，然后使用模型进行训练和预测。让我们看看它是如何为我们工作的：
- en: '![Figure 15.8 – Aggregate results using the static, time-varying, and scale
    features ](img/B22389_15_08.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.8 – 使用静态、时间变化和尺度特征的汇总结果](img/B22389_15_08.png)'
- en: 'Figure 15.8: Aggregate results using the static, time-varying, and scale features'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.8：使用静态、时间变化和尺度特征的聚合结果
- en: The scale information has improved the model yet again. With that, let’s look
    at one of the last strategies we will be covering in this book.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 尺度信息再次改善了模型。有了这些，我们来看看本书最后将讨论的一种策略。
- en: Balancing the sampling procedure
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡采样过程
- en: 'We saw a few strategies for improving a global deep learning model by adding
    new types of features. Now, let’s look at a different aspect that is relevant
    in a global modeling context. In an earlier section, when we were talking about
    global deep learning models, we talked about how the process by which we sample
    a window of sequence to feed to our model can be thought of as a two-step process:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了几种通过添加新特征类型来改进全球深度学习模型的策略。现在，让我们看一下全球建模上下文中相关的另一个方面。在前面的章节中，当我们谈到全球深度学习模型时，我们讨论了将一个序列窗口采样输入模型的过程可以被看作是一个两步过程：
- en: Sampling a time series out of a set of time series.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一组时间序列中采样一个时间序列。
- en: Sampling a window out of that time series.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从时间序列中采样一个窗口。
- en: Let’s use an analogy to make the concept clearer. Imagine we have a large bowl
    that we have filled with *N* balls. Each ball in the bowl represents a time series
    in the dataset (a household in our dataset). Now, each ball, *i*, has *M*[i] chits
    of paper representing all the different windows of samples we can draw from it.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个类比来使这个概念更清晰。想象我们有一个大碗，里面填满了*N*个球。碗中的每个球代表数据集中的一个时间序列（我们数据集中的一个家庭）。现在，每个球，*i*，都有*M*[i]张纸片，表示我们可以从中抽取的所有不同样本窗口。
- en: 'In the batch sampling we use by default, we open all the balls, dump all the
    chits into the bowl, and discard the balls. Now, with our eyes closed, we pick
    *B* chits out of this bowl and set them aside. This is a batch that we sample
    from our dataset. We do not have any information that separates the chits from
    each other so the probability of picking any chit is equal, which can be formulated
    as:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们默认使用的批量采样中，我们打开所有的球，将所有纸片倒入碗中，然后丢弃这些球。现在，闭上眼睛，我们从碗中随机挑选*B*张纸片，将它们放到一边。这就是我们从数据集中采样的一个批次。我们没有任何信息来区分纸片之间的差异，所以抽到任何纸片的概率是相等的，这可以表示为：
- en: '![](img/B22389_15_003.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_15_003.png)'
- en: Now, let’s add something to our analogy to the data. We know that we have different
    kinds of time series—different lengths, different levels of consumption, and so
    on. Let’s pick one aspect, the length of the series, for our example (although
    it applies to other aspects as well). So, if we discretize the length of our time
    series, we end up with different bins; let’s assign a color for each bin. So,
    now we have *C* different-colored balls in the bowl and the chits of paper also
    are colored accordingly.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在数据类比中加入一些内容。我们知道，时间序列有不同的种类——不同的长度、不同的消费水平等等。我们选择其中一个方面，即序列的长度，作为我们的例子（尽管它同样适用于其他方面）。所以，如果我们将时间序列的长度离散化，我们就会得到不同的区间；我们为每个区间分配一个颜色。现在，我们有*C*种不同颜色的球在碗里，纸片也会按相应的颜色来标记。
- en: In our current sampling strategy (where we dump all the chits of paper, now
    colored, and pick *B* chits at random), we would end up replicating the probability
    distribution of our bowl in a batch. It is not a stretch to understand that if
    the bowl has more of the longer time series than shorter ones, the chits we draw
    will also have that bias. Consequently, the batch will also be biased toward a
    long time series. What happens because of that?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的采样策略中（我们将所有纸片都倒入碗中，并随机抽取*B*张纸片），我们最终会在一个批次中复制碗的概率分布。可以理解的是，如果碗中包含更多的长时间序列而不是短时间序列，那么我们抽到的纸片也会偏向于这一点。因此，批次也会偏向长时间序列。那会发生什么呢？
- en: In mini-batch stochastic gradient descent (we saw this in *Chapter 11*, *Introduction
    to Deep Learning*), we do a gradient update every mini-batch, and we use this
    gradient to update the model parameters so that we move closer to the minima of
    the loss function. Therefore, if a mini-batch is biased toward a particular type
    of case, then the gradient updates would be biased toward a solution that works
    better for them. There are good parallels to be drawn here to imbalanced learning.
    Longer time series and shorter time series may have different patterns, and having
    this sampling imbalance causes the model to learn patterns that work well for
    the longer time series and not so well for the shorter ones.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在小批量随机梯度下降（我们在*第 11 章*，《深度学习导论》中看到过）中，我们在每个小批次后进行一次梯度更新，并使用该梯度来更新模型参数，从而使得模型更接近损失函数的最小值。因此，如果一个小批次偏向某一类型的样本，那么梯度更新将会偏向一个对这些样本效果更好的解。这和不平衡学习有着很好的类比。较长的时间序列和较短的时间序列可能有不同的模式，而这种采样不平衡导致模型学到的模式可能对长时间序列效果较好，而对短时间序列的效果不太理想。
- en: Visualizing the data distribution
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化数据分布
- en: 'We calculated the length of each household (`LCLid`) and binned them into `10`
    bins—`bin_0` for the shortest bin and `bin_9` for the longest bin:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了每个家庭的长度（`LCLid`），并将它们分到 10 个区间中——`bin_0`代表最短的区间，`bin_9`代表最长的区间：
- en: '[PRE16]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s visualize the distribution of the bins in the original data:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化原始数据中区间的分布：
- en: '![Figure 15.9 – Distribution of length of time series ](img/B22389_15_09.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.9 – 时间序列长度分布](img/B22389_15_09.png)'
- en: 'Figure 15.9: Distribution of length of time series'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.9：时间序列长度分布
- en: 'We can see that `bin_5` and `bin_6` are the most common lengths while `bin_0`
    is the least common. Now, let’s get the first 50 batches from the dataloader and
    plot them as a stacked bar chart to check the distribution in each batch:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`bin_5`和`bin_6`是最常见的长度，而`bin_0`是最不常见的。现在，让我们从数据加载器中获取前 50 个批次，并将它们绘制为堆叠柱状图，以检查每个批次的分布：
- en: '![Figure 15.10 – Stacked bar chart of batch distribution ](img/B22389_15_10.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.10 – 批次分布的堆叠柱状图](img/B22389_15_10.png)'
- en: 'Figure 15.10: Stacked bar chart of batch distribution'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.10：批次分布的堆叠柱状图
- en: We can see that the same distribution you saw in *Figure 15.9* is replicated
    in the batch distributions as well with `bin_5` and `bin_6` leading the pack.
    `bin_0` is barely making an appearance and LCLids that are in `bin_0` would not
    have been learned that well.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在批次分布中，和*图 15.9*中看到的相同的分布也得到了复现，`bin_5`和`bin_6`占据了领先位置。`bin_0`几乎没有出现，而在`bin_0`中的LCLid将不会被学得很好。
- en: Tweaking the sampling procedure
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整采样过程
- en: Now what do we do? Let’s step into the analogy of bowls with chits inside for
    a bit. We were picking a ball at random, and we saw that the resulting distribution
    was identical to the original distribution of colors. Therefore, to get a more
    balanced distribution of colors in a batch, we need to sample different colored
    chits at different probabilities. In other words, we should be sampling more from
    colors that have low representation in the original distribution and less from
    colors that dominate the original representation.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 那么接下来我们该怎么办？让我们暂时进入一个装有纸条的碗的类比。我们在随机挑选一个球，结果发现分布和原始的颜色分布完全一致。因此，为了让批次中的颜色分布更加平衡，我们需要按不同的概率从不同颜色的纸条中抽取。换句话说，我们应该从原始分布中低频的颜色上抽取更多，而从占主导地位的颜色中抽取更少。
- en: Let’s look at the process by which we are selecting the chits from the bowl
    from another perspective. We know that the probability of selecting each chit
    in the bowl is equal. So, another way to select chits from the bowl is by using
    a uniform random number generator. We pick a chit from the bowl, generate a random
    number between 0 and 1 (*p*), and select the chit if the random number is less
    than 0.5 (*p* < 0.5). So, it is equally likely that we select or reject the chit.
    We continue this until we get *B* samples. Although a bit more inefficient than
    the previous procedure, this sampling process approximates the original procedure
    closely. The advantage here is that we have a threshold now with which we can
    tweak our sampling to suit our needs. Having a lower threshold makes the chit
    harder to accept under this sampling procedure, and having a higher threshold
    makes it easier to accept.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从另一个角度来看选择碗中筹码的过程。我们知道选择碗中每个筹码的概率是相等的。所以，另一种选择筹码的方法是使用均匀随机数生成器。我们从碗中抽取一个筹码，生成一个介于0和1之间的随机数（*p*），如果随机数小于0.5（*p*
    < 0.5），则选择该筹码。所以，我们选择或拒绝筹码的概率是相等的。我们继续进行，直到得到*B*个样本。虽然这个过程比前一个过程稍微低效一些，但它与原始过程非常接近。这里的优势在于，我们现在有了一个阈值，可以调整我们的采样以适应需求。较低的阈值使得在该采样过程中更难接受筹码，而较高的阈值则使其更容易被接受。
- en: Now that we have a threshold with which we can tweak the sampling procedure,
    all we need to do is find out the right thresholds for each of the chits so that
    the resulting batch has a uniform representation of all the colors.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个可以调整采样过程的阈值，我们需要做的就是找到每个筹码的合适阈值，以便最终的批次能够均匀地代表所有颜色。
- en: In other words, we need to find and assign the right weight to each LCLid such
    that the resulting batch will have an even distribution of all length bins.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们需要找到并为每个LCLid分配正确的权重，以使最终的批次能够均匀分布所有长度区间。
- en: How do we do that? There is a very simple strategy for that. We want the weights
    to be lower for length bins that have a lot of samples, and higher for length
    bins that have fewer samples. We can get this kind of weight by taking the inverse
    of the count of each bin. If there are *C* LCLids in a bin, the weight of the
    bin can be 1/*C*. The *Further reading* section has a link where you can read
    more about weighted random sampling and the different algorithms used for the
    purpose.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们该如何做呢？有一个非常简单的策略。我们希望对于样本多的长度区间，权重较低；对于样本少的长度区间，权重较高。我们可以通过取每个区间样本数量的倒数来得到这种权重。如果一个区间中有*C*个LCLid，则该区间的权重可以是1/*C*。*进一步阅读*部分有一个链接，您可以通过它了解更多关于加权随机采样和为此目的使用的不同算法。
- en: '`TimeSeriesDataset` has an internal index, which is a DataFrame with all the
    samples it can draw from the dataset. We can use that to construct our array of
    weights:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`TimeSeriesDataset`有一个内部索引，这是一个包含它可以从数据集抽取的所有样本的DataFrame。我们可以使用它来构建我们的权重数组：'
- en: '[PRE17]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This way ensures that the `probabilities` array has the same length as the internal
    index over which `TimeSeriesDataset` samples and that is a mandatory requirement
    when using this technique—each possible window should have a corresponding weight
    attached to it.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这样可以确保`probabilities`数组的长度与`TimeSeriesDataset`进行采样时的内部索引长度一致，这是使用这种技术时的强制要求——每个可能的窗口应该有一个对应的权重。
- en: 'Now that we have this weight, there is an easy way to put this into practice.
    We can use `WeightedRandomSampler` from PyTorch, which has been created specifically
    for this purpose:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了这个权重，有一个简单的方法可以将其付诸实践。我们可以使用PyTorch中的`WeightedRandomSampler`，它是专门为此目的创建的：
- en: '[PRE18]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Using and visualizing the dataloader with WeightedRandomSampler
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用并可视化带有`WeightedRandomSampler`的dataloader
- en: 'Now, we can use this sampler in the dataloaders we create from `TimeSeriesDataset`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在我们从`TimeSeriesDataset`创建的dataloader中使用这个采样器：
- en: '[PRE19]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let’s visualize the first 50 batches like before and see the difference:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们像之前一样可视化前50个批次，看看有什么不同：
- en: '![Figure 15.11 – Stacked bar chart of batch distribution with weighted random
    sampling ](img/B22389_15_11.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.11 – 带有加权随机采样的批次分布堆叠柱状图](img/B22389_15_11.png)'
- en: 'Figure 15.11: Stacked bar chart of batch distribution with weighted random
    sampling'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.11：带有加权随机采样的批次分布堆叠柱状图
- en: 'Now, we can see a more uniform distribution of bins in each batch. Let’s also
    see the results after training the model using this new dataloader:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到每个批次中更均匀的区间分布。让我们也看看使用这个新dataloader训练模型后的结果：
- en: '![Figure 15.12 – Aggregate results using the static, time-varying, and scale
    features along with batch samplers ](img/B22389_15_12.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.12 – 使用静态、时间变化和规模特征以及批量采样器聚合结果](img/B22389_15_12.png)'
- en: 'Figure 15.12: Aggregate results using the static, time-varying, and scale features
    along with batch samplers'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.12：使用静态、时间变化和规模特征以及批量采样器聚合结果
- en: Looks like the sampler also made a good improvement in the model in all metrics,
    except `Forecast Bias`. Although we have not achieved better results than the
    GFM(ML) (which had an MAE of 0.079581), we are close enough. Maybe with some hyperparameter
    tuning, partitioning, or stronger models, we might reach closer to that number,
    or we may not. We used a custom sampling option to make the length of the time
    series balanced in a batch. We can use the same techniques to balance it on other
    aspects such as the level of consumption, region, or any other aspect that seems
    relevant. As always in machine learning, we will need to go with our experiments
    to say anything for sure, and all we need to do is form our hypothesis about the
    problem statement and construct experiments to validate that hypothesis.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，采样器在所有指标上对模型的改进都很大，除了`预测偏差`之外。虽然我们没有比GFM(ML)（它的MAE为0.079581）取得更好的结果，但我们已经足够接近了。也许通过一些超参数调整、数据划分或更强的模型，我们能够更接近那个数值，或者可能不能。我们使用了自定义采样选项，使得每批数据中的时间序列长度保持平衡。我们可以使用相同的技术在其他方面进行平衡，比如消费水平、地区，或任何其他相关的方面。像机器学习中的所有事情一样，我们需要通过实验来确定一切，而我们需要做的就是根据问题陈述形成假设，并构建实验来验证这个假设。
- en: With that, we have come to the end of yet another practical-heavy (and compute-heavy)
    chapter. Congratulations on making it through the chapter; feel free to go back
    and refer to any points that haven’t quite landed yet.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们已完成又一章以实践为主（且计算量大）的内容。恭喜你顺利完成本章；如果有任何概念尚未完全理解，随时可以回去查阅。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小结
- en: After having built a strong foundation on deep learning models in the last few
    chapters, we started to look at a new paradigm of global models in the context
    of deep learning models. We learned how to use PyTorch Forecasting, an open-source
    library for forecasting using deep learning, and used the feature-filled `TimeSeriesDataset`
    to start developing our own models.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几章中，我们已经打下了深度学习模型的坚实基础，现在开始探讨在深度学习模型背景下的全球模型新范式。我们学习了如何使用PyTorch Forecasting，这是一个用于深度学习预测的开源库，并利用功能丰富的`TimeSeriesDataset`开始开发自己的模型。
- en: We started off with a very simple LSTM in the global context and saw how we
    can add time-varying information, static information, and the scale of individual
    time series to the features to make models better. We closed by looking at an
    alternating sampling procedure for mini-batches that helps us present a more balanced
    view of the problem in each batch. This chapter is by no means an exhaustive list
    of all such techniques to make the forecasting models better. Instead, this chapter
    aims to build the right kind of thinking that is necessary to work on your own
    models and make them work better than before.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个非常简单的LSTM开始，在全球背景下看到了如何将时间变化信息、静态信息和单个时间序列的规模添加到特征中，以改善模型性能。最后，我们讨论了交替采样程序，它帮助我们在每个批次中提供问题的更平衡视图。本章绝不是列出所有使预测模型更好的技术的详尽清单。相反，本章旨在培养一种正确的思维方式，这是在自己模型上工作并使其比之前更好地运行所必需的。
- en: Now that we have a strong foundation in deep learning and global models, it
    is time to take a look at a few specialized deep learning architectures that have
    been proposed over the years for time series forecasting in the next chapter.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经建立了深度学习和全球模型的坚实基础，是时候在下一章中探索一些多年来为时间序列预测提出的专用深度学习架构了。
- en: Further reading
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can check out the following sources for further reading:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查阅以下资料进行进一步阅读：
- en: '*How to use custom data and implement custom models and metrics* (PyTorch Forecasting):
    [https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/building.html](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/building.html)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何使用自定义数据并实现自定义模型和指标*（PyTorch Forecasting）：[https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/building.html](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/building.html)'
- en: '*Random Sampling from Databases* by Frank Olken, pages 22–23: [https://dsf.berkeley.edu/papers/UCB-PhD-olken.pdf](https://dsf.berkeley.edu/papers/UCB-PhD-olken.pdf)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《数据库中的随机抽样》作者：Frank Olken，第22-23页：[https://dsf.berkeley.edu/papers/UCB-PhD-olken.pdf](https://dsf.berkeley.edu/papers/UCB-PhD-olken.pdf)
- en: Join our community on Discord
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们在 Discord 上的社区
- en: 'Join our community’s Discord space for discussions with authors and other readers:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mts](https://packt.link/mts)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mts](https://packt.link/mts)'
- en: '![](img/QR_Code15080603222089750.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code15080603222089750.png)'
