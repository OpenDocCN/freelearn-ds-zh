- en: Chapter 1. Introducing Kafka
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。介绍Kafka
- en: In today's world, real-time information is continuously being generated by applications
    (business, social, or any other type), and this information needs easy ways to
    be reliably and quickly routed to multiple types of receivers. Most of the time,
    applications that produce information and applications that are consuming this
    information are well apart and inaccessible to each other. These heterogeneous
    application leads to redevelopment for providing an integration point between
    them. Therefore, a mechanism is required for the seamless integration of information
    from producers and consumers to avoid any kind of application rewriting at either
    end.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今世界，实时信息不断由应用程序（商业、社交或任何其他类型）生成，并且这些信息需要可靠快速地路由到多种类型的接收方。大多数情况下，生成信息的应用程序和消费此信息的应用程序相距甚远，彼此无法访问。这些异构应用程序导致了为它们之间提供集成点的重新开发。因此，需要一种机制来无缝集成来自生产者和消费者的信息，以避免任何一端的应用程序重写。
- en: Welcome to the world of Apache Kafka
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欢迎来到Apache Kafka的世界
- en: 'In the present big-data era, the very first challenge is to collect the data
    as it is a huge amount of data and the second challenge is to analyze it. This
    analysis typically includes the following types of data and much more:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的大数据时代，第一个挑战是收集数据，因为数据量巨大，第二个挑战是分析数据。这种分析通常包括以下类型的数据以及更多：
- en: User behavior data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户行为数据
- en: Application performance tracing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序性能跟踪
- en: Activity data in the form of logs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以日志形式的活动数据
- en: Event messages
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件消息
- en: Message publishing is a mechanism for connecting various applications with the
    help of messages that are routed between—for example, by a message broker such
    as Kafka. Kafka is a solution to the real-time problems of any software solution;
    that is to say, dealing with real-time volumes of information and routing it to
    multiple consumers quickly. Kafka provides seamless integration between information
    from producers and consumers without blocking the producers of the information
    and without letting producers know who the final consumers are.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 消息发布是一种通过消息连接各种应用程序的机制，例如通过消息代理（如Kafka）。Kafka是解决任何软件解决方案的实时问题的解决方案；也就是说，处理实时信息量并快速路由到多个消费者。Kafka提供了生产者和消费者信息之间的无缝集成，而不会阻塞信息的生产者，也不会让生产者知道最终的消费者是谁。
- en: 'Apache Kafka is an open source, distributed, partitioned, and replicated commit-log-based
    publish-subscribe messaging system, mainly designed with the following characteristics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka是一个开源的、分布式的、分区的、复制的基于提交日志的发布-订阅消息系统，主要具有以下特点：
- en: '**Persistent messaging**: To derive the real value from big data, any kind
    of information loss cannot be afforded. Apache Kafka is designed with O(1) disk
    structures that provide constant-time performance even with very large volumes
    of stored messages that are in the order of TBs. With Kafka, messages are persisted
    on disk as well as replicated within the cluster to prevent data loss.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持久化消息**：为了从大数据中获得真正的价值，不能承受任何信息丢失。Apache Kafka设计了O(1)磁盘结构，即使存储的消息量达到TB级别，也能提供恒定的性能。使用Kafka，消息被持久化在磁盘上，并在集群内复制，以防止数据丢失。'
- en: '**High throughput**: Keeping big data in mind, Kafka is designed to work on
    commodity hardware and to handle hundreds of MBs of reads and writes per second
    from large number of clients.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高吞吐量**：考虑到大数据，Kafka被设计为在商品硬件上运行，并且能够处理来自大量客户端的每秒数百MB的读写。'
- en: '**Distributed**: Apache Kafka with its cluster-centric design explicitly supports
    message partitioning over Kafka servers and distributing consumption over a cluster
    of consumer machines while maintaining per-partition ordering semantics. Kafka
    cluster can grow elastically and transparently without any downtime.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式**：Apache Kafka以其集群中心的设计明确支持Kafka服务器上的消息分区，并在维护每个分区的顺序语义的同时，在消费者机器集群上分发消费。Kafka集群可以在没有任何停机时间的情况下弹性地透明地增长。'
- en: '**Multiple client support**: The Apache Kafka system supports easy integration
    of clients from different platforms such as Java, .NET, PHP, Ruby, and Python.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多客户端支持**：Apache Kafka系统支持轻松集成来自不同平台的客户端，如Java、.NET、PHP、Ruby和Python。'
- en: '**Real time**: Messages produced by the producer threads should be immediately
    visible to consumer threads; this feature is critical to event-based systems such
    as **Complex Event Processing** (**CEP**) systems.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时**：生产者线程产生的消息应立即对消费者线程可见；这个特性对于基于事件的系统（如**复杂事件处理**（**CEP**）系统）至关重要。'
- en: Kafka provides a real-time publish-subscribe solution that overcomes the challenges
    of consuming the real-time and batch data volumes that may grow in order of magnitude
    to be larger than the real data. Kafka also supports parallel data loading in
    the Hadoop systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka提供了一个实时的发布-订阅解决方案，克服了消费实时和批量数据量的挑战，这些数据量可能增长到比真实数据更大的数量级。Kafka还支持在Hadoop系统中进行并行数据加载。
- en: 'The following diagram shows a typical big data aggregation-and-analysis scenario
    supported by the Apache Kafka messaging system:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Apache Kafka消息系统支持的典型大数据聚合和分析场景：
- en: '![Welcome to the world of Apache Kafka](img/3090OS_01_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: 欢迎来到Apache Kafka的世界
- en: 'On the production side, there are different kinds of producers, such as the
    following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产方面，有不同类型的生产者，例如以下类型：
- en: Frontend web applications generating application logs
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成应用程序日志的前端Web应用程序
- en: Producer proxies generating web analytics logs
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成网络分析日志的生产者代理
- en: Producer adapters generating transformation logs
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成转换日志的生产者适配器
- en: Producer services generating invocation trace logs
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成调用跟踪日志的生产者服务
- en: 'On the consumption side, there are different kinds of consumers, such as the
    following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费方面，有不同类型的消费者，例如以下类型：
- en: Offline consumers that are consuming messages and storing them in Hadoop or
    traditional data warehouse for offline analysis
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费消息并将其存储在Hadoop或传统数据仓库中进行离线分析的离线消费者
- en: Near real-time consumers that are consuming messages and storing them in any
    NoSQL datastore, such as HBase or Cassandra, for near real-time analytics
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者几乎实时地消费消息并将其存储在任何NoSQL数据存储中，例如HBase或Cassandra，以进行几乎实时的分析
- en: Real-time consumers, such as Spark or Storm, that filter messages in-memory
    and trigger alert events for related groups
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时消费者，如Spark或Storm，在内存中过滤消息并触发相关组的警报事件
- en: Why do we need Kafka?
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们为什么需要Kafka？
- en: A large amount of data is generated by companies having any form of web- or
    device-based presence and activity. Data is one of the newer ingredients in these
    Internet-based systems and typically includes user activity; events corresponding
    to logins; page visits; clicks; social networking activities such as likes, shares,
    and comments; and operational and system metrics. This data is typically handled
    by logging and traditional log aggregation solutions due to high throughput (millions
    of messages per second). These traditional solutions are the viable solutions
    for providing logging data to an offline analysis system such as Hadoop. However,
    the solutions are very limiting for building real-time processing systems.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 任何形式的网络或设备活动都会产生大量数据。数据是这些基于互联网的系统中的新成分之一，通常包括用户活动；与登录对应的事件；页面访问；点击；社交网络活动，如点赞、分享和评论；以及操作和系统指标。由于吞吐量高（每秒数百万条消息），这些数据通常由日志记录和传统的日志聚合解决方案处理。这些传统解决方案是为向离线分析系统（如Hadoop）提供日志数据而设计的可行解决方案。然而，这些解决方案对于构建实时处理系统来说非常有限。
- en: 'According to the new trends in Internet applications, activity data has become
    a part of production data and is used to run analytics in real time. These analytics
    can be:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 根据互联网应用的新趋势，活动数据已成为生产数据的一部分，并用于实时运行分析。这些分析可以是：
- en: Search-based on relevance
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于相关性的搜索
- en: Recommendations based on popularity, co-occurrence, or sentimental analysis
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于流行度、共现或情感分析的推荐
- en: Delivering advertisements to the masses
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向大众投放广告
- en: Internet application security from spam or unauthorized data scraping
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互联网应用程序安全，防止垃圾邮件或未经授权的数据抓取
- en: Device sensors sending high-temperature alerts
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备传感器发送高温警报
- en: Any abnormal user behavior or application hacking
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何异常的用户行为或应用程序黑客攻击
- en: Real-time usage of these multiple sets of data collected from production systems
    has become a challenge because of the volume of data collected and processed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于收集和处理的数据量大，从生产系统中收集的这些多组数据的实时使用已经成为一个挑战。
- en: Apache Kafka aims to unify offline and online processing by providing a mechanism
    for parallel load in Hadoop systems as well as the ability to partition real-time
    consumption over a cluster of machines. Kafka can be compared with Scribe or Flume
    as it is useful for processing activity stream data; but from the architecture
    perspective, it is closer to traditional messaging systems such as ActiveMQ or
    RabitMQ.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka旨在通过提供在Hadoop系统中进行并行加载的机制以及在一组机器的集群上对实时消费进行分区的能力，统一离线和在线处理。Kafka可以与Scribe或Flume进行比较，因为它对于处理活动流数据非常有用；但从架构的角度来看，它更接近于传统的消息系统，如ActiveMQ或RabitMQ。
- en: Kafka use cases
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka的用例
- en: 'There are number of ways in which Kafka can be used in any architecture. This
    section discusses some of the popular use cases for Apache Kafka and the well-known
    companies that have adopted Kafka. The following are the popular Kafka use cases:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka可以在任何架构中以多种方式使用。本节讨论了Apache Kafka的一些热门用例以及采用Kafka的知名公司。以下是热门的Kafka用例：
- en: '**Log aggregation**: This is the process of collecting physical log files from
    servers and putting them in a central place (a file server or HDFS) for processing.
    Using Kafka provides clean abstraction of log or event data as a stream of messages,
    thus taking away any dependency over file details. This also gives lower-latency
    processing and support for multiple data sources and distributed data consumption.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志聚合：这是从服务器收集物理日志文件并将它们放在一个中心位置（文件服务器或HDFS）进行处理的过程。使用Kafka提供了对日志或事件数据的干净抽象，作为一系列消息流，从而消除了对文件细节的任何依赖。这还提供了更低的延迟处理和对多个数据源和分布式数据消费的支持。
- en: '**Stream processing**: Kafka can be used for the use case where collected data
    undergoes processing at multiple stages—an example is raw data consumed from topics
    and enriched or transformed into new Kafka topics for further consumption. Hence,
    such processing is also called stream processing.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流处理：Kafka可用于收集的数据在多个阶段进行处理的用例，一个例子是从主题消耗的原始数据，并对其进行丰富或转换为新的Kafka主题以供进一步消费。因此，这种处理也被称为流处理。
- en: '**Commit logs**: Kafka can be used to represent external commit logs for any
    large scale distributed system. Replicated logs over Kafka cluster help failed
    nodes to recover their states.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提交日志：Kafka可用于表示任何大规模分布式系统的外部提交日志。Kafka集群上的复制日志帮助失败的节点恢复其状态。
- en: '**Click stream tracking**: Another very important use case for Kafka is to
    capture user click stream data such as page views, searches, and so on as real-time
    publish-subscribe feeds. This data is published to central topics with one topic
    per activity type as the volume of the data is very high. These topics are available
    for subscription, by many consumers for a wide range of applications including
    real-time processing and monitoring.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击流跟踪：Kafka的另一个非常重要的用例是捕获用户点击流数据，例如页面浏览，搜索等，作为实时发布订阅源。这些数据以每种活动类型一个主题的形式发布到中央主题，因为数据量非常大。这些主题可供订阅，由许多消费者用于各种应用，包括实时处理和监控。
- en: '**Messaging**: Message brokers are used for decoupling data processing from
    data producers. Kafka can replace many popular message brokers as it offers better
    throughput, built-in partitioning, replication, and fault-tolerance.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息传递：消息代理用于将数据处理与数据生产者解耦。Kafka可以取代许多流行的消息代理，因为它提供更好的吞吐量、内置分区、复制和容错性。
- en: 'Some of the companies that are using Apache Kafka in their respective use cases
    are as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一些正在使用Apache Kafka的公司及其各自的用例如下：
- en: '**LinkedIn** ([www.linkedin.com](http://www.linkedin.com)): Apache Kafka is
    used at LinkedIn for the streaming of activity data and operational metrics. This
    data powers various products such as LinkedIn News Feed and LinkedIn Today, in
    addition to offline analytics systems such as Hadoop.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LinkedIn（www.linkedin.com）：Apache Kafka在LinkedIn用于活动数据和运营指标的流式传输。这些数据支持LinkedIn新闻动态和LinkedIn今日等各种产品，以及Hadoop等离线分析系统。
- en: '**DataSift** ([www.datasift.com](http://www.datasift.com)): At DataSift, Kafka
    is used as a collector to monitor events and as a tracker of users'' consumption
    of data streams in real time.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataSift（www.datasift.com）：在DataSift，Kafka用作事件监视器的收集器，以及实时跟踪用户对数据流的消耗。
- en: '**Twitter** ([www.twitter.com](http://www.twitter.com)): Twitter uses Kafka
    as a part of its Storm—a stream-processing infrastructure.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter（www.twitter.com）：Twitter将Kafka作为其Storm流处理基础设施的一部分使用。
- en: '**Foursquare** ([www.foursquare.com](http://www.foursquare.com)): Kafka powers
    online-to-online and online-to-offline messaging at Foursquare. It is used to
    integrate Foursquare monitoring and production systems with Foursquare-and Hadoop-based
    offline infrastructures.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foursquare（www.foursquare.com）：Kafka在Foursquare的在线到在线和在线到离线消息传递中发挥作用。它用于将Foursquare监控和生产系统与基于Foursquare和Hadoop的离线基础设施集成。
- en: '**Square** ([www.squareup.com](http://www.squareup.com)): Square uses Kafka
    as a *bus* to move all system events through Square''s various datacenters. This
    includes metrics, logs, custom events, and so on. On the consumer side, it outputs
    into Splunk, Graphite, or Esper-like real-time alerting.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Square（www.squareup.com）：Square使用Kafka作为*总线*，将所有系统事件通过Square的各个数据中心传输。这包括指标、日志、自定义事件等。在消费者端，它输出到Splunk、Graphite或类似实时警报的Esper中。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The source of the preceding information is [https://cwiki.apache.org/confluence/display/KAFKA/Powered+By](https://cwiki.apache.org/confluence/display/KAFKA/Powered+By).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上述信息的来源是https://cwiki.apache.org/confluence/display/KAFKA/Powered+By。
- en: Installing Kafka
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Kafka
- en: 'Kafka is an Apache project and its current version 0.8.1.1 is available as
    a stable release. This Kafka 0.8.x offers many advanced features compared to the
    older version (prior to 0.8.x). A few of its advancements are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka是一个Apache项目，其当前版本0.8.1.1可作为稳定版本使用。与旧版本（0.8.x之前）相比，Kafka 0.8.x提供了许多高级功能。其一些进步如下：
- en: Prior to 0.8.x, any unconsumed partition of data within the topic could be lost
    if the broker failed. Now the partitions are provided with a replication factor.
    This ensures that any committed message would not be lost, as at least one replica
    is available.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在0.8.x之前，如果代理失败，主题中的任何未消耗分区都可能会丢失。现在，分区提供了一个复制因子。这确保了任何已提交的消息不会丢失，因为至少有一个副本可用。
- en: The previous feature also ensures that all the producers and consumers are replication-aware
    (the replication factor is a configurable property). By default, the producer's
    message sending request is blocked until the message is committed to all active
    replicas; however, producers can also be configured to commit messages to a single
    broker.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先前的功能还确保所有生产者和消费者都具有复制意识（复制因子是可配置属性）。默认情况下，生产者的消息发送请求会被阻塞，直到消息提交到所有活动副本；但是，生产者也可以配置为将消息提交到单个代理。
- en: Like Kafka producers, the Kafka consumer polling model changes to a long-pulling
    model and gets blocked until a committed message is available from the producer,
    which avoids frequent pulling.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Kafka生产者一样，Kafka消费者的轮询模型变为长轮询模型，并在从生产者获取可用的已提交消息之前被阻塞，从而避免频繁轮询。
- en: Additionally, Kafka 0.8.x also comes with a set of administrative tools, such
    as controlled cluster shutdown and the Lead replica election tool, for managing
    the Kafka cluster.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，Kafka 0.8.x还配备了一套管理工具，例如受控集群关闭和领导副本选举工具，用于管理Kafka集群。
- en: The major limitation with Kafka version 0.8.x is that it can't replace the version
    prior to 0.8, as it is not backward-compatible.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 0.8.x的主要限制是它无法替换0.8之前的版本，因为它不向后兼容。
- en: Coming back to installing Kafka, as a first step we need to download the available
    stable release (all the processes have been tested on 64-bit CentOS 6.4 OS and
    may differ on other kernel-based OS). Now let's see what steps need to be followed
    in order to install Kafka.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 回到安装Kafka，作为第一步，我们需要下载可用的稳定版本（所有过程都在64位CentOS 6.4操作系统上进行了测试，可能在其他基于内核的操作系统上有所不同）。现在让我们看看安装Kafka需要遵循哪些步骤。
- en: Installing prerequisites
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装先决条件
- en: Kafka is implemented in Scala and uses build tool **Gradle** to build Kafka
    binaries. Gradle is a build automation tool for Scala, Groovy, and Java projects
    that requires Java 1.7 or later.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka是用Scala实现的，并使用构建工具Gradle构建Kafka二进制文件。Gradle是Scala、Groovy和Java项目的构建自动化工具，需要Java
    1.7或更高版本。
- en: Installing Java 1.7 or higher
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Java 1.7或更高版本
- en: 'Perform the following steps to install Java 1.7 or later:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤安装Java 1.7或更高版本：
- en: 'Download the `jdk-7u67-linux-x64.rpm` release from Oracle''s website: [http://www.oracle.com/technetwork/java/javase/downloads/index.html](http://www.oracle.com/technetwork/java/javase/downloads/index.html).'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Oracle的网站下载`jdk-7u67-linux-x64.rpm`版本：http://www.oracle.com/technetwork/java/javase/downloads/index.html。
- en: 'Change the file mode as follows:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改文件模式如下：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Change to the directory in which you want to perform the installation. To do
    so, type the following command:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到要执行安装的目录。为此，请键入以下命令：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For example, to install the software in the `/usr/java/` directory, type the
    following command:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要在`/usr/java/`目录中安装软件，请键入以下命令：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Run the installer using the following command:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行安装程序：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, add the environment variable `JAVA_HOME`. The following command will
    write the `JAVA_HOME` environment variable to the file `/etc/profile` that contains
    a system-wide environment configuration:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，添加环境变量`JAVA_HOME`。以下命令将`JAVA_HOME`环境变量写入包含系统范围环境配置的文件`/etc/profile`：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Downloading Kafka
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载Kafka
- en: 'Perform the following steps to download Kafka release 0.8.1.1:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤下载Kafka 0.8.1.1版本：
- en: 'Download the current beta release of Kafka (0.8) into a folder on your filesystem
    (for example, `/opt`) using the following command:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载当前的Kafka（0.8）beta版本到文件系统上的文件夹中（例如，`/opt`），使用以下命令：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding URL may change. Check the correct download version and location
    at [http://kafka.apache.org/downloads.html](http://kafka.apache.org/downloads.html).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 上述URL可能会更改。请在[http://kafka.apache.org/downloads.html](http://kafka.apache.org/downloads.html)检查正确的下载版本和位置。
- en: 'Extract the downloaded `kafka_2.9.2-0.8.1.1.tgz` file using the following command:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令解压下载的`kafka_2.9.2-0.8.1.1.tgz`文件：
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After extraction of the `kafka_2.9.2-0.8.1.1.tgz` file, the directory structure
    for Kafka 0.8.1.1 looks as follows:![Downloading Kafka](img/3090OS_01_02.jpg)
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解压`kafka_2.9.2-0.8.1.1.tgz`文件后，Kafka 0.8.1.1的目录结构如下所示：![下载Kafka](img/3090OS_01_02.jpg)
- en: 'Finally, add the Kafka bin folder to `PATH` as follows:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将Kafka bin文件夹添加到`PATH`中，如下所示：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Building Kafka
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建Kafka
- en: 'The default Scala version that is used to build Kafka release 0.8.1.1 is Scala
    2.9.2 but the Kafka source code can also be compiled from other Scala versions
    as well, such as 2.8.0, 2.8.2, 2.9.1, or 2.10.1\. Use the following the command
    to build the Kafka source:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 用于构建Kafka 0.8.1.1版本的默认Scala版本是Scala 2.9.2，但Kafka源代码也可以从其他Scala版本编译，比如2.8.0、2.8.2、2.9.1或2.10.1。使用以下命令构建Kafka源代码：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In Kafka 8.x onwards, the Gradle tool is used to compile the Kafka source code
    (available in `kafka-0.8.1.1-src.tgz`) and build the Kafka binaries (JAR files).
    Similar to Kafka JAR, the unit test or source JAR can also be built using the
    Gradle build tool. For more information on build-related instructions, refer to
    [https://github.com/apache/kafka/blob/0.8.1/README.md](https://github.com/apache/kafka/blob/0.8.1/README.md).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kafka 8.x及以后的版本中，Gradle工具用于编译Kafka源代码（包含在`kafka-0.8.1.1-src.tgz`中）并构建Kafka二进制文件（JAR文件）。类似于Kafka
    JAR，单元测试或源代码JAR也可以使用Gradle构建工具构建。有关构建相关说明的更多信息，请参阅[https://github.com/apache/kafka/blob/0.8.1/README.md](https://github.com/apache/kafka/blob/0.8.1/README.md)。
- en: Summary
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have seen how companies are evolving the mechanism of collecting
    and processing application-generated data, and are learning to utilize the real
    power of this data by running analytics over it.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到公司如何演变收集和处理应用生成的数据的机制，并学会了通过对其进行分析来利用这些数据的真正力量。
- en: You also learned how to install 0.8.1.x. The following chapter discusses the
    steps required to set up single- or multi-broker Kafka clusters.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您还学会了如何安装0.8.1.x。以下章节讨论了设置单个或多个broker Kafka集群所需的步骤。
