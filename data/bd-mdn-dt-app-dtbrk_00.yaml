- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: As datasets have exploded in size with the introduction of cheap cloud storage
    and processing data in near real time has become an industry standard, many organizations
    have turned to the lakehouse architecture, which combines the fast BI speeds of
    a traditional data warehouse with the scalable ETL processing of big data in the
    cloud. The Databricks Data Intelligence Platform – built upon several open source
    technologies, including Apache Spark, Delta Lake, MLflow, and Unity Catalog –
    eliminates friction points and accelerates the design and deployment of modern
    data applications built for the l akehouse.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据集大小的爆炸性增长，廉价云存储的引入，以及近实时数据处理成为行业标准，许多组织转向湖屋架构，它将传统数据仓库的快速商业智能（BI）速度与云中大数据的可扩展
    ETL 处理相结合。Databricks 数据智能平台——建立在包括 Apache Spark、Delta Lake、MLflow 和 Unity Catalog
    在内的多个开源技术之上——消除了摩擦点，加速了现代数据应用的设计和部署，这些应用是为湖屋而构建的。
- en: In this book, you’ll start with an overview of the Delta Lake format, cover
    core concepts of the Databricks Data Intelligence Platform, and master building
    data pipelines using the Delta Live Tables framework. We’ll dive into applying
    data transformations, how to implement the Databricks medallion architecture,
    and how to continuously monitor the quality of data landing in your lakehouse.
    You’ll learn how to react to incoming data using the Databricks Auto Loader feature
    and automate real-time data processing using Databricks workflows. You’ll learn
    how to use CI/CD tools such as **Terraform** and **Databricks Asset Bundles**
    ( **DABs** ) to deploy data pipeline changes automatically across deployment environments,
    as well as monitor, control, and optimize cloud costs along the way. By the end
    of this book, you will have mastered building a production-ready, modern data
    application using the Databricks Data Intelligence Platform.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，您将首先概览 Delta Lake 格式，涵盖 Databricks 数据智能平台的核心概念，并掌握使用 Delta Live Tables
    框架构建数据管道的技巧。我们将深入探讨数据转换的应用，如何实现 Databricks 镶嵌架构，以及如何持续监控数据在数据湖屋中的质量。您将学习如何使用 Databricks
    Auto Loader 功能对接收到的数据做出响应，并通过 Databricks 工作流实现实时数据处理的自动化。您还将学习如何使用 CI/CD 工具，如**Terraform**和**Databricks
    资产包**（**DABs**），自动化部署数据管道更改到不同的部署环境，并在此过程中监控、控制和优化云成本。在本书结束时，您将掌握如何使用 Databricks
    数据智能平台构建一个生产就绪的现代数据应用。
- en: With Databricks recently named a Leader in the 2024 Gartner Magic Quadrant for
    Data Science and Machine Learning Platforms, the demand for mastering a skillset
    in the Databricks Data Intelligence Platform is only expected to grow in the coming
    years.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Databricks 最近被评为 2024 年 Gartner 数据科学与机器学习平台魔力象限的领导者，预计未来几年对掌握 Databricks
    数据智能平台技能的需求将持续增长。
- en: Who this book is for
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适用人群
- en: This book is for data engineers, data scientists, and data stewards tasked with
    enterprise data processing for their organizations. This book will simplify learning
    advanced data engineering techniques on Databricks, making implementing a cutting-edge
    lakehouse accessible to individuals with varying technical expertise. However,
    beginner-level knowledge of Apache Spark and Python is needed to make the most
    out of the code examples in this book.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本书适用于数据工程师、数据科学家和负责组织企业数据处理的数据管理员。本书将简化在 Databricks 上学习高级数据工程技术的过程，使实现前沿的湖屋架构对具有不同技术水平的个人都变得可访问。然而，为了最大化利用本书中的代码示例，您需要具备
    Apache Spark 和 Python 的初级知识。
- en: What this book covers
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容
- en: '[*Chapter 1*](B22011_01.xhtml#_idTextAnchor014) , *An Introduction to Delta
    Live Tables* , discusses building near-real-time data pipelines using the Delta
    Live Tables framework. It covers the fundamentals of pipeline design as well as
    the core concepts of the Delta Lake format. The chapter concludes with a simple
    example of building a Delta Live Table pipeline from start to finish.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第1章*](B22011_01.xhtml#_idTextAnchor014)，*Delta Live Tables 介绍*，讨论了如何使用 Delta
    Live Tables 框架构建近实时数据管道。它涵盖了管道设计的基础知识，以及 Delta Lake 格式的核心概念。本章最后通过一个简单的示例展示了如何从头到尾构建一个
    Delta Live Table 数据管道。'
- en: '[*Chapter 2*](B22011_02.xhtml#_idTextAnchor052) , *Applying Data Transformations
    Using Delta Live Tables* , explores data transformations using Delta Live Tables,
    guiding you through the process of cleaning, refining, and enriching data to meet
    specific business requirements. You will learn how to use Delta Live Tables to
    ingest data from a variety of input sources, register datasets in Unity Catalog,
    and effectively apply changes to downstream tables.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第2章*](B22011_02.xhtml#_idTextAnchor052)，*使用Delta Live Tables应用数据转化*，探讨了如何使用Delta
    Live Tables进行数据转化，指导你清洗、精炼和丰富数据以满足特定的业务需求。你将学习如何使用Delta Live Tables从多种输入源中摄取数据，在Unity
    Catalog中注册数据集，并有效地将变更应用于下游表。'
- en: '[*Chapter 3*](B22011_03.xhtml#_idTextAnchor079) , *Managing Data Quality Using
    Delta Live Tables* , introduces several techniques for enforcing data quality
    requirements on newly arriving data. You will learn how to define data quality
    constraints using Expectations in the Delta Live Tables framework, as well as
    monitor the data quality of a pipeline in near real time.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第3章*](B22011_03.xhtml#_idTextAnchor079)，*使用Delta Live Tables管理数据质量*，介绍了对新到数据强制执行数据质量要求的几种方法。你将学习如何使用Delta
    Live Tables框架中的期望定义数据质量约束，并实时监控管道的数据质量。'
- en: '[*Chapter 4*](B22011_04.xhtml#_idTextAnchor103) , *Scaling DLT Pipelines* ,
    explains how to scale a **Delta Live Tables** ( **DLT** ) pipeline to handle the
    unpredictable demands of a typical production environment. You will take a deep
    dive into configuring pipeline settings using the DLT UI and Databricks Pipeline
    REST API. You will also gain a better understanding of the daily DLT maintenance
    tasks that are run in the background and how to optimize table layouts to improve
    performance.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第4章*](B22011_04.xhtml#_idTextAnchor103)，*扩展DLT管道*，解释了如何扩展**Delta Live Tables**（**DLT**）管道，以应对典型生产环境中不可预测的需求。你将深入了解如何使用DLT
    UI和Databricks Pipeline REST API配置管道设置。此外，你还将更好地理解日常运行在后台的DLT维护任务，以及如何优化表格布局以提升性能。'
- en: '[*Chapter 5*](B22011_05.xhtml#_idTextAnchor126) , *Mastering Data Governance
    in the Lakehouse* *with* *Unity Catalog* , provides a comprehensive guide to enhancing
    data governance and compliance of your lakehouse using Unity Catalog. You will
    learn how to enable Unity Catalog on a Databricks workspace, enable data discovery
    using metadata tags, and implement fine-grained row and column-level access control
    of datasets.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第5章*](B22011_05.xhtml#_idTextAnchor126)，*通过Unity Catalog精通湖仓中的数据治理*，提供了一个全面的指南，帮助你使用Unity
    Catalog增强湖仓的数据治理和合规性。你将学习如何在Databricks工作区启用Unity Catalog，使用元数据标签启用数据发现，并实施数据集的精细粒度行级和列级访问控制。'
- en: '[*Chapter 6*](B22011_06.xhtml#_idTextAnchor148) , *Managing Data Locations
    in Unity Catalog* , explores how to effectively manage storage locations using
    Unity Catalog. You will learn how to govern data access across various roles and
    departments within an organization while ensuring security and auditability with
    the Databricks Data Intelligence Platform.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第6章*](B22011_06.xhtml#_idTextAnchor148)，*在Unity Catalog中管理数据位置*，探讨了如何使用Unity
    Catalog有效地管理存储位置。你将学习如何在组织内部跨不同角色和部门管理数据访问，同时确保安全性和可审计性，利用Databricks数据智能平台实现这一目标。'
- en: '[*Chapter 7*](B22011_07.xhtml#_idTextAnchor165) , *Viewing Data Lineage using
    Unity Catalog* , discusses tracing data origins, visualizing data transformations,
    and identifying upstream and downstream dependencies by tracing data lineage in
    Unity Catalog. By the end of the chapter, You will be equipped with the skills
    needed to validate that data is coming from trusted sources.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第7章*](B22011_07.xhtml#_idTextAnchor165)，*使用Unity Catalog查看数据血缘*，讨论了追踪数据来源、可视化数据转化过程，以及通过在Unity
    Catalog中追踪数据血缘来识别上下游依赖关系。在本章结束时，你将掌握验证数据来源是否可信的技能。'
- en: '[*Chapter 8*](B22011_08.xhtml#_idTextAnchor185) , *Deploy* *ing* *, Maintain*
    *ing* *, and Administrat* *ing* *DLT* *Pipelines Using Terraform* , covers deploying
    DLT pipelines using the Databricks Terraform provider. You will learn how to set
    up a local development environment and automate a continuous build and deployment
    pipeline, along with best practices and future considerations.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第8章*](B22011_08.xhtml#_idTextAnchor185)，*使用Terraform部署、维护和管理DLT管道*，介绍了如何使用Databricks
    Terraform提供程序来部署DLT管道。你将学习如何设置本地开发环境，并自动化构建和部署管道，同时涵盖最佳实践和未来考虑事项。'
- en: '[*Chapter 9*](B22011_09.xhtml#_idTextAnchor222) , *Leveraging Databricks Asset
    Bundles to Streamline Data Pipeline Deployment* , explores how DABs can be used
    to streamline the deployment of data analytics projects and improve cross-team
    collaboration. You will gain an understanding of the practical use of DABs through
    several hands-on examples.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 9 章*](B22011_09.xhtml#_idTextAnchor222)，*利用 Databricks 资产包简化数据管道部署*，探讨了如何使用
    DABs 简化数据分析项目的部署，并促进跨团队协作。你将通过几个实践案例深入理解 DABs 的实际应用。'
- en: '[*Chapter 10*](B22011_10.xhtml#_idTextAnchor249) *,* *Monitoring Data Pipelines
    in Production* , delves into the crucial task of monitoring data pipelines in
    Databricks. You will learn various mechanisms for tracking pipeline health, performance,
    and data quality within the Databricks Data Intelligence Platform.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 10 章*](B22011_10.xhtml#_idTextAnchor249)，*监控生产中的数据管道*，深入探讨了在 Databricks
    中监控数据管道这一关键任务。你将学习到在 Databricks 数据智能平台中追踪管道健康、性能和数据质量的各种机制。'
- en: To get the most out of this book
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为了充分利用本书
- en: 'While not a mandatory requirement, to get the most out of this book, it’s recommended
    that you have beginner-level knowledge of Python and Apache Spark, and at least
    some knowledge of navigating around the Databricks Data Intelligence Platform.
    It’s also recommended to have the following dependencies installed locally in
    order to follow along with the hands-on exercises and code examples throughout
    the book:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不是强制要求，但为了充分利用本书，建议你具备 Python 和 Apache Spark 的初学者水平知识，并且至少对如何在 Databricks
    数据智能平台中导航有一定了解。为了配合本书中的实践练习和代码示例，建议在本地安装以下依赖项：
- en: '| **Software/hardware covered in** **the book** | **Operating** **system requirements**
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **书中涵盖的软件/硬件** | **操作系统要求** |'
- en: '| Python 3.6+ | Windows, macOS, or Linux |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| Python 3.6+ | Windows、macOS 或 Linux |'
- en: '| Databricks CLI 0.205+ |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Databricks CLI 0.205+ |'
- en: Furthermore, it’s recommended that you have a Databricks account and workspace
    to log in, import notebooks, create clusters, and create new data pipelines. If
    you do not have a Databricks account, you can sign up for a free trial on the
    Databricks website [https://www.databricks.com/try-databricks](https://www.databricks.com/try-databricks)
    .
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，建议你拥有一个 Databricks 账户和工作区，以便登录、导入笔记本、创建集群和创建新的数据管道。如果你没有 Databricks 账户，可以在
    Databricks 网站上注册免费试用：[https://www.databricks.com/try-databricks](https://www.databricks.com/try-databricks)。
- en: '**If you are using the digital version of this book, we advise you to type
    the code yourself or access the code from the book’s GitHub repository (a link
    is available in the next section). Doing so will help you avoid any potential
    errors related to the copying and pasting** **of code.**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你使用的是本书的数字版本，我们建议你自己输入代码，或者从本书的 GitHub 仓库获取代码（下一个章节中有链接）。这样做可以帮助你避免与代码复制和粘贴相关的潜在错误**
    **。**'
- en: Download the example code files
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: You can download the example code files for this book from GitHub at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse)
    . If there’s an update to the code, it will be updated in the GitHub repository.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 GitHub 下载本书的示例代码文件：[https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse)。如果代码有更新，将会在
    GitHub 仓库中更新。
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)
    . Check them out!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还从丰富的书籍和视频目录中提供其他代码包，可以在 [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)
    找到。快来看看吧！
- en: Conventions used
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的约定
- en: There are a number of text conventions used throughout this book.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了若干文本约定。
- en: '**Code in text** : Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: “The result of the data generator notebook should
    be three tables in total: **youtube_channels** , **youtube_channel_artists** ,
    and **combined_table** .”'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本中的代码**：表示文本中的代码词汇、数据库表名、文件夹名称、文件名、文件扩展名、路径名、虚拟 URL、用户输入和 Twitter 用户名。举个例子：“数据生成器笔记本的结果应为三个表：**youtube_channels**，**youtube_channel_artists**，和
    **combined_table**。”'
- en: 'A block of code is set as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块的设置如下：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望引起您对代码块中特定部分的注意时，相关的行或项目会以粗体显示：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出均按如下格式书写：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Bold** : Indicates a new term, an important word, or words that you see onscreen.
    For instance, words in menus or dialog boxes appear in **bold** . Here is an example:
    “Click the **Run all** button at the top right of the Databricks workspace to
    execute all the notebook cells, verifying that all cells execute successfully.”'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示新术语、重要词汇或您在屏幕上看到的词语。例如，菜单或对话框中的文字会以**粗体**显示。以下是一个例子：“点击Databricks工作区右上方的**运行全部**按钮，执行所有笔记本单元格，验证所有单元格是否成功执行。”'
- en: Tips or important notes
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 提示或重要说明
- en: Appear like this.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 显示如下。
- en: Get in touch
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与我们联系
- en: Feedback from our readers is always welcome.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终欢迎读者的反馈。
- en: '**General feedback** : If you have questions about any aspect of this book,
    email us at [customercare@packtpub.com](mailto:customercare@packtpub.com) and
    mention the book title in the subject of your message.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**常见反馈**：如果您对本书的任何内容有疑问，请通过电子邮件联系我们：[customercare@packtpub.com](mailto:customercare@packtpub.com)，并在邮件主题中注明书名。'
- en: '**Errata** : Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)
    and fill in the form.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**勘误**：虽然我们已经尽力确保内容的准确性，但错误还是可能发生。如果您在本书中发现了错误，我们将非常感激您向我们报告。请访问[www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)并填写表格。'
- en: '**Piracy** : If you come across any illegal copies of our works in any form
    on the internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at [copyright@packtpub.com](mailto:copyright@packtpub.com)
    with a link to the material.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**盗版**：如果您在互联网上发现我们作品的任何非法副本，我们将非常感激您提供该位置地址或网站名称。请通过[版权@packtpub.com](mailto:copyright@packtpub.com)与我们联系，附上相关链接。'
- en: '**If you are interested in becoming an author** : If there is a topic that
    you have expertise in and you are interested in either writing or contributing
    to a book, please visit [authors.packtpub.com](http://authors.packtpub.com) .'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果您有兴趣成为作者**：如果您在某个领域拥有专业知识，并且有兴趣撰写或参与编写书籍，请访问[authors.packtpub.com](http://authors.packtpub.com)。'
- en: Share Your Thoughts
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享您的想法
- en: Once you’ve read *Building Modern Data Applications Using Databricks Lakehouse*
    , we’d love to hear your thoughts! Please [click here to go straight to the Amazon
    review page](https://packt.link/r/1-801-07323-6) for this book and share your
    feedback.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完*《使用Databricks Lakehouse构建现代数据应用》*后，我们非常期待听到您的想法！请[点击这里直接访问亚马逊的书评页面](https://packt.link/r/1-801-07323-6)，并分享您的反馈。
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您的反馈对我们和技术社区非常重要，它将帮助我们确保提供优质的内容。
- en: Download a free PDF copy of this book
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载本书的免费PDF副本
- en: Thanks for purchasing this book!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您购买本书！
- en: Do you like to read on the go but are unable to carry your print books everywhere?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢随时随地阅读，但又无法随身携带纸质书籍吗？
- en: Is your eBook purchase not compatible with the device of your choice?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您的电子书购买无法与您选择的设备兼容吗？
- en: Don’t worry, now with every Packt book you get a DRM-free PDF version of that
    book at no cost.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不用担心，现在每本Packt书籍都会提供免费的无DRM版本PDF。
- en: Read anywhere, any place, on any device. Search, copy, and paste code from your
    favorite technical books directly into your application.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 任何地方、任何设备上都能阅读。可以搜索、复制并粘贴您最喜欢的技术书籍中的代码，直接用于您的应用程序。
- en: The perks don’t stop there, you can get exclusive access to discounts, newsletters,
    and great free content in your inbox daily
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 优惠不止于此，您还可以独享折扣、新闻通讯和每日发送到您邮箱的优质免费内容
- en: 'Follow these simple steps to get the benefits:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这些简单步骤获取福利：
- en: Scan the QR code or visit the link below
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扫描二维码或访问以下链接
- en: '![img](img/B22011_QR_Free_PDF.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![img](img/B22011_QR_Free_PDF.jpg)'
- en: '[https://packt.link/free-ebook/978-1-80107-323-3](https://packt.link/free-ebook/978-1-80107-323-3)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/free-ebook/978-1-80107-323-3](https://packt.link/free-ebook/978-1-80107-323-3)'
- en: Submit your proof of purchase
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提交您的购买凭证
- en: That’s it! We’ll send your free PDF and other benefits to your email directly
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就是这样！我们会将您的免费PDF和其他福利直接发送到您的邮箱
- en: Part 1:Near-Real-Time Data Pipelines for the Lakehouse
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一部分：湖仓的近实时数据管道
- en: In this first part of the book, we’ll introduce the core concepts of the **Delta
    Live Tables** ( **DLT** ) framework. We’ll cover how to ingest data from a variety
    of input sources and apply the latest changes to downstream tables. We’ll also
    explore how to enforce requirements on incoming data so that your data teams can
    be alerted of potential data quality issues that might contaminate your l akehouse.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的第一部分将介绍**Delta Live Tables**（**DLT**）框架的核心概念。我们将讨论如何从各种输入源中获取数据，并将最新的更改应用到下游表格中。我们还将探讨如何对传入的数据施加要求，以便在可能污染湖仓的数据质量问题发生时，及时通知数据团队。
- en: 'This part contains the following chapters:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 1*](B22011_01.xhtml#_idTextAnchor014) , *An Introduction to Delta
    Live Tables*'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第1章*](B22011_01.xhtml#_idTextAnchor014) , *Delta Live Tables简介*'
- en: '[*Chapter 2*](B22011_02.xhtml#_idTextAnchor052) , *Applying Data Transformations
    Using Delta Live Tables*'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第2章*](B22011_02.xhtml#_idTextAnchor052) , *使用Delta Live Tables应用数据转换*'
- en: '[*Chapter 3*](B22011_03.xhtml#_idTextAnchor079) , *Managing Data Quality Using
    Delta Live Tables*'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第3章*](B22011_03.xhtml#_idTextAnchor079) , *使用Delta Live Tables管理数据质量*'
- en: '[*Chapter 4*](B22011_04.xhtml#_idTextAnchor103) , *Scaling DLT Pipelines*'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B22011_04.xhtml#_idTextAnchor103) , *扩展DLT数据管道*'
