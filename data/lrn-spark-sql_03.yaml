- en: Using Spark SQL for Data Exploration
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL进行数据探索
- en: In this chapter, we will introduce you to using Spark SQL for exploratory data
    analysis. We will introduce preliminary techniques to compute some basic statistics,
    identify outliers, and visualize, sample, and pivot data. A series of hands-on
    exercises in this chapter will enable you to use Spark SQL along with tools such
    as Apache Zeppelin for developing an intuition about your data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何使用Spark SQL进行探索性数据分析。我们将介绍计算一些基本统计数据、识别异常值和可视化、抽样和透视数据的初步技术。本章中的一系列实践练习将使您能够使用Spark
    SQL以及Apache Zeppelin等工具来开发对数据的直觉。
- en: 'In this chapter, we shall look at the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: What is Exploratory Data Analysis (EDA)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是探索性数据分析（EDA）
- en: Why is EDA important?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么EDA很重要？
- en: Using Spark SQL for basic data analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark SQL进行基本数据分析
- en: Visualizing data with Apache Zeppelin
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Zeppelin可视化数据
- en: Sampling data with Spark SQL APIs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark SQL API对数据进行抽样
- en: Using Spark SQL for creating pivot tables
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark SQL创建透视表
- en: Introducing Exploratory Data Analysis (EDA)
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入探索性数据分析（EDA）
- en: '**Exploratory Data Analysis** (**EDA**), or **Initial Data Analysis** (**IDA**),
    is an approach to data analysis that attempts to maximize insight into data. This
    includes assessing the quality and structure of the data, calculating summary
    or descriptive statistics, and plotting appropriate graphs. It can uncover underlying
    structures and suggest how the data should be modeled. Furthermore, EDA helps
    us detect outliers, errors, and anomalies in our data, and deciding what to do
    about such data is often more important than other, more sophisticated analysis.
    EDA enables us to test our underlying assumptions, discover clusters and other
    patterns in our data, and identify the possible relationships between various
    variables. A careful EDA process is vital to understanding the data and is sometimes
    sufficient to reveal such poor data quality that using a more sophisticated model-based
    analysis is not justified.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）或初始数据分析（IDA）是一种试图最大程度地洞察数据的数据分析方法。这包括评估数据的质量和结构，计算摘要或描述性统计数据，并绘制适当的图表。它可以揭示潜在的结构，并建议如何对数据进行建模。此外，EDA帮助我们检测数据中的异常值、错误和异常，并决定如何处理这些数据通常比其他更复杂的分析更重要。EDA使我们能够测试我们的基本假设，发现数据中的聚类和其他模式，并确定各种变量之间可能的关系。仔细的EDA过程对于理解数据至关重要，有时足以揭示数据质量差劣，以至于使用基于模型的更复杂分析是不合理的。
- en: Typically, the graphical techniques used in EDA are simple, consisting of plotting
    the raw data and simple statistics. The focus is on the structures and models
    revealed by the data or best fit the data. EDA techniques include scatter plots,
    box plots, histograms, probability plots, and so on. In most EDA techniques, we
    use all of the data, without making any underlying assumptions. The analyst builds
    intuition, or gets a "feel", for the Dataset as a result of such exploration.
    More specifically, the graphical techniques allow us to efficiently select and
    validate appropriate models, test our assumptions, identify relationships, select
    estimators, detect outliers, and so on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 典型情况下，探索性数据分析（EDA）中使用的图形技术是简单的，包括绘制原始数据和简单的统计。重点是数据所揭示的结构和模型，或者最适合数据的模型。EDA技术包括散点图、箱线图、直方图、概率图等。在大多数EDA技术中，我们使用所有数据，而不做任何基本假设。分析师通过这种探索建立直觉或获得对数据集的“感觉”。更具体地说，图形技术使我们能够有效地选择和验证适当的模型，测试我们的假设，识别关系，选择估计量，检测异常值等。
- en: EDA involves a lot of trial and error, and several iterations. The best way
    is to start simple and then build in complexity as you go along. There is a major
    trade-off in modeling between the simple and the more accurate ones. Simple models
    may be much easier to interpret and understand. These models can get you to 90%
    accuracy very quickly, versus a more complex model that might take weeks or months
    to get you an additional 2% improvement. For example, you should plot simple histograms
    and scatter plots to quickly start developing an intuition for your data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: EDA涉及大量的试错和多次迭代。最好的方法是从简单开始，然后随着进展逐渐增加复杂性。在建模中存在着简单和更准确之间的重大折衷。简单模型可能更容易解释和理解。这些模型可以让您很快达到90%的准确性，而更复杂的模型可能需要几周甚至几个月才能让您获得额外的2%的改进。例如，您应该绘制简单的直方图和散点图，以快速开始对数据进行直觉开发。
- en: Using Spark SQL for basic data analysis
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL进行基本数据分析
- en: Interactively, processing and visualizing large data is challenging as the queries
    can take a long time to execute and the visual interface cannot accommodate as
    many pixels as data points. Spark supports in-memory computations and a high degree
    of parallelism to achieve interactivity with large distributed data. In addition,
    Spark is capable of handling petabytes of data and provides a set of versatile
    programming interfaces and libraries. These include SQL, Scala, Python, Java and
    R APIs, and libraries for distributed statistics and machine learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式地处理和可视化大型数据是具有挑战性的，因为查询可能需要很长时间才能执行，而可视化界面无法容纳与数据点一样多的像素。Spark支持内存计算和高度的并行性，以实现与大规模分布式数据的交互性。此外，Spark能够处理百万亿字节的数据，并提供一组多功能的编程接口和库。这些包括SQL、Scala、Python、Java和R
    API，以及用于分布式统计和机器学习的库。
- en: For data that fits into a single computer, there are many good tools available,
    such as R, MATLAB, and others. However, if the data does not fit into a single
    machine, or if it is very complicated to get the data to that machine, or if a
    single computer cannot easily process the data, then this section will offer some
    good tools and techniques for data exploration.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于适合放入单台计算机的数据，有许多好的工具可用，如R、MATLAB等。然而，如果数据不适合放入单台计算机，或者将数据传输到该计算机非常复杂，或者单台计算机无法轻松处理数据，那么本节将提供一些用于数据探索的好工具和技术。
- en: In this section, we will go through some basic data exploration exercises to
    understand a sample Dataset. We will use a Dataset that contains data related
    to direct marketing campaigns (phone calls) of a Portuguese banking institution.
    The marketing campaigns were based on phone calls to customers. We'll use the
    `bank-additional-full.csv` file that contains 41,188 records and 20 input fields,
    ordered by date (from May 2008 to November 2010). The Dataset has been contributed
    by S. Moro, P. Cortez, and P. Rita, and can be downloaded from [https://archive.ics.uci.edu/ml/datasets/Bank+Marketing](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进行一些基本的数据探索练习，以了解一个样本数据集。我们将使用一个包含与葡萄牙银行机构的直接营销活动（电话营销）相关数据的数据集。这些营销活动是基于对客户的电话呼叫。我们将使用包含41,188条记录和20个输入字段的`bank-additional-full.csv`文件，按日期排序（从2008年5月到2010年11月）。该数据集由S.
    Moro、P. Cortez和P. Rita贡献，并可从[https://archive.ics.uci.edu/ml/datasets/Bank+Marketing](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)下载。
- en: 'As a first step, let''s define a schema and read in the CSV file to create
    a DataFrame. You can use `:paste` command to paste initial set of statements in
    your Spark shell session (use *Ctrl*+*D* to exit the paste mode), as shown:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们定义一个模式并读取CSV文件以创建一个数据框架。您可以使用`:paste`命令将初始一组语句粘贴到您的Spark shell会话中（使用*Ctrl*+*D*退出粘贴模式），如下所示：
- en: '![](img/00048.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00048.jpeg)'
- en: 'After the DataFrame has been created, we first verify the number of records:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了数据框架之后，我们首先验证记录的数量：
- en: '![](img/00049.jpeg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00049.jpeg)'
- en: 'We can also define a `case` class called `Call` for our input records, and
    then create a strongly-typed Dataset, as follows:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以为我们的输入记录定义一个名为`Call`的`case`类，然后创建一个强类型的数据集，如下所示：
- en: '![](img/00050.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00050.jpeg)'
- en: In the next section, we will begin our data exploration by identifying missing
    data in our Dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将通过识别数据集中的缺失数据来开始我们的数据探索。
- en: Identifying missing data
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别缺失数据
- en: Missing data can occur in Datasets due to reasons ranging from negligence to
    a refusal on the part of respondants to provide a specific data point. However,
    in all cases, missing data is a common occurrence in real-world Datasets. Missing
    data can create problems in data analysis and sometimes lead to wrong decisions
    or conclusions. Hence, it is very important to identify missing data and devise
    effective strategies to deal with it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的缺失数据可能是由于从疏忽到受访者拒绝提供特定数据点的原因而导致的。然而，在所有情况下，缺失数据都是真实世界数据集中的常见现象。缺失数据可能会在数据分析中造成问题，有时会导致错误的决策或结论。因此，识别缺失数据并制定有效的处理策略非常重要。
- en: In this section, we analyze the numbers of records with missing data fields
    in our sample Dataset. In order to simulate missing data, we will edit our sample
    Dataset by replacing fields containing "unknown" values with empty strings.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了样本数据集中具有缺失数据字段的记录数量。为了模拟缺失数据，我们将编辑我们的样本数据集，将包含“unknown”值的字段替换为空字符串。
- en: 'First, we created a DataFrame/Dataset from our edited file, as shown:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从我们编辑的文件中创建了一个数据框架/数据集，如下所示：
- en: '![](img/00051.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00051.jpeg)'
- en: 'The following two statements give us a count of rows with certain fields having
    missing data:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两个语句给出了具有某些字段缺失数据的行数：
- en: '![](img/00052.gif)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00052.gif)'
- en: In [Chapter 4](part0057.html#1MBG20-e9cbc07f866e437b8aa14e841622275c), *Using
    Spark SQL for Data Munging*, we will look at effective ways of dealing with missing
    data. In the next section, we will compute some basic statistics for our sample
    Dataset to improve our understanding of the data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](part0057.html#1MBG20-e9cbc07f866e437b8aa14e841622275c)中，*使用Spark SQL进行数据整理*，我们将探讨处理缺失数据的有效方法。在下一节中，我们将计算样本数据集的一些基本统计数据，以改善我们对数据的理解。
- en: Computing basic statistics
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算基本统计数据
- en: 'Computing basic statistics is essential for a good preliminary understanding
    of our data. First, for convenience, we create a case class and a Dataset containing
    a subset of fields from our original DataFrame. In the following example, we choose
    some of the numeric fields and the outcome field, that is, the "term deposit subscribed"
    field:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 计算基本统计数据对于对我们的数据有一个良好的初步了解是至关重要的。首先，为了方便起见，我们创建了一个案例类和一个数据集，其中包含来自我们原始数据框架的一部分字段。在以下示例中，我们选择了一些数值字段和结果字段，即“订阅定期存款”的字段：
- en: '![](img/00053.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00053.jpeg)'
- en: Next, we use `describe()` compute the `count`, `mean`, `stdev`, `min`, and `max`
    values for the numeric columns in our Dataset. The `describe()` command gives
    a way to do a quick sense-check on your data. For example, the counts of rows
    of each of the columns selected matches the total number records in the DataFrame
    (no null or invalid rows),whether the average and range of values for the age
    column matching your expectations, and so on. Based on the values of the means
    and standard deviations, you can get select certain data elements for deeper analysis.
    For example, assuming normal distribution, the mean and standard deviation values
    for age suggest most values of age are in the range 30 to 50 years, for other
    columns the standard deviation values may be indicative of a skew in the data
    (as the standard deviation is greater than the mean).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`describe()`计算数据集中数值列的`count`、`mean`、`stdev`、`min`和`max`值。`describe()`命令提供了一种快速检查数据的方法。例如，所选列的行数与数据框架中的总记录数匹配（没有空值或无效行），年龄列的平均值和值范围是否符合您的预期等。根据平均值和标准差的值，您可以选择某些数据元素进行更深入的分析。例如，假设正态分布，年龄的平均值和标准差值表明大多数年龄值在30到50岁的范围内，对于其他列，标准差值可能表明数据的偏斜（因为标准差大于平均值）。
- en: '![](img/00054.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00054.jpeg)'
- en: Further, we can use the stat package to compute additional statistics such as
    covariance and Pearson's correlation coefficient. The covariance indicates the
    joint variability of two random variables. As we are in the EDA phase, these measures
    can give us indicators of how one variable varies vis-a-vis another one. For example,
    the sign of the covariance indicates the direction of variability between the
    two variables. In the following example, the covariance between age and duration
    of the last contact move in opposite directions, that is as age increases the
    duration decreases. Correlation gives a magnitude for the strength of this relationship
    between these two variables.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以使用stat包计算额外的统计数据，如协方差和Pearson相关系数。协方差表示两个随机变量的联合变异性。由于我们处于EDA阶段，这些测量可以为我们提供有关一个变量如何相对于另一个变量变化的指标。例如，协方差的符号表示两个变量之间变异性的方向。在以下示例中，年龄和最后一次联系的持续时间之间的协方差方向相反，即随着年龄的增加，持续时间减少。相关性给出了这两个变量之间关系强度的大小。
- en: '![](img/00055.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00055.jpeg)'
- en: We can create cross tabulations or crosstabs between two variables to evaluate
    interrelationships between them. For example, in the following example we create
    a crosstab between age and marital status represented as a 2x2 contingency table.
    From the table, we understand, for a given age the breakup of the total number
    of individuals across the various marital statuses. We can also extract items
    that occur most frequently in data columns of the DataFrame. Here, we choose the
    education level as the column and specify a support level of `0.3`, that is, we
    want to find education levels that occur with a frequency greater than `0.3` (observed
    30% of the time, at a minimum) in the DataFrame. Lastly, we can also compute approximate
    quantiles of the numeric columns in the DataFrame. Here, we compute the same for
    the age column with specified quantile probabilities of `0.25`, `0.5` and `0.75`,
    (a value of `0` is the minimum, `1` is the maximum, and `0.5` is the median).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建两个变量之间的交叉表或交叉表，以评估它们之间的相互关系。例如，在以下示例中，我们创建了一个代表2x2列联表的年龄和婚姻状况的交叉表。从表中，我们了解到，对于给定年龄，各种婚姻状况下的个体总数的分布情况。我们还可以提取数据DataFrame列中最频繁出现的项目。在这里，我们选择教育水平作为列，并指定支持水平为`0.3`，即我们希望在DataFrame中找到出现频率大于`0.3`（至少观察到30%的时间）的教育水平。最后，我们还可以计算DataFrame中数值列的近似分位数。在这里，我们计算年龄列的分位数概率为`0.25`、`0.5`和`0.75`（值为`0`是最小值，`1`是最大值，`0.5`是中位数）。
- en: '![](img/00056.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00056.jpeg)'
- en: 'Next, we use the typed aggregation functions to summarize our data to understand
    it better. In the following statement, we aggregate the results by whether a term
    deposit was subscribed along with the total customers contacted, average number
    of calls made per customer, the average duration of the calls, and the average
    number of previous calls made to such customers. The results are rounded to two
    decimal points:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用聚合函数对我们的数据进行汇总，以更好地了解它。在以下语句中，我们按是否订阅定期存款以及联系的客户总数、每位客户平均拨打电话次数、通话平均持续时间和向这些客户拨打的平均上次电话次数进行聚合。结果四舍五入到小数点后两位：
- en: '![](img/00057.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00057.jpeg)'
- en: 'Similarly, executing the following statement gives similar results by customer''s
    age:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，执行以下语句会按客户年龄给出类似的结果：
- en: '![](img/00058.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00058.jpeg)'
- en: After getting a better understanding of our data by computing basic statistics,
    we shift our focus to identifying outliers in our data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过计算基本统计数据更好地了解我们的数据之后，我们将重点转向识别数据中的异常值。
- en: Identifying data outliers
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别数据异常值
- en: An outlier or an anomaly is an observation of the data that deviates significantly
    from other observations in the Dataset. These erroneous outliers can be due to
    errors in the data-collection or variability in measurement. They can impact the
    results significantly so it is imperative to identify them during the EDA process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值或异常值是数据中明显偏离数据集中其他观察值的观察值。这些错误的异常值可能是由于数据收集中的错误或测量的变异性。它们可能会对结果产生重大影响，因此在EDA过程中识别它们至关重要。
- en: However, these techniques define outliers as points, which do not lie in clusters.
    The user has to model the data points using statistical distributions, and the
    outliers are identified depending on how they appear in relation to the underlying
    model. The main problem with these approaches is that during EDA, the user typically
    does not have enough knowledge about the underlying data distribution.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些技术将异常值定义为不属于簇的点。用户必须使用统计分布对数据点进行建模，并根据它们在与基础模型的关系中的出现方式来识别异常值。这些方法的主要问题是在EDA过程中，用户通常对基础数据分布没有足够的了解。
- en: 'EDA, using a modeling and visualizing approach, is a good way of achieving
    a deeper intuition of our data. Spark MLlib supports a large (and growing) set
    of distributed machine learning algorithms to make this task simpler.  For example,
    we can apply clustering algorithms and visualize the results to detect outliers
    in a combination columns. In the following example, we use the last contact duration,
    in seconds (duration), number of contacts performed during this campaign, for
    this client (campaign), number of days that have passed by after the client was
    last contacted from a previous campaign (pdays) and the previous: number of contacts
    performed before this campaign and for this client (prev) values to compute two
    clusters in our data by applying the k-means clustering algorithm:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用建模和可视化方法进行探索性数据分析（EDA）是获得对数据更深刻理解的好方法。Spark MLlib支持大量（并不断增加）的分布式机器学习算法，使这项任务变得更简单。例如，我们可以应用聚类算法并可视化结果，以检测组合列中的异常值。在以下示例中，我们使用最后一次联系持续时间（以秒为单位）、在此客户（campaign）的此次活动期间执行的联系次数、在上一次活动期间客户最后一次联系后经过的天数（pdays）和在此客户（prev）的此次活动之前执行的联系次数来应用k均值聚类算法在我们的数据中计算两个簇：
- en: '![](img/00059.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00059.jpeg)'
- en: Other distributed algorithms useful for EDA include classification, regression,
    dimensionality reduction, correlation, and hypothesis testing. More details on
    using Spark SQL and these algorithms are covered in [Chapter 6](part0103.html#3279U0-e9cbc07f866e437b8aa14e841622275c),
    *Using Spark SQL in Machine Learning Applications.*
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 用于探索性数据分析的其他分布式算法包括分类、回归、降维、相关性和假设检验。有关使用Spark SQL和这些算法的更多细节，请参阅[第6章](part0103.html#3279U0-e9cbc07f866e437b8aa14e841622275c)中的*在机器学习应用中使用Spark
    SQL*。
- en: Visualizing data with Apache Zeppelin
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Zeppelin可视化数据
- en: Typically, we will generate many graphs to verify our hunches about the data.
    A lot of these quick and dirty graphs used during EDA are, ultimately, discarded.
    Exploratory data visualization is critical for data analysis and modeling. However,
    we often skip exploratory visualization with large data because it is hard. For
    instance, browsers cannot typically cannot handle millions of data points. Hence,
    we have to summarize, sample, or model our data before we can effectively visualize
    it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会生成许多图表来验证我们对数据的直觉。在探索性数据分析期间使用的许多快速而肮脏的图表最终被丢弃。探索性数据可视化对于数据分析和建模至关重要。然而，我们经常因为难以处理而跳过大数据的探索性可视化。例如，浏览器通常无法处理数百万个数据点。因此，我们必须在有效可视化数据之前对数据进行总结、抽样或建模。
- en: Traditionally, BI tools provided extensive aggregation and pivoting features
    to visualize the data. However, these tools typically used nightly jobs to summarize
    large volumes of data. The summarized data was subsequently downloaded and visualized
    on the practitioner's workstations. Spark can eliminate many of these batch jobs
    to support interactive data visualization.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，BI工具提供了广泛的聚合和透视功能来可视化数据。然而，这些工具通常使用夜间作业来总结大量数据。随后，总结的数据被下载并在从业者的工作站上可视化。Spark可以消除许多这些批处理作业，以支持交互式数据可视化。
- en: 'In this section, we will explore some basic data visualization techniques using
    Apache Zeppelin. Apache Zeppelin is a web-based tool that supports interactive
    data analysis and visualization. It supports several language interpreters and
    comes with built-in Spark integration. Hence, it is quick and easy to get started
    with exploratory data analysis using Apache Zeppelin:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Apache Zeppelin探索一些基本的数据可视化技术。Apache Zeppelin是一个支持交互式数据分析和可视化的基于Web的工具。它支持多种语言解释器，并具有内置的Spark集成。因此，使用Apache
    Zeppelin进行探索性数据分析是快速而简单的：
- en: 'You can download Appache Zeppelin from [https://zeppelin.apache.org/](https://zeppelin.apache.org/).
    Unzip the package on your hard drive and start Zeppelin using the following command:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以从[https://zeppelin.apache.org/](https://zeppelin.apache.org/)下载Appache Zeppelin。在硬盘上解压缩软件包，并使用以下命令启动Zeppelin：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You should see the following message:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该看到以下消息：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You should be able to see the Zeppelin home page at: `http://localhost:8080/`:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该能够在`http://localhost:8080/`看到Zeppelin主页：
- en: '![](img/00060.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00060.jpeg)'
- en: Click on the Create new note link and specify a path and name for your notebook,
    as shown:![](img/00061.jpeg)
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击“创建新笔记”链接，并指定笔记本的路径和名称，如下所示：![](img/00061.jpeg)
- en: 'In the next step, we paste the same code as in the beginning of this chapter
    to create a DataFrame for our sample Dataset:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，我们将粘贴本章开头的相同代码，以创建我们样本数据集的DataFrame：
- en: '![](img/00062.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00062.jpeg)'
- en: 'We can execute typical DataFrame operations, as follows:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以执行典型的DataFrame操作，如下所示：
- en: '![](img/00063.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00063.jpeg)'
- en: 'Next, we create a table from our DataFrame and execute some SQL on it. The
    results of the SQL statements'' execution can be charted by clicking on the appropriate
    chart-type required. Here, we create bar charts as an illustrative example of
    summarizing and visualizing data:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们从DataFrame创建一个表，并对其执行一些SQL。单击所需的图表类型，可以对SQL语句的执行结果进行图表化。在这里，我们创建条形图，作为总结和可视化数据的示例：
- en: '![](img/00064.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00064.jpeg)'
- en: 'We can create a scatter plot, as shown in the following figure:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以创建散点图，如下图所示：
- en: '![](img/00065.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00065.jpeg)'
- en: 'You can also read the coordinate values of each of the points plotted:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以读取每个绘制点的坐标值：
- en: '![](img/00066.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00066.jpeg)'
- en: 'Additionally, we can create a textbox that accepts input values to make the
    experience interactive. In the following figure, we create a textbox that can
    accept different values for the age parameter and the bar chart is updated accordingly:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们可以创建一个接受输入值的文本框，使体验更加交互式。在下图中，我们创建了一个文本框，可以接受不同的年龄参数值，并相应地更新条形图：
- en: '![](img/00067.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00067.jpeg)'
- en: 'Similarly, we can also create drop-down lists where the user can select the
    appropriate option:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，我们还可以创建下拉列表，用户可以选择适当的选项：
- en: '![](img/00068.jpeg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00068.jpeg)'
- en: 'And, the table of values or chart automatically gets updated:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表格的值或图表会自动更新：
- en: '![](img/00069.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00069.jpeg)'
- en: We will explore more advanced visualizations using Spark SQL and SparkR in [Chapter
    8](part0149.html#4E33Q0-e9cbc07f866e437b8aa14e841622275c), *Using Spark SQL with
    SparkR**.* In the next section, we will explore the methods used to generate samples
    from our data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第8章](part0149.html#4E33Q0-e9cbc07f866e437b8aa14e841622275c)中使用Spark SQL和SparkR进行更高级的可视化。在下一节中，我们将探讨用于从数据中生成样本的方法。
- en: Sampling data with Spark SQL APIs
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL API对数据进行抽样
- en: Often, we need to visualize individual data points to understand the nature
    of our data. Statisticians use sampling techniques extensively for data analysis.
    Spark supports both approximate and exact sample generation. Approximate sampling
    is faster and is often good enough in most cases.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们需要可视化个别数据点以了解我们数据的性质。统计学家广泛使用抽样技术进行数据分析。Spark支持近似和精确的样本生成。近似抽样速度更快，在大多数情况下通常足够好。
- en: In this section, we will explore Spark SQL APIs used for generating samples.
    We will work through some examples of generating approximate and exact stratified
    samples, with and without replacement, using the DataFrame/Dataset API and RDD-based
    methods.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索用于生成样本的Spark SQL API。我们将通过一些示例来演示使用DataFrame/Dataset API和基于RDD的方法生成近似和精确的分层样本，有放回和无放回。
- en: Sampling with the DataFrame/Dataset API
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DataFrame/Dataset API进行抽样
- en: We can use the `sampleBy` to create a stratified sample without replacement.
    We can specify the fractions for the percentages of each value to be selected
    in the sample.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`sampleBy`创建一个无放回的分层样本。我们可以指定每个值被选入样本的百分比。
- en: 'The size of the sample and the number of record of each type are shown here:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 样本的大小和每种类型的记录数如下所示：
- en: '![](img/00070.jpeg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00070.jpeg)'
- en: 'Next, we create a sample with replacement that selects a fraction of rows (10%
    of the total records) using a random seed. Using `sample`  is not guaranteed to
    provide the exact fraction of the total number of records in the Dataset. We also
    print out the numbers of each type of records in the sample:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个有放回的样本，选择总记录的一部分（总记录的10%），并使用随机种子。使用`sample`不能保证提供数据集中总记录数的确切分数。我们还打印出样本中每种类型的记录数：
- en: '![](img/00071.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00071.jpeg)'
- en: In the next section, we will explore sampling methods using RDDs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探索使用RDD的抽样方法。
- en: Sampling with the RDD API
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RDD API进行抽样
- en: In this section, we use RDDs for creating stratified samples with and without
    replacement.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用RDD来创建有放回和无放回的分层样本。
- en: 'First, we create an RDD from our DataFrame:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从DataFrame创建一个RDD：
- en: '![](img/00072.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00072.jpeg)'
- en: 'We can specify the fractions of each record-type in our sample, as illustrated:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以指定样本中每种记录类型的分数，如图所示：
- en: '![](img/00073.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00073.jpeg)'
- en: 'In the following illustration, we use the `sampleByKey` and `sampleByKeyExact`
    methods to create our samples. The former is an approximate sample while the latter
    is an exact sample. The first parameter specifies whether the sample is generated
    with or without replacement:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们使用`sampleByKey`和`sampleByKeyExact`方法来创建我们的样本。前者是一个近似样本，而后者是一个精确样本。第一个参数指定样本是有放回还是无放回生成的：
- en: '![](img/00074.jpeg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00074.jpeg)'
- en: 'Next, we print out the total number of records in the population and in each
    of the samples. You will notice that the `sampleByKeyExact` gives you exact numbers
    of records as per the specified fractions:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们打印出人口总记录数和每个样本中的记录数。您会注意到`sampleByKeyExact`会给出与指定分数完全相符的记录数：
- en: '![](img/00075.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00075.jpeg)'
- en: 'The sample method can be used to create a random sample containing the specified
    fraction of records in the sample. Next, we create a sample with replacement,
    containing 10% of the total records:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: sample方法可用于创建包含指定记录分数的随机样本。接下来，我们创建一个有放回的样本，包含总记录的10%：
- en: '![](img/00076.jpeg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00076.jpeg)'
- en: Other statistical operations, such as hypothesis testing, random data generation,
    visualizing probability distributions, and so on, will be covered in the later
    chapters. In the next section, we will explore our data using Spark SQL for creating
    pivot tables.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其他统计操作，如假设检验、随机数据生成、可视化概率分布等，将在后面的章节中介绍。在下一节中，我们将使用Spark SQL来创建数据透视表来探索我们的数据。
- en: Using Spark SQL for creating pivot tables
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL创建数据透视表
- en: 'Pivot tables create alternate views of your data and are commonly used during
    data exploration. In the following example, we demonstrate pivoting using Spark
    DataFrames:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 数据透视表创建数据的替代视图，在数据探索过程中通常被使用。在下面的示例中，我们演示了如何使用Spark DataFrames进行数据透视：
- en: '![](img/00077.jpeg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00077.jpeg)'
- en: 'The following example pivots on housing loan taken and computes the numbers
    by marital status:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例在已经采取的住房贷款上进行数据透视，并按婚姻状况计算数字：
- en: '![](img/00078.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00078.jpeg)'
- en: 'In the next example, we create a DataFrame with appropriate column names for
    the total and average number of calls:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们创建一个DataFrame，其中包含适当的列名，用于呼叫总数和平均呼叫次数：
- en: '![](img/00079.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00079.jpeg)'
- en: 'In the following example, we create a DataFrame with appropriate column names
    for the total and average duration of calls for each job category:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们创建一个DataFrame，其中包含适当的列名，用于每个工作类别的呼叫总数和平均持续时间：
- en: '![](img/00080.jpeg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00080.jpeg)'
- en: 'In the following example, we show pivoting to compute average call duration
    for each job category, while also specifying a subset of marital status:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们展示了数据透视，计算每个工作类别的平均呼叫持续时间，同时指定了一些婚姻状况的子集：
- en: '![](img/00081.jpeg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00081.jpeg)'
- en: 'The following example is the same as the preceding one, except that we split
    the average call duration values by the housing loan field as well in this case:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例与前一个相同，只是在这种情况下，我们还按住房贷款字段拆分了平均呼叫持续时间值：
- en: '![](img/00082.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00082.jpeg)'
- en: 'Next, we show how you can create a DataFrame of pivot table of term deposits
    subscribed by month, save it to disk, and read it back into a RDD:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将展示如何创建一个按月订阅的定期存款数据透视表的DataFrame，将其保存到磁盘，并将其读取回RDD：
- en: '![](img/00083.jpeg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00083.jpeg)'
- en: 'Further, we use the RDD in the preceding step to compute quarterly totals of
    customers who subscribed and did not subscribe to term loans:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们使用前面步骤中的RDD来计算订阅和未订阅定期贷款的季度总数：
- en: '![](img/00084.jpeg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00084.jpeg)'
- en: We will introduce a detailed analysis of other types of data, including streaming
    data, large-scale graphs, time-series data, and so on, later in this book.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的后面介绍其他类型数据的详细分析，包括流数据、大规模图形、时间序列数据等。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we demonstrated using Spark SQL for exploring Datasets, performing
    basic data quality checks, generating samples and pivot tables, and visualizing
    data with Apache Zeppelin.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们演示了使用Spark SQL来探索数据集，执行基本数据质量检查，生成样本和数据透视表，并使用Apache Zeppelin可视化数据。
- en: In the next chapter, we will shift our focus to data munging/wrangling. We will
    introduce techniques to handle missing data, bad data, duplicate records, and
    so on. We will also use extensive hands-on sessions for demonstrating the use
    of Spark SQL for common data munging tasks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把重点转移到数据处理/整理。我们将介绍处理缺失数据、错误数据、重复记录等技术。我们还将进行大量的实践演练，演示使用Spark SQL处理常见数据整理任务。
