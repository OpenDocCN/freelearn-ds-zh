- en: Powerful Exploratory Data Analysis with MLlib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MLlib进行强大的探索性数据分析
- en: In this chapter, we will explore Spark's capability to perform regression tasks
    with models such as linear regression and **support-vector machines** (**SVMs**).
    We will learn how to compute summary statistics with MLlib, and discover correlations
    in datasets using Pearson and Spearman correlations. We will also test our hypothesis
    on large datasets.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索Spark执行回归任务的能力，使用线性回归和支持向量机等模型。我们将学习如何使用MLlib计算汇总统计，并使用Pearson和Spearman相关性发现数据集中的相关性。我们还将在大型数据集上测试我们的假设。
- en: 'We will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Computing summary statistics with MLlib
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MLlib计算汇总统计
- en: Using the Pearson and Spearman methods to discover correlations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Pearson和Spearman方法发现相关性
- en: Testing our hypotheses on large datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大型数据集上测试我们的假设
- en: Computing summary statistics with MLlib
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MLlib计算汇总统计
- en: 'In this section, we will be answering the following questions:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回答以下问题：
- en: What are summary statistics?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是汇总统计？
- en: How do we use MLlib to create summary statistics?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何使用MLlib创建汇总统计？
- en: MLlib is the machine learning library that comes with Spark. There has been
    a recent new development that allows us to use Spark's data-processing capabilities
    to pipe into machine learning capabilities native to Spark. This means that we
    can use Spark not only to ingest, collect, and transform data, but we can also
    analyze and use it to build machine learning models on the PySpark platform, which
    allows us to have a more seamless deployable solution.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib是随Spark一起提供的机器学习库。最近有一个新的发展，允许我们使用Spark的数据处理能力传输到Spark本身的机器学习能力。这意味着我们不仅可以使用Spark来摄取、收集和转换数据，还可以分析和使用它来构建PySpark平台上的机器学习模型，这使我们能够拥有更无缝的可部署解决方案。
- en: Summary statistics are a very simple concept. We are familiar with average,
    or standard deviation, or the variance of a particular variable. These are summary
    statistics of a dataset. The reason why it's called a summary statistic is that it
    gives you a summary of something via a certain statistic. For example, when we
    talk about the average of a dataset, we're summarizing one characteristic of that
    dataset, and that characteristic is the average.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 汇总统计是一个非常简单的概念。我们熟悉某个变量的平均值、标准差或方差。这些是数据集的汇总统计。之所以称其为汇总统计，是因为它通过某个统计量给出了某个东西的摘要。例如，当我们谈论数据集的平均值时，我们正在总结数据集的一个特征，而这个特征就是平均值。
- en: Let's check how to compute summary statistics in Spark. The key factor here
    is the `colStats` function. The `colStats` function computes the column-wise summary
    statistics for an `rdd` input. The `colStats` function accepts one parameter,
    that is `rdd`, and it allows us to compute different summary statistics using
    Spark.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Spark中计算汇总统计。关键因素在于`colStats`函数。`colStats`函数计算`rdd`输入的逐列汇总统计。`colStats`函数接受一个参数，即`rdd`，并允许我们使用Spark计算不同的汇总统计。
- en: 'Let''s look at the code in the Jupyter Notebook (available at [https://github.com/PacktPublishing/Hands-On-Big-Data-Analytics-with-PySpark/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Big-Data-Analytics-with-PySpark/tree/master/Chapter05))
    for this chapter in `Chapter5.ipynb`. We will first collect the data from the `kddcup.data.gz`
    text file and pipe this into the `raw_data` variable as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下Jupyter Notebook中的代码（可在[https://github.com/PacktPublishing/Hands-On-Big-Data-Analytics-with-PySpark/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Big-Data-Analytics-with-PySpark/tree/master/Chapter05)找到），在`Chapter5.ipynb`中的本章。我们将首先从`kddcup.data.gz`文本文件中收集数据，并将其传输到`raw_data`变量中：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `kddcup.data` file is a **comma-separated value** (**CSV**) file. We have
    to split this data by the `,` character and put it in the `csv` variable as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`kddcup.data`文件是一个逗号分隔值（CSV）文件。我们必须通过`,`字符拆分这些数据，并将其放入`csv`变量中，如下所示：'
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s take the first feature `x[0]` of the data file; this feature represents
    the `duration`, that is, aspects of the data. We will transform it into an integer
    here, and also wrap it in a list as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们取数据文件的第一个特征`x[0]`；这个特征代表`持续时间`，也就是数据的方面。我们将把它转换为整数，并将其包装成列表，如下所示：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This helps us do summary statistics over multiple variables, and not just one
    of them. To activate the `colStats` function, we need to import the `Statistics`
    package, as shown in the following snippet:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于我们对多个变量进行汇总统计，而不仅仅是其中一个。要激活`colStats`函数，我们需要导入`Statistics`包，如下面的代码片段所示：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This `Statistics` package is a sub package of `pyspark.mllib.stat`. Now, we
    need to call the `colStats` function in the `Statistics` package and feed it some
    data. Here, we are talking about the `duration` data from the dataset and we''re
    feeding the summary statistics into the `summary` variable:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`Statistics`包是`pyspark.mllib.stat`的一个子包。现在，我们需要在`Statistics`包中调用`colStats`函数，并向其提供一些数据。这里，我们谈论的是数据集中的`持续时间`数据，并将汇总统计信息输入到`summary`变量中：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To access different summary statistics, such as the mean, standard deviation,
    and so on, we can call the functions of the `summary` objects, and access different
    summary statistics. For example, we can access the `mean`, and since we have only
    one feature in our `duration` dataset, we can index it by the `00` index, and
    we''ll get the mean of the dataset as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问不同的汇总统计，如平均值、标准差等，我们可以调用`summary`对象的函数，并访问不同的汇总统计。例如，我们可以访问`mean`，由于我们的`持续时间`数据集中只有一个特征，我们可以通过`00`索引对其进行索引，然后得到数据集的平均值，如下所示：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will give us the following output:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Similarly, if we import the `sqrt` function from the Python standard library,
    we can create the standard deviation of the durations seen in the datasets, as
    demonstrated in the following code snippet:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果我们从Python标准库中导入`sqrt`函数，我们可以创建数据集中持续时间的标准差，如下面的代码片段所示：
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will give us the following output:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If we don''t index the summary statistics with `[0]`, we can see that `summary.max()`
    and `summary.min()` gives us back an array, of which the first element is the
    summary statistic that we desire, as shown in the following code snippet:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不使用`[0]`对摘要统计信息进行索引，我们可以看到`summary.max()`和`summary.min()`会返回一个数组，其中第一个元素是我们所需的摘要统计信息，如下面的代码片段所示：
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Using Pearson and Spearman correlations to discover correlations
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Pearson和Spearman相关性来发现相关性
- en: In this section, we will look at two different ways of computing correlations
    in your datasets, and these two methods are called Pearson and Spearman correlations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将看到在数据集中计算相关性的两种不同方法，这两种方法分别称为Pearson和Spearman相关性。
- en: The Pearson correlation
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pearson相关性
- en: The Pearson correlation coefficient shows us how two different variables vary
    at the same time, and then adjusts it for how much they vary. This is probably
    one of the most popular ways to compute a correlation if you have a dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Pearson相关系数向我们展示了两个不同变量同时变化的程度，然后根据它们的变化程度进行调整。如果你有一个数据集，这可能是计算相关性最流行的方法之一。
- en: The Spearman correlation
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spearman相关性
- en: Spearman's rank correlation is not the default correlation calculation that
    is built into PySpark, but it is very useful. The Spearman correlation coefficient
    is the Pearson correlation coefficient between the ranked variables. Using different
    ways of looking at correlation gives us more dimensions of understanding on how
    correlation works. Let's look at how we calculate this in PySpark.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spearman秩相关不是内置在PySpark中的默认相关计算，但它非常有用。Spearman相关系数是排名变量之间的Pearson相关系数。使用不同的相关性观察方法可以让我们更全面地理解相关性的工作原理。让我们看看在PySpark中如何计算这个。
- en: Computing Pearson and Spearman correlations
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算Pearson和Spearman相关性
- en: 'To understand this, let''s assume that we are taking the first three numeric
    variables from our dataset. For this, we want to access the `csv` variable that
    we defined previously, where we simply split `raw_data` using a comma (`,`). We
    will consider only the first three columns that are numeric. We will not take
    anything that contains words; we''re only interested in features that are purely
    based on numbers. In our case, in `kddcup.data`, the first feature is indexed
    at `0`; feature 5 and feature 6 are indexed at `4` and `5`, respectively which
    are the numeric variables that we have. We use a `lambda` function to take all
    three of these into a list and put it into the `metrics` variable:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，让我们假设我们正在从数据集中取出前三个数值变量。为此，我们要访问之前定义的`csv`变量，我们只需使用逗号（`,`）分割`raw_data`。我们只考虑前三列是数值的特征。我们不会取包含文字的任何内容；我们只对纯粹基于数字的特征感兴趣。在我们的例子中，在`kddcup.data`中，第一个特征的索引是`0`；特征5和特征6的索引分别是`4`和`5`，这些是我们拥有的数值变量。我们使用`lambda`函数将这三个变量放入一个列表中，并将其放入`metrics`变量中：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will give us the following output:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the *Computing summary statistics with MLlib* section, we simply took the
    first feature into a list and created a list with a length of one. Here, we're
    taking three quantities of three variables into the same list. Now, each list
    has a length of three.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在*使用MLlib计算摘要统计信息*部分，我们只是将第一个特征放入一个列表中，并创建了一个长度为1的列表。在这里，我们将三个变量的三个量放入同一个列表中。现在，每个列表的长度都是三。
- en: To compute the correlations, we call the `corr` method on the `metrics` variable and
    specify the `method` as `"spearman"`. PySpark would give us a very simple matrix
    telling us the correlation between the variables. In our example, the third variable
    in our `metrics` variable is more correlated than the second variable.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算相关性，我们在`metrics`变量上调用`corr`方法，并指定`method`为`"spearman"`。PySpark会给我们一个非常简单的矩阵，告诉我们变量之间的相关性。在我们的例子中，`metrics`变量中的第三个变量比第二个变量更相关。
- en: If we run `corr` on `metrics` again, but specify that the method is `pearson`,
    then it gives us Pearson correlations. So, let's examine why we need to be qualified
    as the data scientist or machine learning researcher to call these two simple
    functions and simply change the value of the second parameter. A lot of machine
    learning and data science revolves around our understanding of statistics, understanding
    how data behaves, an understanding of how machine learning models are grounded,
    and what gives them their predictive power.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次在`metrics`上运行`corr`，但指定方法为`pearson`，那么它会给我们Pearson相关性。因此，让我们看看为什么我们需要有资格称为数据科学家或机器学习研究人员来调用这两个简单的函数，并简单地改变第二个参数的值。许多机器学习和数据科学都围绕着我们对统计学的理解，对数据行为的理解，对机器学习模型基础的理解以及它们的预测能力是如何产生的。
- en: So, as a machine learning practitioner or a data scientist, we simply use PySpark
    as a big calculator. When we use a calculator, we never complain that the calculator
    is simple to use—in fact, it helps us to complete the goal in a more straightforward
    way. It is the same case with PySpark; once we move from the data engineering
    side to the MLlib side, we will notice that the code gets incrementally easier.
    It tries to hide the complexity of the mathematics underneath it, but we need
    to understand the difference between different correlations, and we also need
    to know how and when to use them.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作为一个机器学习从业者或数据科学家，我们只是把PySpark当作一个大型计算器来使用。当我们使用计算器时，我们从不抱怨计算器使用简单——事实上，它帮助我们以更直接的方式完成目标。PySpark也是一样的情况；一旦我们从数据工程转向MLlib，我们会注意到代码变得逐渐更容易。它试图隐藏数学的复杂性，但我们需要理解不同相关性之间的差异，也需要知道如何以及何时使用它们。
- en: Testing our hypotheses on large datasets
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在大型数据集上测试我们的假设
- en: In this section, we will look at hypothesis testing, and also learn how to test
    the hypotheses using PySpark. Let's look at one particular type of hypothesis
    testing that is implemented in PySpark. This form of hypothesis testing is called
    Pearson's chi-square test. Chi-square tests how likely it is that the differences
    in the two datasets are there by chance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究假设检验，并学习如何使用PySpark测试假设。让我们看看PySpark中实现的一种特定类型的假设检验。这种假设检验称为Pearson卡方检验。卡方检验评估了两个数据集之间的差异是由偶然因素引起的可能性有多大。
- en: For example, if we had a retail store without any footfall, and suddenly you
    get footfall, how likely is it that this is random, or is there even any statistically
    significant difference in the level of visitors that we are getting now in comparison
    to before? The reason why this is called the chi-square test is that the test
    itself references the chi-square distributions. You can refer to online documentation
    to understand more about chi-square distributions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个没有任何人流量的零售店，突然之间有了人流量，那么这是随机发生的可能性有多大，或者现在我们得到的访客水平与以前相比是否有任何统计学上显著的差异？之所以称之为卡方检验，是因为测试本身参考了卡方分布。您可以参考在线文档了解更多关于卡方分布的信息。
- en: There are three variations within Pearson's chi-square test. We will check whether
    the observed datasets are distributed differently than in a theoretical dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Pearson的卡方检验有三种变体。我们将检查观察到的数据集是否与理论数据集分布不同。
- en: Let's see how we can implement this. Let's start by importing the `Vectors`
    package from `pyspark.mllib.linalg`. Using this vector, we're going to create
    a dense vector of the visitor frequencies by day in our store.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何实现这一点。让我们从“pyspark.mllib.linalg”中导入“Vectors”包开始。使用这个向量，我们将创建一个存储中每天访客频率的密集向量。
- en: 'Let''s imagine that the frequencies go from `0.13` an hour to `0.61`, `0.8`,
    and `0.5`, finally ending on Friday at `0.3`. So, we are putting these visitor
    frequencies into the `visitors_freq` variable. Since we''re using PySpark, it
    is very simple for us to run a chi-square test from the `Statistics` package,
    which we have already imported as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设访问频率从每小时的0.13到0.61，0.8，0.5，最后在星期五结束时为0.3。因此，我们将这些访客频率放入“visitors_freq”变量中。由于我们使用PySpark，我们可以很容易地从“Statistics”包中运行卡方检验，我们已经导入如下：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'By running the chi-square test, the `visitors_freq` variable gives us a bunch
    of useful information, as demonstrated in the following screenshot:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行卡方检验，“visitors_freq”变量为我们提供了大量有用的信息，如下截图所示：
- en: '![](img/ec0a248d-d599-476c-bbd7-665a504a76bc.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec0a248d-d599-476c-bbd7-665a504a76bc.png)'
- en: The preceding output shows the chi-square test summary. We've used the `pearson`
    method, where there are `4` degrees of freedom in our Pearson chi-square test,
    and the statistics are `0.585`, which means that the `pValue` is `0.964`. This
    results in no presumption against the null hypothesis. In this way, the observed
    data follows the same distribution as expected, which means our visitors are not
    actually different. This gives us a good understanding of hypothesis testing.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出显示了卡方检验的摘要。我们使用了“pearson”方法，在我们的Pearson卡方检验中有4个自由度，统计数据为0.585，这意味着“pValue”为0.964。这导致没有反对零假设的推定。这样，观察到的数据遵循与预期相同的分布，这意味着我们的访客实际上并没有不同。这使我们对假设检验有了很好的理解。
- en: Summary
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned summary statistics and computing the summary statistics
    with MLlib. We also learned about Pearson and Spearman correlations, and how we
    can discover these correlations in our datasets using PySpark. Finally, we learned
    one particular way of performing hypothesis testing, which is called the Pearson
    chi-square test. We then used PySpark's hypothesis-testing functions to test our
    hypotheses on large datasets.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了摘要统计信息并使用MLlib计算摘要统计信息。我们还了解了Pearson和Spearman相关性，以及如何使用PySpark在数据集中发现这些相关性。最后，我们学习了一种特定的假设检验方法，称为Pearson卡方检验。然后，我们使用PySpark的假设检验函数在大型数据集上测试了我们的假设。
- en: In the next chapter, we're going to look at putting the structure on our big
    data with Spark SQL.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何在Spark SQL中处理大数据的结构。
