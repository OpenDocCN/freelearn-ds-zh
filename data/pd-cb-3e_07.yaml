- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Reshaping DataFrames
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重塑DataFrame
- en: Working with data is hard. Rarely, if ever, can you just collect data and have
    it immediately yield insights. Often, significant time and effort must be put
    into cleansing, transforming, and *reshaping* your data to get it into a format
    that is usable, digestible, and/or understandable.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据很困难。很少，甚至从未，有人能够仅仅收集数据就能直接获得洞见。通常，必须投入大量时间和精力进行数据清洗、转换和*重塑*，以便将数据转化为可用、可消化和/或可理解的格式。
- en: Is your source data a collection of CSV files, where each file represents a
    different day? Proper use of `pd.concat` will help you take those files and combine
    them into one with ease.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 您的源数据是多个CSV文件的集合吗？每个文件代表一天的数据？通过正确使用`pd.concat`，您可以轻松地将这些文件合并为一个。
- en: Does the relational database you use as a source store data in a normalized
    form, while the target columnar database would prefer to ingest data all in one
    table? `pd.merge` can help you combine your data together.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您使用的关系型数据库作为数据源是否以规范化形式存储数据，而目标列式数据库更倾向于将所有数据存储在一个表中？`pd.merge`可以帮助您将数据合并在一起。
- en: What if your boss asks you to take millions of rows of data and, from that,
    produce a nice summary report that anyone in the business can understand? `pd.pivot_table`
    is the right tool for the job here, allowing you to quickly and easily summarize
    your data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的老板要求您从数百万行数据中提取并生成一份任何业务人员都能理解的简明报告，该怎么办？`pd.pivot_table`是完成此任务的正确工具，它能够快速、轻松地汇总您的数据。
- en: Ultimately, the reasons why you need to reshape your data come from different
    places. Whether your requirements are driven by systems or people, pandas can
    help you manipulate data as needed.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，您需要重塑数据的原因来自不同的地方。无论是系统还是人们的需求，pandas都可以帮助您按需操作数据。
- en: Throughout this chapter, we will walk you through the functions and methods
    that pandas offers to reshape your data. Equipped with the proper knowledge and
    some creativity, reshaping with pandas can be one of the most fun and rewarding
    parts of your analytical process.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将逐步介绍pandas提供的函数和方法，帮助您重塑数据。掌握正确的知识和一些创造力后，使用pandas重塑数据可以成为您分析过程中的最有趣和最具回报的部分之一。
- en: 'We are going to cover the following recipes in this chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍以下内容：
- en: Concatenating `pd.DataFrame` objects
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接`pd.DataFrame`对象
- en: Merging DataFrames with `pd.merge`
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pd.merge`合并DataFrame
- en: Joining DataFrames with `pd.DataFrame.join`
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pd.DataFrame.join`连接DataFrame
- en: Reshaping with `pd.DataFrame.stack` and `pd.DataFrame.unstack`
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pd.DataFrame.stack`和`pd.DataFrame.unstack`进行重塑
- en: Reshaping with `pd.DataFrame.melt`
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pd.DataFrame.melt`进行重塑
- en: Reshaping with `pd.wide_to_long`
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pd.wide_to_long`进行重塑
- en: Reshaping with `pd.DataFrame.pivot` and `pd.pivot_table`
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pd.DataFrame.pivot`和`pd.pivot_table`进行重塑
- en: Reshaping with `pd.DataFrame.explode`
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pd.DataFrame.explode`进行重塑
- en: Transposing with `pd.DataFrame.T`
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pd.DataFrame.T`进行转置
- en: Concatenating pd.DataFrame objects
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接pd.DataFrame对象
- en: 'The term *concatenation* in pandas refers to the process of taking two or more
    `pd.DataFrame` objects and stacking them in some manner. Most commonly, users
    in pandas perform what we would consider to be *vertical* concatenation, which
    places the `pd.DataFrame` objects on top of one another:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas中，*连接*一词指的是将两个或更多的`pd.DataFrame`对象以某种方式堆叠起来的过程。最常见的，pandas用户通常会进行我们认为的*垂直*连接，即将`pd.DataFrame`对象堆叠在彼此之上：
- en: '![](img/B31091_07_01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_07_01.png)'
- en: 'Figure 7.1: Vertical concatenation of two pd.DataFrame objects'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：两个pd.DataFrame对象的垂直连接
- en: 'However, pandas also has the flexibility to take your `pd.DataFrame` objects
    and stack them side by side, through a process called *horizontal* concatenation:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，pandas还具有灵活性，可以将您的`pd.DataFrame`对象并排堆叠，这个过程称为*水平*连接：
- en: '![A diagram of a table  Description automatically generated](img/B31091_07_02.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的表格图示](img/B31091_07_02.png)'
- en: 'Figure 7.2: Vertical concatenation of two pd.DataFrame objects'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：两个pd.DataFrame对象的垂直连接
- en: These figures may provide you with a good grasp of what concatenation is all
    about, but there are some potential issues to consider. What should happen if
    we try to concatenate vertically, but our column labels are not the same across
    all of the objects? On the flip side, what should happen if we try to concatenate
    horizontally, and not all of the row labels are the same?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表可能会帮助您很好地理解连接的概念，但也有一些潜在问题需要考虑。如果我们尝试进行垂直连接，但各个对象的列标签不相同，应该怎么办？相反，如果我们尝试进行水平连接，而不是所有的行标签都相同，又该如何处理？
- en: Regardless of the direction along which you would like to concatenate, and regardless
    of how your labels may or may not align, concatenation in pandas is controlled
    entirely through the `pd.concat` function. This recipe will walk you through the
    basics of `pd.concat`, while showing you how you can control its behavior when
    you aren’t always working with like-labeled `pd.DataFrame` objects.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您想要沿着哪个方向进行连接，也无论您的标签是否对齐，pandas中的连接完全受`pd.concat`函数的控制。本文将介绍`pd.concat`的基础知识，同时向您展示在处理不像标记的`pd.DataFrame`对象时如何控制其行为。
- en: How to do it
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到
- en: 'Let’s imagine we have collected data about the stock performance of various
    companies across two different quarters. To best showcase how concatenation works,
    we have intentionally made it so that the two `pd.DataFrame` objects cover different
    time periods, show different companies, and even contain different columns:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经收集了关于不同公司在两个季度内股票表现的数据。为了最好地展示如何进行连接操作，我们故意使这两个`pd.DataFrame`对象涵盖不同的时间段，显示不同的公司，甚至包含不同的列：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The most basic call to `pd.concat` would accept both of these `pd.DataFrame`
    objects in a list. By default, this will stack the objects vertically, i.e., the
    first `pd.DataFrame` is simply stacked on top of the second.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对`pd.concat`的最基本调用将接受这两个`pd.DataFrame`对象的列表。默认情况下，这将垂直堆叠对象，即第一个`pd.DataFrame`简单地堆叠在第二个上面。
- en: 'While most of the columns in our `pd.DataFrame` objects overlap, `df_q1` does
    not have a `close` column, whereas `df_q2` does. To still make the concatenation
    work, pandas will include the *close* column in the result of `pd.concat`, assigning
    a missing value to the rows that came from `df_q1`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的`pd.DataFrame`对象中大多数列是重叠的，但`df_q1`没有`close`列，而`df_q2`有。为了让连接仍然生效，pandas将在`pd.concat`的结果中包括*close*列，并为来自`df_q1`的行分配缺失值：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should also take note of the row index that pandas gives in the result.
    In essence, pandas takes the index values of `df_q1`, which range from 0–2, and
    then takes the index values of `df_q2`, which range from 0–3\. When creating the
    new row index, pandas simply retains those values, stacking them vertically in
    the result. If you do not care for that behavior, you can pass in `ignore_index=True`
    to `pd.concat`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您还应该注意pandas在结果中给出的行索引。实际上，pandas获取了`df_q1`的索引值，范围从0到2，然后获取了`df_q2`的索引值，范围从0到3。在创建新的行索引时，pandas简单地保留了这些值，并在结果中垂直堆叠它们。如果您不喜欢这种行为，可以向`pd.concat`传递`ignore_index=True`：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Another potential issue is that we can no longer see which `pd.DataFrame` our
    records originally come from. To retain that information, we can pass through
    a `keys=` argument, providing custom labels to denote the source of our data:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个潜在的问题是我们不能再看到我们的记录最初来自哪个`pd.DataFrame`了。为了保留这些信息，我们可以通过`keys=`参数传递自定义标签，以表示数据的来源：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`pd.concat` also allows you to control the direction in which things are being
    concatenated. Instead of the default behavior to stack vertically, we can pass
    `axis=1` to see things stacked horizontally:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.concat`还允许您控制连接的方向。与默认的垂直堆叠行为不同，我们可以传递`axis=1`来水平堆叠：'
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: While this gave us back a result without error, a closer inspection of the result
    reveals some issues. The first two rows of data cover both `AAPL` and `MSFT`,
    respectively, so there is no cause for concern there. However, the third row of
    data shows `AMZN` as the Q1 ticker and `IBM` as the Q2 ticker – what gives?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这样做使我们得到了一个没有错误的结果，但仔细检查结果后发现了一些问题。数据的前两行分别涵盖了`AAPL`和`MSFT`，所以在这里没有什么好担心的。然而，数据的第三行显示`AMZN`作为Q1的股票代码，而`IBM`作为Q2的股票代码
    - 这是怎么回事？
- en: 'The problem is that pandas is aligning on the values of the index, and not
    on any other column like `ticker`, which is what we are probably interested in.
    If we wanted `pd.concat` to align by the `ticker`, we could set that as the row
    index of the two `pd.DataFrame` objects before concatenation:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 的问题在于它根据索引的值进行对齐，而不是像`ticker`这样的其他列，这可能是我们感兴趣的。如果我们希望`pd.concat`根据`ticker`进行对齐，在连接之前，我们可以将这两个`pd.DataFrame`对象的`ticker`设置为行索引：
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'One last thing we might want to control about the alignment behavior is how
    it treats labels that appear in at least one, but not all, of the objects being
    concatenated. By default, `pd.concat` performs an “outer” join, which will take
    all of the index values (in our case, the `ticker` symbols) and show them in the
    output, using a missing value indicator where applicable. Passing `join="inner"`
    as an argument, by contrast, will only show index labels that appear in all of
    the objects being concatenated:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能想要控制的最后一个对齐行为是如何处理至少在一个对象中出现但不是所有对象中都出现的标签。默认情况下，`pd.concat` 执行“外连接”操作，这将取所有的索引值（在我们的例子中是
    `ticker` 符号），并将它们显示在输出中，适用时使用缺失值指示符。相对地，传递 `join="inner"` 作为参数，只会显示在所有被连接对象中都出现的索引标签：
- en: '[PRE14]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There’s more…
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: '`pd.concat` is an expensive operation, and should never be called from within
    a Python loop. If you create a bunch of `pd.DataFrame` objects within a loop and
    eventually do want to concatenate them together, you are better off storing them
    in a sequence first, only calling `pd.concat` once after the sequence has been
    fully populated.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.concat` 是一个开销较大的操作，绝对不应该在 Python 循环中调用。如果你在循环中创建了一堆 `pd.DataFrame` 对象，并且最终希望将它们连接在一起，最好先将它们存储在一个序列中，等到序列完全填充后再调用一次
    `pd.concat`。'
- en: 'We can use the IPython `%%time` magic function to profile the difference in
    approaches. Let’s start with the anti-pattern of using `pd.concat` within a loop:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 IPython 的 `%%time` 魔法函数来分析不同方法之间的性能差异。让我们从在循环中使用 `pd.concat` 的反模式开始：
- en: '[PRE16]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This code will yield the equivalent result but follows the practice of appending
    to a Python list during the loop, and only calling `pd.concat` once at the very
    end:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将产生等效的结果，但遵循在循环中追加到 Python 列表的做法，并且仅在最后调用一次 `pd.concat`：
- en: '[PRE18]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Merging DataFrames with pd.merge
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `pd.merge` 合并 DataFrame
- en: 'Another common task in reshaping data is referred to as *merging*, or in some
    cases, *joining*, with the latter term being used frequently in database terminology.
    Where concatenation “stacks” objects on top of or next to one another, a *merge*
    works by finding a common key (or set of keys) between two entities and using
    that to blend other columns from the entities together:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据重塑中的另一个常见任务称为*合并*，在某些情况下也叫*连接*，后者术语在数据库术语中使用得较多。与连接操作将对象上下堆叠或并排放置不同，*合并*通过查找两个实体之间的共同键（或一组键）来工作，并使用这个键将其他列合并在一起：
- en: '![A diagram of a number  Description automatically generated](img/B31091_07_03.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图示：数字描述自动生成](img/B31091_07_03.png)'
- en: 'Figure 7.3: Merging two pd.DataFrame objects'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：合并两个 `pd.DataFrame` 对象
- en: The most commonly used method in pandas to perform merges is `pd.merge`, whose
    functionality will be covered throughout this recipe. Another viable, though less
    commonly used, `pd.DataFrame.join` method can be used as well, although knowing
    `pd.merge` first is helpful before discussing that (we will cover `pd.DataFrame.join`
    in the next recipe).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pandas 中，最常用的合并方法是 `pd.merge`，其功能将在本食谱中详细介绍。另一个可行的（但不太常用的）方法是 `pd.DataFrame.join`，尽管在讨论它之前，先了解
    `pd.merge` 是有帮助的（我们将在下一个食谱中介绍 `pd.DataFrame.join`）。
- en: How to do it
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'Let’s continue along with the stock `pd.DataFrame` objects we created in the
    *Concatenating pd.DataFrame objects* recipe:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接着我们继续使用在*连接 `pd.DataFrame` 对象*示例中创建的股票 `pd.DataFrame` 对象：
- en: '[PRE20]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In one of the examples in that recipe, we saw how you could use a combination
    of `pd.concat` and `pd.DataFrame.set_index` to merge our two `pd.DataFrame` objects
    by the `ticker` column:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在该示例中，我们看到你可以通过结合使用 `pd.concat` 和 `pd.DataFrame.set_index` 来通过 `ticker` 列合并这两个
    `pd.DataFrame` 对象：
- en: '[PRE24]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'With `pd.merge`, you can express this much more succinctly by passing an argument
    to `on=`, which clarifies the column(s) you would like pandas to use for alignment:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pd.merge`，你可以通过传递 `on=` 参数更简洁地表达这一点，明确表示你希望 pandas 使用哪一列（或哪几列）进行对齐：
- en: '[PRE26]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As you can see, the result is not exactly the same, but we can get a little
    closer by toggling the merge behavior. By default, `pd.merge` performs an *inner*
    merge; if we wanted a result more similar to our `pd.concat` example, we could
    pass `how="outer"`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，结果并不完全相同，但我们可以通过切换合并行为来更接近原来的结果。默认情况下，`pd.merge` 执行*内连接*；如果我们想要一个更类似于 `pd.concat`
    示例的结果，可以传递 `how="outer"`：
- en: '[PRE28]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'While `pd.concat` only allows you to perform *inner* or *outer* merges, `pd.merge`
    additionally supports *left* merges, which retain all data from the first `pd.DataFrame`,
    merging in data from the second `pd.DataFrame` as key fields can be matched:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`pd.concat`只允许执行*内连接*或*外连接*，但`pd.merge`还支持*左连接*，它保留第一个`pd.DataFrame`中的所有数据，并根据关键字段匹配将第二个`pd.DataFrame`中的数据合并进来：
- en: '[PRE30]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`how="right"` reverses that, ensuring that every row from the second `pd.DataFrame`
    is represented in the output:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`how="right"`则反转了这一点，确保第二个`pd.DataFrame`中的每一行都出现在输出中：'
- en: '[PRE32]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'An additional feature when using `how="outer"` is the ability to provide an
    `indicator=` argument, which will tell you where each row in the resulting `pd.DataFrame`
    was sourced from:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`how="outer"`时的一个额外功能是可以提供一个`indicator=`参数，这将告诉你结果`pd.DataFrame`中的每一行来自哪里：
- en: '[PRE34]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: A value of “both” means that the key(s) used to perform the merge were found
    in both `pd.DataFrame` objects, which you can see is applicable to the `AAPL`
    and `MSFT` tickers. A value of `left_only` means the key(s) only appeared in the
    left `pd.DataFrame`, as is the case for `AMZN`. `right_only` highlights key(s)
    that only appeared in the right `pd.DataFrame`, like `GE` and `IBM`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: “both”的值表示用于执行合并的键在两个`pd.DataFrame`对象中都找到了，这在`AAPL`和`MSFT`的股票代码中是适用的。`left_only`的值意味着该键仅出现在左侧`pd.DataFrame`中，正如`AMZN`的情况。`right_only`则突出显示仅出现在右侧`pd.DataFrame`中的键，例如`GE`和`IBM`。
- en: Another difference between our `pd.concat` output and what we get with `pd.merge`
    is that the former generates a `pd.MultiIndex` in the columns, essentially preventing
    any clashes from column labels that appear in both `pd.DataFrame` objects. `pd.merge`,
    by contrast, appends a suffix to columns that appear in both of the `pd.DataFrame`
    objects to disambiguate. The column coming from the left `pd.DataFrame` will be
    suffixed with `_x`, whereas a suffix of `_y` indicates that the column came from
    the right `pd.DataFrame`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`pd.concat`输出和`pd.merge`的区别之一是，前者在列中生成了`pd.MultiIndex`，从而有效地防止了两个`pd.DataFrame`对象中出现的列标签冲突。相比之下，`pd.merge`会为在两个`pd.DataFrame`对象中都出现的列添加后缀，以进行区分。来自左侧`pd.DataFrame`的列会附加`_x`后缀，而`_y`后缀则表示该列来自右侧`pd.DataFrame`。
- en: 'For more control over this suffix, you can pass a tuple argument to `suffixes=`.
    With our sample data, this argument can be used to easily identify Q1 versus Q2
    data:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 若想更好地控制这个后缀，可以将元组作为参数传递给`suffixes=`。在我们的示例数据中，这个参数可以方便地区分Q1和Q2的数据：
- en: '[PRE36]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'However, you should be aware that the suffixes are only applied if the column
    name appears in both `pd.DataFrame` objects. If a column only appears in one but
    not both objects, no suffix will be applied:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你应该知道，后缀只会在列名同时出现在两个`pd.DataFrame`对象中时才会应用。如果某个列只出现在其中一个对象中，则不会应用后缀：
- en: '[PRE38]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'If our key column(s) has different names in the two `pd.DataFrame` objects,
    would that be a problem? Of course not! No need to take my word for it though
    – let’s just rename the `ticker` column in one of our `pd.DataFrame` objects to
    `SYMBOL`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的键列在两个`pd.DataFrame`对象中有不同的名称，那会是个问题吗？当然不会！不过不必只听我说——让我们把其中一个`pd.DataFrame`对象中的`ticker`列重命名为`SYMBOL`试试看：
- en: '[PRE40]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'With `pd.merge`, the only thing that changes is that you now need to pass two
    different arguments to `left_on=` and `right_on=`, instead of just one argument
    to `on=`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pd.merge`时，唯一改变的是你现在需要将两个不同的参数传递给`left_on=`和`right_on=`，而不再是将一个参数传递给`on=`：
- en: '[PRE42]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To finish off this recipe, let’s consider a case where there are multiple columns
    that should comprise our merge key. We can start down this path by creating one
    `pd.DataFrame` that lists out the ticker, quarter, and low price:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个示例，让我们考虑一个案例，其中有多个列应作为我们的合并键。我们可以通过创建一个`pd.DataFrame`来列出股票代码、季度和最低价来开始：
- en: '[PRE44]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'A second `pd.DataFrame` will also contain the ticker and quarter (albeit with
    different names), but will show the highs instead of the lows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个`pd.DataFrame`也会包含股票代码和季度（尽管名称不同），但会显示最高值而不是最低值：
- en: '[PRE46]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'With the layout of these `pd.DataFrame` objects, our key field now becomes
    the combination of the ticker and the quarter. By passing the appropriate labels
    as arguments to `left_on=` and `right_on=`, pandas is still able to perform this
    merge:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些`pd.DataFrame`对象的布局下，我们的关键字段现在变成了股票代码和季度的组合。通过将适当的标签作为参数传递给`left_on=`和`right_on=`，pandas仍然可以执行这个合并：
- en: '[PRE48]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: There’s more…
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: An extra consideration when trying to merge data is the uniqueness of the key(s)
    in both `pd.DataFrame` objects. Having a poor or incorrect understanding of this
    can lead to very hard-to-detect errors appearing in your applications. Fortunately,
    `pd.merge` can help detect these issues upfront.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试合并数据时，另一个需要考虑的因素是两个`pd.DataFrame`对象中键的唯一性。对这一点理解不清或理解错误，可能会导致在应用程序中出现难以察觉的错误。幸运的是，`pd.merge`可以帮助我们提前发现这些问题。
- en: 'To illustrate what we mean when we talk about uniqueness, highlight the issues
    it can cause, and show you how to solve them with pandas, let’s start with a small
    `pd.DataFrame` that shows hypothetical sales by salesperson over time:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明我们在谈论唯一性时的意思，突出它可能引发的问题，并展示如何通过pandas解决这些问题，我们首先从一个小的`pd.DataFrame`开始，展示假设的销售数据，按销售人员随时间变化：
- en: '[PRE50]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let’s also create a separate `pd.DataFrame` that maps each salesperson to a
    particular region:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再创建一个单独的`pd.DataFrame`，将每个销售人员映射到一个特定的地区：
- en: '[PRE52]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: If you have ever worked at a small company or within a small department, chances
    are you’ve seen data sources built this way. As far as employees in that space
    are concerned, everyone knows who `John` is, so they are content with the decision
    to lay out data in this fashion.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾在一家小公司或小部门工作过，你可能见过以这种方式构建的数据源。在那个环境中，员工们都知道`John`是谁，因此他们对这种数据布局方式感到满意。
- en: In the sales data, `John` appears multiple times, but in the regions data, `John`
    appears only once. Therefore, using `salesperson` as the merge key, the relationship
    from sales to regions is many-to-one (*n*-to-1). Conversely, the relationship
    from regions to sales is one-to-many (1-to-*n*).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在销售数据中，`John`出现了多次，但在地区数据中，`John`只出现了一次。因此，使用`salesperson`作为合并键时，销售与地区之间的关系是多对一（*n*-to-1）。反之，地区与销售之间的关系是单对多（1-to-*n*）。
- en: 'With these types of relationships, merges do not introduce any unexpected behavior.
    A `pd.merge` between these two objects will simply display the multiple rows of
    sales data alongside the corresponding region information:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些类型的关系中，合并不会引入任何意外的行为。对这两个对象进行`pd.merge`将简单地显示销售数据的多行，并与相应的地区信息并列显示：
- en: '[PRE54]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'If we were to try and sum the sales of this after the merge, we would still
    get the appropriate amount of `60`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在合并后尝试对销售额进行求和，我们仍然会得到正确的`60`：
- en: '[PRE56]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'As the company or department grows, it becomes inevitable that another `John`
    gets hired. To accommodate this, our `regions`, `pd.DataFrame` gets updated to
    add a new `last_name` column, and add a new entry for `John Newhire`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 随着公司或部门的扩展，另一个`John`被雇佣是不可避免的。为了解决这个问题，我们的`regions`、`pd.DataFrame`被更新，增加了一个新的`last_name`列，并为`John
    Newhire`添加了一条新记录：
- en: '[PRE58]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Suddenly, the same `pd.merge` we performed before yields a different result:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 突然，我们之前执行的相同的`pd.merge`产生了不同的结果：
- en: '[PRE60]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This is a definite programming mistake. If you were to try and sum the `sales`
    column from the merged `pd.DataFrame`, you would end up doubling the true amount
    of things that were actually sold. In sum, we only sold 60 units, but with the
    introduction of `John Newhire` into our `regions`, `pd.DataFrame` suddenly changed
    the relationship between the two `pd.DataFrame` objects to many-to-many (or *n*-to-*n*),
    which duplicates much of our data and yields the wrong number of sales:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个明确的编程错误。如果你尝试从合并后的`pd.DataFrame`中对`sales`列进行求和，你最终会将实际销售的数量加倍。总之，我们只卖出了60个单位，但通过引入`John
    Newhire`到我们的`regions`、`pd.DataFrame`中，突然改变了两个`pd.DataFrame`对象之间的关系，变成了多对多（或*n*-to-*n*），这使得我们的数据被重复，从而导致了错误的销售数字：
- en: '[PRE62]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'To catch these surprises upfront with pandas, you can provide a `validate=`
    argument to `pd.merge`, which establishes the expected relationship of the merge
    key between the two objects. A validation of `many_to_one` with our original `pd.DataFrame`
    objects would have been fine:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用pandas提前捕捉到这些意外情况，你可以在`pd.merge`中提供`validate=`参数，这样可以明确合并键在两个对象之间的预期关系。如果使用我们原始的`pd.DataFrame`对象，`many_to_one`的验证是可以的：
- en: '[PRE64]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Yet that same validation would have thrown an error when `John Newhire` made
    his way into our merge:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当`John Newhire`进入我们的合并时，同样的验证会抛出一个错误：
- en: '[PRE66]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: In this simplistic example, we could have avoided this issue by modeling our
    data differently upfront, by either using a natural key comprising multiple columns
    in our sales `pd.DataFrame` or by opting for surrogate keys in both `pd.DataFrame`
    objects. Because these examples were so small, we could have also visually identified
    that there was a problem with our structure.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的例子中，如果我们一开始就以不同的方式建模数据，就可以避免这个问题，方法是使用由多个列组成的自然键来建模销售 `pd.DataFrame`，或在两个
    `pd.DataFrame` 对象中都使用替代键。因为这些例子数据量很小，我们也可以通过目测发现结构上存在的问题。
- en: In the real world, detecting issues like this is not so simple. You may be trying
    to merge thousands or millions of rows of data, so even if a large number of rows
    were affected by relationship issues, they could be easily overlooked. Attempting
    to detect issues like this by hand is akin to finding a needle in a haystack,
    so I strongly advise using this data validation feature to avoid surprises.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，检测类似的问题并不那么简单。你可能需要合并成千上万甚至数百万行数据，即使大量行受到关系问题的影响，也可能很容易被忽视。手动检测此类问题就像是在大海捞针，因此我强烈建议使用数据验证功能，以避免意外情况发生。
- en: While a failure is less than ideal, in this case, you have *failed loudly* and
    can easily identify where your modeling assumptions went wrong. Without these
    checks, your users will *silently* see incorrect data, which is, more often than
    not, a worse outcome.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然失败并非理想的结果，但在这种情况下，你已经*大声失败*，并且可以轻松识别你的建模假设出现问题的地方。如果没有这些检查，用户将*默默*看到不正确的数据，这往往是更糟糕的结果。
- en: Joining DataFrames with pd.DataFrame.join
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `pd.DataFrame.join` 合并 DataFrame
- en: While `pd.merge` is the most common approach for merging two different `pd.DataFrame`
    objects, the lesser used yet functionally similar `pd.DataFrame.join` method is
    another viable option. Stylistically, you can think of `pd.DataFrame.join` as
    a shortcut for when you want to augment an existing `pd.DataFrame` with a few
    more columns; by contrast, `pd.merge` defaults to treating both `pd.DataFrame`
    objects with equal importance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `pd.merge` 是合并两个不同 `pd.DataFrame` 对象的最常用方法，但功能上类似但使用较少的 `pd.DataFrame.join`
    方法是另一个可行的选择。从风格上讲，`pd.DataFrame.join` 可以被视为当你想要在现有的 `pd.DataFrame` 中添加更多列时的快捷方式；而相比之下，`pd.merge`
    默认将两个 `pd.DataFrame` 对象视为具有相等重要性的对象。
- en: How to do it
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'To drive home the point about `pd.DataFrame.join` being a shortcut to augment
    an existing `pd.DataFrame`, let’s imagine a sales table where the row index corresponds
    to a salesperson but uses a surrogate key instead of a natural key:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强调 `pd.DataFrame.join` 是增强现有 `pd.DataFrame` 的一种快捷方式，假设我们有一个销售表格，其中行索引对应于销售人员，但使用的是替代键而不是自然键：
- en: '[PRE68]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Let’s also then consider a dedicated `pd.DataFrame` that stores the metadata
    for some (but not all) of these salespeople:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们还可以考虑一个专门的 `pd.DataFrame`，它存储了某些（但不是全部）销售人员的元数据：
- en: '[PRE70]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Since the data we want to use to join these two `pd.DataFrame` objects together
    is in the row index, you would have to write out `left_index=True` and `right_index=True`
    while calling `pd.merge`. Also note that, because we have a `salesperson_id` of
    `9000` in our sales `pd.DataFrame` but no corresponding entry in `salesperson`,
    you would have to use `how="left"` to make sure records are not lost during the
    merge:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想要用来连接这两个 `pd.DataFrame` 对象的数据位于行索引中，因此在调用 `pd.merge` 时，你需要写出 `left_index=True`
    和 `right_index=True`。同时请注意，因为我们在销售 `pd.DataFrame` 中有 `salesperson_id` 为 `9000`
    的记录，但在 `salesperson` 中没有对应的条目，所以你需要使用 `how="left"` 来确保合并时记录不会丢失：
- en: '[PRE72]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'That rather lengthy call to `pd.merge` describes the default behavior of `pd.DataFrame.join`,
    so you may find it easier just to use the latter:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 那个相对较长的 `pd.merge` 调用描述了 `pd.DataFrame.join` 的默认行为，因此你可能会发现直接使用后者更为简便：
- en: '[PRE74]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'While `pd.DataFrame.join` defaults to a left join, you can also choose a different
    behavior through the argument to `how=`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 `pd.DataFrame.join` 默认进行左连接，你也可以通过传递 `how=` 参数选择不同的行为：
- en: '[PRE76]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Ultimately, there is no requirement to use `pd.DataFrame.join` over `pd.merge`.
    The former is simply a shortcut and a stylistic indication that the calling `pd.DataFrame`
    (here, `sales`) should not drop any records when being joined against another
    `pd.DataFrame`, like `salesperson`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，没有强制要求必须使用 `pd.DataFrame.join` 而非 `pd.merge`。前者只是一个快捷方式，并且是一种风格上的指示，表示调用的
    `pd.DataFrame`（此处是 `sales`）在与另一个 `pd.DataFrame`（如 `salesperson`）合并时不应该丢失任何记录。
- en: Reshaping with pd.DataFrame.stack and pd.DataFrame.unstack
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `pd.DataFrame.stack` 和 `pd.DataFrame.unstack` 重塑数据
- en: 'Before we jump into the terms *stacking* and *unstacking*, let’s take a step
    back and compare two tables of data. Do you notice anything different about:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨*堆叠*和*拆分*这两个术语之前，让我们退后一步，比较两张数据表。你注意到它们之间有什么不同吗：
- en: '|  | a | b | c |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | a | b | c |'
- en: '| x | 1 | 2 | 3 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| x | 1 | 2 | 3 |'
- en: '| y | 4 | 5 | 6 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| y | 4 | 5 | 6 |'
- en: 'Table 7.1: A table in wide format'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 7.1：宽格式的表格
- en: 'compared to:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 与：
- en: '| x | a | 1 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| x | a | 1 |'
- en: '| x | b | 2 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| x | b | 2 |'
- en: '| x | c | 3 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| x | c | 3 |'
- en: '| y | a | 4 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| y | a | 4 |'
- en: '| y | b | 5 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| y | b | 5 |'
- en: '| y | c | 6 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| y | c | 6 |'
- en: 'Table 7.2: A table in long format'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 7.2：长格式的表格
- en: Of course, visually, the tables have different shapes, but the data contained
    in each is the same. The former table would commonly be referred to as a table
    in *wide* format, as it stores data strewn across different columns. By contrast,
    in the second table, which many would say is stored in the *long* format, new
    rows are used to represent the various bits of data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，从视觉上看，表格的形状不同，但它们所包含的数据是相同的。前一个表格通常被称为*宽格式*表格，因为它将数据分散存储在不同的列中。相比之下，第二个表格（许多人会说它是存储在*长格式*中）则使用新行来表示不同的数据项。
- en: Which format is better? The answer to this is *it depends* – namely, on your
    audience and/or the systems you interact with. An executive at your company may
    prefer to see data stored in the wide format, as it is easier to read at a glance.
    A columnar database would prefer the long format, as it can better optimize for
    millions and billions of rows than it could for an equal number of columns.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 哪种格式更好？答案是*视情况而定*——也就是说，这取决于你的受众和/或你所交互的系统。你公司的一位高管可能更喜欢查看以宽格式存储的数据，因为这样一目了然。柱状数据库则更倾向于长格式，因为它在处理数百万甚至数十亿行数据时，能比处理相同数量的列更加优化。
- en: Knowing that there is no single way to store data, you will likely need to reshape
    data in and out of both of these formats, which brings us to the terms *stacking*
    and *unstacking*.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 既然没有一种存储数据的统一方式，你可能需要在这两种格式之间来回转换数据，这就引出了*堆叠*和*拆分*这两个术语。
- en: '*Stacking* refers to the process of taking your columns and pushing them down
    into the rows, essentially, helping to move from a wide format into a long format:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*堆叠*指的是将列压入行中的过程，本质上是帮助将宽格式转换为长格式：'
- en: '![](img/B31091_07_04.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_07_04.png)'
- en: 'Figure 7.4: Stacking a pd.DataFrame from wide to long format'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：将 `pd.DataFrame` 从宽格式堆叠到长格式
- en: '*Unstacking* goes in the opposite direction, moving data that is stored in
    a long format into a wide format:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*拆分*则是相反的过程，将存储在长格式中的数据转换为宽格式：'
- en: '![A diagram of a number and a number  Description automatically generated with
    medium confidence](img/B31091_07_05.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![一张数字和数字的图示，描述自动生成，信心中等](img/B31091_07_05.png)'
- en: 'Figure 7.5: Unstacking a pd.DataFrame from long to wide format'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：将 `pd.DataFrame` 从长格式拆分到宽格式
- en: In this recipe, we will walk you through the proper usage of the `pd.DataFrame.stack`
    and `pd.DataFrame.unstack` methods, which can be used for these reshaping purposes.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将引导你正确使用 `pd.DataFrame.stack` 和 `pd.DataFrame.unstack` 方法，这些方法可以用于数据格式转换。
- en: How to do it
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'Let’s start with the following `pd.DataFrame`, which summarizes the amount
    of fruits being grown in different states:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从以下 `pd.DataFrame` 开始，它总结了不同州种植的水果数量：
- en: '[PRE78]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: In data modeling terminology, we would consider this to be a “wide” table. Each
    row represents one state with the different numbers of each crop situated in its
    own column.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据建模术语中，我们会将此视为一个“宽”表格。每一行代表一个州，并且每种作物的不同数量存储在各自的列中。
- en: 'If we wanted to convert our table to “long” form, we would essentially want
    to see each `state` and `fruit` combination as a separate row. `pd.DataFrame.stack`
    will help us do this, by taking our fruits out of the column index and forming
    a new `pd.MultiIndex` in our rows, which contains both state and fruit:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将表格转换为“长”格式，我们基本上希望将每个 `state` 和 `fruit` 的组合作为一行展示。`pd.DataFrame.stack`
    将帮助我们实现这一目标，它通过将水果从列索引中移除，形成一个新的 `pd.MultiIndex` 在行中，其中包含状态和水果信息：
- en: '[PRE80]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'After a call to `pd.DataFrame.stack`, many users will chain in a call to `pd.Series.reset_index`
    with a `name=` argument. This converts the `pd.Series` with a `pd.MultiIndex`
    created from the `pd.DataFrame.stack` back into a `pd.DataFrame` with meaningful
    column names:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用 `pd.DataFrame.stack` 后，许多用户会接着调用 `pd.Series.reset_index` 方法，并使用 `name=`
    参数。这将把由 `pd.DataFrame.stack` 创建的带有 `pd.MultiIndex` 的 `pd.Series` 转换回具有有意义列名的 `pd.DataFrame`：
- en: '[PRE82]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: This long form of storing data is preferred for storage by many databases and
    is also the expected shape of the `pd.DataFrame` to be passed to libraries like
    Seaborn, which we showcased in the *Seaborn introduction* recipe back in *Chapter
    6*, *Visualization*.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据存储的长格式被许多数据库偏好用于存储，并且是传递给像Seaborn这样的库时`pd.DataFrame`的预期格式，我们在*第6章 可视化*中的*Seaborn简介*食谱中曾展示过。
- en: However, sometimes, you may want to go in the opposite direction, converting
    your long `pd.DataFrame` into a wider format. This can be particularly useful
    when wanting to summarize data in a compact area; utilizing both dimensions for
    display is more effective than asking your viewer to scroll through many lines
    of data.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时你可能想反向操作，将你的长格式`pd.DataFrame`转换为宽格式。这在需要在紧凑区域中总结数据时尤其有用；同时利用两个维度进行显示，比让观众滚动查看大量数据行更为有效。
- en: 'To see this in action, let’s create a new `pd.Series` from one of the `pd.DataFrame.stack`
    calls we just made:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到这一效果，让我们从我们刚才进行的`pd.DataFrame.stack`调用中创建一个新的`pd.Series`：
- en: '[PRE84]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'To go in the opposite direction and move one of our index levels from the rows
    to the columns, you simply need to make a call to `pd.Series.unstack`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要反向操作，将某个索引层级从行移到列，只需要调用`pd.Series.unstack`：
- en: '[PRE86]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'By default, a call to `pd.Series.unstack` moves the innermost level of the
    row index, which, in our case, was the `fruit`. However, we could have passed
    `level=0` to have it take the very first level instead of the innermost, in the
    case that we wanted to see the states summarized across the columns:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，调用`pd.Series.unstack`会移动行索引中最内层的层级，在我们的例子中是`fruit`。然而，我们可以传递`level=0`，使其移动最外层的第一个层级，而不是最内层的层级，这样可以将状态汇总到列中：
- en: '[PRE88]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Because our `pd.MultiIndex` levels have names, we could also have referred
    to the level we wanted to be moved by name instead of by position:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的`pd.MultiIndex`层级有名称，我们也可以通过名称而不是位置来引用我们想要移动的层级：
- en: '[PRE90]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Reshaping with pd.DataFrame.melt
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`pd.DataFrame.melt`进行数据重塑
- en: In the *Reshaping with pd.DataFrame.stack and pd.DataFrame.unstack* recipe,
    we discovered that you could convert a wide `pd.DataFrame` into long form by setting
    the appropriate row and column index(es) before calling `pd.DataFrame.stack`.
    `pd.TheDataFrame.melt` function also lets you convert your `pd.DataFrame` from
    wide to long, but can do so without having to set the row and column index values
    in an intermediate step, while also offering more control over what other columns
    may or may not be included as part of the wide to long conversion.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在*使用pd.DataFrame.stack和pd.DataFrame.unstack进行重塑*食谱中，我们发现，你可以通过在调用`pd.DataFrame.stack`之前设置合适的行和列索引，将宽格式的`pd.DataFrame`转换为长格式。`pd.DataFrame.melt`函数也能将你的`pd.DataFrame`从宽格式转换为长格式，但无需在中间步骤设置行和列索引值，同时还能对宽到长的转换中是否包含其他列进行更多控制。
- en: How to do it
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'Let’s once again create a summary of the different fruits being grown in different
    states. However, unlike the *Reshaping with pd.DataFrame.stack and pd.DataFrame.unstack*
    recipe, we will not be setting the row index to the state values, and instead,
    just treating it as another column in our `pd.DataFrame`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次总结不同水果在不同州的种植情况。然而，与*使用pd.DataFrame.stack和pd.DataFrame.unstack进行重塑*食谱不同，我们不会将行索引设置为州值，而是将其视为`pd.DataFrame`中的另一列：
- en: '[PRE92]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'To convert to long format with `pd.DataFrame.stack`, we would have to chain
    together a few calls to get back a `pd.DataFrame` without a `pd.MultiIndex`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过`pd.DataFrame.stack`转换为长格式，我们需要将几个调用链起来，最终得到一个没有`pd.MultiIndex`的`pd.DataFrame`：
- en: '[PRE94]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'The column name `level_1` is created by default during our `pd.DataFrame.stack`
    operation because the column index we start with is unnamed. We also see that
    we get an auto-generated column name of `0` for the newly introduced values in
    our long format, so we would still need to chain in a rename to get us a more
    readable `pd.DataFrame`:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 列名`level_1`在我们的`pd.DataFrame.stack`操作中默认创建，因为我们开始时的列索引没有名称。我们还看到，对于长格式中新引入的值，会自动生成一个`0`的列名，所以我们仍然需要链式调用重命名操作来获得一个更具可读性的`pd.DataFrame`：
- en: '[PRE96]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '`pd.DataFrame.melt` gets us a lot closer to our desired `pd.DataFrame`, simply
    by providing an `id_vars=` argument that corresponds to the row index you would
    have used with `pd.DataFrame.stack`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.DataFrame.melt`通过提供一个`id_vars=`参数，直接让我们接近我们想要的`pd.DataFrame`，这个参数对应于你在使用`pd.DataFrame.stack`时会用到的行索引：'
- en: '[PRE98]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'With `pd.DataFrame.melt`, the newly created column from our variables (here,
    the different fruits) is given the name `variable`, and the value column is given
    the default name of `value`. We can override these defaults through the use of
    the `var_name=` and `value_name=` arguments:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pd.DataFrame.melt`时，我们从变量（这里是不同的水果）创建的新列被命名为`variable`，值列的默认名称为`value`。我们可以通过使用`var_name=`和`value_name=`参数来覆盖这些默认值：
- en: '[PRE100]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'As an added bonus, `pd.DataFrame.melt` gives you an easy way to control which
    columns are included as part of the wide-to-long conversion. For instance, if
    we don’t care to include the `banana` values in our newly formed long table, we
    could just pass the other columns of `apple` and `orange` as arguments to `value_vars=`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外的好处，`pd.DataFrame.melt`为你提供了一种简单的方法来控制在宽转长的转换中包含哪些列。例如，如果我们不想在新创建的长表中包含`banana`的值，我们可以只将`apple`和`orange`的其他列作为参数传递给`value_vars=`：
- en: '[PRE102]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Reshaping with pd.wide_to_long
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pd.wide_to_long重塑数据
- en: So far, we have encountered two very viable ways of converting data from wide
    to long format, whether it be through the use of the `pd.DataFrame.stack` method,
    introduced in our *Reshaping with pd.DataFrame.stack and pd.DataFrame.unstack*
    recipe, or through the use of the `pd.DataFrame.melt`, as we saw in the *Reshaping
    with pd.DataFrame.melt* recipe.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经遇到了两种非常可行的方法，将数据从宽格式转换为长格式，无论是通过使用`pd.DataFrame.stack`方法（我们在*使用pd.DataFrame.stack和pd.DataFrame.unstack重塑数据*食谱中介绍的），还是通过使用`pd.DataFrame.melt`（我们在*使用pd.DataFrame.melt重塑数据*食谱中看到的）。
- en: If those aren’t enough, pandas offers the `pd.wide_to_long` function, which
    can help with that conversion given that your columns follow a particular naming
    pattern, as we will see in this recipe.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些还不够，pandas提供了`pd.wide_to_long`函数，如果你的列遵循特定的命名模式，它可以帮助完成这种转换，正如我们在本食谱中所看到的。
- en: How to do it
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'Let’s assume we have the following `pd.DataFrame`, where we have one `id` variable
    of `widget` and four columns representing sales from a business quarter. Each
    column of sales begins with `"quarter_"`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下`pd.DataFrame`，其中有一个`id`变量为`widget`，以及四列代表一个商业季度的销售额。每一列的销售额以`"quarter_"`开头：
- en: '[PRE104]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Going back to our example of `pd.DataFrame.stack`, we could convert this from
    wide to long using the following methods:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们`pd.DataFrame.stack`的例子，我们可以使用以下方法将其从宽格式转换为长格式：
- en: '[PRE106]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'For a more succinct solution, we could use `pd.DataFrame.melt`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个更简洁的解决方案，我们可以使用`pd.DataFrame.melt`：
- en: '[PRE108]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'But there is a feature that `pd.wide_to_long` offers that neither of these
    approaches handles directly – namely, to create a new variable out of the column
    labels that are being converted into variables. So far, we see the new `quarter`
    values as `quarter_1`, `quarter_2`, `quarter_3`, and `quarter_4`, but `pd.wide_to_long`
    can extract that string out of the newly created variables, more simply leaving
    you with the digits `1`, `2`, `3`, and `4`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 但是`pd.wide_to_long`提供了一个特性，是这两种方法都没有直接处理的——即从正在转换为变量的列标签中创建一个新变量。到目前为止，我们看到新的`quarter`值为`quarter_1`、`quarter_2`、`quarter_3`和`quarter_4`，但`pd.wide_to_long`可以从新创建的变量中提取该字符串，更简单地只留下数字`1`、`2`、`3`和`4`：
- en: '[PRE110]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: Reshaping with pd.DataFrame.pivot and pd.pivot_table
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pd.DataFrame.pivot和pd.pivot_table重塑数据
- en: So far in this chapter, we have seen that `pd.DataFrame.stack`, `pd.DataFrame.melt`,
    and `pd.wide_to_long` can all be used to help you convert your `pd.DataFrame`
    from a wide to a long format. On the flip side, we have only seen `pd.Series.unstack`
    helps us go from long to wide, but that method has the downside of requiring us
    to assign a proper row index before we can use it. With `pd.DataFrame.pivot`,
    you can skip any intermediate steps and go directly from a long to a wide format.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们已经看到`pd.DataFrame.stack`、`pd.DataFrame.melt`和`pd.wide_to_long`都可以帮助你将`pd.DataFrame`从宽格式转换为长格式。另一方面，我们只看到`pd.Series.unstack`帮助我们从长格式转换为宽格式，但该方法有一个缺点，需要我们在使用之前为其分配一个合适的行索引。使用`pd.DataFrame.pivot`，你可以跳过任何中间步骤，直接从长格式转换为宽格式。
- en: Beyond `pd.DataFrame.pivot`, pandas offers a `pd.pivot_table` function, which
    can not only reshape from long to wide but allows you to perform aggregations
    as part of the reshape.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`pd.DataFrame.pivot`之外，pandas还提供了`pd.pivot_table`函数，它不仅可以将数据从长格式转换为宽格式，还允许你在重塑的同时进行聚合。
- en: '![](img/B31091_07_06.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_07_06.png)'
- en: 'Figure 7.6: Using pd.pivot_table to reshape with sum aggregation'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6：使用pd.pivot_table进行求和聚合重塑
- en: Effective use of `pd.pivot_table` allows you to perform very complex calculations
    with a compact and concise syntax.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 有效使用`pd.pivot_table`可以让你使用紧凑简洁的语法执行非常复杂的计算。
- en: How to do it
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'In many of the preceding recipes, we have started with data in wide form and
    reshaped it to long form. For this recipe, let’s start with data that appears
    in long form from the outset. We are also going to add a new column for `number_eaten`
    to showcase the aggregation capabilities when pivoting within pandas:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的一些示例中，我们从宽格式数据开始，之后将其重塑为长格式。在这个示例中，我们将从一开始就使用长格式数据。我们还会添加一个新列`number_eaten`，以展示在pandas中透视时的聚合功能：
- en: '[PRE112]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'As we learned back in the *Reshaping with pd.DataFrame.stack and pd.DataFrame.unstack*
    recipe, if we wanted to convert this from long format into wide, we could do so
    with the clever use of `pd.DataFrame.set_index` paired with `pd.DataFrame.unstack`:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*使用pd.DataFrame.stack和pd.DataFrame.unstack重塑数据*一节中学到的那样，如果我们希望将数据从长格式转换为宽格式，可以通过巧妙使用`pd.DataFrame.set_index`配合`pd.DataFrame.unstack`来实现：
- en: '[PRE114]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '`pd.DataFrame.pivot` lets us tackle this in one method call. A basic usage
    of this method requires `index=` and `columns=` arguments, to dictate which column(s)
    should appear in the row and column indexes, respectively:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.DataFrame.pivot`让我们通过一次方法调用来解决这个问题。这个方法的基本用法需要`index=`和`columns=`参数，用来指定哪些列应该出现在行和列的索引中：'
- en: '[PRE116]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '`pd.DataFrame.pivot` will take any column that is not specified as an argument
    to `index=` or `columns=`, and try to convert that column into the values of the
    resulting `pd.DataFrame`. However, if you did not care for all of the remaining
    columns to be a part of the pivoted `pd.DataFrame`, you could specify what you
    want to keep with the `values=` argument. For example, if we only cared to pivot
    the `number_grown` column and ignore the `number_eaten` column, we could write
    this as:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.DataFrame.pivot`会将任何未指定为`index=`或`columns=`参数的列，尝试转换为结果`pd.DataFrame`中的值。然而，如果你不希望所有剩余的列都成为透视后`pd.DataFrame`的一部分，你可以使用`values=`参数指定需要保留的列。例如，如果我们只关心透视`number_grown`列，而忽略`number_eaten`列，可以写成这样：'
- en: '[PRE118]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'In the case where you only wanted to keep one value, the generated `pd.MultiIndex`
    in the columns may seem superfluous. Fortunately, this can be dropped with a simple
    call to `pd.DataFrame.droplevel`, where you indicate the `axis=` where you would
    like to drop a level (specify `1` for the columns) and the index level you would
    like to drop (here, `0` represents the first level):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只想保留一个值，那么在列中生成的`pd.MultiIndex`可能显得多余。幸运的是，通过简单调用`pd.DataFrame.droplevel`，你可以删除它，在这个函数中你需要指明`axis=`，以指定你希望删除哪个级别（对于列，指定`1`），以及你希望删除的索引级别（这里`0`代表第一级）：
- en: '[PRE120]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'While `pd.DataFrame.pivot` is useful for reshaping, it can only help in the
    case that none of the values used to form your rows and columns are duplicated.
    To see this limitation, let’s work with a slightly modified `pd.DataFrame` that
    shows how different fruits have been consumed or grown in different states and
    years:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`pd.DataFrame.pivot`对于重塑数据很有用，但它仅适用于那些用于形成行和列的值没有重复的情况。为了看到这个限制，我们来看一个稍微修改过的`pd.DataFrame`，展示不同水果在不同州和年份的消费或种植情况：
- en: '[PRE122]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'We would be able to still use `pd.DataFrame.pivot` on this `pd.DataFrame` if
    we placed `state`, `fruit`, and `year` all in either the rows or the columns:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将`state`、`fruit`和`year`放入行或列中，我们仍然能够在这个`pd.DataFrame`上使用`pd.DataFrame.pivot`：
- en: '[PRE124]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'But what if we didn’t want to see the `year` as part of our output? Just removing
    it from our `pd.DataFrame.pivot` arguments will raise an exception:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 那如果我们不想在输出中看到`year`呢？只需从`pd.DataFrame.pivot`的参数中移除它就会抛出异常：
- en: '[PRE126]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'For `pd.pivot_table`, the lack of a `year` column is no problem at all:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`pd.pivot_table`，缺少`year`列完全不成问题：
- en: '[PRE128]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: This works because `pd.pivot_table` aggregates the values while reshaping them
    into a wide form. Taking Arizona apples as an example, our input data showed that
    a whopping three were grown in the year 2023 before doubling to a magnificent
    six in 2024\. In our call to `pd.pivot_table`, this is shown as `4.5`. By default,
    `pd.pivot_table` will take the average of values you supply to it during a reshape.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这之所以有效，是因为`pd.pivot_table`在重塑数据时会对值进行聚合，并转换为宽格式。以亚利桑那州的苹果为例，输入数据显示在2023年种植了三颗苹果，到了2024年数量翻倍达到了六颗。在我们调用`pd.pivot_table`时，这显示为`4.5`。默认情况下，`pd.pivot_table`会在重塑过程中取你提供的值的平均值。
- en: 'You can, of course, control the aggregation function being used. In this particular
    case, we may be more interested in knowing how many fruits were grown in each
    state in total, rather than taking an average by year. By passing a different
    aggregation function as a parameter to `aggfunc=`, you can easily get a summation
    instead:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以控制使用的聚合函数。在这种特定情况下，我们可能更关心知道每个州总共种了多少水果，而不是按年份计算平均数。通过将不同的聚合函数作为参数传递给
    `aggfunc=`，你可以轻松获得总和：
- en: '[PRE130]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'For more advanced use cases, you can even provide a dictionary of values to
    `aggfunc=`, where each key/value pair of the dictionary dictates the column and
    the type of aggregation(s) to be applied, respectively:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更高级的使用场景，你甚至可以向 `aggfunc=` 提供一个值的字典，其中字典的每个键/值对分别指定要应用的列和聚合类型：
- en: '[PRE132]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: Reshaping with pd.DataFrame.explode
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 pd.DataFrame.explode 进行数据重塑
- en: The world would be so simple if every piece of data fitted perfectly as a scalar
    into a two-dimensional `pd.DataFrame`. Alas, life is not so simple. Especially
    when working with semi-structured sources of data like JSON, it is not uncommon
    to have individual items in your `pd.DataFrame` contain non-scalar sequences like
    lists and tuples.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每一条数据都能完美地作为标量适应一个二维的 `pd.DataFrame`，那该多好啊。然而，生活并非如此简单。特别是当处理像 JSON 这样的半结构化数据源时，`pd.DataFrame`
    中的单个项包含非标量序列（如列表和元组）并不罕见。
- en: You may find it acceptable to leave data in that state, but other times, there
    is value to normalizing the data and potentially extracting out sequences contained
    within a column into individual elements.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能觉得将数据保持在这种状态下是可以接受的，但有时，将数据规范化并可能将列中的序列提取为单独的元素是有价值的。
- en: '![](img/B31091_07_07.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_07_07.png)'
- en: 'Figure 7.7: Using pd.DataFrame.explode to extract list elements to individual
    rows'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7：使用 pd.DataFrame.explode 将列表元素提取到单独的行
- en: To that end, `pd.DataFrame.explode` is the right tool for the job. It may not
    be a function you use every day, but when you eventually need to use it, you will
    be happy to have known about it. Attempting to replicate the same functionality
    outside of pandas can be error-prone and non-performant!
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，`pd.DataFrame.explode` 是完成此任务的正确工具。它可能不是你每天都使用的函数，但当你最终需要使用它时，你会很高兴知道它。试图在
    pandas 之外复制相同的功能可能容易出错且性能较差！
- en: How to do it
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: Since we mentioned JSON as a good source for semi-structured data in the introduction
    to this recipe, let’s start by imagining that we have to interact with a REST
    API for an HR system. The HR system should tell us who each person is in the company,
    as well as who, if anyone, reports to them.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在本食谱的介绍中提到过 JSON 是一个很好的半结构化数据源，让我们假设需要与一个 HR 系统的 REST API 交互。HR 系统应该告诉我们公司中每个人是谁，以及谁（如果有的话）向他们报告。
- en: 'The hierarchy between employees is rather easy to represent in a semi-structured
    format like JSON, so the REST API might return something like:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 员工之间的层级关系很容易用像 JSON 这样的半结构化格式表示，因此 REST API 可能会返回类似如下的内容：
- en: '[PRE134]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'The pandas library will also let us load this data into a `pd.DataFrame`, albeit
    with the `direct_reports` column containing lists:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 库还允许我们将这些数据加载到 `pd.DataFrame` 中，尽管 `direct_reports` 列包含的是列表：
- en: '[PRE135]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'With `pd.DataFrame.explode`, you can unpack those `direct_reports` into separate
    rows of the `pd.DataFrame`:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pd.DataFrame.explode`，你可以将 `direct_reports` 拆分成 `pd.DataFrame` 中的单独行：
- en: '[PRE137]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'Building off of the knowledge we picked up about merging/joining data from
    our *Merging DataFrames with pd.merge* recipe, we can very easily take our exploded
    information and merge in the names of direct reports, yielding an easy summary
    of who works at the company and who, if anyone, reports to them:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们在 *使用 pd.merge 合并数据框* 食谱中学到的合并/连接数据的知识，我们可以非常轻松地将爆炸后的信息与直接汇报人员的名字合并，从而生成一个关于谁在公司工作以及谁（如果有的话）向他们汇报的简易总结：
- en: '[PRE139]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: There’s more…
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'While we did not introduce it in our review of types in *Chapter 3*, *Data
    Types*, PyArrow does offer a struct data type which, when used in a `pd.Series`,
    exposes a `pd.Series.struct.explode` method:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在*第三章*的类型回顾中没有介绍它，*数据类型*，但 PyArrow 确实提供了一种结构体数据类型，当它在 `pd.Series` 中使用时，会暴露出
    `pd.Series.struct.explode` 方法：
- en: '[PRE141]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: 'Unlike `pd.DataFrame.explode`, which generates new rows of data, `pd.Series.struct.explode`
    generates new columns of data from its struct members:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `pd.DataFrame.explode` 不同，后者会生成新的数据行，`pd.Series.struct.explode` 会根据其结构成员生成新的数据列：
- en: '[PRE143]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: This could be particularly useful if you are dealing with a semi-structured
    data source like JSON. If you are able to fit nested data from such a source into
    the typed struct that PyArrow has to offer, `pd.Series.struct.explode` can save
    you a significant amount of trouble when trying to unnest that data.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你处理的是类似 JSON 的半结构化数据源，这特别有用。如果你能将来自这样的数据源的嵌套数据适配到 PyArrow 提供的类型化结构中，那么 `pd.Series.struct.explode`
    可以在尝试展开数据时为你节省大量麻烦。
- en: Transposing with pd.DataFrame.T
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 pd.DataFrame.T 进行转置
- en: 'For the final recipe in this chapter, let’s explore one of the easier reshaping
    features of pandas. *Transposition* refers to the process of inverting your `pd.DataFrame`
    so that the rows become the columns and the columns become the rows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后一个实例，让我们来探索 pandas 中一个较为简单的重塑功能。*转置*是指将你的 `pd.DataFrame` 反转，使行变成列，列变成行的过程：
- en: '![](img/B31091_07_08.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_07_08.png)'
- en: 'Figure 7.8: Transposing a pd.DataFrame'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8：转置一个 pd.DataFrame
- en: In this recipe, we will see how to transpose with the `pd.DataFrame.T` method
    while discussing how this might be useful.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，我们将看到如何使用 `pd.DataFrame.T` 方法进行转置，同时讨论这可能会如何有用。
- en: How to do it
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'Transposition in pandas is straightforward. Take any `pd.DataFrame`:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 中的转置非常简单。只需要获取任何 `pd.DataFrame`：
- en: '[PRE145]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'You can simply access the `pd.DataFrame.T` attribute and watch as your rows
    become your columns and your columns become your rows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要访问 `pd.DataFrame.T` 属性，就能看到你的行变成列，列变成行：
- en: '[PRE147]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: There are an endless number of reasons why you may want to transpose, ranging
    from simply thinking *it looks better* in a given format to cases where you find
    it easier to select by a row index label instead of by a column index label.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 转置的原因无穷无尽，从单纯地觉得*在给定格式下看起来更好*，到更容易通过行索引标签选择而不是列索引标签选择的情况。
- en: 'However, one of the main use cases to transpose will be to get your `pd.DataFrame`
    in an *optimal* format before applying functions. As we learned back in *Chapter
    5*, *Algorithms and How to Apply Them*, pandas has the ability to apply aggregations
    to each column:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，转置的主要用例之一是，在应用函数之前，将你的 `pd.DataFrame` 转换为*最佳*格式。正如我们在*第五章*《算法与如何应用它们》中所学到的，pandas
    能够对每一列进行聚合：
- en: '[PRE149]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'as well as to each row with the `axis=1` argument:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 以及对每一行使用`axis=1`参数：
- en: '[PRE151]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: Unfortunately, using the `axis=1` argument can drastically reduce the performance
    of your applications. If you find yourself scattering a lot of `axis=1` calls
    throughout your code, chances are you would be much better off transposing your
    data first and then applying functions with the default `axis=0`.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，使用 `axis=1` 参数可能会显著降低应用程序的性能。如果你发现自己在代码中散布了大量的 `axis=1` 调用， chances are
    你最好先进行数据转置，再使用默认的 `axis=0` 来应用函数。
- en: 'To see the difference, let’s look at a `pd.DataFrame` that is rather wide:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看出差异，让我们看一个相当宽的 `pd.DataFrame`：
- en: '[PRE153]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: 'Ultimately, we will get the same result, whether we sum the rows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，无论是求行和：
- en: '[PRE155]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: 'or transpose first, and then use the default summation of the columns:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 或者先转置，再使用默认的列求和：
- en: '[PRE157]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: However, if you were to repeatedly make calls using `axis=1` as an argument,
    you would find that transposing first can save significant time.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你反复使用 `axis=1` 作为参数，你会发现，先进行转置可以节省大量时间。
- en: 'To measure this, let’s use IPython and check how long it takes to perform our
    sum 100 times:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量这一点，我们可以使用 IPython 来检查执行 100 次求和操作所需的时间：
- en: '[PRE159]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: 'Comparatively, transposing first and then performing the sum will be much faster:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，先进行转置然后求和会更快：
- en: '[PRE161]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: Overall, using `pd.DataFrame.T` to avoid subsequent calls with `axis=1` is a
    highly encouraged practice.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，使用 `pd.DataFrame.T` 来避免后续使用 `axis=1` 调用是非常推荐的做法。
- en: Join our community on Discord
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的社区，加入Discord讨论
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者讨论：
- en: '[https://packt.link/pandas](https://packt.link/pandas)'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/pandas](https://packt.link/pandas)'
- en: '![](img/QR_Code5040900042138312.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5040900042138312.png)'
