- en: '2'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '2'
- en: Data Cleaning and Advanced Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据清洗与高级机器学习
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将能够：
- en: Plan a machine learning classification strategy
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划机器学习分类策略
- en: Preprocess data to prepare it for machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理数据以为机器学习做好准备
- en: Train classification models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练分类模型
- en: Use validation curves to tune model parameters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用验证曲线来调整模型参数
- en: Use dimensionality reduction to enhance model performance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用降维技术来提升模型性能
- en: In this chapter, you will learn data preprocessing and machine learning by completing
    several practical exercises.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将通过完成几个实际练习来学习数据预处理和机器学习。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Consider a small food-delivery business that is looking to optimize their product.
    An analyst might look at the appropriate data and determine what type of food
    people are enjoying most. Perhaps they find a large amount of people are ordering
    the spiciest food options, indicating the business might be losing out on customers
    who desire something even more spicy. This is quite basic, or as some might say,
    "vanilla" analytics.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一家小型外卖企业，正在寻求优化他们的产品。分析师可能会查看相关数据，并确定人们最喜欢的食物类型。也许他们发现有大量人正在订购最辣的食物选项，表明企业可能失去了那些想要更辣食物的顾客。这是相当基础的，或者有些人可能会说，这是“普通”的分析。
- en: In a separate task, the analyst could employ predictive analytics by modeling
    the order volumes over time. With enough data, they could predict the future order
    volumes and therefore guide the restaurant as to how many staff are required each
    day. This model could take factors such as the weather into account to make the
    best predictions. For instance, a heavy rainstorm could be an indicator to staff
    more delivery personnel to make up for slow travel times. With historical weather
    data, that type of signal could be encoded into the model. This prediction model
    would save a business the time of having to consider these problems manually,
    and money by keeping customers happy and thereby increasing customer retention.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个单独的任务中，分析师可以通过对订单量随时间变化进行建模，使用预测分析。通过足够的数据，他们可以预测未来的订单量，从而指导餐厅每天所需的员工数量。这个模型可以考虑天气等因素，以做出最佳预测。例如，一场暴雨可能是增加外卖人员的指示，以弥补交通延误的时间。通过历史天气数据，这类信号可以被编码到模型中。这个预测模型可以节省企业手动考虑这些问题的时间，并通过让顾客满意来节省资金，从而提高客户留存率。
- en: The goal of data analytics in general is to uncover actionable insights that
    result in positive business outcomes. In the case of predictive analytics, the
    aim is to do this by determining the most likely future outcome of a target, based
    on previous trends and patterns.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析的总体目标是揭示可操作的洞察，以实现积极的商业结果。在预测分析的情况下，目标是通过确定目标的最可能未来结果，基于先前的趋势和模式，来实现这一点。
- en: The benefits of predictive analytics are not restricted to big technology companies.
    Any business can find ways to benefit from machine learning, given the right data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 预测分析的好处并不仅限于大型科技公司。任何企业只要有合适的数据，都可以找到受益于机器学习的方法。
- en: Companies all around the world are collecting massive amounts of data and using
    predictive analytics to cut costs and increase profits. Some of the most prevalent
    examples of this are from the technology giants Google, Facebook, and Amazon,
    who utilize big data on a huge scale. For example, Google and Facebook serve you
    personalized ads based on predictive algorithms that guess what you are most likely
    to click on. Similarly, Amazon recommends personalized products that you are most
    likely to buy, given your previous purchases.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 世界各地的公司正在收集大量数据，并使用预测分析来降低成本并提高利润。最常见的例子来自科技巨头谷歌、Facebook和亚马逊，他们在大规模使用大数据。例如，谷歌和Facebook根据预测算法向你展示个性化广告，这些算法猜测你最有可能点击的内容。同样，亚马逊根据你之前的购买推荐你最有可能购买的个性化产品。
- en: Modern predictive analytics is done with machine learning, where computer models
    are trained to learn patterns from data. As we saw briefly in the previous chapter,
    software such as scikit-learn can be used with Jupyter Notebooks to efficiently
    build and test machine learning models. As we will continue to see, Jupyter Notebooks
    are an ideal environment for doing this type of work, as we can perform ad-hoc
    testing and analysis, and easily save the results for reference later.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现代预测分析是通过机器学习完成的，其中计算机模型被训练以从数据中学习模式。正如我们在上一章简要看到的那样，像 scikit-learn 这样的软件可以与
    Jupyter Notebooks 一起使用，以高效地构建和测试机器学习模型。正如我们将继续看到的，Jupyter Notebooks 是进行这类工作的理想环境，因为我们可以进行临时测试和分析，并且可以轻松保存结果以供以后参考。
- en: In this chapter, we will again take a hands-on approach by running through various
    examples and activities in a Jupyter Notebook. Where we saw a couple of examples
    of machine learning in the previous chapter, here we'll take a much slower and
    more thoughtful approach. Using an employee retention problem as our overarching
    example for the chapter, we will discuss how to approach predictive analytics,
    what things to consider when preparing the data for modeling, and how to implement
    and compare a variety of models using Jupyter Notebooks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将再次采取动手实践的方式，通过在 Jupyter Notebook 中运行各种示例和活动来进行学习。在上一章中我们看到了几个机器学习的例子，在这里我们将采取更慢、更深思熟虑的方式。以员工留存问题作为本章的总体示例，我们将讨论如何进行预测分析，在为建模准备数据时需要考虑哪些因素，以及如何使用
    Jupyter Notebooks 实现和比较多种模型。
- en: Preparing to Train a Predictive Model
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备训练预测模型
- en: Here, we will cover the preparation required to train a predictive model. Although
    not as technically glamorous as training the models themselves, this step should
    not be taken lightly. It's very important to ensure you have a good plan before
    proceeding with the details of building and training a reliable model. Furthermore,
    once you've decided on the right plan, there are technical steps in preparing
    the data for modeling that should not be overlooked.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论训练预测模型所需的准备工作。虽然这一步骤不像训练模型本身那样在技术上引人注目，但它不应被轻视。在开始构建和训练一个可靠的模型之前，确保你有一个良好的计划是非常重要的。此外，一旦你决定了正确的计划，在为建模准备数据时，还有一些技术步骤不能被忽视。
- en: Note
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: We must be careful not to go so deep into the weeds of technical tasks that
    we lose sight of the goal. Technical tasks include things that require programming
    skills, for example, constructing visualizations, querying databases, and validating
    predictive models. It's easy to spend hours trying to implement a specific feature
    or get the plots looking just right. Doing this sort of thing is certainly beneficial
    to our programming skills, but we should not forget to ask ourselves if it's really
    worth our time with respect to the current project.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须小心，不要过于深入技术任务的细节，以至于忽视了最终目标。技术任务包括需要编程技能的内容，例如构建可视化图表、查询数据库和验证预测模型。很容易花费数小时尝试实现特定功能，或者让图表看起来恰到好处。做这种事情确实有助于提高我们的编程技能，但我们不应该忘记问自己，考虑到当前的项目，是否值得花费这么多时间。
- en: Also, keep in mind that Jupyter Notebooks are particularly well-suited for this
    step, as we can use them to document our plan, for example, by writing rough notes
    about the data or a list of models we are interested in training. Before starting
    to train models, its good practice to even take this a step further and write
    out a well- structured plan to follow. Not only will this help you stay on track
    as you build and test the models, but it will allow others to understand what
    you're doing when they see your work.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请记住，Jupyter Notebooks 特别适合这个步骤，因为我们可以用它们来记录我们的计划，例如，写下关于数据的粗略笔记或我们有兴趣训练的模型列表。在开始训练模型之前，最好进一步走一步，写出一个结构良好的计划来遵循。这样不仅有助于你在构建和测试模型时保持正轨，还能让别人看到你的工作时了解你在做什么。
- en: After discussing the preparation, we will also cover another step in preparing
    to train the predictive model, which is cleaning the dataset. This is another
    thing that Jupyter Notebooks are well-suited for, as they offer an ideal testing
    ground for performing dataset transformations and keeping track of the exact changes.
    The data transformations required for cleaning raw data can quickly become intricate
    and convoluted; therefore, it's important to keep track of your work. As discussed
    in the first chapter, tools other than Jupyter Notebooks just don't offer very
    good options for doing this efficiently.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论完准备工作后，我们还将介绍准备训练预测模型的另一步骤——清理数据集。这是 Jupyter Notebooks 非常适合的一项工作，因为它们提供了一个理想的测试环境，可以执行数据集转换并追踪精确的变化。清理原始数据所需的数据转换可能会迅速变得复杂而难以理解，因此，跟踪你的工作非常重要。如第一章所讨论的，其他工具则不提供非常好的高效操作选项。
- en: Before we progress to the next section, let's pause and think about these ideas
    in the context of a real-life example.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一部分之前，我们先暂停一下，思考一下这些想法，并将其放到现实生活中的示例中来考虑。
- en: 'Consider the following situation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下情况：
- en: You are hired by an online video game marketplace who want to increase the conversion
    rate of people visiting their site. They ask you to use predictive analytics to
    determine what genre of game the user will like, so they can display specialized
    content that will encourage the user to make a purchase. They want to do this
    without having to ask the customer their preference of game genre.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你被一家在线视频游戏平台聘用，目的是提高访问他们网站的用户的转化率。他们要求你利用预测分析来判断用户喜欢哪种类型的游戏，以便展示能够促使用户购买的专门化内容。并且他们希望做到这一点，而无需询问客户的游戏偏好。
- en: Is this a problem that can be solved? What type of data would be required? What
    would be the business implications?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可以解决的问题吗？需要什么类型的数据？这对业务会有什么影响？
- en: To address this challenge, we could consider making the prediction based on
    users' browsing cookies. For example, if they have a cookie from previously visiting
    a World of Warcraft website, this would act as an indicator that they like role
    playing games.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个挑战，我们可以考虑基于用户的浏览器 Cookie 来进行预测。例如，如果他们曾经访问过一个《魔兽世界》网站，那么这个 Cookie 可以作为他们喜欢角色扮演游戏的一个指示器。
- en: Another valuable piece of data would be a history of the games that user has
    previously bought in the marketplace. This could be the target variable in a machine
    learning algorithm, for example, a model that could predict which games the user
    would be interested in, based on the type of cookies in their browsing session.
    An alternate target variable could be constructed by setting up a survey in the
    marketplace to collect data on user preferences.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项有价值的数据是用户在平台上购买过的游戏历史。例如，这可以作为机器学习算法中的目标变量，例如一个能够预测用户可能感兴趣的游戏的模型，这个预测模型可以基于他们浏览会话中的
    Cookie 类型。另一种可替代的目标变量可以通过在平台上设置一个调查来收集用户的偏好数据。
- en: 'In terms of the business implications, being able to accurately predict the
    genre of game is very important to the success of the campaign. In fact, getting
    the prediction wrong is doubly problematic: not only do we miss out on the opportunity
    to target users, but we may end up showing users content that would be negatively
    perceived. This could lead to more people leaving the site and fewer sales.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在业务影响方面，能够准确预测游戏类型对营销活动的成功至关重要。实际上，预测错误的问题是双重的：不仅错失了瞄准用户的机会，而且还可能向用户展示负面印象的内容。这可能导致更多人离开网站并减少销售。
- en: Determining a Plan for Predictive Analytics
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定预测分析的计划
- en: When formulating a plan for doing predictive modeling, one should start by considering
    stakeholder needs. A perfect model will be useless if it doesn't solve a relevant
    problem. Planning a strategy around business needs ensures that a successful model
    will lead to actionable insights.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在制定预测建模计划时，应该首先考虑利益相关者的需求。如果模型无法解决相关问题，那么它将是无用的。围绕业务需求制定策略可以确保成功的模型带来可行的洞察力。
- en: Although it may be possible in principle to solve many business problems, the
    ability to deliver the solution will always depend on the availability of the
    necessary data. Therefore, it's important to consider the business needs in the
    context of the available data sources. When data is plentiful, this will have
    little effect, but as the amount of available data becomes smaller, so too does
    the scope of problems that can be solved.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管原则上可以解决许多业务问题，但解决方案的实现总是取决于是否能获得必要的数据。因此，在考虑业务需求时，必须结合可用的数据源。如果数据丰富，这几乎不会有影响，但随着可用数据量的减少，能够解决的问题范围也会缩小。
- en: 'These ideas can be formed into a standard process for determining a predictive
    analytics plan, which goes as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些想法可以形成一个标准的过程，用于确定预测分析计划，步骤如下：
- en: '**Look at the available data** to understand the range of realistically solvable
    business problems. At this stage, it might be too early to think about the exact
    problems that can be solved. Make sure you understand the data fields available
    and the timeframes they apply to.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查看可用数据**，了解现实中可解决的业务问题的范围。在这个阶段，可能还太早去考虑能够解决的具体问题。确保你理解可用的数据字段及其适用的时间范围。'
- en: '**Determine the business needs** by speaking with key stakeholders. Seek out
    a problem where the solution will lead to actionable business decisions.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**通过与关键利益相关者交谈，确定业务需求**。寻找一个解决方案能够带来可操作的商业决策的问题。'
- en: '**Assess the data for suitability** by considering the availability of a sufficiently
    diverse and large feature space. Also, take into account the condition of the
    data: are there large chunks of missing values for certain variables or time ranges?'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估数据的适用性**，通过考虑特征空间的多样性和规模是否足够大。同时，考虑数据的状态：某些变量或时间范围是否存在大量缺失值？'
- en: Steps 2 and 3 should be repeated until a realistic plan has taken shape. At
    this point, you will already have a good idea of what the model input will be
    and what you might expect as output.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步和第3步应重复进行，直到形成一个现实的计划。此时，你应该已经对模型输入和期望的输出有了清晰的了解。
- en: 'Once you''ve identified a problem that can be solved with machine learning,
    along with the appropriate data sources, we should answer the following questions
    to lay a framework for the project. Doing this will help us determine which types
    of machine learning models we can use to solve the problem. The following image
    provides an overview of the choices available depending on the type of data:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你确定了可以用机器学习解决的问题，并且找到了合适的数据源，我们应该回答以下问题，以为项目奠定框架。这样做有助于我们确定可以用来解决问题的机器学习模型类型。以下图像提供了根据数据类型可用选择的概述：
- en: '![Figure 2.1: A flow chart for machine learning strategy based on the type
    of data](img/C13018_02_01.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1：根据数据类型的机器学习策略流程图](img/C13018_02_01.jpg)'
- en: 'Figure 2.1: A flow chart for machine learning strategy based on the type of
    data'
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.1：根据数据类型的机器学习策略流程图
- en: 'The above image describes the path you can choose depending of the type of
    data: labeled or unlabeled.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 上图描述了根据数据类型（有标签或无标签）你可以选择的路径。
- en: As can be seen, either one can chose supervised or unsupervised learning. Supervised
    learning comprises either classification or regression problem. In regression,
    variables are continuous; for example, the amount of rainfall. In regression,
    the variables are discrete and we predict class labels. Simplest type of classification
    problem is binary; for example, will it rain today? (yes/no)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，可以选择有监督学习或无监督学习。有监督学习包括分类或回归问题。在回归中，变量是连续的；例如，降水量。在回归中，变量是离散的，我们预测类别标签。最简单的分类问题是二分类问题；例如，今天会下雨吗？（是/否）
- en: For unsupervised learning, cluster analysis is a commonly used method. Here,
    labels are assigned to the nearest cluster for each sample.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无监督学习，聚类分析是常用的方法。此处，标签会分配给每个样本的最近聚类。
- en: 'However, not only the type but also the size and origin of data sources would
    be a factor while deciding on machine learning strategy. Specifically, following
    points should be note:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，决定机器学习策略时，除了数据类型外，数据源的大小和来源也是一个重要因素。具体而言，以下几点应注意：
- en: The size of data in terms of the width (no. of columns) and height (no. of rows)
    should be considered before apply a machine learning algorithm.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用机器学习算法之前，应考虑数据的规模，特别是数据的宽度（列数）和高度（行数）。
- en: Certain algorithms are better at handling certain features than the others.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某些算法在处理某些特征时优于其他算法。
- en: General, the larger the dataset, the better in terms of accuracy. However, this
    can be time consuming
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般来说，数据集越大，准确性越高。然而，这也可能会非常耗时。
- en: One can reduce time by using dimensionality reduction techniques.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用降维技术可以节省时间。
- en: For multiple data sources, one can consider merging them in a single table.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于多个数据源，可以考虑将它们合并到一个表格中。
- en: If this cannot be done, we can train models for each and consider an ensemble
    average for final prediction.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果无法做到这一点，我们可以为每个模型分别训练，并考虑最终预测时使用这些模型的平均结果。
- en: 'An example where we may want to do this is with various sets of times series
    data on different scales. Consider we have the following data sources: a table
    with the AAPL stock closing prices on a daily time scale and iPhone sales data
    on a monthly time scale. We could merge the data by adding the monthly sales data
    to each sample in the daily time scale table, or grouping the daily data by month,
    but it might be better to build two models, one for each dataset, and use a combination
    of the results from each in the final prediction model.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一个我们可能想要这样做的例子是，对于不同尺度的多组时间序列数据。假设我们有以下数据源：一张包含AAPL股票每日收盘价的表格，以及一张包含iPhone每月销量的表格。我们可以通过将每月的销量数据添加到每日时间尺度表格的每个样本中来合并数据，或者按月对每日数据进行分组，但更好的做法可能是为每个数据集构建两个模型，并将它们的结果结合起来用于最终的预测模型。
- en: Data preprocessing has a huge impact on machine learning. Like the saying "you
    are what you eat," the model's performance is a direct reflection of the data
    it's trained on. Many models depend on the data being transformed so that the
    continuous feature values have comparable limits. Similarly, categorical features
    should be encoded into numerical values. Although important, these steps are relatively
    simple and do not take very long.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理对机器学习有巨大影响。就像俗话说的“你就是你吃的”，模型的表现直接反映了它所训练的数据。许多模型依赖于数据的转换，以便连续特征值具有可比的范围。同样，类别特征应当被编码为数值类型。尽管这些步骤重要，但它们相对简单且不会花费太长时间。
- en: 'The aspect of preprocessing that usually takes the longest is cleaning up messy
    data. Some estimates suggest that data scientists spend around two thirds of their
    work time cleaning and organizing datasets:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，预处理过程中花费最多时间的环节是清理杂乱的数据。有些估计表明，数据科学家大约将三分之二的工作时间花在清理和整理数据集上：
- en: '![Figure 2.2: A pie chart distribution of the time spend on different data
    tasks](img/C13018_02_02.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2：不同数据任务花费时间的饼图分布](img/C13018_02_02.jpg)'
- en: 'Figure 2.2: A pie chart distribution of the time spend on different data tasks'
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.2：不同数据任务花费时间的饼图分布。
- en: 'To know more about the preprocessing stage, refer to: [https://www.forbes.com/sites/gilpress/2016/03/23/data-](http://www.forbes.com/sites/gilpress/2016/03/23/data-)
    preparation-most-time-consuming-least-enjoyable-data- science-task-survey-says/2/#17c66c7e1492.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于预处理阶段的信息，请参考：[https://www.forbes.com/sites/gilpress/2016/03/23/data-](http://www.forbes.com/sites/gilpress/2016/03/23/data-)
    preparation-most-time-consuming-least-enjoyable-data- science-task-survey-says/2/#17c66c7e1492.
- en: Another thing to consider is the size of the datasets being used by many data
    scientists. As the dataset size increases, the prevalence of messy data increases
    as well, along with the difficulty in cleaning it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的因素是许多数据科学家使用的数据集的大小。随着数据集大小的增加，数据杂乱无章的情况也会增加，清理这些数据的难度也随之增加。
- en: Simply dropping the missing data is usually not the best option, because it's
    hard to justify throwing away samples where most of the fields have values. In
    doing so, we could lose valuable information that may hurt final model performance.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 单纯丢弃缺失数据通常不是最佳选择，因为很难合理化丢弃那些大部分字段都有值的样本。在这样做时，我们可能会失去一些宝贵的信息，这可能会影响最终模型的表现。
- en: Note
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In this exercise, we practice preprocessing the data by creating two DataFrames,
    and performing an inner merge and outer merge on the DataFrames and remove the
    null (`NaN`) values.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们通过创建两个DataFrame，执行内连接和外连接，并去除空值（`NaN`）来实践数据预处理。
- en: 'The steps involved in data preprocessing can be grouped as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理的步骤可以分为以下几类：
- en: '**Merging data** sets on common fields to bring all data into a single table'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合并数据**集，依据公共字段将所有数据整合到一个表格中。'
- en: '**Feature engineering** to improve the quality of data, for example, the use
    of dimensionality reduction techniques to build new features'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程**用于提高数据质量，例如，使用降维技术构建新特征。'
- en: '**Cleaning the data** by dealing with duplicate rows, incorrect or missing
    values, and other issues that arise'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清理数据**，通过处理重复行、错误或缺失的值以及其他可能出现的问题'
- en: '**Building the training data sets** by standardizing or normalizing the required
    data and splitting it into training and testing sets'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建训练数据集**，通过标准化或归一化所需的数据，并将其划分为训练集和测试集'
- en: Let's explore some of the tools and methods for doing the preprocessing.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一些进行数据预处理的工具和方法。
- en: 'Exercise 8: Explore Data Preprocessing Tools and Methods'
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 8：探索数据预处理工具和方法
- en: Start the `NotebookApp` from the project directory by executing `jupyter notebook`.
    Navigate to the `Lesson-2` directory and open up the `lesson- 2-workbook.ipynb`
    file. Find the cell near the top where the packages are loaded, and run it.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在项目目录下通过执行`jupyter notebook`启动`NotebookApp`。进入`Lesson-2`目录并打开`lesson-2-workbook.ipynb`文件。找到加载包的单元并运行它。
- en: We are going to start by showing off some basic tools from Pandas and sci-kit
    learn. Then, we'll take a deeper dive into methods for rebuilding missing data.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将从展示 Pandas 和 Sci-kit Learn 的一些基本工具开始。接着，我们将深入探讨重建缺失数据的方法。
- en: 'Scroll down to `Subtopic B: Preparing data for machine learning` and run the
    cell containing `pd.merge?` to display the docstring for the merge function in
    the notebook:![Figure 2.3: Docstring for the merge function](img/C13018_02_03.jpg)'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向下滚动到`子主题 B：为机器学习准备数据`，并运行包含`pd.merge?`的单元，以在笔记本中显示合并函数的文档字符串：![图 2.3：合并函数的文档字符串](img/C13018_02_03.jpg)
- en: 'Figure 2.3: Docstring for the merge function'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.3：合并函数的文档字符串
- en: As we can see, the function accepts a left and right DataFrame to merge. You
    can specify one or more columns to group on as well as how they are grouped, that
    is, to use the left, right, outer, or inner sets of values. Let's see an example
    of this in use.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们所见，函数接受左右两个 DataFrame 进行合并。你可以指定一个或多个用于分组的列，并且指定它们的分组方式，即使用左、右、外部或内部的值集合。我们来看一个具体的例子。
- en: 'Exit the help popup and run the cell containing the following sample DataFrames:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 退出帮助弹窗并运行包含以下示例 DataFrame 的单元：
- en: '[PRE0]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we will build two simple DataFrames from scratch. As can be seen, they
    contain a `product` column with some shared entries.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将从头开始构建两个简单的 DataFrame。如图所示，它们包含一个带有共享条目的`product`列。
- en: 'Run the next cell to perform the inner merge:![Figure 2.4: Inner merge of columns](img/C13018_02_04.jpg)'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行下一个单元来执行内连接合并：![图 2.4：列的内连接合并](img/C13018_02_04.jpg)
- en: 'Figure 2.4: Inner merge of columns'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.4：列的内连接合并
- en: Note how only the shared items, **red shirt** and **white dress**, are included.
    To include all entries from both tables, we can do an outer merge instead. Let's
    do this now.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，只有共享的项目，**红色衬衫**和**白色连衣裙**，被包括在内。为了包括两个表中的所有条目，我们可以改为进行外连接合并。现在就来做这个。
- en: 'Run the next cell to perform an outer merge:![Figure 2.5: Outer merge of columns](img/C13018_02_05.jpg)'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行下一个单元来执行外连接合并：![图 2.5：列的外连接合并](img/C13018_02_05.jpg)
- en: 'Figure 2.5: Outer merge of columns'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.5：列的外连接合并
- en: This returns all of the data from each table where missing values have been
    labeled with `NaN`.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将返回每个表中的所有数据，缺失值会被标记为`NaN`。
- en: '![Figure 2.6: Code for using NumPy to test for quality](img/C13018_02_06.jpg)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.6：使用 NumPy 测试质量的代码](img/C13018_02_06.jpg)'
- en: 'Figure 2.6: Code for using NumPy to test for quality'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.6：使用 NumPy 测试质量的代码
- en: You may have noticed that our most recently merged table has duplicated data
    in the first few rows. This will be addressed in the next step.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可能已经注意到我们最近合并的表格在前几行有重复的数据。这个问题将在下一步解决。
- en: 'Run the cell containing `df.drop_duplicates()` to return a version of the DataFrame
    with no duplicate rows:![Figure 2.7: Table with dropped duplicate rows](img/C13018_02_07.jpg)'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行包含`df.drop_duplicates()`的单元，返回一个没有重复行的 DataFrame 版本：![图 2.7：删除重复行后的表格](img/C13018_02_07.jpg)
- en: 'Figure 2.7: Table with dropped duplicate rows'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.7：删除重复行后的表格
- en: This is the easiest and "standard" way to drop duplicate rows. To apply these
    changes to `df`, we can either set `inplace=True` or do something like `df = df.drop_duplicated()`.
    Let's see another method, which uses masking to select or drop duplicate rows.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是删除重复行的最简单和“标准”方法。为了将这些更改应用到`df`，我们可以设置`inplace=True`，或者像`df = df.drop_duplicated()`这样操作。接下来，我们来看另一种方法，它使用掩码来选择或删除重复行。
- en: 'Run the cell containing `df.duplicated()` to print the True/False series, marking
    duplicate rows:![Figure 2.8: Printing True/False values for duplicate rows](img/C13018_02_08.jpg)'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行包含 `df.duplicated()` 的单元格以打印 True/False 序列，标记重复行：![图 2.8：打印重复行的 True/False
    值](img/C13018_02_08.jpg)
- en: 'Figure 2.8: Printing True/False values for duplicate rows'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.8：打印重复行的 True/False 值
- en: 'Sum the result to determine how many rows have been duplicated by running the
    following code:![Figure 2.9: Summing the result to check the number of duplicate
    rows](img/C13018_02_09.jpg)'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果相加，以确定通过运行以下代码检查重复行的数量：![图 2.9：汇总结果以检查重复行的数量](img/C13018_02_09.jpg)
- en: 'Figure 2.9: Summing the result to check the number of duplicate rows'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.9：汇总结果以检查重复行的数量
- en: 'Run the following code and convince yourself the output is the same as that
    from `df.drop_duplicates()`:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码并确认输出与 `df.drop_duplicates()` 的输出相同：
- en: '[PRE1]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Figure 2.10: Output from the df.[~df.duplicated()] function](img/C13018_02_10.jpg)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.10：来自 df.[~df.duplicated()] 函数的输出](img/C13018_02_10.jpg)'
- en: 'Figure 2.10: Output from the df.[~df.duplicated()] function'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.10：来自 df.[~df.duplicated()] 函数的输出
- en: 'Run the cell containing the following code to drop duplicates from a subset
    of the full DataFrame:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行包含以下代码的单元格，从完整的 DataFrame 中删除重复项：
- en: '[PRE2]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Figure 2.11: Output after dropping duplicates](img/C13018_02_11.jpg)'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.11：删除重复项后的输出](img/C13018_02_11.jpg)'
- en: 'Figure 2.11: Output after dropping duplicates'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.11：删除重复项后的输出
- en: 'Here, we are doing the following things:'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们做了以下操作：
- en: creating a mask (a True/False series) for the product row, where duplicates
    are marked with `True`;
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为产品行创建掩码（一个 True/False 序列），其中重复项标记为 `True`；
- en: using the tilde (~) to take the opposite of that mask, so that duplicates are
    instead marked with False and everything else is `True`;
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用波浪线（~）来取反该掩码，以便将重复项标记为False，其他所有内容标记为`True`；
- en: using that mask to filter out the `False` rows of `df`, which correspond to
    the duplicated products.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用该掩码来筛选出 `False` 行，即对应于重复产品的 `df` 行。
- en: As expected, we now see that only the first `df` with a deduplicated version
    of itself. This can be done by running `drop_duplicates` and passing the parameter
    `inplace=True`.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们现在看到只有第一个 `df`，它是去重后的版本。这可以通过运行 `drop_duplicates` 并传递参数 `inplace=True`
    来实现。
- en: 'Deduplicate the DataFrame and save the result by running the cell containing
    the following code:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 DataFrame 进行去重并保存结果，方法是运行包含以下代码的单元格：
- en: '[PRE3]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Continuing on to other preprocessing methods, let's ignore the duplicated rows and
    first deal with the missing data. This is necessary because models cannot be trained
    on incomplete samples. Using the missing price data for blue pants and white tuxedo
    as an example, let's show some different options for handling `NaN` values.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 继续其他预处理方法，我们先忽略重复行，处理缺失数据。这是必要的，因为模型不能在不完整的样本上进行训练。以蓝色裤子和白色晚礼服的缺失价格数据为例，展示几种处理
    `NaN` 值的不同方法。
- en: 'Drop rows, especially if your `NaN` samples are missing data, by running the
    cell containing `df.dropna()`:![Figure 2.12: Output after dropping incomplete
    rows](img/C13018_02_12.jpg)'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除行，尤其是在 `NaN` 样本缺失数据的情况下，通过运行包含 `df.dropna()` 的单元格：![图 2.12：删除不完整行后的输出](img/C13018_02_12.jpg)
- en: 'Figure 2.12: Output after dropping incomplete rows'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.12：删除不完整行后的输出
- en: 'Drop entire columns that have most values missing for a feature. Do this by
    running the cell containing the same method as before, but this time with the
    axes parameter passed to indicate columns instead of rows:![Figure 2.13: Output
    after dropping entire columns with missing values for a feature](img/C13018_02_13.jpg)'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除具有大部分缺失值的整个列。通过运行包含与之前相同的方法的单元格，但这次传递 `axes` 参数以指示列而不是行来实现：![图 2.13：删除缺失值特征列后的输出](img/C13018_02_13.jpg)
- en: 'Figure 2.13: Output after dropping entire columns with missing values for a
    feature'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.13：删除缺失值特征列后的输出
- en: Simply dropping the `NaN` values is usually not the best option, because losing
    data is never good, especially if only a small fraction of the sample values is
    missing. Pandas offers a method for filling in `NaN` entries in a variety of different
    ways, some of which we'll illustrate now.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单地删除 `NaN` 值通常不是最佳选择，因为丢失数据从来不是好事，特别是当只有少部分样本值丢失时。Pandas 提供了一种方法，可以通过多种不同方式填充
    `NaN` 条目，下面将展示其中的一些方法。
- en: 'Run the cell containing `df.fillna?` to print the docstring for the Pandas
    `NaN-fill` method:![Figure 2.14: Docstring for the NaN-fill method](img/C13018_02_14.jpg)'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行包含 `df.fillna?` 的单元格，打印 Pandas `NaN-fill` 方法的文档字符串：![图 2.14：NaN 填充方法的文档字符串](img/C13018_02_14.jpg)
- en: 'Figure 2.14: Docstring for the NaN-fill method'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.14：NaN 填充方法的文档字符串
- en: Note the options for the value parameter; this could be, for example, a single
    value or a dictionary/series type map based on index. Alternatively, we can leave
    the value as `None` and pass a `fill` method instead. We'll see examples of each
    in this chapter.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意值参数的选项；这可以是例如单个值或基于索引的字典/系列类型映射。或者，我们可以将值保留为 `None`，并传递一个 `fill` 方法代替。在本章中我们将看到每种方法的示例。
- en: 'Fill in the missing data with the average product price by running the cell
    containing the following code:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，用平均产品价格填充缺失数据：
- en: '[PRE4]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure 2.15: Output after filling missing data with average product price](img/C13018_02_15.jpg)'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.15：使用平均产品价格填充缺失数据后的输出](img/C13018_02_15.jpg)'
- en: 'Figure 2.15: Output after filling missing data with average product price'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.15：使用平均产品价格填充缺失数据后的输出
- en: 'Fill in the missing data using the pad method by running the cell containing
    the following code instead:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，使用填充方法填充缺失数据：
- en: '[PRE5]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Figure 2.16: Output after filling data using the pad method](img/C13018_02_16.jpg)'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.16：使用填充方法填充数据后的输出](img/C13018_02_16.jpg)'
- en: 'Figure 2.16: Output after filling data using the pad method'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.16：使用填充方法填充数据后的输出
- en: Notice how the **white dress** price was used to pad the missing values below
    it.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意**白色裙子**的价格被用来填充下面缺失的值。
- en: To conclude this exercise, we will prepare our simple table to be used for training
    a machine learning algorithm. Don't worry, we won't actually try to train any
    models on such a small dataset! We start this process by encoding the class labels
    for the categorical data.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了完成本练习，我们将准备一个简单的表格，用于训练机器学习算法。别担心，我们不会在这么小的数据集上训练模型！我们从为类别数据编码类标签开始这一过程。
- en: 'Run the first cell in the `Building training data sets` section to add another
    column of data representing the average product ratings before encoding the labels:![Figure
    2.17: Output after adding the rating column](img/C13018_02_17.jpg)'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `Building training data sets` 部分中的第一个单元格，添加另一列数据，表示编码标签之前的平均产品评分：![图 2.17：添加评分列后的输出](img/C13018_02_17.jpg)
- en: 'Figure 2.17: Output after adding the rating column'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.17：添加评分列后的输出
- en: Considering we want to use this table to train a predictive model, we should
    first think about changing all the variables to numeric types.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑到我们想用这个表格来训练预测模型，我们首先应该考虑将所有变量转换为数值类型。
- en: 'Convert the handle `in_stock`., which is a Boolean list, to numeric values;
    for example, `0` and `1`. This should be done before using it to train a predictive
    model. This can be done in many ways, for example, by running the cell containing
    the following code:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `in_stock`（一个布尔值列表）转换为数字值；例如，`0` 和 `1`。在用它训练预测模型之前，应该进行此转换。这可以通过许多方法实现，例如，运行包含以下代码的单元格：
- en: '[PRE6]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Figure 2.18: Output after converting in_stock to binary](img/C13018_02_18.jpg)'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.18：将 in_stock 转换为二进制后的输出](img/C13018_02_18.jpg)'
- en: 'Figure 2.18: Output after converting in_stock to binary'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.18：将 in_stock 转换为二进制后的输出
- en: 'Run the cell containing the following code to map class labels to integers
    at a higher level. We use sci-kit learn''s `LabelEncoder` for this purpose:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行包含以下代码的单元格，在更高层次上将类标签映射为整数。我们使用 sci-kit learn 的 `LabelEncoder` 来实现这一目的：
- en: '[PRE7]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 2.19: Output after mapping class labels to integers](img/C13018_02_19_.jpg)'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.19：将类标签映射到整数后的输出](img/C13018_02_19_.jpg)'
- en: 'Figure 2.19: Output after mapping class labels to integers'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.19：将类标签映射到整数后的输出
- en: This might bring to mind the preprocessing we did in the previous chapter, when
    building the polynomial model. Here, we instantiate a label encoder and then "train"
    it and "transform" our data using the `fit_transform` method. We apply the result
    to a copy of our DataFrame, `_df`.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可能会让你想起我们在上一章中做的预处理，当时我们构建了多项式模型。在这里，我们实例化了一个标签编码器，然后使用 `fit_transform` 方法对其进行“训练”和“转换”数据。我们将结果应用到我们的
    DataFrame 副本 `_df` 上。
- en: 'Re-convert the features using the class we reference with the variable `rating_encoder`,
    by running `rating_encoder.inverse_ transform(df.rating)`:![Figure 2.20: Output
    after performing inverse transform](img/C13018_02_20.jpg)'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行 `rating_encoder.inverse_transform(df.rating)`，使用我们通过变量 `rating_encoder`
    引用的类重新转换特征：![图 2.20：执行逆变换后的输出](img/C13018_02_20.jpg)
- en: 'Figure 2.20: Output after performing inverse transform'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.20：执行逆变换后的输出
- en: You may notice a problem here. We are working with a so-called "ordinal" feature,
    where there's an inherent order to the labels. In this case, we should expect
    that a rating of "low" would be encoded with a 0 and a rating of "high" would
    be encoded with a 2\. However, this is not the result we see. In order to achieve
    proper ordinal label encoding, we should again use map, and build the dictionary
    ourselves.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可能会注意到一个问题。我们正在处理一个所谓的“顺序”特征，其中标签有固有的顺序。在这种情况下，我们应该期望“low”评级会被编码为0，而“high”评级会被编码为2。然而，这不是我们看到的结果。为了实现正确的顺序标签编码，我们应该再次使用`map`，并自己构建字典。
- en: 'Encode the ordinal labels properly by running the cell containing the following
    code:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，正确地对顺序标签进行编码：
- en: '[PRE8]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Figure 2.21: Output after encoding ordinal labels](img/C13018_02_21.jpg)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.21：编码顺序标签后的输出](img/C13018_02_21.jpg)'
- en: 'Figure 2.21: Output after encoding ordinal labels'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.21：编码顺序标签后的输出
- en: We first create the mapping dictionary. This is done using a dictionary comprehension
    and enumeration, but looking at the result, we see that it could just as easily
    be defined manually instead. Then, as done earlier for the `in_stock` column,
    we apply the dictionary mapping to the feature. Looking at the result, we see
    that rating now makes more sense than before, where `low` is labeled with `0`,
    `medium` with `1`, and `high` with `2`.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们首先创建映射字典。这是通过字典推导和枚举来完成的，但从结果来看，我们也可以直接手动定义。然后，像之前对`in_stock`列进行的操作一样，我们将字典映射应用到特征上。查看结果后，我们发现评分现在比之前更合理，其中`low`被标记为`0`，`medium`为`1`，`high`为`2`。
- en: Now that you've discussed ordinal features, let's touch on another type called
    nominal features. These are fields with no inherent order, and in our case, we
    see that `product` is a perfect example.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，你已经讨论了顺序特征，接下来我们将简要提及另一种特征，称为名义特征。这些字段没有固有的顺序，在我们的案例中，`product`就是一个完美的例子。
- en: Most scikit-learn models can be trained on data like this, where we have strings
    instead of integer-encoded labels. In this situation, the necessary conversions
    are done under the hood. However, this may not be the case for all models in scikit-learn,
    or other machine learning and deep learning libraries. Therefore, it's good practice
    to encode these ourselves during preprocessing.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数scikit-learn模型可以在这种数据上进行训练，其中我们使用字符串而非整数编码的标签。在这种情况下，必要的转换会在后台自动完成。然而，这并不适用于所有scikit-learn模型，或其他机器学习和深度学习库。因此，在预处理阶段我们自己进行编码是一个好的做法。
- en: 'Convert the class labels from strings to numerical values by running the cell
    containing the following code:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，将类别标签从字符串转换为数值：
- en: '[PRE9]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The final DataFrame then looks as follows:'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终的DataFrame如下所示：
- en: '![Figure 2.22: Final DataFrame](img/C13018_02_201.jpg)'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.22：最终的DataFrame](img/C13018_02_201.jpg)'
- en: 'Figure 2.22: Final DataFrame'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.22：最终的DataFrame
- en: 'Here, we see the result of one-hot encoding: the `product` column has been
    split into 4, one for each unique value. Within each column, we find either a
    `1` or `0` representing whether that row contains the particular value or product.'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，我们看到的是独热编码的结果：`product`列被拆分为4列，每个列代表一个唯一的值。在每一列中，我们看到`1`或`0`，表示该行是否包含特定值或产品。
- en: Moving on and ignoring any data scaling (which should usually be done), the
    final step is to split the data into training and test sets to use for machine
    learning. This can be done using scikit-learn's `train_test_split`. Let's assume
    we are going to try to predict whether an item is in stock, given the other feature
    values.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，忽略任何数据缩放（通常应该进行缩放），最后一步是将数据拆分为训练集和测试集，以便用于机器学习。这可以通过scikit-learn的`train_test_split`来完成。假设我们要尝试预测某个商品是否有库存，前提是其他特征值已知。
- en: Note
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: When we call the values attribute in the preceding code, we are converting the
    Pandas series (that is, the DataFrame column) into a NumPy array. This is good
    practice because it strips out unnecessary information from the series object,
    such as the index and name.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们调用前面代码中的`values`属性时，我们将Pandas系列（即DataFrame列）转换为NumPy数组。这是一个好的做法，因为它去除了系列对象中不必要的信息，例如索引和名称。
- en: 'Split the data into training and test sets by running the cell containing the
    following code:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，将数据拆分为训练集和测试集：
- en: '[PRE10]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Figure 2.23: Splitting data intro training and test sets](img/C13018_02_23.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.23：将数据拆分为训练集和测试集](img/C13018_02_23.jpg)'
- en: 'Figure 2.23: Splitting data intro training and test sets'
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.23：将数据划分为训练集和测试集
- en: Here, we are selecting subsets of the data and feeding them into the `train_test_split`
    function. This function has four outputs, which are unpacked into the training
    and testing splits for features (`X`) and the target (`y`).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择数据的子集，并将它们传入`train_test_split`函数。该函数有四个输出，这些输出被拆分成特征（`X`）和目标（`y`）的训练集和测试集。
- en: Observe the shape of the output data, where the test set has roughly 30% of
    the samples and the training set has roughly 70%.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 观察输出数据的形状，其中测试集大约占样本的30%，训练集大约占70%。
- en: We'll see similar code blocks later, when preparing real data to use for training
    predictive models.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后我们会看到类似的代码块，用于准备真实数据以进行预测模型的训练。
- en: This concludes the training exercise on cleaning data for use in machine learning
    applications. Let's take a minute to note how effective our Jupyter Notebook was
    for testing various methods of transforming the data, and ultimately documenting
    the pipeline we decided upon. This could easily be applied to an updated version
    of the data by altering only specific cells of code, prior to processing. Also,
    should we desire any changes to the processing, these can easily be tested in
    the notebook, and specific cells may be changed to accommodate the alterations.
    The best way to achieve this would probably be to copy the notebook over to a
    new file, so that we can always keep a copy of the original analysis for reference.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 本次训练练习已结束，主题是清理数据以供机器学习应用使用。我们稍作停留，来注意一下我们的Jupyter Notebook在测试各种数据转换方法时的有效性，并最终记录我们决定使用的数据处理流程。通过仅修改特定的代码单元格，就可以轻松应用到更新版的数据，在处理之前进行调整。此外，如果我们希望对数据处理做出任何更改，也可以轻松地在Notebook中测试这些更改，并可以修改特定单元格以适应调整。实现这一点的最好方法可能是将Notebook复制到新文件中，以便我们始终保留原始分析的副本作为参考。
- en: Moving on to an activity, we'll now apply the concepts from this section to
    a large dataset as we prepare it for use in training predictive models.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将应用本节中的概念，将它们应用于一个大型数据集，为训练预测模型做准备。
- en: 'Activity 2: Preparing to Train a Predictive Model for the Employee-Retention
    Problem'
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 2：准备为员工留任问题训练预测模型
- en: Suppose you are hired to do freelance work for a company who wants to find insights
    into why their employees are leaving. They have compiled a set of data they think
    will be helpful in this respect. It includes details on employee satisfaction
    levels, evaluations, time spent at work, department, and salary.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你受雇为一家公司做自由职业工作，帮助他们分析员工离职的原因。他们已经收集了一些数据，认为这些数据对分析离职原因有帮助。这些数据包括员工的满意度、评估、工作时间、部门和薪资等信息。
- en: The company shares their data with you by sending you a file called hr_data.csv
    and asking what you think can be done to help stop employees from leaving.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 该公司通过发送名为hr_data.csv的文件与你共享他们的数据，并请你提出解决方案，以帮助阻止员工离职。
- en: Our aim is to
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是
- en: 'apply the concepts you''ve learned thus far to a real-life problem. In particular,
    we seek to:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 将你到目前为止学到的概念应用到实际问题中。特别是，我们希望：
- en: Determine a plan for using predictive analytics to provide impactful business
    insights, given the available data.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 制定一个计划，利用预测分析为业务提供有影响力的洞察，基于现有的数据。
- en: Prepare the data for use in machine learning models.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备数据以供机器学习模型使用。
- en: Note
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Starting with this activity and continuing through the remainder of this chapter,
    we''ll be using Human Resources Analytics dataset, which is a Kaggle dataset.
    The link to the dataset can be found here: [https://bit.ly/2OXWFUs](https://bit.ly/2OXWFUs).
    The data is simulated, meaning the samples are artificially generated and do not
    represent real people. We''ll ignore this fact as we analyze and model the data.
    There is a small difference between the dataset we use in this book and the online
    version. Our human resource analytics data contains some NaN values. These were
    manually removed from the online version of the dataset, for the purposes of illustrating
    data cleaning techniques. We have also added a column of data called is_smoker,
    for the same purposes.'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从本活动开始，直到本章结束，我们将使用人力资源分析数据集，这是一个Kaggle数据集。数据集的链接可以在这里找到：[https://bit.ly/2OXWFUs](https://bit.ly/2OXWFUs)。这些数据是模拟数据，意味着样本是人工生成的，并不代表真实的人。我们在分析和建模数据时将忽略这一点。我们使用的数据集与在线版本存在一些小差异。我们的HR分析数据集包含一些NaN值。这些值已经在在线版本中手动删除，以说明数据清理技术。我们还为同样的目的，添加了一列名为is_smoker的数据。
- en: 'In order to achieve this, following steps have to be executed:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，需要执行以下步骤：
- en: Scroll to the `Activity A` section of the `lesson-2-workbook.ipynb` notebook
    file.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到`lesson-2-workbook.ipynb`笔记本文件中的`Activity A`部分。
- en: Check the head of the table to verify that it is in standard CSV format.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查表格的头部，确认它是标准的CSV格式。
- en: Load the data with Pandas.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Pandas加载数据。
- en: 'Inspect the columns by printing `df.columns` and make sure the data has loaded
    as expected by printing the DataFrame `head` and `tail` with `df.head()` and `df.tail()`:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过打印`df.columns`检查列，并通过打印DataFrame的`head`和`tail`（使用`df.head()`和`df.tail()`）来确保数据已按预期加载：
- en: Check the number of rows (including the header) in the CSV file.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查CSV文件中的行数（包括标题）。
- en: 'Compare this result to `len(df)` to make sure we''ve loaded all the data:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此结果与`len(df)`进行比较，确保我们已经加载了所有数据：
- en: Assess the target variable and check the distribution and number of missing entries.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估目标变量并检查缺失值的分布和数量。
- en: Print the data type of each feature.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印每个特征的数据类型。
- en: Display the feature distributions.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示特征分布。
- en: 'Check how many `NaN` values are in each column by running the following code:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码检查每一列中的`NaN`值数量：
- en: Drop the `is_smoker` column as there is barely any information in this metric.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除`is_smoker`列，因为该指标几乎没有任何信息。
- en: Fill the `NaN` values in the `time_spend_company` column.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填充`time_spend_company`列中的`NaN`值。
- en: Make a boxplot of `average_montly_hours` segmented by `number_project`.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制`average_montly_hours`按`number_project`分段的箱型图。
- en: 'Calculate the mean of each group by running the following code:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码计算每组的均值：
- en: Fill the `NaN` values in `average_montly_hours`.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填充`average_montly_hours`中的`NaN`值。
- en: Confirm that `df` has no more `NaN` values by running the assertion test.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行断言测试，确认`df`中不再有`NaN`值。
- en: Transform the string and Boolean fields into integer representations.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将字符串和布尔字段转换为整数表示。
- en: Print `df.columns` to show the fields
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`df.columns`以显示字段
- en: Save our preprocessed data.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存我们预处理后的数据。
- en: Note
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The detailed steps along with the solutions are presented in the *Appendix
    A*     (pg. no. 150).'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 详细步骤和解决方案在*附录A*（第150页）中列出。
- en: Again, we pause here to note how well the Jupyter Notebook suited our needs
    when performing this initial data analysis and clean-up. Imagine, for example,
    we left this project in its current state for a few months. Upon returning to
    it, we would probably not remember what exactly was going on when we left it.
    Referring back to this notebook though, we would be able to retrace our steps
    and quickly recall what we previously learned about the data. Furthermore, we
    could update the data source with any new data and re-run the notebook to prepare
    the new set of data for use in our machine learning algorithms. Recall that in
    this situation, it would be best to make a copy of the notebook first, so as not
    to lose the initial analysis.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们在这里暂停，注意到在执行初步数据分析和清理时，Jupyter Notebook非常适合我们的需求。例如，假设我们将这个项目暂时搁置几个月。返回时，我们可能不记得当时到底发生了什么。但如果查看这个笔记本，我们可以重新追溯我们的步骤，并快速回忆起我们之前对数据的理解。此外，我们可以用任何新数据更新数据源，并重新运行笔记本，为机器学习算法准备新的数据集。请记住，在这种情况下，最好先复制一份笔记本，以免丢失初始分析。
- en: 'To summarize, you''ve learned and applied methods for preparing to train a
    machine learning model. We started by discussing steps for identifying a problem
    that can be solved with predictive analytics. This consisted of:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，你已经学习并应用了为训练机器学习模型做准备的方法。我们从讨论识别可以通过预测分析解决的问题的步骤开始。这个过程包括：
- en: Looking at the available data
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看可用数据
- en: Determining the business needs
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定业务需求
- en: Assessing the data for suitability
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估数据的适用性
- en: We also discussed how to identify supervised versus unsupervised and regression
    versus classification problems.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了如何区分监督学习与无监督学习，以及回归问题与分类问题。
- en: After identifying our problem, we learned techniques for using Jupyter Notebooks
    to build and test a data transformation pipeline. These techniques included methods
    and best practices for filling missing data, transforming categorical features,
    and building train/test data sets.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定问题后，我们学习了使用Jupyter Notebooks构建和测试数据转换管道的技术。这些技术包括填充缺失数据、转换分类特征以及构建训练/测试数据集的方法和最佳实践。
- en: In the remainder of this chapter, we will use this preprocessed data to train
    a variety of classification models. To avoid blindly applying algorithms we don't
    understand, we start by introducing them and overviewing how they work. Then,
    we use Jupyter to train and compare their predictive capabilities. Here, we have
    the opportunity to discuss more advanced topics in machine learning like overfitting,
    k-fold cross-validation, and validation curves.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将使用这些预处理后的数据来训练各种分类模型。为了避免盲目应用我们不了解的算法，我们首先介绍它们并概述它们的工作原理。然后，我们使用Jupyter训练并比较它们的预测能力。在这里，我们有机会讨论机器学习中的更高级话题，比如过拟合、k折交叉验证和验证曲线。
- en: Training Classification Models
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练分类模型
- en: As you've already seen in the previous chapter, using libraries such as scikit-learn
    and platforms such as Jupyter, predictive models can be trained in just a few
    lines of code. This is possible by abstracting away the difficult computations
    involved with optimizing model parameters. In other words, we deal with a black
    box where the internal operations are hidden instead. With this simplicity also
    comes the danger of misusing algorithms, for example, by overfitting during training
    or failing to properly test on unseen data. We'll show how to avoid these pitfalls
    while training classification models and produce trustworthy results with the
    use of k-fold cross validation and validation curves.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在上一章中已经看到的，通过使用像scikit-learn这样的库和像Jupyter这样的平台，可以用几行代码训练预测模型。这之所以可能，是因为它将优化模型参数所涉及的复杂计算进行了抽象。换句话说，我们处理的是一个“黑箱”，其内部操作是隐藏的。然而，这种简化也带来了滥用算法的风险，例如，在训练过程中出现过拟合，或者未能在未见数据上进行适当的测试。我们将展示如何在训练分类模型时避免这些陷阱，并通过使用k折交叉验证和验证曲线来生成可靠的结果。
- en: Introduction to Classification Algorithms
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类算法简介
- en: 'Recall the two types of supervised machine learning: regression and classification.
    In regression, we predict a continuous target variable. For example, recall the
    linear and polynomial models from the first chapter. In this chapter, we focus
    on the other type of supervised machine learning: classification. Here, the goal
    is to predict the class of a sample using the available metrics.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下监督学习的两种类型：回归和分类。在回归中，我们预测一个连续的目标变量。例如，回想一下第一章中的线性和多项式模型。在本章中，我们关注的是另一种监督学习类型：分类。这里的目标是使用可用的度量来预测样本的类别。
- en: In the simplest case, there are only two possible classes, which means we are
    doing binary classification. This is the case for the example problem in this
    chapter, where we try to predict whether an employee has left or not. If we have
    more than two class labels instead, we are doing multi-class classification.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的情况下，只有两个可能的类别，这意味着我们正在进行二分类。我们在本章的示例问题中就是这种情况，我们尝试预测一个员工是否已经离职。如果我们有两个以上的类别标签，那么我们就是在进行多分类。
- en: Although there is little difference between binary and multi-class classification
    when training models with scikit-learn, what's done inside the "black box" is
    notably different. In particular, multi-class classification models often use
    the one-versus-rest method. This works as follows for a case with three class
    labels. When the model is "fit" with the data, three models are trained, and each
    model predicts whether the sample is part of an individual class or part of some
    other class. This might bring to mind the one-hot encoding for features that we
    did earlier. When a prediction is made for a sample, the class label with the
    highest confidence level is returned.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在使用scikit-learn训练模型时，二分类和多分类之间几乎没有区别，但在“黑箱”内部所做的工作却有显著不同。特别是，多分类模型通常使用一对多的方法。这对于具有三个类别标签的情况是这样工作的。当模型用数据进行“拟合”时，会训练三个模型，每个模型预测样本是否属于某个特定类别或属于其他类别。这可能让你想起我们之前为特征做的一次性编码。当为某个样本做出预测时，将返回具有最高置信度的类别标签。
- en: 'In this chapter, we''ll train three types of classification models: Support
    Vector Machines, Random Forests, and k-Nearest Neighbors classifiers. Each of
    these algorithms are quite different. As we will see, however, they are quite
    similar to train and use for predictions thanks to scikit-learn. Before swapping
    over to the Jupyter Notebook and implementing these, we''ll briefly see how they
    work.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将训练三种分类模型：支持向量机、随机森林和k最近邻分类器。这些算法各自有很大的不同。然而，正如我们将看到的，由于scikit-learn，它们在训练和用于预测时非常相似。在切换到Jupyter
    Notebook并实现这些模型之前，我们将简要了解它们是如何工作的。
- en: SVMs attempt to find the best hyperplane to divide classes by. This is done
    by maximizing the distance between the hyperplane and the closest samples of each
    class, which are called support vectors.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）尝试找到最佳的超平面来划分类别。这是通过最大化超平面与每个类别中最接近样本之间的距离来实现的，这些最接近的样本被称为支持向量。
- en: This linear method can also be used to model nonlinear classes using the kernel
    trick. This method maps the features into a higher-dimensional space in which
    the hyperplane is determined. This hyperplane is also referred to as the decision
    surface, and we'll visualize it when training our models.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这种线性方法也可以通过核技巧用于建模非线性类别。该方法将特征映射到更高维的空间中，在其中确定超平面。这个超平面也被称为决策面，我们将在训练模型时将其可视化。
- en: K-Nearest Neighbors classification algorithms memorize the training data and
    make predictions depending on the K nearest samples in the feature space. With
    three features, this can be visualized as a sphere surrounding the prediction
    sample. Often, however, we are dealing with more than three features and therefore
    hyperspheres are drawn to find the closest K samples.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: K最近邻分类算法记住训练数据，并根据特征空间中K个最近的样本进行预测。对于三个特征，这可以被可视化为围绕预测样本的一个球体。然而，通常我们处理的是超过三个特征的数据，因此会绘制超球体来找到最近的K个样本。
- en: Random Forests are an ensemble of decision trees, where each has been trained
    on different subsets of the training data.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一组决策树，每棵树都在不同的训练数据子集上进行训练。
- en: A decision tree algorithm classifies a sample based on a series of decisions.
    For example, the first decision might be "if feature x_1 is less than or greater
    than 0." The data would then be split on this condition and fed into descending
    branches of the tree. Each step in the decision tree is decided based on the feature
    split that maximizes the information gain. Essentially, this term describes the
    mathematics that attempts to pick the best possible split of the target variable.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法根据一系列决策对样本进行分类。例如，第一个决策可能是“如果特征x_1小于或大于0”。然后，数据会根据这个条件进行划分，并被输入到树的下级分支中。决策树中的每一步都是基于特征划分来决定的，这一划分最大化信息增益。实际上，这个术语描述了试图选择目标变量最佳划分的数学方法。
- en: Training a Random Forest consists of creating bootstrapped (that is, randomly
    sampled data with replacement) datasets for a set of decision trees. Predictions
    are then made based on the majority vote. These have the benefit of less overfitting
    and better generalizability.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 训练随机森林的过程包括为一组决策树创建自助采样（即带有替换的随机采样）数据集。然后，根据多数投票做出预测。这些模型的优势在于减少了过拟合，并且具有更好的泛化能力。
- en: Note
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Decision trees can be used to model a mix of continuous and categorical data,
    which make them very useful. Furthermore, as we will see later in this chapter,
    the tree depth can be limited to reduce overfitting. For a detailed (but brief)
    look into the decision tree algorithm, check out this popular StackOverflow answer:
    [https://stackoverflow.com/a/1859910/3511819](https://stackoverflow.com/a/1859910/3511819).
    There, the author shows a simple example and discusses concepts such as node purity,
    information gain, and entropy.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以用于建模连续数据和类别数据的混合，这使得它们非常有用。此外，正如我们将在本章稍后看到的，可以限制树的深度以减少过拟合。要详细（但简洁）了解决策树算法，请查看这个流行的StackOverflow回答：[https://stackoverflow.com/a/1859910/3511819](https://stackoverflow.com/a/1859910/3511819)。在那里，作者展示了一个简单的示例，并讨论了节点纯度、信息增益和熵等概念。
- en: 'Exercise 9: Training Two-Feature Classification Models With Scikit-learn'
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 9：使用Scikit-learn训练两特征分类模型
- en: 'We''ll continue working on the employee retention problem that we introduced
    in the first topic. We previously prepared a dataset for training a classification
    model, in which we predicted whether an employee has left or not. Now, we''ll
    take that data and use it to train classification models:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续处理在第一个主题中介绍的员工留存问题。我们之前准备了一个数据集，用于训练分类模型，预测员工是否已经离职。现在，我们将利用这些数据来训练分类模型：
- en: 'Start the `NotebookApp` and open the `lesson-2-workbook.ipynb` file. Scroll
    down to `Topic B: Training classification models`. Run the first couple of cells
    to set the default figure size and load the processed data that we previously
    saved to a CSV file. For this example, we''ll be training classification models
    on two continuous features: `satisfaction_level and last_evaluation`.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '启动 `NotebookApp` 并打开 `lesson-2-workbook.ipynb` 文件。向下滚动到 `Topic B: Training
    classification models` 部分。运行前几个单元格以设置默认图形大小并加载之前保存到 CSV 文件的处理过的数据。在这个示例中，我们将在两个连续特征上训练分类模型：`satisfaction_level
    和 last_evaluation`。'
- en: 'Draw the bivariate and univariate graphs of the continuous target variables
    by running the cell with the following code:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元，绘制连续目标变量的双变量和单变量图：
- en: '[PRE11]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Figure 2.24: Bivariate and univariate graphs for satisfaction_level and last_evaluation](img/C13018_02_24.jpg)'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.24：满意度和最后评估的双变量与单变量图](img/C13018_02_24.jpg)'
- en: 'Figure 2.24: Bivariate and univariate graphs for satisfaction_level and last_evaluation'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.24：满意度和最后评估的双变量与单变量图
- en: As you can see in the preceding image, there are some very distinct patterns
    in the data.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如你在前面的图像中看到的，数据中有一些非常明显的模式。
- en: 'Re-plot the bivariate distribution, segmenting on the target variable, by running
    the cell containing the following code:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元，重新绘制按目标变量分段的双变量分布：
- en: '[PRE12]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Figure 2.25: Bivariate distribution for satisfaction_level and last_evaluation](img/C13018_02_25.jpg)'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.25：满意度和最后评估的双变量分布](img/C13018_02_25.jpg)'
- en: 'Figure 2.25: Bivariate distribution for satisfaction_level and last_evaluation'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.25：满意度和最后评估的双变量分布
- en: Now, we can see how the patterns are related to the target variable. For the
    remainder of this exercise, we'll try to exploit these patterns to train effective
    classification models.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们可以看到这些模式如何与目标变量相关。在本次练习的剩余部分，我们将尝试利用这些模式来训练有效的分类模型。
- en: 'Split the data into training and test sets by running the cell containing the
    following code:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元，将数据分割为训练集和测试集：
- en: '[PRE13]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Our first two models, the Support Vector Machine and k-Nearest Neighbors algorithm,
    are most effective when the input data is scaled so that all of the features are
    on the same order. We'll accomplish this with scikit-learn's `StandardScaler`.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的前两个模型——支持向量机和 k 最近邻算法，在输入数据经过缩放，使所有特征处于相同量级时效果最好。我们将使用 scikit-learn 的 `StandardScaler`
    来实现这一点。
- en: 'Load `StandardScaler` and create a new instance, as referenced by the scaler
    variable. Fit the scaler on the training set and transform it. Then, transform
    the test set. Run the cell containing the following code:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 `StandardScaler` 并创建一个新的实例，如缩放器变量所示。对训练集进行拟合并转换，然后转换测试集。运行包含以下代码的单元：
- en: '[PRE14]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: An easy mistake to make when doing machine learning is to "fit" the scaler on
    the whole dataset, when in fact it should only be "fit" to the training data.
    For example, scaling the data before splitting into training and testing sets
    is a mistake. We don't want this because the model training should not be influenced
    in any way by the test data.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在进行机器学习时，一个常见的错误是将缩放器（scaler）“拟合”到整个数据集，而实际上它应该只对训练数据进行“拟合”。例如，在将数据分割为训练集和测试集之前进行缩放就是一个错误。我们不希望这样做，因为模型训练不应该受到测试数据的任何影响。
- en: 'Import the scikit-learn support vector machine class and fit the model on the
    training data by running the cell containing the following code:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 scikit-learn 的支持向量机类，并通过运行包含以下代码的单元，在训练数据上拟合模型：
- en: '[PRE15]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Compute the accuracy of this model on unseen data by running the cell containing
    the following code:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元，计算该模型在未见数据上的准确度：
- en: '[PRE16]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We predict the targets for our test samples and then use scikit-learn's `accuracy_score`
    function to determine the accuracy. The result looks promising at ~75%! Not bad
    for our first model. Recall, though, the target is imbalanced. Let's see how accurate
    the predictions are for each class.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们预测测试样本的目标值，然后使用 scikit-learn 的 `accuracy_score` 函数来确定准确率。结果看起来很有希望，大约是 75%！对于我们的第一个模型来说，还不错。但请记住，目标是失衡的。让我们看看每个类别的预测准确度如何。
- en: 'Calculate the confusion matrix and then determine the accuracy within each
    class by running the cell containing the following code:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算混淆矩阵，然后通过运行包含以下代码的单元来确定每个类别中的准确度：
- en: '[PRE17]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It looks like the model is simply classifying every sample as 0, which is clearly not
    helpful at all. Let's use a contour plot to show the predicted class at each point
    in the feature space. This is commonly known as the decision- regions plot.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 看起来模型仅仅将每个样本分类为 0，这显然是完全没有帮助的。让我们使用等高线图来显示特征空间中每个点的预测类别。这通常被称为决策区域图。
- en: 'Plot the decision regions using a helpful function from the `mlxtend` library.
    Run the cell containing the following code:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `mlxtend` 库中的有用函数绘制决策区域。运行包含以下代码的单元：
- en: '[PRE18]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Figure 2.26: Plot of the decision regions](img/C13018_02_26.jpg)'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.26: 决策区域的绘图](img/C13018_02_26.jpg)'
- en: 'Figure 2.26: Plot of the decision regions'
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 2.26: 决策区域的绘图'
- en: The function plots decision regions along with a set of samples passed as arguments.
    In order to see the decision regions properly without too many samples obstructing
    our view, we pass only a 200-sample subset of the test data to the `plot_decision_regions`
    function. In this case, of course, it does not matter. We see the result is entirely
    red, indicating every point in the feature space would be classified as 0.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数绘制决策区域，并与作为参数传递的一组样本一起显示。为了正确显示决策区域，避免太多样本阻挡视线，我们只将测试数据的 200 个样本子集传递给 `plot_decision_regions`
    函数。当然，在这种情况下，样本数量并不重要。我们看到结果完全是红色，表示特征空间中的每个点都会被分类为 0。
- en: It shouldn't be surprising that a linear model can't do a good job of describing
    these nonlinear patterns. Recall earlier we mentioned the kernel trick for using
    SVMs to classify nonlinear problems. Let's see if doing this can improve the result.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性模型无法很好地描述这些非线性模式，这并不令人惊讶。回想一下，我们之前提到过通过 SVM 使用核技巧来分类非线性问题。让我们看看这样做是否能改善结果。
- en: 'Print the docstring for scikit-learn''s SVM by running the cell containing
    SVC. Scroll down and check out the parameter descriptions. Notice the kernel option,
    which is actually enabled by default as `rbf`. Use this `kernel` option to train
    a new SVM by running the cell containing the following code:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行包含 SVC 的单元以打印 scikit-learn 的文档字符串。向下滚动并查看参数描述。注意内核选项，默认情况下实际上是启用 `rbf`。使用这个
    `kernel` 选项训练一个新的 SVM，运行包含以下代码的单元：
- en: '[PRE19]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Figure 2.27: Training a new SVM ](img/C13018_02_27.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.27: 训练一个新的 SVM](img/C13018_02_27.jpg)'
- en: 'Figure 2.27: Training a new SVM'
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 2.27: 训练一个新的 SVM'
- en: '![Figure 2.28: Enhanced results with non-linear patterns](img/C13018_02_28.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.28: 使用非线性模式增强的结果](img/C13018_02_28.jpg)'
- en: 'Figure 2.28: Enhanced results with non-linear patterns'
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 2.28: 使用非线性模式增强的结果'
- en: The result is much better. Now, we are able to capture some of the non-linear
    patterns in the data and correctly classify the majority of the employees who
    have left.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 结果明显更好。现在，我们能够捕捉到数据中的一些非线性模式，并正确分类大多数已经离职的员工。
- en: The plot_decision_regions Function
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: plot_decision_regions 函数
- en: The `plot_decision_regions` function is provided by `mlxtend`, a Python library
    developed by Sebastian Raschka. It's worth taking a peek at the source code (which
    is of course written in Python) to understand how these plots are drawn. It's
    really not too complicated.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot_decision_regions` 函数由 `mlxtend` 提供，这是由 Sebastian Raschka 开发的一个 Python
    库。值得看看源代码（当然是用 Python 编写的），以理解这些图是如何绘制的。其实并不复杂。'
- en: 'In a Jupyter Notebook, import the function with `from mlxtend.plotting import
    plot_decision_regions` and then pull up the help with `plot_decision_regions?`
    and scroll to the bottom to see the local file path:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebook 中，使用 `from mlxtend.plotting import plot_decision_regions`
    导入函数，然后通过 `plot_decision_regions?` 调出帮助并滚动到底部查看本地文件路径：
- en: '![Figure 2.29: Local file path](img/C13018_02_29.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.29: 本地文件路径](img/C13018_02_29.jpg)'
- en: 'Figure 2.29: Local file path'
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 2.29: 本地文件路径'
- en: 'Then, open up the file and read through it. For example, you could run `cat`
    in the notebook:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，打开文件并阅读它。例如，您可以在笔记本中运行 `cat` 命令：
- en: '![Figure 2.30: Running cat in the notebook](img/C13018_02_30.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![图2.30：在笔记本中运行cat命令](img/C13018_02_30.jpg)'
- en: 'Figure 2.30: Running cat in the notebook'
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.30：在笔记本中运行cat命令
- en: This is okay, but not ideal as there's no color markup for the code. It's better
    to copy it (so you don't accidentally alter the original) and open it with your
    favorite text editor.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这虽然可以，但不理想，因为代码没有颜色标记。最好复制它（这样就不会意外修改原始代码），然后用你喜欢的文本编辑器打开它。
- en: When drawing attention to the code responsible for mapping the decision regions,
    we see a contour plot of predictions Z over an array `X_predict` that spans the
    feature space.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将注意力集中在负责映射决策区域的代码时，我们可以看到一个关于预测Z的等高线图，这些预测覆盖了特征空间的数组`X_predict`。
- en: '![Figure 2.31: The screenshot of the code for mapping decision regions'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.31：映射决策区域的代码截图](img/C13018_02_31.jpg)'
- en: '](img/C13018_02_31.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13018_02_31.jpg)'
- en: 'Figure 2.31: The screenshot of the code for mapping decision regions'
  id: totrans-274
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.31：映射决策区域的代码截图
- en: Let's move to training our model on k-Nearest Neighbors.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始训练我们的k-近邻模型。
- en: 'Exercise 10: Training K-nearest Neighbors for Our Model'
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习10：为我们的模型训练K-近邻分类器
- en: 'Load the scikit-learn KNN classification model and print the docstring by running
    the cell containing the following code:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 scikit-learn KNN 分类模型，并通过运行包含以下代码的单元格打印文档字符串：
- en: '[PRE20]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `n_neighbors` parameter decides how many samples to use when making a classification.
    If the weights parameter is set to uniform, then class labels are decided by majority
    vote. Another useful choice for the weights is distance, where closer samples
    have a higher weight in the voting. Like most model parameters, the best choice
    for this depends on the particular dataset.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`n_neighbors` 参数决定了分类时使用多少个样本。如果权重参数设置为 uniform，则类标签由多数投票决定。权重的另一个有用选择是距离，在这种情况下，离得较近的样本在投票中的权重较高。像大多数模型参数一样，最佳选择取决于特定的数据集。'
- en: 'Train the KNN classifier with `n_neighbors=3`, and then compute the accuracy
    and decision regions. Run the cell containing the following code:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`n_neighbors=3`训练KNN分类器，然后计算准确率和决策区域。运行包含以下代码的单元格：
- en: '[PRE21]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Figure 2.32: Training the kNN classifier with n_negihbours=3](img/C13018_02_32.jpg)'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图2.32：使用n_neighbors=3训练kNN分类器](img/C13018_02_32.jpg)'
- en: 'Figure 2.32: Training the kNN classifier with n_negihbours=3'
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.32：使用n_neighbors=3训练kNN分类器
- en: '![Figure 2.33: Enhanced results after training](img/C13018_02_33.jpg)'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图2.33：训练后增强的结果](img/C13018_02_33.jpg)'
- en: 'Figure 2.33: Enhanced results after training'
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.33：训练后增强的结果
- en: We see an increase in overall accuracy and a significant improvement for class
    1 in particular. However, the decision region plot would indicate we are overfitting
    the data. This is evident by the hard, "choppy" decision boundary, and small pockets
    of blue everywhere. We can soften the decision boundary and decrease overfitting
    by increasing the number of nearest neighbors.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们看到整体准确率有所提高，尤其是类1的准确性显著提升。然而，决策区域图表显示我们可能存在过拟合数据的情况。这一点通过硬性、"锯齿状"的决策边界以及到处可见的小蓝色区域得以体现。通过增加最近邻的数量，我们可以软化决策边界并减少过拟合。
- en: 'Train a KNN model with `n_neighbors=25` by running the cell containing the
    following code:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，使用`n_neighbors=25`训练一个KNN模型：
- en: '[PRE22]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Figure 2.34: Training the kNN classifier with n_negihbours=25](img/C13018_02_34.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![图2.34：使用n_neighbors=25训练kNN分类器](img/C13018_02_34.jpg)'
- en: 'Figure 2.34: Training the kNN classifier with n_negihbours=25'
  id: totrans-290
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.34：使用n_neighbors=25训练kNN分类器
- en: '![Figure 2.35: Output after training with n_neighbours=25 ](img/C13018_02_35.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图2.35：使用n_neighbors=25训练后的输出](img/C13018_02_35.jpg)'
- en: 'Figure 2.35: Output after training with n_neighbours=25'
  id: totrans-292
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.35：使用n_neighbors=25训练后的输出
- en: As we can see, the decision boundaries are significantly less choppy, and there
    are far less pockets of blue. The accuracy for class 1 is slightly less, but we
    would need to use a more comprehensive method such as k-fold cross validation
    to decide if there's a significant difference between the two models.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，决策边界明显平滑了很多，蓝色区域也少了很多。类1的准确率略低，但我们需要使用更全面的方法，比如k折交叉验证，来决定这两个模型之间是否存在显著差异。
- en: Note that increasing `n_neighbors` has no effect on training time, as the model
    is simply memorizing the data. The prediction time, however, will be greatly affected.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，增加 `n_neighbors` 对训练时间没有影响，因为模型只是记忆数据。然而，预测时间将受到极大影响。
- en: Note
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: When doing machine learning with real-world data, it's important for the algorithms
    to run quick enough to serve their purposes. For example, a script to predict
    tomorrow's weather that takes longer than a day to run is completely useless!
    Memory is also a consideration that should be taken into account when dealing
    with substantial amounts of data.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用真实世界数据进行机器学习时，算法的运行速度足够快以满足需求非常重要。例如，一个预测明天天气的脚本如果运行超过一天才完成，那完全没有意义！在处理大量数据时，内存也是需要考虑的一个因素。
- en: We will now train a Random Forest.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将训练一个随机森林。
- en: 'Exercise 11: Training a Random Forest'
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 11：训练随机森林
- en: Note
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Observe how similar it is to train and make predictions on each model, despite
    them each being so different internally.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 观察尽管每个模型内部差异巨大，但它们在训练和预测时是如此相似。
- en: 'Train a Random Forest classification model composed of 50 decision trees, each
    with a max depth of 5\. Run the cell containing the following code:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个由 50 棵决策树组成的随机森林分类模型，每棵树的最大深度为 5。运行包含以下代码的单元：
- en: '[PRE23]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Figure 2.36: Training a Random Forest with a max depth of 5](img/C13018_02_36.jpg)'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.36：训练最大深度为 5 的随机森林](img/C13018_02_36.jpg)'
- en: 'Figure 2.36: Training a Random Forest with a max depth of 5'
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.36：训练最大深度为 5 的随机森林
- en: '![Figure 2.37: Output after training with a max depth of 5](img/C13018_02_37.jpg)'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.37：训练最大深度为 5 后的输出](img/C13018_02_37.jpg)'
- en: 'Figure 2.37: Output after training with a max depth of 5'
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.37：训练最大深度为 5 后的输出
- en: Note the distinctive axes-parallel decision boundaries produced by decision
    tree machine learning algorithms.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意决策树机器学习算法所产生的独特的与轴平行的决策边界。
- en: We can access any of the individual decision trees used to build the Random
    Forest. These trees are stored in the `estimators_attribute` of the model. Let's
    draw one of these decision trees to get a feel for what's going on. Doing this
    requires the **graphviz** dependency, which can sometimes be difficult to install.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以访问用于构建随机森林的任意单棵决策树。这些树存储在模型的 `estimators_attribute` 中。让我们画出其中一棵决策树，以便了解发生了什么。这样做需要**graphviz**依赖项，而它有时安装起来比较麻烦。
- en: 'Draw one of the decision trees in the Jupyter Notebook by running the cell
    containing the following code:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元，绘制 Jupyter Notebook 中的一棵决策树：
- en: '[PRE24]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![Figure 2.38: Decision tree obtained using graphviz](img/C13018_02_38.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.38：使用 graphviz 获得的决策树](img/C13018_02_38.jpg)'
- en: 'Figure 2.38: Decision tree obtained using graphviz'
  id: totrans-312
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.38：使用 graphviz 获得的决策树
- en: We can see that each path is limited to five nodes as a result of setting max_depth=5\.
    The orange boxes represent predictions of no (has not left the company), and the
    blue boxes represent yes (has left the company). The shade of each box (light,
    dark, and so on) indicates the confidence level, which is related to the gini
    value.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，由于设置了 max_depth=5，每条路径被限制为五个节点。橙色框代表“否”（员工未离开公司）的预测，蓝色框代表“是”（员工已离开公司）的预测。每个框的阴影（浅色、深色等）表示置信度水平，这与基尼值相关。
- en: 'To summarize, we have accomplished two of the learning objectives in this section:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们已经完成了本节中的两个学习目标：
- en: We gained a qualitative understanding of support vector machines (SVMs), k-Nearest
    Neighbor classifiers (kNNs), and Random Forest
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们获得了支持向量机（SVM）、k 最近邻分类器（kNN）和随机森林的定性理解。
- en: We are now able to train a variety of models using scikit-learn and Jupyter
    Notebooks so that we can confidently build and compare predictive models
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在能够使用 scikit-learn 和 Jupyter Notebooks 训练各种模型，从而可以自信地构建和比较预测模型。
- en: 'In particular, we used the preprocessed data from our employee retention problem
    to train classification models to predict whether an employee has left the company
    or not. For the purposes of keeping things simple and focusing on the algorithms,
    we built models to predict this given only two features: the satisfaction level
    and last evaluation value. This two-dimensional feature space also allowed us
    to visualize the decision boundaries and identify what overfitting looks like.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们使用了来自员工流失问题的预处理数据，训练了分类模型来预测员工是否已离开公司。为了简化问题并专注于算法，我们构建了只利用两个特征：满意度水平和最后评估值，来预测这一问题的模型。这个二维特征空间还使我们能够可视化决策边界并识别过拟合的情况。
- en: 'In the following section, we will introduce two important topics in machine
    learning: k-fold cross-validation and validation curves.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍机器学习中的两个重要主题：k 折交叉验证和验证曲线。
- en: Assessing Models With K-fold Cross-Validation and Validation Curves
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用K折交叉验证和验证曲线评估模型
- en: Thus far, we have trained models on a subset of the data and then assessed performance
    on the unseen portion, called the test set. This is good practice because the
    model performance on training data is not a good indicator of its effectiveness
    as a predictor. It's very easy to increase accuracy on a training dataset by overfitting
    a model, which can result in poorer performance on unseen data.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在数据的子集上训练了模型，然后评估了未见部分的表现，称为测试集。这是良好的做法，因为模型在训练数据上的表现并不能很好地指示它作为预测器的有效性。通过对模型过拟合，很容易在训练数据集上提高准确性，但这可能导致在未见数据上的表现更差。
- en: That said, simply training models on data split in this way is not good enough.
    There is a natural variance in data that causes accuracies to be different (if
    even slightly) depending on the training and test splits. Furthermore, using only
    one training/test split to compare models can introduce bias towards certain models
    and lead to overfitting.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，仅仅按照这种方式划分数据并训练模型是不够的。数据本身有天然的变异性，这导致准确性会有所不同（即使是轻微的），取决于训练和测试的分割。此外，仅使用一个训练/测试分割来比较模型可能会引入偏向某些模型的偏差，并导致过拟合。
- en: '**K-fold cross validation** offers a solution to this problem and allows the
    variance to be accounted for by way of an error estimate on each accuracy calculation.
    This, in turn, naturally leads to the use of validation curves for tuning model
    parameters. These plot the accuracy as a function of a hyper parameter such as
    the number of decision trees used in a Random Forest or the max depth.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**K折交叉验证**提供了这个问题的解决方案，并允许通过每次准确性计算的误差估计来考虑方差。反过来，这自然导致了使用验证曲线来调整模型参数。这些曲线将准确性作为超参数的函数进行绘制，例如在随机森林中使用的决策树数量或最大深度。'
- en: Note
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This is our first time using the term hyperparameter. It references a parameter
    that is defined when initializing a model, for example, the C parameter of the
    SVM. This is in contradistinction to a parameter of the trained model, such as
    the equation of the decision boundary hyperplane for a trained SVM.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们第一次使用“超参数”这个术语。它指的是在初始化模型时定义的参数，例如SVM的C参数。这与训练后模型的参数不同，例如训练后的SVM的决策边界超平面的方程。
- en: 'The method is illustrated in the following diagram, where we see how the k-folds
    can be selected from the dataset:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法在以下图示中进行了说明，展示了如何从数据集中选择k折：
- en: '![Figure 2.39: Selecting k-folds from a data set](img/C13018_02_39.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![图2.39：从数据集中选择k折](img/C13018_02_39.jpg)'
- en: 'Figure 2.39: Selecting k-folds from a data set'
  id: totrans-327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.39：从数据集中选择k折
- en: 'The k-fold cross validation algorithm goes as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: K折交叉验证算法如下：
- en: Split data into k "folds" of near-equal size.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分成k个近似相等的“折叠”。
- en: Test and train k models on different fold combinations. Each model will include
    k - 1 folds of training data and the left-out fold is used for testing. In this
    method, each fold ends up being used as the validation data exactly once.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在不同的折叠组合上测试和训练k个模型。每个模型将包括k - 1个折叠的训练数据，剩下的折叠用于测试。在这种方法中，每个折叠最终会被用作验证数据，且仅使用一次。
- en: Calculate the model accuracy by taking the mean of the k values. The standard
    deviation is also calculated to provide error bars on the value.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过取k个值的均值来计算模型的准确性。还会计算标准偏差，以提供准确性值的误差条。
- en: It's standard to set *k = 10*, but smaller values for k should be considered
    if using a big data set.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 通常设置*k = 10*，但如果使用大数据集，则应考虑使用较小的k值。
- en: This validation method can be used to reliably compare model performance with
    different hyperparameters (for example, the C parameter for an SVM or the number
    of nearest neighbors in a KNN classifier). It's also suitable for comparing entirely
    different models.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这种验证方法可以用来可靠地比较不同超参数下的模型表现（例如SVM的C参数或KNN分类器中的最近邻数）。它也适用于比较完全不同的模型。
- en: Once the *best model* has been identified, it should be re-trained on the entirety
    of the dataset before being used to predict actual classifications.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了*最佳模型*，应在整个数据集上重新训练它，然后再用于预测实际分类。
- en: When implementing this with scikit-learn, it's common to use a slightly improved
    variation of the normal k-fold algorithm instead. This is called **stratified
    k-fold**. The improvement is that stratified k-fold cross validation maintains
    roughly even class label populations in the folds. As you can imagine, this reduces
    the overall variance in the models and decreases the likelihood of highly unbalanced
    models causing bias.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 scikit-learn 实现时，通常会使用一种稍微改进的标准 k-fold 算法变体。这种变体叫做**分层 k-fold**。其改进之处在于，分层
    k-fold 交叉验证在每个折叠中保持大致均匀的类别标签分布。正如你能想象的，这减少了模型的总体方差，并降低了高度不平衡的模型可能引起偏差的概率。
- en: '**Validation curves** are plots of a training and validation metric as a function
    of some model parameter. They allow to us to make good model parameter selections.
    In this book, we will use the accuracy score as our metric for these plots.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '**验证曲线**是训练和验证指标作为某些模型参数的函数的图表。它们可以帮助我们做出好的模型参数选择。在本书中，我们将使用准确率得分作为这些图表的指标。'
- en: Note
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The documentation for plot validation curves is available here: [http://scikit-learn.org/stable/auto_examples/](http://scikit-learn.org/stable/auto_examples/)
    model_selection/plot_validation_curve.html.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 关于绘制验证曲线的文档可在此查阅：[http://scikit-learn.org/stable/auto_examples/](http://scikit-learn.org/stable/auto_examples/)
    model_selection/plot_validation_curve.html。
- en: 'Consider this validation curve, where the accuracy score is plotted as a function
    of the gamma SVM parameter:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考此验证曲线，其中准确率分数作为 gamma SVM 参数的函数进行绘制：
- en: '![Figure 2.40: Validation curve with SVM](img/C13018_02_40.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.40：使用支持向量机（SVM）的验证曲线](img/C13018_02_40.jpg)'
- en: 'Figure 2.40: Validation curve with SVM'
  id: totrans-341
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.40：使用支持向量机（SVM）的验证曲线
- en: Starting on the left side of the plot, we can see that both sets of data are
    agreeing on the score, which is good. However, the score is also quite low compared
    to other gamma values, so therefore we say the model is underfitting the data.
    Increasing the gamma, we can see a point where the error bars of these two lines
    no longer overlap. From this point on, we see the classifier overfitting the data
    as the models behave increasingly well on the training set compared to the validation
    set. The optimal value for the gamma parameter can be found by looking for a high
    validation score with overlapping error bars on the two lines.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表的左侧开始，我们可以看到两组数据的得分一致，这是好的。然而，与其他 gamma 值相比，得分也相对较低，因此我们可以说模型存在欠拟合现象。当增加
    gamma 值时，我们可以看到一个点，在这个点上，这两条线的误差条不再重叠。从这个点开始，我们看到分类器在训练集上的表现逐渐优于验证集，说明模型过拟合数据。可以通过寻找具有重叠误差条的高验证得分来找到
    gamma 参数的最优值。
- en: Keep in mind that a learning curve for some parameter is only valid while the
    other parameters remain constant. For example, if training the SVM in this plot,
    we could decide to pick gamma on the order of. However, we may want to optimize
    the `C` parameter as well. With a different value for `C`, the preceding plot
    would be different and our selection for gamma may no longer be optimal.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，某些参数的学习曲线仅在其他参数保持不变时有效。例如，在此图中训练 SVM 时，我们可以选择 `gamma` 的某个值。然而，我们可能还需要优化
    `C` 参数。对于不同的 `C` 值，前述的图形会有所不同，我们对 `gamma` 的选择也可能不再最优。
- en: 'Exercise 12: Using K-fold Cross Validation and Validation Curves in Python
    With Scikit-learn'
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 12：在 Python 中使用 K-fold 交叉验证和验证曲线，借助 Scikit-learn
- en: 'Start the `NotebookApp` and open the `lesson-2-workbook.ipynb` file. Scroll
    down to `Subtopic B: K-fold cross-validation and validation curves`.'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 `NotebookApp` 并打开 `lesson-2-workbook.ipynb` 文件。向下滚动至 `子主题 B：K-fold 交叉验证和验证曲线`
    部分。
- en: The training data should already be in the notebook's memory, but let's reload
    it as a reminder of what exactly we're working with.
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练数据应该已经存在于笔记本的内存中，但我们还是重新加载一次，以便提醒自己到底在使用什么数据。
- en: 'Load the data and select the `satisfaction_level` and `last_evaluation` features
    for the training/validation set. We will not use the train-test split this time
    because we are going to use k-fold validation instead. Run the cell containing
    the following code:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据并选择 `satisfaction_level` 和 `last_evaluation` 特征作为训练/验证集。由于我们将使用 k-fold 验证，而不是训练-测试拆分，因此这次我们不使用拆分。运行包含以下代码的单元格：
- en: '[PRE25]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Instantiate a Random Forest model by running the cell containing the following
    code:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行包含以下代码的单元格，实例化一个随机森林模型：
- en: '[PRE26]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: To train the model with stratified k-fold cross validation, we'll use the `model_
    selection.cross_val_score` function.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使用分层 k-fold 交叉验证训练模型，我们将使用 `model_selection.cross_val_score` 函数。
- en: 'Train 10 variations of our model `clf` using stratified k-fold validation.
    Note that scikit-learn''s `cross_val_score` does this type of validation by default.
    Run the cell containing the following code:'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用分层k折验证训练我们模型`clf`的10个变体。请注意，scikit-learn的`cross_val_score`默认会进行这种类型的验证。运行包含以下代码的单元格：
- en: '[PRE27]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note how we use `np.random.seed` to set the seed for the random number generator,
    therefore ensuring reproducibility with respect to the randomly selected samples
    for each fold and decision tree in the Random Forest.
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们如何使用`np.random.seed`来设置随机数生成器的种子，从而确保每个折和随机森林中的决策树所选样本的随机性是可重现的。
- en: 'Calculate the accuracy as the average of each fold. We can also see the individual
    accuracies for each fold by printing scores. To see these, `run print(scores)`:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将准确率计算为每个折的平均值。我们还可以通过打印分数来查看每个折的单独准确率。要查看这些分数，请运行`print(scores)`：
- en: '[PRE28]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Using `cross_val_score` is very convenient, but it doesn''t tell us about the
    accuracies within each class. We can do this manually with the `model_ selection.StratifiedKFold`
    class. This class takes the number of folds as an initialization parameter, then
    the split method is used to build randomly sampled "masks" for the data. A mask
    is simply an array containing indexes of items in another array, where the items
    can then be returned by doing this: `data[mask]`.'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`cross_val_score`非常方便，但它并不能告诉我们每个类别中的准确率。我们可以通过`model_selection.StratifiedKFold`类手动实现这一点。该类将折数作为初始化参数，然后使用split方法为数据构建随机采样的“掩码”。掩码实际上是一个数组，包含另一个数组中项的索引，之后可以通过如下方式返回这些项：`data[mask]`。
- en: 'Define a custom class for calculating k-fold cross validation class accuracies.
    Run the cell containing the following code:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个自定义类，用于计算k折交叉验证的类别准确率。运行包含以下代码的单元格：
- en: '[PRE29]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the complete code, refer to the following: [https://bit.ly/2O5uP3h](https://bit.ly/2O5uP3h).'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关完整代码，请参阅以下链接：[https://bit.ly/2O5uP3h](https://bit.ly/2O5uP3h)。
- en: 'We can then calculate the class accuracies with code that''s very similar to
    step 4\. Do this by running the cell containing the following code:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用与第4步非常相似的代码计算类别准确率。通过运行包含以下代码的单元格来实现：
- en: '[PRE30]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the complete code, refer to the following: [https://bit.ly/2EKK7Lp](https://bit.ly/2EKK7Lp).'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关完整代码，请参阅以下链接：[https://bit.ly/2EKK7Lp](https://bit.ly/2EKK7Lp)。
- en: Now we can see the class accuracies for each fold! Pretty neat, right?
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以看到每个折中的类别准确率了！很棒，对吧？
- en: Calculate a validation curve using `model_selection.validation_curve`. This
    function uses stratified k-fold cross validation to train models for various values
    of a given parameter.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`model_selection.validation_curve`计算验证曲线。此函数使用分层k折交叉验证来训练模型，并针对给定参数的不同值进行训练。
- en: 'Do the calculations required to plot a validation curve by training Random
    Forests over a range of `max_depth` values. Run the cell containing the following
    code:'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过在一系列`max_depth`值上训练随机森林来执行绘制验证曲线所需的计算。运行包含以下代码的单元格：
- en: '[PRE31]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This will return arrays with the cross validation scores for each model, where
    the models have different max depths. In order to visualize the results, we'll
    leverage a function provided in the scikit-learn documentation.
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将返回每个模型的交叉验证分数数组，其中模型具有不同的最大深度。为了可视化结果，我们将利用scikit-learn文档中提供的一个函数。
- en: 'Run the cell in which `plot_validation_curve` is defined. Then, run the cell
    containing the following code to draw the plot:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行定义`plot_validation_curve`的单元格。然后，运行包含以下代码的单元格来绘制图表：
- en: '[PRE32]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![Figure 2.41: Plot validation curve](img/C13018_02_41.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.41：绘制验证曲线](img/C13018_02_41.jpg)'
- en: 'Figure 2.41: Plot validation curve'
  id: totrans-374
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.41：绘制验证曲线
- en: Recall how setting the max depth for decision trees limits the amount of overfitting.
    This is reflected in the validation curve, where we see overfitting taking place
    for large max depth values to the right. A good value for `max_depth` appears
    to be 6, where we see the training and validation accuracies in agreement. When
    `max_depth` is equal to 3, we see the model underfitting the data as training
    and validation accuracies are lower.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，设置决策树的最大深度是如何限制过拟合的。这在验证曲线中有所体现，在右侧我们可以看到过拟合的现象，尤其是对于较大的最大深度值。一个好的`max_depth`值似乎是6，这时训练准确率和验证准确率一致。当`max_depth`为3时，模型对数据出现欠拟合，因为训练准确率和验证准确率较低。
- en: To summarize, we have learned and implemented two important techniques for building
    reliable predictive models. The first such technique was k-foldcross-validation,
    which is used to split the data into various train/test batches and generate a
    set accuracy. From this set, we then calculated the average accuracy and the standard
    deviation as a measure of the error. This is important so that we have a gauge
    of the variability of our model and we can produce trustworthy accuracy.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们已经学习并实施了两种用于构建可靠预测模型的重要技术。第一种技术是k折交叉验证，它用于将数据划分为不同的训练/测试批次并生成一组准确度。从这一组中，我们计算了平均准确度和标准差作为误差度量。这很重要，因为它让我们可以衡量模型的变异性，并生成可信的准确度。
- en: 'We also learned about another such technique to ensure we have trustworthy
    results: validation curves. These allow us to visualize when our model is overfitting
    based on comparing training and validation accuracies. By plotting the curve over
    a range of our selected hyperparameter, we are able to identify its optimal value.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了另一种确保结果可信的方法：验证曲线。这些曲线使我们能够通过比较训练和验证准确度来可视化模型何时出现过拟合。通过绘制在我们选定的超参数范围内的曲线，我们能够识别其最佳值。
- en: 'In the final section of this chapter, we take everything we have learned so
    far and put it together in order to build our final predictive model for the employee
    retention problem. We seek to improve the accuracy, compared to the models trained
    thus far, by including all of the features from the dataset in our model. We''ll
    see now-familiar topics such as k-fold cross-validation and validation curves,
    but we''ll also introduce something new: dimensionality reduction techniques.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们将把到目前为止学到的所有内容整合在一起，以便构建我们的最终员工留存问题预测模型。通过将数据集中的所有特征包含在模型中，我们旨在提高与目前为止训练的模型相比的准确性。我们将看到现在已经熟悉的主题，如k折交叉验证和验证曲线，但我们还会引入一些新的内容：降维技术。
- en: Dimensionality Reduction Techniques
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降维技术
- en: Dimensionality reduction can simply involve removing unimportant features from
    the training data, but more exotic methods exist, such as **Principal Component
    Analysis** (**PCA**) and Linear Discriminant Analysis (LDA). These techniques
    allow for data compression, where the most important information from a large
    group of features can be encoded in just a few features.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 降维可以简单地通过去除训练数据中不重要的特征来实现，但也存在更为复杂的方法，如**主成分分析**（**PCA**）和线性判别分析（LDA）。这些技术允许进行数据压缩，其中来自一组大量特征的最重要信息可以仅通过几个特征来编码。
- en: In this subtopic, we'll focus on PCA. This technique transforms the data by
    projecting it into a new subspace of orthogonal "principal components," where
    the components with the highest eigenvalues encode the most information for training
    the model. Then, we can simply select a few of these principal components in place
    of the original high-dimensional dataset. For example, PCA could be used to encode
    the information from every pixel in an image. In this case, the original feature
    space would have dimensions equal to the number of pixels in the image. This high-dimensional
    space could then be reduced with PCA, where the majority of useful information
    for training predictive models might be reduced to just a few dimensions. Not
    only does this save time when training and using models, it allows them to perform
    better by removing noise in the dataset.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个子主题中，我们将重点讨论PCA。这项技术通过将数据投影到一个新的正交“主成分”子空间中来转换数据，其中具有最高特征值的成分编码了训练模型所需的最重要信息。然后，我们可以简单地选择这些主成分中的一部分，代替原始的高维数据集。例如，PCA可以用于编码图像中每个像素的信息。在这种情况下，原始特征空间的维度等于图像中的像素数。然后，通过PCA可以减少这个高维空间，其中用于训练预测模型的大部分有用信息可能会被缩减到只有几个维度。这不仅节省了训练和使用模型的时间，还通过去除数据集中的噪声来提高模型的性能。
- en: Like the models you've seen, it's not necessary to have a detailed understanding
    of PCA in order to leverage the benefits. However, we'll dig into the technical
    details of PCA just a bit further so that we can conceptualize it better. The
    key insight of PCA is to identify patterns between features based on correlations,
    so the PCA algorithm calculates the covariance matrix and then decomposes this
    into eigenvectors and eigenvalues. The vectors are then used to transform the
    data into a new subspace, from which a fixed number of principal components can
    be selected.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 就像你看到的模型一样，利用PCA的好处并不需要深入了解PCA。然而，我们将稍微探讨一下PCA的技术细节，以便更好地理解它。PCA的关键思想是基于特征间的相关性识别模式，因此，PCA算法计算协方差矩阵，然后将其分解为特征向量和特征值。接着，这些向量被用来将数据转换到一个新的子空间，从中可以选择固定数量的主成分。
- en: In the following exercise, we'll see an example of how PCA can be used to improve
    our Random Forest model for the employee retention problem we have been working
    on. This will be done after training a classification model on the full feature
    space, to see how our accuracy is affected by dimensionality reduction.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将看到一个例子，展示如何使用PCA来改善我们在员工流失问题上使用的随机森林模型。这将在使用完整特征空间训练分类模型之后进行，以便观察降维对我们准确率的影响。
- en: 'Exercise 13: Training a Predictive Model for the Employee Retention Problem'
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习13：为员工流失问题训练预测模型
- en: We have already spent considerable effort planning a machine learning strategy,
    preprocessing the data, and building predictive models for the employee retention
    problem. Recall that our business objective was to help the client prevent employees
    from leaving. The strategy we decided upon was to build a classification model
    that would predict the probability of employees leaving. This way, the company
    can assess the likelihood of current employees leaving and take action to prevent
    it.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经花费了相当多的精力来规划机器学习策略、预处理数据，并为员工流失问题构建预测模型。回顾一下我们的商业目标是帮助客户防止员工离职。我们决定的策略是构建一个分类模型，预测员工离职的概率。这样，公司就可以评估当前员工离职的可能性，并采取措施加以防止。
- en: 'Given our strategy, we can summarize the type of predictive modeling we are
    doing as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的策略，我们可以总结出我们正在进行的预测建模类型如下：
- en: Supervised learning on labeled training data
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有监督学习（使用带标签的训练数据）
- en: Classification problems with two class labels (binary)
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二分类问题
- en: In particular, we are training models to determine whether an employee has left
    the company, given a set of continuous and categorical features. After preparing
    the data for machine learning in *Activity 1, Preparing to Train a Predictive
    Model for the Employee-Retention Problem*, we went on to implement SVM, k-Nearest
    Neighbors, and Random Forest algorithms using just two features. These models
    were able to make predictions with over 90% overall accuracy. When looking at
    the specific class accuracies, however, we found that employees who had left (`class-
    label 1`) could only be predicted with 70-80% accuracy.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们正在训练模型，以便根据一组连续和分类特征来判断员工是否已离开公司。在*活动1：为员工流失问题训练预测模型*中准备数据后，我们实现了SVM、k最近邻和随机森林算法，只使用了两个特征。这些模型的整体预测准确率超过了90%。然而，在查看具体类别的准确率时，我们发现，离职员工（`class-label
    1`）的预测准确率只有70%-80%。
- en: Let's see how much this can be improved by utilizing the full feature space.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看通过利用完整的特征空间，能提高多少效果。
- en: Scroll down to the code for this section in the `lesson-2-workbook.ipynb` notebook.
    We should already have the preprocessed data loaded from the previous exercises,
    but this can be done again, if desired, by executing `df = pd.read_csv`(`'../data/hr-analytics/hr_data_processed.csv'`).
    Then, print the DataFrame columns with `print(df.columns)`.
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到`lesson-2-workbook.ipynb`笔记本中的代码部分。我们应该已经加载了之前练习中的预处理数据，但如果需要，可以通过执行`df =
    pd.read_csv`(`'../data/hr-analytics/hr_data_processed.csv'`)再次加载。然后，通过`print(df.columns)`打印DataFrame的列。
- en: 'Define a list of all the features by copy and pasting the output from `df.columns`
    into a new list (making sure to remove the target variable `left`). Then, define
    `X` and `Y` as we have done before. This goes as follows:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制并粘贴`df.columns`的输出到一个新列表中（确保移除目标变量`left`），定义所有特征的列表。然后，像之前一样定义`X`和`Y`。具体步骤如下：
- en: '[PRE33]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the complete code, refer to the following: [https://bit.ly/2D3WOQ2](https://bit.ly/2D3WOQ2).'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完整代码请参考：[https://bit.ly/2D3WOQ2](https://bit.ly/2D3WOQ2)。
- en: Looking at the feature names, recall what the values look like for each one.
    Scroll up to the set of histograms we made in the first activity to help jog your
    memory. The first two features are continuous; these are what we used for training
    models in the previous two exercises. After that, we have a few discrete features,
    such as `number_project` and `time_spend_company`, followed by some binary fields
    such as `work_accident` and `promotion_last_5years`. We also have a bunch of binary
    features, such as `department_ IT` and `department_accounting`, which were created
    by one-hot encoding.
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查看特征名称，回想每个特征的值长什么样。向上滚动查看我们在第一个活动中制作的直方图，以帮助唤起记忆。前两个特征是连续型的；这些是我们在之前的两个练习中用于训练模型的特征。之后，我们有一些离散型特征，例如`number_project`和`time_spend_company`，接着是一些二元特征，如`work_accident`和`promotion_last_5years`。我们还有一些二元特征，例如`department_IT`和`department_accounting`，这些是通过独热编码创建的。
- en: Given a mix of features like this, Random Forests are a very attractive type
    of model. For one thing, they're compatible with feature sets composed of both
    continuous and categorical data, but this is not particularly special; for instance,
    an SVM can be trained on mixed feature types as well (given proper preprocessing).
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑到这样的特征混合，随机森林是一种非常有吸引力的模型类型。首先，它们可以兼容由连续数据和分类数据组成的特征集，但这并不特别独特；例如，支持向量机（SVM）也可以在混合类型的特征上进行训练（只要有适当的预处理）。
- en: Note
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you''re interested in training an SVM or k-Nearest Neighbors classifier
    on mixed-type input features, you can use the data-scaling prescription from this
    StackExchange answer: [https://stats.stackexchange.com/questions/82923/mixing-continuous-and-binary-data-with-linear-svm/83086#83086](https://stats.stackexchange.com/questions/82923/mixing-continuous-and-binary-data-with-linear-svm/83086#83086).'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你有兴趣在混合类型的输入特征上训练支持向量机（SVM）或k近邻分类器，可以使用此StackExchange回答中的数据缩放建议：[https://stats.stackexchange.com/questions/82923/mixing-continuous-and-binary-data-with-linear-svm/83086#83086](https://stats.stackexchange.com/questions/82923/mixing-continuous-and-binary-data-with-linear-svm/83086#83086)。
- en: 'A simple approach would be to preprocess data as follows:'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种简单的方法是按如下方式预处理数据：
- en: standardize continuous variables; one-hot-encode categorical features; shift
    binary values to -1 and 1 instead of 0 and 1\. Finally, the mixed-feature data
    could be used to train a variety of classification models.
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对连续变量进行标准化；对分类特征进行独热编码；将二元值从0和1转换为-1和1。最后，可以使用混合特征数据来训练各种分类模型。
- en: 'Tune the `max_depth` hyperparameter using a validation curve to figure out
    the best parameters for our Random Forest model. Calculate the training and validation
    accuracies by running the following code:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用验证曲线调整`max_depth`超参数，找出我们随机森林模型的最佳参数。通过运行以下代码计算训练和验证准确率：
- en: '[PRE34]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We are testing 10 models with k-fold cross validation. By setting k = 5, we
    produce five estimates of the accuracy for each model, from which we extract the
    mean and standard deviation to plot in the validation curve. In total, we train
    50 models, and since `n_estimators` is set to 20, we are training a total of 1,000
    decision trees! All in roughly 10 seconds!
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们正在使用k折交叉验证测试10个模型。通过设置k = 5，我们为每个模型生成五个准确率估计值，并从中提取均值和标准差来绘制验证曲线。总共，我们训练了50个模型，并且由于`n_estimators`设置为20，所以我们一共训练了1,000棵决策树！这一切大约在10秒钟内完成！
- en: 'Plot the validation curve using our custom `plot_validation_curve` function
    from the last exercise. Run the following code:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们在上一个练习中创建的自定义`plot_validation_curve`函数绘制验证曲线。运行以下代码：
- en: '[PRE35]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Figure 2.42: Plot validation curve for different values of max_depths](img/C13018_02_42.jpg)'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图2.42：为不同的max_depth值绘制验证曲线](img/C13018_02_42.jpg)'
- en: 'Figure 2.42: Plot validation curve for different values of max_depths'
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.42：为不同的max_depth值绘制验证曲线
- en: For small max depths, we see the model underfitting the data. Total accuracies
    dramatically increase by allowing the decision trees to be deeper and encode more
    complicated patterns in the data. As the max depth is increased further and the
    accuracy approaches 100%, we find the model overfits the data, causing the training
    and validation accuracies to grow apart. Based on this figure, let's select a
    `max_depth` of 6 for our model.
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于较小的最大深度，我们看到模型对数据的拟合不足。通过允许决策树更深，从而能够编码数据中的更复杂模式，整体准确率显著提高。随着最大深度的进一步增加且准确率接近100%，我们发现模型开始对数据进行过拟合，导致训练和验证准确率出现分歧。根据这个图，我们选择将模型的`max_depth`设为6。
- en: We should really do the same for `n_estimators`, but in the spirit of saving
    time, we'll skip it. You are welcome to plot it on your own; you should find agreement
    between training and validation sets for a large range of values. Usually it's
    better to use more decision tree estimators in the random forest, but this comes
    at the cost of increased training times. We'll use 200 estimators to train our
    model.
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们本应对 `n_estimators` 做同样的操作，但为了节省时间，我们跳过了这一步。你可以自行绘制，应该会发现训练集和验证集在较大范围的值上是一致的。通常，在随机森林中使用更多的决策树估计器效果更好，但代价是训练时间增加。我们将使用
    200 个估计器来训练模型。
- en: Use `cross_val_class_score`, the k-fold cross validation by class function we
    created earlier, to test the selected model, a Random Forest with `max_ depth
    = 6` and `n_estimators = 200:`
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们之前创建的按类的 k 折交叉验证函数 `cross_val_class_score` 来测试所选模型，一个随机森林模型，`max_depth =
    6` 和 `n_estimators = 200`：
- en: '[PRE36]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The accuracies are way higher now that we're using the full feature set, compared
    to before when we only had the two continuous features!
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们使用完整的特征集，准确度大大提高，相比之前只有两个连续特征时的结果！
- en: 'Visualize the accuracies with a boxplot by running the following code:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码使用箱形图可视化准确度：
- en: '[PRE37]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![Figure 2.43: Visualizing the accuracy with a box plot](img/C13018_02_43.jpg)'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.43：使用箱形图可视化准确度](img/C13018_02_43.jpg)'
- en: 'Figure 2.43: Visualizing the accuracy with a box plot'
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.43：使用箱形图可视化准确度
- en: Random forests can provide an estimate of the feature performances.
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机森林可以提供特征表现的估算。
- en: Note
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The feature importance in scikit-learn is calculated based on how the node
    impurity changes with respect to each feature. For a more detailed explanation,
    take a look at the following StackOverflow thread about how feature importance
    is determined in Random Forest Classifier: [https://stackoverflow.com](https://stackoverflow.com)'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，特征重要性是根据节点不纯度如何随每个特征的变化而计算的。欲了解更详细的解释，请查看以下关于如何在随机森林分类器中确定特征重要性的
    StackOverflow 讨论：[https://stackoverflow.com](https://stackoverflow.com)
- en: 'Plot the feature importance, as stored in the attribute `feature_importances_`,
    by running the following code:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码绘制存储在`feature_importances_`属性中的特征重要性：
- en: '[PRE38]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![Figure 2.44: Plot of feature_importance](img/C13018_02_44.jpg)'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.44：特征重要性的图表](img/C13018_02_44.jpg)'
- en: 'Figure 2.44: Plot of feature_importance'
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.44：特征重要性的图表
- en: 'It doesn''t look like we''re getting much in the way of useful contribution
    from the one-hot encoded variables: department and salary. Also, the `promotion_last_5years`
    and `work_accident` features don''t appear to be very useful.'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看起来来自 one-hot 编码变量（部门和薪水）的贡献不大。此外，`promotion_last_5years` 和 `work_accident`
    特征似乎也不是很有用。
- en: Let's use PCA to condense all of these weak features into just a few principal
    components.
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们使用 PCA 将这些弱特征压缩为几个主成分。
- en: Import the `PCA` class from scikit-learn and transform the features. Run the
    following code
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 scikit-learn 导入 `PCA` 类并转换特征。运行以下代码：
- en: '[PRE39]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE40]'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Since we asked for the top three components, we get three vectors returned.
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们请求了前三个主成分，因此返回了三个向量。
- en: 'Add the new features to our DataFrame with the following code:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将新特征添加到我们的数据框中：
- en: '[PRE41]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Select our reduced-dimension feature set to train a new Random Forest with.
    Run the following code:'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择我们的降维特征集，使用它来训练一个新的随机森林。运行以下代码：
- en: '[PRE42]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Assess the new model''s accuracy with k-fold cross validation. This can be
    done by running the same code as before, where X now points to different features.
    The code is as follows:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 k 折交叉验证评估新模型的准确度。可以通过运行与之前相同的代码来实现，其中 X 指向不同的特征。代码如下：
- en: '[PRE43]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Visualize the result in the same way as before, using a box plot. The code
    is as follows:'
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用与之前相同的方式，通过箱形图可视化结果。代码如下：
- en: '[PRE44]'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![Figure 2.45: Box plot to visualize accuracy](img/C13018_02_45.jpg)'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.45：用于可视化准确度的箱形图](img/C13018_02_45.jpg)'
- en: 'Figure 2.45: Box plot to visualize accuracy'
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.45：用于可视化准确度的箱形图
- en: Comparing this to the previous result, we find an improvement in the class 1
    accuracy! Now, the majority of the validation sets return an accuracy greater
    than 90%. The average accuracy of 90.6% can be compared to the accuracy of 85.6%
    prior to dimensionality reduction!
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将此结果与之前的结果进行比较，我们发现类 1 的准确度有所提高！现在，大部分验证集的准确度超过了 90%。平均准确度为 90.6%，相比于降维前的 85.6%
    要高很多！
- en: Let's select this as our final model. We'll need to re-train it on the full
    sample space before using it in production.
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们选择这个作为最终模型。我们需要在完整样本空间上重新训练它，然后再投入生产使用。
- en: 'Train the final predictive model by running the following code:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码训练最终的预测模型：
- en: '[PRE45]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Save the trained model to a binary file using `externals.joblib.dump`. Run
    the following code:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`externals.joblib.dump`将训练好的模型保存到二进制文件。运行以下代码：
- en: '[PRE46]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Check that it''s saved into the working directory, for example, by running:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查它是否已保存到工作目录中，例如，通过运行：
- en: '`!ls *.pkl`. Then, test that we can load the model from the file by running
    the following code:'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`!ls *.pkl`。然后，运行以下代码测试我们是否可以从文件加载模型：'
- en: '[PRE47]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Congratulations! You've trained the final predictive model! Now, let's see an
    example of how it can be used to provide business insights for the client. Say
    we have a particular employee, who we'll call Sandra. Management has noticed she
    is working very hard and reported low job satisfaction in a recent survey. They
    would therefore like to know how likely it is that she will quit. For the sake
    of simplicity, let's take her feature values as a sample from the training set
    (but pretend that this is unseen data instead).
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 恭喜！你已经训练好了最终的预测模型！现在，让我们来看一个示例，展示如何利用该模型为客户提供商业洞察。假设我们有一个特定的员工，我们称她为Sandra。管理层注意到她工作非常努力，并且在最近的调查中报告了低的工作满意度。因此，他们希望了解她离职的可能性有多大。为了简化起见，我们将她的特征值作为训练集的一个样本（但假装这是未见过的数据）。
- en: 'List the feature values for Sandra by running the following code:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码列出Sandra的特征值：
- en: '[PRE48]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The next step is to ask the model which group it thinks she should be in.
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下一步是询问模型它认为Sandra应该属于哪个组。
- en: 'Predict the class label for Sandra by running the following code:'
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下代码预测Sandra的类别标签：
- en: '[PRE49]'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The model classifies her as having already left the company; not a good sign!
    We can take this a step further and calculate the probabilities of each class
    label.
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型将她分类为已经离开公司；这可不是个好兆头！我们可以进一步计算每个类别标签的概率。
- en: 'Use `clf.predict_proba` to predict the probability of our model predicting
    that Sandra has quit. Run the following code:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`clf.predict_proba`来预测我们的模型预测Sandra已经离职的概率。运行以下代码：
- en: '[PRE50]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We see the model predicting that she has quit with 93% accuracy. Since this
    is clearly a red flag for management, they decide on a plan to reduce her number
    of monthly hours to 100 and the time spent at the company to 1.
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们看到模型预测她已经离职的准确率为93%。由于这显然是管理层的一个红旗信号，他们决定采取一个计划，将她的每月工作时间减少到100小时，将在公司的时间减少到1年。
- en: 'Calculate the new probabilities with Sandra''s newly planned metrics. Run the
    following code:'
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Sandra新的计划指标计算新的概率。运行以下代码：
- en: '[PRE51]'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Excellent! We can now see that the model returns a mere 38% likelihood that
    she has quit! Instead, it now predicts she will not have left the company.
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 太棒了！我们现在可以看到，模型预测她已经离职的概率仅为38%！相反，模型现在预测她不会离开公司。
- en: Our model has allowed management to make a data-driven decision. By reducing
    her amount of time with the company by this particular amount, the model tells
    us that she will most likely remain an employee at the company!
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型帮助管理层做出了数据驱动的决策。通过将她在公司工作的时间减少到这个特定的数值，模型告诉我们她最有可能继续在公司工作！
- en: Summary
  id: totrans-465
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have seen how predictive models can be trained in Jupyter
    Notebooks.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到如何在Jupyter Notebooks中训练预测模型。
- en: To begin with, we talked about how to plan a machine learning strategy. We thought
    about how to design a plan that can lead to actionable business insights and stressed
    the importance of using the data to help set realistic business goals. We also
    explained machine learning terminology such as supervised learning, unsupervised
    learning, classification, and regression.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们讨论了如何规划机器学习策略。我们考虑了如何设计一个能够提供可操作商业洞察的计划，并强调了使用数据来帮助设定现实商业目标的重要性。我们还解释了机器学习术语，如监督学习、无监督学习、分类和回归。
- en: 'Next, we discussed methods for preprocessing data using scikit-learn and pandas.
    This included lengthy discussions and examples of a surprisingly time-consuming
    part of machine learning: dealing with missing data.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论了使用scikit-learn和pandas进行数据预处理的方法。这包括对机器学习中一个出人意料且耗时的部分——处理缺失数据——的详细讨论和示例。
- en: In the latter half of the chapter, we trained predictive classification models
    for our binary problem, comparing how decision boundaries are drawn for various
    models such as the SVM, k-Nearest Neighbors, and Random Forest. We then showed
    how validation curves can be used to make good parameter choices and how dimensionality
    reduction can improve model performance. Finally, at the end of our activity,
    we explored how the final model can be used in practice to make data-driven decisions.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后半部分，我们为二分类问题训练了预测分类模型，并比较了如何为不同模型（如支持向量机、k-最近邻和随机森林）绘制决策边界。然后，我们展示了如何利用验证曲线做出合理的参数选择，并且展示了降维如何提高模型性能。最后，在活动结束时，我们探讨了如何将最终模型应用于实际，做出基于数据的决策。
- en: In the next chapter, we will focus on data acquisition. Specifically, we will
    analyze HTTP requests, scrape tabular data from a web page, build and transform
    Pandas DataFrames, and finally create visualizations.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将重点讨论数据获取。具体来说，我们将分析HTTP请求，从网页抓取表格数据，构建和转换Pandas DataFrame，最后创建可视化图表。
