- en: Implementing Text Analytics with Spark 2.0 ML Library
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0 ML库实现文本分析
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下示例：
- en: Doing term frequency with Spark - everything that counts
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行词频统计-所有都计算
- en: Displaying similar words with Spark using Word2Vec
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Word2Vec在Spark中显示相似的单词
- en: Downloading a complete dump of Wikipedia for a real-life Spark ML project
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载维基百科的完整转储，用于实际的Spark ML项目
- en: Using Latent Semantic Analysis for text analytics with Spark 2.0
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用潜在语义分析进行文本分析，使用Spark 2.0
- en: Topic modeling with Latent Dirichlet allocation in Spark 2.0
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用潜在狄利克雷分配进行主题建模
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Text analytics is at the intersection of machine learning, mathematics, linguistics,
    and natural language processing. Text analytics, referred to as text mining in
    older literature, attempts to extract information and infer higher level concepts,
    sentiment, and semantic details from unstructured and semi-structured data. It
    is important to note that the traditional keyword searches are insufficient to
    deal with noisy, ambiguous, and irrelevant tokens and concepts that need to be
    filtered out based on the actual context.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分析处于机器学习、数学、语言学和自然语言处理的交叉点。文本分析，在旧文献中称为文本挖掘，试图从非结构化和半结构化数据中提取信息并推断出更高级别的概念、情感和语义细节。重要的是要注意，传统的关键字搜索无法处理嘈杂、模糊和无关的标记和概念，需要根据实际上下文进行过滤。
- en: Ultimately, what we are trying to do is for a given set of documents (text,
    tweets, web, and social media), is determine what the gist of the communication
    is and what concepts it is trying to convey (topics and concepts). These days,
    breaking down a document into its parts and taxonomy is too primitive to be considered
    text analytics. We can do better.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们试图做的是针对一组给定的文档（文本、推文、网络和社交媒体），确定沟通的要点以及它试图传达的概念（主题和概念）。如今，将文档分解为其部分和分类是太原始了，无法被视为文本分析。我们可以做得更好。
- en: Spark provides a set of tools and facilities to make text analytics easier,
    but it is up to the users to combine the techniques to come up with a viable system
    (for example, KKN clustering and topic modelling).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了一套工具和设施，使文本分析变得更容易，但用户需要结合技术来构建一个可行的系统（例如KKN聚类和主题建模）。
- en: 'It is worth mentioning that many of the commercially available systems use
    a combination of techniques to come up with the final answer. While Spark has
    a sufficient number of techniques that work very well at scale, it would not be
    hard to imagine that any text analytics system can benefit from a graphical model
    (that is, GraphFrame, GraphX). The following figure is a summary of the tools
    and facilities provided by Spark for text analytics:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，许多商业系统使用多种技术的组合来得出最终答案。虽然Spark拥有足够数量的技术，在规模上运行得非常好，但可以想象，任何文本分析系统都可以从图形模型（即GraphFrame、GraphX）中受益。下图总结了Spark提供的文本分析工具和设施：
- en: '![](img/00272.gif)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00272.gif)'
- en: Text analytics is an upcoming and important area due to its application to many
    fields such as security, customer engagement, sentiment analysis, social media,
    and online learning. Using text analytics techniques, one can combine traditional
    data stores (that is, structured data and database tables) with unstructured data
    (that is, customer reviews, sentiments, and social media interaction) to ascertain
    a higher order of understanding and a more complete view of the business unit,
    which was not possible before. This is especially important when dealing with
    millennials that have chosen social media and unstructured text as their primary
    means of communication.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分析是一个新兴且重要的领域，因为它适用于许多领域，如安全、客户参与、情感分析、社交媒体和在线学习。使用文本分析技术，可以将传统数据存储（即结构化数据和数据库表）与非结构化数据（即客户评论、情感和社交媒体互动）结合起来，以确定更高级的理解和更全面的业务单位视图，这在以前是不可能的。在处理选择社交媒体和非结构化文本作为其主要沟通方式的千禧一代时，这尤为重要。
- en: '![](img/00273.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00273.jpeg)'
- en: The main challenge with unstructured text is that you cannot use the traditional
    data platforming tools such as ETL to extract and force order on the data. We
    need new data wrangling, ML, and statistical methods combined with NLP techniques
    that can extract information and insight. Social media and customer interactions,
    such as transcriptions of calls in a call center, contain valuable information
    that can no longer be ignored without losing one's competitive edge.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化文本的主要挑战在于无法使用传统的数据平台工具，如ETL来提取并对数据进行排序。我们需要结合NLP技术的新数据整理、ML和统计方法，可以提取信息和洞察力。社交媒体和客户互动，比如呼叫中心的通话记录，包含了有价值的信息，如果不加以重视就会失去竞争优势。
- en: We not only need text analytics to be able to address big data at rest, but
    must also consider big data in motion, such as tweets and streams, to be effective.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅需要文本分析来处理静态的大数据，还必须考虑到动态的大数据，比如推文和数据流，才能有效。
- en: There are several approaches to deal with unstructured data. The following figure
    given is a depiction of the techniques in today's toolkit. While the rule-based
    system can be a good fit for limited text and domains, it fails to generalize
    due to its specific decision boundaries designed to be effective in that particular
    domain. The newer systems use statistical and NLP techniques to achieve better
    accuracy and scale.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 处理非结构化数据有几种方法。下图展示了当今工具包中的技术。虽然基于规则的系统可能适用于有限的文本和领域，但由于其特定的决策边界设计为在特定领域中有效，因此无法推广。新系统使用统计和NLP技术以实现更高的准确性和规模。
- en: '![](img/00274.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00274.jpeg)'
- en: In this chapter, we cover four recipes and two real-life datasets to demonstrate
    Spark's facilities for handling unstructured text analytics at scale.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了四个示例和两个实际数据集，以演示Spark在规模上处理非结构化文本分析的能力。
- en: First, we start with a simple recipe to not only mimic the early days of web
    search (keyword frequency) but also to provide insight into TF-IDF in raw code
    format. This recipe attempts to find out how often a word or phrase occurs in
    a document. As unbelievable as it sounds, there was a US patent issued for this
    technique!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从一个简单的配方开始，不仅模仿早期的网络搜索（关键词频率），而且还以原始代码格式提供了TF-IDF的见解。这个配方试图找出一个单词或短语在文档中出现的频率。尽管听起来难以置信，但实际上美国曾对这种技术发出了专利！
- en: Second, we proceed with a well-known algorithm, Word2Vec, which attempts to
    answer the question, i*f I give you a word, can you tell me the surrounding words,
    or what is in its neighborhood?* This is a good way to ask for synonyms inside
    a document using statistical techniques.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们使用一个众所周知的算法Word2Vec，它试图回答这样一个问题，即*如果我给你一个单词，你能告诉我周围的单词，或者它的邻居是什么吗？*这是使用统计技术在文档中寻找同义词的好方法。
- en: Third, we implement a **Latent Semantic Analysis** (**LSA**) which is a form
    of topic extraction. This method was invented at the University of Colorado Boulder
    and has been the workhorse in social sciences.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们实现了**潜在语义分析**（**LSA**），这是一种主题提取方法。这种方法是在科罗拉多大学博尔德分校发明的，并且一直是社会科学的主要工具。
- en: Fourth, we implement a **Latent Dirichlet Allocation** (**LDA**) to demonstrate
    topic modelling in which abstract concepts are extracted and associated with phrases
    or words (that is, less primitive constructs) in a scalable and meaningful way
    (for example, home, happiness, love, mother, family pet, children, shopping, and
    parties can be extracted into a single topic).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，我们实现了**潜在狄利克雷分配**（**LDA**）来演示主题建模，其中抽象概念以可扩展和有意义的方式（例如，家庭，幸福，爱情，母亲，家庭宠物，孩子，购物和聚会）提取并与短语或单词相关联。
- en: '![](img/00275.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00275.jpeg)'
- en: Doing term frequency with Spark - everything that counts
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行词频统计 - 一切都计算在内
- en: For this recipe, we will download a book in text format from Project Gutenberg,
    from [http://www.gutenberg.org/cache/epub/62/pg62.txt](http://www.gutenberg.org/cache/epub/62/pg62.txt).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，我们将从Project Gutenberg下载一本文本格式的书籍，网址为[http://www.gutenberg.org/cache/epub/62/pg62.txt](http://www.gutenberg.org/cache/epub/62/pg62.txt)。
- en: Project Gutenberg offers over 50,000 free eBooks in various formats for human
    consumption. Please read their terms of use; let us not use command-line tools
    to download any books.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Project Gutenberg提供了超过5万本各种格式的免费电子书供人类使用。请阅读他们的使用条款；让我们不要使用命令行工具下载任何书籍。
- en: When you look at the contents of the file, you will notice the title and author
    of the book is *The Project Gutenberg EBook of A Princess of Mars* by Edgar Rice
    Burroughs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当您查看文件的内容时，您会注意到书的标题和作者是《火星公主》的作者是埃德加·赖斯·伯勒斯。
- en: This eBook is for the use of anyone, anywhere, at no cost, and with almost no
    restrictions whatsoever. You may copy it, give it away, or reuse it under the
    terms of the Project Gutenberg License included with this eBook online at [http://www.gutenberg.org/](http://www.gutenberg.org/).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这本电子书可以供任何人在任何地方免费使用，几乎没有任何限制。您可以复制它，赠送它，或者根据本电子书在线附带的Project Gutenberg许可证条款进行重复使用，网址为[http://www.gutenberg.org/](http://www.gutenberg.org/)。
- en: We then use the downloaded book to demonstrate the classic word count program
    with Scala and Spark. The example may seem somewhat simple at first, but we are
    beginning the process of feature extraction for text processing. Also, a general
    understanding of counting word occurrences in a document will go a long way to
    help us understand the concept of TF-IDF.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用下载的书籍来演示Scala和Spark的经典单词计数程序。这个例子一开始可能看起来有些简单，但我们正在开始进行文本处理的特征提取过程。此外，对于理解TF-IDF的概念，对文档中单词出现次数的一般理解将有所帮助。
- en: How to do it...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中开始一个新项目。确保包含必要的JAR文件。
- en: 'The `package` statement for the recipe is as follows:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该配方的`package`语句如下：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the necessary packages for Scala, Spark, and JFreeChart:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Scala、Spark和JFreeChart所需的包：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will define a function to display our JFreeChart within a window:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将定义一个函数来在窗口中显示我们的JFreeChart：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let us define the location of our book file:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义我们书籍文件的位置：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create a Spark session with configurations using the factory builder pattern:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用工厂构建器模式创建一个带有配置的Spark会话：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We should set the logging level to warning, otherwise output will be difficult
    to follow:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该将日志级别设置为警告，否则输出将难以跟踪：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We read in the file of stop words which will be used as a filter later:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取停用词文件，稍后将用作过滤器：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The stop words file contains commonly used words which show no relevant value
    in matching or comparing documents, therefore they will be excluded from the pool
    of terms by a filter.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停用词文件包含常用词，这些词在匹配或比较文档时没有相关价值，因此它们将被排除在术语池之外。
- en: 'We now load the book to tokenize, analyze, apply stop words, filter, count,
    and sort:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在加载书籍进行标记化、分析、应用停用词、过滤、计数和排序：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We take top the 25 words which have the highest frequency:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们取出出现频率最高的25个单词：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We loop through every element in the resulting RDD, generating a category dataset
    model to build our chart of word occurrences:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们循环遍历结果RDD中的每个元素，生成一个类别数据集模型来构建我们的单词出现图表：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Display a bar chart of the word count:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 显示单词计数的条形图：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following chart displays the word count:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了单词计数：
- en: '![](img/00276.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00276.jpeg)'
- en: 'We close the program by stopping the SparkContext:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止SparkContext来关闭程序：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works...
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We began by loading the downloaded book and tokenizing it via a regular expression.
    The next step was to convert all tokens to lowercase and exclude stop words from
    our token list, followed by filtering out any words less than two characters long.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过正则表达式加载下载的书籍并对其进行标记化。下一步是将所有标记转换为小写，并从我们的标记列表中排除停用词，然后过滤掉任何少于两个字符长的单词。
- en: The removal of stop words and words of a certain length reduce the number of
    features we have to process. It may not seem obvious, but the removal of particular
    words based on various processing criteria reduce the number of dimensions our
    machine learning algorithms will later process.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 去除停用词和特定长度的单词会减少我们需要处理的特征数量。这可能并不明显，但根据各种处理标准去除特定单词会减少我们的机器学习算法后续处理的维度数量。
- en: Finally, we sorted the resulting word count in descending order, taking the
    top 25, which we displayed a bar chart for.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们按降序对结果进行了排序，取前25个，并为其显示了条形图。
- en: There's more...
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In this recipe, we have the base of what a keyword search would do. It is important
    to understand the difference between topic modelling and keyword search. In a
    keyword search, we try to associate a phrase with a given document based on the
    occurrences. In this case, we will point the user to a set of documents that has
    the most number of occurrences.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们有了关键词搜索的基础。重要的是要理解主题建模和关键词搜索之间的区别。在关键词搜索中，我们试图根据出现的次数将短语与给定文档关联起来。在这种情况下，我们将指导用户查看出现次数最多的一组文档。
- en: See also
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The next step in the evolution of this algorithm, that a developer can try as
    an extension, would be to add weights and come up with a weighted average, but
    then Spark provides a facility which we explore in the upcoming recipes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的演进的下一步，开发者可以尝试作为扩展的一部分，是添加权重并得出加权平均值，但是Spark提供了一个我们将在即将到来的食谱中探讨的设施。
- en: Displaying similar words with Spark using Word2Vec
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Word2Vec在Spark中显示相似的单词
- en: In this recipe, we will explore Word2Vec, which is Spark's tool for assessing
    word similarity. The Word2Vec algorithm is inspired by the *distributional hypothesis*
    in general linguistics. At the core, what it tries to say is that the tokens which
    occur in the same context (that is, distance from the target) tend to support
    the same primitive concept/meaning.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将探讨Word2Vec，这是Spark用于评估单词相似性的工具。Word2Vec算法受到了一般语言学中的*分布假设*的启发。在本质上，它试图表达的是在相同上下文中出现的标记（即，与目标的距离）倾向于支持相同的原始概念/含义。
- en: The Word2Vec algorithm was invented by a team of researchers at Google. Please
    refer to a white paper mentioned in the *There's more...* section of this recipe
    which describes Word2Vec in more detail.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec算法是由Google的一个研究团队发明的。请参考本食谱中*还有更多...*部分提到的一篇白皮书，其中更详细地描述了Word2Vec。
- en: How to do it...
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'The `package` statement for the recipe is as follows:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本食谱的`package`语句如下：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Import the necessary packages for Scala and Spark:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Scala和Spark所需的包：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let us define the location of our book file:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义我们的书籍文件的位置：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create a Spark session with configurations using the factory builder pattern:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用工厂构建器模式创建具有配置的Spark会话：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We should set the logging level to warning, otherwise output will be difficult
    to follow:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该将日志级别设置为警告，否则输出将难以跟踪：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We load in the book and convert it to a DataFrame:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载书籍并将其转换为DataFrame：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We now transform each line into a bag of words utilizing Spark''s regular expression
    tokenizer, converting each term into lowercase and filtering away any term which
    has a character length of less than four:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将每一行转换为一个词袋，利用Spark的正则表达式标记器，将每个术语转换为小写，并过滤掉任何字符长度少于四个的术语：
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We remove stop words by using Spark''s `StopWordRemover` class:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用Spark的`StopWordRemover`类来去除停用词：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We apply the Word2Vec machine learning algorithm to extract features:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应用Word2Vec机器学习算法来提取特征：
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We find ten synonyms from the book for *martian*:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从书中找到*火星*的十个同义词：
- en: '[PRE21]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Display the results of ten synonyms found by the model:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示模型找到的十个同义词的结果：
- en: '[PRE22]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/00277.gif)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00277.gif)'
- en: 'We close the program by stopping the SparkContext:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止SparkContext来关闭程序：
- en: '[PRE23]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: How it works...
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Word2Vec in Spark uses skip-gram and not **continuous bag of words** (**CBOW**)
    which is more suitable for a **Neural Net** (**NN**). At its core, we are attempting
    to compute the representation of the words. It is highly recommended for the user
    to understand the difference between local representation versus distributed presentation,
    which is very different to the apparent meaning of the words themselves.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的Word2Vec使用skip-gram而不是**连续词袋**（**CBOW**），后者更适合**神经网络**（**NN**）。在本质上，我们试图计算单词的表示。强烈建议用户了解局部表示与分布式表示之间的区别，这与单词本身的表面含义非常不同。
- en: If we use distributed vector representation for words, it is natural that similar
    words will fall close together in the vector space, which is a desirable generalization
    technique for pattern abstraction and manipulation (that is, we reduce the problem
    to vector arithmetic).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用分布式向量表示单词，那么相似的单词自然会在向量空间中靠在一起，这是一种理想的模式抽象和操作的泛化技术（即，我们将问题简化为向量运算）。
- en: What we want to do for a given set of words *{Word[1,] Word[2, .... ,]Word[n]}*
    that are cleaned and ready for processing, is define a maximum likelihood function
    (for example, log likelihood) for the sequence, and then proceed to maximize likelihood
    (that is, typical ML). For those familiar with NN, this is a simple multi class
    softmax model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一组经过清理并准备好进行处理的单词*{Word[1,] Word[2, .... ,]Word[n]}*，我们要做的是定义一个最大似然函数（例如，对数似然），然后继续最大化似然（即，典型的ML）。对于熟悉NN的人来说，这是一个简单的多类softmax模型。
- en: '![](img/00278.jpeg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00278.jpeg)'
- en: We start off with loading the free book into the memory and tokenizing it into
    terms. The terms are then converted into lowercase and we filter out any words
    less than four. We finally apply the stop words followed by the Word2Vec computation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将免费书籍加载到内存中，并将其标记为术语。然后将术语转换为小写，并过滤掉任何少于四个字的单词。最后应用停用词，然后进行Word2Vec计算。
- en: There's more...
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: How would you find similar words anyhow? How many algorithms are there that
    can solve this problem, and how do they vary? The Word2Vec algorithm has been
    around for a while and has a counterpart called CBOW. Please bear in mind that
    Spark provides the skip-gram method as the implementation technique.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，你如何找到相似的单词？有多少算法可以解决这个问题，它们又有什么不同？Word2Vec算法已经存在一段时间了，还有一个叫做CBOW的对应算法。请记住，Spark提供了skip-gram方法作为实现技术。
- en: 'The variations of the Word2Vec algorithm are as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec算法的变体如下：
- en: '**Continuous Bag of Words (CBOW)**: Given a central word, what are the surrounding
    words?'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Continuous Bag of Words (CBOW)**：给定一个中心词，周围的词是什么？'
- en: '**Skip-gram**: If we know the words surrounding, can we guess the missing word?'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Skip-gram**：如果我们知道周围的单词，我们能猜出缺失的单词吗？'
- en: There is a variation of the algorithm that is called **skip-gram model with
    negative sampling** (**SGNS**), which seems to outperform other variants.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种称为**skip-gram模型与负采样**（**SGNS**）的算法变体，似乎优于其他变体。
- en: The co-occurrence is the fundamental concept underlying both CBOW and skip-gram.
    Even though the skip-gram does not directly use a co-occurrence matrix, it is
    using it indirectly.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 共现是CBOW和skip-gram的基本概念。尽管skip-gram没有直接使用共现矩阵，但它间接使用了它。
- en: In this recipe, we used the *stop words* techniques from NLP to have a cleaner
    corpus before running our algorithm. The stop words are English words such as
    "*the*" that need to be removed since they are not contributing to any improvement
    in the outcome.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们使用了NLP中的*停用词*技术，在运行算法之前对我们的语料库进行了清理。停用词是英语单词，比如“*the*”，需要被移除，因为它们对结果没有任何改进。
- en: Another important concept is* stemming*, which is not covered here, but will
    be demonstrated in later recipes. Stemming removes extra language artefacts and
    reduces the word to its root (for example, *Engineering*, *Engineer*, and *Engineers*
    become *Engin* which is the root).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的概念是*词干提取*，这里没有涉及，但将在以后的食谱中演示。词干提取去除额外的语言构件，并将单词减少到其根（例如，“工程”、“工程师”和“工程师”变成“Engin”，这是根）。
- en: 'The white paper found at the following URL should provide deeper explanation
    for Word2Vec:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下URL找到的白皮书应该对Word2Vec提供更深入的解释：
- en: '[http://arxiv.org/pdf/1301.3781.pdf](http://arxiv.org/pdf/1301.3781.pdf)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://arxiv.org/pdf/1301.3781.pdf](http://arxiv.org/pdf/1301.3781.pdf)'
- en: See also
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Documentation for the Word2Vec recipe:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec食谱的文档：
- en: '`Word2Vec()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2Vec](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2Vec)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Word2Vec()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2Vec](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2Vec)'
- en: '`Word2VecModel()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2VecModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2VecModel)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Word2VecModel()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2VecModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2VecModel)'
- en: '`StopWordsRemover()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StopWordsRemover()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover)'
- en: Downloading a complete dump of Wikipedia for a real-life Spark ML project
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载维基百科的完整转储以进行真实的Spark ML项目
- en: In this recipe, we will be downloading and exploring a dump of Wikipedia so
    we can have a real-life example. The dataset that we will be downloading in this
    recipe is a dump of Wikipedia articles. You will either need the command-line
    tool **curl**, or a browser to retrieve a compressed file, which is about 13.6
    GB at this time. Due to the size, we recommend the curl command-line tool.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将下载并探索维基百科的转储，以便我们可以有一个现实生活的例子。在这个食谱中，我们将下载的数据集是维基百科文章的转储。您将需要命令行工具**curl**或浏览器来检索一个压缩文件，目前大约为13.6
    GB。由于文件大小，我们建议使用curl命令行工具。
- en: How to do it...
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'You can start with downloading the dataset using the following command:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用以下命令开始下载数据集：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now you want to decompress the ZIP file:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你想要解压ZIP文件：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This should create an uncompressed file which is named `enwiki-latest-pages-articles-multistream.xml`
    and is about 56 GB.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为`enwiki-latest-pages-articles-multistream.xml`的未压缩文件，大约为56 GB。
- en: 'Let us take a look at the Wikipedia XML file:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看维基百科的XML文件：
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: There's more...
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We recommend working with the XML file in chunks, and using sampling for your
    experiments until you are ready for a final job submit. It will save a tremendous
    amount of time and effort.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议使用XML文件的分块，并对实验使用抽样，直到准备好进行最终的作业提交。这将节省大量的时间和精力。
- en: See also
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for Wiki download is available at [https://en.wikipedia.org/wiki/Wikipedia:Database_download](https://en.wikipedia.org/wiki/Wikipedia:Database_download).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 维基下载的文档可在[https://en.wikipedia.org/wiki/Wikipedia:Database_download](https://en.wikipedia.org/wiki/Wikipedia:Database_download)找到。
- en: Using Latent Semantic Analysis for text analytics with Spark 2.0
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0进行文本分析的潜在语义分析
- en: In this recipe, we will explore LSA utilizing a data dump of articles from Wikipedia.
    LSA translates into analyzing a corpus of documents to find hidden meaning or
    concepts in those documents.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将利用维基百科文章的数据转储来探索LSA。LSA意味着分析一系列文档，以找出这些文档中的隐藏含义或概念。
- en: In the first recipe of this chapter, we covered the basics of the TF (that is,
    term frequency) technique. In this recipe, we use HashingTF for calculating TF
    and use IDF to fit a model into the calculated TF. At its core, LSA uses **singular
    value decomposition** (**SVD**) on the term frequency document to reduce dimensionality
    and therefore extract the most important concepts. There are other cleanup steps
    that we need to do (for example, stop words and stemming) that will clean up the
    bag of words before we start analyzing it.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一个示例中，我们介绍了TF（即术语频率）技术的基础知识。在这个示例中，我们使用HashingTF来计算TF，并使用IDF将模型拟合到计算的TF中。在其核心，LSA使用**奇异值分解**（**SVD**）对术语频率文档进行降维，从而提取最重要的概念。在我们开始分析之前，还有其他一些清理步骤需要做（例如，停用词和词干处理）来清理词袋。
- en: How to do it...
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'The package statement for the recipe is as follows:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该示例的包语句如下：
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Import the necessary packages for Scala and Spark:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Scala和Spark所需的包：
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following two statements import the `Cloud9` library toolkit elements necessary
    for processing Wikipedia XML dumps/objects. `Cloud9` is a library toolkit that
    makes accessing, wrangling, and processing the Wikipedia XML dumps easier for
    developers. See the following lines of code for more detailed information:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两个语句导入了处理维基百科XML转储/对象所需的`Cloud9`库工具包元素。`Cloud9`是一个库工具包，使得开发人员更容易访问、整理和处理维基百科XML转储。有关更详细信息，请参阅以下代码行：
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Wikipedia is a free body of knowledge that can be freely downloaded as a dump
    of XML chunks/objects via the following Wikipedia download link:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科是一个免费的知识体，可以通过以下维基百科下载链接免费下载为XML块/对象的转储：
- en: '[https://en.wikipedia.org/wiki/Wikipedia:Database_download](https://en.wikipedia.org/wiki/Wikipedia:Database_download)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Wikipedia:Database_download](https://en.wikipedia.org/wiki/Wikipedia:Database_download)'
- en: The complexity of text and its structure can be easily handled using the `Cloud9`
    toolkit which facilitates accessing and processing the text using the `import`
    statements listed previously.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 文本的复杂性和结构可以通过`Cloud9`工具包轻松处理，该工具包可以使用之前列出的`import`语句来访问和处理文本。
- en: 'The following link provides some information regarding the `Cloud9` library:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接提供了有关`Cloud9`库的一些信息：
- en: Main page is available at [https://lintool.github.io/Cloud9/docs/content/wikipedia.html](https://lintool.github.io/Cloud9/docs/content/wikipedia.html).
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主页位于[https://lintool.github.io/Cloud9/docs/content/wikipedia.html](https://lintool.github.io/Cloud9/docs/content/wikipedia.html)。
- en: Source code is available at [http://grepcode.com/file/repo1.maven.org/maven2/edu.umd/cloud9/2.0.0/edu/umd/cloud9/collection/wikipedia/WikipediaPage.java](http://grepcode.com/file/repo1.maven.org/maven2/edu.umd/cloud9/2.0.0/edu/umd/cloud9/collection/wikipedia/WikipediaPage.java)
    and [http://grepcode.com/file/repo1.maven.org/maven2/edu.umd/cloud9/2.0.1/edu/umd/cloud9/collection/wikipedia/language/EnglishWikipediaPage.java](http://grepcode.com/file/repo1.maven.org/maven2/edu.umd/cloud9/2.0.1/edu/umd/cloud9/collection/wikipedia/language/EnglishWikipediaPage.java).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源代码可在[http://grepcode.com/file/repo1.maven.org/maven2/edu.umd/cloud9/2.0.0/edu/umd/cloud9/collection/wikipedia/WikipediaPage.java](http://grepcode.com/file/repo1.maven.org/maven2/edu.umd/cloud9/2.0.0/edu/umd/cloud9/collection/wikipedia/WikipediaPage.java)和[http://grepcode.com/file/repo1.maven.org/maven2/edu.umd/cloud9/2.0.1/edu/umd/cloud9/collection/wikipedia/language/EnglishWikipediaPage.java](http://grepcode.com/file/repo1.maven.org/maven2/edu.umd/cloud9/2.0.1/edu/umd/cloud9/collection/wikipedia/language/EnglishWikipediaPage.java)上找到。
- en: 'Next, perform the following steps:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，执行以下步骤：
- en: 'We define a function to parse a Wikipedia page and return the title and content
    text of the page:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义一个函数来解析维基百科页面并返回页面的标题和内容文本：
- en: '[PRE30]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We define a short function to apply the Porter stemming algorithm to terms:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义一个简短的函数来应用Porter词干算法到术语上：
- en: '[PRE31]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We define a function to tokenize content text of a page into terms:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义一个函数将页面的内容文本标记为术语：
- en: '[PRE32]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let us define the location of the Wikipedia data dump:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义维基百科数据转储的位置：
- en: '[PRE33]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create a job configuration for Hadoop XML streaming:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Hadoop XML流处理创建一个作业配置：
- en: '[PRE34]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We set up the data path for Hadoop XML streaming processing:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Hadoop XML流处理设置数据路径：
- en: '[PRE35]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Create a `SparkSession` with configurations using the factory builder pattern:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用工厂构建器模式创建一个带有配置的`SparkSession`：
- en: '[PRE36]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We should set the logging level to warning, otherwise output will be difficult
    to follow:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该将日志级别设置为警告，否则输出将难以跟踪：
- en: '[PRE37]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We begin to process the huge Wikipedia data dump into article pages, taking
    a sample of the file:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们开始处理庞大的维基百科数据转储成文章页面，取样文件：
- en: '[PRE38]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we process our sample data into an RDD containing a tuple of title and
    page context text:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将样本数据处理成包含标题和页面内容文本的RDD：
- en: '[PRE39]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We now output the number of Wikipedia articles we will process:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在输出我们将处理的维基百科文章的数量：
- en: '[PRE40]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We load into memory the stop words for filtering the page content text:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将加载停用词以过滤页面内容文本：
- en: '[PRE41]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We tokenize the page content text, turning it into terms for further processing:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们标记化页面内容文本，将其转换为术语以进行进一步处理：
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We use Spark''s `HashingTF` class to compute term frequency of our tokenized
    page context text:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用Spark的`HashingTF`类来计算我们标记化的页面内容文本的术语频率：
- en: '[PRE43]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We take term frequencies and compute the inverse document frequency utilizing
    Spark''s IDF class:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获取术语频率并利用Spark的IDF类计算逆文档频率：
- en: '[PRE44]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We generate a `RowMatrix` using the inverse document frequency and compute
    singular value decomposition:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用逆文档频率生成一个`RowMatrix`并计算奇异值分解：
- en: '[PRE45]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**U**: The rows will be documents and the columns will be concepts.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**U**：行将是文档，列将是概念。'
- en: '**S**: The elements will be the amount variation from each concept.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**S**：元素将是每个概念的变化量。'
- en: '**V**: The rows will be terms and the columns will be concepts.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**V**：行将是术语，列将是概念。'
- en: 'We close the program by stopping the SparkContext:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止SparkContext来关闭程序：
- en: '[PRE46]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: How it works...
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The example starts off by loading a dump of Wikipedia XML using Cloud9 Hadoop
    XML streaming tools to process the enormous XML document. Once we have parsed
    out the page text, the tokenization phase invokes turning our stream of Wikipedia
    page text into tokens. We used the Porter stemmer during the tokenization phase
    to help reduce words to a common base form.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例首先通过使用Cloud9 Hadoop XML流处理工具加载维基百科XML的转储来开始。一旦我们解析出页面文本，标记化阶段调用将我们的维基百科页面文本流转换为标记。在标记化阶段，我们使用Porter词干提取器来帮助将单词减少到一个共同的基本形式。
- en: More details on stemming is available at [https://en.wikipedia.org/wiki/Stemming](https://en.wikipedia.org/wiki/Stemming).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 有关词干处理的更多细节，请参阅[https://en.wikipedia.org/wiki/Stemming](https://en.wikipedia.org/wiki/Stemming)。
- en: The next step was to use Spark HashingTF on each page token to compute the term
    frequency. After this phase was completed, we utilized Spark's IDF to generate
    the inverse document frequency.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是对每个页面标记使用Spark HashingTF计算词项频率。完成此阶段后，我们利用了Spark的IDF生成逆文档频率。
- en: Finally, we took the TF-IDF API and applied a singular value decomposition to
    handle factorization and dimensionality reduction.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用TF-IDF API并应用奇异值分解来处理因子分解和降维。
- en: 'The following screenshot shows the steps and flow of the recipe:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了该步骤和配方的流程：
- en: '![](img/00279.jpeg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00279.jpeg)'
- en: 'The Cloud9 Hadoop XML tools and several other required dependencies can be
    found at:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud9 Hadoop XML工具和其他一些必需的依赖项可以在以下链接找到：
- en: '`bliki-core-3.0.19.jar`: [http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bliki-core-3.0.19.jar`: [http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)'
- en: '`cloud9-2.0.1.jar`: [http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar](http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cloud9-2.0.1.jar`: [http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar](http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar)'
- en: '`hadoop-streaming-2.7.4.jar`: [http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar](http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hadoop-streaming-2.7.4.jar`: [http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar](http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar)'
- en: '`lucene-snowball-3.0.3.jar`: [http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar](http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lucene-snowball-3.0.3.jar`: [http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar](http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar)'
- en: There's more...
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: It should be obvious by now that even though Spark does not provide a direct
    LSA implementation, the combination of TF-IDF and SVD will let us construct and
    then decompose the large corpus matrix into three matrices, which can help us
    interpret the results by applying the dimensionality reduction via SVD. We can
    concentrate on the most meaningful clusters (similar to a recommendation algorithm).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在显而易见，即使Spark没有提供直接的LSA实现，TF-IDF和SVD的组合也能让我们构建然后分解大语料库矩阵为三个矩阵，这可以通过SVD的降维来帮助我们解释结果。我们可以集中精力在最有意义的聚类上（类似于推荐算法）。
- en: SVD will factor the term frequency document (that is, documents by attributes)
    to three distinct matrices that are much more efficient to extract to *N* concepts
    (that is, *N=27* in our example) from a large matrix that is hard and expensive
    to handle. In ML, we always prefer the tall and skinny matrices (that is, *U*
    matrix in this case) to other variations.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: SVD将分解词项频率文档（即文档按属性）为三个不同的矩阵，这些矩阵更有效地提取出*N*个概念（在我们的例子中为*N=27*）从一个难以处理且昂贵的大矩阵中。在机器学习中，我们总是更喜欢高瘦的矩阵（在这种情况下是*U*矩阵）而不是其他变体。
- en: 'The following is the technique for SVD:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是SVD的技术：
- en: '![](img/00280.gif)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00280.gif)'
- en: The primary goal of SVD is dimensionality reduction to cure desired (that is,
    top *N*) topics or abstract concepts. We will use the following input to get the
    output stated in the following section.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: SVD的主要目标是降维以获得所需的（即前*N*个）主题或抽象概念。我们将使用以下输入来获得以下部分中所述的输出。
- en: As input, we'll take a large matrix of *m x n* (*m* is the number of documents,
    *n* is the number of terms or attributes).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输入，我们将采用*m x n*（*m*为文档数，*n*为术语或属性数）的大矩阵。
- en: 'This is the output that we should get:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们应该得到的输出：
- en: '![](img/00281.gif)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00281.gif)'
- en: 'For a more detailed example and short tutorial on SVD, please see the following
    links:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 有关SVD的更详细示例和简短教程，请参见以下链接：
- en: '[http://home.iitk.ac.in/~crkrish/MLT/PreRequisites/linalgWithSVD.pdf](http://home.iitk.ac.in/~crkrish/MLT/PreRequisites/linalgWithSVD.pdf)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://home.iitk.ac.in/~crkrish/MLT/PreRequisites/linalgWithSVD.pdf](http://home.iitk.ac.in/~crkrish/MLT/PreRequisites/linalgWithSVD.pdf)'
- en: '[http://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf](http://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf](http://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf)'
- en: 'You can also refer to a write up from RStudio, which is available at the following
    link:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以参考RStudio的写作，链接如下：
- en: '[http://rstudio-pubs-static.s3.amazonaws.com/222293_1c40c75d7faa42869cc59df879547c2b.html](http://rstudio-pubs-static.s3.amazonaws.com/222293_1c40c75d7faa42869cc59df879547c2b.html)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://rstudio-pubs-static.s3.amazonaws.com/222293_1c40c75d7faa42869cc59df879547c2b.html](http://rstudio-pubs-static.s3.amazonaws.com/222293_1c40c75d7faa42869cc59df879547c2b.html)'
- en: See also
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: SVD has been covered in detail in [Chapter 11](part0494.html#EN3LS0-4d291c9fed174a6992fd24938c2f9c77),
    *Curse of High**-Dimensionality in Big Data*.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: SVD在[第11章](part0494.html#EN3LS0-4d291c9fed174a6992fd24938c2f9c77)中有详细介绍，*大数据中的高维度诅咒*。
- en: For a pictorial representation of SVD, please see the recipe *Using Singular
    Value Decomposition (SVD) to address high-dimensionality* in [Chapter 11](part0494.html#EN3LS0-4d291c9fed174a6992fd24938c2f9c77), *Curse
    of High-Dimensionality in Big Data*.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 有关SVD的图示表示，请参阅[第11章](part0494.html#EN3LS0-4d291c9fed174a6992fd24938c2f9c77)中的示例*使用奇异值分解（SVD）解决高维度问题*，*大数据中的高维度问题*。
- en: More details on `SingularValueDecomposition()` can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`SingularValueDecomposition()`的更多详细信息，请参考[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition)。
- en: Please refer to [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix) for
    more details on `RowMatrix()`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`RowMatrix()`的更多详细信息，请参考[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix)。
- en: Topic modeling with Latent Dirichlet allocation in Spark 2.0
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用潜在狄利克雷分配进行主题建模
- en: In this recipe, we will be demonstrating topic model generation by utilizing
    Latent Dirichlet Allocation to infer topics from a collection of documents.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将利用潜在狄利克雷分配来演示主题模型生成，以从一系列文档中推断主题。
- en: We have covered LDA in previous chapters as it applies to clustering and topic
    modelling, but in this chapter, we demonstrate a more elaborate example to show
    its application to text analytics using more real-life and complex datasets.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的章节中已经涵盖了LDA，因为它适用于聚类和主题建模，但在本章中，我们演示了一个更详细的示例，以展示它在文本分析中对更真实和复杂的数据集的应用。
- en: We also apply NLP techniques such as stemming and stop words to provide a more
    realistic approach to LDA problem-solving. What we are trying to do is to discover
    a set of latent factors (that is, different from the original) that can solve
    and describe the solution in a more efficient way in a reduced computational space.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应用NLP技术，如词干处理和停用词，以提供更真实的LDA问题解决方法。我们试图发现一组潜在因素（即与原始因素不同），可以以更高效的方式在减少的计算空间中解决和描述解决方案。
- en: 'The first question that always comes up when using LDA and topic modelling
    is w*hat is Dirichlet?* Dirichlet is simply a type of distribution and nothing
    more. Please see the following link from the University of Minnesota for details:
    [http://www.tc.umn.edu/~horte005/docs/Dirichletdistribution.pdf](http://www.tc.umn.edu/~horte005/docs/Dirichletdistribution.pdf).'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用LDA和主题建模时，经常出现的第一个问题是*狄利克雷是什么？* 狄利克雷只是一种分布，没有别的。请参阅明尼苏达大学的以下链接了解详情：[http://www.tc.umn.edu/~horte005/docs/Dirichletdistribution.pdf](http://www.tc.umn.edu/~horte005/docs/Dirichletdistribution.pdf)。
- en: How to do it...
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'The `package` statement for the recipe is as follows:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该示例的`package`语句如下：
- en: '[PRE47]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Import the necessary packages for Scala and Spark:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Scala和Spark所需的包：
- en: '[PRE48]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We define a function to parse a Wikipedia page and return the title and content
    text of the page:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义一个函数来解析维基百科页面，并返回页面的标题和内容文本：
- en: '[PRE49]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let us define the location of the Wikipedia data dump:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义维基百科数据转储的位置：
- en: '[PRE50]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We create a job configuration for Hadoop XML streaming:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为Hadoop XML流创建作业配置：
- en: '[PRE51]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We set up the data path for Hadoop XML streaming processing:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为Hadoop XML流处理设置了数据路径：
- en: '[PRE52]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Create a `SparkSession` with configurations using the factory builder pattern:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用工厂构建器模式创建带有配置的`SparkSession`：
- en: '[PRE53]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We should set the logging level to warning, otherwise output will be difficult
    to follow:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该将日志级别设置为警告，否则输出将难以跟踪：
- en: '[PRE54]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We begin to process the huge Wikipedia data dump into article pages taking
    a sample of the file:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们开始处理庞大的维基百科数据转储，将其转换为文章页面并对文件进行抽样：
- en: '[PRE55]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, we process our sample data into an RDD containing a tuple of title and
    page context text to finally generate a DataFrame:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将我们的样本数据处理成包含标题和页面上下文文本的元组的RDD，最终生成一个DataFrame：
- en: '[PRE56]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We now transform the text column of the DataFrame into raw words using Spark''s
    `RegexTokenizer` for each Wikipedia page:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用Spark的`RegexTokenizer`将DataFrame的文本列转换为原始单词，以处理每个维基百科页面：
- en: '[PRE57]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The next step is to filter raw words by removing all stop words from the tokens:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是通过从标记中删除所有停用词来过滤原始单词：
- en: '[PRE58]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We generate term counts for the filtered tokens by using Spark''s `CountVectorizer`
    class, resulting in a new DataFrame containing the column features:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过使用Spark的`CountVectorizer`类为过滤后的标记生成术语计数，从而生成包含特征列的新DataFrame：
- en: '[PRE59]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The "MinDF" specifies the minimum number of different document terms that must
    appear in order to be included in the vocabulary.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '"MinDF"指定必须出现的不同文档术语的最小数量，才能包含在词汇表中。'
- en: 'We now invoke Spark''s LDA class to generate topics and the distributions of
    tokens to topics:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们调用Spark的LDA类来生成主题和标记到主题的分布：
- en: '[PRE60]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The "K" refers to how many topics and "MaxIter" maximum iterations to execute.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '"K"指的是主题数量，"MaxIter"指的是执行的最大迭代次数。'
- en: 'We finally describe the top five generated topics and display:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们描述了生成的前五个主题并显示：
- en: '[PRE61]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![](img/00282.jpeg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00282.jpeg)'
- en: 'Now display, topics and terms associated with them:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在显示，与它们相关的主题和术语：
- en: '[PRE62]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The console output will be as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台输出将如下所示：
- en: '![](img/00283.gif)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00283.gif)'
- en: 'We close the program by stopping the SparkContext:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止SparkContext来关闭程序：
- en: '[PRE63]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: How it works...
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We began with loading the dump of Wikipedia articles and parsed the page text
    into tokens using Hadoop XML leveraging streaming facilities API. The feature
    extraction process utilized several classes to set up the final processing by
    the LDA class, letting the tokens flow from Spark's `RegexTokenize`, `StopwordsRemover`,
    and `HashingTF`. Once we had the term frequencies, the data was passed to the
    LDA class for clustering the articles together under several topics.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载了维基百科文章的转储，并使用Hadoop XML利用流式处理API将页面文本解析为标记。特征提取过程利用了几个类来设置最终由LDA类进行处理，让标记从Spark的`RegexTokenize`，`StopwordsRemover`和`HashingTF`中流出。一旦我们有了词频，数据就被传递给LDA类，以便将文章在几个主题下进行聚类。
- en: 'The Hadoop XML tools and several other required dependencies can be found at:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop XML工具和其他一些必需的依赖项可以在以下位置找到：
- en: '`bliki-core-3.0.19.jar`: [http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bliki-core-3.0.19.jar`: [http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)'
- en: '`cloud9-2.0.1.jar`: [http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar](http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cloud9-2.0.1.jar`: [http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar](http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar)'
- en: '`hadoop-streaming-2.7.4.jar`: [http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar](http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar)'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hadoop-streaming-2.7.4.jar`: [http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar](http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar)'
- en: '`lucene-snowball-3.0.3.jar`: [http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar](http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lucene-snowball-3.0.3.jar`: [http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar](http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar)'
- en: There's more...
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Please see the recipe LDA to classify documents and text into topics in [Chapter
    8](part0401.html#BUDHI0-4d291c9fed174a6992fd24938c2f9c77), *Unsupervised Clustering
    with Apache Spark 2.0* for a more detailed explanation of the LDA algorithm itself.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[第8章](part0401.html#BUDHI0-4d291c9fed174a6992fd24938c2f9c77)中的LDA配方，了解更多关于LDA算法本身的详细解释。*Apache
    Spark 2.0无监督聚类*
- en: The following white paper from the *Journal of Machine Learning Research (JMLR)*
    provides a comprehensive treatment for those who would like to do an extensive
    analysis. It is a well written paper, and a person with a basic background in
    stat and math should be able to follow it without any problems.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 来自*机器学习研究杂志（JMLR）*的以下白皮书为那些希望进行深入分析的人提供了全面的处理。这是一篇写得很好的论文，具有基本统计和数学背景的人应该能够毫无问题地理解。
- en: Refer to the [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) link
    for more details of JMLR; an alternative link is [https://www.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/BleiNgJordan2003.pdf](https://www.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/BleiNgJordan2003.pdf).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 有关JMLR的更多详细信息，请参阅[http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)链接；另一个链接是[https://www.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/BleiNgJordan2003.pdf](https://www.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/BleiNgJordan2003.pdf)。
- en: See also
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还可以参考
- en: Documentation for constructor is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)找到
- en: Documentation for LDAModel is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDAModel的文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)找到
- en: 'See also Spark''s Scala API documentation for the following:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以参考Spark的Scala API文档：
- en: DistributedLDAModel
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DistributedLDAModel
- en: EMLDAOptimizer
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMLDAOptimizer
- en: LDAOptimizer
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDAOptimizer
- en: LocalLDAModel
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LocalLDAModel
- en: OnlineLDAOptimizer
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OnlineLDAOptimizer
